<FrameworkSwitchCourse {fw} />

# Models

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}

μ΄λ² μ„Ήμ…μ—μ„λ” λ¨λΈμ„ μƒμ„±ν•κ³  μ‚¬μ©ν•λ” λ°©λ²•μ— λ€ν•΄ μμ„Έν μ•μ•„λ³΄κ² μµλ‹λ‹¤. μ²΄ν¬ν¬μΈνΈμ—μ„ λ¨λΈμ„ μΈμ¤ν„΄μ¤ν™”ν•λ” λ° μ μ©ν• `AutoModel` ν΄λμ¤λ¥Ό μ‚¬μ©ν•  κ²ƒμ…λ‹λ‹¤.

μ΄ `AutoModel` ν΄λμ¤μ™€ κ΄€λ ¨ ν΄λμ¤λ“¤μ€ μ‹¤μ λ΅ λΌμ΄λΈλ¬λ¦¬μ— μλ” λ‹¤μ–‘ν• λ¨λΈλ“¤μ„ κ°μ‹Έκ³  μλ” κ°„λ‹¨ν• λνΌμ…λ‹λ‹¤. μ΄ λνΌλ” μ²΄ν¬ν¬μΈνΈμ— μ ν•©ν• λ¨λΈ μ•„ν‚¤ν…μ²λ¥Ό μλ™μΌλ΅ μ¶”μΈ΅ν•κ³ , μ΄ μ•„ν‚¤ν…μ²λ¥Ό κ°€μ§„ λ¨λΈμ„ μΈμ¤ν„΄μ¤ν™”ν•λ” κ²ƒλ„ λ‘λ‘ν•κ² μ²λ¦¬ν•©λ‹λ‹¤.

{:else}
μ΄λ² μ„Ήμ…μ—μ„λ” λ¨λΈμ„ μƒμ„±ν•κ³  μ‚¬μ©ν•λ” λ°©λ²•μ— λ€ν•΄ μμ„Έν μ•μ•„λ³΄κ² μµλ‹λ‹¤. μ²΄ν¬ν¬μΈνΈμ—μ„ λ¨λΈμ„ μΈμ¤ν„΄μ¤ν™”ν•λ” λ° μ μ©ν• `TFAutoModel` ν΄λμ¤λ¥Ό μ‚¬μ©ν•  κ²ƒμ…λ‹λ‹¤.

μ΄ `TFAutoModel` ν΄λμ¤μ™€ κ΄€λ ¨ ν΄λμ¤λ“¤μ€ μ‹¤μ λ΅ λΌμ΄λΈλ¬λ¦¬μ— μλ” λ‹¤μ–‘ν• λ¨λΈλ“¤μ„ κ°μ‹Έκ³  μλ” κ°„λ‹¨ν• λνΌμ…λ‹λ‹¤. μ΄ λνΌλ” μ²΄ν¬ν¬μΈνΈμ— μ ν•©ν• λ¨λΈ μ•„ν‚¤ν…μ²λ¥Ό μλ™μΌλ΅ μ¶”μΈ΅ν•κ³ , μ΄ μ•„ν‚¤ν…μ²λ¥Ό κ°€μ§„ λ¨λΈμ„ μΈμ¤ν„΄μ¤ν™”ν•λ” κ²ƒλ„ λ‘λ‘ν•κ² μ²λ¦¬ν•©λ‹λ‹¤.

{/if}

ν•μ§€λ§, λ§μ•½ λ¨λΈμ μ•„ν‚¤ν…μ²λ¥Ό μ§μ ‘ μ •μν•κ³  μ‹¶λ‹¤λ©΄, ν•΄λ‹Ή λ¨λΈμ ν΄λμ¤λ¥Ό μ‚¬μ©ν•  μ μμµλ‹λ‹¤. BERT λ¨λΈμ„ μλ΅ λ“¤μ–΄λ³΄κ² μµλ‹λ‹¤.

## Creating a Transformer (Transformer μƒμ„±ν•κΈ°)

BERT λ¨λΈμ„ μ΄κΈ°ν™” ν•κΈ° μ„ν•΄μ„λ” λ¨Όμ € λ¨λΈμ ν™κ²½μ„¤μ •μ„ λ΅λ“ν•΄μ•Ό ν•©λ‚λ‹¤.

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```
{/if}

μ΄ ν™κ²½μ„¤μ •μ€ λ¨λΈμ„ κµ¬μ¶•ν•λ”λ° μ‚¬μ©λλ” λ§μ€ μ†μ„±λ“¤μ„ ν¬ν•¨ν•κ³  μμµλ‹λ‹¤:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

μ•„μ§ μ΄ μ†μ„±λ“¤μ΄ λ¬΄μ—‡μ„ μλ―Έν•λ”μ§€λ” λ¨λ¥΄κ² μ§€λ§, λ‡λ‡μ€ μµμ™ν•  κ²ƒμ…λ‹λ‹¤: `hidden_size` μ†μ„±μ€ `hidden_states` λ²΅ν„°μ ν¬κΈ°λ¥Ό μ •μν•κ³ , `num_hidden_layers`λ” Transformer λ¨λΈμ΄ κ°€μ§€κ³  μλ” λ μ΄μ–΄μ μλ¥Ό μ •μν•©λ‹λ‹¤.

### Different loading methods (λ‹¤λ¥Έ λ΅λ”© λ°©λ²•)

λ¬΄μ‘μ„ κ°’μ„ ν†µν• κΈ°λ³Έ ν™κ²½μ„¤μ •μΌλ΅ λ¨λΈ μƒμ„±

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}

μ΄ λ¨λΈμ€ λ¬΄μ‘μ„λ΅ μ΄κΈ°ν™”λμ–΄ μκΈ° λ•λ¬Έμ—, μ•„μ§μ€ μ•„λ¬΄λ° μ μ©ν• μ •λ³΄λ¥Ό ν¬ν•¨ν•κ³  μμ§€ μ•μµλ‹λ‹¤. μ΄ λ¨λΈμ„ ν›λ ¨μ‹ν‚¤κΈ° μ„ν•΄μ„λ”, λ¨Όμ € ν›λ ¨ λ°μ΄ν„°λ¥Ό μ¤€λΉ„ν•΄μ•Ό ν•©λ‹λ‹¤. μ°λ¦¬κ°€ λ°”λ‹¥λ¶€ν„° ν•™μµμ„ ν•  μ μμ§€λ§, μ΄ κ³Όμ •μ€ [Chapter 1](/course/chapter1)μ—μ„ ν™•μΈ ν–λ“―μ΄, λ§μ€ μ‹κ°„κ³Ό λ§μ€ λ°μ΄ν„°κ°€ ν•„μ”ν•λ©°, ν•™μµ ν™κ²½μ—λ„ ν° μν–¥μ„ λ―ΈμΉ©λ‹λ‹¤.

μ΄λ¬ν• λ¶ν•„μ”ν• μ¤‘λ³µλ λ…Έλ ¥μ„ ν”Όν•κΈ° μ„ν•΄μ„, μ΄λ―Έ ν›λ ¨λ λ¨λΈμ„ κ³µμ ν•κ³  μ¬μ‚¬μ©ν•  μ μμ–΄μ•Ό ν•©λ‹λ‹¤.

ν›λ ¨λ Transformer λ¨λΈμ„ λ¶λ¬μ¤λ” κ²ƒμ€ λ§¤μ° κ°„λ‹¨ν•©λ‹λ‹¤ - `from_pretrained` λ©”μ†λ“λ¥Ό μ‚¬μ©ν•λ©΄ λ©λ‹λ‹¤.

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

μ΄μ „μ— λ΄¤λ“―μ΄, `BertModel` λ€μ‹  `AutoModel` ν΄λμ¤λ¥Ό μ‚¬μ©ν•  μλ„ μμµλ‹λ‹¤. μ΄μ λ¶€ν„°λ” μ²΄ν¬ν¬μΈνΈμ— λ…λ¦½μ μΈ μ½”λ“λ¥Ό μƒμ„±ν•κΈ° μ„ν•΄ `AutoModel`λ¥Ό μ‚¬μ©ν•κ² μµλ‹λ‹¤. λ§μ•½ μ½”λ“κ°€ ν• μ²΄ν¬ν¬μΈνΈμ—μ„ μ λ™μ‘ν•λ‹¤λ©΄, λ‹¤λ¥Έ μ²΄ν¬ν¬μΈνΈμ—μ„λ„ μ λ™μ‘ν•΄μ•Ό ν•©λ‹λ‹¤. μ΄λ” μ²΄ν¬ν¬μΈνΈμ μ•„ν‚¤ν…μ²κ°€ λ‹¤λ¥΄λ”λΌλ„, λΉ„μ·ν• μ‘μ—…(μλ¥Ό λ“¤μ–΄, κ°μ„± λ¶„μ„ μ‘μ—…)μ„ μ„ν•΄ ν›λ ¨λ κ²½μ°μ—λ„ μ μ©λ©λ‹λ‹¤.

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

μ΄μ „μ— λ΄¤λ“―μ΄, `TFBertModel` λ€μ‹  `TFAutoModel` ν΄λμ¤λ¥Ό μ‚¬μ©ν•  μλ„ μμµλ‹λ‹¤. μ΄μ λ¶€ν„°λ” μ²΄ν¬ν¬μΈνΈμ— λ…λ¦½μ μΈ μ½”λ“λ¥Ό μƒμ„±ν•κΈ° μ„ν•΄ `TFAutoModel`λ¥Ό μ‚¬μ©ν•κ² μµλ‹λ‹¤. λ§μ•½ μ½”λ“κ°€ ν• μ²΄ν¬ν¬μΈνΈμ—μ„ μ λ™μ‘ν•λ‹¤λ©΄, λ‹¤λ¥Έ μ²΄ν¬ν¬μΈνΈμ—μ„λ„ μ λ™μ‘ν•΄μ•Ό ν•©λ‹λ‹¤. μ΄λ” μ²΄ν¬ν¬μΈνΈμ μ•„ν‚¤ν…μ²κ°€ λ‹¤λ¥΄λ”λΌλ„, λΉ„μ·ν• μ‘μ—…(μλ¥Ό λ“¤μ–΄, κ°μ„± λ¶„μ„ μ‘μ—…)μ„ μ„ν•΄ ν›λ ¨λ κ²½μ°μ—λ„ μ μ©λ©λ‹λ‹¤.

{/if}

μ΄ μ½”λ“ μƒν”μ—μ„λ” `BertConfig`λ¥Ό μ‚¬μ©ν•μ§€ μ•μ•κ³ , λ€μ‹  `bert-base-cased` μ‹λ³„μλ¥Ό ν†µν•΄ μ‚¬μ „ ν›λ ¨λ λ¨λΈμ„ λ¶λ¬μ™”μµλ‹λ‹¤. μ΄λ” BERTμ μ €μλ“¤μ΄ μ§μ ‘ ν›λ ¨μ‹ν‚¨ μ²΄ν¬ν¬μΈνΈμ…λ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ [λ¨λΈ μΉ΄λ“](https://huggingface.co/bert-base-cased)μ—μ„ ν™•μΈν•  μ μμµλ‹λ‹¤.

μ΄ λ¨λΈμ€ μ²΄ν¬ν¬μΈνΈμ λ¨λ“  κ°€μ¤‘μΉλ΅ μ΄κΈ°ν™”λμ—μµλ‹λ‹¤. μ΄ λ¨λΈμ€ μ²΄ν¬ν¬μΈνΈμ—μ„ ν›λ ¨λ μ‘μ—…μ— λ€ν•΄ μ§μ ‘ μ¶”λ΅ μ— μ‚¬μ©ν•  μ μμΌλ©°, μƒλ΅μ΄ μ‘μ—…μ— λ€ν•΄ λ―Έμ„Έ μ΅°μ •ν•  μλ„ μμµλ‹λ‹¤. μ‚¬μ „ ν›λ ¨λ κ°€μ¤‘μΉλ΅λ¶€ν„° ν•™μµμ„ μ§„ν–‰ν•λ©΄, λΉ μƒνƒμ—μ„ ν›λ ¨μ„ μ‹μ‘ν•λ” κ²ƒλ³΄λ‹¤ λΉ λ¥΄κ² μΆ‹μ€ κ²°κ³Όλ¥Ό μ–»μ„ μ μμµλ‹λ‹¤.

λ¨λΈμ„ λ¶λ¬μ¤λ” λ λ‹¤λ¥Έ λ°©λ²•μ€ `from_pretrained()` λ©”μ„λ“λ¥Ό μ‚¬μ©ν•λ” κ²ƒμ…λ‹λ‹¤. μ΄ λ©”μ„λ“λ” μ²΄ν¬ν¬μΈνΈλ¥Ό λ‹¤μ΄λ΅λ“ν•κ³ , μΊμ‹μ— μ €μ¥ν•©λ‹λ‹¤(μ΄ν›„ `from_pretrained()` λ©”μ„λ“λ¥Ό νΈμ¶ν•  λ• λ‹¤μ‹ λ‹¤μ΄λ΅λ“ν•μ§€ μ•μµλ‹λ‹¤). μΊμ‹ ν΄λ”λ” κΈ°λ³Έμ μΌλ΅ *~/.cache/huggingface/transformers*μ— μ €μ¥λ©λ‹λ‹¤. μΊμ‹ ν΄λ”λ¥Ό μ‚¬μ©μ μ •μν•λ ¤λ©΄ `HF_HOME` ν™κ²½ λ³€μλ¥Ό μ„¤μ •ν•λ©΄ λ©λ‹λ‹¤.

λ¨λΈμ„ λ¶λ¬μ¤λ” μ‹λ³„μλ” BERT μ•„ν‚¤ν…μ²μ™€ νΈν™λλ” κ²½μ° λ¨λΈ ν—λΈμ λ¨λ“  λ¨λΈμ μ‹λ³„μκ°€ λ  μ μμµλ‹λ‹¤. BERT μ²΄ν¬ν¬μΈνΈμ μ „μ²΄ λ©λ΅μ€ [μ—¬κΈ°](https://huggingface.co/models?filter=bert)μ—μ„ ν™•μΈν•  μ μμµλ‹λ‹¤.

### Saving methods (μ €μ¥ λ°©λ²•)

λ¨λΈμ„ μ €μ¥ν•λ” λ°©λ²•μ€ λ¶λ¬μ¤λ” λ°©λ²•μ²λΌ μ‰½μµλ‹λ‹¤. `save_pretrained()` λ©”μ„λ“λ¥Ό μ‚¬μ©ν•λ©΄ λ©λ‹λ‹¤. μ΄ λ©”μ„λ“λ” `from_pretrained()` λ©”μ„λ“μ™€ μ μ‚¬ν•©λ‹λ‹¤.

```py
model.save_pretrained("directory_on_my_computer")
```

μ΄λ” 2κ°€μ§€ νμΌμ„ μ €μ¥ν•κ² λ©λ‹λ‹¤:

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

*config.json* νμΌμ€ λ¨λΈ μ•„ν‚¤ν…μ²λ¥Ό κµ¬μ¶•ν•λ” λ° ν•„μ”ν• μ†μ„±μ„ μ•λ ¤μ¤λ‹λ‹¤. μ΄ νμΌμ—λ” μ²΄ν¬ν¬μΈνΈκ°€ μ–΄λ””μ—μ„ μƒμ„±λμ—λ”μ§€, λ§μ§€λ§‰μΌλ΅ μ²΄ν¬ν¬μΈνΈλ¥Ό μ €μ¥ν•  λ• μ‚¬μ©ν• π¤— Transformers λ²„μ „ λ“±μ λ©”νƒ€λ°μ΄ν„°λ„ ν¬ν•¨λμ–΄ μμµλ‹λ‹¤.

{#if fw === 'pt'}
The *pytorch_model.bin* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.

{:else}
The *tf_model.h5* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.

{/if}

## Using a Transformer model for inference (Transformer λ¨λΈμ„ μ¶”λ΅ μ— μ‚¬μ©ν•κΈ°)

Now that you know how to load and save a model, let's try using it to make some predictions. Transformer models can only process numbers β€” numbers that the tokenizer generates. But before we discuss tokenizers, let's explore what inputs the model accepts.

μ΄μ  λ¨λΈμ„ λ¶λ¬μ¤κ³  μ €μ¥ν•λ” λ°©λ²•μ„ μ•μ•μΌλ‹, λ¨λΈμ„ μ‚¬μ©ν•μ—¬ μμΈ΅μ„ λ§λ“¤μ–΄ λ³΄κ² μµλ‹λ‹¤. Transformer λ¨λΈμ€ ν† ν¬λ‚μ΄μ €κ°€ μƒμ„±ν•λ” μ«μλ§ μ²λ¦¬ν•  μ μμµλ‹λ‹¤. κ·Έλ¬λ‚ ν† ν¬λ‚μ΄μ €μ— λ€ν•΄ λ…Όμν•κΈ° μ „μ— λ¨λΈμ΄ λ°›λ” μ…λ ¥μ— λ€ν•΄ μ•μ•„λ³΄κ² μµλ‹λ‹¤.

ν† ν¬λ‚μ΄μ €λ” μ…λ ¥μ„ μ μ ν• ν”„λ μ„μ›ν¬μ ν…μ„λ΅ λ³€ν™ν•  μ μμ§€λ§, μ΄ν•΄λ„λ¥Ό λ†’μ΄κΈ° μ„ν•΄ λ¨λΈμ— μ…λ ¥μ„ λ³΄λ‚΄κΈ° μ „ λ¬΄μ—‡μ„ λ°λ“μ‹ ν•΄μ•Ό ν•λ”μ§€ κ°„λ‹¨ν μ‚΄ν΄λ³΄κ² μµλ‹λ‹¤.

μ°λ¦¬κ°€ μ—¬λ¬ μ‹ν€€μ¤λ“¤μ΄ μλ‹¤κ³  κ°€μ •ν•΄ λ΄…μ‹λ‹¤:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

ν† ν¬λ‚μ΄μ €λ” μ΄λ¥Ό λ‹¨μ–΄ μΈλ±μ¤λ΅ λ³€ν™ν•©λ‹λ‹¤. μ΄λ¥Ό *input IDs*λΌκ³  ν•©λ‹λ‹¤. κ° μ‹ν€€μ¤λ” μ΄μ  μ«μ λ©λ΅μ…λ‹λ‹¤! κ²°κ³Όλ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

μ΄λ” μΈμ½”λ”©λ μ‹ν€€μ¤μ λ©λ΅μ…λ‹λ‹¤. ν…μ„λ” μ •μ‚¬κ°ν• λ¨μ–‘λ§ λ°›μ„ μ μμµλ‹λ‹¤ (ν–‰λ ¬μ„ μƒκ°ν•΄ λ³΄μ„Έμ”). μ΄ "λ°°μ—΄"μ€ μ΄λ―Έ μ •μ‚¬κ°ν• λ¨μ–‘μ΄λ―€λ΅ ν…μ„λ΅ λ³€ν™ν•λ” κ²ƒμ€ μ‰½μµλ‹λ‹¤:

This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This "array" is already of rectangular shape, so converting it to a tensor is easy:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### Using the tensors as inputs to the model (ν…μ„λ¥Ό λ¨λΈμ μ…λ ¥μΌλ΅ μ‚¬μ©ν•κΈ°)

λ¨λΈμ ν…μ„λ¥Ό μ‚¬μ©ν•λ” κ²ƒμ€ λ§¤μ° κ°„λ‹¨ν•©λ‹λ‹¤. λ¨λΈμ— μ…λ ¥μ„ λ„£κΈ°λ§ ν•λ©΄ λ©λ‹λ‹¤:

```py
output = model(model_inputs)
```

λ¨λΈμ΄ λ‹¤μ–‘ν• μ–΄κ·λ¨ΌνΈλ¥Ό λ°›λ” μ¤‘μ—, μ…λ ¥μ€ input IDs λ§ ν•„μ”ν•©λ‹λ‹¤. λ‚λ¨Έμ§€ μ–΄κ·λ¨ΌνΈλ“¤μ€ μ–Έμ  ν•„μ”ν•μ§€, μ–΄λ–¤ μ—­ν• μ„ ν•λ”μ§€λ” λ‚μ¤‘μ— μ„¤λ…ν•κ² μµλ‹λ‹¤. λ¨Όμ € ν† ν¬λ‚μ΄μ €μ— λ€ν•΄ μΆ€ λ” μμ„Έν μ•μ•„λ³΄κ² μµλ‹λ‹¤.