<FrameworkSwitchCourse {fw} />

# Äáº±ng sau pipeline

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
ÄÃ¢y lÃ  pháº§n Ä‘áº§u tiÃªn cÃ³ ná»™i dung hÆ¡i khÃ¡c má»™t chÃºt tÃ¹y thuá»™c vÃ o viá»‡c báº¡n sá»­ dá»¥ng PyTorch hay TensorFlow. Chuyá»ƒn Ä‘á»•i cÃ´ng táº¯c trÃªn Ä‘áº§u tiÃªu Ä‘á» Ä‘á»ƒ chá»n ná»n táº£ng báº¡n thÃ­ch!
</Tip>

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

HÃ£y báº¯t Ä‘áº§u vá»›i má»™t vÃ­ dá»¥ hoÃ n chá»‰nh, cÃ¹ng xem nhá»¯ng gÃ¬ xáº£y ra phÃ­a sau khi chÃºng tÃ´i thá»±c thi Ä‘oáº¡n mÃ£ sau trong [ChÆ°Æ¡ng 1](/course/chapter1):

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

vÃ  thu Ä‘Æ°á»£c:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 1](/course/chapter1), pipeline nÃ y nhÃ³m ba bÆ°á»›c láº¡i vá»›i nhau: tiá»n xá»­ lÃ½, Ä‘Æ°a cÃ¡c Ä‘áº§u vÃ o qua mÃ´ hÃ¬nh vÃ  háº­u xá»­ lÃ½:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
</div>

HÃ£y cÃ¹ng Ä‘i qua tá»«ng pháº§n nÃ y.

## Tiá»n xá»­ lÃ½ vá»›i má»™t tokenizer

Giá»‘ng nhÆ° cÃ¡c máº¡ng nÆ¡-ron khÃ¡c, cÃ¡c mÃ´ hÃ¬nh Transformers khÃ´ng thá»ƒ xá»­ lÃ½ trá»±c tiáº¿p vÄƒn báº£n thÃ´, vÃ¬ váº­y bÆ°á»›c Ä‘áº§u tiÃªn trong quy trÃ¬nh cá»§a chÃºng ta lÃ  chuyá»ƒn cÃ¡c Ä‘áº§u vÃ o vÄƒn báº£n thÃ nh dáº¡ng sá»‘ mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng ta sá»­ dá»¥ng *tokenizer*, hÃ m sáº½ chá»‹u trÃ¡ch nhiá»‡m vá»:

- TÃ¡ch Ä‘áº§u vÃ o thÃ nh cÃ¡c tá»«, tá»« phá»¥, hoáº·c kÃ½ hiá»‡u (nhÆ° dáº¥u cháº¥m cÃ¢u) Ä‘Æ°á»£c gá»i lÃ  *tokens*
- Ãnh xáº¡ má»—i token thÃ nh má»™t sá»‘ nguyÃªn
- ThÃªm Ä‘áº§u vÃ o bá»• sung cÃ³ thá»ƒ há»¯u Ã­ch cho mÃ´ hÃ¬nh

Táº¥t cáº£ quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ nÃ y cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n giá»‘ng há»‡t nhÆ° khi mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, vÃ¬ váº­y trÆ°á»›c tiÃªn chÃºng ta cáº§n táº£i xuá»‘ng thÃ´ng tin Ä‘Ã³ tá»« [Model Hub](https://huggingface.co/models). Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng lá»›p `AutoTokenizer` vÃ  phÆ°Æ¡ng thá»©c `from_pretrained()` cá»§a nÃ³. Sá»­ dá»¥ng tÃªn checkpoint mÃ´ hÃ¬nh cá»§a chÃºng ta, nÃ³ sáº½ tá»± Ä‘á»™ng tÃ¬m náº¡p dá»¯ liá»‡u Ä‘Æ°á»£c liÃªn káº¿t vá»›i tokenizer cá»§a mÃ´ hÃ¬nh vÃ  lÆ°u vÃ o bá»™ nhá»› cache (vÃ¬ váº­y nÃ³ chá»‰ Ä‘Æ°á»£c táº£i xuá»‘ng láº§n Ä‘áº§u tiÃªn báº¡n cháº¡y mÃ£ bÃªn dÆ°á»›i).

VÃ¬ checkpoint máº·c Ä‘á»‹nh cá»§a `sentiment-analysis` lÃ  `distilbert-base-unsased-finetuned-sst-2-english` (báº¡n cÃ³ thá»ƒ xem tháº» mÃ´ hÃ¬nh cá»§a nÃ³ [táº¡i Ä‘Ã¢y](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), chÃºng ta cháº¡y nhÆ° sau:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

Khi cÃ³ tokenizer rá»“i, chÃºng ta cÃ³ thá»ƒ truyá»n trá»±c tiáº¿p cÃ¡c cÃ¢u cá»§a mÃ¬nh vÃ o bÃªn trong vÃ  nháº­n láº¡i má»™t tá»« Ä‘iá»ƒn Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ cung cáº¥p cho mÃ´ hÃ¬nh! Viá»‡c duy nháº¥t cáº§n lÃ m lÃ  chuyá»ƒn Ä‘á»•i danh sÃ¡ch cÃ¡c ID Ä‘áº§u vÃ o thÃ nh cÃ¡c tensor.

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng ğŸ¤— Transformers mÃ  khÃ´ng pháº£i lo láº¯ng vá» khung ML nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng phÃ­a dÆ°á»›i; nÃ³ cÃ³ thá»ƒ lÃ  PyTorch hoáº·c TensorFlow hoáº·c Flax Ä‘á»‘i vá»›i má»™t sá»‘ mÃ´ hÃ¬nh. Tuy nhiÃªn, cÃ¡c mÃ´ hÃ¬nh Transformer chá»‰ cháº¥p nháº­n *tensor* lÃ m Ä‘áº§u vÃ o. Náº¿u Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn báº¡n nghe vá» tensor, báº¡n cÃ³ thá»ƒ nghÄ© chÃºng nhÆ° lÃ  máº£ng NumPy. Máº£ng NumPy cÃ³ thá»ƒ lÃ  giÃ¡ trá»‹ vÃ´ hÆ°á»›ng (0D), vectÆ¡ (1D), ma tráº­n (2D) hoáº·c cÃ³ nhiá»u kÃ­ch thÆ°á»›c hÆ¡n. NÃ³ thá»±c sá»± lÃ  má»™t tensor; CÃ¡c tensor cá»§a cÃ¡c khung ML khÃ¡c hoáº¡t Ä‘á»™ng tÆ°Æ¡ng tá»± vÃ  thÆ°á»ng khá»Ÿi táº¡o Ä‘Æ¡n giáº£n nhÆ° cÃ¡c máº£ng NumPy.

Äá»ƒ chá»‰ Ä‘á»‹nh loáº¡i tensors mÃ  chÃºng ta muá»‘n tráº£ vá» (PyTorch, TensorFlow hoáº·c thuáº§n NumPy), ta sá»­ dá»¥ng tham sá»‘ `return_tensors`:

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

Äá»«ng lo láº¯ng vá» padding (Ä‘á»‡m) vÃ  truncation (cáº¯t bá»›t) vá»™i; chÃºng tÃ´i sáº½ giáº£i thÃ­ch nhá»¯ng Ä‘iá»u Ä‘Ã³ sau. Nhá»¯ng Ä‘iá»u chÃ­nh cáº§n nhá»› á»Ÿ Ä‘Ã¢y lÃ  báº¡n cÃ³ thá»ƒ chuyá»ƒn má»™t cÃ¢u hoáº·c má»™t danh sÃ¡ch cÃ¡c cÃ¢u, cÅ©ng nhÆ° chá»‰ Ä‘á»‹nh loáº¡i tensors báº¡n muá»‘n láº¥y láº¡i (náº¿u khÃ´ng cÃ³ loáº¡i nÃ o Ä‘Æ°á»£c truyá»n vÃ o, máº·c Ä‘á»‹nh báº¡n sáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ tráº£ vá» lÃ  má»™t danh sÃ¡ch).

{#if fw === 'pt'}

ÄÃ¢y lÃ  káº¿t quáº£ tÆ°Æ¡ng á»©ng tensor PyTorch:

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

ÄÃ¢y lÃ  káº¿t quáº£ tÆ°Æ¡ng á»©ng tensor Tensorflow:

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

Báº£n thÃ¢n káº¿t quáº£ Ä‘áº§u ra lÃ  má»™t tá»« Ä‘iá»ƒn cÃ³ chá»©a hai khÃ³a, `input_ids` vÃ  `attention_mask`. `input_ids` chá»©a hai hÃ ng sá»‘ nguyÃªn (má»™t cho má»—i cÃ¢u) lÃ  sá»‘ nháº­n dáº¡ng duy nháº¥t cá»§a token trong má»—i cÃ¢u. ChÃºng tÃ´i sáº½ giáº£i thÃ­ch `attention_mask` lÃ  gÃ¬ á»Ÿ pháº§n sau cá»§a chÆ°Æ¡ng nÃ y.

## Äi qua mÃ´ hÃ¬nh

{#if fw === 'pt'}
ChÃºng ta cÃ³ thá»ƒ táº£i xuá»‘ng mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a mÃ¬nh giá»‘ng nhÆ° cÃ¡ch Ä‘Ã£ lÃ m vá»›i tokenizer. ğŸ¤— Transformers cung cáº¥p má»™t lá»›p `AutoModel` cÅ©ng cÃ³ phÆ°Æ¡ng thá»©c `from_pretrained()`:

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
ChÃºng ta cÃ³ thá»ƒ táº£i xuá»‘ng mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a mÃ¬nh giá»‘ng nhÆ° cÃ¡ch Ä‘Ã£ lÃ m vá»›i tokenizer. ğŸ¤— Transformers cung cáº¥p má»™t lá»›p `TFAutoModel` cÅ©ng cÃ³ phÆ°Æ¡ng thá»©c `from_pretrained()`:

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

Trong Ä‘oáº¡n mÃ£ nÃ y, chÃºng ta Ä‘Ã£ táº£i xuá»‘ng cÃ¹ng má»™t checkpoint Ä‘Ã£ sá»­ dá»¥ng trong pipeline cá»§a mÃ¬nh trÆ°á»›c Ä‘Ã¢y (nÃ³ Ä‘Æ°á»£c lÆ°u vÃ o bá»™ nhá»› Ä‘á»‡m rá»“i) vÃ  khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh vá»›i nÃ³.

Kiáº¿n trÃºc nÃ y chá»‰ chá»©a mÃ´-Ä‘un Transformer cÆ¡ sá»Ÿ: vá»›i má»™t sá»‘ Ä‘áº§u vÃ o, nÃ³ xuáº¥t ra cÃ¡i mÃ  chÃºng ta sáº½ gá»i lÃ  *hidden states* (*tráº¡ng thÃ¡i áº©n*), cÃ²n Ä‘Æ°á»£c gá»i lÃ  *Ä‘áº·c trÆ°ng*. Äá»‘i vá»›i má»—i Ä‘áº§u vÃ o mÃ´ hÃ¬nh, chÃºng ta sáº½ truy xuáº¥t má»™t vectÆ¡ Ä‘a chiá»u Ä‘áº¡i diá»‡n cho **sá»± hiá»ƒu theo ngá»¯ cáº£nh cá»§a Ä‘áº§u vÃ o Ä‘Ã³ báº±ng mÃ´ hÃ¬nh Transformer**.

Náº¿u Ä‘iá»u nÃ y khÃ´ng há»£p lÃ½, Ä‘á»«ng lo láº¯ng vá» nÃ³. ChÃºng tÃ´i sáº½ giáº£i thÃ­ch táº¥t cáº£ sau.

Máº·c dÃ¹ nhá»¯ng tráº¡ng thÃ¡i áº©n nÃ y cÃ³ thá»ƒ tá»± há»¯u Ã­ch, nhÆ°ng chÃºng thÆ°á»ng lÃ  Ä‘áº§u vÃ o cho má»™t pháº§n khÃ¡c cá»§a mÃ´ hÃ¬nh, Ä‘Æ°á»£c gá»i lÃ  *head* (*Ä‘áº§u*). Trong [Chapter 1](/course/chapter1), cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i cÃ¹ng má»™t kiáº¿n trÃºc, nhÆ°ng má»—i tÃ¡c vá»¥ nÃ y sáº½ cÃ³ má»™t pháº§n Ä‘áº§u khÃ¡c nhau Ä‘Æ°á»£c liÃªn káº¿t vá»›i nÃ³.

### Má»™t vectÆ¡ Ä‘a chiá»u

Äáº§u ra vectÆ¡ cá»§a mÃ´-Ä‘un Transformer thÆ°á»ng lá»›n vá»›i ba chiá»u:

- **KÃ­ch thÆ°á»›c batch (lÃ´)**: Sá»‘ chuá»—i Ä‘Æ°á»£c xá»­ lÃ½ táº¡i má»™t thá»i Ä‘iá»ƒm (trong vÃ­ dá»¥ cá»§a chÃºng tÃ´i lÃ  2).
- **Äá»™ dÃ i chuá»—i**: Äá»™ dÃ i biá»ƒu diá»…n sá»‘ cá»§a chuá»—i (trong vÃ­ dá»¥ cá»§a chÃºng tÃ´i lÃ  16).
- **KÃ­ch thÆ°á»›c áº©n**: KÃ­ch thÆ°á»›c vectÆ¡ cá»§a má»—i Ä‘áº§u vÃ o mÃ´ hÃ¬nh.

NÃ³ Ä‘Æ°á»£c cho lÃ  "cÃ³ sá»‘ chiá»u cao" vÃ¬ giÃ¡ trá»‹ cuá»‘i cÃ¹ng. KÃ­ch thÆ°á»›c áº©n cÃ³ thá»ƒ ráº¥t lá»›n (768 lÃ  giÃ¡ trá»‹ phá»• biáº¿n cho cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n vÃ  trong cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n, con sá»‘ nÃ y cÃ³ thá»ƒ Ä‘áº¡t tá»›i 3072 hoáº·c hÆ¡n).

CÃ³ thá»ƒ tháº¥y Ä‘iá»u nÃ y náº¿u chÃºng ta cung cáº¥p cÃ¡c Ä‘áº§u vÃ o Ä‘Ã£ xá»­ lÃ½ trÆ°á»›c cho mÃ´ hÃ¬nh cá»§a mÃ¬nh:

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

LÆ°u Ã½ ráº±ng Ä‘áº§u ra cá»§a cÃ¡c mÃ´ hÃ¬nh ğŸ¤— Transformers hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° cÃ¡c `namedtuple` hoáº·c tá»« Ä‘iá»ƒn. Báº¡n cÃ³ thá»ƒ truy cáº­p cÃ¡c pháº§n tá»­ theo thuá»™c tÃ­nh (nhÆ° chÃºng ta Ä‘Ã£ lÃ m) hoáº·c theo khÃ³a (`outputs["last_hidden_state"]`), hoáº·c tháº­m chÃ­ theo chá»‰ má»¥c náº¿u báº¡n biáº¿t chÃ­nh xÃ¡c nÆ¡i báº¡n Ä‘ang tÃ¬m kiáº¿m (`outputs[0]`).

### Äáº§u mÃ´ hÃ¬nh: Há»£p lÃ½ tá»i tá»«ng con sá»‘

CÃ¡c Ä‘áº§u mÃ´ hÃ¬nh láº¥y vector Ä‘a chiá»u cá»§a cÃ¡c tráº¡ng thÃ¡i áº©n lÃ m Ä‘áº§u vÃ o vÃ  chiáº¿u chÃºng lÃªn má»™t chiá»u khÃ¡c. ChÃºng thÆ°á»ng bao gá»“m má»™t hoáº·c má»™t vÃ i lá»›p tuyáº¿n tÃ­nh:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

Äáº§u ra cá»§a mÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c gá»­i trá»±c tiáº¿p Ä‘áº¿n Ä‘áº§u mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘Æ°á»£c xá»­ lÃ½.

Trong biá»ƒu Ä‘á»“ nÃ y, mÃ´ hÃ¬nh Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng lá»›p nhÃºng cá»§a nÃ³ vÃ  cÃ¡c lá»›p tiáº¿p theo. Lá»›p nhÃºng chuyá»ƒn Ä‘á»•i má»—i ID trong Ä‘áº§u vÃ o Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh má»™t vectÆ¡ Ä‘áº¡i diá»‡n cho token Ä‘Æ°á»£c liÃªn káº¿t. CÃ¡c lá»›p tiáº¿p theo thao tÃ¡c cÃ¡c vectÆ¡ Ä‘Ã³ báº±ng cÃ¡ch sá»­ dá»¥ng cÆ¡ cháº¿ chÃº Ã½ Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n cuá»‘i cÃ¹ng cá»§a cÃ¡c cÃ¢u.

CÃ³ nhiá»u kiáº¿n trÃºc khÃ¡c nhau cÃ³ sáºµn trong ğŸ¤— Transformers, vá»›i má»—i kiáº¿n trÃºc Ä‘Æ°á»£c thiáº¿t káº¿ xoay quanh má»™t tÃ¡c vá»¥ cá»¥ thá»ƒ. ÄÃ¢y lÃ  danh sÃ¡ch khÃ´ng Ä‘áº§y Ä‘á»§:

- `*Model` (truy xuáº¥t cÃ¡c tráº¡ng thÃ¡i áº©n)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- and others ğŸ¤—

{#if fw === 'pt'}
Vá»›i vÃ­ dá»¥ cá»§a mÃ¬nh, chÃºng ta sáº½ cáº§n má»™t mÃ´ hÃ¬nh cÃ³ Ä‘áº§u phÃ¢n loáº¡i tuáº§n tá»± (Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c cÃ¢u lÃ  kháº³ng Ä‘á»‹nh hoáº·c phá»§ Ä‘á»‹nh). VÃ¬ váº­y, ta sáº½ khÃ´ng sá»­ dá»¥ng lá»›p `AutoModel` mÃ  lÃ  `AutoModelForSequenceClassification`:

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
Vá»›i vÃ­ dá»¥ cá»§a mÃ¬nh, chÃºng ta sáº½ cáº§n má»™t mÃ´ hÃ¬nh cÃ³ Ä‘áº§u phÃ¢n loáº¡i tuáº§n tá»± (Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c cÃ¢u lÃ  kháº³ng Ä‘á»‹nh hoáº·c phá»§ Ä‘á»‹nh). VÃ¬ váº­y, ta sáº½ khÃ´ng sá»­ dá»¥ng lá»›p `TFAutoModel` mÃ  lÃ  `TFAutoModelForSequenceClassification`:

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

Giá» thÃ¬ náº¿u chÃºng ta nhÃ¬n vÃ o hÃ¬nh dáº¡ng cÃ¡c Ä‘áº§u vÃ o cá»§a mÃ¬nh, kÃ­ch thÆ°á»›c sáº½ tháº¥p hÆ¡n nhiá»u: Ä‘áº§u mÃ´ hÃ¬nh láº¥y cÃ¡c vectÆ¡ Ä‘a chiá»u mÃ  chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã¢y vÃ  xuáº¥t ra cÃ¡c vectÆ¡ cÃ³ chá»©a hai giÃ¡ trá»‹ (má»—i giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng má»™t nhÃ£n):

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

VÃ¬ chÃºng ta chá»‰ cÃ³ hai cÃ¢u vÃ  hai nhÃ£n, káº¿t quáº£ nháº­n Ä‘Æ°á»£c tá»« mÃ´ hÃ¬nh cá»§a chÃºng ta lÃ  dáº¡ng 2 x 2.

## Háº­u xá»­ lÃ½ Ä‘áº§u ra

CÃ¡c giÃ¡ trá»‹ chÃºng ta nháº­n Ä‘Æ°á»£c dÆ°á»›i dáº¡ng Ä‘áº§u ra tá»« mÃ´ hÃ¬nh khÃ´ng nháº¥t thiáº¿t pháº£i tá»± cÃ³ nghÄ©a. HÃ£y cÃ¹ng xem:

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

MÃ´ hÃ¬nh Ä‘Ã£ dá»± Ä‘oÃ¡n `[-1.5607, 1.6123]` cho cÃ¢u Ä‘áº§u tiÃªn vÃ  `[4.1692, -3.3464]` cho cÃ¢u thá»© hai. ÄÃ³ khÃ´ng pháº£i lÃ  xÃ¡c suáº¥t mÃ  lÃ  *logits*, Ä‘iá»ƒm sá»‘ thÃ´, chÆ°a chuáº©n hÃ³a Ä‘Æ°á»£c xuáº¥t ra bá»Ÿi lá»›p cuá»‘i cÃ¹ng cá»§a mÃ´ hÃ¬nh. Äá»ƒ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh xÃ¡c suáº¥t, chÃºng cáº§n pháº£i tráº£i qua lá»›p [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) (táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh ğŸ¤— Transformers Ä‘á»u xuáº¥t ra logits, vÃ¬ hÃ m máº¥t mÃ¡t cho viá»‡c huáº¥n luyá»‡n thÆ°á»ng sáº½ káº¿t há»£p hÃ m kÃ­ch hoáº¡t cuá»‘i cÃ¹ng, cháº³ng háº¡n nhÆ° SoftMax, vá»›i hÃ m máº¥t mÃ¡t thá»±c táº¿, cháº³ng háº¡n nhÆ° entropy chÃ©o):

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng mÃ´ hÃ¬nh Ä‘Ã£ dá»± Ä‘oÃ¡n `[0.0402, 0.9598]` cho cÃ¢u Ä‘áº§u tiÃªn vÃ  `[0.9995, 0.0005]` cho cÃ¢u thá»© hai. ÄÃ¢y lÃ  nhá»¯ng Ä‘iá»ƒm xÃ¡c suáº¥t dá»… nháº­n biáº¿t.

Äá»ƒ láº¥y cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng vá»›i tá»«ng vá»‹ trÃ­, chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra thuá»™c tÃ­nh `id2label` cá»§a cáº¥u hÃ¬nh mÃ´ hÃ¬nh (tÃ¬m hiá»ƒu thÃªm vá» Ä‘iá»u nÃ y trong pháº§n tiáº¿p theo):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ káº¿t luáº­n ráº±ng mÃ´ hÃ¬nh Ä‘Ã£ dá»± Ä‘oÃ¡n nhÆ° sau:

- CÃ¢u Ä‘áº§u tiÃªn: TIÃŠU Cá»°C: 0,0402, TÃCH Cá»°C: 0,9598
- CÃ¢u thá»© hai: TIÃŠU Cá»°C: 0,9995, TÃCH Cá»°C: 0,0005

ChÃºng tÃ´i Ä‘Ã£ tÃ¡i táº¡o thÃ nh cÃ´ng ba bÆ°á»›c cá»§a quy trÃ¬nh: tiá»n xá»­ lÃ½ báº±ng tokenizers, Ä‘Æ°a Ä‘áº§u vÃ o qua mÃ´ hÃ¬nh vÃ  háº­u xá»­ lÃ½! Giá» thÃ¬ chÃºng ta hÃ£y dÃ nh má»™t chÃºt thá»i gian Ä‘á»ƒ Ä‘i sÃ¢u hÆ¡n vÃ o tá»«ng bÆ°á»›c Ä‘Ã³.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Chá»n hai (hoáº·c nhiá»u) vÄƒn báº£n cá»§a riÃªng báº¡n vÃ  cháº¡y chÃºng thÃ´ng qua `sentiment-analysis`. Sau Ä‘Ã³, tá»± mÃ¬nh láº·p láº¡i cÃ¡c bÆ°á»›c báº¡n Ä‘Ã£ tháº¥y á»Ÿ Ä‘Ã¢y vÃ  kiá»ƒm tra xem báº¡n cÃ³ thu Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± khÃ´ng!

</Tip>
