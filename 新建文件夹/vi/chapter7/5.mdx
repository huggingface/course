<FrameworkSwitchCourse {fw} />

# TÃ³m táº¯t

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section5_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section5_tf.ipynb"},
]} />

{/if}

Trong pháº§n nÃ y, chÃºng ta sáº½ xem xÃ©t cÃ¡ch cÃ¡c mÃ´ hÃ¬nh Transformer cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cÃ´ Ä‘á»ng cÃ¡c tÃ i liá»‡u dÃ i thÃ nh cÃ¡c báº£n tÃ³m táº¯t, má»™t tÃ¡c vá»¥ Ä‘Æ°á»£c gá»i lÃ  _text summarization_ hay _tÃ³m táº¯t vÄƒn báº£n_. ÄÃ¢y lÃ  má»™t trong nhá»¯ng tÃ¡c vá»¥ NLP thÃ¡ch thá»©c nháº¥t vÃ¬ nÃ³ Ä‘Ã²i há»i nhiá»u kháº£ nÄƒng, cháº³ng háº¡n nhÆ° hiá»ƒu cÃ¡c Ä‘oáº¡n vÄƒn dÃ i vÃ  táº¡o ra vÄƒn báº£n máº¡ch láº¡c náº¯m báº¯t cÃ¡c chá»§ Ä‘á» chÃ­nh trong tÃ i liá»‡u. Tuy nhiÃªn, khi Ä‘Æ°á»£c thá»±c hiá»‡n tá»‘t, tÃ³m táº¯t vÄƒn báº£n lÃ  má»™t cÃ´ng cá»¥ máº¡nh máº½ cÃ³ thá»ƒ tÄƒng tá»‘c cÃ¡c quy trÃ¬nh kinh doanh khÃ¡c nhau báº±ng cÃ¡ch giáº£m bá»›t gÃ¡nh náº·ng cho cÃ¡c chuyÃªn gia miá»n pháº£i Ä‘á»c chi tiáº¿t cÃ¡c tÃ i liá»‡u dÃ i.

<Youtube id="yHnr5Dk2zCI"/>

Máº·c dÃ¹ Ä‘Ã£ tá»“n táº¡i nhiá»u mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh khÃ¡c nhau Ä‘á»ƒ tÃ³m táº¯t trÃªn [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), háº§u háº¿t táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh nÃ y chá»‰ phÃ¹ há»£p vá»›i cÃ¡c tÃ i liá»‡u tiáº¿ng Anh. VÃ¬ váº­y, Ä‘á»ƒ táº¡o thÃªm má»™t Ä‘iá»ƒm nháº¥n trong pháº§n nÃ y, chÃºng tÃ´i sáº½ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh song ngá»¯ cho tiáº¿ng Anh vÃ  tiáº¿ng TÃ¢y Ban Nha. Äáº¿n cuá»‘i pháº§n nÃ y, báº¡n sáº½ cÃ³ má»™t [mÃ´ hÃ¬nh](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) cÃ³ thá»ƒ tÃ³m táº¯t cÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ Ä‘Ã¢y:

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

NhÆ° chÃºng ta sáº½ tháº¥y, nhá»¯ng báº£n tÃ³m táº¯t nÃ y ngáº¯n gá»n vÃ¬ chÃºng Ä‘Æ°á»£c há»c tá»« cÃ¡c tiÃªu Ä‘á» mÃ  khÃ¡ch hÃ ng cung cáº¥p trong cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ sáº£n pháº©m cá»§a há». HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch táº­p há»£p má»™t kho ngá»¯ liá»‡u song ngá»¯ phÃ¹ há»£p cho tÃ¡c vá»¥ nÃ y.

## Chuáº©n bá»‹ kho ngá»¯ liá»‡u Ä‘a ngÃ´n ngá»¯

ChÃºng ta sáº½ sá»­ dá»¥ng [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) Ä‘á»ƒ táº¡o trÃ¬nh tÃ³m táº¯t song ngá»¯. Táº­p tÃ i liá»‡u nÃ y bao gá»“m cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ sáº£n pháº©m cá»§a Amazon báº±ng sÃ¡u ngÃ´n ngá»¯ vÃ  thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c bá»™ phÃ¢n loáº¡i Ä‘a ngÃ´n ngá»¯. Tuy nhiÃªn, vÃ¬ má»—i bÃ i Ä‘Ã¡nh giÃ¡ Ä‘i kÃ¨m vá»›i má»™t tiÃªu Ä‘á» ngáº¯n, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c tiÃªu Ä‘á» nÃ y lÃ m nhÃ£n tÃ³m táº¯t cho mÃ´ hÃ¬nh cá»§a chÃºng ta Ä‘á»ƒ há»c! Äá»ƒ báº¯t Ä‘áº§u, hÃ£y táº£i xuá»‘ng cÃ¡c táº­p há»£p con tiáº¿ng Anh vÃ  tiáº¿ng TÃ¢y Ban Nha tá»« Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, Ä‘á»‘i vá»›i má»—i ngÃ´n ngá»¯, cÃ³ 200,000 Ä‘Ã¡nh giÃ¡ cho pháº§n `huáº¥n luyá»‡n` vÃ  5,000 nháº­n xÃ©t cho má»—i pháº§n `kiá»ƒm Ä‘á»‹nh` vÃ  `kiá»ƒm thá»­`. ThÃ´ng tin Ä‘Ã¡nh giÃ¡ mÃ  chÃºng ta quan tÃ¢m Ä‘Æ°á»£c chá»©a trong cá»™t `review_body` vÃ  `review_title`. HÃ£y xem má»™t vÃ i vÃ­ dá»¥ báº±ng cÃ¡ch táº¡o má»™t hÃ m Ä‘Æ¡n giáº£n láº¥y má»™t máº«u ngáº«u nhiÃªn tá»« táº­p huáº¥n luyá»‡n vá»›i cÃ¡c ká»¹ thuáº­t chÃºng ta Ä‘Ã£ há»c trong [ChÆ°Æ¡ng 5](/course/chapter5):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itâ€™s job and itâ€™s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Thay Ä‘á»•i seed ngáº«u nhiÃªn trong lá»‡nh `Dataset.shuffle()` Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ khÃ¡c trong kho tÃ i liá»‡u. Náº¿u báº¡n lÃ  ngÆ°á»i nÃ³i tiáº¿ng TÃ¢y Ban Nha, hÃ£y xem má»™t sá»‘ bÃ i Ä‘Ã¡nh giÃ¡ trong `spanish_dataset` Ä‘á»ƒ xem liá»‡u cÃ¡c tiÃªu Ä‘á» cÃ³ giá»‘ng nhÆ° nhá»¯ng báº£n tÃ³m táº¯t há»£p lÃ½ hay khÃ´ng.

</Tip>

Máº«u nÃ y cho tháº¥y sá»± Ä‘a dáº¡ng cá»§a cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ mÃ  ngÆ°á»i ta thÆ°á»ng tÃ¬m tháº¥y trÃªn máº¡ng, tá»« tÃ­ch cá»±c Ä‘áº¿n tiÃªu cá»±c (vÃ  má»i thá»© á»Ÿ giá»¯a!). Máº·c dÃ¹ vÃ­ dá»¥ vá»›i tiÃªu Ä‘á» "meh" khÃ´ng nhiá»u thÃ´ng tin, nhÆ°ng cÃ¡c tiÃªu Ä‘á» khÃ¡c trÃ´ng giá»‘ng nhÆ° nhá»¯ng báº£n tÃ³m táº¯t phÃ¹ há»£p vá» báº£n thÃ¢n cÃ¡c Ä‘Ã¡nh giÃ¡. Viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tÃ³m táº¯t cho táº¥t cáº£ 400,000 bÃ i Ä‘Ã¡nh giÃ¡ sáº½ máº¥t quÃ¡ nhiá»u thá»i gian trÃªn má»™t GPU, vÃ¬ váº­y thay vÃ o Ä‘Ã³, chÃºng ta sáº½ táº­p trung vÃ o viá»‡c táº¡o tÃ³m táº¯t cho má»™t miá»n sáº£n pháº©m. Äá»ƒ biáº¿t tÃªn miá»n mÃ  chÃºng ta cÃ³ thá»ƒ chá»n, hÃ£y chuyá»ƒn Ä‘á»•i `english_dataset` thÃ nh `pandas.DataFrame` vÃ  tÃ­nh toÃ¡n sá»‘ lÆ°á»£ng Ä‘Ã¡nh giÃ¡ cho má»—i danh má»¥c sáº£n pháº©m:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Hiá»ƒn thá»‹ sá»‘ lÆ°á»£ng cho 20 sáº£n pháº©m hÃ ng Ä‘áº§u
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

CÃ¡c sáº£n pháº©m phá»• biáº¿n nháº¥t trong táº­p dá»¯ liá»‡u tiáº¿ng Anh lÃ  vá» Ä‘á»“ gia dá»¥ng, quáº§n Ã¡o vÃ  thiáº¿t bá»‹ Ä‘iá»‡n tá»­ khÃ´ng dÃ¢y. Tuy nhiÃªn, Ä‘á»ƒ gáº¯n bÃ³ vá»›i chá»§ Ä‘á» Amazon, chÃºng ta hÃ£y táº­p trung vÃ o viá»‡c tÃ³m táº¯t cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ sÃ¡ch - xÃ©t cho cÃ¹ng, Ä‘Ã¢y lÃ  nhá»¯ng gÃ¬ cÃ´ng ty Ä‘Æ°á»£c thÃ nh láº­p! ChÃºng tÃ´i cÃ³ thá»ƒ tháº¥y hai danh má»¥c sáº£n pháº©m phÃ¹ há»£p vá»›i hÃ³a Ä‘Æ¡n (`book` vÃ  `digital_ebook_purchase`), vÃ¬ váº­y, hÃ£y lá»c táº­p dá»¯ liá»‡u báº±ng cáº£ hai ngÃ´n ngá»¯ chá»‰ cho cÃ¡c sáº£n pháº©m nÃ y. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 5](/course/chapter5), hÃ m `Dataset.filter()` cho phÃ©p chÃºng ta cáº¯t má»™t táº­p dá»¯ liá»‡u ráº¥t hiá»‡u quáº£, vÃ¬ váº­y chÃºng ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh má»™t hÃ m Ä‘Æ¡n giáº£n Ä‘á»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

BÃ¢y giá» khi chÃºng ta Ã¡p dá»¥ng hÃ m nÃ y cho `english_dataset` vÃ  `spanish_dataset`, káº¿t quáº£ sáº½ chá»‰ chá»©a nhá»¯ng hÃ ng liÃªn quan Ä‘áº¿n danh má»¥c sÃ¡ch. TrÆ°á»›c khi Ã¡p dá»¥ng bá»™ lá»c, hÃ£y chuyá»ƒn Ä‘á»‹nh dáº¡ng cá»§a `english_dataset` tá»« `"pandas"` trá»Ÿ láº¡i `"arrow"`:

```python
english_dataset.reset_format()
```

Sau Ä‘Ã³, chÃºng tÃ´i cÃ³ thá»ƒ Ã¡p dá»¥ng chá»©c nÄƒng bá»™ lá»c vÃ  Ä‘á»ƒ kiá»ƒm tra má»™t máº«u Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ xem chÃºng cÃ³ thá»±c sá»± lÃ  vá» sÃ¡ch hay khÃ´ng:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

ÄÆ°á»£c rá»“i, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ khÃ´ng hoÃ n toÃ n vá» sÃ¡ch vÃ  cÃ³ thá»ƒ Ä‘á» cáº­p Ä‘áº¿n nhá»¯ng thá»© nhÆ° lá»‹ch vÃ  cÃ¡c á»©ng dá»¥ng Ä‘iá»‡n tá»­ nhÆ° OneNote. Tuy nhiÃªn, máº£ng nÃ y cÃ³ váº» thÃ­ch há»£p Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tÃ³m táº¯t. TrÆ°á»›c khi xem xÃ©t cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau phÃ¹ há»£p cho tÃ¡c vá»¥ nÃ y, chÃºng ta cÃ²n má»™t bÆ°á»›c chuáº©n bá»‹ dá»¯ liá»‡u cuá»‘i cÃ¹ng cáº§n lÃ m: káº¿t há»£p cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ báº±ng tiáº¿ng Anh vÃ  tiáº¿ng TÃ¢y Ban Nha dÆ°á»›i dáº¡ng má»™t Ä‘á»‘i tÆ°á»£ng `DatasetDict` duy nháº¥t. ğŸ¤— Datasets cung cáº¥p má»™t hÃ m `concatenate_datasets()` tiá»‡n dá»¥ng (nhÆ° tÃªn cho tháº¥y) sáº½ xáº¿p chá»“ng hai Ä‘á»‘i tÆ°á»£ng `Dataset` lÃªn trÃªn nhau. VÃ¬ váº­y, Ä‘á»ƒ táº¡o táº­p dá»¯ liá»‡u song ngá»¯ cá»§a mÃ¬nh, chÃºng ta sáº½ láº·p láº¡i tá»«ng pháº§n dá»¯ liá»‡u, ná»‘i cÃ¡c táº­p dá»¯ liá»‡u cho pháº§n Ä‘Ã³ vÃ  xÃ¡o trá»™n káº¿t quáº£ Ä‘á»ƒ Ä‘áº£m báº£o mÃ´ hÃ¬nh khÃ´ng quÃ¡ phÃ¹ há»£p vá»›i má»™t ngÃ´n ngá»¯ duy nháº¥t:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Chá»n ra má»™t vÃ i máº«u
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÃ‘ADO'
'>> Review: Me llegÃ³ el dÃ­a que tocaba, junto a otros libros que pedÃ­, pero la caja llegÃ³ en mal estado lo cual daÃ±Ã³ las esquinas de los libros porque venÃ­an sin protecciÃ³n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

ÄÃ¢y cháº¯c cháº¯n lÃ  sá»± káº¿t há»£p giá»¯a cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ báº±ng tiáº¿ng Anh vÃ  tiáº¿ng TÃ¢y Ban Nha! BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ má»™t kho tÃ i liá»‡u huáº¥n luyá»‡n, má»™t Ä‘iá»u cuá»‘i cÃ¹ng cáº§n kiá»ƒm tra lÃ  sá»± phÃ¢n bá»‘ cÃ¡c tá»« trong cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ vÃ  tiÃªu Ä‘á» cá»§a chÃºng. Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng Ä‘á»‘i vá»›i cÃ¡c tÃ¡c vá»¥ tÃ³m táº¯t, trong Ä‘Ã³ cÃ¡c tÃ³m táº¯t tham chiáº¿u ngáº¯n trong dá»¯ liá»‡u cÃ³ thá»ƒ lÃ m sai lá»‡ch mÃ´ hÃ¬nh chá»‰ xuáº¥t ra má»™t hoáº·c hai tá»« trong cÃ¡c tÃ³m táº¯t Ä‘Ã£ táº¡o. CÃ¡c biá»ƒu Ä‘á»“ bÃªn dÆ°á»›i hiá»ƒn thá»‹ cÃ¡c phÃ¢n bá»‘ tá»« vÃ  chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c tiÃªu Ä‘á» bá»‹ lá»‡ch nhiá»u vá» chá»‰ 1-2 tá»«:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng ta sáº½ lá»c ra cÃ¡c vÃ­ dá»¥ cÃ³ tiÃªu Ä‘á» ráº¥t ngáº¯n Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ táº¡o ra cÃ¡c báº£n tÃ³m táº¯t thÃº vá»‹ hÆ¡n. VÃ¬ chÃºng ta Ä‘ang xá»­ lÃ½ cÃ¡c vÄƒn báº£n tiáº¿ng Anh vÃ  tiáº¿ng TÃ¢y Ban Nha, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p thÃ´ Ä‘á»ƒ phÃ¢n chia cÃ¡c tiÃªu Ä‘á» theo dáº¥u cÃ¡ch vÃ  sau Ä‘Ã³ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p `Dataset.filter()` Ä‘Ã¡ng tin cáº­y cá»§a mÃ¬nh nhÆ° sau:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

BÃ¢y giá» chÃºng ta Ä‘Ã£ chuáº©n bá»‹ kho tÃ i liá»‡u cá»§a mÃ¬nh, hÃ£y cÃ¹ng xem má»™t vÃ i máº«u Transformer kháº£ thi mÃ  ngÆ°á»i ta cÃ³ thá»ƒ tinh chá»‰nh nÃ³!

## CÃ¡c mÃ´ hÃ¬nh cho tÃ³m táº¯t vÄƒn báº£n

Náº¿u báº¡n nghÄ© vá» nÃ³, tÃ³m táº¯t vÄƒn báº£n lÃ  má»™t loáº¡i tÃ¡c vá»¥ tÆ°Æ¡ng tá»± nhÆ° dá»‹ch mÃ¡y: chÃºng ta cÃ³ má»™t pháº§n ná»™i dung vÄƒn báº£n giá»‘ng nhÆ° má»™t bÃ i Ä‘Ã¡nh giÃ¡ mÃ  ta muá»‘n "dá»‹ch" thÃ nh má»™t phiÃªn báº£n ngáº¯n hÆ¡n Ä‘á»ƒ náº¯m báº¯t cÃ¡c tÃ­nh nÄƒng ná»•i báº­t cá»§a Ä‘áº§u vÃ o. Theo Ä‘Ã³, háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘á»ƒ tÃ³m táº¯t Ä‘á»u Ã¡p dá»¥ng kiáº¿n trÃºc bá»™ mÃ£ hÃ³a-giáº£i mÃ£ mÃ  chÃºng ta Ä‘Ã£ gáº·p láº§n Ä‘áº§u tiÃªn trong [ChÆ°Æ¡ng 1](/course/chapter1), máº·c dÃ¹ cÃ³ má»™t sá»‘ ngoáº¡i lá»‡ nhÆ° há» mÃ´ hÃ¬nh GPT cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ³m táº¯t trong cÃ i Ä‘áº·t few-shot. Báº£ng sau Ä‘Ã¢y liá»‡t kÃª má»™t sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c phá»• biáº¿n cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ tÃ³m táº¯t.

| MÃ´ hÃ¬nh Transformer | MÃ´ táº£                                                                                                                                                                                                    | Äa ngÃ´n ngá»¯? |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | Máº·c dÃ¹ Ä‘Æ°á»£c huáº¥n luyá»‡n nhÆ° má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy, báº¡n cÃ³ thá»ƒ dÃ¹ng GPT-2 Ä‘á»ƒ táº¡o ra cÃ¡c báº£n tÃ³m táº¯t báº±ng cÃ¡ch ná»‘i "TL;DR" cuá»‘i má»—i Ä‘oáº¡n Ä‘áº§u vÃ o. |      âŒ       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | Sá»­ dá»¥ng hÃ m má»¥c tiÃªu huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c cÃ¢u bá»‹ áº©n Ä‘i trong vÄƒn báº£n Ä‘a cÃ¢u. CÆ¡ cháº¿ nÃ y gáº§n vá»›i tÃ³m táº¯t hÆ¡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ vanilla vÃ  Ä‘áº¡t Ä‘iá»ƒm cao hÆ¡n trÃªn cÃ¡c chuáº©n phá»• biáº¿n. |      âŒ       |
|     [T5](https://huggingface.co/t5-base)      | Má»™t kiáº¿n trÃºc Transformer phá»• quÃ¡t táº¡o ra táº¥t cáº£ cÃ¡c tÃ¡c vá»¥ trong khung vÄƒn báº£n sang vÄƒn báº£n; vÃ­ dá»¥,Ä‘á»‹nh dáº¡ng Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh Ä‘á»ƒ tÃ³m táº¯t tÃ i liá»‡u lÃ  `summarize: ARTICLE`. |      âŒ       |
|     [mT5](https://huggingface.co/google/mt5-base)     | Má»™t phiÃªn báº£n Ä‘a ngÃ´n ngá»¯ cá»§a T5, Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn kho ngá»¯ liá»‡u Common Crawl corpus (mC4), bao gá»“m 101 ngÃ´n ngá»¯. |      âœ…       |
|    [BART](https://huggingface.co/facebook/bart-base)     | Má»™t kiáº¿n trÃºc Transformer má»›i vá»›i cáº£ bá»™ mÃ£ hÃ³a vÃ  giáº£i mÃ£ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ tÃ¡i táº¡o láº¡i Ä‘áº§u vÃ o bá»‹ phÃ¡, káº¿t há»£p cÃ¡c cÆ¡ cháº¿ huáº¥n luyá»‡n trÆ°á»›c cá»§a BERT vÃ  GPT-2.                 |      âŒ       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50) | PhiÃªn báº£n Ä‘a ngÃ´n ngá»¯ cá»§a BART, huáº¥n luyá»‡n trÆ°á»›c trÃªn 50 ngÃ´n ngá»¯.|      âœ…       |

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y tá»« báº£ng nÃ y, pháº§n lá»›n cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘á»ƒ tÃ³m táº¯t (vÃ  thá»±c sá»± lÃ  háº§u háº¿t cÃ¡c tÃ¡c vá»¥ NLP) lÃ  Ä‘Æ¡n ngá»¯. Äiá»u nÃ y tháº­t tuyá»‡t náº¿u tÃ¡c vá»¥ cá»§a báº¡n sá»­ dá»¥ng ngÃ´n ngá»¯ "nhiá»u tÃ i nguyÃªn" nhÆ° tiáº¿ng Anh hoáº·c tiáº¿ng Äá»©c, nhÆ°ng bá»›t dáº§n Ä‘á»‘i vá»›i hÃ ng nghÃ¬n ngÃ´n ngá»¯ khÃ¡c Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn kháº¯p tháº¿ giá»›i. May máº¯n thay, cÃ³ má»™t loáº¡i mÃ´ hÃ¬nh Transformer Ä‘a ngÃ´n ngá»¯, nhÆ° mT5 vÃ  mBART, ra Ä‘á»i Ä‘á»ƒ giáº£i cá»©u ta. Nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c báº±ng cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯, nhÆ°ng cÃ³ má»™t Ä‘iá»ƒm khÃ¡c biá»‡t: thay vÃ¬ huáº¥n luyá»‡n ngá»¯ liá»‡u cá»§a má»™t ngÃ´n ngá»¯, chÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ¹ng lÃºc vá» cÃ¡c vÄƒn báº£n báº±ng hÆ¡n 50 ngÃ´n ngá»¯ cÃ¹ng má»™t lÃºc!

ChÃºng ta sáº½ táº­p trung vÃ o mT5, má»™t kiáº¿n trÃºc thÃº vá»‹ dá»±a trÃªn T5 Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trong khung vÄƒn báº£n sang vÄƒn báº£n. Trong T5, má»i tÃ¡c vá»¥ NLP Ä‘Æ°á»£c xÃ¢y dá»±ng dÆ°á»›i dáº¡ng tiá»n tá»‘ nháº¯c nhÆ° `summarize:` Ä‘iá»u kiá»‡n nÃ o Ä‘á»ƒ mÃ´ hÃ¬nh Ä‘iá»u chá»‰nh vÄƒn báº£n Ä‘Æ°á»£c táº¡o thÃ nh lá»i nháº¯c. NhÆ° thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i, Ä‘iá»u nÃ y lÃ m cho T5 trá»Ÿ nÃªn cá»±c ká»³ linh hoáº¡t, vÃ¬ báº¡n cÃ³ thá»ƒ giáº£i quyáº¿t nhiá»u tÃ¡c vá»¥ chá»‰ vá»›i má»™t mÃ´ hÃ¬nh duy nháº¥t!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

mT5 khÃ´ng sá»­ dá»¥ng tiá»n tá»‘, nhÆ°ng chia sáº» pháº§n lá»›n tÃ­nh linh hoáº¡t cá»§a T5 vÃ  cÃ³ lá»£i tháº¿ lÃ  Ä‘a ngÃ´n ngá»¯. Giá» ta Ä‘Ã£ chá»n má»™t mÃ´ hÃ¬nh, hÃ£y xem xÃ©t viá»‡c chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n.


<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Khi báº¡n Ä‘Ã£ lÃ m qua pháº§n nÃ y, hÃ£y xem mT5 so vá»›i mBART tá»‘t nhÆ° tháº¿ nÃ o báº±ng cÃ¡ch tinh chá»‰nh pháº§n sau vá»›i cÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng tá»±. Äá»ƒ cÃ³ Ä‘iá»ƒm thÆ°á»Ÿng, báº¡n cÅ©ng cÃ³ thá»ƒ thá»­ tinh chá»‰nh T5 chá»‰ trÃªn cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ tiáº¿ng Anh. VÃ¬ T5 cÃ³ tiá»n tá»‘ nháº¯c Ä‘áº·c biá»‡t, báº¡n sáº½ cáº§n thÃªm  `summarize:` vÃ o trÆ°á»›c cÃ¡c máº«u Ä‘áº§u vÃ o trong cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ bÃªn dÆ°á»›i.

</Tip>

## Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

<Youtube id="1m7BerpSq8A"/>

TÃ¡c vá»¥ tiáº¿p theo cá»§a chÃºng ta lÃ  tokenize vÃ  mÃ£ hÃ³a cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ vÃ  tiÃªu Ä‘á» cá»§a chÃºng. NhÆ° thÆ°á»ng lá»‡, ta báº¯t Ä‘áº§u báº±ng cÃ¡ch táº£i tokenizer Ä‘Æ°á»£c liÃªn káº¿t vá»›i checkpoint mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. ChÃºng ta sáº½ sá»­ dá»¥ng `mt5-small` lÃ m checkpoint Ä‘á»ƒ cÃ³ thá»ƒ tinh chá»‰nh mÃ´ hÃ¬nh trong má»™t khoáº£ng thá»i gian há»£p lÃ½:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

ğŸ’¡ Trong giai Ä‘oáº¡n Ä‘áº§u cá»§a cÃ¡c dá»± Ã¡n NLP cá»§a báº¡n, má»™t phÆ°Æ¡ng phÃ¡p hay lÃ  huáº¥n luyá»‡n má»™t lá»›p cÃ¡c mÃ´ hÃ¬nh "nhá»" trÃªn má»™t máº«u dá»¯ liá»‡u nhá». Äiá»u nÃ y cho phÃ©p báº¡n gá»¡ lá»—i vÃ  láº·p láº¡i nhanh hÆ¡n Ä‘á»‘i vá»›i quy trÃ¬nh lÃ m viá»‡c Ä‘áº§u cuá»‘i. Má»™t khi báº¡n tá»± tin vÃ o káº¿t quáº£, báº¡n luÃ´n cÃ³ thá»ƒ má»Ÿ rá»™ng mÃ´ hÃ¬nh báº±ng cÃ¡ch thay Ä‘á»•i checkpoint cá»§a mÃ´ hÃ¬nh!
</Tip>

HÃ£y thá»­ nghiá»‡m mT5 tokenizer trÃªn má»™t vÃ­ dá»¥ nhá»:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

á» Ä‘Ã¢y, chÃºng ta cÃ³ thá»ƒ tháº¥y `input_ids` vÃ  `attention_mask` mÃ  chÃºng ta Ä‘Ã£ gáº·p trong cÃ¡c thá»­ nghiá»‡m tinh chá»‰nh Ä‘áº§u tiÃªn cá»§a chÃºng ta trong [ChÆ°Æ¡ng 3](/course/chapter3). HÃ£y giáº£i mÃ£ cÃ¡c ID Ä‘áº§u vÃ o nÃ y báº±ng hÃ m `convert_ids_to_tokens()` cá»§a tokenizer Ä‘á»ƒ xem chÃºng ta Ä‘ang xá»­ lÃ½ loáº¡i tokenizer nÃ o:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']
```

KÃ½ tá»± Unicode Ä‘áº·c biá»‡t `â–` vÃ  token cuá»‘i chuá»—i `</s>` cho biáº¿t ta Ä‘ang xá»­ lÃ½ SentencePiece tokenizer, dá»±a trÃªn thuáº­t toÃ¡n phÃ¢n Ä‘oáº¡n Unigram Ä‘Æ°á»£c tháº£o luáº­n trong [ChÆ°Æ¡ng 6](/course/chapter6). Unigram Ä‘áº·c biá»‡t há»¯u Ã­ch Ä‘á»‘i vá»›i kho ngá»¯ liá»‡u Ä‘a ngÃ´n ngá»¯ vÃ¬ nÃ³ cho phÃ©p SentencePiece báº¥t kháº£ tri vá» dáº¥u, dáº¥u cÃ¢u vÃ  thá»±c táº¿ lÃ  nhiá»u ngÃ´n ngá»¯, nhÆ° tiáº¿ng Nháº­t, khÃ´ng cÃ³ kÃ½ tá»± khoáº£ng tráº¯ng.

Äá»ƒ mÃ£ hÃ³a kho tÃ i liá»‡u cá»§a mÃ¬nh, chÃºng ta pháº£i xá»­ lÃ½ má»™t cÃ¡ch tÃ­nh táº¿ vá»›i tÃ³m táº¯t: bá»Ÿi vÃ¬ cÃ¡c nhÃ£n cÅ©ng lÃ  vÄƒn báº£n, cÃ³ thá»ƒ chÃºng vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c ngá»¯ cáº£nh tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng ta cáº§n Ã¡p dá»¥ng viá»‡c cáº¯t bá»›t cho cáº£ cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ vÃ  tiÃªu Ä‘á» cá»§a chÃºng Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng truyá»n cÃ¡c Ä‘áº§u vÃ o quÃ¡ dÃ i cho mÃ´ hÃ¬nh cá»§a mÃ¬nh. CÃ¡c tokenizer trong ğŸ¤— Transformers cung cáº¥p má»™t hÃ m `as_target_tokenizer()` tiá»‡n lá»£i cho phÃ©p báº¡n mÃ£ hÃ³a cÃ¡c nhÃ£n song song vá»›i cÃ¡c Ä‘áº§u vÃ o. Äiá»u nÃ y thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng trÃ¬nh quáº£n lÃ½ ngá»¯ cáº£nh bÃªn trong má»™t chá»©c nÄƒng tiá»n xá»­ lÃ½, trÆ°á»›c tiÃªn mÃ£ hÃ³a cÃ¡c Ä‘áº§u vÃ o, sau Ä‘Ã³ mÃ£ hÃ³a cÃ¡c nhÃ£n dÆ°á»›i dáº¡ng má»™t cá»™t riÃªng biá»‡t. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ vá» má»™t hÃ m nhÆ° váº­y cho mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"], max_length=max_input_length, truncation=True
    )
    # Thiáº¿t láº­p tokenizer cho nhÃ£n
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["review_title"], max_length=max_target_length, truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

HÃ£y xem qua Ä‘oáº¡n mÃ£ nÃ y Ä‘á»ƒ hiá»ƒu Ä‘iá»u gÃ¬ Ä‘ang xáº£y ra. Äiá»u Ä‘áº§u tiÃªn chÃºng ta Ä‘Ã£ lÃ m lÃ  xÃ¡c Ä‘á»‹nh cÃ¡c giÃ¡ trá»‹ cho `max_input_length` vÃ  `max_target_length`, Ä‘áº·t giá»›i háº¡n trÃªn cho thá»i lÆ°á»£ng cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ vÃ  tiÃªu Ä‘á» cá»§a chÃºng. VÃ¬ ná»™i dung Ä‘Ã¡nh giÃ¡ thÆ°á»ng lá»›n hÆ¡n nhiá»u so vá»›i tiÃªu Ä‘á», chÃºng ta Ä‘Ã£ Ä‘iá»u chá»‰nh cÃ¡c giÃ¡ trá»‹ nÃ y cho phÃ¹ há»£p. Sau Ä‘Ã³, trong chÃ­nh `preprocess_function()`, chÃºng ta cÃ³ thá»ƒ tháº¥y cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c tokenize Ä‘áº§u tiÃªn, tiáº¿p theo lÃ  cÃ¡c tiÃªu Ä‘á» vá»›i `as_target_tokenizer()`.

Vá»›i `preprocess_function()`, viá»‡c tokenize toÃ n bá»™ kho dá»¯ liá»‡u báº±ng hÃ m `Dataset.map()` tiá»‡n dá»¥ng mÃ  chÃºng ta Ä‘Ã£ sá»­ dá»¥ng rá»™ng rÃ£i trong suá»‘t khÃ³a há»c nÃ y lÃ  má»™t váº¥n Ä‘á» Ä‘Æ¡n giáº£n:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

BÃ¢y giá» kho dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trÆ°á»›c, chÃºng ta hÃ£y xem xÃ©t má»™t sá»‘ chá»‰ sá»‘ thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ³m táº¯t. NhÆ° chÃºng ta sáº½ tháº¥y, khÃ´ng cÃ³ giáº£i phÃ¡p dá»… dÃ ng vÃ  nhanh chÃ³ng khi nÃ³i Ä‘áº¿n viá»‡c Ä‘o lÆ°á»ng cháº¥t lÆ°á»£ng cá»§a vÄƒn báº£n do mÃ¡y táº¡o ra.

<Tip>

ğŸ’¡ Báº¡n cÃ³ thá»ƒ nháº­n tháº¥y ráº±ng chÃºng ta Ä‘Ã£ sá»­ dá»¥ng `batched=True` trong hÃ m`Dataset.map()` á»Ÿ trÃªn. Äiá»u nÃ y mÃ£ hÃ³a cÃ¡c máº«u theo lÃ´ 1,000 (máº·c Ä‘á»‹nh) vÃ  cho phÃ©p báº¡n sá»­ dá»¥ng kháº£ nÄƒng Ä‘a luá»“ng cá»§a cÃ¡c bá»™ tokenizer nhanh trong ğŸ¤— Transformers. Náº¿u cÃ³ thá»ƒ, hÃ£y thá»­ sá»­ dá»¥ng `batched=True` Ä‘á»ƒ táº­n dá»¥ng tá»‘i Ä‘a quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ cá»§a báº¡n!

</Tip>

## ThÆ°á»›c Ä‘o cho tÃ³m táº¯t vÄƒn báº£n

<Youtube id="TMshhnrEXlg"/>

So vá»›i háº§u háº¿t cÃ¡c tÃ¡c vá»¥ khÃ¡c mÃ  chÃºng ta Ä‘Ã£ Ä‘á» cáº­p trong khÃ³a há»c nÃ y, viá»‡c Ä‘o lÆ°á»ng hiá»‡u suáº¥t cá»§a cÃ¡c tÃ¡c vá»¥ táº¡o vÄƒn báº£n nhÆ° tÃ³m táº¯t hoáº·c dá»‹ch khÃ´ng Ä‘Æ¡n giáº£n báº±ng. VÃ­ dá»¥: Ä‘Æ°á»£c Ä‘Æ°a ra má»™t bÃ i Ä‘Ã¡nh giÃ¡ nhÆ° "I loved reading the Hunger Games", cÃ³ nhiá»u báº£n tÃ³m táº¯t há»£p lá»‡, cháº³ng háº¡n nhÆ° "I loved the Hunger Games" hoáº·c "Hunger Games is a great read". RÃµ rÃ ng, viá»‡c Ã¡p dá»¥ng má»™t sá»‘ loáº¡i Ä‘á»‘i sÃ¡nh chÃ­nh xÃ¡c nÃ o Ä‘Ã³ giá»¯a báº£n tÃ³m táº¯t Ä‘Æ°á»£c táº¡o vÃ  nhÃ£n khÃ´ng pháº£i lÃ  má»™t giáº£i phÃ¡p tá»‘t - ngay cáº£ con ngÆ°á»i cÅ©ng sáº½ Ä‘Ã¡nh giÃ¡ tháº¥p hÆ¡n theo má»™t chá»‰ sá»‘ nhÆ° váº­y, bá»Ÿi vÃ¬ táº¥t cáº£ chÃºng ta Ä‘á»u cÃ³ phong cÃ¡ch viáº¿t riÃªng cá»§a mÃ¬nh.

Äá»ƒ tÃ³m táº¯t, má»™t trong nhá»¯ng chá»‰ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n nháº¥t lÃ  [Ä‘iá»ƒm ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (viáº¿t táº¯t cá»§a Recall-Oriented Understudy for Gisting Assessment). Ã tÆ°á»Ÿng cÆ¡ báº£n Ä‘áº±ng sau thÆ°á»›c Ä‘o nÃ y lÃ  so sÃ¡nh má»™t báº£n tÃ³m táº¯t Ä‘Ã£ táº¡o vá»›i má»™t táº­p há»£p cÃ¡c báº£n tÃ³m táº¯t tham chiáº¿u thÆ°á»ng do con ngÆ°á»i táº¡o ra. Äá»ƒ lÃ m cho Ä‘iá»u nÃ y chÃ­nh xÃ¡c hÆ¡n, giáº£ sá»­ chÃºng ta muá»‘n so sÃ¡nh hai báº£n tÃ³m táº¯t sau:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

Má»™t cÃ¡ch Ä‘á»ƒ cÃ³ thá»ƒ so sÃ¡nh chÃºng lÃ  Ä‘áº¿m sá»‘ tá»« trÃ¹ng láº·p, trong trÆ°á»ng há»£p nÃ y sáº½ lÃ  6. Tuy nhiÃªn, Ä‘iá»u nÃ y hÆ¡i thÃ´, vÃ¬ váº­y thay vÃ o Ä‘Ã³ ROUGE dá»±a trÃªn viá»‡c tÃ­nh toÃ¡n Ä‘iá»ƒm sá»‘ _precision_  vÃ  _recall_ cho sá»± trÃ¹ng láº·p.

<Tip>

ğŸ™‹ Äá»«ng lo láº¯ng náº¿u Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn báº¡n nghe nÃ³i vá» precision vÃ  recall - chÃºng ta sáº½ cÃ¹ng nhau Ä‘iá»ƒm qua má»™t sá»‘ vÃ­ dá»¥ rÃµ rÃ ng Ä‘á»ƒ lÃ m rÃµ táº¥t cáº£. CÃ¡c chá»‰ sá»‘ nÃ y thÆ°á»ng gáº·p trong cÃ¡c tÃ¡c vá»¥ phÃ¢n loáº¡i, vÃ¬ váº­y náº¿u báº¡n muá»‘n hiá»ƒu cÃ¡ch xÃ¡c Ä‘á»‹nh precision vÃ  recall trong ngá»¯ cáº£nh Ä‘Ã³, chÃºng tÃ´i khuyÃªn báº¡n nÃªn xem [hÆ°á»›ng dáº«n `scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html).

</Tip>

Äá»‘i vá»›i ROUGE, recall Ä‘o lÆ°á»ng má»©c Ä‘á»™ tÃ³m táº¯t tham chiáº¿u thu Ä‘Æ°á»£c tá»« cÃ¡i Ä‘Ã£ táº¡o. Náº¿u chÃºng ta chá»‰ so sÃ¡nh cÃ¡c tá»«, recall cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c sau:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

Äá»‘i vá»›i vÃ­ dá»¥ Ä‘Æ¡n giáº£n á»Ÿ trÃªn, cÃ´ng thá»©c nÃ y cho phÃ©p nhá»› hoÃ n háº£o lÃ  6/6 = 1; tá»©c lÃ  táº¥t cáº£ cÃ¡c tá»« trong báº£n tÃ³m táº¯t tham chiáº¿u Ä‘Ã£ Ä‘Æ°á»£c táº¡o ra bá»Ÿi mÃ´ hÃ¬nh. Äiá»u nÃ y nghe cÃ³ váº» tuyá»‡t vá»i, nhÆ°ng hÃ£y tÆ°á»Ÿng tÆ°á»£ng náº¿u báº£n tÃ³m táº¯t Ä‘Æ°á»£c táº¡o cá»§a chÃºng ta lÃ  "I really really loved reading the Hunger Games all night". Äiá»u nÃ y cÅ©ng sáº½ cÃ³ má»™t recall hoÃ n háº£o, nhÆ°ng Ä‘Æ°á»£c cho lÃ  má»™t báº£n tÃ³m táº¯t tá»“i tá»‡ hÆ¡n vÃ¬ nÃ³ dÃ i dÃ²ng. Äá»ƒ Ä‘á»‘i phÃ³ vá»›i nhá»¯ng tÃ¬nh huá»‘ng nÃ y, chÃºng ta cÅ©ng tÃ­nh toÃ¡n Ä‘á»™ precision, trong ngá»¯ cáº£nh ROUGE Ä‘o lÆ°á»ng má»©c Ä‘á»™ liÃªn quan cá»§a báº£n tÃ³m táº¯t Ä‘Ã£ táº¡o:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

Ãp dá»¥ng Ä‘iá»u nÃ y cho báº£n tÃ³m táº¯t dÃ i dÃ²ng cá»§a chÃºng ta sáº½ cho precision lÃ  6/10 = 0.6, tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i precision 6/7 = 0.86 thu Ä‘Æ°á»£c báº±ng phÆ°Æ¡ng phÃ¡p ngáº¯n hÆ¡n cá»§a mÃ¬nh. Trong thá»±c táº¿, cáº£ precision vÃ  recall thÆ°á»ng Ä‘Æ°á»£c tÃ­nh toÃ¡n, vÃ  sau Ä‘Ã³ Ä‘iá»ƒm F1 (giÃ¡ trá»‹ trung bÃ¬nh hÃ i hÃ²a cá»§a precision vÃ  recall) Ä‘Æ°á»£c bÃ¡o cÃ¡o. ChÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n viá»‡c nÃ y dá»… dÃ ng trong ğŸ¤— Datasets báº±ng cÃ¡ch cÃ i Ä‘áº·t gÃ³i `rouge_score` trÆ°á»›c:

```py
!pip install rouge_score
```

vÃ  sau Ä‘Ã³ táº£i chá»‰ sá»‘ ROUGE nhÆ° sau:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

Sau Ä‘Ã³ ta cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m `rouge_score.compute()` Ä‘á»ƒ tÃ­nh táº¥t cáº£ cÃ¡c chá»‰ sá»‘ trong má»™t láº§n:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

ChÃ , cÃ³ ráº¥t nhiá»u thÃ´ng tin trong Ä‘áº§u ra Ä‘Ã³ - táº¥t cáº£ cÃ³ nghÄ©a lÃ  gÃ¬? Äáº§u tiÃªn, ğŸ¤— Datasets thá»±c sá»± tÃ­nh toÃ¡n khoáº£ng tin cáº­y cho precision, recall vÃ  F1 score; Ä‘Ã¢y lÃ  cÃ¡c thuá»™c tÃ­nh `low`, `mid` vÃ  `high` mÃ  báº¡n cÃ³ thá»ƒ xem á»Ÿ Ä‘Ã¢y. HÆ¡n ná»¯a, ğŸ¤— Datasets tÃ­nh toÃ¡n nhiá»u Ä‘iá»ƒm ROUGE khÃ¡c nhau dá»±a trÃªn cÃ¡c loáº¡i vÄƒn báº£n chi tiáº¿t khÃ¡c nhau khi so sÃ¡nh cÃ¡c tÃ³m táº¯t Ä‘Ã£ táº¡o vÃ  tham chiáº¿u. Biáº¿n thá»ƒ `rouge1` lÃ  sá»± chá»“ng chÃ©o cá»§a cÃ¡c khá»‘i Ä‘Æ¡n - Ä‘Ã¢y chá»‰ lÃ  má»™t cÃ¡ch nÃ³i hoa má»¹ Ä‘á»ƒ nÃ³i vá» sá»± chá»“ng chÃ©o cá»§a cÃ¡c tá»« vÃ  chÃ­nh xÃ¡c lÃ  sá»‘ liá»‡u mÃ  chÃºng ta Ä‘Ã£ tháº£o luáº­n á»Ÿ trÃªn. Äá»ƒ xÃ¡c minh Ä‘iá»u nÃ y, chÃºng ta hÃ£y láº¥y ra giÃ¡ trá»‹ `mid` Ä‘iá»ƒm sá»‘ cá»§a mÃ¬nh:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

Tuyá»‡t vá»i, precision vÃ  recall láº¡i khá»›p vá»›i nhau! CÃ²n nhá»¯ng Ä‘iá»ƒm ROUGE khÃ¡c thÃ¬ sao? `rouge2` Ä‘o lÆ°á»ng sá»± trÃ¹ng láº·p giá»¯a cÃ¡c bigram (hÃ£y nghÄ© ráº±ng Ä‘Ã³ lÃ  sá»± chá»“ng chÃ©o cá»§a cÃ¡c cáº·p tá»«), trong khi `rougeL` vÃ  `rougeLsum` Ä‘o lÆ°á»ng cÃ¡c chuá»—i tá»« phÃ¹ há»£p dÃ i nháº¥t báº±ng cÃ¡ch tÃ¬m kiáº¿m cÃ¡c chuá»—i con chung dÃ i nháº¥t trong cÃ¡c báº£n tÃ³m táº¯t Ä‘Æ°á»£c táº¡o vÃ  tham chiáº¿u. "sum" trong `rougeLsum` Ä‘á» cáº­p Ä‘áº¿n thá»±c táº¿ lÃ  chá»‰ sá»‘ nÃ y Ä‘Æ°á»£c tÃ­nh trÃªn toÃ n bá»™ báº£n tÃ³m táº¯t, trong khi `rougeL` Ä‘Æ°á»£c tÃ­nh lÃ  giÃ¡ trá»‹ trung bÃ¬nh trÃªn cÃ¡c cÃ¢u riÃªng láº».

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Táº¡o vÃ­ dá»¥ cá»§a riÃªng báº¡n vá» báº£n tÃ³m táº¯t Ä‘Æ°á»£c táº¡o vÃ  tham kháº£o, vÃ  xem liá»‡u Ä‘iá»ƒm káº¿t quáº£ ROUGE cÃ³ giá»‘ng vá»›i tÃ­nh toÃ¡n thá»§ cÃ´ng dá»±a trÃªn cÃ¡c cÃ´ng thá»©c vá» precision vÃ  recall hay khÃ´ng. Äá»ƒ cÃ³ Ä‘iá»ƒm thÆ°á»Ÿng, hÃ£y chia vÄƒn báº£n thÃ nh bigrams vÃ  so sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c vÃ  thu há»“i cho chá»‰ sá»‘ `rouge2`.

</Tip>

ChÃºng tÃ´i sáº½ sá»­ dá»¥ng cÃ¡c Ä‘iá»ƒm ROUGE nÃ y Ä‘á»ƒ theo dÃµi hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh, nhÆ°ng trÆ°á»›c khi lÃ m Ä‘iá»u Ä‘Ã³, hÃ£y lÃ m Ä‘iá»u mÃ  má»i ngÆ°á»i thá»±c hÃ nh NLP giá»i nÃªn lÃ m: táº¡o má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ máº¡nh máº½ nhÆ°ng Ä‘Æ¡n giáº£n!

### Táº¡o má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ máº¡nh máº½

MÃ´t mÃ´ hÃ¬nh cÆ¡ sá»Ÿ cho tÃ³m táº¯t vÄƒn báº£n, Ä‘Ã³ lÃ  chá»‰ cáº§n láº¥y ba cÃ¢u Ä‘áº§u tiÃªn cá»§a má»™t bÃ i bÃ¡o, thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  _lead-3_ hay _3 bÃ i Ä‘áº§u_. ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c dáº¥u cháº¥m Ä‘á»ƒ theo dÃµi ranh giá»›i cÃ¢u, nhÆ°ng Ä‘iá»u nÃ y sáº½ khÃ´ng thÃ nh cÃ´ng Ä‘á»‘i vá»›i cÃ¡c tá»« viáº¿t táº¯t nhÆ° "U.S." hoáº·c "U.N." - vÃ¬ váº­y thay vÃ o Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng thÆ° viá»‡n `nltk`, bao gá»“m má»™t thuáº­t toÃ¡n tá»‘t hÆ¡n Ä‘á»ƒ xá»­ lÃ½ nhá»¯ng trÆ°á»ng há»£p nÃ y. Báº¡n cÃ³ thá»ƒ cÃ i Ä‘áº·t gÃ³i báº±ng cÃ¡ch sá»­ dá»¥ng `pip` nhÆ° sau:

```python
!pip install nltk
```

vÃ  sau Ä‘Ã³ táº£i cÃ¡c quy táº¯c dáº¥u cÃ¢u:

```python
import nltk

nltk.download("punkt")
```

Tiáº¿p theo, chÃºng ta nháº­p tokenizer cÃ¢u tá»« `nltk` vÃ  táº¡o má»™t hÃ m Ä‘Æ¡n giáº£n Ä‘á»ƒ trÃ­ch xuáº¥t ba cÃ¢u Ä‘áº§u tiÃªn trong má»™t bÃ i Ä‘Ã¡nh giÃ¡. Quy Æ°á»›c trong pháº§n tÃ³m táº¯t vÄƒn báº£n lÃ  tÃ¡ch tá»«ng pháº§n tÃ³m táº¯t báº±ng má»™t dÃ²ng má»›i, vÃ¬ váº­y hÃ£y bao gá»“m pháº§n nÃ y vÃ  kiá»ƒm tra nÃ³ trÃªn má»™t vÃ­ dá»¥ huáº¥n luyá»‡n:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

Äiá»u nÃ y cÃ³ váº» hiá»‡u quáº£, vÃ¬ váº­y bÃ¢y giá» chÃºng ta hÃ£y triá»ƒn khai má»™t hÃ m trÃ­ch xuáº¥t cÃ¡c "tÃ³m táº¯t" nÃ y tá»« táº­p dá»¯ liá»‡u vÃ  tÃ­nh toÃ¡n Ä‘iá»ƒm ROUGE cho mÃ´ hÃ¬nh cÆ¡ sá»Ÿ:

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m nÃ y Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘iá»ƒm ROUGE trÃªn táº­p kiá»ƒm Ä‘á»‹nh vÃ  kiá»ƒm tra chÃºng má»™t chÃºt báº±ng cÃ¡ch sá»­ dá»¥ng Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng Ä‘iá»ƒm `rouge2` tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i pháº§n cÃ²n láº¡i; Ä‘iá»u nÃ y cÃ³ thá»ƒ pháº£n Ã¡nh thá»±c táº¿ lÃ  tiÃªu Ä‘á» bÃ i Ä‘Ã¡nh giÃ¡ thÆ°á»ng ngáº¯n gá»n vÃ  do Ä‘Ã³, mÃ´ hÃ¬nh cÆ¡ sá»Ÿ cá»§a 3 bÃ i Ä‘áº§u quÃ¡ dÃ i dÃ²ng. BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ má»™t cÆ¡ sá»Ÿ tá»‘t Ä‘á»ƒ lÃ m viá»‡c, hÃ£y chuyá»ƒn sá»± chÃº Ã½ cá»§a chÃºng ta sang viá»‡c tinh chá»‰nh mT5!

{#if fw === 'pt'}

## Tinh chá»‰nh mT5 vá»›i API `Trainer`

Viá»‡c tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘á»ƒ tÃ³m táº¯t ráº¥t giá»‘ng vá»›i cÃ¡c tÃ¡c vá»¥ khÃ¡c mÃ  chÃºng ta Ä‘Ã£ Ä‘á» cáº­p trong chÆ°Æ¡ng nÃ y. Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  táº£i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tá»« checkpoint `mt5-small`. VÃ¬ tÃ³m táº¯t lÃ  má»™t tÃ¡c vá»¥ chuá»—i sang chuá»—i, chÃºng ta cÃ³ thá»ƒ táº£i mÃ´ hÃ¬nh báº±ng lá»›p `AutoModelForSeq2SeqLM`, lá»›p nÃ y sáº½ tá»± Ä‘á»™ng táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› cache cÃ¡c trá»ng sá»‘:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## Tinh chá»‰nh mT5 vá»›i Keras

Viá»‡c tinh chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘á»ƒ tÃ³m táº¯t ráº¥t giá»‘ng vá»›i cÃ¡c tÃ¡c vá»¥ khÃ¡c mÃ  chÃºng ta Ä‘Ã£ Ä‘á» cáº­p trong chÆ°Æ¡ng nÃ y. Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  táº£i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tá»« checkpoint `mt5-small`. VÃ¬ tÃ³m táº¯t lÃ  má»™t tÃ¡c vá»¥ chuá»—i sang chuá»—i, chÃºng ta cÃ³ thá»ƒ táº£i mÃ´ hÃ¬nh báº±ng lá»›p `TFAutoModelForSeq2SeqLM`, lá»›p nÃ y sáº½ tá»± Ä‘á»™ng táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› cache cÃ¡c trá»ng sá»‘:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

ğŸ’¡ Náº¿u báº¡n Ä‘ang tá»± há»i táº¡i sao báº¡n khÃ´ng tháº¥y báº¥t ká»³ cáº£nh bÃ¡o nÃ o vá» viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh trÃªn má»™t tÃ¡c vá»¥ phÃ­a sau, Ä‘Ã³ lÃ  bá»Ÿi vÃ¬ Ä‘á»‘i vá»›i cÃ¡c tÃ¡c vá»¥ chuá»—i sang chuá»—i, chÃºng ta giá»¯ táº¥t cáº£ cÃ¡c trá»ng sá»‘ cá»§a máº¡ng. So sÃ¡nh mÃ´ hÃ¬nh nÃ y vá»›i mÃ´ hÃ¬nh phÃ¢n loáº¡i vÄƒn báº£n trong [ChÆ°Æ¡ng 3](/course/chapter3), trong Ä‘Ã³ pháº§n Ä‘áº§u cá»§a mÃ´ hÃ¬nh Ä‘á»‹nh sáºµn Ä‘Æ°á»£c thay tháº¿ báº±ng má»™t máº¡ng Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn.

</Tip>

Äiá»u tiáº¿p theo chÃºng ta cáº§n lÃ m lÃ  Ä‘Äƒng nháº­p vÃ o Hugging Face Hub. Náº¿u báº¡n Ä‘ang cháº¡y Ä‘oáº¡n mÃ£ nÃ y trong notebook, báº¡n cÃ³ thá»ƒ lÃ m nhÆ° váº­y vá»›i hÃ m tiá»‡n Ã­ch sau:

```python
from huggingface_hub import notebook_login

notebook_login()
```

sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p cá»§a mÃ¬nh. NgoÃ i ra, báº¡n cÃ³ thá»ƒ cháº¡y lá»‡nh nÃ y trong thiáº¿t bá»‹ Ä‘áº§u cuá»‘i cá»§a mÃ¬nh vÃ  Ä‘Äƒng nháº­p vÃ o Ä‘Ã³:

```
huggingface-cli login
```

{#if fw === 'pt'}

ChÃºng ta sáº½ cáº§n táº¡o tÃ³m táº¯t Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm ROUGE trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. May máº¯n thay, ğŸ¤— Transformers cung cáº¥p cÃ¡c lá»›p `Seq2SeqTrainingArguments` vÃ  `Seq2SeqTrainer` chuyÃªn dá»¥ng cÃ³ thá»ƒ tá»± Ä‘á»™ng lÃ m viá»‡c nÃ y cho chÃºng ta! Äá»ƒ xem cÃ¡ch nÃ y hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o, trÆ°á»›c tiÃªn chÃºng ta hÃ£y xÃ¡c Ä‘á»‹nh cÃ¡c siÃªu tham sá»‘ vÃ  cÃ¡c tham sá»‘ khÃ¡c cho cÃ¡c thá»­ nghiá»‡m cá»§a mÃ¬nh:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Hiá»‡n thá»‹ máº¥t mÃ¡t huáº¥n luyá»‡n má»—i epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

á» Ä‘Ã¢y, tham sá»‘ `predict_with_generate` Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº·t Ä‘á»ƒ chá»‰ ra ráº±ng chÃºng ta nÃªn táº¡o cÃ¡c báº£n tÃ³m táº¯t trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ cÃ³ thá»ƒ tÃ­nh toÃ¡n Ä‘iá»ƒm ROUGE cho má»—i epoch. NhÆ° Ä‘Ã£ tháº£o luáº­n trong [ChÆ°Æ¡ng 1](/course/chapter1), bá»™ giáº£i mÃ£ thá»±c hiá»‡n luáº­n suy báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»«ng token vÃ  Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi phÆ°Æ¡ng thá»©c `generate()` cá»§a mÃ´ hÃ¬nh. Äáº·t `predict_with_generate=True` sáº½ cho `Seq2SeqTrainer` sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Ä‘Ã³ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡. ChÃºng ta cÅ©ng Ä‘Ã£ Ä‘iá»u chá»‰nh má»™t sá»‘ siÃªu tham sá»‘ máº·c Ä‘á»‹nh, nhÆ° tá»‘c Ä‘á»™ há»c, sá»‘ epoch vÃ  giáº£m trá»ng sá»‘ vÃ  chÃºng ta Ä‘Ã£ Ä‘áº·t tÃ¹y chá»n `save_total_limit` Ä‘á»ƒ chá»‰ lÆ°u tá»‘i Ä‘a 3 checkpoint trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n - Ä‘iá»u nÃ y lÃ  do ngay cáº£ phiÃªn báº£n "nhá»" cá»§a mT5 sá»­ dá»¥ng khoáº£ng má»™t GB dung lÆ°á»£ng á»• cá»©ng vÃ  chÃºng ta cÃ³ thá»ƒ tiáº¿t kiá»‡m má»™t chÃºt dung lÆ°á»£ng báº±ng cÃ¡ch giá»›i háº¡n sá»‘ lÆ°á»£ng báº£n sao ta lÆ°u.

Tham sá»‘ `push_to_hub=True` sáº½ cho phÃ©p chÃºng ta Ä‘áº©y mÃ´ hÃ¬nh vÃ o Hub sau khi huáº¥n luyá»‡n; báº¡n sáº½ tÃ¬m tháº¥y kho lÆ°u trá»¯ bÃªn dÆ°á»›i há»“ sÆ¡ ngÆ°á»i dÃ¹ng cá»§a mÃ¬nh á»Ÿ vá»‹ trÃ­ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi `output_dir`. LÆ°u Ã½ ráº±ng báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n báº±ng tham sá»‘ `hub_model_id` (cá»¥ thá»ƒ lÃ  báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y Ä‘áº¿n má»™t tá»• chá»©c). VÃ­ dá»¥: khi chÃºng ta Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course), chÃºng ta Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` thÃ nh `Seq2SeqTrainingArguments`.

Äiá»u tiáº¿p theo chÃºng ta cáº§n lÃ m lÃ  cung cáº¥p cho ngÆ°á»i huáº¥n luyá»‡n má»™t hÃ m `compute_metrics()` Ä‘á»ƒ chÃºng ta cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a mÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Äá»ƒ tÃ³m táº¯t, Ä‘iá»u nÃ y cáº§n nhiá»u hÆ¡n lÃ  Ä‘Æ¡n giáº£n gá»i `rouge_score.compute()` trÃªn cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh, vÃ¬ chÃºng ta cáº§n _giáº£i mÃ£_ cÃ¡c káº¿t quáº£ Ä‘áº§u ra vÃ  nhÃ£n thÃ nh vÄƒn báº£n trÆ°á»›c khi chÃºng ta cÃ³ thá»ƒ tÃ­nh Ä‘iá»ƒm ROUGE. HÃ m sau thá»±c hiá»‡n chÃ­nh xÃ¡c Ä‘iá»u Ä‘Ã³ vÃ  cÅ©ng sá»­ dá»¥ng hÃ m `sent_tokenize()` tá»« `nltk` Ä‘á»ƒ tÃ¡ch cÃ¡c cÃ¢u tÃ³m táº¯t báº±ng cÃ¡c dÃ²ng má»›i:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Giáº£i mÃ£ cÃ¡c tÃ³m táº¯t Ä‘Æ°á»£c táº¡o ra thÃ nh vÄƒn báº£n
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Thay -100 vÃ o nhÃ£n vÃ¬ ta khÃ´ng thá»ƒ giáº£i mÃ£ chÃºng
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Giáº£i mÃ£ cÃ¡c tÃ³m táº¯t máº«u thÃ nh vÄƒn báº£n
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE kÃ¬ vá»ng dÃ²ng má»›i sau má»—i cÃ¢u
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # TÃ­nh Ä‘iá»ƒm ROUGE
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # TrÃ­ch xuáº¥t Ä‘iá»ƒm trung vá»‹
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

Tiáº¿p theo, chÃºng ta cáº§n pháº£i xÃ¡c Ä‘á»‹nh má»™t bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u cho tÃ¡c vá»¥ chuá»—i sang chuá»—i cá»§a chÃºng ta. VÃ¬ mT5 lÃ  mÃ´ hÃ¬nh Transformer bá»™ mÃ£ hÃ³a-giáº£i mÃ£, má»™t Ä‘iá»u tinh táº¿ khi chuáº©n bá»‹ cÃ¡c lÃ´ lÃ  trong quÃ¡ trÃ¬nh giáº£i mÃ£, chÃºng ta cáº§n chuyá»ƒn cÃ¡c nhÃ£n sang pháº£i tá»«ng nhÃ£n. Äiá»u nÃ y lÃ  báº¯t buá»™c Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng bá»™ giáº£i mÃ£ chá»‰ nhÃ¬n tháº¥y cÃ¡c nhÃ£n trÆ°á»›c Ä‘Ã³ chá»© khÃ´ng pháº£i nhÃ£n hiá»‡n táº¡i hoáº·c tÆ°Æ¡ng lai, Ä‘iá»u nÃ y sáº½ giÃºp mÃ´ hÃ¬nh dá»… dÃ ng ghi nhá»›. Äiá»u nÃ y tÆ°Æ¡ng tá»± nhÆ° cÃ¡ch Ã¡p dá»¥ng masked self-attention cho cÃ¡c Ä‘áº§u vÃ o trong má»™t tÃ¡c vá»¥ nhÆ° [mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£](/course/chapter7/6).

May máº¯n thay, ğŸ¤— Transformers cung cáº¥p má»™t bá»™ Ä‘á»‘i chiáº¿u `DataCollatorForSeq2Seq` sáº½ tá»± Ä‘á»™ng Ä‘á»‡m cÃ¡c Ä‘áº§u vÃ o vÃ  nhÃ£n cho chÃºng ta. Äá»ƒ khá»Ÿi táº¡o bá»™ Ä‘á»‘i chiáº¿u nÃ y, chÃºng ta chá»‰ cáº§n cung cáº¥p `tokenizer` vÃ  `model`:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

HÃ£y xem nhá»¯ng gÃ¬ mÃ  mÃ¡y Ä‘á»‘i chiáº¿u nÃ y táº¡o ra khi Ä‘Æ°á»£c cung cáº¥p má»™t loáº¡t cÃ¡c vÃ­ dá»¥ nhá». Äáº§u tiÃªn, chÃºng ta cáº§n xÃ³a cÃ¡c cá»™t cÃ³ chuá»—i vÃ¬ trÃ¬nh Ä‘á»‘i chiáº¿u sáº½ khÃ´ng biáº¿t cÃ¡ch chÃ¨n cÃ¡c pháº§n tá»­ nÃ y:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

VÃ¬ bá»™ Ä‘á»‘i chiáº¿u mong Ä‘á»£i má»™t danh sÃ¡ch cÃ¡c `dict`, trong Ä‘Ã³ má»—i `dict` Ä‘áº¡i diá»‡n cho má»™t vÃ­ dá»¥ duy nháº¥t trong táº­p dá»¯ liá»‡u, chÃºng ta cÅ©ng cáº§n cuá»™n dá»¯ liá»‡u thÃ nh Ä‘á»‹nh dáº¡ng mong Ä‘á»£i trÆ°á»›c khi chuyá»ƒn nÃ³ Ä‘áº¿n bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

Äiá»u chÃ­nh cáº§n chÃº Ã½ á»Ÿ Ä‘Ã¢y lÃ  máº«u Ä‘áº§u tiÃªn dÃ i hÆ¡n thá»© hai, do Ä‘Ã³, `input_ids` vÃ  `attention_mask` cá»§a máº«u thá»© hai Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‡m á»Ÿ bÃªn pháº£i báº±ng `[PAD]` (cÃ³ ID lÃ  `0`). TÆ°Æ¡ng tá»±, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng `labels` Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‡m báº±ng `-100`, Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c token Ä‘á»‡m Ä‘Æ°á»£c bá» qua bá»Ÿi hÃ m máº¥t mÃ¡t. VÃ  cuá»‘i cÃ¹ng, chÃºng ta cÃ³ thá»ƒ tháº¥y má»™t `decoder_input_ids` má»›i Ä‘Ã£ chuyá»ƒn cÃ¡c nhÃ£n sang bÃªn pháº£i báº±ng cÃ¡ch chÃ¨n `[PAD]` vÃ o má»¥c nháº­p Ä‘áº§u tiÃªn.

{#if fw === 'pt'}

Cuá»‘i cÃ¹ng thÃ¬ chÃºng ta cÅ©ng cÃ³ táº¥t cáº£ nhá»¯ng nguyÃªn liá»‡u cáº§n thiáº¿t Ä‘á»ƒ luyá»‡n táº­p! BÃ¢y giá» chÃºng ta chá»‰ cáº§n khá»Ÿi táº¡o trÃ¬nh huáº¥n luyá»‡n vá»›i cÃ¡c tham sá»‘ tiÃªu chuáº©n:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

vÃ  khá»Ÿi cháº¡y chÆ°Æ¡ng trÃ¬nh huáº¥n luyá»‡n cá»§a mÃ¬nh:

```python
trainer.train()
```

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, báº¡n sáº½ tháº¥y máº¥t mÃ¡t huáº¥n luyá»‡n giáº£m vÃ  Ä‘iá»ƒm ROUGE tÄƒng lÃªn theo tá»«ng epoch. Sau khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t, báº¡n cÃ³ thá»ƒ xem Ä‘iá»ƒm ROUGE cuá»‘i cÃ¹ng báº±ng cÃ¡ch cháº¡y `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

Tá»« Ä‘iá»ƒm sá»‘, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng mÃ´ hÃ¬nh cá»§a mÃ¬nh Ä‘Ã£ vÆ°á»£t trá»™i so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ vá»›i 3 bÃ i Ä‘áº§u tiÃªn - tháº­t tuyá»‡t! Äiá»u cuá»‘i cÃ¹ng cáº§n lÃ m lÃ  Ä‘áº©y cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh vÃ o Hub, nhÆ° sau:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

Thao tÃ¡c nÃ y sáº½ lÆ°u cÃ¡c checkpoint vÃ  cÃ¡c tá»‡p cáº¥u hÃ¬nh vÃ o `output_dir`, trÆ°á»›c khi táº£i táº¥t cáº£ cÃ¡c tá»‡p lÃªn Hub. Báº±ng cÃ¡ch chá»‰ Ä‘á»‹nh tham sá»‘ `tags`, chÃºng ta cÅ©ng Ä‘áº£m báº£o ráº±ng tiá»‡n Ã­ch con trÃªn Hub sáº½ lÃ  má»™t tiá»‡n Ã­ch con dÃ nh cho quy trÃ¬nh tÃ³m táº¯t thay vÃ¬ tiá»‡n Ã­ch táº¡o vÄƒn báº£n máº·c Ä‘á»‹nh Ä‘Æ°á»£c liÃªn káº¿t vá»›i kiáº¿n trÃºc mT5 (Ä‘á»ƒ biáº¿t thÃªm thÃ´ng tin vá» tháº» mÃ´ hÃ¬nh, hÃ£y xem tÃ i liá»‡u [ğŸ¤— Hub ](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-detined)). Äáº§u ra tá»« `trainr.push_to_hub())` lÃ  má»™t URL tá»›i hÃ m bÄƒm cam káº¿t Git, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ dá»… dÃ ng xem cÃ¡c thay Ä‘á»•i Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»‘i vá»›i kho lÆ°u trá»¯ mÃ´ hÃ¬nh!

Äá»ƒ káº¿t thÃºc pháº§n nÃ y, hÃ£y xem cÃ¡ch chÃºng ta cÅ©ng cÃ³ thá»ƒ tinh chá»‰nh mT5 báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c tÃ­nh nÄƒng cáº¥p tháº¥p do ğŸ¤— Accelerate cung cáº¥p.

{:else}

ChÃºng tÃ´i gáº§n nhÆ° Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n! ChÃºng ta chá»‰ cáº§n chuyá»ƒn Ä‘á»•i táº­p dá»¯ liá»‡u cá»§a mÃ¬nh thÃ nh `tf.data.Dataset` báº±ng cÃ¡ch sá»­ dá»¥ng trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘Ã£ xÃ¡c Ä‘á»‹nh á»Ÿ trÃªn, sau Ä‘Ã³ mÃ´ hÃ¬nh `compile()` vÃ  `fit()`. Äáº§u tiÃªn, bá»™ dá»¯ liá»‡u:

```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

BÃ¢y giá», chÃºng ta xÃ¡c Ä‘á»‹nh cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n cá»§a mÃ¬nh vÃ  biÃªn dá»‹ch:

```python
from transformers import create_optimizer
import tensorflow as tf

# Sá»‘ bÆ°á»›c huáº¥n luyá»‡n lÃ  sá»‘ lÆ°á»£ng máº«u trong táº­p dá»¯ liá»‡u, chia cho kÃ­ch thÆ°á»›c lÃ´ sau Ä‘Ã³ nhÃ¢n
# vá»›i tá»•ng sá»‘ epoch. LÆ°u Ã½ ráº±ng tf_train_dataset á»Ÿ Ä‘Ã¢y lÃ  tf.data.Dataset theo lÃ´,
# khÃ´ng pháº£i lÃ  Hugging Face Dataset ban Ä‘áº§u, vÃ¬ váº­y len() cá»§a nÃ³ vá»‘n lÃ  num_samples // batch_size.

num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Huáº¥n luyá»‡n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

And finally, we fit the model. We use a `PushToHubCallback` to save the model to the Hub after each epoch, which will allow us to use it for inference later:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

ChÃºng ta nháº­n Ä‘Æ°á»£c má»™t sá»‘ giÃ¡ trá»‹ máº¥t mÃ¡t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, nhÆ°ng thá»±c sá»± chÃºng ta muá»‘n xem cÃ¡c chá»‰ sá»‘ ROUGE mÃ  ta Ä‘Ã£ tÃ­nh toÃ¡n trÆ°á»›c Ä‘Ã³. Äá»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c sá»‘ liá»‡u Ä‘Ã³, chÃºng tÃ´i sáº½ cáº§n táº¡o káº¿t quáº£ Ä‘áº§u ra tá»« mÃ´ hÃ¬nh vÃ  chuyá»ƒn Ä‘á»•i chÃºng thÃ nh chuá»—i. HÃ£y xÃ¢y dá»±ng má»™t sá»‘ danh sÃ¡ch nhÃ£n vÃ  dá»± Ä‘oÃ¡n cho chá»‰ sá»‘ ROUGE Ä‘á»ƒ so sÃ¡nh (lÆ°u Ã½ ráº±ng náº¿u báº¡n gáº·p lá»—i nháº­p cho pháº§n nÃ y, báº¡n cÃ³ thá»ƒ cáº§n pháº£i `!pip install tqdm`):

```python
from tqdm import tqdm
import numpy as np

all_preds = []
all_labels = []
for batch in tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = batch["labels"].numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

Khi chÃºng ta cÃ³ danh sÃ¡ch cÃ¡c chuá»—i nhÃ£n vÃ  chuá»—i dá»± Ä‘oÃ¡n, viá»‡c tÃ­nh toÃ¡n Ä‘iá»ƒm ROUGE tháº­t dá»… dÃ ng:

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## Tinh chá»‰nh mT5 vá»›i ğŸ¤— Accelerate

Tinh chá»‰nh mÃ´ hÃ¬nh cá»§a chÃºng ta vá»›i ğŸ¤— Accelerate ráº¥t giá»‘ng vá»›i vÃ­ dá»¥ phÃ¢n loáº¡i vÄƒn báº£n mÃ  chÃºng ta Ä‘Ã£ gáº·p trong [ChÆ°Æ¡ng 3](/course/chapter3). Sá»± khÃ¡c biá»‡t chÃ­nh sáº½ lÃ  nhu cáº§u táº¡o tÃ³m táº¯t cá»§a chÃºng ta má»™t cÃ¡ch rÃµ rÃ ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  xÃ¡c Ä‘á»‹nh cÃ¡ch chÃºng ta tÃ­nh Ä‘iá»ƒm ROUGE (nhá»› láº¡i ráº±ng `Seq2SeqTrainer` Ä‘Ã£ lÃ m cho chÃºng ta). HÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n hai yÃªu cáº§u nÃ y trong ğŸ¤— Accelerate!

### Chuáº©n bá»‹ má»i thá»© cho huáº¥n luyá»‡n

Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  táº¡o má»™t `DataLoader` cho má»—i pháº§n tÃ¡ch cá»§a chÃºng ta. VÃ¬ cÃ¡c bá»™ lÆ°u trá»¯ dá»¯ liá»‡u PyTorch mong Ä‘á»£i hÃ ng loáº¡t cÃ¡c tensor, chÃºng ta cáº§n Ä‘áº·t Ä‘á»‹nh dáº¡ng thÃ nh `"torch"` trong bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh:

```python
tokenized_datasets.set_format("torch")
```

BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ táº­p dá»¯ liá»‡u chá»‰ bao gá»“m cÃ¡c tensor, viá»‡c tiáº¿p theo cáº§n lÃ m lÃ  khá»Ÿi táº¡o láº¡i `DataCollatorForSeq2Seq`. Äá»‘i vá»›i Ä‘iá»u nÃ y, chÃºng ta cáº§n cung cáº¥p má»™t phiÃªn báº£n má»›i cá»§a mÃ´ hÃ¬nh, vÃ¬ váº­y hÃ£y táº£i láº¡i tá»« bá»™ nhá»› cache cá»§a mÃ¬nh:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ khá»Ÿi táº¡o trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u vÃ  sá»­ dá»¥ng cÃ´ng cá»¥ nÃ y Ä‘á»ƒ xÃ¡c Ä‘á»‹nh bá»™ lÆ°u dá»¯ liá»‡u cá»§a mÃ¬nh:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

Äiá»u tiáº¿p theo cáº§n lÃ m lÃ  xÃ¡c Ä‘á»‹nh trÃ¬nh tá»‘i Æ°u hÃ³a mÃ  chÃºng ta muá»‘n sá»­ dá»¥ng. NhÆ° trong cÃ¡c vÃ­ dá»¥ khÃ¡c, chÃºng ta sáº½ sá»­ dá»¥ng `AdamW`, trÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t cho háº§u háº¿t cÃ¡c váº¥n Ä‘á»:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Cuá»‘i cÃ¹ng, chÃºng ta cung cáº¥p mÃ´ hÃ¬nh, trÃ¬nh tá»‘i Æ°u hÃ³a vÃ  bá»™ ghi dá»¯ liá»‡u cá»§a mÃ¬nh vÃ o phÆ°Æ¡ng thá»©c `accelerator.prepare()`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ Náº¿u báº¡n Ä‘ang huáº¥n luyá»‡n trÃªn TPU, báº¡n sáº½ cáº§n chuyá»ƒn táº¥t cáº£ Ä‘oáº¡n mÃ£ á»Ÿ trÃªn vÃ o má»™t hÃ m huáº¥n luyá»‡n chuyÃªn dá»¥ng. Xem [ChÆ°Æ¡ng 3(/course/chapter3) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

</Tip>

BÃ¢y giá» chÃºng ta Ä‘Ã£ chuáº©n bá»‹ cÃ¡c Ä‘á»‘i tÆ°á»£ng cá»§a mÃ¬nh, cÃ²n ba viá»‡c cáº§n lÃ m:

* XÃ¡c Ä‘á»‹nh lá»‹ch trÃ¬nh tá»‘c Ä‘á»™ há»c.
* Thá»±c hiá»‡n chá»©c nÄƒng háº­u xá»­ lÃ½ cÃ¡c báº£n tÃ³m táº¯t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.
* Táº¡o má»™t kho lÆ°u trá»¯ trÃªn Hub mÃ  ta cÃ³ thá»ƒ Ä‘áº©y mÃ´ hÃ¬nh cá»§a mÃ¬nh lÃªn Ä‘Ã³.

Äá»‘i vá»›i lá»‹ch trÃ¬nh tá»‘c Ä‘á»™ há»c, chÃºng ta sáº½ sá»­ dá»¥ng lá»‹ch trÃ¬nh tuyáº¿n tÃ­nh tiÃªu chuáº©n tá»« cÃ¡c pháº§n trÆ°á»›c:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Äá»ƒ háº­u xá»­ lÃ½, chÃºng ta cáº§n má»™t hÃ m chia cÃ¡c tÃ³m táº¯t Ä‘Ã£ táº¡o thÃ nh cÃ¡c cÃ¢u Ä‘Æ°á»£c phÃ¢n tÃ¡ch báº±ng cÃ¡c dÃ²ng má»›i. ÄÃ¢y lÃ  Ä‘á»‹nh dáº¡ng mÃ  chá»‰ sá»‘ ROUGE mong Ä‘á»£i vÃ  chÃºng ta cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng Ä‘oáº¡n mÃ£ sau:

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE kÃ¬ vá»ng dÃ²ng má»›i sau má»—i cÃ¢u
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

Äiá»u nÃ y sáº½ trÃ´ng quen thuá»™c vá»›i báº¡n náº¿u báº¡n nhá»› láº¡i cÃ¡ch chÃºng ta Ä‘Ã£ Ä‘á»‹nh nghÄ©a hÃ m `compute_metrics()` cá»§a `Seq2SeqTrainer`.

Cuá»‘i cÃ¹ng, chÃºng ta cáº§n táº¡o má»™t kho lÆ°u trá»¯ mÃ´ hÃ¬nh trÃªn Hugging Face Hub. Äá»‘i vá»›i Ä‘iá»u nÃ y, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng thÆ° viá»‡n ğŸ¤— Hub cÃ³ tiÃªu Ä‘á» thÃ­ch há»£p. ChÃºng ta chá»‰ cáº§n xÃ¡c Ä‘á»‹nh tÃªn cho kho lÆ°u trá»¯ cá»§a mÃ¬nh vÃ  thÆ° viá»‡n cÃ³ chá»©c nÄƒng tiá»‡n Ã­ch Ä‘á»ƒ káº¿t há»£p ID kho lÆ°u trá»¯ vá»›i há»“ sÆ¡ ngÆ°á»i dÃ¹ng:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng tÃªn kho lÆ°u trá»¯ nÃ y Ä‘á»ƒ sao chÃ©p phiÃªn báº£n cá»¥c bá»™ vÃ o thÆ° má»¥c káº¿t quáº£ cá»§a chÃºng ta, nÆ¡i sáº½ lÆ°u trá»¯ cÃ¡c táº¡o tÃ¡c huáº¥n luyá»‡n:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Äiá»u nÃ y sáº½ cho phÃ©p chÃºng ta Ä‘áº©y cÃ¡c táº¡o tÃ¡c trá»Ÿ láº¡i Hub báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `repo.push_to_hub()` trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n! BÃ¢y giá» chÃºng ta hÃ£y káº¿t thÃºc phÃ¢n tÃ­ch báº±ng cÃ¡ch viáº¿t ra vÃ²ng láº·p huáº¥n luyá»‡n.

### VÃ²ng láº·p huáº¥n luyá»‡n

VÃ²ng láº·p huáº¥n luyá»‡n Ä‘á»ƒ tÃ³m táº¯t khÃ¡ giá»‘ng vá»›i cÃ¡c vÃ­ dá»¥ ğŸ¤— Accelerate khÃ¡c mÃ  chÃºng ta Ä‘Ã£ gáº·p vÃ  gáº§n nhÆ° Ä‘Æ°á»£c chia thÃ nh bá»‘n bÆ°á»›c chÃ­nh:

1. Huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng cÃ¡ch láº·p láº¡i táº¥t cáº£ cÃ¡c vÃ­ dá»¥ trong `train_dataloader` cho má»—i epoch.
2. Táº¡o tÃ³m táº¯t mÃ´ hÃ¬nh vÃ o cuá»‘i má»—i epoch, báº±ng cÃ¡ch táº¡o token Ä‘áº§u tiÃªn vÃ  sau Ä‘Ã³ giáº£i mÃ£ chÃºng (vÃ  tÃ³m táº¯t tham chiáº¿u) thÃ nh vÄƒn báº£n.
3. TÃ­nh toÃ¡n Ä‘iá»ƒm ROUGE báº±ng cÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng tá»± mÃ  chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³.
4. LÆ°u cÃ¡c checkpoint vÃ  Ä‘áº©y má»i thá»© vÃ o Hub. á» Ä‘Ã¢y, chÃºng ta dá»±a vÃ o tham sá»‘ `blocking=False` tiá»‡n lá»£i cá»§a Ä‘á»‘i tÆ°á»£ng `Repository` Ä‘á»ƒ cÃ³ thá»ƒ Ä‘áº©y cÃ¡c checkpoint á»Ÿ má»—i epoch _báº¥t Ä‘á»“ng bá»™_. Äiá»u nÃ y cho phÃ©p ta tiáº¿p tá»¥c huáº¥n luyá»‡n mÃ  khÃ´ng pháº£i Ä‘á»£i táº£i lÃªn hÆ¡i cháº­m vá»›i mÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c GB!

CÃ¡c bÆ°á»›c nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c nhÃ¬n tháº¥y trong khá»‘i mÃ£ sau:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Huáº¥n luyá»‡n
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ÄÃ¡nh giÃ¡
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # Náº¿u ta khÃ´ng Ä‘á»‡m Ä‘áº¿n Ä‘á»™ giáº£i tá»‘i Ä‘a, ta cáº§n Ä‘á»‡m cáº£ nhÃ£n ná»¯a
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Thay -100 á»Ÿ nhÃ£n vÃ¬ ta khÃ´ng thá»ƒ  giáº£i mÃ£ chÃºng
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘
    result = rouge_score.compute()
    # TrÃ­ch xuáº¥t Ä‘iá»ƒm trung vá»‹ ROUGE
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # LÆ°u vÃ  táº£i
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

VÃ  tháº¿ Ä‘Ã³! Khi báº¡n cháº¡y nÃ³, báº¡n sáº½ cÃ³ má»™t mÃ´ hÃ¬nh vÃ  káº¿t quáº£ khÃ¡ giá»‘ng vá»›i nhá»¯ng mÃ´ hÃ¬nh mÃ  chÃºng ta thu Ä‘Æ°á»£c vá»›i `Trainer`.

{/if}

## Sá»­ dá»¥ng mÃ´ hÃ¬nh tinh chá»‰nh cá»§a báº¡n

Khi báº¡n Ä‘Ã£ Ä‘áº©y mÃ´ hÃ¬nh vÃ o Hub, báº¡n cÃ³ thá»ƒ chÆ¡i vá»›i nÃ³ thÃ´ng qua tiá»‡n Ã­ch luáº­n suy hoáº·c vá»›i Ä‘á»‘i tÆ°á»£ng `pipeline`, nhÆ° sau:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

ChÃºng ta cÃ³ thá»ƒ cung cáº¥p má»™t sá»‘ máº«u tá»« bá»™ kiá»ƒm thá»­ (mÃ  mÃ´ hÃ¬nh chÆ°a tháº¥y) vÃ o pipeline Ä‘á»ƒ cÃ³ cáº£m nháº­n vá» cháº¥t lÆ°á»£ng cá»§a cÃ¡c báº£n tÃ³m táº¯t. TrÆ°á»›c tiÃªn, hÃ£y triá»ƒn khai má»™t chá»©c nÄƒng Ä‘Æ¡n giáº£n Ä‘á»ƒ hiá»ƒn thá»‹ bÃ i Ä‘Ã¡nh giÃ¡, tiÃªu Ä‘á» vÃ  báº£n tÃ³m táº¯t Ä‘Ã£ táº¡o cÃ¹ng nhau:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

HÃ£y xem má»™t trong nhá»¯ng máº«u tiáº¿ng Anh mÃ  chÃºng ta nháº­n Ä‘Æ°á»£c:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnâ€™t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itâ€™s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

Äiá»u nÃ y khÃ´ng quÃ¡ tá»‡! ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng mÃ´ hÃ¬nh cá»§a mÃ¬nh Ä‘Ã£ thá»±c sá»± cÃ³ thá»ƒ thá»±c hiá»‡n tÃ³m táº¯t _trá»«u tÆ°á»£ng_ báº±ng cÃ¡ch tÄƒng cÆ°á»ng cÃ¡c pháº§n cá»§a bÃ i Ä‘Ã¡nh giÃ¡ báº±ng cÃ¡c tá»« má»›i. VÃ  cÃ³ láº½ khÃ­a cáº¡nh thÃº vá»‹ nháº¥t cá»§a mÃ´ hÃ¬nh cá»§a chÃºng ta lÃ  nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng song ngá»¯, vÃ¬ váº­y ta cÅ©ng cÃ³ thá»ƒ táº¡o tÃ³m táº¯t cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ báº±ng tiáº¿ng TÃ¢y Ban Nha:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

Báº£n tÃ³m táº¯t Ä‘Æ°á»£c dá»‹ch thÃ nh "Very easy to read" trong tiáº¿ng Anh, mÃ  chÃºng ta cÃ³ thá»ƒ tháº¥y trong trÆ°á»ng há»£p nÃ y Ä‘Æ°á»£c trÃ­ch trá»±c tiáº¿p tá»« Ä‘Ã¡nh giÃ¡. Tuy nhiÃªn, Ä‘iá»u nÃ y cho tháº¥y tÃ­nh linh hoáº¡t cá»§a mÃ´ hÃ¬nh mT5 vÃ  cho báº¡n biáº¿t cáº£m giÃ¡c triá»ƒn khai vá»›i kho ngá»¯ liá»‡u Ä‘a ngÃ´n ngá»¯ lÃ  nhÆ° tháº¿ nÃ o!

Tiáº¿p theo, chÃºng ta sáº½ chuyá»ƒn sá»± chÃº Ã½ sang má»™t tÃ¡c vá»¥ phá»©c táº¡p hÆ¡n má»™t chÃºt: huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»« Ä‘áº§u.
