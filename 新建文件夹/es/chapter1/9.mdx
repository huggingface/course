# Resumen

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

En este cap铆tulo viste c贸mo abordar diferentes tareas de PLN usando la funci贸n de alto nivel `pipeline()` de  Transformers. Tambi茅n viste como buscar modelos en el Hub, as铆 como usar la API de Inferencia para probar los modelos directamente en tu navegador.

Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o s贸lo el codificador o decodificador, dependiendo de qu茅 tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:

| Modelo                    | Ejemplos                                   | Tareas                                                                                              |
|---------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------------|
| Codificador               | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificaci贸n de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas |
| Decodificador             | CTRL, GPT, GPT-2, Transformer XL           | Generaci贸n de texto                                                                                 |
| Codificador-decodificador | BART, T5, Marian, mBART                    | Resumen, traducci贸n, respuesta generativa a preguntas                                               |
