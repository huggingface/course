---
local: wordpiece
sections:
- local: ''
  title: è®­ç»ƒç®—æ³•
- local: ''
  title: æ ‡è®°åŒ–ç®—æ³•
- local: wordpiece
  title: å®ç° WordPiece
title: 'WordPiece æ ‡è®°åŒ–  '
---
<script lang="ts">
import {onMount} from "svelte";
import Tip from "$lib/Tip.svelte";
import Youtube from "$lib/Youtube.svelte";
import Docstring from "$lib/Docstring.svelte";
import CodeBlock from "$lib/CodeBlock.svelte";
import CodeBlockFw from "$lib/CodeBlockFw.svelte";
import DocNotebookDropdown from "$lib/DocNotebookDropdown.svelte";
import IconCopyLink from "$lib/IconCopyLink.svelte";
import FrameworkContent from "$lib/FrameworkContent.svelte";
import Markdown from "$lib/Markdown.svelte";
import Question from "$lib/Question.svelte";
import FrameworkSwitchCourse from "$lib/FrameworkSwitchCourse.svelte";
import InferenceApi from "$lib/InferenceApi.svelte";
import TokenizersLanguageContent from "$lib/TokenizersLanguageContent.svelte";
import ExampleCodeBlock from "$lib/ExampleCodeBlock.svelte";
let fw: "pt" | "tf" = "pt";
onMount(() => {
    const urlParams = new URLSearchParams(window.location.search);
    fw = urlParams.get("fw") || "pt";
});
</script>
<svelte:head>
  <meta name="hf:doc:metadata" content={JSON.stringify(metadata)} >
</svelte:head>
<h1 id="wordpiece">WordPiece æ ‡è®°åŒ–  </h1>

&amp;lt;CourseFloatingBanner chapter=&amp;lcub;6}
  classNames="absolute z-10 right-0 top-0"
  notebooks=&amp;lcub;[
    &amp;lcub;label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section6.ipynb"},
    &amp;lcub;label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section6.ipynb"},
]} />

WordPiece æ˜¯ Google ä¸ºé¢„è®­ç»ƒ BERT è€Œå¼€å‘çš„æ ‡è®°åŒ–ç®—æ³•ã€‚æ­¤å,å®ƒåœ¨ä¸å°‘åŸºäº BERT çš„ Transformer æ¨¡å‹ä¸­å¾—åˆ°é‡ç”¨,ä¾‹å¦‚ DistilBERTã€MobileBERTã€Funnel Transformers å’Œ MPNETã€‚å®ƒåœ¨è®­ç»ƒæ–¹é¢ä¸ BPE éå¸¸ç›¸ä¼¼,ä½†å®é™…æ ‡è®°åŒ–çš„æ–¹å¼ä¸åŒã€‚

<Youtube id="qpv6ms_t_1A"/>

<Tip>

ğŸ’¡ æœ¬èŠ‚æ·±å…¥ä»‹ç» WordPiece,ç”šè‡³å±•ç¤ºå®Œæ•´çš„å®ç°ã€‚å¦‚æœæ‚¨åªæƒ³å¤§è‡´äº†è§£æ ‡è®°åŒ–ç®—æ³•,å¯ä»¥è·³åˆ°æœ€åã€‚

</Tip>

<h2 id="">è®­ç»ƒç®—æ³•</h2>

<Tip warning={true}>

âš ï¸ Google ä»æœªå¼€æº WordPiece è®­ç»ƒç®—æ³•çš„å®ç°,å› æ­¤ä»¥ä¸‹æ˜¯æˆ‘ä»¬åŸºäºå·²å‘è¡¨æ–‡çŒ®çš„æœ€ä½³çŒœæµ‹ã€‚å®ƒå¯èƒ½ä¸æ˜¯ 100% å‡†ç¡®çš„ã€‚

</Tip>

ä¸ BPE ä¸€æ ·,WordPiece ä»ä¸€ä¸ªå°è¯æ±‡è¡¨å¼€å§‹,åŒ…æ‹¬æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°å’Œåˆå§‹å­—æ¯è¡¨ã€‚å› ä¸ºå®ƒé€šè¿‡æ·»åŠ å‰ç¼€æ¥è¯†åˆ«å­è¯ (å¦‚åŒ `##` å¯¹äº BERT),æ¯ä¸ªå•è¯æœ€åˆæ˜¯é€šè¿‡å°†è¯¥å‰ç¼€æ·»åŠ åˆ°å•è¯å†…çš„æ‰€æœ‰å­—ç¬¦æ¥æ‹†åˆ†çš„ã€‚æ‰€ä»¥,ä¾‹å¦‚ `"word"` ,åƒè¿™æ ·æ‹†åˆ†:

```
w ##o ##r ##d
```

å› æ­¤,åˆå§‹å­—æ¯è¡¨åŒ…å«å‡ºç°åœ¨å•è¯å¼€å¤´çš„æ‰€æœ‰å­—ç¬¦ä»¥åŠå‡ºç°åœ¨å•è¯å†…éƒ¨çš„ä»¥ WordPiece å‰ç¼€å¼€å¤´çš„å­—ç¬¦ã€‚

ç„¶å,å†æ¬¡åƒ BPE ä¸€æ ·,WordPiece å­¦ä¹ åˆå¹¶è§„åˆ™ã€‚ä¸»è¦åŒºåˆ«åœ¨äºé€‰æ‹©è¦åˆå¹¶çš„å¯¹çš„æ–¹å¼ã€‚WordPiece ä¸æ˜¯é€‰æ‹©æœ€é¢‘ç¹çš„å¯¹ï¼Œè€Œæ˜¯ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—æ¯å¯¹çš„åˆ†æ•°:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

é€šè¿‡å°†é…å¯¹çš„é¢‘ç‡é™¤ä»¥å…¶æ¯ä¸ªéƒ¨åˆ†çš„é¢‘ç‡çš„ä¹˜ç§¯, è¯¥ç®—æ³•ä¼˜å…ˆåˆå¹¶å•ä¸ªéƒ¨åˆ†åœ¨è¯æ±‡è¡¨ä¸­é¢‘ç‡è¾ƒä½çš„å¯¹ã€‚ä¾‹å¦‚,å®ƒä¸ä¸€å®šä¼šåˆå¹¶ `("un", "##able")` å³ä½¿è¿™å¯¹åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°çš„é¢‘ç‡å¾ˆé«˜,å› ä¸º `"un"` å’Œ `"##able"` å¾ˆå¯èƒ½æ¯ä¸ªè¯éƒ½å‡ºç°åœ¨å¾ˆå¤šå…¶ä»–è¯ä¸­å¹¶ä¸”å‡ºç°é¢‘ç‡å¾ˆé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹,åƒ `("hu", "##gging")` å¯èƒ½ä¼šæ›´å¿«åœ°åˆå¹¶ (å‡è®¾ "hugging" ç»å¸¸å‡ºç°åœ¨è¯æ±‡è¡¨ä¸­),å› ä¸º `"hu"` å’Œ `"##gging"` è¿™ä¸¤ä¸ªè¯å•ç‹¬å‡ºç°åœ°é¢‘ç‡å¯èƒ½è¾ƒä½ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åœ¨ BPE è®­ç»ƒç¤ºä¾‹ä¸­ä½¿ç”¨çš„ç›¸åŒè¯æ±‡:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

è¿™é‡Œçš„æ‹†åˆ†å°†æ˜¯:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

æ‰€ä»¥æœ€åˆçš„è¯æ±‡å°†æ˜¯ `["b", "h", "p", "##g", "##n", "##s", "##u"]` (å¦‚æœæˆ‘ä»¬æš‚æ—¶å¿˜è®°ç‰¹æ®Šæ ‡è®°)ã€‚æœ€é¢‘ç¹çš„ä¸€å¯¹æ˜¯ `("##u", "##g")` (ç›®å‰20æ¬¡),ä½† `"##u"` å•ç‹¬å‡ºç°çš„é¢‘ç‡éå¸¸é«˜,æ‰€ä»¥å®ƒçš„åˆ†æ•°ä¸æ˜¯æœ€é«˜çš„(å®ƒæ˜¯ 1 / 36)ã€‚æ‰€æœ‰å¸¦æœ‰ `"##u"` çš„å¯¹å®é™…ä¸Šéƒ½æœ‰ç›¸åŒçš„åˆ†æ•°(1 / 36),æ‰€ä»¥åˆ†æ•°æœ€é«˜çš„å¯¹æ˜¯ `("##g", "##s")` -- å”¯ä¸€æ²¡æœ‰ `"##u"` çš„å¯¹-- 1 / 20,æ‰€ä»¥å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶æ˜¯ `("##g", "##s") -> ("##gs")`ã€‚

è¯·æ³¨æ„,å½“æˆ‘ä»¬åˆå¹¶æ—¶,æˆ‘ä»¬åˆ é™¤äº†ä¸¤ä¸ªæ ‡è®°ä¹‹é—´çš„ `##`,æ‰€ä»¥æˆ‘ä»¬æ·»åŠ  `"##gs"` åˆ°è¯æ±‡è¡¨ä¸­,å¹¶åœ¨è¯­æ–™åº“çš„å•è¯ä¸­åº”ç”¨è¯¥åˆå¹¶:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

åœ¨è¿™ä¸€ç‚¹ä¸­, `"##u"` æ˜¯åœ¨æ‰€æœ‰å¯èƒ½çš„å¯¹ä¸­,å› æ­¤å®ƒä»¬æœ€ç»ˆéƒ½å…·æœ‰ç›¸åŒçš„åˆ†æ•°ã€‚å‡è®¾åœ¨è¿™ç§æƒ…å†µä¸‹,ç¬¬ä¸€å¯¹è¢«åˆå¹¶, `("h", "##u") -> "hu"`ã€‚è¿™ä½¿å¾—æˆ‘ä»¬:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ç„¶åä¸‹ä¸€ä¸ªæœ€é«˜çš„åˆ†æ•°ç”± `("hu", "##g")` å’Œ `("hu", "##gs")` å…±äº«(1/15,ä¸å…¶ä»–æ‰€æœ‰å¯¹çš„ 1/21 ç›¸æ¯”),å› æ­¤åˆå¹¶å¾—åˆ†æœ€é«˜çš„ç¬¬ä¸€å¯¹:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

æˆ‘ä»¬ç»§ç»­è¿™æ ·å¤„ç†,ç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æ‰€éœ€çš„è¯æ±‡é‡ã€‚

<Tip>

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†!** ä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ
</Tip>

<h2 id="">æ ‡è®°åŒ–ç®—æ³•</h2>

WordPiece å’Œ BPE ä¸­çš„æ ‡è®°åŒ–çš„ä¸åŒåœ¨äº WordPiece åªä¿å­˜æœ€ç»ˆè¯æ±‡,è€Œä¸æ˜¯å­¦ä¹ çš„åˆå¹¶è§„åˆ™ã€‚ä»è¦æ ‡è®°çš„å•è¯å¼€å§‹,WordPiece æ‰¾åˆ°è¯æ±‡è¡¨ä¸­æœ€é•¿çš„å­è¯,ç„¶åå¯¹å…¶è¿›è¡Œæ‹†åˆ†ã€‚ä¾‹å¦‚,å¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸Šé¢ä¾‹å­ä¸­å­¦åˆ°çš„è¯æ±‡,å¯¹äºå•è¯ `"hugs"`,è¯æ±‡è¡¨ä¸­ä»å¤´å¼€å§‹çš„æœ€é•¿å­è¯æ˜¯ `"hug"`,æ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œæ‹†åˆ†å¹¶å¾—åˆ° `["hug", "##s"]`ã€‚ ç„¶åæˆ‘ä»¬ç»§ç»­ä½¿ç”¨è¯æ±‡è¡¨ä¸­çš„ `"##s"`,å› æ­¤ `"hugs"` çš„æ ‡è®°åŒ–æ˜¯ `["hug", "##s"]`.

ä½¿ç”¨ BPE, æˆ‘ä»¬å°†æŒ‰é¡ºåºåº”ç”¨å­¦ä¹ åˆ°çš„åˆå¹¶å¹¶å°†å…¶æ ‡è®°ä¸º `["hu", "##gs"]`,æ‰€ä»¥ç¼–ç ä¸åŒã€‚

å†ä¸¾ä¸€ä¸ªä¾‹å­,è®©æˆ‘ä»¬çœ‹çœ‹ `"bugs"` å°†å¦‚ä½•è¢«æ ‡è®°åŒ–ã€‚ `"b"` æ˜¯ä»è¯æ±‡è¡¨ä¸­å•è¯å¼€å¤´å¼€å§‹çš„æœ€é•¿å­è¯,æ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œæ‹†åˆ†å¹¶å¾—åˆ° `["b", "##ugs"]`ã€‚ç„¶å `"##u"` æ˜¯è¯æ±‡è¡¨ä¸­ä» `"##ugs"` å¼€å§‹çš„æœ€é•¿çš„å­è¯,æ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œæ‹†åˆ†å¹¶å¾—åˆ° `["b", "##u, "##gs"]`ã€‚æœ€å, `"##gs"` åœ¨è¯æ±‡è¡¨ä¸­,æ‰€ä»¥æœ€åä¸€ä¸ªåˆ—è¡¨æ˜¯ `"bugs"` çš„æ ‡è®°åŒ–ã€‚

å½“åˆ†è¯è¾¾åˆ°æ— æ³•åœ¨è¯æ±‡è¡¨ä¸­æ‰¾åˆ°å­è¯çš„é˜¶æ®µæ—¶, æ•´ä¸ªè¯è¢«æ ‡è®°ä¸ºæœªçŸ¥ -- ä¾‹å¦‚, `"mug"` å°†è¢«æ ‡è®°ä¸º `["[UNK]"]`,å°±åƒ `"bum"` (å³ä½¿æˆ‘ä»¬å¯ä»¥ä»¥ `"b"` å’Œ `"##u"` å¼€å§‹, `"##m"` ä¸åœ¨è¯æ±‡è¡¨ä¸­,ç”±æ­¤äº§ç”Ÿçš„æ ‡è®°å°†åªæ˜¯ `["[UNK]"]`, ä¸æ˜¯ `["b", "##u", "[UNK]"]`)ã€‚è¿™æ˜¯ä¸ BPE çš„å¦ä¸€ä¸ªåŒºåˆ«,BPE åªä¼šå°†ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•ä¸ªå­—ç¬¦åˆ†ç±»ä¸ºæœªçŸ¥ã€‚

<Tip>

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†!** `"pugs"` å°†è¢«å¦‚ä½•æ ‡è®°?

</Tip>

<h2 id="wordpiece">å®ç° WordPiece</h2>

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ WordPiece ç®—æ³•çš„å®ç°ã€‚ä¸ BPE ä¸€æ ·,è¿™åªæ˜¯æ•™å­¦,ä½ å°†æ— æ³•åœ¨å¤§å‹è¯­æ–™åº“ä¸­ä½¿ç”¨å®ƒã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ä¸­ç›¸åŒçš„è¯­æ–™åº“:

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é¦–å…ˆ,æˆ‘ä»¬éœ€è¦å°†è¯­æ–™åº“é¢„å…ˆæ ‡è®°ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤åˆ¶ WordPiece æ ‡è®°å™¨ (å¦‚ BERT),å› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `bert-base-cased` æ ‡è®°å™¨ç”¨äºé¢„æ ‡è®°åŒ–:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ç„¶åæˆ‘ä»¬åœ¨è¿›è¡Œé¢„æ ‡è®°åŒ–æ—¶è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„,å­—æ¯è¡¨æ˜¯ç”±å•è¯çš„æ‰€æœ‰ç¬¬ä¸€ä¸ªå­—æ¯ç»„æˆçš„å”¯ä¸€é›†åˆ,ä»¥åŠå‡ºç°åœ¨å‰ç¼€ä¸º `##` çš„å…¶ä»–å­—æ¯:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ã€‚åœ¨ä½¿ç”¨ BERT çš„æƒ…å†µä¸‹,å®ƒæ˜¯åˆ—è¡¨ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦æ‹†åˆ†æ¯ä¸ªå•è¯, æ‰€æœ‰ä¸æ˜¯ç¬¬ä¸€ä¸ªå­—æ¯çš„å­—æ¯éƒ½ä»¥ `##` ä¸ºå‰ç¼€:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è®­ç»ƒäº†,è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹çš„åˆ†æ•°ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒ:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªå­—å…¸åœ¨åˆå§‹æ‹†åˆ†åçš„ä¸€éƒ¨åˆ†:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

ç°åœ¨,æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„å¯¹åªéœ€è¦ä¸€ä¸ªå¿«é€Ÿå¾ªç¯:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score &amp;lt; score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶æ˜¯ `('a', '##b') -> 'ab'`, å¹¶ä¸”æˆ‘ä»¬æ·»åŠ  `'ab'` åˆ°è¯æ±‡è¡¨ä¸­:

```python
vocab.append("ab")
```

è¦ç»§ç»­æ¥ä¸‹æ¥çš„æ­¥éª¤,æˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„ `æ‹†åˆ†` å­—å…¸ä¸­åº”ç”¨è¯¥åˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™å¦ä¸€ä¸ªå‡½æ•°:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i &amp;lt; len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœ:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†å¾ªç¯æ‰€éœ€çš„ä¸€åˆ‡,ç›´åˆ°æˆ‘ä»¬å­¦ä¼šäº†æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰åˆå¹¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡è¯æ±‡é‡ä¸º70:

```python
vocab_size = 70
while len(vocab) &amp;lt; vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score &amp;lt; score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

ç„¶åæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„è¯æ±‡è¡¨:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„,ä¸ BPE ç›¸æ¯”,è¿™ä¸ªæ ‡è®°å™¨å°†å•è¯çš„ä¸€éƒ¨åˆ†ä½œä¸ºæ ‡è®°å­¦ä¹ å¾—æ›´å¿«ä¸€äº›ã€‚

<Tip>

ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸º ğŸ¤— Tokenizers åº“æ²¡æœ‰ä¸ºè®­ç»ƒå®ç° WordPiece(å› ä¸ºæˆ‘ä»¬ä¸å®Œå…¨ç¡®å®šå®ƒçš„å†…éƒ¨ç»“æ„),è€Œæ˜¯ä½¿ç”¨ BPEã€‚

</Tip>

ä¸ºäº†å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯,æˆ‘ä»¬å¯¹å…¶è¿›è¡Œé¢„åˆ†è¯ã€æ‹†åˆ†,ç„¶åå¯¹æ¯ä¸ªå•è¯åº”ç”¨åˆ†è¯ç®—æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´,æˆ‘ä»¬ä»ç¬¬ä¸€ä¸ªè¯çš„å¼€å¤´å¯»æ‰¾æœ€å¤§çš„å­è¯å¹¶å°†å…¶æ‹†åˆ†,ç„¶åæˆ‘ä»¬åœ¨ç¬¬äºŒéƒ¨åˆ†é‡å¤è¿™ä¸ªè¿‡ç¨‹,å¯¹äºè¯¥è¯çš„å…¶ä½™éƒ¨åˆ†å’Œæ–‡æœ¬ä¸­çš„ä»¥ä¸‹è¯,ä¾æ­¤ç±»æ¨:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##&amp;lcub;word}"
    return tokens
```

è®©æˆ‘ä»¬ç”¨è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªå•è¯å’Œå¦ä¸€ä¸ªä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯è¿›è¡Œæµ‹è¯•:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

ç°åœ¨,è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªæ ‡è®°æ–‡æœ¬çš„å‡½æ•°:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ–‡æœ¬ä¸Šå°è¯•:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

è¿™å°±æ˜¯ WordPiece ç®—æ³•çš„å…¨éƒ¨å†…å®¹!ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ Unigramã€‚
