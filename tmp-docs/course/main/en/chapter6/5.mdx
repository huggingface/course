---
local: ''
sections:
- local: ''
  title: è®­ç»ƒç®—æ³•
- local: ''
  title: æ ‡è®°åŒ–ç®—æ³•
- local: bpe
  title: å®ç° BPE
title: å­—èŠ‚å¯¹ç¼–ç æ ‡è®°åŒ–
---
<script lang="ts">
import {onMount} from "svelte";
import Tip from "$lib/Tip.svelte";
import Youtube from "$lib/Youtube.svelte";
import Docstring from "$lib/Docstring.svelte";
import CodeBlock from "$lib/CodeBlock.svelte";
import CodeBlockFw from "$lib/CodeBlockFw.svelte";
import DocNotebookDropdown from "$lib/DocNotebookDropdown.svelte";
import IconCopyLink from "$lib/IconCopyLink.svelte";
import FrameworkContent from "$lib/FrameworkContent.svelte";
import Markdown from "$lib/Markdown.svelte";
import Question from "$lib/Question.svelte";
import FrameworkSwitchCourse from "$lib/FrameworkSwitchCourse.svelte";
import InferenceApi from "$lib/InferenceApi.svelte";
import TokenizersLanguageContent from "$lib/TokenizersLanguageContent.svelte";
import ExampleCodeBlock from "$lib/ExampleCodeBlock.svelte";
let fw: "pt" | "tf" = "pt";
onMount(() => {
    const urlParams = new URLSearchParams(window.location.search);
    fw = urlParams.get("fw") || "pt";
});
</script>
<svelte:head>
  <meta name="hf:doc:metadata" content={JSON.stringify(metadata)} >
</svelte:head>
<h1 id="">å­—èŠ‚å¯¹ç¼–ç æ ‡è®°åŒ–</h1>

&amp;lt;CourseFloatingBanner chapter=&amp;lcub;6}
  classNames="absolute z-10 right-0 top-0"
  notebooks=&amp;lcub;[
    &amp;lcub;label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb"},
    &amp;lcub;label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb"},
]} />

å­—èŠ‚å¯¹ç¼–ç (BPE)æœ€åˆè¢«å¼€å‘ä¸ºä¸€ç§å‹ç¼©æ–‡æœ¬çš„ç®—æ³•,ç„¶ååœ¨é¢„è®­ç»ƒ GPT æ¨¡å‹æ—¶è¢« OpenAI ç”¨äºæ ‡è®°åŒ–ã€‚è®¸å¤š Transformer æ¨¡å‹éƒ½ä½¿ç”¨å®ƒ,åŒ…æ‹¬ GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTaã€‚

<Youtube id="HEikzVL-lZU"/>

<Tip>

ğŸ’¡ æœ¬èŠ‚æ·±å…¥ä»‹ç»äº†BPE,ç”šè‡³å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„å®ç°ã€‚å¦‚æœä½ åªæƒ³å¤§è‡´äº†è§£æ ‡è®°åŒ–ç®—æ³•,å¯ä»¥è·³åˆ°æœ€åã€‚

</Tip>

<h2 id="">è®­ç»ƒç®—æ³•</h2>

BPE è®­ç»ƒé¦–å…ˆè®¡ç®—è¯­æ–™åº“ä¸­ä½¿ç”¨çš„å”¯ä¸€å•è¯é›†(åœ¨å®Œæˆæ ‡å‡†åŒ–å’Œé¢„æ ‡è®°åŒ–æ­¥éª¤ä¹‹å),ç„¶åé€šè¿‡è·å–ç”¨äºç¼–å†™è¿™äº›å•è¯çš„æ‰€æœ‰ç¬¦å·æ¥æ„å»ºè¯æ±‡è¡¨ã€‚ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­,å‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“ä½¿ç”¨äº†è¿™äº”ä¸ªè¯:

```
"hug", "pug", "pun", "bun", "hugs"
```

åŸºç¡€è¯æ±‡å°†æ˜¯ `["b", "g", "h", "n", "p", "s", "u"]`ã€‚å¯¹äºå®é™…æƒ…å†µ,åŸºæœ¬è¯æ±‡è¡¨å°†åŒ…å«æ‰€æœ‰ ASCII å­—ç¬¦,è‡³å°‘,å¯èƒ½è¿˜åŒ…å«ä¸€äº› Unicode å­—ç¬¦ã€‚å¦‚æœæ‚¨æ­£åœ¨æ ‡è®°çš„ç¤ºä¾‹ä½¿ç”¨ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­çš„å­—ç¬¦,åˆ™è¯¥å­—ç¬¦å°†è½¬æ¢ä¸ºæœªçŸ¥æ ‡è®°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®¸å¤š NLP æ¨¡å‹åœ¨åˆ†æå¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„å†…å®¹æ–¹é¢éå¸¸ç³Ÿç³•çš„åŸå› ä¹‹ä¸€ã€‚

<Tip>

TGPT-2 å’Œ RoBERTa æ ‡è®°å™¨(éå¸¸ç›¸ä¼¼)æœ‰ä¸€ä¸ªèªæ˜çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜: ä»–ä»¬ä¸æŠŠå•è¯çœ‹æˆæ˜¯ç”¨ Unicode å­—ç¬¦å†™çš„ï¼Œè€Œæ˜¯ç”¨å­—èŠ‚å†™çš„ã€‚è¿™æ ·,åŸºæœ¬è¯æ±‡è¡¨çš„å¤§å°å¾ˆå°(256),ä½†ä½ èƒ½æƒ³åˆ°çš„æ¯ä¸ªå­—ç¬¦ä»å°†è¢«åŒ…å«åœ¨å†…,è€Œä¸ä¼šæœ€ç»ˆè½¬æ¢ä¸ºæœªçŸ¥æ ‡è®°ã€‚è¿™ä¸ªæŠ€å·§è¢«ç§°ä¸º *å­—èŠ‚çº§ BPE*ã€‚

</Tip>

è·å¾—è¿™ä¸ªåŸºæœ¬è¯æ±‡å,æˆ‘ä»¬æ·»åŠ æ–°çš„æ ‡è®°,ç›´åˆ°é€šè¿‡å­¦ä¹ *åˆå¹¶*è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡é‡,è¿™æ˜¯å°†ç°æœ‰è¯æ±‡è¡¨çš„ä¸¤ä¸ªå…ƒç´ åˆå¹¶ä¸ºä¸€ä¸ªæ–°å…ƒç´ çš„è§„åˆ™ã€‚å› æ­¤,åœ¨å¼€å§‹æ—¶,è¿™äº›åˆå¹¶å°†åˆ›å»ºå…·æœ‰ä¸¤ä¸ªå­—ç¬¦çš„æ ‡è®°,ç„¶å,éšç€è®­ç»ƒçš„è¿›è¡Œ,ä¼šåˆ›å»ºæ›´é•¿çš„å­è¯ã€‚

åœ¨åˆ†è¯å™¨è®­ç»ƒæœŸé—´çš„ä»»ä½•ä¸€æ­¥,BPE ç®—æ³•éƒ½ä¼šæœç´¢æœ€å¸¸è§çš„ç°æœ‰æ ‡è®°å¯¹ ("å¯¹",è¿™é‡Œæˆ‘ä»¬æŒ‡çš„æ˜¯å•è¯ä¸­çš„ä¸¤ä¸ªè¿ç»­æ ‡è®°)ã€‚æœ€é¢‘ç¹çš„ä¸€å¯¹å°†è¢«åˆå¹¶,æˆ‘ä»¬å†²æ´—å¹¶é‡å¤ä¸‹ä¸€æ­¥ã€‚

å›åˆ°æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­,è®©æˆ‘ä»¬å‡è®¾å•è¯å…·æœ‰ä»¥ä¸‹é¢‘ç‡:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ„å‘³ç€ `"hug"` åœ¨è¯­æ–™åº“ä¸­å‡ºç°äº†10æ¬¡, `"pug"` 5æ¬¡, `"pun"` 12æ¬¡, `"bun"` 4æ¬¡, ä»¥åŠ `"hugs"` 5æ¬¡ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå­—ç¬¦(å½¢æˆæˆ‘ä»¬åˆå§‹è¯æ±‡è¡¨çš„å­—ç¬¦)æ¥å¼€å§‹è®­ç»ƒ,è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†æ¯ä¸ªå•è¯è§†ä¸ºä¸€ä¸ªæ ‡è®°åˆ—è¡¨:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ç„¶åæˆ‘ä»¬çœ‹æˆå¯¹ã€‚è¿™å¯¹ `("h", "u")` å‡ºç°åœ¨å•è¯ `"hug"` å’Œ `"hugs"`ä¸­,æ‰€ä»¥è¯­æ–™åº“ä¸­æ€»å…±æœ‰15æ¬¡ã€‚ä¸è¿‡,è¿™å¹¶ä¸æ˜¯æœ€é¢‘ç¹çš„ä¸€å¯¹:è¿™ä¸ªè£èª‰å±äº `("u", "g")`,å®ƒå‡ºç°åœ¨ `"hug"`, `"pug"`, ä»¥åŠ `"hugs"`ä¸­,åœ¨è¯æ±‡è¡¨ä¸­æ€»å…± 20 æ¬¡ã€‚

å› æ­¤,æ ‡è®°å™¨å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "g") -> "ug"`,æ„æ€å°±æ˜¯ `"ug"` å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­,å¹¶ä¸”è¿™å¯¹åº”è¯¥åˆå¹¶åˆ°è¯­æ–™åº“çš„æ‰€æœ‰å•è¯ä¸­ã€‚åœ¨è¿™ä¸ªé˜¶æ®µç»“æŸæ—¶,è¯æ±‡è¡¨å’Œè¯­æ–™åº“çœ‹èµ·æ¥åƒè¿™æ ·:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€äº›å¯¼è‡´æ ‡è®°é•¿äºä¸¤ä¸ªå­—ç¬¦çš„å¯¹: ä¾‹å¦‚ `("h", "ug")`, åœ¨è¯­æ–™åº“ä¸­å‡ºç°15æ¬¡ã€‚ç„¶è€Œ,è¿™ä¸ªé˜¶æ®µæœ€é¢‘ç¹çš„å¯¹æ˜¯ `("u", "n")`,åœ¨è¯­æ–™åº“ä¸­å‡ºç°16æ¬¡,æ‰€ä»¥å­¦åˆ°çš„ç¬¬äºŒä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "n") -> "un"`ã€‚å°†å…¶æ·»åŠ åˆ°è¯æ±‡è¡¨å¹¶åˆå¹¶æ‰€æœ‰ç°æœ‰çš„è¿™ä¸ªå¯¹,å°†å‡ºç°:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æœ€é¢‘ç¹çš„ä¸€å¯¹æ˜¯ `("h", "ug")`,æ‰€ä»¥æˆ‘ä»¬å­¦ä¹ äº†åˆå¹¶è§„åˆ™ `("h", "ug") -> "hug"`,è¿™ç»™äº†æˆ‘ä»¬ç¬¬ä¸€ä¸ªä¸‰ä¸ªå­—æ¯çš„æ ‡è®°ã€‚åˆå¹¶å,è¯­æ–™åº“å¦‚ä¸‹æ‰€ç¤º:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

æˆ‘ä»¬ç»§ç»­è¿™æ ·åˆå¹¶,ç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æ‰€éœ€çš„è¯æ±‡é‡ã€‚

<Tip>

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†!**ä½ è®¤ä¸ºä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

</Tip>

<h2 id="">æ ‡è®°åŒ–ç®—æ³•</h2>

æ ‡è®°åŒ–ç´§è·Ÿè®­ç»ƒè¿‡ç¨‹,ä»æŸç§æ„ä¹‰ä¸Šè¯´,é€šè¿‡åº”ç”¨ä»¥ä¸‹æ­¥éª¤å¯¹æ–°è¾“å…¥è¿›è¡Œæ ‡è®°:

1. è§„èŒƒåŒ–
2. é¢„æ ‡è®°åŒ–
3. å°†å•è¯æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦
4. å°†å­¦ä¹ çš„åˆå¹¶è§„åˆ™æŒ‰é¡ºåºåº”ç”¨äºè¿™äº›æ‹†åˆ†

è®©æˆ‘ä»¬ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç¤ºä¾‹ä¸ºä¾‹,å­¦ä¹ ä¸‰ä¸ªåˆå¹¶è§„åˆ™:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

è¿™ä¸ªå•è¯ `"bug"` å°†è¢«æ ‡è®°ä¸º `["b", "ug"]`ã€‚ç„¶è€Œ `"mug"`,å°†è¢«æ ‡è®°ä¸º `["[UNK]", "ug"]`,å› ä¸ºå­—æ¯ `"m"` ä¸å†åŸºæœ¬è¯æ±‡è¡¨ä¸­ã€‚åŒæ ·,å•è¯`"thug"` ä¼šè¢«æ ‡è®°ä¸º `["[UNK]", "hug"]`: å­—æ¯ `"t"` ä¸åœ¨åŸºæœ¬è¯æ±‡è¡¨ä¸­,åº”ç”¨åˆå¹¶è§„åˆ™é¦–å…ˆå¯¼è‡´ `"u"` å’Œ `"g"` è¢«åˆå¹¶,ç„¶åæ˜¯ `"hu"` å’Œ `"g"` è¢«åˆå¹¶ã€‚

<Tip>

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†!** ä½ è®¤ä¸ºè¿™ä¸ªè¯ `"unhug"` å°†å¦‚ä½•è¢«æ ‡è®°ï¼Ÿ

</Tip>

<h2 id="bpe">å®ç° BPE</h2>

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ BPE ç®—æ³•çš„å®ç°ã€‚è¿™ä¸ä¼šæ˜¯ä½ å¯ä»¥åœ¨å¤§å‹è¯­æ–™åº“ä¸Šå®é™…ä½¿ç”¨çš„ä¼˜åŒ–ç‰ˆæœ¬;æˆ‘ä»¬åªæ˜¯æƒ³å‘ä½ å±•ç¤ºä»£ç ,ä»¥ä¾¿ä½ å¯ä»¥æ›´å¥½åœ°ç†è§£ç®—æ³•

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯­æ–™åº“,æ‰€ä»¥è®©æˆ‘ä»¬ç”¨å‡ å¥è¯åˆ›å»ºä¸€ä¸ªç®€å•çš„è¯­æ–™åº“:

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

æ¥ä¸‹æ¥,æˆ‘ä»¬éœ€è¦å°†è¯¥è¯­æ–™åº“é¢„å…ˆæ ‡è®°ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤åˆ¶ BPE æ ‡è®°å™¨(å¦‚ GPT-2),æˆ‘ä»¬å°†ä½¿ç”¨ `gpt2` æ ‡è®°å™¨ä½œä¸ºé¢„æ ‡è®°åŒ–çš„æ ‡è®°å™¨:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ç„¶åæˆ‘ä»¬åœ¨è¿›è¡Œé¢„æ ‡è®°åŒ–æ—¶è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

ä¸‹ä¸€æ­¥æ˜¯è®¡ç®—åŸºæœ¬è¯æ±‡,ç”±è¯­æ–™åº“ä¸­ä½¿ç”¨çš„æ‰€æœ‰å­—ç¬¦ç»„æˆ:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ã€‚å¯¹äºGPT-2,å”¯ä¸€çš„ç‰¹æ®Šæ ‡è®°æ˜¯ `"<|endoftext|>"`:

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

æˆ‘ä»¬ç°åœ¨éœ€è¦å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå•ç‹¬çš„å­—ç¬¦,ä»¥ä¾¿èƒ½å¤Ÿå¼€å§‹è®­ç»ƒ:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ç°åœ¨æˆ‘ä»¬å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒ,è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹çš„é¢‘ç‡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒ:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªå­—å…¸åœ¨åˆå§‹æ‹†åˆ†åçš„ä¸€éƒ¨åˆ†:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

ç°åœ¨, æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹åªéœ€è¦ä¸€ä¸ªå¿«é€Ÿçš„å¾ªç¯:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq &amp;lt; freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ä ', 't') 7
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶æ˜¯ `('Ä ', 't') -> 'Ä t'`, æˆ‘ä»¬æ·»åŠ  `'Ä t'` åˆ°è¯æ±‡è¡¨:

```python
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

è¦ç»§ç»­æ¥ä¸‹æ¥çš„æ­¥éª¤,æˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„`åˆ†è¯`å­—å…¸ä¸­åº”ç”¨è¯¥åˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™å¦ä¸€ä¸ªå‡½æ•°:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i &amp;lt; len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœ:

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```python out
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†å¾ªç¯æ‰€éœ€çš„ä¸€åˆ‡,ç›´åˆ°æˆ‘ä»¬å­¦ä¼šäº†æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰åˆå¹¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯æ±‡é‡è¾¾åˆ°50:

```python
vocab_size = 50

while len(vocab) &amp;lt; vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq &amp;lt; freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

ç»“æœ,æˆ‘ä»¬å­¦ä¹ äº† 19 æ¡åˆå¹¶è§„åˆ™(åˆå§‹è¯æ±‡è¡¨çš„å¤§å° 31 -- 30 å­—æ¯å­—ç¬¦,åŠ ä¸Šç‰¹æ®Šæ ‡è®°):

```py
print(merges)
```

```python out
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

è¯æ±‡è¡¨ç”±ç‰¹æ®Šæ ‡è®°ã€åˆå§‹å­—æ¯å’Œæ‰€æœ‰åˆå¹¶ç»“æœç»„æˆ:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

<Tip>

ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸ºå½“æœ‰æœ€é¢‘ç¹å¯¹çš„é€‰æ‹©æ—¶,æˆ‘ä»¬é€‰æ‹©é‡åˆ°çš„ç¬¬ä¸€ä¸ª, è€Œ ğŸ¤— Tokenizers åº“æ ¹æ®å†…éƒ¨IDé€‰æ‹©ç¬¬ä¸€ä¸ªã€‚

</Tip>

ä¸ºäº†å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯,æˆ‘ä»¬å¯¹å…¶è¿›è¡Œé¢„åˆ†è¯ã€æ‹†åˆ†ï¼Œç„¶ååº”ç”¨å­¦åˆ°çš„æ‰€æœ‰åˆå¹¶è§„åˆ™:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i &amp;lt; len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•ç”±å­—æ¯è¡¨ä¸­çš„å­—ç¬¦ç»„æˆçš„æ–‡æœ¬ä¸Šå°è¯•è¿™ä¸ª:

```py
tokenize("This is not a token.")
```

```python out
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

<Tip warning={true}>

âš ï¸ å¦‚æœå­˜åœ¨æœªçŸ¥å­—ç¬¦,æˆ‘ä»¬çš„å®ç°å°†æŠ›å‡ºé”™è¯¯,å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åšä»»ä½•å¤„ç†å®ƒä»¬ã€‚GPT-2 å®é™…ä¸Šæ²¡æœ‰æœªçŸ¥æ ‡è®°(ä½¿ç”¨å­—èŠ‚çº§ BPE æ—¶ä¸å¯èƒ½å¾—åˆ°æœªçŸ¥å­—ç¬¦),ä½†è¿™å¯èƒ½å‘ç”Ÿåœ¨è¿™é‡Œ,å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åœ¨åˆå§‹è¯æ±‡è¡¨ä¸­åŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ã€‚ BPE çš„è¿™æ–¹é¢è¶…å‡ºäº†æœ¬èŠ‚çš„èŒƒå›´,å› æ­¤æˆ‘ä»¬å¿½ç•¥äº†ç»†èŠ‚ã€‚

</Tip>

è¿™å°±æ˜¯ BPE ç®—æ³•ï¼æ¥ä¸‹æ¥,æˆ‘ä»¬å°†çœ‹çœ‹ WordPieceã€‚