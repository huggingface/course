# Local Inference Tools

So far in this course we've covered the fundamentals and seen a basic example of inference. In this chapter we'll dive deeper into the different tools that enable you to run LLMs directly on your own computer. We'll walk through [Ollama](https://ollama.com/), [LMStudio](https://lmstudio.ai/), [Jan AI](https://jan.ai/), and [llama.cpp](https://github.com/ggerganov/llama.cpp), examining their features, use cases, and how to effectively leverage each tool for local inference.

These tools are primarily focused on running models locally and not for deployment in production. For that, we'll cover Text Generation Inference (TGI) and vLLM in the next section.

<Tip>
These are four very different tools that run user interfaces, command line interfaces, and provide Python integrations. So some students may want to focus on a tool that fits their skillset and expertise. Do it!
</Tip>

## Why Run Models Locally?

Before diving into the tools, let's remind ourselves why we might want to run models locally:

1. **Privacy**: Keep sensitive data on your machine without sending it to cloud services
2. **Cost-effectiveness**: No usage-based billing or API costs
3. **Offline access**: Work without internet connectivity
4. **Learning opportunity**: Better understand how LLMs work
5. **Customization**: Full control over model parameters and behavior

Let's look at some of the most common use cases for local LLMs.

| Use Case | Description |
|----------|-------------|
| Document Analysis | Upload PDFs, Word documents, and text files to their local machine for processing. The models can generate comprehensive summaries, extract valuable insights, and identify key information from these documents, all while keeping the data secure and private. |
| Offline Research Assistant | Researchers can leverage these tools to enhance their note-taking process with AI assistance, generate thought-provoking research questions, and create concise summaries of their findings. This capability is particularly valuable when working in environments with limited internet connectivity or when dealing with sensitive research materials. |
| Privacy-First Development | Developers can process sensitive data directly on their machines without the risk of exposure to external services. This allows them to develop and test AI features securely, ensuring that confidential information never leaves their local environment. This approach is particularly valuable for businesses and organizations that handle sensitive customer data or proprietary information. |

## Tool Comparison

We will start with a high-level comparison of these tools:

| Feature | Ollama | LMStudio | Jan AI | llama.cpp |
|---------|---------|-----------|---------|------------|
| Interface | CLI | GUI | GUI | CLI |
| Ease of Use | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Performance | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Customization | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Learning Curve | ü™´ | ü™´ | ü™´ | üîã |
| Best For | Development | Experimentation | Beginners | Performance |
| Quantization | 4-bit, 8-bit | 4-bit, 8-bit | 4-bit, 8-bit | 2-bit to 8-bit |
| REST API | üî¥ | ‚úÖ | ‚úÖ | ‚úÖ |
| OpenAI API Compatible | ‚úì | ‚úì | ‚úì | Via Python bindings |
| Python Integration | üî¥ | ‚úÖ | ‚úÖ | ‚úÖ |
| Multi-modal Support | üü† | üü† | ‚úÖ | üü† |
| GPU Acceleration | ‚úì | ‚úì | ‚úì | ‚úì |
| Memory Requirements | 8GB+ | 16GB+ | 16GB+ | 4GB+ |

As said above, you'll want to select the tool that best fits your skillset and project. In most cases, you'll work with multiple tools in your project, but it's good to have a starting point.

## LMStudio

We will start with one of the most user-friendly and feature rich tools, LMStudio.

LMStudio provides a graphical interface for those who prefer not to use the command line. It's like having a "ChatGPT-style interface" but your local models with way more customization. 

![LMStudio Interface](https://149868225.v2.pressablecdn.com/wp-content/uploads/2024/01/lmstudio.png)

The key features of LMStudio are:

- **Intuitive GUI for model management**: Select and configure models, parameters, and prompts.
- **Built-in chat interface**: Similar to ChatGPT's interface
- **Model discovery and downloading**: Browse and download models easily
- **Performance monitoring tools**: Track memory usage and inference speed
- **Multiple model format support**: Works with various model types
- **Fine-tuning capabilities**: Customize models for your needs

<Tip>
Installation instructions can be found at [LMStudio Downloads](https://lmstudio.ai/download/). It's available for macOS, Linux, and Windows.
</Tip>

From the LMStudio interface, you can select a model and start chatting with it immediately, or serve the model as an API.

You can also compare multiple models side by side to see which one is best for your use case. Or define and experiment with different prompts.

```python
# Example API usage with LMStudio
from openai import OpenAI

client = OpenAI(base_url="http://localhost:1234/v1", api_key="not-needed")

response = client.chat.completions.create(
    model="local-model", messages=[{"role": "user", "content": "Explain neural networks"}]
)
```

| Pros | Cons |
|------|------|
| No coding required for basic usage | Higher resource overhead |
| Visual performance metrics | Less relevant for deployment |
| Easy model comparison | Limited automation capabilities |
| Built-in prompt engineering tools | May be overwhelming for just chatting |
| Supports multiple model formats | |

### Technical aspects of LMStudio

LMStudio provides a REST API for model management and inference, which can be used to get model statistics, list models, and more.

```python
# Get model statistics
response = requests.get("http://localhost:1234/v1/stats")
stats = response.json()
print(f"Tokens/second: {stats['tokens_per_second']}")
print(f"Memory usage: {stats['memory_usage']}")
```

Like many of the other tools, LMStudio can be used as an API with the OpenAI client. This allows you to integrate it into your own applications.

```python
from openai import OpenAI

# Initialize client with LMStudio server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="not-needed")

# Chat completion with advanced parameters
response = client.chat.completions.create(
    model="local-model",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Explain neural networks"},
    ],
)
```

## Jan AI

Let's take things down one level in complexity. Jan AI is a tool that provides a complete ChatGPT alternative that runs entirely on your computer. It's designed for those who want a polished, user-friendly interface with the privacy benefits of local inference.

![Jan AI Interface](https://jan.ai/assets/images/jan-hero.png)

<Tip>
Installation instructions can be found at [Jan AI Downloads](https://jan.ai/download/). It's available for macOS, Linux, and Windows.
</Tip>

The key features of Jan AI are:

- **Pre-installed model collection**: Start immediately with included models
- **Local document processing**: Analyze your documents privately
- **Model import from Hugging Face**: Access thousands of models from the Hugging Face Hub
- **Cross-platform support**: Use on any major operating system
- **Customizable inference parameters**: Fine-tune token selection through temperature, top-p, and repetition penalty.
- **Extensions support**: Add optimizations with TensorRT, Inference Nitro, and more.

| Pros | Cons |
|------|------|
| Ready-to-use model library | Performance varies by hardware |
| Clean, intuitive interface | Limited enterprise features |
| Complete offline operation | Requires significant storage |
| Strong community support | Some features still in development |
| Free and open-source | |
| Privacy-focused design | |

## Ollama

Now, we'll move on to command line tools. Ollama is a modern, user-friendly tool that simplifies running large language models locally. Think of it as "Docker for AI models" - it makes managing and running models as simple as typing a few commands. The following is a basic overview of the tool.

- **Simple CLI and REST API interface**: Run models with commands like `ollama run SmolLM-365M-Instruct`
- **Pre-configured model library**: Access popular models without complex setup
- **Docker-like commands**: Familiar syntax if you know Docker (pull, run, etc.)
- **Cross-platform support**: Works on macOS, Linux, and Windows
- **Built-in model quantization**: Automatically optimizes models for your hardware
- **GPU acceleration support**: Utilizes available GPU for faster inference

Let's walk through a basic example of how to use Ollama.

<Tip>
Installation instructions can be found at [Ollama Installation Guide](https://github.com/ollama/ollama#installation). It's hardware specific, so make sure to follow the instructions for your operating system and hardware to get the best performance.
</Tip>

Download the DeepSeek 1.5B parameter model to your local machine. This will fetch and store the model locally for future use:

```bash
ollama pull deepseek-r1:1.5b
```

Launch an interactive chat session with the DeepSeek model. This opens a REPL-like interface where you can have a conversation. If the model is already downloaded, it will start immediately.

```bash
ollama run deepseek-r1:1.5b
```

Send a one-off prompt to the model and get a single response. This is useful for scripting or when you don't need an interactive session:

```bash
ollama run deepseek-r1:1.5b "Explain quantum computing in simple terms"
```

Start the Ollama API server on a default port (11434). This allows other applications to interact with your models via REST API:

```bash
ollama serve
```

| Pros | Cons |
|------|------|
| Extremely user-friendly command-line interface | Limited customization compared to llama.cpp |
| Seamless model downloading and management | Fewer advanced configuration options |
| Active community and regular updates | Model selection restricted to supported library |
| Easy integration with applications | |
| Efficient memory management | |

### Technical aspects of Ollama

Under the hood, Ollama uses llama.cpp as its inference engine, which we'll cover in more detail in the next section. In short, Ollama is a user-friendly wrapper around llama.cpp that makes it easier to use. But it also adds a few practical features to improve usability:

#### Model Management System

Ollama gives you a simple way to manage your models. You can list all the models available on your machine all from the REST API or command line.

```python
import requests

# List available models
response = requests.get("http://localhost:11434/api/tags")
models = response.json()

# Generate completion
response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "deepseek-r1:1.5b",
        "prompt": "Explain quantum computing",
        "system": "You are a helpful assistant",
        "format": "json",
        "options": {"temperature": 0.7, "top_p": 0.9, "num_ctx": 4096},
    },
)
```

#### Custom Model Creation

Ollama allows you to create your own custom models. You can define your own prompts, parameters, and system messages which can be saved and reused. This is useful for creating reusable templates for your models.

```python
# Example Modelfile
"""
FROM deepseek-r1:1.5b
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
SYSTEM You are an AI assistant specialized in quantum computing
"""

# Create custom model via API
response = requests.post(
    "http://localhost:11434/api/create",
    json={"name": "quantum-assistant", "modelfile": modelfile_content},
)
```

## llama.cpp

Let's end on the most technical tool, but also lowest resource usage tool.

llama.cpp is the foundation many other tools build upon, including Ollama. It's a highly optimized C++ implementation for running LLMs, focusing on maximum performance and efficiency. Think of it as the "engine" that powers much of the local AI inference ecosystem.

By using this engine directly, you can have the most control over the model and inference. However, it does require a bit more technical expertise to use than the other tools.

<Tip>
Installation instructions can be found at [llama.cpp GitHub repository](https://github.com/ggerganov/llama.cpp#build). Instructions are hardware specific, so make sure to follow the instructions for your operating system and hardware to get the best performance.
</Tip>

The core features of llama.cpp are:

- **Highly optimized C++ implementation**: Maximum performance
- **Advanced quantization options**: Reduce model size
- **Metal/GPU/CPU acceleration**: Use available hardware
- **Extensive model format support**: Run various models
- **Low-level control over inference**: Fine-tune everything
- **Memory-efficient operation**: Run on limited hardware

### Technical Deep Dive into llama.cpp

Let's look at basic examples of how to use llama.cpp through both its server and command-line interfaces:

```bash
# Start the server with specific model and parameters
llama-server \
    --model unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF \
    --port 8080 \
    --host 0.0.0.0 

# Run direct inference from command line
llama-run \
    --model unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF \
    --prompt "Explain quantum computing:" \
    --temp 0.7 \
    --top-p 0.95 \
    --repeat-penalty 1.1
```

### Advantages
| Pros | Cons |
|------|------|
| Maximum performance and efficiency | Steeper learning curve |
| Granular control over model parameters | Requires technical expertise |
| Supports wide range of hardware | Manual compilation may be needed |
| Minimal resource requirements | Less user-friendly interface |
| Excellent for embedded systems | |

### Disadvantages
- **Steeper learning curve**: Technical expertise needed
- **Requires technical expertise**: Not for beginners
- **Manual compilation may be needed**: Complex setup
- **Less user-friendly interface**: Command-line only

Build instructions are available in the [llama.cpp GitHub repository](https://github.com/ggerganov/llama.cpp#build).

### Python Integration
The `llama-cpp-python` package provides comprehensive Python bindings:

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="models/7B/ggml-model-q4_0.gguf", n_ctx=2048, n_batch=512, n_gpu_layers=32
)

# Generate completion
output = llm("Explain quantum computing: ", max_tokens=2048, stop=["Q:", "\n"], echo=True)
```

Beyond the basic usage, there are many advanced features available:

- **Quantization Control**: Fine-grained model compression
- **Memory Mapping**: Efficient model loading
- **GPU Layer Management**: Control GPU resource usage
- **Custom Sampling**: Advanced text generation control

## Choosing the Right Tool

Your choice of tool should depend on your specific needs and technical expertise:

### Choose **Ollama** when you:
- Want to quickly get started with local AI
- Need to integrate AI into your applications
- Prefer a command-line interface but want simplicity
- Are building standard chat and completion features

### Choose **LMStudio** when you:
- Prefer a visual interface for working with models
- Need to experiment with different models
- Want to focus on prompt engineering
- Are learning about AI and want immediate feedback

### Choose **Jan AI** when you:
- Want a ChatGPT-like experience offline
- Need to process documents locally
- Value privacy in your AI workflows
- Want to combine local and cloud AI services

### Choose **llama.cpp** when you:
- Need maximum performance
- Have resource constraints
- Are building custom AI applications
- Want complete control over model behavior

## Next Steps

Now that you understand the available tools, try them out in this order:
1. Start with Jan AI or LMStudio for a gentle introduction
2. Move to Ollama when you're comfortable with basic concepts
3. Experiment with llama.cpp when you need more control or performance

Remember, the best way to learn is by doing. Start with simple projects and gradually increase complexity as you become more comfortable with these tools.

