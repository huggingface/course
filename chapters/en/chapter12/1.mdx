# Advanced Inference

Inference is the process of using a model to generate an output like text, images, predictions, or other modalities. In this chapter, we'll focus on inference with LLMs for text generation.

While inference might seem like most straightforward part of using a model, deploying and inferring efficiently requires some consideration of the various factors like performance, cost, and reliability. Large Language Models (LLMs) present unique challenges due to their size and computational requirements.

We'll explore the challenge of inference from multiple perspectives. We'll go from simple pipelines for vibe testing to production-ready solutions for large-scale deployments. We'll also explore inference both via APIs and through local inference, and we'll cover the various frameworks, libraries, and applications that can help you deploy and use LLMs.

## Contents

1️⃣ Fundamentals of Inference for LLMs - Understanding the core concepts of how LLMs process and generate text
2️⃣ APIs vs Local Inference - Comparing cloud-based and local approaches to running LLMs
3️⃣ Basic Pipeline Inference - Getting started with Hugging Face's pipeline abstraction for easy model usage
4️⃣ Optimized local inference with Ollama/ LMStudio/ llama.cpp - Exploring tools for running models efficiently on your own hardware
5️⃣ Optimized deployment inference with TGI/ vLLM - Learning about production-ready solutions for large-scale deployments

## Resources

- [Hugging Face Pipeline Tutorial](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference/en/index)
- [Pipeline WebServer Guide](https://huggingface.co/docs/transformers/en/pipeline_tutorial#using-pipelines-for-a-webserver)
- [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference)
- [Hugging Face Model Deployment Documentation](https://huggingface.co/docs/inference-endpoints/index)
- [vLLM: High-throughput LLM Serving](https://github.com/vllm-project/vllm)
- [Optimizing Transformer Inference](https://huggingface.co/blog/optimize-transformer-inference)