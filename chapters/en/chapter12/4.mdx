# Basic Pipeline Inference

Let's move on to inferring with LLMs for real! In this chapter, we'll explore the `pipeline` abstraction in Transformers, a powerful tool that simplifies the complex process of working with language models. While we introduced pipelines in [the previous chapter](chapters/en/chapter2/3.mdx), here we'll focus specifically on using them for text generation, one of the most common LLM tasks.

<Tip>
The `pipeline` is like a well-organized assembly line for your LLM tasks. It handles all the complex machinery behind the scenes, making it perfect for getting started and prototyping. However, for production systems that need maximum performance, you might want to explore more optimized approaches (which we'll cover in later chapters).
</Tip>

## How Pipelines Work

Building a machine learning application typically involves numerous technical steps and configurations. The Transformers pipeline abstracts away this complexity by automating the entire process from raw input to human-readable output through three  stages:

**1. Preprocessing Stage: Getting Your Data Model-Ready**

Just as a chef needs to prep ingredients before cooking, the pipeline prepares your inputs for the model. For text tasks, this means tokenization - breaking down your words and sentences into tokens (smaller pieces) that the model can process. This stage ensures your input is in the exact format the model expects.

**2. Model Inference: The Magic Happens**

During inference, the pipeline orchestrates several crucial operations:
- Efficiently batches your inputs to maximize computational resources
- Automatically selects the optimal device (CPU or GPU) for computation
- Applies performance optimizations like half-precision (FP16) inference where beneficial
- Manages the complex flow of data through the model's neural networks

**3. Postprocessing Stage: Output the Results**

Finally, the pipeline transforms the model's raw numerical outputs into meaningful, human-readable results by:
- Converting token IDs back into readable text
- Transforming mathematical logits into interpretable probability scores
- Using these probabilities to generate coherent text sequences

While the pipeline handles many technical details automatically, understanding these stages helps you make informed decisions about configuration options.

## Basic Usage

Getting started with text generation using a pipeline is straightforward. Here's a clear example that demonstrates the essential components:

```python
from transformers import pipeline

# Initialize the text generation pipeline
generator = pipeline(
    "text-generation",  # Specify the task type
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",  # Choose your model
    torch_dtype="auto",  # Optimize data types automatically
    device_map="auto",  # Select the best available device
)

# Generate text with your configured pipeline
response = generator(
    "Write a short poem about coding:",  # Your input prompt
    max_new_tokens=100,  # Control response length
    do_sample=True,  # Enable creative generation
    temperature=0.7,  # Fine-tune creativity level
)
print(response[0]["generated_text"])
```

## Key Configuration Options

How can we customize the pipeline to suit our needs? 

### Model Loading: Choosing Where to Run

Let's set up our infrastructure for running the pipeline and the model that we'll use.

```python
# Let's run on CPU - perfect for smaller models or when GPU isn't available
generator = pipeline(
    "text-generation", model="HuggingFaceTB/SmolLM2-135M-Instruct", device="cpu"
)

# Run on GPU (device 0) - great for faster processing!
generator = pipeline(
    "text-generation", model="HuggingFaceTB/SmolLM2-135M-Instruct", device=0
)

# Let the pipeline decide what's best - it's smart enough to figure it out!
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    device_map="auto",
    torch_dtype="auto",
)
```

In this example we loaded the model twice. Once with the device set to `cpu` and once with the device set to `0` which is the first GPU. The pipeline will automatically choose the best device for the model.

### Generation Parameters: Fine-tuning Your Output

Building on our understanding of [the token selection process](chapters/en/chapter12/2.mdx), let's explore how to control the model's text generation behavior. These parameters act like creative controls on a professional camera - each setting affects the final output in specific ways, and finding the right balance is key to getting optimal results.

```python
response = generator(
    "Translate this to French:",
    max_new_tokens=100,  # Maximum length of generated text
    do_sample=True,  # Enable probabilistic sampling for more natural text
    temperature=0.7,  # Balance between creativity (higher) and focus (lower)
    top_k=50,  # Limit consideration to the 50 most likely next tokens
    top_p=0.95,  # Sample from tokens comprising 95% of probability mass
    num_return_sequences=1,  # Number of alternative generations to produce
)
```

### Advanced Optimization: Balancing Speed and Quality

Beyond basic generation controls, the pipeline offers several ways to optimize performance. Let's explore these options while keeping our focus on practical applications.

#### 1. Quantization: Efficient Model Compression
Quantization reduces model size and increases inference speed while preserving most of the model's capabilities. Think of it like compressing a high-resolution image - you save space while keeping the important details. We'll explore specific quantization tools in [the next section](chapters/en/chapter12/5.mdx).

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Configure quantization settings
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Enable 4-bit precision
    bnb_4bit_compute_dtype="float16",  # Use efficient computation format
    bnb_4bit_quant_type="nf4",  # Select modern quantization method
)

# Load the model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "HuggingFaceTB/SmolLM2-1.7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
)
```

#### 2. Memory Management: Optimizing GPU Resources

GPUs excel at parallel processing, but their effectiveness depends on efficient memory usage. Just as a clean workspace helps you work better, proper GPU memory management ensures optimal performance.

```python
import torch

# Clear GPU memory cache
torch.cuda.empty_cache()

# Monitor memory usage
torch.cuda.memory_summary()
```

## Processing Multiple Inputs: Harnessing Batch Processing

One of the pipeline's most powerful features is its ability to process multiple inputs simultaneously, significantly improving throughput. Here's how to leverage batch processing effectively:

```python
# Prepare multiple prompts
prompts = [
    "Write a haiku about programming:",
    "Explain what an API is:",
    "Write a short story about a robot:",
]

# Process all prompts efficiently
responses = generator(
    prompts,
    batch_size=4,  # Process prompts in groups of 4
    max_new_tokens=100,  # Set consistent output length
    do_sample=True,  # Enable creative variation
    temperature=0.7,  # Maintain creativity balance
)

# Display results clearly
for prompt, response in zip(prompts, responses):
    print(f"üìù Prompt: {prompt}")
    print(f"ü§ñ Response: {response[0]['generated_text']}\n")
```

## Learn More

To deepen your understanding of pipelines and LLM inference, explore these comprehensive resources:

- [Hugging Face Pipeline Tutorial](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Pipeline API Reference](https://huggingface.co/docs/transformers/en/main_classes/pipelines)
- [Text Generation Parameters](https://huggingface.co/docs/transformers/en/main_classes/text_generation)
- [Model Quantization Guide](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one)


