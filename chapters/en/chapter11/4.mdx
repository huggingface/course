<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb"},
]} />

# Fine-Tuning Process with SFTTrainer in Transformers Reinforcement Learning

In this section, we'll walk through the process of fine-tuning a model using the `SFTTrainer` class from the Transformers Reinforcement Learning (TRL) library, which is built on top of the `transformers` library.

## Dataset Preparation

The supervised fine-tuning process involves adjusting a model's weights on a task-specific dataset. Your dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model. The data format needs to be compatible with the model's chat template. Below is an example of a dataset that can be used for supervised fine-tuning.

<iframe
  src="https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0"
  frameborder="0"
  width="100%"
  height="360px"
></iframe>

## Understanding Training Dynamics

When fine-tuning language models, understanding the training dynamics is crucial for monitoring progress and ensuring successful adaptation. Let's look at how to interpret the training process through loss curves.

### Loss Patterns

The training loss curve typically follows a characteristic pattern. Initially, you'll observe a sharp drop in loss as the model begins adapting to the new data distribution, task objectives, and chat template. This early phase is crucial as it indicates whether the model is successfully learning from the training data.

### The Path to Convergence

As training progresses, the loss curve should gradually stabilize. The key indicator of healthy training is a small gap between training and validation loss, suggesting the model is learning generalizable patterns rather than memorizing specific examples. The absolute loss values will vary depending on your task and dataset.

### Monitoring Training Progress

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter11/loss_curve.png" alt="Training and validation loss curves showing healthy convergence" width="600"/>
</div>

The graph above shows a typical training progression. Notice how both training and validation loss decrease sharply at first, then gradually level off. This pattern indicates the model is learning effectively while maintaining generalization ability.

### Warning Signs to Watch For

Several patterns in the loss curves can indicate potential issues:

1. If the validation loss starts increasing while training loss continues to decrease, your model is likely overfitting to the training data. Consider:
   - Reducing the model size or training time
   - Adding regularization
   - Increasing the dataset size
   - Using techniques like early stopping

2. If the loss doesn't show significant improvement, the model might be:
   - Learning too slowly (try increasing the learning rate)
   - Struggling with the task (check data quality and task complexity)
   - Hitting architecture limitations (consider a different model)

3. Extremely low loss values could suggest memorization rather than learning. This is particularly concerning if:
   - The model performs poorly on new, similar examples
   - The outputs lack diversity
   - The responses are too similar to training examples

<Tip warning={true}>

Monitor both the loss values and the model's actual outputs during training. Sometimes the loss can look good while the model develops unwanted behaviors. Regular qualitative evaluation of the model's responses helps catch issues that metrics alone might miss.

</Tip>

## Training Configuration

We will configure SFT trainer with the following parameters:

| Parameter | Description |
|-----------|-------------|
| num_train_epochs | The total number of training epochs to run (e.g., 1-3 epochs) |
| per_device_train_batch_size | The number of training examples processed per GPU in one forward/backward pass (typically 2-8 for large models) |
| gradient_accumulation_steps | Number of updates to accumulate before performing a backward pass, effectively increasing batch size |
| learning_rate | The step size for model weight updates during training (typically 2e-4 for fine-tuning) |
| gradient_checkpointing | Memory optimization technique that trades computation for memory by recomputing intermediate activations |
| warmup_ratio | Portion of training steps used for learning rate warmup (e.g., 0.03 = 3% of steps) |
| logging_steps | Frequency of logging training metrics and progress (e.g., every 10 steps) |
| save_strategy | When to save model checkpoints (e.g., "epoch" saves after each epoch, "steps" saves every N steps) |

In general, start with a small number of epochs and data using the default parameters in `trl.SFTTrainer`. As you get more comfortable with the process, you can experiment with different configurations to see how they affect the model's performance.

## Training and Evaluation

Fortunately, the `SFTTrainer` class handles the training and evaluation process for us. We just need to pass in the appropriate parameters and call the `train()` method. For the sake of education, let's break down what happens behind the scenes.

- Iterating over the dataset
- Computing the loss
- Updating the model's parameters
- Regular evaluation on a validation set

Throughout the process, continuous evaluation is essential. You'll want to monitor the model's performance on a validation set to ensure it's learning the desired behaviors without losing its general capabilities. 

## `SFTTrainer` from Transformer Reinforcement Learning

Transformer Reinforcement Learning (TRL) is a toolkit used to train transformer language models using reinforcement learning (RL) and post-training techniques. Built on top of the Hugging Face Transformers library, TRL allows users to directly load pretrained language models and supports most decoder and encoder-decoder architectures. The library facilitates major processes of RL used in language modelling, including supervised fine-tuning (SFT), reward modeling (RM), proximal policy optimization (PPO), and Direct Preference Optimization (DPO). But we'll focus on SFT in this chapter.

Here's a basic simplified example of how to use `SFTTrainer` to fine-tune a model. We'll expand on this example in the next few sections, but for now let's just focus on the basics.

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset(path="HuggingFaceTB/smoltalk", name="everyday-conversations")

training_args = SFTConfig(
    max_seq_length=512,
    output_dir="/tmp",
)

trainer = SFTTrainer(
    model_name="HuggingFaceTB/SmolLM2-135M",
    train_dataset=dataset,
    args=training_args,
)
trainer.train()
```

Just like in `transformers`, we work through the following steps:

1. Load the dataset
2. Configure the SFTTrainer with appropriate parameters
3. Train the model and monitor its progress
4. Save and evaluate the fine-tuned model

<Tip>

‚úèÔ∏è **Try it out!** Use the `HuggingFaceTB/smoltalk` dataset to fine-tune a `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model.

For this exercise, you'll need to:
1. Load and prepare your chosen dataset
2. Configure the SFTTrainer with appropriate parameters
3. Train the model and monitor its progress
4. Save and evaluate the fine-tuned model

</Tip>

# Supervised Fine-Tuning with SFTTrainer

In this section we will unpack the `SFTTrainer` class and see how it works. We'll also see how to use it to fine-tune a model. We will demonstrate how to fine-tune the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model.

## Load the base model

Here we'll load the base model and tokenizer. We'll also set up the chat format for the model.

```python
# Import necessary libraries
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer, setup_chat_format
import torch

# Set the device to use for training
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps" if torch.backends.mps.is_available() else "cpu"
)

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
).to(device)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)

# Set up the chat format
model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)
```

## Generate with the base model

First we will try out the base model which does not have a chat template. Later, we can compare the results of the base model with the fine-tuned model.

```python
# Let's test the base model before training
prompt = "Write a haiku about programming"

# Format with template
messages = [{"role": "user", "content": prompt}]
formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)

# Generate response
inputs = tokenizer(formatted_prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=100)
```

## Dataset Preparation

We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.

**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,.

```python
dataset = load_dataset(path="HuggingFaceTB/smoltalk", name="everyday-conversations")
```

## Configuring the SFTTrainer

The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources.

```python
# Configure the SFTTrainer
sft_config = SFTConfig(
    output_dir="./sft_output",
    max_steps=1000,  # Adjust based on dataset size and desired training duration
    per_device_train_batch_size=4,  # Set according to your GPU memory capacity
    learning_rate=5e-5,  # Common starting point for fine-tuning
    logging_steps=10,  # Frequency of logging training metrics
    save_steps=100,  # Frequency of saving model checkpoints
    evaluation_strategy="steps",  # Evaluate the model at regular intervals
    eval_steps=50,  # Frequency of evaluation
    use_mps_device=(
        True if device == "mps" else False
    ),  # Use MPS for mixed precision training
    hub_model_id=finetune_name,  # Set a unique name for your model
)

# Initialize the SFTTrainer
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=ds["train"],
    tokenizer=tokenizer,
    eval_dataset=ds["test"],
)
```

## Training the model

With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss.

```python
trainer.train()
```



<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/nlp_course_sft_loss_graphic.png" alt="SFTTrainer Training" />

## üíê Nice work!

This page provided a step-by-step guide to fine-tuning the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:

- Try this notebook on a harder difficulty
- Review a colleagues PR
- Improve the course material via an Issue or PR.


## Resources

- [TRL Documentation](https://huggingface.co/docs/trl)
- [SFT Examples Repository](https://github.com/huggingface/trl/tree/main/examples/sft)