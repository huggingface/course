<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb"},
]} />

# Supervised Fine-Tuning

Supervised Fine-Tuning (SFT) is a critical process for adapting pre-trained language models to specific tasks or domains. While pre-trained models have impressive general capabilities, they often need to be customized to excel at particular use cases. SFT bridges this gap by further training the model on relevant datasets with human-validated examples.

Because of the supervised structure of the task, the model can learn to generate structured outputs. For example, the chat templates we created in the previous sections.

## Understanding Supervised Fine-Tuning

Supervised fine-tuning is about teaching a pre-trained model to perform specific tasks, and use specific output structures, through examples of labeled tokens. The process involves showing the model many examples of the desired input-output behavior, allowing it to learn the patterns specific to your use case. 

SFT is effective because it uses the foundational knowledge acquired during pre-training while adapting the model's behavior to match your specific needs.

## When to Use Supervised Fine-Tuning

The decision to use SFT often comes down to the gap between your model's current capabilities and your specific requirements. SFT becomes particularly valuable when you need precise control over the model's outputs or when working in specialized domains.

Two core reasons to use SFT are:

1. **Template Control**: SFT allows you to control the output structure of the model, ensuring that it generates outputs in a specific format. For example, you need a specific chat template to generate structured outputs.

2. **Domain-Specific Requirements**: SFT is effective when you need precise control over the model's outputs in specialized domains. For example, if you're developing a customer service application, you might want your model to consistently follow company guidelines and handle technical queries in a standardized way. SFT can help align the model's responses with professional standards and domain expertise.

## Quiz

### 1. What is the primary purpose of Supervised Fine-Tuning (SFT)?

<Question
	choices={[
		{
			text: "To train a language model from scratch",
			explain: "SFT builds upon pre-trained models rather than training from scratch."
		},
		{
			text: "To adapt a pre-trained model to specific tasks or domains while maintaining its foundational knowledge",
			explain: "Correct! SFT allows models to learn specific tasks while leveraging their pre-trained capabilities.",
			correct: true
		},
		{
			text: "To compress a large language model into a smaller one",
			explain: "This is more related to model distillation, not SFT."
		}
	]}
/>

### 2. Which of the following are valid reasons to use SFT?

<Question
	choices={[
		{
			text: "Template Control - ensuring the model generates outputs in a specific format",
			explain: "Yes! SFT helps enforce specific output structures through training examples.",
			correct: true
		},
		{
			text: "Domain Adaptation - teaching the model domain-specific knowledge and terminology",
			explain: "Correct! SFT is excellent for adapting models to specialized domains.",
			correct: true
		},
		{
			text: "Model Architecture Changes - modifying the underlying structure of the model",
			explain: "SFT doesn't change the model architecture, it only updates the weights."
		}
	]}
/>

### 3. What is required for effective Supervised Fine-Tuning?

<Question
	choices={[
		{
			text: "A pre-trained language model",
			explain: "Yes! SFT starts with a pre-trained model as its foundation.",
			correct: true
		},
		{
			text: "Validated examples of desired input-output behavior",
			explain: "Correct! Quality training data is crucial for successful SFT.",
			correct: true
		},
		{
			text: "A high performing reference model",
			explain: "SFT uses existing architectures rather than creating new ones."
		}
	]}
/>

### 4. How does SFT relate to chat templates?

<Question
	choices={[
		{
			text: "SFT can train models to consistently follow specific chat templates",
			explain: "Correct! SFT helps models learn to generate responses in the desired template format.",
			correct: true
		},
		{
			text: "Chat templates are not compatible with SFT",
			explain: "Incorrect! Chat templates are commonly used with SFT for structured outputs."
		},
		{
			text: "SFT automatically creates chat templates",
			explain: "SFT doesn't create templates, it trains models to use existing templates."
		}
	]}
/>

### 5. What distinguishes SFT from pre-training?

<Question
	choices={[
		{
			text: "SFT uses labeled data for specific tasks",
			explain: "Yes! SFT requires examples of desired behavior for specific tasks.",
			correct: true
		},
		{
			text: "SFT is faster than pre-training",
			explain: "The speed difference isn't a defining characteristic; it depends on various factors."
		},
		{
			text: "SFT requires more data than pre-training",
			explain: "Actually, SFT typically uses less data than pre-training, focusing on task-specific examples."
		}
	]}
/>

# Fine-Tuning Process with SFTTrainer in Transformers Reinforcement Learning

In this section, we'll walk through the process of fine-tuning a model using the `SFTTrainer` class from the Transformers Reinforcement Learning (TRL) library, which is built on top of the `transformers` library.

## Dataset Preparation

The supervised fine-tuning process involves adjusting a model's weights on a task-specific dataset. Your dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model. The data format needs to be compatible with the model's chat template. Below is an example of a dataset that can be used for supervised fine-tuning.

<iframe
  src="https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0"
  frameborder="0"
  width="100%"
  height="360px"
></iframe>

## Understanding Training Dynamics

When fine-tuning language models, understanding the training dynamics is crucial for monitoring progress and ensuring successful adaptation. Let's look at how to interpret the training process through loss curves.

### Loss Patterns

The training loss curve typically follows a characteristic pattern. Initially, you'll observe a sharp drop in loss as the model begins adapting to the new data distribution, task objectives, and chat template. This early phase is crucial as it indicates whether the model is successfully learning from the training data.

### The Path to Convergence

As training progresses, the loss curve should gradually stabilize. The key indicator of healthy training is a small gap between training and validation loss, suggesting the model is learning generalizable patterns rather than memorizing specific examples. The absolute loss values will vary depending on your task and dataset.

### Monitoring Training Progress

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter11/loss_curve.png" alt="Training and validation loss curves showing healthy convergence" width="600"/>
</div>

The graph above shows a typical training progression. Notice how both training and validation loss decrease sharply at first, then gradually level off. This pattern indicates the model is learning effectively while maintaining generalization ability.

### Warning Signs to Watch For

Several patterns in the loss curves can indicate potential issues:

1. If the validation loss starts increasing while training loss continues to decrease, your model is likely overfitting to the training data. Consider:
   - Reducing the model size or training time
   - Adding regularization
   - Increasing the dataset size
   - Using techniques like early stopping

2. If the loss doesn't show significant improvement, the model might be:
   - Learning too slowly (try increasing the learning rate)
   - Struggling with the task (check data quality and task complexity)
   - Hitting architecture limitations (consider a different model)

3. Extremely low loss values could suggest memorization rather than learning. This is particularly concerning if:
   - The model performs poorly on new, similar examples
   - The outputs lack diversity
   - The responses are too similar to training examples

<Tip warning={true}>

Monitor both the loss values and the model's actual outputs during training. Sometimes the loss can look good while the model develops unwanted behaviors. Regular qualitative evaluation of the model's responses helps catch issues that metrics alone might miss.

</Tip>

## Training Configuration

We will configure SFT trainer with the following parameters:

| Parameter | Description |
|-----------|-------------|
| num_train_epochs | The total number of training epochs to run (e.g., 1-3 epochs) |
| per_device_train_batch_size | The number of training examples processed per GPU in one forward/backward pass (typically 2-8 for large models) |
| gradient_accumulation_steps | Number of updates to accumulate before performing a backward pass, effectively increasing batch size |
| learning_rate | The step size for model weight updates during training (typically 2e-4 for fine-tuning) |
| gradient_checkpointing | Memory optimization technique that trades computation for memory by recomputing intermediate activations |
| warmup_ratio | Portion of training steps used for learning rate warmup (e.g., 0.03 = 3% of steps) |
| logging_steps | Frequency of logging training metrics and progress (e.g., every 10 steps) |
| save_strategy | When to save model checkpoints (e.g., "epoch" saves after each epoch, "steps" saves every N steps) |

In general, start with a small number of epochs and data using the default parameters in `trl.SFTTrainer`. As you get more comfortable with the process, you can experiment with different configurations to see how they affect the model's performance.

## Training and Evaluation

Fortunately, the `SFTTrainer` class handles the training and evaluation process for us. We just need to pass in the appropriate parameters and call the `train()` method. For the sake of education, let's break down what happens behind the scenes.

- Iterating over the dataset
- Computing the loss
- Updating the model's parameters
- Regular evaluation on a validation set

Throughout the process, continuous evaluation is essential. You'll want to monitor the model's performance on a validation set to ensure it's learning the desired behaviors without losing its general capabilities. 

## `SFTTrainer` from Transformer Reinforcement Learning

Transformer Reinforcement Learning (TRL) is a toolkit used to train transformer language models using reinforcement learning (RL) and post-training techniques. Built on top of the Hugging Face Transformers library, TRL allows users to directly load pretrained language models and supports most decoder and encoder-decoder architectures. The library facilitates major processes of RL used in language modelling, including supervised fine-tuning (SFT), reward modeling (RM), proximal policy optimization (PPO), and Direct Preference Optimization (DPO). But we'll focus on SFT in this chapter.

Here's a basic simplified example of how to use `SFTTrainer` to fine-tune a model. We'll expand on this example in the next few sections, but for now let's just focus on the basics.

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset(path="HuggingFaceTB/smoltalk", name="everyday-conversations")

training_args = SFTConfig(
    max_seq_length=512,
    output_dir="/tmp",
)

trainer = SFTTrainer(
    model_name="HuggingFaceTB/SmolLM2-135M",
    train_dataset=dataset,
    args=training_args,
)
trainer.train()
```

Just like in `transformers`, we work through the following steps:

1. Load the dataset
2. Configure the SFTTrainer with appropriate parameters
3. Train the model and monitor its progress
4. Save and evaluate the fine-tuned model

<Tip>

‚úèÔ∏è **Try it out!** Use the `HuggingFaceTB/smoltalk` dataset to fine-tune a `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model.

For this exercise, you'll need to:
1. Load and prepare your chosen dataset
2. Configure the SFTTrainer with appropriate parameters
3. Train the model and monitor its progress
4. Save and evaluate the fine-tuned model

</Tip>

# Supervised Fine-Tuning with SFTTrainer

Let's dive into the `SFTTrainer` class and see how it works. We'll also see how to use it to fine-tune a model. We will demonstrate how to fine-tune the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model.

## Load the base model

Here we'll load the base model and tokenizer. We'll also set up the chat format for the model.

```python
# Import necessary libraries
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer, setup_chat_format
import torch

# Set the device to use for training
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps" if torch.backends.mps.is_available() else "cpu"
)

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
).to(device)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)

# Set up the chat format
model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)
```

## Generate with the base model

First we will try out the base model which does not have a chat template. Later, we can compare the results of the base model with the fine-tuned model.

```python
# Let's test the base model before training
prompt = "Write a haiku about programming"

# Format with template
messages = [{"role": "user", "content": prompt}]
formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)

# Generate response
inputs = tokenizer(formatted_prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=100)
```

## Dataset Preparation

We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.

**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,.

```python
dataset = load_dataset(path="HuggingFaceTB/smoltalk", name="everyday-conversations")
```

## Configuring the SFTTrainer

The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources.

```python
# Configure the SFTTrainer
sft_config = SFTConfig(
    output_dir="./sft_output",
    max_steps=1000,  # Adjust based on dataset size and desired training duration
    per_device_train_batch_size=4,  # Set according to your GPU memory capacity
    learning_rate=5e-5,  # Common starting point for fine-tuning
    logging_steps=10,  # Frequency of logging training metrics
    save_steps=100,  # Frequency of saving model checkpoints
    evaluation_strategy="steps",  # Evaluate the model at regular intervals
    eval_steps=50,  # Frequency of evaluation
    use_mps_device=(
        True if device == "mps" else False
    ),  # Use MPS for mixed precision training
    hub_model_id=finetune_name,  # Set a unique name for your model
)

# Initialize the SFTTrainer
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=ds["train"],
    tokenizer=tokenizer,
    eval_dataset=ds["test"],
)
```

## Training the model

With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss.

```python
trainer.train()
```



<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/nlp_course_sft_loss_graphic.png" alt="SFTTrainer Training" />

## üíê Nice work!

This page provided a step-by-step guide to fine-tuning the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:

- Try this notebook on a harder difficulty
- Review a colleagues PR
- Improve the course material via an Issue or PR.


## Resources

- [TRL Documentation](https://huggingface.co/docs/trl)
- [SFT Examples Repository](https://github.com/huggingface/trl/tree/main/examples/sft)