# Classifying audio

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter10/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter10/section4.ipynb"},
]} />

Audio classification is one of the most common applications of transformers in audio and speech processing. Like other classification tasks in machine learning, this task involves assigning a label to a given audio clip. For example, in the case of speech, we might want to detect keywords like "Hey Siri", or infer an intent like "temperature" from a spoken query like "What is the weather doing today?". Environmental sounds provide another example, where we might want to automatically distinguish between sounds such as ‚Äúcar horn‚Äù, ‚Äúsiren‚Äù, ‚Äúdog barking‚Äù, etc.

In this section, we'll look at how pretrained audio transformers can be used to classify songs into genres like pop and rock. This is an important part of music streaming platforms like [Spotify](https://en.wikipedia.org/wiki/Spotify), which recommend songs that are similar to the ones the user is listening to.

By the end of this section, you'll know how to:

* Use the ü§ó Datasets library load and preprocess audio data
* Find suitable pretrained models for audio classification
* Extract audio features from raw audio data
* Fine-tune a pretrained model to classify songs by genre
* Build a Gradio demo that lets you classify your own songs

To get started, let's download some music from the Hugging Face Hub!

## The dataset

To train our model, we'll use the [GTZAN dataset](https://huggingface.co/datasets/marsyas/gtzan), which is a popular dataset of 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning disco to metal. We can get the audio files from the Hugging Face Hub with the `load_dataset()` function from ü§ó Datasets:

```python
from datasets import load_dataset

gtzan = load_dataset("marsyas/gtzan")
gtzan
```

```out
Dataset({
    features: ['file', 'audio', 'genre'],
    num_rows: 999
})
```

<Tip warning={true}>

One of the recordings in GTZAN is corrupted, so it's been removed from the dataset. That's why we have 999 examples instead of 1,000.

</Tip>


GTZAN doesn't provide a predefined validation set, so we'll have to create one ourselves. The dataset is balanced across genres, so we can use the `train_test_split()` method to quickly create a 90/10 split as follows::

```python
gtzan = gtzan["train"].train_test_split(seed=42, test_size=0.1)
gtzan
```

```out
DatasetDict({
    train: Dataset({
        features: ['file', 'audio', 'genre'],
        num_rows: 899
    })
    test: Dataset({
        features: ['file', 'audio', 'genre'],
        num_rows: 100
    })
})
```

Great, now that we've got our training and validation sets, let's take a look at one of the audio files:

```python
gtzan["train"][0]
```

```out
{
    "file": "~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav",
    "audio": {
        "path": "~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav",
        "array": array(
            [
                0.10720825,
                0.16122437,
                0.28585815,
                ...,
                -0.22924805,
                -0.20629883,
                -0.11334229,
            ],
            dtype=float32,
        ),
        "sampling_rate": 22050,
    },
    "genre": 7,
}
```

As we saw in [section 2](/course/chapter10/2), the audio files are represented as NumPy arrays in the `audio.array` feature. For these songs, the sampling rate is 22,050 Hz, so we'll have to keep that in mind when using a pretrained model with a different sampling rate. We can also see the genre is represented as an integer, so let's use the `int2str()` method of the `genre` feature to map these integers to human-readable names:

```python
id2label_fn = gtzan["train"].features["genre"].int2str
id2label_fn(gtzan["train"][0]["genre"])
```

```out
'pop'
```

This label looks correct, since it matches the filename of the audio file. Let's now listen to a few more examples by using Gradio to create a simple interface with the `Blocks` API:

```python
def generate_audio():
    example = gtzan["train"].shuffle()[0]
    audio = example["audio"]
    return (
        audio["sampling_rate"],
        audio["array"],
    ), id2label_fn(example["genre"])


with gr.Blocks() as demo:
    with gr.Column():
        for _ in range(4):
            audio, label = generate_audio()
            output = gr.Audio(audio, label=label)

demo.launch(debug=True)
```

<iframe src="https://course-demos-gtzan-samples.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

From these samples we can certainly hear the difference between genres, but can a transformer do this too? Let's train a model to find out! First, we'll need to find a suitable pretrained model for this task. Let's see how we can do that.

## Picking a pretrained model for audio classification

To get started, let's pick a suitable pretrained model for audio classification. In this domain, pretraining is typically carried out on large amounts of unlabeled speech data, using datasets like [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) and ][Voxpopuli](https://huggingface.co/datasets/facebook/voxpopuli). The best way to find these models on the Hugging Face Hub is to use the "Automatic Speech Recognition" or "Feature Extraction" filters. Although models like Wav2Vec2 and HuBERT are very poopular, we'll use a model called DistilHuBERT. This is a much smaller version of [HuBERT](https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/hubert#overview), which trains around 73% faster, yet preserves most of the performance.

>> ADD ASR plot

<iframe src="https://autoevaluate-leaderboards.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

## From audio to machine learning features

## Preprocessing the data

Similar to tokenization in NLP, audio and speech Transformers require the input to be encoded in a format that the model can process. In ü§ó Transformers, the conversion from audio to the input format is handled by the _feature extractor_ of the model. Similar to tokenizers, ü§ó Transformers provides a convenient `AutoFeatureExtractor` class that can automatically select the correct feature extractor for a given model. To see how we can process our audio files, let's begin by instantiating the feature extractor for DistilHuBERT:


```python
from transformers import AutoFeatureExtractor

model_id = "ntu-spml/distilhubert"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)
```

If we apply this feature extractor to one of our audio files, we'll get a dictionary with an `input_values` field that is the audio analog of a tokenized input:

```python
feature_extractor(gtzan["train"][0]["audio"]["array"])
```

```out
{
    "input_values": array(
        [
            [
                0.10720825,
                0.16122437,
                0.28585815,
                ...,
                -0.22924805,
                -0.20629883,
                -0.11334229,
            ]
        ],
        dtype=float32,
    )
}
```


HuBERT models operate directly on the raw waveform, so no further processing is required. However, the sampling rate of the model and the dataset are different, so we'll have to resample the audio file to 16,000 Hz before passing it to the feature extractor. We can do this by first obtaining the model's sample rate from the feature extractor:

```python
sampling_rate = feature_extractor.sampling_rate
sampling_rate
```

```out
16000
```

Next, we resample the dataset using the `cast_column()` method and `Audio` feature from ü§ó Datasets:

```python
from datasets import Audio

gtzan = gtzan.cast_column("audio", Audio(sampling_rate=sampling_rate))
```

If we now apply the feature extractor to the first training example, we'll see the array values have changed:

```python
feature_extractor(gtzan["train"][0]["audio"]["array"], sampling_rate=sampling_rate)
```

```out
{
    "input_values": array(
        [[0.09204921, 0.20080882, 0.4805243, ..., -0.18821615, -0.23222451, 0.0]],
        dtype=float32,
    )
}
```

<Tip warning={true}>

**Note**: It's always a good idea to pass an explicit value for the `sampling_rate` argument when processing audio with the feature extractor. If you don't pass this argument, the feature extractor will assume the input audio has the same sampling rate as the model and this can lead to subtle errors.

<Tip>


Great, so now we know how to process our resampled audio files, the last thing to do is define a function that we can apply to all the examples in the dataset. Since we expect the audio clips to be 30 seconds in length, we'll also truncate any longer clips by using the `max_length` and `truncation` arguments of the feature extractor as follows:


```python
max_duration = 30.0


def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=feature_extractor.sampling_rate,
        max_length=int(feature_extractor.sampling_rate * max_duration),
        truncation=True,
    )
    return inputs
```

With this function defined, we can now apply it to the dataset using the `map()` method. Since audio datasets can be quite slow to process, it is usually a good idea to use multiprocessing. We can do this by passing the `num_proc` argument to `map()` and we'll use Python's `psutil` module to determine the number of CPU cores on the system:

```python
import psutil

num_proc = psutil.cpu_count()
gtzan_encoded = gtzan.map(
    preprocess_function, remove_columns=["audio", "file"], batched=True, num_proc=num_proc
)
gtzan_encoded
```

```out
DatasetDict({
    train: Dataset({
        features: ['genre', 'input_values'],
        num_rows: 899
    })
    test: Dataset({
        features: ['genre', 'input_values'],
        num_rows: 100
    })
})
```

To simplify the training, we've removed the `audio` and `file` columns from the dataset. The `input_values` column contains the encoded audio files, and the `genre` column contains the corresponding labels. To enable the `Trainer` to process the labels, the last step is to rename

```
gtzan_encoded = gtzan_encoded.rename_column("genre", "label")
```

OK, we've now got a dataset that's ready for training! Let's take a look at how we can train a model on this dataset.


## Fine-tuning the model

To fine-tune the model, we'll use the `Trainer` class from ü§ó Transformers. As we've seen in other chapters, the `Trainer` is a high-level API that is designed to handle the most common training scenarios. In this case, we'll use the `Trainer` to fine-tune the model on GTZAN. To do this, we'll first need to load a model for this task. We can do this by using the `AutoModelForAudioClassification` class, which will automatically add the appropriate classification head to our pretrained DistilHuBERT model. Before doing so, however, we'll need to obtain the label mappings from the dataset. We can do this by using the `int2str()` method as follows:

```python
id2label = {
    str(i): id2label_fn(i) for i in range(len(gtzan["train"].features["genre"].names))
}
label2id = {v: k for k, v in id2label.items()}
id2label["7"]
```

```out
'pop'
```

This matches what we had earlier, so let's now instantiate the model:

```python
from transformers import AutoModelForAudioClassification

num_labels = len(id2label)

model = AutoModelForAudioClassification.from_pretrained(
    model_id,
    num_labels=num_labels,
    label2id=label2id,
    id2label=id2label,
)
```

The next step is to define the training arguments. We'll push our model to the Hub, so first we'll need to login to our Hugging Face account:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Once we've provided an access token to log in, we can define the training arguments as usual:

```python
from transformers import TrainingArguments

model_name = model_id.split("/")[-1]
batch_size = 4
num_train_epochs = 10

args = TrainingArguments(
    f"{model_name}-finetuned-gtzan",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=2,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    warmup_ratio=0.1,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,
    push_to_hub=True,
)
```

The last thing we need to do is define the metrics. Since the dataset is balanced, we'll use accuracy as our metric and load it using the ü§ó Evaluate library:

```python
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    """Computes accuracy on a batch of predictions"""
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

We've now got all the pieces! Let's instantiate the `Trainer` and train the model:

```python
from transformers import Trainer

trainer = Trainer(
    model,
    args,
    train_dataset=gtzan_encoded["train"],
    eval_dataset=gtzan_encoded["test"],
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics,
)

trainer.train()
```

```python
trainer.evaluate()
```

```out
{'eval_loss': 0.6693804860115051,
 'eval_accuracy': 0.82,
 'eval_runtime': 18.0643,
 'eval_samples_per_second': 5.536,
 'eval_steps_per_second': 0.72,
 'epoch': 9.99}
```

```python
kwargs = {"tags": "hf-course"}
trainer.push_to_hub(**kwargs)
```

## Building a demo with our model

```python
import librosa
import gradio as gr
from transformers import pipeline

pipe = pipeline("audio-classification", model="lewtun/distilhubert-finetuned-gtzan")


def classify_audio(filepath):
    audio, sampling_rate = librosa.load(filepath, sr=16_000)
    preds = pipe(audio)
    outputs = {}
    for p in preds:
        outputs[p["label"]] = p["score"]
    return outputs


demo = gr.Interface(
    fn=classify_audio, inputs=gr.Audio(type="filepath"), outputs=gr.outputs.Label()
)
demo.launch(debug=True)
```

<iframe src="https://course-demos-song-classifier.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>