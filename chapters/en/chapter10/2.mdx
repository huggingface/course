# A tour of audio applications

Similar to natural language processing, ü§ó Transformers can be applied to tackle some of the most common audio tasks that you're likely to encounter in the wild. Here are some examples:

* **Audio clasification:** Classify audio clips into different categories, e.g. "dog" or "cat" for a recording of a dog barking or a cat meowing.
* **Automatic speech recognition:** Transcribe audio clips into text, e.g. "How are you doing today?" for a recording of someone speaking.
* **Speaker diarization:** Identify which speaker is speaking at any given time in an audio clip, e.g. "Alice" or "Bob" for a recording of two people talking.
* **Automative speaker verification:** Verify whether the speaker of a pair of utterances are the same.

In this section we'll show you how to load audio datasets from the Hugging Face Hub, and how to analyze them with the `pipeline()` function from ü§ó Transformers. Let's start by loading some audio data and exploring its features.

## Working with audio data

Every audio or speech task starts with an audio file. Audio files come in many types and sizes, but they all have a few important properties:

- **Sampling rate**: the number of audio samples per second. Sampling rate is measured in Hertz (Hz), and the higher the number, the more detailed the audio. For example, CD-quality audio has a sampling rate of 44,100 Hz, while high-fidelity audio has a sampling rate of 192,000 Hz.
- **Bit depth**: the number of bits used to represent each audio sample. The higher the bit depth, the more detailed the audio. For example, CD-quality audio has a bit depth of 16 bits, while high-fidelity audio has a bit depth of 24 bits.
- **Duration**: the length of the audio file in seconds. Audio files can be as short as a few seconds, or as long as a few hours.

For this example, we'll use the [MINDS-14 dataset](https://huggingface.co/datasets/PolyAI/minds14), which contains recordings of people asking an e-banking system questions in several languages and dialects. The dataset is small and easy to use, so it's a great place to start. Let's begin by downloading the Australian subset of recordings from the Hub by using the `load_dataset()` function from ü§ó Datasets:

```python
from datasets import load_dataset

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
minds
```

```out
Dataset({
    features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
    num_rows: 654
})
```

The dataset contains 654 audio files, each of which is accompanied by a transcription, an English translation, and a label indicating the _intent_ behind the person's query. The `audio` column contains the raw audio data. Let's take a closer look at one of the examples:

```python
sample = minds[0]
sample
```

```out
{
    "path": "~/.cache/huggingface/datasets/downloads/extracted/8071243becc3009a383b66d678bc6cd6f5ef850f08482452d1e8289c21bbac47/en-AU~PAY_BILL/response_4.wav",
    "audio": {
        "path": "~/.cache/huggingface/datasets/downloads/extracted/8071243becc3009a383b66d678bc6cd6f5ef850f08482452d1e8289c21bbac47/en-AU~PAY_BILL/response_4.wav",
        "array": array(
            [0.0, 0.00024414, -0.00024414, ..., -0.00024414, 0.00024414, 0.0012207],
            dtype=float32,
        ),
        "sampling_rate": 8000,
    },
    "transcription": "I would like to pay my electricity bill using my card can you please assist",
    "english_transcription": "I would like to pay my electricity bill using my card can you please assist",
    "intent_class": 13,
    "lang_id": 2,
}
```

We can see that the `audio` column consists of three fields:

* `path`: The path to the audio file.
* `array`: The decoded audio data, represented as a 1-dimensional NumPy array.
* `sampling_rate`. The sampling rate of the audio file, which is the number of audio samples per second (8,000 Hz in this example).

We can also use the `int2str()` method of the `intent_class` feature to convert the integer labels to human-readable strings:

```python
id2label = minds.features["intent_class"].int2str
id2label(sample["intent_class"])
```

```out
'pay_bill'
```

Looking at the transcription, we can see that the audio file has recorded a person asking a question about paying a bill. The `intent_class` label confirms that the intent behind the query is indeed "paying a bill". Now that we've inspected the raw contents of the dataset, let's listen to a few examples! We'll use the `Blocks` and `Audio` features from Gradio to decode a few random samples from the dataset:

```python
import gradio as gr


def generate_audio():
    example = minds.shuffle()[0]
    audio = example["audio"]
    return (
        audio["sampling_rate"],
        audio["array"],
    ), id2label(example["intent_class"])


with gr.Blocks() as demo:
    with gr.Column():
        for _ in range(4):
            audio, label = generate_audio()
            output = gr.Audio(audio, label=label)

demo.launch(debug=True)
```

<iframe src="https://course-demos-audio-samples.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


Here we used the `Blocks` feature to display multiple audio samples in a column, while the `Audio` feature lets us play the audio files in the browser. From these samples you can hear that there are different speakers, different questions, and different backgrounds. This is a good thing! It means that the dataset is realistic and that it is challenging for a model to learn to detect human intent in the presence of all this variability.

<Tip>

‚úèÔ∏è **Try it out!** Download another dialect or language of the MINDS-14 dataset and listen to some samples to get a sense of the variation in the whole dataset. You can find the full list of available languages [here](https://huggingface.co/datasets/PolyAI/minds14).

</Tip>

Now that we've heard and seen the audio data, let's use the `pipeline()` function to automatically classify the intents!

## Audio classification

To classify an audio recording into a set of classes, we can use the `audio-classification` pipeline. By default, this pipeline uses a model trained for a task known as _keyword spotting_, which involves detecting a specific word in an audio recording. In our case, we need a model that's been fine-tuned for intent classification, and specifically on the MINDS-14 dataset. Luckily for us, the Hub has a model that does just that! Let's load it by using the `pipeline()` function:

```python
from transformers import pipeline

classifier = pipeline(
    "audio-classification",
    model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```

This pipeline expects the audio data as a NumPy array, so let's pass our sample from earlier:

```python
classifier(sample["audio"])
```

```out
[
    {"score": 0.9950760006904602, "label": "app_error"},
    {"score": 0.001438894192688167, "label": "address"},
    {"score": 0.0013526444090530276, "label": "high_value_payment"},
    {"score": 0.0007368774386122823, "label": "card_issues"},
    {"score": 0.0003506039211060852, "label": "abroad"},
]
```

Hmm, this doesn't look right. The model seems to have predicted the wrong intent! If we take a closer at the [model card](https://huggingface.co/facebook/wav2vec2-xls-r-300m) from the pretrained XLS-R model, we can see that it was pretrained on a variety of audio datasets at a sampling rate of 16 kHz. On the other hand, the MINDS-14 dataset is sampled at 8 kHz, which means we need to upsample it before we can use it with the model. Let's do that by using the `cast_column()` method with the `Audio` feature from ü§ó Datasets:

```python
minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

A visualisation of the resampling process is shown below:

<div class="flex justify-center">
    <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample.gif"/>
    <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/resample-dark.gif"/>
</div>

Now that the audio has been upsampled, let's try classifying it again:

```python
classifier(minds[0]["audio"]["array"])
```

```out
[
    {"score": 0.9631526470184326, "label": "pay_bill"},
    {"score": 0.028196919709444046, "label": "freeze"},
    {"score": 0.0032787465024739504, "label": "card_issues"},
    {"score": 0.0019414430717006326, "label": "abroad"},
    {"score": 0.0008378690690733492, "label": "high_value_payment"},
]
```

Great, now the model is predicting the correct intent! Let's now take a look at another very common task in speech processing, _automatic speech recognition_.


## Automatic speech recognition

Automatic speech recognition (ASR) is the task of transcribing audio data into text. This is a very useful task for many applications, including voice assistants, speech-to-text transcription, and more. In this section, we'll use the `automatic-speech-recognition` pipeline to transcribe an audio recording of a person asking a question about paying a bill:

```python
asr = pipeline("automatic-speech-recognition")
asr(minds[0]["audio"]["array"])
```

```out
{'text': 'I WOULD LIKE TO PAY MY ELECTRICITY BILL USING MY COD CAN YOU PLEASE ASSIST'}
```

The model seems to have done a pretty good job at transcribing the audio! It only got one word wrong ("card") compared to the original transcription, which is pretty good considering the speaker has an Australian accent, where the letter "r" is often silent. Having said that, I wouldn't recommend trying to pay your next electricity bill with a fish!

<Tip>

‚úèÔ∏è **Try it out!** Download another dialect or language of the MINDS-14 dataset and try transcribing some audio samples. For non-English audio, you will need to find an appropriate ASR model from the Hub. You can find a list of available models [here](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition).

</Tip>

Now that we've seen how to use pipelines for audio tasks, it's time to get our hands dirty and train our own models! In the next section, we'll see how to fine-tune a pretrained audio model to classify different genres of music!