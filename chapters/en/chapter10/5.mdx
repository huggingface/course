# Automatically recognising speech

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
]} />

In this section, we‚Äôll take a look at how Transformers can be used to convert speech into text, a task known _speech recognition_.

TODO: insert Speech -> Model -> Text diagram

Speech recognition, also known as automatic speech recognition (ASR) or speech-to-text (STT), is one of the most popular and exciting spoken language processing tasks. It‚Äôs used in a wide range of applications, including dictation, voice assistants, video captioning and meeting transcriptions.

You‚Äôve probably used a speech recognition system many times before without realising it! Consider the digital assistant in your smartphone device (Siri, Google Assistant, Alexa). When you use these assistants, the first thing that they do is transcribe your speech to written text so it'll be ready for use in a downstream task.

Image: https://images.app.goo.gl/J6SU6RYryXXVT9uj7
Demo: https://huggingface.co/facebook/wav2vec2-base-960h

Speech recognition is challenging as it requires joint knowledge of audio and text. The input audio might have lots of background noise, making it difficult to understand what's being said. The written text might have characters that don‚Äôt have an acoustic sound, such as punctuation, which are difficult to infer from audio alone. These are all hurdles we‚Äôll have to tackle when building effective speech recognition systems.

[//]: # Speech recognition is challenging as it requires a model to leverage knowledge about two domains concurrently: the input audio and the output text. Systems need to extract the relevant features from the input audio, possibly disentangling the spoken speech from background noise. From these features, they must then infer the contextual relation of words and sentences to form accurate and coherent sentences.

Now that we‚Äôve defined our task, we can begin looking into speech recognition in more detail. Specifically, we‚Äôll cover:
* How to choose a dataset
* How to load a dataset
* What models we can use
* How to prepare audio-text data
* Metrics for speech recognition
* How to fine-tune an ASR system with the Trainer API

### How to choose a dataset
As with any machine learning problem, our model is only as good as the data that we train it on. Speech recognition datasets vary considerably in how they are curated and the domains that they cover. To pick the right dataset, we need to match our criteria with the features that a dataset offers.

Before we pick a dataset, we first need to understand some of its key defining features.

#### 1. Number of hours
Simply put, the number of training hours indicates how large the dataset is. It‚Äôs analogous to the number of training examples in an NLP dataset. However, bigger datasets aren‚Äôt necessarily better. If we want a model that generalises well, we want a diverse dataset with lots of different speakers, domains and speaking styles.

#### 2. Domain
The domain entails where the data was sourced from, whether it be audiobooks, podcasts, YouTube, or financial meetings. Each domain has a different distribution of data. For example, audiobooks are recorded in high-quality studio conditions (with no background noise) and text that is taken from written literature. Whereas for YouTube, the audio likely contains more background noise, and the text is more informal since the speaker is more likely to be using casual language.

We need to match our domain to the conditions we anticipate at inference time. For instance, if we train our model on audiobooks, we can‚Äôt expect our model to perform well in noisy environments.

#### 3. Speaking style
The speaking style falls into one of two categories:
* Narrated: read from a script
* Spontaneous: unscripted, conversational speech

The audio and text data reflect the style of speaking. Since narrated text is scripted, it tends to be spoken articulately and without any errors:

‚ÄúConsider the task of training a model on a speech recognition dataset‚Äù

Whereas for spontaneous speech, we can expect a more colloquial style of speech, with the inclusion of repetitions, hesitations and false-starts:

‚ÄúLet‚Äôs <uhm> take a look at how <silence> you‚Äôd you‚Äôd go about training a model on <uhm> a sp- speech recognition dataset‚Äù (this is an extreme example)

#### 4. Transcription style
Whether the text has punctuation, casing or both. If we want a system to generate fully formatted text that could be used for a publication or meeting transcription, we require training data with punctuation and casing. If we just require the spoken words in an unformatted structure, neither punctuation nor casing are necessary.

A summary of datasets on the Hub:

|                | Train Hours | Domain                                | Speaking Style         | Transcription Casing | Transcription Punctuation | Recommended Usage                                 |
|----------------|-------------|---------------------------------------|------------------------|----------------------|---------------------------|---------------------------------------------------|
| LibriSpeech    | 960         | Audiobooks                            | Narrated               | ‚ùå                    | ‚ùå                         | Academic benchmarks                               |
| Common Voice 9 | 2224        | Wikipedia text + crowd-sourced speech | Narrated               | ‚úÖ                    | ‚úÖ                         | Day-to-day speech                                 |
| TEDLIUM        | 452         | TED talks                             | Narrated               | ‚ùå                    | ‚ùå                         | Technical scientific, political and social topics |
| Voxpopuli      | 543         | European Parliament recordings        | Spontaneous            | ‚ùå                    | ‚ùå                         | Non-native English speakers                       |
| GigaSpeech     | 10000       | Audiobook, podcast, youtube           | Narrated & spontaneous | ‚ùå                    | ‚úÖ                         | Robustness over multiple domains                  |
| SPGISpeech     | 5000        | Financial meetings                    | Narrated & spontaneous | ‚úÖ                    | ‚úÖ                         | Fully formatted transcriptions                    |


Let‚Äôs take an example where we want to train a speech recognition system to transcribe speech on the topic of machine learning (highly relevant!). Our speech will be narrated (scripted), and we‚Äôre not worried about punctuation or casing in our text output. From our reference table, it looks like TED-LIUM is a good choice!

[//]: # On review, this sub-section might be too involved. Happy to omit it and dive straight into how to load a dataset.

### How to load an audio dataset
Before we can handle any audio datasets, we need to make sure we have the right dependencies installed. Specifically, we'll need the [`Audio`](https://huggingface.co/docs/datasets/installation#audio) feature from ü§ó Datasets. This should take care of the main Python packages we need to read audio files from byte form and convert them to arrays.

Right! Now we're ready to go ahead and download our data. For this tutorial, we'll use the [_LibriSpeech ASR_](https://huggingface.co/datasets/librispeech_asr) corpus. LibriSpeech is one of the most popular datasets for benchmarking speech recognition systems in both academia and industry. It consists of approximately 1000 hours of narrated audiobooks collected from the [LibriVox](https://librivox.org/) project.


Let‚Äôs take a look at the [all-time leaderboard](https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean) on Papers with Code to get a feel for current model performances.
We can see that there‚Äôs been tremendous progress on the LibriSpeech benchmark in the last three years, with many of the top spots being occupied by Wav2Vec2 or Wav2Vec2-like systems! As mentioned in Section 4, Wav2Vec2 revolutionised the field of speech recognition by introducing an effective pre-training regime, much the same way as BERT revolutionised NLP. We can see that the word error rates (WERs) for this dataset are extremely low! We‚Äôll cover WER in more detail in this section, but for now we can see that the best performing system achieves a WER of just 1.4%, or a word accuracy rate of 100 - 1.4 = 98.6%! This is very impressive! We‚Äôll see how close we can get with our system ü§ó

Now that we've got better idea about the dataset that we're working with, let's download it using the `load_dataset()` function:

```python
from datasets import load_dataset, DatasetDict

raw_datasets = DatasetDict()

raw_datasets["train"] = load_dataset("librispeech_asr", "clean", split="train.100")
raw_datasets["validation"] = load_dataset("librispeech_asr", "clean", split="validation")
raw_datasets["test"] = load_dataset("librispeech_asr", "clean", split="test")
```
Note that audio datasets are quite large! For 100h of training data and 10h of validation/test data, we can expect to download ~60GB of raw data.

Let's inspect the dataset:
```python
raw_datasets
```

```
DatasetDict({
    train: Dataset({
        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
        num_rows: 28539
    })
    validation: Dataset({
        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
        num_rows: 2703
    })
    test: Dataset({
        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
        num_rows: 2620
    })
})
```
Great! We have our training, validation and test splits ready, with 28539, 2703 and 2620 samples respectively. The two data columns `audio` and `text` contain the most important information for our task: the raw audio inputs and the target text outputs.

Let‚Äôs take a look at the first example of the train split:
```python
raw_datasets["train"][0]
```

```
{'file': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/d2da1969fe9e7d06661b5dc370cf2e3c119a14c35950045bcb76243b264e4f01/374-180298-0000.flac',
 'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/d2da1969fe9e7d06661b5dc370cf2e3c119a14c35950045bcb76243b264e4f01/374-180298-0000.flac',
  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,
         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),
  'sampling_rate': 16000},
 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',
 'speaker_id': 374,
 'chapter_id': 180298,
 'id': '374-180298-0000'}
```

Alright! We can see that we have our target text sample ready - it looks very much like it's taken from an audiobook! But what on earth is going on with the audio? ü§î

As it turns out, we represent audio data digitally as a 1-dimensional array. Each array value denotes the amplitude of our audio signal at a particular time step. From the amplitude information, we can reconstruct the frequency spectrum of the audio and recover all the acoustic features.

Since our audio input is continuous, we can't represent an amplitude value for every possible time step. Instead, we _sample_ amplitude values at fixed time steps.

The interval with which we sample our audio is known as the _sampling rate_. For our dataset, we can see that the sampling rate is 16000, meaning 16000 amplitude values are provided each second. Keep the sampling rate in the back of your mind, it'll be important when we come to processing our data later on!

While 1-dimensional arrays are a suitable format for machines, they're not of much use to us as humans! We want to be able to **listen** to our audio to get a feel for the speech and recording conditions. Here, we have two options:

1. Convert our 1-dimensional array into a human-friendly format (mp3 or wav).
2. Look to the Hugging Face Hub!

Option 2 seems much easier to me! Let's head over to the [LibriSpeech ASR dataset card](https://huggingface.co/datasets/librispeech_asr) on the Hugging Face Hub.

Slap bang in the middle of the dataset card we have exactly what we're looking for: the dataset viewer! The dataset viewer shows us the first 100 samples of any dataset. What's more, it's already loaded the audio samples for us to listen to in real-time! If we hit the play button on the first sample, we can listen to the audio and see the corresponding text. Have a scroll through the samples in the train and test sets to get a better feel for the audio data that we're dealing with. You‚Äôll notice how clear the audio is and how well-spoken the sentences are. This gives us a clue as to why the top models do so well on LibriSpeech! The audio conditions are very conducive to high system performance.

### Models for speech recognition

We can decompose speech recognition models into two parts:
1. Encoder: an acoustic model that maps the raw audio input into a sequence of hidden-states.

diagram

2. Decoder: maps the sequence of hidden-states to logits over the vocabulary.

diagram

The encoder is typically a Wav2Vec2 architecture, introduced in Section 4 of this Chapter. We have some flexibility in our choice of the decoder. We could use a simple linear layer that maps the Wav2Vec2 hidden states directly to output logits over our vocabulary. Or, we could pair our Wav2Vec2 encoder with a decoder model, giving a speech encoder-decoder style model analogous to the NLP encoder-decoder model introduced in Chapter 1.

A simple linear layer will give a smaller, faster overall model but will be more susceptible to spelling and grammatical errors. Adding a decoder model greatly improves the quality of transcriptions because it is pre-trained on a large corpus of text, enabling us to leverage its learned text representations. The tradeoff is a larger, slower model.
Since we want our system to be robust to spelling and grammar, let‚Äôs go ahead and define a speech encoder-decoder style model.

We‚Äôll pair a pre-trained Wav2Vec2 encoder with a pre-trained BART decoder, yielding a Wav2Vev2-BART model:
```python
from transformers import SpeechEncoderDecoderModel

encoder_id = "facebook/wav2vec2-large-lv60"
decoder_id = "facebook/bart-large"

model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
    encoder_id, decoder_id, encoder_add_adapter=True
)

# set special tokens for generation
model.config.decoder_start_token_id = model.decoder.config.bos_token_id
model.config.pad_token_id = model.decoder.config.pad_token_id
model.config.eos_token_id = model.decoder.config.eos_token_id
```

The `encoder_add_adapter` argument introduces a small convolutional network between the encoder and decoder models. This adapter network helps interface the encoder and decoder by downsampling the encoder's hidden-states to match the timescale of the decoder better. In practice, including this adapter results in superior performance than the encoder and decoder models alone:

diagram

#### Preprocessing the data
Great! Now that we've defined our model we can start preparing our data for training. For this, we'll need to define two objects: a feature extractor and a tokenizer.

```python
from transformers import AutoFeatureExtractor, AutoTokenizer, Wav2Vec2Processor

# load feature extractor (for audio inputs) and tokenizer (for text outputs)
feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
tokenizer = AutoTokenizer.from_pretrained(decoder_id)
# combine under one class
processor = Wav2Vec2Processor(feature_extractor, tokenizer)
```

We're already pretty familiar with the tokenizer: it converts the text data into a set of input IDs that can be interpreted by the model. We‚Äôll use these to prepare our target labels. But the feature extractor is new! Simply put, the feature extractor prepares our input audio data.

A defining feature of Wav2Vec2 is that it accepts a float array corresponding to the raw waveform of the speech signal as an input. We mentioned that the audio data is represented as a 1-dimensional array, so it's already in the right format to be read by the model (a set of continuous inputs at discrete time steps).

So, what exactly does the feature extractor do?

Well, the audio data is in the right format, but we've imposed no restrictions on the values it can take. For our model to work optimally, we want to keep all the inputs within the same dynamic range. This is going to make sure we get a similar range of activations and gradients for our samples, helping with stability and convergence during training.

To do this, we _normalise_ our audio data by rescaling each sample to zero mean and unit variance, a process called _feature scaling_. It's exactly this feature normalisation that our feature extractor performs!

We can take a look at the feature extractor in operation by applying it to our first audio sample. First, let's compute the mean and standard deviation of our raw audio data:

```python
import numpy as np

sample = raw_datasets["train"][0]["audio"]

print(f"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}")
```

```
Mean: -2.17e-05, Variance: 0.00379
```
We can see that the mean is close to zero already, but the standard deviation is a long way off! This would cause our model problems, as the dynamic range of the audio data would be very small and difficult to separate. Let's apply the feature extractor and see what the outputs look like:

```python
inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])

print(f"inputs keys: {list(inputs.keys())}")

print(
    f"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}"
)
```

```
inputs keys: ['input_values', 'attention_mask']
Mean: -2.8e-09, Variance: 1.0
```
Alright! Our feature extractor returns a dictionary of two arrays: `input_values` and `attention_mask`. The `input_values` are the preprocessed audio inputs that we'd pass to the Wav2Vec2 model. The `attention_mask` would be used if we processed a _batch_ of audio inputs at once.

We can see that the mean value is now very much closer to zero, and the variance bang-on one! This is exactly the form we want our audio samples in prior to feeding them to the Wav2Vec2 encoder.

Note how we've passed the sampling rate of our audio data to our feature extractor. This is good practice, as the feature extractor performs a check under-the-hood to make sure the sampling rate of our audio data matches the sampling rate expected by the model.


Let's combine our feature extractor and tokenizer into one function to jointly preprocess our audio and text data:

```python
def prepare_dataset(batch):
    # process audio
    sample = batch["audio"]
    inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])
    # process audio length
    batch["input_values"] = inputs.input_values[0]
    batch["input_length"] = len(batch["input_values"])

    # process targets
    input_str = batch["text"].lower()
    # tokenize
    batch["labels"] = tokenizer(input_str).input_ids
    return batch


vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=next(iter(raw_datasets.values())).column_names,
    desc="preprocess dataset",
)
```

Note that it is important to align the decoder's vocabulary with the speech transcriptions of the dataset.

For example, Librispeech only has capitalised letters in the transcriptions, whereas BART was pretrained mostly on lower-cased text.

Filter the audio samples to a maximum length of 20s:

```python
MAX_INPUT_LENGTH_IN_SECONDS = 20

max_input_length = MAX_INPUT_LENGTH_IN_SECONDS * feature_extractor.sampling_rate


# filter data that is longer than max_input_length
def is_audio_in_length_range(length):
    return length < max_input_length


vectorized_datasets = vectorized_datasets.filter(
    is_audio_in_length_range,
    input_columns=["input_length"],
)
```

Define the data collator:
```python
from dataclasses import dataclass


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """
    Data collator that will dynamically pad the inputs received.
    Args:
        processor ([`Wav2Vec2Processor`])
            The processor used for processing the data.
        decoder_start_token_id (`int`)
            The begin-of-sentence of the decoder.
    """

    processor: Wav2Vec2Processor
    decoder_start_token_id: int

    def __call__(self, features):
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [
            {"input_values": feature["input_values"]} for feature in features
        ]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        # pad the audio inputs to max length
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # pad the token targets to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's appended later
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch


data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor, decoder_start_token_id=model.config.decoder_start_token_id
)
```

#### Metrics for speech recognition

WER

WAR

CER

#### Fine-tuning a speech recognition system with the Trainer API
Oh golly quite a lot to do...

