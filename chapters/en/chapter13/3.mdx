# Understanding the DeepSeek R1 Paper

This chapter is a crash course paper reading. We will walk through the paper in simple terms, and then we will break down the key concepts and takeaways.

DeepSeek R1 represents a significant advancement in language model training, particularly in developing reasoning capabilities through reinforcement learning. The paper introduces a new reinforcement learning algorithm called Generalized Policy Rectification Optimization (GRPO).

![DeepSeek R1 Overview](https://huggingface.co/nlp-course/images/resolve/main/grpo/4.png)

In the next chapter, we will build on this knowledge and implement GRPO in practice.

The initial goal of the paper was to explore whether pure reinforcement learning could develop reasoning capabilities without supervised fine-tuning. 

<Tip>
Up until that point, all the popular LLMs were trained using supervised fine-tuning, which we explored in [chapter 11](/chapters/en/chapter11/1).
</Tip>

The breakthrough was achieved when DeepSeek-R1-Zero was able to achieve remarkable results but had limitations. The final evolution was DeepSeek-R1, which emerged as a refined solution that addressed these limitations while maintaining strong reasoning capabilities.

| Component | Specification | Details |
|-----------|--------------|----------|
| Parameters | 70B | Active parameters per token |
| Architecture | DeepSeek-V3-Base | Modified with RFTrans |
| Training Data | 2T tokens | High-quality curated data |
| Infrastructure | 24,576 GPUs | Distributed training setup |
| Training Steps | 200B tokens | Full training process |

## The Breakthrough 'Aha' Moment

![The 'Aha Moment'](https://huggingface.co/nlp-course/images/resolve/main/grpo/9.png)

One of the most remarkable discoveries in R1-Zero's training was the emergence of a phenomenon known as the "Aha Moment." This phenomenon is similar to how humans experience sudden realizations while problem-solving. Here's how it works:

1. Initial Attempt: The model makes an initial attempt at solving a problem
2. Recognition: It recognizes potential errors or inconsistencies
3. Self-Correction: It adjusts its approach based on this recognition
4. Explanation: It can explain why the new approach is better

This breakthrough resonates with learners and feels like a "Eureka" moment. It demonstrates true learning rather than mere memorization. Let's take a moment to imagine what it feels like to have an "Aha" moment.

For example, imagine you're trying to solve a puzzle:
- First try: "This piece should go here based on the color"
- Recognition: "But wait, the shape doesn't quite fit"
- Correction: "Ah, it actually belongs over there"
- Explanation: "Because both the color and shape pattern match in this position"

This ability emerged naturally from RL training, without being explicitly programmed, demonstrating true learning rather than mere memorization of a process.

## The Training Process

Training R1 was a multi-phase process. Let's break down the phases and the key innovations in each phase.

The final process results in two models:
- DeepSeek-R1-Zero: A model trained purely using reinforcement learning.
- DeepSeek-R1: A model that builds on the foundation of DeepSeek-R1-Zero and adds supervised fine-tuning.

| Feature | DeepSeek-R1-Zero | DeepSeek-R1 |
|---------|------------------|--------------|
| Training Approach | Pure RL | Multi-phase (SFT + RL) |
| Fine-tuning | None | Supervised fine-tuning |
| Reasoning Capability | Emergent | Enhanced |
| AIME Performance | 71.0% | 79.8% |
| Key Characteristics | Strong reasoning but readability issues | Better language consistency and readability |

While DeepSeek-R1-Zero demonstrates the potential of pure reinforcement learning for developing reasoning capabilities, DeepSeek-R1 builds upon this foundation with a more balanced approach that prioritizes both performance and usability.

The training process involves four phases:

1. Cold Start Phase
2. Reasoning RL Phase
3. Rejection Sampling Phase
4. Diverse RL Phase

Let's break down each phase:

### Cold Start Phase (Quality Foundation)

![Cold Start Phase](https://huggingface.co/nlp-course/images/resolve/main/grpo/5.png)

This phase is designed to establish a strong foundation for the model's readability and response quality. It uses a small dataset of high-quality samples from R1-Zero to fine-tune the V3-Base model. Starting with the DeepSeek-V3-Base model, the team used thousands of validated, high-quality samples from R1-Zero for supervised fine-tuning. This innovative approach uses a small but high quality dataset to establish strong baseline readability and response quality.

### Reasoning RL Phase (Capability Building)

![Reasoning RL Phase](https://huggingface.co/nlp-course/images/resolve/main/grpo/6.png)

The Reasoning RL Phase focuses on developing core reasoning capabilities across domains including mathematics, coding, science, and logic. This phase employs rule-based reinforcement learning, with rewards directly tied to solution correctness. 

Crucially, all the tasks in this phase are 'verifiable' so we can check if the model's answer is correct or not. For example, in the case of mathematics, we can check if the model's answer is correct by using a mathematical solver.

What makes this phase particularly innovative is its direct optimization approach that eliminates the need for a separate reward model, streamlining the training process.

### Rejection Sampling Phase (Quality Control)

![Rejection Sampling Phase](https://huggingface.co/nlp-course/images/resolve/main/grpo/7.png)

During the Rejection Sampling Phase, the model generates samples which are then filtered through a quality control process. DeepSeek-V3 serves as the quality judge, evaluating outputs across a broad scope that extends beyond pure reasoning tasks. The filtered data is then used for supervised fine-tuning. This phase's innovation lies in its ability to combine multiple quality signals to ensure high-standard outputs.

### Diverse RL Phase (Broad Alignment)

![Diverse RL Phase](https://huggingface.co/nlp-course/images/resolve/main/grpo/8.png)

The final Diverse RL Phase tackles multiple task types using a sophisticated hybrid approach. For deterministic tasks, it employs rule-based rewards, while subjective tasks are evaluated through LLM feedback. This phase aims to achieve human preference alignment through its innovative hybrid reward approach, combining the precision of rule-based systems with the flexibility of language model evaluation.

## The Algorithm: Generalized Policy Rectification Optimization (GRPO)

Now that we have a good understanding of the training process, let's look at the algorithm that was used to train the model.

The authors describe GRPO as a breakthrough in model fine-tuning:

![GRPO Process](https://huggingface.co/nlp-course/images/resolve/main/grpo/10.png)

GRPO's novelty lies in its capacity to "directly optimize for preference rectification." This signifies a more direct and efficient route to aligning the model with desired outputs, contrasting with traditional Reinforcement Learning algorithms such as PPO. Let's break down how GRPO works through its three main components.

### Group Formation: Creating Multiple Solutions

The first step in GRPO is remarkably intuitive - it's similar to how a student might solve a difficult problem by trying multiple approaches. When given a prompt, the model doesn't just generate one response; instead, it creates multiple attempts at solving the same problem (usually 4, 8, or 16 different attempts).

Imagine you're teaching a model to solve math problems. For a question about counting chickens on a farm, the model might generate several different solutions:
- One solution might break down the problem step by step: first counting total chickens, then subtracting roosters, and finally accounting for non-laying hens
- Another might use a different but equally valid approach
- Some attempts might contain mistakes or less efficient solutions

All these attempts are kept together as a group, much like having multiple students' solutions to compare and learn from.

![Group Formation](https://huggingface.co/nlp-course/images/resolve/main/grpo/11.jpg)

### Preference Learning: Understanding What Makes a Good Solution

This is where GRPO really shines in its simplicity. Unlike traditional methods that need a separate neural network to predict how good a solution might be, GRPO evaluates each solution directly. It's like having a teacher grade multiple homework submissions at once.

The evaluation process looks at various aspects of each solution:
- Is the final answer correct?
- Did the solution follow proper formatting (like using the right XML tags)?
- Does the reasoning match the answer provided?

What makes this approach particularly clever is how it handles the scoring. Instead of just giving absolute scores, GRPO normalizes the rewards within each group. It uses a simple but effective formula:

```
Advantage = (reward - mean(group_rewards)) / std(group_rewards)
```

![Preference Learning](https://huggingface.co/nlp-course/images/resolve/main/grpo/12.jpg)

This normalization is like grading on a curve, but for AI. It helps the model understand which solutions within the group were better or worse compared to their peers, rather than just looking at absolute scores.

### Optimization: Learning from Experience

The final step is where GRPO teaches the model to improve based on what it learned from evaluating the group of solutions. This process is both powerful and stable, using two main principles:

1. It encourages the model to produce more solutions like the successful ones while moving away from less effective approaches
2. It includes a safety mechanism (called KL divergence penalty) that prevents the model from changing too drastically all at once

This approach proves more stable than traditional methods because:
- It looks at multiple solutions together rather than comparing just two at a time
- The group-based normalization helps prevent issues with reward scaling
- The KL penalty acts like a safety net, ensuring the model doesn't forget what it already knows while learning new things

<Tip>

GRPO's key innovations are:
- Learning directly from preference data, eliminating the need for a separate reward model.
- Group-based learning, which is more stable and efficient than traditional methods like pairwise comparisons.

</Tip>

This breakdown is complex, but the key takeaway is that GRPO is a more efficient and stable way to train a model to reason. 

## Results and Impact

Now that we've explored the algorithm, let's look at the results. DeepSeek R1 achieves state-of-the-art performance across multiple domains:

| Domain | Key Results |
|--------|-------------|
| Mathematics | • 79.8% on AIME 2024<br>• 97.3% on MATH-500 |
| Coding | • Codeforces Rating: 2029<br>• LiveCodeBench: 65.9% |
| General Knowledge | • MMLU: 90.8%<br>• GPQA Diamond: 71.5% |
| Language Tasks | • AlpacaEval 2.0: 87.6% win rate<br>• FRAMES: 82.5% |

The model's practical impact extends beyond benchmarks through its cost-effective API pricing ($0.14 per million input tokens) and successful model distillation across various sizes (1.5B to 70B parameters). Notably, even the 7B model achieves 55.5% on AIME 2024, while the 70B distilled version approaches o1-mini performance on MATH-500 (94.5%), demonstrating effective capability preservation at different scales.

## Conclusion

The DeepSeek R1 paper represents a significant milestone in language model development. The Generalized Policy Rectification Optimization (GRPO) algorithm has demonstrated that pure reinforcement learning can indeed develop strong reasoning capabilities, challenging previous assumptions about the necessity of supervised fine-tuning.

Perhaps most importantly, DeepSeek R1 has shown that it's possible to balance high performance with practical considerations like cost-effectiveness and accessibility. The successful distillation of the model's capabilities across different sizes, from 1.5B to 70B parameters, demonstrates a path forward for making advanced AI capabilities more widely available.

---

In the next section, we'll explore practical implementations of these concepts, focusing on how to leverage GRPO and RFTrans in your own language model development projects.

## Quiz

### 1. What is the main innovation of the DeepSeek R1 paper?

<Question
    choices={[
        {
            text: "The GRPO algorithm that enables direct preference optimization without a reward model",
            explain: "Correct! GRPO's ability to learn directly from preferences without needing a separate reward model is its key innovation.",
            correct: true
        },
        {
            text: "Using more GPUs for training than any previous model",
            explain: "While the model used significant computational resources, this wasn't the main innovation."
        },
        {
            text: "Creating a larger language model than existing ones",
            explain: "The model size (70B parameters) wasn't particularly innovative compared to other models."
        }
    ]}
/>

### 2. What are the four phases of the DeepSeek R1 training process?

<Question
    choices={[
        {
            text: "Cold Start, Reasoning RL, Rejection Sampling, and Diverse RL",
            explain: "Correct! These four phases form the complete training pipeline for DeepSeek R1.",
            correct: true
        },
        {
            text: "Pre-training, Fine-tuning, Testing, and Deployment",
            explain: "These are general ML pipeline steps, not the specific phases used in DeepSeek R1."
        },
        {
            text: "Data Collection, Model Training, Evaluation, and Optimization",
            explain: "While these are important steps, they don't match the specific four-phase process of DeepSeek R1."
        }
    ]}
/>

### 3. What is the 'Aha Moment' phenomenon in R1-Zero's training?

<Question
    choices={[
        {
            text: "A process where the model recognizes errors, self-corrects, and explains its corrections",
            explain: "Correct! The 'Aha Moment' demonstrates true learning through recognition, correction, and explanation.",
            correct: true
        },
        {
            text: "The point where the model reaches human-level performance",
            explain: "The 'Aha Moment' refers to the model's learning process, not its performance benchmarks."
        },
        {
            text: "When the model completes its training process",
            explain: "This describes the end of training, not the specific learning phenomenon of the 'Aha Moment'."
        }
    ]}
/>

### 4. How does GRPO's group formation work?

<Question
    choices={[
        {
            text: "It generates multiple solutions (4-16) for the same problem and evaluates them together",
            explain: "Correct! GRPO's group-based approach generates and evaluates multiple solutions simultaneously.",
            correct: true
        },
        {
            text: "It combines multiple models into one ensemble",
            explain: "GRPO uses a single model to generate multiple solutions, not an ensemble of models."
        },
        {
            text: "It splits the training data into different groups",
            explain: "This describes data partitioning, not GRPO's solution generation approach."
        }
    ]}
/>

### 5. What is the key difference between DeepSeek-R1-Zero and DeepSeek-R1?

<Question
    choices={[
        {
            text: "R1-Zero uses pure RL while R1 combines RL with supervised fine-tuning",
            explain: "Correct! This fundamental difference leads to R1's better language consistency and readability while maintaining strong reasoning.",
            correct: true
        },
        {
            text: "R1-Zero is smaller than R1",
            explain: "Both models have the same architecture and parameter count."
        },
        {
            text: "R1-Zero was trained on less data",
            explain: "The difference is in the training approach, not the amount of training data."
        }
    ]}
/>

