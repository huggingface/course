<FrameworkSwitchCourse {fw} />

# Clasificarea tokenilor[[token-classification]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb"},
]} />

{/if}

Prima aplica탵ie pe care o vom explora este clasificarea tokenilor. Aceast캒 sarcin캒 generic캒 cuprinde orice problem캒 care poate fi formulat캒 ca "atribuirea unui label fiec캒rui token dintr-o propozi탵ie", cum ar fi:

- **Named entity recognition(NER)**: G캒sirea entit캒탵ilor (cum ar fi persoane, loca탵ii sau organiza탵ii) 칥ntr-o propozi탵ie. Acest lucru poate fi formulat ca atribuirea unui label fiec캒rui token, av칙nd o clas캒 pentru fiecare entitate 탳i o clas캒 pentru "nicio entitate".
- **Part-of-speech tagging (POS)**: Marcheaz캒 fiecare cuv칙nt dintr-o propozi탵ie ca corespunz칙nd unei anumite p캒r탵i de vorbire (cum ar fi substantiv, verb, adjectiv etc.).
- **Chunking**: G캒sirea tokenilor care apar탵in aceleia탳i entit캒탵i. Aceast캒 sarcin캒 (care poate fi combinat캒 cu POS sau NER) poate fi formulat캒 ca atribuirea unui label (de obicei `B-`) tuturor tokenilor care se afl캒 la 칥nceputul unui chunk, a unui alt label (de obicei `I-`) la tokeni care se afl캒 칥n interiorul unui chunk 탳i a unui al treilea label (de obicei `O`) la tokeni care nu apar탵in niciunui chunk.

<Youtube id="wVHdVlPScxA"/>

Desigur, exist캒 multe alte tipuri de probleme de clasificare a tokenilor; acestea sunt doar c칙teva exemple reprezentative. 칉n aceast캒 sec탵iune, vom pune la punct un model (BERT) pe o sarcin캒 NER, care va fi apoi capabil s캒 calculeze predic탵ii precum aceasta:

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoding labels pentru r캒spunderea la 칥ntreb캒ri."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels pentru r캒spunderea la 칥ntreb캒ri."/>
</a>

Pute탵i g캒si modelul pe care 칥l vom antrena 탳i 칥nc캒rca 칥n Hub 탳i pute탵i verifica de dou캒 ori predic탵iile sale [aici](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).

## Preg캒tirea datelor[[preparing-the-data]]

칉n primul r칙nd, avem nevoie de un dataset adecvat pentru clasificarea simbolurilor. 칉n aceast캒 sec탵iune vom utiliza [datasetul CoNLL-2003] (https://huggingface.co/datasets/conll2003), care con탵ine 탳tiri de la Reuters.

> [!TIP]
> 游눠 At칙t timp c칙t datasetul vostru const캒 칥n texte 칥mp캒r탵ite 칥n cuvinte cu labelurile corespunz캒toare, ve탵i putea adapta procedurile de preprocesare a datelor descrise aici la propriul dataset. Consulta탵i [Capitolul 5](/course/chapter5) dac캒 ave탵i nevoie de o recapitulare a modului de 칥nc캒rcare a propriilor date personalizate 칥ntr-un `Dataset`.

### Datasetul CoNLL-2003 [[the-conll-2003-dataset]]

Pentru a 칥nc캒rca datasetul CoNLL-2003, folosim metoda `load_dataset()` din biblioteca 游뱅 Datasets:

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

Acest lucru va desc캒rca 탳i va stoca 칥n cache datasetul, a탳a cum am v캒zut 칥n [Capitolul 3](/course/chapter3) pentru setul de date GLUE MRPC. Inspectarea acestui obiect ne arat캒 coloanele prezente 탳i 칥mp캒r탵irea 칥ntre seturile de antrenare, validare 탳i testare:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

칉n special, putem vedea c캒 datasetul con탵ine labeluri pentru cele trei sarcini pe care le-am men탵ionat mai devreme: NER, POS 탳i Chunking. O mare diferen탵캒 fa탵캒 de alte datseturi este c캒 textele de intrare nu sunt prezentate ca propozi탵ii sau documente, ci ca liste de cuvinte (ultima coloan캒 se nume탳te `tokens`, dar con탵ine cuvinte 칥n sensul c캒 acestea sunt inputuri pre-tokenizate care mai trebuie s캒 treac캒 prin tokenizer pentru tokenizarea subcuvintelor).

S캒 arunc캒m o privire la primul element al setului de formare:

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

Deoarece dorim s캒 efectu캒m named entity recognition, ne vom uita la etichetele NER:

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

Acestea sunt labelurile ca numere 칥ntregi preg캒tite pentru antrenare, dar nu sunt neap캒rat utile atunci c칙nd dorim s캒 inspect캒m datele. La fel ca 칥n cazul clasific캒rii textului, putem accesa coresponden탵a dintre aceste numere 칥ntregi 탳i numele labelurilor consult칙nd atributul `features` al datasetului nostru:

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

Deci, aceast캒 coloan캒 con탵ine elemente care sunt secven탵e a `ClassLabel`. Tipul elementelor din secven탵캒 se afl캒 칥n atributul `feature` al acestei `ner_feature`, iar noi putem accesa lista de nume consult칙nd atributul `names` al acelei `feature`:

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

Am v캒zut deja aceste labeluri atunci c칙nd am cercetat pipelineul `token-classification` 칥n [Capitolul 6](/course/chapter6/3), dar pentru o re칥mprosp캒tare rapid캒:

- `O` 칥nseamn캒 c캒 cuv칙ntul nu corespunde niciunei entit캒탵i.
- `B-PER`/`I-PER` 칥nseamn캒 c캒 cuv칙ntul corespunde 칥nceputului/este 칥n interiorul unei entit캒탵i de tip *persoan캒*.
- `B-ORG`/`I-ORG` 칥nseamn캒 c캒 cuv칙ntul corespunde 칥nceputului/este 칥n interiorul unei entit캒탵i de tip *organiza탵ionale*.
- `B-LOC`/`I-LOC` 칥nseamn캒 c캒 cuv칙ntul corespunde 칥nceputului/este 칥n interiorul unei entit캒탵i de tip  *loca탵ie*.
- `B-MISC`/`I-MISC` 칥nseamn캒 c캒 cuv칙ntul corespunde 칥nceputului/este 칥n interiorul unei entit캒탵i de tip *miscellaneous.

Acum, decodificarea labelurilor pe care le-am v캒zut mai devreme ne d캒 urm캒torul rezultat:

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

탲i pentru un exemplu de amestecare a labelurilor `B-` 탳i `I-`, iat캒 ce ne ofer캒 acela탳i cod pentru elementul din setul de antrenare de la indexul 4:

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

Dup캒 cum putem vedea, entit캒탵ilor care cuprind dou캒 cuvinte, precum "Uniunea European캒" 탳i "Werner Zwingmann", li se atribuie un label `B-` pentru primul cuv칙nt 탳i un label `I-` pentru al doilea.

> [!TIP]
> 九勇 **E r칙ndul t캒u!** Afi탳a탵i acelea탳i dou캒 propozi탵ii cu labelurile POS sau chunking.

### Procesarea datelor[[processing-the-data]]

<Youtube id="iY2AZYdZAr0"/>

Ca de obicei, textele noastre trebuie s캒 fie convertite 칥n ID-uri token 칥nainte ca modelul s캒 le poat캒 칥n탵elege. Dup캒 cum am v캒zut 칥n [Capitolul 6](/course/chapter6/), o mare diferen탵캒 칥n cazul sarcinilor de clasificare a tokenilor este c캒 avem inputuri pre-tokenizate. Din fericire, API-ul tokenizerului poate face fa탵캒 acestei situa탵ii destul de u탳or; trebuie doar s캒 avertiz캒m `tokenizerul` cu un indicator special.

Pentru 칥nceput, hai s캒 cre캒m obiectul `tokenizer`. Dup캒 cum am mai spus, vom utiliza un model BERT preantrenat, deci vom 칥ncepe prin a desc캒rca 탳i a stoca 칥n cache tokenizerul asociat:

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Pute탵i 칥nlocui `model_checkpoint` cu orice alt model preferat din [Hub](https://huggingface.co/models) sau cu un folder local 칥n care a탵i salvat un model preantreant 탳i un tokenizer. Singura constr칙ngere este c캒 tokenizerul trebuie s캒 fie sus탵inut de biblioteca 游뱅 Tokenizers, pentru ca s캒 existe o versiune "rapid캒" disponibil캒. Pute탵i vedea toate arhitecturile care vin cu o versiune rapid캒 칥n [acest tabel mare] (https://huggingface.co/transformers/#supported-frameworks), iar pentru a verifica dac캒 obiectul `tokenizer` pe care 칥l utiliza탵i este 칥ntr-adev캒r sus탵inut de 游뱅 Tokenizers, v캒 pute탵i uita la atributul s캒u `is_fast`:

```py
tokenizer.is_fast
```

```python out
True
```

Pentru a tokeniza un input pre-tokenizat, putem utiliza `tokenizer` ca de obicei 탳i s캒 ad캒ug캒m `is_split_into_words=True`:

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

Dup캒 cum putem vedea, tokenizerul a ad캒ugat tokenii speciali utiliza탵i de model (`[CLS]` la 칥nceput 탳i `[SEP]` la sf칙r탳it) 탳i a l캒sat majoritatea cuvintelor neatinse. Cu toate acestea, cuv칙ntul `lamb` a fost tokenizat 칥n dou캒 subcuvinte, `la` 탳i `##mb`. Acest lucru introduce o neconcordan탵캒 칥ntre inputurile noastre 탳i labeluri: lista de labeluri are doar 9 elemente, 칥n timp ce inputurile noastre au acum 12 tokeni. S캒 lu캒m 칥n considerare tokenii speciali este u탳or (탳tim c캒 sunt la 칥nceput 탳i la sf칙r탳it), dar trebuie s캒 ne asigur캒m c캒 aliniem toate labelurile cu cuvintele corespunz캒toare.

Din fericire, pentru c캒 folosim un tokenizer rapid, avem acces la superputerile 游뱅 Tokenizers, ceea ce 칥nseamn캒 c캒 putem mapa cu u탳urin탵캒 fiecare token la cuv칙ntul corespunz캒tor (dup캒 cum se vede 칥n [Capitolul 6](/course/chapter6/3)):

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

Cu pu탵in캒 munc캒, putem apoi extinde lista de labeluri pentru a se potrivi cu tokenii. Prima regul캒 pe care o vom aplica este c캒 tokenii special primesc un label `-100`. Acest lucru se datoreaz캒 faptului c캒 칥n mod implicit `-100` este un indice care este ignorat 칥n func탵ia de pierdere pe care o vom utiliza (cross-entropy). Apoi, fiecare token prime탳te acela탳i label ca 탳i tokenul care a ini탵iat cuv칙ntul 칥n care se afl캒, deoarece fac parte din aceea탳i entitate. Pentru tokenii din interiorul unui cuv칙nt, dar care nu se afl캒 la 칥nceput, 칥nlocuim `B-` cu `I-` (deoarece tokenul nu 칥ncepe entitatea):


```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

Hai s캒 칥l 칥ncerc캒m cu prima noastr캒 propozi탵ie:

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

Dup캒 cum putem vedea, func탵ia noastr캒 a ad캒ugat `-100` pentru cei doi tokeni speciali de la 칥nceput 탳i de la sf칙r탳it, 탳i un nou `0` pentru cuv칙ntul nostru care a fost 칥mp캒r탵it 칥n doi tokeni.

> [!TIP]
> 九勇 ** R칙ndul t캒u!** Unii cercet캒tori prefer캒 s캒 atribuie un singur label pe cuv칙nt 탳i s캒 atribuie `-100` celorlal탵i subtokeni dintr-un cuv칙nt dat. Aceasta are loc pentru a evita ca cuvintele lungi care se 칥mpart 칥n mai mul탵i subtokeni s캒 contribuie puternic la pierdere. Modifica탵i func탵ia anterioar캒 pentru a alinia labelurile cu ID-urile de input urm칙nd aceast캒 regul캒.

Pentru a preprocesa 칥ntregul nostru dataset, trebuie s캒 tokeniz캒m toate inputurile 탳i s캒 aplic캒m `align_labels_with_tokens()` pe toate labelurile. Pentru a profita de viteza tokenizerului nostru rapid, este mai bine s캒 tokeniz캒m multe texte 칥n acela탳i timp, a탳a c캒 vom scrie o func탵ie care proceseaz캒 o list캒 de exemple 탳i vom folosi metoda `Dataset.map()` cu op탵iunea `batched=True`. Singurul lucru diferit fa탵캒 de exemplul nostru anterior este c캒 func탵ia `word_ids()` trebuie s캒 ob탵in캒 indexul exemplului din care dorim ID-urile cuvintelor atunci c칙nd inputurile c캒tre tokenizer sunt liste de texte (sau, 칥n cazul nostru, liste de liste de cuvinte), a탳a c캒 ad캒ug캒m 탳i acest lucru:

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

Re탵ine탵i c캒 nu am completat 칥nc캒 inputurile; vom face acest lucru mai t칙rziu, atunci c칙nd vom crea batchurile cu un data collator.

Acum putem aplica toate aceste preproces캒ri dintr-o dat캒 celorlalte frac탵iuni ale datasetului nostru:

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

Am f캒cut partea cea mai grea! Acum, c캒 datele au fost preprocesate, antrenarea efectiv캒 va sem캒na mult cu ceea ce am f캒cut 칥n [Capitolul 3](/course/chapter3).

{#if fw === 'pt'}

## Fine-tuningul modelului cu `Trainer` API[[fine-tuning-the-model-with-the-trainer-api]]

Codul real care utilizeaz캒 `Trainer` va fi acela탳i ca 칥nainte; singurele modific캒ri sunt modul 칥n care datele sunt collated 칥ntr-un batch 탳i metric computation function.

{:else}

## Fine-tuningul modelului cu Keras[[fine-tuning-the-model-with-keras]]

Codul real care utilizeaz캒 `Keras` va fi acela탳i ca 칥nainte; singurele modific캒ri sunt modul 칥n care datele sunt collated 칥ntr-un batch 탳i metric computation function.

{/if}


### Data collation[[data-collation]]

Nu putem folosi doar un `DataCollatorWithPadding` ca 칥n [Capitolul 3](/course/chapter3) deoarece acesta doar adaug캒 padding inputurilor(input IDs, attention mask, and token type IDs). Aici labelurile noastre ar trebui s캒 fie padded exact 칥n acela탳i mod ca 탳i inputurile, astfel 칥nc칙t s캒 r캒m칙n캒 de aceea탳i dimensiune, folosind `-100` ca valoare, astfel 칥nc칙t predic탵iile corespunz캒toare s캒 fie ignorate 칥n calculul pierderilor.

Toate acestea sunt realizate de un [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification). La fel ca `DataCollatorWithPadding`, acesta ia `tokenizer`-ul folosit pentru preprocesarea inputurilor:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

Pentru a testa acest lucru pe c칙teva sampleuri, 칥l putem apela doar pe o list캒 de exemple din setul nostru de antrenat tokenizat:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

Hai s캒 compar캒m acest lucru cu labelurile pentru primul 탳i al doilea element din datasetul nostru:

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

Dup캒 cum se poate observa, al doilea set de labeluri a fost padded la lungimea primului folosind `-100`.

{:else}

Data collatorul nostru de date este gata de utilizare! Acum s캒 칥l folosim pentru a crea un `tf.data.Dataset` cu metoda `to_tf_dataset()`. De asemenea, pute탵i utiliza `model.prepare_tf_dataset()` pentru a face acest lucru cu un pic mai pu탵in cod repetitiv - ve탵i vedea acest lucru 칥n unele dintre celelalte sec탵iuni ale acestui capitol.

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```


Urm캒toarea oprire: modelul 칥n sine.

{/if}

{#if fw === 'tf'}

### Definirea modelului[[defining-the-model]]

Deoarece lucr캒m la o problem캒 de clasificare a tokenilor, vom utiliza clasa `TFAutoModelForTokenClassification`. Principalul lucru de care trebuie s캒 ne amintim atunci c칙nd definim acest model este s캒 transmitem informa탵ii privind num캒rul de labeluri pe care le avem. Cel mai simplu mod de a face acest lucru este s캒 transmite탵i acest num캒r cu argumentul `num_labels`, dar dac캒 dorim un inference widget frumos care s캒 func탵ioneze ca cel pe care l-am v캒zut la 칥nceputul acestei sec탵iuni, este mai bine s캒 seta탵i 칥n schimb coresponden탵ele corecte ale labelurilor.

Acestea ar trebui s캒 fie stabilite de dou캒 dic탵ionare, `id2label` 탳i `label2id`, care con탵in coresponden탵a de la ID la label 탳i viceversa:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
``` 

Acum putem s캒 le transmitem metodei `TFAutoModelForTokenClassification.from_pretrained()`, iar acestea vor fi setate 칥n configura탵ia modelului, apoi salvate corespunz캒tor 탳i 칥nc캒rcate 칥n Hub:

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

La fel ca atunci c칙nd am definit `TFAutoModelForSequenceClassification` 칥n [Capitolul 3](/course/chapter3), crearea modelului emite un avertisment c캒 unele weights nu au fost utilizate (cele din headul de preantrenare) 탳i c캒 alte weights sunt ini탵ializate aleatoriu (cele din headul de clasificare a tokenilor noi) 탳i c캒 acest model ar trebui s캒 fie antrenat. Vom face acest lucru 칥ntr-un minut, dar mai 칥nt칙i s캒 verific캒m de dou캒 ori c캒 modelul nostru are num캒rul corect de labeluri:

```python
model.config.num_labels
```

```python out
9
```

> [!WARNING]
> 丘멆잺 Dac캒 ave탵i un model cu un num캒r gre탳it de labeluri, ve탵i primi o eroare obscur캒 atunci c칙nd apela탵i `model.fit()` mai t칙rziu. Acest lucru poate fi enervant pentru debbuging, a탳a c캒 asigura탵i-v캒 c캒 face탵i aceast캒 verificare pentru a confirma c캒 ave탵i num캒rul a탳teptat de labeluri.

### Fine-tuningul modelului[[fine-tuning-the-model]]

Acum suntem gata s캒 ne antren캒m modelul! Totu탳i, mai avem doar c칙teva lucruri de f캒cut mai 칥nt칙i: ar trebui s캒 ne conect캒m la Hugging Face 탳i s캒 definim hiperparametrii no탳tri de antrenare. Dac캒 lucra탵i 칥ntr-un notebook, exist캒 o func탵ie convenabil캒 pentru a v캒 ajuta cu acest lucru:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Aceasta va afi탳a un widget 칥n care pute탵i introduce datele tale de autentificare Hugging Face.

Dac캒 nu lucra탵i 칥ntr-un notebook, tasta탵i urm캒toarea linie 칥n terminal:

```bash
huggingface-cli login
```

Dup캒 autentificare, putem preg캒ti tot ce avem nevoie pentru a compila modelul nostru. 游뱅 Transformers ofer캒 o func탵ie convenabil캒 `create_optimizer()` care v캒 va oferi un optimizator `AdamW` cu set캒ri adecvate pentru weight decay 탳i learning rate decay, ambele 칥mbun캒t캒탵ind performan탵a modelului vsotru 칥n compara탵ie cu optimizatorul `Adam` 칥ncorporat:

```python
from transformers import create_optimizer
import tensorflow as tf

# Antrenarea 칥n mixed-precision float16
# Comenta탵i aceast캒 linie dac캒 utiliza탵i un GPU care nu va beneficia de acest lucru
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# Num캒rul de etape de antrenare este num캒rul de sampleuri din dataset, 칥mp캒r탵it la dimensiunea batch-ului, apoi multiplicat
# cu num캒rul total de epoci. Re탵ine탵i c캒 tf_train_dataset de aici este un batched tf.data.Dataset,
# nu este originalul Hugging Face Dataset, deci len() este deja num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

Re탵ine탵i, de asemenea, c캒 nu furniz캒m un argument `loss` la `compile()`. Acest lucru se datoreaz캒 faptului c캒 modelele pot calcula de fapt pierderea intern - dac캒 compila탵i f캒r캒 o pierdere 탳i furniza탵i labelurile 칥n dic탵ionarul de intrare (a탳a cum facem 칥n dataseturile noastre), atunci modelul se va antrena folosind acea pierdere intern캒, care va fi adecvat캒 pentru sarcina 탳i tipul de model pe care le-a탵i ales.

칉n continuare, definim un `PushToHubCallback` pentru a 칥nc캒rca modelul nostru 칥n Hub 칥n timpul antren캒rii 탳i pentru a potrivi modelul cu acel callback:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

Pute탵i specifica numele complet al repositoriului c캒tre care dori탵i s캒 efectua탵i push cu argumentul `hub_model_id` (칥n special, va trebui s캒 utiliza탵i acest argument pentru a efectua push c캒tre o organiza탵ie). De exemplu, atunci c칙nd am trimis modelul c캒tre organiza탵ia [`huggingface-course`](https://huggingface.co/huggingface-course), am ad캒ugat `hub_model_id="huggingface-course/bert-finetuned-ner"`. 칉n mod implicit, repositoriul utilizat va fi 칥n namespace-ul denumit dup캒 directory output pe care l-a탵i stabilit, de exemplu `"cool_huggingface_user/bert-finetuned-ner"`.

> [!TIP]
> 游눠 Dac캒 directory output pe care 칥l utiliza탵i exist캒 deja, acesta trebuie s캒 fie o clon캒 local캒 a repositoriului c캒tre care dori탵i s캒 face탵i push. Dac캒 nu este, ve탵i primi o eroare atunci c칙nd apela탵i `model.fit()` 탳i va trebui s캒 seta탵i un nume nou.

Re탵ine탵i c캒, 칥n timpul antrenamentului, de fiecare dat캒 c칙nd modelul este salvat (aici, la fiecare epoc캒), acesta este 칥nc캒rcat pe Hub 칥n fundal. 칉n acest fel, ve탵i putea s캒 relua탵i formarea pe o alt캒 ma탳in캒, dac캒 este necesar.

칉n acest stadiu, pute탵i utiliza inference widget de pe Model Hub pentru a testa modelul vostru 탳i pentru a-l partaja cu prietenii. A탵i f캒cut fine-tune cu succes unui model pentru o sarcin캒 de clasificare a tokenilor - felicit캒ri! Dar c칙t de bun este modelul nostru, de fapt? Ar trebui s캒 evalu캒m anumi탵i parametri pentru a afla.

{/if}


### Metrici[[metrics]]

{#if fw === 'pt'}

Pentru ca `Trainer` s캒 calculeze o metric캒 칥n fiecare epoc캒, va trebui s캒 definim o func탵ie `compute_metrics()` care prime탳te matricele de predic탵ii 탳i labelurile 탳i returneaz캒 un dic탵ionar cu numele 탳i valorile metricilor.

Frameworkul tradi탵ional utilizat pentru a evalua predic탵ia clasific캒rii a tokenilor este [*seqeval*](https://github.com/chakki-works/seqeval). Pentru a utiliza aceast캒 metric캒, trebuie mai 칥nt칙i s캒 instal캒m biblioteca *seqeval*:

```py
!pip install seqeval
```

Apoi 칥l putem 칥nc캒rca prin intermediul func탵iei `evaluate.load()` a탳a cum am f캒cut 칥n [Capitolul 3](/course/chapter3):

{:else}

Frameworkul tradi탵ional utilizat pentru a evalua predic탵ia clasific캒rii tokenilor este [*seqeval*](https://github.com/chakki-works/seqeval). Pentru a utiliza aceast캒 metric캒, trebuie mai 칥nt칙i s캒 instal캒m biblioteca *seqeval*:

```py
!pip install seqeval
```

Apoi 칥l putem 칥nc캒rca prin intermediul func탵iei `evaluate.load()` a탳a cum am f캒cut 칥n [Capitolul 3](/course/chapter3):

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

Aceast캒 metric캒 nu se comport캒 ca precizia standard: de fapt, va lua listele de labeluri ca 탳iruri de caractere, nu ca numere 칥ntregi, deci va trebui s캒 decodific캒m complet predic탵iile 탳i labelurile 칥nainte de a le trece 칥n metrice. S캒 vedem cum func탵ioneaz캒. 칉n primul r칙nd, vom ob탵ine labelurile pentru primul nostru exemplu de antrenare:

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

Putem apoi crea predic탵ii false pentru acestea prin simpla schimbare a valorii la indexul 2:

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

Re탵ine탵i c캒 metrica ia o list캒 de predic탵ii (nu doar una) 탳i o list캒 de labels. Iat캒 rezultatul:

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

Acesta trimite 칥napoi o mul탵ime de informa탵ii! Noi ob탵inem precizia, recallul 탳i scorul F1 pentru fiecare entitate 칥n parte, precum 탳i scorul general. Pentru calculul nostru metric, vom p캒stra doar scorul global, dar nu ezita탵i s캒 modifica탵i func탵ia `compute_metrics()` pentru a returna toate metricile pe care dori탵i s캒 le raporta탵i.

Aceast캒 func탵ie `compute_metrics()` ia mai 칥nt칙i argmaxul logiturilor pentru a le converti 칥n predic탵ii (ca de obicei, logiturile 탳i probabilit캒탵ile sunt 칥n aceea탳i ordine, deci nu trebuie s캒 aplic캒m softmaxul). Apoi trebuie s캒 convertim at칙t labelurile, c칙t 탳i predic탵iile din numere 칥ntregi 칥n 탳iruri de caractere. Elimin캒m toate valorile 칥n care labelul este `-100`, apoi transmitem rezultatele metodei `metric.compute()`:

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

Acum c캒 acest lucru este f캒cut, suntem aproape gata s캒 definim `Trainer`-ul nostru. Avem nevoie doar de un `model` pentru a face fine-tune!

{:else}

Aceasta trimite 칥napoi o mul탵ime de informa탵ii! Ob탵inem precizia, recallul 탳i scorul F1 pentru fiecare entitate 칥n parte, precum 탳i 칥n ansamblu. Acum s캒 vedem ce se 칥nt칙mpl캒 dac캒 칥ncerc캒m s캒 folosim predic탵iile modelului nostru real pentru a calcula ni탳te scoruri reale.

TensorFlow nu apreciaz캒 concatenarea predic탵iilor noastre, deoarece acestea au lungimi de secven탵e variabile. Aceasta 칥nseamn캒 c캒 nu putem folosi pur 탳i simplu `model.predict()` - dar asta nu ne va opri. Vom ob탵ine unele predic탵ii pe r칙nd 탳i le vom concatena 칥ntr-o singur캒 list캒 mare 탳i lung캒, elimin칙nd simbolurile `-100` care indic캒 mascarea/paddingul, apoi vom calcula metrici pe lista de la sf칙r탳it:

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict_on_batch(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

Cum s-a descurcat modelul t캒u, comparativ cu al nostru? Dac캒 a탵i ob탵inut cifre similare, antrenarea a fost un succes!

{/if}

{#if fw === 'pt'}

### Definirea modelului[[defining-the-model]]

Deoarece lucr캒m la o problem캒 de clasificare a tokenilor, vom utiliza clasa `AutoModelForTokenClassification`. Principalul lucru de care trebuie s캒 ne amintim atunci c칙nd definim acest model este s캒 transmitem informa탵ii privind num캒rul de labeluri pe care le avem. Cel mai simplu mod de a face acest lucru este s캒 transmite탵i acest num캒r cu argumentul `num_labels`, dar dac캒 dorim un inferencea widget frumos care s캒 func탵ioneze ca cel pe care l-am v캒zut la 칥nceputul acestei sec탵iuni, este mai bine s캒 seta탵i 칥n schimb coresponden탵ele corecte ale labelurilor.

Acestea ar trebui s캒 fie stabilite de dou캒 dic탵ionare, `id2label` 탳i `label2id`, care con탵in coresponden탵ele de la ID la labeluri 탳i viceversa:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

Acum putem doar s캒 le transmitem metodei `AutoModelForTokenClassification.from_pretrained()`, iar acestea vor fi setate 칥n configura탵ia modelului 탳i apoi salvate 탳i 칥nc캒rcate corespunz캒tor 칥n Hub:

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

La fel ca atunci c칙nd am definit `AutoModelForSequenceClassification` 칥n [Capitolul 3](/course/chapter3), crearea modelului emite un avertisment c캒 unele weights nu au fost utilizate (cele din headul de antrenare) 탳i alte weights sunt ini탵ializate aleatoriu (cele din headul de clasificare a tokenilor noi) 탳i c캒 acest model ar trebui s캒 fie format. Vom face acest lucru 칥ntr-un minut, dar mai 칥nt칙i s캒 verific캒m de dou캒 ori c캒 modelul nostru are num캒rul corect de labeluri:

```python
model.config.num_labels
```

```python out
9
```

> [!WARNING]
> 丘멆잺 Dac캒 ave탵i un model cu un num캒r gre탳it de labeluri, ve탵i primi o eroare obscur캒 atunci c칙nd apela탵i metoda `Trainer.train()` mai t칙rziu (ceva de genul "CUDA error: device-side assert triggered"). Aceasta este cauza num캒rul unu a erorilor raportate de utilizatori pentru astfel de erori, a탳a c캒 asigura탵i-v캒 c캒 face탵i aceast캒 verificare pentru a confirma c캒 ave탵i num캒rul de labeluri a탳teptat.

### Fine-tuningul modelului[[fine-tuning-the-model]]

Acum suntem gata s캒 ne antren캒m modelul! Trebuie doar s캒 facem ultimele dou캒 lucruri 칥nainte de a defini modelul nostru `Trainer`: s캒 ne conect캒m la Hugging Face 탳i s캒 definim argumentele de antrenare. Dac캒 lucra탵i 칥ntr-un notebook, exist캒 o func탵ie convenabil캒 pentru a v캒 ajuta cu acest lucru:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Aceasta va afi탳a un widget 칥n care pute탵i introduce datele tale de autentificare Hugging Face.

Dac캒 nu lucra탵i 칥ntr-un notebook, tasta탵i urm캒toarea linie 칥n terminal:

```bash
huggingface-cli login
```

Odat캒 f캒cut acest lucru, putem defini`TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

A탵i mai v캒zut cele mai multe dintre acestea: stabilim ni탳te hiperparametri (cum ar fi learning rate, num캒rul de epoci pentru care s캒 ne antren캒m 탳i weights decay) 탳i specific캒m `push_to_hub=True` pentru a indica faptul c캒 dorim s캒 salv캒m modelul 탳i s캒 칥l evalu캒m la sf칙r탳itul fiec캒rei epoci 탳i c캒 dorim s캒 칥nc캒rc캒m rezultatele noastre 칥n Model Hub. Re탵ine탵i c캒 pute탵i specifica numele repositoriului c캒tre care dori탵i s캒 face탵i push cu argumentul `hub_model_id` (칥n special, va trebui s캒 utiliza탵i acest argument pentru a face push c캒tre o organiza탵ie). De exemplu, atunci c칙nd am trimis modelul c캒tre organiza탵ia [`huggingface-course`](https://huggingface.co/huggingface-course), am ad캒ugat `hub_model_id="huggingface-course/bert-finetuned-ner"` la `TrainingArguments`. 칉n mod implicit, repositoriul utilizat va fi 칥n namespaceul t캒u 탳i denumit dup캒 output directory-ul pe care l-a탵i setat, deci 칥n cazul nostru va fi `"sgugger/bert-finetuned-ner"`.

> [!TIP]
> 游눠 Dac캒 output directory-ul pe care 칥l utiliza탵i exist캒 deja, acesta trebuie s캒 fie o clon캒 local캒 a repositoriul c캒tre care dori탵i s캒 face탵i push. Dac캒 nu este a탳a, ve탵i primi o eroare la definirea `Trainer` 탳i va trebui s캒 seta탵i un nume nou.

칉n final, transmitem totul c캒tre `Trainer` 탳i lans캒m antrenarea:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

Re탵ine탵i c캒, 칥n timpul antren캒rii, de fiecare dat캒 c칙nd modelul este salvat (aici, la fiecare epoc캒), acesta este 칥nc캒rcat pe Hub 칥n fundal. 칉n acest fel, ve탵i putea s캒 relua탵i antrenareape o alt캒 ma탳in캒, dac캒 este necesar.

Odat캒 ce antrenamentul este complet, folosim metoda `push_to_hub()` pentru a ne asigura c캒 칥nc캒rc캒m cea mai recent캒 versiune a modelului:

```py
trainer.push_to_hub(commit_message="Training complete")
```

Aceast캒 comand캒 returneaz캒 URL-ul comitului pe care tocmai l-ai f캒cut, dac캒 dori탵i s캒 o inspecta탵i:

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

De asemenea, `Trainer` redacteaz캒 un model card cu toate rezultatele evalu캒rii 탳i o 칥ncarc캒. 칉n acest stadiu, pute탵i utiliza inference widgetul de pe Model Hub pentru a testa modelul 탳i a-l partaja cu prietenii. A탵i f캒cut fine-tune cu succes un model pentru o sarcin캒 de clasificare a tokenilor - felicit캒ri!

Dac캒 dori탵i s캒 v캒 scufunda탵i pu탵in mai profund 칥n bucla de antrenare, v캒 vom ar캒ta acum cum s캒 face탵i acela탳i lucru utiliz칙nd 游뱅 Accelerate.

## Un training loop personalizat[[a-custom-training-loop]]

S캒 arunc캒m acum o privire la training loopul complet, astfel 칥nc칙t s캒 pute탵i personaliza cu u탳urin탵캒 p캒r탵ile de care ave탵i nevoie. Va sem캒na foarte mult cu ceea ce am f캒cut 칥n [Capitolul 3](/course/chapter3/4), cu c칙teva modific캒ri pentru evaluare.

### Preg캒ti탵i totul pentru antrenare[[preparing-everything-for-training]]

Mai 칥nt칙i trebuie s캒 construim `DataLoader`s din dataseturile noastre. Vom reutiliza `data_collator` ca un `collate_fn` 탳i vom amesteca setul de antrenare, dar nu 탳i setul de validare:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

칉n continuare, reinstan탵iem modelul, pentru a ne asigura c캒 nu continu캒m fine-tuningul de dinainte, ci pornim din nou de la modelul preantrenat BERT:

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Atunci vom avea nevoie de un optimizator. Vom folosi clasicul `AdamW`, care este ca `Adam`, dar cu o corec탵ie 칥n modul 칥n care se aplic캒 weight decay-ul:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Odat캒 ce avem toate aceste obiecte, le putem trimite la metoda `accelerator.prepare()`:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

> [!TIP]
> 游뚿 Dac캒 v캒 antrena탵i pe un TPU, va trebui s캒 muta탵i tot codul 칥ncep칙nd de la celula de mai sus 칥ntr-o func탵ie de antrenament dedicat캒. Consulta탵i [Capitolul 3](/course/chapter3) pentru mai multe detalii.

Acum c캒 am trimis `train_dataloader` la `accelerator.prepare()`, putem utiliza lungimea acestuia pentru a calcula num캒rul de pa탳i de antrenare. Re탵ine탵i c캒 ar trebui s캒 facem 칥ntotdeauna acest lucru dup캒 ce preg캒tim dataloaderul, deoarece aceast캒 metod캒 칥i va modifica lungimea. Utiliz캒m un classic liner schedule de la rata de 칥nv캒탵are la 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

칉n cele din urm캒, pentru a trimite modelul nostru c캒tre Hub, va trebui s캒 cre캒m un obiect `Repository` 칥ntr-un folder de lucru. 칉n primul r칙nd, conecta탵i-v캒 la Hugging Face, dac캒 nu sunte탵i deja conectat. Vom determina numele repositoriului pornind de la ID-ul modelului pe care dorim s캒 칥l d캒m modelului nostru (nu ezita탵i s캒 칥nlocui탵i `repo_name` cu propriul nume; trebuie doar s캒 con탵in캒 numele vostru de utilizator, ceea ce face func탵ia `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

Apoi putem clona acel repositoriu 칥ntr-un folder local. Dac캒 exist캒 deja, acest folder local ar trebui s캒 fie o clon캒 existent캒 a repositoriului cu care lucr캒m:

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Acum putem 칥nc캒rca orice salv캒m 칥n `output_dir` prin apelarea metodei `repo.push_to_hub()`. Acest lucru ne va ajuta s캒 칥nc캒rc캒m modelele intermediare la sf칙r탳itul fiec캒rei epoci.

### Loopul de antrenare[[training-loop]]

Acum suntem preg캒ti탵i s캒 scriem bucla de antrenare complet캒. Pentru a simplifica partea sa de evaluare, definim func탵ia `postprocess()` care preia predic탵iile 탳i labelurile 탳i le converte탳te 칥n liste de 탳iruri de caractere, a탳a cum se a탳teapt캒 obiectul nostru `metric`:

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

Apoi putem scrie bucla de antrenare. Dup캒 definirea unei bare de progres pentru a urm캒ri modul 칥n care decurge antrenarea, bucla are trei p캒r탵i:

- Antrenarea 칥n sine, care este itera탵ia clasic캒 peste `train_dataloader`, trecerea 칥nainte prin model, apoi trecerea 칥napoi 탳i pasul optimizatorului.
- Evaluarea, 칥n care exist캒 o noutate dup캒 ob탵inerea outputurilor modelului nostru pe un batch: din moment ce dou캒 procese pot ar fi putut face padding inputurilor 탳i labelurile la forme diferite, trebuie s캒 folosim `accelerator.pad_across_processes()` pentru a face predic탵iile 탳i labelurile s캒 aib캒 aceea탳i form캒 칥nainte de a apela metoda `gather()`. Dac캒 nu facem acest lucru, evaluarea va da eroare sau se va bloca pentru totdeauna. Apoi trimitem rezultatele la `metric.add_batch()` 탳i apel캒m `metric.compute()` odat캒 ce bucla de evaluare s-a 칥ncheiat.
- Salvarea 탳i 칥nc캒rcarea, unde mai 칥nt칙i salv캒m modelul 탳i tokenizerul, apoi apel캒m `repo.push_to_hub()`. Observa탵i c캒 folosim argumentul `blocking=False` pentru a spune bibliotecii 游뱅 Hub s캒 efectueze push-ul 칥ntr-un proces asincron. 칉n acest fel, antrenamentul continu캒 normal, iar aceast캒 instruc탵iune (lung캒) este executat캒 칥n fundal.

Iat캒 codul complet pentru bucla de antrenare:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

칉n cazul 칥n care este prima dat캒 c칙nd vede탵i un model salvat cu 游뱅 Accelerate, s캒 ne oprim pu탵in pentru a inspecta cele trei linii de cod care 칥l 칥nso탵esc:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

Prima linie se explic캒 de la sine: aceasta spune tuturor proceselor s캒 a탳tepte p칙n캒 c칙nd toat캒 lumea se afl캒 칥n etapa respectiv캒 칥nainte de a continua. Acest lucru are rolul de a ne asigura c캒 avem acela탳i model 칥n fiecare proces 칥nainte de a salva. Apoi lu캒m `unwrapped_model`, care este modelul de baz캒 pe care l-am definit. Metoda `accelerator.prepare()` modific캒 modelul pentru a func탵iona 칥n antrenarea distribuit캒, deci nu va mai avea metoda `save_pretrained()`; metoda `accelerator.unwrap_model()` anuleaz캒 acest pas. 칉n cele din urm캒, apel캒m metoda `save_pretrained()`, dar 칥i spunem s캒 foloseasc캒 metoda `accelerator.save()` 칥n loc de `torch.save()`.

Odat캒 f캒cut acest lucru, ar trebui s캒 ave탵i un model care produce rezultate destul de asem캒n캒toare cu cel antrenat cu `Trainer`. Pute탵i verifica modelul pe care l-am antrenat folosind acest cod la [*huggingface-course/bert-finetuned-ner-accelerate*] (https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate). 탲i dac캒 dori탵i s캒 testa탵i orice modific캒ri ale buclei de antrenare, le pute탵i implementa direct prin editarea codului prezentat mai sus!


{/if}

## Utilizarea model fine-tuned[[using-the-fine-tuned-model]]

V-am ar캒tat deja cum pute탵i utiliza modelul pe care l-am ajustat pe Model Hub cu inference widget. Pentru a-l utiliza la nivel local 칥ntr-un `pipeline`, trebuie doar s캒 specifica탵i identificatorul de model corespunz캒tor:

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Grozav! Modelul nostru func탵ioneaz캒 la fel de bine ca cel implicit pentru aceast pipeline!