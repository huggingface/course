<FrameworkSwitchCourse {fw} />

# Fine-tuningul la un masked language model[[fine-tuning-a-masked-language-model]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_tf.ipynb"},
]} />

{/if}

Pentru multe aplicaÈ›ii NLP care implicÄƒ modele Transformer, puteÈ›i lua pur È™i simplu un model preantrenat de pe Hugging Face Hub È™i sÄƒ Ã®l faceÈ›i fine-tune direct pe datele voastre pentru sarcina datÄƒ. Cu condiÈ›ia cÄƒ corpusul utilizat pentru preantrenare sÄƒ nu fie prea diferit de corpusul utilizat pentru fine-tuning, Ã®nvÄƒÈ›area prin transfer va produce de obicei rezultate bune.

Cu toate acestea, existÄƒ cÃ¢teva cazuri Ã®n care veÈ›i dori sÄƒ faceÈ›i fine-tune mai Ã®ntÃ¢i modelelor lingvistice pe datele voastre, Ã®nainte de a antrena un head specific sarcinii. De exemplu, dacÄƒ datasetul vostru conÈ›ine contracte juridice sau articole È™tiinÈ›ifice, un model Transformer obiÈ™nuit, precum BERT, va trata de obicei cuvintele specifice domeniului din corpus ca pe niÈ™te tokeni rari, iar performanÈ›a rezultatÄƒ poate fi mai puÈ›in satisfÄƒcÄƒtoare. Prin fine-tuningul  modelului lingvistic pe baza datelor din domeniu, puteÈ›i creÈ™te performanÈ›a multor sarcini, ceea ce Ã®nseamnÄƒ cÄƒ, de obicei, trebuie sÄƒ efectuaÈ›i acest pas o singurÄƒ datÄƒ!

Acest proces de fine-tuning a unui model lingvistic preantrenat pe date din domeniu se numeÈ™te de obicei _adaptare la domeniu_. Acesta a fost popularizat Ã®n 2018 de [ULMFiT](https://arxiv.org/abs/1801.06146), care a fost una dintre primele arhitecturi neuronale (bazate pe LSTM-uri) care a fÄƒcut ca Ã®nvÄƒÈ›area prin transfer sÄƒ funcÈ›ioneze cu adevÄƒrat pentru NLP. Un exemplu de adaptare la domeniu cu ULMFiT este prezentat Ã®n imaginea de mai jos; Ã®n aceastÄƒ secÈ›iune vom face ceva similar, dar cu un Transformer Ã®n loc de un LSTM!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg" alt="ULMFiT."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg" alt="ULMFiT."/>
</div>

PÃ¢nÄƒ la sfÃ¢rÈ™itul acestei secÈ›iuni, veÈ›i avea un [model de limbaj mascat](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) pe Hub, care poate completa automat propoziÈ›ii, dupÄƒ cum se poate vedea mai jos:

<iframe src="https://course-demos-distilbert-base-uncased-finetuned-imdb.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Hai sÄƒ Ã®ncepem!

<Youtube id="mqElG5QJWUg"/>

> [!TIP]
> ğŸ™‹ DacÄƒ termenii "masked language modeling" È™i "pretrained model" nu vÄƒ sunÄƒ familiar, mergeÈ›i sÄƒ verificaÈ›i [Capitolul 1](/course/chapter1), unde vÄƒ explicÄƒm toate aceste concepte de bazÄƒ, cu videoclipuri!

## Alegerea unui model preantrenat pentru masked language modeling[[picking-a-pretrained-model-for-masked-language-modeling]]

Pentru a Ã®ncepe, sÄƒ alegem un model preantrenat adecvat pentru modelarea limbajului mascat. DupÄƒ cum se vede Ã®n urmÄƒtoarea capturÄƒ de ecran, puteÈ›i gÄƒsi o listÄƒ de candidaÈ›i prin aplicarea filtrului "Fill-Mask" pe [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads):

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png" alt="Hub models." width="80%"/>
</div>

DeÈ™i modelele din familia BERT È™i RoBERTa sunt cele mai descÄƒrcate, vom utiliza un model numit [DistilBERT](https://huggingface.co/distilbert-base-uncased)
care poate fi antrenat mult mai rapid, cu o pierdere micÄƒ sau nulÄƒ a performanÈ›ei Ã®n aval. Acest model a fost antrenat folosind o tehnicÄƒ specialÄƒ numitÄƒ [_knowledge distillation_](https://en.wikipedia.org/wiki/Knowledge_distillation), Ã®n care un "model profesor" mare, precum BERT, este folosit pentru a ghida antrenarea unui "model elev" care are mult mai puÈ›ini parametrii. O explicaÈ›ie a detaliilor privind distilarea cunoÈ™tinÈ›elor ne-ar duce prea departe Ã®n aceastÄƒ secÈ›iune, dar dacÄƒ eÈ™ti interesat, poÈ›i citi totul despre aceasta Ã®n [_Natural Language Processing with Transformers_](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) (cunoscut sub numele colocvial de Transformers textbooks).

{#if fw === 'pt'}

SÄƒ continuÄƒm È™i sÄƒ descÄƒrcÄƒm modelul DistilBERT folosind clasa `AutoModelForMaskedLM`:

```python
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Putem vedea cÃ¢È›i parametri are acest model prin apelarea metodei `num_parameters()`:

```python
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")
```

```python out
'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'
```

{:else}

SÄƒ continuÄƒm È™i sÄƒ descÄƒrcÄƒm modelul DistilBERT folosind clasa `TFAutoModelForMaskedLM`:

```python
from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Putem vedea cÃ¢È›i parametri are acest model prin apelarea metodei `summary()`:

```python
model.summary()
```

```python out
Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________
```

{/if}

Cu aproximativ 67 de milioane de parametri, DistilBERT este de aproximativ douÄƒ ori mai mic decÃ¢t modelul de bazÄƒ BERT, ceea ce se traduce aproximativ printr-o creÈ™tere de douÄƒ ori a vitezei de antrenare - super! SÄƒ vedem acum ce tipuri de tokeni prezice acest model ca fiind cele mai probabile completÄƒri ale unui mic sample de text:

```python
text = "This is a great [MASK]."
```

Ca oameni, ne putem imagina multe posibilitÄƒÈ›i pentru tokenul `[MASK]`, cum ar fi "day", "ride" sau "painting". Pentru modelele preantrenate, predicÈ›iile depind de corpusul pe care modelul a fost antrenat, deoarece acesta Ã®nvaÈ›Äƒ sÄƒ detecteze tiparele statistice prezente Ã®n date. La fel ca BERT, DistilBERT a fost preantrenat pe dataseturile [English Wikipedia](https://huggingface.co/datasets/wikipedia) È™i [BookCorpus](https://huggingface.co/datasets/bookcorpus), astfel Ã®ncÃ¢t ne aÈ™teptÄƒm ca predicÈ›iile pentru `[MASK]` sÄƒ reflecte aceste domenii. Pentru a prezice masca, avem nevoie de tokenizerul DistilBERT pentru a produce inputurile pentru model, deci hai sÄƒ-l descÄƒrcÄƒm È™i pe acesta din Hub:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Cu un tokenizer È™i un model, putem acum sÄƒ transmitem exemplul nostru de text modelului, sÄƒ extragem logiturile È™i sÄƒ tipÄƒrim primii 5 candidaÈ›i:

{#if fw === 'pt'}

```python
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")
```

{:else}

```python
import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
# We negate the array before argsort to get the largest, not the smallest, logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")
```

{/if}

```python out
'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'
```
Putem vedea din rezultate cÄƒ predicÈ›iile modelului se referÄƒ la termeni din viaÈ›a de zi cu zi, ceea ce poate cÄƒ nu este surprinzÄƒtor avÃ¢nd Ã®n vedere fundamentul Wikipedia Ã®n limba englezÄƒ. SÄƒ vedem cum putem schimba acest domeniu Ã®n ceva puÈ›in mai niÈ™at - recenzii de filme foarte polarizate!


## Datasetul[[the-dataset]]

Pentru a prezenta adaptarea la domeniu, vom utiliza faimosul [Large Movie Review Dataset] (https://huggingface.co/datasets/imdb) (sau IMDb pe scurt), care este un corpus de recenzii de filme care este adesea utilizat pentru a evalua modelele de analizÄƒ a sentimentelor. Prin fine-tuningul aplicat asupra DistilBERT pe acest corpus, ne aÈ™teptÄƒm ca modelul de limbaj sÄƒ Ã®È™i adapteze vocabularul de la datele factuale din Wikipedia pe care a fost antrenat Ã®n prealabil la elementele mai subiective ale recenziilor de film. Putem obÈ›ine datele din Hugging Face Hub cu funcÈ›ia `load_dataset()` din ğŸ¤— Datasets:

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```

Putem vedea cÄƒ segmentele `train` È™i `test` conÈ›in fiecare 25.000 de recenzii, Ã®n timp ce existÄƒ un segment fÄƒrÄƒ label numitÄƒ `unsupervised` care conÈ›ine 50.000 de recenzii. SÄƒ aruncÄƒm o privire la cÃ¢teva sampleuri pentru a ne face o idee despre tipul de text cu care avem de-a face. AÈ™a cum am fÄƒcut Ã®n capitolele anterioare ale cursului, vom combina funcÈ›iile `Dataset.shuffle()` È™i `Dataset.select()` pentru a crea un sample aleatoriu:

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")
```

```python out

'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clichÃ©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'
```

Da, acestea sunt cu siguranÈ›Äƒ recenzii de film È™i, dacÄƒ sunteÈ›i suficient de bÄƒtrÃ¢ni, aÈ›i putea chiar Ã®nÈ›elege comentariul din ultima recenzie despre deÈ›inerea unei versiuni VHS ğŸ˜œ! DeÈ™i nu vom avea nevoie de labeluri pentru modelarea limbajului, putem vedea deja cÄƒ un `0` denotÄƒ o recenzie negativÄƒ, Ã®n timp ce un `1` corespunde uneia pozitive.

> [!TIP]
> âœï¸ **ÃncearcÄƒ!** CreaÈ›i un sample aleatoriu din segmentul `unsupervised` È™i verificaÈ›i cÄƒ labelurile nu sunt nici `0`, nici `1`. Ãn acelaÈ™i timp, aÈ›i putea verifica È™i dacÄƒ labelurile din segmentele `train` È™i `test` sunt Ã®ntr-adevÄƒr `0` sau `1` - aceasta este o verificare utilÄƒ pe care orice practicant NLP ar trebui sÄƒ o efectueze la Ã®nceputul unui nou proiect!

Acum cÄƒ am aruncat o privire rapidÄƒ asupra datelor, sÄƒ ne apucÄƒm sÄƒ le pregÄƒtim pentru modelarea limbajului mascat. DupÄƒ cum vom vedea, existÄƒ cÃ¢teva etape suplimentare pe care trebuie sÄƒ le parcurgem Ã®n comparaÈ›ie cu sarcinile de clasificare a secvenÈ›elor pe care le-am vÄƒzut Ã®n [Capitolul 3](/course/chapter3). SÄƒ Ã®ncepem!

## Preprocesarea datelor[[preprocessing-the-data]]

<Youtube id="8PmhEIXhBvI"/>

AtÃ¢t pentru auto-regressive cÃ¢t È™i pentru masked language modeling, un pas comun de preprocesare este concatenarea tuturor exemplelor È™i apoi Ã®mpÄƒrÈ›irea Ã®ntregului corpus Ã®n bucÄƒÈ›i de dimensiuni egale. Acest lucru este destul de diferit de abordarea noastrÄƒ obiÈ™nuitÄƒ, Ã®n care pur È™i simplu tokenizÄƒm exemplele individuale. De ce sÄƒ concatenÄƒm totul Ã®mpreunÄƒ? Motivul este cÄƒ exemplele individuale ar putea fi trunchiate dacÄƒ sunt prea lungi, ceea ce ar duce la pierderea de informaÈ›ii care ar putea fi utile pentru sarcina de modelare a limbajului!

Deci, pentru a Ã®ncepe, vom tokeniza mai Ã®ntÃ¢i corpusul nostru ca de obicei, dar _fÄƒrÄƒ_ a seta opÈ›iunea `truncation=True` Ã®n tokenizerul nostru. De asemenea, vom prelua ID-urile cuvintelor dacÄƒ acestea sunt disponibile (ceea ce va fi cazul dacÄƒ folosim un tokenizer rapid, aÈ™a cum este descris Ã®n [Capitolul 6](/course/chapter6/3)), deoarece vom avea nevoie de ele mai tÃ¢rziu pentru a face mascarea Ã®ntregului cuvÃ¢nt. Vom include acest lucru Ã®ntr-o funcÈ›ie simplÄƒ È™i, Ã®n acelaÈ™i timp, vom elimina coloanele `text` È™i `label`, deoarece nu mai avem nevoie de ele:

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# UtilizaÈ›i batched=True pentru a activa multithreadingul!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})
```

Deoarece DistilBERT este un model de tip BERT, putem vedea cÄƒ textele codate constau din `input_ids` È™i `attention_mask` pe care le-am vÄƒzut Ã®n alte capitole, precum È™i din `word_ids` pe care le-am adÄƒugat.

Acum cÄƒ am tokenizat recenziile de filme, urmÄƒtorul pas este sÄƒ le grupÄƒm pe toate È™i sÄƒ Ã®mpÄƒrÈ›im rezultatul Ã®n chunkuri. Dar cÃ¢t de mari ar trebui sÄƒ fie aceste chunkuri? Acest lucru va fi determinat Ã®n cele din urmÄƒ de cantitatea de memorie GPU pe care o aveÈ›i disponibilÄƒ, dar un bun punct de plecare este sÄƒ vedeÈ›i care este dimensiunea maximÄƒ a contextului modelului. Aceasta poate fi dedusÄƒ prin inspectarea atributului `model_max_length` al tokenizerului:

```python
tokenizer.model_max_length
```

```python out
512
```

AceastÄƒ valoare este derivatÄƒ din fiÈ™ierul *tokenizer_config.json* asociat cu un checkpoint; Ã®n acest caz putem vedea cÄƒ dimensiunea contextului este de 512 tokeni, la fel ca Ã®n cazul BERT.

> [!TIP]
> âœï¸ **ÃncearcÄƒ!** Unele modele Transformer, precum [BigBird](https://huggingface.co/google/bigbird-roberta-base) È™i [Longformer](hf.co/allenai/longformer-base-4096), au o lungime de context mult mai mare decÃ¢t BERT È™i alte modele Transformer mai vechi. IniÈ›ializaÈ›i tokenizerul pentru unul dintre aceste checkpointuri È™i verificaÈ›i dacÄƒ `model_max_length` este Ã®n concordanÈ›Äƒ cu ceea ce este menÈ›ionat pe model card.

Prin urmare, pentru a derula experimentele pe GPU-uri precum cele de pe Google Colab, vom alege ceva mai mic care sÄƒ Ã®ncapÄƒ Ã®n memorie:

```python
chunk_size = 128
```

> [!WARNING]
> ReÈ›ineÈ›i cÄƒ utilizarea unei dimensiuni mici a chunkurilor poate fi dÄƒunÄƒtor Ã®n scenariile din lumea realÄƒ, astfel Ã®ncÃ¢t ar trebui sÄƒ utilizaÈ›i o dimensiune care corespunde cazului de utilizare la care veÈ›i aplica modelul.

Acum vine partea distractivÄƒ. Pentru a arÄƒta cum funcÈ›ioneazÄƒ concatenarea, sÄƒ luÄƒm cÃ¢teva recenzii din setul nostru de antrenare tokenizat È™i sÄƒ imprimÄƒm numÄƒrul de tokeni per recenzie:

```python
# Slicingul produce o listÄƒ de liste pentru fiecare caracteristicÄƒ
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python out
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

Putem apoi concatena toate aceste exemple cu un dictionary comprehension, dupÄƒ cum urmeazÄƒ:

```python
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")
```

```python out
'>>> Concatenated reviews length: 951'
```

Minunat, lungimea totalÄƒ se verificÄƒ - aÈ™a cÄƒ acum sÄƒ Ã®mpÄƒrÈ›im recenziile concatenate Ã®n chunkuri de dimensiunea datÄƒ de `chunk_size`. Pentru a face acest lucru, iterÄƒm peste caracteristicile din `concatenated_examples` È™i folosim un list comprehension pentru a crea slice-uri ale fiecÄƒrei caracteristici. Rezultatul este un dicÈ›ionar de chunkuri pentru fiecare caracteristicÄƒ:

```python
chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")
```

```python out
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

DupÄƒ cum puteÈ›i vedea Ã®n acest exemplu, ultimul fragment va fi Ã®n general mai mic decÃ¢t dimensiunea maximÄƒ a fragmentului. ExistÄƒ douÄƒ strategii principale pentru a face faÈ›Äƒ acestei situaÈ›ii:

* AruncaÈ›i ultimul chunk dacÄƒ este mai mic decÃ¢t `chunk_size`.
* FaceÈ›i padding ultimului chunk pÃ¢nÄƒ cÃ¢nd lungimea sa este egalÄƒ cu `chunk_size`.

Vom adopta prima abordare aici, aÈ™a cÄƒ hai sÄƒ Ã®ncorporÄƒm toatÄƒ logica de mai sus Ã®ntr-o singurÄƒ funcÈ›ie pe care o putem aplica dataseturilor tokenizate:

```python
def group_texts(examples):
    # Concatenarea tuturor textelor
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # Calcularea lungimii textelor concatenate
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # RenunÈ›Äƒm la ultimul chunk dacÄƒ este mai mic decÃ¢t chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # ÃmpÄƒrÈ›iÈ›i pe bucÄƒÈ›i de max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # CreaÈ›i o nouÄƒ coloanÄƒ de labeluri
    result["labels"] = result["input_ids"].copy()
    return result
```

ObservaÈ›i cÄƒ Ã®n ultimul pas al `group_texts()` creÄƒm o nouÄƒ coloanÄƒ `labels` care este o copie a coloanei `input_ids`. DupÄƒ cum vom vedea Ã®n curÃ¢nd, acest lucru se datoreazÄƒ faptului cÄƒ Ã®n modelarea limbajului mascat obiectivul este de a prezice tokeni mascaÈ›i aleatoriu Ã®n input batch, iar prin crearea unei coloane `labels` furnizÄƒm adevÄƒrul de bazÄƒ din care modelul nostru de limbaj poate sÄƒ Ã®nveÈ›e.

SÄƒ aplicÄƒm acum funcÈ›ia `group_texts()` dataseturilor tokenizate folosind funcÈ›ia noastrÄƒ de Ã®ncredere `Dataset.map()`:

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})
```

PuteÈ›i vedea cÄƒ gruparea È™i apoi fragmentarea textelor a produs mult mai multe exemple decÃ¢t cele 25.000 iniÈ›iale pentru spliturile `train` È™i `test`. Acest lucru se datoreazÄƒ faptului cÄƒ acum avem exemple care implicÄƒ _contigous tokens_ care se Ã®ntind pe mai multe exemple din corpusul original. PuteÈ›i vedea acest lucru Ã®n mod explicit cÄƒutÃ¢nd tokenii speciali `[SEP]` È™i `[CLS]` Ã®ntr-unul dintre chunkuri:

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

Ãn acest exemplu puteÈ›i vedea douÄƒ recenzii de film care se suprapun, una despre un film de liceu È™i cealaltÄƒ despre persoanele fÄƒrÄƒ adÄƒpost. SÄƒ verificÄƒm, de asemenea, cum aratÄƒ labelurile pentru modelarea limbajului mascat:

```python out
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

AÈ™a cum era de aÈ™teptat de la funcÈ›ia noastrÄƒ `group_texts()` de mai sus, acest lucru pare identic cu `input_ids` decodificat - dar atunci cum poate modelul nostru sÄƒ Ã®nveÈ›e ceva? Ne lipseÈ™te un pas cheie: inserarea tokenilor `[MASK]` Ã®n poziÈ›ii aleatorii Ã®n inputuri! SÄƒ vedem cum putem face acest lucru din mers, Ã®n timpul fine-tuningului, folosind un data collator special.

## Fine-tuningul asupra DistilBERT cu API-ul `Trainer`[[fine-tuning-distilbert-with-the-trainer-api]]

Fine-tuningul unui model lingvistic mascat este aproape identic cu fine-tuningul a unui model de clasificare a secvenÈ›elor, aÈ™a cum am fÄƒcut Ã®n [Capitolul 3](/course/chapter3). Singura diferenÈ›Äƒ este cÄƒ avem nevoie de un data collator care poate masca aleatoriu o parte dintre tokeni din fiecare batch de texte. Din fericire, ğŸ¤— Transformers vine pregÄƒtit cu un `DataCollatorForLanguageModeling` dedicat tocmai pentru aceastÄƒ sarcinÄƒ. Trebuie doar sÄƒ Ã®i transmitem tokenizerul È™i un argument `mlm_probability` care specificÄƒ ce fracÈ›iune din tokeni trebuie mascatÄƒ. Vom alege 15%, care este cantitatea utilizatÄƒ pentru BERT È™i o alegere comunÄƒ Ã®n literatura de specialitate:

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

Pentru a vedea cum funcÈ›ioneazÄƒ mascarea aleatorie, sÄƒ introducem cÃ¢teva exemple Ã®n data collator. Deoarece se aÈ™teaptÄƒ la o listÄƒ de "dict"-uri, Ã®n care fiecare "dict" reprezintÄƒ un singur chunk de text continuu, mai Ã®ntÃ¢i iterÄƒm peste dataset Ã®nainte de a trimite batchul cÄƒtre data collator. EliminÄƒm cheia `"word_ids"` pentru acest data collator, deoarece acesta nu o aÈ™teaptÄƒ:

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george å®‡in stated )å…¬ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

Frumos, a funcÈ›ionat! Putem vedea cÄƒ tokenul `[MASK]` a fost inserat aleatoriu Ã®n diferite locuri din textul nostru. Acestea vor fi tokenii pe care modelul nostru va trebui sÄƒ le prezicÄƒ Ã®n timpul antrenamentului - iar frumuseÈ›ea data collatorului este cÄƒ va introduce aleatoriu tokenul `[MASK]` cu fiecare batch!

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** RulaÈ›i fragmentul de cod de mai sus de mai multe ori pentru a vedea cum se Ã®ntÃ¢mplÄƒ mascarea aleatorie Ã®n faÈ›a ochilor voÈ™tri! De asemenea, Ã®nlocuiÈ›i metoda `tokenizer.decode()` cu `tokenizer.convert_ids_to_tokens()` pentru a vedea cÄƒ uneori un singur token dintr-un cuvÃ¢nt dat este mascat, È™i nu celelalte.

{#if fw === 'pt'}

Un efect secundar al mascÄƒrii aleatorii este faptul cÄƒ metricile noastre de evaluare nu vor fi deterministe atunci cÃ¢nd folosim `Trainer`, deoarece folosim acelaÈ™i data collator pentru seturile de antrenare È™i testare. Vom vedea mai tÃ¢rziu, cÃ¢nd ne vom uita la aplicarea fine-tuningului cu ğŸ¤— Accelerate, cum putem folosi flexibilitatea unei bucle de evaluare personalizate pentru a Ã®ngheÈ›a caracterul aleatoriu.

{/if}

La antrenarea modelelor pentru modelarea limbajului mascat, o tehnicÄƒ care poate fi utilizatÄƒ este mascarea cuvintelor Ã®ntregi Ã®mpreunÄƒ, nu doar a tokenilor individuali. AceastÄƒ abordare se numeÈ™te _whole word masking_. DacÄƒ dorim sÄƒ utilizÄƒm mascarea Ã®ntregului cuvÃ¢nt, va trebui sÄƒ construim noi Ã®nÈ™ine un data collator. Un data collator este doar o funcÈ›ie care preia o listÄƒ de sampleuri È™i le converteÈ™te Ã®ntr-un batch, aÈ™a cÄƒ hai sÄƒ facem asta acum! Vom utiliza ID-urile cuvintelor calculate mai devreme pentru a realiza o hartÄƒ Ã®ntre indicii cuvintelor È™i tokenii corespunzÄƒtori, apoi vom decide aleatoriu ce cuvinte sÄƒ mascÄƒm È™i vom aplica masca respectivÄƒ asupra inputurilor. ReÈ›ineÈ›i cÄƒ labelurile sunt toate `-100`, cu excepÈ›ia celor care corespund cuvintelor mascate.

{#if fw === 'pt'}

```py
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Create a map between words and corresponding token indices
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Randomly mask words
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return default_data_collator(features)
```

{:else}

```py
import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Create a map between words and corresponding token indices
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Randomly mask words
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return tf_default_data_collator(features)
```

{/if}

Ãn continuare, Ã®l putem Ã®ncerca pe aceleaÈ™i sampleuri ca Ã®nainte:

```py
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python out
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** RulaÈ›i fragmentul de cod de mai sus de mai multe ori pentru a vedea cum se Ã®ntÃ¢mplÄƒ mascarea aleatorie Ã®n faÈ›a ochilor voÈ™tri! De asemenea, Ã®nlocuiÈ›i metoda `tokenizer.decode()` cu `tokenizer.convert_ids_to_tokens()` pentru a vedea cÄƒ tokenii dintr-un cuvÃ¢nt dat sunt Ã®ntotdeauna mascaÈ›i Ã®mpreunÄƒ.

Acum, cÄƒ avem douÄƒ data collators, restul paÈ™ilor fine-tuning sunt standard. PregÄƒtirea poate dura ceva timp pe Google Colab dacÄƒ nu sunteÈ›i suficient de norocos sÄƒ obÈ›ineÈ›i un GPU P100 mitic ğŸ˜­, aÈ™a cÄƒ vom reduce mai Ã®ntÃ¢i dimensiunea setului de antrenare la cÃ¢teva mii de exemple. Nu vÄƒ faceÈ›i griji, vom obÈ›ine Ã®n continuare un model lingvistic destul de decent! O modalitate rapidÄƒ de a reduce sampleurile unui dataset Ã®n ğŸ¤— Datasets este prin intermediul funcÈ›iei `Dataset.train_test_split()` pe care am vÄƒzut-o Ã®n [Capitolul 5](/course/chapter5):

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})
```

Acest lucru a creat Ã®n mod automat noi splituri de `train` È™i `test`, cu dimensiunea setului de antrenare setatÄƒ la 10.000 de exemple È™i a setului de validare la 10% din aceasta - nu ezitaÈ›i sÄƒ mÄƒriÈ›i aceastÄƒ valoare dacÄƒ aveÈ›i un GPU puternic! UrmÄƒtorul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ ne conectÄƒm la Hugging Face Hub. DacÄƒ executaÈ›i acest cod Ã®ntr-un notebook, puteÈ›i face acest lucru cu urmÄƒtoarea funcÈ›ie utilitarÄƒ:

```python
from huggingface_hub import notebook_login

notebook_login()
```

care va afiÈ™a un widget Ã®n care vÄƒ puteÈ›i introduce credenÈ›ialele. Alternativ, puteÈ›i rula:

```
huggingface-cli login
```

in your favorite terminal and log in there. 

{#if fw === 'tf'}

OdatÄƒ ce ne-am conectat, putem crea dataseturile `tf.data`. Pentru a face acest lucru, vom utiliza metoda `prepare_tf_dataset()`, care utilizeazÄƒ modelul nostru pentru a deduce automat ce coloane ar trebui sÄƒ intre Ã®n dataset. DacÄƒ doriÈ›i sÄƒ controlaÈ›i exact ce coloane sÄƒ utilizaÈ›i, puteÈ›i folosi Ã®n schimb metoda `Dataset.to_tf_dataset()`. Pentru a simplifica lucrurile, vom utiliza aici doar data collatorul standard, dar puteÈ›i Ã®ncerca È™i whole word masking collator È™i puteÈ›i compara rezultatele ca un exerciÈ›iu:

```python
tf_train_dataset = model.prepare_tf_dataset(
    downsampled_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = model.prepare_tf_dataset(
    downsampled_dataset["test"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

Ãn continuare, setÄƒm hiperparametrii de antrenare È™i compilÄƒm modelul nostru. UtilizÄƒm funcÈ›ia `create_optimizer()` din biblioteca ğŸ¤— Transformers, care ne oferÄƒ un optimizator `AdamW` cu o scÄƒdere liniarÄƒ a ratei de Ã®nvÄƒÈ›are. UtilizÄƒm, de asemenea, pierderea Ã®ncorporatÄƒ Ã®n model, care este cea implicitÄƒ atunci cÃ¢nd nu este specificatÄƒ nicio pierdere ca argument pentru `compile()`, È™i setÄƒm precizia de antrenare la `"mixed_float16"`. ReÈ›ineÈ›i cÄƒ, dacÄƒ utilizaÈ›i un GPU Colab sau alt GPU care nu are suport accelerat pentru float16, ar trebui probabil sÄƒ comentaÈ›i aceastÄƒ linie.

Ãn plus, am configurat un `PushToHubCallback` care va salva modelul Ã®n Hub dupÄƒ fiecare epocÄƒ. PuteÈ›i specifica numele repositoriului cÄƒtre care doriÈ›i sÄƒ faceÈ›i push cu argumentul `hub_model_id` (Ã®n special, va trebui sÄƒ utilizaÈ›i acest argument pentru a face push cÄƒtre o organizaÈ›ie). De exemplu, pentru a trimite modelul cÄƒtre organizaÈ›ia [`huggingface-course`](https://huggingface.co/huggingface-course), am adÄƒugat `hub_model_id="huggingface-course/distilbert-finetuned-imdb"`. Ãn mod implicit, repositoriul utilizat va fi Ã®n namespaceul vostru È™i numit dupÄƒ output directory-ul pe care l-aÈ›i stabilit, deci Ã®n cazul nostru va fi `"lewtun/distilbert-finetuned-imdb"`.

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

model_name = model_checkpoint.split("/")[-1]
callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)
```

Acum suntem gata sÄƒ executÄƒm `model.fit()` - dar Ã®nainte de a face acest lucru, sÄƒ ne uitÄƒm pe scurt la _perplexitate_, care este o metricÄƒ comunÄƒ pentru a evalua performanÈ›a modelelor de limbaj.

{:else}

OdatÄƒ ce suntem conectaÈ›i, putem specifica argumentele pentru `Trainer`:

```python
from transformers import TrainingArguments

batch_size = 64
# Show the training loss with every epoch
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)
```

Aici am modificat cÃ¢teva dintre opÈ›iunile implicite, inclusiv `logging_steps` pentru a ne asigura cÄƒ urmÄƒrim pierderea de antrenare cu fiecare epocÄƒ. De asemenea, am folosit `fp16=True` pentru a activa antrenarea cu precizie mixtÄƒ, ceea ce ne oferÄƒ un alt impuls vitezei. Ãn mod implicit, `Trainer` va elimina toate coloanele care nu fac parte din metoda `forward()` a modelului. Aceasta Ã®nseamnÄƒ cÄƒ, dacÄƒ utilizaÈ›i whole word masking collator, va trebui sÄƒ setaÈ›i È™i `remove_unused_columns=False` pentru a vÄƒ asigura cÄƒ nu pierdem coloana `word_ids` Ã®n timpul antrenamentului.

ReÈ›ineÈ›i cÄƒ puteÈ›i specifica numele repositoriului cÄƒtre care doriÈ›i sÄƒ faceÈ›i push cu argumentul `hub_model_id` (Ã®n special, va trebui sÄƒ utilizaÈ›i acest argument pentru a face push cÄƒtre o organizaÈ›ie). De exemplu, atunci cÃ¢nd am fÄƒcut push modelului cÄƒtre organizaÈ›ia [`huggingface-course`](https://huggingface.co/huggingface-course), am adÄƒugat `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` la `TrainingArguments`. Ãn mod implicit, repositoriul utilizat va fi Ã®n namespaceul vostru È™i denumit dupÄƒ output directory-ul pe care l-aÈ›i stabilit, deci Ã®n cazul nostru va fi `"lewtun/distilbert-finetuned-imdb"`.

Acum avem toate ingredientele pentru iniÈ›ializarea `Trainer`. Aici folosim doar `data_collator` standard, dar puteÈ›i Ã®ncerca whole word masking collator È™i sÄƒ comparaÈ›i rezultatele ca un exerciÈ›iu:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Acum suntem gata sÄƒ rulÄƒm `trainer.train()` - dar Ã®nainte de a face acest lucru, sÄƒ analizÄƒm pe scurt _perplexitatea_, care este o metricÄƒ comunÄƒ de evaluare a performanÈ›ei modelelor de limbaj.

{/if}

### Perplexity pentru language models[[perplexity-for-language-models]]

<Youtube id="NURcDHhYe98"/>

Spre deosebire de alte sarcini, cum ar fi clasificarea textului sau rÄƒspunderea la Ã®ntrebÄƒri, unde ni se oferÄƒ un corpus labeled pe care sÄƒ antrenÄƒm, cu modelarea limbajului nu avem labeluri explicite. AÈ™adar, cum determinÄƒm ce face un model lingvistic bun? La fel ca Ã®n cazul funcÈ›iei de autocorectare din telefon, un model lingvistic bun este unul care atribuie probabilitÄƒÈ›i ridicate propoziÈ›iilor corecte din punct de vedere gramatical È™i probabilitÄƒÈ›i scÄƒzute propoziÈ›iilor fÄƒrÄƒ sens. Pentru a vÄƒ face o idee mai bunÄƒ despre cum aratÄƒ acest lucru, puteÈ›i gÄƒsi online seturi Ã®ntregi de "autocorrect fails", Ã®n care modelul din telefonul unei persoane a produs niÈ™te completÄƒri destul de amuzante (È™i adesea nepotrivite)!

{#if fw === 'pt'}

PresupunÃ¢nd cÄƒ setul nostru de testare constÄƒ Ã®n cea mai mare parte din propoziÈ›ii corecte din punct de vedere gramatical, atunci o modalitate de a mÄƒsura calitatea modelului nostru lingvistic este de a calcula probabilitÄƒÈ›ile pe care le atribuie urmÄƒtorului cuvÃ¢nt Ã®n toate propoziÈ›iile din setul de testare. Probabilitatea ridicatÄƒ indicÄƒ faptul cÄƒ modelul nu este "surprins" sau "perplex" de exemplele nevÄƒzute È™i sugereazÄƒ cÄƒ a Ã®nvÄƒÈ›at tiparele gramaticale de bazÄƒ ale limbii. ExistÄƒ diverse definiÈ›ii matematice ale perplexitÄƒÈ›ii, dar cea pe care o vom utiliza o defineÈ™te ca the exponential of the cross-entropy loss. Astfel, putem calcula perplexitatea modelului nostru preantrenat utilizÃ¢nd funcÈ›ia `Trainer.evaluate()` pentru a calcula pierderea de cross-entropy pe setul de testare È™i apoi luÃ¢nd exponenÈ›iala rezultatului:

```python
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

PresupunÃ¢nd cÄƒ setul nostru de testare constÄƒ Ã®n cea mai mare parte din propoziÈ›ii corecte din punct de vedere gramatical, atunci o modalitate de a mÄƒsura calitatea modelului nostru lingvistic este de a calcula probabilitÄƒÈ›ile pe care le atribuie urmÄƒtorului cuvÃ¢nt Ã®n toate propoziÈ›iile din setul de testare. ProbabilitÄƒÈ›ile ridicate indicÄƒ faptul cÄƒ modelul indicÄƒ faptul cÄƒ modelul nu este "surprins" sau "perplex" de exemplele nevÄƒzute È™i sugereazÄƒ cÄƒ a Ã®nvÄƒÈ›at modelele de bazÄƒ ale gramaticii limbii. ExistÄƒ diverse definiÈ›ii matematice ale perplexitÄƒÈ›ii, dar cea pe care o vom folosi o defineÈ™te ca the exponential of the cross-entropy loss. Astfel, putem calcula perplexitatea modelului nostru preantrenat folosind metoda `model.evaluate()` pentru a calcula pierderea de entropie Ã®ncruciÈ™atÄƒ pe setul de testare È™i apoi luÃ¢nd exponenÈ›iala rezultatului:

```python
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 21.75
```

Un scor de perplexitate mai mic Ã®nseamnÄƒ un model lingvistic mai bun, iar aici putem vedea cÄƒ modelul nostru iniÈ›ial are o valoare oarecum mare. SÄƒ vedem dacÄƒ o putem reduce prin fine-tuning! Pentru a face acest lucru, vom rula mai Ã®ntÃ¢i bucla de antrenare:

{#if fw === 'pt'}

```python
trainer.train()
```

{:else}

```python
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

È™i apoi calculaÈ›i perplexitatea rezultatÄƒ pe setul de testare ca Ã®nainte:

{#if fw === 'pt'}

```python
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

```python
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 11.32
```

Grozav - aceasta este o reducere destul de mare a perplexitÄƒÈ›ii, ceea ce ne spune cÄƒ modelul a Ã®nvÄƒÈ›at ceva despre domeniul recenziilor de filme!

{#if fw === 'pt'}

OdatÄƒ ce antrenarea este finalizatÄƒ, putem trimite cardul modelului cu informaÈ›iile de antrenare cÄƒtre Hub (checkpointurile sunt salvate Ã®n timpul antrenare):

```python
trainer.push_to_hub()
```

{/if}

> [!TIP]
> âœï¸ **RÃ¢ndul tÄƒu!** RulaÈ›i antrenamentul de mai sus dupÄƒ schimbarea data collatorului cu whole word masking collator. ObÈ›ineÈ›i rezultate mai bune?

{#if fw === 'pt'} 

Ãn cazul nostru de utilizare, nu a fost nevoie sÄƒ facem nimic special cu bucla de antrenare, dar Ã®n unele cazuri s-ar putea sÄƒ fie nevoie sÄƒ implementaÈ›i o logicÄƒ personalizatÄƒ. Pentru aceste aplicaÈ›ii, puteÈ›i utiliza ğŸ¤— Accelerate -- sÄƒ aruncÄƒm o privire!

## Fine-tuningul DistilBERT cu ğŸ¤— Accelerate[[fine-tuning-distilbert-with-accelerate]]

AÈ™a cum am vÄƒzut cu `Trainer`, fine-tuningul unui model de limbaj mascat este foarte asemÄƒnÄƒtor cu exemplul de clasificare a textului din [Capitolul 3](/course/chapter3). De fapt, singura subtilitate este utilizarea unui data collator special, pe care l-am abordat mai devreme Ã®n aceastÄƒ secÈ›iune!

Cu toate acestea, am vÄƒzut cÄƒ `DataCollatorForLanguageModeling` aplicÄƒ, de asemenea, o mascare aleatorie cu fiecare evaluare, astfel Ã®ncÃ¢t vom vedea unele fluctuaÈ›ii Ã®n scorurile noastre de perplexitate cu fiecare rulare de antrenament. O modalitate de a elimina aceastÄƒ sursÄƒ de dezordine este de a aplica mascarea _o singurÄƒ datÄƒ_ pe Ã®ntregul set de teste È™i apoi de a utiliza data collatorul implicit din ğŸ¤— Transformers pentru a colecta batch-urile Ã®n timpul evaluÄƒrii. Pentru a vedea cum funcÈ›ioneazÄƒ acest lucru, sÄƒ implementÄƒm o funcÈ›ie simplÄƒ care aplicÄƒ mascarea pe un batch, similarÄƒ cu prima noastrÄƒ Ã®ntÃ¢lnire cu `DataCollatorForLanguageModeling`:

```python
def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # Create a new "masked" column for each column in the dataset
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}
```

Ãn continuare, vom aplica aceastÄƒ funcÈ›ie setului nostru de testare È™i vom elimina coloanele nemascate pentru a le putea Ã®nlocui cu cele mascate. PuteÈ›i utiliza whole word masking prin Ã®nlocuirea `data_collator` de mai sus cu cel corespunzÄƒtor, caz Ã®n care trebuie sÄƒ eliminaÈ›i prima linie de aici:

```py
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)
```

Putem configura apoi dataloaderele ca de obicei, dar vom folosi `default_data_collator` de la ğŸ¤— Transformers pentru setul de evaluare:

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)
```

De aici, vom urma paÈ™ii standard cu ğŸ¤— Accelerate. Ãn primul rÃ¢nd, se Ã®ncarcÄƒ o versiune nouÄƒ a modelului antrenat:

```
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Apoi trebuie sÄƒ specificÄƒm optimizatorul; vom folosi standardul `AdamW`:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Cu aceste obiecte, acum putem pregÄƒti totul pentru antrenare cu obiectul `Accelerator`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

Acum cÄƒ modelul, optimizatorul È™i dataloaderul sunt configurate, putem specifica learning rate schedulerul cum urmeazÄƒ:

```python
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Mai este un singur lucru de fÄƒcut Ã®nainte de antrenare: creaÈ›i un repositoriu de modele pe Hugging Face Hub! Putem utiliza biblioteca ğŸ¤— Hub pentru a genera mai Ã®ntÃ¢i numele complet al repositoriul nostru:

```python
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'
```

apoi creaÈ›i È™i clonaÈ›i repositoriul folosind clasa `Repository` din ğŸ¤— Hub:

```python
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)
```

OdatÄƒ ce acest lucru este fÄƒcut, trebuie doar sÄƒ scriem ciclul complet de antrenare È™i evaluare:

```python
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409
```

MiÈ™to, am reuÈ™it sÄƒ evaluÄƒm perplexitatea cu fiecare epocÄƒ È™i sÄƒ ne asigurÄƒm cÄƒ mai multe runde de antrenament sunt reproductibile!

{/if}

## Utilizarea modelului fine-tuned[[using-our-fine-tuned-model]]

PuteÈ›i interacÈ›iona cu modelul vostru fine-tuned utilizÃ¢nd widgetul sÄƒu de pe Hub, fie local cu `pipeline` din ğŸ¤— Transformers. SÄƒ folosim aceasta din urmÄƒ pentru a descÄƒrca modelul nostru folosind pipelineuul `fill-mask`:

```python
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

Putem apoi sÄƒ alimentÄƒm pipelineul cu exemplul nostru de text "This is a hrea [MASK]" È™i sÄƒ vedem care sunt primele 5 predicÈ›ii:

```python
preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")
```

```python out
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

Frumos - modelul nostru È™i-a adaptat Ã®n mod clar weighturile pentru a prezice cuvintele care sunt asociate mai puternic cu filmele!

<Youtube id="0Oxphw4Q9fo"/>

Acest lucru Ã®ncheie primul nostru experiment de antrenare a unui model lingvistic. Ãn [secÈ›iunea 6](/course/ro/chapter7/6) veÈ›i Ã®nvÄƒÈ›a cum sÄƒ antrenaÈ›i de la zero un model auto-regressive precum GPT-2; mergeÈ›i acolo dacÄƒ doriÈ›i sÄƒ vedeÈ›i cum vÄƒ puteÈ›i preantrena propriul model Transformer!

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** Pentru a cuantifica beneficiile adaptÄƒrii domeniului, faceÈ›i fine-tune unui clasificator pe labelurile IMDb atÃ¢t pentru checkpointurile DistilBERT preantrenate, cÃ¢t È™i pentru cele fine-tuned. DacÄƒ aveÈ›i nevoie de o recapitulare a clasificÄƒrii textului, consultaÈ›i [Capitolul 3](/course/chapter3).
