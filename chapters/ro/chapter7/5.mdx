<FrameworkSwitchCourse {fw} />

# Sumarizare[[summarization]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
]} />

{/if}


Ãn aceastÄƒ secÈ›iune vom analiza modul Ã®n care modelele Transformer pot fi utilizate pentru a condensa documente lungi Ã®n rezumate, o sarcinÄƒ cunoscutÄƒ sub numele de _text summarization_. Aceasta este una dintre cele mai dificile sarcini NLP, deoarece necesitÄƒ o gamÄƒ largÄƒ de abilitÄƒÈ›i, cum ar fi Ã®nÈ›elegerea pasajelor lungi È™i generarea unui text coerent care integreazÄƒ principalele subiecte dintr-un document. Cu toate acestea, atunci cÃ¢nd este bine realizatÄƒ, rezumarea textului este un instrument puternic care poate accelera diverse procese de business prin scutirea experÈ›ilor Ã®ntr-un anumit domeniu de a citi documente lungi Ã®n detaliu.

<Youtube id="yHnr5Dk2zCI"/>

DeÈ™i existÄƒ deja diverse modele bine puse la punct pentru sumarizare pe [Hugging Face Hub] (https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), aproape toate acestea sunt potrivite numai pentru documentele Ã®n limba englezÄƒ. Prin urmare, pentru a adÄƒuga o Ã®ntorsÄƒturÄƒ Ã®n aceastÄƒ secÈ›iune, vom antrena un model bilingv pentru englezÄƒ È™i spaniolÄƒ. PÃ¢nÄƒ la sfÃ¢rÈ™itul acestei secÈ›iuni, veÈ›i avea un [model](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) care poate rezuma recenziile clienÈ›ilor precum cel prezentat aici:


<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

DupÄƒ cum vom vedea, aceste rezumate sunt concise deoarece sunt Ã®nvÄƒÈ›ate din titlurile pe care clienÈ›ii le furnizeazÄƒ Ã®n recenziile lor despre produse. SÄƒ Ã®ncepem prin alcÄƒtuirea unui corpus bilingv adecvat pentru aceastÄƒ sarcinÄƒ.

## PregÄƒtirea unui corpus multilingv[[preparing-a-multilingual-corpus]]

Vom utiliza [Multilingual Amazon Reviews Corpus] (https://huggingface.co/datasets/amazon_reviews_multi) pentru a crea bilingv summarizerul nostru. Acest corpus este format din recenzii ale produselor Amazon Ã®n È™ase limbi È™i este utilizat de obicei pentru a evalua clasificatoarele multilingve. Cu toate acestea, deoarece fiecare recenzie este Ã®nsoÈ›itÄƒ de un titlu scurt, putem folosi titlurile ca rezumate È›intÄƒ din care modelul nostru sÄƒ Ã®nveÈ›e! Pentru a Ã®ncepe, sÄƒ descÄƒrcÄƒm subseturile Ã®n englezÄƒ È™i spaniolÄƒ de la Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

DupÄƒ cum puteÈ›i vedea, pentru fiecare limbÄƒ existÄƒ 200.000 de recenzii pentru splitul `train` È™i 5.000 de recenzii pentru fiecare dintre spliturile `validation` È™i `test`. InformaÈ›iile despre recenzii care ne intereseazÄƒ sunt conÈ›inute Ã®n coloanele `review_body` È™i `review_title`. SÄƒ analizÄƒm cÃ¢teva exemple prin crearea unei funcÈ›ii simple care preia un sample aleatoriu din setul de antrenare cu ajutorul tehnicilor Ã®nvÄƒÈ›ate Ã®n [Capitolul 5](/course/chapter5):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itâ€™s job and itâ€™s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** SchimbaÈ›i seedul aleatoriu Ã®n comanda `Dataset.shuffle()` pentru a explora alte recenzii din corpus. DacÄƒ sunteÈ›i vorbitor de spaniolÄƒ, aruncaÈ›i o privire la unele dintre recenziile din `spanish_dataset` pentru a vedea dacÄƒ È™i titlurile par a fi rezumate rezonabil.

Acest sample aratÄƒ diversitatea recenziilor pe care le gÄƒsim de obicei online, variind de la pozitive la negative (È™i totul Ã®ntre ele!). DeÈ™i exemplul cu titlul "meh" nu este foarte informativ, celelalte titluri par a fi rezumate decente ale recenziilor Ã®n sine. Antrenarea unui model de rezumare pe toate cele 400 000 de recenzii ar dura mult prea mult pe un singur GPU, aÈ™a cÄƒ ne vom concentra pe generarea de rezumate pentru un singur domeniu de produse. Pentru a avea o idee despre domeniile din care putem alege, sÄƒ convertim `english_dataset` Ã®ntr-un `pandas.DataFrame` È™i sÄƒ calculÄƒm numÄƒrul de recenzii per categorie de produse:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

Cele mai populare produse din datasetul Ã®n limba englezÄƒ sunt despre articole de uz casnic, Ã®mbrÄƒcÄƒminte È™i electronice fÄƒrÄƒ fir. Cu toate acestea, pentru a rÄƒmÃ¢ne la Amazontheme, sÄƒ ne concentrÄƒm pe rezumatul recenziilor de cÄƒrÈ›i - la urma urmei, acesta este motivul pentru care compania a fost fondatÄƒ! Putem vedea douÄƒ categorii de produse care se potrivesc (`book` È™i `digital_ebook_purchase`), deci sÄƒ filtrÄƒm dataseturile Ã®n ambele limbi doar pentru aceste produse. DupÄƒ cum am vÄƒzut Ã®n [Capitolul 5](/course/chapter5), funcÈ›ia `Dataset.filter()` ne permite sÄƒ tÄƒiem un dataset foarte eficient, deci putem defini o funcÈ›ie simplÄƒ pentru a face acest lucru:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

Acum, cÃ¢nd aplicÄƒm aceastÄƒ funcÈ›ie la `english_dataset` È™i `spanish_dataset`, rezultatul va conÈ›ine doar acele rÃ¢nduri care implicÄƒ categoriile de cÄƒrÈ›i. Ãnainte de a aplica filtrul, sÄƒ schimbÄƒm formatul din `english_dataset` din `"pandas"` Ã®napoi Ã®n `"arrow"`:

```python
english_dataset.reset_format()
```

Putem aplica apoi funcÈ›ia de filtrare È™i, ca o verificare a corectitudinii, sÄƒ inspectÄƒm un sample de recenzii pentru a vedea dacÄƒ acestea sunt Ã®ntr-adevÄƒr despre cÄƒrÈ›i:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

Bine, putem vedea cÄƒ recenziile nu sunt strict despre cÄƒrÈ›i È™i se pot referi la lucruri precum calendare È™i aplicaÈ›ii electronice precum OneNote. Cu toate acestea, domeniul pare potrivit pentru a antrena un model de sumarizare. Ãnainte de a analiza diferitele modele care sunt potrivite pentru aceastÄƒ sarcinÄƒ, trebuie sÄƒ mai pregÄƒtim puÈ›in datele: sÄƒ combinÄƒm recenziile Ã®n englezÄƒ È™i spaniolÄƒ ca un singur obiect `DatasetDict`. ğŸ¤— Datasets oferÄƒ o funcÈ›ie utilÄƒ `concatenate_datasets()` care (dupÄƒ cum sugereazÄƒ È™i numele) va concatena douÄƒ obiecte `Dataset` unul peste celÄƒlalt. AÈ™adar, pentru a crea datasetul nostru bilingv, vom parcurge Ã®n buclÄƒ fiecare Ã®mpÄƒrÈ›ire, vom concatena dataseturile pentru acel split È™i vom amesteca rezultatul pentru a ne asigura cÄƒ modelul nostru nu se adapteazÄƒ excesiv la o singurÄƒ limbÄƒ:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÃ‘ADO'
'>> Review: Me llegÃ³ el dÃ­a que tocaba, junto a otros libros que pedÃ­, pero la caja llegÃ³ en mal estado lo cual daÃ±Ã³ las esquinas de los libros porque venÃ­an sin protecciÃ³n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

Acest lucru aratÄƒ cu siguranÈ›Äƒ ca un amestec de recenzii Ã®n englezÄƒ È™i spaniolÄƒ! Acum cÄƒ avem un corpus de antrenament, un ultim lucru de verificat este distribuÈ›ia cuvintelor Ã®n recenzii È™i Ã®n titlurile acestora. Acest lucru este deosebit de important pentru sarcinile de sumarizare, Ã®n cazul Ã®n care rezumatele scurte de referinÈ›Äƒ din date pot influenÈ›a modelul sÄƒ producÄƒ doar unul sau douÄƒ cuvinte Ã®n rezumatele generate. Graficele de mai jos aratÄƒ distribuÈ›ia cuvintelor È™i putem observa cÄƒ titlurile sunt puternic Ã®nclinate spre 1-2 cuvinte:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="DistribuÈ›ia numÄƒrului de cuvinte pentru titlurile È™i textele recenziei."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="DistribuÈ›ia numÄƒrului de cuvinte pentru titlurile È™i textele recenziei."/>
</div>

Pentru a rezolva acest lucru, vom filtra exemplele cu titluri foarte scurte, astfel Ã®ncÃ¢t modelul nostru sÄƒ poatÄƒ produce rezumate mai interesante. Deoarece avem de-a face cu texte Ã®n englezÄƒ È™i spaniolÄƒ, putem folosi rough heuristic pentru a face split titlurilor pe baza spaÈ›iului alb È™i apoi sÄƒ folosim metoda noastrÄƒ de Ã®ncredere `Dataset.filter()` dupÄƒ cum urmeazÄƒ:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

Now that we've prepared our corpus, let's take a look at a few possible Transformer models that one might fine-tune on it!

## Modele pentru sumarizarea textului[[models-for-text-summarization]]

DacÄƒ vÄƒ gÃ¢ndiÈ›i bine, rezumarea textului este o sarcinÄƒ similarÄƒ cu machine translation: avem un corp de text, cum ar fi o recenzie, pe care am dori sÄƒ o "traducem" Ã®ntr-o versiune mai scurtÄƒ care sÄƒ capteze caracteristicile principale ale datelor de intrare. Ãn consecinÈ›Äƒ, majoritatea modelelor Transformer pentru rezumare adoptÄƒ arhitectura codificator-decodificator pe care am Ã®ntÃ¢lnit-o pentru prima datÄƒ Ã®n [Capitolul 1](/course/chapter1), deÈ™i existÄƒ unele excepÈ›ii, cum ar fi familia de modele GPT, care poate fi, de asemenea, utilizatÄƒ pentru rezumare Ã®n few-shot settings. Tabelul de mai jos enumerÄƒ cÃ¢teva modele preantrenate populare care pot fi ajustate pentru sumarizare.

| Transformer model | Description                                                                                                                                                                                                    | Multilingual? |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | DeÈ™i este antrenat ca un model lingvistic autoregresiv, puteÈ›i face GPT-2 sÄƒ genereze rezumate prin adÄƒugarea "TL;DR" la sfÃ¢rÈ™itul textului de intrare.                                                                          |      âŒ       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | UtilizeazÄƒ un obiectiv de preantrenare pentru a prezice propoziÈ›ii mascate Ã®n texte cu mai multe propoziÈ›ii. Acest obiectiv de preantrenare este mai apropiat de rezumare decÃ¢t de vanilla language modeling È™i obÈ›ine scoruri ridicate la standardele populare. |      âŒ       |
|     [T5](https://huggingface.co/t5-base)      | O arhitecturÄƒ Transformer universalÄƒ care formuleazÄƒ toate sarcinile Ã®ntr-un framework text-text; de exemplu, formatul de intrare pentru modelul de rezumare a unui document este `summarize: ARTICOL`.                              |      âŒ       |
|     [mT5](https://huggingface.co/google/mt5-base)     | O versiune multilingvÄƒ a T5, preantrenatÄƒ pe corpusul multilingv Common Crawl (mC4), care acoperÄƒ 101 limbi.                                                                                                |      âœ…       |
|    [BART](https://huggingface.co/facebook/bart-base)     | O nouÄƒ arhitecturÄƒ Transformer cu un encoder È™i un stack de decodere antrenate pentru a reconstrui intrarea coruptÄƒ care combinÄƒ schemele de preantrenare ale BERT È™i GPT-2.                                   |      âŒ       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | O versiune multilingvÄƒ a BART, preantrenatÄƒ pe 50 de limbi.                                                                                                                                                     |      âœ…       |

DupÄƒ cum puteÈ›i vedea din acest tabel, majoritatea modelelor Transformer pentru rezumare (È™i, Ã®ntr-adevÄƒr, majoritatea sarcinilor NLP) sunt monolingve. Acest lucru este grozav dacÄƒ sarcina voastrÄƒeste Ã®ntr-o limbÄƒ cu  multe resurse precum engleza sau germana, dar mai puÈ›in pentru miile de alte limbi utilizate Ã®n Ã®ntreaga lume. Din fericire, existÄƒ o clasÄƒ de modele Transformer multilingve, precum mT5 È™i mBART, care vin Ã®n ajutor. Aceste modele sunt preantrenate folosind modelarea limbajului, dar cu o Ã®ntorsÄƒturÄƒ: Ã®n loc sÄƒ fie antrenate pe un corpus dintr-o singurÄƒ limbÄƒ, ele sunt antrenate Ã®mpreunÄƒ pe texte Ã®n peste 50 de limbi deodatÄƒ!

Ne vom concentra asupra mT5, o arhitecturÄƒ interesantÄƒ bazatÄƒ pe T5, care a fost preantrenatÄƒ Ã®ntr-un cadru text-to-text. Ãn T5, fiecare sarcinÄƒ NLP este formulatÄƒ Ã®n termenii unui prompt prefix precum `summarize:` care condiÈ›ioneazÄƒ modelul sÄƒ adapteze textul generat la prompt. DupÄƒ cum se aratÄƒ Ã®n figura de mai jos, acest lucru face T5 extrem de versatil, deoarece puteÈ›i rezolva multe sarcini cu un singur model!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Diferite sarcini Ã®ndeplinite de arhitectura T5."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Diferite sarcini Ã®ndeplinite de arhitectura T5."/>
</div>

mT5 nu utilizeazÄƒ prefixe, dar Ã®mpÄƒrtÄƒÈ™eÈ™te o mare parte din versatilitatea T5 È™i are avantajul de a fi multilingv. Acum cÄƒ am ales un model, sÄƒ aruncÄƒm o privire la pregÄƒtirea datelor noastre pentru antrenare.


> [!TIP]
> âœï¸ **ÃncercaÈ›i!** DupÄƒ ce aÈ›i parcurs aceastÄƒ secÈ›iune, vedeÈ›i cÃ¢t de bine se comparÄƒ mT5 cu mBART prin aplicarea fine-tuningului acestuia din urmÄƒ cu aceleaÈ™i tehnici. Pentru puncte bonus, puteÈ›i Ã®ncerca, de asemenea, fine-tuningul a T5 doar pe recenziile Ã®n limba englezÄƒ. Deoarece T5 are un prefix prompt special, va trebui sÄƒ adÄƒugaÈ›i `summarize:` la exemplele de intrare Ã®n paÈ™ii de preprocesare de mai jos.

## Preprocessing the data[[preprocessing-the-data]]

<Youtube id="1m7BerpSq8A"/>

UrmÄƒtoarea noastrÄƒ sarcinÄƒ este sÄƒ tokenizÄƒm È™i sÄƒ codificÄƒm recenziile È™i titlurile acestora. Ca de obicei, Ã®ncepem prin Ã®ncÄƒrcarea tokenizelui asociat cu checkpointul modelului preantrenat. Vom folosi `mt5-small` ca checkpoint, astfel Ã®ncÃ¢t sÄƒ putem face fine-tune modelului Ã®ntr-un timp rezonabil:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

> [!TIP]
> ğŸ’¡ Ãn stadiile iniÈ›iale ale proiectelor NLP, o bunÄƒ practicÄƒ este de a antrena o clasÄƒ de modele "mici" pe un sample mic de date. Acest lucru vÄƒ permite sÄƒ faceÈ›i debug È™i sÄƒ iteraÈ›i mai rapid cÄƒtre un flux de lucru end-to-end. OdatÄƒ ce sunteÈ›i Ã®ncrezÄƒtor Ã®n rezultate, puteÈ›i oricÃ¢nd sÄƒ mÄƒriÈ›i modelul prin simpla schimbare a checkpointului modelului!

SÄƒ testÄƒm tokenizerul mT5 pe un mic exemplu:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Aici putem vedea binecunoscutele `input_ids` È™i `attention_mask` pe care le-am Ã®ntÃ¢lnit Ã®n primele noastre experimente de fine-tuning Ã®n [Capitolul 3](/course/chapter3). SÄƒ decodificÄƒm aceste ID-uri de intrare cu funcÈ›ia `convert_ids_to_tokens()` a tokenizerului pentru a vedea cu ce fel de tokenizer avem de-a face:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']
```

Caracterul Unicode special `â–` È™i tokenul de sfÃ¢rÈ™it de secvenÈ›Äƒ `</s>` indicÄƒ faptul cÄƒ avem de-a face cu tokenizeerul SentencePiece, care se bazeazÄƒ pe algoritmul de segmentare Unigram discutat Ã®n [Capitolul 6](/course/chapter6). Unigram este deosebit de util pentru corpusurile multilingve, deoarece permite SentencePiece sÄƒ fie agnostic Ã®n ceea ce priveÈ™te accentele, punctuaÈ›ia È™i faptul cÄƒ multe limbi, precum japoneza, nu au caractere de spaÈ›iu alb.

Pentru a tokeniza corpusul nostru, trebuie sÄƒ ne ocupÄƒm de o subtilitate asociatÄƒ cu rezumarea: deoarece labelurile noastre sunt, de asemenea, text, este posibil ca acestea sÄƒ depÄƒÈ™eascÄƒ dimensiunea maximÄƒ a contextului modelului. Acest lucru Ã®nseamnÄƒ cÄƒ trebuie sÄƒ aplicÄƒm trunchierea atÃ¢t a recenziilor, cÃ¢t È™i a titlurilor acestora, pentru a ne asigura cÄƒ nu trecem inputuri excesiv de lungi modelului nostru. Tokenizerele din ğŸ¤— Transformers oferÄƒ un argument ingenios, `text_target`,  care vÄƒ permite sÄƒ tokenizaÈ›i labelurile Ã®n paralel cu inputurile. IatÄƒ un exemplu al modului Ã®n care inputurile È™i targeturile sunt procesate pentru mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

SÄƒ parcurgem acest cod pentru a Ã®nÈ›elege ce se Ã®ntÃ¢mplÄƒ. Primul lucru pe care l-am fÄƒcut a fost sÄƒ definim valorile pentru `max_input_length` È™i `max_target_length`, care stabilesc limitele superioare pentru cÃ¢t de lungi pot fi recenziile È™i titlurile noastre. Deoarece corpul recenziei este de obicei mult mai mare decÃ¢t titlul, am mÄƒrit aceste valori Ã®n consecinÈ›Äƒ.

Cu ajutorul funcÈ›iei `preprocess_function()`, este simplu sÄƒ tokenizÄƒm Ã®ntregul corpus cu ajutorul funcÈ›iei practice `Dataset.map()` pe care am folosit-o la greu pe parcursul acestui curs:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

Acum cÄƒ corpusul a fost preprocesat, sÄƒ aruncÄƒm o privire asupra unor metrici care sunt utilizate Ã®n mod obiÈ™nuit pentru sumarizare. DupÄƒ cum vom vedea, nu existÄƒ un glonÈ› de argint atunci cÃ¢nd vine vorba de mÄƒsurarea calitÄƒÈ›ii textului generat de calculator.

> [!TIP]
> ğŸ’¡ Poate aÈ›i observat cÄƒ am folosit `batched=True` Ã®n funcÈ›ia noastrÄƒ `Dataset.map()` de mai sus. Aceasta codificÄƒ exemplele Ã®n batchuri de 1.000 (implicit) È™i vÄƒ permite sÄƒ utilizaÈ›i capacitÄƒÈ›ile multithreading ale tokenizerilor rapizi din ğŸ¤— Transformers. Atunci cÃ¢nd este posibil, Ã®ncercaÈ›i sÄƒ utilizaÈ›i `batched=True` pentru a profita la maximum de preprocesare!


## Metrice pentru sumarizare[[metrics-for-text-summarization]]

<Youtube id="TMshhnrEXlg"/>

Ãn comparaÈ›ie cu majoritatea celorlalte sarcini pe care le-am abordat Ã®n acest curs, mÄƒsurarea performanÈ›ei sarcinilor de generare a textului, precum sumarizare sau traducerea, nu este la fel de simplÄƒ. De exemplu, avÃ¢nd Ã®n vedere o recenzie precum "Mi-a plÄƒcut sÄƒ citesc Hunger Games", existÄƒ mai multe rezumate valide, precum "Mi-a plÄƒcut Hunger Games" sau "Hunger Games este o lecturÄƒ excelentÄƒ". Ãn mod clar, aplicarea unui exact match Ã®ntre rezumatul generat È™i label nu este o soluÈ›ie bunÄƒ - chiar È™i oamenii s-ar descurca prost cu un astfel de metric, deoarece toÈ›i avem propriul nostru stil de scriere.

Pentru rezumare, una dintre cele mai frecvent utilizate metrici este [ROUGE score](https://en.wikipedia.org/wiki/ROUGE_(metric)) (prescurtarea de la Recall-Oriented Understudy for Gisting Evaluation). Ideea de bazÄƒ din spatele acestei metrici este de a compara un rezumat generat cu un set de rezumate de referinÈ›Äƒ care sunt de obicei create de oameni. Pentru a face acest lucru mai precis, sÄƒ presupunem cÄƒ dorim sÄƒ comparÄƒm urmÄƒtoarele douÄƒ rezumate:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

O modalitate de a le compara ar fi sÄƒ numÄƒraÈ›i numÄƒrul de cuvinte care se suprapun, care Ã®n acest caz ar fi 6. Cu toate acestea, acest lucru este un pic crud, astfel Ã®ncÃ¢t, Ã®n schimb, ROUGE se bazeazÄƒ pe calcularea scorurilor _preicision_ È™i _recall_ pentru suprapunere.

> [!TIP]
> ğŸ™‹ Nu vÄƒ faceÈ›i griji dacÄƒ aceasta este prima datÄƒ cÃ¢nd auziÈ›i de precision È™i recall - vom trece Ã®mpreunÄƒ prin cÃ¢teva exemple explicite pentru a clarifica totul. Aceste metrici sunt de obicei Ã®ntÃ¢lnite Ã®n sarcinile de clasificare, deci dacÄƒ doriÈ›i sÄƒ Ã®nÈ›elegeÈ›i cum sunt definite precizia È™i recallul Ã®n acest context, vÄƒ recomandÄƒm sÄƒ consultaÈ›i [ghidurile `scikit-learn`] (https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html).

Pentru ROUGE, recall mÄƒsoarÄƒ cÃ¢t de mult din rezumatul de referinÈ›Äƒ este capturat de cel generat. DacÄƒ comparÄƒm doar cuvinte, recall poate fi calculatÄƒ conform urmÄƒtoarei formule:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

Pentru exemplul nostru simplu de mai sus, aceastÄƒ formulÄƒ oferÄƒ un recall perfect de 6/6 = 1; adicÄƒ, toate cuvintele din rezumatul de referinÈ›Äƒ au fost produse de model. Acest lucru poate pÄƒrea grozav, dar imaginaÈ›i-vÄƒ dacÄƒ rezumatul nostru generat ar fi fost "Mi-a plÄƒcut foarte mult sÄƒ citesc Jocurile Foamei toatÄƒ noaptea". Aceasta ar avea, de asemenea, o reamintire perfectÄƒ, dar este, fÄƒrÄƒ Ã®ndoialÄƒ, un rezumat mai prost, deoarece are mai multe cuvinte. Pentru a face faÈ›Äƒ acestor scenarii, calculÄƒm È™i precizia, care, Ã®n contextul ROUGE, mÄƒsoarÄƒ cÃ¢t de mult din rezumatul generat a fost relevant:


$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

AplicÃ¢nd acest lucru la rezumatul nostru cu mai multe cuvinte, se obÈ›ine o precizie de 6/10 = 0,6, ceea ce este considerabil mai rÄƒu decÃ¢t precizia de 6/7 = 0,86 obÈ›inutÄƒ de rezumatul nostru mai scurt. Ãn practicÄƒ, se calculeazÄƒ de obicei atÃ¢t precision, cÃ¢t È™i recallul, iar apoi se raporteazÄƒ scorul F1 (media armonicÄƒ a precision È™i recall). Putem face acest lucru cu uÈ™urinÈ›Äƒ Ã®n ğŸ¤— Datasets instalÃ¢nd mai Ã®ntÃ¢i biblioteca `rouge_score`:

```py
!pip install rouge_score
```

È™i apoi Ã®ncÄƒrcÃ¢nd metrica ROUGE dupÄƒ cum urmeazÄƒ:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

Apoi, putem utiliza funcÈ›ia `rouge_score.compute()` pentru a calcula toate metricile odatÄƒ:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

Whoa, existÄƒ o mulÈ›ime de informaÈ›ii Ã®n aceast output - ce Ã®nseamnÄƒ toate acestea? Ãn primul rÃ¢nd, ğŸ¤— Datasets calculeazÄƒ de fapt confidence intervalurile pentru precision, recall È™i scorul F1; acestea sunt atributele `low`, `mid` È™i `high` pe care le puteÈ›i vedea aici. Ãn plus, ğŸ¤— Datasets calculeazÄƒ o varietate de scoruri ROUGE care se bazeazÄƒ pe diferite tipuri de granularitate a textului atunci cÃ¢nd comparÄƒ rezumatele generate È™i de referinÈ›Äƒ. Varianta `rouge1` este suprapunerea unigramelor - acesta este doar un mod elegant de a spune suprapunerea cuvintelor È™i este exact metrica pe care am discutat-o mai sus. Pentru a verifica acest lucru, sÄƒ extragem valoarea `mid` a scorurilor noastre:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

Grozav, numerele de precision È™i de recall se potrivesc! Acum ce se Ã®ntÃ¢mplÄƒ cu celelalte scoruri ROUGE? `rouge2` mÄƒsoarÄƒ suprapunerea dintre bigrame (suprapunerea perechilor de cuvinte), Ã®n timp ce `rougeL` È™i `rougeLsum` mÄƒsoarÄƒ cele mai lungi secvenÈ›e de cuvinte care se potrivesc, cÄƒutÃ¢nd cele mai lungi substraturi comune Ã®n rezumatele generate È™i de referinÈ›Äƒ. Termenul "sum" din `rougeLsum` se referÄƒ la faptul cÄƒ aceastÄƒ metricÄƒ este calculatÄƒ pentru un rezumat Ã®ntreg, Ã®n timp ce `rougeL` este calculatÄƒ ca medie a propoziÈ›iilor individuale.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** CreaÈ›i propriul exemplu de rezumat generat È™i de referinÈ›Äƒ È™i vedeÈ›i dacÄƒ scorurile ROUGE rezultate sunt Ã®n concordanÈ›Äƒ cu un calcul manual bazat pe formulele de precision È™i recall. Pentru puncte bonus, Ã®mpÄƒrÈ›iÈ›i textul Ã®n bigrame È™i comparaÈ›i precizia È™i recallul pentru metrica `rouge2`.

Vom folosi aceste scoruri ROUGE pentru a urmÄƒri performanÈ›a modelului nostru, dar Ã®nainte de a face acest lucru, sÄƒ facem ceva ce orice bun practician NLP ar trebui sÄƒ facÄƒ: sÄƒ creÄƒm un baseline puternic, dar simplu!

### Crearea unui baseline bun[[creating-a-strong-baseline]]

un baseline obiÈ™nuit pentru rezumarea textului este de a lua pur È™i simplu primele trei propoziÈ›ii ale unui articol, adesea numit _lead-3_ baseline. Am putea folosi puncte de oprire pentru a urmÄƒri limitele propoziÈ›iei, dar acest lucru va eÈ™ua Ã®n cazul acronimelor precum "U.S." sau "U.N." - aÈ™a cÄƒ vom folosi Ã®n schimb biblioteca `nltk`, care include un algoritm mai bun pentru a gestiona aceste cazuri. PuteÈ›i instala pachetul folosind `pip` dupÄƒ cum urmeazÄƒ:

```python
!pip install nltk
```

È™i apoi descÄƒrcaÈ›i regulile de punctuaÈ›ie:

```python
import nltk

nltk.download("punkt")
```

Ãn continuare, importÄƒm tokenizerul de propoziÈ›ii din `nltk` È™i creÄƒm o funcÈ›ie simplÄƒ pentru a extrage primele trei propoziÈ›ii dintr-o recenzie. ConvenÈ›ia Ã®n rezumarea textului este de a separa fiecare rezumat cu o linie nouÄƒ, deci sÄƒ includem È™i aceasta È™i sÄƒ o testÄƒm pe un exemplu de antrenare:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

Acest lucru pare sÄƒ funcÈ›ioneze, deci sÄƒ implementÄƒm acum o funcÈ›ie care extrage aceste "rezumate" dintr-un dataset È™i calculeazÄƒ scorurile ROUGE pentru baseline:

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

Putem folosi apoi aceastÄƒ funcÈ›ie pentru a calcula scorurile ROUGE pe setul de validare È™i pentru a le Ã®nfrumuseÈ›a puÈ›in folosind Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

Putem vedea cÄƒ scorul `rouge2` este semnificativ mai mic decÃ¢t restul; acest lucru reflectÄƒ probabil faptul cÄƒ titlurile recenziilor sunt de obicei concise È™i, prin urmare, baselineul lead-3 are prea multe cuvinte. Acum, cÄƒ avem un baseline bun de lucru, sÄƒ ne Ã®ndreptÄƒm atenÈ›ia cÄƒtre fine-tuningul mT5!

{#if fw === 'pt'}

## Fine-tuningul mT5 cu API-ul `Trainer`[[fine-tuning-mt5-with-the-trainer-api]]

Fine-tuningul unui model pentru rezumare este foarte asemÄƒnÄƒtor cu celelalte sarcini pe care le-am acoperit Ã®n acest capitol. Primul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ Ã®ncÄƒrcÄƒm modelul preantrenat din checkpointul `mt5-small`. Deoarece sumarizarea este o sarcinÄƒ de la secvenÈ›Äƒ la secvenÈ›Äƒ, putem Ã®ncÄƒrca modelul cu clasa `AutoModelForSeq2SeqLM`, care va descÄƒrca automat È™i va stoca Ã®n cache weighturile:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## Fine-tuningul mT5 cu Keras[[fine-tuning-mt5-with-keras]]

Fine-tuningul unui model pentru sumarizare este foarte asemÄƒnÄƒtoare cu celelalte sarcini pe care le-am acoperit Ã®n acest capitol. Primul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ Ã®ncÄƒrcÄƒm modelul preantrenat din punctul de control `mt5-small`. Deoarece rezumarea este o sarcinÄƒ de la secvenÈ›Äƒ la secvenÈ›Äƒ, putem Ã®ncÄƒrca modelul cu clasa `TFAutoModelForSeq2SeqLM`, care va descÄƒrca È™i va stoca Ã®n cache weighturile:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

> [!TIP]
> ğŸ’¡ DacÄƒ vÄƒ Ã®ntrebaÈ›i de ce nu vedeÈ›i niciun avertisment cu privire la fine-tuningul modelului pe un downstream task, acest lucru se datoreazÄƒ faptului cÄƒ pentru sarcinile secvenÈ›Äƒ-la-secvenÈ›Äƒ pÄƒstrÄƒm toate weighturile reÈ›elei. ComparaÈ›i acest lucru cu modelul nostru de clasificare a textului din [Capitolul 3](/course/chapter3), unde headul modelului preantrenat a fost Ã®nlocuit cu o reÈ›ea iniÈ›ializatÄƒ aleatoriu.

UrmÄƒtorul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ ne conectÄƒm la Hugging Face Hub. DacÄƒ executaÈ›i acest cod Ã®ntr-un notebook, puteÈ›i face acest lucru cu urmÄƒtoarea funcÈ›ie:

```python
from huggingface_hub import notebook_login

notebook_login()
```

care va afiÈ™a un widget Ã®n care puteÈ›i introduce credenÈ›ialele. Alternativ, puteÈ›i rula aceastÄƒ comandÄƒ Ã®n terminal È™i sÄƒ vÄƒ conectaÈ›i acolo:

```
huggingface-cli login
```

{#if fw === 'pt'}

Va trebui sÄƒ generÄƒm rezumate pentru a calcula scorurile ROUGE Ã®n timpul antrenÄƒrii. Din fericire, ğŸ¤— Transformers oferÄƒ clase dedicate `Seq2SeqTrainingArguments` È™i `Seq2SeqTrainer` care pot face acest lucru pentru noi Ã®n mod automat! Pentru a vedea cum funcÈ›ioneazÄƒ acest lucru, sÄƒ definim mai Ã®ntÃ¢i hiperparametrii È™i alte argumente pentru experimentele noastre:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

Aici, argumentul `predict_with_generate` a fost setat pentru a indica faptul cÄƒ ar trebui sÄƒ generÄƒm rezumate Ã®n timpul evaluÄƒrii, astfel Ã®ncÃ¢t sÄƒ putem calcula scorurile ROUGE pentru fiecare epocÄƒ. DupÄƒ cum s-a discutat Ã®n [Capitolul 1](/course/chapter1), decodificatorul realizeazÄƒ inference-ul prin prezicerea tokenilor unul cÃ¢te unul, iar acest lucru este implementat de metoda `generate()` a modelului. Setarea `predict_with_generate=True` Ã®i spune lui `Seq2SeqTrainer` sÄƒ utilizeze aceastÄƒ metodÄƒ pentru evaluare. Am ajustat, de asemenea, unii dintre hiperparametrii impliciÈ›i, cum ar fi rata de Ã®nvÄƒÈ›are, numÄƒrul de epoci È™i scÄƒderea weighturilor È™i am setat opÈ›iunea `save_total_limit` pentru a salva numai pÃ¢nÄƒ la 3 checkpointuri Ã®n timpul antrenamentului - acest lucru se datoreazÄƒ faptului cÄƒ chiar È™i versiunea "micÄƒ" a mT5 utilizeazÄƒ aproximativ 1 GB de spaÈ›iu pe hard disk È™i putem economisi puÈ›in spaÈ›iu prin limitarea numÄƒrului de copii salvate.

Argumentul `push_to_hub=True` ne va permite sÄƒ trimitem modelul Ã®n Hub dupÄƒ antrenare; veÈ›i gÄƒsi repositoriul Ã®n profilul vostru de utilizator, Ã®n locaÈ›ia definitÄƒ de `output_dir`. ReÈ›ineÈ›i cÄƒ puteÈ›i specifica numele repositoriului cÄƒtre care doriÈ›i sÄƒ trimiteÈ›i modelul cu argumentul `hub_model_id` (Ã®n special, va trebui sÄƒ utilizaÈ›i acest argument pentru a trimite modelul cÄƒtre o organizaÈ›ie). De exemplu, atunci cÃ¢nd am trimis modelul cÄƒtre organizaÈ›ia [`huggingface-course`](https://huggingface.co/huggingface-course), am adÄƒugat `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` la `Seq2SeqTrainingArguments`.

UrmÄƒtorul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ oferim trainerului o funcÈ›ie `compute_metrics()`, astfel Ã®ncÃ¢t sÄƒ ne putem evalua modelul Ã®n timpul antrenÄƒrii. Pentru sumarizare, acest lucru este un pic mai complicat decÃ¢t simpla apelare a funcÈ›iei `rouge_score.compute()` pentru predicÈ›iile modelului, deoarece trebuie sÄƒ _decodÄƒm_ rezultatele È™i labelurile din text Ã®nainte de a putea calcula scorurile ROUGE. UrmÄƒtoarea funcÈ›ie face exact acest lucru È™i, de asemenea, utilizeazÄƒ funcÈ›ia `sent_tokenize()` din `nltk` pentru a separa propoziÈ›iile rezumate cu linii noi:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

Ãn continuare, trebuie sÄƒ definim un data collator pentru sarcina noastrÄƒ secvenÈ›Äƒ-la-secvenÈ›Äƒ. Deoarece mT5 este un transformer model encoder-decoder, o subtilitate a pregÄƒtirii batch-urilor noastre este cÄƒ, Ã®n timpul decodificÄƒrii, trebuie sÄƒ deplasÄƒm labelurile la dreapta cu una. Acest lucru este necesar pentru a ne asigura cÄƒ decodificatorul vede doar labelurile anterioare ale adevÄƒrului de bazÄƒ È™i nu pe cele actuale sau viitoare, care ar fi uÈ™or de memorat de cÄƒtre model. Acest lucru este similar cu modul Ã®n care masked self-attention este aplicatÄƒ inputurilor Ã®ntr-o sarcinÄƒ precum [causal language modeling](/course/chapter7/6).

Din fericire, ğŸ¤— Transformers oferÄƒ un collator `DataCollatorForSeq2Seq` care va face padding dinamic inputurilor È™i labelurilor pentru noi. Pentru a iniÈ›ializa acest collator, trebuie doar sÄƒ furnizÄƒm `tokenizer` È™i `model`:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

SÄƒ vedem ce produce acest collator atunci cÃ¢nd este alimentat cu un mic batch de exemple. Ãn primul rÃ¢nd, trebuie sÄƒ eliminÄƒm coloanele cu È™iruri de caractere, deoarece collatorul nu va È™ti cum sÄƒ completeze aceste elemente:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

Deoarece collatorul aÈ™teaptÄƒ o listÄƒ de `dict`, unde fiecare `dict` reprezintÄƒ un singur exemplu din dataset, trebuie sÄƒ transformÄƒm datele Ã®n formatul aÈ™teptat Ã®nainte de a le transmite data collatorului:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

Principalul lucru care trebuie observat aici este cÄƒ primul exemplu este mai lung decÃ¢t al doilea, astfel Ã®ncÃ¢t `input_ids` È™i `attention_mask` din al doilea exemplu au primit padding Ã®n dreapta cu un simbol `[PAD]` (al cÄƒrui ID este `0`). Ãn mod similar, putem vedea cÄƒ `labels` au primit padding cu `-100`, pentru a ne asigura cÄƒ tokenii de padding sunt ignoraÈ›i de funcÈ›ia de pierdere. Ãn sfÃ¢rÈ™it, putem vedea un nou `decoder_input_ids` care a deplasat labelurile spre dreapta prin inserarea unui simbol `[PAD]` Ã®n prima intrare.

{#if fw === 'pt'}

Avem Ã®n sfÃ¢rÈ™it toate ingredientele de care avem nevoie pentru a antrenare! Acum trebuie doar sÄƒ iniÈ›ializÄƒm trainerul cu argumentele standard:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

È™i sÄƒ lansÄƒm cursa noastrÄƒ de antrenare:

```python
trainer.train()
```

Ãn timpul antrenamentului, ar trebui sÄƒ vedeÈ›i cum training loss scade È™i scorurile ROUGE cresc cu fiecare epocÄƒ. DupÄƒ ce antrenamentul este complet, puteÈ›i vedea scorurile ROUGE finale executÃ¢nd `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

Din scorurile obÈ›inute, putem vedea cÄƒ modelul nostru a depÄƒÈ™it cu mult modelul nostru de bazÄƒ lead-3 - frumos! Ultimul lucru de fÄƒcut este sÄƒ introducem weighturile modelului Ã®n Hub, dupÄƒ cum urmeazÄƒ:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

Acest lucru va salva checkpointul È™i fiÈ™ierele de configurare Ã®n `output_dir`, Ã®nainte de a Ã®ncÄƒrca toate fiÈ™ierele Ã®n Hub. SpecificÃ¢nd argumentul `tags`, ne asigurÄƒm, de asemenea, cÄƒ widgetul de pe Hub va fi unul pentru un pipeline de rezumare Ã®n locul celui implicit de generare de text asociat arhitecturii mT5 (pentru mai multe informaÈ›ii despre labelurile modelului, consultaÈ›i [ğŸ¤—DocumentaÈ›ia Hub](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). Rezultatul din `trainer.push_to_hub()` este o adresÄƒ URL cÄƒtre hash-ul Git commit, astfel Ã®ncÃ¢t sÄƒ puteÈ›i vedea cu uÈ™urinÈ›Äƒ modificÄƒrile care au fost fÄƒcute Ã®n repositoriul modelului!

Pentru a Ã®ncheia aceastÄƒ secÈ›iune, sÄƒ aruncÄƒm o privire la modul Ã®n care putem, de asemenea, sÄƒ facem fine-tune la mT5 folosind featururile de nivel scÄƒzut oferite de ğŸ¤— Accelerate.

{:else}

Suntem aproape gata de antrenament! Trebuie doar sÄƒ convertim dataseturile Ã®n `tf.data.Dataset`s folosind data collatorul pe care l-am definit mai sus, iar apoi aplicarea `compil()` È™i `fit()` modelul. Mai Ã®ntÃ¢i, dataseturile:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

Acum, ne definim hiperparametrii de antrenare È™i compilare:

```python
from transformers import create_optimizer
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Ãn cele din urmÄƒ, facem fit modelului modelului. Folosim un `PushToHubCallback` pentru a salva modelul Ã®n Hub dupÄƒ fiecare epocÄƒ, ceea ce ne va permite sÄƒ Ã®l folosim ulterior pentru inference:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

Am obÈ›inut unele valori ale pierderilor Ã®n timpul antrenÄƒrii, dar am dori sÄƒ vedem metricile ROUGE pe care le-am calculat mai devreme. Pentru a obÈ›ine aceste metrici, va trebui sÄƒ generÄƒm outputuri din model È™i sÄƒ le convertim Ã®n È™iruri de caractere. SÄƒ construim cÃ¢teva liste de labels È™i predicÈ›ii pentru a compara metrica ROUGE (reÈ›ineÈ›i cÄƒ, dacÄƒ obÈ›ineÈ›i erori de import pentru aceastÄƒ secÈ›iune, este posibil sÄƒ fie necesar sÄƒ faceÈ›i `!pip install tqdm`). De asemenea, vom utiliza un truc care creÈ™te dramatic performanÈ›a - compilarea codului nostru de generare cu [XLA](https://www.tensorflow.org/xla), compilatorul accelerat de algebrÄƒ liniarÄƒ al TensorFlow. XLA aplicÄƒ diverse optimizÄƒri graficului de calcul al modelului È™i are ca rezultat Ã®mbunÄƒtÄƒÈ›iri semnificative ale vitezei È™i utilizÄƒrii memoriei. DupÄƒ cum se descrie Ã®n [blogul Hugging Face](https://huggingface.co/blog/tf-xla-generate), XLA funcÈ›ioneazÄƒ cel mai bine atunci cÃ¢nd formele noastre de input nu variazÄƒ prea mult. Pentru a face faÈ›Äƒ acestui lucru, vom face padding inputurilor la multipli a 128 È™i vom crea un nou dataset cu padding collatorul, iar apoi vom aplica decoratorul `@tf.function(jit_compile=True)` funcÈ›iei noastre de generare, care marcheazÄƒ Ã®ntreaga funcÈ›ie pentru compilare cu XLA.

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

OdatÄƒ ce avem listele noastre de labeluri È™i de È™iruri de predicÈ›ie, calcularea scorului ROUGE este uÈ™oarÄƒ:

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## Fine-tuningul mT5 cu ğŸ¤— Accelerate[[fine-tuning-mt5-with-accelerate]]

Fine-tuningul nostru cu ğŸ¤— Accelerate este foarte asemÄƒnÄƒtor cu exemplul de clasificare a textului pe care l-am Ã®ntÃ¢lnit Ã®n [Capitolul 3](/course/chapter3). Principalele diferenÈ›e vor fi necesitatea de a genera Ã®n mod explicit rezumatele noastre Ã®n timpul antrenÄƒrii È™i de a defini modul Ã®n care calculÄƒm scorurile ROUGE (reamintim cÄƒ `Seq2SeqTrainer` a avut grijÄƒ de generare pentru noi). SÄƒ aruncÄƒm o privire la modul Ã®n care putem implementa aceste douÄƒ cerinÈ›e Ã®n cadrul ğŸ¤— Accelerate!

### PregÄƒtirea pentru antrenare[[preparing-everything-for-training]]

Primul lucru pe care trebuie sÄƒ-l facem este sÄƒ creÄƒm un `DataLoader` pentru fiecare dintre spliturile noastre. Deoarece dataloaders PyTorch aÈ™teaptÄƒ batchuri de tensori, trebuie sÄƒ setÄƒm formatul la `"torch"` Ã®n dataseturile noastre:

```python
tokenized_datasets.set_format("torch")
```

Acum cÄƒ avem dataseturi formate doar din tensori, urmÄƒtorul lucru este iniÈ›ializarea `DataCollatorForSeq2Seq`. Pentru aceasta trebuie sÄƒ furnizÄƒm o versiune nouÄƒ a modelului, aÈ™a cÄƒ hai sÄƒ Ã®l Ã®ncÄƒrcÄƒm din nou din cache:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

Putem apoi sÄƒ instanÈ›iem data collatorul È™i sÄƒ Ã®l folosim pentru a ne defini dataloaders:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

UrmÄƒtorul lucru este definirea optimizatorului pe care dorim sÄƒ Ã®l utilizÄƒm. Ca È™i Ã®n celelalte exemple, vom folosi `AdamW`, care funcÈ›ioneazÄƒ bine pentru majoritatea problemelor:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Ãn cele din urmÄƒ, introducem modelul, optimizatorul È™i dataloaders Ã®n metoda `accelerator.prepare()`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

> [!TIP]
> ğŸš¨ DacÄƒ faceÈ›i antrenarea pe un TPU, va trebui sÄƒ mutaÈ›i tot codul de mai sus Ã®ntr-o funcÈ›ie de antrenare aparte. ConsultaÈ›i [Capitolul 3](/course/chapter3) pentru mai multe detalii.

Acum cÄƒ ne-am pregÄƒtit obiectele, mai avem trei lucruri de fÄƒcut:

* Definirea learning rate schedule.
* Implementarea unei funcÈ›ii de post-procesare a rezumatelor pentru evaluare.
* Crearea unui repositoriu pe Hub Ã®n care sÄƒ putem trimite modelul nostru.

Pentru learning rate schedule, Ã®l vom utiliza pe cel liniar standard din secÈ›iunile anterioare:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Pentru post-procesare, avem nevoie de o funcÈ›ie care sÄƒ Ã®mpartÄƒ rezumatele generate Ã®n propoziÈ›ii separate prin linii noi. Acesta este formatul pe care Ã®l aÈ™teaptÄƒ metrica ROUGE, iar noi putem realiza acest lucru cu urmÄƒtorul fragment de cod:

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

Acest lucru ar trebui sÄƒ vÄƒ parÄƒ familiar dacÄƒ vÄƒ amintiÈ›i cum am definit funcÈ›ia `compute_metrics()` a `Seq2SeqTrainer`.

Ãn cele din urmÄƒ, trebuie sÄƒ creÄƒm un repositoriu de modele pe Hugging Face Hub. Pentru aceasta, putem utiliza biblioteca ğŸ¤— Hub intitulatÄƒ corespunzÄƒtor . Trebuie doar sÄƒ definim un nume pentru repositoriul nostru, iar biblioteca are o funcÈ›ie utilitarÄƒ pentru a combina ID-ul repositoriul cu profilul utilizatorului:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

Acum putem folosi numele repositoriului pentru a clona o versiune localÄƒ Ã®n folderul nostru cu rezultate care va stoca artefactele de antrenare:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Acest lucru ne va permite sÄƒ trimitem artefactele Ã®napoi la Hub prin apelarea metodei `repo.push_to_hub()` Ã®n timpul antrenÄƒrii! SÄƒ Ã®ncheiem acum analiza noastrÄƒ prin scrierea buclei de antrenare.

### Bucla de antrenare[[training-loop]]

Bucla de formare pentru sumarizare este destul de asemÄƒnÄƒtoare cu celelalte exemple ğŸ¤— Accelerate pe care le-am Ã®ntÃ¢lnit È™i este Ã®mpÄƒrÈ›itÄƒ aproximativ Ã®n patru etape principale:

1. Antrenarea modelului prin iterarea peste toate exemplele din `train_dataloader` pentru fiecare epocÄƒ.
2. Generarea rezumatelor modelului la sfÃ¢rÈ™itul fiecÄƒrei epoci, mai Ã®ntÃ¢i prin generarea token-urilor È™i apoi prin decodarea lor (È™i a rezumatelor de referinÈ›Äƒ) Ã®n text.
3. Calcularea scorurilor ROUGE folosind aceleaÈ™i tehnici pe care le-am vÄƒzut mai devreme.
4. SalvaÈ›i checkpointurile È™i Ã®ncÄƒrcaÈ›i totul pe Hub. Aici ne bazÄƒm pe argumentul `blocking=False` al obiectului `Repository` astfel Ã®ncÃ¢t sÄƒ putem Ã®mpinge checkointurile pentru fiecare epocÄƒ _asincron_. Acest lucru ne permite sÄƒ continuÄƒm antrenamentul fÄƒrÄƒ a fi nevoiÈ›i sÄƒ aÈ™teptÄƒm Ã®ncÄƒrcarea oarecum lentÄƒ asociatÄƒ cu un model de dimensiunea unui GB!

AceÈ™ti paÈ™i pot fi observaÈ›i Ã®n urmÄƒtorul bloc de cod:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

È˜i asta e tot! DupÄƒ ce executaÈ›i acest lucru, veÈ›i avea un model È™i rezultate care sunt destul de asemÄƒnÄƒtoare cu cele obÈ›inute cu `Trainer`.

{/if}

## Utilizarea modelului fine-tuned[[using-your-fine-tuned-model]]

OdatÄƒ ce aÈ›i Ã®ncÄƒrcat modelul Ã®n Hub, vÄƒ puteÈ›i juca cu el prin widgetul de inference, fie cu un obiect `pipeline`, dupÄƒ cum urmeazÄƒ:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

Putem introduce cÃ¢teva exemple din setul de testare (pe care modelul nu le-a vÄƒzut) Ã®n pipelineul noastru pentru a avea o idee despre calitatea rezumatelor. Mai Ã®ntÃ¢i, sÄƒ implementÄƒm o funcÈ›ie simplÄƒ pentru a afiÈ™a Ã®mpreunÄƒ recenzia, titlul È™i rezumatul generat:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

SÄƒ aruncÄƒm o privire la unul dintre exemplele englezeÈ™ti pe care le primim:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnâ€™t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itâ€™s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

This is not too bad! We can see that our model has actually been able to perform _abstractive_ summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

Rezumatul se traduce prin "Very easy to read" Ã®n limba englezÄƒ, ceea ce putem vedea cÄƒ Ã®n acest caz a fost extras direct din recenzie. Cu toate acestea, acest lucru aratÄƒ versatilitatea modelului mT5 È™i v-a dat o idee despre cum este sÄƒ aveÈ›i de-a face cu un corpus multilingv!

Ãn continuare, ne vom Ã®ndrepta atenÈ›ia cÄƒtre o sarcinÄƒ puÈ›in mai complexÄƒ: antrenarea unui model lingvistic de la zero.