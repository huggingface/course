<FrameworkSwitchCourse {fw} />

# Traducere[[translation]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
]} />

{/if}

SÄƒ trecem acum la traducere. Aceasta este o altÄƒ [sarcinÄƒ de sequence-to-sequence](/course/chapter1/7), ceea ce Ã®nseamnÄƒ cÄƒ este o problemÄƒ care poate fi formulatÄƒ ca trecerea de la o secvenÈ›Äƒ la alta. Ãn acest sens, problema este destul de apropiatÄƒ de [sumarizare](/course/chapter7/6) È™i aÈ›i putea adapta ceea ce vom vedea aici la alte probleme de la sequence-to-sequence, cum ar fi:

- **Style transfer**: Crearea unui model care *traduce* texte scrise Ã®ntr-un anumit stil Ã®n altul (de exemplu, din formal Ã®n casual sau din engleza ShakespearianÄƒ Ã®n engleza modernÄƒ)
- **Generative question answering**: Crearea unui model care genereazÄƒ rÄƒspunsuri la Ã®ntrebÄƒri, fiind dat un context

<Youtube id="1JvfrvZgi6c"/>

DacÄƒ aveÈ›i un corpus suficient de mare de texte Ã®n douÄƒ (sau mai multe) limbi, puteÈ›i antrena un nou model de traducere de la zero, aÈ™a cum vom face Ã®n secÈ›iunea despre [causal language modeling] (/course/chapter7/6). Cu toate acestea, va fi mai rapid sÄƒ faceÈ›i fine-tune unui model de traducere existent, fie cÄƒ este vorba de un model multilingv precum mT5 sau mBART, pe care doriÈ›i sÄƒ Ã®i faceÈ›i fine-tune pentru o anumitÄƒ pereche de limbi, sau chiar un model specializat pentru traducerea dintr-o limbÄƒ Ã®n alta, pe care doriÈ›i sÄƒ Ã®l perfecÈ›ionaÈ›i pentru corpusul vostru specific.

Ãn aceastÄƒ secÈ›iune, vom pune face fine-tune unui model Marian preantrenat pentru a traduce din englezÄƒ Ã®n francezÄƒ (deoarece mulÈ›i dintre angajaÈ›ii Hugging Face vorbesc ambele limbi) pe [datasetul KDE4](https://huggingface.co/datasets/kde4), care este un set de fiÈ™iere localizate pentru [KDE apps](https://apps.kde.org/). Modelul pe care Ã®l vom utiliza a fost preantrenat pe un corpus mare de texte Ã®n francezÄƒ È™i englezÄƒ preluate din [datasetul Opus](https://opus.nlpl.eu/), care conÈ›ine de fapt datasetul KDE4. Dar chiar dacÄƒ modelul preantrenat pe care Ã®l folosim a vÄƒzut aceste date Ã®n timpul preantrenÄƒrii sale, vom vedea cÄƒ putem obÈ›ine o versiune mai bunÄƒ a acestuia dupÄƒ fine-tuning.

OdatÄƒ ce am terminat, vom avea un model capabil sÄƒ facÄƒ predicÈ›ii ca acesta:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels pentru rÄƒspunderea la Ã®ntrebÄƒri."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels rÄƒspunderea la Ã®ntrebÄƒri."/>
</a>

Ca È™i Ã®n secÈ›iunile anterioare, puteÈ›i gÄƒsi modelul pe care Ã®l vom antrena È™i Ã®ncÄƒrca Ã®n Hub utilizÃ¢nd codul de mai jos È™i puteÈ›i verifica predicÈ›iile acestuia [aici](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## PregÄƒtirea datelor[[preparing-the-data]]

Pentru a face fine-tune sau a antrena un model de traducere de la zero, vom avea nevoie de un dataset adecvat pentru aceastÄƒ sarcinÄƒ. DupÄƒ cum am menÈ›ionat anterior, vom utiliza [datasetul KDE4](https://huggingface.co/datasets/kde4) Ã®n aceastÄƒ secÈ›iune, dar puteÈ›i adapta codul pentru a utiliza propriile date destul de uÈ™or, atÃ¢ta timp cÃ¢t aveÈ›i perechi de propoziÈ›ii Ã®n cele douÄƒ limbi din È™i Ã®n care doriÈ›i sÄƒ traduceÈ›i. ConsultaÈ›i [Capitolul 5](/course/chapter5) dacÄƒ aveÈ›i nevoie de o reamintire a modului de Ã®ncÄƒrcare a datelor personalizate Ã®ntr-un `Dataset`.

### Datasetul KDE4[[the-kde4-dataset]]

Ca de obicei, descÄƒrcÄƒm datasetul nostru folosind funcÈ›ia `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

DacÄƒ doriÈ›i sÄƒ lucraÈ›i cu o pereche de limbi diferitÄƒ, le puteÈ›i specifica prin codurile lor. Un total de 92 de limbi sunt disponibile pentru acest dataset; le puteÈ›i vedea pe toate prin extinderea labelurilor de limbÄƒ pe [dataset cardul acesteia](https://huggingface.co/datasets/kde4).

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Limba disponibilÄƒ pentru datasetul KDE4." width="100%">

SÄƒ aruncÄƒm o privire la dataset:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

Avem 210 173 de perechi de propoziÈ›ii, dar Ã®ntr-o singurÄƒ Ã®mpÄƒrÈ›ire, deci va trebui sÄƒ ne creÄƒm propriul set de validare. AÈ™a cum am vÄƒzut Ã®n [Capitolul 5](/course/chapter5), un `Dataset` are o metodÄƒ `train_test_split()` care ne poate ajuta. Vom furniza un seed pentru reproductibilitate:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

Putem redenumi cheia `"test"` Ã®n `"validation"` astfel:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

Acum sÄƒ aruncÄƒm o privire la un element al datasetului:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

ObÈ›inem un dicÈ›ionar cu douÄƒ propoziÈ›ii Ã®n perechea de limbi solicitate. O particularitate a acestui dataset plin de termeni tehnici din domeniul informaticii este cÄƒ toate sunt traduse integral Ã®n francezÄƒ. Cu toate acestea, inginerii francezi lasÄƒ majoritatea cuvintelor specifice informaticii Ã®n englezÄƒ atunci cÃ¢nd vorbesc. Aici, de exemplu, cuvÃ¢ntul "threads" ar putea foarte bine sÄƒ aparÄƒ Ã®ntr-o propoziÈ›ie Ã®n limba francezÄƒ, Ã®n special Ã®ntr-o conversaÈ›ie tehnicÄƒ; dar Ã®n acest dataset a fost tradus Ã®n "fils de discussion". Modelul preantrenat pe care Ã®l folosim, care a fost preantrenat pe un corpus mai mare de propoziÈ›ii Ã®n francezÄƒ È™i englezÄƒ, alege opÈ›iunea mai uÈ™oarÄƒ de a lÄƒsa cuvÃ¢ntul aÈ™a cum este:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

Un alt exemplu al acestui comportament poate fi vÄƒzut cu cuvÃ¢ntul "plugin", care nu este oficial un cuvÃ¢nt francez, dar pe care majoritatea vorbitorilor nativi Ã®l vor Ã®nÈ›elege È™i nu se vor obosi sÄƒ Ã®l traducÄƒ.
Ãn datasetul KDE4, acest cuvÃ¢nt a fost tradus Ã®n francezÄƒ Ã®n termenul puÈ›in mai oficial "module d'extension":

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

Modelul nostru preantrenat, cu toate acestea, rÄƒmÃ¢ne la cuvÃ¢ntul englez compact È™i familiar:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

Va fi interesant de vÄƒzut dacÄƒ modelul nostru fine-tuned reÈ›ine aceste particularitÄƒÈ›i ale datasetului(spoiler alert: va reÈ›ine).

<Youtube id="0Oxphw4Q9fo"/>

> [!TIP]
> âœï¸ **RÃ¢ndul tÄƒu!** Un alt cuvÃ¢nt englezesc care este adesea folosit Ã®n francezÄƒ este "email". GÄƒsiÈ›i primul sample din datasetul de antrenare care utilizeazÄƒ acest cuvÃ¢nt. Cum este tradus? Cum traduce modelul preantrenat aceeaÈ™i propoziÈ›ie Ã®n limba englezÄƒ?

### Procesarea datelor[[processing-the-data]]

<Youtube id="XAR8jnZZuUs"/>

Ar trebui sÄƒ È™tiÈ›i deja cum stÄƒ treaba: toate textele trebuie convertite Ã®n seturi de ID-uri token, astfel Ã®ncÃ¢t modelul sÄƒ le poatÄƒ Ã®nÈ›elege. Pentru aceastÄƒ sarcinÄƒ, va trebui sÄƒ tokenizÄƒm atÃ¢t inputurile, cÃ¢t È™i targeturile. Prima noastrÄƒ sarcinÄƒ este sÄƒ creÄƒm obiectul `tokenizer`. DupÄƒ cum am menÈ›ionat mai devreme, vom utiliza un model Marian preantrenat din englezÄƒ Ã®n francezÄƒ. DacÄƒ Ã®ncercaÈ›i acest cod cu o altÄƒ pereche de limbi, asiguraÈ›i-vÄƒ cÄƒ adaptaÈ›i checkpointul modelului. OrganizaÈ›ia [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) oferÄƒ mai mult de o mie de modele Ã®n mai multe limbi.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

De asemenea, puteÈ›i Ã®nlocui `model_checkpoint` cu orice alt model preferat din [Hub](https://huggingface.co/models) sau cu un folder local Ã®n care aÈ›i salvat un model preantrenat È™i un tokenizer.

> [!TIP]
> ğŸ’¡ DacÄƒ utilizaÈ›i un tokenizer multilingv, cum ar fi mBART, mBART-50 sau M2M100, va trebui sÄƒ setaÈ›i codurile de limbÄƒ ale inputurilor È™i targeturilor Ã®n tokenizer prin setarea `tokenizer.src_lang` È™i `tokenizer.tgt_lang` la valorile corecte.

PregÄƒtirea datelor noastre este destul de simplÄƒ. Trebuie sÄƒ reÈ›ineÈ›i un singur lucru; trebuie sÄƒ vÄƒ asiguraÈ›i cÄƒ tokenizerul proceseazÄƒ targeturile Ã®n limba de ieÈ™ire (aici, francezÄƒ). PuteÈ›i face acest lucru trecÃ¢nd targeturile la argumentul `text_targets` al metodei `__call__` a tokenizerului.

Pentru a vedea cum funcÈ›ioneazÄƒ acest lucru, sÄƒ procesÄƒm un sample din fiecare limbÄƒ din setul de antrenare:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

DupÄƒ cum putem vedea, rezultatul conÈ›ine ID-urile de input asociate cu propoziÈ›ia Ã®n limba englezÄƒ, Ã®n timp ce ID-urile asociate cu cea Ã®n limba francezÄƒ sunt stocate Ã®n cÃ¢mpul `labels`. DacÄƒ uitaÈ›i sÄƒ indicaÈ›i cÄƒ tokenizaÈ›i labelurile, acestea vor fi tokenizate de cÄƒtre tokenizerul de input, ceea ce Ã®n cazul unui model Marian nu va merge deloc bine:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

DupÄƒ cum se poate observa, utilizarea tokenizerului englez pentru preprocesarea unei propoziÈ›ii franceze are ca rezultat mult mai mulÈ›i tokeni, deoarece tokenizerul nu cunoaÈ™te niciun cuvÃ¢nt francez (cu excepÈ›ia celor care apar È™i Ã®n limba englezÄƒ, precum "discussion").

Deoarece `inputs` este un dicÈ›ionar cu cheile noastre obiÈ™nuite (ID-uri de input, attention mask etc.), ultimul pas este sÄƒ definim funcÈ›ia de preprocesare pe care o vom aplica dataseturilor:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

ReÈ›ineÈ›i cÄƒ am stabilit aceeaÈ™i lungime maximÄƒ pentru inputurile È™i outputurile noastre. Deoarece textele cu care avem de-a face par destul de scurte, vom folosi 128.

> [!TIP]
> ğŸ’¡ DacÄƒ utilizaÈ›i un model T5 (mai precis, unul dintre checkpointurile `t5-xxx`), modelul se va aÈ™tepta ca inputurile text sÄƒ aibÄƒ un prefix care sÄƒ indice sarcina Ã®n cauzÄƒ, cum ar fi `translate: din englezÄƒ Ã®n francezÄƒ:`.

> [!WARNING]
> âš ï¸ Nu acordÄƒm atenÈ›ie attention maskului a targeturilor, deoarece modelul nu se va aÈ™tepta la aceasta. Ãn schimb, labelurile corespunzÄƒtoare unui padding token trebuie setate la `-100`, astfel Ã®ncÃ¢t acestea sÄƒ fie ignorate Ã®n calculul pierderilor. Acest lucru va fi fÄƒcut mai tÃ¢rziu de data collatorul nostru, deoarece aplicÄƒm padding dinamic, dar dacÄƒ utilizaÈ›i padding aici, ar trebui sÄƒ adaptaÈ›i funcÈ›ia de preprocesare pentru a seta toate labelurile care corespund simbolului de padding la `-100`.

Acum putem aplica aceastÄƒ preprocesare dintr-o singurÄƒ datÄƒ pe toate diviziunile datasetului nostru:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

Acum cÄƒ datele au fost preprocesate, suntem pregÄƒtiÈ›i sÄƒ facem fine-tune modelul nostru preantrenat!

{#if fw === 'pt'}

## Fine-tuningul modelului cu API-ul `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

Codul real care utilizeazÄƒ `Trainer` va fi acelaÈ™i ca Ã®nainte, cu o singurÄƒ micÄƒ schimbare: folosim aici un [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer), care este o subclasÄƒ a `Trainer` care ne va permite sÄƒ ne ocupÄƒm Ã®n mod corespunzÄƒtor de evaluare, folosind metoda `generate()` pentru a prezice rezultatele din inputuri. Vom analiza acest aspect mai Ã®n detaliu atunci cÃ¢nd vom vorbi despre calculul metricelor.

Ãn primul rÃ¢nd, avem nevoie de un model pe care sÄƒ Ã®l aplicÄƒm fine-tuningul. Vom utiliza API-ul obiÈ™nuit `AutoModel`:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## Fine-tuningul modelului cu Keras[[fine-tuning-the-model-with-keras]]

Ãn primul rÃ¢nd, avem nevoie de un model pe care sÄƒ Ã®l aplicÄƒm fine-tuningul. Vom folosi API-ul obiÈ™nuit `AutoModel`:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

ğŸ’¡ Checkpointul `Helsinki-NLP/opus-mt-en-fr` are doar weighturi PyTorch, deci
veÈ›i primi o eroare dacÄƒ Ã®ncercaÈ›i sÄƒ Ã®ncÄƒrcaÈ›i modelul fÄƒrÄƒ a utiliza argumentul
`from_pt=True` Ã®n metoda `from_pretrained()`. Atunci cÃ¢nd specificaÈ›i
`from_pt=True`, biblioteca va descÄƒrca È™i va converti automat
weighturile PyTorch pentru voi. DupÄƒ cum puteÈ›i vedea, este foarte simplu sÄƒ vÄƒ schimbaÈ›i Ã®ntre
frameworkuri Ã®n ğŸ¤— Transformers!

</Tip>

{/if}

ReÈ›ineÈ›i cÄƒ de data aceasta folosim un model care a fost antrenat pe o sarcinÄƒ de traducere È™i care poate fi utilizat deja, astfel Ã®ncÃ¢t nu existÄƒ niciun avertisment cu privire la weighturile lipsÄƒ sau la cele nou iniÈ›ializate.

### Data collation[[data-collation]]

Vom avea nevoie de un data collator care sÄƒ se ocupe de paddingul pentru batching-ul dinamic. Nu putem folosi un `DataCollatorWithPadding` ca Ã®n [Capitolul 3](/course/chapter3) Ã®n acest caz, pentru cÄƒ acesta doar face padding inputurilor (input Is, attention mask È™i token type IDs). Labelurile noastre ar trebui, de asemenea, sÄƒ fie padded la lungimea maximÄƒ Ã®ntÃ¢lnitÄƒ Ã®n labeluri. È˜i, aÈ™a cum am menÈ›ionat anterior, valoarea de padding utilizatÄƒ pentru a face padding labelurilor ar trebui sÄƒ fie `-100` È™i nu padding tokenul tokenizerului, pentru a ne asigura cÄƒ aceste valori de padding sunt ignorate Ã®n calculul pierderilor.

Toate acestea sunt realizate de un [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). Ca È™i `DataCollatorWithPadding`, acesta preia `tokenizer` utilizat pentru preprocesarea inputurilor, dar preia È™i `modelul`. Acest lucru se datoreazÄƒ faptului cÄƒ acest data collator va fi, de asemenea, responsabil de pregÄƒtirea ID-urilor de input ale decoderului, care sunt versiuni schimbate ale labelurilor cu un token special la Ã®nceput. Deoarece aceastÄƒ schimbare se face uÈ™or diferit pentru diferite arhitecturi, `DataCollatorForSeq2Seq` trebuie sÄƒ cunoascÄƒ obiectul `model`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

Pentru a testa acest lucru pe cÃ¢teva sampleuri, Ã®l apelÄƒm doar pe o listÄƒ de sampleuri din setul nostru de antrenare tokenizat:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

Putem verifica dacÄƒ labelurile noastre au fost padded la lungimea maximÄƒ a batchului, folosind `-100`:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

De asemenea, putem arunca o privire la ID-urile de input ale decoderului, pentru a vedea cÄƒ acestea sunt versiuni schimbate ale labelurilor:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

IatÄƒ labelurile pentru primul È™i al doilea element din datasetul nostru:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

Vom transmite acest `data_collator` cÄƒtre `Seq2SeqTrainer`. Ãn continuare, sÄƒ aruncÄƒm o privire la metrice.

{:else}

Acum putem folosi acest `data_collator` pentru a converti fiecare dintre dataseturile Ã®ntr-un `tf.data.Dataset`, gata pentru antrenare:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### Metrice[[metrics]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

Caracteristica pe care `Seq2SeqTrainer` o adaugÄƒ superclasei sale `Trainer` este capacitatea de a utiliza metoda `generate()` Ã®n timpul evaluÄƒrii sau predicÈ›iei. Ãn timpul antrenÄƒrii, modelul va utiliza `decoder_input_ids` cu un attention mask care asigurÄƒ cÄƒ nu utilizeazÄƒ tokenii de dupÄƒ tokenul pe care Ã®ncearcÄƒ sÄƒ Ã®l prezicÄƒ, pentru a accelera antrenarea. Ãn timpul inferenÈ›ei, nu le vom putea utiliza deoarece nu vom avea labeluri, deci este o idee bunÄƒ sÄƒ ne evaluÄƒm modelul cu aceeaÈ™i configuraÈ›ie.

DupÄƒ cum am vÄƒzut Ã®n [Capitolul 1](/course/chapter1/6), decoderul realizeazÄƒ inferenÈ›a prin prezicerea tokenilor unul cÃ¢te unul - lucru care este implementat behind the scenes Ã®n ğŸ¤— Transformers prin metoda `generate()`. `Seq2SeqTrainer` ne va permite sÄƒ folosim aceastÄƒ metodÄƒ pentru evaluare dacÄƒ setÄƒm `predict_with_generate=True`.

{/if}

Metricele tradiÈ›ionale utilizate pentru traducere sunt [scorul BLEU](https://en.wikipedia.org/wiki/BLEU), introdus Ã®n [un articol din 2002](https://aclanthology.org/P02-1040.pdf) de Kishore Papineni et al. Scorul BLEU evalueazÄƒ cÃ¢t de apropiate sunt traducerile de labelurile lor. Acesta nu mÄƒsoarÄƒ inteligibilitatea sau corectitudinea gramaticalÄƒ a rezultatelor generate de model, ci utilizeazÄƒ reguli statistice pentru a se asigura cÄƒ toate cuvintele din rezultatele generate apar È™i Ã®n targets. Ãn plus, existÄƒ reguli care penalizeazÄƒ repetiÈ›iile aceloraÈ™i cuvinte dacÄƒ acestea nu sunt repetate È™i Ã®n targets(pentru a evita ca modelul sÄƒ producÄƒ propoziÈ›ii de tipul `"the the the the the the"`) È™i sÄƒ producÄƒ propoziÈ›ii care sunt mai scurte decÃ¢t cele din targets(pentru a evita ca modelul sÄƒ producÄƒ propoziÈ›ii de tipul `"the"`).

Un punct slab al BLEU este cÄƒ se aÈ™teaptÄƒ ca textul sÄƒ fie deja tokenizat, ceea ce face dificilÄƒ compararea scorurilor Ã®ntre modele care utilizeazÄƒ tokenizere diferite. Ãn schimb, cea mai frecvent utilizatÄƒ mÄƒsurÄƒ pentru evaluarea comparativÄƒ a modelelor de traducere este [SacreBLEU](https://github.com/mjpost/sacrebleu), care abordeazÄƒ acest punct slab (È™i altele) prin standardizarea etapei de tokenizare. Pentru a utiliza aceastÄƒ metricÄƒ, trebuie mai Ã®ntÃ¢i sÄƒ instalÄƒm biblioteca SacreBLEU:

```py
!pip install sacrebleu
```

Ãl putem Ã®ncÄƒrca apoi prin `evaluate.load()` aÈ™a cum am fÄƒcut Ã®n [Capitolul 3](/course/chapter3):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

AceastÄƒ metricÄƒ va lua texte ca inputuri È™i targeturi. Este conceput pentru a lua mai multe obiective acceptabile, deoarece existÄƒ adesea mai multe traduceri acceptabile ale aceleiaÈ™i propoziÈ›ii - datasetul pe care Ã®l folosim oferÄƒ doar una, dar nu este neobiÈ™nuit Ã®n NLP sÄƒ gÄƒsim dataseturi care oferÄƒ mai multe propoziÈ›ii ca labeluri. Deci, predicÈ›iile ar trebui sÄƒ fie o listÄƒ de propoziÈ›ii, dar referinÈ›ele ar trebui sÄƒ fie o listÄƒ de liste de propoziÈ›ii.

Hai sÄƒ Ã®ncercÄƒm acest exemplu:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

Se obÈ›ine un scor BLEU de 46,75, ceea ce este destul de bine - ca referinÈ›Äƒ, modelul original Transformer din lucrarea ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) a obÈ›inut un scor BLEU de 41,8 la o sarcinÄƒ similarÄƒ de traducere Ã®ntre englezÄƒ È™i francezÄƒ! (Pentru mai multe informaÈ›ii despre parametrii individuali, precum `counts` È™i `bp`, consultaÈ›i [repositoriul SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) Pe de altÄƒ parte, dacÄƒ Ã®ncercÄƒm cu cele douÄƒ tipuri de predicÈ›ii proaste (multe repetÄƒri sau prea scurte) care rezultÄƒ adesea din modelele de traducere, vom obÈ›ine scoruri BLEU destul de proaste:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

Scorul poate varia de la 0 la 100, iar mai mare Ã®nseamnÄƒ un scor mai bun.

{#if fw === 'tf'}

Pentru a trece de la rezultatele modelului la texte pe care metricele le pot utiliza, vom utiliza metoda `tokenizer.batch_decode()`. Trebuie doar sÄƒ curÄƒÈ›Äƒm toate `-100` din labeluri; tokenizerul va face automat acelaÈ™i lucru pentru padding token. SÄƒ definim o funcÈ›ie care sÄƒ preia modelul nostru È™i un dataset È™i sÄƒ calculeze metrici pe acesta. De asemenea, vom utiliza un truc care creÈ™te dramatic performanÈ›a - compilarea codului nostru de generare cu [XLA](https://www.tensorflow.org/xla), compilatorul accelerat de algebrÄƒ liniarÄƒ al TensorFlow. XLA aplicÄƒ diverse optimizÄƒri graficului de calcul al modelului È™i are ca rezultat Ã®mbunÄƒtÄƒÈ›iri semnificative ale vitezei È™i utilizÄƒrii memoriei. DupÄƒ cum se descrie Ã®n [blogul](https://huggingface.co/blog/tf-xla-generate) Hugging Face, XLA funcÈ›ioneazÄƒ cel mai bine atunci cÃ¢nd shaperulire noastre de input nu variazÄƒ prea mult. Pentru a face faÈ›Äƒ acestui lucru, vom aplica padding inputurilor cu multipli ai 128 È™i vom crea un nou dataset cu padding collatorul, iar apoi vom aplica decoratorul `@tf.function(jit_compile=True)` funcÈ›iei noastre de generare, care marcheazÄƒ Ã®ntreaga funcÈ›ie pentru compilare cu XLA.

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []

    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

Pentru a trece de la rezultatele modelului la textele pe care metricele le pot utiliza, vom utiliza metoda `tokenizer.batch_decode()`. Trebuie doar sÄƒ curÄƒÈ›Äƒm toate `-100` din labeluri(tokenizerul va face automat acelaÈ™i lucru pentru padding token):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # In case the model returns more than the prediction logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100s in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

Acum cÄƒ am fÄƒcut acest lucru, suntem gata sÄƒ facem fine-tune modelului!


### Fine-tuningul modelului[[fine-tuning-the-model]]

Primul pas pe care trebuie sÄƒ Ã®l faceÈ›i este sÄƒ vÄƒ conectaÈ›i la Hugging Face, astfel Ã®ncÃ¢t sÄƒ vÄƒ puteÈ›i Ã®ncÄƒrca rezultatele Ã®n Model Hub. ExistÄƒ o funcÈ›ie convenabilÄƒ care vÄƒ ajuta cu acest lucru Ã®ntr-un notebook:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Aceasta va afiÈ™a un widget Ã®n care puteÈ›i introduce datele voastre de autentificare Hugging Face.

DacÄƒ nu lucraÈ›i Ã®ntr-un notebook, tastaÈ›i urmÄƒtoarea linie Ã®n terminal:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

Ãnainte de a Ã®ncepe, sÄƒ vedem ce fel de rezultate obÈ›inem de la modelul nostru fÄƒrÄƒ niciun antrenament:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

OdatÄƒ fÄƒcut acest lucru, putem pregÄƒti tot ce avem nevoie pentru a compila È™i antrena modelul nostru. ObservaÈ›i utilizarea `tf.keras.mixed_precision.set_global_policy("mixed_float16")` -- aceasta Ã®i va spune lui Keras sÄƒ se antreneze folosind float16, ceea ce poate oferi o creÈ™tere semnificativÄƒ a vitezei pe GPU-urile care o acceptÄƒ (Nvidia 20xx/V100 sau mai noi).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# NumÄƒrul etapelor de antrenare este numÄƒrul de sampleuri din dataset, Ã®mpÄƒrÈ›it la dimensiunea batchului, apoi Ã®nmulÈ›it
# cu numÄƒrul total de epoci. ReÈ›ineÈ›i cÄƒ datasetul tf_train_dataset de aici este un dataset tf.data.Dataset Ã®n batchuri,
# nu datasetul original Hugging Face, deci len() este deja num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Antrenarea Ã®n float16 cu precizie mixtÄƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Ãn continuare, definim un `PushToHubCallback` pentru a Ã®ncÄƒrca modelul nostru Ã®n Hub Ã®n timpul antrenamentului, aÈ™a cum am vÄƒzut Ã®n [secÈ›iunea 2]((/course/chapter7/2)), iar apoi pur È™i simplu facem fit modelului cu acel callback:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

ReÈ›ineÈ›i cÄƒ puteÈ›i specifica numele repositoriului cÄƒtre care doriÈ›i sÄƒ faceÈ›i push cu argumentul `hub_model_id` (Ã®n special, va trebui sÄƒ utilizaÈ›i acest argument pentru a face push cÄƒtre o organizaÈ›ie). De exemplu, atunci cÃ¢nd am trimis modelul cÄƒtre organizaÈ›ia [`huggingface-course`](https://huggingface.co/huggingface-course), am adÄƒugat `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` la `Seq2SeqTrainingArguments`. Ãn mod implicit, repositoriul utilizat va fi Ã®n namespaceul vostru È™i denumit dupÄƒ output directory-ul pe care l-aÈ›i stabilit, deci aici va fi `"sgugger/marian-finetuned-kde4-en-to-fr"` (care este modelul la care am fÄƒcut legÄƒtura la Ã®nceputul acestei secÈ›iuni).

> [!TIP]
> ğŸ’¡ DacÄƒ output directory-ul pe care Ã®l utilizaÈ›i existÄƒ deja, acesta trebuie sÄƒ fie o clonÄƒ localÄƒ a repositoriului cÄƒtre care doriÈ›i sÄƒ faceÈ›i push. DacÄƒ nu este, veÈ›i primi o eroare atunci cÃ¢nd apelaÈ›i `model.fit()` È™i va trebui sÄƒ setaÈ›i un nume nou.

Ãn cele din urmÄƒ, hai sÄƒ vedem cum aratÄƒ metricele noastre acum cÄƒ antrenarea s-a Ã®ncheiat:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

Ãn acest moment, puteÈ›i utiliza widgetul de inferenÈ›Äƒ de pe Model Hub pentru a testa modelul È™i pentru a-l partaja cu prietenii voÈ™trii. AÈ›i fÄƒcut fine-tune cu succes unui model pentru o sarcinÄƒ de traducere - felicitÄƒri!

{:else}

OdatÄƒ fÄƒcut acest lucru, putem defini `Seq2SeqTrainingArguments`. Ca È™i pentru `Trainer`, folosim o subclasÄƒ a `TrainingArguments` care conÈ›ine cÃ¢teva cÃ¢mpuri suplimentare:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

Ãn afarÄƒ de hiperparametrii obiÈ™nuiÈ›i (cum ar fi rata de Ã®nvÄƒÈ›are, numÄƒrul de epoci, dimensiunea batch-ului È™i o anumitÄƒ scÄƒdere a weighturilor), aici sunt cÃ¢teva schimbÄƒri Ã®n comparaÈ›ie cu ceea ce am vÄƒzut Ã®n secÈ›iunile anterioare:

- Nu setÄƒm nicio evaluare periodicÄƒ, deoarece evaluarea dureazÄƒ; ne vom evalua modelul doar o datÄƒ Ã®nainte È™i dupÄƒ antrenare.
- SetÄƒm `fp16=True`, care accelereazÄƒ antrenarea pe GPU-urile moderne.
- SetÄƒm `predict_with_generate=True`, aÈ™a cum am discutat mai sus.
- UtilizÄƒm `push_to_hub=True` pentru a Ã®ncÄƒrca modelul Ã®n Hub la sfÃ¢rÈ™itul fiecÄƒrei epoci.

ReÈ›ineÈ›i cÄƒ puteÈ›i specifica numele complet al repositoriului cÄƒtre care doriÈ›i sÄƒ faceÈ›i push cu argumentul `hub_model_id` (Ã®n special, va trebui sÄƒ utilizaÈ›i acest argument pentru a face push cÄƒtre o organizaÈ›ie). De exemplu, atunci cÃ¢nd am trimis modelul cÄƒtre organizaÈ›ia [`huggingface-course`](https://huggingface.co/huggingface-course), am adÄƒugat `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` la `Seq2SeqTrainingArguments`. Ãn mod implicit, repositoriul utilizat va fi Ã®n namespaceul vostru È™i denumit dupÄƒ output directory-ul pe care l-aÈ›i stabilit, deci Ã®n cazul nostru va fi `"sgugger/marian-finetuned-kde4-en-to-fr"` (care este modelul la care am fÄƒcut legÄƒtura la Ã®nceputul acestei secÈ›iuni).

> [!TIP]
> ğŸ’¡ DacÄƒ output directory-ul pe care Ã®l utilizaÈ›i existÄƒ deja, acesta trebuie sÄƒ fie o clonÄƒ localÄƒ a repositoriului cÄƒtre care doriÈ›i sÄƒ faceÈ›i push. Ãn caz contrar, veÈ›i primi o eroare atunci cÃ¢nd vÄƒ definiÈ›i `Seq2SeqTrainer` È™i va trebui sÄƒ stabiliÈ›i un nume nou.

Ãn final, transmitem totul cÄƒtre `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Ãnainte de antrenare, ne vom uita mai Ã®ntÃ¢i la scorul obÈ›inut de modelul nostru, pentru a verifica dacÄƒ nu Ã®nrÄƒutÄƒÈ›im lucrurile prin fine-tuning. AceastÄƒ comandÄƒ va dura ceva timp, aÈ™a cÄƒ puteÈ›i lua o cafea sau douÄƒ Ã®n timp ce se executÄƒ:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

Un scor BLEU de 39 nu este prea rÄƒu, ceea ce reflectÄƒ faptul cÄƒ modelul nostru este deja bun la traducerea propoziÈ›iilor din englezÄƒ Ã®n francezÄƒ.

UrmeazÄƒ antrenarea, care va necesita, de asemenea, puÈ›in timp:

```python
trainer.train()
```

ReÈ›ineÈ›i cÄƒ, Ã®n timpul antrenamentului, de fiecare datÄƒ cÃ¢nd modelul este salvat (aici, la fiecare epocÄƒ), acesta este Ã®ncÄƒrcat Ã®n Hub Ã®n fundal. Ãn acest fel, veÈ›i putea sÄƒ reluaÈ›i antrenarea pe o altÄƒ maÈ™inÄƒ, dacÄƒ este necesar.

OdatÄƒ ce formarea este terminatÄƒ, evaluÄƒm din nou modelul nostru - sperÄƒm cÄƒ vom vedea o Ã®mbunÄƒtÄƒÈ›ire a scorului BLEU!

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

Aceasta este o Ã®mbunÄƒtÄƒÈ›ire de aproape 14 puncte, ceea ce este minunat.

Ãn final, folosim metoda `push_to_hub()` pentru a ne asigura cÄƒ Ã®ncÄƒrcÄƒm cea mai recentÄƒ versiune a modelului. De asemenea, `Trainer` redacteazÄƒ un model card cu toate rezultatele evaluÄƒrii È™i o Ã®ncarcÄƒ. Acest model card conÈ›ine metadate care ajutÄƒ Model Hub sÄƒ aleagÄƒ widgetul pentru demonstraÈ›ia de inferenÈ›Äƒ. De obicei, nu este nevoie sÄƒ se menÈ›ioneze nimic, deoarece poate deduce widgetul potrivit din clasa modelului, dar Ã®n acest caz, aceeaÈ™i clasÄƒ de model poate fi utilizatÄƒ pentru toate tipurile de probleme de tip sequence-to-sequence, aÈ™a cÄƒ specificÄƒm cÄƒ este un model de traducere:

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

AceastÄƒ comandÄƒ returneazÄƒ URL-ul comitului pe care tocmai l-am fÄƒcut, dacÄƒ doriÈ›i sÄƒ Ã®l inspectaÈ›i:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

Ãn aceastÄƒ etapÄƒ, puteÈ›i utiliza widget-ul de inferenÈ›Äƒ de pe Model Hub pentru a vÄƒ testa modelul È™i pentru a-l partaja cu prietenii. AÈ›i fÄƒcut fine-tune cu succes un model pentru o sarcinÄƒ de traducere - felicitÄƒri!

DacÄƒ doriÈ›i sÄƒ pÄƒtrundeÈ›i puÈ›in mai adÃ¢nc Ã®n bucla de antrenare, vÄƒ vom arÄƒta acum sÄƒ faceÈ›i acelaÈ™i lucru folosind ğŸ¤— Accelerate.

{/if}

{#if fw === 'pt'}

## O buclÄƒ de antrenare personalizatÄƒ[[a-custom-training-loop]]

SÄƒ aruncÄƒm acum o privire la bucla de antrenare completÄƒ, astfel Ã®ncÃ¢t sÄƒ puteÈ›i personaliza cu uÈ™urinÈ›Äƒ pÄƒrÈ›ile de care aveÈ›i nevoie. Va arÄƒta foarte asemÄƒnÄƒtor cu ceea ce am fÄƒcut Ã®n [secÈ›iunea 2](/course/chapter7/2) È™i [capitolul 3](/course/chapter7/4).

### PregÄƒtirea tuturor lucrulilor pentru antrenare[[preparing-everything-for-training]]

AÈ›i vÄƒzut toate acestea de cÃ¢teva ori pÃ¢nÄƒ acum, aÈ™a cÄƒ vom trece prin cod destul de repede. Mai Ã®ntÃ¢i vom construi `DataLoader`s din dataseturile noastre, dupÄƒ ce vom seta dataseturile Ã®n formatul `"torch"` astfel Ã®ncÃ¢t sÄƒ obÈ›inem tensori PyTorch:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

Ãn continuare, reiniÈ›ializÄƒm modelul, pentru a ne asigura cÄƒ nu continuÄƒm fine-tuningul de dinainte, ci pornim din nou de la modelul preantrenat:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

Pe urmÄƒ vom avea nevoie de un optimizator:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

OdatÄƒ ce avem toate aceste obiecte, le putem trimite la metoda `accelerator.prepare()`. AmintiÈ›i-vÄƒ cÄƒ, dacÄƒ doriÈ›i sÄƒ vÄƒ antrenaÈ›i pe un TPU Ã®ntr-un notebook Colab, va trebui sÄƒ mutaÈ›i tot acest cod Ã®ntr-o funcÈ›ie de antrenare, care nu ar trebui sÄƒ execute nicio celulÄƒ care iniÈ›ializeazÄƒ un `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

Acum cÄƒ am trimis `train_dataloader` la `accelerator.prepare()`, putem utiliza lungimea acestuia pentru a calcula numÄƒrul de paÈ™i de antrenare. AmintiÈ›i-vÄƒ cÄƒ trebuie sÄƒ facem acest lucru Ã®ntotdeauna dupÄƒ pregÄƒtirea data loaderului, deoarece metoda respectivÄƒ va modifica lungimea `DataLoader`. UtilizÄƒm un program liniar clasic de la rata de Ã®nvÄƒÈ›are la 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Ãn cele din urmÄƒ, pentru a trimite modelul nostru cÄƒtre Hub, va trebui sÄƒ creÄƒm un obiect `Repository` Ã®ntr-un folder de lucru. Ãn primul rÃ¢nd, conectaÈ›i-vÄƒ la Hugging Face Hub, dacÄƒ nu sunteÈ›i deja conectat. Vom determina numele repositoriului pornind de la ID-ul modelului pe care dorim sÄƒ Ã®l atribuim modelului nostru (nu ezitaÈ›i sÄƒ Ã®nlocuiÈ›i `repo_name` cu propria alegere; acesta trebuie doar sÄƒ conÈ›inÄƒ numele vostru de utilizator, ceea ce face funcÈ›ia `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

Apoi putem clona acel repositoriu Ã®ntr-un folder local. DacÄƒ existÄƒ deja, acest folder local ar trebui sÄƒ fie o clonÄƒ a repositoriului cu care lucrÄƒm:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Acum putem Ã®ncÄƒrca orice salvÄƒm Ã®n `output_dir` prin apelarea metodei `repo.push_to_hub()`. Acest lucru ne va ajuta sÄƒ Ã®ncÄƒrcÄƒm modelele intermediare la sfÃ¢rÈ™itul fiecÄƒrei epoci.

### Bucla de antrenare[[training-loop]]

Acum suntem pregÄƒtiÈ›i sÄƒ scriem bucla de antrenare completÄƒ. Pentru a simplifica partea de evaluare, definim aceastÄƒ funcÈ›ie `postprocess()` care preia predicÈ›iile È™i labelurile È™i le converteÈ™te Ã®n liste de stringuri pe care obiectul nostru `metric` le va aÈ™tepta:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

Bucla de antrenare seamÄƒnÄƒ foarte mult cu cele din [secÈ›iunea 2](/course/chapter7/2) È™i [capitolul 3](/course/chapter3), cu cÃ¢teva diferenÈ›e Ã®n partea de evaluare - aÈ™a cÄƒ hai sÄƒ ne concentrÄƒm pe asta!

Primul lucru pe care Ã®l putem remarca este cÄƒ folosim metoda `generate()` pentru a calcula predicÈ›iile, dar aceasta este o metodÄƒ a modelul nostru de bazÄƒ, nu pe modelul wrapped ğŸ¤— Accelerate creat Ã®n metoda `prepare()`. De aceea, mai Ã®ntÃ¢i facem unwrap modelului, apoi apelÄƒm aceastÄƒ metodÄƒ.

Al doilea lucru este cÄƒ, la fel ca Ã®n cazul [token classification](/course/chapter7/2), este posibil ca douÄƒ procese sÄƒ fi fÄƒcut padding inputurilor È™i labelurilor cu forme diferite, aÈ™a cÄƒ folosim `accelerator.pad_across_processes()` pentru a face predicÈ›iile È™i labelurile de aceeaÈ™i formÄƒ Ã®nainte de a apela metoda `gather()`. DacÄƒ nu facem acest lucru, evaluarea va da o eroare sau se va bloca pentru totdeauna.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

OdatÄƒ fÄƒcut acest lucru, ar trebui sÄƒ aveÈ›i un model care are rezultate destul de asemÄƒnÄƒtoare cu cel antrenat cu `Seq2SeqTrainer`. Ãl puteÈ›i verifica pe cel pe care l-am antrenat folosind acest cod la [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*] (https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). È˜i dacÄƒ doriÈ›i sÄƒ testaÈ›i orice modificÄƒri ale buclei de antrenare, le puteÈ›i implementa direct prin editarea codului prezentat mai sus!

{/if}

## Utilizarea modelului fine-tuned[[using-the-fine-tuned-model]]

V-am arÄƒtat deja cum puteÈ›i utiliza modelul pe cÄƒruia i-am aplicat fine-tune pe Model Hub cu widgetul de inferenÈ›Äƒ. Pentru a-l utiliza la nivel local Ã®ntr-un `pipeline`, trebuie doar sÄƒ specificÄƒm identificatorul de model corespunzÄƒtor:

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

AÈ™a cum era de aÈ™teptat, modelul nostru preantrenat È™i-a adaptat cunoÈ™tinÈ›ele la corpusul pe care i-am fÄƒcut fine-tune È™i, Ã®n loc sÄƒ lase cuvÃ¢ntul englezesc "threads", acum Ã®l traduce Ã®n versiunea oficialÄƒ francezÄƒ. AcelaÈ™i lucru este valabil È™i pentru "plugin":

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

Un alt exemplu excelent de adaptare a domeniului!

> [!TIP]
> âœï¸ **E rÃ¢ndul tÄƒu!** Care este rezultatul modelului pentru sampleul cuvÃ¢ntul "email" pe care l-ai identificat mai devreme?
