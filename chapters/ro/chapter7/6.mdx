<FrameworkSwitchCourse {fw} />

# Antrenarea de la zero a unui model de limbaj cauzal[[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

P칙n캒 acum, am folosit 칥n principal modele preantrenate 탳i le-am f캒cut fine-tuning pentru noi cazuri de utilizare prin reutilizarea weighturilor din preantrenare. Dup캒 cum am v캒zut 칥n [Capitolul 1](/course/chapter1), acest lucru este denumit 칥n mod obi탳nuit _칥nv캒탵are prin transfer_ 탳i este o strategie foarte reu탳it캒 pentru aplicarea modelelor Transformer la majoritatea cazurilor de utilizare din lumea real캒 칥n care datele etichetate sunt pu탵ine. 칉n acest capitol, vom adopta o abordare diferit캒 탳i vom antrena un model complet nou de la zero. Aceasta este o abordare bun캒 dac캒 ave탵i multe date 탳i este foarte diferit캒 de datele de preantrenare utilizate pentru modelele disponibile. Cu toate acestea, preantrenarea unui model lingvistic necesit캒, de asemenea, mult mai multe resurse de calcul dec칙t fine-tuningul unui model existent. Printre exemplele 칥n care poate fi util캒 antrenarea unui nou model se num캒r캒 dataseturile formate din note muzicale, secven탵e moleculare precum ADN sau limbaje de programare. Acestea din urm캒 au c칙탳tigat recent teren datorit캒 unor instrumente precum TabNine 탳i Copilot de la GitHub, alimentate de modelul Codex al OpenAI, care pot genera secven탵e lungi de cod. Aceast캒 sarcin캒 de generare a textului este cel mai bine abordat캒 cu modele de limbaj autoregresive sau cauzale, cum ar fi GPT-2.

칉n aceast캒 sec탵iune vom construi o versiune la scar캒 redus캒 a unui model de generare a codului: ne vom concentra pe complet캒ri de o linie 칥n loc de func탵ii sau clase complete, folosind un subset de cod Python. Atunci c칙nd lucra탵i cu date 칥n Python, sunte탵i 칥n contact frecvent Python data science stack, format캒 din bibliotecile `matplotlib`, `seaborn`, `pandas` 탳i `scikit-learn`. Atunci c칙nd se utilizeaz캒 aceste cadre, este frecvent s캒 fie nevoie s캒 se caute comenzi specifice, astfel 칥nc칙t ar fi bine dac캒 am putea utiliza un model care s캒 efectueze aceste apeluri pentru noi.

<Youtube id="Vpjb1lu0MDk"/>

칉n [Capitolul 6](/course/chapter6) am creat un tokenizer eficient pentru a procesa codul surs캒 Python, dar avem nevoie de un dataset la scar캒 larg캒 pe care s캒 preantren캒m un model. Aici, vom aplica tokenizerul nostru la un corpus de cod Python derivat din repositoriile GitHub. Vom utiliza apoi API-ul `Trainer` 탳i 游뱅 Accelerate pentru a antrena modelul. S캒 trecem la treab캒!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Aceasta este de fapt o prezentare a modelului care a fost antrenat 탳i 칥nc캒rcat 칥n Hub folosind codul prezentat 칥n aceast캒 sec탵iune. 칉l pute탵i g캒si [aici](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). Re탵ine탵i c캒, deoarece are loc o anumit캒 randomizare 칥n generarea textului, ve탵i ob탵ine probabil un rezultat u탳or diferit.
 
## Colectarea datelor[[gathering-the-data]]

Codul Python este disponibil din abunden탵캒 칥n repositorii de cod, cum ar fi GitHub, pe care le putem utiliza pentru a crea un dataset prin scraping pentru fiecare repositoriu Python. Aceasta a fost abordarea adoptat캒 칥n [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) pentru a preantrena un model GPT-2. Folosind o desc캒rcare GitHub de aproximativ 180 GB care con탵ine aproximativ 20 de milioane de fi탳iere Python numit캒 `codeparrot`, autorii au construit un dataset pe care l-au oferit apoi pe [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot).

Cu toate acestea, antrenarea pe 칥ntregul corpus consum캒 timp 탳i puterea calculatorului, iar noi avem nevoie doar de subsetul datasetului referitor la Python data science stack. A탳adar, s캒 칥ncepem prin filtrarea datasetului `codeparrot` pentru toate fi탳ierele care includ oricare dintre bibliotecile din aceast stack. Din cauza dimensiunii datasetului, dorim s캒 evit캒m desc캒rcarea acestuia; 칥n schimb, vom utiliza func탵ia de streaming pentru a-l filtra din mers. Pentru a ne ajuta s캒 filtr캒m exemplele de cod care utilizeaz캒 bibliotecile pe care le-am men탵ionat mai devreme, vom utiliza urm캒toarea func탵ie:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

S캒-l test캒m pe dou캒 exemple:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

Putem folosi acest lucru pentru a crea o func탵ie care va transmite 칥n flux datasetul 탳i va filtra elementele dorite:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

Apoi, putem aplica pur 탳i simplu aceast캒 func탵ie pe datasetului din flux:

```py
# This cell will take a very long time to execute, so you should skip it and go to
# the next one!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

Acest lucru ne las캒 cu aproximativ 3% din setul de date original, care este 칥nc캒 destul de mare - datasetul rezultat este de 6 GB 탳i const캒 din 600.000 de scripturi Python!

Filtrarea datasetului complet poate dura 2-3 ore, 칥n func탵ie de calculator 탳i de bandwidth. Dac캒 nu dori탵i s캒 parcurge탵i singur acest proces 칥ndelungat, v캒 punem la dispozi탵ie datasetul filtrat pe Hub pentru a-l desc캒rca:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

> [!TIP]
> Preantrenarea modelului de limbaj va dura ceva timp. V캒 suger캒m s캒 rula탵i mai 칥nt칙i bucla de antrenare pe un sample de date prin decomentarea celor dou캒 linii par탵iale de mai sus 탳i s캒 v캒 asigura탵i c캒 antrenarea se finalizeaz캒 cu succes 탳i c캒 modelele sunt stocate. Nimic nu este mai frustrant dec칙t o rulare de antrenare care e탳ueaz캒 la ultimul pas pentru c캒 a탵i uitat s캒 crea탵i un folder sau pentru c캒 exist캒 o gre탳eal캒 de tipar la sf칙r탳itul buclei de antrenare!

S캒 ne uit캒m la un exemplu din dataset. Vom ar캒ta doar primele 200 de caractere din fiecare c칙mp:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

Putem vedea c캒ci c칙mpul `content` con탵ine codul pe care dorim ca modelul nostru s캒 se antreneze. Acum c캒 avem un dataset, trebuie s캒 preg캒tim textele astfel 칥nc칙t acestea s캒 fie 칥ntr-un format adecvat pentru preantrenare.

## Preg캒tirea datasetului[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

Primul pas va fi tokenizarea datelor, astfel 칥nc칙t s캒 le putem utiliza pentru antrenare. Deoarece obiectivul nostru este de a autocompleta 칥n principal apeluri scurte de func탵ii, putem p캒stra dimensiunea contextului relativ mic캒. Acest lucru are avantajul c캒 putem antrena modelul mult mai rapid 탳i c캒 necesit캒 semnificativ mai pu탵in캒 memorie. Dac캒 este important pentru aplica탵ia voastr캒 s캒 ave탵i mai mult context (de exemplu, dac캒 dori탵i ca modelul s캒 scrie teste unitare pe baza unui fi탳ier cu defini탵ia func탵iei), asigura탵i-v캒 c캒 m캒ri탵i acest num캒r, dar re탵ine탵i, de asemenea, c캒 acest lucru vine cu utilizare mai mare de memorie GPU. Pentru moment, s캒 fix캒m dimensiunea contextului la 128 de tokeni, spre deosebire de 1 024 sau 2 048 utilizate 칥n GPT-2 sau respectiv GPT-3.

Majoritatea documentelor con탵in mult mai mult de 128 de cuvinte, astfel 칥nc칙t trunchierea simpl캒 a inputurilor la lungimea maxim캒 ar elimina o mare parte din datasetul nostru. 칉n schimb, vom utiliza op탵iunea `return_overflowing_tokens` pentru a tokeniza 칥ntreagul input 탳i a o 칥mp캒r탵i 칥n mai multe buc캒탵i, a탳a cum am f캒cut 칥n [Capitolul 6](/course/chapter6/4). De asemenea, vom utiliza op탵iunea `return_length` pentru a returna automat lungimea fiec캒rui fragment creat. Adesea, ultimul fragment va fi mai mic dec칙t dimensiunea contextului, iar noi vom sc캒pa de aceste buc캒탵i pentru a evita problemele de padding; nu avem nevoie de ele, deoarece oricum avem o mul탵ime de date.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Fragmentarea unui text mare 칥n mai multe buc캒탵i."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Fragmentarea unui text mare 칥n mai multe buc캒탵i."/>
</div>

S캒 vedem exact cum func탵ioneaz캒 acest lucru analiz칙nd primele dou캒 exemple:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

Putem vedea c캒 ob탵inem 34 de segmente 칥n total din aceste dou캒 exemple. Uit칙ndu-ne la lungimea segmentelor, putem vedea c캒 segmentele de la sf칙r탳itul ambelor documente au mai pu탵in de 128 de token-uri (117 탳i, respectiv, 41). Acestea reprezint캒 doar o mic캒 parte din totalul segmentelor pe care le avem, a탳a c캒 le putem 탳terge 칥n siguran탵캒. Cu ajutorul c칙mpului `overflow_to_sample_mapping`, putem, de asemenea, s캒 reconstituim care segmente au apar탵inut c캒ror probe de intrare.

Cu aceast캒 opera탵iune folosim o caracteristic캒 util캒 a func탵iei `Dataset.map()` din 游뱅 Datasets, 탳i anume c캒 nu necesit캒 one-to-one maps; a탳a cum am v캒zut 칥n [sec탵iunea 3](/course/chaptero7/3), putem crea batch-uri cu mai multe sau mai pu탵ine elemente dec칙t batch-ul de intrare. Acest lucru este util atunci c칙nd efectu캒m opera탵iuni precum augmentarea sau filtrarea datelor care modific캒 num캒rul de elemente. 칉n cazul nostru, atunci c칙nd tokeniz캒m fiecare element 칥n segmente de dimensiunea contextului specificat, cre캒m multe probe din fiecare document. Trebuie doar s캒 ne asigur캒m c캒 탳tergem coloanele existente, deoarece acestea au o dimensiune conflictual캒. Dac캒 am dori s캒 le p캒str캒m, am putea s캒 le repet캒m 칥n mod corespunz캒tor 탳i s캒 le return캒m 칥n cadrul apelului `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

Avem acum 16,7 milioane de exemple cu 128 de tokenii fiecare, ceea ce corespunde unui total de aproximativ 2,1 miliarde de tokeni. Ca referin탵캒, modelele GPT-3 탳i Codex ale OpenAI sunt antrenate pe 300 탳i, respectiv, 100 de miliarde de tokeni, unde modelele Codex sunt ini탵ializate din checkpointurile GPT-3. Scopul nostru 칥n aceast캒 sec탵iune nu este de a concura cu aceste modele, care pot genera texte lungi 탳i coerente, ci de a crea o versiune la scar캒 redus캒 care s캒 ofere o func탵ie rapid캒 de autocompletare pentru data scientists.

Acum c캒 avem datasetul gata, hai s캒 configur캒m modelul!

> [!TIP]
> 九勇 **칉ncerca탵i!** Eliminarea tuturor buc캒탵ilor care sunt mai mici dec칙t dimensiunea contextului nu a fost o problem캒 major캒 aici, deoarece folosim ferestre de context mici. Pe m캒sur캒 ce cre탳te탵i dimensiunea contextului (sau dac캒 ave탵i un corpus de documente scurte), frac탵iunea de segmente care sunt aruncate va cre탳te 탳i ea. O modalitate mai eficient캒 de a preg캒ti datele este de a uni toate sampleurile tokenizate 칥ntr-un batch cu un token `eos_token_id` 칥ntre ele, iar apoi de a efectua chunkingul pe secven탵ele concatenate. Ca exerci탵iu, modifica탵i func탵ia `tokenize()` pentru a utiliza aceast캒 abordare. Re탵ine탵i c캒 ve탵i dori s캒 seta탵i `truncation=False` 탳i s캒 elimina탵i celelalte argumente din tokenizer pentru a ob탵ine secven탵a complet캒 de token IDs.


## Ini탵ializarea unui nou model[[initializing-a-new-model]]

Primul nostru pas este s캒 ini탵ializ캒m un model GPT-2. Vom utiliza aceea탳i configura탵ie pentru modelul nostru ca 탳i pentru modelul GPT-2 mic, deci 칥nc캒rc캒m configura탵ia preantrenat캒, ne asigur캒m c캒 dimensiunea tokenizerlui corespunde cu dimensiunea vocabularului modelului 탳i transmitem ID-urile tokenilor `bos` 탳i `eos` (칥nceputul 탳i sf칙r탳itul secven탵ei):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Cu aceast캒 configura탵ie, putem 칥nc캒rca un nou model. Re탵ine탵i c캒 aceasta este prima dat캒 c칙nd nu folosim func탵ia `from_pretrained()`, deoarece ini탵ializ캒m noi 칥n탳ine un model:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Cu aceast캒 configura탵ie, putem 칥nc캒rca un nou model. Re탵ine탵i c캒 aceasta este prima dat캒 c칙nd nu folosim func탵ia `from_pretrained()`, deoarece ini탵ializ캒m noi 칥n탳ine un model:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

Modelul nostru are 124 milioane de parametri pe care va trebui s캒 le facem tune. 칉nainte de a 칥ncepe antrenarea, trebuie s캒 configur캒m un data collator care se va ocupa de crearea batch-urilor. Putem utiliza colatorul `DataCollatorForLanguageModeling`, care este conceput special pentru modelarea limbajului (dup캒 cum sugereaz캒 subtil numele). Pe l칙ng캒 stacking 탳i paddingul batchurilor, acesta se ocup캒 탳i de crearea labelurilor modelului lingvistic - 칥n modelarea cauzal캒 a limbajului, inputurile servesc 탳i ca labels (doar c캒 sunt decalate cu un element), iar acest data collator le creeaz캒 din mers 칥n timpul antren캒rii, astfel 칥nc칙t s캒 nu fie nevoie s캒 duplic캒m `input_ids`.

Re탵ine탵i c캒 `DataCollatorForLanguageModeling` accept캒 at칙t masked language masking (MLM), c칙t 탳i causal language modeling(CLM). 칉n mod implicit, acesta preg캒te탳te datele pentru MLM, dar putem trece la CLM prin setarea argumentului `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

S캒 arunc캒m o privire la un exemplu:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

Putem vedea c캒 exemplele sunt stacked 탳i c캒 to탵i tensorii au aceea탳i form캒.

{#if fw === 'tf'}

Acum putem utiliza metoda `prepare_tf_dataset()` pentru a converti dataseturile noastre 칥n dataseturi TensorFlow cu ajutorul data collatorului pe care l-am creat mai sus:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

> [!WARNING]
> 丘멆잺 Schimbarea inputurilor 탳i a labelurilor pentru a le alinia are loc 칥n interiorul modelului, astfel 칥nc칙t data collatorului doar copiaz캒 inputurile pentru a crea labeluri.


Acum avem totul preg캒tit pentru a ne antrena modelul - p칙n캒 la urm캒 nu a fost at칙t de greu! 칉nainte de a 칥ncepe antrenamentul, trebuie s캒 ne conect캒m la Hugging Face. Dac캒 lucra탵i 칥ntr-un notebook, pute탵i face acest lucru cu urm캒toarea func탵ie de utilitate:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Aceasta va afi탳a un widget 칥n care pute탵i introduce datele voastre de autentificare Hugging Face.

Dac캒 nu lucra탵i 칥ntr-un notebook, tasta탵i urm캒toarea linie 칥n terminal:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Tot ce a mai r캒mas de f캒cut este s캒 configur캒m argumentele de antrenare 탳i s캒 pornim `Trainer`-ul. Vom utiliza un cosine learning rate schedule cu un warmup 탳i o dimensiune efectiv캒 a batch-ului de 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). Acumularea gradientului este utilizat캒 atunci c칙nd un singur batch nu 칥ncape 칥n memorie 탳i construie탳te treptat gradientul prin mai multe treceri 칥nainte/칥napoi. Vom vedea acest lucru 칥n ac탵iune atunci c칙nd vom crea bucla de antrenare cu 游뱅 Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

Acum putem doar s캒 pornim `Trainer`-ul 탳i s캒 a탳tept캒m ca antrenamentul s캒 se termine. 칉n func탵ie de executarea antren캒rii pe 칥ntregul set de antrenarea sau pe un subset al acestuia, va dura 20 minute, sau respectiv 2 ore, a탳a c캒 lua탵i c칙teva cafelu탵e 탳i o carte bun캒 de citit!

```py
trainer.train()
```

Dup캒 finalizarea antren캒rii, putem trimite modelul 탳i tokenizerul c캒tre Hub:

```py
trainer.push_to_hub()
```

{:else}

Tot ce r캒m칙ne de f캒cut este s캒 configura탵i hiperparametrii de antrenament 탳i s캒 apela탵i `compile()` 탳i `fit()`. Vom utiliza un program al learning rate cu un anumit warmup pentru a 칥mbun캒t캒탵i stabilitatea antren캒rii:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Acum putem apela `model.fit()` 탳i s캒 a탳tept캒m ca antrenarea s캒 se 칥ncheie. 칉n func탵ie de executarea antren캒rii pe 칥ntregul set de antrenarea sau pe un subset al acestuia, va dura 20 minute, sau respectiv 2 ore, a탳a c캒 lua탵i c칙teva cafelu탵e 탳i o carte bun캒 de citit!

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

> [!TIP]
> 九勇 **Try it out!** Ne-a luat doar aproximativ 30 de linii de cod 칥n plus fa탵캒 de `TrainingArguments` pentru a ajunge de la texte brute la antrenarea GPT-2. 칉ncerca탵i antrenarea cu propriul dataset 탳i vede탵i dac캒 pute탵i ob탵ine rezultate bune!

> [!TIP]
> {#if fw === 'pt'}
>
> 游눠 Dac캒 ave탵i acces la un calculator cu mai multe GPU-uri, 칥ncerca탵i s캒 rula탵i codul acolo. `Trainer` gestioneaz캒 automat mai multe calculatoare, iar acest lucru poate accelera foarte mult antrenamentul.
>
> {:else}
>
> 游눠 Dac캒 ave탵i acces la un calculator cu mai multe GPU-uri, pute탵i 칥ncerca s캒 utiliza탵i un context `MirroredStrategy` pentru a accelera substan탵ial antrenarea. Va trebui s캒 crea탵i un obiect `tf.distribute.MirroredStrategy` 탳i s캒 v캒 asigura탵i c캒 toate metodele `to_tf_dataset()` sau `prepare_tf_dataset()`, precum 탳i crearea modelului 탳i apelul la `fit()` sunt rulate 칥n contextul s캒u `scope()`. Pute탵i vedea documenta탵ia despre acest lucru [aici] (https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).
>
> {/if}

## Generarea codului cu un pipeline[[code-generation-with-a-pipeline]]

Acum este momentul adev캒rului: s캒 vedem c칙t de bine func탵ioneaz캒 de fapt modelul antrenat! Putem vedea 칥n loguri c캒 pierderea a sc캒zut 칥n mod constant, dar pentru a testa modelul, hai s캒 vedem c칙t de bine func탵ioneaz캒 la c칙teva 칥ncer캒ri. Pentru a face acest lucru, vom 칥ncorpora modelul 칥ntr-un `pipeline` de generare a textului 탳i 칥l vom pune pe un GPU pentru genera탵ii rapide, dac캒 exist캒 unul disponibil:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

S캒 칥ncepem cu sarcina simpl캒 de a crea un scatter plot:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# crearea unui scatter plot cu x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# crearea unor date
x = np.random.randn(100)
y = np.random.randn(100)

# crearea scatter plot cu x, y
plt.scatter(x, y)

# crearea scatter
```

Rezultatul pare corect. Func탵ioneaz캒 탳i pentru o opera탵ie `pandas`? S캒 vedem dac캒 putem crea un `DataFrame` din dou캒 array-uri:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# crea탵i dataframeul din x 탳i y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# creaz캒 un dataframe din x 탳i y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

Excelent, acesta este r캒spunsul corect - de탳i apoi introduce din nou coloana `x`. Deoarece num캒rul de tokeni generate este limitat, urm캒toarea bucl캒 `for` este 칥ntrerupt캒. S캒 vedem dac캒 putem face ceva un pic mai complex 탳i dac캒 modelul ne ajut캒 s캒 folosim opera탵ia `groupby`:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calcula탵i venitul mediu pe profesie
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculez캒 venitul mediu pe profesie
profession = df.groupby(['profession']).mean()

# calculeaz캒
```

Nu este r캒u; acesta este modul corect de a face acest lucru. 칉n cele din urm캒, s캒 vedem dac캒 칥l putem folosi 탳i pentru `scikit-learn` 탳i s캒 configur캒m un model Random Forest:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# ajusta탵i modelul Random Forest cu 300 de estimatori pe X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# ajusta탵i modelul Random Forest cu 300 de estimatori pe X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

Privind la aceste c칙teva exemple, se pare c캒 modelul a 칥nv캒탵at o parte din sintaxa Python data science stack. Desigur, ar trebui s캒 evalu캒m modelul mai am캒nun탵it 칥nainte de a-l implementa 칥n lumea real캒, totu탳i acesta este un prototip impresionant.

{:else}

Privind aceste c칙teva exemple, se pare c캒 modelul a 칥nv캒탵at o parte din sintaxa Python data science stack(desigur, ar trebui s캒 o evalu캒m mai bine 칥nainte de a implementa modelul 칥n lumea real캒). Cu toate acestea, uneori este nevoie de o mai mare personalizare a antren캒rii modelului pentru a ob탵ine performan탵a necesar캒 pentru un anumit caz de utilizare. De exemplu, dac캒 am dori s캒 actualiz캒m dinamic dimensiunea batch-ului sau s캒 avem o bucl캒 de antrenare condi탵ionat캒 care trece peste exemplele proaste din mers? O op탵iune ar fi s캒 facem subclass la `Trainer` 탳i s캒 ad캒ug캒m modific캒rile necesare, dar uneori este mai simplu s캒 scriem bucla de antrenare de la zero. Aici intervine 游뱅 Accelerate.

{/if}

{#if fw === 'pt'}

## Antrenarea cu 游뱅 Accelerate[[training-with-accelerate]]

Am v캒zut cum s캒 antren캒m un model cu `Trainer`, care poate permite o anumit캒 personalizare. Cu toate acestea, uneori dorim control deplin asupra buclei de antrenare sau dorim s캒 facem unele schimb캒ri exotice. 칉n acest caz, 游뱅 Accelerate este o alegere excelent캒, iar 칥n aceast캒 sec탵iune vom parcurge pa탳ii de utilizare a acestuia pentru a ne antrena modelul. Pentru a face lucrurile mai interesante, vom ad캒uga 탳i un twist buclei de antrenare.

<Youtube id="Hm8_PgVTFuc"/>

Deoarece suntem interesa탵i 칥n principal de o autocompletare sensibil캒 pentru bibliotecile din domeniul data science, este logic s캒 acord캒m mai mult캒 importan탵캒 exemplelor de antrenare care utilizeaz캒 mai mult aceste biblioteci. Putem identifica cu u탳urin탵캒 aceste exemple prin utilizarea unor cuvinte-cheie precum `plt`, `pd`, `sk`, `fit` 탳i `predict`, care sunt cele mai frecvente nume de import pentru `matplotlib.pyplot`, `pandas` 탳i `sklearn`, precum 탳i modelul fit/predict al acestora din urm캒. Dac캒 acestea sunt reprezentate fiecare ca un singur simbol, putem verifica cu u탳urin탵캒 dac캒 apar 칥n secven탵a de input. Tokenii pot avea un prefix de spa탵iu, deci vom verifica 탳i aceste versiuni 칥n vocabularul tokenizerului. Pentru a verifica dac캒 func탵ioneaz캒, vom ad캒uga un token de test care ar trebui s캒 fie 칥mp캒r탵it 칥n mai mul탵i tokeni:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

Grozav, se pare c캒 func탵ioneaz캒 bine! Acum putem scrie o func탵ie de pierdere personalizat캒 care ia ca secven탵a de input, logurile 탳i tokenii cheie pe care tocmai le-am selectat. 칉n primul r칙nd, trebuie s캒 aliniem logurile 탳i inputurile: secven탵a de intrare deplasat캒 cu o unitate la dreapta formeaz캒 labeluri, deoarece urm캒torul tokenul este labelul pentru tokenul curent. Putem realiza acest lucru 칥ncep칙nd cu labelurile de la al doilea token al secven탵ei de intrare, deoarece modelul nu face o predic탵ie pentru primul token 칥n orice caz. Apoi t캒iem ultimul logit, deoarece nu avem un label pentru tokenul care urmeaz캒 secven탵ei complete de intrare. Astfel, putem calcula pierderea per sample 탳i putem num캒ra apari탵iile tuturor cuvintelor-cheie 칥n fiecare sample. 칉n cele din urm캒, calcul캒m media weighturilor pe fiecare sample folosind apari탵iile ca weighturi. Deoarece nu dorim s캒 elimin캒m toate sampleurile care nu au cuvinte-cheie, ad캒ug캒m 1 la weighturi:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

칉nainte de a 칥ncepe antrenamentul cu aceast캒 nou캒 func탵ie de pierdere minunat캒, trebuie s캒 preg캒tim c칙teva lucruri:

- Avem nevoie de dataloaders pentru a 칥nc캒rca datele 칥n batch-uri.
- Trebuie s캒 configur캒m parametrii de sc캒dere a weighturilor.
- Din c칙nd 칥n c칙nd, dorim s캒 evalu캒m, astfel 칥nc칙t este logic s캒 includem codul de evaluare 칥ntr-o func탵ie.

S캒 칥ncepem cu dataloaders. Trebuie doar s캒 set캒m formatul datasetului la `"torch"`, iar apoi 칥l putem trece la un `DataLoader` PyTorch cu dimensiunea corespunz캒toare a batch-lui:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

칉n continuare, grup캒m parametrii astfel 칥nc칙t optimizatorul s캒 탳tie care dintre ace탳tia vor primi o sc캒dere suplimentar캒 a weighturilor. De obicei, to탵i termenii weighturilor bias 탳i LayerNorm sunt scuti탵i de acest lucru; iat캒 cum putem face acest lucru:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

Deoarece dorim s캒 evalu캒m modelul 칥n mod regulat pe setul de validare 칥n timpul antren캒rii, trebuie s캒 scriem o func탵ie 탳i pentru acest lucru. Aceasta ruleaz캒 pur 탳i simplu prin dataloaderul de evaluare 탳i adun캒 toate pierderile 칥n cadrul proceselor:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

Cu func탵ia `evaluate()` putem raporta pierderile 탳i [perplexitatea](/course/chapter7/3) la intervale regulate. 칉n continuare, redefinim modelul nostru pentru a ne asigura c캒 antren캒m din nou de la zero:

```py
model = GPT2LMHeadModel(config)
```

Apoi putem defini optimizatorul nostru, folosind func탵ia de mai devreme pentru a 칥mp캒r탵i parametrii pentru sc캒derea weighturilor:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

Acum s캒 preg캒tim modelul, optimizatorul 탳i dataloaderurile, astfel 칥nc칙t s캒 putem 칥ncepe antrenamentul:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

> [!TIP]
> 游뚿 Dac캒 antrena탵i pe un TPU, va trebui s캒 muta탵i tot codul 칥ncep칙nd cu celula de mai sus 칥ntr-o func탵ie de antrenare dedicat캒. Consulta탵i [Capitolul 3](/course/chapter3) pentru mai multe detalii.

Acum c캒 am trimis `train_dataloader` la `accelerator.prepare()`, putem utiliza lungimea acestuia pentru a calcula num캒rul de pa탳i de antrenare. Re탵ine탵i c캒 ar trebui s캒 facem acest lucru 칥ntotdeauna dup캒 ce preg캒tim dataloaderurile, deoarece aceast캒 metod캒 칥i va modifica lungimea. Utiliz캒m un program liniar clasic de la rata de 칥nv캒탵are la 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

칉n cele din urm캒, pentru a trimite modelul nostru c캒tre Hub, va trebui s캒 cre캒m un obiect `Repository` 칥ntr-un folder de lucru. 칉n primul r칙nd, conecta탵i-v캒 la Hugging Face Hub, dac캒 nu sunte탵i deja conectat. Vom determina numele repositoriului pornind de la ID-ul modelului pe care dorim s캒 칥l atribuim modelului nostru (nu ezita탵i s캒 칥nlocui탵i `repo_name` cu propria alegere; acesta trebuie doar s캒 con탵in캒 numele vostru de utilizator, ceea ce face func탵ia `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

Apoi putem clona acel repositoriu 칥ntr-un folder local. Dac캒 exist캒 deja, acest folder local ar trebui s캒 fie o clon캒 existent캒 a repositoriul cu care lucr캒m:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Acum putem 칥nc캒rca orice salv캒m 칥n `output_dir` prin apelarea metodei `repo.push_to_hub()`. Acest lucru ne va ajuta s캒 칥nc캒rc캒m modelele intermediare la sf칙r탳itul fiec캒rei epoci.

칉nainte de antrenament, s캒 efectu캒m un test rapid pentru a vedea dac캒 func탵ia de evaluare func탵ioneaz캒 corect:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

Acestea sunt valori foarte ridicate pentru pierdere 탳i perplexitate, dar acest lucru nu este surprinz캒tor, deoarece nu am antrenat 칥nc캒 modelul. Astfel, avem totul preg캒tit pentru a scrie partea principal캒 a scriptului de antrenare: bucla de antrenare. 칉n bucla de antrenare, iter캒m peste dataloader 탳i transmitem batch-urile c캒tre model. Cu logurile, putem apoi evalua func탵ia noastr캒 de pierdere personalizat캒. Redimension캒m pierderea 칥n func탵ie de num캒rul de etape de acumulare a gradientului pentru a nu crea pierderi mai mari atunci c칙nd agreg캒m mai multe etape. 칉nainte de a optimiza, comprim캒m, de asemenea, gradien탵ii pentru o mai bun캒 convergen탵캒. 칉n cele din urm캒, la fiecare c칙탵iva pa탳i, evalu캒m modelul pe setul de evaluare cu noua noastr캒 func탵ie `evaluate()`:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

탲i asta e tot - acum ave탵i propria bucl캒 de antrenare personalizat캒 pentru modele de limbaj cauzal, cum ar fi GPT-2, pe care o pute탵i personaliza 칥n continuare 칥n func탵ie de nevoile voastre.

> [!TIP]
> 九勇 **칥ncerca탵i!** Fie v캒 crea탵i propria func탵ie de pierdere personalizat캒, adaptat캒 la cazul vostru de utilizare, fie ad캒uga탵i un alt pas personalizat 칥n bucla de antrenare.

> [!TIP]
> 九勇 **칥ncerca탵i!** Atunci c칙nd efectua탵i experimente de antrenare de lung캒 durat캒, este o idee bun캒 s캒 칥nregistra탵i parametrii importan탵i utiliz칙nd instrumente precum TensorBoard sau Weights & Biases. Ad캒uga탵i o logare adecvat캒 la bucla de antrenare, astfel 칥nc칙t s캒 pute탵i verifica 칥ntotdeauna cum decurge antrenarea.

{/if}
