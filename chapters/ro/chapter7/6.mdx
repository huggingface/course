<FrameworkSwitchCourse {fw} />

# Antrenarea unui model de limbaj cauzal de la zero[[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

P칙n캒 acum, am folosit 칥n principal modele preantrenate 탳i le-am f캒cut fine-tuning pentru noi cazuri de utilizare prin reutilizarea weighturilor din preantrenare. Dup캒 cum am v캒zut 칥n [Capitolul 1](/course/chapter1), acest lucru este denumit 칥n mod obi탳nuit _칥nv캒탵are prin transfer_ 탳i este o strategie foarte reu탳it캒 pentru aplicarea modelelor Transformer la majoritatea cazurilor de utilizare din lumea real캒 칥n care datele etichetate sunt pu탵ine. 칉n acest capitol, vom adopta o abordare diferit캒 탳i vom antrena un model complet nou de la zero. Aceasta este o abordare bun캒 dac캒 ave탵i multe date 탳i este foarte diferit캒 de datele de preantrenare utilizate pentru modelele disponibile. Cu toate acestea, preantrenarea unui model lingvistic necesit캒, de asemenea, mult mai multe resurse de calcul dec칙t fine-tuningul unui model existent. Printre exemplele 칥n care poate fi util캒 antrenarea unui nou model se num캒r캒 dataseturile formate din note muzicale, secven탵e moleculare precum ADN sau limbaje de programare. Acestea din urm캒 au c칙탳tigat recent teren datorit캒 unor instrumente precum TabNine 탳i Copilot de la GitHub, alimentate de modelul Codex al OpenAI, care pot genera secven탵e lungi de cod. Aceast캒 sarcin캒 de generare a textului este cel mai bine abordat캒 cu modele de limbaj autoregresive sau cauzale, cum ar fi GPT-2.

칉n aceast캒 sec탵iune vom construi o versiune la scar캒 redus캒 a unui model de generare a codului: ne vom concentra pe complet캒ri de o linie 칥n loc de func탵ii sau clase complete, folosind un subset de cod Python. Atunci c칙nd lucra탵i cu date 칥n Python, sunte탵i 칥n contact frecvent Python data science stack, format캒 din bibliotecile `matplotlib`, `seaborn`, `pandas` 탳i `scikit-learn`. Atunci c칙nd se utilizeaz캒 aceste cadre, este frecvent s캒 fie nevoie s캒 se caute comenzi specifice, astfel 칥nc칙t ar fi bine dac캒 am putea utiliza un model care s캒 efectueze aceste apeluri pentru noi.

<Youtube id="Vpjb1lu0MDk"/>

칉n [Capitolul 6](/course/chapter6) am creat un tokenizer eficient pentru a procesa codul surs캒 Python, dar avem nevoie de un dataset la scar캒 larg캒 pe care s캒 preantren캒m un model. Aici, vom aplica tokenizerul nostru la un corpus de cod Python derivat din repositoriile GitHub. Vom utiliza apoi API-ul `Trainer` 탳i 游뱅 Accelerate pentru a antrena modelul. S캒 trecem la treab캒!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Aceasta este de fapt o prezentare a modelului care a fost antrenat 탳i 칥nc캒rcat 칥n Hub folosind codul prezentat 칥n aceast캒 sec탵iune. 칉l pute탵i g캒si [aici](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). Re탵ine탵i c캒, deoarece are loc o anumit캒 randomizare 칥n generarea textului, ve탵i ob탵ine probabil un rezultat u탳or diferit.
 
## Colectarea datelor[[gathering-the-data]]

Codul Python este disponibil din abunden탵캒 칥n repositorii de cod, cum ar fi GitHub, pe care le putem utiliza pentru a crea un dataset prin scraping pentru fiecare repositoriu Python. Aceasta a fost abordarea adoptat캒 칥n [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) pentru a preantrena un model GPT-2. Folosind o desc캒rcare GitHub de aproximativ 180 GB care con탵ine aproximativ 20 de milioane de fi탳iere Python numit캒 `codeparrot`, autorii au construit un dataset pe care l-au oferit apoi pe [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot).

Cu toate acestea, antrenarea pe 칥ntregul corpus consum캒 timp 탳i puterea calculatorului, iar noi avem nevoie doar de subsetul datasetului referitor la Python data science stack. A탳adar, s캒 칥ncepem prin filtrarea datasetului `codeparrot` pentru toate fi탳ierele care includ oricare dintre bibliotecile din aceast stack. Din cauza dimensiunii datasetului, dorim s캒 evit캒m desc캒rcarea acestuia; 칥n schimb, vom utiliza func탵ia de streaming pentru a-l filtra din mers. Pentru a ne ajuta s캒 filtr캒m exemplele de cod care utilizeaz캒 bibliotecile pe care le-am men탵ionat mai devreme, vom utiliza urm캒toarea func탵ie:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

S캒-l test캒m pe dou캒 exemple:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

Putem folosi acest lucru pentru a crea o func탵ie care va transmite 칥n flux datasetul 탳i va filtra elementele dorite:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

Apoi, putem aplica pur 탳i simplu aceast캒 func탵ie pe datasetului din flux:

```py
# This cell will take a very long time to execute, so you should skip it and go to
# the next one!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

Acest lucru ne las캒 cu aproximativ 3% din setul de date original, care este 칥nc캒 destul de mare - datasetul rezultat este de 6 GB 탳i const캒 din 600.000 de scripturi Python!

Filtrarea datasetului complet poate dura 2-3 ore, 칥n func탵ie de calculator 탳i de bandwidth. Dac캒 nu dori탵i s캒 parcurge탵i singur acest proces 칥ndelungat, v캒 punem la dispozi탵ie datasetul filtrat pe Hub pentru a-l desc캒rca:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

Preantrenarea modelului de limbaj va dura ceva timp. V캒 suger캒m s캒 rula탵i mai 칥nt칙i bucla de antrenare pe un sample de date prin decomentarea celor dou캒 linii par탵iale de mai sus 탳i s캒 v캒 asigura탵i c캒 antrenarea se finalizeaz캒 cu succes 탳i c캒 modelele sunt stocate. Nimic nu este mai frustrant dec칙t o rulare de antrenare care e탳ueaz캒 la ultimul pas pentru c캒 a탵i uitat s캒 crea탵i un folder sau pentru c캒 exist캒 o gre탳eal캒 de tipar la sf칙r탳itul buclei de antrenare!

</Tip>

S캒 ne uit캒m la un exemplu din dataset. Vom ar캒ta doar primele 200 de caractere din fiecare c칙mp:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

Putem vedea c캒ci c칙mpul `content` con탵ine codul pe care dorim ca modelul nostru s캒 se antreneze. Acum c캒 avem un dataset, trebuie s캒 preg캒tim textele astfel 칥nc칙t acestea s캒 fie 칥ntr-un format adecvat pentru preantrenare.

## Preg캒tirea datasetului[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

Primul pas va fi tokenizarea datelor, astfel 칥nc칙t s캒 le putem utiliza pentru antrenare. Deoarece obiectivul nostru este de a autocompleta 칥n principal apeluri scurte de func탵ii, putem p캒stra dimensiunea contextului relativ mic캒. Acest lucru are avantajul c캒 putem antrena modelul mult mai rapid 탳i c캒 necesit캒 semnificativ mai pu탵in캒 memorie. Dac캒 este important pentru aplica탵ia voastr캒 s캒 ave탵i mai mult context (de exemplu, dac캒 dori탵i ca modelul s캒 scrie teste unitare pe baza unui fi탳ier cu defini탵ia func탵iei), asigura탵i-v캒 c캒 m캒ri탵i acest num캒r, dar re탵ine탵i, de asemenea, c캒 acest lucru vine cu utilizare mai mare de memorie GPU. Pentru moment, s캒 fix캒m dimensiunea contextului la 128 de tokeni, spre deosebire de 1 024 sau 2 048 utilizate 칥n GPT-2 sau respectiv GPT-3.

Majoritatea documentelor con탵in mult mai mult de 128 de cuvinte, astfel 칥nc칙t trunchierea simpl캒 a inputurilor la lungimea maxim캒 ar elimina o mare parte din datasetul nostru. 칉n schimb, vom utiliza op탵iunea `return_overflowing_tokens` pentru a tokeniza 칥ntreagul input 탳i a o 칥mp캒r탵i 칥n mai multe buc캒탵i, a탳a cum am f캒cut 칥n [Capitolul 6](/course/chapter6/4). De asemenea, vom utiliza op탵iunea `return_length` pentru a returna automat lungimea fiec캒rui fragment creat. Adesea, ultimul fragment va fi mai mic dec칙t dimensiunea contextului, iar noi vom sc캒pa de aceste buc캒탵i pentru a evita problemele de padding; nu avem nevoie de ele, deoarece oricum avem o mul탵ime de date.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Fragmentarea unui text mare 칥n mai multe buc캒탵i."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Fragmentarea unui text mare 칥n mai multe buc캒탵i."/>
</div>

S캒 vedem exact cum func탵ioneaz캒 acest lucru analiz칙nd primele dou캒 exemple:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

Putem vedea c캒 ob탵inem 34 de segmente 칥n total din aceste dou캒 exemple. Uit칙ndu-ne la lungimea segmentelor, putem vedea c캒 segmentele de la sf칙r탳itul ambelor documente au mai pu탵in de 128 de token-uri (117 탳i, respectiv, 41). Acestea reprezint캒 doar o mic캒 parte din totalul segmentelor pe care le avem, a탳a c캒 le putem 탳terge 칥n siguran탵캒. Cu ajutorul c칙mpului `overflow_to_sample_mapping`, putem, de asemenea, s캒 reconstituim care segmente au apar탵inut c캒ror probe de intrare.

Cu aceast캒 opera탵iune folosim o caracteristic캒 util캒 a func탵iei `Dataset.map()` din 游뱅 Datasets, 탳i anume c캒 nu necesit캒 one-to-one maps; a탳a cum am v캒zut 칥n [sec탵iunea 3](/course/chaptero7/3), putem crea batch-uri cu mai multe sau mai pu탵ine elemente dec칙t batch-ul de intrare. Acest lucru este util atunci c칙nd efectu캒m opera탵iuni precum augmentarea sau filtrarea datelor care modific캒 num캒rul de elemente. 칉n cazul nostru, atunci c칙nd tokeniz캒m fiecare element 칥n segmente de dimensiunea contextului specificat, cre캒m multe probe din fiecare document. Trebuie doar s캒 ne asigur캒m c캒 탳tergem coloanele existente, deoarece acestea au o dimensiune conflictual캒. Dac캒 am dori s캒 le p캒str캒m, am putea s캒 le repet캒m 칥n mod corespunz캒tor 탳i s캒 le return캒m 칥n cadrul apelului `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

Avem acum 16,7 milioane de exemple cu 128 de tokenii fiecare, ceea ce corespunde unui total de aproximativ 2,1 miliarde de tokeni. Ca referin탵캒, modelele GPT-3 탳i Codex ale OpenAI sunt antrenate pe 300 탳i, respectiv, 100 de miliarde de tokeni, unde modelele Codex sunt ini탵ializate din checkpointurile GPT-3. Scopul nostru 칥n aceast캒 sec탵iune nu este de a concura cu aceste modele, care pot genera texte lungi 탳i coerente, ci de a crea o versiune la scar캒 redus캒 care s캒 ofere o func탵ie rapid캒 de autocompletare pentru data scientists.

Acum c캒 avem datasetul gata, hai s캒 configur캒m modelul!

<Tip>

九勇 **칉ncerca탵i!** Eliminarea tuturor buc캒탵ilor care sunt mai mici dec칙t dimensiunea contextului nu a fost o problem캒 major캒 aici, deoarece folosim ferestre de context mici. Pe m캒sur캒 ce cre탳te탵i dimensiunea contextului (sau dac캒 ave탵i un corpus de documente scurte), frac탵iunea de segmente care sunt aruncate va cre탳te 탳i ea. O modalitate mai eficient캒 de a preg캒ti datele este de a uni toate sampleurile tokenizate 칥ntr-un batch cu un token `eos_token_id` 칥ntre ele, iar apoi de a efectua chunkingul pe secven탵ele concatenate. Ca exerci탵iu, modifica탵i func탵ia `tokenize()` pentru a utiliza aceast캒 abordare. Re탵ine탵i c캒 ve탵i dori s캒 seta탵i `truncation=False` 탳i s캒 elimina탵i celelalte argumente din tokenizer pentru a ob탵ine secven탵a complet캒 de token IDs.

</Tip>


## Ini탵ializarea unui nou model[[initializing-a-new-model]]

Primul nostru pas este s캒 ini탵ializ캒m un model GPT-2. Vom utiliza aceea탳i configura탵ie pentru modelul nostru ca 탳i pentru modelul GPT-2 mic, deci 칥nc캒rc캒m configura탵ia preantrenat캒, ne asigur캒m c캒 dimensiunea tokenizerlui corespunde cu dimensiunea vocabularului modelului 탳i transmitem ID-urile tokenilor `bos` 탳i `eos` (칥nceputul 탳i sf칙r탳itul secven탵ei):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Cu aceast캒 configura탵ie, putem 칥nc캒rca un nou model. Re탵ine탵i c캒 aceasta este prima dat캒 c칙nd nu folosim func탵ia `from_pretrained()`, deoarece ini탵ializ캒m noi 칥n탳ine un model:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Cu aceast캒 configura탵ie, putem 칥nc캒rca un nou model. Re탵ine탵i c캒 aceasta este prima dat캒 c칙nd nu folosim func탵ia `from_pretrained()`, deoarece ini탵ializ캒m noi 칥n탳ine un model:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

Modelul nostru are 124 milioane de parametri pe care va trebui s캒 le facem tune. 칉nainte de a 칥ncepe antrenarea, trebuie s캒 configur캒m un data collator care se va ocupa de crearea batch-urilor. Putem utiliza colatorul `DataCollatorForLanguageModeling`, care este conceput special pentru modelarea limbajului (dup캒 cum sugereaz캒 subtil numele). Pe l칙ng캒 stacking 탳i paddingul batchurilor, acesta se ocup캒 탳i de crearea labelurilor modelului lingvistic - 칥n modelarea cauzal캒 a limbajului, inputurile servesc 탳i ca labels (doar c캒 sunt decalate cu un element), iar acest data collator le creeaz캒 din mers 칥n timpul antren캒rii, astfel 칥nc칙t s캒 nu fie nevoie s캒 duplic캒m `input_ids`.

Re탵ine탵i c캒 `DataCollatorForLanguageModeling` accept캒 at칙t masked language masking (MLM), c칙t 탳i causal language modeling(CLM). 칉n mod implicit, acesta preg캒te탳te datele pentru MLM, dar putem trece la CLM prin setarea argumentului `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

S캒 arunc캒m o privire la un exemplu:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

Putem vedea c캒 exemplele sunt stacked 탳i c캒 to탵i tensorii au aceea탳i form캒.

{#if fw === 'tf'}

Acum putem utiliza metoda `prepare_tf_dataset()` pentru a converti dataseturile noastre 칥n dataseturi TensorFlow cu ajutorul data collatorului pe care l-am creat mai sus:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

丘멆잺 Schimbarea inputurilor 탳i a labelurilor pentru a le alinia are loc 칥n interiorul modelului, astfel 칥nc칙t data collatorului doar copiaz캒 inputurile pentru a crea labeluri.

</Tip>


Acum avem totul preg캒tit pentru a ne antrena modelul - p칙n캒 la urm캒 nu a fost at칙t de greu! 칉nainte de a 칥ncepe antrenamentul, trebuie s캒 ne conect캒m la Hugging Face. Dac캒 lucra탵i 칥ntr-un notebook, pute탵i face acest lucru cu urm캒toarea func탵ie de utilitate:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Aceasta va afi탳a un widget 칥n care pute탵i introduce datele voastre de autentificare Hugging Face.

Dac캒 nu lucra탵i 칥ntr-un notebook, tasta탵i urm캒toarea linie 칥n terminal:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Tot ce a mai r캒mas de f캒cut este s캒 configur캒m argumentele de antrenare 탳i s캒 pornim `Trainer`-ul. Vom utiliza un cosine learning rate schedule cu un warmup 탳i o dimensiune efectiv캒 a batch-ului de 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). Acumularea gradientului este utilizat캒 atunci c칙nd un singur batch nu 칥ncape 칥n memorie 탳i construie탳te treptat gradientul prin mai multe treceri 칥nainte/칥napoi. Vom vedea acest lucru 칥n ac탵iune atunci c칙nd vom crea bucla de antrenare cu 游뱅 Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

Acum putem doar s캒 pornim `Trainer`-ul 탳i s캒 a탳tept캒m ca antrenamentul s캒 se termine. 칉n func탵ie de executarea antren캒rii pe 칥ntregul set de antrenarea sau pe un subset al acestuia, va dura 20 minute, sau respectiv 2 ore, a탳a c캒 lua탵i c칙teva cafelu탵e 탳i o carte bun캒 de citit!

```py
trainer.train()
```

Dup캒 finalizarea antren캒rii, putem trimite modelul 탳i tokenizerul c캒tre Hub:

```py
trainer.push_to_hub()
```

{:else}

Tot ce r캒m칙ne de f캒cut este s캒 configura탵i hiperparametrii de antrenament 탳i s캒 apela탵i `compile()` 탳i `fit()`. Vom utiliza un program al learning rate cu un anumit warmup pentru a 칥mbun캒t캒탵i stabilitatea antren캒rii:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Acum putem apela `model.fit()` 탳i s캒 a탳tept캒m ca antrenarea s캒 se 칥ncheie. 칉n func탵ie de executarea antren캒rii pe 칥ntregul set de antrenarea sau pe un subset al acestuia, va dura 20 minute, sau respectiv 2 ore, a탳a c캒 lua탵i c칙teva cafelu탵e 탳i o carte bun캒 de citit!

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

九勇 **Try it out!** Ne-a luat doar aproximativ 30 de linii de cod 칥n plus fa탵캒 de `TrainingArguments` pentru a ajunge de la texte brute la antrenarea GPT-2. 칉ncerca탵i antrenarea cu propriul dataset 탳i vede탵i dac캒 pute탵i ob탵ine rezultate bune!

</Tip>

<Tip>

{#if fw === 'pt'}

游눠 Dac캒 ave탵i acces la un calculator cu mai multe GPU-uri, 칥ncerca탵i s캒 rula탵i codul acolo. `Trainer` gestioneaz캒 automat mai multe calculatoare, iar acest lucru poate accelera foarte mult antrenamentul.

{:else}

游눠 Dac캒 ave탵i acces la un calculator cu mai multe GPU-uri, pute탵i 칥ncerca s캒 utiliza탵i un context `MirroredStrategy` pentru a accelera substan탵ial antrenarea. Va trebui s캒 crea탵i un obiect `tf.distribute.MirroredStrategy` 탳i s캒 v캒 asigura탵i c캒 toate metodele `to_tf_dataset()` sau `prepare_tf_dataset()`, precum 탳i crearea modelului 탳i apelul la `fit()` sunt rulate 칥n contextul s캒u `scope()`. Pute탵i vedea documenta탵ia despre acest lucru [aici] (https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## Generarea codului cu un pipeline[[code-generation-with-a-pipeline]]

Acum este momentul adev캒rului: s캒 vedem c칙t de bine func탵ioneaz캒 de fapt modelul antrenat! Putem vedea 칥n loguri c캒 pierderea a sc캒zut 칥n mod constant, dar pentru a testa modelul, hai s캒 vedem c칙t de bine func탵ioneaz캒 la c칙teva 칥ncer캒ri. Pentru a face acest lucru, vom 칥ncorpora modelul 칥ntr-un `pipeline` de generare a textului 탳i 칥l vom pune pe un GPU pentru genera탵ii rapide, dac캒 exist캒 unul disponibil:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

S캒 칥ncepem cu sarcina simpl캒 de a crea un scatter plot:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# crearea unui scatter plot cu x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# crearea unor date
x = np.random.randn(100)
y = np.random.randn(100)

# crearea scatter plot cu x, y
plt.scatter(x, y)

# crearea scatter
```

Rezultatul pare corect. Func탵ioneaz캒 탳i pentru o opera탵ie `pandas`? S캒 vedem dac캒 putem crea un `DataFrame` din dou캒 array-uri:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# crea탵i dataframeul din x 탳i y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# creaz캒 un dataframe din x 탳i y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

Excelent, acesta este r캒spunsul corect - de탳i apoi introduce din nou coloana `x`. Deoarece num캒rul de tokeni generate este limitat, urm캒toarea bucl캒 `for` este 칥ntrerupt캒. S캒 vedem dac캒 putem face ceva un pic mai complex 탳i dac캒 modelul ne ajut캒 s캒 folosim opera탵ia `groupby`:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calcula탵i venitul mediu pe profesie
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculez캒 venitul mediu pe profesie
profession = df.groupby(['profession']).mean()

# calculeaz캒
```

Nu este r캒u; acesta este modul corect de a face acest lucru. 칉n cele din urm캒, s캒 vedem dac캒 칥l putem folosi 탳i pentru `scikit-learn` 탳i s캒 configur캒m un model Random Forest:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# ajusta탵i modelul Random Forest cu 300 de estimatori pe X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# ajusta탵i modelul Random Forest cu 300 de estimatori pe X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

Privind la aceste c칙teva exemple, se pare c캒 modelul a 칥nv캒탵at o parte din sintaxa Python data science stack. Desigur, ar trebui s캒 evalu캒m modelul mai am캒nun탵it 칥nainte de a-l implementa 칥n lumea real캒, totu탳i acesta este un prototip impresionant.

{:else}

Privind aceste c칙teva exemple, se pare c캒 modelul a 칥nv캒탵at o parte din sintaxa Python data science stack(desigur, ar trebui s캒 o evalu캒m mai bine 칥nainte de a implementa modelul 칥n lumea real캒). Cu toate acestea, uneori este nevoie de o mai mare personalizare a antren캒rii modelului pentru a ob탵ine performan탵a necesar캒 pentru un anumit caz de utilizare. De exemplu, dac캒 am dori s캒 actualiz캒m dinamic dimensiunea batch-ului sau s캒 avem o bucl캒 de antrenare condi탵ionat캒 care trece peste exemplele proaste din mers? O op탵iune ar fi s캒 facem subclass la `Trainer` 탳i s캒 ad캒ug캒m modific캒rile necesare, dar uneori este mai simplu s캒 scriem bucla de antrenare de la zero. Aici intervine 游뱅 Accelerate.

{/if}

{#if fw === 'pt'}

## Antrenarea cu 游뱅 Accelerate[[training-with-accelerate]]

We've seen how to train a model with the `Trainer`, which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case 游뱅 Accelerate is a great choice, and in this section we'll go through the steps to use it to train our model. To make things more interesting, we'll also add a twist to the training loop.

<Youtube id="Hm8_PgVTFuc"/>

Since we are mainly interested in sensible autocompletion for the the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these examples through the use of keywords such as `plt`, `pd`, `sk`, `fit`, and `predict`, which are the most frequent import names for `matplotlib.pyplot`, `pandas`, and `sklearn` as well as the fit/predict pattern of the latter. If these are each represented as a single token, we can easily check if they occur in the input sequence. Tokens can have a whitespace prefix, so we'll also check for those versions in the tokenizer vocabulary. To verify that it works, we'll add one test token which should be split into multiple tokens:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

Great, that seems to work nicely! We can now write a custom loss function that takes the input sequence, the logits, and the key tokens we just selected as inputs. First we need to align the logits and inputs: the input sequence shifted by one to the right forms the labels, since the next token is the label for the current token. We can achieve this by starting the labels from the second token of the input sequence, since the model does not make a prediction for the first token anyway. Then we cut off the last logit, as we don't have a label for the token that follows the full input sequence. With that we can compute the loss per sample and count the occurrences of all keywords in each sample. Finally, we calculate the weighted average over all samples using the occurrences as weights. Since we don't want to throw away all the samples that have no keywords, we add 1 to the weights:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

Before we can start training with this awesome new loss function, we need to prepare a few things:

- We need dataloaders to load the data in batches.
- We need to set up weight decay parameters.
- From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function.

Let's start with the dataloaders. We only need to set the dataset's format to `"torch"`, and then we can pass it to a PyTorch `DataLoader` with the appropriate batch size:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

Next, we group the parameters so that the optimizer knows which ones will get an additional weight decay. Usually, all bias and LayerNorm weights terms are exempt from this; here's how we can do this:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

Since we want to evaluate the model regularly on the validation set during training, let's write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

With the `evaluate()` function we can report loss and [perplexity](/course/chapter7/3) at regular intervals. Next, we redefine our model to make sure we train from scratch again:

```py
model = GPT2LMHeadModel(config)
```

We can then define our optimizer, using the function from before to split the parameters for weight decay:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

Now let's prepare the model, optimizer, and dataloaders so we can start training:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

游뚿 If you're training on a TPU, you'll need to move all the code starting at the cell above into a dedicated training function. See [Chapter 3](/course/chapter3) for more details.

</Tip>

Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren't logged in already. We'll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.

Before we train, let's run a quick test to see if the evaluation function works properly:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

Those are very high values for loss and perplexity, but that's not surprising as we haven't trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new `evaluate()` function:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

And that's it -- you now have your own custom training loop for causal language models such as GPT-2 that you can further customize to your needs. 

<Tip>

九勇 **Try it out!** Either create your own custom loss function tailored to your use case, or add another custom step into the training loop.

</Tip>

<Tip>

九勇 **Try it out!** When running long training experiments it's a good idea to log important metrics using tools such as TensorBoard or Weights & Biases. Add proper logging to the training loop so you can always check how the training is going.

</Tip>

{/if}
