# ÃnÈ›elegerea NLP[[mastering-nlp]]

<CourseFloatingBanner
    chapter={7}
    classNames="absolute z-10 right-0 top-0"
/>

DacÄƒ aÈ›i ajuns pÃ¢nÄƒ aici Ã®n curs, felicitÄƒri - acum aveÈ›i toate cunoÈ™tinÈ›ele È™i instrumentele necesare pentru a aborda (aproape) orice sarcinÄƒ NLP cu ğŸ¤— Transformers È™i ecosistemul Hugging Face!

Am vÄƒzut o mulÈ›ime de data collators, aÈ™a cÄƒ am fÄƒcut acest mic videoclip pentru a vÄƒ ajuta sÄƒ gÄƒsiÈ›i cel pe care sÄƒ Ã®l utilizaÈ›i pentru fiecare sarcinÄƒ:

<Youtube id="-RPeakdlHYo"/>

DupÄƒ finalizarea acestui tur fulger prin sarcinile de bazÄƒ ale NLP, ar trebui sÄƒ:

* È˜tiÈ›i care arhitecturi (codificator, decodificator sau codificator-decodificator) sunt cele mai potrivite pentru fiecare sarcinÄƒ
* ÃnÈ›elegeÈ›i diferenÈ›a dintre preantrenare È™i fine-tuningul a unui model lingvistic
* È˜tiÈ›i cum sÄƒ antrenaÈ›i modele Transformer utilizÃ¢nd API-ul `Trainer` È™i caracteristicile de antrenare distribuitÄƒ ale ğŸ¤— Accelerate sau TensorFlow È™i Keras, Ã®n funcÈ›ie de calea pe care aÈ›i urmat-o
* ÃnÈ›elegeÈ›i semnificaÈ›ia È™i limitele metricilor precum ROUGE È™i BLEU pentru sarcinile de generare a textului
* È˜tiÈ›i cum sÄƒ interacÈ›ionaÈ›i cu modelele dvs. ajustate, atÃ¢t pe Hub, cÃ¢t È™i utilizÃ¢nd `pipeline` din ğŸ¤— Transformers

Ãn ciuda tuturor acestor cunoÈ™tinÈ›e, va veni un moment Ã®n care fie veÈ›i Ã®ntÃ¢lni un bug dificil Ã®n codul vostru fie veÈ›i avea o Ã®ntrebare despre cum sÄƒ rezolvaÈ›i o anumitÄƒ problemÄƒ NLP. Din fericire, comunitatea Hugging Face este aici pentru a vÄƒ ajuta! Ãn ultimul capitol al acestei pÄƒrÈ›i a cursului, vom explora modul Ã®n care vÄƒ puteÈ›i face debugging modelelor Transformer È™i solicita ajutor Ã®n mod eficient.