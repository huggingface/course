# ÃnÈ›elegerea LLM-urilor[[mastering-llms]]

<CourseFloatingBanner
    chapter={7}
    classNames="absolute z-10 right-0 top-0"
/>

DacÄƒ aÈ›i ajuns pÃ¢nÄƒ aici Ã®n curs, felicitÄƒri â€“ acum aveÈ›i toate cunoÈ™tinÈ›ele È™i instrumentele necesare pentru a aborda (aproape) orice sarcinÄƒ de procesare a limbajului cu ğŸ¤— Transformers È™i ecosistemul Hugging Face!

## De la NLP la LLM-uri

DeÈ™i am acoperit multe sarcini tradiÈ›ionale de NLP Ã®n acest curs, domeniul a fost revoluÈ›ionat de Modelele Mari de Limbaj (LLM-uri). Aceste modele au extins dramatic ceea ce este posibil Ã®n procesarea limbajului:

- Pot gestiona mai multe sarcini fÄƒrÄƒ fine-tuning specific pentru fiecare sarcinÄƒ
- ExceleazÄƒ la urmarea instrucÈ›iunilor È™i adaptarea la contexte diferite
- Pot genera text coerent È™i adecvat contextului pentru diverse aplicaÈ›ii
- Pot realiza raÈ›ionamente È™i rezolva probleme complexe prin tehnici precum chain-of-thought prompting

AbilitÄƒÈ›ile fundamentale de NLP pe care le-aÈ›i Ã®nvÄƒÈ›at sunt Ã®n continuare esenÈ›iale pentru a lucra eficient cu LLM-urile. ÃnÈ›elegerea tokenizÄƒrii, a arhitecturilor de modele, a metodelor de fine-tuning È™i a metricilor de evaluare vÄƒ oferÄƒ cunoÈ™tinÈ›ele necesare pentru a valorifica la maximum potenÈ›ialul LLM-urilor.

Am vÄƒzut o mulÈ›ime de data collators, aÈ™a cÄƒ am fÄƒcut acest mic videoclip pentru a vÄƒ ajuta sÄƒ gÄƒsiÈ›i cel pe care sÄƒ Ã®l utilizaÈ›i pentru fiecare sarcinÄƒ:

<Youtube id="-RPeakdlHYo"/>

DupÄƒ finalizarea acestui tur fulger prin sarcinile de bazÄƒ ale procesÄƒrii limbajului, ar trebui sÄƒ:

* È˜tiÈ›i care arhitecturi (encoder, decoder sau encoder-decoder) sunt cele mai potrivite pentru fiecare sarcinÄƒ
* ÃnÈ›elegeÈ›i diferenÈ›a dintre preantrenarea È™i fine-tuning-ul unui model lingvistic
* È˜tiÈ›i cum sÄƒ antrenaÈ›i modele Transformer folosind fie API-ul `Trainer` È™i funcÈ›ionalitÄƒÈ›ile de antrenare distribuitÄƒ ale ğŸ¤— Accelerate, fie TensorFlow È™i Keras, Ã®n funcÈ›ie de traseul pe care l-aÈ›i urmat
* ÃnÈ›elegeÈ›i semnificaÈ›ia È™i limitele metricilor precum ROUGE È™i BLEU pentru sarcinile de generare de text
* È˜tiÈ›i cum sÄƒ interacÈ›ionaÈ›i cu modelele voastre ajustate, atÃ¢t pe Hub, cÃ¢t È™i folosind `pipeline` din ğŸ¤— Transformers
* SÄƒ apreciaÈ›i modul Ã®n care LLM-urile se bazeazÄƒ pe È™i extind tehnicile tradiÈ›ionale de NLP

Ãn ciuda tuturor acestor cunoÈ™tinÈ›e, va veni un moment Ã®n care fie veÈ›i Ã®ntÃ¢lni un bug dificil Ã®n codul vostru, fie veÈ›i avea o Ã®ntrebare despre cum sÄƒ rezolvaÈ›i o anumitÄƒ problemÄƒ de procesare a limbajului. Din fericire, comunitatea Hugging Face este aici pentru a vÄƒ ajuta! Ãn ultimul capitol al acestei pÄƒrÈ›i a cursului, vom explora cum puteÈ›i depana modelele Transformer È™i cum puteÈ›i solicita ajutor Ã®n mod eficient.