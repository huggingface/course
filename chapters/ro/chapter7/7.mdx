<FrameworkSwitchCourse {fw} />

# RÄƒspuns la Ã®ntrebÄƒri[[question-answering]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_tf.ipynb"},
]} />

{/if}

Este timpul sÄƒ analizÄƒm rÄƒspunsul la Ã®ntrebÄƒri! AceastÄƒ sarcinÄƒ are mai multe variante, dar cea pe care ne vom concentra Ã®n aceastÄƒ secÈ›iune se numeÈ™te rÄƒspuns *extractiv* la Ã®ntrebÄƒri. Aceasta presupune formularea de Ã®ntrebÄƒri cu privire la un document È™i identificarea rÄƒspunsurilor ca _intervale de text_ Ã®n documentul Ã®n sine.

<Youtube id="ajPx5LwJD-I"/>

Vom face fine-tuning unui-model BERT pe [datasetul SQuAD] (https://rajpurkar.github.io/SQuAD-explorer/), care constÄƒ din Ã®ntrebÄƒri adresate de mulÈ›imea de lucrÄƒtori pe un set de articole Wikipedia. Acest lucru ne va oferi un model capabil sÄƒ calculeze predicÈ›ii precum aceasta:

<iframe src="https://course-demos-bert-finetuned-squad.hf.space" frameBorder="0" height="450" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Aceasta este de fapt o prezentare a modelului care a fost antrenat È™i Ã®ncÄƒrcat Ã®n Hub folosind codul prezentat Ã®n aceastÄƒ secÈ›iune. PuteÈ›i sÄƒ-l gÄƒsiÈ›i È™i sÄƒ verificaÈ›i predicÈ›iile [aici](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F).

> [!TIP]
> ğŸ’¡ Modelele bazate doar pe encoding, cum ar fi BERT, tind sÄƒ fie foarte bune la extragerea rÄƒspunsurilor la Ã®ntrebÄƒri de tip factoid, cum ar fi "Cine a inventat arhitectura Transformer?", dar nu se descurcÄƒ prea bine atunci cÃ¢nd primesc Ã®ntrebÄƒri deschise, cum ar fi "De ce este cerul albastru?" Ãn aceste cazuri mai dificile, modelele encoder-decoder precum T5 È™i BART sunt utilizate de obicei pentru a sintetiza informaÈ›iile Ã®ntr-un mod destul de similar cu [rezumarea textului](/course/chapter7/5). DacÄƒ sunteÈ›i interesat de acest tip de rÄƒspuns *generativ* la Ã®ntrebÄƒri, vÄƒ recomandÄƒm sÄƒ consultaÈ›i [demo-ul](https://yjernite.github.io/lfqa.html) nostru bazat pe [datasetul ELI5](https://huggingface.co/datasets/eli5).

## PregÄƒtirea datelor[[preparing-the-data]]

Datasetul care este cel mai utilizat ca referinÈ›Äƒ academicÄƒ pentru rÄƒspunderea extractivÄƒ la Ã®ntrebÄƒri este [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), deci acesta este cel pe care Ã®l vom utiliza aici. ExistÄƒ, de asemenea, un benchmark mai dificil [SQuAD v2](https://huggingface.co/datasets/squad_v2), care include Ã®ntrebÄƒri care nu au un rÄƒspuns. AtÃ¢t timp cÃ¢t propriul dataset conÈ›ine o coloanÄƒ pentru contexte, o coloanÄƒ pentru Ã®ntrebÄƒri È™i o coloanÄƒ pentru rÄƒspunsuri, ar trebui sÄƒ puteÈ›i adapta paÈ™ii de mai jos.

### Datasetul SQuD[[the-squad-dataset]]

Ca de obicei, putem descÄƒrca È™i stoca Ã®n cache datasetul Ã®ntr-un singur pas datoritÄƒ funcÈ›iei `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("squad")
```

Ne putem uita apoi la acest obiect pentru a afla mai multe despre datasetul SQuAD:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})
```

Se pare cÄƒ avem tot ce ne trebuie cu cÃ¢mpurile `context`, `question` È™i `answers`, aÈ™a cÄƒ sÄƒ le afiÈ™Äƒm pentru primul element al datasetului nostru de antrenare:

```py
print("Context: ", raw_datasets["train"][0]["context"])
print("Question: ", raw_datasets["train"][0]["question"])
print("Answer: ", raw_datasets["train"][0]["answers"])
```

```python out
Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'
Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'
Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
```

CÃ¢mpurile `context` È™i `question` sunt foarte simplu de utilizat. CÃ¢mpul `answers` este un pic mai complicat, deoarece conÈ›ine un dicÈ›ionar cu douÄƒ cÃ¢mpuri care sunt ambele liste. Acesta este formatul care va fi aÈ™teptat de metrica `squad` Ã®n timpul evaluÄƒrii; dacÄƒ utilizaÈ›i propriile date, nu trebuie neapÄƒrat sÄƒ vÄƒ faceÈ›i griji cu privire la plasarea rÄƒspunsurilor Ã®n acelaÈ™i format. CÃ¢mpul `text` este destul de evident, iar cÃ¢mpul `answer_start` conÈ›ine indicele caracterului de Ã®nceput al fiecÄƒrui rÄƒspuns din context.

Ãn timpul antrenamentului, existÄƒ un singur rÄƒspuns posibil. Putem verifica acest lucru folosind metoda `Dataset.filter()`:

```py
raw_datasets["train"].filter(lambda x: len(x["answers"]["text"]) != 1)
```

```python out
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
```

Cu toate acestea, pentru evaluare, existÄƒ mai multe rÄƒspunsuri posibile pentru fiecare sample, care pot fi identice sau diferite:

```py
print(raw_datasets["validation"][0]["answers"])
print(raw_datasets["validation"][2]["answers"])
```

```python out
{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}
{'text': ['Santa Clara, California', "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], 'answer_start': [403, 355, 355]}
```

Nu ne vom aprofunda Ã®n scriptul de evaluare, deoarece totul va fi Ã®ncorporat de o metricÄƒ ğŸ¤— Datasets pentru noi, dar versiunea scurtÄƒ este cÄƒ unele dintre Ã®ntrebÄƒri au mai multe rÄƒspunsuri posibile, iar acest script va compara un rÄƒspuns prezis cu toate rÄƒspunsurile acceptabile È™i va lua cel mai bun scor. DacÄƒ ne uitÄƒm la sampleul de la indexul 2, de exemplu:

```py
print(raw_datasets["validation"][2]["context"])
print(raw_datasets["validation"][2]["question"])
```

```python out
'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.'
'Where did Super Bowl 50 take place?'
```

putem vedea cÄƒ rÄƒspunsul poate fi Ã®ntr-adevÄƒr una dintre cele trei posibilitÄƒÈ›i pe care le-am vÄƒzut anterior.

### Procesarea datelor de antrenare[[processing-the-training-data]]

<Youtube id="qgaM0weJHpA"/>

SÄƒ Ã®ncepem cu preprocesarea datelor de antrenare. Partea dificilÄƒ va fi generarea labelurilor pentru rÄƒspunsul la Ã®ntrebare, care vor fi poziÈ›iile de Ã®nceput È™i de sfÃ¢rÈ™it ale tokenilor corespunzÄƒtoare rÄƒspunsului Ã®n context.

Dar sÄƒ nu ne grÄƒbim. Ãn primul rÃ¢nd, trebuie sÄƒ convertim textul din datele de intrare Ã®n ID-uri pe care modelul sÄƒ le poatÄƒ Ã®nÈ›elege, utilizÃ¢nd un tokenizer:

```py
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

DupÄƒ cum am menÈ›ionat anterior, vom face fine-tune unui model BERT, dar puteÈ›i utiliza orice alt tip de model, atÃ¢ta timp cÃ¢t are implementat un tokenizer rapid. PuteÈ›i vedea toate arhitecturile care vin cu o versiune rapidÄƒ Ã®n [acest tabel mare] (https://huggingface.co/transformers/#supported-frameworks), iar pentru a verifica dacÄƒ obiectul `tokenizer` pe care Ã®l utilizaÈ›i este Ã®ntr-adevÄƒr susÈ›inut de ğŸ¤— Tokenizers, vÄƒ puteÈ›i uita la atributul sÄƒu `is_fast`:

```py
tokenizer.is_fast
```

```python out
True
```

Putem transmite Ã®mpreunÄƒ Ã®ntrebarea È™i contextul cÄƒtre tokenizerul nostru, iar acesta va introduce Ã®n mod corespunzÄƒtor tokenii speciali pentru a forma o propoziÈ›ie ca aceasta:

```
[CLS] question [SEP] context [SEP]
```

Hai sÄƒ verificÄƒm de douÄƒ ori:

```py
context = raw_datasets["train"][0]["context"]
question = raw_datasets["train"][0]["question"]

inputs = tokenizer(question, context)
tokenizer.decode(inputs["input_ids"])
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '
'the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin '
'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '
'upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred '
'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '
'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '
'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '
'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

Labelurile vor fi apoi indexul tokenilor care Ã®ncep È™i terminÄƒ rÄƒspunsul, iar modelul va fi Ã®nsÄƒrcinat sÄƒ prezicÄƒ un logit de Ã®nceput È™i de sfÃ¢rÈ™it pentru fiecare token din intrare, labelurile teoretice fiind urmÄƒtoarele:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg" alt="One-hot encoded label pentru rÄƒspunderea la Ã®ntrebÄƒri."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels-dark.svg" alt="One-hot encoded label pentru rÄƒspunderea la Ã®ntrebÄƒri."/>
</div>

Ãn acest caz, contextul nu este prea lung, dar unele dintre exemplele din dataset au contexte foarte lungi care vor depÄƒÈ™i lungimea maximÄƒ pe care am stabilit-o (care este de 384 Ã®n acest caz). DupÄƒ cum am vÄƒzut Ã®n [Capitolul 6](/course/chapter6/4) cÃ¢nd am explorat elementele interne ale pipelineului `question-answering`, vom trata contextele lungi prin crearea mai multor caracteristici de antrenare dintr-un sample din datasetul nostru, cu un sliding window Ã®ntre ele.

Pentru a vedea cum funcÈ›ioneazÄƒ acest lucru folosind exemplul curent, putem limita lungimea la 100 È™i putem utiliza un sliding window de 50 de tokeni. VÄƒ reamintim cÄƒ folosim:

- `max_length` pentru a stabili lungimea maximÄƒ (aici 100)
- `truncation="only_second"` pentru a trunchia contextul (care este Ã®n poziÈ›ia a doua) atunci cÃ¢nd Ã®ntrebarea cu contextul sÄƒu este prea lungÄƒ
- `stride` pentru a seta numÄƒrul de tokeni care se suprapun Ã®ntre douÄƒ bucÄƒÈ›i succesive (aici 50)
- `return_overflowing_tokens=True` pentru ca tokenizerul sÄƒ È™tie cÄƒ dorim tokenii care se suprapun

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

DupÄƒ cum se poate observa, exemplul nostru a fost Ã®mpÄƒrÈ›it Ã®n patru inputuri, fiecare dintre acestea conÈ›inÃ¢nd Ã®ntrebarea È™i o parte din context. ReÈ›ineÈ›i cÄƒ rÄƒspunsul la Ã®ntrebare ("Bernadette Soubirous") apare doar Ã®n al treilea È™i ultimul input, astfel Ã®ncÃ¢t, prin tratarea contextelor lungi Ã®n acest mod, vom crea cÃ¢teva exemple de antrenament Ã®n care rÄƒspunsul nu este inclus Ã®n context. Pentru aceste exemple, labelurile vor fi `start_position = end_position = 0` (deci vom prezice tokenul `[CLS]`). Vom seta aceste labeluri È™i Ã®n cazul nefericit Ã®n care rÄƒspunsul a fost trunchiat, astfel Ã®ncÃ¢t avem doar Ã®nceputul (sau sfÃ¢rÈ™itul) acestuia. Pentru exemplele Ã®n care rÄƒspunsul este complet Ã®n context, labelurile vor fi indicele tokenului Ã®n care Ã®ncepe rÄƒspunsul È™i indicele tokenului Ã®n care se terminÄƒ rÄƒspunsul.

Datasetul ne oferÄƒ caracterul de Ã®nceput al rÄƒspunsului Ã®n context, iar prin adÄƒugarea lungimii rÄƒspunsului, putem gÄƒsi caracterul de sfÃ¢rÈ™it Ã®n context. Pentru a le corela cu indicii tokenilor, va trebui sÄƒ folosim offset mapping pe care le-am studiat Ã®n [Capitolul 6](/course/chapter6/4). Putem face ca tokenizatorul nostru sÄƒ le returneze trecÃ¢nd `return_offsets_mapping=True`:

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
inputs.keys()
```

```python out
dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])
```

DupÄƒ cum putem vedea, primim Ã®napoi ID-urile obiÈ™nuite de intrare, ID-urile tipului de token È™i attention maskul, precum È™i offset mapping necesar È™i o cheie suplimentarÄƒ, `overflow_to_sample_mapping`. Valoarea corespunzÄƒtoare ne va fi de folos atunci cÃ¢nd vom tokeniza mai multe texte Ã®n acelaÈ™i timp (ceea ce ar trebui sÄƒ facem pentru a beneficia de faptul cÄƒ tokenizerul nostru este susÈ›inut de Rust). Deoarece un sample poate oferi mai multe caracteristici, aceasta mapeazÄƒ fiecare caracteristicÄƒ la exemplul din care provine. Deoarece aici am tokenizat un singur exemplu, obÈ›inem o listÄƒ de `0`:

```py
inputs["overflow_to_sample_mapping"]
```

```python out
[0, 0, 0, 0]
```

Dar dacÄƒ vom tokeniza mai multe exemple, acest lucru va deveni mai util:

```py
inputs = tokenizer(
    raw_datasets["train"][2:6]["question"],
    raw_datasets["train"][2:6]["context"],
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)

print(f"The 4 examples gave {len(inputs['input_ids'])} features.")
print(f"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.")
```

```python out
'The 4 examples gave 19 features.'
'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'
```

DupÄƒ cum se poate observa, primele trei exemple (la indicii 2, 3 È™i 4 din setul de antrenare) au dat fiecare cÃ¢te patru caracteristici, iar ultimul exemplu (la indicele 5 din setul de antrenare) a dat 7 caracteristici.

Aceste informaÈ›ii vor fi utile pentru a corela fiecare caracteristicÄƒ obÈ›inutÄƒ cu labeul corespunzÄƒtor. DupÄƒ cum am menÈ›ionat anterior, aceste labelurile sunt:

- `(0, 0)` dacÄƒ rÄƒspunsul nu se aflÄƒ Ã®n intervalul corespunzÄƒtor al contextului
- `(start_position, end_position)` dacÄƒ rÄƒspunsul se aflÄƒ Ã®n intervalul corespunzÄƒtor al contextului, cu `start_position` fiind indicele tokenului (Ã®n ID-urile de intrare) la Ã®nceputul rÄƒspunsului È™i `end_position` fiind indicele tokenului (Ã®n ID-urile de intrare) unde se terminÄƒ rÄƒspunsul

Pentru a determina care dintre acestea este cazul È™i, dacÄƒ este relevant, poziÈ›iile tokenilor, vom gÄƒsi mai Ã®ntÃ¢i indicii care Ã®ncep È™i terminÄƒ contextul Ã®n ID-urile de intrare. Am putea folosi ID-urile tipului de token pentru a face acest lucru, dar deoarece acestea nu existÄƒ neapÄƒrat pentru toate modelele (DistilBERT nu le solicitÄƒ, de exemplu), vom folosi Ã®n schimb metoda `sequence_ids()` a `BatchEncoding` pe care tokenizerul nostru o returneazÄƒ.

OdatÄƒ ce avem indicii tokenilor, ne uitÄƒm la offseturile corespunzÄƒtoare, care sunt tupeluri de douÄƒ numere Ã®ntregi reprezentÃ¢nd intervalul de caractere din contextul original. Astfel, putem detecta dacÄƒ bucÄƒÈ›ica de context din aceastÄƒ caracteristicÄƒ Ã®ncepe dupÄƒ rÄƒspuns sau se terminÄƒ Ã®nainte de Ã®nceperea rÄƒspunsului (caz Ã®n care eticheta este `(0, 0)`). DacÄƒ nu este cazul, facem o buclÄƒ pentru a gÄƒsi primul È™i ultimul token al rÄƒspunsului:

```py
answers = raw_datasets["train"][2:6]["answers"]
start_positions = []
end_positions = []

for i, offset in enumerate(inputs["offset_mapping"]):
    sample_idx = inputs["overflow_to_sample_mapping"][i]
    answer = answers[sample_idx]
    start_char = answer["answer_start"][0]
    end_char = answer["answer_start"][0] + len(answer["text"][0])
    sequence_ids = inputs.sequence_ids(i)

    # Find the start and end of the context
    idx = 0
    while sequence_ids[idx] != 1:
        idx += 1
    context_start = idx
    while sequence_ids[idx] == 1:
        idx += 1
    context_end = idx - 1

    # If the answer is not fully inside the context, label is (0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # Otherwise it's the start and end token positions
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python out
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

SÄƒ aruncÄƒm o privire la cÃ¢teva rezultate pentru a verifica dacÄƒ abordarea noastrÄƒ este corectÄƒ. Pentru prima caracteristicÄƒ gÄƒsim `(83, 85)` ca labeluri, aÈ™a cÄƒ comparÄƒm rÄƒspunsul teoretic cu intervalul decodat de tokeni de la 83 la 85 (inclusiv):

```py
idx = 0
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs["input_ids"][idx][start : end + 1])

print(f"Theoretical answer: {answer}, labels give: {labeled_answer}")
```

```python out
'Theoretical answer: the Main Building, labels give: the Main Building'
```

Deci, asta e o potrivire! Acum sÄƒ verificÄƒm indexul 4, unde am setat labelurile la `(0, 0)`, ceea ce Ã®nseamnÄƒ cÄƒ rÄƒspunsul nu se aflÄƒ Ã®n chunkul de context al acelei caracteristici:

```py
idx = 4
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

decoded_example = tokenizer.decode(inputs["input_ids"][idx])
print(f"Theoretical answer: {answer}, decoded example: {decoded_example}")
```

```python out
'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'
```

Ãntr-adevÄƒr, nu vedem rÄƒspunsul Ã®n interiorul contextului.

> [!TIP]
> âœï¸ **E rÃ¢ndul tÄƒu!** Atunci cÃ¢nd se utilizeazÄƒ arhitectura XLNet, paddingul este aplicat la stÃ¢nga, iar Ã®ntrebarea È™i contextul sunt schimbate. AdaptaÈ›i tot codul pe care tocmai l-am vÄƒzut la arhitectura XLNet (È™i adÄƒugaÈ›i `padding=True`). FiÈ›i conÈ™tienÈ›i de faptul cÄƒ tokenul `[CLS]` ar putea sÄƒ nu se afle la poziÈ›ia 0 Ã®n cazul aplicÄƒrii paddingului.

Acum cÄƒ am vÄƒzut pas cu pas cum sÄƒ preprocesÄƒm datele de antrenare, le putem grupa Ã®ntr-o funcÈ›ie pe care o vom aplica Ã®ntregului dataset de antrenare. Vom umple fiecare caracteristicÄƒ la lungimea maximÄƒ pe care am stabilit-o, deoarece majoritatea contextelor vor fi lungi (iar sampleurile corespunzÄƒtoare vor fi Ã®mpÄƒrÈ›ite Ã®n mai multe caracteristici), astfel Ã®ncÃ¢t nu existÄƒ niciun beneficiu real pentru aplicarea paddingului dinamic aici:

```py
max_length = 384
stride = 128


def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label is (0, 0)
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs
```

ReÈ›ineÈ›i cÄƒ am definit douÄƒ constante pentru a determina lungimea maximÄƒ utilizatÄƒ, precum È™i lungimea al sliding window, È™i cÄƒ am adÄƒugat o micÄƒ curÄƒÈ›are Ã®nainte de tokenizare: unele dintre Ã®ntrebÄƒrile din datasetul SQuAD au spaÈ›ii suplimentare la Ã®nceput È™i la sfÃ¢rÈ™it care nu adaugÄƒ nimic (È™i ocupÄƒ spaÈ›iu atunci cÃ¢nd sunt tokenizate dacÄƒ utilizaÈ›i un model precum RoBERTa), aÈ™a cÄƒ am eliminat aceste spaÈ›ii suplimentare.

Pentru a aplica aceastÄƒ funcÈ›ie Ã®ntregului set de antrenare, folosim metoda `Dataset.map()` cu flagul `batched=True`. Acesta este necesar aici, deoarece modificÄƒm lungimea datasetului (deoarece un exemplu poate oferi mai multe caracteristici de antrenare):

```py
train_dataset = raw_datasets["train"].map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
len(raw_datasets["train"]), len(train_dataset)
```

```python out
(87599, 88729)
```

DupÄƒ cum putem vedea, preprocesarea a adÄƒugat aproximativ 1.000 de caracteristici. Setul nostru de antrenare este acum gata de utilizare - sÄƒ trecem la preprocesarea setului de validare!

### Procesarea datelor de validare[[processing-the-validation-data]]

Preprocesarea datelor de validare va fi puÈ›in mai uÈ™oarÄƒ, deoarece nu trebuie sÄƒ generÄƒm labeluri (cu excepÈ›ia cazului Ã®n care dorim sÄƒ calculÄƒm o pierdere de validare, dar acest numÄƒr nu ne va ajuta sÄƒ Ã®nÈ›elegem cÃ¢t de bun este modelul). AdevÄƒrata bucurie va fi sÄƒ interpretÄƒm predicÈ›iile modelului Ã®n intervale ale contextului original. Pentru aceasta, va trebui doar sÄƒ stocÄƒm atÃ¢t offset mappings, cÃ¢t È™i o modalitate de a corela fiecare caracteristicÄƒ creatÄƒ cu exemplul original din care provine. Deoarece existÄƒ o coloanÄƒ ID Ã®n datasetul original, vom utiliza acel ID.

Singurul lucru pe care Ã®l vom adÄƒuga aici este o micÄƒ curÄƒÈ›are a offset mappings. Acestea vor conÈ›ine offseturi pentru Ã®ntrebare È™i context, dar odatÄƒ ajunÈ™i Ã®n etapa de postprocesare nu vom avea nicio modalitate de a È™ti care parte a ID-urilor de intrare corespunde contextului È™i care parte este Ã®ntrebarea (metoda `sequence_ids()` pe care am folosit-o este disponibilÄƒ doar pentru ieÈ™irea tokenizerului). Prin urmare, vom seta offseturile corespunzÄƒtoare Ã®ntrebÄƒrii la `None`:


```py
def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs
```

Putem aplica aceastÄƒ funcÈ›ie pe Ã®ntregul dataset de validare, ca È™i Ã®nainte:

```py
validation_dataset = raw_datasets["validation"].map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
len(raw_datasets["validation"]), len(validation_dataset)
```

```python out
(10570, 10822)
```

Ãn acest caz, am adÄƒugat doar cÃ¢teva sute de sampleuri, astfel Ã®ncÃ¢t se pare cÄƒ contextele din datasetul de validare sunt un pic mai scurte.

Acum cÄƒ am preprocesat toate datele, putem trece la antrenare.

{#if fw === 'pt'}

## Fine-tuningul modelului cu API-ul `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

Codul de antrenare pentru acest exemplu va semÄƒna foarte mult cu codul din secÈ›iunile anterioare - cel mai greu lucru va fi sÄƒ scriem funcÈ›ia `compute_metrics()`. Deoarece am fÄƒcut padding tuturor sampleurilor la lungimea maximÄƒ pe care am stabilit-o, nu trebuie definit niciun data collator, astfel Ã®ncÃ¢t acest calcul al metricii este singurul lucru de care trebuie sÄƒ ne facem griji. Partea dificilÄƒ va fi sÄƒ postprocesÄƒm predicÈ›iile modelului Ã®n intervale de text Ã®n exemplele originale; odatÄƒ ce am fÄƒcut acest lucru, metrica din biblioteca ğŸ¤— Datasets va face cea mai mare parte a muncii pentru noi.

{:else}

## Fine-tuningul unui model cu Keras[[fine-tuning-the-model-with-keras]]

Codul de antrenare pentru acest exemplu va semÄƒna foarte mult cu codul din secÈ›iunile anterioare, dar calcularea metricilor va fi o provocare unicÄƒ. Din moment ce am fÄƒcut padding tuturor sampleurilor la lungimea maximÄƒ pe care am stabilit-o, nu existÄƒ niciun data collator care sÄƒ fie definit, astfel Ã®ncÃ¢t acest calcul al metricii este singurul lucru de care trebuie sÄƒ ne facem griji. Partea dificilÄƒ va fi sÄƒ postprocesÄƒm predicÈ›iile modelului Ã®n intervale de text Ã®n exemplele originale; odatÄƒ ce am fÄƒcut acest lucru, metrica din biblioteca ğŸ¤— Datasets va face cea mai mare parte a muncii pentru noi.

{/if}

### Post-procesare[[post-processing]]

{#if fw === 'pt'}

<Youtube id="BNy08iIWVJM"/>

{:else}

<Youtube id="VN67ZpN33Ss"/>

{/if}

Modelul va produce logits pentru poziÈ›iile de Ã®nceput È™i de sfÃ¢rÈ™it ale rÄƒspunsului Ã®n ID-urile de intrare, aÈ™a cum am vÄƒzut Ã®n timpul explorÄƒrii [`question-answering` pipeline](/course/chapter6/3b). Etapa de post-procesare va fi similarÄƒ cu ceea ce am fÄƒcut acolo, aÈ™a cÄƒ iatÄƒ vÄƒ reamintim ce acÈ›iuni am luat:

- Am mascat logiturile de Ã®nceput È™i de sfÃ¢rÈ™it corespunzÄƒtoare tokenilor din afara contextului.
- Am convertit apoi logiturile de Ã®nceput È™i de sfÃ¢rÈ™it Ã®n probabilitÄƒÈ›i utilizÃ¢nd un softmax.
- Am atribuit un scor fiecÄƒrei perechi `(start_token, end_token)` prin calcularea produsului celor douÄƒ probabilitÄƒÈ›i corespunzÄƒtoare.
- Am cÄƒutat perechea cu scorul maxim care a dat un rÄƒspuns valid (de exemplu, `start_token` mai mic decÃ¢t `end_token`).

Aici vom schimba uÈ™or acest proces, deoarece nu trebuie sÄƒ calculÄƒm scorurile reale (doar rÄƒspunsul prezis). Aceasta Ã®nseamnÄƒ cÄƒ putem sÄƒri peste etapa softmax. De asemenea, pentru a merge mai repede, nu vom puncta toate perechile posibile `(start_token, end_token)`, ci doar pe cele care corespund celor mai mari `n_best` logits (cu `n_best=20`). Deoarece vom trece peste softmax, aceste scoruri vor fi scoruri logit È™i vor fi obÈ›inute prin Ã®nsumarea logiturilor de Ã®nceput È™i de sfÃ¢rÈ™it (Ã®n loc de produs, datoritÄƒ regulii \\(\log(ab) = \log(a) + \log(b)\\)).

Pentru a demonstra toate acestea, vom avea nevoie de un fel de predicÈ›ii. Deoarece nu ne-am antrenat Ã®ncÄƒ modelul, vom utiliza modelul implicit pentru pipelineuri QA pentru a genera unele predicÈ›ii pe o micÄƒ parte a setului de validare. Putem utiliza aceeaÈ™i funcÈ›ie de procesare ca Ã®nainte; deoarece se bazeazÄƒ pe constanta globalÄƒ `tokenizer`, trebuie doar sÄƒ schimbÄƒm acest obiect cu tokenizerul modelului pe care dorim sÄƒ Ã®l utilizÄƒm temporar:

```python
small_eval_set = raw_datasets["validation"].select(range(100))
trained_checkpoint = "distilbert-base-cased-distilled-squad"

tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)
eval_set = small_eval_set.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
```

Acum cÄƒ preprocesarea este terminatÄƒ, schimbÄƒm tokenizerul Ã®napoi la cel pe care l-am ales iniÈ›ial:

```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Apoi eliminÄƒm coloanele din `eval_set` care nu sunt aÈ™teptate de model, construim un batch cu Ã®ntregul set de validare È™i Ã®l trecem prin model. DacÄƒ este disponibil un GPU, Ã®l folosim pentru a merge mai repede:

{#if fw === 'pt'}

```python
import torch
from transformers import AutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}
trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(
    device
)

with torch.no_grad():
    outputs = trained_model(**batch)
```

Deoarece `Trainer` ne va oferi predicÈ›ii sub formÄƒ de matrici NumPy, preluÄƒm logiturile de Ã®nceput È™i de sfÃ¢rÈ™it È™i le convertim Ã®n acest format:

```python
start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()
```

{:else}

```python
import tensorflow as tf
from transformers import TFAutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("numpy")

batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}
trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)

outputs = trained_model(**batch)
```

Pentru uÈ™urinÈ›a experimentelor, sÄƒ convertim aceste rezultate Ã®n array-uri NumPy:

```python
start_logits = outputs.start_logits.numpy()
end_logits = outputs.end_logits.numpy()
```

{/if}

Acum, trebuie sÄƒ gÄƒsim rÄƒspunsul prezis pentru fiecare exemplu din `small_eval_set`. Este posibil ca un exemplu sÄƒ fi fost Ã®mpÄƒrÈ›it Ã®n mai multe caracteristici Ã®n `eval_set`, astfel Ã®ncÃ¢t primul pas constÄƒ Ã®n maparea fiecÄƒrui exemplu din `small_eval_set` la caracteristicile corespunzÄƒtoare din `eval_set`:

```python
import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(eval_set):
    example_to_features[feature["example_id"]].append(idx)
```

Cu acest lucru Ã®n mÃ¢nÄƒ, ne putem apuca de treabÄƒ trecÃ¢nd prin toate exemplele È™i, pentru fiecare exemplu, prin toate caracteristicile asociate. AÈ™a cum am spus mai devreme, ne vom uita la scorurile logit pentru `n_cele mai bune` logits de Ã®nceput È™i de sfÃ¢rÈ™it, excluzÃ¢nd poziÈ›iile care dau:

- Un rÄƒspuns care nu ar fi Ã®n interiorul contextului
- Un rÄƒspuns cu lungime negativÄƒ
- Un rÄƒspuns care este prea lung (limitÄƒm posibilitÄƒÈ›ile la `max_answer_length=30`)

OdatÄƒ ce avem toate rÄƒspunsurile posibile scored pentru un exemplu, Ã®l alegem pe cel cu cel mai bun scor logit:

```python
import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in small_eval_set:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = eval_set["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Skip answers that are not fully in the context
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Skip answers with a length that is either < 0 or > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
```

Formatul final al rÄƒspunsurilor prezise este cel care va fi aÈ™teptat de metrica pe care o vom utiliza. Ca de obicei, o putem Ã®ncÄƒrca cu ajutorul bibliotecii ğŸ¤— Evaluate:

```python
import evaluate

metric = evaluate.load("squad")
```

AceastÄƒ metricÄƒ aÈ™teaptÄƒ rÄƒspunsurile prezise Ã®n formatul pe care l-am vÄƒzut mai sus (o listÄƒ de dicÈ›ionare cu o cheie pentru ID-ul exemplului È™i o cheie pentru textul prezis) È™i rÄƒspunsurile teoretice Ã®n formatul de mai jos (o listÄƒ de dicÈ›ionare cu o cheie pentru ID-ul exemplului È™i o cheie pentru rÄƒspunsurile posibile):

```python
theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]} for ex in small_eval_set
]
```

Acum putem verifica dacÄƒ obÈ›inem rezultate rezonabile analizÃ¢nd primul element din ambele liste:

```python
print(predicted_answers[0])
print(theoretical_answers[0])
```

```python out
{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}
```

Nu-i deloc rÄƒu! Acum putem arunca o privire la scorul pe care ni-l oferÄƒ metrica:

```python
metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

Din nou, acest lucru este destul de bun, avÃ¢nd Ã®n vedere cÄƒ, Ã®n conformitate cu [documentul acestuia] (https://arxiv.org/abs/1910.01108v2), DistilBERT fine-tuned pe SQuAD obÈ›ine 79,1 È™i 86,9 pentru aceste scoruri pe Ã®ntregul dataset.

{#if fw === 'pt'}

Acum sÄƒ punem tot ce am fÄƒcut Ã®ntr-o funcÈ›ie `compute_metrics()` pe care o vom folosi Ã®n `Trainer`. Ãn mod normal, aceastÄƒ funcÈ›ie `compute_metrics()` primeÈ™te doar un tuple `eval_preds` cu logiÈ›i È™i labels. Aici vom avea nevoie de ceva mai mult, deoarece trebuie sÄƒ cÄƒutÄƒm Ã®n dataset de caracteristici pentru offset È™i Ã®n datasetul de exemple pentru contextele originale, astfel Ã®ncÃ¢t nu vom putea utiliza aceastÄƒ funcÈ›ie pentru a obÈ›ine rezultate de evaluare regulate Ã®n timpul antrenÄƒrii. O vom utiliza doar la sfÃ¢rÈ™itul antrenamentului pentru a verifica rezultatele.

FuncÈ›ia `compute_metrics()` grupeazÄƒ aceiaÈ™i paÈ™i ca Ã®nainte; adÄƒugÄƒm doar o micÄƒ verificare Ã®n cazul Ã®n care nu obÈ›inem niciun rÄƒspuns valid (caz Ã®n care prezicem un È™ir gol).

{:else}

Acum sÄƒ punem tot ce tocmai am fÄƒcut Ã®ntr-o funcÈ›ie `compute_metrics()` pe care o vom folosi dupÄƒ antrenarea modelului nostru. Va trebui sÄƒ transmitem puÈ›in mai mult decÃ¢t output logits, deoarece trebuie sÄƒ cÄƒutÄƒm Ã®n datasetul de caracteristici pentru offset È™i Ã®n datasetul de exemple pentru contextele originale:

{/if}

```python
from tqdm.auto import tqdm


def compute_metrics(start_logits, end_logits, features, examples):
    example_to_features = collections.defaultdict(list)
    for idx, feature in enumerate(features):
        example_to_features[feature["example_id"]].append(idx)

    predicted_answers = []
    for example in tqdm(examples):
        example_id = example["id"]
        context = example["context"]
        answers = []

        # Loop through all features associated with that example
        for feature_index in example_to_features[example_id]:
            start_logit = start_logits[feature_index]
            end_logit = end_logits[feature_index]
            offsets = features[feature_index]["offset_mapping"]

            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Skip answers that are not fully in the context
                    if offsets[start_index] is None or offsets[end_index] is None:
                        continue
                    # Skip answers with a length that is either < 0 or > max_answer_length
                    if (
                        end_index < start_index
                        or end_index - start_index + 1 > max_answer_length
                    ):
                        continue

                    answer = {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                    answers.append(answer)

        # Select the answer with the best score
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append(
                {"id": example_id, "prediction_text": best_answer["text"]}
            )
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    theoretical_answers = [{"id": ex["id"], "answers": ex["answers"]} for ex in examples]
    return metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

Putem verifica dacÄƒ funcÈ›ioneazÄƒ pe baza predicÈ›iilor noastre:

```python
compute_metrics(start_logits, end_logits, eval_set, small_eval_set)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

AratÄƒ bine! Acum sÄƒ folosim acest lucru pentru a face fine-tune modelului nostru.

### Fine-tuningul modelului[[fine-tuning-the-model]]

{#if fw === 'pt'}

Acum suntem gata sÄƒ antrenÄƒm modelul. SÄƒ-l creÄƒm mai Ã®ntÃ¢i, folosind clasa `AutoModelForQuestionAnswering` ca È™i Ã®nainte:

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{:else}

Acum suntem gata sÄƒ antrenÄƒm modelul. SÄƒ-l creÄƒm mai Ã®ntÃ¢i, folosind clasa `TFAutoModelForQuestionAnswering` ca mai Ã®nainte:

```python
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{/if}

Ca de obicei, primim un avertisment cÄƒ unele weighturi nu sunt utilizate (cele din headul de preantrenare), iar altele sunt iniÈ›ializate aleatoriu (cele pentru headul pentru rÄƒspuns la Ã®ntrebÄƒri). Ar trebui sÄƒ fiÈ›i obiÈ™nuiÈ›i cu acest lucru pÃ¢nÄƒ acum, dar Ã®nseamnÄƒ cÄƒ acest model nu este Ã®ncÄƒ pregÄƒtit pentru a fi utilizat È™i trebuie sÄƒ fie fine-tuned - bine cÄƒ suntem pe cale sÄƒ facem asta!

Pentru a putea trimite modelul nostru cÄƒtre Hub, va trebui sÄƒ ne conectÄƒm la Hugging Face. DacÄƒ executaÈ›i acest cod Ã®ntr-un notebook, puteÈ›i face acest lucru cu urmÄƒtoarea funcÈ›ie utilitarÄƒ, care afiÈ™eazÄƒ un widget Ã®n care puteÈ›i introduce datele voastre de autentificare:


```python
from huggingface_hub import notebook_login

notebook_login()
```

DacÄƒ nu lucraÈ›i Ã®ntr-un notebook, tastaÈ›i urmÄƒtoarea linie Ã®n terminal:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

OdatÄƒ fÄƒcut acest lucru, ne putem defini `TrainingArguments`. AÈ™a cum am spus atunci cÃ¢nd am definit funcÈ›ia noastrÄƒ pentru a calcula metrica, nu vom putea avea o buclÄƒ de evaluare obiÈ™nuitÄƒ din cauza parametrilor funcÈ›iei `compute_metrics()`. Am putea scrie propria noastrÄƒ subclasÄƒ a `Trainer` pentru a face acest lucru (o abordare pe care o puteÈ›i gÄƒsi Ã®n [scriptul exemplu pentru rÄƒspunderea la Ã®ntrebÄƒri](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)), dar este un pic prea lung pentru aceastÄƒ secÈ›iune. Ãn schimb, aici vom evalua modelul doar la sfÃ¢rÈ™itul antrenÄƒrii È™i vÄƒ vom arÄƒta cum sÄƒ efectuaÈ›i o evaluare obiÈ™nuitÄƒ Ã®n secÈ›iunea "O buclÄƒ de antrenare personalizatÄƒ" de mai jos.

Acesta este Ã®ntr-adevÄƒr locul Ã®n care API-ul `Trainer` Ã®È™i aratÄƒ limitele È™i biblioteca ğŸ¤— Accelerate strÄƒluceÈ™te: personalizarea clasei pentru un caz de utilizare specific poate fi greu de implementat, dar modificarea unei bucle de antrenare complet expuse este uÈ™oarÄƒ.

SÄƒ aruncÄƒm o privire la `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-squad",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)
```

Am mai vÄƒzut cele mai multe dintre acestea: stabilim niÈ™te hiperparametrii (cum ar fi learning rate, numÄƒrul de epoci pentru antrenament È™i o anumitÄƒ scÄƒdere a weighturilor) È™i indicÄƒm cÄƒ dorim sÄƒ salvÄƒm modelul la sfÃ¢rÈ™itul fiecÄƒrei epoci, sÄƒ sÄƒrim peste evaluare È™i sÄƒ Ã®ncÄƒrcÄƒm rezultatele noastre Ã®n Model Hub. De asemenea, activÄƒm antrenarea cu precizie mixtÄƒ cu `fp16=True`, deoarece aceasta poate accelera foarte mult antrenarea pe un GPU recent.

{:else}

Acum, putem crea dataseturile TF. De data aceasta, putem utiliza data collatorul de bazÄƒ:

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

È˜i acum creÄƒm dataseturile ca de obicei.

```python
tf_train_dataset = model.prepare_tf_dataset(
    train_dataset,
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)
tf_eval_dataset = model.prepare_tf_dataset(
    validation_dataset,
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

Ãn continuare, stabilim hiperparametrii de antrenament È™i compilÄƒm modelul nostru:

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# NumÄƒrul etapelor de antrenare este numÄƒrul de sampleuri din dataset, Ã®mpÄƒrÈ›it la dimensiunea batch-ului, apoi Ã®nmulÈ›it
# cu numÄƒrul total de epoci. ReÈ›ineÈ›i cÄƒ datasetul tf_train_dataset de aici este un batched tf.data.Dataset,
# nu datasetul original Hugging Face, deci len() este deja num_samples // batch_size.
num_train_epochs = 3
num_train_steps = len(tf_train_dataset) * num_train_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Antrenarea Ã®n mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Ãn cele din urmÄƒ, suntem gata sÄƒ ne antrenÄƒm cu `model.fit()`. Folosim un `PushToHubCallback` pentru a Ã®ncÄƒrca modelul Ã®n Hub dupÄƒ fiecare epocÄƒ.

{/if}

Ãn mod implicit, repositoriul utilizat va fi Ã®n namespaceul vostru È™i numit dupÄƒ foldrul de ieÈ™ire pe care l-aÈ›i stabilit, deci Ã®n cazul nostru va fi Ã®n `"sgugger/bert-finetuned-squad"`. Putem trece peste acest lucru prin trecerea unui `hub_model_id`; de exemplu, pentru a Ã®ncÄƒrca modelul Ã®n organizaÈ›ia `huggingface_course`, am folosit `hub_model_id="huggingface_course/bert-finetuned-squad"` (care este modelul la care am fÄƒcut legÄƒtura la Ã®nceputul acestei secÈ›iuni).


{#if fw === 'pt'}

> [!TIP]
> ğŸ’¡ DacÄƒ folderul de ieÈ™ire pe care Ã®l utilizaÈ›i existÄƒ, acesta trebuie sÄƒ fie o clonÄƒ localÄƒ a repositoriul Ã®n care doriÈ›i sÄƒ faceÈ›i push (deci setaÈ›i un nume nou dacÄƒ primiÈ›i o eroare la definirea `Trainer`).

Ãn cele din urmÄƒ, trecem totul Ã®n clasa `Trainer` È™i lansÄƒm antrenarea:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

{:else}

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-squad", tokenizer=tokenizer)

# Vom face validarea dupÄƒ aceea, deci nu vom face o validare Ã®n timpul antrenÄƒrii
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)
```

{/if}

ReÈ›ineÈ›i cÄƒ, Ã®n timpul antrenamentului, de fiecare datÄƒ cÃ¢nd modelul este salvat (aici, la fiecare epocÄƒ), acesta este Ã®ncÄƒrcat Ã®n Hub pe fundal. Ãn acest fel, veÈ›i putea sÄƒ reluaÈ›i antrenarea pe un alt device, dacÄƒ este necesar. Ãntreaga pregÄƒtire dureazÄƒ ceva timp (puÈ›in peste o orÄƒ pe un Titan RTX), aÈ™a cÄƒ puteÈ›i sÄƒ vÄƒ luaÈ›i o cafea sau sÄƒ recitiÈ›i unele dintre pÄƒrÈ›ile cursului care vi s-au pÄƒrut mai dificile Ã®n timp ce se desfÄƒÈ™oarÄƒ. De asemenea, reÈ›ineÈ›i cÄƒ, de Ã®ndatÄƒ ce se terminÄƒ prima epocÄƒ, veÈ›i vedea cÃ¢teva weighturu Ã®ncÄƒrcate Ã®n Hub È™i puteÈ›i Ã®ncepe sÄƒ vÄƒ jucaÈ›i cu modelul vostru pe pagina acestuia.

{#if fw === 'pt'}

OdatÄƒ ce antrenamentul este complet, putem Ã®n cele din urmÄƒ sÄƒ evaluÄƒm modelul (È™i sÄƒ ne rugÄƒm sÄƒ nu fi petrecut tot timpul de calcul degeaba). Metoda `predict()` a `Trainer` va returna un tuple Ã®n care primele elemente vor fi predicÈ›iile modelului (aici o pereche cu logiturile de Ã®nceput È™i de sfÃ¢rÈ™it). Trimitem acest rezultat funcÈ›iei noastre `compute_metrics()`:

```python
predictions, _, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets["validation"])
```

{:else}

OdatÄƒ ce antrenamentul este complet, putem Ã®n sfÃ¢rÈ™it sÄƒ ne evaluÄƒm modelul (È™i sÄƒ ne rugÄƒm sÄƒ nu fi cheltuit tot timpul de calcul degeaba). Metoda `predict()` a modelului nostru `model` se va ocupa de obÈ›inerea predicÈ›iilor È™i, deoarece am fÄƒcut toatÄƒ munca grea de definire a unei funcÈ›ii `compute_metrics()` mai devreme, putem obÈ›ine rezultatele noastre Ã®ntr-o singurÄƒ linie:

```python
predictions = model.predict(tf_eval_dataset)
compute_metrics(
    predictions["start_logits"],
    predictions["end_logits"],
    validation_dataset,
    raw_datasets["validation"],
)
```

{/if}

```python out
{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}
```

Super! Ca o comparaÈ›ie, scorurile de bazÄƒ raportate Ã®n articolul BERT pentru acest model sunt 80,8 È™i 88,5, deci suntem exact unde ar trebui sÄƒ fim.

{#if fw === 'pt'}

Ãn final, folosim metoda `push_to_hub()` pentru a ne asigura cÄƒ Ã®ncÄƒrcÄƒm cea mai recentÄƒ versiune a modelului:

```py
trainer.push_to_hub(commit_message="Training complete")
```

Aceasta returneazÄƒ URL-ul commit-ului pe care tocmai l-a fÄƒcut, dacÄƒ doriÈ›i sÄƒ Ã®l inspectaÈ›i:

```python out
'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68'
```

De asemenea, `Trainer` redacteazÄƒ un model card cu toate rezultatele evaluÄƒrii È™i o Ã®ncarcÄƒ.

{/if}

Ãn aceastÄƒ etapÄƒ, puteÈ›i utiliza widgetul de inferenÈ›Äƒ de pe Model Hub pentru a testa modelul È™i pentru a-l oferi prietenilor, familia È™i animalele de companie preferate. AÈ›i fÄƒcut fine-tune cu succes unui model pentru o sarcinÄƒ de rÄƒspundere a unei Ã®ntrebari - felicitÄƒri!

> [!TIP]
> âœï¸ **E rÃ¢ndul tÄƒu!** ÃncearcÄƒ un alt model de arhitecturÄƒ pentru a vedea dacÄƒ are performanÈ›e mai bune la aceastÄƒ sarcinÄƒ!

{#if fw === 'pt'}

DacÄƒ doriÈ›i sÄƒ pÄƒtrundeÈ›i puÈ›in mai adÃ¢nc Ã®n bucla de antrenare, vÄƒ vom arÄƒta acum cum sÄƒ faceÈ›i acelaÈ™i lucru folosind ğŸ¤— Accelerate.

## O buclÄƒ de antrenare personalizatÄƒ[[a-custom-training-loop]]

SÄƒ aruncÄƒm acum o privire la bucla de antrenare completÄƒ, astfel Ã®ncÃ¢t sÄƒ puteÈ›i personaliza cu uÈ™urinÈ›Äƒ pÄƒrÈ›ile de care aveÈ›i nevoie. Aceasta va semÄƒna foarte mult cu bucla de antrenare din [Capitolul 3](/course/chapter3/4), cu excepÈ›ia buclei de evaluare. Vom putea evalua modelul Ã®n mod regulat, deoarece nu mai suntem constrÃ¢nÈ™i de clasa `Trainer`.

### PregÄƒtirea pentru antrenament[[preparing-everything-for-training]]

Mai Ã®ntÃ¢i trebuie sÄƒ construim `DataLoader`s din dataseturile noastre. Am setat formatul acestor dataseturi la `"torch"` È™i am eliminat coloanele din setul de validare care nu sunt utilizate de model. Apoi, putem utiliza `default_data_collator` furnizat de Transformers ca un `collate_fn` È™i sÄƒ amestecÄƒm setul de antrenare, dar nu È™i setul de validare:

```py
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_dataset.set_format("torch")
validation_set = validation_dataset.remove_columns(["example_id", "offset_mapping"])
validation_set.set_format("torch")

train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=default_data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    validation_set, collate_fn=default_data_collator, batch_size=8
)
```

Ãn continuare, reiniÈ›ializÄƒm modelul, pentru a ne asigura cÄƒ nu continuÄƒm fine-tuningul de dinainte, ci pornim din nou de la modelul preantrenat BERT:

```py
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

Atunci vom avea nevoie de un optimizator. Ca de obicei, folosim clasicul `AdamW`, care este ca Adam, dar cu o corecÈ›ie Ã®n modul Ã®n care se aplicÄƒ scÄƒderea weighturilor:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

OdatÄƒ ce avem toate aceste obiecte, le putem trimite metodei `accelerator.prepare()`. AmintiÈ›i-vÄƒ cÄƒ, dacÄƒ doriÈ›i sÄƒ vÄƒ antrenaÈ›i pe un TPU Ã®ntr-un notebook Colab, va trebui sÄƒ mutaÈ›i tot acest cod Ã®ntr-o funcÈ›ie de antrenament, care nu ar trebui sÄƒ execute nicio celulÄƒ care iniÈ›ializeazÄƒ un `Accelerator`. Putem forÈ›a antrenarea cu precizie mixtÄƒ trecÃ¢nd `fp16=True` la `Accelerator` (sau, dacÄƒ executaÈ›i codul ca un script, asiguraÈ›i-vÄƒ cÄƒ completaÈ›i `config` Ã®n ğŸ¤— Accelerate Ã®n mod corespunzÄƒtor).

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

DupÄƒ cum ar trebui sÄƒ È™tiÈ›i din secÈ›iunile anterioare, putem utiliza lungimea `train_dataloader` pentru a calcula numÄƒrul de paÈ™i de antrenare numai dupÄƒ ce a trecut prin metoda `accelerator.prepare()`. UtilizÄƒm acelaÈ™i program liniar ca Ã®n secÈ›iunile anterioare:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Pentru a trimite modelul nostru cÄƒtre Hub, va trebui sÄƒ creÄƒm un obiect `Repository` Ã®ntr-un folder de lucru. Ãn primul rÃ¢nd, conectaÈ›i-vÄƒ la Hugging Face Hub, dacÄƒ nu sunteÈ›i deja conectat. Vom determina numele repositoriul pornind de la ID-ul modelului pe care dorim sÄƒ Ã®l atribuim modelului nostru (nu ezitaÈ›i sÄƒ Ã®nlocuiÈ›i `repo_name` cu propria alegere; acesta trebuie doar sÄƒ conÈ›inÄƒ numele vostru de utilizator, ceea ce face funcÈ›ia `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-squad-accelerate'
```

Apoi putem clona acel repositoriu Ã®ntr-un folder local. DacÄƒ existÄƒ deja, acest folder local ar trebui sÄƒ fie o clonÄƒ a repositoriului cu care lucrÄƒm:

```py
output_dir = "bert-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Acum putem Ã®ncÄƒrca orice salvÄƒm Ã®n `output_dir` prin apelarea metodei `repo.push_to_hub()`. Acest lucru ne va ajuta sÄƒ Ã®ncÄƒrcÄƒm modelele intermediare la sfÃ¢rÈ™itul fiecÄƒrei epoci.

## Bucla de antrenare[[training-loop]]

Acum suntem pregÄƒtiÈ›i sÄƒ scriem bucla de antrenare completÄƒ. DupÄƒ definirea unei bare de progres pentru a urmÄƒri modul Ã®n care decurge antrenamentul, bucla are trei pÄƒrÈ›i:

- PregÄƒtirea Ã®n sine, care este iteraÈ›ia clasicÄƒ peste `train_dataloader`, trecerea Ã®nainte prin model, apoi trecerea Ã®napoi È™i pasul optimizatorului.
- Evaluarea, Ã®n care adunÄƒm toate valorile pentru `start_logits` È™i `end_logits` Ã®nainte de a le converti Ã®n matrici NumPy. OdatÄƒ ce bucla de evaluare este terminatÄƒ, concatenÄƒm toate rezultatele. ReÈ›ineÈ›i cÄƒ trebuie sÄƒ truncÄƒm deoarece `Accelerator` ar fi putut adÄƒuga cÃ¢teva exemple la sfÃ¢rÈ™it pentru a ne asigura cÄƒ avem acelaÈ™i numÄƒr de exemple Ã®n fiecare proces.
- Salvarea È™i Ã®ncÄƒrcarea, unde mai Ã®ntÃ¢i salvÄƒm modelul È™i tokenizatorul, apoi apelÄƒm `repo.push_to_hub()`. Ca È™i Ã®nainte, folosim argumentul `blocking=False` pentru a spune bibliotecii ğŸ¤— Hub sÄƒ efectueze push-ul Ã®ntr-un proces asincron. Ãn acest fel, antrenamentul continuÄƒ normal, iar aceastÄƒ instrucÈ›iune (lungÄƒ) este executatÄƒ pe fundal.

IatÄƒ codul complet pentru bucla de antrenare:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    start_logits = []
    end_logits = []
    accelerator.print("Evaluation!")
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())
        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())

    start_logits = np.concatenate(start_logits)
    end_logits = np.concatenate(end_logits)
    start_logits = start_logits[: len(validation_dataset)]
    end_logits = end_logits[: len(validation_dataset)]

    metrics = compute_metrics(
        start_logits, end_logits, validation_dataset, raw_datasets["validation"]
    )
    print(f"epoch {epoch}:", metrics)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

Ãn cazul Ã®n care este prima datÄƒ cÃ¢nd vedeÈ›i un model salvat cu ğŸ¤— Accelerate, sÄƒ ne oprim puÈ›in pentru a inspecta cele trei linii de cod care Ã®l Ã®nsoÈ›esc:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

Prima linie se explicÄƒ de la sine: aceasta spune tuturor proceselor sÄƒ aÈ™tepte pÃ¢nÄƒ cÃ¢nd toatÄƒ lumea se aflÄƒ Ã®n etapa respectivÄƒ Ã®nainte de a continua. Acest lucru are rolul de a ne asigura cÄƒ avem acelaÈ™i model Ã®n fiecare proces Ã®nainte de salvare. Apoi luÄƒm `unwrapped_model`, care este modelul de bazÄƒ pe care l-am definit. Metoda `accelerator.prepare()` modificÄƒ modelul pentru a funcÈ›iona Ã®n antrenarea distribuitÄƒ, deci nu va mai avea metoda `save_pretrained()`; metoda `accelerator.unwrap_model()` anuleazÄƒ acest pas. Ãn cele din urmÄƒ, apelÄƒm metoda `save_pretrained()`, dar Ã®i spunem sÄƒ foloseascÄƒ metoda `accelerator.save()` Ã®n loc de `torch.save()`.

OdatÄƒ fÄƒcut acest lucru, ar trebui sÄƒ aveÈ›i un model care produce rezultate destul de asemÄƒnÄƒtoare cu cel antrenat cu `Trainer`. PuteÈ›i verifica modelul pe care l-am antrenat folosind acest cod pe [*huggingface-course/bert-finetuned-squad-accelerate*] (https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate). È˜i dacÄƒ doriÈ›i sÄƒ testaÈ›i orice modificÄƒri ale buclei de antrenare, le puteÈ›i implementa direct prin editarea codului prezentat mai sus!

{/if}

## Utilizarea modelului fine-tuned[[using-the-fine-tuned-model]]

V-am arÄƒtat deja cum puteÈ›i utiliza modelul cÄƒruia i-am fÄƒcut fine-tune pe Model Hub cu widgetul de inferenÈ›Äƒ. Pentru a-l utiliza local Ã®ntr-un `pipeline`, trebuie doar sÄƒ specificaÈ›i identificatorul modelului:

```py
from transformers import pipeline

# ÃnlocuiÈ›i acest checkpoint cu propriul checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-squad"
question_answerer = pipeline("question-answering", model=model_checkpoint)

context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.9979003071784973,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Grozav! Modelul nostru funcÈ›ioneazÄƒ la fel de bine ca cel implicit pentru aceast pipeline!
