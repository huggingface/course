# Introducere[[introducere]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

## Bun venit la ğŸ¤— curs![[bun-venit-la-curs]]

<Youtube id="00GKzGyWFEs" />

Acest curs are scopul de a vÄƒ Ã®nvÄƒÈ›a despre procesarea limbajelor naturale (NLP) folosind biblioteci din ecosistemul [Hugging Face](https://huggingface.co/) â€” [ğŸ¤— Transformers](https://github.com/huggingface/transformers), [ğŸ¤— Datasets](https://github.com/huggingface/datasets), [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers), È™i [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate) â€” precum È™i [Hugging Face Hub](https://huggingface.co/models). Cursul este complet gratuit È™i nu conÈ›ine reclame.

## ÃnÈ›elegerea NLP È™i a LLM-urilor[[Ã®nÈ›elegerea-nlp-È™i-a-llm-urilor]]

DeÈ™i acest curs a fost iniÈ›ial axat pe NLP (Procesarea Limbajului Natural), el a evoluat pentru a pune accentul pe Large Language Models (LLM-uri), care reprezintÄƒ cele mai recente progrese din domeniu.

**Care este diferenÈ›a?**
- **NLP (Procesarea Limbajului Natural)** este domeniul mai larg care se concentreazÄƒ pe permiterea computerelor sÄƒ Ã®nÈ›eleagÄƒ, interpreteze È™i sÄƒ genereze limbajul uman. NLP include multe tehnici È™i sarcini, cum ar fi analiza sentimentelor, recunoaÈ™terea entitÄƒÈ›ilor numite È™i traducerea automatÄƒ.
- **LLM-uri (Modele Mari de Limbaj)** sunt un subset puternic al modelelor NLP, caracterizate prin dimensiuni masive, volume mari de date de antrenament È™i abilitatea de a Ã®ndeplini o gamÄƒ largÄƒ de sarcini lingvistice cu un antrenament specific minim. Modele precum seriile Llama, GPT sau Claude sunt exemple de LLM-uri care au revoluÈ›ionat ceea ce este posibil Ã®n domeniul NLP.

Pe parcursul acestui curs, vei Ã®nvÄƒÈ›a atÃ¢t concepte tradiÈ›ionale din NLP, cÃ¢t È™i tehnici de ultimÄƒ generaÈ›ie legate de LLM-uri, deoarece Ã®nÈ›elegerea fundamentelor NLP este esenÈ›ialÄƒ pentru a lucra eficient cu LLM-uri.

## La ce sÄƒ te aÈ™tepÈ›i?[[la-ce-sa-te-aÈ™tepÈ›i]]

Aceasta este o scurtÄƒ prezentare a cursului:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- Capitolele 1-4 oferÄƒ o introducere Ã®n conceptele fundamentale ale bibliotecii ğŸ¤— Transformers. PÃ¢nÄƒ la finalul acestei pÄƒrÈ›i a cursului, veÈ›i fi familiarizaÈ›i cu modul Ã®n care funcÈ›ioneazÄƒ modelele Transformer È™i veÈ›i È™ti cum sÄƒ utilizaÈ›i un model din [Hugging Face Hub](https://huggingface.co/models), sÄƒ Ã®l ajustaÈ›i pe un set de date È™i sÄƒ vÄƒ partajaÈ›i rezultatele pe Hub!
- Capitolele 5-8 predau elementele de bazÄƒ ale Datasets ğŸ¤— È™i ale Tokenizatoarelor ğŸ¤— Ã®nainte de a vÄƒ scufunda Ã®n sarcinile NLP. PÃ¢nÄƒ la sfÃ¢rÈ™itul acestei pÄƒrÈ›i, veÈ›i fi capabil sÄƒ abordaÈ›i singur cele mai frecvente probleme NLP.
- Capitolele 9-12 trec dincolo de NLP È™i exploreazÄƒ modul Ã®n care modelele Transformer pot fi utilizate pentru a aborda sarcini din domeniul procesÄƒrii semnalelor vorbirii È™i computer vision. Pe parcurs, veÈ›i Ã®nvÄƒÈ›a cum sÄƒ construiÈ›i È™i sÄƒ partajaÈ›i demo-uri ale modelelor dumneavoastrÄƒ È™i cum sÄƒ le optimizaÈ›i pentru mediul de producÈ›ie. PÃ¢nÄƒ la finalul acestei pÄƒrÈ›i, veÈ›i fi gata sÄƒ aplicaÈ›i ğŸ¤— Transformers la (aproape) orice problemÄƒ de machine learning!

Acest curs:

* NecesitÄƒ o bunÄƒ cunoaÈ™tere a limbajului Python.
* Este recomandat sÄƒ fie parcurs dupÄƒ un curs introductiv de deep learning, cum ar fi [fast.ai's](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) sau unul dintre cursurile oferite de [DeepLearning.AI](https://www.deeplearning.ai/).
* Nu se aÈ™teaptÄƒ la cunoÈ™tinÈ›e anterioare despre [PyTorch](https://pytorch.org/) sau [TensorFlow](https://www.tensorflow.org/), deÈ™i o familiaritate cu oricare dintre acestea va fi de ajutor.

DupÄƒ ce aÈ›i completat acest curs, vÄƒ recomandÄƒm sÄƒ accesaÈ›i [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh) de la DeepLearning.AI, care acoperÄƒ o gamÄƒ largÄƒ de modele NLP clasice, cum ar fi naive Bayes È™i LSTMs, despre care este bine sÄƒ È™tiÈ›i!

## Cine suntem noi?[[cine-suntem-noi]]

Despre autori:

[**Abubakar Abid**](https://huggingface.co/abidlabs) È™i-a susÈ›inut doctoratul la Stanford, Ã®n domeniul Applied Machine Learning. Ãn timpul doctoratului sÄƒu, a fondat [Gradio](https://github.com/gradio-app/gradio), o bibliotecÄƒ Python open-source care a fost utilizatÄƒ pentru a construi peste 600.000 de demo-uri de Machine Learning. Gradio a fost achiziÈ›ionatÄƒ de Hugging Face, unde Abubakar activeazÄƒ acum ca lider al echipei de Machine Learning.

[**Ben Burtenshaw**](https://huggingface.co/burtenshaw) este Machine Learning Engineer la Hugging Face. A obÈ›inut doctoratul Ã®n Procesarea Limbajului Natural la Universitatea din Antwerp, unde a aplicat modele Transformer pentru a genera poveÈ™ti pentru copii cu scopul de a Ã®mbunÄƒtÄƒÈ›i abilitÄƒÈ›ile de literaÈ›ie. De atunci, s-a concentrat pe materiale educaÈ›ionale È™i instrumente pentru comunitatea largÄƒ.

[**Matthew Carrigan**](https://huggingface.co/Rocketknight1) este inginer de Machine Learning la Hugging Face. LocuieÈ™te Ã®n Dublin, Irlanda È™i anterior a fost inginer ML la Parse.ly È™i Ã®nainte de asta cercetÄƒtor postdoctoral la Trinity College Dublin. El considerÄƒ cÄƒ atingerea AGI nu se va realiza prin scalarea arhitecturilor existente, dar este optimist Ã®n legÄƒturÄƒ cu viitorul roboÈ›ilor.

[**Lysandre Debut**](https://huggingface.co/lysandre) este inginer de Machine Learning la Hugging Face È™i a lucrat la biblioteca ğŸ¤— Transformers Ã®ncÄƒ din primele etape de dezvoltare. Scopul sÄƒu este de a face NLP accesibil pentru toatÄƒ lumea prin dezvoltarea de instrumente cu un API foarte simplu.

[**Sylvain Gugger**](https://huggingface.co/sgugger) este inginer de cercetare Ã®n Machine Learning la Hugging Face È™i unul dintre principalii Ã®ntreÈ›inÄƒtori ai bibliotecii ğŸ¤— Transformers. Anterior a fost cercetÄƒtor È™tiinÈ›ific la fast.ai È™i a fost coautor la _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ cu Jeremy Howard. Obiectivul principal al cercetÄƒrii sale este de a face Ã®nvÄƒÈ›area profundÄƒ mai accesibilÄƒ, prin proiectarea È™i Ã®mbunÄƒtÄƒÈ›irea tehnicilor care permit modelelor sÄƒ se antreneze rapid pe resurse limitate.

[**Dawood Khan**](https://huggingface.co/dawoodkhan82) este inginer de Machine Learning la Hugging Face. Este originar din New York È™i a absolvit Universitatea din New York, unde a studiat Informatica. DupÄƒ ce a lucrat ca inginer iOS timp de cÃ¢È›iva ani, Dawood a renunÈ›at la poziÈ›ia sa pentru a Ã®nfiinÈ›a Gradio Ã®mpreunÄƒ cu colegii sÄƒi co-fondatori. Gradio a fost Ã®n cele din urmÄƒ achiziÈ›ionat de Hugging Face.

[**Merve Noyan**](https://huggingface.co/merve) este un susÈ›inÄƒtor al dezvoltatorilor la Hugging Face, lucrÃ¢nd la dezvoltarea de instrumente È™i la crearea de conÈ›inut Ã®n jurul acestora pentru a facilita Ã®nvÄƒÈ›area automatÄƒ pentru toatÄƒ lumea. 

[**Lucile Saulnier**](https://huggingface.co/SaulLu) este inginer de Machine Learning la Hugging Face, dezvoltÃ¢nd È™i susÈ›inÃ¢nd utilizarea de instrumente open source. De asemenea, este implicatÄƒ activ Ã®n multe proiecte de cercetare Ã®n domeniul procesÄƒrii limbajului natural, cum ar fi formarea colaborativÄƒ È™i BigScience.

[**Lewis Tunstall**](https://huggingface.co/lewtun) este inginer de Machine Learning la Hugging Face, concentrÃ¢ndu-se pe dezvoltarea de instrumente open-source È™i pe asigurarea accesibilitÄƒÈ›ii acestora pentru Ã®ntreaga comunitate. De asemenea, este coautor al cÄƒrÈ›ii O'Reilly [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).

[**Leandro von Werra**](https://huggingface.co/lvwerra) este inginer de Machine Learning Ã®n cadrul echipei open-source de la Hugging Face È™i, de asemenea, coautor al cÄƒrÈ›ii O'Reilly [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). El are mai mulÈ›i ani de experienÈ›Äƒ Ã®n industrie, aducÃ¢nd proiecte NLP Ã®n stadiul de producÈ›ie, lucrÃ¢nd pe Ã®ntreaga stivÄƒ de Ã®nvÄƒÈ›are automatÄƒ.

## FAQ[[faq]]

IatÄƒ cÃ¢teva rÄƒspunsuri la Ã®ntrebÄƒri frecvente:

- **Acest curs permite obÈ›inerea unei certificÄƒri?**
Momentan nu avem nicio certificare pentru acest curs. Cu toate acestea, lucrÄƒm la un program de certificare pentru ecosistemul Hugging Face - rÄƒmÃ¢neÈ›i pe fazÄƒ!

- **CÃ¢t timp ar trebui sÄƒ dedic acestui curs?**
Fiecare capitol al acestui curs este gÃ¢ndit sÄƒ fie parcurs Ã®ntr-o sÄƒptÄƒmÃ¢nÄƒ, necesitÃ¢nd aproximativ 6â€“8 ore de lucru. TotuÈ™i, puteÈ›i avansa Ã®n ritmul propriu È™i finaliza cursul Ã®n timpul care vi se potriveÈ™te cel mai bine.

- **Unde pot sÄƒ pun o Ã®ntrebare dacÄƒ am una?**
DacÄƒ aveÈ›i o Ã®ntrebare legatÄƒ de orice secÈ›iune a cursului, faceÈ›i clic pe bannerul â€Pune o Ã®ntrebareâ€ din partea de sus a paginii pentru a fi redirecÈ›ionat automat cÄƒtre secÈ›iunea corespunzÄƒtoare din [forumurile Hugging Face](https://discuss.huggingface.co/):

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png" alt="Link to the Hugging Face forums" width="75%">

De asemenea, o listÄƒ de [idei de proiecte](https://discuss.huggingface.co/c/course/course-event/25) este disponibilÄƒ pe forumuri, Ã®n cazul Ã®n care doriÈ›i sÄƒ exersaÈ›i mai mult dupÄƒ ce aÈ›i terminat cursul.

- **De unde pot obÈ›ine codul pentru curs?**
Pentru fiecare secÈ›iune, faceÈ›i clic pe bannerul din partea de sus a paginii pentru a rula codul Ã®n Google Colab sau Amazon SageMaker Studio Lab:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png" alt="Link to the Hugging Face course notebooks" width="75%">

Notebook-urile Jupyter care conÈ›in Ã®ntregul cod al cursului sunt gÄƒzduite Ã®n repo-ul [`huggingface/notebooks`](https://github.com/huggingface/notebooks). DacÄƒ doriÈ›i sÄƒ le generaÈ›i local, consultaÈ›i instrucÈ›iunile din repo-ul [`course`](https://github.com/huggingface/course#-jupyter-notebooks) de pe GitHub.


- **Cum pot contribui la curs?**
ExistÄƒ multe modalitÄƒÈ›i de a contribui la curs! DacÄƒ gÄƒsiÈ›i o greÈ™ealÄƒ de tipar sau o eroare, vÄƒ rugÄƒm sÄƒ creaÈ›i o cerere Ã®n repo-ul [`course`](https://github.com/huggingface/course). DacÄƒ doriÈ›i sÄƒ ajutaÈ›i la traducerea cursului Ã®n limba dumneavoastrÄƒ maternÄƒ, consultaÈ›i instrucÈ›iunile [aici](https://github.com/huggingface/course#translating-the-course-into-your-language).

- **Care au fost alegerile fÄƒcute pentru fiecare traducere?**
Fiecare traducere are un glosar È™i un fiÈ™ier `TRANSLATING.txt` care detaliazÄƒ alegerile care au fost fÄƒcute pentru jargonul de Machine Learning etc. PuteÈ›i gÄƒsi un exemplu Ã®n limba germanÄƒ [aici](https://github.com/huggingface/course/blob/main/chapters/de/TRANSLATING.txt).


- **Pot reutiliza acest curs?**
Desigur! Cursul este distribuit sub licenÈ›a [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0.html), o licenÈ›Äƒ permisivÄƒ. Aceasta permite reutilizarea materialului cu condiÈ›ia sÄƒ acordaÈ›i credit autorilor, sÄƒ includeÈ›i un link cÄƒtre licenÈ›Äƒ È™i sÄƒ menÈ›ionaÈ›i eventualele modificÄƒri efectuate. Acest lucru poate fi realizat Ã®n orice mod rezonabil, atÃ¢ta timp cÃ¢t nu implicaÈ›i Ã®n mod fals cÄƒ licenÈ›iatorul susÈ›ine Ã®n mod explicit persoana sau utilizarea dvs. DacÄƒ doriÈ›i sÄƒ citaÈ›i acest curs, vÄƒ rugÄƒm sÄƒ folosiÈ›i urmÄƒtorul formatBibTeX:

```
@misc{huggingfacecourse,
  author = {Hugging Face},
  title = {The Hugging Face Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year = {2022},
  note = "[Online; accessed <today>]"
}
```

## SÄƒ Ã®ncepem!
SunteÈ›i gata sÄƒ Ã®ncepeÈ›i? Ãn acest capitol, veÈ›i Ã®nvÄƒÈ›a:

* Cum sÄƒ utilizaÈ›i funcÈ›ia `pipeline()` pentru a rezolva sarcini NLP precum generarea È™i clasificarea textului
* Despre arhitectura Transformer
* Cum sÄƒ faceÈ›i distincÈ›ia Ã®ntre arhitecturile È™i cazurile de utilizare ale encoderului, decoderului È™i encoder-decoder.
