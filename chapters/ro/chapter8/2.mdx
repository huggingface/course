# Ce sÄƒ faci cÃ¢nd primeÈ™ti o eroare[[ce-sa-fac-cand-primesti-o-eroare]]

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
]} />

Ãn aceastÄƒ secÈ›iune vom examina cÃ¢teva erori comune care pot apÄƒrea cÃ¢nd Ã®ncercaÈ›i sÄƒ generaÈ›i predicÈ›ii din modelul Transformer pe care l-aÈ›i ajustat recent. Aceasta vÄƒ va pregÄƒti pentru [secÈ›iunea 4](/course/chapter8/section4), unde vom explora cum sÄƒ depanaÈ›i faza de antrenament Ã®n sine.

<Youtube id="DQ-CpJn6Rc4"/>

Am pregÄƒtit un [repository model È™ablon](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) pentru aceastÄƒ secÈ›iune, È™i dacÄƒ doriÈ›i sÄƒ rulaÈ›i codul din acest capitol trebuie mai Ã®ntÃ¢i sÄƒ copiaÈ›i modelul Ã®n contul dumneavoastrÄƒ pe [Hugging Face Hub](https://huggingface.co). Pentru a face acest lucru, mai Ã®ntÃ¢i conectaÈ›i-vÄƒ rulÃ¢nd fie urmÄƒtoarele Ã®ntr-un notebook Jupyter:

```python
from huggingface_hub import notebook_login

notebook_login()
```

sau urmÄƒtoarele Ã®n terminalul dumneavoastrÄƒ preferat:

```bash
huggingface-cli login
```

Aceasta vÄƒ va solicita sÄƒ introduceÈ›i numele de utilizator È™i parola È™i va salva un token sub *~/.cache/huggingface/*. OdatÄƒ ce v-aÈ›i conectat, puteÈ›i copia repository-ul È™ablon cu urmÄƒtoarea funcÈ›ie:

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Clone the repo and extract the local path
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Create an empty repo on the Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Clone the empty repo
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copy files
    copy_tree(template_repo_dir, new_repo_dir)
    # Push to Hub
    repo.push_to_hub()
```

Acum cÃ¢nd apelaÈ›i `copy_repository_template()`, aceasta va crea o copie a repository-ului È™ablon sub contul dumneavoastrÄƒ.

## Depanarea pipeline-ului din ğŸ¤— Transformers[[depanarea-pipeline-ului-din-transformers]]

Pentru a Ã®ncepe cÄƒlÄƒtoria noastrÄƒ Ã®n lumea minunatÄƒ a depanÄƒrii modelelor Transformer, consideraÈ›i urmÄƒtorul scenariu: lucraÈ›i cu un coleg la un proiect de rÄƒspuns la Ã®ntrebÄƒri pentru a ajuta clienÈ›ii unui site web de e-commerce sÄƒ gÄƒseascÄƒ rÄƒspunsuri despre produsele de consum. Colegul dumneavoastrÄƒ vÄƒ trimite un mesaj precum:

> Salut! Tocmai am rulat un experiment folosind tehnicile din [Capitolul 7](/course/chapter7/7) al cursului Hugging Face È™i am obÈ›inut rezultate grozave pe SQuAD! Cred cÄƒ putem folosi acest model ca punct de plecare pentru proiectul nostru. ID-ul modelului pe Hub este "lewtun/distillbert-base-uncased-finetuned-squad-d5716d28". Simte-te liber sÄƒ Ã®l testezi :)

Iar primul lucru la care vÄƒ gÃ¢ndiÈ›i este sÄƒ Ã®ncÄƒrcaÈ›i modelul folosind `pipeline` din ğŸ¤— Transformers:

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Oh nu, se pare cÄƒ ceva a mers prost! DacÄƒ sunteÈ›i nou Ã®n programare, acest tip de erori pot pÄƒrea puÈ›in criptice la Ã®nceput (ce este un `OSError`?!). Eroarea afiÈ™atÄƒ aici este doar ultima parte dintr-un raport de eroare mult mai mare numit _Python traceback_ (cunoscut È™i ca stack trace). De exemplu, dacÄƒ rulaÈ›i acest cod pe Google Colab, ar trebui sÄƒ vedeÈ›i ceva similar cu urmÄƒtoarea capturÄƒ de ecran:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png" alt="Un traceback Python." width="100%"/>
</div>

ExistÄƒ multe informaÈ›ii conÈ›inute Ã®n aceste rapoarte, aÈ™a cÄƒ sÄƒ parcurgem Ã®mpreunÄƒ pÄƒrÈ›ile cheie. Primul lucru de reÈ›inut este cÄƒ traceback-urile ar trebui citite _de jos Ã®n sus_. Aceasta poate pÄƒrea ciudat dacÄƒ sunteÈ›i obiÈ™nuiÈ›i sÄƒ citiÈ›i textul Ã®n englezÄƒ de sus Ã®n jos, dar reflectÄƒ faptul cÄƒ traceback-ul aratÄƒ secvenÈ›a de apeluri de funcÈ›ii pe care `pipeline` le face cÃ¢nd descarcÄƒ modelul È™i tokenizer-ul. (ConsultaÈ›i [Capitolul 2](/course/chapter2) pentru mai multe detalii despre cum funcÈ›ioneazÄƒ `pipeline` Ã®n culise.)

> [!TIP]
> ğŸš¨ VedeÈ›i acea casetÄƒ albastrÄƒ din jurul "6 frames" Ã®n traceback-ul din Google Colab? Aceasta este o caracteristicÄƒ specialÄƒ a Colab, care comprimÄƒ traceback-ul Ã®n "frame-uri". DacÄƒ nu reuÈ™iÈ›i sÄƒ gÄƒsiÈ›i sursa unei erori, asiguraÈ›i-vÄƒ cÄƒ extindeÈ›i traceback-ul complet fÄƒcÃ¢nd clic pe acele douÄƒ sÄƒgeÈ›i mici.

Aceasta Ã®nseamnÄƒ cÄƒ ultima linie a traceback-ului indicÄƒ ultimul mesaj de eroare È™i dÄƒ numele excepÈ›iei care a fost ridicatÄƒ. Ãn acest caz, tipul excepÈ›iei este `OSError`, care indicÄƒ o eroare legatÄƒ de sistem. DacÄƒ citim mesajul de eroare Ã®nsoÈ›itor, putem vedea cÄƒ pare sÄƒ existe o problemÄƒ cu fiÈ™ierul *config.json* al modelului È™i ni se dau douÄƒ sugestii pentru a o rezolva:

```python out
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

> [!TIP]
> ğŸ’¡ DacÄƒ Ã®ntÃ¢lniÈ›i un mesaj de eroare care este dificil de Ã®nÈ›eles, doar copiaÈ›i È™i lipiÈ›i mesajul Ã®n bara de cÄƒutare Google sau [Stack Overflow](https://stackoverflow.com/) (da, chiar!). ExistÄƒ o È™ansÄƒ bunÄƒ cÄƒ nu sunteÈ›i prima persoanÄƒ care Ã®ntÃ¢lneÈ™te eroarea, È™i aceasta este o modalitate bunÄƒ de a gÄƒsi soluÈ›ii pe care alÈ›ii din comunitate le-au postat. De exemplu, cÄƒutarea pentru `OSError: Can't load config for` pe Stack Overflow dÄƒ mai multe [rezultate](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+) care ar putea fi folosite ca punct de plecare pentru rezolvarea problemei.

Prima sugestie ne cere sÄƒ verificÄƒm dacÄƒ ID-ul modelului este Ã®ntr-adevÄƒr corect, aÈ™a cÄƒ primul lucru de fÄƒcut este sÄƒ copiem identificatorul È™i sÄƒ Ã®l lipim Ã®n bara de cÄƒutare a Hub-ului:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png" alt="Numele greÈ™it al modelului." width="100%"/>
</div>

Hmm, Ã®ntr-adevÄƒr pare cÄƒ modelul colegului nostru nu este pe Hub... aha, dar existÄƒ o greÈ™ealÄƒ de tipar Ã®n numele modelului! DistilBERT are doar un "l" Ã®n numele sÄƒu, aÈ™a cÄƒ sÄƒ corectÄƒm asta È™i sÄƒ cÄƒutÄƒm "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28" Ã®n schimb:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png" alt="Numele corect al modelului." width="100%"/>
</div>

Bun, aceasta a dat un rezultat. Acum sÄƒ Ã®ncercÄƒm sÄƒ descÄƒrcÄƒm din nou modelul cu ID-ul corect:

```python
model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Argh, eÈ™uat din nou -- bun venit Ã®n viaÈ›a de zi cu zi a unui inginer de machine learning! Deoarece am corectat ID-ul modelului, problema trebuie sÄƒ fie Ã®n repository Ã®n sine. O modalitate rapidÄƒ de a accesa conÈ›inutul unui repository pe ğŸ¤— Hub este prin funcÈ›ia `list_repo_files()` din biblioteca `huggingface_hub`:

```python
from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)
```

```python out
['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']
```

Interesant -- nu pare sÄƒ existe un fiÈ™ier *config.json* Ã®n repository! Nu e de mirare cÄƒ `pipeline`-ul nostru nu a putut Ã®ncÄƒrca modelul; colegul nostru trebuie sÄƒ fi uitat sÄƒ Ã®mpingÄƒ acest fiÈ™ier pe Hub dupÄƒ ce l-a ajustat fin. Ãn acest caz, problema pare destul de simplÄƒ de rezolvat: am putea sÄƒ Ã®i cerem sÄƒ adauge fiÈ™ierul, sau, deoarece putem vedea din ID-ul modelului cÄƒ modelul preantrenat folosit a fost [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased), putem descÄƒrca configuraÈ›ia pentru acest model È™i sÄƒ o Ã®mpingem Ã®n repository-ul nostru pentru a vedea dacÄƒ aceasta rezolvÄƒ problema. SÄƒ Ã®ncercÄƒm asta. Folosind tehnicile pe care le-am Ã®nvÄƒÈ›at Ã®n [Capitolul 2](/course/chapter2), putem descÄƒrca configuraÈ›ia modelului cu clasa `AutoConfig`:

```python
from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)
```

> [!WARNING]
> ğŸš¨ Abordarea pe care o adoptÄƒm aici nu este infailibilÄƒ, deoarece colegul nostru poate sÄƒ fi modificat configuraÈ›ia `distilbert-base-uncased` Ã®nainte de a ajusta fin modelul. Ãn viaÈ›a realÄƒ, am vrea sÄƒ verificÄƒm cu ei mai Ã®ntÃ¢i, dar Ã®n scopurile acestei secÈ›iuni vom presupune cÄƒ au folosit configuraÈ›ia implicitÄƒ.

Apoi putem Ã®mpinge aceasta Ã®n repository-ul nostru de model cu funcÈ›ia `push_to_hub()` a configuraÈ›iei:

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

Acum putem testa dacÄƒ aceasta a funcÈ›ionat Ã®ncÄƒrcÃ¢nd modelul din cel mai recent commit pe ramura `main`:

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

ğŸ¤— Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)
```

```python out
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}
```

Woohoo, a funcÈ›ionat! SÄƒ recapitulÄƒm ce aÈ›i Ã®nvÄƒÈ›at tocmai:

- Mesajele de eroare Ã®n Python sunt cunoscute ca _traceback-uri_ È™i sunt citite de jos Ã®n sus. Ultima linie a mesajului de eroare conÈ›ine de obicei informaÈ›iile de care aveÈ›i nevoie pentru a localiza sursa problemei.
- DacÄƒ ultima linie nu conÈ›ine informaÈ›ii suficiente, parcurgeÈ›i traceback-ul Ã®n sus È™i vedeÈ›i dacÄƒ puteÈ›i identifica unde Ã®n codul sursÄƒ apare eroarea.
- DacÄƒ niciunul dintre mesajele de eroare nu vÄƒ poate ajuta sÄƒ depanaÈ›i problema, Ã®ncercaÈ›i sÄƒ cÄƒutaÈ›i online o soluÈ›ie la o problemÄƒ similarÄƒ.
- Biblioteca `huggingface_hub` oferÄƒ o suitÄƒ de instrumente pe care le puteÈ›i folosi pentru a interacÈ›iona cu È™i a depana repository-urile de pe Hub.

Acum cÄƒ È™tiÈ›i cum sÄƒ depanaÈ›i un pipeline, sÄƒ aruncÄƒm o privire la un exemplu mai complicat Ã®n forward pass-ul modelului Ã®n sine.

## Depanarea forward pass-ului modelului dumneavoastrÄƒ[[depanarea-forward-pass-ului-modelului-dumneavoastra]]

DeÈ™i `pipeline` este grozav pentru majoritatea aplicaÈ›iilor unde aveÈ›i nevoie sÄƒ generaÈ›i rapid predicÈ›ii, uneori va trebui sÄƒ accesaÈ›i logit-urile modelului (sÄƒ zicem, dacÄƒ aveÈ›i o post-procesare personalizatÄƒ pe care aÈ›i dori sÄƒ o aplicaÈ›i). Pentru a vedea ce poate merge prost Ã®n acest caz, sÄƒ extragem mai Ã®ntÃ¢i modelul È™i tokenizer-ul din `pipeline`-ul nostru:

```python
tokenizer = reader.tokenizer
model = reader.model
```

Apoi avem nevoie de o Ã®ntrebare, aÈ™a cÄƒ sÄƒ vedem dacÄƒ framework-urile noastre preferate sunt suportate:

```python
question = "Which frameworks can I use?"
```

AÈ™a cum am vÄƒzut Ã®n [Capitolul 7](/course/chapter7), paÈ™ii obiÈ™nuiÈ›i pe care trebuie sÄƒ Ã®i facem sunt tokenizarea intrÄƒrilor, extragerea logit-urilor token-urilor de Ã®nceput È™i sfÃ¢rÈ™it, È™i apoi decodarea span-ului de rÄƒspuns:

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Oh nu, se pare cÄƒ avem o eroare Ã®n codul nostru! Dar nu ne temem de puÈ›inÄƒ depanare. PuteÈ›i folosi debugger-ul Python Ã®ntr-un notebook:

<Youtube id="rSPyvPw0p9k"/>

sau Ã®ntr-un terminal:

<Youtube id="5PkZ4rbHL6c"/>

Aici, citirea mesajului de eroare ne spune cÄƒ `'list' object has no attribute 'size'`, È™i putem vedea o sÄƒgeatÄƒ `-->` care indicÄƒ linia unde a fost ridicatÄƒ problema Ã®n `model(**inputs)`. PuteÈ›i depana aceasta interactiv folosind debugger-ul Python, dar pentru moment vom afiÈ™a pur È™i simplu o porÈ›iune din `inputs` pentru a vedea ce avem:

```python
inputs["input_ids"][:5]
```

```python out
[101, 2029, 7705, 2015, 2064]
```

Aceasta cu siguranÈ›Äƒ aratÄƒ ca o `list` Python obiÈ™nuitÄƒ, dar sÄƒ verificÄƒm din nou tipul:

```python
type(inputs["input_ids"])
```

```python out
list
```

Da, aceasta este cu siguranÈ›Äƒ o `list` Python. Deci ce a mers prost? AmintiÈ›i-vÄƒ din [Capitolul 2](/course/chapter2) cÄƒ clasele `AutoModelForXxx` din ğŸ¤— Transformers opereazÄƒ pe _tensori_ (fie Ã®n PyTorch sau TensorFlow), È™i o operaÈ›ie comunÄƒ este sÄƒ extragi dimensiunile unui tensor folosind `Tensor.size()` Ã®n, sÄƒ zicem, PyTorch. SÄƒ aruncÄƒm din nou o privire la traceback, pentru a vedea care linie a declanÈ™at excepÈ›ia:

```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

Se pare cÄƒ codul nostru a Ã®ncercat sÄƒ apeleze `input_ids.size()`, dar aceasta Ã®n mod clar nu va funcÈ›iona pentru o `list` Python, care este doar un container. Cum putem rezolva aceastÄƒ problemÄƒ? CÄƒutarea mesajului de eroare pe Stack Overflow oferÄƒ destul de multe [rezultate](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f) relevante. FÄƒcÃ¢nd clic pe primul afiÈ™eazÄƒ o Ã®ntrebare similarÄƒ cu a noastrÄƒ, cu rÄƒspunsul arÄƒtat Ã®n captura de ecran de mai jos:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png" alt="Un rÄƒspuns de pe Stack Overflow." width="100%"/>
</div>

RÄƒspunsul recomandÄƒ sÄƒ adÄƒugÄƒm `return_tensors='pt'` la tokenizer, aÈ™a cÄƒ sÄƒ vedem dacÄƒ aceasta funcÈ›ioneazÄƒ pentru noi:

```python out
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""
```

Minunat, a funcÈ›ionat! Acesta este un exemplu grozav de cÃ¢t de util poate fi Stack Overflow: prin identificarea unei probleme similare, am putut beneficia de experienÈ›a altora din comunitate. Cu toate acestea, o cÄƒutare ca aceasta nu va da Ã®ntotdeauna un rÄƒspuns relevant, aÈ™a cÄƒ ce puteÈ›i face Ã®n astfel de cazuri? Din fericire, existÄƒ o comunitate primitoare de dezvoltatori pe [forumurile Hugging Face](https://discuss.huggingface.co/) care vÄƒ pot ajuta! Ãn secÈ›iunea urmÄƒtoare, vom arunca o privire asupra modului Ã®n care puteÈ›i formula Ã®ntrebÄƒri bune pe forum care au È™anse sÄƒ primeascÄƒ rÄƒspuns. 