<FrameworkSwitchCourse {fw} />

# Debugging-ul pipeline-ului de antrenament[[debugging-pipeline-ului-de-antrenament]]

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section4.ipynb"},
]} />

A탵i scris un script frumos pentru a antrena sau ajusta fin un model pe o sarcin캒 dat캒, urm칙nd cu aten탵ie sfaturile din [Capitolul 7](/course/chapter7). Dar c칙nd lansa탵i comanda `trainer.train()`, se 칥nt칙mpl캒 ceva oribil: primi탵i o eroare 游땸! Sau mai r캒u, totul pare s캒 fie 칥n regul캒 탳i antrenamentul ruleaz캒 f캒r캒 eroare, dar modelul rezultat este prost. 칉n aceast캒 sec탵iune, v캒 vom ar캒ta ce pute탵i face pentru a depana acest tip de probleme.

## Debugging-ul pipeline-ului de antrenament[[debugging-pipeline-ului-de-antrenament]]

<Youtube id="L-WSwUWde1U"/>

Problema c칙nd 칥nt칙lni탵i o eroare 칥n `trainer.train()` este c캒 ar putea veni din mai multe surse, deoarece `Trainer` de obicei pune 칥mpreun캒 multe lucruri. Converte탳te seturile de date 칥n dataloader-e, a탳a c캒 problema ar putea fi ceva gre탳it 칥n setul vostru de date, sau o problem캒 c칙nd 칥ncearc캒 s캒 grupeze elementele seturilor de date 칥mpreun캒. Apoi ia un batch de date 탳i 칥l alimenteaz캒 la model, a탳a c캒 problema ar putea fi 칥n codul modelului. Dup캒 aceea, calculeaz캒 gradien탵ii 탳i efectueaz캒 pasul de optimizare, a탳a c캒 problema ar putea fi 탳i 칥n optimizatorul vostru. 탲i chiar dac캒 totul merge bine pentru antrenament, ceva ar putea merge prost 칥n timpul evalu캒rii dac캒 exist캒 o problem캒 cu metrica voastr캒.

Cea mai bun캒 modalitate de a face debugging la o eroare care apare 칥n `trainer.train()` este s캒 parcurge탵i manual 칥ntregul pipeline pentru a vedea unde au mers lucrurile prost. Eroarea este apoi adesea foarte u탳or de rezolvat.

Pentru a demonstra aceasta, vom folosi urm캒torul script care (칥ncearc캒 s캒) ajusteze fin un model DistilBERT pe [setul de date MNLI](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

Dac캒 칥ncerca탵i s캒 칥l executa탵i, ve탵i fi 칥nt칙mpina탵i cu o eroare destul de criptic캒:

```python out
'ValueError: You have to specify either input_ids or inputs_embeds'
```

### Verifica탵i datele voastre[[verificati-datele-voastre]]

Aceasta este de la sine 칥n탵eles, dar dac캒 datele voastre sunt corupte, `Trainer` nu va putea forma batch-uri, cu at칙t mai pu탵in s캒 v캒 antreneze modelul. Deci primul lucru, trebuie s캒 arunca탵i o privire asupra a ceea ce este 칥n setul vostru de antrenament.

Pentru a evita nenum캒rate ore petrecute 칥ncerc칙nd s캒 repara탵i ceva care nu este sursa bug-ului, v캒 recomand캒m s캒 folosi탵i `trainer.train_dataset` pentru verific캒rile voastre 탳i nimic altceva. S캒 facem asta aici:

```py
trainer.train_dataset[0]
```

```python out
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```

Observa탵i ceva gre탳it? Aceasta, 칥n conjunc탵ie cu mesajul de eroare despre `input_ids` lips캒, ar trebui s캒 v캒 fac캒 s캒 realiza탵i c캒 acelea sunt texte, nu numere pe care modelul le poate 칥n탵elege. Aici, eroarea original캒 este foarte 칥n탳el캒toare deoarece `Trainer` elimin캒 automat coloanele care nu se potrivesc cu semn캒tura modelului (adic캒 argumentele a탳teptate de model). Aceasta 칥nseamn캒 c캒 aici, totul 칥n afar캒 de etichete a fost eliminat. Nu a existat astfel nicio problem캒 cu crearea batch-urilor 탳i apoi trimiterea lor la model, care la r칙ndul s캒u s-a pl칙ns c캒 nu a primit intrarea potrivit캒.

De ce nu au fost procesate datele? Am folosit metoda `Dataset.map()` pe seturi de date pentru a aplica tokenizer-ul pe fiecare e탳antion. Dar dac캒 v캒 uita탵i cu aten탵ie la cod, ve탵i vedea c캒 am f캒cut o gre탳eal캒 c칙nd am trecut seturile de antrenament 탳i evaluare la `Trainer`. 칉n loc s캒 folosim `tokenized_datasets` aici, am folosit `raw_datasets` 游뱑. S캒 repar캒m asta!

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

Acest cod nou va da acum o eroare diferit캒 (progres!):

```python out
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```

Uit칙ndu-ne la traceback, putem vedea c캒 eroarea se 칥nt칙mpl캒 칥n pasul de colectare a datelor:

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

Deci, ar trebui s캒 ne 칥ndrept캒m c캒tre asta. 칉nainte s캒 facem asta, totu탳i, s캒 termin캒m de inspectat datele noastre, doar pentru a fi 100% siguri c캒 sunt corecte.

Un lucru pe care ar trebui s캒 칥l face탵i 칥ntotdeauna c칙nd depana탵i o sesiune de antrenament este s캒 arunca탵i o privire asupra intr캒rilor decodate ale modelului vostru. Nu putem 칥n탵elege numerele pe care le aliment캒m direct, a탳a c캒 ar trebui s캒 ne uit캒m la ceea ce reprezint캒 acele numere. 칉n computer vision, de exemplu, aceasta 칥nseamn캒 s캒 ne uit캒m la imaginile decodate ale pixelilor pe care 칥i trece탵i, 칥n vorbire 칥nseamn캒 s캒 asculta탵i e탳antioanele audio decodate, 탳i pentru exemplul nostru NLP aici 칥nseamn캒 s캒 folosim tokenizer-ul nostru pentru a decoda intr캒rile:

```py
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python out
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

Deci pare corect. Ar trebui s캒 face탵i aceasta pentru toate cheile din intr캒ri:

```py
trainer.train_dataset[0].keys()
```

```python out
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

Re탵ine탵i c캒 cheile care nu corespund intr캒rilor acceptate de model vor fi eliminate automat, a탳a c캒 aici vom p캒stra doar `input_ids`, `attention_mask`, 탳i `label` (care va fi redenumit `labels`). Pentru a verifica din nou semn캒tura modelului, pute탵i afi탳a clasa modelului vostru, apoi s캒 verifica탵i documenta탵ia sa:

```py
type(trainer.model)
```

```python out
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

Deci 칥n cazul nostru, putem verifica parametrii accepta탵i pe [aceast캒 pagin캒](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification). `Trainer` va 칥nregistra de asemenea coloanele pe care le elimin캒.

Am verificat c캒 ID-urile de intrare sunt corecte prin decodarea lor. Urm캒torul este `attention_mask`:

```py
trainer.train_dataset[0]["attention_mask"]
```

```python out
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

Deoarece nu am aplicat padding 칥n preprocesarea noastr캒, aceasta pare perfect natural캒. Pentru a fi siguri c캒 nu exist캒 nicio problem캒 cu acea masc캒 de aten탵ie, s캒 verific캒m c캒 are aceea탳i lungime ca ID-urile noastre de intrare:

```py
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python out
True
```

Asta e bun! 칉n sf칙r탳it, s캒 verific캒m eticheta noastr캒:

```py
trainer.train_dataset[0]["label"]
```

```python out
1
```

Ca ID-urile de intrare, acesta este un num캒r care nu are cu adev캒rat sens de unul singur. A탳a cum am v캒zut 칥nainte, maparea 칥ntre 칥ntregi 탳i numele etichetelor este stocat캒 칥n atributul `names` al *feature*-ului corespunz캒tor al setului de date:

```py
trainer.train_dataset.features["label"].names
```

```python out
['entailment', 'neutral', 'contradiction']
```

Deci `1` 칥nseamn캒 `neutral`, ceea ce 칥nseamn캒 c캒 cele dou캒 propozi탵ii pe care le-am v캒zut mai sus nu sunt 칥n contradic탵ie, 탳i prima nu implic캒 a doua. Pare corect!

Nu avem ID-uri de tip token aici, deoarece DistilBERT nu le a탳teapt캒; dac캒 ave탵i unele 칥n modelul vostru, ar trebui s캒 v캒 asigura탵i de asemenea c캒 se potrivesc corespunz캒tor unde sunt prima 탳i a doua propozi탵ie 칥n intrare.

> [!TIP]
> 九勇 **R칙ndul vostru!** Verifica탵i c캒 totul pare corect cu al doilea element al setului de date de antrenament.

Facem verificarea doar pe setul de antrenament aici, dar ar trebui desigur s캒 verifica탵i din nou seturile de validare 탳i test 칥n acela탳i mod.

Acum c캒 탳tim c캒 seturile noastre de date arat캒 bine, este timpul s캒 verific캒m urm캒torul pas al pipeline-ului de antrenament.

### De la seturi de date la dataloader-e[[de-la-seturi-de-date-la-dataloader-e]]

Urm캒torul lucru care poate merge prost 칥n pipeline-ul de antrenament este c칙nd `Trainer` 칥ncearc캒 s캒 formeze batch-uri din setul de antrenament sau validare. Odat캒 ce sunte탵i siguri c캒 seturile de date ale `Trainer` sunt corecte, pute탵i 칥ncerca s캒 forma탵i manual un batch execut칙nd urm캒toarele (칥nlocui탵i `train` cu `eval` pentru dataloader-ul de validare):

```py
for batch in trainer.get_train_dataloader():
    break
```

Acest cod creeaz캒 dataloader-ul de antrenament, apoi itereaz캒 prin el, oprindu-se la prima itera탵ie. Dac캒 codul se execut캒 f캒r캒 eroare, ave탵i primul batch de antrenament pe care 칥l pute탵i inspecta, 탳i dac캒 codul d캒 eroare, 탳ti탵i sigur c캒 problema este 칥n dataloader, a탳a cum este cazul aici:

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

Inspectarea ultimului frame al traceback-ului ar trebui s캒 fie suficient캒 pentru a v캒 da un indiciu, dar s캒 facem pu탵in캒 s캒p캒tur캒. Majoritatea problemelor 칥n timpul cre캒rii batch-ului apar din cauza colect캒rii exemplelor 칥ntr-un singur batch, a탳a c캒 primul lucru de verificat c칙nd ave탵i 칥ndoieli este ce `collate_fn` folose탳te `DataLoader`-ul vostru:

```py
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python out
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

Deci acesta este `default_data_collator`, dar nu este ceea ce vrem 칥n acest caz. Vrem s캒 facem padding la exemplele noastre la cea mai lung캒 propozi탵ie din batch, ceea ce se face de colectorul `DataCollatorWithPadding`. 탲i acest colector de date ar trebui s캒 fie folosit 칥n mod implicit de `Trainer`, deci de ce nu este folosit aici?

R캒spunsul este pentru c캒 nu am trecut `tokenizer` la `Trainer`, a탳a c캒 nu a putut crea `DataCollatorWithPadding` pe care 칥l vrem. 칉n practic캒, nu ar trebui s캒 ezita탵i niciodat캒 s캒 trece탵i explicit colectorul de date pe care dori탵i s캒 칥l folosi탵i, pentru a v캒 asigura c캒 evita탵i acest tip de erori. S캒 adapt캒m codul nostru pentru a face exact asta:

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

Vestea bun캒? Nu primim aceea탳i eroare ca 칥nainte, ceea ce este cu siguran탵캒 progres. Vestea proast캒? Primim 칥n schimb o eroare CUDA infam캒:

```python out
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```

Aceasta este rea deoarece erorile CUDA sunt extrem de greu de depanat 칥n general. Vom vedea 칥ntr-un minut cum s캒 rezolv캒m aceasta, dar mai 칥nt칙i s캒 termin캒m analiza noastr캒 a cre캒rii batch-ului.

Dac캒 sunte탵i siguri c캒 colectorul vostru de date este cel potrivit, ar trebui s캒 칥ncerca탵i s캒 칥l aplica탵i pe c칙teva e탳antioane din setul vostru de date:

```py
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```

Acest cod va e탳ua deoarece `train_dataset` con탵ine coloane string, pe care `Trainer` le elimin캒 de obicei. Le pute탵i elimina manual, sau dac캒 dori탵i s캒 replica탵i exact ceea ce face `Trainer` 칥n culise, pute탵i apela metoda privat캒 `Trainer._remove_unused_columns()` care face asta:

```py
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```

Ar trebui apoi s캒 pute탵i depana manual ce se 칥nt칙mpl캒 칥n interiorul colectorului de date dac캒 eroarea persist캒.

Acum c캒 am depanat procesul de creare a batch-ului, este timpul s캒 trecem unul prin model! 

### Trecerea prin model[[trecerea-prin-model]]

Ar trebui s캒 pute탵i ob탵ine un batch execut칙nd urm캒toarea comand캒:

```py
for batch in trainer.get_train_dataloader():
    break
```

Dac캒 rula탵i acest cod 칥ntr-un notebook, s-ar putea s캒 primi탵i o eroare CUDA similar캒 cu cea pe care am v캒zut-o mai devreme, caz 칥n care trebuie s캒 reporni탵i notebook-ul 탳i s캒 reexecuta탵i ultimul fragment f캒r캒 linia `trainer.train()`. Acesta este al doilea lucru cel mai enervant despre erorile CUDA: ele stric캒 iremediabil kernel-ul vostru. Cel mai enervant lucru despre ele este faptul c캒 sunt greu de depanat.

De ce este a탳a? Are de-a face cu modul 칥n care func탵ioneaz캒 GPU-urile. Ele sunt extrem de eficiente la executarea multor opera탵ii 칥n paralel, dar dezavantajul este c캒 atunci c칙nd una dintre acele instruc탵iuni rezult캒 칥ntr-o eroare, nu 탳ti탵i instantaneu. Doar c칙nd programul apeleaz캒 o sincronizare a multiplelor procese de pe GPU va realiza c캒 ceva a mers prost, a탳a c캒 eroarea este de fapt ridicat캒 칥ntr-un loc care nu are nimic de-a face cu ceea ce a creat-o. De exemplu, dac캒 ne uit캒m la traceback-ul nostru anterior, eroarea a fost ridicat캒 칥n timpul backward pass-ului, dar vom vedea 칥ntr-un minut c캒 de fapt provine din ceva din forward pass.

Deci cum depan캒m aceste erori? R캒spunsul este simplu: nu o facem. Dac캒 eroarea voastr캒 CUDA nu este o eroare out-of-memory (ceea ce 칥nseamn캒 c캒 nu exist캒 suficient캒 memorie 칥n GPU-ul vostru), ar trebui s캒 v캒 칥ntoarce탵i 칥ntotdeauna la CPU pentru a o depana.

Pentru a face aceasta 칥n cazul nostru, trebuie doar s캒 punem modelul 칥napoi pe CPU 탳i s캒 칥l apel캒m pe batch-ul nostru -- batch-ul returnat de `DataLoader` nu a fost 칥nc캒 mutat pe GPU:

```python
outputs = trainer.model.cpu()(**batch)
```

```python out
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```

Deci, imaginea devine mai clar캒. 칉n loc s캒 avem o eroare CUDA, acum avem un `IndexError` 칥n calculul loss-ului (deci nimic de-a face cu backward pass-ul, a탳a cum am spus mai devreme). Mai precis, putem vedea c캒 este target 2 care creeaz캒 eroarea, a탳a c캒 acesta este un moment foarte bun pentru a verifica num캒rul de etichete al modelului nostru:

```python
trainer.model.config.num_labels
```

```python out
2
```

Cu dou캒 etichete, doar 0 탳i 1 sunt permise ca target-uri, dar conform mesajului de eroare am primit un 2. Primirea unui 2 este de fapt normal캒: dac캒 ne amintim numele etichetelor pe care le-am extras mai devreme, erau trei, deci avem indicii 0, 1 탳i 2 칥n setul nostru de date. Problema este c캒 nu am spus asta modelului nostru, care ar fi trebuit s캒 fie creat cu trei etichete. S캒 repar캒m asta!

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

Nu includem 칥nc캒 linia `trainer.train()`, pentru a ne lua timpul s캒 verific캒m c캒 totul arat캒 bine. Dac캒 cerem un batch 탳i 칥l trecem la modelul nostru, acum func탵ioneaz캒 f캒r캒 eroare!

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

Urm캒torul pas este apoi s캒 ne 칥ntoarcem la GPU 탳i s캒 verific캒m c캒 totul 칥nc캒 func탵ioneaz캒:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

Dac캒 칥nc캒 primi탵i o eroare, asigura탵i-v캒 c캒 reporni탵i notebook-ul 탳i executa탵i doar ultima versiune a scriptului.

### Efectuarea unui pas de optimizare[[efectuarea-unui-pas-de-optimizare]]

Acum c캒 탳tim c캒 putem construi batch-uri care trec efectiv prin model, suntem gata pentru urm캒torul pas al pipeline-ului de antrenament: calcularea gradien탵ilor 탳i efectuarea unui pas de optimizare.

Prima parte este doar o chestiune de a apela metoda `backward()` pe loss:

```py
loss = outputs.loss
loss.backward()
```

Este destul de rar s캒 primi탵i o eroare 칥n aceast캒 etap캒, dar dac캒 o face탵i, asigura탵i-v캒 s캒 v캒 칥ntoarce탵i la CPU pentru a ob탵ine un mesaj de eroare util.

Pentru a efectua pasul de optimizare, trebuie doar s캒 cre캒m `optimizer` 탳i s캒 apel캒m metoda sa `step()`:

```py
trainer.create_optimizer()
trainer.optimizer.step()
```

Din nou, dac캒 folosi탵i optimizatorul implicit 칥n `Trainer`, nu ar trebui s캒 primi탵i o eroare 칥n aceast캒 etap캒, dar dac캒 ave탵i un optimizator personalizat, ar putea fi c칙teva probleme de depanat aici. Nu uita탵i s캒 v캒 칥ntoarce탵i la CPU dac캒 primi탵i o eroare CUDA ciudat캒 칥n aceast캒 etap캒. Vorbind despre erorile CUDA, mai devreme am men탵ionat un caz special. S캒 arunc캒m o privire asupra acestuia acum.

### Gestionarea erorilor CUDA out-of-memory[[gestionarea-erorilor-cuda-out-of-memory]]

Ori de c칙te ori primi탵i un mesaj de eroare care 칥ncepe cu `RuntimeError: CUDA out of memory`, aceasta indic캒 faptul c캒 nu ave탵i memorie GPU. Aceasta nu este legat캒 direct de codul vostru 탳i se poate 칥nt칙mpla cu un script care ruleaz캒 perfect. Aceast캒 eroare 칥nseamn캒 c캒 a탵i 칥ncercat s캒 pune탵i prea multe lucruri 칥n memoria intern캒 a GPU-ului vostru, 탳i aceasta a rezultat 칥ntr-o eroare. Ca 탳i cu alte erori CUDA, va trebui s캒 reporni탵i kernel-ul pentru a fi 칥ntr-un punct unde pute탵i rula din nou antrenamentul.

Pentru a rezolva aceast캒 problem캒, trebuie doar s캒 folosi탵i mai pu탵in spa탵iu GPU -- ceva care este adesea mai u탳or de spus dec칙t de f캒cut. 칉n primul r칙nd, asigura탵i-v캒 c캒 nu ave탵i dou캒 modele pe GPU 칥n acela탳i timp (dac캒 nu este necesar pentru problema voastr캒, desigur). Apoi, probabil ar trebui s캒 reduce탵i dimensiunea batch-ului, deoarece aceasta afecteaz캒 direct dimensiunile tuturor ie탳irilor intermediare ale modelului 탳i gradien탵ii lor. Dac캒 problema persist캒, considera탵i folosirea unei versiuni mai mici a modelului vostru.

> [!TIP]
> 칉n urm캒toarea parte a cursului, vom examina tehnici mai avansate care v캒 pot ajuta s캒 reduce탵i amprenta de memorie 탳i s캒 v캒 permit캒 s캒 ajusta탵i fin cele mai mari modele.

### Evaluarea modelului[[evaluarea-modelului]]

Acum c캒 am rezolvat toate problemele cu codul nostru, totul este perfect 탳i antrenamentul ar trebui s캒 ruleze f캒r캒 probleme, nu? Nu at칙t de repede! Dac캒 rula탵i comanda `trainer.train()`, totul va ar캒ta bine la 칥nceput, dar dup캒 un timp ve탵i primi urm캒toarele:

```py
# Aceasta va dura mult timp 탳i va da eroare, a탳a c캒 nu ar trebui s캒 rula탵i aceast캒 celul캒
trainer.train()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

Ve탵i realiza c캒 aceast캒 eroare apare 칥n timpul fazei de evaluare, deci acesta este ultimul lucru pe care va trebui s캒 칥l depan캒m.

Pute탵i rula bucla de evaluare a `Trainer` independent de antrenament astfel:

```py
trainer.evaluate()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

> [!TIP]
> 游눠 Ar trebui s캒 v캒 asigura탵i 칥ntotdeauna c캒 pute탵i rula `trainer.evaluate()` 칥nainte de a lansa `trainer.train()`, pentru a evita risipa multor resurse de calcul 칥nainte de a 칥nt칙lni o eroare.

칉nainte de a 칥ncerca s캒 depana탵i o problem캒 칥n bucla de evaluare, ar trebui s캒 v캒 asigura탵i mai 칥nt칙i c캒 v-a탵i uitat la date, pute탵i forma un batch corespunz캒tor 탳i pute탵i rula modelul pe el. Am completat to탵i ace탳ti pa탳i, a탳a c캒 urm캒torul cod poate fi executat f캒r캒 eroare:

```py
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

Eroarea vine mai t칙rziu, la sf칙r탳itul fazei de evaluare, 탳i dac캒 ne uit캒m la traceback vedem aceasta:

```python trace
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

Aceasta ne spune c캒 eroarea provine din modulul `datasets/metric.py` -- deci aceasta este o problem캒 cu func탵ia noastr캒 `compute_metrics()`. Aceasta ia un tuplu cu logit-urile 탳i etichetele ca array-uri NumPy, a탳a c캒 s캒 칥ncerc캒m s캒 칥i aliment캒m asta:

```py
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

Primim aceea탳i eroare, deci problema se afl캒 cu siguran탵캒 칥n acea func탵ie. Dac캒 ne uit캒m 칥napoi la codul ei, vedem c캒 doar transmite `predictions` 탳i `labels` la `metric.compute()`. Deci exist캒 o problem캒 cu acea metod캒? Nu chiar. S캒 arunc캒m o privire rapid캒 asupra formelor:

```py
predictions.shape, labels.shape
```

```python out
((8, 3), (8,))
```

Predic탵iile noastre sunt 칥nc캒 logit-uri, nu predic탵iile reale, de aceea metrica returneaz캒 aceast캒 eroare (oarecum obscur캒). Repara탵ia este destul de u탳oar캒; trebuie doar s캒 ad캒ug캒m un argmax 칥n func탵ia `compute_metrics()`:

```py
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))
```

```python out
{'accuracy': 0.625}
```

Acum eroarea noastr캒 este reparat캒! Aceasta a fost ultima, a탳a c캒 scriptul nostru va antrena acum un model corespunz캒tor.

Pentru referin탵캒, iat캒 scriptul complet reparat:

```py
import numpy as np
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

칉n acest caz, nu mai sunt probleme, 탳i scriptul nostru va ajusta fin un model care ar trebui s캒 dea rezultate rezonabile. Dar ce putem face c칙nd antrenamentul continu캒 f캒r캒 nicio eroare, 탳i modelul antrenat nu performeaz캒 deloc bine? Aceasta este partea cea mai dificil캒 a machine learning-ului, 탳i v캒 vom ar캒ta c칙teva tehnici care pot ajuta.

> [!TIP]
> 游눠 Dac캒 folosi탵i o bucl캒 de antrenament manual캒, aceia탳i pa탳i se aplic캒 pentru a v캒 depana pipeline-ul de antrenament, dar este mai u탳or s캒 칥i separa탵i. Asigura탵i-v캒 c캒 nu a탵i uitat `model.eval()` sau `model.train()` la locurile potrivite, sau `zero_grad()` la fiecare pas, totu탳i!

## Depanarea erorilor silen탵ioase 칥n timpul antrenamentului[[depanarea-erorilor-silentioase-in-timpul-antrenamentului]]

Ce putem face pentru a depana un antrenament care se completeaz캒 f캒r캒 eroare dar nu ob탵ine rezultate bune? V캒 vom da c칙teva indicii aici, dar fi탵i con탳tien탵i c캒 acest tip de depanare este partea cea mai grea a machine learning-ului, 탳i nu exist캒 un r캒spuns magic.

### Verifica탵i datele voastre (din nou!)[[verificati-datele-voastre-din-nou]]

Modelul vostru va 칥nv캒탵a ceva doar dac캒 este de fapt posibil s캒 칥nve탵e ceva din datele voastre. Dac캒 exist캒 o eroare care corupe datele sau etichetele sunt atribuite aleatoriu, este foarte probabil c캒 nu ve탵i ob탵ine niciun antrenament de model pe setul vostru de date. Deci 칥ncepe탵i 칥ntotdeauna prin a verifica din nou intr캒rile 탳i etichetele voastre decodate, 탳i 칥ntreba탵i-v캒 urm캒toarele 칥ntreb캒ri:

- Sunt datele decodate 칥n탵elegibile?
- Sunte탵i de acord cu etichetele?
- Exist캒 o etichet캒 care este mai comun캒 dec칙t altele?
- Care ar trebui s캒 fie loss-ul/metrica dac캒 modelul ar prezice un r캒spuns aleatoriu/칥ntotdeauna acela탳i r캒spuns?

> [!WARNING]
> 丘멆잺 Dac캒 face탵i antrenament distribuit, afi탳a탵i e탳antioane din setul vostru de date 칥n fiecare proces 탳i verifica탵i de trei ori c캒 ob탵ine탵i acela탳i lucru. O eroare comun캒 este s캒 ave탵i o surs캒 de aleatoriu 칥n crearea datelor care face ca fiecare proces s캒 aib캒 o versiune diferit캒 a setului de date.

Dup캒 ce v캒 uita탵i la datele voastre, trece탵i prin c칙teva dintre predic탵iile modelului 탳i decoda탵i-le 탳i pe ele. Dac캒 modelul prezice 칥ntotdeauna acela탳i lucru, ar putea fi pentru c캒 setul vostru de date este p캒rtinitor c캒tre o categorie (pentru problemele de clasificare); tehnici precum supraesantionarea claselor rare ar putea ajuta.

Dac캒 loss-ul/metrica pe care o ob탵ine탵i pe modelul vostru ini탵ial este foarte diferit캒 de loss-ul/metrica pe care a탵i a탳tepta-o pentru predic탵ii aleatorii, verifica탵i din nou modul 칥n care loss-ul sau metrica voastr캒 este calculat캒, deoarece probabil exist캒 o eroare acolo. Dac캒 folosi탵i mai multe loss-uri pe care le ad캒uga탵i la sf칙r탳it, asigura탵i-v캒 c캒 sunt de aceea탳i scar캒.

C칙nd sunte탵i siguri c캒 datele voastre sunt perfecte, pute탵i vedea dac캒 modelul este capabil s캒 se antreneze pe ele cu un test simplu.

### Supraajusta탵i modelul vostru pe un batch[[supraajustati-modelul-vostru-pe-un-batch]]

Supraajustarea este de obicei ceva pe care 칥ncerc캒m s캒 칥l evit캒m c칙nd antren캒m, deoarece 칥nseamn캒 c캒 modelul nu 칥nva탵캒 s캒 recunoasc캒 caracteristicile generale pe care vrem s캒 le recunoasc캒, ci 칥n schimb doar memoreaz캒 e탳antioanele de antrenament. Cu toate acestea, 칥ncercarea de a v캒 antrena modelul pe un batch din nou 탳i din nou este un test bun pentru a verifica dac캒 problema a탳a cum a탵i formulat-o poate fi rezolvat캒 de modelul pe care 칥ncerca탵i s캒 칥l antrena탵i. De asemenea, v캒 va ajuta s캒 vede탵i dac캒 rata voastr캒 de 칥nv캒탵are ini탵ial캒 este prea mare.

F캒c칙nd aceasta odat캒 ce a탵i definit `Trainer` este foarte u탳or; doar lua탵i un batch de date de antrenament, apoi rula탵i o bucl캒 mic캒 de antrenament manual folosind doar acel batch pentru ceva ca 20 de pa탳i:

```py
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

> [!TIP]
> 游눠 Dac캒 datele voastre de antrenament sunt dezechilibrate, asigura탵i-v캒 s캒 construi탵i un batch de date de antrenament care con탵ine toate etichetele.

Modelul rezultat ar trebui s캒 aib캒 rezultate aproape perfecte pe acela탳i `batch`. S캒 calcul캒m metrica pe predic탵iile rezultate:

```py
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python out
{'accuracy': 1.0}
```

100% acurate탵e, acum acesta este un exemplu frumos de supraajustare (ceea ce 칥nseamn캒 c캒 dac캒 칥ncerca탵i modelul vostru pe orice alt캒 propozi탵ie, foarte probabil v캒 va da un r캒spuns gre탳it)!

Dac캒 nu reu탳i탵i s캒 face탵i modelul vostru s캒 ob탵in캒 rezultate perfecte ca aceasta, 칥nseamn캒 c캒 exist캒 ceva gre탳it cu modul 칥n care a탵i formulat problema sau datele voastre, a탳a c캒 ar trebui s캒 repara탵i asta. Doar c칙nd reu탳i탵i s캒 trece탵i testul de supraajustare pute탵i fi siguri c캒 modelul vostru poate 칥nv캒탵a de fapt ceva.

> [!WARNING]
> 丘멆잺 Va trebui s캒 v캒 recrea탵i modelul 탳i `Trainer` dup캒 acest test, deoarece modelul ob탵inut probabil nu va putea s캒 se recupereze 탳i s캒 칥nve탵e ceva util pe setul vostru complet de date.

### Nu ajusta탵i nimic p칙n캒 nu ave탵i o prim캒 linie de baz캒[[nu-ajustati-nimic-pana-nu-aveti-o-prima-linie-de-baza]]

Ajustarea hiperparametrilor este 칥ntotdeauna subliniat캒 ca fiind partea cea mai grea a machine learning-ului, dar este doar ultimul pas pentru a v캒 ajuta s캒 c칙탳tiga탵i pu탵in la metric캒. Majoritatea timpului, hiperparametrii implici탵i ai `Trainer` vor func탵iona bine pentru a v캒 da rezultate bune, a탳a c캒 nu v캒 lansa탵i 칥ntr-o c캒utare de hiperparametri consumatoare de timp 탳i costisitoare p칙n캒 nu ave탵i ceva care bate linia de baz캒 pe care o ave탵i pe setul vostru de date.

Odat캒 ce ave탵i un model suficient de bun, pute탵i 칥ncepe s캒 ajusta탵i pu탵in. Nu 칥ncerca탵i s캒 lansa탵i o mie de rul캒ri cu hiperparametri diferi탵i, ci compara탵i c칙teva rul캒ri cu valori diferite pentru un hiperparametru pentru a avea o idee despre care are cel mai mare impact.

Dac캒 ajusta탵i modelul 칥n sine, p캒stra탵i-l simplu 탳i nu 칥ncerca탵i nimic pe care nu 칥l pute탵i justifica 칥n mod rezonabil. Asigura탵i-v캒 칥ntotdeauna c캒 v캒 칥ntoarce탵i la testul de supraajustare pentru a verifica c캒 schimbarea voastr캒 nu a avut consecin탵e neinten탵ionate.

### Cere탵i ajutor[[cereti-ajutor]]

Sper캒m c캒 ve탵i fi g캒sit c칙teva sfaturi 칥n aceast캒 sec탵iune care v-au ajutat s캒 v캒 rezolva탵i problema, dar dac캒 nu este cazul, aminti탵i-v캒 c캒 pute탵i 칥ntotdeauna s캒 cere탵i comunit캒탵ii pe [forumuri](https://discuss.huggingface.co/).

Iat캒 c칙teva resurse suplimentare care se pot dovedi utile:

- ["Reproducibility as a vehicle for engineering best practices"](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) de Joel Grus
- ["Checklist for debugging neural networks"](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) de Cecelia Shao
- ["How to unit test machine learning code"](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) de Chase Roberts
- ["A Recipe for Training Neural Networks"](http://karpathy.github.io/2019/04/25/recipe/) de Andrej Karpathy

Desigur, nu fiecare problem캒 pe care o 칥nt칙lni탵i c칙nd antrena탵i re탵ele neuronale este vina voastr캒! Dac캒 칥nt칙lni탵i ceva 칥n biblioteca 游뱅 Transformers sau 游뱅 Datasets care nu pare corect, s-ar putea s캒 fi 칥nt칙lnit o eroare. Ar trebui cu siguran탵캒 s캒 ne spune탵i totul despre aceasta, 탳i 칥n sec탵iunea urm캒toare v캒 vom explica exact cum s캒 face탵i asta. 