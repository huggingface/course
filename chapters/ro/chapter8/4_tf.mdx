<FrameworkSwitchCourse {fw} />

# Debugging-ul pipeline-ului de antrenament[[debugging-pipeline-ului-de-antrenament]]

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section4_tf.ipynb"},
]} />

A탵i scris un script frumos pentru a antrena sau ajusta fin un model pe o sarcin캒 dat캒, urm칙nd cu aten탵ie sfaturile din [Capitolul 7](/course/chapter7). Dar c칙nd lansa탵i comanda `model.fit()`, se 칥nt칙mpl캒 ceva oribil: primi탵i o eroare 游땸! Sau mai r캒u, totul pare s캒 fie 칥n regul캒 탳i antrenamentul ruleaz캒 f캒r캒 eroare, dar modelul rezultat este prost. 칉n aceast캒 sec탵iune, v캒 vom ar캒ta ce pute탵i face pentru a depana acest tip de probleme.

## Debugging-ul pipeline-ului de antrenament[[debugging-pipeline-ului-de-antrenament]]

<Youtube id="N9kO52itd0Q"/>

Problema c칙nd 칥nt칙lni탵i o eroare 칥n `model.fit()` este c캒 ar putea veni din mai multe surse, deoarece antrenamentul aduce de obicei 칥mpreun캒 multe lucruri la care a탵i lucrat p칙n캒 칥n acel punct. Problema ar putea fi ceva gre탳it 칥n setul vostru de date, sau o problem캒 c칙nd 칥ncearc캒 s캒 grupeze elementele seturilor de date 칥mpreun캒. Sau ar putea fi ceva gre탳it 칥n codul modelului, sau func탵ia voastr캒 de loss sau optimizatorul. 탲i chiar dac캒 totul merge bine pentru antrenament, ceva ar putea merge prost 칥n timpul evalu캒rii dac캒 exist캒 o problem캒 cu metrica voastr캒.

Cea mai bun캒 modalitate de a face debugging la o eroare care apare 칥n `model.fit()` este s캒 parcurge탵i manual 칥ntregul pipeline pentru a vedea unde au mers lucrurile prost. Eroarea este apoi adesea foarte u탳or de rezolvat.

Pentru a demonstra aceasta, vom folosi urm캒torul script care (칥ncearc캒 s캒) ajusteze fin un model DistilBERT pe [setul de date MNLI](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)
```

Dac캒 칥ncerca탵i s캒 칥l executa탵i, a탵i putea primi ni탳te `VisibleDeprecationWarning`-uri c칙nd face탵i conversia setului de date -- aceasta este o problem캒 UX cunoscut캒 pe care o avem, deci v캒 rug캒m s캒 o ignora탵i. Dac캒 citi탵i cursul dup캒, s캒 zicem, noiembrie 2021 탳i 칥nc캒 se 칥nt칙mpl캒, atunci trimite탵i tweet-uri furioase la @carrigmat p칙n캒 c칙nd o repar캒.

Ce este o problem캒 mai serioas캒, totu탳i, este c캒 primim o eroare direct캒. 탲i este 칥ntr-adev캒r, 칥nfrico탳캒tor de lung캒:

```python out
ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']
```

Ce 칥nseamn캒 asta? Am 칥ncercat s캒 antren캒m pe datele noastre, dar nu am primit niciun gradient? Aceasta este destul de nedumeritoare; cum 칥ncepem s캒 depan캒m ceva ca aceasta? C칙nd eroarea pe care o primi탵i nu sugereaz캒 imediat unde este problema, cea mai bun캒 solu탵ie este adesea s캒 parcurge탵i lucrurile 칥n secven탵캒, asigur칙ndu-v캒 la fiecare etap캒 c캒 totul arat캒 corect. 탲i desigur, locul de 칥nceput este 칥ntotdeauna s캒...

### Verifica탵i datele voastre[[verificati-datele-voastre]]

Aceasta este de la sine 칥n탵eles, dar dac캒 datele voastre sunt corupte, Keras nu va putea s캒 le repare pentru voi. Deci primul lucru, trebuie s캒 arunca탵i o privire asupra a ceea ce este 칥n setul vostru de antrenament.

De탳i este tentant s캒 v캒 uita탵i 칥n interiorul `raw_datasets` 탳i `tokenized_datasets`, v캒 recomand캒m puternic s캒 merge탵i la date chiar la punctul unde vor intra 칥n model. Aceasta 칥nseamn캒 s캒 citi탵i o ie탳ire din `tf.data.Dataset` pe care l-a탵i creat cu func탵ia `to_tf_dataset()`! Deci cum facem asta? Obiectele `tf.data.Dataset` ne dau batch-uri 칥ntregi odat캒 탳i nu suport캒 indexarea, a탳a c캒 nu putem s캒 cerem pur 탳i simplu `train_dataset[0]`. Putem, totu탳i, s캒 칥i cerem politicos un batch:

```py
for batch in train_dataset:
    break
```

`break` termin캒 bucla dup캒 o itera탵ie, a탳a c캒 aceasta prinde primul batch care iese din `train_dataset` 탳i 칥l salveaz캒 ca `batch`. Acum, s캒 arunc캒m o privire asupra a ceea ce este 칥n interior:

```python out
{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}
```

Aceasta pare corect캒, nu? Transmitem `labels`, `attention_mask`, 탳i `input_ids` la model, care ar trebui s캒 fie tot ce are nevoie pentru a calcula ie탳irile 탳i a calcula loss-ul. Deci de ce nu avem un gradient? Privi탵i mai atent: transmitem un singur dic탵ionar ca intrare, dar un batch de antrenament este de obicei un tensor de intrare sau dic탵ionar, plus un tensor de etichete. Etichetele noastre sunt doar o cheie 칥n dic탵ionarul nostru de intrare.

Este aceasta o problem캒? Nu 칥ntotdeauna, de fapt! Dar este una dintre cele mai comune probleme pe care le ve탵i 칥nt칙lni c칙nd antrena탵i modele Transformer cu TensorFlow. Modelele noastre pot calcula toate loss-ul intern, dar pentru a face asta etichetele trebuie s캒 fie transmise 칥n dic탵ionarul de intrare. Acesta este loss-ul care este folosit c칙nd nu specific캒m o valoare de loss la `compile()`. Keras, pe de alt캒 parte, se a탳teapt캒 de obicei ca etichetele s캒 fie transmise separat de dic탵ionarul de intrare, 탳i calculele de loss vor e탳ua de obicei dac캒 nu face탵i asta.

Problema a devenit acum mai clar캒: am transmis un argument `loss`, ceea ce 칥nseamn캒 c캒 칥i cerem lui Keras s캒 calculeze loss-urile pentru noi, dar am transmis etichetele noastre ca intr캒ri la model, nu ca etichete 칥n locul pe care Keras le a탳teapt캒. Trebuie s캒 alegem una sau alta: fie folosim loss-ul intern al modelului 탳i p캒str캒m etichetele unde sunt, fie continu캒m s캒 folosim loss-urile Keras, dar mut캒m etichetele 칥n locul pe care Keras le a탳teapt캒. Pentru simplitate, s캒 adopt캒m prima abordare. Schimba탵i apelul la `compile()` s캒 citeasc캒:

```py
model.compile(optimizer="adam")
```

Acum vom folosi loss-ul intern al modelului, 탳i aceast캒 problem캒 ar trebui s캒 fie rezolvat캒! 

> [!TIP]
> 九勇 **R칙ndul vostru!** Ca o provocare op탵ional캒 dup캒 ce am rezolvat celelalte probleme, pute탵i 칥ncerca s캒 v캒 칥ntoarce탵i la acest pas 탳i s캒 face탵i modelul s캒 func탵ioneze cu loss-ul original calculat de Keras 칥n loc de loss-ul intern. Va trebui s캒 ad캒uga탵i `"labels"` la argumentul `label_cols` al `to_tf_dataset()` pentru a v캒 asigura c캒 etichetele sunt scoase corect, ceea ce v캒 va da gradien탵i -- dar mai exist캒 o problem캒 cu loss-ul pe care l-am specificat. Antrenamentul va rula 칥nc캒 cu aceast캒 problem캒, dar 칥nv캒탵area va fi foarte lent캒 탳i va ajunge la un platou la un loss de antrenament ridicat. Pute탵i s캒 v캒 da탵i seama ce este?
>
> Un indiciu codificat ROT13, dac캒 sunte탵i bloca탵i: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf `ybtvgf`. Jung ner ybtvgf?
>
> 탲i un al doilea indiciu: Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?

Acum, s캒 칥ncerc캒m antrenamentul. Ar trebui s캒 primim gradien탵i acum, a탳a c캒 cu speran탵캒 (muzic캒 sinistr캒 se aude aici) putem pur 탳i simplu s캒 apel캒m `model.fit()` 탳i totul va func탵iona bine!

```python out
  246/24543 [..............................] - ETA: 15:52 - loss: nan
```

Oh nu.

`nan` nu este o valoare foarte 칥ncurajatoare pentru loss. Cu toate acestea, am verificat datele noastre, 탳i pare destul de bun캒. Dac캒 aceea nu este problema, unde putem merge urm캒torul? Urm캒torul pas evident este s캒...

### Verifica탵i modelul vostru[[verificati-modelul-vostru]]

`model.fit()` este o func탵ie de convenien탵캒 foarte grozav캒 칥n Keras, dar face multe lucruri pentru voi, 탳i aceasta poate face mai complicat s캒 g캒si탵i exact unde a ap캒rut o problem캒. Dac캒 depana탵i modelul vostru, o strategie care poate ajuta cu adev캒rat este s캒 trimitie탵i doar un singur batch la model, 탳i s캒 v캒 uita탵i la ie탳irile pentru acel batch 칥n detaliu. Un alt sfat foarte util dac캒 modelul arunc캒 erori este s캒 `compile()` modelul cu `run_eagerly=True`. Aceasta 칥l va face mult mai lent, dar va face mesajele de eroare mult mai comprehensibile, deoarece vor indica exact unde 칥n codul modelului vostru a ap캒rut problema.

Pentru moment, totu탳i, nu avem nevoie de `run_eagerly` 칥nc캒. S캒 rul캒m `batch`-ul pe care l-am primit 칥nainte prin model 탳i s캒 vedem cum arat캒 ie탳irile:

```py
model(batch)
```

```python out
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)
```

Ei bine, aceasta este complicat캒. Totul este `nan`! Dar aceasta este ciudat캒, nu? Cum ar deveni toate logit-urile noastre `nan`? `nan` 칥nseamn캒 "not a number" (nu un num캒r). Valorile `nan` apar adesea c칙nd efectua탵i o opera탵ie interzis캒, cum ar fi 칥mp캒r탵irea la zero. Dar un lucru care este foarte important de 탳tiut despre `nan` 칥n machine learning este c캒 aceast캒 valoare tinde s캒 se *propage*. Dac캒 칥nmul탵i탵i un num캒r cu `nan`, ie탳irea este de asemenea `nan`. 탲i dac캒 primi탵i un `nan` oriunde 칥n ie탳irea voastr캒, loss-ul voastru sau gradientul voastru, atunci se va r캒sp칙ndi rapid prin 칥ntreaga voastr캒 re탵ea -- deoarece c칙nd acea valoare `nan` este propagat캒 칥napoi prin re탵eaua voastr캒, ve탵i primi gradien탵i `nan`, 탳i c칙nd actualiz캒rile de greut캒탵i sunt calculate cu acei gradien탵i, ve탵i primi greut캒탵i `nan`, 탳i acele greut캒탵i vor calcula 탳i mai multe ie탳iri `nan`! 칉n cur칙nd 칥ntreaga re탵ea va fi doar un bloc mare de `nan`-uri. Odat캒 ce se 칥nt칙mpl캒 asta, este destul de greu s캒 vede탵i unde a 칥nceput problema. Cum putem izola unde `nan` a intrat pentru prima dat캒?

R캒spunsul este s캒 칥ncerc캒m s캒 *reini탵ializ캒m* modelul nostru. Odat캒 ce am 칥nceput antrenamentul, am primit un `nan` undeva 탳i s-a propagat rapid prin 칥ntregul model. Deci, s캒 칥nc캒rc캒m modelul dintr-un checkpoint 탳i s캒 nu facem nicio actualizare de greut캒탵i, 탳i s캒 vedem unde primim o valoare `nan`:

```py
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)
```

C칙nd rul캒m asta, primim:

```py out
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)
```

*Acum* ajungem undeva! Nu exist캒 valori `nan` 칥n logit-urile noastre, ceea ce este lini탳titor. Dar vedem c칙teva valori `nan` 칥n loss-ul nostru! Exist캒 ceva special despre acele e탳antioane 칥n particular care cauzeaz캒 aceast캒 problem캒? S캒 vedem care sunt (re탵ine탵i c캒 dac캒 rula탵i acest cod voi 칥n탳iv캒, a탵i putea primi indici diferi탵i deoarece setul de date a fost amestecat):

```python
import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices
```

```python out
array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])
```

S캒 ne uit캒m la e탳antioanele din care au venit ace탳ti indici:

```python
input_ids = batch["input_ids"].numpy()
input_ids[indices]
```

```python out
array([[  101,  2007,  2032, ...,     0,     0,     0],
       [  101,  1998,  6814, ...,     0,     0,     0],
       [  101,  1998,  2007, ...,     0,     0,     0],
       ...,
       [  101, 13543,  1999, ...,     0,     0,     0]])
```

Ei bine, exist캒 multe aici, dar nimic nu iese 칥n eviden탵캒 ca fiind neobi탳nuit. S캒 ne uit캒m la etichete:

```python out
labels = batch['labels'].numpy()
labels[indices]
```

```python out
array([2, 2, 2, 2, 2, 2, 2, 2, 2])
```

Ah! E탳antioanele `nan` au toate aceea탳i etichet캒, 탳i este eticheta 2. Aceasta este un indiciu foarte puternic. Faptul c캒 primim doar un loss de `nan` c칙nd eticheta noastr캒 este 2 sugereaz캒 c캒 acesta este un moment foarte bun pentru a verifica num캒rul de etichete 칥n modelul nostru:

```python
model.config.num_labels
```

```python out
2
```

Acum vedem problema: modelul crede c캒 sunt doar dou캒 clase, dar etichetele merg p칙n캒 la 2, ceea ce 칥nseamn캒 c캒 exist캒 de fapt trei clase (deoarece 0 este de asemenea o clas캒). A탳a am primit un `nan` -- prin 칥ncercarea de a calcula loss-ul pentru o clas캒 inexistent캒! S캒 칥ncerc캒m s캒 schimb캒m asta 탳i s캒 re칥ncadr캒m modelul:

```
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)
```

```python out
  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032
```

Antren캒m! Nu mai sunt `nan`-uri, 탳i loss-ul nostru scade... cumva. Dac캒 칥l urm캒ri탵i pentru o vreme, a탵i putea 칥ncepe s캒 deveni탵i pu탵in ner캒bd캒tori, deoarece valoarea loss-ului r캒m칙ne 칥nc캒p캒탵칙nat de mare. S캒 oprim antrenamentul aici 탳i s캒 칥ncerc캒m s캒 ne g칙ndim la ce ar putea cauza aceast캒 problem캒. 칉n acest punct, suntem destul de siguri c캒 at칙t datele c칙t 탳i modelul sunt 칥n regul캒, dar modelul nostru nu 칥nva탵캒 bine. Ce altceva r캒m칙ne? Este timpul s캒...

### Verifica탵i hiperparametrii vo탳tri[[verificati-hiperparametrii-vostri]]

Dac캒 v캒 uita탵i 칥napoi la codul de mai sus, a탵i putea s캒 nu pute탵i vedea niciun hiperparametru deloc, 칥n afar캒 de poate `batch_size`, 탳i acela nu pare un vinovat probabil. Nu v캒 l캒sa탵i 칥n탳ela탵i, totu탳i; 칥ntotdeauna exist캒 hiperparametri, 탳i dac캒 nu 칥i pute탵i vedea, 칥nseamn캒 doar c캒 nu 탳ti탵i la ce sunt seta탵i. 칉n particular, aminti탵i-v캒 un lucru critic despre Keras: dac캒 seta탵i un loss, optimizator, sau func탵ie de activare cu un string, _toate argumentele sale vor fi setate la valorile lor implicite_. Aceasta 칥nseamn캒 c캒 de탳i folosirea string-urilor pentru aceasta este foarte convenabil캒, ar trebui s캒 fi탵i foarte aten탵i c칙nd o face탵i, deoarece poate ascunde u탳or lucruri critice de la voi. (Oricine 칥ncearc캒 provocarea op탵ional캒 de mai sus ar trebui s캒 ia not캒 cu aten탵ie de acest fapt.)

칉n acest caz, unde am setat un argument cu un string? Set캒m optimizatorul cu un string. Ar putea ascunde asta ceva de la noi? S캒 arunc캒m o privire la [argumentele sale](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).

Iese ceva 칥n eviden탵캒 aici? Exact -- rata de 칥nv캒탵are! C칙nd folosim doar string-ul `'adam'`, vom primi rata de 칥nv캒탵are implicit캒, care este 0.001, sau 1e-3. Aceasta este mult prea mare pentru un model Transformer! 칉n general, recomand캒m s캒 칥ncerca탵i rate de 칥nv캒탵are 칥ntre 1e-5 탳i 1e-4 pentru modelele voastre; aceasta este undeva 칥ntre de 10X 탳i 100X mai mic캒 dec칙t valoarea pe care o folosim de fapt aici. Sun캒 a o problem캒 major캒, a탳a c캒 s캒 칥ncerc캒m s캒 o reducem. Pentru a face asta, trebuie s캒 import캒m obiectul `optimizer` real. 칉n timp ce suntem la asta, s캒 reini탵ializ캒m modelul din checkpoint, 칥n cazul 칥n care antrenamentul cu rata de 칥nv캒탵are mare i-a deteriorat greut캒탵ile:

```python
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))
```

> [!TIP]
> 游눠 Pute탵i de asemenea importa func탵ia `create_optimizer()` din 游뱅 Transformers, care v캒 va da un optimizator AdamW cu weight decay corect precum 탳i warmup 탳i decay pentru rata de 칥nv캒탵are. Acest optimizator va produce adesea rezultate pu탵in mai bune dec칙t cele pe care le ob탵ine탵i cu optimizatorul Adam implicit.

Acum, putem 칥ncerca s캒 칥ncadr캒m modelul cu noua rat캒 de 칥nv캒탵are 칥mbun캒t캒탵it캒:

```python
model.fit(train_dataset)
```

```python out
319/24543 [..............................] - ETA: 16:07 - loss: 0.9718
```

Acum loss-ul nostru chiar merge undeva! Antrenamentul pare 칥n sf칙r탳it c캒 func탵ioneaz캒. Exist캒 o lec탵ie aici: c칙nd modelul vostru ruleaz캒 dar loss-ul nu scade, 탳i sunte탵i siguri c캒 datele voastre sunt 칥n regul캒, este o idee bun캒 s캒 verifica탵i hiperparametri ca rata de 칥nv캒탵are 탳i weight decay. Setarea oric캒ruia dintre acestea prea mare este foarte probabil s캒 cauzeze antrenamentul s캒 "stagneze" la o valoare mare de loss.

## Alte probleme poten탵iale[[alte-probleme-potentiale]]

Am acoperit problemele din scriptul de mai sus, dar exist캒 mai multe erori comune cu care v-a탵i putea confrunta. S캒 arunc캒m o privire la o list캒 (foarte incomplet캒).

### Gestionarea erorilor out-of-memory[[gestionarea-erorilor-out-of-memory]]

Semnul revelator al r캒m칙nerii f캒r캒 memorie este o eroare ca "OOM when allocating tensor" -- OOM este prescurtare pentru "out of memory". Aceasta este o problem캒 foarte comun캒 c칙nd ave탵i de-a face cu modele de limb캒 mari. Dac캒 칥nt칙lni탵i aceasta, o strategie bun캒 este s캒 v캒 칥njum캒t캒탵i탵i dimensiunea batch-ului 탳i s캒 칥ncerca탵i din nou. Re탵ine탵i, totu탳i, c캒 unele modele sunt *foarte* mari. De exemplu, GPT-2 complet are 1.5B parametri, ceea ce 칥nseamn캒 c캒 ve탵i avea nevoie de 6 GB de memorie doar pentru a stoca modelul, 탳i 칥nc캒 6 GB pentru gradien탵ii s캒i! Antrenamentul modelului GPT-2 complet va necesita de obicei peste 20 GB de VRAM indiferent de dimensiunea batch-ului pe care 칥l folosi탵i, pe care doar c칙teva GPU-uri 칥l au. Modele mai u탳oare cum ar fi `distilbert-base-cased` sunt mult mai u탳or de rulat, 탳i se antreneaz캒 mult mai rapid de asemenea.

> [!TIP]
> 칉n urm캒toarea parte a cursului, vom examina tehnici mai avansate care v캒 pot ajuta s캒 reduce탵i amprenta de memorie 탳i s캒 v캒 permit캒 s캒 ajusta탵i fin cele mai mari modele.

### TensorFlow fl캒m칙nd fl캒m칙nd 游붙[[tensorflow-flamand-flamand]]

O particularitate specific캒 a TensorFlow de care ar trebui s캒 fi탵i con탳tien탵i este c캒 aloc캒 *toat캒* memoria GPU pentru sine de 칥ndat캒 ce 칥nc캒rca탵i un model sau face탵i orice antrenament, 탳i apoi 칥mparte acea memorie dup캒 cum este necesar. Aceasta este diferit캒 de comportamentul altor framework-uri, cum ar fi PyTorch, care aloc캒 memorie dup캒 cum este necesar cu CUDA 칥n loc s캒 o fac캒 intern. Un avantaj al abord캒rii TensorFlow este c캒 poate da adesea erori utile c칙nd r캒m칙ne탵i f캒r캒 memorie, 탳i se poate recupera din acea stare f캒r캒 s캒 strice 칥ntregul kernel CUDA. Dar exist캒 de asemenea un dezavantaj important: dac캒 rula탵i dou캒 procese TensorFlow simultan, atunci **o s캒 ave탵i probleme**.

Dac캒 rula탵i pe Colab nu trebuie s캒 v캒 face탵i griji pentru aceasta, dar dac캒 rula탵i local aceasta este cu siguran탵캒 ceva de care ar trebui s캒 fi탵i aten탵i. 칉n particular, fi탵i con탳tien탵i c캒 칥nchiderea unei file de notebook nu 칥nchide neap캒rat acel notebook! A탵i putea fi nevoi탵i s캒 selecta탵i notebook-urile care ruleaz캒 (cele cu o iconi탵캒 verde) 탳i s캒 le 칥nchide탵i manual 칥n listarea de directoare. Orice notebook care rula 탳i folosea TensorFlow ar putea s캒 탵in캒 칥nc캒 o gr캒mad캒 din memoria GPU, 탳i aceasta 칥nseamn캒 c캒 orice notebook nou pe care 칥l porni탵i ar putea 칥nt칙lni probleme foarte ciudate.

Dac캒 칥ncepe탵i s캒 primi탵i erori despre CUDA, BLAS, sau cuBLAS 칥n cod care func탵iona 칥nainte, aceasta este adesea de vin캒. Pute탵i folosi o comand캒 cum ar fi `nvidia-smi` pentru a verifica -- c칙nd 칥nchide탵i sau reporni탵i notebook-ul curent, este cea mai mult캒 memorie liber캒, sau este 칥nc캒 칥n uz? Dac캒 este 칥nc캒 칥n uz, altceva o 탵ine!

### Verifica탵i din nou datele voastre![[verificati-din-nou-datele-voastre]]

Modelul vostru va 칥nv캒탵a ceva doar dac캒 este de fapt posibil s캒 칥nve탵e ceva din datele voastre. Dac캒 exist캒 o eroare care corupe datele sau etichetele sunt atribuite aleatoriu, este foarte probabil c캒 nu ve탵i ob탵ine niciun antrenament de model pe setul vostru de date. Un instrument util aici este `tokenizer.decode()`. Aceasta va transforma `input_ids` 칥napoi 칥n string-uri, a탳a c캒 pute탵i vizualiza datele 탳i vedea dac캒 datele voastre de antrenament 칥nva탵캒 ceea ce vre탵i s캒 칥nve탵e. De exemplu, dup캒 ce ob탵ine탵i un `batch` din `tf.data.Dataset`-ul vostru cum am f캒cut mai sus, pute탵i decoda primul element astfel:

```py
input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])
```

Apoi pute탵i s캒 칥l compara탵i cu prima etichet캒, astfel:

```py
labels = batch["labels"].numpy()
label = labels[0]
```

Odat캒 ce pute탵i vizualiza datele voastre 칥n acest mod, v캒 pute탵i 칥ntreba urm캒toarele 칥ntreb캒ri:

- Sunt datele decodate 칥n탵elegibile?
- Sunte탵i de acord cu etichetele?
- Exist캒 o etichet캒 care este mai comun캒 dec칙t altele?
- Care ar trebui s캒 fie loss-ul/metrica dac캒 modelul ar prezice un r캒spuns aleatoriu/칥ntotdeauna acela탳i r캒spuns?

Dup캒 ce v캒 uita탵i la datele voastre, trece탵i prin c칙teva dintre predic탵iile modelului -- dac캒 modelul vostru scoate tokenuri, 칥ncerca탵i s캒 le decoda탵i 탳i pe acelea! Dac캒 modelul prezice 칥ntotdeauna acela탳i lucru, ar putea fi pentru c캒 setul vostru de date este p캒rtinitor c캒tre o categorie (pentru problemele de clasificare), a탳a c캒 tehnici precum supraesantionarea claselor rare ar putea ajuta. Alternativ, aceasta poate fi de asemenea cauzat캒 de probleme de antrenament cum ar fi set캒ri proaste de hiperparametri.

Dac캒 loss-ul/metrica pe care o ob탵ine탵i pe modelul vostru ini탵ial 칥nainte de orice antrenament este foarte diferit캒 de loss-ul/metrica pe care a탵i a탳tepta-o pentru predic탵ii aleatorii, verifica탵i din nou modul 칥n care loss-ul sau metrica voastr캒 este calculat캒, deoarece probabil exist캒 o eroare acolo. Dac캒 folosi탵i mai multe loss-uri pe care le ad캒uga탵i la sf칙r탳it, asigura탵i-v캒 c캒 sunt de aceea탳i scar캒.

C칙nd sunte탵i siguri c캒 datele voastre sunt perfecte, pute탵i vedea dac캒 modelul este capabil s캒 se antreneze pe ele cu un test simplu.

### Supraajusta탵i modelul vostru pe un batch[[supraajustati-modelul-vostru-pe-un-batch]]

Supraajustarea este de obicei ceva pe care 칥ncerc캒m s캒 칥l evit캒m c칙nd antren캒m, deoarece 칥nseamn캒 c캒 modelul nu 칥nva탵캒 s캒 recunoasc캒 caracteristicile generale pe care vrem s캒 le recunoasc캒, ci 칥n schimb doar memoreaz캒 e탳antioanele de antrenament. Cu toate acestea, 칥ncercarea de a v캒 antrena modelul pe un batch din nou 탳i din nou este un test bun pentru a verifica dac캒 problema a탳a cum a탵i formulat-o poate fi rezolvat캒 de modelul pe care 칥ncerca탵i s캒 칥l antrena탵i. De asemenea, v캒 va ajuta s캒 vede탵i dac캒 rata voastr캒 de 칥nv캒탵are ini탵ial캒 este prea mare.

F캒c칙nd aceasta odat캒 ce a탵i definit `model`-ul vostru este foarte u탳or; doar lua탵i un batch de date de antrenament, apoi trata탵i acel `batch` ca 칥ntreg setul vostru de date, antren칙nd pe el pentru un num캒r mare de epoci:

```py
for batch in train_dataset:
    break

# Asigura탵i-v캒 c캒 a탵i rulat model.compile() 탳i a탵i setat optimizatorul,
# 탳i loss-ul/metricile voastre dac캒 le folosi탵i

model.fit(batch, epochs=20)
```

> [!TIP]
> 游눠 Dac캒 datele voastre de antrenament sunt dezechilibrate, asigura탵i-v캒 s캒 construi탵i un batch de date de antrenament care con탵ine toate etichetele.

Modelul rezultat ar trebui s캒 aib캒 rezultate aproape perfecte pe `batch`, cu un loss care scade rapid c캒tre 0 (sau valoarea minim캒 pentru loss-ul pe care 칥l folosi탵i).

Dac캒 nu reu탳i탵i s캒 face탵i modelul vostru s캒 ob탵in캒 rezultate perfecte ca aceasta, 칥nseamn캒 c캒 exist캒 ceva gre탳it cu modul 칥n care a탵i formulat problema sau datele voastre, a탳a c캒 ar trebui s캒 repara탵i asta. Doar c칙nd reu탳i탵i s캒 trece탵i testul de supraajustare pute탵i fi siguri c캒 modelul vostru poate 칥nv캒탵a de fapt ceva.

> [!WARNING]
> 丘멆잺 Va trebui s캒 v캒 recrea탵i modelul 탳i s캒 recompila탵i dup캒 acest test de supraajustare, deoarece modelul ob탵inut probabil nu va putea s캒 se recupereze 탳i s캒 칥nve탵e ceva util pe setul vostru complet de date.

### Nu ajusta탵i nimic p칙n캒 nu ave탵i o prim캒 linie de baz캒[[nu-ajustati-nimic-pana-nu-aveti-o-prima-linie-de-baza]]

Ajustarea intens캒 a hiperparametrilor este 칥ntotdeauna subliniat캒 ca fiind partea cea mai grea a machine learning-ului, dar este doar ultimul pas pentru a v캒 ajuta s캒 c칙탳tiga탵i pu탵in la metric캒. Valori *foarte* proaste pentru hiperparametrii vo탳tri, cum ar fi folosirea ratei de 칥nv캒탵are implicite Adam de 1e-3 cu un model Transformer, vor face 칥nv캒탵area s캒 procedeze foarte lent sau s캒 stagneze complet, desigur, dar majoritatea timpului hiperparametri "rezonabili", cum ar fi o rat캒 de 칥nv캒탵are de la 1e-5 la 5e-5, vor func탵iona bine pentru a v캒 da rezultate bune. Deci, nu v캒 lansa탵i 칥ntr-o c캒utare de hiperparametri consumatoare de timp 탳i costisitoare p칙n캒 nu ave탵i ceva care bate linia de baz캒 pe care o ave탵i pe setul vostru de date.

Odat캒 ce ave탵i un model suficient de bun, pute탵i 칥ncepe s캒 ajusta탵i pu탵in. Nu 칥ncerca탵i s캒 lansa탵i o mie de rul캒ri cu hiperparametri diferi탵i, ci compara탵i c칙teva rul캒ri cu valori diferite pentru un hiperparametru pentru a avea o idee despre care are cel mai mare impact.

Dac캒 ajusta탵i modelul 칥n sine, p캒stra탵i-l simplu 탳i nu 칥ncerca탵i nimic pe care nu 칥l pute탵i justifica 칥n mod rezonabil. Asigura탵i-v캒 칥ntotdeauna c캒 v캒 칥ntoarce탵i la testul de supraajustare pentru a verifica c캒 schimbarea voastr캒 nu a avut consecin탵e neinten탵ionate.

### Cere탵i ajutor[[cereti-ajutor]]

Sper캒m c캒 ve탵i fi g캒sit c칙teva sfaturi 칥n aceast캒 sec탵iune care v-au ajutat s캒 v캒 rezolva탵i problema, dar dac캒 nu este cazul, aminti탵i-v캒 c캒 pute탵i 칥ntotdeauna s캒 cere탵i comunit캒탵ii pe [forumuri](https://discuss.huggingface.co/).

Iat캒 c칙teva resurse suplimentare care se pot dovedi utile:

- ["Reproducibility as a vehicle for engineering best practices"](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) de Joel Grus
- ["Checklist for debugging neural networks"](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) de Cecelia Shao
- ["How to unit test machine learning code"](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) de Chase Roberts
- ["A Recipe for Training Neural Networks"](http://karpathy.github.io/2019/04/25/recipe/) de Andrej Karpathy

Desigur, nu fiecare problem캒 pe care o 칥nt칙lni탵i c칙nd antrena탵i re탵ele neuronale este vina voastr캒! Dac캒 칥nt칙lni탵i ceva 칥n biblioteca 游뱅 Transformers sau 游뱅 Datasets care nu pare corect, s-ar putea s캒 fi 칥nt칙lnit o eroare. Ar trebui cu siguran탵캒 s캒 ne spune탵i totul despre aceasta, 탳i 칥n sec탵iunea urm캒toare v캒 vom explica exact cum s캒 face탵i asta. 