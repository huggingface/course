<FrameworkSwitchCourse {fw} />

# Ãn spatele pipeline-ului[[Ã®n-spatele-pipeline-ului]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
]} />

{/if}

> [!TIP]
> Aceasta este prima secÈ›iune Ã®n care conÈ›inutul este uÈ™or diferit Ã®n funcÈ›ie de utilizarea PyTorch sau TensorFlow. SchimbaÈ›i comutatorul din partea de sus a titlului pentru a selecta platforma pe care o preferaÈ›i!

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

SÄƒ Ã®ncepem cu un exemplu complet, aruncÃ¢nd o privire la ceea ce s-a Ã®ntÃ¢mplat Ã®n spate atunci cÃ¢nd am executat urmÄƒtorul cod Ã®n [Capitolul 1](/course/chapter1):


```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

È™i am obÈ›inut:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

DupÄƒ cum am vÄƒzut Ã®n [Capitolul 1](/course/chapter1), acest pipeline grupeazÄƒ trei etape: preprocesarea, trecerea intrÄƒrilor prin model È™i postprocesarea:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
</div>

SÄƒ trecem rapid prin fiecare dintre acestea.

## Preprocesarea cu un tokenizator[[preprocesarea-cu-un-tokenizator]]

La fel ca alte reÈ›ele neuronale, modelele Transformer nu pot procesa direct text brut, astfel Ã®ncÃ¢t primul pas al pipeline-ului nostru este de a converti intrÄƒrile de text Ã®n numere pe care modelul le poate Ã®nÈ›elege. Pentru a face acest lucru, folosim un *tokenizer*, care va fi responsabil pentru:

- ÃmpÄƒrÈ›irea datelor de intrare Ã®n cuvinte, subcuvinte sau simboluri (cum ar fi punctuaÈ›ia) care se numesc *tokens*
- Maparea fiecÄƒrui token Ã®ntr-un numÄƒr Ã®ntreg
- AdÄƒugarea de intrÄƒri suplimentare care pot fi utile pentru model

ToatÄƒ aceastÄƒ preprocesare trebuie efectuatÄƒ exact Ã®n acelaÈ™i mod ca atunci cÃ¢nd modelul a fost preantrenat, aÈ™a cÄƒ mai Ã®ntÃ¢i trebuie sÄƒ descÄƒrcÄƒm aceste informaÈ›ii din [Model Hub](https://huggingface.co/models). Pentru a face acest lucru, folosim clasa `AutoTokenizer` È™i metoda sa `from_pretrained()`. Folosind numele checkpoint-ului modelului nostru, aceasta va prelua automat datele asociate cu tokenizer-ul modelului È™i le va stoca Ã®n cache (astfel Ã®ncÃ¢t acestea sÄƒ fie descÄƒrcate doar prima datÄƒ cÃ¢nd executaÈ›i codul de mai jos).

Deoarece punctul de control implicit al pipeline-ului `sentiment-analysis` este `distilbert-base-uncased-finetuned-sst-2-english` (puteÈ›i vedea fiÈ™a modelului [aici](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), executÄƒm urmÄƒtoarele:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

OdatÄƒ ce avem tokenizatorul, putem sÄƒ Ã®i transmitem direct propoziÈ›iile noastre È™i vom primi Ã®napoi un dicÈ›ionar care este gata sÄƒ fie introdus Ã®n modelul nostru! Singurul lucru rÄƒmas de fÄƒcut este sÄƒ convertim lista de ID-uri de intrare Ã®n tensori.

PuteÈ›i utiliza ğŸ¤— Transformers fÄƒrÄƒ a trebui sÄƒ vÄƒ faceÈ›i griji cu privire la cadrul ML utilizat ca backend; ar putea fi PyTorch sau TensorFlow, sau Flax pentru unele modele. Cu toate acestea, modelele Transformer acceptÄƒ numai *tensori * ca intrare. DacÄƒ este prima datÄƒ cÃ¢nd auziÈ›i despre tensori, vÄƒ puteÈ›i gÃ¢ndi la ei ca la array-uri NumPy. Un array NumPy poate fi un scalar (0D), un vector (1D), o matrice (2D) sau poate avea mai multe dimensiuni. Este de fapt un tensor; tensorii altor cadre ML se comportÄƒ similar È™i sunt de obicei la fel de simplu de instanÈ›iat ca È™i array-urile NumPy.

Pentru a specifica tipul de tensori pe care dorim sÄƒ Ã®i primim Ã®napoi (PyTorch, TensorFlow sau NumPy simplu), folosim argumentul `return_tensors`:


{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

 
Nu vÄƒ faceÈ›i Ã®ncÄƒ griji cu privire la padding È™i trunchiere; le vom explica mai tÃ¢rziu. Principalele lucruri de reÈ›inut aici sunt cÄƒ puteÈ›i trece o propoziÈ›ie sau o listÄƒ de propoziÈ›ii, precum È™i specificarea tipului de tensori pe care doriÈ›i sÄƒ Ã®i primiÈ›i Ã®napoi (dacÄƒ nu este specificat niciun tip, veÈ›i primi o listÄƒ de liste ca rezultat).
{#if fw === 'pt'}

IatÄƒ cum aratÄƒ rezultatele ca tensori PyTorch:

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

IatÄƒ cum aratÄƒ rezultatele ca tensori TensorFlow:

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

 
Rezultatul Ã®n sine este un dicÈ›ionar care conÈ›ine douÄƒ chei, `input_ids` È™i `attention_mask`. `input_ids` conÈ›ine douÄƒ rÃ¢nduri de numere Ã®ntregi (unul pentru fiecare propoziÈ›ie) care sunt identificatorii unici ai simbolurilor din fiecare propoziÈ›ie. Vom explica ce este `attention_mask` mai tÃ¢rziu Ã®n acest capitol. 

## Parcurgerea modelului[[parcurgerea-modelului]]

{#if fw === 'pt'}
Putem descÄƒrca modelul nostru preantrenat Ã®n acelaÈ™i mod Ã®n care am fÄƒcut-o cu tokenizatorul nostru. ğŸ¤— Transformers oferÄƒ o clasÄƒ `AutoModel` care are È™i o metodÄƒ `from_pretrained()`:

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
 
Putem descÄƒrca modelul nostru preantrenat Ã®n acelaÈ™i mod Ã®n care am fÄƒcut-o cu tokenizatorul nostru. ğŸ¤— Transformers oferÄƒ o clasÄƒ `TFAutoModel` care are È™i o metodÄƒ `from_pretrained`:

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

 
Ãn acest fragment de cod, am descÄƒrcat acelaÈ™i checkpoint pe care l-am folosit anterior Ã®n pipeline-ul nostru (de fapt, ar fi trebuit sÄƒ fie deja Ã®n cache) È™i am instanÈ›iat un model cu acesta.

AceastÄƒ arhitecturÄƒ conÈ›ine doar modulul Transformer de bazÄƒ: avÃ¢nd Ã®n vedere anumite intrÄƒri, acesta produce ceea ce vom numi *stÄƒri ascunse*, cunoscute È™i ca *caracteristici*. Pentru fiecare intrare a modelului, vom extrage un vector multidimensional care reprezintÄƒ **Ã®nÈ›elegerea contextualÄƒ a intrÄƒrii respective de cÄƒtre modelul Transformer**.

DacÄƒ acest lucru nu are sens, nu vÄƒ faceÈ›i griji. Vom explica totul mai tÃ¢rziu.

DeÈ™i aceste stÄƒri ascunse pot fi utile pe cont propriu, ele sunt de obicei intrÄƒri pentru o altÄƒ parte a modelului, cunoscutÄƒ sub numele de *head*. Ãn [Capitolul 1](/course/chapter1), diferitele sarcini ar fi putut fi efectuate cu aceeaÈ™i arhitecturÄƒ, dar fiecÄƒreia dintre aceste sarcini Ã®i va fi asociat un head diferit.

### Un vector multidimensional?[[un-vector-multidimensional]]

 
Vectorul emis de modulul Transformator este de obicei de dimensiuni mari. Acesta are Ã®n general trei dimensiuni:

- **Dimensiunea batch-ului**: NumÄƒrul de secvenÈ›e prelucrate simultan (2 Ã®n exemplul nostru).
- **Lungimea secvenÈ›ei**: Lungimea reprezentÄƒrii numerice a secvenÈ›ei (16 Ã®n exemplul nostru).
- **Dimensiunea ascunsÄƒ**: Dimensiunea vectorialÄƒ a fiecÄƒrei intrÄƒri a modelului.

Se spune cÄƒ este multidimensional din cauza ultimei valori. Dimensiunea ascunsÄƒ poate fi foarte mare (768 este comunÄƒ pentru modelele mai mici, iar Ã®n modelele mai mari aceasta poate ajunge la 3072 sau mai mult).

Putem vedea acest lucru dacÄƒ introducem Ã®n modelul nostru intrÄƒrile pe care le-am preprocesat:

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

 
ReÈ›ineÈ›i cÄƒ ieÈ™irile modelelor ğŸ¤— Transformers se comportÄƒ ca `namedtuple`s sau dicÈ›ionare. PuteÈ›i accesa elementele prin atribute (aÈ™a cum am fÄƒcut noi) sau prin cheie (`outputs[â€last_hidden_stateâ€]`), sau chiar prin index dacÄƒ È™tiÈ›i exact unde se aflÄƒ lucrul pe care Ã®l cÄƒutaÈ›i (`outputs[0]`).

### Modele de head-uri: ÃnÈ›elegerea numerelor[[modele-de-head-uri-Ã®nÈ›elegerea-numerelor]]

Head-urile modelelor iau ca intrare vectorul multidimensional al stÄƒrilor ascunse È™i le proiecteazÄƒ pe o altÄƒ dimensiune. Acestea sunt de obicei compuse din unul sau cÃ¢teva straturi liniare:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

 
Rezultatul modelului Transformer este trimis direct la head-ul modelului pentru a fi prelucrat.

Ãn aceastÄƒ diagramÄƒ, modelul este reprezentat de stratul sÄƒu de Ã®ncorporare È™i de straturile urmÄƒtoare. Stratul de Ã®ncorporare converteÈ™te fiecare ID de intrare din intrarea tokenizatÄƒ Ã®ntr-un vector care reprezintÄƒ tokenul asociat. Straturile ulterioare manipuleazÄƒ aceÈ™ti vectori folosind mecanismul de atenÈ›ie pentru a produce reprezentarea finalÄƒ a propoziÈ›iilor.

ExistÄƒ multe arhitecturi diferite disponibile Ã®n ğŸ¤— Transformers, fiecare fiind conceputÄƒ Ã®n jurul abordÄƒrii unei sarcini specifice. IatÄƒ o listÄƒ neexhaustivÄƒ:

- `*Model` (extragerea stÄƒrilor ascunse)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- È™i altele ğŸ¤—

{#if fw === 'pt'}
 Pentru exemplul nostru, vom avea nevoie de un model cu un head de clasificare a secvenÈ›elor (pentru a putea clasifica propoziÈ›iile ca fiind pozitive sau negative). AÈ™adar, nu vom utiliza clasa `AutoModel`, ci `AutoModelForSequenceClassification`:

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
 Pentru exemplul nostru, vom avea nevoie de un model cu un head de clasificare a secvenÈ›elor (pentru a putea clasifica propoziÈ›iile ca fiind pozitive sau negative). AÈ™adar, nu vom utiliza clasa `TFAutoModel`, ci `TFAutoModelForSequenceClassification`:

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}
 
Acum, dacÄƒ ne uitÄƒm la forma ieÈ™irilor noastre, dimensiunea va fi mult mai micÄƒ: head-ul modelului ia ca intrare vectorii multidimensionali pe care i-am vÄƒzut Ã®nainte È™i scoate vectori care conÈ›in douÄƒ valori (una pentru fiecare etichetÄƒ):

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

 
Deoarece avem doar douÄƒ propoziÈ›ii È™i douÄƒ etichete, rezultatul pe care Ã®l obÈ›inem din modelul nostru este de forma 2 x 2.

## Postprocesarea rezultatului[[postprocesarea-rezultatului]]

Valorile pe care le obÈ›inem ca rezultat al modelului nostru nu au neapÄƒrat sens Ã®n sine. SÄƒ aruncÄƒm o privire:

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

Modelul nostru a prezis `[-1.5607, 1.6123]` pentru prima propoziÈ›ie È™i `[ 4.1692, -3.3464]` pentru cea de-a doua. Acestea nu sunt probabilitÄƒÈ›i, ci *logits*, scorurile brute, nenormalizate, emise de ultimul strat al modelului. Pentru a fi convertite Ã®n probabilitÄƒÈ›i, acestea trebuie sÄƒ treacÄƒ printr-un strat [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) (toate modelele ğŸ¤— Transformers produc logits, deoarece funcÈ›ia de pierdere pentru formare va fuziona Ã®n general ultima funcÈ›ie de activare, cum ar fi SoftMax, cu funcÈ›ia de pierdere realÄƒ, cum ar fi entropia Ã®ncruciÈ™atÄƒ):

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

 
Acum putem vedea cÄƒ modelul a prezis `[0.0402, 0.9598]` pentru prima propoziÈ›ie È™i `[0.9995, 0.0005]` pentru cea de-a doua. Acestea sunt scoruri de probabilitate care pot fi recunoscute.

Pentru a obÈ›ine etichetele corespunzÄƒtoare fiecÄƒrei poziÈ›ii, putem inspecta atributul `id2label` din configuraÈ›ia modelului (mai multe despre acest lucru Ã®n secÈ›iunea urmÄƒtoare):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```
Acum putem concluziona cÄƒ modelul a prezis urmÄƒtoarele:
 
- Prima propoziÈ›ie: NEGATIV: 0.0402, POZITIV: 0.9598
- A doua propoziÈ›ie: NEGATIVÄ‚: 0.9995, POZITIVÄ‚: 0.0005

Am reprodus cu succes cele trei etape ale pipeline-ului: preprocesarea cu tokenizatoare, trecerea intrÄƒrilor prin model È™i postprocesarea! Acum haideÈ›i sÄƒ analizÄƒm Ã®n profunzime fiecare dintre aceste etape.

> [!TIP]
> âœï¸  **ÃncercaÈ›i!** AlegeÈ›i douÄƒ (sau mai multe) texte proprii È™i treceÈ›i-le prin conducta `sentiment-analysis`. Apoi repetaÈ›i paÈ™ii pe care i-aÈ›i vÄƒzut aici È™i verificaÈ›i dacÄƒ obÈ›ineÈ›i aceleaÈ™i rezultate!
