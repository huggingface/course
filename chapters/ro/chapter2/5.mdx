<FrameworkSwitchCourse {fw} />

# Gestionarea secven탵elor multiple[[gestionarea-secven탵elor-multiple]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

칉n sec탵iunea anterioar캒, am explorat cel mai simplu caz de utilizare: realizarea de inferen탵e pe o singur캒 secven탵캒 scurt캒. Cu toate acestea, apar deja unele 칥ntreb캒ri:

- Cum gestion캒m secven탵e multiple?
- Cum gestion캒m secven탵e multiple *de lungimi diferite*?
- Indicii vocabularului sunt singurele intr캒ri care permit unui model s캒 func탵ioneze bine?
- Exist캒 o secven탵캒 prea lung캒?

S캒 vedem ce tipuri de probleme prezint캒 aceste 칥ntreb캒ri 탳i cum le putem rezolva folosind API-ul 游뱅 Transformers.


## Modelele a탳teapt캒 un batch de intr캒ri[[modelele-a탳teapt캒-un-batch-de-intr캒ri]]

칉n exerci탵iul anterior a탵i v캒zut cum secven탵ele sunt transformate 칥n liste de numere. S캒 convertim aceast캒 list캒 de numere 칥ntr-un tensor 탳i s캒 o transmitem modelului:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# Aceast캒 linie va e탳ua.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# Aceast캒 linie va e탳ua.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

Oh, nu! De ce a e탳uat? Am urmat pa탳ii din pipeline-ul din sec탵iunea 2.

Problema este c캒 am trimis o singur캒 secven탵캒 c캒tre model, 칥n timp ce modelele 游뱅 Transformers a탳teapt캒 칥n mod implicit mai multe propozi탵ii. Aici am 칥ncercat s캒 facem tot ce a f캒cut tokenizatorul 칥n fundal atunci c칙nd l-am aplicat unei `secven탵e`. Dar, dac캒 v캒 uita탵i cu aten탵ie, ve탵i vedea c캒 tokenizatorul nu a convertit doar lista de ID-uri de intrare 칥ntr-un tensor, ci a ad캒ugat 탳i o dimensiune peste aceasta:

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

S캒 칥ncerc캒m din nou 탳i s캒 ad캒ug캒m o nou캒 dimensiune:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

Imprim캒m ID-urile de intrare, precum 탳i logit-urile rezultate - iat캒 rezultatul:

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

Batching-ul este ac탵iunea de a trimite mai multe propozi탵ii prin model, toate odat캒. Dac캒 ave탵i o singur캒 propozi탵ie, pute탵i crea un lot cu o singur캒 secven탵캒: 

```
batched_ids = [ids, ids]
```

Acesta este un batch de dou캒 secven탵e identice!

> [!TIP]
> 九勇 **칉ncerca탵i!** Converti탵i aceast캒 list캒 `batched_ids` 칥ntr-un tensor 탳i trece탵i-o prin modelul dumneavoastr캒. Verifica탵i dac캒 ob탵ine탵i acelea탳i logits ca 칥nainte (dar de dou캒 ori)!

Batching-ul permite modelului s캒 func탵ioneze atunci c칙nd 칥i furniza탵i mai multe secven탵e. Utilizarea mai multor secven탵e este la fel de simpl캒 ca 탳i crearea unui lot cu o singur캒 secven탵캒. Exist캒 칥ns캒 o a doua problem캒. Atunci c칙nd 칥ncerca탵i s캒 combina탵i dou캒 (sau mai multe) propozi탵ii, acestea pot avea lungimi diferite. Dac캒 a탵i mai lucrat vreodat캒 cu tensori, 탳ti탵i c캒 ace탳tia trebuie s캒 aib캒 o form캒 dreptunghiular캒, deci nu ve탵i putea converti direct lista de ID-uri de intrare 칥ntr-un tensor. Pentru a rezolva aceast캒 problem캒, de obicei *umplem* datele de intrare.

## Padding-ul datelor de intrare[[padding-ul-datelor-de-intrare]]

Urm캒toarea serie de liste nu poate fi convertit캒 칥ntr-un tensor:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Pentru a ocoli acest lucru, vom folosi *padding-ul* pentru ca tensorii no탳tri s캒 aib캒 o form캒 dreptunghiular캒. Padding-ul (umplerea datelor) asigur캒 c캒 toate propozi탵iile noastre au aceea탳i lungime prin ad캒ugarea unui cuv칙nt special numit *padding token* la propozi탵iile cu mai pu탵ine valori. De exemplu, dac캒 ave탵i 10 propozi탵ii cu 10 cuvinte 탳i 1 propozi탵ie cu 20 de cuvinte, padding-ul va asigura c캒 toate propozi탵iile au 20 de cuvinte. 칉n exemplul nostru, tensorul rezultat arat캒 astfel:

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

ID-ul token-ului de padding poate fi g캒sit 칥n `tokenizer.pad_token_id`. S캒-l folosim 탳i s캒 trimitem cele dou캒 propozi탵ii prin model 칥n mod separat 탳i grupate 칥mpreun캒:

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

Este ceva 칥n neregul캒 cu logit-urile din predic탵iile noastre grupate: al doilea r칙nd ar trebui s캒 fie identic cu logit-urile pentru a doua propozi탵ie, dar avem valori complet diferite!

Acest lucru se datoreaz캒 faptului c캒 principala caracteristic캒 a modelelor Transformer sunt straturile de aten탵ie care *contextualizeaz캒* fiecare token. Acestea vor lua 칥n considerare token-urile de umplutur캒, deoarece se ocup캒 de toate token-urile unei secven탵e. Pentru a ob탵ine acela탳i rezultat atunci c칙nd trecem propozi탵ii individuale de diferite lungimi prin model sau atunci c칙nd trecem un batch cu acelea탳i propozi탵ii 탳i acela탳i umplutur캒 aplicat캒, trebuie s캒 spunem acelor straturi de aten탵ie s캒 ignore token-urile de umplutur캒. Acest lucru se realizeaz캒 prin utilizarea unei m캒탳ti de aten탵ie (attention mask).

## Attention masks(m캒탳ti de aten탵ie)[[attention-masks]]

*M캒탳tile de aten탵ie* sunt tensori cu exact aceea탳i form캒 ca tensorul ID-urilor de intrare, completate cu 0 탳i 1: valorile 1 indic캒 faptul c캒 ar trebui s캒 se acorde aten탵ie token-urilor corespunz캒toare, iar valorile 0 indic캒 faptul c캒 nu ar trebui s캒 se acorde aten탵ie token-urilor corespunz캒toare (adic캒 acestea ar trebui ignorate de straturile de aten탵ie ale modelului).

S캒 complet캒m exemplul anterior cu o masc캒 de aten탵ie:

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

Acum ob탵inem aceea탳i logits pentru a doua propozi탵ie din batch.

Observ캒m cum ultima valoare a celei de-a doua secven탵e este un ID de padding, care este o valoare 0 칥n masca de aten탵ie.

> [!TIP]
> 九勇 **칉ncerca탵i!** Aplica탵i manual tokenizarea pe cele dou캒 propozi탵ii utilizate 칥n sec탵iunea 2 ("I've been waiting for a HuggingFace course my whole life." 탳i "I hate this so much!"). Trece탵i-le prin model 탳i verifica탵i dac캒 ob탵ine탵i aceea탳i logi탵i ca 칥n sec탵iunea 2. Acum grupa탵i-le 칥mpreun캒 folosind token-ul de padding, apoi crea탵i masca de aten탵ie corespunz캒toare. Verifica탵i dac캒 ob탵ine탵i acelea탳i rezultate atunci c칙nd parcurge탵i modelul!

## Secven탵e mai lungi[[secven탵e-mai-lungi]]

Cu modelele Transformer, exist캒 o limit캒 a lungimii secven탵elor pe care le putem transmite modelelor. Majoritatea modelelor gestioneaz캒 secven탵e de p칙n캒 la 512 sau 1024 de token-uri 탳i se vor bloca atunci c칙nd li se cere s캒 proceseze secven탵e mai lungi. Exist캒 dou캒 solu탵ii la aceast캒 problem캒:

- Utiliza탵i un model cu o lungime de secven탵캒 acceptat캒 mai mare.
- Trunchia탵i secven탵ele.

Modelele au diferite lungimi de secven탵캒 acceptate, iar unele sunt specializate 칥n tratarea secven탵elor foarte lungi. [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) este un exemplu, iar altul este [LED](https://huggingface.co/docs/transformers/model_doc/led). Dac캒 lucra탵i la o sarcin캒 care necesit캒 secven탵e foarte lungi, v캒 recomand캒m s캒 arunca탵i o privire la aceste modele.

칉n caz contrar, v캒 recomand캒m s캒 v캒 trunchia탵i secven탵ele prin specificarea parametrului `max_sequence_length`:
```py
sequence = sequence[:max_sequence_length]
```
