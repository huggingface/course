# E timpul sÄƒ tÄƒiem È™i sÄƒ analizÄƒm datele[[time-to-slice-and-dice]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section3.ipynb"},
]} />

Ãn cea mai mare parte, datele cu care lucrezi nu vor fi perfect pregÄƒtite pentru antrenarea modelelor. Ãn aceastÄƒ secÈ›iune vom explora features variate pe care ğŸ¤— Datasets le oferÄƒ pentru curÄƒÈ›irea dataseturilor.

<Youtube id="tqfSFcPMgOI"/>

## Slicing È™i dicing asupra datelor[[slicing-and-dicing-our-data]]

Asemenea Pandas, ğŸ¤— Datasets oferÄƒ mai multe funcÈ›ii pentru a manipula conÈ›inutul obiectelor `Dataset` È™i `DatasetDict`. Am Ã®ntÃ¢lnit deja metoda `Dataset.map()` Ã®n [Capitolul 3](/course/chapter3), iar Ã®n aceastÄƒ secÈ›iune vom explora alte funcÈ›ii de care dispunem.

Ãn acest exemplu, vom folosi [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) gÄƒzduit pe [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), care conÈ›ine reviewurile pacienÈ›ilor privind diverse medicamente, alÄƒturi de bolile care sunt tratate È™i o evaluare de 10 stele a satisfacÈ›iei pacientului.

Ãn primul rÃ¢nd trebuie sÄƒ descÄƒrcÄƒm È™i sÄƒ extragem datele, ceea ce se poate de fÄƒcut cu comenzile `wget` È™i `unzip`:

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

Deoarece TSV este o variantÄƒ a CSV care foloseÈ™te taburi Ã®n loc de virgulÄƒ ca separator, putem Ã®ncÄƒrca aceste fiÈ™iere prin folosirea scriptului de Ã®ncÄƒrcare `csv` È™i specificarea argumentului `delimiter` Ã®n funcÈ›ia `load_dataset()` astfel:

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t este caracterul tab de Python
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

O practicÄƒ bunÄƒ atunci cÃ¢nd faceÈ›i orice fel de analizÄƒ a datelor este sÄƒ vÄƒ luaÈ›i un mic random sample pentru a Ã®nÈ›elege cu ce tip de date lucraÈ›i. Ãn ğŸ¤— Datasets, putem crea o colecÈ›ie aleatorie prin legarea funcÈ›iilor `Dataset.shuffle()` È™i `Dataset.select()`:

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# VizualizaÈ›i primele cÃ¢teva exemple
drug_sample[:3]
```

```python out
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

AtrageÈ›i atenÈ›ia cÄƒ am fixat seedul Ã®n `Dataset.shuffle()` pentru posibilitatea de reproducere. `Dataset.select()` se aÈ™teaptÄƒ la un iterabil cu indices, deci noi am scris `range(1000)` pentru a primi primele 1000 de exemple din datasetul amestecat. Din acest sample putem vedea cÃ¢teva ciudÄƒÈ›enii Ã®n datasetul nostru:

* Coloana `Unnamed: 0` are un aspect neobiÈ™nuit, care sugereazÄƒ cÄƒ este un anonymized ID a fiecÄƒrui pacient.
* Coloana `condition` conÈ›ine labeluri majuscule È™i minuscule.
* Recenziile au lungimi variate È™i conÈ›in caractere Python precum `\r\n` ÅŸi caractere HTML ca `&\#039;`.

Hai sÄƒ vedem cum putem folosi ğŸ¤— Datasets pentru a face faÈ›Äƒ fiecÄƒrei dintre aceste probleme. Pentru a testa ipoteza identificÄƒrii pacientului pentru coloana `Unnamed: 0`, putem folosi funcÈ›ia `Dataset.unique()` pentru a verifica dacÄƒ numÄƒrul de ID-uri corespunde cu numÄƒrul de rÃ¢nduri Ã®n fiecare split:

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

Aceasta pare sÄƒ confirme ipoteza, deci putem curÄƒÈ›a datasetul puÈ›in, redenumind coloana `Unnamed: 0` pentru a-i da un nume mai interpretabil. Putem folosi funcÈ›ia `DatasetDict.rename_column()` pentru a renumea coloana Ã®n acelaÈ™i timp Ã®n ambele splituri:

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

> [!TIP]
> âœï¸ **ÃncearcÄƒ!** FolosiÈ›i funcÈ›ia `Dataset.unique()` pentru a gÄƒsi numÄƒrul de medicamente È™i condiÈ›ii unice Ã®n seturile de antrenare È™i testare.

Ãn continuare, vom normaliza toate `condition` labels folosind `Dataset.map()`. La fel cum am fÄƒcut cu tokenizarea Ã®n [Capitolul 3](/course/chapter3), putem defini o funcÈ›ie simplÄƒ care poate fi aplicatÄƒ pe toate rÃ¢ndurile fiecÄƒrui split din `drug_dataset`:

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}


drug_dataset.map(lowercase_condition)
```

```python out
AttributeError: 'NoneType' object has no attribute 'lower'
```

Oh no, am Ã®ntÃ¢mpinat o problemÄƒ cu funcÈ›ia map! Din eroarea noastrÄƒ se poate deduce cÄƒ unele intrÄƒri din coloana `condition` sunt `None`, care nu pot fi convertite la caracterul mic pentru cÄƒ nu sunt string-uri. Vom elimina aceste rÃ¢nduri folosind `Dataset.filter()`, care funcÈ›ioneazÄƒ Ã®n mod similar cu `Dataset.map()` È™i se aÈ™teaptÄƒ o funcÈ›ie care primeÈ™te un exemplu al datasetului.

Ãn loc de a scrie o funcÈ›ie explicitÄƒ ca:

```py
def filter_nones(x):
    return x["condition"] is not None
```

È™i apoi sÄƒ rulÄƒm `drug_dataset.filter(filter_nones)`, putem face acest lucru Ã®ntr-o linie folosind o _funcÈ›ie lambda_. Ãn Python, funcÈ›iile lambda sunt funcÈ›ii mici care pot fi definite fÄƒrÄƒ a le numi. Ele au forma generalÄƒ:

```
lambda <argumente> : <expresie>
```

unde `lambda` este unul dintre [cuvintele cheie](https://docs.python.org/3/reference/lexical_analysis.html#keywords) Python, `<argumente>` reprezintÄƒ o listÄƒ/set de valori separate prin virgulÄƒ care definesc inputurile funcÈ›iei È™i `<expresie>` reprezintÄƒ operaÈ›iile pe care dorim sÄƒ le executÄƒm. De exemplu, putem defini o funcÈ›ie lambda care ridicÄƒ un numÄƒr la pÄƒtrat:

```py
lambda x: x * x
```

Pentru a aplica aceastÄƒ funcÈ›ie la un input, trebuie sÄƒ Ã®i facem wrap È™i pe sÄƒ punem inputul Ã®n paranteze:

```py
(lambda x: x * x)(3)
```

```python out
9
```

La fel, putem defini funcÈ›ii lambda cu mai multe argumente prin separarea acestora prin virgulÄƒ. De exemplu, putem calcula suprafaÈ›a unui triunghi ca:

```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

FuncÈ›iile lambda sunt utile atunci cÃ¢nd dorim sÄƒ definim funcÈ›ii mici, pentru o singurÄƒ folosire (pentru mai multe informaÈ›ii despre ele, recomandÄƒm citirea excelentului [Real Python tutorial](https://realpython.com/python-lambda/) scris de Andre Burgaud). Ãn contextul ğŸ¤— Datasets, putem utiliza funcÈ›iile lambda pentru a defini operaÈ›ii simple de map È™i filter, astfel Ã®ncÃ¢t sÄƒ eliminÄƒm intrÄƒrile `None` din datasetul nostru:

```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

Cu intrÄƒrile `None` eliminate, putem normaliza coloana `condition`:

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# VerificÄƒm dacÄƒ lowercasing a funcÈ›ionat
drug_dataset["train"]["condition"][:3]
```

```python out
['left ventricular dysfunction', 'adhd', 'birth control']
```

FuncÈ›ioneazÄƒ! Acum cÄƒ am curÄƒÈ›at labelurile, sÄƒ vedem cum putem curÄƒÈ›i È™i recenziile.

## Crearea de noi coloane[[creating-new-columns]]

Atunci cÃ¢nd lucraÈ›i cu recenziile clienÈ›ilor, o practicÄƒ bunÄƒ este sÄƒ verificaÈ›i numÄƒrul de cuvinte Ã®n fiecare recenzie. O recenzie poate fi doar un singur cuvÃ¢nt, cum ar fi "Excelent!" sau un eseu complet care are sute de cuvinte È™i depinde de cazul pe care Ã®l aveÈ›i la vedere, aici trebuie sÄƒ vÄƒ asiguraÈ›i cÄƒ faceÈ›i faÈ›Äƒ acestor extreme diferit. Pentru a calcula numÄƒrul de cuvinte Ã®n fiecare recenzie, vom folosi un heuristic aproximativ bazat pe splittingul textului prin spaÈ›ii.

Vom defini o funcÈ›ie simplÄƒ care numÄƒrÄƒ numÄƒrul de cuvinte din fiecare recenzie:

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

Spre deosebire de funcÈ›ia `lowercase_condition()`, `compute_review_length()` returneazÄƒ un dicÈ›ionar ale cÄƒrui key nu corespund uneia dintre numele coloanelor din dataset. Ãn acest caz, atunci cÃ¢nd `compute_review_length()` este transmis Ã®n `Dataset.map()`, el va fi aplicat pe toate rÃ¢ndurile din dataset pentru a crea o nouÄƒ coloanÄƒ `review_length`:

```py
drug_dataset = drug_dataset.map(compute_review_length)
# InspectÄƒm primul exemplu de training
drug_dataset["train"][0]
```

```python out
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

AÈ™a cum era de aÈ™teptat, putem vedea o nouÄƒ coloanÄƒ `review_length` adÄƒugatÄƒ la setul de antrenare. Putem sorta aceastÄƒ nouÄƒ coloanÄƒ cu `Dataset.sort()` pentru a vedea cum valorile extreme aratÄƒ:

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python out
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

Precum am presupus, unele recenii conÈ›in doar un singur cuvÃ¢nt, ceea ce, deÈ™i ar putea fi OK pentru analiza sentimentului, nu ar fi informativ dacÄƒ vrem sÄƒ prezicem condiÈ›iei.

> [!TIP]
> ğŸ™‹ O alternativÄƒ la adÄƒugarea unei noi coloane Ã®ntr-un dataset este funcÈ›ia `Dataset.add_column()`. Aceasta permite sÄƒ oferiÈ›i coloana ca o listÄƒ Python sau array NumPy È™i poate fi utilÄƒ Ã®n situaÈ›ii Ã®n care `Dataset.map()` nu este bine adaptat pentru analiza dumneavoastrÄƒ.

Hai sÄƒ folosim funcÈ›ia `Dataset.filter()` pentru a elimina recenziile care conÈ›in mai puÈ›in de 30 de cuvinte. Similar cum am fÄƒcut Ã®n cazul coloanei `condition`, putem elimina recenziile foarte scurte cerÃ¢nd ca recenziile sÄƒ aibÄƒ o lungime mai mare decÃ¢t acest prag:

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python out
{'train': 138514, 'test': 46108}
```

DupÄƒ cum vedeÈ›i, aceasta a eliminat aproximativ 15% din recenziile noastrem, din seturile originale de antrenare È™i testare.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** FolosiÈ›i funcÈ›ia `Dataset.sort()` pentru a inspecta recenziile cu cele mai mari numere de cuvinte. Vezi [documentaÈ›ia](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.sort) pentru a vedea ce argument trebuie sÄƒ foloseÈ™ti pentru a sorta recenziile Ã®n ordine descrescÄƒtoare.

Ultima chestie de care trebuie sÄƒ ne ocupÄƒm este prezenÈ›a caracterelor HTML Ã®n recenziile noastre. Putem folosi modulul `html` din Python pentru a face unescape acestor caractere:

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python out
"I'm a transformer called BERT"
```

Vom folosi `Dataset.map()` pentru a face unescape toate caracterele HTML din corpus:

```py
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

Ãn mod evident, metoda `Dataset.map()` este foarte utilÄƒ pentru procesarea datelor â€“ È™i nu am abordat decÃ¢t o micÄƒ parte din ceea ce poate face!

## Superputerile metodei `map()`[[the-map-methods-superpowers]]

Metoda `Dataset.map()` acceptÄƒ un argument `batched` care, dacÄƒ este setat pe `True`, cauzeazÄƒ ca ea sÄƒ trimitÄƒ un batch de exemple la funcÈ›ia map Ã®n acelaÈ™i timp (dimensiunea batchului poate fi configuratÄƒ dar defaultul este 1.000). De exemplu, anterior am folosit o funcÈ›ie map care a fÄƒcut unescaped toate caracterele HTML din recenziile noastre È™i i-a luat cÃ¢teva secunde sÄƒ execute (puteÈ›i citi timpul pe progress bars). Putem accelera acest lucru prin procesarea mai multor elemente Ã®n acelaÈ™i timp folosind list comprehension.

CÃ¢nd specificaÈ›i `batched=True` funcÈ›ia primeÈ™te un dicÈ›ionar cu cÃ¢mpurile datasetului, dar fiecare valoare este acum _list of values_ È™i nu doar o singurÄƒ valoare. Valoarea de return a `Dataset.map()` ar trebui sÄƒ fie la fel: un dicÈ›ionar cu cÃ¢mpurile pe care dorim sÄƒ le actualizÄƒm sau adÄƒugÄƒm Ã®n datasetul nostru, È™i o listÄƒ de valori. De exemplu, mai jos este alt mod de a face unescape tuturor caracterelor HTML din recenziile noastre, folosind `batched=True`:

```py
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

DacÄƒ executaÈ›i acest cod Ã®ntr-un notebook, veÈ›i vedea cÄƒ aceastÄƒ comandÄƒ se executÄƒ mult mai rapid decÃ¢t cea anteriorÄƒ. È˜i nu pentru cÄƒ recenziile noastre au fost deja HTML-unescaped â€“ dacÄƒ reexecutaÈ›i instrucÈ›ia precedentÄƒ (fÄƒrÄƒ `batched=True`), ea va lua acelaÈ™i timp ca Ã®nainte. Acest lucru se datoreazÄƒ faptului cÄƒ list comprehension sunt  mai rapide decÃ¢t executarea aceluiaÈ™i cod Ã®ntr-un `for` loop, È™i am cÃ¢È™tigat, de asemenea, puÈ›inÄƒ performanÈ›p accesÃ¢nd multe elemente Ã®n acelaÈ™i timp, Ã®n loc unul cÃ¢te unul.

Folosirea `Dataset.map()` cu `batched=True` este esenÈ›ialÄƒ pentru a obÈ›ine viteza "rapidÄƒ" a tokenizerilor pe care Ã®i vom Ã®ntÃ¢lni Ã®n [Capitolul 6](/course/chapter6), care pot repede sÄƒ tokenizeze listelor mari de texte. De exemplu, pentru tokenizarea tuturor recenziilor medicamentelor cu un tokenizer rapid, putem folosi o funcÈ›ie ca aceasta:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

AÈ™a cum am vÄƒzut Ã®n [Capitolul 3](/course/chapter3), putem transmite un singur sau cÃ¢teva exemple cÄƒtre tokenizer, aÈ™adar putem folosi aceastÄƒ funcÈ›ie cu sau fÄƒrÄƒ `batched=True`. Hai sÄƒ ne folosim de aceastÄƒ oportunitate È™i sÄƒ comparÄƒm performanÈ›a diferitelor opÈ›iuni. Ãntr-un notebook puteÈ›i mÄƒsura timpul unei instrucÈ›iie de o line prin adÄƒugarea `%time` Ã®naintea acelei linii de cod pe care doriÈ›i sÄƒ o mÄƒsuraÈ›i:

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

PuteÈ›i È™i sÄƒ mÄƒsuraÈ›i un Ã®ntreg cell prin scrierea `%%time` la Ã®nceputul celulei. Ãn hardware-ul pe care l-am executat, acest lucru a arÄƒtat 10.8s pentru aceastÄƒ instrucÈ›ie (este numÄƒrul scris dupÄƒ "Wall time").

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** ExecutaÈ›i aceeaÈ™i instrucÈ›ie cu È™i fÄƒrÄƒ `batched=True`, apoi Ã®ncercaÈ›i-o cu un tokenizer lent (adaugaÈ›i `use_fast=False` Ã®n metoda `AutoTokenizer.from_pretrained()`), astfel sÄƒ puteÈ›i vedea ce numere obÈ›ineÈ›i pe hardwareul vostru.

Aici sunt rezultatele pe care le-am obÈ›inut cu È™i fÄƒrÄƒ batching, folosind un tokenizer rapid È™i lent:

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

Aceasta Ã®nseamnÄƒ cÄƒ utilizarea unui tokenizer rapid cu opÈ›iunea `batched=True` este de 30 de ori mai rapidÄƒ decÃ¢t varianta lentÄƒ fÄƒrÄƒ batching - acest lucru este pur È™i simplu uimitor! Acesta este motivul principal pentru care tokenizerii rapizi sunt setaÈ›i implicit cÃ¢nd se utilizeazÄƒ `AutoTokenizer` (È™i de ce sunt numiÈ›i "rapizi"). Ei pot atinge o asemenea acceleraÈ›ie datoritÄƒ faptului cÄƒ codul de tokenizare este executat Ã®n Rust, care este un limbaj care faciliteazÄƒ paralelizarea execuÈ›iei.

Parallelization este È™i motivul pentru care tokenizerul rapid realizeazÄƒ o accelerare de aproape 6 ori cu batching: nu puteÈ›i paraleliza o singurÄƒ operaÈ›ie de tokenizare, dar atunci cÃ¢nd doriÈ›i sÄƒ tokenizaÈ›i multe texte Ã®n acelaÈ™i timp, puteÈ›i sÄƒ faceÈ›i split execuÈ›iei pe mai multe procese, fiecare rÄƒspunzÃ¢nd pentru propriile texte.

`Dataset.map()` are È™i o capacitate de parallelization proprie. Deoarece nu sunt susÈ›inute de Rust, nu pot sÄƒ le ofere aceeaÈ™i acceleraÈ›ie tokenizerilori Ã®nceÈ›i ca tokenizerilor rapizi, dar pot Ã®ncÄƒ fi utili (Ã®n special dacÄƒ utilizaÈ›i un tokenizer care nu are o variantÄƒ rapidÄƒ). Pentru a activa multiprocessingul, folosiÈ›i argumentul `num_proc` È™i specificaÈ›i numÄƒrul de procese sÄƒ fie utilizate Ã®n apelul `Dataset.map()`:

```py
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

PuteÈ›i experimenta puÈ›in cu timpii pentru a determina numÄƒrul de procese optime; Ã®n cazul nostru 8 s-a dovedit a produce cea mai mare acceleraÈ›ie. Aici sunt rezultatele pe care le-am obÈ›inut cu È™i fÄƒrÄƒ multiprocessing:

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s
`batched=True`, `num_proc=8`  | 6.52s          | 41.3s
`batched=False`, `num_proc=8` | 9.49s          | 45.2s

Aceste rezultate sunt mult mai bune pentru tokenizerul lent, dar È™i performanÈ›a tokenizerului rapid a fost semnificativ Ã®mbunÄƒtÄƒÈ›itÄƒ. Cu toate acestea, trebuie sÄƒ reamintim cÄƒ aceasta nu va fi Ã®ntotdeauna cazul - testele noastre au arÄƒtat cÄƒ este mai rapid sÄƒ utilizaÈ›i `batched=True` fÄƒrÄƒ acest argument Ã®n cazurile Ã®n care valoarea lui `num_proc` diferÄƒ de 8. Ãn general, nu vÄƒ recomandÄƒm utilizarea multiplicÄƒrii proceselor pentru tokenizorii rapizi cu `batched=True`.

<tip>

Utilizarea `num_proc` pentru a accelera procesarea este de obicei o idee excelentÄƒ, atÃ¢ta timp cÃ¢t funcÈ›ia pe care o utilizaÈ›i nu utilizeazÄƒ deja multiprocessing.

</tip>

Toate aceste funcÈ›ionalitÄƒÈ›i condensate Ã®ntr-o singurÄƒ metodÄƒ este foarte impresionant, dar asta nu e totul! Cu `Dataset.map()` È™i `batched=True` puteÈ›i modifica numÄƒrul de elemente din datasetul dumneavoastrÄƒ. Acesta este extrem de util Ã®n numeroase situaÈ›ii Ã®n care doriÈ›i sÄƒ creaÈ›i mai multe caracteristici de antrenare dintr-un singur exemplu, È™i vom avea nevoie de acest lucru ca parte a preprocesÄƒrii pentru cÃ¢teva dintre sarcinile NLP pe care le vom discuta Ã®n [Capitolul 7](/course/chapter7).

<tip>

ğŸ’¡ Ãn machine learning, un _exemplu_ este de obicei definit ca fiind un set de _features_ care se oferÄƒ modelului. Ãn unele contexte, acestea vor fi seturile de coloane dintr-un `Dataset`, dar Ã®n altele (ca È™i aici È™i pentru rÄƒspunderea la Ã®ntrebÄƒri) mai multe caracteristici pot fi extrase dintr-un singur exemplu È™i sÄƒ aparÈ›inÄƒ unei singure coloane.

</tip>

Hai sÄƒ vedem cum funcÈ›ioneazÄƒ! Aici vom tokeniza exemplele È™i le vom face truncatela lungimea maximÄƒ de 128, dar vom cere tokenizerului sÄƒ returneze *toate* chunkurile de text Ã®n loc de prima. Acest lucru poate fi fÄƒcut cu `return_overflowing_tokens=True`:

```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

Hai sÄƒ testÄƒm acest lucru pe un exemplu Ã®nainte de a folosi `Dataset.map()` pentru Ã®ntreg datasetul:

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python out
[128, 49]
```

AÈ™adar, primul nostru exemplu din setul de antrenare a devenit douÄƒ features pentru cÄƒ a fost tokenizat mai mult decÃ¢t lungimea maximÄƒ de tokenuri pe care am specificat-o: prima cu lungimea 128 È™i a doua cu lungimea 49. Acum trebuie sÄƒ facem acest lucru pentru toate elementele din dataset!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python out
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

Oh no! Acesta nu a funcÈ›ionat! De ce? Citind eroarea vom afla motivul: existenÈ›a unei incompatibilitÄƒÈ›i Ã®n lungimea uneia dintre coloane - una cu o lungime de 1,463 È™i alta de 1,000. DacÄƒ aÈ›i privit [documentaÈ›ia](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) Dataset.map(), puteÈ›i sÄƒ vÄƒ reamintiÈ›i cÄƒ este numÄƒrul de sampleuri care sunt oferite funcÈ›iei pe care noi le facem mapping; aici aceste 1,000 exemplare creeazÄƒ 1,463 features noi, ceea ce duce la un shape error.

Problema este cÄƒ Ã®ncercÄƒm sÄƒ amestecÄƒm douÄƒ dataseturi cu mÄƒrimi diferite: coloanele `drug_dataset` vor avea un numÄƒr determinat de exemple (cele 1,000 din eroare), dar `tokenized_dataset` pe care Ã®l construim va fi mai mare (cel cu 1,463 din eroare; el este mai mare decÃ¢t 1,000 pentru cÄƒ tokenizÄƒm reviewrile lungi Ã®n mai multe exemple folosind `return_overflowing_tokens=True`). Acest lucru nu funcÈ›ioneazÄƒ pentru un `Dataset`, aÈ™adar trebuie sÄƒ eliminÄƒm sau sÄƒ modificÄƒmm coloanele din datasetul vechi pentru a se potrivi dimensiunea cu cea din noul dataset. Putem face ultima din cele douÄƒ opÈ›iuni folosind argumentul `remove_columns`:

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

Acum acest lucru funcÈ›ioneazÄƒ fÄƒrÄƒ erori. Putem verifica cÄƒ noul dataset are mai multe elemente decÃ¢t datasetul original prin compararea lungimilor:

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

Am menÈ›ionat cÄƒ putem rezolva problema lungimilor diferite are coloanelor prin schimbarea vechilor coloane la aceeaÈ™i dimensiune cu cele noi. Pentru aceasta, vom avea nevoie de cÃ¢mpul `overflow_to_sample_mapping` returnat de tokenizer atunci cÃ¢nd setÄƒm `return_overflowing_tokens=True`. El ne oferÄƒ un mapping de la un nou feature index la indicele sampleului din care a provenit. Prin intermediul acesta, putem asocia fiecÄƒrei key prezentÄƒ Ã®n datasetul original cu o listÄƒ de valori de dimensiune corectÄƒ prin repetarea valorilor fiecÄƒrui exemplu atÃ¢ta timp cÃ¢t produce caracteristici noi:

```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extragem maparea Ã®ntre noul È™i vechiul indice
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

Putem observa cÄƒ funcÈ›ioneazÄƒ cu `Dataset.map()` fÄƒrÄƒ ca noi sÄƒ avem nevoie sÄƒ eliminÄƒm coloanele vechi:

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

ObÈ›inem acelaÈ™i numÄƒr de features training ca È™i Ã®nainte, dar acum am pÄƒstrat toate cÃ¢mpurile vechi. DacÄƒ ai nevoie de ele pentru post-procesare dupÄƒ aplicarea modelului, ar fi util sÄƒ folosiÈ›i aceastÄƒ abordare.

Acum aÈ›i Ã®nvÄƒÈ›at cum pot fi utilizate ğŸ¤— Datasets pentru preprocesarea datelor prin metode diferite. DeÈ™i funcÈ›iile de preprocesare ale ğŸ¤— Datasets vor acoperi majoritatea nevoilor de antrenare a modelului,
existÄƒ momente Ã®n care veÈ›i avea nevoie sÄƒ treceÈ›i la Pandas pentru accesul la caracteristici mai puternice, cum ar fi `DataFrame.groupby()` sau high-level APIs pentru vizualizare. Din fericire, ğŸ¤— Dataset a fost proiectat astfel Ã®ncÃ¢t sÄƒ fie interoperabil cu biblioteci precum Pandas, NumPy, PyTorch, TensorFlow È™i JAX. Hai sÄƒ vedem cum se face acest lucru.

## De la `Dataset`s la `DataFrame`s È™i Ã®napoi[[from-datasets-to-dataframes-and-back]]

<Youtube id="tfcY1067A5Q"/>

Pentru a permite conversia Ã®ntre diferite biblioteci, ğŸ¤— Datasets oferÄƒ o funcÈ›ia `Dataset.set_format()`. AceastÄƒ funcÈ›ie schimbÄƒ doar _output format_ al datasetului, deci puteÈ›i uÈ™or sÄƒ treceÈ›i la un alt format fÄƒrÄƒ a afecta _data format_ de bazÄƒ, care este Apache Arrow. Formatarea se face direct. Pentru a demonstra acest lucru, hai sÄƒ convertim datasetul nostru Ã®n Pandas:

```py
drug_dataset.set_format("pandas")
```

Acum, atunci cÃ¢nd accesÄƒm elementele din dataset, obÈ›inem `pandas.DataFrame` Ã®n loc de un dicÈ›ionar:

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

Ãn continuare, vom crea un `pandas.DataFrame` pentru Ã®ntregul set de antrenare prin selectarea tuturor elementelor din `drug_dataset["train"]`:

```py
train_df = drug_dataset["train"][:]
```

> [!TIP]
> ğŸš¨ Ãn spatele scenei, `Dataset.set_format()` schimbÄƒ formatul returnat pentru  `__getitem__()` dunder method a datasetului. Asta Ã®nseamnÄƒ cÄƒ atunci cÃ¢nd dorim sÄƒ creÄƒm un nou obiect ca `train_df` dintr-un `Dataset` Ã®n formatul `"pandas"`, trebuie sÄƒ tÄƒiem Ã®ntreg datasetul pentru a obÈ›ine un `pandas.DataFrame`. PuteÈ›i verifica voi Ã®nÅŸivÄƒ cÄƒ tipul lui `drug_dataset["train"]` este `Dataset`, indiferent de output format.

Acum putem utiliza toate funcÈ›ionalitÄƒÈ›ile Pandas pe care le dorim. De exemplu, putem face fancy chaining pentru a calcula distribuÈ›ia clasei printre intrÄƒrile `condition`:

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>


È˜i odatÄƒ ce suntem gata cu analiza noastrÄƒ Pandas, putem crea Ã®ntotdeauna un nou obiect `Dataset` prin utilizarea funcÈ›iei `Dataset.from_pandas()`:


```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```


> [!TIP]
> âœï¸ **ÃncercaÈ›i!** CalculaÈ›i media ratingului per medicament È™i salvaÈ›i rezultatul Ã®ntr-un nou `Dataset`.


Acest lucru completeazÄƒ turul nostru de tehnici de preprocesare disponibile Ã®n ğŸ¤— Datasets. Pentru a finisa secÈ›iunea, vom crea un set de validare pentru a pregÄƒti datasetul pentru antrenarea unui clasificator. Ãnainte de a face asta, noi vom reseta output formatul `drug_dataset` de la `"pandas"` la `"arrow" :


```python
drug_dataset.reset_format()
```

## Crearea unui set de validare [[creating-a-validation-set]]

DeÈ™i avem deja un set de testare pe care Ã®l putem folosi pentru evaluare, este o bunÄƒ practicÄƒ sÄƒ lÄƒsÄƒm setul de test neschimbat È™i sÄƒ creÄƒm un set de validare Ã®n timpul developmentului. OdatÄƒ ce sunteÈ›i fericiÈ›i cu performanÈ›a modelului pe setul de validare, puteÈ›i face o verificare finalÄƒ a setului de test. Acest proces ajutÄƒ la reducerea riscului ca sÄƒ vÄƒ adaptaÈ›i prea mult setul de test È™i sÄƒ depuneÈ›i un model care poate eÈ™ua analizÃ¢nd date reale.

ğŸ¤— Datasets oferÄƒ o funcÈ›ie `Dataset.train_test_split()` bazatÄƒ pe funcÈ›ionalitatea celebrÄƒ din `scikit-learn`. O vom folosi pentru a Ã®mpÄƒrÈ›i setul nostru de antrenare Ã®n split-uri de `train` È™i `validation` (setÄƒm argumentul `seed` pentru reproductabilitate):

```python
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# RenumeÈ™te implicit "test" split-ul la "valide"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# AdaugÄƒ setul de test Ã®n `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

Foarte bine, am pregÄƒtit acum un dataset care este gata pentru antrenarea unor modele! Ãn [SecÈ›iunea 5](/course/chapter5/5) vom arÄƒta cum puteÈ›i Ã®ncÄƒrca seturile de date Ã®n Hugging Face Hub, dar Ã®n acest moment hai sÄƒ punem capÄƒt analizei noastre prin examinarea a cÃ¢teva modalitÄƒÈ›i de salvare a seturilor de date pe dispozitivele locale.

## Salvarea unui dataset[[saving-a-dataset]]

<Youtube id="blF9uxYcKHo"/>

DeÈ™i ğŸ¤— Datasets va face cache fiecÄƒrui dataset È™i a operaÈ›iilor efectuate asupra acestuia, existÄƒ momente Ã®n care veÈ›i dori sÄƒ salvaÈ›i un dataset pe disc (de exemplu, Ã®n cazul Ã®n care cache-ul se È™terge). DupÄƒ cum vedeÈ›i Ã®n tabelul de mai jos, ğŸ¤— Datasets oferÄƒ trei funcÈ›ii principale pentru salvarea datelor Ã®n formate diferite:

| Format de date | FuncÈ›ie |
| :---------: | :--------------------: |
|    Arrow    | `Dataset.save_to_disk()` |
|     CSV     |    `Dataset.to_csv()`    |
|    JSON     |   `Dataset.to_json()`    |

Spre exemplu, hai sÄƒ salvÄƒm datasetul nostru curÄƒÈ›at Ã®n formatul Arrow:

```python
drug_dataset_clean.save_to_disk("drug-reviews")
```

Acest lucru va crea un folder cu urmÄƒtoarea structurÄƒ:

```
drug-reviews/
â”œâ”€â”€ dataset_dict.json
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ dataset.arrow
â”‚   â”œâ”€â”€ dataset_info.json
â”‚   â””â”€â”€ state.json
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ dataset.arrow
â”‚   â”œâ”€â”€ dataset_info.json
â”‚   â”œâ”€â”€ indices.arrow
â”‚   â””â”€â”€ state.json
â””â”€â”€ validation
    â”œâ”€â”€ dataset.arrow
    â”œâ”€â”€ dataset_info.json
    â”œâ”€â”€ indices.arrow
    â””â”€â”€ state.json
```

unde se poate vedea cÄƒ fiecare split este asociat cu propriul sÄƒu tabel `dataset.arrow`, iar unele metadate Ã®n `dataset_info.json` È™i `state.json`. PoÈ›i sÄƒ te gÃ¢ndeÈ™ti la formatul Arrow ca fiind un tabel fancy de coloane È™i rÃ¢nduri, optimizat pentru construirea aplicaÈ›iilor high-performance care proceseazÄƒ È™i transportÄƒ dataseturi mari.

OdatÄƒ ce setul de date este salvat, putem Ã®ncÄƒrca-o folosind funcÈ›ia `load_from_disk()` urmÄƒtoarea:

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

Pentru formatele CSV È™i JSON, trebuie sÄƒ stocaÈ›i fiecare split Ã®ntr-un fiÈ™ier separat. Un mod de a face acest lucru estw iterarea asupra cheilor È™i valorilor obiectului `DatasetDict`:

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

Acesta salveazÄƒ fiecare split Ã®n [JSON Lines format](https://jsonlines.org), unde fiecare rÃ¢nd din setul de date este stocat ca o singurÄƒ linie JSON. Aici puteÈ›i vedea cum aratÄƒ primul exemplu:

```bash
!head -n 1 drug-reviews-train.jsonl
```

```python out
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

Putem apoi folosi tehnicile de la [SecÈ›iunea 2](/course/chapter5/2) pentru Ã®ncÄƒrcarea fiÈ™ierelor JSON:

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

È˜i Äƒsta este finalul excursiei noastre Ã®n lumea manipulÄƒrii datelor cu ğŸ¤— Datasets! Acum cÄƒ avem un dataset curat, pregÄƒtit pentru antrenarea unui model, aici sunt cÃ¢teva idei pe care le puteÈ›i Ã®ncerca:

1. FolosiÈ›i tehnicile din [Capitolul 3](/course/chapter3) pentru a antrena un classifier care poate prezice starea pacientului pe baza recenziei medicamentului.
2. FolosiÈ›i pipelineul `summarization` din [Capitolul 1](/course/chapter1) pentru a genera rezumate ale recenziilor.

Ãn urmÄƒtoarea secÈ›iune, vom vedea cum ğŸ¤— Datasets vÄƒ permite sÄƒ lucraÈ›i cu dataseturi mari fÄƒrÄƒ ca laptopul tÄƒu sÄƒ explodeze :)!