# Big data? ğŸ¤— Datasets vine Ã®n ajutor![[big-data-datasets-to-the-rescue]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[ 
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
]} />

Ãn prezent, nu este de neaÈ™teptat sÄƒ te confrunÈ›i cu dataseturi de cÃ¢È›iva gigabytes, mai ales dacÄƒ planifici sÄƒ preantreenezi un transformer ca BERT sau GPT-2 de la zero. Ãn aceste cazuri, chiar _loading_ a datelor poate fi o provocare. De exemplu, corpusul WebText folosit pentru a preantreena GPT-2 conÈ›ine peste 8 milioane de documente È™i 40 GB de text â€“ Ã®ncÄƒrcarea acestuia Ã®n memoria RAM a laptopului tÄƒu este probabil sÄƒ-i facÄƒ un atac cardiac!

Norocul este cÄƒ Datasets ğŸ¤— a fost proiectat pentru a depÄƒÈ™i aceste limitÄƒri. El te elibereazÄƒ de problemele de gestionare a memoriei, tratarea dataseturilor ca fiÈ™iere _memory-mapped_, È™i limitele hard driveului prin _streamingul_ intrÄƒrilor dintr-un corpus.

<Youtube id="JwISwTCPPWo"/>

Ãn aceastÄƒ secÈ›iune vom explora aceste caracteristici ale ğŸ¤—Datasets cu un corpus de 825 GB numit [Pile](https://pile.eleuther.ai). SÄƒ Ã®ncepem!

## Ce este Pile?[[what-is-the-pile]]

Pile este un corpus de text englezesc creat de [EleutherAI](https://www.eleuther.ai) pentru antrenarea large-scale language models. Acesta include o varietate diversÄƒ de dataseturi, cuprinzÃ¢nd articole È™tiinÈ›ifice, repositoriuri GitHub È™i texte web filtrate. Corpusul de antrenare este disponibil Ã®n chunkuri de [14 GB](https://the-eye.eu/public/AI/pile/), dar Ã®n acelaÈ™i timp puteÈ›i descÄƒrca È™i cÃ¢teva dintre [componenetele 
individuale](https://the-eye.eu/public/AI/pile_preliminary_components/). SÄƒ Ã®ncepem prin examinarea datasetului PubMed Abstracts, care este un corpus de rezumate din 15 milioane de publicaÈ›ii È™tiinÈ›ifice biomedicale de pe [PubMed](https://pubmed.ncbi.nlm.nih.gov/). Datasetul este Ã®n format JSON È™i este comprimat cu librÄƒria `zstandard`, aÈ™adar prima datÄƒ ne trebuie sÄƒ o instalÄƒm pe aceasta:

```py
!pip install zstandard
```

Ãn continuare, putem Ã®ncÄƒrca datasetul utilizÃ¢nd metoda pentru fiÈ™ierele remote pe care am Ã®nvÄƒÈ›at-o Ã®n [secÈ›iunea 2](/course/chapter5/2):

```py
from datasets import load_dataset

# Acest lucru dureazÄƒ cÃ¢teva minute, aÈ™adar poÈ›i sÄƒ te duci sÄƒ Ã®È›i iei un ceai sau o cafea Ã®ntre timp :))
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

Putem observa cÄƒ existÄƒ 15.518.009 de linii È™i douÄƒ coloane Ã®n datasetul nostru â€“ e foarte mult!

> [!TIP]
> âœ De abia acum, ğŸ¤— Datasets va descompresa fiÈ™ierele necesare pentru Ã®ncÄƒrcarea datasetului. DacÄƒ doriÈ›i sÄƒ salvaÈ›i spaÈ›iu pe hard drive-ul dvs. , puteÈ›i transmite `DownloadConfig(delete_extracted=True)` la argumentul `download_config` al `load_dataset()`. VedeÈ›i mai multe detalii Ã®n [documentaÈ›ie](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig).

Acum hai sÄƒ analizÄƒm conÈ›inutul primei linii:

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Okay, acesta pare a fi un rezumat dintr-un articol medical. SÄƒ vedem cÃ¢t de mult spaÈ›iu RAM am folosit pentru Ã®ncÄƒrcarea datasetului!

## Magia memory mappingului[[the-magic-of-memory-mapping]]

Una dintre modalitÄƒÈ›ile simple de mÄƒsurare a utilizÄƒrii memoriei Ã®n Python este cu biblioteca `psutil`, care poate fi instalatÄƒ cu `pip`:

```python
!pip install psutil
```

Biblioteca oferÄƒ o clasÄƒ `Process` ce ne permite sÄƒ verificÄƒm utilizarea memoriei procesului curent astfel:

```py
import psutil

# Process.memory_info este exprimat Ã®n bytes, aÈ™adar convertim la megabyte
print(f"Utilizare RAM: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
Utilizarea RAM: 5678.33 MB
```

Aici `rss` se referÄƒ la _resident set size_ , care este partea memoriei ocupatÄƒ de proces Ã®n RAM. AceastÄƒ mÄƒsurare include È™i memoria folositÄƒ de Python interpreter È™i librariile pe care le-am Ã®ncÄƒrcat, aÈ™adar cantitatea realÄƒ de memorie utilizatÄƒ pentru Ã®ncÄƒrcarea datelor este puÈ›in mai micÄƒ. Pentru comparaÈ›ie, putem sÄƒ vedem cÃ¢t de mare este datasetul pe disc folosind atributul `dataset_size`. Deoarece rezultatul este exprimat Ã®n bytes, putem sÄƒ Ã®l convertim manual la gigabyte:

```py
print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
```


Nice â€“ deÈ™i este aproximativ 20 GB, putem Ã®ncÄƒrca È™i accesa datasetul cu mult mai puÈ›in RAM!

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** AlegeÈ›i una dintre [subseturile](https://the-eye.eu/public/AI/pile_preliminary_components/) din Pile care este mai mare decÃ¢t memoria RAM a laptopului sau dispozitivului tÄƒu, Ã®ncÄƒrcaÈ›i-o cu ğŸ¤— Datasets È™i mÄƒsuraÈ›i cantitatea de memorie folositÄƒ. Pentru o mÄƒsurare precisÄƒ, veÈ›i dori sÄƒ faceÈ›i acest lucru Ã®ntr-un proces nou. PuteÈ›i gÄƒsi dimensiunile decomprimate ale fiecÄƒrui subset Ã®n Tabelul 1 din [Pile paper](https://arxiv.org/abs/2101.00027).

DacÄƒ sunteÈ›i familiarizaÈ›i cu Pandas, rezultatul acesta poate veni ca o surprizÄƒ din cauza celebrei [rule of thumbÄƒ](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) al lui Wes Kinney, care spune cÄƒ Ã®n mod normal aveÈ›i nevoie de 5 pÃ¢nÄƒ la 10 ori mai mult spaÈ›iu pe RAM decÃ¢t mÄƒrimea datasetului. Deci ğŸ¤— Datasets aceastÄƒ problemÄƒ de memory management? ğŸ¤— Datasets trateazÄƒ fiecare dataset ca un [memory-mapped file](https://en.wikipedia.org/wiki/Memory-mapped_file), care oferÄƒ un mapping Ã®ntre spaÈ›iul RAM È™i stocarea pe sistem, ceea ce permite bibliotecii sÄƒ acceseze È™i sÄƒ opereze asupra elementelor datasetului fÄƒrÄƒ a trebui sÄƒ-l Ã®ncarce Ã®n totalitate Ã®n memorie.

Memory-mapped files pot fi È™i distribuite Ã®ntre mai multe procese, ceea ce permite metodelor cum ar fi `Dataset.map()` sÄƒ fie parallelized fÄƒrÄƒ necesitatea mutÄƒrii sau copierii datasetului. Ãn spatele acestor facilitÄƒÈ›i se aflÄƒ [formatul de memorie Apache Arrow](https://arrow.apache.org) È™i biblioteca [`pyarrow`](https://arrow.apache.org/docs/python/index.html), care realizeazÄƒ Ã®ncÄƒrcarea datelor È™i procesarea la viteze fulgerÄƒtoare. (Pentru mai multe detalii despre Apache Arrow È™i compararea sa cu Pandas, vÄƒ rugÄƒm sÄƒ citiÈ›i [blogul lui Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) Pentru a vedea acest lucru Ã®n acÈ›iune, hai sÄƒ Ã®ncercÄƒm un speed test prin iterarea asupra tuturor elementelor din datasetul PubMed Abstracts:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

Aici am folosit modulul `timeit` al Python pentru a mÄƒsura timpul de execuÈ›ie necesar pentru a rula `code_snippet`. Ãn mod normal veÈ›i putea trece peste un dataset la viteze de cÃ¢teva sute de MB/s pÃ¢nÄƒ la cÃ¢È›iva GB/s. Acest lucru funcÈ›ioneazÄƒ bine pentru majoritatea aplicaÈ›iilor, dar uneori veÈ›i avea nevoie sÄƒ lucraÈ›i cu un dataset care este prea mare ca sÄƒ Ã®ncapÄƒ pe hard driveul laptopului tÄƒu. De exemplu, dacÄƒ am Ã®ncerca sÄƒ descarcÄƒm Pile Ã®n Ã®ntregime, am avea nevoie de 825 GB de spaÈ›iu liber! Pentru a vÄƒ ajuta cu astfel de cazuri, ğŸ¤— Datasets oferÄƒ o feature de streaming care permite accesarea È™i descÄƒrcarea elementelor, fÄƒrÄƒ a trebui sÄƒ descÄƒrcaÈ›i Ã®ntregul dataset. Hai sÄƒ vedem cum funcÈ›ioneazÄƒ!

> [!TIP]
> ğŸ’¡Ãn Jupyter notebooks poÈ›i sÄƒ mÄƒsori timpul unei celule utilizÃ¢nd [funcÈ›ia magicÄƒ `%%timeit`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).

## Streamingul dataseturilor[[streaming-datasets]]

Pentru a activa dataset streaming este suficient sÄƒ daÈ›i argumentul `streaming=True` funcÈ›iei `load_dataset()`. De exemplu, sÄƒ Ã®ncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm datasetul PubMed Abstracts Ã®n streaming mode:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Ãn locul `Dataset`-ului obiÈ™nuit pe care l-am Ã®ntÃ¢lnit pÃ¢nÄƒ acum Ã®n acest capitol, obiectul returnat cu `streaming=True` este un `IterableDataset`. DupÄƒ nume putem deduce cÄƒ pentru a accesa elementele dintr-un `IterableDataset`, trebuie sÄƒ iterÄƒm prin el. Prin urmare, putem accesa primul element al streamed datased astfel:

```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Elementele unui streamed dataset pot fi procesate din mers folosind `IterableDataset.map()`, ceea ce este util Ã®n timpul antrenÄƒrii dacÄƒ aveÈ›i nevoie sÄƒ tokenizeÈ›i inputurile. Procesarea se face exact la fel ca È™i Ã®n [Capitolul 3](/course/chapter3), cu singura deosebire fiind cÄƒ rezultatele sunt returnate una cÃ¢te una:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

> [!TIP]
> ğŸ’¡ Pentru a accelera tokenizarea cu streaming puteÈ›i seta `batched=True`, ca È™i Ã®n secÈ›iunea precedentÄƒ. Acest lucru va procesa exemplele, batch cu batch; dimensiunea implicitÄƒ a batchului este de 1,000 È™i poate fi specificatÄƒ cu argumentul `batch_size`.

De asemenea, puteÈ›i amesteca un streamed dataset utilizÃ¢nd `IterableDataset.shuffle()`, dar faÈ›Äƒ de `Dataset.shuffle()` acest lucru va amesteca doar elementele dintr-un `buffer_size` predefinit:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

Ãn acest exemplu, am selectat un exemplu aleatoriu din primele 10,000 exemple din buffer. OdatÄƒ ce un exemplu este accesat, locul lui Ã®n buffer este completat cu urmÄƒtorul exemplu din corpus (adica exemplarul 10,001 Ã®n cazul de mai sus). PuteÈ›i selecta elemente dintr-un streamed dataset utilizÃ¢nd funcÈ›iile `IterableDataset.take()` È™i `IterableDataset.skip()`, care acÈ›ioneazÄƒ Ã®n mod similar cu `Dataset.select()`. De exemplu, pentru a selecta primele 5 exemple din PubMed Abstracts dataset putem face urmÄƒtorul lucru:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

Ãn mod similar puteÈ›i utiliza funcÈ›ia `IterableDataset.skip()` pentru a crea splituri de antrenare È™i validare dintr-un set de date amestecat astfel:

```py
# SÄƒriÈ›i primele 1,000 exemple È™i includeÈ›i restul Ã®n setul de antrenare
train_dataset = shuffled_dataset.skip(1000)
# LuaÈ›i primele 1,000 de exemple pentru setul de validare
validation_dataset = shuffled_dataset.take(1000)
```

Hai sÄƒ terminÄƒm explorarea streamingului asupra datasetului cu o aplicaÈ›ie comunÄƒ: combinarea multiplelor dataseturi Ã®mpreunÄƒ pentru a crea un singur corpus. ğŸ¤— Datasets oferÄƒ o funcÈ›ie `interleave_datasets()` care converteÈ™te o listÄƒ de obiecte `IterableDataset` Ã®ntr-un singur `IterableDataset`, unde elementele noului datasetsunt obÈ›inute prin alternarea Ã®ntre exemplele sursÄƒ. AceastÄƒ funcÈ›ie este utilÄƒ, Ã®n special atunci cÃ¢nd Ã®ncercaÈ›i sÄƒ combinaÈ›i dataseturi mari, ca exemplu vom face stream al subsetul FreeLaw din Pile, care reprezintÄƒ un dataset de 51 GB de opinii juridice din instanÈ›e din SUA:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

AceastÄƒ dataset este suficient de mare Ã®ncÃ¢t sÄƒ punÄƒ presiune asupra RAM-ului a majoritÄƒÈ›ii laptopurilor, dar am reuÈ™it sÄƒ Ã®l Ã®ncarcÄƒm È™i sÄƒ Ã®l accesÄƒm fÄƒrÄƒ sÄƒ ne facem griji. Acum hai sÄƒ combinÄƒm exemplele din dataseturile FreeLaw È™i PubMed Abstracts cu funcÈ›ia `interleave_datasets()`:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

Aici am folosit funcÈ›ia `islice()` din modulul Python `itertools` pentru a selecta primele douÄƒ exemple din datasetul combinat, È™i putem vedea cÄƒ acestea corespund primelor exemple din fiecare dintre cele douÄƒ dataseturi originale.

Ãn final, dacÄƒ doriÈ›i sÄƒ faceÈ›i streaming la Pile Ã®n Ã®ntregime (825 GB), puteÈ›i rula toate fiÈ™ierele pregÄƒtite dupÄƒ Ã®n acest mod:

```py
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play â€œSurvival of the Tastiestâ€ on Android, and on the web...'}
```

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** UtilizaÈ›i unul dintre cele mari corpusuri Common Crawl ca [`mc4`](https://huggingface.co/datasets/mc4) sau [`oscar`](https://huggingface.co/datasets/oscar) pentru a crea un streaming dataset multilingv care reprezintÄƒ proporÈ›ia limbii vorbite Ã®ntr-o È›arÄƒ aleasÄƒ de tine. De exemplu, cele patru limbi naÈ›ionale din ElveÈ›ia sunt germana, franceza, italiana È™i romansha, aÈ™adar puteÈ›i Ã®ncerca sÄƒ creaÈ›i un corpus elveÈ›ian prin samplingul subseturilor Oscar Ã®n funcÈ›ie de proporÈ›ia lor vorbitÄƒ.

Acum aveÈ›i toate instrumentele necesare pentru a Ã®ncÄƒrca È™i procesa dataseturi de orice formÄƒ È™i dimensiune â€“ dar, din pÄƒcate, va veni un moment Ã®n care veÈ›i trebui sÄƒ creaÈ›i voi Ã®nÈ™ivÄƒ un dataset pentru a rezolva problema pe care o aveÈ›i. Acesta este subiectul urmÄƒtoarei secÈ›iuni!
