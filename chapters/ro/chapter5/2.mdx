# Ce fac dacÄƒ dataset-ul meu nu este pe Hub?[[what-if-my-dataset-isnt-on-the-hub]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

È˜tiÈ›i cum sÄƒ folosiÈ›i [Hugging Face Hub](https://huggingface.co/datasets) pentru a descÄƒrca dataseturi, dar vÄƒ veÈ›i gÄƒsi adesea lucrÃ¢nd cu date care sunt stocate fie pe laptopul dumneavoastrÄƒ, fie pe un server remote. Ãn aceastÄƒ secÈ›iune vÄƒ vom arÄƒta cum poate fi utilizat ğŸ¤— Datasets pentru a Ã®ncÄƒrca dataseturi care nu sunt disponibile pe Hugging Face Hub.

<Youtube id="HyQgpJTkRdE"/>

## LucrÃ¢nd cu dataseturi locale È™i remote[[working-with-local-and-remote-datasets]]

ğŸ¤— Datasets oferÄƒ scripturi de Ã®ncÄƒrcare pentru a gestiona Ã®ncÄƒrcarea dataseturilor locale È™i remote. SuportÄƒ mai multe formate de date comune, cum ar fi:

|    Data format     | Loading script |                         Example                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Text files     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

AÈ™a cum se aratÄƒ Ã®n tabel, pentru fiecare format de date trebuie doar sÄƒ specificÄƒm tipul scriptului de Ã®ncÄƒrcare Ã®n funcÈ›ia `load_dataset()`, alÄƒturi de un argument `data_files` care specificÄƒ calea cÄƒtre unul sau mai multe fiÈ™iere. SÄƒ Ã®ncepem prin Ã®ncÄƒrcarea unui dataset din fiÈ™iere locale; mai tÃ¢rziu vom vedea cum sÄƒ facem acelaÈ™i lucru cu fiÈ™iere remote.

## ÃncÄƒrcarea unui dataset local[[loading-a-local-dataset]]

Pentru acest exemplu vom folosi [datasetul SQuAD-it](https://github.com/crux82/squad-it/), care este un dataset la scarÄƒ largÄƒ pentru Ã®ntrebÄƒri È™i rÄƒspunsuri Ã®n italianÄƒ.

Seturile de antrenare È™i test sunt gÄƒzduite pe GitHub, deci le putem descÄƒrca cu o comandÄƒ simplÄƒ `wget`:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Aceasta va descÄƒrca douÄƒ fiÈ™iere comprimate numite *SQuAD_it-train.json.gz* È™i *SQuAD_it-test.json.gz*, pe care le putem decomprima cu comanda Linux `gzip`:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

Putem vedea cÄƒ fiÈ™ierele comprimate au fost Ã®nlocuite cu _SQuAD_it-train.json_ È™i _SQuAD_it-test.json_, È™i cÄƒ datele sunt stocate Ã®n formatul JSON.

> [!TIP]
> âœ DacÄƒ vÄƒ Ã®ntrebaÈ›i de ce existÄƒ un caracter `!` Ã®n comenzile shell de mai sus, aceasta este pentru cÄƒ le rulÄƒm Ã®ntr-un notebook Jupyter. Pur È™i simplu eliminaÈ›i prefixul dacÄƒ doriÈ›i sÄƒ descÄƒrcaÈ›i È™i sÄƒ dezarhivaÈ›i datasetul Ã®ntr-un terminal.

Pentru a Ã®ncÄƒrca un fiÈ™ier JSON cu funcÈ›ia `load_dataset()`, trebuie doar sÄƒ È™tim dacÄƒ avem de-a face cu JSON obiÈ™nuit (similar cu un dicÈ›ionar imbricat) sau JSON Lines (JSON separat pe linii). Ca multe dataseturi de Ã®ntrebÄƒri È™i rÄƒspunsuri, SQuAD-it foloseÈ™te formatul imbricat, cu tot textul stocat Ã®ntr-un cÃ¢mp `data`. Aceasta Ã®nseamnÄƒ cÄƒ putem Ã®ncÄƒrca datasetul specificÃ¢nd argumentul `field` dupÄƒ cum urmeazÄƒ:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Ãn mod implicit, Ã®ncÄƒrcarea fiÈ™ierelor locale creeazÄƒ un obiect `DatasetDict` cu un split `train`. Putem vedea acest lucru prin inspectarea obiectului `squad_it_dataset`:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Aceasta ne aratÄƒ numÄƒrul de rÃ¢nduri È™i numele coloanelor asociate cu setul de antrenare. Putem vizualiza unul dintre exemple prin indexarea Ã®n splitul `train` dupÄƒ cum urmeazÄƒ:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si Ã¨ verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Excelent, am Ã®ncÄƒrcat primul nostru dataset local! Dar Ã®n timp ce acest lucru a funcÈ›ionat pentru setul de antrenare, ceea ce dorim cu adevÄƒrat este sÄƒ includem atÃ¢t spliturile `train` cÃ¢t È™i `test` Ã®ntr-un singur obiect `DatasetDict` astfel Ã®ncÃ¢t sÄƒ putem aplica funcÈ›iile `Dataset.map()` pe ambele splituri deodatÄƒ. Pentru a face acest lucru, putem furniza un dicÈ›ionar argumentului `data_files` care mapeazÄƒ fiecare nume de split la un fiÈ™ier asociat cu acel split:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Acesta este exact ceea ce am dorit. Acum, putem aplica diverse tehnici de preprocesare pentru a curÄƒÈ›a datele, tokeniza recenziile È™i aÈ™a mai departe.

> [!TIP]
> Argumentul `data_files` al funcÈ›iei `load_dataset()` este destul de flexibil È™i poate fi fie o singurÄƒ cale de fiÈ™ier, o listÄƒ de cÄƒi de fiÈ™iere, sau un dicÈ›ionar care mapeazÄƒ numele spliturilor la cÄƒile fiÈ™ierelor. De asemenea, puteÈ›i folosi glob pentru fiÈ™iere care se potrivesc unui model specificat conform regulilor folosite de shell-ul Unix (de exemplu, puteÈ›i face glob pentru toate fiÈ™ierele JSON dintr-un director ca un singur split prin setarea `data_files="*.json"`). ConsultaÈ›i [documentaÈ›ia](https://huggingface.co/docs/datasets/loading#local-and-remote-files) ğŸ¤— Datasets pentru mai multe detalii.

Scripturile de Ã®ncÄƒrcare din ğŸ¤— Datasets suportÄƒ de fapt decomprimarea automatÄƒ a fiÈ™ierelor de intrare, deci am fi putut sÄƒ sÄƒrim peste folosirea `gzip` prin indicarea argumentului `data_files` direct cÄƒtre fiÈ™ierele comprimate:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Acest lucru poate fi util dacÄƒ nu doriÈ›i sÄƒ decomprimaÈ›i manual multe fiÈ™iere GZIP. Decomprimarea automatÄƒ se aplicÄƒ È™i altor formate comune precum ZIP È™i TAR, deci trebuie doar sÄƒ pointaÈ›i `data_files` cÄƒtre fiÈ™ierele comprimate È™i sunteÈ›i gata!

Acum cÄƒ È™tiÈ›i cum sÄƒ Ã®ncÄƒrcaÈ›i fiÈ™iere locale pe laptopul sau desktop-ul dumneavoastrÄƒ, sÄƒ aruncÄƒm o privire la Ã®ncÄƒrcarea fiÈ™ierelor remote.

## ÃncÄƒrcarea unui dataset remote[[loading-a-remote-dataset]]

DacÄƒ lucraÈ›i ca data scientist sau programator Ã®ntr-o companie, existÄƒ o È™ansÄƒ bunÄƒ ca dataseturile pe care doriÈ›i sÄƒ le analizaÈ›i sÄƒ fie stocate pe un server remote. Din fericire, Ã®ncÄƒrcarea fiÈ™ierelor remote este la fel de simplÄƒ ca Ã®ncÄƒrcarea celor locale! Ãn loc sÄƒ furnizaÈ›i o cale cÄƒtre fiÈ™iere locale, pointaÈ›i argumentul `data_files` al `load_dataset()` cÄƒtre unul sau mai multe URL-uri unde sunt stocate fiÈ™ierele remote. De exemplu, pentru datasetul SQuAD-it gÄƒzduit pe GitHub, putem pur È™i simplu sÄƒ pointÄƒm `data_files` cÄƒtre URL-urile _SQuAD_it-*.json.gz_ dupÄƒ cum urmeazÄƒ:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Aceasta returneazÄƒ acelaÈ™i obiect `DatasetDict` obÈ›inut mai sus, dar ne economiseÈ™te pasul de a descÄƒrca È™i decomprima manual fiÈ™ierele _SQuAD_it-*.json.gz_. Aceasta Ã®ncheie incursiunea noastrÄƒ Ã®n diversele modalitÄƒÈ›i de Ã®ncÄƒrcare a dataseturilor care nu sunt gÄƒzduite pe Hugging Face Hub. Acum cÄƒ avem un dataset cu care sÄƒ ne jucÄƒm, sÄƒ explorÄƒm diverse tehnici de manipulare a datelor!

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** AlegeÈ›i un alt dataset gÄƒzduit pe GitHub sau [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) È™i Ã®ncercaÈ›i sÄƒ Ã®l Ã®ncÄƒrcaÈ›i atÃ¢t local cÃ¢t È™i remote folosind tehnicile introduse mai sus. Pentru puncte bonus, Ã®ncercaÈ›i sÄƒ Ã®ncÄƒrcaÈ›i un dataset care este stocat Ã®n format CSV sau text (consultaÈ›i [documentaÈ›ia](https://huggingface.co/docs/datasets/loading#local-and-remote-files) pentru mai multe informaÈ›ii despre aceste formate).




