<FrameworkSwitchCourse {fw} />

# CÄƒutare semanticÄƒ cu FAISS[[semantic-search-with-faiss]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
]} />

{/if}

Ãn [secÈ›iunea 5](/course/chapter5/5), am creat un dataset cu issues È™i comentarii din repositoriul ğŸ¤— Datasets. Ãn aceastÄƒ secÈ›iune, vom utiliza aceste informaÈ›ii pentru a construi un motor de cÄƒutare care ne poate ajuta sÄƒ gÄƒsim rÄƒspunsurile la  cele mai importante cele mai importante Ã®ntrebÄƒri despre bibliotecÄƒ!

<Youtube id="OATCgQtNX2o"/>

## Utilizarea embeddings pentru cÄƒutare semanticÄƒ[[using-embeddings-for-semantic-search]]

DupÄƒ cum am vÄƒzut Ã®n [Capitolul 1](/course/chapter1), Transformer-based language models reprezintÄƒ fiecare token Ã®ntr-un fragment de text ca un _embedding vector_. S-a dovedit cÄƒ se poate face "pool" embeddingurilor individuale pentru a crea o reprezentare vectorialÄƒ pentru fraze Ã®ntregi, paragrafe sau (Ã®n anumite cazuri) documente. Aceste embeddings pot fi apoi utilizate pentru a gÄƒsi documente similare Ã®n corpus prin calcularea dot-product similarity (sau a unei alte metrice de similaritate) Ã®ntre fiecare embedding È™i returnarea documentelor cu cea mai mare suprapunere.

Ãn aceastÄƒ secÈ›iune, vom utiliza embeddings pentru a dezvolta un motor de cÄƒutare semanticÄƒ. Aceste motoare de cÄƒutare oferÄƒ mai multe avantaje faÈ›Äƒ de abordÄƒrile convenÈ›ionale bazate pe cÄƒutarea de cuvinte cheie Ã®ntr-un query cu documente.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="CÄƒutare semanticÄƒ."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="CÄƒutare semanticÄƒ."/>
</div>

## ÃncÄƒrcarea È™i pregÄƒtirea datasetului[[loading-and-preparing-the-dataset]]

Prima lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ descÄƒrcÄƒm datasetul nostru cu GitHub issues, aÈ™a cÄƒ folosim funcÈ›ia `load_dataset()` ca de obicei:

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

Aici am specificat splitul default `train` Ã®n `load_dataset()`, astfel Ã®ncÃ¢t returneazÄƒ un `Dataset` Ã®n loc de `DatasetDict`. Primul lucru care treubuie fÄƒcut este sÄƒ filtrÄƒm pull requesturile, deoarece acestea rareori tind sÄƒ fie utilizate pentru a rÄƒspunde la Ã®ntrebÄƒrile utilizatorilor È™i vor introduce noise Ã®n motorul nostru de cÄƒutare. AÈ™a cum ar trebuie deja sÄƒ È™tiÈ›i, putem utiliza funcÈ›ia `Dataset.filter()` pentru a exclude aceste rÃ¢nduri din datasetul nostru. Ãn timp ce suntem aici, putem sÄƒ filtrÄƒm È™i rÃ¢ndurile fÄƒrÄƒ comentari, deoarece acestea nu oferÄƒ niciun rÄƒspuns la Ã®ntrebÄƒrile utilizatorilor:

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    caracteristici: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

Putem vedea cÄƒ existÄƒ multe coloane Ã®n datasetul nostru, majoritatea dintre care nu sunt necesare pentru a construi motorul nostru de cÄƒutare. Din perspectiva cÄƒutÄƒrii, cele mai informative coloane sunt `title`, `body` È™i `comments`, Ã®n timp ce `html_url` ne oferÄƒ un link Ã®napoi la problema sursÄƒ. Hai sÄƒ utilizÄƒm funcÈ›ia `Dataset.remove_columns()` pentru a elimina restul:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

Pentru a crea embeddedurile noastre, vom completa fiecare comentariu cu titlul È™i body-ul problemei, deoarece aceste cÃ¢mpuri adesea includ informaÈ›ii contextuale utile. Deoarece coloana noastrÄƒ `comments` este Ã®n prezent o listÄƒ de comentarii pentru fiecare issue, trebuie sÄƒ "explodÄƒm" coloana, astfel Ã®ncÃ¢t fiecare rÃ¢nd sÄƒ fie format dintr-un tuple `(html_url, title, body, comment)`. Ãn Pandas, putem face acest lucru cu funcÈ›ia [`DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), care creeazÄƒ un rÃ¢nd nou pentru fiecare element dintr-o coloanÄƒ asemÄƒnÄƒtoare cu o listÄƒ, Ã®n timp ce copiazÄƒ toate celelalte valori ale coloanelor. Pentru a vedea acest lucru Ã®n acÈ›iune, sÄƒ trecem la formatul pandas `DataFrame` mai Ã®ntÃ¢i:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

DacÄƒ inspectÄƒm primul rÃ¢nd din acest `DataFrame`, putem vedea cÄƒ existÄƒ patru comentarii asociate acestei probleme:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in ï¼š\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

CÃ¢nd facem explode `df`, ne aÈ™teptÄƒm sÄƒ obÈ›inem un rÃ¢nd pentru fiecare dintre aceste comentarii. HaideÈ›i sÄƒ verificÄƒm dacÄƒ Äƒsta e cazul:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in ï¼š\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

Great, putem vedea cÄƒ rÃ¢ndurile au fost reproduse, cu coloanele `comments` incluzÃ¢nd È™i comentariile individuale. Acum cÄƒ am terminat cu Pandas, noi putem sÄƒ schimÄƒm rapid Ã®napoi la un `Dataset` Ã®nÄƒrcÃ¢nd `DataFrame` Ã®n memorie:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

Okay, acest lucru ne-a oferit cÃ¢teva mii de comentarii cu care sÄƒ lucrÄƒm!


> [!TIP]
> âœï¸ **ÃncercaÈ›i!** Vezi dacÄƒ poÈ›i utiliza `Dataset.map()` pentru a exploda coloana `comments` din `issues_dataset` _fÄƒrÄƒ_ a recurge la utilizarea Pandas. Acest lucru este puÈ›in dificil; s-ar putea sÄƒ gÄƒsiÈ›i utilÄƒ secÈ›iunea ["Mapping batch"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) din documentaÈ›ia ğŸ¤— Datasets pentru aceastÄƒ sarcinÄƒ.

Acum cÄƒ avem un singur comentariu pe rÃ¢nd, sÄƒ creÄƒm o nouÄƒ coloanÄƒ `comments_length` care conÈ›ine numÄƒrul de cuvinte din fiecare comentariu:

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

Putem utiliza aceastÄƒ nouÄƒ coloanÄƒ pentru a filtra comentariile scurte, care de obicei includ lucruri precum "cc @lewtun" sau "MulÈ›umesc!" care nu sunt relevante pentru motorul nostru de cÄƒutare. Nu existÄƒ un numÄƒr precis care trebuie selectat pentru filtru, dar aproximativ 15 cuvinte pare a fi un bun punct de plecare:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

DupÄƒ ce am curÄƒÈ›at puÈ›in setul nostru de date, putem sÄƒ concatenÄƒm titlul, descrirea È™i comentariile problemei Ã®mpreunÄƒ Ã®ntr-o nouÄƒ coloanÄƒ `text`. Ca de obicei, vom scrie o funcÈ›ie simplÄƒ pe care o putem transmite Ã®n `Dataset.map()`:

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)
```

Suntem Ã®n final pregÄƒtiÈ›i sÄƒ creÄƒm niÈ™te embeddings! HaideÈ›i sÄƒ vedem cum facem acest lucru.

## Crearea embeddings-urilor de text [[creating-text-embeddings]]

Am vÄƒzut Ã®n [Capitolul 2](/course/chapter2) cÄƒ putem obÈ›ine token embeddings prin utilizarea clasei `AutoModel`. Tot ce trebuie sÄƒ facem este sÄƒ alegem un checkpoint potrivit pentru a Ã®ncÄƒrca modelul. Din fericire, existÄƒ o bibliotecÄƒ numitÄƒ `sentence-transformers` care se ocupÄƒ de crearea embeddingurilor. AÈ™a cum se descrie Ã®n [documentaÈ›ia](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) bibliotecii, cazul nostru este un exemplu de _cÄƒutare semanticÄƒ asimetricÄƒ_ deoarece avem o Ã®ntrebare scurtÄƒ al cÄƒrei rÄƒspuns ne-ar plÄƒcea sÄƒ Ã®l gÄƒsim Ã®ntr-un document mai lung, precum un comentariu la un issue. [Tabelul] (https://www.sbert.net/docs/pretrained_models.html#model-overview) util de prezentare a modelului din documentaÈ›ie indicÄƒ faptul cÄƒ checkpointul `multi-qa-mpnet-base-dot-v1` are cea mai bunÄƒ performanÈ›Äƒ pentru cÄƒutarea semanticÄƒ, aÈ™a cÄƒ Ã®l vom folosi acesta pentru aplicaÈ›ia noastrÄƒ. De asemenea, vom Ã®ncÄƒrca tokenizer-ul folosind acelaÈ™i checkpoint:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

Pentru a accelera procesul de embedding, este util sÄƒ punem modelul È™i inputurile pe un GPU, deci hai sÄƒ facem asta acum:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

NotÄƒm cÄƒ am setat `from_pt=True` ca argument al metodei `from_pretrained()`. Acest lucru se datoreazÄƒ faptului cÄƒ checkpoint-ul `multi-qa-mpnet-base-dot-v1` are doar PyTorch weights, astfel Ã®ncÃ¢t setarea `from_pt=True` le va converti automat Ã®n format TensorFlow pentru noi. DupÄƒ cum puteÈ›i vedea, este foarte simplu sÄƒ treci de la un framework la altul Ã®n ğŸ¤— Transformers!

{/if}

DupÄƒ cum am menÈ›ionat anterior, dorim sÄƒ reprezentÄƒm fiecare intrare din corpusul nostru de GitHub issues sub forma unui vector, astfel Ã®ncÃ¢t avem nevoie sÄƒ "agregÄƒm" sau sÄƒ facem media la tokem embeddings Ã®ntr-un anumit mod. O abordare popularÄƒ este de a efectua *CLS pooling* pe outputurile modelului nostru, unde pur È™i simplu colectÄƒm ultimul stadiu ascuns pentru tokenul special `[CLS]`. UrmÄƒtoarea funcÈ›ie face acest lucru pentru noi:

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

Ãn continuare, vom crea o funcÈ›ie ajutÄƒtoare care va tokeniza o listÄƒ de documente, va plasa tensorii pe GPU, Ã®i va alimenta Ã®n model È™i, Ã®n final, va aplica CLS pooling la outputuri:

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Putem testa funcÈ›ia oferindui prima intrare de text Ã®n corpusul nostru È™i analizÃ¢nd output shapeul:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

Minunat, am transformat prima intrare din corpusul nostru Ã®ntr-un vector de 768 de dimensiuni! Putem utiliza `Dataset.map()` pentru a aplica funcÈ›ia noastrÄƒ `get_embeddings()` la fiecare rÃ¢nd din corpusul nostru, astfel Ã®ncÃ¢t sÄƒ creÄƒm o nouÄƒ coloanÄƒ `embeddings` Ã®n acest mod:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Putem testa funcÈ›ia oferindui prima intrare de text Ã®n corpusul nostru È™i analizÃ¢nd output shapeul:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

Minunat, am transformat prima intrare din corpusul nostru Ã®ntr-un vector de 768 de dimensiuni! Putem utiliza `Dataset.map()` pentru a aplica funcÈ›ia noastrÄƒ `get_embeddings()` la fiecare rÃ¢nd din corpusul nostru, astfel Ã®ncÃ¢t sÄƒ creÄƒm o nouÄƒ coloanÄƒ `embeddings` Ã®n acest mod:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

ObservÄƒm cÄƒ am transformat embeddingurile Ã®n matrice NumPy -- acest lucru este necesar deoarece biblioteca ğŸ¤— Datasets cere acest format atunci cÃ¢nd Ã®ncercÄƒm sÄƒ indexÄƒm cu FAISS, ceea ce vom face Ã®n continuare.

## Utilizarea FAISS pentru cÄƒutare de similaritate eficientÄƒ[[using-faiss-for-efficient-similarity-search]]

Acum cÄƒ avem un dataset de embeddings, avem nevoie de o modalitate de a cÄƒuta printre ele. Pentru a face acest lucru, vom utiliza o structurÄƒ de date specialÄƒ din ğŸ¤— Datasets, numitÄƒ _FAISS index_. [FAISS](https://faiss.ai/) (prescurtat de la Facebook AI Similarity Search) este o bibliotecÄƒ care oferÄƒ algoritmi eficienÈ›i pentru cÄƒutarea rapidÄƒ È™i clusteringul al embedding vectors.

Ideea de bazÄƒ din spatele FAISS este crearea unei structuri de date speciale numite _index_, care permite gÄƒsirea embeddingurilor similare cu un input embedding. Crearea unui index FAISS Ã®n ğŸ¤— Datasets este simplu -- utilizÄƒm funcÈ›ia `Dataset.add_faiss_index()` È™i specificÄƒm care coloanÄƒ a datasetului nostru dorim sÄƒ indexÄƒm:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

Acum putem efectua queries pe acest index realizÃ¢nd o cÄƒutare a celor mai apropiaÈ›i vecini cu funcÈ›ia `Dataset.get_nearest_examples()`. HaideÈ›i sÄƒ testÄƒm acest lucru prin Ã®ncorporarea unei Ã®ntrebÄƒri astfel:

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

La fel ca È™i Ã®n cazul documentelor, acum avem un vector de 768 de dimensiuni care reprezintÄƒ query-ul, pe care Ã®l putem compara cu Ã®ntregul corpus pentru a gÄƒsi embeddingurile cele mai similare:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

FuncÈ›ia `Dataset.get_nearest_examples()` returneazÄƒ un tuple cu scorurile care clasificÄƒ suprapunerea dintre query È™i document, È™i un set corespunzÄƒtor de sampleuri (Ã®n acest caz, cele 5 match-uri). HaideÈ›i sÄƒ colectÄƒm acestea Ã®ntr-un `pandas.DataFrame` pentru a le putea sorta cu uÈ™urinÈ›Äƒ:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Acum putem itera peste primele rÃ¢nduri pentru a veadea cÃ¢t de bine un query se potriveÈ™te cu comentariile disponibile:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Nu-i rÄƒu! A doua Ã®ncercare se pare cÄƒ se potriveÈ™te cu query-ul!

> [!TIP]
> âœï¸ **ÃncearcÄƒ!** CreeazÄƒ propriul tÄƒu query È™i vezi dacp poÈ›i gÄƒsi un rÄƒspuns È™i sÄƒ extragi documentele. S-ar putea sÄƒ trebuieÈ™ti sÄƒ creÈ™ti parametrul `k` Ã®n `Dataset.get_nearest_examples()` pentru a mÄƒri cÄƒutarea.