# Crearea propriului tÄƒu dataset[[creating-your-own-dataset]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section5.ipynb"},
]} />

Uneori, datasetul necesar pentru a construi o aplicaÈ›ie NLP nu existÄƒ, astfel Ã®ncÃ¢t veÈ›i trebui sÄƒ-l creaÈ›i singuri. Ãn aceastÄƒ secÈ›iune vom arÄƒta cum sÄƒ creaÈ›i un corpus de [GitHub issues](https://github.com/features/issues/), care sunt utilizate Ã®n mod obiÈ™nuit pentru a urmÄƒri erorile sau feature-urile din repositoriile GitHub. Acest corpus poate fi folosit pentru diverse scopuri, inclusiv:

* Explorarea timpului necesar pentru Ã®nchiderea unor issues deschise sau pull requesturi
* Antrenarea unui _multilabel classifier_ care poate eticheta issue-urile cu metadate pe baza descrierii issue-urilor (de exemplu, "bug", "enhancement" sau "question")
* Crearea unui motor de cÄƒutare semanticÄƒ pentru a gÄƒsi care issues se potrivesc query-ului utilizatorului

Ãn aceastÄƒ secÈ›iune ne vom focusa pe crearea corpusului, È™i Ã®n urmÄƒtoarea vom aborda aplicaÈ›ia motorului de cÄƒutare semantic. Pentru a pÄƒstra lucrurile meta, vom folosi issue-urile GitHub asociate cu un proiect open source popular: ğŸ¤— Datasets! SÄƒ vedem cum sÄƒ obÈ›inem datele È™i sÄƒ explorÄƒm informaÈ›iile conÈ›inute Ã®n aceste issue-uri.

## ObÈ›inerea datelor[[getting-the-data]]

PuteÈ›i gÄƒsi toate issue-urile din ğŸ¤— Datasets navigÃ¢nd cÄƒtre tabul [Issues](https://github.com/huggingface/datasets/issues) al repositorului. AÈ™a cum aratÄƒ urmÄƒtorul screenshot, la momentul scrierii acestui text existau 331 de issues deschise È™i 668 Ã®nchise.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues.png" alt="Issue-urile GitHub asociate cu ğŸ¤— Datasets." width="80%"/>
</div>

DacÄƒ aÈ›i da clic pe una dintre aceste issue-uri veÈ›i gÄƒsi cÄƒ aceasta conÈ›ine un titlu, o descriere È™i un set de labeluri care caracterizeazÄƒ issue-ul. Un exemplu este prezentat Ã®n screenshotul urmÄƒtor.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues-single.png" alt="Un issue tipic Ã®n GitHub din repositoriul ğŸ¤— Datasets." width="80%"/>
</div>

Pentru a descÄƒrca toate issue-urile din repositoriu, vom folosi [GitHub REST API](https://docs.github.com/en/rest) pentru a enumera [`Issues` endpoint](https://docs.github.com/en/rest/reference/issues#list-repository-issues). Aceast endpoint returneazÄƒ o listÄƒ de obiecte JSON, cu fiecare obiect conÈ›inÃ¢nd un numÄƒr mare de cÃ¢mpuri care includ titlul È™i descrierea precum È™i metadata despre starea issue-ului È™i aÈ™a mai departe.

Un mod convenabil de descÄƒrcare a issue-urilor este prin utilizarea librÄƒriei `requests`, care este modalitatea standard pentru a face cereri HTTP Ã®n Python. PuteÈ›i instala libraria rulÃ¢nd comanda:

```python
!pip install requests
```

OdatÄƒ cu instalarea librariei, puteÈ›i face cereri GET la `Issues` endpoint prin invocarea funcÈ›iei `requests.get()`. De exemplu, puteÈ›i rula urmÄƒtorul cod pentru a obÈ›ine primul issue din prima paginÄƒ:

```py
import requests

url = "https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1"
response = requests.get(url)
```

Obiectul `response` conÈ›ine o cantitate mare de informaÈ›ii utile despre requestul efectuat, inclusiv HTTP status code:

```py
response.status_code
```

```python out
200
```

unde statusul `200` Ã®nseamnÄƒ cÄƒ cererea a fost reuÈ™itÄƒ (puteÈ›i gÄƒsi o listÄƒ completÄƒ de status coduri [aici](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)). De ceea ce suntem Ã®nsÄƒ interesaÈ›i este _payload_, care poate fi accesat Ã®n diverse formaturi precum bytes, string sau JSON. Deoarece È™tim cÄƒ issue-urile noastre sunt Ã®n format JSON, sÄƒ inspectÄƒm payload-ul astfel:

```py
response.json()
```

```python out
[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',
  'repository_url': 'https://api.github.com/repos/huggingface/datasets',
  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/labels{/name}',
  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/comments',
  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/events',
  'html_url': 'https://github.com/huggingface/datasets/pull/2792',
  'id': 968650274,
  'node_id': 'MDExOlB1bGxSZXF1ZXN0NzEwNzUyMjc0',
  'number': 2792,
  'title': 'Update GooAQ',
  'user': {'login': 'bhavitvyamalik',
   'id': 19718818,
   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',
   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',
   'gravatar_id': '',
   'url': 'https://api.github.com/users/bhavitvyamalik',
   'html_url': 'https://github.com/bhavitvyamalik',
   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',
   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',
   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',
   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',
   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',
   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',
   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',
   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',
   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',
   'type': 'User',
   'site_admin': False},
  'labels': [],
  'state': 'open',
  'locked': False,
  'assignee': None,
  'assignees': [],
  'milestone': None,
  'comments': 1,
  'created_at': '2021-08-12T11:40:18Z',
  'updated_at': '2021-08-12T12:31:17Z',
  'closed_at': None,
  'author_association': 'CONTRIBUTOR',
  'active_lock_reason': None,
  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2792',
   'html_url': 'https://github.com/huggingface/datasets/pull/2792',
   'diff_url': 'https://github.com/huggingface/datasets/pull/2792.diff',
   'patch_url': 'https://github.com/huggingface/datasets/pull/2792.patch'},
  'body': '[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.',
  'performed_via_github_app': None}]
```

Uau, aceasta e o cantitate mare de informaÈ›ie! Putem vedea cÃ¢mpuri utile cum ar fi `title`, `body` È™i `number` care descriu problema, precum È™i informaÈ›ii despre utilizatorul GitHub care a deschis issue-ul.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** FaceÈ›i clic pe cÃ¢teva dintre URL-urile din payload-ul JSON de mai sus pentru a vÄƒ familiariza cu tipul de informaÈ›ii cÄƒtre care se face referire pentru fiecare GitHub issue.

DupÄƒ cum este descris Ã®n [documentaÈ›ia](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting) GitHub, solicitÄƒrile neautentificate sunt limitate la 60 de solicitÄƒri pe orÄƒ. DeÈ™i puteÈ›i creÈ™te `per_page` query parameter pentru a reduce numÄƒrul de solicitÄƒri pe care le faceÈ›i, oricum veÈ›i atinge limita pentru orice repository care are mai mult de cÃ¢teva mii de issues. Prin urmare, ar trebui sÄƒ urmaÈ›i [instrucÈ›iunile](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) GitHub pentru crearea unui _personal access token_ astfel Ã®ncÃ¢t sÄƒ puteÈ›i creÈ™te limita la 5.000 de solicitÄƒri pe orÄƒ. OdatÄƒ ce aveÈ›i tokenul, Ã®l puteÈ›i include ca parte a request header:

```py
GITHUB_TOKEN = xxx  # Copy your GitHub token here
headers = {"Authorization": f"token {GITHUB_TOKEN}"}
```

> [!WARNING]
> âš ï¸ Nu oferiÈ›i nimÄƒnui un notebook cu `GITHUB_TOKEN` Ã®n el . VÄƒ recomandÄƒm sÄƒ È™tergeÈ›i ultima celulÄƒ odatÄƒ ce aÈ›i executat-o pentru a evita scurgerea accidentalÄƒ a acestor informaÈ›ii. Chiar mai bine, stocaÈ›i tokenul Ã®ntr-un fiÈ™ier *.env* È™i utilizaÈ›i biblioteca `python-dotenv` pentru a Ã®l Ã®ncÄƒrca automat ca variabilÄƒ de mediu.

Acum cÄƒ avem tokenul de acces, hai sÄƒ creÄƒm o funcÈ›ie care sÄƒ poatÄƒ descÄƒrca toate issue-urile dintr-un repositoriu GitHub:

```py
import time
import math
from pathlib import Path
import pandas as pd
from tqdm.notebook import tqdm


def fetch_issues(
    owner="huggingface",
    repo="datasets",
    num_issues=10_000,
    rate_limit=5_000,
    issues_path=Path("."),
):
    if not issues_path.is_dir():
        issues_path.mkdir(exist_ok=True)

    batch = []
    all_issues = []
    per_page = 100  # Number of issues to return per page
    num_pages = math.ceil(num_issues / per_page)
    base_url = "https://api.github.com/repos"

    for page in tqdm(range(num_pages)):
        # Query with state=all to get both open and closed issues
        query = f"issues?page={page}&per_page={per_page}&state=all"
        issues = requests.get(f"{base_url}/{owner}/{repo}/{query}", headers=headers)
        batch.extend(issues.json())

        if len(batch) > rate_limit and len(all_issues) < num_issues:
            all_issues.extend(batch)
            batch = []  # Flush batch for next time period
            print(f"Reached GitHub rate limit. Sleeping for one hour ...")
            time.sleep(60 * 60 + 1)

    all_issues.extend(batch)
    df = pd.DataFrame.from_records(all_issues)
    df.to_json(f"{issues_path}/{repo}-issues.jsonl", orient="records", lines=True)
    print(
        f"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl"
    )
```

Acum cÃ¢nd apelÄƒm `fetch_issues()` va descÄƒrca toate problemele Ã®n batch-uri pentru a evita depÄƒÈ™irea limitei GitHub pe numÄƒrul de solicitÄƒri pe orÄƒ; rezultatul va fi stocat Ã®ntr-un fiÈ™ier `_repository_name-issues.jsonl`, unde fiecare linie este un obiect JSON care reprezintÄƒ un issue. Mai jos folosim aceastÄƒ funcÈ›ie pentru a obÈ›ine toate issue-urile de la ğŸ¤— Datasets:

```py
# Ãn dependenÈ›Äƒ de conexiunea ta la internet, acest lucru poate dura cÃ¢teva minute...
fetch_issues()
```

OdatÄƒ ce issue-urile sunt descÄƒrcate, le putem Ã®ncÄƒrca local utilizÃ¢nd abilitÄƒÈ›ile noastre dobÃ¢ndite Ã®n [secÈ›iunea 2](/course/chapter5/2):

```py
issues_dataset = load_dataset("json", data_files="datasets-issues.jsonl", split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app'],
    num_rows: 3019
})
```

Great, am creat primul nostru dataset de la zero! Dar de ce sunt mai mult de cÃ¢teva mii de issue-uri atunci cÃ¢nd tabul de issue-uri al repositoriului ğŸ¤— Datasets afiÈ™eazÄƒ doar aproximativ 1.000 de issue-uri Ã®n total ğŸ¤”? Conform descris Ã®n [documentaÈ›ia](https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user) GitHub, acest lucru s-a Ã®ntÃ¢mplat pentru cÄƒ am descÄƒrcat È™i toate pull requesturile:

> GitHub's REST API v3 considers every pull request an issue, but not every issue is a pull request. For this reason, "Issues" endpoints may return both issues and pull requests in the response. You can identify pull requests by the `pull_request` key. Be aware that the `id` of a pull request returned from "Issues" endpoints will be an issue id.

Deoarece conÈ›inutul issue-urilor È™i pull requesturilor este destul de diferit, hai sÄƒ preprocesÄƒm puÈ›in datele pentru a ne permite sÄƒ le diferenÈ›iem Ã®ntre ele.

## CurÄƒÈ›area datelor[[cleaning-up-the-data]]

Fragmentul de mai sus din documentaÈ›ia GitHub ne spune cÄƒ coloana `pull_request` poate fi utilizatÄƒ pentru a diferenÈ›ia Ã®ntre issues È™i pull requests. SÄƒ analizÄƒm un sampple aleatoriu pentru a vedea care este diferenÈ›a. AÈ™a cum am fÄƒcut Ã®n [secÈ›iunea 3](/course/chapter5/3), vom Ã®nlÄƒnÈ›ui `Dataset.shuffle()` È™i `Dataset.select()` pentru a crea un sample aleatoriu È™i apoi vom Ã®mperechea coloanele `html_url` È™i `pull_request` pentru a putea compara diversele URL-uri:

```py
sample = issues_dataset.shuffle(seed=666).select(range(3))

# Print out the URL and pull request entries
for url, pr in zip(sample["html_url"], sample["pull_request"]):
    print(f">> URL: {url}")
    print(f">> Pull request: {pr}\n")
```

```python out
>> URL: https://github.com/huggingface/datasets/pull/850
>> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/850', 'html_url': 'https://github.com/huggingface/datasets/pull/850', 'diff_url': 'https://github.com/huggingface/datasets/pull/850.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/850.patch'}

>> URL: https://github.com/huggingface/datasets/issues/2773
>> Pull request: None

>> URL: https://github.com/huggingface/datasets/pull/783
>> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/783', 'html_url': 'https://github.com/huggingface/datasets/pull/783', 'diff_url': 'https://github.com/huggingface/datasets/pull/783.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/783.patch'}
```

Aici putem vedea cÄƒ fiecare pull request este asociat cu diverse URL-uri, Ã®n timp ce issue-urile obiÈ™nuite au o intrare `None`. Putem utiliza aceastÄƒ distincÈ›ie pentru a crea o nouÄƒ coloanÄƒ `is_pull_request` care verificÄƒ dacÄƒ cÃ¢mpul `pull_request` este `None` sau nu:

```py
issues_dataset = issues_dataset.map(
    lambda x: {"is_pull_request": False if x["pull_request"] is None else True}
)
```

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** CalculaÈ›i timpul mediu necesar pentru Ã®nchiderea issue-urilor Ã®n Datasets. VÄƒ poate fi utilÄƒ funcÈ›ia `Dataset.filter()` pentru a filtra pull requesturile È™i issue-urile deschise, È™i puteÈ›i utiliza funcÈ›ia `Dataset.set_format()` pentru a converti datasetul Ã®ntr-un `DataFrame` astfel Ã®ncÃ¢t sÄƒ puteÈ›i manipula cu uÈ™urinÈ›Äƒ timestampurile `created_at` È™i `closed_at`. Pentru puncte bonus, calculaÈ›i timpul mediu necesar pentru Ã®nchiderea pull requesturilor.

DeÈ™i am putea continua sÄƒ curÄƒÈ›Äƒm datasetul prin eliminarea sau redenumirea unor coloane, este, Ã®n general, o practicÄƒ bunÄƒ sÄƒ pÄƒstrÄƒm datasetul cÃ¢t mai "raw" posibil la acest stadiu, astfel Ã®ncÃ¢t sÄƒ poatÄƒ fi utilizat uÈ™or Ã®n multiple aplicaÈ›ii.

Ãnainte de a Ã®ncÄƒrca datasetul Ã®n Hugging Face Hub, trebuie sÄƒ rezolvÄƒm chestie care lipseÈ™te din el: comentariile asociate fiecÄƒrui issue È™i pull request. Le vom adÄƒuga Ã®n continuare cu-- aÈ›i ghicit -- GitHub REST API!

## ÃmbunÄƒtÄƒÈ›irea datasetului[[augmenting-the-dataset]]

DupÄƒ cum se vede Ã®n urmÄƒtorul screenshot, comentariile asociate unui issue sau pull request oferÄƒ o sursÄƒ bogatÄƒ de informaÈ›ii, Ã®n special dacÄƒ suntem interesaÈ›i sÄƒ construim un motor de cÄƒutare pentru a rÄƒspunde la Ã®ntrebÄƒrile utilizatorilor despre bibliotecÄƒ.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues-comment.png" alt="Comentariile asociate unei probleme despre ğŸ¤— Datasets." width="80%"/>
</div>

GitHub REST API oferÄƒ un endpoint [`Comments`](https://docs.github.com/en/rest/reference/issues#list-issue-comments) care returneazÄƒ toate comentariile asociate numÄƒrului problemei. SÄƒ testÄƒm endpointul pentru a vedea ce returneazÄƒ:

```py
issue_number = 2792
url = f"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments"
response = requests.get(url, headers=headers)
response.json()
```

```python out
[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',
  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',
  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',
  'id': 897594128,
  'node_id': 'IC_kwDODunzps41gDMQ',
  'user': {'login': 'bhavitvyamalik',
   'id': 19718818,
   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',
   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',
   'gravatar_id': '',
   'url': 'https://api.github.com/users/bhavitvyamalik',
   'html_url': 'https://github.com/bhavitvyamalik',
   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',
   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',
   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',
   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',
   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',
   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',
   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',
   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',
   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',
   'type': 'User',
   'site_admin': False},
  'created_at': '2021-08-12T12:21:52Z',
  'updated_at': '2021-08-12T12:31:17Z',
  'author_association': 'CONTRIBUTOR',
  'body': "@albertvillanova my tests are failing here:\r\n```\r\ndataset_name = 'gooaq'\r\n\r\n    def test_load_dataset(self, dataset_name):\r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\r\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\r\n\r\ntests/test_dataset_common.py:234: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntests/test_dataset_common.py:187: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) > 0)\r\nE   AssertionError: False is not true\r\n```\r\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?",
  'performed_via_github_app': None}]
```

Putem vedea cÄƒ comentariul este stocat Ã®n cÃ¢mpul `body`, aÈ™a cÄƒ putem scrie o funcÈ›ie simplÄƒ care returneazÄƒ toate comentariile asociate unei probleme prin extragerea conÈ›inutului `body` pentru fiecare element Ã®n `response.json()`:

```py
def get_comments(issue_number):
    url = f"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments"
    response = requests.get(url, headers=headers)
    return [r["body"] for r in response.json()]


# TestÄƒm dacÄƒ funcÈ›ia lucreazÄƒ cum ne dorim
get_comments(2792)
```

```python out
["@albertvillanova my tests are failing here:\r\n```\r\ndataset_name = 'gooaq'\r\n\r\n    def test_load_dataset(self, dataset_name):\r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\r\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\r\n\r\ntests/test_dataset_common.py:234: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntests/test_dataset_common.py:187: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) > 0)\r\nE   AssertionError: False is not true\r\n```\r\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?"]
```

AratÄƒ bine. Acum hai sÄƒ folosim `Dataset.map()` pentru a adÄƒuga noi coloane `comments` fiecÄƒrui issue Ã®n datasetul nostru:

```py
# Depending on your internet connection, this can take a few minutes...
issues_with_comments_dataset = issues_dataset.map(
    lambda x: {"comments": get_comments(x["number"])}
)
```

Ultimul pas este sÄƒ facem push datasetului nostru pe Hub. SÄƒ vedem cum putem face asta.

## ÃncÄƒrcarea datasetului pe Hugging Face Hub[[uploading-the-dataset-to-the-hugging-face-hub]]

<Youtube id="HaN6qCr_Afc"/>

Acum cÄƒ avem datasetul nostru augmentat, este timpul sÄƒ Ã®i facem push pe Hub pentru a-l oferi comunitÄƒÈ›ii! ÃncÄƒrcarea unui dataset este foarte simplu: la fel ca modelele È™i tokenizerrii din ğŸ¤— Transformers, putem utiliza o metodÄƒ `push_to_hub()` pentru a face push unui dataset. Pentru a face asta, avem nevoie de un token de autentificare, care poate fi obÈ›inut prin autentificarea pe Hugging Face Hub cu funcÈ›ia `notebook_login()`:

```py
from huggingface_hub import notebook_login

notebook_login()
```

Acest lucru va crea un widget unde poÈ›i sÄƒ scrii usernameul È™i parola ta, iar un API token va fi salvat Ã®n *~/.huggingface/token*. DacÄƒ rulezi codeul Ã®ntr-un terminal, te poÈ›i loga cu ajutor CLI: 
This will create a widget where you can enter your username and password, and an API token will be saved in *~/.huggingface/token*. If you're running the code in a terminal, you can log in via the CLI instead:

```bash
huggingface-cli login
```
O datÄƒ ce ai fÄƒcut asta, putem Ã®ncÄƒrca datasetul rulÃ¢nd:

```py
issues_with_comments_dataset.push_to_hub("github-issues")
```

De acum, orice poate sÄƒ descarce datasetul, utilizÃ¢nd `load_dataset()` cu ID-ul repositoriului ca `path` argument:

```py
remote_dataset = load_dataset("lewtun/github-issues", split="train")
remote_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

Cool, am Ã®ncÄƒrcat datasetul nostru pe Hub È™i acum este disponibil pentru alÈ›ii sÄƒ Ã®l utilizeze! Mai este doar un lucru important de fÄƒcut: adÄƒugarea unui _dataset card_ care explicÄƒ cum a fost creat corpusul È™i oferÄƒ alte informaÈ›ii utile pentru comunitate.

> [!TIP]
> ğŸ’¡ De asemenea, puteÈ›i Ã®ncÄƒrca un dataset pe Hugging Face Hub direct din terminal utilizÃ¢nd `huggingface-cli` È™i puÈ›inÄƒ magie Git. ConsultaÈ›i [ghidul ğŸ¤— Datasets](https://huggingface.co/docs/datasets/share#share-a-dataset-using-the-cli) pentru detalii despre cum puteÈ›i face asta.

## Crearea unei dataset card[[creating-a-dataset-card]]

DatasetiroÈ™e bine documentate sunt mai probabil sÄƒ fie utile altora (inclusiv È›ie din viitor!), deoarece furnizeazÄƒ contextul pentru a permite utilizatorilor sÄƒ decidÄƒ dacÄƒ datasetul este relevant pentru taskul lor È™i sÄƒ evalueze eventualele biasuri sau riscurile asociate cu utilizarea datasetului.

Pe Hugging Face Hub, aceastÄƒ informaÈ›ie este stocatÄƒ Ã®n fiÈ™ierul *README.md* al fiecÄƒrui dataset repository. Sunt doi paÈ™i principali pe care trebuie sÄƒ Ã®i efectuaÈ›i Ã®nainte de a crea acest fiÈ™ier:

1. UtilizaÈ›i aplicaÈ›ia [`datasets-tagging`](https://huggingface.co/datasets/tagging/) pentru a crea etichete de metadate Ã®n format YAML. Aceste taguri sunt utilizate pentru o varietate de funcÈ›ionalitÄƒÈ›i de cÄƒutare pe Hugging Face Hub È™i asigurÄƒ cÄƒ datasetul poate fi gÄƒsit uÈ™or de membrii comunitÄƒÈ›ii. Deoarece am creat un dataset custom aici, veÈ›i fi nevoiÈ›i sÄƒ clonaÈ›i repositoriul `datasets-tagging` È™i sÄƒ rulaÈ›i aplicaÈ›ia local. IatÄƒ cum aratÄƒ interfaÈ›a:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-tagger.png" alt="InterfaÈ›a `datasets-tagging`." width="80%"/>
</div>

2. CitiÈ›i [ghidul ğŸ¤— Datasets](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) despre crearea de dataset cards informative È™i utilizaÈ›i-l ca È™ablon.

PuteÈ›i crea fiÈ™ierul *README.md* direct pe Hub È™i puteÈ›i gÄƒsi un template pentru dataset card Ã®n repositoriul `lewtun/github-issues`. Un screenshot a dataset card completatÄƒ este afiÈ™atÄƒ mai jos.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/dataset-card.png" alt="Dataset card." width="80%"/>
</div>

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** UtilizaÈ›i aplicaÈ›ia `dataset-tagging` È™i [ghidul ğŸ¤— Datasets](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) pentru a completa fiÈ™ierul *README.md* pentru datasetul de probleme GitHub.

Astfel, am vÄƒzut Ã®n aceastÄƒ secÈ›iune cÄƒ crearea unui dataset bun poate fi destul de complicatÄƒ, dar, spre norocul nsotru, Ã®ncÄƒrcarea È™i oferirea acestuia comunitÄƒÈ›ii nu sunt. Ãn secÈ›iunea urmÄƒtoare, vom utiliza datasetul nou pentru a crea un motor de cÄƒutare semantic cu ğŸ¤— Datasets care poate sÄƒ asocieze Ã®ntrebÄƒri cu cele mai relevante issues È™i comentarii.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** TreceÈ›i prin paÈ™ii pe care i-am fÄƒcut Ã®n aceastÄƒ secÈ›iune pentru a crea un dataset de issues GitHub pentru o biblioteca open source care Ã®È›i place(alegeÈ›i altceva Ã®nafarÄƒ de ğŸ¤— Datasets, desigur!). Pentru puncte bonus, faceÈ›i fine-tune unui multilabel classifier pentru a prezice tagurile prezente Ã®n cÃ¢mpul `labels`.
