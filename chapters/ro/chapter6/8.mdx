# Construirea unui tokenizer, bloc cu bloc[[building-a-tokenizer-block-by-block]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
]} />

DupÄƒ cum am vÄƒzut Ã®n secÈ›iunile anterioare, tokenizarea cuprinde mai multe etape:

- Normalizare (orice curÄƒÈ›are a textului care este consideratÄƒ necesarÄƒ, cum ar fi eliminarea spaÈ›iilor sau a accentelor, normalizarea Unicode etc.)
- Pre-tokenizarea (Ã®mpÄƒrÈ›irea inputului Ã®n cuvinte)
- Rularea inputului prin model (utilizarea cuvintelor pre-tokenizate pentru a produce o secvenÈ›Äƒ de tokeni)
- Post-procesare (adÄƒugarea tokenilor speciali ai tokenizerului, generarea attention maskului È™i a ID-urilor de tip token)

Ca un reminder, iatÄƒ o altÄƒ perspectivÄƒ asupra procesului general:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

Biblioteca ğŸ¤— Tokenizers a fost construitÄƒ pentru a oferi mai multe opÈ›iuni pentru fiecare dintre aceÈ™ti paÈ™i, pe care le puteÈ›i amesteca È™i combina Ã®mpreunÄƒ. Ãn aceastÄƒ secÈ›iune vom vedea cum putem construi un tokenizer de la zero, spre deosebire de antrenarea unui tokenizer nou dintr-unul vechi, aÈ™a cum am fÄƒcut Ã®n [secÈ›iunea 2](/course/chapter6/2). VeÈ›i putea apoi sÄƒ construiÈ›i orice fel de tokenizer la care vÄƒ puteÈ›i gÃ¢ndi!

<Youtube id="MR8tZm5ViWU"/>

Mai exact, biblioteca este construitÄƒ Ã®n jurul unei clase centrale `Tokenizer` cu building grupate Ã®n submodule:

- `normalizers` conÈ›ine toate tipurile posibile de `Normalizer` pe care le puteÈ›i folosi (lista completÄƒ [aici](https://huggingface.co/docs/tokenizers/api/normalizers)).
- `pre_tokenizers` conÈ›ine toate tipurile de `PreTokenizer` pe care le poÈ›i folosi(lista completÄƒ [aici](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)).
- `models` conÈ›ine diferitele tipuri de `Model` pe care le puteÈ›i folosi, precum `BPE`, `WordPiece` È™i `Unigram` (lista completÄƒ [aici](https://huggingface.co/docs/tokenizers/api/models)).
- `trainers` conÈ›ine toate tipurile diferite de `Trainer` pe care le puteÈ›i folosi pentru a vÄƒ antrena modelul pe un corpus (unul pentru fiecare tip de model; lista completÄƒ [aici](https://huggingface.co/docs/tokenizers/api/trainers)).
- `post_processors` conÈ›ine diferitele tipuri de `PostProcessor` pe care le puteÈ›i utiliza (lista completÄƒ [aici](https://huggingface.co/docs/tokenizers/api/post-processors)).
- `decoders` conÈ›ine diferitele tipuri de `Decoder` pe care le puteÈ›i utiliza pentru a decoda rezultatele tokenizÄƒrii (lista completÄƒ [aici](https://huggingface.co/docs/tokenizers/components#decoders)).

PuteÈ›i gÄƒsi Ã®ntreaga listÄƒ de blocuri [aici](https://huggingface.co/docs/tokenizers/components).

## ObÈ›inerea unui corpus[[acquiring-a-corpus]]

Pentru a antrena noul nostru tokenizer, vom utiliza un corpus mic de text (astfel Ã®ncÃ¢t exemplele sÄƒ ruleze rapid). PaÈ™ii pentru obÈ›inerea corpusului sunt similari cu cei pe care i-am urmat la [Ã®nceputul acestui capitol](/course/chapter6/2), dar de data aceasta vom utiliza datasetul [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

FuncÈ›ia `get_training_corpus()` este un generator care va produce batch-uri de 1 000 de texte, pe care le vom utiliza pentru a antrena tokenizerul.

ğŸ¤— Tokenizers pot fi, de asemenea, antrenate direct pe fiÈ™iere text. IatÄƒ cum putem genera un fiÈ™ier text care sÄƒ conÈ›inÄƒ toate textele/inputurile din WikiText-2 pe care le putem utiliza local:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

Ãn continuare vÄƒ vom arÄƒta cum sÄƒ vÄƒ construiÈ›i propriile tokenizere BERT, GPT-2 È™i XLNet, bloc cu bloc. Acest lucru ne va oferi un exemplu pentru fiecare dintre cei trei algoritmi principali de tokenizare: WordPiece, BPE È™i Unigram. SÄƒ Ã®ncepem cu BERT!

## Construirea unui tokenizator WordPiece de la zero[[building-a-wordpiece-tokenizer-from-scratch]]

Pentru a construi un tokenizer cu biblioteca ğŸ¤— Tokenizers, Ã®ncepem prin a iniÈ›ializa un obiect `Tokenizer` cu un `model`, apoi Ã®i setÄƒm atributele `normalizer`, `pre_tokenizer`, `post_processor` È™i `decoder` la valorile dorite.

Pentru acest exemplu, vom crea un `Tokenizer` cu un model WordPiece:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

Trebuie sÄƒ specificÄƒm `unk_token` astfel Ã®ncÃ¢t modelul sÄƒ È™tie ce sÄƒ returneze atunci cÃ¢nd Ã®ntÃ¢lneÈ™te caractere necunoscute. Alte argumente pe care le putem seta aici includ `vocab` al modelului nostru (vom antrena modelul, deci nu este nevoie sÄƒ setÄƒm acest lucru) È™i `max_input_chars_per_word`, care specificÄƒ o lungime maximÄƒ pentru fiecare cuvÃ¢nt (cuvintele mai lungi decÃ¢t valoarea trecutÄƒ vor fi divizate).

Primul pas al tokenizÄƒrii este normalizarea, aÈ™a cÄƒ sÄƒ Ã®ncepem cu aceasta. Deoarece BERT este utilizat pe scarÄƒ largÄƒ, existÄƒ un `BertNormalizer` cu opÈ›iunile clasice pe care le putem seta pentru BERT: `lowercase` È™i `strip_accents`, care se explicÄƒ de la sine; `clean_text` pentru a elimina toate caracterele de control È™i a Ã®nlocui spaÈ›iile repetate cu unul singur; È™i `handle_chinese_chars`, care plaseazÄƒ spaÈ›ii Ã®n jurul caracterelor chinezeÈ™ti. Pentru a replica tokenizerul `bert-base-uncased`, putem seta doar acest normalizator:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

Ãn general, atunci cÃ¢nd construiÈ›i un nou tokenizer, nu veÈ›i avea acces la un normalizator atÃ¢t de util, deja implementat Ã®n biblioteca ğŸ¤— Tokenizers - aÈ™a cÄƒ sÄƒ vedem cum sÄƒ creÄƒm manual normalizatorul BERT. Biblioteca oferÄƒ un normalizator `Lowercase` È™i un normalizator `StripAccents` È™i puteÈ›i compune mai multe normalizatoare folosind un `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

De asemenea, folosim un normalizator Unicode `NFD`, deoarece Ã®n caz contrar normalizatorul `StripAccents` nu va recunoaÈ™te corect caracterele accentuate È™i astfel nu le va elimina.

DupÄƒ cum am mai vÄƒzut, putem folosi metoda `normalize_str()` a `normalizer` pentru a verifica efectele pe care le are asupra unui text dat:

```python
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
hello how are u?
```

> [!TIP]
> **Pentru a merge mai departe** DacÄƒ testaÈ›i cele douÄƒ versiuni ale normalizatorilor anteriori pe un È™ir care conÈ›ine caracterul Unicode `u"\u0085"` veÈ›i observa cu siguranÈ›Äƒ cÄƒ aceÈ™ti doi normalizatori nu sunt exact echivalenÈ›i.
> Pentru a nu complica prea mult versiunea cu `normalizers.Sequence` , nu am inclus Ã®nlocuirile Regex pe care `BertNormalizer` le cere atunci cÃ¢nd argumentul `clean_text` este setat la `True` - care este comportamentul implicit. Dar nu vÄƒ faceÈ›i griji: este posibil sÄƒ obÈ›ineÈ›i exact aceeaÈ™i normalizare fÄƒrÄƒ a utiliza utilul `BertNormalizer` prin adÄƒugarea a douÄƒ `normalizers.Replace` la secvenÈ›a normalizers.

UrmeazÄƒ etapa de pre-tokenizare. Din nou, existÄƒ un `BertPreTokenizer` pre-construit pe care Ã®l putem utiliza:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

Sau Ã®l putem construi de la zero:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

ReÈ›ineÈ›i cÄƒ pre-tokenizatorul `Whitespace` separÄƒ spaÈ›iul È™i toate caracterele care nu sunt litere, cifre sau caracterul underscore, deci tehnic separÄƒ spaÈ›iul È™i punctuaÈ›ia:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

DacÄƒ doriÈ›i sÄƒ separaÈ›i doar spaÈ›iile, ar trebui sÄƒ utilizaÈ›i Ã®n schimb pre-tokenizerul `WhitespaceSplit`:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

Ca È™i Ã®n cazul normalizatorilor, puteÈ›i utiliza un `Sequence` pentru a compune mai mulÈ›i pre-tokenizeri:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

UrmÄƒtorul pas Ã®n pipelineul de tokenizare este rularea inputurilor prin model. Am specificat deja modelul nostru Ã®n iniÈ›ializare, dar mai trebuie sÄƒ Ã®l antrenÄƒm, ceea ce va necesita un `WordPieceTrainer`. Principalul lucru de reÈ›inut atunci cÃ¢nd iniÈ›ializaÈ›i un trainer Ã®n ğŸ¤— Tokenizers este cÄƒ trebuie sÄƒ Ã®i transmiteÈ›i toÈ›i tokenii speciali pe care intenÈ›ionaÈ›i sÄƒ Ã®i utilizaÈ›i - Ã®n caz contrar, acesta nu le va adÄƒuga la vocabular, deoarece acestea nu se aflÄƒ Ã®n corpusul de antrenare:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

Pe lÃ¢ngÄƒ specificarea `vocab_size` È™i `special_tokens`, putem seta `min_frequency` (numÄƒrul de ori Ã®n care un simbol trebuie sÄƒ aparÄƒ pentru a fi inclus Ã®n vocabular) sau putem schimba `continuing_subword_prefix` (dacÄƒ dorim sÄƒ folosim ceva diferit de `##`).

Pentru a antrena modelul nostru folosind iteratorul pe care l-am definit anterior, trebuie doar sÄƒ executÄƒm aceastÄƒ comandÄƒ:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

De asemenea, putem utiliza fiÈ™iere text pentru a ne antrena tokenizerul, care ar arÄƒta astfel (Ã®n prealabil, reiniÈ›ializÄƒm modelul cu un `WordPiece` gol):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Ãn ambele cazuri, putem apoi testa tokenizerul pe un text prin apelarea metodei `encode()`:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

`encoding`-ul obÈ›inut este un `Encoding`, care conÈ›ine toate rezultatele necesare ale tokenizerului Ã®n diferitele sale atribute: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, È™i `overflowing`.

Ultimul pas Ã®n pipelineul de tokenizare este postprocesarea. Trebuie sÄƒ adÄƒugÄƒm tokenul `[CLS]` la Ã®nceput È™i tokenul `[SEP]` la sfÃ¢rÈ™it (sau dupÄƒ fiecare propoziÈ›ie, dacÄƒ avem o pereche de propoziÈ›ii). Vom folosi un `TemplateProcessor` pentru aceasta, dar mai Ã®ntÃ¢i trebuie sÄƒ cunoaÈ™tem ID-urile tokenilor `[CLS]` È™i `[SEP]` din vocabular:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

Pentru a scrie templateul pentru `TemplateProcessor`, trebuie sÄƒ specificÄƒm cum sÄƒ tratÄƒm o singurÄƒ propoziÈ›ie È™i o pereche de propoziÈ›ii. Pentru ambele, scriem tokeni speciali pe care dorim sÄƒ Ã®i folosim; prima (sau singura) propoziÈ›ie este reprezentatÄƒ de `$A`, Ã®n timp ce a doua propoziÈ›ie (dacÄƒ facem encoding unei perechi) este reprezentatÄƒ de `$B`. Pentru fiecare dintre acestea (tokeni speciali È™i propoziÈ›ii), specificÄƒm È™i ID-ul tipului de token corespunzÄƒtor dupÄƒ douÄƒ puncte.

Modelul clasic BERT este astfel definit dupÄƒ cum urmeazÄƒ:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

ReÈ›ineÈ›i cÄƒ trebuie sÄƒ transmitem ID-urile tokenilor speciali, astfel Ã®ncÃ¢t tokenizerul sÄƒ le poatÄƒ converti corect Ã®n ID-urile lor.

OdatÄƒ ce acest lucru este adÄƒugat, revenind la exemplul nostru anterior vom obÈ›ine:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

È˜i pe o pereche de propoziÈ›ii, obÈ›inem rezultatul corect:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

Aproape am terminat de construit acest tokenizer de la zero - ultimul pas este sÄƒ includem un decodor:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

HaideÈ›i sÄƒ-l testÄƒm pe `encoding`-ul nostru anterior:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

Grozav! Putem salva tokenizatorul nostru Ã®ntr-un singur fiÈ™ier JSON, astfel:

```python
tokenizer.save("tokenizer.json")
```

Apoi putem reÃ®ncÄƒrca acel fiÈ™ier Ã®ntr-un obiect `Tokenizer` cu metoda `from_file()`:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

Pentru a utiliza acest tokenizer Ã®n ğŸ¤— Transformers, trebuie sÄƒ Ã®l Ã®ncorporÄƒm Ã®n `PreTrainedTokenizerFast`. Putem fie sÄƒ folosim clasa genericÄƒ, fie, dacÄƒ tokenizerul nostru corespunde unui model existent, sÄƒ folosim clasa respectivÄƒ (aici, `BertTokenizerFast`). DacÄƒ aplicaÈ›i aceastÄƒ lecÈ›ie pentru a construi un tokenizer nou, va trebui sÄƒ utilizaÈ›i prima opÈ›iune.

Pentru a include tokenizatorul Ã®ntr-un `PreTrainedTokenizerFast`, putem fie sÄƒ transmitem tokenizerul construit ca `tokenizer_object`, fie sÄƒ transmitem fiÈ™ierul tokenizerului salvat ca `tokenizer_file`. Cel mai important lucru de reÈ›inut este cÄƒ trebuie sÄƒ setÄƒm manual toÈ›i tokenii speciali, deoarece aceastÄƒ clasÄƒ nu poate deduce din obiectul `tokenizer` care token este tokenul mascÄƒ, tokenul `[CLS]`, etc.:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

DacÄƒ utilizaÈ›i o clasÄƒ specificÄƒ de tokenizer (cum ar fi `BertTokenizerFast`), va trebui sÄƒ specificaÈ›i doar tokenii speciali care sunt diferiÈ›i de cei impliciÈ›i (aici, niciunul):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

Apoi puteÈ›i utiliza acest tokenizer ca orice alt tokenizer ğŸ¤— Transformers. Ãl puteÈ›i salva cu metoda `save_pretrained()` sau Ã®l puteÈ›i Ã®ncÄƒrca Ã®n Hub cu metoda `push_to_hub()`.

Acum cÄƒ am vÄƒzut cum sÄƒ construim un tokenizer WordPiece, hai sÄƒ facem acelaÈ™i lucru pentru un tokenizer BPE. Vom merge un pic mai repede, deoarece cunoaÈ™teÈ›i toÈ›i paÈ™ii, È™i vom evidenÈ›ia doar diferenÈ›ele.

## Construirea unui tokenizer BPE de la zero[[building-a-bpe-tokenizer-from-scratch]]

SÄƒ construim acum un tokenizer GPT-2. Ca È™i pentru tokenizer BERT, Ã®ncepem prin iniÈ›ializarea unui `Tokenizer` cu un model BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

De asemenea, la fel ca Ã®n cazul BERT, am putea iniÈ›ializa acest model cu un vocabular, dacÄƒ am avea unul (Ã®n acest caz, ar trebui sÄƒ oferim `vocab` È™i `merges`), dar din moment ce vom antrena de la zero, nu avem nevoie sÄƒ facem acest lucru. De asemenea, nu trebuie sÄƒ specificÄƒm un `unk_token` deoarece GPT-2 utilizeazÄƒ BPE la nivel de bytes, care nu necesitÄƒ acest lucru.

GPT-2 nu utilizeazÄƒ un normalizator, deci sÄƒrim peste acest pas È™i trecem direct la pre-tokenizare:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

OpÈ›iunea pe care am adÄƒugat-o aici la `ByteLevel` nu este pentru a adÄƒuga un spaÈ›iu unei propoziÈ›ii (care este implicit Ã®n caz contrar). Putem arunca o privire la pre-tokenizarea unui text exemplu ca Ã®nainte:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

UrmeazÄƒ modelul, care are nevoie de antrenare. Pentru GPT-2, singurul token special este tokenul de sfÃ¢rÈ™it de text:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Ca È™i Ã®n cazul `WordPieceTrainer`, precum È™i `vocab_size` È™i `special_tokens`, putem specifica `min_frequency` dacÄƒ dorim, sau dacÄƒ avem un sufix de sfÃ¢rÈ™it de cuvÃ¢nt (cum ar fi `</w>`), Ã®l putem seta cu `end_of_word_suffix`.

Acest tokenizer poate fi antrenat È™i pe fiÈ™iere text:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

SÄƒ aruncÄƒm o privire la tokenizarea unui exemplu de text:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

AplicÄƒm postprocesarea la nivel de bytes pentru tokenizerul GPT-2 dupÄƒ cum urmeazÄƒ:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

OpÈ›iunea `trim_offsets = False` indicÄƒ post-procesorului cÄƒ ar trebui sÄƒ lÄƒsÄƒm offseturile tokenilor care Ã®ncep cu "Ä " aÈ™a cum sunt: Ã®n acest fel, Ã®nceputul offseturilor va indica spaÈ›iul dinaintea cuvÃ¢ntului, nu primul caracter al cuvÃ¢ntului (deoarece spaÈ›iul face parte din punct de vedere tehnic din token). SÄƒ aruncÄƒm o privire asupra rezultatului cu textul pe cÄƒruia tocmai i-am fÄƒcut encoding, unde `'Ä test'` este tokenul de la indexul 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

Ãn cele din urmÄƒ, adÄƒugÄƒm un decoder la nivel de bytes:

```python
tokenizer.decoder = decoders.ByteLevel()
```

È™i putem verifica de douÄƒ ori dacÄƒ funcÈ›ioneazÄƒ corect:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

Grozav! Acum cÄƒ am terminat, putem salva tokenizatorul ca Ã®nainte È™i Ã®l putem Ã®ncorpora Ã®ntr-un `PreTrainedTokenizerFast` sau `GPT2TokenizerFast` dacÄƒ dorim sÄƒ Ã®l folosim Ã®n ğŸ¤— Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

or:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

Ca un ultim exemplu, vÄƒ vom arÄƒta cum sÄƒ construiÈ›i un tokenizer Unigram de la zero.

## Construirea unui tokenizer Unigram de la zero[[building-a-unigram-tokenizer-from-scratch]]

SÄƒ construim acum un tokenizer XLNet. Ca È™i Ã®n cazul tokenizerelor anterioare, Ã®ncepem prin iniÈ›ializarea unui `Tokenizer` cu un model Unigram:

```python
tokenizer = Tokenizer(models.Unigram())
```

Din nou, am putea iniÈ›ializa acest model cu un vocabular, dacÄƒ am avea unul.

Pentru normalizare, XLNet utilizeazÄƒ cÃ¢teva Ã®nlocuiri (care provin din SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

Acest lucru Ã®nlocuiteÈ™te <code>``</code> È™i <code>''</code> cu <code>"</code> È™i orice secvenÈ›Äƒ de douÄƒ sau mai multe spaÈ›ii cu un singur spaÈ›iu, precum È™i È™tergerea accentelor Ã®n textele ce trebuie tokenizate.

Pre-tokenizerul care trebuie utilizat pentru orice tokenizer SentencePiece este `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

Putem arunca o privire la pre-tokenizarea unui exemplu de text ca mai Ã®nainte:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("â–Let's", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', (14, 29))]
```

UrmeazÄƒ modelul, care are nevoie de antrenare. XLNet are destul de mulÈ›i tokeni speciali:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Un argument foarte important care nu trebuie uitat pentru `UnigramTrainer` este `unk_token`. Putem trece È™i alte argumente specifice algoritmului Unigram, cum ar fi `shrinking_factor` pentru fiecare pas Ã®n care eliminÄƒm tokeni (valoarea implicitÄƒ este 0,75) sau `max_piece_length` pentru a specifica lungimea maximÄƒ a unui token dat (valoarea implicitÄƒ este 16).

Acest tokenizer poate fi antrenat È™i pe fiÈ™iere text:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

SÄƒ aruncÄƒm o privire la tokenizarea unui exemplu de text:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']
```

O particularitate a XLNet este cÄƒ pune tokenul `<cls>` la sfÃ¢rÈ™itul propoziÈ›iei, cu un ID de tip 2 (pentru a-l distinge de ceilalÈ›i tokeni). Ca urmare, este fÄƒcut padding la stÃ¢nga. Putem trata toÈ›i tokenii speciali È™i ID-urile de tip ale tokenilor cu un template, ca Ã®n cazul BERT, dar mai Ã®ntÃ¢i trebuie sÄƒ obÈ›inem ID-urile tokenilor `<cls>` È™i `<sep>`:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

Templateul aratÄƒ astfel:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

È˜i putem testa cÄƒ funcÈ›ioneazÄƒ prin codificarea unei perechi de propoziÈ›ii:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

Ãn cele din urmÄƒ, adÄƒugÄƒm un decoder `Metaspace`:

```python
tokenizer.decoder = decoders.Metaspace()
```

È™i am terminat cu acest tokenizer! Putem salva tokenizerul ca Ã®nainte È™i Ã®l putem Ã®ncorpora Ã®ntr-un `PreTrainedTokenizerFast` sau `XLNetTokenizerFast` dacÄƒ dorim sÄƒ Ã®l folosim Ã®n ğŸ¤— Transformers. Un lucru de reÈ›inut atunci cÃ¢nd se utilizeazÄƒ `PreTrainedTokenizerFast` este cÄƒ, pe lÃ¢ngÄƒ tokenii speciali, trebuie sÄƒ spunem bibliotecii ğŸ¤— Transformers sÄƒ facÄƒ padding la stÃ¢nga:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

Sau alternativ:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

Acum cÄƒ aÈ›i vÄƒzut cum sunt utilizate diferitele blocuri de construcÈ›ie pentru a construi tokenizeri existenÈ›i, ar trebui sÄƒ puteÈ›i scrie orice tokenizer doriÈ›i cu biblioteca ğŸ¤— Tokenizers È™i sÄƒ Ã®l puteÈ›i utiliza Ã®n ğŸ¤— Transformers.
