# Normalizare È™i pre-tokenizare[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

Ãnainte de a analiza Ã®n profunzime cei mai comuni trei algoritmi de subword tokenization utilizaÈ›i cu modelele Transformer (Byte-Pair Encoding [BPE], WordPiece È™i Unigram), vom arunca mai Ã®ntÃ¢i o privire la preprocesarea pe care fiecare tokenizer o aplicÄƒ textului. IatÄƒ o prezentare generalÄƒ a etapelor din pipelineul de tokenizare:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

Ãnainte de a Ã®mpÄƒrÈ›i un text Ã®n subtokens (Ã®n conformitate cu modelul sÄƒu), tokenizerul efectueazÄƒ doi paÈ™i: _normalization__ È™i _pre-tokenization_.

## Normalization[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

Etapa de normalizare implicÄƒ o curÄƒÈ›are generalÄƒ, cum ar fi eliminarea spaÈ›iilor inutile, a le face minuscule, È™i/sau È™tergerea accentelor. DacÄƒ sunteÈ›i familiarizat cu [Unicode normalization](http://www.unicode.org/reports/tr15/) (cum ar fi NFC sau NFKC), acest lucru poate fi aplicat È™i de tokenizer.

`Tokenizer`-ul ğŸ¤— Transformers are un atribut numit `backend_tokenizer` care oferÄƒ acces la tokenizatorul de bazÄƒ din biblioteca ğŸ¤— Tokenizers:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

Atributul `normalizer` al obiectului `tokenizer` are o metodÄƒ `normalize_str()` pe care o putem folosi pentru a vedea cum se realizeazÄƒ normalizarea:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
'hello how are u?'
```

Ãn acest exemplu, din moment ce am ales checkpointul `bert-base-uncased`, normalizarea a aplicat scrierea cu minusculÄƒ È™i a eliminat accentele.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** ÃncÄƒrcaÈ›i un tokenizer din checkpointul `bert-base-cased` È™i treceÈ›i-i acelaÈ™i exemplu. Care sunt principalele diferenÈ›e pe care le puteÈ›i observa Ã®ntre versiunile cased È™i uncased ale tokenizerului?

## Pre-tokenization[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

DupÄƒ cum vom vedea Ã®n secÈ›iunile urmÄƒtoare, un tokenizer nu poate fi antrenat doar pe text raw. Ãn schimb, trebuie mai Ã®ntÃ¢i sÄƒ Ã®mpÄƒrÈ›im textele Ã®n entitÄƒÈ›i mici, cum ar fi cuvintele. Aici intervine etapa de pre-tokenizare. DupÄƒ cum am vÄƒzut Ã®n [Capitolul 2](/course/chapter2), un tokenizer bazat pe cuvinte poate Ã®mpÄƒrÈ›i pur È™i simplu un text raw Ã®n cuvinte pe baza spaÈ›iului È™i a punctuaÈ›iei. Aceste cuvinte vor fi limitele subtokenurilor pe care tokenizerul le poate Ã®nvÄƒÈ›a Ã®n timpul instruirii sale.

Pentru a vedea cum un tokenizator rapid efectueazÄƒ pre-tokenizarea, putem utiliza metoda `pre_tokenize_str()` a atributului `pre_tokenizer` al obiectului `tokenizer`:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

ObservaÈ›i cum tokenizatorul È›ine deja evidenÈ›a offseturilor, acesta fiind modul Ã®n care ne poate oferi mappingul offseturilor pe care l-am folosit Ã®n secÈ›iunea anterioarÄƒ. Aici, tokenizatorul ignorÄƒ cele douÄƒ spaÈ›ii È™i le Ã®nlocuieÈ™te cu unul singur, dar offsetul sare Ã®ntre `are` È™i `you` pentru a È›ine cont de acest lucru.

Deoarece utilizÄƒm un tokenizer BERT, pre-tokenizarea implicÄƒ separarea spaÈ›iilor È™i a punctuaÈ›iei. AlÈ›i tokenizatori pot avea reguli diferite pentru acest pas. De exemplu, dacÄƒ folosim tokenizatorul GPT-2:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

va despÄƒrÈ›i Ã®n spaÈ›ii È™i punctuaÈ›ie, dar va pÄƒstra spaÈ›iile È™i le va Ã®nlocui cu un simbol `Ä `, permiÈ›Ã¢ndu-i sÄƒ recupereze spaÈ›iile originale dacÄƒ facem decode tokenilor:

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('Ä how', (6, 10)), ('Ä are', (10, 14)), ('Ä ', (14, 15)), ('Ä you', (15, 19)),
 ('?', (19, 20))]
```

De asemenea, reÈ›ineÈ›i cÄƒ, spre deosebire de tokenizatorul BERT, acest tokenizator nu ignorÄƒ spaÈ›iul dublu.

Pentru un ultim exemplu, sÄƒ aruncÄƒm o privire la tokenizerul T5, care se bazeazÄƒ pe algoritmul SentencePiece:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('â–Hello,', (0, 6)), ('â–how', (7, 10)), ('â–are', (11, 14)), ('â–you?', (16, 20))]
```

Ca È™i tokenizatorul GPT-2, acesta pÄƒstreazÄƒ spaÈ›iile È™i le Ã®nlocuieÈ™te cu un token specific (`_`), dar tokenizatorul T5 separÄƒ doar spaÈ›iile, nu È™i punctuaÈ›ia. De asemenea, observaÈ›i cÄƒ a adÄƒugat un spaÈ›iu implicit la Ã®nceputul propoziÈ›iei (Ã®nainte de `Hello`) È™i a ignorat spaÈ›iul dublu dintre `are` È™i `you`.

Acum cÄƒ am vÄƒzut puÈ›in din modul Ã®n care diferite tokenizere proceseazÄƒ textul, putem Ã®ncepe sÄƒ explorÄƒm algoritmii care stau la baza acestora. Vom Ã®ncepe cu o privire rapidÄƒ asupra SentencePiece, care se aplicÄƒ pe scarÄƒ largÄƒ; apoi, Ã®n urmÄƒtoarele trei secÈ›iuni, vom examina modul Ã®n care funcÈ›ioneazÄƒ cei trei algoritmi principali utilizaÈ›i pentru tokenizarea subcuvintelor.

## SentencePiece[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) este un algoritm de tokenizare pentru preprocesarea textului pe care Ã®l puteÈ›i utiliza cu oricare dintre modelele pe care le vom vedea Ã®n urmÄƒtoarele trei secÈ›iuni. Acesta considerÄƒ textul ca o secvenÈ›Äƒ de caractere Unicode È™i Ã®nlocuieÈ™te spaÈ›iile cu un caracter special, `â–`. Folosit Ã®mpreunÄƒ cu algoritmul Unigram (a se vedea [secÈ›iunea 7](/course/chapter7/7)), nu necesitÄƒ nici mÄƒcar o etapÄƒ de pre-tokenizare, ceea ce este foarte util pentru limbile Ã®n care nu se foloseÈ™te caracterul spaÈ›iu (cum ar fi chineza sau japoneza).

CealaltÄƒ caracteristicÄƒ principalÄƒ a SentencePiece este *reversible tokenization*: deoarece nu existÄƒ un tratament special al spaÈ›iilor, decodarea tokenurilor se face pur È™i simplu prin concatenarea lor È™i Ã®nlocuirea `_` cu spaÈ›ii - acest lucru rezultÄƒ Ã®n textul normalizat. DupÄƒ cum am vÄƒzut mai devreme, tokenizatorul BERT eliminÄƒ spaÈ›iile care se repetÄƒ, deci tokenizarea sa nu este reversibilÄƒ.

## Prezentare generalÄƒ a algoritmului[[algorithm-overview]]

Ãn urmÄƒtoarele secÈ›iuni, vom analiza cei trei algoritmi principali de tokenizare a subcuvintelor: BPE (utilizat de GPT-2 È™i alÈ›ii), WordPiece (utilizat de exemplu de BERT) È™i Unigram (utilizat de T5 È™i alÈ›ii). Ãnainte de a Ã®ncepe, iatÄƒ o scurtÄƒ prezentare generalÄƒ a modului Ã®n care funcÈ›ioneazÄƒ fiecare dintre acestea. Nu ezitaÈ›i sÄƒ reveniÈ›i la acest tabel dupÄƒ ce citiÈ›i fiecare dintre secÈ›iunile urmÄƒtoare, dacÄƒ Ã®ncÄƒ nu are sens pentru dumneavoastrÄƒ.

Model | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Training | PorneÈ™te de la un vocabular restrÃ¢ns È™i Ã®nvaÈ›Äƒ reguli de Ã®mbinare a tokenilor |  PorneÈ™te de la un vocabular restrÃ¢ns È™i Ã®nvaÈ›Äƒ reguli de Ã®mbinare a tokenilor | PorneÈ™te de la un vocabular mare È™i Ã®nvaÈ›Äƒ regulile de eliminare a tokenilor
Training step | CombinÄƒ tokenii corespunzÄƒtori celei mai comune perechi | CombinÄƒ tokenii corespunzÄƒtori perechii cu cel mai bun scor pe baza frecvenÈ›ei perechii, privilegiind perechile Ã®n care fiecare token individual este mai puÈ›in frecvent| EliminÄƒ toÈ›i tokenii din vocabular care vor minimiza pierderea calculatÄƒ pe Ã®ntregul corpus
Learns | Reguli de combinare È™i un vocabular| Doar un vocabular| Un vocabular cu un anumit scor pentru fiecare token
Encoding | Ãmparte un cuvÃ¢nt Ã®n caractere È™i aplicÄƒ Ã®mbinÄƒrile Ã®nvÄƒÈ›ate Ã®n timpul antrenÄƒrii | GÄƒseÈ™te cel mai lung subcuvÃ¢nt Ã®ncepÃ¢nd de la Ã®nceput care se aflÄƒ Ã®n vocabular, apoi face acelaÈ™i lucru pentru restul cuvÃ¢ntului | GÄƒseÈ™te cea mai probabilÄƒ Ã®mpÄƒrÈ›ire Ã®n tokens, folosind scorurile Ã®nvÄƒÈ›ate Ã®n timpul antrenÄƒrii

Acum hai sÄƒ trecem la BPE!