<FrameworkSwitchCourse {fw} />

# Tokenizerii rapizi Ã®n pipeline-ul de QA[[fast-tokenizers-in-the-qa-pipeline]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_tf.ipynb"},
]} />

{/if}

Acum ne vom aprofunda Ã®n pipelineul `question-answering` È™i sÄƒ vedem cum putem valorifica offesturile pentru a primi rÄƒspunzuri la Ã®ntrebÄƒri la Ã®ndemÃ¢nÄƒ din context, asemÄƒnÄƒtor cum am fÄƒcut cu entitÄƒÈ›ile grupate Ã®n secÈ›iunea precedentÄƒ. Pe urmÄƒ vom vedea cum vom face faÈ›Äƒ contextelor foarte lungi care ajung truncate. PuteÈ›i trece peste aceastÄƒ secÈ›iune dacÄƒ nu sunteÈ›i interesat Ã®n Ã®ntrebarea care rÄƒspunde la sarcina aceasta.

{#if fw === 'pt'}

<Youtube id="_wxyB3j3mk4"/>

{:else}

<Youtube id="b3u8RzBCX9Y"/>

{/if}

## Folosind `question-answering` pipeline[[using-the-question-answering-pipeline]]

Cum am vÄƒzut Ã®n [Capitolul 1](/course/chapter1), noi putem folosi pipelineul `question-answering` ca acesta pentru a rÄƒspunde la o Ã®ntrebare:

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Spre deosebire de alte pipelineuri, care nu put trunca È™i face split la text care este mai lung decÃ¢t lungimea maxim acceptatÄƒ de model(È™i, prin urmare, pot pierde informaÈ›ii la sfÃ¢rÈ™itul unui document), accest pipeline poate face faÈ›Äƒ contextelor foarte lungi È™i va returna rÄƒspunsul la Ã®ntrebare chiar dacÄƒ aceasta se aflÄƒ la sfÃ¢rÈ™it:

```py
long_context = """
ğŸ¤— Transformers: State of the Art NLP

ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Hai sÄƒ vedem cum el face toate astea!

## Folosind un model pentru rÄƒspunderea la Ã®ntrebÄƒri[[using-a-model-for-question-answering]]

Ca Ã®n cazul oricÄƒrui altui pipeline, Ã®ncepem prin tokenizarea datelor de intrare È™i apoi le trimitem prin model. Checkpointul utilizat Ã®n mod implicit pentru pipelineul `question-answering` este [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) ("squad" din nume provine de la datasetul pe care modelul a fost ajustat; vom vorbi mai multe despre datasetul SQuAD Ã®n [Capitolul 7](/course/chapter7/7)):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)
```

{/if}

ObservaÈ›i cÄƒ noi tokenizÄƒm Ã®ntrebrea È™i contextul ca o perecehe, cu Ã®ntrebarea prima.


<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

Modelele create pentru rÄƒspunderea la Ã®ntrebÄƒri funcÈ›ioneazÄƒ puÈ›in diferit de modelele pe care le-am vÄƒzut pÃ¢nÄƒ acum. Folosind imaginea de mai sus ca exemplu, modelul a fost antrenat pentru a prezice indicele tokenului cu care Ã®ncepe rÄƒspunsului (aici 21) È™i indicele simbolului la care se terminÄƒ rÄƒspunsul (aici 24). Acesta este motivul pentru care modelele respective nu returneazÄƒ un singur tensor de logits, ci douÄƒ: unul pentru logits-ul corespunzÄƒtori tokenului cu care Ã®ncepe rÄƒspunsului È™i unul pentru logits-ul corespunzÄƒtor tokenului de sfÃ¢rÈ™it al rÄƒspunsului. Deoarece Ã®n acest caz avem un singur input care conÈ›ine 66 de token-uri, obÈ›inem:

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

{:else}

```python out
(1, 66) (1, 66)
```

{/if}

Pentru a converti aceÈ™ti logits Ã®n probabilitÄƒÈ›i, vom aplica o funcÈ›ie softmax - dar Ã®nainte de aceasta, trebuie sÄƒ ne asigurÄƒm cÄƒ mascÄƒm indicii care nu fac parte din context. Inputul nostru este `[CLS] Ã®ntrebare [SEP] context [SEP]`, deci trebuie sÄƒ mascÄƒm token-urile Ã®ntrebÄƒrii, precum È™i tokenul `[SEP]`. Cu toate acestea, vom pÄƒstra simbolul `[CLS]`, deoarece unele modele Ã®l folosesc pentru a indica faptul cÄƒ rÄƒspunsul nu se aflÄƒ Ã®n context.

Deoarece vom aplica ulterior un softmax, trebuie doar sÄƒ Ã®nlocuim logiturile pe care dorim sÄƒ le mascÄƒm cu un numÄƒr negativ mare. Aici, folosim `-10000`:

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Acum cÄƒ am mascat Ã®n mod corespunzÄƒtor logiturile corespunzÄƒtoare poziÈ›iilor pe care nu dorim sÄƒ le prezicem, putem aplica softmax:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

La acest stadiu, am putea lua argmax al probabilitÄƒÈ›ilor de Ã®nceput È™i de sfÃ¢rÈ™it - dar am putea ajunge la un indice de Ã®nceput care este mai mare decÃ¢t indicele de sfÃ¢rÈ™it, deci trebuie sÄƒ luÄƒm cÃ¢teva precauÈ›ii suplimentare. Vom calcula probabilitÄƒÈ›ile fiecÄƒrui  `start_index` È™i `end_index` posibil Ã®n cazul Ã®n care `start_index <= end_index`, apoi vom lua un tuple `(start_index, end_index)` cu cea mai mare probabilitate.

PresupunÃ¢nd cÄƒ evenimentele "The answer starts at `start_index`" È™i "The answer ends at `end_index`" sunt independente, probabilitatea ca rÄƒspunsul sÄƒ Ã®nceapÄƒ la `start_index` È™i sÄƒ se termine la `end_index` este:

$$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]$$ 

Deci, pentru a calcula toate scorurile, trebuie doar sÄƒ calculÄƒm toate produsele \\(\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]\\) unde `start_index <= end_index`.

Mai Ã®ntÃ¢i hai sÄƒ calculÄƒm toate produsele posibile:

```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

{#if fw === 'pt'}

Apoi vom masca valorile Ã®n care `start_index > end_index` prin stabilirea lor la `0` (celelalte probabilitÄƒÈ›i sunt toate numere pozitive). FuncÈ›ia `torch.triu()` returneazÄƒ partea triunghiularÄƒ superioarÄƒ a tensorului 2D trecut ca argument, deci va face aceastÄƒ mascare pentru noi:

```py
scores = torch.triu(score)
```

{:else}

Apoi vom masca valorile Ã®n care `start_index > end_index` prin stabilirea lor la `0` (celelalte probabilitÄƒÈ›i sunt toate numere pozitive). FuncÈ›ia `np.triu()` returneazÄƒ partea triunghiularÄƒ superioarÄƒ a tensorului 2D trecut ca argument, deci va face aceastÄƒ mascare pentru noi:

```py
import numpy as np

scores = np.triu(scores)
```

{/if}

Acum trebuie doar sÄƒ obÈ›inem indicele maximului. Deoarece PyTorch va returna indicele Ã®n tensorul aplatizat, trebuie sÄƒ folosim operaÈ›iile floor division `//` È™i modulusul `%` pentru a obÈ›ine `start_index` È™i `end_index`:

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

Nu am terminat Ã®ncÄƒ, dar cel puÈ›in avem deja scorul corect pentru rÄƒspuns (puteÈ›i verifica acest lucru comparÃ¢ndu-l cu primul rezultat din secÈ›iunea anterioarÄƒ):

```python out
0.97773
```

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** CalculaÈ›i indicii de Ã®nceput È™i de sfÃ¢rÈ™it pentru cele mai probabile cinci rÄƒspunsuri.

Avem `start_index` È™i `end_index` ale rÄƒspunsului Ã®n termeni de tokens, deci acum trebuie doar sÄƒ convertim Ã®n character indices Ã®n context. Acesta este momentul Ã®n care offseturile vor fi foarte utile. Putem sÄƒ le luÄƒm È™i sÄƒ le folosim aÈ™a cum am fÄƒcut Ã®n sarcina de clasificare a tokenurilor:

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

Acum trebuie doar sÄƒ formatÄƒm totul pentru a obÈ›ine rezultatul nostru:

```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

Grozav! Este la fel ca Ã®n primul nostru exemplu!

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** UtilizaÈ›i cele mai bune scoruri pe care le-aÈ›i calculat anterior pentru a afiÈ™a cele mai probabile cinci rÄƒspunsuri. Pentru a vÄƒ verifica rezultatele, Ã®ntoarceÈ›i-vÄƒ la primul pipeline È™i introduceÈ›i `top_k=5` atunci cÃ¢nd Ã®l apelaÈ›i.

## Gestionarea contextelor lungi[[handling-long-contexts]]

DacÄƒ Ã®ncercÄƒm sÄƒ tokenizÄƒm Ã®ntrebarea È™i contextul lung pe care le-am folosit ca un exemplu anterior, vom obÈ›ine un numÄƒr de tokenuri mai mare decÃ¢t lungimea maximÄƒ utilizatÄƒ Ã®n pipelineul `question-answering` (care este 384):

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

Prin urmare, va trebui sÄƒ trunchiem inputurile la lungimea maximÄƒ. ExistÄƒ mai multe modalitÄƒÈ›i prin care putem face acest lucru, dar nu dorim sÄƒ trunchiem Ã®ntrebarea, ci doar contextul. Deoarece contextul este a doua propoziÈ›ie, vom utiliza strategia de trunchiere `"only_second"`. Problema care apare atunci este cÄƒ rÄƒspunsul la Ã®ntrebare poate sÄƒ nu fie Ã®n contextul trunchiat. Aici, de exemplu, am ales o Ã®ntrebare la care rÄƒspunsul se aflÄƒ spre sfÃ¢rÈ™itul contextului, iar atunci cÃ¢nd Ã®l trunchiem, rÄƒspunsul nu este prezent:

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""
```

Aceasta Ã®nseamnÄƒ cÄƒ modelul va avea dificultÄƒÈ›i Ã®n a alege rÄƒspunsul corect. Pentru a rezolva acest lucru, pipelineul `question-answering` ne permite sÄƒ Ã®mpÄƒrÈ›im contextul Ã®n bucÄƒÈ›i mai mici, specificÃ¢nd lungimea maximÄƒ. Pentru a ne asigura cÄƒ nu Ã®mpÄƒrÈ›im contextul exact Ã®n locul nepotrivit pentru a face posibilÄƒ gÄƒsirea rÄƒspunsului, aceasta include È™i o anumitÄƒ suprapunere Ã®ntre bucÄƒÈ›i.

Putem cere tokenizerului (rapid sau lent) sÄƒ facÄƒ acest lucru pentru noi adÄƒugÃ¢nd `return_overflowing_tokens=True`, È™i putem specifica suprapunerea doritÄƒ cu argumentul `stride`. IatÄƒ un exemplu, folosind o propoziÈ›ie mai micÄƒ:

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

DupÄƒ cum putem vedea, propoziÈ›ia a fost Ã®mpÄƒrÈ›itÄƒ Ã®n bucÄƒÈ›i astfel Ã®ncÃ¢t fiecare intrare din `inputs["input_ids"]` sÄƒ aibÄƒ cel mult 6 token-uri (aici ar trebui sÄƒ adÄƒugÄƒm padding pentru ca ultima intrare sÄƒ aibÄƒ aceeaÈ™i dimensiune ca celelalte) È™i existÄƒ o suprapunere de 2 tokenuri Ã®ntre fiecare intrare.

SÄƒ aruncÄƒm o privire mai atentÄƒ la rezultatul tokenizÄƒrii:

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

AÈ™a cum era de aÈ™teptat, obÈ›inem ID-uri de intrare È™i un attention mask. Ultima cheie, `overflow_to_sample_mapping`, este o hartÄƒ care ne spune cÄƒrei propoziÈ›ii Ã®i corespunde fiecare dintre rezultate - aici avem 7 rezultate care provin toate din (singura) propoziÈ›ie pe care am transmis-o tokenizerului:

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

Acest lucru este mai util atunci cÃ¢nd tokenizÄƒm mai multe propoziÈ›ii Ã®mpreunÄƒ. De exemplu, aceasta:

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

gets us:

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

ceea ce Ã®nseamnÄƒ cÄƒ prima propoziÈ›ie este Ã®mpÄƒrÈ›itÄƒ Ã®n 7 fragmente ca Ã®nainte, iar urmÄƒtoarele 4 fragmente provin din a doua propoziÈ›ie.

Acum sÄƒ ne Ã®ntoarcem la contextul nostru lung. Ãn mod implicit, pipelineul `question-answering` utilizeazÄƒ o lungime maximÄƒ de 384, aÈ™a cum am menÈ›ionat mai devreme, È™i un stride de 128, care corespund modului Ã®n care modelul a fost fine-tuned (puteÈ›i ajusta aceÈ™ti parametri prin trecerea argumentelor `max_seq_len` È™i `stride` atunci cÃ¢nd apelaÈ›i pipelineul). Astfel, vom utiliza aceÈ™ti parametri la tokenizare. Vom adÄƒuga, de asemenea, padding (pentru a avea sampleuri de aceeaÈ™i lungime, astfel Ã®ncÃ¢t sÄƒ putem construi tensori), precum È™i pentru a solicita offsets:

```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

Aceste "inputuri" vor conÈ›ine ID-urile de input È™i attention maskurile aÈ™teptate de model, precum È™i offseturile È™i "overflow_to_sample_mapping" despre care tocmai am vorbit. Deoarece cei doi nu sunt parametri utilizaÈ›i de model, Ã®i vom scoate din `inputs` (È™i nu vom stoca harta, deoarece nu este utilÄƒ aici) Ã®nainte de a-l converti Ã®ntr-un tensor:

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python out
(2, 384)
```

{/if}

Contextul nostru lung a fost Ã®mpÄƒrÈ›it Ã®n douÄƒ, ceea ce Ã®nseamnÄƒ cÄƒ, dupÄƒ ce trece prin modelul nostru, vom avea douÄƒ seturi de logits de Ã®nceput È™i de sfÃ¢rÈ™it:

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python out
(2, 384) (2, 384)
```

{/if}

Ca È™i Ã®nainte, mai Ã®ntÃ¢i mascÄƒm tokenii care nu fac parte din context Ã®nainte de a lua softmax. De asemenea, mascÄƒm toÈ›i padding tokens(marcate de attention mask):

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Then we can use the softmax to convert our logits to probabilities:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()
```

{/if}

UrmÄƒtorul pas este similar cu ceea ce am fÄƒcut pentru contextul mic, dar Ã®l repetÄƒm pentru fiecare dintre cele douÄƒ chunkuri. Atribuim un scor tuturor intervalelor posibile de rÄƒspuns, apoi luÄƒm intervalul cu cel mai bun scor:

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

Cei doi candidaÈ›i corespund celor mai bune rÄƒspunsuri pe care modelul le-a putut gÄƒsi Ã®n fiecare parte. Modelul este mult mai Ã®ncrezÄƒtor cÄƒ rÄƒspunsul corect se aflÄƒ Ã®n a doua parte (ceea ce este un semn bun!). Acum trebuie doar sÄƒ facem map celor douÄƒ intervale de tokenuri cu intervalele de caractere din context (trebuie sÄƒ o punem Ã®n corespondenÈ›Äƒ doar pe a doua pentru a avea rÄƒspunsul nostru, dar este interesant sÄƒ vedem ce a ales modelul Ã®n prima parte).

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** AdaptaÈ›i codul de mai sus pentru a returna scorurile È™i spanurile intervalele pentru cele mai probabile cinci rÄƒspunsuri (Ã®n total, nu pe chunk).

`offsets`-urile pe care le-am luat mai devreme este de fapt o listÄƒ de offsets, cu o listÄƒ pentru fiecare chunk de text:

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python out
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

DacÄƒ ignorÄƒm primul rezultat, obÈ›inem acelaÈ™i rezultat ca È™i pipelineul noastru pentru acest context lung - yay!

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** UtilizaÈ›i cele mai bune scoruri pe care le-aÈ›i calculat Ã®nainte pentru a afiÈ™a cele mai probabile cinci rÄƒspunsuri (pentru Ã®ntregul context, nu pentru fiecare chunk). Pentru a vÄƒ verifica rezultatele, Ã®ntoarceÈ›i-vÄƒ la primul pipeline È™i introduceÈ›i `top_k=5` atunci cÃ¢nd Ã®l apelaÈ›i.

Aici se Ã®ncheie scufundarea noastrÄƒ Ã®n capacitÄƒÈ›ile tokenizerului. Vom pune toate acestea din nou Ã®n practicÄƒ Ã®n capitolul urmÄƒtor, cÃ¢nd vÄƒ vom arÄƒta cum sÄƒ ajustaÈ›i un model pentru o serie de sarcini NLP comune.
