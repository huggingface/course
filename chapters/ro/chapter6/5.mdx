# Tokenizarea Byte-Pair Encoding[[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

Byte-Pair Encoding (BPE) a fost ini탵ial dezvoltat ca un algoritm de comprimare a textelor 탳i apoi utilizat de OpenAI pentru tokenizare la preantrenarea modelului GPT. Acesta este utilizat de o mul탵ime de modele Transformers, inclusiv GPT, GPT-2, RoBERTa, BART 탳i DeBERTa.

<Youtube id="HEikzVL-lZU"/>

> [!TIP]
> 游눠 Aceast캒 sec탵iune acoper캒 BPE 칥n profunzime, merg칙nd p칙n캒 la prezentarea unei implement캒ri complete. Pute탵i s캒ri la sf칙r탳it dac캒 dori탵i doar o prezentare general캒 a algoritmului de tokenizare.

## Algoritmul de antrenare[[training-algorithm]]

Antrenarea BPE 칥ncepe prin calcularea setului unic de cuvinte utilizate 칥n corpus (dup캒 finalizarea etapelor de normalizare 탳i pre-tokenizare), apoi construirea vocabularului prin preluarea tuturor simbolurilor utilizate pentru scrierea acestor cuvinte. Ca un exemplu foarte simplu, s캒 spunem c캒 corpusul nostru utilizeaz캒 aceste cinci cuvinte:

```
"hug", "pug", "pun", "bun", "hugs"
```

Vocabularul de baz캒 va fi atunci `["b", "g", "h", "n", "p", "s", "u"]`. Pentru cazurile din lumea real캒, vocabularul de baz캒 va con탵ine cel pu탵in toate caracterele ASCII 탳i, probabil, 탳i unele caractere Unicode. Dac캒 un exemplu pe care 칥l tokeniza탵i utilizeaz캒 un caracter care nu se afl캒 칥n corpusul de antrenare, acel caracter va fi convertit 칥ntr-un token necunoscut. Acesta este unul dintre motivele pentru care o mul탵ime de modele NLP sunt foarte proaste la analizarea con탵inutului cu emoji, de exemplu.

> [!TIP]
> Tokenizerele GPT-2 탳i RoBERTa (care sunt destul de asem캒n캒toare) au o modalitate inteligent캒 de a rezolva acest lucru: ele nu privesc cuvintele ca fiind scrise cu caractere Unicode, ci cu bytes. 칉n acest fel, vocabularul de baz캒 are o dimensiune mic캒 (256), dar fiecare caracter la care v캒 pute탵i g칙ndi va fi inclus 탳i nu va ajunge s캒 fie convertit 칥ntr-un token necunoscut. Acest truc se nume탳te *byte-level BPE*.

Dup캒 ob탵inerea acestui vocabular de baz캒, ad캒ug캒m noi tokeni p칙n캒 c칙nd se atinge dimensiunea dorit캒 a vocabularului prin 칥nv캒탵area prin *merges*, care sunt reguli de merge a dou캒 elemente ale vocabularului existent 칥ntr-unul nou. Astfel, la 칥nceput, aceste fuziuni vor crea tokenuri cu dou캒 caractere, iar apoi, pe m캒sur캒 ce antrenamentul progreseaz캒, subwords mai lungi.

칉n orice etap캒 din timpul antren캒rii tokenizerului, algoritmul BPE va c캒uta cea mai frecvent캒 pereche de tokenuri existente (prin "pereche" 칥n탵elegem aici doi tokeni consecutivi 칥ntr-un cuv칙nt). Acea pereche cea mai frecvent캒 este cea care va fi mergea, iar noi 탳tergem 탳i repet캒m pentru pasul urm캒tor.

Revenind la exemplul nostru anterior, s캒 presupunem c캒 cuvintele au urm캒toarele frecven탵e:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

칥n탵eleg칙nd c캒 `"hug"` a fost prezent de 10 ori 칥n corpus, `"pug"` de 5 ori, `"pun"` de 12 ori, `"bun"` de 4 ori, iar `"hugs"` de 5 ori. 칉ncepem antrenamentul 칥mp캒r탵ind fiecare cuv칙nt 칥n caractere (cele care formeaz캒 vocabularul nostru ini탵ial), astfel 칥nc칙t s캒 putem vedea fiecare cuv칙nt ca pe o list캒 de tokeni:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

Apoi ne uit캒m la perechi. Perechea `("h", "u")` este prezent캒 칥n cuvintele `"hug"` 탳i `"hugs"`, deci de 15 ori 칥n total 칥n corpus. Totu탳i, nu este cea mai frecvent캒 pereche: aceast캒 onoare revine perechii `("u", "g")`, care este prezent캒 칥n cuvintele `"hug"`, `"pug"` 탳i `"hugs"`, pentru un total de 20 de ori 칥n vocabular.

Astfel, prima regul캒 de merge 칥nv캒탵at캒 de tokenizer este `("u", "g") -> "ug"`, ceea ce 칥nseamn캒 c캒 `"ug"` va fi ad캒ugat la vocabular, iar perechea ar trebui s캒 fie merged 칥n toate cuvintele din corpus. La sf칙r탳itul acestei etape, vocabularul 탳i corpus-ul arat캒 astfel:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

Acum avem c칙teva perechi care rezult캒 칥ntr-un token mai lung de dou캒 caractere: perechea `("h", "ug")`, de exemplu (prezent캒 de 15 ori 칥n corpus). Cea mai frecvent캒 pereche 칥n aceast캒 etap캒 este `("u", "n")`, prezent캒 칥ns캒 de 16 ori 칥n corpus, astfel 칥nc칙t a doua regul캒 de 칥mbinare 칥nv캒탵at캒 este `("u", "n") -> "un"`. Ad캒ugarea acestei reguli la vocabular 탳i mergingul tuturor apari탵iilor existente ne conduce la:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

Acum, cea mai frecvent캒 pereche este `("h", "ug")`, a탳a c캒 칥nv캒탵캒m regula de erge `("h", "ug") -> "hug"`, care ne ofer캒 primul nostru simbol din trei litere. Dup캒 fuzionare, corpusul arat캒 astfel:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

탲i continu캒m astfel p칙n캒 c칙nd ajungem la dimensiunea dorit캒 a vocabularului.

> [!TIP]
> 九勇 **Acum e r칙ndul t캒u!** Care crezi c캒 va fi urm캒toarea regul캒 de fuziune?

## Algoritmul de tokenizare[[tokenization-algorithm]]

Tokenizarea urmeaz캒 칥ndeaproape procesul de antrenare, 칥n sensul c캒 noile inputuri sunt tokenizate prin aplicarea urm캒toarelor etape:

1. Normalizare
2. Pre-tokenizare
3. Divizarea cuvintelor 칥n caractere individuale
4. Aplicarea regulilor de merge 칥nv캒탵ate 칥n ordine asupra acestor 칥mp캒r탵iri

S캒 lu캒m exemplul pe care l-am folosit 칥n timpul antrenamentului, cu cele trei reguli de merge 칥nv캒탵ate:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

Cuv칙ntul `"bug"` va fi tokenizat ca `["b", "ug"]`. Cu toate acestea, cuv칙ntul `"mug"` va fi tokenizat ca `["[UNK]", "ug"]` deoarece litera `"m"` nu a fost 칥n vocabularul de baz캒. De asemenea, cuv칙ntul `"thug"` va fi tokenizat ca `["[UNK]", "hug"]`: litera `"t"` nu se afl캒 칥n vocabularul de baz캒, iar aplicarea regulilor de merge duce mai 칥nt칙i la fuzionarea lui `"u"` 탳i `"g"` 탳i apoi la fuzionarea lui `"h"` 탳i `"ug"`.

> [!TIP]
> 九勇 **Acum e r칙ndul t캒u!** Cum crezi c캒 va fi tokenizat cuv칙ntul `"unhug"`?

## Implementarea BPE[[implementing-bpe]]

Acum s캒 arunc캒m o privire la implementarea algoritmului BPE. Aceasta nu va fi o versiune optimizat캒 pe care o pute탵i utiliza pe un corpus mare; dorim doar s캒 v캒 ar캒t캒m codul pentru a putea 칥n탵elege algoritmul pu탵in mai bine.

칉n primul r칙nd avem nevoie de un corpus, a탳a c캒 haide탵i s캒 cre캒m unul simplu cu c칙teva propozi탵ii:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Apoi, trebuie s캒 pre-tokeniz캒m acest corpus 칥n cuvinte. Deoarece replic캒m un tokenizator BPE (precum GPT-2), vom utiliza tokenizatorul `gpt2` pentru pre-tokenizare:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Apoi, calcul캒m frecven탵ele fiec캒rui cuv칙nt din corpus la fel ca 칥n cazul pre-tokeniz캒rii:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, '말s': 2, '맚he': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, '맊hapter': 1,
    '마bout': 1, '맚okenization': 1, '맙ection': 1, '맙hows': 1, '맙everal': 1, '맚okenizer': 1, '마lgorithms': 1,
    'Hopefully': 1, ',': 1, '맟ou': 1, '망ill': 1, '막e': 1, '마ble': 1, '맚o': 1, '맛nderstand': 1, '맏ow': 1,
    '맚hey': 1, '마re': 1, '맚rained': 1, '마nd': 1, '많enerate': 1, '맚okens': 1})
```

Urm캒torul pas este calcularea vocabularului de baz캒, format din toate caracterele utilizate 칥n corpus:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', '']
```

De asemenea, ad캒ug캒m tokenurile speciale utilizate de model la 칥nceputul vocabularului respectiv. 칉n cazul GPT-2, singurul simbol special este `"<|endoftext|>"`:

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

Acum trebuie s캒 칥mp캒r탵im fiecare cuv칙nt 칥n caractere individuale, pentru a putea 칥ncepe antrenarea:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

Acum c캒 suntem preg캒ti탵i pentru antrenare, s캒 scriem o func탵ie care calculeaz캒 frecven탵a fiec캒rei perechi. Va trebui s캒 folosim aceast캒 func탵ie la fiecare etap캒 a antrenare:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

S캒 arunc캒m o privire la o parte din acest dic탵ionar dup캒 separ캒rile ini탵iale:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('', 'i'): 2
('', 't'): 7
('t', 'h'): 3
```

Acum, pentru a g캒si cea mai frecvent캒 pereche este nevoie doar de o loop rapid:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('', 't') 7
```

A탳adar, prima 칥mbinare care trebuie 칥nv캒탵at캒 este `('', 't') -> '맚'`, 탳i ad캒ug캒m `'맚'` la vocabular:

```python
merges = {("", "t"): "맚"}
vocab.append("맚")
```

Pentru a continua, trebuie s캒 aplic캒m aceast merge 칥n dic탵ionarul nostru `splits`. S캒 scriem o alt캒 func탵ie pentru acest lucru:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

탲i putem arunca o privire la rezultatul primului merge:

```py
splits = merge_pair("", "t", splits)
print(splits["맚rained"])
```

```python out
['맚', 'r', 'a', 'i', 'n', 'e', 'd']
```

Acum avem tot ce ne trebuie pentru a face bucle p칙n캒 c칙nd vom 칥nv캒탵a toate mergeu-rile dorite. Ne focus캒m pe o m캒rime a vocabularui de 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

Ca rezultat, am 칥nv캒탵at 19 reguli de merge(vocabularul ini탵ial avea o dimensiune de 31 -- 30 de caractere din alfabet, plus simbolul special):

```py
print(merges)
```

```python out
{('', 't'): '맚', ('i', 's'): 'is', ('e', 'r'): 'er', ('', 'a'): '마', ('맚', 'o'): '맚o', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('맚o', 'k'): '맚ok',
 ('맚ok', 'en'): '맚oken', ('n', 'd'): 'nd', ('', 'is'): '말s', ('맚', 'h'): '맚h', ('맚h', 'e'): '맚he',
 ('i', 'n'): 'in', ('마', 'b'): '마b', ('맚oken', 'i'): '맚okeni'}
```

Iar vocabularul este compus din simbolul special, alfabetul ini탵ial 탳i toate rezultatele merge-urilor:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', '', '맚', 'is', 'er', '마', '맚o', 'en', 'Th', 'This', 'ou', 'se',
 '맚ok', '맚oken', 'nd', '말s', '맚h', '맚he', 'in', '마b', '맚okeni']
```

> [!TIP]
> 游눠 Folosind `train_new_from_iterator()` pe acela탳i corpus nu va rezulta exact acela탳i vocabular. Acest lucru se datoreaz캒 faptului c캒 atunci c칙nd exist캒 o alegere a celei mai frecvente perechi, am selectat-o pe prima 칥nt칙lnit캒, 칥n timp ce biblioteca 游뱅 Tokenizers o selecteaz캒 pe prima pe baza ID-urilor sale interne.

Pentru a tokeniza un text nou, 칥l pre-tokeniz캒m, 칥l 칥mp캒r탵im, apoi aplic캒m toate regulile de merge 칥nv캒탵ate:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

Putem 칥ncerca acest lucru pe orice text compus din caractere din alfabet:

```py
tokenize("This is not a token.")
```

```python out
['This', '말s', '', 'n', 'o', 't', '마', '맚oken', '.']
```

> [!WARNING]
> 丘멆잺 Implementarea noastr캒 va arunca o eroare dac캒 exist캒 un caracter necunoscut, deoarece nu am f캒cut nimic pentru a le gestiona. GPT-2 nu are de fapt un token necunoscut (este imposibil s캒 ob탵ine탵i un caracter necunoscut atunci c칙nd utiliza탵i BPE la nivel de bytes), dar acest lucru s-ar putea 칥nt칙mpla aici deoarece nu am inclus toate byte-urile posibile 칥n vocabularul ini탵ial. Acest aspect al BPE dep캒탳e탳te domeniul de aplicare al acestei sec탵iuni, a탳a c캒 am omis detaliile.

Asta e tot pentru algoritmul BPE! 칉n continuare, ne vom uita la WordPiece.