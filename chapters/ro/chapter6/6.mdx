# Tokenizarea WordPiece[[wordpiece-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
]} />

WordPiece este algoritmul de tokenizare dezvoltat de Google pentru preantrenarea BERT. De atunci, acesta a fost reutilizat 칥n numeroase modele Transformers bazate pe BERT, cum ar fi DistilBERT, MobileBERT, Funnel Transformers 탳i MPNET. Este foarte similar cu BPE 칥n ceea ce prive탳te antrenarea, dar tokenizarea efectiv캒 se face diferit.

<Youtube id="qpv6ms_t_1A"/>

> [!TIP]
> 游눠 Aceast캒 sec탵iune acoper캒 WordPiece 칥n profunzime, merg칙nd p칙n캒 la prezentarea unei implement캒ri complete. Pute탵i s캒ri la sf칙r탳it dac캒 dori탵i doar o prezentare general캒 a algoritmului de tokenizare.

## Algoritmul de antrenare[[training-algorithm]]

> [!WARNING]
> 丘멆잺 Google nu a publicat niciodat캒 implementarea algoritmului de formare a WordPiece, astfel 칥nc칙t ceea ce urmeaz캒 este cea mai bun캒 presupunere a noastr캒 bazat캒 pe literatura publicat캒. Este posibil s캒 nu fie 100% exact캒.

La fel ca BPE, WordPiece porne탳te de la un vocabular restr칙ns care include simbolurile speciale utilizate de model 탳i alfabetul ini탵ial. Deoarece identific캒 subcuvinte prin ad캒ugarea unui prefix (cum ar fi `##` pentru BERT), fiecare cuv칙nt este ini탵ial 칥mp캒r탵it prin ad캒ugarea prefixului respectiv la toate caracterele din cuv칙nt. Astfel, de exemplu, `"word"` este 칥mp캒r탵it astfel:

```
w ##o ##r ##d
```

Astfel, alfabetul ini탵ial con탵ine toate caracterele prezente la 칥nceputul unui cuv칙nt 탳i caracterele prezente 칥n interiorul unui cuv칙nt cu prefixul WordPiece.

Apoi, la fel ca BPE, WordPiece 칥nva탵캒 reguli de merge. Principala diferen탵캒 este modul 칥n care este selectat캒 perechea care urmeaz캒 s캒 fie merged. 칉n loc s캒 selecteze cea mai frecvent캒 pereche, WordPiece calculeaz캒 un scor pentru fiecare pereche, utiliz칙nd urm캒toarea formul캒:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

칉mp캒r탵ind frecven탵a perechii la produsul frecven탵elor fiec캒rei p캒r탵i a acesteia, algoritmul prioritizeaz캒 fuzionarea perechilor 칥n care p캒r탵ile individuale sunt mai pu탵in frecvente 칥n vocabular. De exemplu, nu va fuziona neap캒rat `("un", "##able")` chiar dac캒 aceast캒 pereche apare foarte frecvent 칥n vocabular, deoarece cele dou캒 perechi `"un"` 탳i `"##able"` vor ap캒rea probabil fiecare 칥ntr-o mul탵ime de alte cuvinte 탳i vor avea o frecven탵캒 ridicat캒. 칉n schimb, o pereche precum `("hu", "##gging")` va fi probabil fuzionat캒 mai repede (presupun칙nd c캒 cuv칙ntul "hugging" apare frecvent 칥n vocabular), deoarece `"hu"` 탳i `"##gging"` sunt probabil mai pu탵in frecvente individual.

S캒 ne uit캒m la acela탳i vocabular pe care l-am folosit 칥n exemplul de antrenare BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

Spliturile aici vor fi:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

deci vocabularul ini탵ial va fi `["b", "h", "p", "##g", "##n", "##s", "##u"]` (dac캒 uit캒m deocamdat캒 de tokenurile speciale). Cea mai frecvent캒 pereche este `("##u", "##g")` (prezent캒 de 20 de ori), dar frecven탵a individual캒 a lui `"##u"` este foarte mare, astfel 칥nc칙t scorul s캒u nu este cel mai mare (este 1 / 36). Toate perechile cu un `"##u"` au de fapt acela탳i scor (1 / 36), astfel 칥nc칙t cel mai bun scor revine perechii `("##g", "##s")` - singura f캒r캒 un `"##u"` - cu 1 / 20, iar prima 칥mbinare 칥nv캒탵at캒 este `("##g", "##s") -> ("##gs")`.

Re탵ine탵i c캒 atunci c칙nd facem merge, elimin캒m `##` dintre cele dou캒 tokenuri, deci ad캒ug캒m `"##gs"` la vocabular 탳i aplic캒m merge 칥n cuvintele din corpus:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

칉n acest moment, `"##u"` se afl캒 칥n toate perechile posibile, deci toate au acela탳i scor. S캒 spunem c캒 칥n acest caz, prima pereche este merged, deci `("h", "##u") -> "hu"`. Acest lucru ne duce la:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

Apoi, urm캒torul scor cu cel mai bun rezultat este 칥mp캒r탵it de `("hu", "##g")` 탳i `("hu", "##gs")` (cu 1/15, comparativ cu 1/21 pentru toate celelalte perechi), astfel 칥nc칙t prima pereche cu cel mai mare scor este merged

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

탳i continu캒m astfel p칙n캒 c칙nd ajungem la dimensiunea dorit캒 a vocabularului.

> [!TIP]
> 九勇 **Acum e r칙ndul t캒u!** Care va fi urm캒toarea regul캒 de merge?

## Algoritm de tokenizare[[tokenization-algorithm]]

Tokenizarea difer캒 칥n WordPiece 탳i BPE prin faptul c캒 WordPiece salveaz캒 doar vocabularul final, nu 탳i regulile de merge 칥nv캒탵ate. Pornind de la cuv칙ntul de tokenizat, WordPiece g캒se탳te cel mai lung subcuv칙nt care se afl캒 칥n vocabular, apoi 칥l 칥mparte. De exemplu, dac캒 folosim vocabularul 칥nv캒탵at 칥n exemplul de mai sus, pentru cuv칙ntul `"hugs"` cel mai lung subcuv칙nt de la 칥nceput care se afl캒 칥n vocabular este `"hug"`, a탳a c캒 칥mp캒r탵im acolo 탳i ob탵inem `["hug", "##s"]`. Apoi continu캒m cu `"##s"`, care se afl캒 칥n vocabular, deci tokenizarea lui `"hugs"` este `["hug", "##s"]`.

Cu BPE, am fi aplicat mergeurile 칥nv캒탵ate 칥n ordine 탳i am fi tokenizat acest lucru ca `["hu", "##gs"]`, deci encodingul este diferit.

Ca un alt exemplu, s캒 vedem cum ar fi tokenizat cuv칙ntul `"bugs"`. `"b"` este cel mai lung subcuv칙nt care 칥ncepe de la 칥nceputul cuv칙ntului care se afl캒 칥n vocabular, a탳a c캒 칥l 칥mp캒r탵im acolo 탳i ob탵inem `["b", "##ugs"]`. Apoi, `"##u"` este cel mai lung subcuv칙nt care 칥ncepe de la 칥nceputul cuv칙ntului `"##ugs"` care se afl캒 칥n vocabular, deci 칥l separ캒m 탳i ob탵inem `["b", "##u, "##gs"]`. 칉n cele din urm캒, `"##gs"` se afl캒 칥n vocabular, deci aceast캒 ultim캒 list캒 este tokenizarea lui `"bugs"`.

Atunci c칙nd tokenizarea ajunge 칥ntr-un stadiu 칥n care nu este posibil캒 g캒sirea unui subcuv칙nt 칥n vocabular, 칥ntregul cuv칙nt este tokenizat ca necunoscut - astfel, de exemplu, `"mug"` ar fi tokenizat ca `["[UNK]"]`, la fel ca `"bum"` (chiar dac캒 putem 칥ncepe cu `"b"` 탳i `"##u"`, `"##m"` nu face parte din vocabular, iar tokenizarea rezultat캒 va fi `["[UNK]"]`, nu `["b", "##u", "[UNK]"]`). Aceasta este o alt캒 diferen탵캒 fa탵캒 de BPE, care ar clasifica doar caracterele individuale care nu se afl캒 칥n vocabular ca necunoscute.

> [!TIP]
> 九勇 **Acum e r칙ndul t캒u!** Cum va fi tokenizat cuv칙ntul `"pugs"`?

## Implement칙nd WordPiece[[implementing-wordpiece]]

Acum s캒 arunc캒m o privire la o implementare a algoritmului WordPiece. La fel ca 칥n cazul BPE, acest lucru este doar pedagogic 탳i nu ve탵i putea s캒 칥l utiliza탵i pe un corpus mare.

Vom utiliza acela탳i corpus ca 칥n exemplul BPE:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

칉n primul r칙nd, trebuie s캒 pre-tokeniz캒m corpusul 칥n cuvinte. Deoarece replic캒m un tokenizator WordPiece (precum BERT), vom utiliza tokenizatorul `bert-base-cased` pentru pre-tokenizare:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Apoi, calcul캒m frecven탵ele fiec캒rui cuv칙nt din corpus la fel ca 칥n cazul pre-tokeniz캒rii:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

Dup캒 cum am v캒zut mai devreme, alfabetul este setul unic compus din toate primele litere ale cuvintelor 탳i toate celelalte litere care apar 칥n cuvintele cu prefixul `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

De asemenea, ad캒ug캒m simbolurile speciale utilizate de model la 칥nceputul vocabularului respectiv. 칉n cazul BERT, este vorba de lista `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]``:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

Apoi trebuie s캒 칥mp캒r탵im fiecare cuv칙nt, cu toate literele care nu sunt primele cu prefixul `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

Acum c캒 suntem preg캒ti탵i pentru antrenare, s캒 scriem o func탵ie care calculeaz캒 scorul fiec캒rei perechi. Va trebui s캒 folosim aceast캒 func탵ie la fiecare etap캒 a antren캒rii:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

S캒 arunc캒m o privire la o parte din acest dic탵ionar dup캒 separ캒rile ini탵iale:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

Acum, pentru a g캒si perechea cu cel mai bun scor este nevoie doar de un loop rapid:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

A탳adar, primul merge de 칥nv캒탵at este `('a', '##b') -> 'ab'`, iar noi ad캒ug캒m `'ab'` la vocabular:

```python
vocab.append("ab")
```

Pentru a continua, trebuie s캒 aplic캒m acest merge 칥n dic탵ionarul nostru `splits`. S캒 scriem o alt캒 func탵ie pentru acest lucru:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

탲i putem arunca o privire la rezultatul primului merge:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

Acum avem tot ce ne trebuie pentru a face bucle p칙n캒 c칙nd vom 칥nv캒탵a toate merge-urile dorite. S캒 ne propunem un vocabular de m캒rimea 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

Ne putem uita apoi la vocabularul generat:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

Dup캒 cum putem vedea, 칥n compara탵ie cu BPE, acest tokenizator 칥nva탵캒 p캒r탵ile din cuvinte ca tokenuri pu탵in mai repede.

> [!TIP]
> 游눠 Folosind `train_new_from_iterator()` pe acela탳i corpus nu va rezulta exact acela탳i vocabular. Acest lucru se datoreaz캒 faptului c캒 biblioteca 游뱅 Tokenizers nu implementeaz캒 WordPiece pentru antrenare (deoarece nu suntem complet siguri cum func탵ioneaz캒 intern), ci utilizeaz캒 BPE 칥n schimb.

Pentru a tokeniza un text nou, 칥l pre-tokeniz캒m, 칥l 칥mp캒r탵im, apoi aplic캒m algoritmul de tokenizare pe fiecare cuv칙nt. Adic캒, c캒ut캒m cel mai mare subcuv칙nt 칥ncep칙nd de la 칥nceputul primului cuv칙nt 탳i 칥l 칥mp캒r탵im, apoi repet캒m procesul pentru a doua parte 탳i a탳a mai departe pentru restul acelui cuv칙nt 탳i pentru urm캒toarele cuvinte din text:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

Haide탵i s캒-l test캒m pe un cuv칙nt care se afl캒 칥n vocabular 탳i pe altul care nu se afl캒:


```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

Acum, scriem o func탵ie care tokenizeaz캒 un text:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

칉l putem 칥ncerca pe orice text:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

Asta e tot pentru algoritmul WordPiece! Acum s캒 arunc캒m o privire la Unigram.
