# Antrenarea unui nou tokenizer dintr-unul vechi[[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

Dac캒 un model de limbaj nu este disponibil 칥n limba dorit캒 sau dac캒 corpusul t캒u este foarte diferit de cel pe care modelul de limbaj a fost antrenat, este probabil c캒 ve탵i dori s캒 antrena탵i modelul de la zero, folosind un tokenizer adaptat datelor tale. Acest lucru va necesita antrenarea unui nou tokenizer pe datasetul t캒u. Dar ce 칥nseamn캒 exact asta? C칙nd am examinat pentru prima dat캒 tokenizatorii 칥n [Capitolul 2](/course/chapter2), am v캒zut c캒 majoritatea modelelor Transformer folosesc un _algoritm de subword tokenization_. Pentru a identifica care subcuvinte sunt de interes 탳i apar cel mai frecvent 칥n corpusul respectiv, tokenizerul trebuie s캒 examineze cu aten탵ie toate textele din corpus - un proces pe care 칥l numim *antrenare*. Regulile exacte care conduc aceast캒 antrenare depind de tipul de tokenizer utilizat 탳i vom prezenta cei trei algoritmi principali mai t칙rziu 칥n acest capitol.

<Youtube id="DJimQynXZsQ"/>

> [!WARNING]
> 丘멆잺 Antrenarea unui tokenizer nu este acela탳i lucru ca antrenarea unui model! Antrenarea modelului folose탳te stochastic gradient descent pentru a face pierderea pu탵in mai mic캒 pentru fiecare batch. Este randomizat캒 prin natur캒 (ceea ce 칥nseamn캒 c캒 trebuie s캒 seta탵i ni탳te seeduri pentru a ob탵ine acelea탳i rezultate atunci c칙nd face탵i aceea탳i antrenare de dou캒 ori). Antrenarea unui tokenizer este un proces statistic care 칥ncearc캒 s캒 identifice care subcuvinte sunt cele mai bune pentru a fi selectate pentru un anumit corpus, 탳i regulile exacte utilizate pentru a le selecta depind de algoritmul de tokenizare. Este determinist, ceea ce 칥nseamn캒 c캒 칥ntotdeauna ob탵ine탵i acelea탳i rezultate atunci c칙nd antrena탵i cu acela탳i algoritm pe acela탳i corpus.

## Asamblarea unui corpus[[assembling-a-corpus]]

Exist캒 o interfa탵캒 API foarte simpl캒 칥n 游뱅 Transformers pe care o pute탵i utiliza pentru a antrena un nou tokenizer cu acelea탳i caracteristici ca unul existent: `AutoTokenizer.train_new_from_iterator()`. Pentru a vedea acest lucru 칥n ac탵iune, s캒 zicem c캒 vrem s캒 antren캒m GPT-2 de la zero, dar 칥ntr-o alt캒 limb캒 dec칙t engleza. Prima noastr캒 sarcin캒 va fi s캒 adun캒m multe date 칥n acea limb캒 칥ntr-un corpus de antrenare. Pentru a oferi exemple pe care toat캒 lumea le poate 칥n탵elege, nu vom folosi o limb캒 ca rus캒 sau chineza aici, ci mai degrab캒 o limb캒 englez캒 specializat캒: codul Python.

Biblioteca [游뱅 Datasets](https://github.com/huggingface/datasets) ne poate ajuta s캒 asambl캒m un corpus de cod surs캒 Python. Vom folosi func탵ia obi탳nuit캒 `load_dataset()` pentru a desc캒rca 탳i a p캒stra 칥n cache datasetul [CodeSearchNet](https://huggingface.co/datasets/code_search_net). Acest dataset a fost creat pentru [Provocarea CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) 탳i con탵ine milioane de func탵ii din biblioteci open-source de pe GitHub 칥n mai multe limbaje de programare. Aici, vom 칥nc캒rca partea Python a acestui dataset:

```py
from datasets import load_dataset

# Acest lucru poate dura c칙teva minute pentru a 칥nc캒rca, a탳a c캒 lua탵i o pauz캒 탳i be탵i o cea탳c캒 de cafea sau ceai 칥n timp ce a탳tepta탵i!
raw_datasets = load_dataset("code_search_net", "python")
```

Putem s캒 ne uit캒m la splitul de antrenare pentru a vedea la care coloane avem acces:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

Putem vedea c캒 dataset-ul separ캒 docstringurile de cod 탳i sugereaz캒 o tokenizare a ambelor. Aici, vom folosi doar coloana `whole_func_string` pentru a antrena tokenizerul nostru. Putem s캒 ne uit캒m la un exemplu al unei astfel de func탵ii prin indexarea 칥n splitul de antrenare:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

care ar trebui s캒 printeze urm캒torul lucru:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

Primul lucru pe care trebuie s캒-l facem este s캒 transform캒m setul de date 칥ntr-un _iterator_ de liste de texte - de exemplu, o list캒 de liste de texte. Utilizarea listelor de texte va permite tokenizerului nostru s캒 func탵ioneze mai rapid (antren칙ndu-se pe batch-uri de texte 칥n loc de a procesa texte individuale unul c칙te unul), iar acesta ar trebui s캒 fie un iterator dac캒 dorim s캒 evit캒m s캒 avem tot 칥n memoria RAM deodat캒. Dac캒 corpusul t캒u este uria탳, ve탵i dori s캒 profita탵i de faptul c캒 游뱅 Datasets nu 칥ncarc캒 totul 칥n memoria RAM, ci stocheaz캒 elementele datasetului pe disc.

Urm캒toarea opera탵ie ar crea o list캒 de liste de 1.000 de texte fiecare, dar ar 칥nc캒rca totul 칥n memorie:

```py
# Nu face탵i uncomment urm캒toarei linii dac캒 datasetul vostru este mare, ci doar dac캒 este mic!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

Utiliz칙nd un generator Python, putem evita ca Python s캒 칥ncarce orice 칥n memorie p칙n캒 c칙nd este realmente necesar. Pentru a crea un astfel de generator, trebuie doar s캒 칥nlocui탵i parantezele p캒trate cu paranteze rotunde:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

Aceast캒 linie de cod nu extrage niciun element al setului de date; doar creeaz캒 un obiect pe care 칥l pute탵i utiliza 칥ntr-un `for` loop din Python. Textele vor fi 칥nc캒rcate doar atunci c칙nd ve탵i avea nevoie de ele(adic캒 atunci c칙nd sunte탵i la pasul loopului `for` care le solicit캒), iar doar 1.000 de texte vor fi 칥nc캒rcate la un moment dat. Acest mod v캒 permite s캒 nu epuiza탵i memoria RAM chiar dac캒 prelucra탵i un set de date uria탳.

Problema cu un obiect generator este c캒 poate fi utilizat doar o dat캒, a탳adar, 칥n loc s캒 ne ofere lista primelor 10 cifre de dou캒 ori:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

noi le primim o dat캒 탳i apoi o list캒 goal캒:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

Din acest motive noi definim o func탵ie ce returneaz캒 칥n schimb un generator: 

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

칉n acela탳i timp po탵i defini un generator 칥n캒untrul unui `for` loop folosing statementul `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

ceea ce va produce acela탳i generator ca 칥nainte, dar 칥탵i va permite s캒 folose탳ti o logic캒 mai complex캒 dec칙t cea pe care ai putea s캒 o folose탳ti 칥ntr-un list comprehension.

## Antrenarea unui nou tokenizer[[training-a-new-tokenizer]]

Acum c캒 avem corpusul nostru sub forma unui iterator de batch-uri de texte, suntem gata s캒 antren캒m un nou tokenizer. Pentru a face acest lucru, trebuie mai 칥nt칙i s캒 칥nc캒rc캒m tokenizerul pe care dorim s캒-l asociem cu modelul nostru (aici, GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Chiar dac캒 urmeaz캒 s캒 antren캒m un nou tokenizer, este o idee bun캒 s캒 facem acest lucru pentru a evita s캒 칥ncepem s캒 facem tot de la zero. Astfel, nu vom fi nevoi탵i s캒 specific캒m nimic despre algoritmul de tokenizare sau despre special tokens pe care 칥i vom utiliza; noul nostru tokenizer va fi exact la fel ca GPT-2, iar singur lucrul care se va schimba este vocabularul, care va fi determinat de antrenarea pe corpusul nostru.

Mai 칥nt칙i, hai s캒 vedem cum ar interpreta acest tokenizer un exemplu de func탵ie:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', '마dd', '_', 'n', 'umbers', '(', 'a', ',', '막', '):', '캙', '', '', '', '"""', 'Add', '맚he', '맚wo',
 '맕umbers', '`', 'a', '`', '마nd', '`', 'b', '`', '."', '""', '캙', '', '', '', 'return', '마', '+', '막']
```

Acest tokenizer are c칙teva simboluri speciale, cum ar fi `` 탳i `캙`, care denot캒 spa탵ii 탳i noi linii, respectiv. A탳a cum se poate vedea, acest lucru nu este prea eficient: tokenizerul returneaz캒 tokenuri individuale pentru fiecare spa탵iu, c칙nd ar putea grupa 칥mpreun캒 nivelurile de indentare (deoarece av칙nd seturi de patru sau opt spa탵ii va fi foarte comun 칥n cod). De asemenea, a divizat numele func탵iei 칥ntr-un mod ciudat, nefiind obi탳nuit s캒 vad캒 cuvinte care con탵in caracterele `_`.

Acum hai s캒 antren캒m un nou tokenizer 탳i s캒 vedem dac캒 rezolv캒 aceste probleme. Pentru aceasta, vom utiliza metoda `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

Aceast캒 comand캒 ar putea dura pu탵in timp dac캒 corpusul este foarte mare, dar pentru acest dataset de 1,6 GB de texte este extrem de rapid (1 minut 탳i 16 secunde pe un procesor AMD Ryzen 9 3900X cu 12 nuclee).

Re탵ine c캒 `AutoTokenizer.train_new_from_iterator()` func탵ioneaz캒 doar dac캒 tokenizerul pe care 칥l utiliza탵i este un tokenizer "rapid". A탳a cum ve탵i vedea 칥n urm캒toarea sec탵iune, biblioteca 游뱅 Transformers con탵ine dou캒 tipuri de tokenizeri: unii sunt scri탳i 칥n pur Python, iar al탵ii (cei rapizi) sunt sus탵inu탵i de biblioteca 游뱅 Tokenizers, care este scris캒 칥n limbajul de programare Rust. Python este limbajul cel mai frecvent utilizat pentru aplica탵ii de data science 탳i deep learning, dar atunci c칙nd orice trebuie s캒 fie paralelizat pentru a fi rapid, trebuie s캒 fie scris 칥ntr-un alt limbaj de programare. De exemplu, multiplic캒rile matricelor care sunt la baza calculelor modelului sunt scrise 칥n CUDA, o bibliotec캒 C optimizat캒 pentru GPU-uri.

Antrenarea unui tokenizer nou 칥n pur  Python ar fi extrem de lent, de aceea am dezvoltat biblioteca 游뱅 Tokenizers. Re탵ine c캒, la fel cum nu a trebuit s캒 칥nv캒탵a탵i limbajul CUDA pentru a putea executa modelul pe un batch de inputuri pe un GPU, nu ve탵i avea nevoie s캒 칥nv캒탵a탵i Rust pentru a utiliza un tokenizer rapid. Biblioteca 游뱅 Tokenizers ofer캒 leg캒turi Python pentru multe metode care apeleaz캒 intern unele buc캒탵i de cod 칥n Rust; de exemplu, pentru a paraleliza antrenarea noului tokenizer sau, a탳a cum am v캒zut 칥n [Capitolul 3](/course/chapter3), tokenizarea unui batch de inputuri.

Majoritatea modelelor Transformer au un tokenizer rapid disponibil (exist캒 unele excep탵ii pe care le pute탵i verifica [aici](https://huggingface.co/transformers/#supported-frameworks)), iar API-ul `AutoTokenizer` selecteaz캒 칥ntotdeauna tokenizerul rapid pentru tine dac캒 este disponibil. 칉n urm캒toarea sec탵iune, vom examina unele dintre celelalte caracteristici speciale ale tokenizerilor rapizi, care vor fi foarte utile pentru sarcini precum clasificarea tokenilor 탳i r캒spunderea la 칥ntreb캒ri. 칉nainte de a face acest lucru, totu탳i, s캒 칥ncerc캒m noul nostru tokenizer pe exemplul anterior:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', '마dd', '_', 'numbers', '(', 'a', ',', '막', '):', '캙먟먟', '"""', 'Add', '맚he', '맚wo', '맕umbers', '`',
 'a', '`', '마nd', '`', 'b', '`."""', '캙먟먟', 'return', '마', '+', '막']
```

Aici din nou vedem simboluri speciale ca `` sau `캙` care denot캒 spa탵ii sau linii noi, dar 칥n acela탳i timp putem vedea c캒 tokenizerul nostru a 칥nv캒탵at c칙탵iva tokens care sunt foarte specifici la corpusul de func탵ii Python: de exemplu, tokenul `캙먟먟` care reprezint캒 indentarea, sau tokenul `"""` care reprezint캒 cele trei ghilimele cu care se 칥ncepe un docstring. Tokenizerul, de asemenea face split corect numelui fun탵iei pe `_`. Aceasta chiar este o reprezentare compact캒: comparativ, utiliz칙nd limba tokenizerului englez pe acela탳i exemplu ne va da o propozi탵ie mai lung캒:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

Hai s캒 ne uit캒m la un alt exemplu:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'Linear', 'Layer', '():', '캙먟먟', '맋ef', '__', 'init', '__(', 'self', ',', '말nput', '_', 'size', ',',
 '맖utput', '_', 'size', '):', '캙먟먟먟먟먟먟', '맙elf', '.', 'weight', '=', '맚orch', '.', 'randn', '(', 'input', '_',
 'size', ',', '맖utput', '_', 'size', ')', '캙먟먟먟먟먟먟', '맙elf', '.', 'bias', '=', '맚orch', '.', 'zeros', '(',
 'output', '_', 'size', ')', '캙캙먟먟', '맋ef', '__', 'call', '__(', 'self', ',', '맞', '):', '캙먟먟먟먟먟먟',
 'return', '맞', '@', '맙elf', '.', 'weights', '+', '맙elf', '.', 'bias', '캙먟먟먟']
```

칉n plus fa탵캒 de tokenul corespunz캒tor unei indent캒ri, aici putem vedea 탳i un token pentru o indentare dubl캒: `캙먟먟먟먟먟먟`. Cuvintele speciale din Python, cum ar fi `class`, `init`, `call`, `self` 탳i `return`, sunt tokenizate fiecare ca un singur token, 탳i putem vedea c캒, pe l칙ng캒 divizarea la `_` 탳i `.`, tokenizerul divizeaz캒 corect chiar 탳i numele scrise 칥n stil camel-case: `LinearLayer` este tokenizeat ca `["Linear", "Layer"]`.

## Salvarea tokenizerului[[saving-the-tokenizer]]

Pentru a ne asigura c캒 칥l putem utiliza mai t칙rziu, trebuie s캒 salv캒m noul nostru tokenizer. Asem캒n캒tor salv캒rii unui model, acest lucru se realizeaz캒 cu metoda `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

Acest lucru va crea un nou folder numit *code-search-net-tokenizer*, care va con탵ine toate fi탳ierele necesare pentru a re칥nc캒rca tokenizerul. Dac캒 dori탵i s캒 partaja탵i acest tokenizer cu colegii 탳i prietenii t캒i, 칥l pute탵i 칥nc캒rca pe Hub prin conectarea la contul t캒u. Dac캒 lucra탵i 칥ntr-un notebook, exist캒 un convenience function pentru a v캒 ajuta cu acest lucru:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Acest lucru va afi탳a un widget 칥n care pute탵i introduce creden탵ialele de conectare Hugging Face. Dac캒 nu lucra탵i 칥ntr-un notebook, introduce탵i simplu urm캒toarea linie 칥n terminal:

```bash
huggingface-cli login
```

Dup캒 ce v-a탵i conectat, pute탵i face push tokenizerului prin executarea urm캒toarei comenzi:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

Acest lucru va crea un nou repositoriu 칥n namespace-ul t캒u cu numele `code-search-net-tokenizer`, care va con탵ine fi탳ierul tokenizerului. Dup캒 aceea, pute탵i 칥nc캒rca tokenizerul de oriunde cu metoda `from_pretrained()`:

```py
# 칉nlocui탵i "huggingface-course" mai jos cu namespace-ul t캒u pentru a utiliza propriul tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

Acum sunte탵i gata s캒 antrena탵i un model de limbaj de la zero 탳i s캒 칥l ajusta탵i pentru sarcina voastr캒! Vom face acest lucru 칥n [Capitolul 7](/course/chapter7), dar mai 칥nt칙i, 칥n continuarea acestui capitol, vom arunca o privire mai atent캒 asupra tokenizerilor rapizi 탳i vom explora 칥n detaliu ce se 칥nt칙mpl캒 atunci c칙nd apela탵i metoda `train_new_from_iterator()`.