<FrameworkSwitchCourse {fw} />

# Superputerile tokenizerilor rapizi[[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}

칉n aceast캒 sec탵iune, vom analiza mai atent capacit캒탵ile tokenizerilor din 游뱅 Transformers. P칙n캒 acum, i-am folosit doar pentru tokenizarea inputurilor sau decodificarea ID-urilor 칥napoi 칥n text, dar tokenizerii - 칥n special cei sus탵inu탵i de biblioteca 游뱅 Tokenizers - pot face mult mai multe. Pentru a ilustra aceste func탵ii suplimentare, vom explora modul de reproducere a rezultatelor pipeline-urilor `token-classification` (pe care le-am numit `ner`) 탳i `question-answering` pe care le-am 칥nt칙lnit pentru prima dat캒 칥n [Capitolul 1](/course/chapter1).

<Youtube id="g8quOxoqhHQ"/>

칉n discu탵ia urm캒toare, vom face adesea distinc탵ia 칥ntre tokenizatori "len탵i" 탳i "rapizi". Tokenizerii len탵i sunt cei scri탳i 칥n Python 칥n interiorul bibliotecii 游뱅 Transformers, 칥n timp ce versiunile rapide sunt cele furnizate de 游뱅 Tokenizers, care sunt scrise 칥n Rust. Dac캒 v캒 aminti탵i de tabelul din [Capitolul 5](/course/chapter5/3) care a raportat c칙t timp a durat un tokenizer rapid 탳i unul lent pentru a tokeniza datasetul Drug Review, ar trebui s캒 ave탵i o idee despre de ce 칥i numim len탵i 탳i rapizi:

|               | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10,8s          | 4min41s
`batched=False` | 59,2s          | 5min3s

> [!WARNING]
> 丘멆잺 Atunci c칙nd tokeniza탵i o singur캒 propozi탵ie, nu ve탵i vedea 칥ntotdeauna o diferen탵캒 de vitez캒 칥ntre versiunea lent캒 탳i rapid캒 ale aceluia탳i tokenizer. De fapt, versiunea rapid캒 poate fi chiar mai lent캒! Abia atunci c칙nd tokeniza탵i multe texte 칥n paralel, 칥n acela탳i timp, ve탵i putea observa clar diferen탵a.

## Batch encoding[[batch-encoding]]

<Youtube id="3umI3tm27Vw"/>

Rezultatul unui tokenizer nu este un simplu dic탵ionar Python; ceea ce ob탵inem este de fapt un obiect special `BatchEncoding`. Este o subclas캒 a unui dic탵ionar (de aceea am putut indexa acel rezultat f캒r캒 nici o problem캒 mai devreme), dar cu metode suplimentare care sunt utilizate 칥n principal de c캒tre tokenizerii rapizi.

Pe l칙ng캒 capacit캒탵ile lor de paralelizare, func탵ionalitatea principal캒 a tokenizerilor rapizi este aceea c캒 ace탳tia 탵in 칥ntotdeauna eviden탵a intervalului original de texte din care provin tokenii finali - o func탵ionalitate pe care o numim *offset mapping*. Acest lucru deblocheaz캒 func탵ii precum mappingul fiec캒rui cuv칙nt la tokens pe care 칥i genereaz캒 sau mappingul fiec캒rui caracter al textului original la tokenul 칥n care se afl캒 탳i invers.

Acum hai s캒 arunc캒m o privire la un exemplu:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

A탳a cum s-a men탵ionat anterior, 칥n outputul tokenizerului ob탵inem un obiect `BatchEncoding`:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

Deoarece clasa `AutoTokenizer` selecteaz캒 un tokenizer rapid 칥n mod implicit, putem utiliza metodele suplimentare pe care acest obiect `BatchEncoding` le ofer캒. Avem dou캒 metode de verificare dac캒 tokenizerul nostru este unul lent sau rapid. Putem verifica fie atributul `is_fast` al tokenizer-ului:

```python
tokenizer.is_fast
```

```python out
True
```

sau acela탳i atribut al `encoding`:

```python
encoding.is_fast
```

```python out
True
```

Hai s캒 vedem ce ne permite un tokenizer rapid s캒 facem. 칉n primul r칙nd, putem accesa tokenul f캒r캒 a fi nevoie s캒 facem convert ID-urile 칥napoi 칥n tokens:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

칉n acest caz tokenul la indexul 5 este `##yl`, ceea ce este o parte a cuv칙ntului "Sylvain" 칥n propozi탵ia original캒. 칉n acela탳i timp putem folosi metoda `word_ids()` pentru a ob탵ine indexul cuv칙ntului din care provine fiecare token:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]

Putem vedea c캒 tokenizerul are tokeni speciali `[CLS]` 탳i `[SEP]` care sunt mapped la `None`, iar apoi fiecare token este mapped la cuv칙ntul din care provine. Acest lucru este deosebit de util pentru a determina dac캒 un token este la 칥nceputul unui cuv칙nt sau dac캒 dou캒 tokenuri sunt 칥n acela탳i cuv칙nt. Ne-am putea baza pe prefixul `##` pentru aceasta, dar func탵ioneaz캒 doar pentru tokenizeri de tip BERT; aceast캒 metod캒 func탵ioneaz캒 pentru orice tip de tokenizator, at칙ta timp c칙t este unul rapid. 칉n capitolul urm캒tor, vom vedea cum putem utiliza aceast캒 capabilitate pentru a aplica labeluri pe care le avem pentru fiecare cuv칙nt 칥n mod corespunz캒tor tokenurilor 칥n sarcini precum named entity recognition (NER) 탳i part-of-speech (POS). De asemenea, 칥l putem utiliza pentru a face mask tuturor tokenurilor care provin din acela탳i cuv칙nt 칥n masked language modeling(o tehnic캒 numit캒 _whole word masking_).

> [!TIP]
> No탵iunea de ceea ce este un cuv칙nt este complicat캒. De exemplu, "I'll" (o prescurtare a "I will") conteaz캒 ca unul sau dou캒 cuvinte? Acest lucru depinde de tokenizer 탳i de opera탵iunea de pre-tokenizare pe care o aplic캒. Unii tokenizeri se divid doar pe spa탵ii, a탳a c캒 vor considera acest lucru ca un singur cuv칙nt. Al탵ii folosesc punctua탵ia pe l칙ng캒 spa탵ii, deci vor considera dou캒 cuvinte.
>
> 九勇 **칉ncerca탵i!** Crea탵i un tokenizer din checkpointurile `bert-base-cased` 탳i `roberta-base` 탳i tokeniza탵i "81s" cu ele. Ce observa탵i? Care sunt ID-urile cuvintelor?

칉n mod similar, exist캒 o metod캒 `sentence_ids()` pe care o putem utiliza pentru a face map unui token la propozi탵ia din care provine (de탳i, 칥n acest caz, `token_type_ids` returnate de tokenizer ne pot oferi aceea탳i informa탵ie).

칉n cele din urm캒, putem face map oric캒rui cuv칙nt sau token la caracterele din textul original 탳i invers, prin intermediul metodelor `word_to_chars()` sau `token_to_chars()` 탳i `char_to_word()` sau `char_to_token()`. De exemplu, metoda `word_ids()` ne-a spus c캒 `##yl` face parte din cuv칙ntul cu indicele 3, dar ce cuv칙nt este 칥n propozi탵ie? Putem afla astfel:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

A탳a cum am men탵ionat anterior, toate acestea sunt posibile datorit캒 faptului c캒 tokenizerul rapid 탵ine eviden탵a spanului de text de la care provine fiecare token 칥ntr-o list캒 de *offseturi*. Pentru a ilustra modul 칥n care se utilizeaz캒 acestea, 칥n continuare v캒 vom ar캒ta cum s캒 replica탵i rezultatele pipelineului `token-classification` manual.

> [!TIP]
> 九勇 **칉ncerca탵i!** Crea탵i propriul exemplu de text 탳i 칥ncerca탵i s캒 칥n탵elege탵i care tokenuri sunt asociate cu ID-ul cuv칙ntului 탳i, de asemenea, cum s캒 extrage탵i spanurile pentru singur cuv칙nt. Pentru puncte bonus, 칥ncerca탵i s캒 utiliza탵i dou캒 propozi탵ii ca inputuri 탳i s캒 vede탵i dac캒 ID-urile propozi탵iilor au sens pentru voi.

## 칉n interiorul pipelineului `token-classification`[[inside-the-token-classification-pipeline]]

칉n [Capitolul 1](/course/chapter1) am avut primul nostru contact aplic칙nd NER - unde sarcina const캒 칥n identificarea p캒r탵ilor textului care corespund entit캒탵ilor precum persoane, loca탵ii sau organiza탵ii - cu func탵ia `pipeline()` din 游뱅 Transformers. Apoi, 칥n [Capitolul 2](/course/chapter2), am v캒zut cum un pipeline grupeaz캒 cele trei etape necesare pentru a ob탵ine predic탵iile de la un text "raw": tokenizare, trecerea inputurilor prin model 탳i post-procesare. Primele dou캒 etape din pipelineul `token-classification` sunt la fel ca 칥n orice alt pipeline, dar post-procesarea este pu탵in mai complex캒 - hai s캒 vedem cum!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### Ob탵inerea rezultatelor de baz캒 cu pipelineul[[getting-the-base-results-with-the-pipeline]]

칉n primul r칙nd, trebuie s캒 lu캒m un token classification pipeline pentru a ob탵ine c칙teva rezultate ca s캒 le putem compara manual. Modelul utilizat 칥n mod implicit este [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); acesta aplic캒 NER pe propozi탵ii:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Modelul a identificat fiecare token generdat de "Sylvain" ca o persoan캒, fiecare token generat de "Hugging Face" ca o organiza탵ie, 탳i fiecare token "Brooklin" ca o loca탵ie". 칉n acela탳i timp putem 칥ntreba pipelineul s캒 grupeze 칥mpreun캒 tokenurile care corespund cu aceea탳i entitate.

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

`Aggregation_strategy` aleas캒 va modifica scorurile calculate pentru fiecare entitate grupat캒. Cu `"simple"`, scorul este doar media scorurilor pentru fiecare token din entitatea dat캒: de exemplu, scorul pentru "Sylvain" este media scorurilor pe care le-am v캒zut 칥n exemplele anterioare pentru token-urile `S`, `##yl`, `##va` 탳i `##in`. Alte strategii disponibile sunt:

- `"first"`, unde scorul fiec캒rei entit캒탵i este scorul primului token al acelei entit캒탵i (astfel, pentru "Sylvain" ar fi 0.993828, scorul token-ului `S`)
- `"max"`, unde scorul fiec캒rei entit캒탵i este scorul maxim al tokenilor din acea entitate (astfel, pentru "Hugging Face" ar fi 0.98879766, scorul "Face")
- `"average"`, unde scorul fiec캒rei entit캒탵i este media scorurilor cuvintelor care compun acea entitate (astfel, pentru "Sylvain" nu ar exista nicio diferen탵캒 fa탵캒 de strategia `"simple"`, dar "Hugging Face" ar avea un scor de 0.9819, media scorurilor pentru "Hugging", 0.975 탳i "Face", 0.98879)

Acum s캒 vedem cum putem ob탵ine aceste rezultate f캒r캒 a folosi func탵ia `pipeline()`!

### De la inputuri la predic탵ii[[from-inputs-to-predictions]]

{#if fw === 'pt'}

칉n primul r칙nd trebuie s캒 tokeniz캒m inputurile 탳i sp le trecem prin model. Acest lucru este f캒cut excat ca 칥n [Capitolul 2](/course/chapter2); noi am ini탵ializat tokenizerul 탳i modelul folosind clasa `AutoXxx` 탳i apoi am folosit-o pe exemplul nostru:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

Deoarece aici folosim `AutoModelForTokenClassification` , noi primim un set de logits pentru fiecare token 칥n input sequence:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

칉n primul r칙nd trebuie s캒 tokeniz캒m inputurile 탳i s캒 le trecem prin model. Acest lucru este f캒cut excat ca 칥n [Capitolul 2](/course/chapter2); noi am ini탵ializat tokenizerul 탳i modelul folosind clasa `TFAutoXxx` 탳i apoi am folosit-o pe exemplul nostru:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

Deoarece aici folosim `TFAutoModelForTokenClassification`, noi primim un set de logits pentru fiecare token 칥n input sequence:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

Noi avem un batch cu 1 secven탵캒 din 19 tokenuri 탳i modelul are 9 labeluri diferite, deci outputul modelului are un shape de 1 x 19 x 9. Ca 탳i pentru text classification pipeline, noi folosim o func탵ie softmax pentru a face convert logiturilor 칥n probabilit캒탵i 탳i lu캒m argmax pentru a ob탵ine predic탵ii(atrage aten탵ia asupra fatpului c캒 putem lua argmax pe logituri deoarece softmax nu schimb캒 ordinea):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

Atributul `model.config.id2label` con;ine mappingul indexilor la labeluri pe care le putem folosi pentru a 칥n탵elege predic탵iile:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

Cum am v캒zut mai devreme, exist캒 9 labeluri: `O` este labelul pentru tokenurile care nu se afl캒 칥n nicio entitate numit캒(aceasta reprezint캒 "exteriorul"), 탳i avem apoi dou캒 labeluri pentru fiecare tip de entitate (divers, persoan캒, organiza탵ie 탳i loca탵ie). Eticheta `B-XXX` indic캒 faptul c캒 tokenul se afl캒 la 칥nceputul entit캒탵ii `XXX` 탳i eticheta `I-XXX` indic캒 faptul c캒 tokenul se afl캒 칥n interiorul entit캒탵ii `XXX`. De exemplu, 칥n exemplul curent ne-am a탳tepta ca modelul nostru s캒 clasifice tokenul `S` ca `B-PER` (칥nceputul unei entit캒탵i de-tip persoan캒) 탳i tokenurile `##yl`, `##va` 탳i `##in` ca `I-PER` (칥n interiorul unei entit캒탵i de tip persoan캒).

S-ar putea s캒 crede탵i c캒 modelul a gre탳it 칥n acest caz, deoarece a atribuit eticheta `I-PER` tuturor acestor patru tokeni, dar acesta nu este 칥n 칥ntregime adev캒rat. Exist캒, de fapt, dou캒 formate pentru labelurile `B-` 탳i `I-`: *IOB1* 탳i *IOB2*. Formatul IOB2 (칥n roz mai jos), este cel pe care l-am introdus, 칥n timp ce formatul IOB1 (칥n albastru), utilizeaz캒 labelurile care 칥ncep cu `B-` doar pentru a separa dou캒 entit캒탵i adiacente de acela탳i tip. Modelul pe care 칥l utiliz캒m a fost fine-tuned pe un dataset care utilizeaz캒 acel format, ceea ce explic캒 de ce atribuie labelul `I-PER` tokenului `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

Cu maparea aceasta, suntem gat a s캒 reproducem(aproape 칥n total) rezultat primului pipeline -- noi putem lua scorul 탳i labelul fiec캒rui token care nu a fost clasificat ca `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

Acest lucru este foarte similar cu ce am avut mai devreme, cu o excep탵ie: pipelineul de asemenea ne-a oferit informa탵ie despre `start` 탳i `end` al fiec캒rei entit캒탵i 칥n propozi탵ia original캒. Acum e momentul c칙nd offset mappingul nostru ne va ajuta. Pentru a ob탵ine offseturile, noi trebuie s캒 set캒m `return_offsets_mapping=True` c칙nd aplic캒m tokenizerul pe inputurile noastre:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

Fiecare tuple este spanul de text care corespunde fiec캒rui token, unde `(0, 0)` este rezervat pentru tokenii speciali. Noi am v캒zut 칥nainte c캒 tokenul la indexul 5 este `##yl`, care are aici `(12, 14)` ca offsets. Dac캒 lu캒m sliceul corespunz캒tor 칥n exemplul nostru:

```py
example[12:14]
```

noi ob탵inem spanul propriu de text f캒r캒 `##`:

```python out
yl
```

Folosind aceasta, putem acum completa rezultatele anterioare:


```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Acest r캒spuns e acela탳i r캒spuns pe care l-am primit de la primul pipeline:

### Gruparea entit캒탵ilor[[grouping-entities]]

Utilizarea offseturilor pentru a determina cheile de start 탳i de sf칙r탳it pentru fiecare entitate este util, dar aceast캒 informa탵ie nu este strict necesar캒. C칙nd dorim s캒 grup캒m entit캒탵ile 칥mpreun캒, totu탳i, offseturile ne vor salva o mul탵ime de messy code. De exemplu, dac캒 am dori s캒 grup캒m 칥mpreun캒 tokenii `Hu`, `##gging` 탳i `Face`, am putea crea reguli speciale care s캒 spun캒 c캒 primele dou캒 ar trebui s캒 fie ata탳ate 탳i s캒 칥nl캒tur캒m `##`, iar `Face` ar trebui ad캒ugat cu un spa탵iu, deoarece nu 칥ncepe cu `##` -- dar acest lucru ar func탵iona doar pentru acest tip particular de tokenizer. Ar trebui s캒 scriem un alt set de reguli pentru un tokenizer SentencePiece sau unul Byte-Pair-Encoding (discutat mai t칙rziu 칥n acest capitol).

Cu offseturile, tot acel cod custom dispare: pur 탳i simplu putem lua spanul din textul original care 칥ncepe cu primul token 탳i se termin캒 cu ultimul token. Deci, 칥n cazul tokenurilor `Hu`, `##gging` 탳i `Face`, ar trebui s캒 칥ncepem la caracterul 33 (칥nceputul lui `Hu`) 탳i s캒 ne oprim 칥nainte de caracterul 45 (sf칙r탳itul lui `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

Pentru a scrie codul care post-proceseaz캒 predic탵iile 칥n timp ce grup캒m entit캒탵ile, vom grupa entit캒탵ile care sunt consecutive 탳i labeled cu `I-XXX`, cu excep탵ia primeia, care poate fi labeled ca `B-XXX` sau `I-XXX` (decidem s캒 oprim gruparea unei entit캒탵i atunci c칙nd 칥nt칙lnim un `O`, un nou tip de entitate, sau un `B-XXX` care ne spune c캒 o entitate de acela탳i tip 칥ncepe):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

탲i ob탵inem acelea탳i r캒spuns ca de la pipelineul secundar!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Alt exemplu de sarcin캒 unde offseturile sunt extrem de useful pentru r캒spunderea la 칥ntreb캒ri. Scufund칙ndu-ne 칥n pipelineuri, un lucru pe care 칥l vom face 칥n urm캒toarea sec탵iune, ne vom premite s캒 ne uit캒m peste o caracteristic캒 a tokenizerului 칥n libr캒ria 游뱅 Transformers: vom avea de-a face cu overflowing tokens c칙nd trunc캒m un input de o anumit캒 lungime.

