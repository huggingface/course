# Tokenizarea Unigram[[unigram-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
]} />

Algoritmul Unigram este adesea utilizat Ã®n SentencePiece, care este algoritmul de tokenizare utilizat de modele precum AlBERT, T5, mBART, Big Bird È™i XLNet.

<Youtube id="TGZfZVuF9Yc"/>

> [!TIP]
> ğŸ’¡ AceastÄƒ secÈ›iune acoperÄƒ Unigram Ã®n profunzime, mergÃ¢nd pÃ¢nÄƒ la prezentarea unei implementÄƒri complete. PuteÈ›i sÄƒri la sfÃ¢rÈ™it dacÄƒ doriÈ›i doar o prezentare generalÄƒ a algoritmului de tokenizare.

## Algoritm de antrenare[[training-algorithm]]

Ãn comparaÈ›ie cu BPE È™i WordPiece, Unigram lucreazÄƒ Ã®n cealaltÄƒ direcÈ›ie: porneÈ™te de la un vocabular mare È™i eliminÄƒ tokeni din acesta pÃ¢nÄƒ cÃ¢nd ajunge la dimensiunea doritÄƒ. ExistÄƒ mai multe opÈ›iuni pentru a construi acel vocabular de bazÄƒ: putem lua, de exemplu, cele mai comune substrings din cuvintele pre-tokenizate sau putem aplica BPE pe corpusul iniÈ›ial cu o dimensiune mare a vocabularului.

La fiecare etapÄƒ a antrenÄƒrii, algoritmul Unigram calculeazÄƒ o pierdere pe corpus oferit, avÃ¢nd Ã®n vedere vocabularul curent. Apoi, pentru fiecare simbol din vocabular, algoritmul calculeazÄƒ cu cÃ¢t ar creÈ™te pierderea globalÄƒ dacÄƒ simbolul ar fi eliminat È™i cautÄƒ simbolurile care ar creÈ™te cel mai puÈ›in pierderea. Aceste simboluri au cel mai redus efect asupra pierderii globale din corpus, deci, Ã®ntr-un fel, sunt "mai puÈ›in necesare" È™i sunt cei mai buni candidaÈ›i pentru eliminare.

Aceasta este o operaÈ›iune foarte costisitoare, aÈ™a cÄƒ nu eliminÄƒm doar simbolul asociat cu cea mai micÄƒ creÈ™tere a pierderii, ci procentul \\(p\\) (\\(p\\) fiind un hyperparametru pe care Ã®l poÈ›i controla, de obicei 10 sau 20) din simbolurile asociate cu cea mai micÄƒ creÈ™tere a pierderilor. Acest proces este se repetÄƒ pÃ¢nÄƒ cÃ¢nd vocabularul atinge dimensiunea doritÄƒ.

ReÈ›ineÈ›i cÄƒ nu eliminÄƒm niciodatÄƒ caracterele de bazÄƒ, pentru a ne asigura cÄƒ orice cuvÃ¢nt poate fi tokenizat.

Acum, acest lucru este Ã®ncÄƒ puÈ›in vag: partea principalÄƒ a algoritmului este de a calcula o pierdere asupra corpusului È™i de a vedea cum se schimbÄƒ atunci cÃ¢nd eliminÄƒm unele tokenuri din vocabular, dar nu am explicat Ã®ncÄƒ cum sÄƒ facem acest lucru. Acest pas se bazeazÄƒ pe algoritmul de tokenizare al unui model Unigram, aÈ™a cÄƒ Ã®l vom analiza Ã®n continuare.

Vom reutiliza corpusul din exemplele anterioare:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

iar pentru acest exemplu, vom lua toate substringurile stricte pentru vocabularul iniÈ›ial:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Algoritm de tokenizare[[tokenization-algorithm]]

Un model Unigram este un tip de model lingvistic care considerÄƒ cÄƒ fiecare token este independent de tokenii anteriori. Este cel mai simplu model lingvistic, Ã®n sensul cÄƒ probabilitatea simbolului X avÃ¢nd Ã®n vedere contextul anterior este doar probabilitatea simbolului X. Astfel, dacÄƒ am utiliza un model lingvistic Unigram pentru a genera text, am prezice Ã®ntotdeauna simbolul cel mai frecvent.

Probabilitatea unui token dat este frecvenÈ›a sa (numÄƒrul de ori Ã®n care Ã®l gÄƒsim) Ã®n corpusul original, Ã®mpÄƒrÈ›itÄƒ la suma tuturor apariÈ›iilor tuturor tokenilor din vocabular (pentru a ne asigura cÄƒ probabilitÄƒÈ›ile sunt egale cu 1). De exemplu, `"ug"` este prezent Ã®n `"hug"`, `"pug"`, È™i `"hugs"`, deci are o frecvenÈ›Äƒ de 20 Ã®n corpusul nostru.

IatÄƒ frecvenÈ›ele tuturor subcuvintelor posibile din vocabular:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

Astfel, suma tuturor frecvenÈ›elor este 210, iar probabilitatea subcuvÃ¢ntului `"ug"` este 20/210.

> [!TIP]
> âœï¸ **Acum este rÃ¢ndul tÄƒu!** Scrie codul pentru a calcula frecvenÈ›ele de mai sus È™i verificÄƒ de douÄƒ ori dacÄƒ rezultatele afiÈ™ate sunt corecte, precum È™i suma totalÄƒ.

Acum, pentru a tokeniza un cuvÃ¢nt dat, ne uitÄƒm la toate segmentÄƒrile posibile Ã®n tokeni È™i calculÄƒm probabilitatea fiecÄƒruia Ã®n conformitate cu modelul Unigram. Deoarece toate token-urile sunt considerate independente, aceastÄƒ probabilitate este doar produsul probabilitÄƒÈ›ii fiecÄƒrui token. De exemplu, tokenizarea `["p", "u", "g"]` a lui `"pug"` are probabilitatea:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

Comparativ, tokenizarea `["pu", "g"]` are probabilitatea:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

astfel Ã®ncÃ¢t una este mult mai probabilÄƒ decÃ¢t alta. Ãn general, tokenizÄƒrile cu cei mai puÈ›ini tokeni posibili vor avea cea mai mare probabilitate (din cauza acelei Ã®mpÄƒrÈ›iri la 210 repetatÄƒ pentru fiecare token), ceea ce corespunde cu ceea ce dorim intuitiv: sÄƒ Ã®mpÄƒrÈ›im un cuvÃ¢nt Ã®n cel mai mic numÄƒr de tokenuri posibil.

Tokenizarea unui cuvÃ¢nt cu modelul Unigram este atunci tokenizarea cu cea mai mare probabilitate. Ãn exemplul `"pug"`, iatÄƒ probabilitÄƒÈ›ile pe care le-am obÈ›ine pentru fiecare segmentare posibilÄƒ:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

Astfel, `"pug"` ar fi tokenizat ca `["p", "ug"]` sau `["pu", "g"]`, Ã®n funcÈ›ie de care dintre aceste segmentÄƒri este Ã®ntÃ¢lnitÄƒ prima (reÈ›ineÈ›i cÄƒ Ã®ntr-un corpus mai mare, cazurile de egalitate ca acesta vor fi rare).

Ãn acest caz, a fost uÈ™or sÄƒ gÄƒsim toate segmentÄƒrile posibile È™i sÄƒ le calculÄƒm probabilitÄƒÈ›ile, dar Ã®n general va fi puÈ›in mai greu. ExistÄƒ un algoritm clasic utilizat pentru acest lucru, numit *algoritmul Viterbi*. Ãn esenÈ›Äƒ, putem construi un grafic pentru a detecta segmentÄƒrile posibile ale unui cuvÃ¢nt dat, spunÃ¢nd cÄƒ existÄƒ o ramurÄƒ de la caracterul _a_ la caracterul _b_ dacÄƒ subcuvÃ¢ntul de la _a_ la _b_ se aflÄƒ Ã®n vocabular, È™i atribuind ramurii respective probabilitatea subcuvÃ¢ntului.

Pentru a gÄƒsi calea din acest grafic care va avea cel mai bun scor, algoritmul Viterbi determinÄƒ, pentru fiecare poziÈ›ie din cuvÃ¢nt, segmentarea cu cel mai bun scor care se terminÄƒ la poziÈ›ia respectivÄƒ. Deoarece mergem de la Ã®nceput la sfÃ¢rÈ™it, cel mai bun scor poate fi gÄƒsit prin parcurgerea Ã®n buclÄƒ a tuturor subcuvintelor care se terminÄƒ la poziÈ›ia curentÄƒ È™i apoi folosind cel mai bun scor de tokenizare de la poziÈ›ia la care Ã®ncepe acest subcuvÃ¢nt. Apoi, trebuie doar sÄƒ derulÄƒm calea parcursÄƒ pentru a ajunge la sfÃ¢rÈ™it.

SÄƒ aruncÄƒm o privire la un exemplu folosind vocabularul nostru È™i cuvÃ¢ntul `"unhug"`. Pentru fiecare poziÈ›ie, subcuvintele cu cele mai bune scoruri care se terminÄƒ acolo sunt urmÄƒtoarele:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

Astfel, `"unhug"` ar fi tokenizat ca `["un", "hug"]`.

> [!TIP]
> âœï¸ **Acum e rÃ¢ndul tÄƒu!** DeterminaÈ›i tokenizarea cuvÃ¢ntului `"huggun"` È™i scorul acestuia.

## Ãnapoi la antrenare[[back-to-training]]

Acum cÄƒ am vÄƒzut cum funcÈ›ioneazÄƒ tokenizarea, putem analiza mai Ã®n profunzime pierderea utilizatÄƒ Ã®n timpul antrenÄƒrii. Ãn orice etapÄƒ datÄƒ, aceastÄƒ pierdere este calculatÄƒ prin tokenizarea fiecÄƒrui cuvÃ¢nt din corpus, utilizÃ¢nd vocabularul curent È™i modelul Unigram determinat de frecvenÈ›ele fiecÄƒrui token din corpus (dupÄƒ cum am vÄƒzut mai devreme).

Fiecare cuvÃ¢nt din corpus are un scor, iar pierderea este negative log likelihood a acestor scoruri - adicÄƒ suma pentru toate cuvintele din corpus a tuturor `-log(P(word))`.

SÄƒ ne Ã®ntoarcem la exemplul nostru cu urmÄƒtorul corpus:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

Tokenizarea fiecÄƒrui cuvÃ¢nt cu scorurile lor respective este:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

Deci, pierderea este:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Acum trebuie sÄƒ calculÄƒm modul Ã®n care eliminarea fiecÄƒrui token afecteazÄƒ pierderea. Acest lucru este destul de plictisitor, aÈ™a cÄƒ Ã®l vom face doar pentru doi tokeni aici È™i vom pÄƒstra Ã®ntregul proces pentru atunci cÃ¢nd vom avea cod care sÄƒ ne ajute. Ãn acest caz (foarte) special, aveam douÄƒ tokenizÄƒri echivalente ale tuturor cuvintelor: dupÄƒ cum am vÄƒzut mai devreme, de exemplu, `"pug"` ar putea fi tokenizat `["p", "ug"]` cu acelaÈ™i scor. Astfel, eliminarea simbolului `"pu"` din vocabular va produce exact aceeaÈ™i pierdere.

Pe de altÄƒ parte, eliminarea lui `"hug"` va agrava pierderea, deoarece tokenizarea lui `"hug"` È™i `"hugs"` va deveni:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Aceste modificÄƒri vor determina creÈ™terea pierderii cu:

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

Prin urmare, tokenul `"pu"` va fi probabil eliminat din vocabular, dar nu È™i `"hug"`.

## Implementarea Unigram[[implementarea-unigram]]

Acum sÄƒ implementÄƒm Ã®n cod tot ceea ce am vÄƒzut pÃ¢nÄƒ acum. Ca È™i Ã®n cazul BPE È™i WordPiece, aceasta nu este o implementare eficientÄƒ a algoritmului Unigram (dimpotrivÄƒ), dar ar trebui sÄƒ vÄƒ ajute sÄƒ-l Ã®nÈ›elegeÈ›i puÈ›in mai bine.

Vom folosi ca exemplu acelaÈ™i corpus ca È™i pÃ¢nÄƒ acum:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

De data aceasta, vom folosi `xlnet-base-cased` ca modelul nostru:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

Ca È™i pentru BPE È™i WordPiece, Ã®ncepem prin a numÄƒra numÄƒrul de apariÈ›ii ale fiecÄƒrui cuvÃ¢nt Ã®n corpus:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Apoi, trebuie sÄƒ iniÈ›ializÄƒm vocabularul nostru la ceva mai mare decÃ¢t dimensiunea vocabularului pe care o vom dori la final. Trebuie sÄƒ includem toate caracterele de bazÄƒ (altfel nu vom putea tokeniza fiecare cuvÃ¢nt), dar pentru substringurile mai mari le vom pÄƒstra doar pe cele mai comune, aÈ™a cÄƒ le vom sorta dupÄƒ frecvenÈ›Äƒ:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Loop through the subwords of length at least 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sortarea subcuvintelor dupÄƒ frecvenÈ›Äƒ
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

GrupÄƒm caracterele cu cele mai bune subcuvinte pentru a ajunge la un vocabular iniÈ›ial de dimensiunea 300:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

> [!TIP]
> ğŸ’¡ SentencePiece utilizeazÄƒ un algoritm mai eficient numit Enhanced Suffix Array (ESA) pentru a crea vocabularul iniÈ›ial.

Ãn continuare, calculÄƒm suma tuturor frecvenÈ›elor, pentru a converti frecvenÈ›ele Ã®n probabilitÄƒÈ›i. Pentru modelul nostru, vom stoca logaritmii probabilitÄƒÈ›ilor, deoarece este mai stabil din punct de vedere numeric sÄƒ adÄƒugÄƒm logaritmi decÃ¢t sÄƒ multiplicÄƒm numere mici, iar acest lucru va simplifica calcularea pierderii modelului:

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Acum funcÈ›ia principalÄƒ este cea care tokenizeazÄƒ cuvintele folosind algoritmul Viterbi. DupÄƒ cum am vÄƒzut mai devreme, acest algoritm calculeazÄƒ cea mai bunÄƒ segmentare a fiecÄƒrui substring din cuvÃ¢nt, pe care o vom stoca Ã®ntr-o variabilÄƒ numitÄƒ `best_segmentations`. Vom stoca un dicÈ›ionar pentru fiecare poziÈ›ie din cuvÃ¢nt (de la 0 la lungimea totalÄƒ a acestuia), cu douÄƒ chei: indicele de Ã®nceput al ultimului token din cea mai bunÄƒ segmentare È™i scorul celei mai bune segmentÄƒri. Cu ajutorul indicelui de Ã®nceput al ultimului token, vom putea extrage segmentarea completÄƒ odatÄƒ ce lista este complet populatÄƒ.

Popularea listei se face cu doar douÄƒ bucle: bucla principalÄƒ trece peste fiecare poziÈ›ie de Ã®nceput, iar a doua buclÄƒ Ã®ncearcÄƒ toate subcuvintele care Ã®ncep la acea poziÈ›ie de Ã®nceput. DacÄƒ substringul se aflÄƒ Ã®n vocabular, avem o nouÄƒ segmentare a cuvÃ¢ntului pÃ¢nÄƒ la acea poziÈ›ie finalÄƒ, pe care o comparÄƒm cu cea din `best_segmentations`.

OdatÄƒ ce bucla principalÄƒ este terminatÄƒ, pornim de la sfÃ¢rÈ™it È™i sÄƒrim de la o poziÈ›ie de Ã®nceput la alta, Ã®nregistrÃ¢nd tokenii pe parcurs, pÃ¢nÄƒ cÃ¢nd ajungem la Ã®nceputul cuvÃ¢ntului:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # This should be properly filled by the previous steps of the loop
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # If we have found a better segmentation ending at end_idx, we update
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # We did not find a tokenization of the word -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

Putem Ã®ncerca deja modelul nostru iniÈ›ial pe cÃ¢teva cuvinte:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Acum este uÈ™or de calculat pierderea modelului pe corpus!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Putem verifica dacÄƒ funcÈ›ioneazÄƒ pe modelul pe care Ã®l avem:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

Nici calcularea scorurilor pentru fiecare token nu este foarte dificilÄƒ; trebuie doar sÄƒ calculÄƒm pierderea pentru modelele obÈ›inute prin È™tergerea fiecÄƒrui tokeb:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # We always keep tokens of length 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

Ãl putem Ã®ncerca pe un token dat:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

Deoarece `"ll"` este folosit Ã®n tokenizarea lui `"Hopefully"`, iar eliminarea lui ne va face, probabil, sÄƒ folosim tokenul `"l"` de douÄƒ ori Ã®n schimb, ne aÈ™teptÄƒm sÄƒ aibÄƒ o pierdere pozitivÄƒ. `"his"` este folosit doar Ã®n interiorul cuvÃ¢ntului `"This"`, care este tokenizat ca el Ã®nsuÈ™i, deci ne aÈ™teptÄƒm sÄƒ aibÄƒ o pierdere zero. IatÄƒ rezultatele:

```python out
6.376412403623874
0.0
```

> [!TIP]
> ğŸ’¡ AceastÄƒ abordare este foarte ineficientÄƒ, astfel Ã®ncÃ¢t SentencePiece utilizeazÄƒ o aproximare a pierderii modelului fÄƒrÄƒ simbolul X: Ã®n loc sÄƒ Ã®nceapÄƒ de la zero, Ã®nlocuieÈ™te simbolul X cu segmentarea sa Ã®n vocabularul rÄƒmas. Ãn acest fel, toate scorurile pot fi calculate odatÄƒ, Ã®n acelaÈ™i timp cu pierderea modelului.

Cu toate acestea la locul lor, ultimul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ adÄƒugÄƒm la vocabular tokeni speciali utilizate de model, apoi sÄƒ facem o buclÄƒ pÃ¢nÄƒ cÃ¢nd am eliminat suficienÈ›i tokeni din vocabular pentru a ajunge la dimensiunea doritÄƒ:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Remove percent_to_remove tokens with the lowest scores.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Apoi, pentru a tokeniza un text, trebuie doar sÄƒ aplicÄƒm pre-tokenizarea È™i apoi sÄƒ folosim funcÈ›ia `encode_word()`:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

Asta e tot pentru Unigram! SperÄƒm cÄƒ pÃ¢nÄƒ acum vÄƒ simÈ›iÈ›i ca un expert Ã®n toate lucrurile legate de tokenizer. Ãn secÈ›iunea urmÄƒtoare, vom aprofunda elementele de bazÄƒ ale bibliotecii ğŸ¤— Tokenizers È™i vÄƒ vom arÄƒta cum le puteÈ›i utiliza pentru a vÄƒ construi propriul tokenizer.
