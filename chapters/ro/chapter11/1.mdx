# Fine-tuning supervizat

Ãn [Capitolul 2 SecÈ›iunea 2](/course/chapter2/2), am vÄƒzut cÄƒ modelele de limbaj generativ pot fi ajustate pentru sarcini specifice, cum ar fi rezumarea È™i rÄƒspunsul la Ã®ntrebÄƒri. Cu toate acestea, Ã®n zilele noastre este mult mai frecvent sÄƒ ajustÄƒm modelele de limbaj pe o gamÄƒ largÄƒ de sarcini simultan; o metodÄƒ cunoscutÄƒ sub numele de fine-tuning supervizat (SFT). Acest proces ajutÄƒ modelele sÄƒ devinÄƒ mai versatile È™i capabile sÄƒ gestioneze diverse cazuri de utilizare. Majoritatea LLM-urilor cu care oamenii interacÈ›ioneazÄƒ pe platforme precum ChatGPT au trecut prin SFT pentru a fi mai utile È™i aliniate cu preferinÈ›ele umane. Vom Ã®mpÄƒrÈ›i acest capitol Ã®n patru secÈ›iuni:

## 1ï¸âƒ£ Template-uri de chat

Template-urile de chat structureazÄƒ interacÈ›iunile dintre utilizatori È™i modelele AI, asigurÃ¢nd rÄƒspunsuri consecvente È™i adecvate contextual. Acestea includ componente precum prompturi de sistem È™i mesaje bazate pe roluri.

## 2ï¸âƒ£ Fine-tuning supervizat

Fine-tuningul supervizat (SFT) este un proces critic pentru adaptarea modelelor de limbaj pre-antrenate la sarcini specifice. Aceasta implicÄƒ antrenarea modelului pe un set de date specific sarcinii cu exemple etichetate. Pentru un ghid detaliat despre SFT, inclusiv paÈ™ii cheie È™i cele mai bune practici, consultaÈ›i [secÈ›iunea de fine-tuning supervizat din documentaÈ›ia TRL](https://huggingface.co/docs/trl/en/sft_trainer).

## 3ï¸âƒ£ Adaptarea de rang scÄƒzut (LoRA)

Adaptarea de rang scÄƒzut (LoRA) este o tehnicÄƒ pentru fine-tuningul modelelor de limbaj prin adÄƒugarea de matrice de rang scÄƒzut la straturile modelului. Aceasta permite un fine-tuning eficient pÄƒstrÃ¢nd Ã®n acelaÈ™i timp cunoÈ™tinÈ›ele pre-antrenate ale modelului. Unul dintre beneficiile cheie ale LoRA este economia semnificativÄƒ de memorie pe care o oferÄƒ, fÄƒcÃ¢nd posibilÄƒ ajustarea modelelor mari pe hardware cu resurse limitate.

## 4ï¸âƒ£ Evaluarea

Evaluarea este un pas crucial Ã®n procesul de fine-tuning. Ne permite sÄƒ mÄƒsurÄƒm performanÈ›a modelului pe un set de date specific sarcinii.

<Tip>
âš ï¸ Pentru a beneficia de toate funcÈ›ionalitÄƒÈ›ile disponibile cu Model Hub È™i ğŸ¤— Transformers, recomandÄƒm <a href="https://huggingface.co/join">crearea unui cont</a>.
</Tip>

## ReferinÈ›e

- [DocumentaÈ›ia Transformers despre template-urile de chat](https://huggingface.co/docs/transformers/main/en/chat_templating)
- [Script pentru fine-tuning supervizat Ã®n TRL](https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py)
- [`SFTTrainer` Ã®n TRL](https://huggingface.co/docs/trl/main/en/sft_trainer)
- [Lucrarea despre optimizarea directÄƒ a preferinÈ›elor](https://arxiv.org/abs/2305.18290)
- [Fine-tuning supervizat cu TRL](https://huggingface.co/docs/trl/sft_trainer)
- [Cum sÄƒ faceÈ›i fine-tuning la Google Gemma cu ChatML È™i Hugging Face TRL](https://github.com/huggingface/alignment-handbook)  
- [Fine-tuning LLM pentru a genera cataloage de produse persane Ã®n format JSON](https://huggingface.co/learn/cookbook/en/fine_tuning_llm_to_generate_persian_product_catalogs_in_json_format) 