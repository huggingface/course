<FrameworkSwitchCourse {fw} />

# Ajustarea unui model cu Keras[[ajustarea-unui-model-cu-keras]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
]} />

Dup캒 ce a탵i efectuat toate lucr캒rile de preprocesare a datelor din ultima sec탵iune, mai ave탵i doar c칙탵iva pa탳i de f캒cut pentru a antrena modelul. Re탵ine탵i, totu탳i, c캒 comanda `model.fit()` va rula foarte lent pe un CPU. Dac캒 nu ave탵i un GPU configurat, pute탵i ob탵ine acces gratuit la GPU sau TPU pe [Google Colab](https://colab.research.google.com/).

Exemplele de cod de mai jos presupun c캒 a탵i executat deja exemplele din sec탵iunea anterioar캒. Iat캒 un scurt rezumat care recapituleaz캒 ceea ce ave탵i nevoie:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### Antrenarea[[antrenarea]]

Modelele TensorFlow importate din 游뱅 Transformers sunt deja modele Keras. 칉n cele ce urmeaz캒 g캒si탵i o scurt캒 introducere 칥n Keras.

<Youtube id="rnTGBy2ax1c"/>

Aceasta 칥nseamn캒 c캒, odat캒 ce avem datele noastre, este nevoie de foarte pu탵in캒 munc캒 pentru a 칥ncepe antrenarea pe baza acestora.

<Youtube id="AUozVp78dhk"/>

Ca 탳i 칥n [capitolul anterior](/course/chapter2), vom folosi clasa `TFAutoModelForSequenceClassification`, cu dou캒 etichete: 

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Ve탵i observa c캒, spre deosebire de [Capitolul 2](/course/chapter2), ve탵i primi un avertisment dup캒 instan탵ierea acestui model preinstruit. Acest lucru se datoreaz캒 faptului c캒 BERT nu a fost preinstruit pentru clasificarea perechilor de propozi탵ii, astfel 칥nc칙t head-ul modelului preinstruit a fost eliminat 탳i a fost introdus 칥n schimb un nou head adecvat pentru clasificarea secven탵elor. Avertiz캒rile indic캒 faptul c캒 unele ponderi nu au fost utilizate (cele corespunz캒toare head-ului de preformare eliminat) 탳i c캒 altele au fost ini탵ializate aleatoriu (cele pentru noul head). 칉n 칥ncheiere, v캒 칥ncurajeaz캒 s캒 antrena탵i modelul, ceea ce vom face acum.

Pentru a realiza fine-tuning-ul modelului pe setul nostru de date, trebuie doar s캒 `compil캒m()` modelul nostru 탳i apoi s캒 transmitem datele noastre metodei `fit()`. Aceasta va 칥ncepe procesul de `fine-tuning` (care ar trebui s캒 dureze c칙teva minute pe un GPU) 탳i va raporta pierderea de formare pe parcurs, plus pierderea de validare la sf칙r탳itul fiec캒rei epoci.

> [!TIP]
> Re탵ine탵i c캒 modelele 游뱅 Transformers au o abilitate special캒 pe care majoritatea modelelor Keras nu o au - ele pot utiliza automat o valoare adecvat캒 a pierderii pe care o calculeaz캒 intern. Ele vor utiliza aceast캒 valoare 칥n mod implicit dac캒 nu seta탵i un argument de pierdere 칥n `compile()`. Re탵ine탵i c캒 pentru a utiliza  valoarea intern캒 a pierderii va trebui s캒 transmite탵i etichetele ca parte a datelor de intrare, nu ca etichet캒 separat캒, care este modul normal de utilizare a etichetelor cu modelele Keras. Ve탵i vedea exemple 칥n acest sens 칥n partea 2 a cursului, unde definirea func탵iei de pierdere corecte poate fi complicat캒. Cu toate acestea, pentru clasificarea secven탵elor, o func탵ie de pierdere Keras standard func탵ioneaz캒 bine, a탳a c캒 aceasta este cea pe care o vom utiliza aici.

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

> [!WARNING]
> Remarca탵i o problem캒 foarte des 칥nt칙lnit캒 aici - *pute탵i* doar s캒 transmite탵i numele pierderii ca 탳ir de caractere c캒tre Keras, dar 칥n mod implicit Keras va presupune c캒 a탵i aplicat deja un softmax rezultatelor dvs. Cu toate acestea, multe modele produc valorile chiar 칥nainte de aplicarea softmax, care sunt cunoscute 탳i sub numele de *logits*. Trebuie s캒 spunem func탵iei de pierdere c캒 asta face modelul nostru, iar singura modalitate de a face acest lucru este s캒 o apel캒m direct, mai degrab캒 dec칙t prin nume cu un 탳ir.


### 칉mbun캒t캒탵irea performan탵elor de instruire[[칉mbun캒t캒탵irea-performan탵elor-de-instruire]]

<Youtube id="cpzq6ESSM5c"/>

Dac캒 칥ncerca탵i codul de mai sus, acesta ruleaz캒 cu siguran탵캒, dar ve탵i constata c캒 valoarea pierderii scade doar lent sau sporadic. Cauza principal캒
este *rata de 칥nv캒탵are*. Ca 탳i 칥n cazul pierderii, atunci c칙nd 칥i transmitem lui Keras numele unui optimizator ca 탳ir de caractere, Keras ini탵ializeaz캒
optimizatorul respectiv cu valori implicite pentru to탵i parametrii, inclusiv rata de 칥nv캒탵are. Totu탳i, din experien탵a 칥ndelungat캒, 탳tim
c캒 modelele transformatoare beneficiaz캒 de o rat캒 de 칥nv캒탵are mult mai mic캒 dec칙t cea implicit캒 pentru Adam, care este 1e-3, scris캒 탳i
ca 10 la puterea -3, sau 0,001. 5e-5 (0,00005), care este de aproximativ dou캒zeci de ori mai mic캒, este un punct de plecare mult mai bun.

칉n plus fa탵캒 de sc캒derea ratei de 칥nv캒탵are, avem un al doilea as 칥n m칙nec캒: putem reduce 칥ncet rata de 칥nv캒탵are
pe parcursul instruirii. 칉n literatura de specialitate, ve탵i vedea uneori c캒 acest lucru este denumit *dec캒dere* sau *analizare*
a ratei de 칥nv캒탵are. 칉n Keras, cel mai bun mod de a face acest lucru este de a utiliza un *programator al ratei de 칥nv캒탵are*. Un program bun de utilizat este
`PolynomialDecay` - 칥n ciuda numelui, cu set캒rile implicite, acesta pur 탳i simplu scade liniar rata de 칥nv캒탵are de la valoarea ini탵ial캒
p칙n캒 la valoarea final캒 pe parcursul instruirii, ceea ce este exact ceea ce ne dorim. Pentru a utiliza corect un programator,
totu탳i, trebuie s캒 칥i spunem c칙t va dura antrenamentul. Calcul캒m acest lucru ca `num_train_steps` mai jos.

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# Num캒rul etapelor de formare este num캒rul de e탳antioane din setul de date, 칥mp캒r탵it la dimensiunea batch-ului, apoi 칥nmul탵it
# cu num캒rul total de epoci. Re탵ine탵i c캒 setul de date tf_train_dataset de aici este un set de date tf.data.Dataset 칥n batch-uri,
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

> [!TIP]
> Biblioteca Transformers 游뱅 are, de asemenea, o func탵ie `create_optimizer()` care va crea un optimizator `AdamW` cu rata de 칥nv캒탵are 칥n sc캒dere. Aceasta este o scurt캒tur캒 convenabil캒 pe care o ve탵i vedea 칥n detaliu 칥n sec탵iunile viitoare ale cursului.

Acum avem optimizatorul nostru complet nou 탳i putem 칥ncerca s캒 ne antren캒m cu el. 칉n primul r칙nd, s캒 re칥nc캒rc캒m modelul, pentru a reseta modific캒rile aduse ponderilor 칥n urma instruirii pe care tocmai am efectuat-o, iar apoi 칥l putem compila cu noul optimizator:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

Acum, ne potrivim din nou:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

> [!TIP]
> 游눠 Dac캒 dori탵i s캒 칥nc캒rca탵i automat modelul dvs. 칥n Hub 칥n timpul instruirii, pute탵i trece un `PushToHubCallback` 칥n metoda `model.fit()`. Vom afla mai multe despre acest lucru 칥n [Capitolul 4](/course/chapter4/3)

### Predic탵iile modelului[[predic탵iile-modelului]]

<Youtube id="nx10eh4CoOs"/>


Este foarte interesant s캒 ne antren캒m 탳i s캒 vedem cum scade valoarea pierderii, dar ce se 칥nt칙mpl캒 dac캒 dorim s캒 ob탵inem rezultate de la modelul antrenat, fie pentru a calcula anumi탵i parametri, fie pentru a utiliza modelul 칥n produc탵ie? Pentru a face acest lucru, putem folosi metoda `predict()`. Aceasta va returna *logit-urile* din rezultatele modelului, unul pentru fiecare clas캒.

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

Putem converti aceste logi탵i 칥n predic탵ii de clas캒 ale modelului folosind `argmax` pentru a g캒si cel mai mare logit, care corespunde celei mai probabile clase:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```
Acum, s캒 folosim `preds` pentru a calcula ni탳te indicatori! Putem 칥nc캒rca indicatorii de m캒surare asocia탵i cu setul de date MRPC la fel de u탳or cum am 칥nc캒rcat setul de date, de data aceasta cu func탵ia `evaluate.load()`. Obiectul returnat are o metod캒 `compute()` pe care o putem folosi pentru a face calculul metric:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Rezultatele exacte pe care le ob탵ine탵i pot varia, deoarece ini탵ializarea aleatorie a head-ului modelului poate schimba parametrii ob탵inu탵i. Aici, putem vedea c캒 modelul nostru are o precizie de 85,78% pe setul de validare 탳i un scor F1 de 89,97. Acestea sunt cele dou캒 metrici utilizate pentru a evalua rezultatele pe setul de date MRPC pentru criteriul de referin탵캒 GLUE. Tabelul din [lucrarea BERT] (https://arxiv.org/pdf/1810.04805.pdf) a raportat un scor F1 de 88,9 pentru modelul de baz캒. Acesta a fost modelul `uncased`, 칥n timp ce noi folosim 칥n prezent modelul `cased`, ceea ce explic캒 rezultatul mai bun.

Aceasta 칥ncheie introducerea la ajustarea fin캒 utiliz칙nd API-ul Keras. 칉n [Capitolul 7](/course/chapter7) va fi prezentat un exemplu de efectuare a acestui lucru pentru cele mai comune sarcini NLP. Dac캒 dori탵i s캒 v캒 perfec탵iona탵i abilit캒탵ile cu API-ul Keras, 칥ncerca탵i s캒 regla탵i un model pe setul de date GLUE SST-2, utiliz칙nd prelucrarea datelor pe care a탵i f캒cut-o 칥n sec탵iunea 2.
