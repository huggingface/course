<FrameworkSwitchCourse {fw} />

# Aplicarea fine-tuningului asupra unui model cu Keras[[fine-tuning-a-model-with-keras]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[ 
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
]} />

OdatÄƒ ce ai finalizat toate operaÈ›iile de preprocesare a datelor din ultima secÈ›iune, ai doar cÃ¢È›iva paÈ™i rÄƒmaÈ™i pentru a antrena modelul. ÃnsÄƒ, observaÈ›i cÄƒ `model.fit()` va rula foarte lent pe un CPU. DacÄƒ nu aveÈ›i un GPU configurat, puteÈ›i accesa GPU-uri sau TPUs pe gratis pe [Google Colab](https://colab.research.google.com/).

Exemplele de cod de mai jos presupun cÄƒ aÈ›i executat deja exemplele din secÈ›iunea precedentÄƒ. Aici este o scurtÄƒ sumarizare care recapituleazÄƒ ceea ce trebuie sÄƒ faceÈ›i:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```


### Antrenare[[antrenare]]


Modelele TensorFlow importate din ğŸ¤— Transformers sunt deja modele Keras. Aici este o scurtÄƒ introducere Ã®n Keras.

<Youtube id="rnTGBy2ax1c"/>


Astfel, odatÄƒ ce avem datele noastre, putem Ã®ncepe antrenarea lor foarte uÈ™or.


<Youtube id="AUozVp78dhk"/>


Ãn acelaÈ™i mod ca È™i Ã®n [capitolul anterior](/course/chapter2), vom folosi clasa `TFAutoModelForSequenceClassification`, cu douÄƒ labeluri:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

VeÈ›i observa cÄƒ Ã®n comparaÈ›ie cu [Capitolul 2](/course/chapter2), veÈ›i primi o avertizare dupÄƒ ce iniÈ›ializaÈ›i acest model preantrenat. Acest lucru se datoreazÄƒ faptului cÄƒ BERT nu a fost antrenat pentru clasificarea perechilor de propoziÈ›ii, astfel Ã®ncÃ¢t Ã®nceputul preantrenat al modelului a fost eliminat È™i un nou Ã®nceput potrivit pentru clasificarea secvenÈ›elor a fost adÄƒugat Ã®n locul lui. AvertizÄƒrile indicÄƒ faptul cÄƒ anumite weights nu au fost folosite (cele corespunzÄƒtoare Ã®nceputului de preantrenare eliminat) È™i cÄƒ altele au fost iniÈ›ializate aleatoriu (cele pentru noul capÄƒt). Avertizarea se Ã®ncheie prin Ã®ncurajarea antrenÄƒrii modelului, ceea ce este exact ceea ce vom face acum.


Pentru a face fine-tune modelului pe datasetul nostru, avem de fÄƒcut `compile()` modelului È™i apoi sÄƒ trasnmitem datele noastre spre metoda `fit()`. Acest lucru va porni procesul de fine-tuning (care ar trebui sÄƒ dureze cÃ¢teva minute pe un GPU) È™i va raporta training loss, plus validation loss la sfÃ¢rÈ™itul fiecÄƒrei epoci.

<Tip>

ObservaÈ›i cÄƒ modelele ğŸ¤— Transformers au o abilitate specialÄƒ care nu este disponibilÄƒ pentru cele mai multe modele Keras - ele pot folosi automat un appropriate loss pe care le calculeazÄƒ intern. Ele vor utiliza aceastÄƒ pierdere implicit dacÄƒ nu veÈ›i specifica un argument de pierdere Ã®n `compile()`. Pentru a folosi internal loss, va trebui sÄƒ transmiteÈ›i labelurile ca parte din input, È™i nu ca nu label separat, ceea ce este modul normal de utilizare a labelurilor cu modelele Keras. VeÈ›i vedea exemple de acest tip Ã®n Partea 2 a cursului, unde definirea funcÈ›iei de loss corecte poate fi dificilÄƒ. Ãn cazul sequence classification, Ã®nsÄƒ, o funcÈ›ie Keras standard pentru loss se va dovedi suficient, astfel o vom folosi Ã®n acest context.

</Tip>


```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```


<Tip warning={true}>

ObservaÈ›i un pericol comun aici â€” puteÈ›i transmite doar numele pierderii ca un string Ã®n Keras, dar implicit Keras va presupune cÄƒ deja aÈ›i aplicat un softmax la outputurile voastre. Multe modele, Ã®nsÄƒ, produc outputurile direct Ã®nainte de aplicarea softmaxului, cunoscute È™i sub numele de *logit*. Trebuie sÄƒ spuneÈ›i funcÈ›iei loss cÄƒ asta este ceea ce produce modelul tÄƒu, iar singura modalitate de a face acest lucru este de a-o chema direct, Ã®n loc de a o specifica numai cu un string.

</Tip>

### ÃmbunÄƒtÄƒÈ›irea performanÈ›ei de antrenare

<Youtube id="cpzq6ESSM5c"/>

DacÄƒ Ã®ncerci codul de mai sus, sigur o sÄƒ ruleze, dar vei constata cÄƒ lossul scade doar lent sau doar sporadic. Principala cauzÄƒ
este *learning rateul*. AÈ™a cum È™i lossul, cÃ¢nd trimitem cÄƒtre Keras numele unui omptimizer ca uin string, Keras iniÈ›iazÄƒ
acel optimizer cu valori default pentru toÈ›i parametrii, inclusiv learning rateul. Din experienÈ›Äƒ, Ã®nsÄƒ, È™tim cÄƒ
modelele transformer se bucurÄƒ de un learning rate mult mai mic decÃ¢t cel default pentru Adam, care este 1e-3, scris È™i ca 
10 la puterea -3 sau 0.001. 5e-5 (0.00005), care este aproximativ douÄƒzeci de ori mai micÄƒ, reprezintÄƒ o bazÄƒ mult mai bunÄƒ.

Ãn plus faÈ›Äƒ de reducerea learning rateului, avem un al doilea truc la dispoziÈ›ie: putem reduce treptat learning rateul
pe parcursul antrenÄƒrii. Ãn documentaÈ›ie, vei gÄƒsi uneori aceastÄƒ practicÄƒ denumitÄƒ *decaying* sau *annealing*
learning rateul. Ãn Keras, cel mai bun mod de a face acest lucru este sÄƒ folosim un *learning rate scheduler*. O opÈ›iune bunÄƒ este
`PolynomialDecay` â€” deÈ™i numele ar putea sugera altceva, cu setÄƒrile default, el doar face decay liniar learning rateului din valoarea iniÈ›ialÄƒ
pÃ¢nÄƒ la valoarea finalÄƒ pe parcursul antrenÄƒrii; exact ceea ce ne-am dori. Pentru a utiliza corect un scheduler,
Ã®nsÄƒ, trebuie sÄƒ-i spunem cÃ¢t timp va dura antrenarea. CalculÄƒm acest lucru sub forma `num_train_steps` de mai jos.

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# NumÄƒrul de paÈ™i de antrenare este numÄƒrul de sampleuri din dataset, Ã®mpÄƒrÈ›it la dimensiunea batchului È™i apoi Ã®nmulÈ›it
# cu numÄƒrul total de epoci. ObservÄƒm cÄƒ `tf_train_dataset` aici este un batched tf.data.Dataset`,
# nu datasetul original Hugging Face, astfel Ã®ncÃ¢t len() sÄƒ fie deja num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

Biblioteca ğŸ¤— Transformers are funcÈ›ia `create_optimizer()` care va crea un optimizer `AdamW` cu learning rate decay. Aceasta este o metodÄƒ convenabilÄƒ pe care o veÈ›i vedea Ã®n detaliu Ã®n viitoarele sec'iuni ale cursului.

</Tip>

Acum, avem noul nostru optimizer È™i putem Ã®ncerca sÄƒ antrenÄƒm cu el. Ãncepem cu reloadul modelului pentru a reseta modificÄƒrile pe weighturole de la ultima rundÄƒ de antrenare ;o apoi putem face compile cu noul optimizer:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

Acum putem face fit din nou:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

ğŸ’¡ DacÄƒ doriÈ›i sÄƒ Ã®ncÄƒrcaÈ›i automat modelul dumneavoastrÄƒ pe Hub Ã®n timpul antrenÄƒrii, puteÈ›i transmite `PushToHubCallback` Ã®n metoda `model.fit()`. Mai multe despre acest lucru veÈ›i Ã®nvÄƒÈ›a Ã®n [Capitol 4](/course/chapter4/3).

</Tip>

### Model prediction

<Youtube id="nx10eh4CoOs"/>


Ãncheierea antrenÄƒrii È™i observarea lossului scÄƒzÃ¢nd este foarte frumos, dar ce se Ã®ntÄƒmplÄƒ dacÄƒ doriÈ›i sÄƒ obÈ›ineÈ›i efectiv outputuruke din modelul antrenat, fie pentru a calcula cÃ¢teva metrice, fie pentru utilizarea modelului Ã®n producÈ›ie? Pentru asta putem folosi metoda `predict()`. Aceasta va returna *logits* de la Ã®nceputul outputului modelului, cÃ¢te unul pe clasÄƒ.

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

PuteÈ›i converti aceste logituri Ã®n predicÈ›iile clasei modelului prin utilizarea `argmax` pentru a gÄƒsi logit-ul cel mai mare, care corespunde clasei celei mai probabile:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

Acum, sÄƒ folosim aceste `preds` pentru a calcula cÃ¢teva metrici! PuteÈ›i Ã®ncÄƒrca metricele asociate cu datasetul MRPC Ã®n acelaÈ™i mod cum am Ã®ncÄƒrcat È™i datasetul, dar de data aceasta cu ajutorul funcÈ›iei `evaluate.load()`. Obiectul returnat are o metodÄƒ de calculare `compute()` pe care o putem utiliza pentru a face metric calculation:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Rezultatele exacte pe care le veÈ›i obÈ›ine pot varia, deoarece iniÈ›ializarea aleatoare a Ã®nceputului modelului care poate schimba metricele pe care le-a atins. Aici putem vedea cÄƒ modelul nostru are o precizie de 85.78% Ã®n setul de validare È™i un scor F1 de 89.97. Acestea sunt exact aceleaÈ™i metrice folosite pentru evaluarea rezultatelor pe datasetul MRPC pentru GLUE benchmark. Tabelul din [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf) a raportat un scor F1 de 88.9 pentru modelul de bazÄƒ. Acela a fost un `uncased` model, dar noi utilizÄƒm acum un `cased` model, ceea ce explicÄƒ rezultatul mai bun.

Acesta este punctul unde vÄƒ prezentÄƒm introducerea la folosirea fine-tuningului cu ajutorul APIului Keras. Un exemplu bun pentru majoritatea sarcinilor NLP va fi dat Ã®n [Capitol 7](/course/chapter7). DacÄƒ doriÈ›i sÄƒ dezvoltaÈ›i abilitÄƒÈ›ile dumneavoastrÄƒ pe API-ul Keras, Ã®ncercaÈ›i sÄƒ faceÈ›i fine-tune unui model pe datasetul GLUE SST-2 folosind procesarea de date din secvenÈ›a 2.