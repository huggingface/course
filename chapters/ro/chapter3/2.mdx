<FrameworkSwitchCourse {fw} />

# Prelucrarea datelor[[prelucrarea-datelor]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
Continu칙nd cu exemplul din [capitolul anterior](/course/chapter2), iat캒 cum am antrena un clasificator de secven탵e pe un batch 칥n PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# La fel ca 칥nainte
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Ceva nou
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Continu칙nd cu exemplul din [capitolul anterior](/course/chapter2), iat캒 cum am antrena un clasificator de secven탵e pe un batch 칥n TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# La fel ca 칥nainte
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Ceva nou
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

Desigur, doar antrenarea modelului pe dou캒 propozi탵ii nu va da rezultate foarte bune. Pentru a ob탵ine rezultate mai bune, va trebui s캒 preg캒ti탵i un set de date mai mare.

칉n aceast캒 sec탵iune vom folosi ca exemplu setul de date MRPC (Microsoft Research Paraphrase Corpus), introdus 칥ntr-o [lucrare](https://www.aclweb.org/anthology/I05-5002.pdf) de William B. Dolan 탳i Chris Brockett. Setul de date este format din 5 801 perechi de propozi탵ii, cu o etichet캒 care indic캒 dac캒 acestea sunt parafraz캒ri sau nu (adic캒, dac캒 ambele propozi탵ii 칥nseamn캒 acela탳i lucru). L-am selectat pentru acest capitol deoarece este un set de date mic, astfel 칥nc칙t este u탳or de experimentat cu formarea pe acesta.

### 칉nc캒rcarea unui set de date din Hub[[칥nc캒rcarea-unui-set-de-date-din-Hub]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub-ul nu con탵ine doar modele, ci 탳i multe seturi de date 칥n limbi diferite. Pute탵i naviga printre seturile de date [aici](https://huggingface.co/datasets) 탳i v캒 recomand캒m s캒 칥ncerca탵i s캒 칥nc캒rca탵i 탳i s캒 procesa탵i un nou set de date dup캒 ce a탵i parcurs aceast캒 sec탵iune (consulta탵i documenta탵ia general캒 [aici](https://huggingface.co/docs/datasets/loading)). Dar, pentru moment, s캒 ne concentr캒m asupra setului de date MRPC! Acesta este unul dintre cele 10 seturi de date care compun [GLUE benchmark](https://gluebenchmark.com/), care este un benchmark academic utilizat pentru a m캒sura performan탵a modelelor ML 칥n 10 sarcini diferite de clasificare a textului.

Biblioteca 游뱅 Datasets ofer캒 o comand캒 foarte simpl캒 pentru a desc캒rca 탳i stoca 칥n cache un set de date pe Hub. Putem desc캒rca setul de date MRPC astfel:

> [!TIP]
> 丘멆잺 **Aten탵ie** Asigura탵i-v캒 c캒 `datasets` este instalat prin rularea `pip install datasets`. Apoi, 칥nc캒rca탵i setul de date MRPC 탳i tip캒ri탵i-l pentru a vedea ce con탵ine. 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Dup캒 cum pute탵i vedea, ob탵inem un obiect `DatasetDict` care con탵ine setul de instruire, setul de validare 탳i setul de testare. Fiecare dintre acestea con탵ine mai multe coloane (`sentence1`, `sentence2`, `label` 탳i `idx`) 탳i un num캒r de r칙nduri variabil, care reprezint캒 num캒rul de elemente din fiecare set (astfel, exist캒 3.668 de perechi de propozi탵ii 칥n setul de instruire, 408 칥n setul de validare 탳i 1.725 칥n setul de testare).

Aceast캒 comand캒 descarc캒 탳i pune 칥n cache setul de date, implicit 칥n *~/.cache/huggingface/datasets*. Reamintim din capitolul 2 c캒 pute탵i personaliza folderul cache prin setarea variabilei de mediu `HF_HOME`.

Putem accesa fiecare pereche de propozi탵ii din obiectul nostru `raw_datasets` prin indexare, ca 칥ntr-un dic탵ionar:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

Putem vedea c캒 etichetele sunt deja numere 칥ntregi, deci nu va trebui s캒 efectu캒m nicio prelucrare prealabil캒. Pentru a 탳ti ce num캒r 칥ntreg corespunde fiec캒rei etichete, putem inspecta `features` din `raw_train_dataset`. Acest lucru ne va indica tipul fiec캒rei coloane:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

칉n culise, `label` este de tipul `ClassLabel`, iar maparea numerelor 칥ntregi 탳i numele etichetei este stocat캒 칥n folderul *names*. `0` corespunde la `not_equivalent`, iar `1` corespunde la `equivalent`.

> [!TIP]
> 九勇 **칉ncerca탵i!** Uita탵i-v캒 la elementul 15 din setul de antrenament 탳i la elementul 87 din setul de validare. Care sunt etichetele lor?

### Preprocesarea unui set de date[[preprocesarea-unui-set-de-date]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Pentru a preprocesa setul de date, trebuie s캒 convertim textul 칥n numere pe care modelul le 칥n탵elege. Dup캒 cum a탵i v캒zut 칥n [capitolul anterior](/course/chapter2), acest lucru se face cu ajutorul unui tokenizer. Putem furniza tokenizatorului o propozi탵ie sau o list캒 de propozi탵ii, astfel 칥nc칙t putem tokeniza direct primele propozi탵ii 탳i toate propozi탵iile secundare din fiecare pereche, astfel:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Cu toate acestea, nu putem pur 탳i simplu s캒 transmitem dou캒 secven탵e modelului 탳i s캒 ob탵inem o predic탵ie care s캒 indice dac캒 cele dou캒 propozi탵ii sunt parafraze sau nu. Trebuie s캒 trat캒m cele dou캒 secven탵e ca pe o pereche 탳i s캒 aplic캒m preprocesarea corespunz캒toare. Din fericire, tokenizatorul poate, de asemenea, s캒 ia o pereche de secven탵e 탳i s캒 le preg캒teasc캒 칥n modul 칥n care se a탳teapt캒 modelul nostru BERT: 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Am discutat despre cheile `input_ids` 탳i `attention_mask` 칥n [Capitolul 2](/course/chapter2), dar am am칙nat discu탵ia despre `token_type_ids`. 칉n acest exemplu, aceasta este ceea ce 칥i spune modelului care parte a intr캒rii este prima propozi탵ie 탳i care este a doua propozi탵ie.

> [!TIP]
> 九勇 **칉ncerca탵i!** Lua탵i elementul 15 din setul de antrenament 탳i tokeniza탵i cele dou캒 propozi탵ii separat apoi ca pe o pereche. Care este diferen탵a dintre cele dou캒 rezultate?

Dac캒 decodific캒m ID-urile din `input_ids` 칥napoi 칥n cuvinte:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

Vom ob탵ine:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

Astfel, modelul se a탳teapt캒 ca intr캒rile s캒 fie de forma `[CLS] sentence1 [SEP] sentence2 [SEP]` atunci c칙nd exist캒 dou캒 propozi탵ii. Alinierea acestui lucru cu `token_type_ids` ne d캒:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Dup캒 cum pute탵i vedea, p캒r탵ile de intrare corespunz캒toare `[CLS] sentence1 [SEP]` au toate un ID de tip token de `0`, 칥n timp ce celelalte p캒r탵i, corespunz캒toare `sentence2 [SEP]`, au toate un ID de tip token de `1`.

Re탵ine탵i c캒, dac캒 selecta탵i un checkpoint diferit, nu ve탵i avea neap캒rat `token_type_ids` 칥n intr캒rile dvs. tokenizate (de exemplu, acestea nu sunt returnate dac캒 utiliza탵i un model DistilBERT). Acestea sunt returnate numai atunci c칙nd modelul va 탳ti ce s캒 fac캒 cu ele, deoarece le-a v캒zut 칥n timpul preinstruirii sale. 

Aici, BERT este preinstruit cu ID-uri de tip token 탳i, pe l칙ng캒 obiectivul de modelare a limbajului mascat despre care am vorbit 칥n [[Chapter 1]](/course/chapter1), are un obiectiv suplimentar numit _next sentence prediction_. Obiectivul acestei sarcini este de a modela rela탵ia dintre perechile de propozi탵ii.

칉n cazul predic탵iei propozi탵iei urm캒toare, modelul prime탳te perechi de propozi탵ii (cu token-uri mascate aleatoriu) 탳i i se cere s캒 prezic캒 dac캒 a doua propozi탵ie o urmeaz캒 pe prima. Pentru ca sarcina s캒 nu fie complicat캒, jum캒tate din timp propozi탵iile se succed reciproc 칥n documentul original din care au fost extrase, iar cealalt캒 jum캒tate din timp cele dou캒 propozi탵ii provin din dou캒 documente diferite. 

칉n general, nu trebuie s캒 v캒 face탵i griji dac캒 exist캒 sau nu `token_type_ids` 칥n intr캒rile dvs. tokenizate: at칙ta timp c칙t utiliza탵i acela탳i checkpoint pentru tokenizator 탳i model, totul va fi bine, deoarece tokenizatorul 탳tie ce s캒 furnizeze modelului s캒u.

Acum c캒 am v캒zut cum tokenizatorul nostru poate trata o pereche de propozi탵ii, 칥l putem folosi pentru a tokeniza 칥ntregul nostru set de date: la fel ca 칥n [previous chapter](/course/chapter2), putem furniza tokenizatorului o list캒 de perechi de propozi탵ii oferindu-i lista primelor propozi탵ii, apoi lista celor de-a doua propozi탵ii. Acest lucru este, de asemenea, compatibil cu op탵iunile de padding 탳i trunchiere pe care le-am v캒zut 칥n [Chapter 2](/course/chapter2). A탳adar, o modalitate de preprocesare a setului de date de instruire este:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Aceast캒 metod캒 func탵ioneaz캒 corespunz캒tor, dar are dezavantajul de a returna un dic탵ionar (cu cheile noastre, `input_ids`, `attention_mask` 탳i `token_type_ids`, 탳i valori care sunt liste ale listelor). De asemenea, va func탵iona numai dac캒 ave탵i suficient캒 memorie RAM pentru a stoca 칥ntregul set de date 칥n timpul tokeniz캒rii (칥n timp ce seturile de date din biblioteca 游뱅 Datasets sunt fi탳iere [Apache Arrow](https://arrow.apache.org/) stocate pe disc, deci p캒stra탵i 칥nc캒rcate 칥n memorie numai e탳antioanele pe care le solicita탵i).

Pentru a p캒stra informa탵iile sub forma unui set de date, vom utiliza metoda [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). Acest lucru ne permite, de asemenea, o flexibilitate sporit캒, 칥n cazul 칥n care avem nevoie de mai multe preproces캒ri dec칙t simpla tokenizare. Metoda `map()` func탵ioneaz캒 prin aplicarea unei func탵ii pe fiecare element al setului de date, deci s캒 definim o func탵ie care s캒 tokenizeze intr캒rile noastre:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Aceast캒 func탵ie accept캒 un dic탵ionar (precum elementele din setul nostru de date) 탳i returneaz캒 un nou dic탵ionar cu cheile `input_ids`, `attention_mask` 탳i `token_type_ids`. Re탵ine탵i c캒 func탵ioneaz캒 탳i 칥n cazul 칥n care dic탵ionarul `example` con탵ine mai multe e탳antioane (fiecare cheie fiind o list캒 de propozi탵ii), deoarece `tokenizer` func탵ioneaz캒 pe liste de perechi de propozi탵ii, a탳a cum am v캒zut anterior. Acest lucru ne va permite s캒 folosim op탵iunea `batched=True` 칥n apelul nostru la `map`, ceea ce va accelera foarte mult tokenizarea. `tokenizer` este sus탵inut de un tokenizer scris 칥n Rust din biblioteca [游뱅 Tokenizers](https://github.com/huggingface/tokenizers). Acest tokenizator poate fi foarte rapid, dar numai dac캒 칥i oferim o mul탵ime de intr캒ri deodat캒.

Re탵ine탵i c캒 am omis deocamdat캒 argumentul `padding` 칥n func탵ia noastr캒 de tokenizare. Acest lucru se datoreaz캒 faptului c캒 umplerea tuturor e탳antioanelor la lungimea maxim캒 nu este eficient캒: este mai bine s캒 umplem e탳antioanele atunci c칙nd construim un batch, deoarece atunci trebuie s캒 umplem doar la lungimea maxim캒 din acel batch, 탳i nu la lungimea maxim캒 din 칥ntregul set de date. Acest lucru poate economisi mult timp 탳i putere de procesare atunci c칙nd intr캒rile au lungimi variate! 

Iat캒 cum aplic캒m func탵ia de tokenizare la toate seturile noastre de date simultan. Utiliz캒m `batched=True` 칥n apelul c캒tre `map`, astfel 칥nc칙t func탵ia s캒 fie aplicat캒 la mai multe elemente ale setului nostru de date simultan, 탳i nu la fiecare element 칥n parte. Acest lucru permite o preprocesare mai rapid캒.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

Modul 칥n care biblioteca 游뱅 Datasets aplic캒 aceast캒 procesare este prin ad캒ugarea de noi c칙mpuri la seturile de date, c칙te unul pentru fiecare cheie din dic탵ionarul returnat de func탵ia de preprocesare:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Pute탵i utiliza chiar 탳i multiprocesarea atunci c칙nd aplica탵i func탵ia de preprocesare cu `map()` prin transmiterea unui argument `num_proc`. Nu am f캒cut acest lucru aici deoarece biblioteca 游뱅 Tokenizers utilizeaz캒 deja mai multe fire pentru a tokeniza mai rapid e탳antioanele noastre, dar dac캒 nu utiliza탵i un tokenizator rapid sus탵inut de aceast캒 bibliotec캒, acest lucru v-ar putea accelera preprocesarea.

Func탵ia noastr캒 `tokenize_function` returneaz캒 un dic탵ionar cu cheile `input_ids`, `attention_mask` 탳i `token_type_ids`, astfel 칥nc칙t aceste trei c칙mpuri sunt ad캒ugate la toate diviziunile setului nostru de date. Re탵ine탵i c캒 am fi putut, de asemenea, s캒 modific캒m c칙mpurile existente dac캒 func탵ia noastr캒 de preprocesare a returnat o nou캒 valoare pentru o cheie existent캒 칥n setul de date c캒ruia i-am aplicat func탵ia `map()`.

Ultimul lucru pe care va trebui s캒 칥l facem este s캒 umplem toate exemplele la lungimea celui mai lung element atunci c칙nd grup캒m elementele 칥mpreun캒 - o tehnic캒 la care ne referim ca *umplere dinamic캒*.

### Umplere dinamic캒[[umplere-dinamic캒]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
Func탵ia care este responsabil캒 de combinarea e탳antioanelor 칥n cadrul unui batch se nume탳te *func탵ie de cola탵ionare*. Este un argument pe care 칥l pute탵i trece atunci c칙nd construi탵i un `DataLoader`, implicit fiind o func탵ie care va converti e탳antioanele 칥n tensori PyTorch 탳i le va concatena (recursiv dac캒 elementele dvs. sunt liste, tupluri sau dic탵ionare). 칉n cazul nostru, va fi de asemenea s캒 aplica탵i umplutur캒 pentru a avea toate intr캒rile de aceea탳i lungime. Clasa `DataCollatorWithPadding` face exact acest lucru (탳i un pic mai mult, dup캒 cum am v캒zut anterior). Ia un tokenizer atunci c칙nd este instan탵iat (pentru a 탳tii ce token de completare s캒 foloseasc캒 탳i dac캒 modelul se a탳teapt캒 la padding 칥n st칙nga sau 칥n dreapta) 탳i va face tot ce ave탵i nevoie. Umplerea dinamic캒 poate fi aplicat캒 칥n func탵ie de necesit캒탵i la fiecare batch 탳i pentru a evita s캒 avem intr캒ri prea lungi cu o mul탵ime de umpluturi. Acest lucru va accelera antrenamentul destul de mult, dar re탵ine탵i c캒, dac캒 v캒 antrena탵i pe o TPU, acest lucru poate cauza probleme - TPU-urile prefer캒 forme fixe, chiar 탳i atunci c칙nd ar putea necesita padding suplimentar.

{:else}

Func탵ia care este responsabil캒 de reunirea e탳antioanelor 칥n cadrul unui batch se nume탳te *func탵ie de cola탵ionare*. Func탵ia de cola탵ionare implicit캒 este o func탵ie care va converti e탳antioanele 칥n tf.Tensor 탳i le va concatena (recursiv dac캒 elementele dvs. sunt liste, tuples sau dic탵ionare). 칉n cazul nostru, va fi de asemenea s캒 aplica탵i umplutur캒 pentru a avea toate intr캒rile de aceea탳i lungime. Clasa `DataCollatorWithPadding` face exact acest lucru (탳i un pic mai mult, dup캒 cum am v캒zut anterior). Ia un tokenizer atunci c칙nd este instan탵iat (pentru a 탳tii ce token de completare s캒 foloseasc캒 탳i dac캒 modelul se a탳teapt캒 la padding 칥n st칙nga sau 칥n dreapta) 탳i va face tot ce ave탵i nevoie. Umplerea dinamic캒 poate fi aplicat캒 칥n func탵ie de necesit캒탵i la fiecare batch 탳i pentru a evita s캒 avem intr캒ri prea lungi cu o mul탵ime de umpluturi. Acest lucru va accelera antrenamentul destul de mult, dar re탵ine탵i c캒, dac캒 v캒 antrena탵i pe o TPU, acest lucru poate cauza probleme - TPU-urile prefer캒 forme fixe, chiar 탳i atunci c칙nd ar putea necesita padding suplimentar.

{/if}

Pentru a face acest lucru 칥n practic캒, trebuie s캒 definim o func탵ie de cola탵ionare care va aplica cantitatea corect캒 de umplutur캒 elementelor din setul de date pe care dorim s캒 le grup캒m. Din fericire, biblioteca 游뱅 Transformers ne ofer캒 o astfel de func탵ie prin `DataCollatorWithPadding`. Aceasta preia un tokenizer atunci c칙nd o instan탵ia탵i (pentru a 탳tii ce token de umplutur캒 s캒 utiliza탵i 탳i dac캒 modelul se a탳teapt캒 ca umplutura s캒 fie la st칙nga sau la dreapta intr캒rilor) 탳i va face tot ceea ce ave탵i nevoie:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Pentru a testa aceast캒 nou캒 op탵iune, s캒 lu캒m c칙teva e탳antioane din setul nostru de formare pe care dorim s캒 le grup캒m. Aici, elimin캒m coloanele `idx`, `sentence1` 탳i `sentence2` deoarece nu vor fi necesare 탳i con탵in 탳iruri de caractere (탳i nu putem crea tensori cu 탳iruri de caractere) 탳i arunc캒m o privire la lungimile fiec캒rei intr캒ri din batch:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Nici o surpriz캒, ob탵inem e탳antioane de diferite lungimi, de la 32 la 67. Umplerea dinamic캒 칥nseamn캒 c캒 toate e탳antioanele din acest batch ar trebui s캒 fie umplute la o lungime de 67, lungimea maxim캒 din cadrul batch-ului. F캒r캒 umplutur캒 dinamic캒, toate e탳antioanele ar trebui s캒 fie umplute la lungimea maxim캒 din 칥ntregul set de date sau la lungimea maxim캒 pe care modelul o poate accepta. S캒 verific캒m de dou캒 ori dac캒 `data_collator` completeaz캒 dinamic batch-ul 칥n mod corespunz캒tor:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Perfect! Acum c캒 am trecut de la text brut la batch-uri cu care modelul nostru se poate descurca, suntem gata s캒 칥l ajust캒m!

{/if}

> [!TIP]
> 九勇 **칉ncearca탵i!** Replica탵i preprocesarea pe setul de date GLUE SST-2. Acesta este pu탵in diferit, deoarece este compus din propozi탵ii simple 칥n loc de perechi, dar restul lucrurilor pe care le-am f캒cut ar trebui s캒 fie la fel. Pentru o provocare mai dificil캒, 칥ncerca탵i s캒 scrie탵i o func탵ie de preprocesare care s캒 func탵ioneze pe oricare dintre sarcinile GLUE.

{#if fw === 'tf'}

Acum c캒 avem setul nostru de date 탳i un compilator de date, trebuie s캒 le punem 칥mpreun캒. Am putea 칥nc캒rca manual batch-uri 탳i s캒 le asambl캒m, dar este mult de lucru 탳i probabil nici nu este foarte eficient. 칉n schimb, exist캒 o metod캒 simpl캒 care ofer캒 o solu탵ie performant캒 la aceast캒 problem캒: `to_tf_dataset()`. Aceasta va 칥mpacheta un `tf.data.Dataset` cu func탵ia de cola탵ionare 칥ncorporat캒. Dac캒 utiliza탵i `DataCollatorWithPadding`, acest lucru nu va fi o problem캒 deoarece ea func탵ioneaz캒 cu dic탵ionare, liste 탳i tensori NumPy, precum 탳i cu tensori TensorFlow!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

탲i asta este tot! Putem folosi aceste seturi de date 칥n cursul urm캒tor, unde instruirea va fi extrem de simpl캒 dup캒 toat캒 munca grea de preprocesare a datelor.

{/if}
