`<FrameworkSwitchCourse {fw} />`

# Procesarea datelor [[processing-the-data]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
    classNames="absolute z-10 right-0 top-0"
    notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
        {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />
{:else}

<CourseFloatingBanner chapter={3}

classNames="absolute z-10 right-0 top-0"
    notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
        {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
ContinuÃ¢nd cu exemplul din [capitol anterior](/course/chapter2), mai jos vedeÈ›i cum putem sÄƒ antrenÄƒm un sequence classifier pe un singur batch Ã®n PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequences = [
    "Am aÈ™teptat toatÄƒ viaÈ›a mea pentru un curs HuggingFace.",
    "Acest curs este fascinant!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Acesta este nou
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
ContinuÃ¢nd cu exemplul din [capitolul anterior](/course/chapter2), mai jos vedeÈ›i cum continuÄƒm antrenarea unui sequence classifier pe un batch Ã®n TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# AcelaÈ™i lucru ca dinainte
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "Am aÈ™teptat toatÄƒ viaÈ›a mea pentru un curs HuggingFace.",
    "Acest curs este fascinant!",
]
batch = dict(tokenizer(sequence, padding=True, truncate=True, return_tensors="tf"))


# Aceasta este nou
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
etichete = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, etichete)
```

De obicei, antrenarea modelului pe douÄƒ propoziÈ›ii nu va da rezultate foarte bune. Pentru a obÈ›ine rezultate mai bune, veÈ›i avea nevoie sÄƒ pregÄƒtiÈ›i un dataset mai mare.

Ãn aceastÄƒ secÈ›iune vom folosi ca exemplu datasetul MRPC (Microsoft Research Paraphrase Corpus), introdus Ã®ntr-un [articol](https://www.aclweb.org/anthology/I05-5002.pdf) de William B. Dolan È™i Chris Brockett. Datasetul constÄƒ din 5,801 perechi de propoziÈ›ii, cu o etichetÄƒ care indicÄƒ dacÄƒ sunt paraphraseuri sau nu (adicÄƒ dacÄƒ ambele propoziÈ›ii au acelaÈ™i Ã®nÈ›eles). Am selectat-o pentru acest capitoldeoare cÄƒ este un dataset mic, aÈ™a Ã®ncÃ¢t sÄƒ fie uÈ™or de experimentat cu antrenarea pe el.

### ÃncÄƒrcarea unui dataset din Hub[[loading-a-dataset-from-the-hub]]

{#if fw === 'ro'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub-ul nu conÈ›ine doar modele; de asemenea, conÈ›ine multiple dataseturi Ã®nm mai multe limbi. PuteÈ›i naviga prin seturile de date [aici](https://huggingface.co/datasets), È™i vÄƒ recomandÄƒm sÄƒ Ã®ncercaÈ›i sÄƒ Ã®ncÄƒrcaÈ›i È™i procuraÈ›i un nou dataset odatÄƒ ce veÈ›i fi trecut prin aceastÄƒ secÈ›iune (vedeÈ›i documentaÈ›ia generalÄƒ [aici](https://huggingface.co/docs/datasets/loading)). Dar pentru moment, sÄƒ ne concentrÄƒm pe datasetul MRPC! Este una dintre cele 10 seturi de date care compun benchmark-ul [GLUE](https://gluebenchmark.com/), care este un benchmark academic folosit pentru a mÄƒsura performanÈ›a modelelor ML Ã®n 10 diferite sarcini de clasificare a textului.

Biblioteca ğŸ¤— Datasets oferÄƒ o comandÄƒ foarte simplÄƒ pentru a descÄƒrca È™i a stoca un dataset din Hub. Putem sÄƒ instalÄƒm datasetul MRPC astfel:

<Tip>
âš ï¸ **Avertizare** AsiguraÈ›i-vÄƒ cÄƒ `datasets` este instalat prin rularea `pip install datasets`. Apoi, Ã®ncÄƒrcaÈ›i dataset-ul MRPC È™i printaÈ›i-l pentru a vedea ce conÈ›ine.
</Tip>

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Ai observat cÄƒ am obÈ›inut un obiect `DatasetDict` care conÈ›ine setul de antrenare, setul de validare È™i setul de testare. Fiecare dintre acestea conÈ›ine cÃ¢teva coloane (`sentence1`, `sentence2`, `label`, È™i `idx`) È™i o numpÄƒr de rÃ¢nduri variabil, care reprezintÄƒ numÄƒrul de elemente Ã®n fiecare set (astfel, existÄƒ 3.668 perechi de fraze Ã®n setul de antrenare, 408 Ã®n setul de validare È™i 1.725 Ã®n setul de testare).

Acest comandÄƒ descarcÄƒ È™i face cache datasetului, Ã®n *~/.cache/huggingface/datasets*(folderul implicit de salvare). Ãn capitolul 2, am vÄƒzut cum putem personaliza folderul de cache stabilind variabila de mediu `HF_HOME`.

Putem accesa fiecare pereche de fraze din obiectul `raw_datasets` prin indexare, asemenea unui dicÈ›ionar:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi l-a acuzat pe fratele sÄƒu, pe care l-a numit â€martorulâ€ , cÄƒ distorsioneazÄƒ intenÈ›ionat probele sale.',
 'sentence2': 'Referindu-se la el doar sub numele de " martorul ", Amrozi l-a acuzat pe fratele sÄƒu cÄƒ distorsioneazÄƒ intenÈ›ionat probele sale.'}
```

Putem vedea cÄƒ labels sunt numere Ã®ntregi, aÈ™adar nu vom trebui sÄƒ facem nicio procesare Ã®n acest sens. Pentru a È™ti care numÄƒr Ã®ntreg corespunde cu care label, putem inspecta variabila `features` al 'raw_train_dataset'. Acest lucru ne va spune tipul fiecÄƒrui coloanÄƒ.

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

Ãn spatele, `label` este obiect de tip `ClassLabel`, iar maparea numerelor Ã®ntregi la numele label-ului este stocat Ã®n folderele *names*. Astfel, `0` corespunde cu `not_equivalent`, iar `1` corespunde cu `equivalent`.

<Tip>

âœï¸ **ÃncercaÈ›i-o!** AratÄƒ elementul 15 din setul de antrenament È™i elementul 87 din setul de validare. Care sunt label-urile acesotra?

</Tip>

### Preprocessing un datset[[preprocessing-a-dataset]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Pentru a prelucra datele, trebuie sÄƒ convertim textul Ã®n numere pe care modelul le poate Ã®nÈ›elege. Cum ai vÄƒzut Ã®n [capitol anterior](/course/chapter2), acest lucru se face cu ajutorul unui tokenizer. Putem sÄƒ oferim tokenizerului o propoziÈ›ie sau o listÄƒ de propoziÈ›ii, aÈ™a cÄƒ putem sÄƒ tokenizÄƒm direct toate primele propoziÈ›ii È™i toate cele de-a doua propoziÈ›ii ale fiecÄƒrei perechi Ã®n felul urmÄƒtor:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ÃnsÄƒ nu putem sÄƒ transmitem doar douÄƒ propoziÈ›ii cÄƒtre model È™i sÄƒ obÈ›inem o predicÈ›ie dacÄƒ cele douÄƒ propoziÈ›ii sunt parafraze sau nu. Trebuie sÄƒ controlÄƒm cele douÄƒ secvenÈ›e ca o pereche, apoi sÄƒ le aplicÄƒm preprocesarea adecvatÄƒ. Norocul este cÄƒ tokenizer-ul poate lua o pereche de propoziÈ›ii È™i le poate pregÄƒti Ã®n felul Ã®n care modelul BERT Ã®l aÈ™teaptÄƒ: 

```py
inputs = tokenizer("Aceasta este prima propoziÈ›ie.", "Aceasta este a doua.")
inputs

```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Am discutat despre `input_ids` È™i `attention_mask` cheii Ã®n [Capitolul 2](/course/chapter2), dar am Ã®ntÃ¢rziat cu abordarea cheii `token_type_ids`. Ãn acest exemplu, aceastÄƒ cheie este ceea ce spune modelului care parte a inputului este prima propoziÈ›ie È™i care este a doua.

<Tip>
âœï¸ **ÃncercaÈ›i!** LuaÈ›i elementul 15 din setul de antrenare È™i tokenizaÈ›i cele douÄƒ propoziÈ›ii separat È™i ca o pereche. Care este diferenÈ›a dintre rezultatele celor douÄƒ? 
</Tip>

DacÄƒ facem decode ID-urilor din `input_ids` Ã®napoi la cuvinte:

```python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

Noi vom obÈ›ine urmÄƒtorul rezultat:

```python
['[CLS]', 'acesta', 'este', 'prima', 'propoziÈ›ie', '.', '[SEP]', 'acesta', 'este', 'a', 'doua', '.', '[SEP]']
```

Prin urmare, modelul aÈ™teaptÄƒ ca inputul sÄƒ fie Ã®n forma `[CLS] sentence1 [SEP] sentence2 [SEP]` atunci cÃ¢nd existÄƒ douÄƒ propoziÈ›ii. Aliniind aceastÄƒ informaÈ›ie cu `token_type_ids`, obÈ›inem:

```python
['[CLS]', 'acesta', 'este', 'prima', 'propoziÈ›ie', '.', '[SEP]', 'acesta', 'este', 'a', 'doua', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ObservÄƒm,cÄƒ inputul corespunzÄƒtor `[CLS] sentence1 [SEP]` au toate un token type ID de `0`, Ã®n timp ce celelalte pÄƒrÈ›i, corespunzÄƒtoare `sentence2 [SEP]`, au toate un token type ID de `1`.

ÃnÈ›elegem cÄƒ dacÄƒ selectÄƒm alt checkpoint al modelului, nu vom avea neapÄƒrat `token_type_ids` Ã®n inputurile tokenizate (de exemplu, ele nu sunt returnate dacÄƒ folosim un model DistilBERT). Acestea sunt returnate doar atunci cÃ¢nd modelul È™tie ce sÄƒ facÄƒ cu ele, pentru cÄƒ le-a vÄƒzut Ã®n timpul pretrainingului.

Ãn acest caz, BERT este antrenat cu token type IDs È™i pe lÃ¢ngÄƒ the masked language modeling objective despre care am vorbit Ã®n [Capitolul 1](/course/chapter1), are un objective suplimentar numit _next sentence prediction_. Objectiveul este destinat modelÄƒrii relaÈ›iei Ã®ntre perechi de propoziÈ›ii.

Cu urmÄƒtoare predicÈ›ie a propoziÈ›iei, modelului i se prezintÄƒ perechi de propoziÈ›ii (cu masked tokens aleatoriu) È™i i se cere sÄƒ prezicÄƒ dacÄƒ a doua propoziÈ›ie urmeazÄƒ primei. Pentru a face sarcina non-trivial, Ã®n jumÄƒtate dintre cazuri propiziÈ›iie urmeazÄƒ una pe alta Ã®n documentul original din care au fost extrase È™i cealaltÄƒ jumÄƒtate ele vin de la douÄƒ documente diferite.

Ãn general, nu trebuie sÄƒ vÄƒ faceÈ›i griji dacÄƒ existÄƒ sau nu `token_type_ids` Ã®n inputurile tokenizate: atÃ¢t timp cÃ¢t folosiÈ›i aceelaÈ™i checkpoint al modelului È™i a tokenizer-ului, totul va fi bine, pentru cÄƒ tokenizer-ul È™tie ce are de oferit modelului.

Acum cÄƒ am vÄƒzut cum poate prelucra tokenizer-ul o pereche de propoziÈ›ii, Ã®l putem folosi acest lucru pentru a tokeniza Ã®ntregul nostru dataset: exact ca Ã®n [capitolul anterior](/course/chapter2), putem sÄƒ oferim tokenizer-ului o listÄƒ de perechi de propoziÈ›ii oferindu-i prima listÄƒ de propoziÈ›ii È™i apoi lista a doua. Acest lucru este compatibil cu padding-ul È™i truncation-ul pe care le-am vÄƒzut Ã®n [Capitolul 2](/course/chapter2). Ãn felul acesta putem prelucra datasetul de antrenare astfel:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

AceastÄƒ metodÄƒ funcÈ›ioneazÄƒ bine, dar are dezavantajul cÄƒ returneazÄƒ un dicÈ›ionar (cu key-urile noastre, `input_ids`, `attention_mask`, È™i `token_type_ids` È™i valori care sunt liste de liste). Va funcÈ›iona doar dacÄƒ aveÈ›i destul de RAM pentru a stoca Ã®ntreg dataset Ã®n tokenizÄƒrii acestuia (iar dataset-urile din biblioteca ğŸ¤— Datasets sunt fiÈ™iere [Apache Arrow](https://arrow.apache.org/) stocate pe disc, deci tu stochezi numai sample-urile pe care le ceri sÄƒ se pÄƒstreaze Ã®n memorie).

Acum cÄƒ am Ã®nÈ›eles modul Ã®n care tokenizer-ul nostru poate gestiona o pereche de fraze, putem folosi el pentru a tokenize Ã®ntregul nostru set de date: la fel cum am fÄƒcut Ã®n [capitolul anterior](/course/chapter2), putem alimenta tokenizer-ul cu lista de prime fraze È™i apoi lista de fraze secunde. AceastÄƒ metodÄƒ este compatibilÄƒ È™i cu opÈ›iunile de umplere È™i tÄƒiere pe care le-am vÄƒzut Ã®n [Capitolul 2](/course/chapter2). Astfel, una dintre modalitÄƒÈ›ile prin care putem preprocesa dataset-ul de antrenare este:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

AceastÄƒ metodÄƒ funcÈ›ioneazÄƒ bine, dar are dezavantajul cÄƒ returneazÄƒ un dicÈ›ionar (cu cheile noastre, `input_ids`, `attention_mask`, È™i `token_type_ids`, È™i valori care sunt liste de liste). Va fi disponibilÄƒ doar dacÄƒ aveÈ›i suficient spaÈ›iu de memorie pentru a stoarca Ã®ntregul set de date Ã®n timpul tokenizÄƒrii (Ã®n timp ce dataset-urile din biblioteca ğŸ¤— Datasets se salveazÄƒ sub forma [Apache Arrow](https://arrow.apache.org/), fiÈ™iere stocate pe disch, astfel Ã®ncÃ¢t doar acele fragmente ale datelor pe care le cerem sÄƒ fie Ã®ncÄƒrcate Ã®n memorie).

Pentru a pÄƒstra datele Ã®n forma de dataset, vom folosi metoda [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). Aceasta ne permite puÈ›in mai multÄƒ flexibilitate, dacÄƒ ne trebuie mai multÄƒ preprocesare pe lÃ¢ngÄƒ tokenization. Metoda `map()` aplicÄƒ o funcÈ›ie asupra fiecÄƒrui element al datasetului, astfel putem sÄƒ definim o funcÈ›ie care tokenizeazÄƒ input-urile noastre:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

AceastÄƒ funcÈ›ie i-a un dicÈ›ionar (ca È™i elementele datasetului) È™i returneazÄƒ un nou dicÈ›ionar cu key-urile `input_ids`, `attention_mask`, È™i `token_type_ids`. Acesta va lucra chiar È™i dacÄƒ dicÈ›ionarul "example" conÈ›ine cÃ¢teva sample-uri(fiecare key ca o list de propoziÈ›ii), Ã®ntrucÃ¢t "tokenizer"-ul lucreazÄƒ È™i cu liste cu perechi de propoziÈ›ii cum am vÄƒzut mai devreme. Aceasta va permite folosirea opÈ›iunii `batched=True` Ã®n apelul funcÈ›iei `map()`, ceea ce va accelera semnificativ tokenizarea. `Tokenizer`-ul nostru este susÈ›inut de un tokenizer scris Ã®n Rust din biblioteca [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers). Acest tokenizer poate fi extrem de rapid, dar numai dacÄƒ Ã®i dÄƒm multe inputuri odatÄƒ.

*AtenÈ›ie*:Am omis argumentul `padding` din funcÈ›ia noastrÄƒ de tokenizare pentru moment. Acest lucru este datorat faptului cÄƒ aplicarea padding-ului tuturor elementelor cu lungimea maximÄƒ nu este eficientÄƒ: Ã®n schimb, este mai bine sÄƒ faceÈ›i padding elementelor atunci cÃ¢nd creaÈ›i un batch, astfel Ã®ncÃ¢t sÄƒ vÄƒ trebuiascÄƒ doar sÄƒ faceÈ›i padding pÃ¢nÄƒ la lungimea maximÄƒ doar Ã®n acest batch È™i nu Ã®ntregului dataset. Acest lucru poate salva mult timp È™i resurse atunci cÃ¢nd input-urile au lungimi foarte variabile.

Aici este modul Ã®n care aplicÄƒm funcÈ›ia de tokenizare asupra tuturor dataset-urilor noastre. Folosim `batched=True` Ã®n apelul funcÈ›iei `map` astfel Ã®ncÃ¢t funcÈ›ia sÄƒ fie aplicatÄƒ asupra mai multor elemente ale datasetului odatÄƒ. Aceasta permite preprocesarea acceleratÄƒ.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

Modul Ã®n care biblioteca ğŸ¤— Datasets aplicÄƒ aceastÄƒ procesare este prin adÄƒugarea de noi cÃ¢mpuri la dataset-uri, unul pentru fiecare key din dicÈ›ionarul returnat de funcÈ›ia de preprocesare:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

PuteÈ›i chiar È™i folosi multiprocessing cÃ¢nd aplicaÈ›i funcÈ›ia de preprocesare cu `map()` prin folosirea unui argument `num_proc`. Nu am fÄƒcut acest lucru aici pentru cÄƒ biblioteca ğŸ¤— Tokenizers utilizeazÄƒ deja mai multe thread-uri pentru a tokeniza sample-urile mai rapid, dar dacÄƒ nu folosiÈ›i un tokenizer rapid cu aceastÄƒ bibliotecÄƒ, acest lucru v-ar putea accelera preprocesarea.

FuncÈ›ia `tokenize_function` returneazÄƒ un dicÈ›ionar cu key-urile `input_ids`, `attention_mask`, È™i `token_type_ids`, astfel cÄƒ aceste trei cÃ¢mpuri sunt adÄƒugate tuturor split-urilor Ã®n dataset-urile noastre. NotaÈ›i-vÄƒ cÄƒ am putea chiar schimba cÃ¢mpurile existente dacÄƒ funcÈ›ia de preprocesare ar returna o nouÄƒ valoare pentru un key din dataset-ul pe care aplicÄƒm `map()`.

Ultimul lucru pe care trebuie sÄƒ Ã®l facem este sÄƒ facem padding tuturor exemplelor pÃ¢nÄƒ la lungimea celui mai mare element atunci cÃ¢nd facem batch elementelor Ã®mpreunÄƒ â€“ o tehnicÄƒ pe care o denumim *dinamic padding*.

### Dinamic Padding[[dynamic-padding]]
<Youtube id="7q5NyFT8REg"/>
{#if fw === 'pt'}

FuncÈ›ia care este responsabilÄƒ de a pune Ã®mpreunÄƒ sample-urile Ã®ntr-un batch se numeÈ™te *collate function*. Este un argument default pe care puteÈ›i sÄƒ-l transmiteÈ›i cÃ¢nd construiÈ›i un `DataLoader`, valoarea implicitÄƒ fiind o funcÈ›ie care va converti sample-urile Ã®n tensors PyTorch È™i le va concatena (recursiv dacÄƒ elementele dumneavoastrÄƒ sunt liste, tuple-uri sau dicÈ›ionare). Acest lucru nu este posibil Ã®n cazul nostru deoarece input-urile noastre nu vor avea toate aceeaÈ™i dimensiune. Am amÃ¢nat intenÈ›ionat padding-ul pentru a o aplica numai atunci cÃ¢nd e nevoie, pe fiecare batch È™i sÄƒ evitÄƒm astfel exemplele cu lungimi prea mari cu multe padding-uri. Acest lucru va accelera antrenarea, dar notaÈ›i-vÄƒ cÄƒ dacÄƒ antrenaÈ›i pe un TPU, acest lucru poate cauza probleme â€“ TPU-urile preferÄƒ forme fixe, chiar dacÄƒ aceasta Ã®nseamnÄƒ padding suplimentar.

{:else}

FuncÈ›ia care este responsabilÄƒ de a pune Ã®mpreunÄƒ sample-urile Ã®ntr-un batch se numeÈ™te *collate function*. Collator-ul default este o funcÈ›ie ce va transforma sample-urile Ã®n tf. FaceÈ›i Tensor È™i concatenaÈ›ile (recursiv dacÄƒ elementele dumneavoastrÄƒ sunt liste, tuple-uri sau dicÈ›ionare). Acest lucru nu este posibil Ã®n cazul nostru deoarece input-urile noastre nu vor avea toate aceeaÈ™i dimensiune. Am amÃ¢nat intenÈ›ionat padding-ul pentru a o aplica numai atunci cÃ¢nd e nevoie, pe fiecare batch È™i sÄƒ evitÄƒm astfel exemplele cu lungimi prea mari cu multe padding-uri. Acest lucru va accelera antrenarea, dar notaÈ›i-vÄƒ cÄƒ dacÄƒ antrenaÈ›i pe un TPU, acest lucru poate cauza probleme â€“ TPU-urile preferÄƒ forme fixe, chiar dacÄƒ aceasta Ã®nseamnÄƒ padding suplimentar.

{/if}

Pentru a face acest lucru, trebuie sÄƒ definim o funcÈ›ie collate care va aplica cantitatea corectÄƒ de padding pentru elementele din dataset-ul pe care vrem sÄƒ-i facem batch. Norocul este cÄƒ biblioteca ğŸ¤— Transformers ne oferÄƒ o asemenea funcÈ›ie prin `DataCollatorWithPadding`. Aceasta i-a un tokenizer atunci cÃ¢nd o iniÈ›ializaÈ›i(pentru a È™ti care padding token sÄƒ folosim È™i dacÄƒ modelul se aÈ™teaptÄƒ ca padding-ul sÄƒ fie pe stÃ¢nga sau dreapta inputu-lui) È™i va face tot ce este nevoie:
{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Pentru a testa acest nou jucÄƒrie, sÄƒ luÄƒm cÃ¢teva sample-uri din setul nostru de antrenare pe care am dori sÄƒ le facem batch Ã®mpreunÄƒ. Aici, È™tergem coloanele `idx`, `sentence1` È™i `sentence2` deoarece nu vor fi necesare È™i conÈ›in stringuri (È™i nu putem crea tensore cu stringuri) È™i uitaÈ›i-vÄƒ la lungimile fiecÄƒrui element din lotul nostru:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Nu e de mirare cÄƒ obÈ›inem sample-uri de lungimi diferite, Ã®ntre 32 È™i 67. Dynamic padding Ã®nseamnÄƒ cÄƒ sample-urilor din acest batch le-ar trebui aplicate padding cu o lungime de 67, lungimea cea mai mare din batch. FÄƒrÄƒ Dynamic padding, toate sample-urile ar fi avut padding cu maximul lungimii din Ã®ntregul dataset sau maximul lungimii pe care modelul poate accepta. SÄƒ verificÄƒm Ã®ncÄƒ o datÄƒ dacÄƒ `data_collator`-ul nostru face dynamic padding pe batch:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Looking good! Acum cÄƒ am trecut de la textul raw la batch-uri pe care modelul nostru poate sÄƒ le proceseze, suntem gata sÄƒ Ã®i facem fine-tune!

{/if}

<Tip>

âœï¸ **ÃncearcÄƒ-È›i norocul!** ImplementeazÄƒ preprocesarea datelor pe dataset-ul GLUE SST-2. Acesta se diferenÈ›Äƒ puÈ›in deoarece nu conÈ›ine perechi de propoziÈ›ii, ci doar cÃ¢te o propoziÈ›ie, dar ceea ce am fÄƒcut ar trebui sÄƒ rÄƒmÃ¢nÄƒ neschimbat. Pentru o provocare mai grea, Ã®ncearcÄƒ sÄƒ scrii o funcÈ›ie de preprocesare care se aplicÄƒ oricÄƒrui task GLUE.

</Tip>

{#if fw === 'tf'}

Acum cÄƒ avem dataset-ul nostru È™i un `data_collator`, este timpul sÄƒ-i punem la lucru. Ãn loc sÄƒ Ã®ncÄƒrcÄƒm manual batch-urile È™i sÄƒ le facem collate, asta Ã®nsemnÃ¢nd prea mult lucru È™i, nu ar fi nici prea performantÄƒ. Ãn schimb, existÄƒ o metodÄƒ simplÄƒ care oferÄƒ o soluÈ›ie performantÄƒ la acest problemÄƒ: `to_tf_dataset()`. Acest lucru va face wrap unui `tf.data.Dataset` Ã®n jurul datasetului nostru, cu o fucÈ›ie opÈ›ionalÄƒ de collation. `tf.data.Dataset` este un format nativ TensorFlow pe care Keras-ul o poate folosi pentru `model.fit()`, astfel Ã®ncÃ¢t aceastÄƒ metodÄƒ face convert unui ğŸ¤— Dataset la un format gata sÄƒ fie antrenat. Hai sÄƒ-l vedem Ã®n acÈ›iune cu datasetul nostru!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

È˜i asta e tot! Putem lua aceste dataset-uri pentru urmÄƒtoarea lecÈ›ie, unde antrenamentul va fi uÈ™or dupÄƒ toatÄƒ munca depusÄƒ pentru prelucrarea datelor.

{/if}