# O antrenare completÄƒ [[a-full-training]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[ 
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"}, 
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"}, 
]} />

<Youtube id="Dh9CL8fyG80"/>

Acum vom vedea cum putem atinge acelaÈ™i rezultat pe care l-am obÈ›inut Ã®n secÈ›iune anterioarÄƒ fÄƒrÄƒ a utiliza clasa `Trainer`. Din nou, noi presupunem cÄƒ ai procesat datele Ã®n secÈ›iunea 2. Ãn continuare, gÄƒsiÈ›i o scurtÄƒ sintezÄƒ asupra tuturor informaÈ›iilor de care aveÈ›i nevoie:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### PregÄƒtirea pentru antrenare[[prepare-for-training]]

Ãnainte de a scrie propriul nostru loop de antrenament, vom avea nevoie sÄƒ definim cÃ¢teva obiecte. Primele dintre ele sunt dataloader-urile pe care le vom utiliza pentru a itera peste batch-uri. Dar Ã®nainte de a putea defini aceste datealoader-uri, trebuie sÄƒ aplicÄƒm o anumitÄƒ postprocesare pe `tokenized_datasets`, pentru a avea grijÄƒ de unele lucruri care au fost automatizate de cÄƒtre `Trainer`. Ãn special:

- EliminÄƒm coloanele corespunzÄƒtoare valorilor pe care modelul nu le aÈ™teaptÄƒ (ca de exemplu, `sentence1` È™i `sentence2`).
- RedenumeÈ™te coloana `label` Ã®n `labels`, deoarece modelul aÈ™teaptÄƒ argumentul sÄƒ fie numit `labels`.
- SetÄƒm formatul dataseturilor astfel Ã®ncÃ¢t ele sÄƒ returneze tensore PyTorch Ã®n loc de liste.

Metodele noastre `tokenized_datasets` au fiecare unul dintre aceÈ™ti paÈ™i:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Vom verifica cÄƒ rezultatul conÈ›ine doar coloanele pe care vor modelul le acceptÄƒ:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Acum cÄƒ am fÄƒcut asta, putem defini cu uÈ™urinÈ›Äƒ dataloader-urile noastre:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

Pentru a verifica rapid cÄƒ nu existÄƒ nici o greÈ™ealÄƒ Ã®n procesarea datelor, putem inspecta un batch-ul astfel:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

ObservaÈ›i cÄƒ shapeurile actuale vor fi probabil uÈ™or diferite pentru tine, deoarece am setat `shuffle=True` pentru dataloader-ul de antrenare È™i am fÄƒcut padding batch-urilor la lungime maximÄƒ Ã®n interiorul lor.

Acum cÄƒ am terminat complet procesarea datelor (un obiectiv satisfÄƒcÄƒtor, dar Ã®nÈ™elÄƒtor pentru orice specialist ML), sÄƒ trecem la model. Ãl iniÈ›ializÄƒm exact ca Ã®n secÈ›iunea precedentÄƒ:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Pentru a ne asigura cÄƒ totul va merge bine Ã®n timpul antrenamentului, vom transmite batch-ul nostru modelului acesta:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

Toate modelele ğŸ¤— Transformers vor returna lossul atunci cÃ¢nd `labels` sunt furnizate È™i vom primi de asemenea logiturile (douÄƒ pentru fiecare input Ã®n batch-ul nostru, deci un tensor cu dimensiunea 8 x 2).

Suntem aproape gata sÄƒ scriem loopul nostru de antrenament! Doar cÄƒ ne lipsesc douÄƒ lucruri: un optimizer È™i learning rate scheduler. Deoarece Ã®ncercÄƒm sÄƒ replicÄƒm ceea ce a fÄƒcut `Trainer` manual, vom folosi aceleaÈ™i argumente default. Optimizerul utilizat de `Trainer` este `AdamW`, care este similar cu Adam, dar cu un twist pentru weight decay regularization (vedeÈ›i ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) de Ilya Loshchilov È™i Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

La final, learning rate schedulerul utilizat este un linear decay de la valoarea maximÄƒ (5e-5) la 0. Pentru a-l defini cu adevÄƒrat, avem nevoie sÄƒ cunoaÈ™tem numÄƒrul de paÈ™i de antrenament pe care Ã®i vom face, care este numÄƒrul de epoci pe care dorim sÄƒ le rulÄƒm Ã®nmulÈ›it cu numÄƒrul de batch-uri de antrenare (care este lungimea dataloader-ului de antrenare). `Trainer` foloseÈ™te trei epoci ca default, deci vom continua cu asta:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### Training Loop[[the-training-loop]]

Un ultim lucru: vom dori sÄƒ folosim un GPU dacÄƒ avem acces la unul (pe un CPU, instruirea poate dura cÃ¢teva ore Ã®n loc de cÃ¢teva minute). Pentru a face acest lucru, definim un `device` pe care vom plasa modelul È™i batch-urile noastre:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

Suntem acum gata pentru antrenare! Pentru a avea o idee cÃ¢nd va fi finalizatÄƒ antrenarea, adÄƒugÄƒm un progress bar peste numÄƒrul nostru de paÈ™i de antrenare, folosind biblioteca `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

PoÈ›i sÄƒ vezi cÄƒ nucleul loopului de instruire are o formÄƒ similarÄƒ cu cea din introducere. Nu am cerut nicio raportare, deci acest loop de antrenare nu ne va spune nimic despre cum se descurcÄƒ modelul. Trebuie sÄƒ adÄƒugÄƒm un evaluation loop pentru a face acest lucru.


### Evaluation Loop[[the-evaluation-loop]]

La fel ca Ã®nainte, vom folosi o metricÄƒ oferitÄƒ de librÄƒria ğŸ¤— Evaluate. Am vÄƒzut deja metoda `metric.compute()`, dar metricile pot acumula batch-urile pentru noi pe mÄƒsurÄƒ ce mergem peste loopurile de  predicÈ›oie cu metoda `add_batch()`. CÃ¢nd am acumulat toate batch-urile, putem sÄƒ obÈ›inem rezultatul final cu `metric.compute()`. Acesta este modul Ã®n care trebuie sÄƒ implementÄƒm acest lucru Ã®ntr-un loop de evaluare:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Rezultatele tale vor fi uÈ™or diferite datoritÄƒ randomizÄƒrii Ã®n iniÈ›ializarea headului modelului È™i a data shuffling, dar ele nu ar trebui sÄƒ fie foart diferite.


<Tip>

âœï¸ **ÃncearcÄƒ!** Modifica loopul de antrenare dinainte pentru a face fine-tune modelul pe dataset-ul SST-2.

</Tip>


### Supercharge Training Loopul cu ğŸ¤— Accelerate[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

Loopul de antrenare pe care l-am definit anterior funcÈ›ioneazÄƒ bine pe un singur CPU sau GPU. Dar folosind libraria [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate), cu cÃ¢teva ajustÄƒri putem activa distributed training pe multiple GPU-uri sau TPU-uri. ÃncepÃ¢nd de la crearea training È™i validation dataloaders, aceasta este este loopul manual de antrenare:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

È˜i aici sunt schimbÄƒrile:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

Prima linie de adÄƒugat este importarea librÄƒriei. A doua linie creeazÄƒ un obiect `Accelerator` care va evalua environmentul È™i va iniÈ›ializa proper distributed setup. ğŸ¤— Accelerate gestioneazÄƒ automat poziÈ›ionarea deviceului pentru tine, aÈ™a cÄƒ poÈ›i È™terge rÃ¢ndurile care pun modelul pe device (sau dacÄƒ preferi, poÈ›i sÄƒ le schimbi cu `accelerator.device` Ã®n loc de `device`). 

Partea principalÄƒ a lucrului se face Ã®n linia ce trimite dataloader-urile, modelul È™i optimizerul la `accelerator.prepare()`. Acaeasta va face wrap acestor obiecte Ã®n containerul potrivit pentru a asigura o antrenare distribuitÄƒ corespunzÄƒtoare. Ultimele schimbÄƒri sunt È™tergerea linei cep une batch-ul pe device(din nou, dacÄƒ vvrei sÄƒ laÈ™i acest lucru poÈ›i sÄƒ le schimbi cu `accelerator.device`) È™i schimbarea `loss.backward()` cu `accelerator.backward(loss)`.

<Tip>
âš ï¸ Pentru a beneficia de viteza oferitÄƒ de Cloud TPUs, recomandÄƒm sÄƒ faceÈ›i padding sampleurilor la o lungime fixÄƒ folosind argumentele `padding="max_length"` È™i `max_length` ale tokenizerului.
</Tip>

DacÄƒ vrei sÄƒ copiezi codul pentru a-l testa, aici este loopul complet de antrenare cu Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Scriind Ã®n `train.py` aceste modificÄƒri, vor face scriptul executabil pe orice tip de distributed setup. Pentru a testa codul Ã®n mediul tÄƒu distribuit, ruleazÄƒ urmÄƒtoarea comandÄƒ:

```bash
accelerate config
```

Ceea ce Ã®È›i va oferi sÄƒ rÄƒspunzi la o serie de Ã®ntrebÄƒri È™i sÄƒ salvezi rÄƒspunsurile Ã®ntr-un fiÈ™ier de configurare folosit de acest modul: 

```bash
accelerate launch train.py
```

AceastÄƒ comandÄƒ va lansa loopul de antrenare pe dispozitivele distribuite.

DacÄƒ doriÈ›i sÄƒ Ã®ncercaÈ›i acest lucru Ã®ntr-un Jupyter Notebook (de exemplu, pentru a testa TPU-urile de pe Colab), Ã®nlocuiÈ›i codul cu urmÄƒtoarea funcÈ›ie:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

Pentru mai multe exemple consultaÈ›i [repo-ul ğŸ¤— Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).