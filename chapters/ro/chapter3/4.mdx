# O instruire completÄƒ[[o-instruire-completÄƒ]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

Acum vom vedea cum sÄƒ obÈ›inem aceleaÈ™i rezultate ca Ã®n secÈ›iunea anterioarÄƒ, dar fÄƒrÄƒ sÄƒ folosim clasa `Trainer`. Din nou, presupunem cÄƒ aÈ›i parcurs deja procesarea datelor din secÈ›iunea 2. IatÄƒ un scurt rezumat al tot ceea ce veÈ›i avea nevoie:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### PregÄƒtirea pentru antrenament[[pregÄƒtirea-pentru-antrenament]]

Ãnainte de a scrie efectiv bucla de antrenament, va trebui sÄƒ definim cÃ¢teva obiecte. Primele sunt Ã®ncÄƒrcÄƒtoarele de date (dataloaders) pe care le vom folosi pentru a itera pe batch-uri. Dar, Ã®nainte de a putea defini acele dataloaders, trebuie sÄƒ aplicÄƒm un pic de postprocesare dataset-urilor noastre `tokenized_datasets`, pentru a ne ocupa de cÃ¢teva lucruri pe care `Trainer` le fÄƒcea automat pentru noi. Mai exact, trebuie sÄƒ:

- EliminÄƒm coloanele care corespund valorilor pe care modelul nu le aÈ™teaptÄƒ (cum ar fi coloanele `sentence1` È™i `sentence2`).
- Redenumim coloana `label` Ã®n `labels` (pentru cÄƒ modelul se aÈ™teaptÄƒ ca argumentul sÄƒ se numeascÄƒ `labels`).
- SetÄƒm formatul dataset-urilor astfel Ã®ncÃ¢t sÄƒ returneze tensori PyTorch Ã®n loc de liste.

`tokenized_datasets` are cÃ¢te o metodÄƒ pentru fiecare dintre aceÈ™ti paÈ™i:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Putem apoi sÄƒ verificÄƒm cÄƒ rezultatul are doar coloanele pe care modelul le va accepta:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Acum, dupÄƒ ce am terminat acest pas, putem defini foarte uÈ™or dataloader-urile noastre:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

Pentru a verifica rapid cÄƒ nu existÄƒ nicio eroare Ã®n procesarea datelor, putem inspecta un batch astfel:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

ObservaÈ›i cÄƒ formele reale ar putea fi uÈ™or diferite pentru voi, pentru cÄƒ am setat `shuffle=True` Ã®n dataloader-ul nostru de antrenament È™i pentru cÄƒ Ã®mpachetÄƒm (padding) la lungimea maximÄƒ Ã®n interiorul batch-ului.

Acum cÄƒ am terminat complet procesarea datelor (un obiectiv satisfÄƒcÄƒtor, dar uneori greu de atins pentru orice practician ML), sÄƒ trecem la model. Ãl instanÈ›iem exact ca Ã®n secÈ›iunea anterioarÄƒ:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Pentru a ne asigura cÄƒ totul va decurge fÄƒrÄƒ probleme Ã®n timpul antrenamentului, trecem batch-ul nostru prin model:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

Toate modelele ğŸ¤— Transformers vor returna pierderea (loss) cÃ¢nd `labels` sunt furnizate, È™i, de asemenea, obÈ›inem logits (douÄƒ pentru fiecare intrare Ã®n batch, deci un tensor de mÄƒrimea 8 x 2).

Suntem aproape gata sÄƒ scriem bucla de antrenament! Ne mai lipsesc douÄƒ lucruri: un optimizer È™i un scheduler pentru rata de Ã®nvÄƒÈ›are. Pentru cÄƒ Ã®ncercÄƒm sÄƒ reproducem ceea ce fÄƒcea `Trainer`, vom folosi aceleaÈ™i valori implicite. Optimizer-ul folosit de `Trainer` este `AdamW`, care este acelaÈ™i cu Adam, dar cu o abordare particularÄƒ pentru regularizarea weight decay (vedeÈ›i lucrarea ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) de Ilya Loshchilov È™i Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Ãn final, scheduler-ul pentru rata de Ã®nvÄƒÈ›are folosit implicit este doar o descreÈ™tere liniarÄƒ de la valoarea maximÄƒ (5e-5) la 0. Pentru a-l defini corect, trebuie sÄƒ È™tim numÄƒrul de paÈ™i de antrenament pe care Ã®i vom face, care este numÄƒrul de epoci dorit Ã®nmulÈ›it cu numÄƒrul de batch-uri de antrenament (care este lungimea dataloader-ului nostru de antrenament). `Trainer` foloseÈ™te trei epoci implicit, aÈ™a cÄƒ vom urma acest exemplu:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### Bucla de antrenament[[bucla-de-antrenament]]

ÃncÄƒ un lucru: vom dori sÄƒ folosim GPU-ul dacÄƒ avem acces la unul (pe un CPU, antrenamentul poate dura cÃ¢teva ore Ã®n loc de cÃ¢teva minute). Pentru asta, definim un `device` pe care vom pune modelul È™i batch-urile noastre:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

Acum suntem gata de antrenament! Pentru a ne face o idee despre momentul Ã®n care se va termina antrenamentul, adÄƒugÄƒm o barÄƒ de progres peste numÄƒrul de paÈ™i de antrenament, folosind biblioteca `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ObservaÈ›i cÄƒ partea principalÄƒ a buclei de antrenament aratÄƒ foarte asemÄƒnÄƒtor cu cea din introducere. Nu am cerut niciun raport, aÈ™a cÄƒ aceastÄƒ buclÄƒ de antrenament nu ne va spune nimic despre performanÈ›a modelului. Pentru a avea feedback, trebuie sÄƒ adÄƒugÄƒm o buclÄƒ de evaluare.


### Bucla de evaluare [[bucla-de-evaluare]]

Ca È™i Ã®nainte, vom folosi o metricÄƒ oferitÄƒ de biblioteca ğŸ¤— Evaluate. Am vÄƒzut deja metoda `metric.compute()`, dar metricle pot de fapt sÄƒ acumuleze batch-uri pentru noi Ã®n timp ce parcurgem bucla de predicÈ›ie, cu metoda `add_batch()`. OdatÄƒ ce am acumulat toate batch-urile, putem obÈ›ine rezultatul final cu `metric.compute()`. IatÄƒ cum sÄƒ implementÄƒm toate acestea Ã®ntr-o buclÄƒ de evaluare:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Din nou, rezultatele voastre vor fi uÈ™or diferite din cauza aleatorietÄƒÈ›ii Ã®n iniÈ›ializarea layer-ului final (model head) È™i a amestecÄƒrii datelor, dar ar trebui sÄƒ fie Ã®n aceeaÈ™i zonÄƒ valoricÄƒ.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** ModificaÈ›i bucla de antrenament anterioarÄƒ pentru a vÄƒ rafina modelul pe dataset-ul SST-2.

### ÃmbunÄƒtÄƒÈ›iÈ›i circuitul de antrenament cu ğŸ¤— Accelerate[[Ã®mbunÄƒtÄƒÈ›iÈ›i-circuitul-de-antrenament-cu-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

Bucla de antrenament pe care am definit-o anterior funcÈ›ioneazÄƒ bine pe un singur CPU sau GPU. Dar, folosind biblioteca [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate), cu doar cÃ¢teva ajustÄƒri putem activa antrenarea distribuitÄƒ pe mai multe GPU-uri sau TPU-uri. Pornind de la crearea dataloader-urilor de antrenament È™i validare, iatÄƒ cum aratÄƒ bucla noastrÄƒ manualÄƒ de antrenament:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Iar aici sunt modificÄƒrile:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

Prima linie de adÄƒugat este linia de import. A doua linie instanÈ›iazÄƒ un obiect `Accelerator` care va examina mediul È™i va iniÈ›ializa setarea distribuitÄƒ corespunzÄƒtoare. ğŸ¤— Accelerate se ocupÄƒ de plasarea pe device pentru voi, aÈ™a cÄƒ puteÈ›i elimina liniile care pun modelul pe device (sau, dacÄƒ preferaÈ›i, le puteÈ›i schimba sÄƒ foloseascÄƒ `accelerator.device` Ã®n loc de `device`).

Apoi, partea principalÄƒ a muncii este fÄƒcutÄƒ Ã®n linia care trimite dataloaders, modelul È™i optimizer-ul la `accelerator.prepare()`. Aceasta va Ã®mpacheta acele obiecte Ã®n containerul potrivit pentru a vÄƒ asigura cÄƒ antrenarea distribuitÄƒ funcÈ›ioneazÄƒ corespunzÄƒtor. Restul modificÄƒrilor constau Ã®n eliminarea liniei care mutÄƒ batch-ul pe `device` (din nou, dacÄƒ doriÈ›i sÄƒ o pÄƒstraÈ›i puteÈ›i doar sÄƒ o schimbaÈ›i sÄƒ foloseascÄƒ `accelerator.device`) È™i Ã®nlocuirea `loss.backward()` cu `accelerator.backward(loss)`.

> [!TIP]
> âš ï¸ Pentru a beneficia de creÈ™terea vitezei oferitÄƒ de Cloud TPU-uri, vÄƒ recomandÄƒm sÄƒ Ã®mpachetaÈ›i mostrele la o lungime fixÄƒ folosind argumentele `padding="max_length"` È™i `max_length` ale tokenizer-ului.

DacÄƒ vreÈ›i sÄƒ copiaÈ›i È™i sÄƒ lipiÈ›i pentru a vÄƒ juca, iatÄƒ cum aratÄƒ bucla completÄƒ de antrenament cu ğŸ¤— Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

PlasÃ¢nd acest cod Ã®ntr-un fiÈ™ier `train.py` Ã®l face rulabil pe orice tip de configurare distribuitÄƒ. Pentru a-l Ã®ncerca Ã®n configurarea voastrÄƒ distribuitÄƒ, rulaÈ›i comanda:

```bash
accelerate config
```

care vÄƒ va cere sÄƒ rÄƒspundeÈ›i la cÃ¢teva Ã®ntrebÄƒri È™i vÄƒ va crea un fiÈ™ier de configurare folosit de comanda:

```
accelerate launch train.py
```

care va porni antrenarea distribuitÄƒ.

DacÄƒ vreÈ›i sÄƒ Ã®ncercaÈ›i asta Ã®ntr-un Notebook (de exemplu, pentru a-l testa cu TPU-uri pe Colab), doar lipiÈ›i codul Ã®ntr-o funcÈ›ie `training_function()` È™i rulaÈ›i Ã®ntr-o celulÄƒ finalÄƒ:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

PuteÈ›i gÄƒsi mai multe exemple Ã®n [repo-ul ğŸ¤— Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).