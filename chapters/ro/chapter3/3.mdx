<FrameworkSwitchCourse {fw} />

# Ajustarea unui model folosind Trainer API[[ajustarea-unui-model-folosind-trainer-api]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

ğŸ¤— Transformers oferÄƒ o clasÄƒ `Trainer` pentru a vÄƒ ajuta sÄƒ reglaÈ›i mai bine oricare dintre modelele preinstruite pe care le oferÄƒ pe setul dvs. de date. DupÄƒ ce aÈ›i fÄƒcut toatÄƒ munca de preprocesare a datelor din ultima secÈ›iune, vÄƒ mai rÄƒmÃ¢n doar cÃ¢È›iva paÈ™i pentru a defini clasa `Trainer`. Cea mai dificilÄƒ parte va fi probabil pregÄƒtirea mediului pentru a rula `Trainer.train()`, deoarece acesta va rula foarte lent pe un CPU. DacÄƒ nu aveÈ›i un GPU configurat, puteÈ›i obÈ›ine acces gratuit la GPU-uri sau TPU-uri pe [Google Colab] (https://colab.research.google.com/).

Exemplele de cod de mai jos presupun cÄƒ aÈ›i executat deja exemplele din secÈ›iunea anterioarÄƒ. IatÄƒ un scurt rezumat care recapituleazÄƒ ceea ce aveÈ›i nevoie:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Antrenarea[[antrenarea]]

Primul pas Ã®nainte de a ne defini modelul `Trainer` este sÄƒ definim o clasÄƒ `TrainingArguments` care va conÈ›ine toÈ›i hiperparametrii pe care `Trainer` Ã®i va utiliza pentru formare È™i evaluare. Singurul argument pe care trebuie sÄƒ Ã®l furnizaÈ›i este un folder Ã®n care va fi salvat modelul antrenat, precum È™i punctele de control de pe parcurs. Pentru tot restul, puteÈ›i lÄƒsa valorile implicite, care ar trebui sÄƒ funcÈ›ioneze destul de bine pentru o ajustare de bazÄƒ.
 
```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

> [!TIP]
> ğŸ’¡ DacÄƒ doriÈ›i sÄƒ Ã®ncÄƒrcaÈ›i automat modelul Ã®n Hub Ã®n timpul instruirii, treceÈ›i `push_to_hub=True` Ã®n `TrainingArguments`. Vom afla mai multe despre acest lucru Ã®n [Capitolul 4](/course/chapter4/3).

Al doilea pas este sÄƒ ne definim modelul. Ca È™i Ã®n [capitolul anterior](/course/chapter2), vom folosi clasa `AutoModelForSequenceClassification`, cu douÄƒ etichete:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

VeÈ›i observa cÄƒ, spre deosebire de [Capitolul 2](/course/chapter2), veÈ›i primi un avertisment dupÄƒ instanÈ›ierea acestui model preinstruit. Acest lucru se datoreazÄƒ faptului cÄƒ BERT nu a fost instruit Ã®n prealabil cu privire la clasificarea perechilor de propoziÈ›ii, astfel Ã®ncÃ¢t head-ul modelului instruit Ã®n prealabil a fost eliminat È™i a fost adÄƒugat Ã®n schimb un nou head adecvat pentru clasificarea secvenÈ›elor. AvertizÄƒrile indicÄƒ faptul cÄƒ unele ponderi nu au fost utilizate (cele corespunzÄƒtoare head-ului de preformare eliminat) È™i cÄƒ altele au fost iniÈ›ializate aleatoriu (cele pentru noul head). Ãn Ã®ncheiere, vÄƒ Ã®ncurajeazÄƒ sÄƒ antrenaÈ›i modelul, ceea ce vom face acum.

OdatÄƒ ce avem modelul nostru, putem defini un `Trainer` transmiÈ›Ã¢ndu-i toate obiectele construite pÃ¢nÄƒ acum - `modelul`, `training_args`, seturile de date de formare È™i validare, `data_collator` È™i `tokenizer`:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

ReÈ›ineÈ›i cÄƒ atunci cÃ¢nd treceÈ›i `tokenizer` aÈ™a cum am fÄƒcut aici, `data_collator` implicit folosit de `Trainer` va fi un `DataCollatorWithPadding` aÈ™a cum a fost definit anterior, deci puteÈ›i sÄƒri peste linia `data_collator=data_collator` Ã®n acest apel. Era totuÈ™i important sÄƒ vÄƒ arÄƒt aceastÄƒ parte a procesÄƒrii Ã®n secÈ›iunea 2!

Pentru a regla cu precizie modelul pe setul nostru de date, trebuie doar sÄƒ apelÄƒm metoda `train()` a `Trainer`:

```py
trainer.train()
```

Acest lucru va Ã®ncepe reglarea finÄƒ (care ar trebui sÄƒ dureze cÃ¢teva minute pe un GPU) È™i va raporta valoarea pierderii de formare la fiecare 500 de paÈ™i. Cu toate acestea, nu vÄƒ va spune cÃ¢t de bine (sau rÄƒu) funcÈ›ioneazÄƒ modelul dumneavoastrÄƒ. Acest lucru se datoreazÄƒ faptului cÄƒ:

1. Nu am precizat cÄƒ `Trainer` trebuie sÄƒ evalueze Ã®n timpul antrenamentului prin setarea `evaluation_strategy` la `â€stepsâ€` (evaluare la fiecare `eval_steps`) sau `â€epochâ€` (evaluare la sfÃ¢rÈ™itul fiecÄƒrei epoch).
2. Nu am furnizat pentru `Trainer` o funcÈ›ie `compute_metrics()` pentru a calcula o valoare metricÄƒ Ã®n timpul evaluÄƒrii (altfel evaluarea ar fi afiÈ™at doar pierderea, care nu este un numÄƒr foarte intuitiv).

### Evaluarea[[evaluarea]]

SÄƒ vedem cum putem construi o funcÈ›ie utilÄƒ `compute_metrics()` È™i sÄƒ o folosim data viitoare cÃ¢nd ne antrenÄƒm. FuncÈ›ia trebuie sÄƒ preia un obiect `EvalPrediction` (care este un tuple numit cu un cÃ¢mp `predictions` È™i un cÃ¢mp `label_ids`) È™i va returna un dicÈ›ionar care mapeazÄƒ È™iruri de caractere Ã®n valori float. Pentru a obÈ›ine unele predicÈ›ii de la modelul nostru, putem utiliza comanda `Trainer.predict()`:

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

Rezultatul metodei `predict()` este un alt tuple numit cu trei cÃ¢mpuri: `predictions`, `label_ids` È™i `metrics`. CÃ¢mpul `metrics` va conÈ›ine doar pierderea pe setul de date transmis, precum È™i unele valori metrice de timp (cÃ¢t timp a durat predicÈ›ia, Ã®n total È™i Ã®n medie). DupÄƒ ce vom finaliza funcÈ›ia `compute_metrics()` È™i o vom transmite cÄƒtre `Trainer`, acest cÃ¢mp va conÈ›ine È™i metrica returnatÄƒ de `compute_metrics()`.

DupÄƒ cum puteÈ›i vedea, `predictions` este un array bidimensional cu forma 408 x 2 (408 fiind numÄƒrul de elemente din setul de date pe care l-am folosit). Acestea sunt logits pentru fiecare element al setului de date pe care l-am transmis la `predict()` (dupÄƒ cum aÈ›i vÄƒzut Ã®n [capitolul anterior](/course/chapter2), toate modelele Transformer returneazÄƒ logits). Pentru a le transforma Ã®n predicÈ›ii pe care le putem compara cu etichetele noastre, trebuie sÄƒ luÄƒm indicele cu valoarea maximÄƒ pe a doua axÄƒ:

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Acum putem compara `preds` cu etichetele. Pentru a construi funcÈ›ia noastrÄƒ `compute_metric()`, ne vom baza pe  valorile metrice din biblioteca ğŸ¤— [Evaluate](https://github.com/huggingface/evaluate/). Putem Ã®ncÄƒrca  valorile metrice asociate cu setul de date MRPC la fel de uÈ™or cum am Ã®ncÄƒrcat setul de date, de data aceasta cu funcÈ›ia `evaluate.load()`. Obiectul returnat are o metodÄƒ `compute()` pe care o putem utiliza pentru a efectua calculul metric:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Rezultatele exacte pe care le obÈ›ineÈ›i pot varia, deoarece iniÈ›ializarea aleatorie a head-ului modelului poate schimba parametrii obÈ›inuÈ›i. Aici, putem vedea cÄƒ modelul nostru are o precizie de 85,78% pe setul de validare È™i un scor F1 de 89,97. Acestea sunt cele douÄƒ valori metrice utilizate pentru a evalua rezultatele pe setul de date MRPC pentru criteriul de referinÈ›Äƒ GLUE. Tabelul din [lucrarea BERT] (https://arxiv.org/pdf/1810.04805.pdf) a raportat un scor F1 de 88,9 pentru modelul de bazÄƒ. Acesta a fost modelul â€fÄƒrÄƒ casareâ€, Ã®n timp ce noi folosim Ã®n prezent modelul â€cu casareâ€, ceea ce explicÄƒ rezultatul mai bun.

AdunÃ¢nd totul, obÈ›inem funcÈ›ia noastrÄƒ `compute_metrics()`:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

È˜i pentru a vedea cum se utilizeazÄƒ Ã®n practicÄƒ pentru a raporta datele metrice la sfÃ¢rÈ™itul fiecÄƒrei perioade, iatÄƒ cum definim un nou `Trainer` cu aceastÄƒ funcÈ›ie `compute_metrics()`:

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ReÈ›ineÈ›i cÄƒ creÄƒm un nou `TrainingArguments` cu `evaluation_strategy` setat la `â€epochâ€` È™i un nou model - Ã®n caz contrar, am continua doar instruirea modelului pe care l-am instruit deja. Pentru a lansa o nouÄƒ rulare de formare, executÄƒm:

```py
trainer.train()
```

De data aceasta, acesta va raporta pierderea de validare È™i metrica la sfÃ¢rÈ™itul fiecÄƒrei perioade, pe lÃ¢ngÄƒ valoarea pierderii de formare. Din nou, scorul exact de acurateÈ›e/F1 pe care Ã®l veÈ›i obÈ›ine ar putea fi puÈ›in diferit de ceea ce am gÄƒsit noi, din cauza iniÈ›ializÄƒrii aleatorii a modelului, dar ar trebui sÄƒ fie Ã®n aceeaÈ™i zonÄƒ.

Modelul `Trainer` va funcÈ›iona din start pe mai multe GPU sau TPU È™i oferÄƒ o mulÈ›ime de opÈ›iuni, cum ar fi formarea cu precizie mixtÄƒ (utilizaÈ›i `fp16 = True` Ã®n argumentele de formare). Vom trece Ã®n revistÄƒ toate funcÈ›iile pe care le suportÄƒ Ã®n capitolul 10.

Aceasta Ã®ncheie introducerea la reglarea finÄƒ cu ajutorul API-ului `Trainer`. Ãn [Capitolul 7](/course/chapter7) va fi prezentat un exemplu de efectuare a acestei operaÈ›ii pentru cele mai comune sarcini NLP, dar pentru moment sÄƒ analizÄƒm cum se poate face acelaÈ™i lucru Ã®n PyTorch pur.

> [!TIP]
> âœï¸ **ÃncercaÈ›i!** AjustaÈ›i un model pe setul de date GLUE SST-2, folosind procesarea datelor efectuatÄƒ Ã®n secÈ›iunea 2.

