<FrameworkSwitchCourse {fw} />

# IntroduÃ§Ã£o

No [CapÃ­tulo 3](/course/chapter3), vocÃª viu como fazer o ajuste fino (fine-tune) de um modelo de classificaÃ§Ã£o de texto. Neste capÃ­tulo, abordaremos as seguintes tarefas de NLP (tambÃ©m conhecido como PLN):

- ClassificaÃ§Ã£o dos Tokens
- Modelagem de linguagem mascarada (como BERT)
- SumarizaÃ§Ã£o 
- TraduÃ§Ã£o
- Modelagem de linguagem causal prÃ©-treinamento (como GPT-2)
- Responder perguntas

{#if fw === 'pt'}

Para fazer isso, terÃ¡ de aproveitar tudo o que aprendeu sobre a API `Trainer` e a biblioteca ğŸ¤— Accelerate no [CapÃ­tulo 3](/course/chapter3), a biblioteca ğŸ¤— Datasets no [CapÃ­tulo 5](/course/chapter5), e a biblioteca ğŸ¤— Tokenizers no [CapÃ­tulo 6](/course/chapter6). TambÃ©m vamos fazer o upload dos nossos resultados para o Model Hub, assim como fizemos no [CapÃ­tulo 4](/course/chapter4), entÃ£o realmente esse Ã© o capÃ­tulo onde tudo se junta!

Cada seÃ§Ã£o pode ser lida de forma independente e irÃ¡ mostrar como treinar um modelo com a API `Trainer` ou com o seu prÃ³prio laÃ§o de treinamento, utilizando ğŸ¤— Accelerate. Sinta-se Ã  vontade para pular qualquer parte e se concentrar na que mais lhe interessa: a API `Trainer` Ã© excelente para o ajuste fino ou para treinar o seu modelo sem se preocupar com o que se passa nos bastidores, enquanto que o laÃ§o de treinamento com `Accelerate` permite personalizar qualquer parte que queira com mais facilidade.

{:else}

Para fazer isso, terÃ¡ de aproveitar tudo o que aprendeu sobre o treinamento de modelo com a API Keras no [CapÃ­tulo 3](/course/chapter3), a biblioteca ğŸ¤— Datasets no [CapÃ­tulo 5](/course/chapter5), e a biblioteca ğŸ¤— Tokenizers no [CapÃ­tulo 6](/course/chapter6). TambÃ©m vamos fazer o upload dos nossos resultados para o Model Hub, assim como fizemos no [CapÃ­tulo 4](/course/chapter4), entÃ£o realmente esse Ã© o capÃ­tulo onde tudo se junta!

Cada seÃ§Ã£o pode ser lida de forma independente.

{/if}


<Tip>

Se ler as seÃ§Ãµes em sequÃªncia, notarÃ¡ que elas tÃªm bastante cÃ³digo e texto em comum. Essa repetiÃ§Ã£o Ã© intencional para que possa mergulhar (ou voltar mais tarde) em qualquer tarefa que lhe interesse e encontrar um exemplo completo.

</Tip>
