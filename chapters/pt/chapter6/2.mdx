# Treinando um novo tokenizador

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},
]} />

Se um modelo de linguagem nÃ£o estiver disponÃ­vel no idioma que vocÃª estiver interessado, ou se o seu corpus for muito diferente do que o seu modelo de linguagem foi treinado, vocÃª muito provavelmente desejarÃ¡ retreinar o modelo do zero usando um tokenizador adaptado para seus dados. Isto exigirÃ¡ um treinamento de um novo tokenizador para seu conjunto de dados. Mas o que isso exatamente significa? Quando observamos os tokenizadores pela primeira vez no [CapÃ­tulo 2](/course/chapter2), nÃ³s vimos que a maioria dos modelos Transformer usa um algoritmo de tokenizaÃ§Ã£o de subpalavras. Para identificar quais subpalavras sÃ£o de interesse e que ocorrem mais frequentemente no corpus em questÃ£o, o tokenizador precisa dar uma boa olhada em todos os textos no corpus -- processo que chamamos de *treinamento*. As regras exatas que governam o treinamento dependem do tipo de tokenizador usado, e veremos os trÃªs algoritmos principais mais adiante neste capÃ­tulo.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

âš ï¸ Treinar um tokenizador nÃ£o Ã© o mesmo que treinar um modelo! O treinamento de um modelo usa o gradiente descendente estocÃ¡stico para fazer a perda um pouquinho menor a cada batch. Portanto, Ã© aleatÃ³rio por natureza (o que significa que vocÃª deve definir seeds para obter o mesmo resultado quando estiver fazendo o mesmo treino novamente). Treinar um tokenizador Ã© um processo estatÃ­stico que tenta identificar que subpalavras sÃ£o as melhores para escolher dependendo do algoritmo de tokenizaÃ§Ã£o. Portanto, este processo Ã© determinÃ­stico, o que significa que vocÃª terÃ¡ sempre o mesmo resultado quando for treinar com o mesmo algoritmo no mesmo corpus.

</Tip>

## Montando um corpus

Existe uma API muito simples em ğŸ¤— Transformers que vocÃª pode usar para treinar um novo tokenizador com as mesmas caracterÃ­sticas de um jÃ¡ existente: `AutoTokenizer.train_new_from_iterator()`. Para ver isso em aÃ§Ã£o, vamos supor que queremos treinar o GPT-2 do zero, mas em um idioma diferente do inglÃªs. Nossa primeira tarefa serÃ¡ obter muitos dados de um idioma em um corpus de treinamento. Para prover exemplos que todo mundo serÃ¡ capaz de entender, nÃ£o usaremos um idioma como russo ou chinÃªs aqui, mas sim in idiome inglÃªs especializado: cÃ³digo Python.

A biblioteca [ğŸ¤— Datasets](https://github.com/huggingface/datasets) pode nos ajudar a montar um corpus de cÃ³digos em Python. NÃ³s usaremos a funÃ§Ã£o usual `load_dataset()` para baixar e armazenar em cache o dataset [CodeSearchNet](https://huggingface.co/datasets/code_search_net). Este dataset foi criado para o [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) e contÃ©m milhÃµes de funÃ§Ãµes de bibliotecas de cÃ³digo aberto no GitHub em diferentes linguagens de programaÃ§Ã£o. Aqui, nÃ³s iremos carregar a parte Python deste dataset:

```py
from datasets import load_dataset

# Isto pode levar alguns minutos para carregar, entÃ£o pegue um copo de cafÃ© enquanto espera!
raw_datasets = load_dataset("code_search_net", "python")
```

Podemos dar uma olhada na divisÃ£o de treinamento para ver a quais colunas temos acesso:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```
Podemos ver que o conjunto de dados separa as docstrings do cÃ³digo e sugere uma tokenizaÃ§Ã£o de ambos. Aqui, usaremos apenas a coluna `whole_func_string` para treinar o nosso tokenizador. Podemos observar um exemplo de uma dessas funÃ§Ãµes indexando na divisÃ£o `train`:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

Que deve resultar na seguinte saÃ­da:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

A primeira coisa que precisamos fazer Ã© transformar o dataset em um iterador) de listas de textos -- por exemplo, uma lista de lista de textos. Usando lista de textos irÃ¡ habilitar o nosso tokenizador para funcionar mais rapidamente (treinando em lotes de textos ao invÃ©s de processar individualmente os textos, um por vez), e deve ser um iterador se quisermos evitar ter tudo na memÃ³ria de uma vez. Se o teu corpus for grande, vocÃª vai querer aproveitar o fato de que ğŸ¤— Datasets nÃ£o carrega tudo na memÃ³ria RAM, mas armazena os elementos do dataset no disco.

Executar o trecho abaixo criaria uma lista de listas de 1000 textos cada, mas carregaria tudo na memÃ³ria:

```py
# NÃ£o remova o comentÃ¡rio da linha abaixo, a menos que o teu dataset seja pequeno!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

Usando um Python generator, nÃ³s podemos evitar que o Python carregue tudo na memÃ³ria atÃ© que realmente seja necessÃ¡rio. Para criar tal generator, vocÃª precisa apenas substituir os colchetes por parÃªnteses:   

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

Esta linha de cÃ³digo nÃ£o busca nenhum elemento no dataset; ele apenas cria um objeto que vocÃª pode usar em um o loop `for` do Python. Os textos sÃ³ serÃ£o carregados quando vocÃª precisar deles (ou seja, quando vocÃª estiver na etapa do loop `for` que os requer), e apenas 1000 textos por vez serÃ£o carregados. Desse modo, vocÃª nÃ£o esgotarÃ¡ toda a sua memÃ³ria, mesmo se vocÃª estiver processando um grande dataset.

O problema com um objeto gerador Ã© que ele sÃ³ pode ser usado uma vez. EntÃ£o, em vez de nos dar a lista dos primeiros 10 dÃ­gitos duas vezes:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

nÃ³s obtemos uma vez e, em seguida, uma lista vazia:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

Ã‰ por isso que definomos uma funÃ§Ã£o que retorna um gerador:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

VocÃª tambÃ©m pode definir o seu gerador dentro de um loop `for` ao usar o comando `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

que irÃ¡ produzir exatamente o mesmo gerador de antes, mas permite que vocÃª use uma lÃ³gica mais complexa do que vocÃª pode em um list comprehension.

## Treinando um novo tokenizador

Agora que nÃ³s temos o nosso corpus na forma de um iterador de lotes de texto, estamos prontos para treinar um novo tokenizador. Para fazer isso, primeiramente nÃ³s precisamos carregar o tokenizador que queremos emparelhar com o nosso modelo (neste caso, GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```
Por mais que iremos treinar um novo tokenizador, Ã© uma boa ideia fazer isso para evitar comeÃ§ar do zero. Dessa forma, nÃ£o precisaremos especificar nada sobre o algoritmo de tokenizaÃ§Ã£o ou tokens especiais que queremos usar; nosso novo tokenizador serÃ¡ exatamente igual ao GPT-2, e a Ãºnica coisa que irÃ¡ mudar Ã© o vocabulÃ¡rio, que serÃ¡ determinado pelo treinamento em nosso corpus. 

Primeiramente, vamos dar uma olhada em como o tokenizador trataria um exemplo de funÃ§Ã£o:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two',
 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '."', '""', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

O tokenizador possui alguns sÃ­mbolos especiais, como `Ä ` e `ÄŠ`, que denotam espaÃ§os e novas linhas, respectivamente. Como podemos observar, isso nÃ£o Ã© tÃ£o eficiente: o tokenizador retorna tokens individuais para cada espaÃ§o, quando poderia agrupar nÃ­veis de indentaÃ§Ã£o (jÃ¡ que ter conjuntos de quatro ou oito espaÃ§os serÃ¡ muito comum no cÃ³digo). O tokenizador tambÃ©m dividiu o nome da funÃ§Ã£o de uma forma um pouco estranha, nÃ£o sendo usado para ver palavras com o caractere `_`.

Vamos treinar um novo tokenizador e ver se isso resolve esses problemas. Para isso, iremos utilizar o mÃ©todo `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

Este comando pode demorar um pouco se o seu corpus for muito grande, mas para este dataset contendo 1.6 GB de textos Ã© extremamente rÃ¡pido (1 minuto e 16 segundos em uma CPU AMD Ryzen 9 3900X com 12 nÃºcleos). 

Observe que `AutoTokenizer.train_new_from_iterator()` funciona apenas se o tokenizador que vocÃª estiver usando Ã© um tokenizador "rÃ¡pido". Como vocÃª verÃ¡ na prÃ³xima seÃ§Ã£o, a biblioteca ğŸ¤— Transformers contÃ©m dois tipos de tokenizers: alguns sÃ£o escritos puramente em Python e outros (os mais rÃ¡pidos) sÃ£o apoiados pela biblioteca ğŸ¤— Tokenizers, que Ã© escrita na linguagem de programaÃ§Ã£o [Rust](https://www.rust-lang.org). Python Ã© a linguagem de programaÃ§Ã£o mais utilizada para CiÃªncia de Dados e aplicaÃ§Ãµes em Deep Learning, mas quando algo precisa ser paralelizado para ser rÃ¡pido, Ã© preciso ser escrito em uma outra linguagem. Por exemplo, as multiplicaÃ§Ãµes de matrizes que estÃ£o na base de modelos de computaÃ§Ã£o sÃ£o escritos em CUDA, uma biblioteca em C otimizada para GPUs. 

Treinar um tokenizador totalmente novo usando apenas Python seria terrivelmente lento, e Ã© por isso que nÃ³s desenvolvemos a biblioteca ğŸ¤— Tokenizers. Observe que, assim como vocÃª nÃ£o precisou aprender a linguagem CUDA para ser capaz de executar seu modelo em um lote de entradas em uma GPU, vocÃª nÃ£o precisarÃ¡ aprender Rust para usar o tokenizador rÃ¡pido. A biblioteca ğŸ¤— Tokenizers fornece ligaÃ§Ã§Ãµes para muitos mÃ©todos que internamente chamam algum trecho de cÃ³digo em Rust; por exemplo, para paralelizar o treinamento do seu novo tokenizador ou, como vimos no [Chapter 3](/course/chapter3), a tokenizaÃ§Ã£o de um lote de entradas.

A maioria dos modelos Transformer possui um tokenizador rÃ¡pido disponÃ­vel (existem algumas exceÃ§Ãµes que vocÃª pode checar [aqui](https://huggingface.co/transformers/#supported-frameworks)), e a API `AutoTokenizer` sempre seleciona o tokenizador rÃ¡pido para vocÃª se estiver disponÃ­vel. Na prÃ³xima seÃ§Ã£o, veremos alguns dos outros recursos especiais que os tokenizers rÃ¡pidos possuem, que serÃ£o realmente Ãºteis para tarefas como classificaÃ§Ã£o de tokens e resposta a perguntas. Antes de aprofundarmos nisso, no entanto, vamos experimentar o nosso novo tokenizador no exemplo anterior:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `',
 'a', '`', 'Ä and', 'Ä `', 'b', '`."""', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

Aqui vemos novamente os sÃ­mbolos especiais `Ä ` and `ÄŠ` que denotam espaÃ§os e novas linhas, mas tambÃ©m podemos observar que o nosso tokenizador aprendeu alguns tokens que sÃ£o altamente especÃ­ficos em um corpus de funÃ§Ãµes em Python: por exemplo, existe um token `ÄŠÄ Ä Ä ` que representa uma indentaÃ§Ã£o, e um token `Ä """` que representa as trÃªs aspas que comeÃ§am uma docstring. O tokenizador tambÃ©m divide corretamente o nome da funÃ§Ã£o em `_`. Esta Ã© uma representaÃ§Ã£o bastante compacta; comparativamente, usando o tokenizador em inglÃªs no mesmo exemplo nos darÃ¡ uma frase mais longa:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```
Vejamos outro exemplo:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',',
 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',
 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']
```

AlÃ©m do token correpondente a uma indentaÃ§Ã£o, aqui podemos ver um token para uma indentaÃ§Ã£o dupla: `ÄŠÄ Ä Ä Ä Ä Ä Ä `. Palavras especiais em Python, como `class`, `init`, `call`, `self`, e `return` sÃ£o tokenizadas como um token, e podemos ver que alÃ©m de dividir em `_` e `.`, o tokenizador divide corretamente atÃ© mesmo nomes em CamelCase: `LinearLayer` Ã© tokenizado como `["Ä Linear", "Layer"]`

## Salvando o tokenizador

Para garantir que podemos usÃ¡-lo mais tarde, precisamos salvar nosso novo tokenizador. Assim como Ã© utilizado para modelos, isso Ã© feito com o mÃ©todo `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```
Isso irÃ¡ criar uma nova pasta chamada *code-search-net-tokenizer*, que irÃ¡ conter todos os arquivos que o tokenizador precisa para ser recarregado. Se vocÃª quiser compartilhar este tokenizador com outros colegas e amigos, vocÃª pode carregÃ¡-lo no Hub fazendo login em sua conta. Se vocÃª estiver trabalhando em um notebook, hÃ¡ uma funÃ§Ã£o conveniente para ajudÃ¡-lo com isso:

```python
from huggingface_hub import notebook_login

notebook_login()
```
Isso exibirÃ¡ um widget onde vocÃª pode inserir suas credenciais de login do Hugging Face. Se vocÃª nÃ£o estiver trabalhando em um notebook, basta digitar a seguinte linha em seu terminal:

```bash
huggingface-cli login
```

Depois de vocÃª logar, vocÃª pode enviar seu tokenizador executando o seguinte comando: 

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

Isso criarÃ¡ um novo repositÃ³rio em seu namespace com o nome `code-search-net-tokenizer`, contendo o arquivo do tokenizador. VocÃª pode entÃ£o carregar o tokenizador de qualquer lugar com o mÃ©todo `from_pretrained()`:

```py
# Substitua "huggingface-course" abaixo pelo seu namespace real para usar seu prÃ³prio tokenizador
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

Agora vocÃª estÃ¡ pronto para treinar um modelo de linguagem do zero e ajustÃ¡-lo para sua tarefa! Chegaremos a isso no [Chapter 7](/course/chapter7), mas primeiro, no resto do capÃ­tulo daremos uma olhada sobre tokenizers rÃ¡pidos e explorar em detalhes o que realmente acontece quando chamamos o mÃ©todo `train_new_from_iterator()`.
