<FrameworkSwitchCourse {fw} />

# Os poderes especiais dos tokenizadores r√°pidos

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter6/section3_tf.ipynb"},
]} />

{/if}

Nesta se√ß√£o, examinaremos mais de perto os recursos dos tokenizadores em ü§ó Transformers. At√© agora, s√≥ os usamos para tokenizar entradas ou decodificar IDs de volta em texto, mas tokenizadores - especialmente aqueles apoiados pela biblioteca ü§ó Tokenizers - podem fazer muito mais. Para ilustrar esses recursos adicionais, exploraremos como reproduzir os resultados dos pipelines `token-classification` (que chamamos de `ner`) e `question-answering` que encontramos pela primeira vez no [Cap√≠tulo 1](/course/chapter1).
<Youtube id="g8quOxoqhHQ"/>

Na discuss√£o a seguir, muitas vezes faremos a distin√ß√£o entre tokenizadores "lentos" e "r√°pidos". Tokenizadores lentos s√£o aqueles escritos em Python dentro da biblioteca ü§ó Transformers, enquanto as vers√µes r√°pidas s√£o aquelas fornecidas por ü§ó Tokenizers, que s√£o escritos em Rust. Se voc√™ se lembrar da tabela do [Cap√≠tulo 5](/course/chapter5/3) que informava quanto tempo levou um tokenizador r√°pido e um lento para tokenizar o conjunto de dados de revis√£o de medicamentos, voc√™ deve ter uma ideia do motivo pelo qual os chamamos de r√°pido e lento:

                | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

‚ö†Ô∏è Ao tokenizar uma √∫nica frase, voc√™ nem sempre ver√° uma diferen√ßa de velocidade entre as vers√µes lenta e r√°pida do mesmo tokenizador. Na verdade, a vers√£o r√°pida pode ser mais lenta! √â somente ao tokenizar muitos textos em paralelo ao mesmo tempo que voc√™ poder√° ver a diferen√ßa com maior nitidez.
</Tip>

## Codifica√ß√£o em lote

<Youtube id="3umI3tm27Vw"/>

A sa√≠da de um tokenizador n√£o √© um simples dicion√°rio em Python; o que obtemos √©, na verdade, um objeto especial chamado `BatchEncoding`. Este objeto √© uma subclasse de um dicion√°rio (e √© por isso que conseguimos indexar esse resultado sem nenhum problema antes), mas com m√©todos adicionais que s√£o usados ‚Äã‚Äãprincipalmente por tokenizadores r√°pidos.

Al√©m de seus recursos de paraleliza√ß√£o, uma funcionalidade importante dos tokenizadores r√°pidos √© que eles sempre acompanham o intervalo original de textos dos quais os tokens finais v√™m - um recurso que chamamos de *mapeamento de offset*. Isso, por sua vez, desbloqueia recursos como o mapeamento de cada palavra para os tokens gerados ou mapeamento de cada caractere do texto original para o token que est√° dentro e vice-versa.

Vamos analisar um exemplo:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

Como mencionado anteriormente, n√≥s obtemos um objeto `BatchEncoding` na sa√≠da do tokenizador:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

Como a classe `AutoTokenizer` escolhe o tokenizador r√°pido como padr√£o, podemos usar os m√©todos adicionais que o objeto `BatchEncoding` fornece. Temos duas formas de verificar se o nosso tokenizador √© r√°pido ou lento. Podemos, por exemplo, avaliar o atributo `is_fast` do tokenizador:

```python
tokenizer.is_fast
```

```python out
True
```

ou checar o mesmo atributo do nosso `encoding`:

```python
encoding.is_fast
```

```python out
True
```

Vejamos o que um tokenizador r√°pido nos permite fazer. Primeiro, podemos acessar os tokens sem precisar converter os IDs de volta em tokens:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

No caso, o token no √≠ndice 5 √© `##yl`, que faz parte da palavra "Sylvain" na senten√ßa original. N√≥s podemos tamb√©m usar o metodo `words_ids()` para obter o √≠ndice da palavra de onde cada palavra vem:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

Podemos observar que as palavras especiais do tokenizador `[CLS]` e `[SEP]` s√£o mapeados para `None`, e ent√£o cada token √© mapeada para a palavra de onde se origina. Isso √© especialmente √∫til para determinar se um token est√° no in√≠cio da palavra ou se dois tokens est√£o em uma mesma palavra. Poder√≠amos contar com o prefix `##` para isso, mas apenas para tokenizadores do tipo BERT; este m√©todo funciona para qualquer tipo de tokenizador, desde que seja do tipo r√°pido. No pr√≥ximo cap√≠tulo, n√≥s veremos como podemos usar esse recurso para aplicar os r√≥tulos que temos para cada palavra adequadamente aos tokens em tarefas como reconhecimento de entidade nomeada (em ingl√™s, Named Entity Recognition, ou NER) e marca√ß√£o de parte da fala (em ingl√™s, part-of-speech, ou POS). Tamb√©m podemos us√°-lo para mascarar todos os tokens provenientes da mesma palavra na modelagem de linguagem mascarada (uma t√©cnica chamada _mascaramento da palavra inteira_)

<Tip>

A no√ß√£o do que √© uma palavra √© complicada. Por exemplo, "d'√°gua" (uma contra√ß√£o de "da √°gua") conta como uma ou duas palavras? Na verdade, depende do tokenizador e da opera√ß√£o de pr√©-tokeniza√ß√£o que √© aplicada. Alguns tokenizadores apenas dividem em espa√ßos, ent√£o eles considerar√£o isso como uma palavra. Outros usam pontua√ß√£o em cima dos espa√ßos, ent√£o considerar√£o duas palavras.

‚úèÔ∏è **Experimente!** Crie um tokenizador a partir dos checkpoints de `bert-base-cased `e `roberta-base` e tokenize "81s" com eles. O que voc√™ observa? Quais s√£o os IDs das palavras?

</Tip>

Da mesma forma, existe um m√©todo `sentence_ids()` que podemos usar para mapear um token para a senten√ßa de onde veio (embora, neste caso, o `token_type_ids` retornado pelo tokenizador possa nos dar a mesma informa√ß√£o).

Por fim, podemos mapear qualquer palavra ou token para caracteres no texto original (e vice-versa) atrav√©s dos m√©todos `word_to_chars()` ou `token_to_chars()` e `char_to_word()` ou `char_to_token()`. Por exemplo, o m√©todo `word_ids()` nos diz que `##yl` √© parte da palavra no √≠ndice 3, mas qual palavra est√° na frase? Podemos descobrir da seguinte forma:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

Como mencionamos anteriormente, isso √© apoiado pelo fato de que o tokenizador r√°pido acompanha o intervalo de texto de cada token em uma lista de *offsets*. Para ilustrar seu uso, mostraremos a seguir como replicar manualmente os resultados do pipeline `token-classification`. 

<Tip>

‚úèÔ∏è **Experimente!** Crie seu pr√≥prio texto de exemplo e veja se voc√™ consegue entender quais tokens est√£o associados ao ID da palavra e tamb√©m como extrair os intervalos de caracteres para uma √∫nica palavra. Como b√¥nus, tente usar duas frases como entrada e veja se os IDs das frases fazem sentido para voc√™.

</Tip>

## Dentro do pipeline `token-classification`

No [Cap√≠tulo 1](/course/chapter1) tivemos o primeiro gosto de aplicar o NER -- onde a tarefa √© identificar quais partes do texto correspondem a entidades como pessoas, locais ou organiza√ß√µes -- com a fun√ß√£o do ü§ó Transformers `pipeline()`. Ent√£o, no [Cap√≠tulo 2](/course/chapter2), vimos como um pipeline agrupa os tr√™s est√°gios necess√°rios para obter as previs√µes de um texto: tokeniza√ß√£o, passagem das entradas pelo modelo e p√≥s-processamento. As duas primeiras etapas do pipeline `token-classification` s√£o as mesmas de qualquer outro pipeline, mas o p√≥s-processamento √© um pouco mais complexo -- vejamos como! 


{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### Obtendo os resultados b√°sicos com o pipeline 

Primeiro, vamos usar um pipeline de classifica√ß√£o de token para que possamos obter alguns resultados para comparar manualmente. O modelo usado por padr√£o √© [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); ele executa NER em frases:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

O modelo identificou corretamente cada token gerado por "Sylvain" como uma pessoa, cada token gerado por "Hugging Face" como uma organiza√ß√£o e o token "Brooklyn" como um local. Tamb√©m podemos pedir ao pipeline para agrupar os tokens que correspondem √† mesma entidade:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

O par√¢metro `aggregation_strategy` escolhido mudar√° as pontua√ß√µes calculadas para cada entidade agrupada. Com o valor `"simple"`, a pontua√ß√£o √© apenas a m√©dia das pontua√ß√µes de cada token na entidade dada: por exemplo, a pontua√ß√£o de "Sylvain" √© a m√©dia das pontua√ß√µes que vimos no exemplo anterior para os tokens `S`, `##yl`, `##va`, e `##in`. Outras estrat√©gias dispon√≠veis s√£o: 

- `"first"`, onde a pontua√ß√£o de cada entidade √© a pontua√ß√£o do primeiro token dessa entidade (portanto, para "Sylvain" seria 0.993828, a pontua√ß√£o do token `S`)
- `"max"`, onde a pontua√ß√£o de cada entidade √© a pontua√ß√£o m√°xima dos tokens naquela entidade (portanto, para "Hugging Face" seria 0.98879766, a pontua√ß√£o do token `"Face"`)
- `"average"`, onde a pontua√ß√£o de cada entidade √© a m√©dia das pontua√ß√µes das palavras que comp√µem aquela entidade (assim para "Sylvain" n√£o haveria diferen√ßa da estrat√©gia `"simple"`, mas `"Hugging Face"` teria uma pontua√ß√£o de 0.9819, a m√©dia das pontua√ß√µes para `"Hugging"`, 0.975, e `"Face"`, 0.98879)

Agora vejamos como obter esses resultados sem usar a fun√ß√£o `pipeline()`!

### Das entradas √†s previs√µes

{#if fw === 'pt'}

Primeiro, precisamos tokenizar nossa entrada e pass√°-la pelo modelo. Isso √© feito exatamente como no [Cap√≠tulo 2](/course/chapter3); instanciamos o tokenizador e o modelo usando as classes `AutoXxx` e depois as usamos em nosso exemplo:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

Como estamos usando `AutoModelForTokenClassification` neste caso, obtemos um conjunto de logits para cada token na sequ√™ncia de entrada:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

Primeiro, precisamos tokenizar nossa entrada e pass√°-la pelo modelo. Isso √© feito exatamente como no [Cap√≠tulo 2](/course/chapter2); instanciamos o tokenizador e o modelo usando as classes `TFAutoXxx` e depois as usamos em nosso exemplo:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

Como estamos usando `TFAutoModelForTokenClassification` neste caso, obtemos um conjunto de logits para cada token na sequ√™ncia de entrada:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

Temos um lote com 1 sequ√™ncia de 19 tokens e o modelo tem 9 r√≥tulos diferentes, ent√£o a sa√≠da do modelo tem um tamanho de 1 x 19 x 9. Assim como para o pipeline de classifica√ß√£o de texto, usamos uma fun√ß√£o softmax para converter esses logits para probabilidades, e pegamos o argmax para obter previs√µes (note que podemos pegar o argmax nos logits porque o softmax n√£o altera a ordem): 

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

O atributo `model.config.id2label` cont√©m o mapeamento de √≠ndices para r√≥tulos que podemos usar para entender as previs√µes:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

Como vimos anteriormente, existem 9 r√≥tulos: `O` √© o r√≥tulo para os tokens que n√£o est√£o em nenhuma entidade nomeada, e ent√£o temos dois r√≥tulos para cada tipo de entidade (miscel√¢nia, pessoa, organiza√ß√£o e localiza√ß√£o). O r√≥tulo `B-XXX` indica que o token est√° no in√≠cio de uma entidade `XXX` e o r√≥tulo `I-XXX` indica que o token est√° dentro da entidade `XXX`. No caso do exemplo atual, esperar√≠amos que o nosso modelo classificasse o token `S` como `B-PER` (in√≠cio de uma entidade pessoa) e os tokens `##yl`, `##va` e `##in` como `I-PER` (dentro da entidade pessoa).

Voc√™ pode pensar que o modelo estava errado neste caso, pois deu o r√≥tulo `I-PER` a todos esses quatro tokens, mas isso n√£o √© totalmente verdade. Na realidade, existem dois formatos para esses r√≥tulos: `B-` e `I-`: *IOB1* e *IOB2*. O formato IOB2 (em rosa abaixo), √© o que introduzimos, enquanto que no formato IOB1 (em azul), os r√≥tulos que come√ßam com `B-` s√£o usados apenas para separar duas entidades adjacentes do mesmo tipo. O modelo que estamos usando foi ajustado em um conjunto de dados usando esse formato, e √© por isso que ele atribui o r√≥tulo `I-PER` ao token `S`. 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

Com este mapa, estamos prontos para reproduzir (quase inteiramente) os resultados do primeiro pipeline -- podemos apenas pegar a pontua√ß√£o e o r√≥tulo de cada token que n√£o foi classificado como `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

Isso √© muito parecido com o que t√≠nhamos antes, com uma exce√ß√£o: o pipeline tamb√©m nos dava informa√ß√µes sobre o `start` e `end` de cada entidade na frase original. √â aqui que nosso mapeamento de offset entrar√° em a√ß√£o. Para obter tais offsets, basta definir `return_offsets_mapping=True` quando aplicamos o tokenizador √†s nossas entradas:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

Cada tupla √© o intervalo de texto correspondente a cada token, onde `(0, 0)` √© reservado para os tokens especiais. Vimos antes que o token no √≠ndice 5 √© `##yl`, que tem `(12, 14)` como offset aqui. Se pegarmos a fatia correspondente em nosso exemplo:

```py
example[12:14]
```

obtemos o intervalo adequado de texto sem o `##`:

```python out
yl
```

Usando isso, agora podemos completar os resultados anteriores:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Este √© o mesmo resultado que obtivemos no primeiro pipeline!

### Agrupando entidades 

Usar os offsets para determinar as chaves inicial e final de cada entidade √© √∫til, mas essa informa√ß√£o n√£o √© estritamente necess√°ria. Quando queremos agrupar as entidades, no entanto, os offsets nos poupar√£o muito c√≥digo confuso. Por exemplo, se quisermos agrupar os tokens `Hu`, `##gging` e `Face`, podemos fazer regras especiais que digam que os dois primeiros devem ser anexados e removido o `##`, e o `Face` deve ser adicionado com um espa√ßo, pois n√£o come√ßa com `##` -- mas isso s√≥ funcionaria para esse tipo espec√≠fico de tokenizador. Ter√≠amos que escrever outro conjunto de regras para um tokenizador SentencePiece ou Byte-Pair-Encoding (discutido mais adiante neste cap√≠tulo).

Com os offsets, todo esse c√≥digo personalizado desaparece: podemos apenas pegar o intervalo no texto original que come√ßa com o primeiro token e termina com o √∫ltimo token. Ent√£o, no caso dos tokens `Hu`, `##ging` e `Face`, devemos come√ßar no caractere 33 (o in√≠cio de `Hu`) e terminar antes do caractere 45 (o final de `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

Para escrever o c√≥digo para o p√≥s-processamento das previs√µes ao agrupar entidades, agruparemos entidades consecutivas e rotuladas com `I-XXX`, excento a primeira, que pode ser rotulada como `B-XXX` ou `I-XXX` (portanto, paramos de agrupar uma entidade quando obtemos um `O`, um novo tipo de entidade ou um `B-XXX` que nos informa que uma entidade do mesmo tipo est√° iniciando):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Removendo o B- ou I-
        label = label[2:]
        start, _ = offsets[idx]

        # Vamos pegar todos os tokens rotulados com I-
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # A pontua√ß√£o √© a m√©dia de todas as pontua√ß√µes dos tokens da entidade agrupada
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

E obtemos os mesmos resultados do nosso segundo pipeline!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Outro exemplo de uma tarefa onde esses offsets s√£o extremamente √∫teis √© a resposta a perguntas. O conhecimento deste pipeline, que faremos na pr√≥xima se√ß√£o, tamb√©m nos permitir√° dar uma olhada em um √∫ltimo recurso dos tokenizadores na biblioteca ü§ó Transformers: lidar com tokens em excesso quando truncamos uma entrada em um determinado comprimento.
