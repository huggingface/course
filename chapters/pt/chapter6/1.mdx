# Introdu√ß√£o

No [Cap√≠tulo 3](/course/chapter3), n√≥s estudamos como realizar o ajuste fino em um modelo para uma dada tarefa. QUando n√≥s fazemos isso, usamos o mesmo tokenizer utilizado pelo modelo pr√©-treinado -- mas o que podemos fazer quando queremos treinar um modelo do in√≠cio? Nestes casos, utilizar um tokenizer que foi pr√©-treinado em um corpus de outro dom√≠nio ou linguagem √© tipicamente sub√≥timo. Por exemplo, um tokenizer que √© treinado em um corpus de lingua inglesa ter√° um desempenho ruim em um corpus de textos em japon√™s, visto que o uso de espa√ßos e pontua√ß√µes √© muito diferente nestas duas linguages.

Neste cap√≠tulo, voc√™ aprender√° como treinar um novo tokenizer em um corpus de textos, para ent√£o ser usado para treinar um modelo de linguagem. Isto tudo ser√° feito com ajuda da biblioteca [ü§ó Tokenizers](https://github.com/huggingface/tokenizers), que prov√™ o tokenizer r√°pido na biblioteca [ü§ó Transformers](https://github.com/huggingface/transformers). Daremos uma olhada a fundo sobre as funcionalidades oferecidas pela biblioteca, e explicar como o tokenizer r√°pido diferente de vers√µes "lentas".

Os t√≥picos que iremos cobrir incluem:

* Como treinar um novo tokenizer semelhante ao usado por um determinado checkpoint em um novo corpus de textos
* Os recursos especiais dos tokenizers r√°pidos
* As diferen√ßas entre os tr√™s principais algoritmos de tokeniza√ß√£o de subpalavras usados ‚Äã‚Äãna PNL hoje
* Como construir um tokenizer do zero com a biblioteca ü§ó Tokenizers e trein√°-lo em alguns dados

As t√©cnicas introduzidas neste cap√≠tulo ir√£o te preparar para a se√ß√£o no [Chapter 7](/course/chapter7/6) onde iremos analisar a cria√ß√£o de um modelo de linguagem para a linguagem Python. Primeiramente, vamos come√ßar analisando o que significa "treinar" um tokenizer.
