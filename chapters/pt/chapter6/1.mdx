# Introdu√ß√£o

No [Cap√≠tulo 3](/course/chapter3), n√≥s estudamos como realizar o ajuste fino em um modelo para uma dada tarefa. Quando n√≥s fazemos isso, usamos o mesmo tokenizador utilizado pelo modelo pr√©-treinado -- mas o que podemos fazer quando queremos treinar um modelo do in√≠cio? Nestes casos, utilizar um tokenizador que foi pr√©-treinado em um corpus de outro dom√≠nio ou linguagem √© tipicamente sub√≥timo. Por exemplo, um tokenizador que √© treinado em um corpus de lingua inglesa ter√° um desempenho ruim em um corpus de textos em japon√™s, visto que o uso de espa√ßos e pontua√ß√µes √© muito diferente nestes dois idiomas.

Neste cap√≠tulo, voc√™ aprender√° como treinar um novo tokenizador em um corpus de textos, para ent√£o ser usado no treinamento de um modelo de linguagem. Isto tudo ser√° feito com ajuda da biblioteca [ü§ó Tokenizers](https://github.com/huggingface/tokenizers), que prov√™ o tokenizador r√°pido na biblioteca [ü§ó Transformers](https://github.com/huggingface/transformers). Daremos uma olhada a fundo sobre as funcionalidades oferecidas pela biblioteca, e explorar como os tokenizadores r√°pidos diferem das vers√µes "lentas".

Os t√≥picos que iremos cobrir incluem:

* Como treinar um novo tokenizador semelhante ao usado por um determinado checkpoint em um novo corpus de textos
* Os recursos especiais dos tokenizadores r√°pidos
* As diferen√ßas entre os tr√™s principais algoritmos de tokeniza√ß√£o de subpalavras usados ‚Äã‚Äãno processamento de linguagem natural hoje
* Como construir um tokenizador do zero com a biblioteca ü§ó Tokenizers e trein√°-lo em alguns dados

As t√©cnicas introduzidas neste cap√≠tulo ir√£o te preparar para a se√ß√£o no [Cap√≠tulo 7](/course/chapter7/6) onde iremos analisar a cria√ß√£o de um modelo de linguagem para a linguagem Python. Primeiramente, vamos come√ßar analisando o que significa "treinar" um tokenizador.
