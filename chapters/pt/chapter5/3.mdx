# Hora de fatiar e dividir os dados

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter5/section3.ipynb"},
]} />

Na maioria das vezes, os dados com os quais voc√™ trabalha n√£o estar√£o perfeitamente preparados para treinamento de modelos. Nesta se√ß√£o vamos explorar as v√°rias caracter√≠sticas que o ü§ó Datasets fornece para limpar seus conjuntos de dados.

<Youtube id="tqfSFcPMgOI"/>

## Slicing and dicing our data

Semelhante ao Pandas, ü§ó Datasets fornece v√°rias fun√ß√µes para manipular o conte√∫do dos objetos `Dataset` e `DatasetDict`. J√° encontramos o m√©todo `Dataset.map()` no [Cap√≠tulo 3](/course/chapter3), e nesta se√ß√£o vamos explorar algumas das outras fun√ß√µes √† nossa disposi√ß√£o.

Para este exemplo, usaremos o [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) que est√° hospedado na [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), que cont√©m avalia√ß√µes de pacientes sobre v√°rios medicamentos, juntamente com a condi√ß√£o a ser tratada e uma classifica√ß√£o de 10 estrelas da satisfa√ß√£o do paciente.

Primeiro precisamos baixar e extrair os dados, o que pode ser feito com os comandos `wget` e `unzip`:

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

Como o TSV √© apenas uma variante do CSV que usa tabula√ß√µes em vez de v√≠rgulas como separador, podemos carregar esses arquivos usando o script de carregamento `csv` e especificando o argumento `delimiter` na fun√ß√£o `load_dataset()` da seguinte forma:

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t is the tab character in Python
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

Uma boa pr√°tica ao fazer qualquer tipo de an√°lise de dados √© pegar uma pequena amostra aleat√≥ria para ter uma ideia r√°pida do tipo de dados com os quais voc√™ est√° trabalhando. Em ü§ó Datasets, podemos criar uma amostra aleat√≥ria encadeando as fun√ß√µes `Dataset.shuffle()` e `Dataset.select()` juntas:

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# Peek at the first few examples
drug_sample[:3]
```

```python out
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

Observe que corrigimos a seed em `Dataset.shuffle()` para fins de reprodutibilidade. `Dataset.select()` espera um iter√°vel de √≠ndices, ent√£o passamos `range(1000)` para pegar os primeiros 1.000 exemplos do conjunto de dados embaralhado. A partir desta amostra j√° podemos ver algumas peculiaridades em nosso conjunto de dados:

* A coluna  `Unnamed: 0` se parece com um ID an√¥nimo para cada paciente.
* A coluna `condition` inclui uma combina√ß√£o de r√≥tulos em mai√∫sculas e min√∫sculas.
* As revis√µes s√£o de tamanho vari√°vel e cont√™m uma mistura de separadores de linha Python (`\r\n`), bem como c√≥digos de caracteres HTML como `&\#039;`.

Vamos ver como podemos usar ü§ó Datasets para lidar com cada um desses problemas. Para testar a hip√≥tese de ID do paciente para a coluna `Unnamed: 0`, podemos usar a fun√ß√£o `Dataset.unique()` para verificar se o n√∫mero de IDs corresponde ao n√∫mero de linhas em cada divis√£o:

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

Isso parece confirmar nossa hip√≥tese, ent√£o vamos limpar um pouco o conjunto de dados renomeando a coluna `Unnamed: 0` para algo um pouco mais interpret√°vel. Podemos usar a fun√ß√£o `DatasetDict.rename_column()` para renomear a coluna em ambas as divis√µes de uma s√≥ vez:

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<Tip>

‚úèÔ∏è **Experimente!** Use a fun√ß√£o `Dataset.unique()` para encontrar o n√∫mero de medicamentos e condi√ß√µes exclusivos nos conjuntos de treinamento e teste.

</Tip>

Em seguida, vamos normalizar todos os r√≥tulos `condition` usando `Dataset.map()`. Como fizemos com a tokeniza√ß√£o no [Cap√≠tulo 3](/course/chapter3), podemos definir uma fun√ß√£o simples que pode ser aplicada em todas as linhas de cada divis√£o em `drug_dataset`:

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}


drug_dataset.map(lowercase_condition)
```

```python out
AttributeError: 'NoneType' object has no attribute 'lower'
```

Oh n√£o, tivemos um problema com nossa fun√ß√£o de mapa! A partir do erro, podemos inferir que algumas das entradas na coluna `condition` s√£o `None`, que n√£o podem ser min√∫sculas, pois n√£o s√£o strings. Vamos eliminar essas linhas usando `Dataset.filter()`, que funciona de maneira semelhante a `Dataset.map()` e espera uma fun√ß√£o que receba um √∫nico exemplo do conjunto de dados. Em vez de escrever uma fun√ß√£o expl√≠cita como:

```py
def filter_nones(x):
    return x["condition"] is not None
```

e ent√£o executando `drug_dataset.filter(filter_nones)`, podemos fazer isso em uma linha usando uma _fun√ß√£o lambda_. Em Python, fun√ß√µes lambda s√£o pequenas fun√ß√µes que voc√™ pode definir sem nome√°-las explicitamente. Eles assumem a forma geral:

```
lambda <arguments> : <expression>
```

onde `lambda` √© uma das [palavras-chave] especiais do Python (https://docs.python.org/3/reference/lexical_analysis.html#keywords), `<arguments>` √© uma lista/conjunto de valores separados por v√≠rgula que defina as entradas para a fun√ß√£o, e `<express√£o>` representa as opera√ß√µes que voc√™ deseja executar. Por exemplo, podemos definir uma fun√ß√£o lambda simples que eleva um n√∫mero ao quadrado da seguinte forma:

```
lambda x : x * x
```

Para aplicar esta fun√ß√£o a uma entrada, precisamos envolv√™-la e a entrada entre par√™nteses:

```py
(lambda x: x * x)(3)
```

```python out
9
```

Da mesma forma, podemos definir fun√ß√µes lambda com v√°rios argumentos, separando-os com v√≠rgulas. Por exemplo, podemos calcular a √°rea de um tri√¢ngulo da seguinte forma:

```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

As fun√ß√µes lambda s√£o √∫teis quando voc√™ deseja definir fun√ß√µes pequenas e de uso √∫nico (para obter mais informa√ß√µes sobre elas, recomendamos a leitura do excelente [tutorial do Real Python](https://realpython.com/python-lambda/) de Andre Burgaud). No contexto ü§ó Datasets, podemos usar fun√ß√µes lambda para definir opera√ß√µes simples de mapa e filtro, ent√£o vamos usar este truque para eliminar as entradas `None` em nosso conjunto de dados:

```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

Com as entradas `None` removidas, podemos normalizar nossa coluna `condition`:

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# Check that lowercasing worked
drug_dataset["train"]["condition"][:3]
```

```python out
['left ventricular dysfunction', 'adhd', 'birth control']
```

Funciona! Agora que limpamos os r√≥tulos, vamos dar uma olhada na limpeza dos pr√≥prios coment√°rios.

## Criando novas colunas

Sempre que estiver lidando com avalia√ß√µes de clientes, uma boa pr√°tica √© verificar o n√∫mero de palavras em cada avalia√ß√£o. Uma avalia√ß√£o pode ser apenas uma √∫nica palavra como "√ìtimo!" ou um ensaio completo com milhares de palavras e, dependendo do caso de uso, voc√™ precisar√° lidar com esses extremos de maneira diferente. Para calcular o n√∫mero de palavras em cada revis√£o, usaremos uma heur√≠stica aproximada baseada na divis√£o de cada texto por espa√ßos em branco.

Vamos definir uma fun√ß√£o simples que conta o n√∫mero de palavras em cada revis√£o:

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

Ao contr√°rio de nossa fun√ß√£o `lowercase_condition()`, `compute_review_length()` retorna um dicion√°rio cuja chave n√£o corresponde a um dos nomes de coluna no conjunto de dados. Nesse caso, quando `compute_review_length()` for passado para `Dataset.map()`, ele ser√° aplicado a todas as linhas do conjunto de dados para criar uma nova coluna `review_length`:

```py
drug_dataset = drug_dataset.map(compute_review_length)
# Inspect the first training example
drug_dataset["train"][0]
```

```python out
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

Como esperado, podemos ver que uma coluna `review_length` foi adicionada ao nosso conjunto de treinamento. Podemos classificar essa nova coluna com `Dataset.sort()` para ver como s√£o os valores extremos:

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python out
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

Como suspeit√°vamos, algumas revis√µes cont√™m apenas uma √∫nica palavra, que, embora possa ser boa para an√°lise de sentimentos, n√£o seria informativa se quisermos prever a condi√ß√£o.

<Tip>

üôã Uma forma alternativa de adicionar novas colunas a um conjunto de dados √© com a fun√ß√£o `Dataset.add_column()`. Isso permite que voc√™ forne√ßa a coluna como uma lista Python ou array NumPy e pode ser √∫til em situa√ß√µes em que `Dataset.map()` n√£o √© adequado para sua an√°lise.

</Tip>

Vamos usar a fun√ß√£o `Dataset.filter()` para remover coment√°rios que contenham menos de 30 palavras. Da mesma forma que fizemos com a coluna "condi√ß√£o", podemos filtrar as reviews muito curtas exigindo que as reviews tenham um comprimento acima desse limite.

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python out
{'train': 138514, 'test': 46108}
```

Como voc√™ pode ver, isso removeu cerca de 15% das avalia√ß√µes de nossos conjuntos de treinamento e teste originais.

<Tip>

‚úèÔ∏è **Experimente!** Use a fun√ß√£o `Dataset.sort()` para inspecionar as resenhas com o maior n√∫mero de palavras. Consulte a [documenta√ß√£o](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) para ver qual argumento voc√™ precisa usar para classificar as avalia√ß√µes por tamanho em ordem decrescente.

</Tip>

A √∫ltima coisa com a qual precisamos lidar √© a presen√ßa de c√≥digos de caracteres HTML em nossas an√°lises. Podemos usar o m√≥dulo `html` do Python para liberar esses caracteres, assim:

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python out
"I'm a transformer called BERT"
```

Usaremos `Dataset.map()` para liberar todos os caracteres HTML em nosso corpus:

```python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

Como voc√™ pode ver, o m√©todo `Dataset.map()` √© bastante √∫til para o processamento de dados -- e ainda nem arranhamos a superf√≠cie de tudo o que ele pode fazer!

## Os superpoderes do m√©todo `map()`

O m√©todo `Dataset.map()` recebe um argumento `batched` que, se definido como `True`, faz com que ele envie um batch de exemplos para a fun√ß√£o map de uma s√≥ vez (o tamanho do batch √© configur√°vel, mas o padr√£o √© 1.000). Por exemplo, a fun√ß√£o map anterior que n√£o escapou de todo o HTML demorou um pouco para ser executada (voc√™ pode ler o tempo gasto nas barras de progresso). Podemos acelerar isso processando v√°rios elementos ao mesmo tempo usando uma compreens√£o de lista.

Quando voc√™ especifica `batched=True` a fun√ß√£o recebe um dicion√°rio com os campos do conjunto de dados, mas cada valor agora √© uma _lista de valores_, e n√£o apenas um valor √∫nico. O valor de retorno de `Dataset.map()` deve ser o mesmo: um dicion√°rio com os campos que queremos atualizar ou adicionar ao nosso conjunto de dados e uma lista de valores. Por exemplo, aqui est√° outra maneira de fazer o scape de todos os caracteres HTML, mas usando `batched=True`:

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

Se voc√™ estiver executando esse c√≥digo em um jupyter notebook, ver√° que esse comando √© executado muito mais r√°pido que o anterior. E n√£o √© porque nossas revis√µes j√° foram sem escape em HTML -- se voc√™ reexecutar a instru√ß√£o da se√ß√£o anterior (sem `batched=True`), levar√° o mesmo tempo que antes. Isso ocorre porque as compreens√µes de lista geralmente s√£o mais r√°pidas do que executar o mesmo c√≥digo em um loop `for`, e tamb√©m ganhamos algum desempenho acessando muitos elementos ao mesmo tempo em vez de um por um.

Usar `Dataset.map()` com `batched=True` ser√° essencial para desbloquear a velocidade dos tokenizers "r√°pidos" que encontraremos no [Cap√≠tulo 6](/course/chapter6), que podem rapidamente tokenizar grandes listas de textos. Por exemplo, para tokenizar todas as an√°lises de medicamentos com um tokenizer r√°pido, poder√≠amos usar uma fun√ß√£o como esta:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

Como voc√™ viu no [Cap√≠tulo 3](/course/chapter3), podemos passar um ou v√°rios exemplos para o tokenizer, ent√£o podemos usar esta fun√ß√£o com ou sem `batched=True`. Vamos aproveitar esta oportunidade para comparar o desempenho das diferentes op√ß√µes. Em um notebook, voc√™ pode cronometrar uma instru√ß√£o de uma linha adicionando `%time` antes da linha de c√≥digo que deseja medir:

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

Voc√™ tamb√©m pode cronometrar uma c√©lula inteira colocando `%%time` no in√≠cio da c√©lula. No hardware em que executamos isso, ele mostrava 10,8s para esta instru√ß√£o (√© o n√∫mero escrito depois de "Wall time").

<Tip>

‚úèÔ∏è **Experimente!** Execute a mesma instru√ß√£o com e sem `batched=True`, ent√£o tente com um tokenizer lento (adicione `use_fast=False` no m√©todo `AutoTokenizer.from_pretrained()`) para que voc√™ possa veja quais n√∫meros voc√™ obt√©m em seu hardware.

</Tip>

Aqui est√£o os resultados que obtivemos com e sem batching, com um tokenizer r√°pido e lento:

Op√ß√µes          | Tokenizador r√°pido | Tokenizador lento
:--------------:|:------------------:|:-------------:
`batched=True`  | 10.8s              | 4min41s
`batched=False` | 59.2s              | 5min3s

Isso significa que usar um tokenizer r√°pido com a op√ß√£o `batched=True` √© 30 vezes mais r√°pido do que seu equivalente lento sem batching -- isso √© realmente incr√≠vel! Essa √© a principal raz√£o pela qual os tokenizers r√°pidos s√£o o padr√£o ao usar o `AutoTokenizer` (e porque eles s√£o chamados de "r√°pidos"). Eles s√£o capazes de alcan√ßar essa acelera√ß√£o porque nos bastidores o c√≥digo de tokeniza√ß√£o √© executado em Rust, que √© uma linguagem que facilita a execu√ß√£o de c√≥digo paralelizado.

A paraleliza√ß√£o tamb√©m √© a raz√£o para a acelera√ß√£o de quase 6x que o tokenizer r√°pido alcan√ßa com o batching: voc√™ n√£o pode paralelizar uma √∫nica opera√ß√£o de tokeniza√ß√£o, mas quando voc√™ deseja tokenizar muitos textos ao mesmo tempo, voc√™ pode simplesmente dividir a execu√ß√£o em v√°rios processos, cada um respons√°vel por seus pr√≥prios textos.

`Dataset.map()` tamb√©m possui alguns recursos de paraleliza√ß√£o pr√≥prios. Como eles n√£o s√£o suportados pelo Rust, eles n√£o permitem que um tokenizer lento alcance um r√°pido, mas ainda podem ser √∫teis (especialmente se voc√™ estiver usando um tokenizer que n√£o possui uma vers√£o r√°pida). Para ativar o multiprocessamento, use o argumento `num_proc` e especifique o n√∫mero de processos a serem usados ‚Äã‚Äãem sua chamada para `Dataset.map()`:

```py
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

Voc√™ pode experimentar um pouco o tempo para determinar o n√∫mero ideal de processos a serem usados; no nosso caso, 8 pareceu produzir o melhor ganho de velocidade. Aqui est√£o os n√∫meros que obtivemos com e sem multiprocessamento:

Op√ß√µes                          | Tokenizador r√°pido | Tokenizador lento
:------------------------------:|:------------------:|:-------------:
`batched=True`                  | 10.8s              | 4min41s
`batched=False`                 | 59.2s              | 5min3s
`batched=True`, `num_proc=8`    | 6.52s              | 41.3s
`batched=False`, `num_proc=8`   | 9.49s              | 45.2s

Esses s√£o resultados muito mais razo√°veis ‚Äã‚Äãpara o tokenizer lento, mas o desempenho do tokenizer r√°pido tamb√©m foi substancialmente melhorado. Observe, no entanto, que nem sempre ser√° o caso -- para valores de `num_proc` diferentes de 8, nossos testes mostraram que era mais r√°pido usar `batched=True` sem essa op√ß√£o. Em geral, n√£o recomendamos o uso de multiprocessamento Python para tokenizers r√°pidos com `batched=True`.

<Tip>

Usar `num_proc` para acelerar seu processamento geralmente √© uma √≥tima id√©ia, desde que a fun√ß√£o que voc√™ est√° usando n√£o esteja fazendo algum tipo de multiprocessamento pr√≥prio.

</Tip>

Toda essa funcionalidade condensada em um √∫nico m√©todo j√° √© incr√≠vel, mas tem mais! Com `Dataset.map()` e `batched=True` voc√™ pode alterar o n√∫mero de elementos em seu conjunto de dados. Isso √© super √∫til em muitas situa√ß√µes em que voc√™ deseja criar v√°rios recursos de treinamento a partir de um exemplo, e precisaremos fazer isso como parte do pr√©-processamento de v√°rias das tarefas de PNL que realizaremos no [Cap√≠tulo 7](/course/chapter7).

<Tip>

üí° No aprendizado de m√°quina, um _exemplo_ geralmente √© definido como o conjunto de _recursos_ que alimentamos o modelo. Em alguns contextos, esses recursos ser√£o o conjunto de colunas em um `Dataset`, mas em outros (como aqui e para resposta a perguntas), v√°rios recursos podem ser extra√≠dos de um √∫nico exemplo e pertencer a uma √∫nica coluna.

</Tip>

Vamos dar uma olhada em como funciona! Aqui vamos tokenizar nossos exemplos e trunc√°-los para um comprimento m√°ximo de 128, mas pediremos ao tokenizer para retornar *todos* os peda√ßos dos textos em vez de apenas o primeiro. Isso pode ser feito com `return_overflowing_tokens=True`:
```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

Vamos testar isso em um exemplo antes de usar `Dataset.map()` em todo o conjunto de dados:

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python out
[128, 49]
```

Assim, nosso primeiro exemplo no conjunto de treinamento se tornou dois recursos porque foi tokenizado para mais do que o n√∫mero m√°ximo de tokens que especificamos: o primeiro de comprimento 128 e o segundo de comprimento 49. Agora vamos fazer isso para todos os elementos do conjunto de dados!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python out
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

Oh n√£o! Isso n√£o funcionou! Por que n√£o? Observar a mensagem de erro nos dar√° uma pista: h√° uma incompatibilidade nos comprimentos de uma das colunas, sendo uma de comprimento 1.463 e a outra de comprimento 1.000. Se voc√™ consultou a [documenta√ß√£o] do `Dataset.map()` (https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map), voc√™ deve se lembrar de que √© o n√∫mero de amostras passadas para a fun√ß√£o que estamos mapeando; aqui, esses 1.000 exemplos forneceram 1.463 novos recursos, resultando em um erro de forma.

O problema √© que estamos tentando misturar dois conjuntos de dados diferentes de tamanhos diferentes: as colunas `drug_dataset` ter√£o um certo n√∫mero de exemplos (os 1.000 em nosso erro), mas o `tokenized_dataset` que estamos construindo ter√° mais (o 1.463 na mensagem de erro). Isso n√£o funciona para um `Dataset`, portanto, precisamos remover as colunas do conjunto de dados antigo ou torn√°-las do mesmo tamanho do novo conjunto de dados. Podemos fazer o primeiro com o argumento `remove_columns`:

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

Agora isso funciona sem erro. Podemos verificar que nosso novo conjunto de dados tem muito mais elementos do que o conjunto de dados original comparando os comprimentos:

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

Mencionamos que tamb√©m podemos lidar com o problema de comprimento incompat√≠vel tornando as colunas antigas do mesmo tamanho das novas. Para fazer isso, precisaremos do campo `overflow_to_sample_mapping` que o tokenizer retorna quando configuramos `return_overflowing_tokens=True`. Ele nos fornece um mapeamento de um novo √≠ndice de recurso para o √≠ndice da amostra da qual ele se originou. Usando isso, podemos associar cada chave presente em nosso conjunto de dados original a uma lista de valores do tamanho certo, repetindo os valores de cada exemplo quantas vezes ele gerar novos recursos:

```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extract mapping between new and old indices
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

Podemos ver que funciona com `Dataset.map()` sem precisarmos remover as colunas antigas:

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

Obtemos o mesmo n√∫mero de recursos de treinamento de antes, mas aqui mantivemos todos os campos antigos. Se voc√™ precisar deles para algum p√≥s-processamento ap√≥s aplicar seu modelo, conv√©m usar essa abordagem.

Agora voc√™ viu como ü§ó Datasets podem ser usados ‚Äã‚Äãpara pr√©-processar um conjunto de dados de v√°rias maneiras. Embora as fun√ß√µes de processamento de ü§ó Datasets cubram a maioria das suas necessidades de treinamento de modelo, pode haver momentos em que voc√™ precisar√° mudar para o Pandas para acessar recursos mais poderosos, como `DataFrame.groupby()` ou APIs de alto n√≠vel para visualiza√ß√£o. Felizmente, ü§ó Datasets foi projetado para ser interoper√°vel com bibliotecas como Pandas, NumPy, PyTorch, TensorFlow e JAX. Vamos dar uma olhada em como isso funciona.

## De `Dataset`s para `DataFrame`s e vice-versa

<Youtube id="tfcY1067A5Q"/>

Para habilitar a convers√£o entre v√°rias bibliotecas de terceiros, ü§ó Datasets fornece uma fun√ß√£o `Dataset.set_format()`. Essa fun√ß√£o altera apenas o _formato de sa√≠da_ do conjunto de dados, para que voc√™ possa alternar facilmente para outro formato sem afetar o _formato de dados_ subjacente, que √© o Apache Arrow. A formata√ß√£o √© feita no local. Para demonstrar, vamos converter nosso conjunto de dados para Pandas:

```py
drug_dataset.set_format("pandas")
```

Agora, quando acessamos os elementos do dataset, obtemos um `pandas.DataFrame` em vez de um dicion√°rio:

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

Vamos criar um `pandas.DataFrame` para todo o conjunto de treinamento selecionando todos os elementos de `drug_dataset["train"]`:

```py
train_df = drug_dataset["train"][:]
```

<Tip>

üö® `Dataset.set_format()` altera o formato de retorno para o m√©todo dunder `__getitem__()` do conjunto de dados. Isso significa que quando queremos criar um novo objeto como `train_df` a partir de um `Dataset` no formato `"pandas"`, precisamos dividir todo o conjunto de dados para obter um `pandas.DataFrame`. Voc√™ pode verificar por si mesmo que o tipo de `drug_dataset["train"]` √© `Dataset`, independentemente do formato de sa√≠da.

</Tip>


A partir daqui, podemos usar todas as funcionalidades do Pandas que queremos. Por exemplo, podemos fazer um encadeamento sofisticado para calcular a distribui√ß√£o de classes entre as entradas `condition`:

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>


E uma vez que terminamos nossa an√°lise de Pandas, sempre podemos criar um novo objeto `Dataset` usando a fun√ß√£o `Dataset.from_pandas()` da seguinte forma:


```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python out
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<Tip>

‚úèÔ∏è **Experimente!** Calcule a classifica√ß√£o m√©dia por medicamento e armazene o resultado em um novo `Dataset`.

</Tip>

Isso encerra nosso tour pelas v√°rias t√©cnicas de pr√©-processamento dispon√≠veis em ü§ó Datasets. Para completar a se√ß√£o, vamos criar um conjunto de valida√ß√£o para preparar o conjunto de dados para treinar um classificador. Antes de fazer isso, vamos redefinir o formato de sa√≠da de `drug_dataset` de `"pandas"` para `"arrow"`:

```python
drug_dataset.reset_format()
```

## Criando um conjunto de valida√ß√£o

Embora tenhamos um conjunto de teste que poder√≠amos usar para avalia√ß√£o, √© uma boa pr√°tica deixar o conjunto de teste intocado e criar um conjunto de valida√ß√£o separado durante o desenvolvimento. Quando estiver satisfeito com o desempenho de seus modelos no conjunto de valida√ß√£o, voc√™ poder√° fazer uma verifica√ß√£o final de sanidade no conjunto de teste. Esse processo ajuda a mitigar o risco de voc√™ se ajustar demais ao conjunto de teste e implantar um modelo que falha em dados do mundo real.

ü§ó Datasets fornece uma fun√ß√£o `Dataset.train_test_split()` que √© baseada na famosa funcionalidade do `scikit-learn`. Vamos us√°-lo para dividir nosso conjunto de treinamento em divis√µes `train` e `validation` (definimos o argumento `seed` para reprodutibilidade):

```py
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# Rename the default "test" split to "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# Add the "test" set to our `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

√ìtimo, agora preparamos um conjunto de dados pronto para treinar alguns modelos! Na [se√ß√£o 5](/course/chapter5/5), mostraremos como fazer upload de conjuntos de dados para o Hugging Face Hub, mas, por enquanto, vamos encerrar nossa an√°lise analisando algumas maneiras de salvar conjuntos de dados em sua m√°quina local .

## Salvando um conjunto de dados

<Youtube id="blF9uxYcKHo"/>

Embora ü§ó Datasets armazene em cache todos os conjuntos de dados baixados e as opera√ß√µes realizadas nele, h√° momentos em que voc√™ deseja salvar um conjunto de dados em disco (por exemplo, caso o cache seja exclu√≠do). Conforme mostrado na tabela abaixo, ü§ó Datasets fornece tr√™s fun√ß√µes principais para salvar seu conjunto de dados em diferentes formatos:

| Formato dos dados | Fun√ß√£o |
| :---------: | :-----------------------------------: |
|    Arrow    | `Dataset.save_to_disk()` |
|     CSV     |    `Dataset.to_csv()`    |
|    JSON     |   `Dataset.to_json()`    |

Por exemplo, vamos salvar nosso conjunto de dados limpo no formato Arrow:

```py
drug_dataset_clean.save_to_disk("drug-reviews")
```

Isso criar√° um diret√≥rio com a seguinte estrutura:

```
drug-reviews/
‚îú‚îÄ‚îÄ dataset_dict.json
‚îú‚îÄ‚îÄ test
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îú‚îÄ‚îÄ train
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îú‚îÄ‚îÄ indices.arrow
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îî‚îÄ‚îÄ validation
    ‚îú‚îÄ‚îÄ dataset.arrow
    ‚îú‚îÄ‚îÄ dataset_info.json
    ‚îú‚îÄ‚îÄ indices.arrow
    ‚îî‚îÄ‚îÄ state.json
```

onde podemos ver que cada divis√£o est√° associada a sua pr√≥pria tabela *dataset.arrow* e alguns metadados em *dataset_info.json* e *state.json*. Voc√™ pode pensar no formato Arrow como uma tabela sofisticada de colunas e linhas otimizada para criar aplicativos de alto desempenho que processam e transportam grandes conjuntos de dados.

Uma vez que o conjunto de dados √© salvo, podemos carreg√°-lo usando a fun√ß√£o `load_from_disk()` da seguinte forma:

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

Para os formatos CSV e JSON, temos que armazenar cada divis√£o como um arquivo separado. Uma maneira de fazer isso √© iterando as chaves e os valores no objeto `DatasetDict`:

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

Isso salva cada divis√£o em [formato de linhas JSON](https://jsonlines.org), em que cada linha no conjunto de dados √© armazenada como uma √∫nica linha de JSON. Veja como √© o primeiro exemplo:

```py
!head -n 1 drug-reviews-train.jsonl
```

```python out
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

Podemos ent√£o usar as t√©cnicas da [se√ß√£o 2](/course/chapter5/2) para carregar os arquivos JSON da seguinte forma:

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

E √© isso para nossa excurs√£o em dados com ü§ó Datasets! Agora que temos um conjunto de dados limpo para treinar um modelo, aqui est√£o algumas ideias que voc√™ pode experimentar:

1. Use as t√©cnicas do [Cap√≠tulo 3](/course/chapter3) para treinar um classificador que possa prever a condi√ß√£o do paciente com base na revis√£o do medicamento.
2. Use o pipeline `summarization` do [Cap√≠tulo 1](/course/chapter1) para gerar resumos das revis√µes.

A seguir, veremos como ü§ó Datasets pode permitir que voc√™ trabalhe com grandes conjuntos de dados sem explodir seu laptop!
