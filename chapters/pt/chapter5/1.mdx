# IntroduÃ§Ã£o

No [CapÃ­tulo 3](/course/chapter3) vocÃª teve seu primeiro gostinho da biblioteca ğŸ¤— Datasets e viu que havia trÃªs passos principais quando se tratava de treinar para melhorar (fine-tuning) um modelo:

1. Carregar um conjunto de dados (dataset) do Hugging Face Hub.
2. PrÃ©-processar os dados com `Dataset.map()`.
3. Carregar e calcular as mÃ©tricas.

Mas isto estÃ¡ apenas arranhando a superfÃ­cie do que ğŸ¤— Dataset.map pode fazer! Neste capÃ­tulo, vamos dar um mergulho profundo na biblioteca. Ao longo do caminho, encontraremos respostas para as seguintes perguntas:

* O que vocÃª faz quando seu conjunto de dados nÃ£o estÃ¡ no Hub?
* Como vocÃª pode separar um conjunto de dados? (E se vocÃª _necessÃ¡rio_ usar Pandas?)
* O que vocÃª faz quando seu conjunto de dados Ã© enorme e derreterÃ¡ a RAM de seu laptop?
* O que diabos sÃ£o "mapeamento de memÃ³ria" e Apache Arrow?
* Como vocÃª pode criar seu prÃ³prio conjunto de dados e enviar para o Hub?

As tÃ©cnicas que vocÃª aprender aqui vÃ£o preparÃ¡-lo para as tarefas avanÃ§adas de tokenization e fine-tuning no [CapÃ­tulo 6](/course/chapter6) e [CapÃ­tulo 7](/course/chapter7) -- entÃ£o pegue um cafÃ© e vamos comeÃ§ar!

