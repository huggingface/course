# E se o meu dataset n√£o estiver no Hub?

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter5/section2.ipynb"},
]} />

Voc√™ sabe como usar o [Hugging Face Hub](https://huggingface.co/datasets) para baixar conjuntos de dados (**datasets**), mas muitas vezes voc√™ se encontrar√° trabalhando com dados que s√£o armazenados em seu laptop ou em um servidor remoto. Nesta se√ß√£o mostraremos como ü§ó Datasets podem ser usados para carregar conjuntos de dados que n√£o est√£o dispon√≠veis no Hugging Face Hub.

<Youtube id="HyQgpJTkRdE"/>

## Trabalhando com datasets locais e remotos


ü§ó Datasets fornece scripts de carregamento para lidar com o carregamento de conjuntos de dados locais e remotos. Ele suporta v√°rios formatos de dados comuns, como por exemplo:

|    Formato do dato  | script de carregamento |                         Exemplo                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Text files     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

Como mostrado na tabela, para cada formato de dados s√≥ precisamos especificar o tipo de script de carregamento na fun√ß√£o `load_dataset()`, junto com um argumento `data_files` que especifica o caminho para um ou mais arquivos. Vamos come√ßar carregando um conjunto de dados de arquivos locais; mais tarde veremos como fazer o mesmo com arquivos remotos.

## Carregando um conjunto de dados local

Para este exemplo usaremos o [SQuAD-it dataset] (https://github.com/crux82/squad-it/), que √© um conjunto de dados em grande escala para resposta a perguntas em italiano.

As divis√µes de treinamento e testes s√£o hospedadas no GitHub, para que possamos baix√°-las com um simples comando `wget`:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Isto ir√° baixar dois arquivos compactados chamados *SQuAD_it-train.json.gz* e *SQuAD_it-test.json.gz*, que podemos descomprimir com o comando Linux `gzip`:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

Podemos ver que os arquivos compactados foram substitu√≠dos por _SQuAD_it-train.json_ e _SQuAD_it-text.json_, e que os dados s√£o armazenados no formato JSON.

<Tip>

‚úé Se voc√™ est√° se perguntando por que h√° um `!` nos comandos shell acima, √© porque estamos executando-os dentro de um Jupyter notebook. Basta remover o prefixo se voc√™ quiser baixar e descompactar o conjunto de dados dentro de um terminal.
</Tip>

Para carregar um arquivo JSON com a fun√ß√£o `load_dataset()`, s√≥ precisamos saber se estamos lidando com o JSON comum (semelhante a um dicion√°rio aninhado) ou Linhas JSON (JSON line-separated JSON). Como muitos conjuntos de dados que respondem a perguntas, o SQuAD utiliza o formato aninhado, com todo o texto armazenado em um campo `data`. Isto significa que podemos carregar o conjunto de dados especificando o argumento  `field` da seguinte forma:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Por padr√£o, o carregamento de arquivos locais cria um objeto `DatasetDict` com uma divis√£o de treino (train). Podemos ver isso inspecionando o objeto `squad_it_dataset`:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Isto nos mostra o n√∫mero de linhas e os nomes das colunas associadas ao conjunto de treinamento. Podemos ver um dos exemplos, indexando na divis√£o de treino da seguinte forma:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si √® verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

√ìtimo, n√≥s carregamos nosso primeiro conjunto de dados local! Mas enquanto isso funcionou para o conjunto de treinamento, o que realmente queremos √© incluir tanto o conjunto de `treino` quanto o de `teste` divididos em um √∫nico objeto `DatasetDict` para que possamos aplicar as fun√ß√µes `Dataset.map()` em ambas as divis√µes de uma s√≥ vez. Para fazer isso, podemos fornecer um dicion√°rio para o argumento `data_files` que mapeia cada nome de divis√£o para um arquivo associado a essa divis√£o:


```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Isto √© exatamente o que quer√≠amos. Agora, podemos aplicar v√°rias t√©cnicas de pr√©-processamento para limpar os dados, assinalar as revis√µes, e assim por diante.


<Tip>

O argumento `data_files` da fun√ß√£o `load_dataset()` √© bastante flex√≠vel e pode ser um √∫nico caminho de arquivo ou uma lista de caminhos de arquivo, ou um dicion√°rio que mapeia nomes divididos para caminhos de arquivo. Voc√™ tamb√©m pode incluir arquivos que correspondam a um padr√£o especificado de acordo com as regras utilizadas pela Unix shell (por exemplo, voc√™ pode adicionar todos os arquivos JSON em um diret√≥rio como uma √∫nica divis√£o, definindo `data_files="*.json"`). Consulte a [documenta√ß√£o](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) do ü§ó Datasets para obter mais detalhes.

</Tip>

Os scripts de carregamento em ü§ó Datasets realmente suportam a descompress√£o autom√°tica dos arquivos de entrada, ent√£o poder√≠amos ter pulado o uso do `gzip` ao apontar o argumento `data_files` diretamente para os arquivos compactados:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Isto pode ser √∫til se voc√™ n√£o quiser descomprimir manualmente muitos arquivos GZIP. A descompress√£o autom√°tica tamb√©m se aplica a outros formatos comuns como ZIP e TAR, ent√£o voc√™ s√≥ precisa apontar `data_files` para os arquivos compactados e est√° pronto para seguir em frente!

Agora que voc√™ sabe como carregar arquivos locais em seu laptop ou desktop, vamos dar uma olhada no carregamento de arquivos remotos.

## Carregando um dataset remoto

Se voc√™ estiver trabalhando como cientista de dados ou programador em uma empresa, h√° uma boa chance de que os conjuntos de dados que voc√™ deseja analisar estejam armazenados em algum servidor remoto. Felizmente, o carregamento de arquivos remotos √© t√£o simples quanto o carregamento de arquivos locais! Em vez de fornecer um caminho para arquivos locais, apontamos o argumento `data_files` de `load_dataset()` para uma ou mais URLs onde os arquivos remotos s√£o armazenados. Por exemplo, para o conjunto de dados SQuAD-it hospedado no GitHub, podemos apenas apontar `data_files` para as URLs _SQuAD_it-*.json.gz_ da seguinte maneira:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Isto retorna o mesmo objeto `DatasetDict` obtido anteriormente, mas nos poupa o passo de baixar e descomprimir manualmente os arquivos _SQuAD_it-*.json.gz_. Isto envolve nas v√°rias formas de carregar conjuntos de dados que n√£o est√£o hospedados no Hugging Face Hub. Agora que temos um conjunto de dados para brincar, vamos sujar as m√£os com v√°rias t√©cnicas de manipula√ß√£o de dados!

<Tip>
‚úèÔ∏è **Tente fazer isso!** Escolha outro conjunto de dados hospedado no GitHub ou no [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) e tente carreg√°-lo tanto local como remotamente usando as t√©cnicas introduzidas acima. Para pontos b√¥nus, tente carregar um conjunto de dados que esteja armazenado em formato CSV ou texto (veja a [documenta√ß√£o](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) para mais informa√ß√µes sobre estes formatos).
</Tip>


