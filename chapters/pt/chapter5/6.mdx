<FrameworkSwitchCourse {fw} />

# Busca sem√¢ntica com o FAISS

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},
]} />

{/if}

Na [se√ß√£o 5](/course/chapter5/5), criamos um conjunto de dados de issues e coment√°rios do GitHub do reposit√≥rio ü§ó Datasets. Nesta se√ß√£o, usaremos essas informa√ß√µes para construir um mecanismo de pesquisa que pode nos ajudar a encontrar respostas para nossas perguntas mais urgentes sobre a biblioteca!

<Youtube id="OATCgQtNX2o"/>

## Usando embeddings para pesquisa sem√¢ntica

Como vimos no [Cap√≠tulo 1](/course/chapter1), os modelos de linguagem baseados em Transformer representam cada token em um intervalo de texto como um _vetor de incorpora√ß√£o_. Acontece que √© poss√≠vel "agrupar" as incorpora√ß√µes individuais para criar uma representa√ß√£o vetorial para frases inteiras, par√°grafos ou (em alguns casos) documentos. Essas incorpora√ß√µes podem ser usadas para encontrar documentos semelhantes no corpus calculando a similaridade do produto escalar (ou alguma outra m√©trica de similaridade) entre cada incorpora√ß√£o e retornando os documentos com maior sobreposi√ß√£o.

Nesta se√ß√£o, usaremos embeddings para desenvolver um mecanismo de pesquisa sem√¢ntica. Esses mecanismos de pesquisa oferecem v√°rias vantagens sobre as abordagens convencionais que se baseiam na correspond√™ncia de palavras-chave em uma consulta com os documentos.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Semantic search."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Semantic search."/>
</div>

## Carregando e preparando o conjunto de dados

A primeira coisa que precisamos fazer √© baixar nosso conjunto de dados de issues do GitHub, ent√£o vamos usar a biblioteca ü§ó Hub para resolver a URL onde nosso arquivo est√° armazenado no Hugging Face Hub:

```py
from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-comments.jsonl",
    repo_type="dataset",
)
```

Com a URL armazenada em `data_files`, podemos carregar o conjunto de dados remoto usando o m√©todo apresentado na [se√ß√£o 2](/course/chapter5/2):

```py
from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

Aqui n√≥s especificamos a divis√£o padr√£o `train` em `load_dataset()`, ent√£o ele retorna um `Dataset` em vez de um `DatasetDict`. A primeira ordem de neg√≥cios √© filtrar os pull request, pois elas tendem a ser raramente usadas para responder a consultas de usu√°rios e introduzir√£o ru√≠do em nosso mecanismo de pesquisa. Como j√° deve ser familiar, podemos usar a fun√ß√£o `Dataset.filter()` para excluir essas linhas em nosso conjunto de dados. Enquanto estamos nisso, tamb√©m vamos filtrar as linhas sem coment√°rios, pois elas n√£o fornecem respostas √†s consultas dos usu√°rios:

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```
Podemos ver que h√° muitas colunas em nosso conjunto de dados, a maioria das quais n√£o precisamos para construir nosso mecanismo de pesquisa. De uma perspectiva de pesquisa, as colunas mais informativas s√£o `title`, `body` e `comments`, enquanto `html_url` nos fornece um link de volta para a issue de origem. Vamos usar a fun√ß√£o `Dataset.remove_columns()` para descartar o resto:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

Para criar nossos embeddings, aumentaremos cada coment√°rio com o t√≠tulo e o corpo da issue, pois esses campos geralmente incluem informa√ß√µes contextuais √∫teis. Como nossa coluna `comments` √© atualmente uma lista de coment√°rios para cada issue, precisamos "explodir" a coluna para que cada linha consista em uma tupla `(html_url, title, body, comment)`. No Pandas podemos fazer isso com a fun√ß√£o [`DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), que cria uma nova linha para cada elemento em uma coluna semelhante a uma lista, enquanto replica todos os outros valores de coluna. Para ver isso em a√ß√£o, vamos primeiro mudar para o formato `DataFrame` do Pandas:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

Se inspecionarmos a primeira linha neste `DataFrame`, podemos ver que h√° quatro coment√°rios associados a esta issue:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in Ôºö\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Quando explodimos `df`, esperamos obter uma linha para cada um desses coment√°rios. Vamos verificar se √© o caso:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in Ôºö\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

√ìtimo, podemos ver que as linhas foram replicadas, com a coluna `comments` contendo os coment√°rios individuais! Agora que terminamos com o Pandas, podemos voltar rapidamente para um `Dataset` carregando o `DataFrame` na mem√≥ria

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

Ok, isso nos deu alguns milhares de coment√°rios para trabalhar!

<Tip>

‚úèÔ∏è **Experimente!** Veja se voc√™ pode usar `Dataset.map()` para explodir a coluna `comments` de `issues_dataset` _sem_ recorrer ao uso de Pandas. Isso √© um pouco complicado; voc√™ pode achar √∫til para esta tarefa a se√ß√£o ["Mapeamento em lote"](https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping) da documenta√ß√£o do ü§ó Dataset.

</Tip>

Agora que temos um coment√°rio por linha, vamos criar uma nova coluna `comments_length` que cont√©m o n√∫mero de palavras por coment√°rio:

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

Podemos usar essa nova coluna para filtrar coment√°rios curtos, que normalmente incluem coisas como "cc @lewtun" ou "Obrigado!" que n√£o s√£o relevantes para o nosso motor de busca. N√£o h√° um n√∫mero preciso para selecionar o filtro, mas cerca de 15 palavras parece um bom come√ßo:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

Depois de limpar um pouco nosso conjunto de dados, vamos concatenar o t√≠tulo, a descri√ß√£o e os coment√°rios da issue em uma nova coluna `text`. Como de costume, escreveremos uma fun√ß√£o simples que podemos passar para `Dataset.map()`:

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)
```

Finalmente estamos prontos para criar alguns embeddings! Vamos dar uma olhada.

## Criando embeddings de texto

Vimos no [Cap√≠tulo 2](/course/chapter2) que podemos obter tokens embeddings usando a classe `AutoModel`. Tudo o que precisamos fazer √© escolher um checkpoint adequado para carregar o modelo. Felizmente, existe uma biblioteca chamada `sentence-transformers` dedicada √† cria√ß√£o de embeddings. Conforme descrito na [documenta√ß√£o da biblioteca](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), nosso caso de uso √© um exemplo de _asymmetric semantic search_ porque temos uma consulta curta cuja resposta gostar√≠amos de encontrar em um documento mais longo, como um coment√°rio da issue. A √∫til [tabela de vis√£o geral do modelo](https://www.sbert.net/docs/pretrained_models.html#model-overview) na documenta√ß√£o indica que o checkpoint `multi-qa-mpnet-base-dot-v1` tem o melhor desempenho para pesquisa sem√¢ntica, ent√£o usaremos isso para nosso aplicativo. Tamb√©m carregaremos o tokenizer usando o mesmo checkpoint:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

Para acelerar o processo de embedding, √© √∫til colocar o modelo e as entradas em um dispositivo GPU, ent√£o vamos fazer isso agora:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

Observe que definimos `from_pt=True` como um argumento do m√©todo `from_pretrained()`. Isso ocorre porque o checkpoint `multi-qa-mpnet-base-dot-v1` s√≥ tem pesos PyTorch, portanto, definir `from_pt=True` ir√° convert√™-los automaticamente para o formato TensorFlow para n√≥s. Como voc√™ pode ver, √© muito simples alternar entre frameworks no ü§ó Transformers!

{/if}

Como mencionamos anteriormente, gostar√≠amos de representar cada entrada em nosso corpus de issues do GitHub como um √∫nico vetor, portanto, precisamos "pool" ou calcular a m√©dia de nossas incorpora√ß√µes de token de alguma forma. Uma abordagem popular √© realizar *CLS pooling* nas sa√≠das do nosso modelo, onde simplesmente coletamos o √∫ltimo estado oculto para o token especial `[CLS]`. A fun√ß√£o a seguir faz o truque para n√≥s:

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

Em seguida, criaremos uma fun√ß√£o auxiliar que tokenizar√° uma lista de documentos, colocar√° os tensores na GPU, os alimentar√° no modelo e, finalmente, aplicar√° o agrupamento CLS √†s sa√≠das:

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Podemos testar o funcionamento da fun√ß√£o alimentando-a com a primeira entrada de texto em nosso corpus e inspecionando a forma de sa√≠da:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

√ìtimo, convertemos a primeira entrada em nosso corpus em um vetor de 768 dimens√µes! Podemos usar `Dataset.map()` para aplicar nossa fun√ß√£o `get_embeddings()` a cada linha em nosso corpus, ent√£o vamos criar uma nova coluna `embeddings` da seguinte forma:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Podemos testar o funcionamento da fun√ß√£o alimentando-a com a primeira entrada de texto em nosso corpus e inspecionando a forma de sa√≠da:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

√ìtimo, convertemos a primeira entrada em nosso corpus em um vetor de 768 dimens√µes! Podemos usar `Dataset.map()` para aplicar nossa fun√ß√£o `get_embeddings()` a cada linha em nosso corpus, ent√£o vamos criar uma nova coluna `embeddings` da seguinte forma:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

Observe que convertemos os embeddings em arrays NumPy -- isso porque ü§ó Datasets requer esse formato quando tentamos index√°-los com FAISS, o que faremos a seguir.


## Usando FAISS para busca de similaridade

Agora que temos um conjunto de dados de embeddings, precisamos de alguma maneira de pesquis√°-los. Para fazer isso, usaremos uma estrutura de dados especial em ü§ó Datasets chamada _FAISS index_. [FAISS](https://faiss.ai/) (abrevia√ß√£o de Facebook AI Similarity Search) √© uma biblioteca que fornece algoritmos eficientes para pesquisar rapidamente e agrupar vetores de incorpora√ß√£o.

A id√©ia b√°sica por tr√°s do FAISS √© criar uma estrutura de dados especial chamada _index_ que permite descobrir quais embeddings s√£o semelhantes a um embedding de entrada. Criar um √≠ndice FAISS em ü§ó Datasets √© simples -- usamos a fun√ß√£o `Dataset.add_faiss_index()` e especificamos qual coluna do nosso conjunto de dados gostar√≠amos de indexar:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

Agora podemos realizar consultas neste √≠ndice fazendo uma pesquisa do vizinho mais pr√≥ximo com a fun√ß√£o `Dataset.get_nearest_examples()`. Vamos testar isso primeiro incorporando uma pergunta da seguinte forma:


{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

Assim como com os documentos, agora temos um vetor de 768 dimens√µes representando a consulta, que podemos comparar com todo o corpus para encontrar os embeddings mais semelhantes:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

A fun√ß√£o `Dataset.get_nearest_examples()` retorna uma tupla de pontua√ß√µes que classificam a sobreposi√ß√£o entre a consulta e o documento e um conjunto correspondente de amostras (aqui, as 5 melhores correspond√™ncias). Vamos colet√°-los em um `pandas.DataFrame` para que possamos classific√°-los facilmente:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Agora podemos iterar nas primeiras linhas para ver como nossa consulta correspondeu aos coment√°rios dispon√≠veis:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Nada mal! Nosso segundo resultado parece corresponder √† consulta.

<Tip>

‚úèÔ∏è **Experimente!** Crie sua pr√≥pria consulta e veja se consegue encontrar uma resposta nos documentos recuperados. Voc√™ pode ter que aumentar o par√¢metro `k` em `Dataset.get_nearest_examples()` para ampliar a pesquisa.

</Tip>