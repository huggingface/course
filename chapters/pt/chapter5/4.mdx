# Big data? ü§ó Datasets ao resgate

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},
]} />


Hoje em dia, n√£o √© incomum encontrar-se trabalhando com conjuntos de dados de v√°rios gigabytes, especialmente se voc√™ planeja pr√©-treinar um transformer como BERT ou GPT-2 do zero. Nesses casos, at√© mesmo _carregar_ os dados pode ser um desafio. Por exemplo, o corpus WebText usado para pr√©-treinar o GPT-2 consiste em mais de 8 milh√µes de documentos e 40 GB de texto - carregar isso na RAM do seu laptop provavelmente lhe causar√° um ataque card√≠aco!

Felizmente, ü§ó Datasets foram projetados para superar essas limita√ß√µes. Ele libera voc√™ de problemas de gerenciamento de mem√≥ria tratando conjuntos de dados como arquivos _memory-mapped_ e de limites de disco r√≠gido por _streaming_ das entradas em um corpus.

<Youtube id="JwISwTCPPWo"/>

Nesta se√ß√£o, exploraremos esses recursos de ü§ó Conjuntos de dados com um enorme corpus de 825 GB conhecido como [the Pile](https://pile.eleuther.ai). Vamos come√ßar!

## O que √© the Pile?

O `The Pile` √© um corpus de texto em ingl√™s que foi criado por [EleutherAI](https://www.eleuther.ai) para treinar modelos de linguagem em larga escala. Ele inclui uma gama diversificada de conjuntos de dados, abrangendo artigos cient√≠ficos, reposit√≥rios de c√≥digo do GitHub e texto da web filtrado. O corpus de treinamento est√° dispon√≠vel em [blocos de 14 GB](https://mystic.the-eye.eu/public/AI/pile/), e voc√™ tamb√©m pode baixar v√°rios dos [componentes individuais](https://mystic.the-eye.eu/public/AI/pile_preliminary_components/). Vamos come√ßar dando uma olhada no conjunto de dados PubMed Abstracts, que √© um corpus de resumos de 15 milh√µes de publica√ß√µes biom√©dicas no [PubMed](https://pubmed.ncbi.nlm.nih.gov/). O conjunto de dados est√° em [formato JSON Lines](https://jsonlines.org) e √© compactado usando a biblioteca `zstandard`, ent√£o primeiro precisamos instal√°-lo:

```py
!pip install zstandard
```

Em seguida, podemos carregar o conjunto de dados usando o m√©todo para arquivos remotos que aprendemos na [se√ß√£o 2](/course/chapter5/2):

```py
from datasets import load_dataset

# This takes a few minutes to run, so go grab a tea or coffee while you wait :)
data_files = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

Podemos ver que h√° 15.518.009 linhas e 2 colunas em nosso conjunto de dados - isso √© muito!

<Tip>

‚úé Por padr√£o, ü§ó Datasets descompactar√° os arquivos necess√°rios para carregar um dataset. Se voc√™ quiser preservar espa√ßo no disco r√≠gido, voc√™ pode passar `DownloadConfig(delete_extracted=True)` para o argumento `download_config` de `load_dataset()`. Consulte a [documenta√ß√£o](https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig) para obter mais detalhes.

</Tip>

Vamos inspecionar o conte√∫do do primeiro exemplo:

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Ok, isso parece o resumo de um artigo m√©dico. Agora vamos ver quanta RAM usamos para carregar o conjunto de dados!

## A magia do mapeamento de mem√≥ria

Uma maneira simples de medir o uso de mem√≥ria em Python √© com a biblioteca [`psutil`](https://psutil.readthedocs.io/en/latest/), que pode ser instalada com `pip` da seguinte forma:

```python
!pip install psutil
```

Ele fornece uma classe `Process` que nos permite verificar o uso de mem√≥ria do processo atual da seguinte forma:

```py
import psutil

# Process.memory_info is expressed in bytes, so convert to megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
RAM used: 5678.33 MB
```

Aqui o atributo `rss` refere-se ao _tamanho do conjunto residente_, que √© a fra√ß√£o de mem√≥ria que um processo ocupa na RAM. Essa medida tamb√©m inclui a mem√≥ria usada pelo interpretador Python e as bibliotecas que carregamos, portanto, a quantidade real de mem√≥ria usada para carregar o conjunto de dados √© um pouco menor. Para compara√ß√£o, vamos ver o tamanho do conjunto de dados no disco, usando o atributo `dataset_size`. Como o resultado √© expresso em bytes como antes, precisamos convert√™-lo manualmente para gigabytes:

```py
print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
```

Legal -- apesar de ter quase 20 GB de tamanho, podemos carregar e acessar o conjunto de dados com muito menos RAM!

<Tip>

‚úèÔ∏è **Experimente!** Escolha um dos [subconjuntos](https://mystic.the-eye.eu/public/AI/pile_preliminary_components/) da `The Pile` que √© maior que a RAM do seu laptop ou desktop, carregue com ü§ó Datasets e me√ßa a quantidade de RAM usada. Observe que, para obter uma medi√ß√£o precisa, voc√™ desejar√° fazer isso em um novo processo. Voc√™ pode encontrar os tamanhos descompactados de cada subconjunto na Tabela 1 do [artigo do `The Pile`](https://arxiv.org/abs/2101.00027).

</Tip>

Se voc√™ estiver familiarizado com Pandas, esse resultado pode ser uma surpresa por causa da famosa [regra de ouro] de Wes Kinney (https://wesmckinney.com/blog/apache-arrow-pandas-internals/) de que voc√™ normalmente precisa de 5 para 10 vezes mais RAM do que o tamanho do seu conjunto de dados. Ent√£o, como ü§ó Datasets resolve esse problema de gerenciamento de mem√≥ria? ü§ó Os conjuntos de dados tratam cada conjunto de dados como um [arquivo mapeado em mem√≥ria](https://en.wikipedia.org/wiki/Memory-mapped_file), que fornece um mapeamento entre RAM e armazenamento do sistema de arquivos que permite que a biblioteca acesse e opere em elementos do conjunto de dados sem precisar carreg√°-lo totalmente na mem√≥ria.

Arquivos mapeados em mem√≥ria tamb√©m podem ser compartilhados em v√°rios processos, o que permite que m√©todos como `Dataset.map()` sejam paralelizados sem a necessidade de mover ou copiar o conjunto de dados. Sob o cap√¥, esses recursos s√£o todos realizados pelo formato de mem√≥ria [Apache Arrow](https://arrow.apache.org) e [`pyarrow`](https://arrow.apache.org/docs/python/index.html), que tornam o carregamento e o processamento de dados extremamente r√°pidos. (Para mais detalhes sobre o Apache Arrow e compara√ß√µes com o Pandas, confira [post do blog de Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) Para ver isso em a√ß√£o, vamos executar um pequeno teste de velocidade iterando sobre todos os elementos no conjunto de dados PubMed Abstracts:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

Aqui usamos o m√≥dulo `timeit` do Python para medir o tempo de execu√ß√£o do `code_snippet`. Normalmente, voc√™ poder√° iterar em um conjunto de dados a uma velocidade de alguns d√©cimos de GB/s a v√°rios GB/s. Isso funciona muito bem para a grande maioria dos aplicativos, mas √†s vezes voc√™ ter√° que trabalhar com um conjunto de dados grande demais para ser armazenado no disco r√≠gido do seu laptop. Por exemplo, se tent√°ssemos baixar o Pile por completo, precisar√≠amos de 825 GB de espa√ßo livre em disco! Para lidar com esses casos, ü§ó Datasets fornece um recurso de streaming que nos permite baixar e acessar elementos em tempo real, sem a necessidade de baixar todo o conjunto de dados. Vamos dar uma olhada em como isso funciona.

<Tip>

üí° Nos notebooks Jupyter, voc√™ tamb√©m pode cronometrar c√©lulas usando a [`%%timeit` fun√ß√£o m√°gica](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).

</Tip>

## Conjuntos de dados em streaming

Para habilitar o streaming do conjunto de dados voc√™ s√≥ precisa passar o argumento `streaming=True` para a fun√ß√£o `load_dataset()`. Por exemplo, vamos carregar o conjunto de dados PubMed Abstracts novamente, mas em modo streaming:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Em vez do familiar `Dataset` que encontramos em outro lugar neste cap√≠tulo, o objeto retornado com `streaming=True` √© um `IterableDataset`. Como o nome sugere, para acessar os elementos de um `IterableDataset` precisamos iterar sobre ele. Podemos acessar o primeiro elemento do nosso conjunto de dados transmitido da seguinte forma:


```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Os elementos de um conjunto de dados transmitido podem ser processados dinamicamente usando `IterableDataset.map()`, o que √© √∫til durante o treinamento se voc√™ precisar tokenizar as entradas. O processo √© exatamente o mesmo que usamos para tokenizar nosso conjunto de dados no [Cap√≠tulo 3](/course/chapter3), com a √∫nica diferen√ßa de que as sa√≠das s√£o retornadas uma a uma:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<Tip>

üí° Para acelerar a tokeniza√ß√£o com streaming voc√™ pode passar `batched=True`, como vimos na √∫ltima se√ß√£o. Ele processar√° os exemplos lote por lote; o tamanho do lote padr√£o √© 1.000 e pode ser especificado com o argumento `batch_size`.

</Tip>

Voc√™ tamb√©m pode embaralhar um conjunto de dados transmitido usando `IterableDataset.shuffle()`, mas, diferentemente de `Dataset.shuffle()`, isso apenas embaralha os elementos em um `buffer_size` predefinido:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

Neste exemplo, selecionamos um exemplo aleat√≥rio dos primeiros 10.000 exemplos no buffer. Uma vez que um exemplo √© acessado, seu lugar no buffer √© preenchido com o pr√≥ximo exemplo no corpus (ou seja, o 10.001¬∫ exemplo no caso acima). Voc√™ tamb√©m pode selecionar elementos de um conjunto de dados transmitido usando as fun√ß√µes `IterableDataset.take()` e `IterableDataset.skip()`, que agem de maneira semelhante a `Dataset.select()`. Por exemplo, para selecionar os primeiros 5 exemplos no conjunto de dados PubMed Abstracts, podemos fazer o seguinte:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

Da mesma forma, voc√™ pode usar a fun√ß√£o `IterableDataset.skip()` para criar divis√µes de treinamento e valida√ß√£o de um conjunto de dados embaralhado da seguinte forma:

```py
# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)
```

Vamos completar nossa explora√ß√£o de streaming de conjuntos de dados com um aplicativo comum: combinar v√°rios conjuntos de dados para criar um √∫nico corpus. ü§ó Datasets fornece uma fun√ß√£o `interleave_datasets()` que converte uma lista de objetos `IterableDataset` em um √∫nico `IterableDataset`, onde os elementos do novo conjunto de dados s√£o obtidos alternando entre os exemplos de origem. Essa fun√ß√£o √© especialmente √∫til quando voc√™ est√° tentando combinar grandes conjuntos de dados, ent√£o, como exemplo, vamos transmitir o subconjunto FreeLaw do Pile, que √© um conjunto de dados de 51 GB de pareceres jur√≠dicos dos tribunais dos EUA:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

Esse conjunto de dados √© grande o suficiente para sobrecarregar a RAM da maioria dos laptops, mas conseguimos carreg√°-lo e acess√°-lo sem suar a camisa! Vamos agora combinar os exemplos dos conjuntos de dados FreeLaw e PubMed Abstracts com a fun√ß√£o `interleave_datasets()`:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

Aqui usamos a fun√ß√£o `islice()` do m√≥dulo `itertools` do Python para selecionar os dois primeiros exemplos do conjunto de dados combinado e podemos ver que eles correspondem aos primeiros exemplos de cada um dos dois conjuntos de dados de origem.

Por fim, se voc√™ quiser transmitir o Pile em sua totalidade de 825 GB, poder√° pegar todos os arquivos preparados da seguinte maneira:

```py
base_url = "https://mystic.the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play ‚ÄúSurvival of the Tastiest‚Äù on Android, and on the web...'}
```

<Tip>

‚úèÔ∏è **Experimente!** Use um dos grandes corpora Common Crawl como [`mc4`](https://huggingface.co/datasets/mc4) ou [`oscar`](https://huggingface.co/datasets/oscar) para criar um conjunto de dados multil√≠ngue de streaming que represente as propor√ß√µes faladas de idiomas em um pa√≠s de sua escolha. Por exemplo, as quatro l√≠nguas nacionais na Su√≠√ßa s√£o alem√£o, franc√™s, italiano e romanche, ent√£o voc√™ pode tentar criar um corpus su√≠√ßo amostrando os subconjuntos do Oscar de acordo com sua propor√ß√£o falada.

</Tip>

Agora voc√™ tem todas as ferramentas necess√°rias para carregar e processar conjuntos de dados de todas as formas e tamanhos, mas, a menos que tenha muita sorte, chegar√° um ponto em sua jornada de PNL em que voc√™ ter√° que criar um conjunto de dados para resolver o problema. problema em m√£os. Esse √© o tema da pr√≥xima se√ß√£o!
