# Big data? ğŸ¤— Datasets ao resgate

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter5/section4.ipynb"},
]} />


Hoje em dia, nÃ£o Ã© incomum encontrar-se trabalhando com conjuntos de dados de vÃ¡rios gigabytes, especialmente se vocÃª planeja prÃ©-treinar um transformer como BERT ou GPT-2 do zero. Nesses casos, atÃ© mesmo _carregar_ os dados pode ser um desafio. Por exemplo, o corpus WebText usado para prÃ©-treinar o GPT-2 consiste em mais de 8 milhÃµes de documentos e 40 GB de texto - carregar isso na RAM do seu laptop provavelmente lhe causarÃ¡ um ataque cardÃ­aco!

Felizmente, ğŸ¤— Datasets foram projetados para superar essas limitaÃ§Ãµes. Ele libera vocÃª de problemas de gerenciamento de memÃ³ria tratando conjuntos de dados como arquivos _memory-mapped_ e de limites de disco rÃ­gido por _streaming_ das entradas em um corpus.

<Youtube id="JwISwTCPPWo"/>

Nesta seÃ§Ã£o, exploraremos esses recursos de ğŸ¤— Conjuntos de dados com um enorme corpus de 825 GB conhecido como [the Pile](https://pile.eleuther.ai). Vamos comeÃ§ar!

## O que Ã© the Pile?

O `The Pile` Ã© um corpus de texto em inglÃªs que foi criado por [EleutherAI](https://www.eleuther.ai) para treinar modelos de linguagem em larga escala. Ele inclui uma gama diversificada de conjuntos de dados, abrangendo artigos cientÃ­ficos, repositÃ³rios de cÃ³digo do GitHub e texto da web filtrado. O corpus de treinamento estÃ¡ disponÃ­vel em [blocos de 14 GB](https://the-eye.eu/public/AI/pile/), e vocÃª tambÃ©m pode baixar vÃ¡rios dos [componentes individuais](https://the-eye.eu/public/AI/pile_preliminary_components/). Vamos comeÃ§ar dando uma olhada no conjunto de dados PubMed Abstracts, que Ã© um corpus de resumos de 15 milhÃµes de publicaÃ§Ãµes biomÃ©dicas no [PubMed](https://pubmed.ncbi.nlm.nih.gov/). O conjunto de dados estÃ¡ em [formato JSON Lines](https://jsonlines.org) e Ã© compactado usando a biblioteca `zstandard`, entÃ£o primeiro precisamos instalÃ¡-lo:

```py
!pip install zstandard
```

Em seguida, podemos carregar o conjunto de dados usando o mÃ©todo para arquivos remotos que aprendemos na [seÃ§Ã£o 2](/course/chapter5/2):

```py
from datasets import load_dataset

# This takes a few minutes to run, so go grab a tea or coffee while you wait :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

Podemos ver que hÃ¡ 15.518.009 linhas e 2 colunas em nosso conjunto de dados - isso Ã© muito!

> [!TIP]
> âœ Por padrÃ£o, ğŸ¤— Datasets descompactarÃ¡ os arquivos necessÃ¡rios para carregar um dataset. Se vocÃª quiser preservar espaÃ§o no disco rÃ­gido, vocÃª pode passar `DownloadConfig(delete_extracted=True)` para o argumento `download_config` de `load_dataset()`. Consulte a [documentaÃ§Ã£o](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig) para obter mais detalhes.

Vamos inspecionar o conteÃºdo do primeiro exemplo:

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Ok, isso parece o resumo de um artigo mÃ©dico. Agora vamos ver quanta RAM usamos para carregar o conjunto de dados!

## A magia do mapeamento de memÃ³ria

Uma maneira simples de medir o uso de memÃ³ria em Python Ã© com a biblioteca [`psutil`](https://psutil.readthedocs.io/en/latest/), que pode ser instalada com `pip` da seguinte forma:

```python
!pip install psutil
```

Ele fornece uma classe `Process` que nos permite verificar o uso de memÃ³ria do processo atual da seguinte forma:

```py
import psutil

# Process.memory_info is expressed in bytes, so convert to megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
RAM used: 5678.33 MB
```

Aqui o atributo `rss` refere-se ao _tamanho do conjunto residente_, que Ã© a fraÃ§Ã£o de memÃ³ria que um processo ocupa na RAM. Essa medida tambÃ©m inclui a memÃ³ria usada pelo interpretador Python e as bibliotecas que carregamos, portanto, a quantidade real de memÃ³ria usada para carregar o conjunto de dados Ã© um pouco menor. Para comparaÃ§Ã£o, vamos ver o tamanho do conjunto de dados no disco, usando o atributo `dataset_size`. Como o resultado Ã© expresso em bytes como antes, precisamos convertÃª-lo manualmente para gigabytes:

```py
print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
```

Legal -- apesar de ter quase 20 GB de tamanho, podemos carregar e acessar o conjunto de dados com muito menos RAM!

> [!TIP]
> âœï¸ **Experimente!** Escolha um dos [subconjuntos](https://the-eye.eu/public/AI/pile_preliminary_components/) da `The Pile` que Ã© maior que a RAM do seu laptop ou desktop, carregue com ğŸ¤— Datasets e meÃ§a a quantidade de RAM usada. Observe que, para obter uma mediÃ§Ã£o precisa, vocÃª desejarÃ¡ fazer isso em um novo processo. VocÃª pode encontrar os tamanhos descompactados de cada subconjunto na Tabela 1 do [artigo do `The Pile`](https://arxiv.org/abs/2101.00027).

Se vocÃª estiver familiarizado com Pandas, esse resultado pode ser uma surpresa por causa da famosa [regra de ouro] de Wes Kinney (https://wesmckinney.com/blog/apache-arrow-pandas-internals/) de que vocÃª normalmente precisa de 5 para 10 vezes mais RAM do que o tamanho do seu conjunto de dados. EntÃ£o, como ğŸ¤— Datasets resolve esse problema de gerenciamento de memÃ³ria? ğŸ¤— Os conjuntos de dados tratam cada conjunto de dados como um [arquivo mapeado em memÃ³ria](https://en.wikipedia.org/wiki/Memory-mapped_file), que fornece um mapeamento entre RAM e armazenamento do sistema de arquivos que permite que a biblioteca acesse e opere em elementos do conjunto de dados sem precisar carregÃ¡-lo totalmente na memÃ³ria.

Arquivos mapeados em memÃ³ria tambÃ©m podem ser compartilhados em vÃ¡rios processos, o que permite que mÃ©todos como `Dataset.map()` sejam paralelizados sem a necessidade de mover ou copiar o conjunto de dados. Sob o capÃ´, esses recursos sÃ£o todos realizados pelo formato de memÃ³ria [Apache Arrow](https://arrow.apache.org) e [`pyarrow`](https://arrow.apache.org/docs/python/index.html), que tornam o carregamento e o processamento de dados extremamente rÃ¡pidos. (Para mais detalhes sobre o Apache Arrow e comparaÃ§Ãµes com o Pandas, confira [post do blog de Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) Para ver isso em aÃ§Ã£o, vamos executar um pequeno teste de velocidade iterando sobre todos os elementos no conjunto de dados PubMed Abstracts:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

Aqui usamos o mÃ³dulo `timeit` do Python para medir o tempo de execuÃ§Ã£o do `code_snippet`. Normalmente, vocÃª poderÃ¡ iterar em um conjunto de dados a uma velocidade de alguns dÃ©cimos de GB/s a vÃ¡rios GB/s. Isso funciona muito bem para a grande maioria dos aplicativos, mas Ã s vezes vocÃª terÃ¡ que trabalhar com um conjunto de dados grande demais para ser armazenado no disco rÃ­gido do seu laptop. Por exemplo, se tentÃ¡ssemos baixar o Pile por completo, precisarÃ­amos de 825 GB de espaÃ§o livre em disco! Para lidar com esses casos, ğŸ¤— Datasets fornece um recurso de streaming que nos permite baixar e acessar elementos em tempo real, sem a necessidade de baixar todo o conjunto de dados. Vamos dar uma olhada em como isso funciona.

> [!TIP]
> ğŸ’¡ Nos notebooks Jupyter, vocÃª tambÃ©m pode cronometrar cÃ©lulas usando a [`%%timeit` funÃ§Ã£o mÃ¡gica](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).

## Conjuntos de dados em streaming

Para habilitar o streaming do conjunto de dados vocÃª sÃ³ precisa passar o argumento `streaming=True` para a funÃ§Ã£o `load_dataset()`. Por exemplo, vamos carregar o conjunto de dados PubMed Abstracts novamente, mas em modo streaming:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Em vez do familiar `Dataset` que encontramos em outro lugar neste capÃ­tulo, o objeto retornado com `streaming=True` Ã© um `IterableDataset`. Como o nome sugere, para acessar os elementos de um `IterableDataset` precisamos iterar sobre ele. Podemos acessar o primeiro elemento do nosso conjunto de dados transmitido da seguinte forma:


```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Os elementos de um conjunto de dados transmitido podem ser processados dinamicamente usando `IterableDataset.map()`, o que Ã© Ãºtil durante o treinamento se vocÃª precisar tokenizar as entradas. O processo Ã© exatamente o mesmo que usamos para tokenizar nosso conjunto de dados no [CapÃ­tulo 3](/course/chapter3), com a Ãºnica diferenÃ§a de que as saÃ­das sÃ£o retornadas uma a uma:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

> [!TIP]
> ğŸ’¡ Para acelerar a tokenizaÃ§Ã£o com streaming vocÃª pode passar `batched=True`, como vimos na Ãºltima seÃ§Ã£o. Ele processarÃ¡ os exemplos lote por lote; o tamanho do lote padrÃ£o Ã© 1.000 e pode ser especificado com o argumento `batch_size`.

VocÃª tambÃ©m pode embaralhar um conjunto de dados transmitido usando `IterableDataset.shuffle()`, mas, diferentemente de `Dataset.shuffle()`, isso apenas embaralha os elementos em um `buffer_size` predefinido:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

Neste exemplo, selecionamos um exemplo aleatÃ³rio dos primeiros 10.000 exemplos no buffer. Uma vez que um exemplo Ã© acessado, seu lugar no buffer Ã© preenchido com o prÃ³ximo exemplo no corpus (ou seja, o 10.001Âº exemplo no caso acima). VocÃª tambÃ©m pode selecionar elementos de um conjunto de dados transmitido usando as funÃ§Ãµes `IterableDataset.take()` e `IterableDataset.skip()`, que agem de maneira semelhante a `Dataset.select()`. Por exemplo, para selecionar os primeiros 5 exemplos no conjunto de dados PubMed Abstracts, podemos fazer o seguinte:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

Da mesma forma, vocÃª pode usar a funÃ§Ã£o `IterableDataset.skip()` para criar divisÃµes de treinamento e validaÃ§Ã£o de um conjunto de dados embaralhado da seguinte forma:

```py
# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)
```

Vamos completar nossa exploraÃ§Ã£o de streaming de conjuntos de dados com um aplicativo comum: combinar vÃ¡rios conjuntos de dados para criar um Ãºnico corpus. ğŸ¤— Datasets fornece uma funÃ§Ã£o `interleave_datasets()` que converte uma lista de objetos `IterableDataset` em um Ãºnico `IterableDataset`, onde os elementos do novo conjunto de dados sÃ£o obtidos alternando entre os exemplos de origem. Essa funÃ§Ã£o Ã© especialmente Ãºtil quando vocÃª estÃ¡ tentando combinar grandes conjuntos de dados, entÃ£o, como exemplo, vamos transmitir o subconjunto FreeLaw do Pile, que Ã© um conjunto de dados de 51 GB de pareceres jurÃ­dicos dos tribunais dos EUA:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

Esse conjunto de dados Ã© grande o suficiente para sobrecarregar a RAM da maioria dos laptops, mas conseguimos carregÃ¡-lo e acessÃ¡-lo sem suar a camisa! Vamos agora combinar os exemplos dos conjuntos de dados FreeLaw e PubMed Abstracts com a funÃ§Ã£o `interleave_datasets()`:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

Aqui usamos a funÃ§Ã£o `islice()` do mÃ³dulo `itertools` do Python para selecionar os dois primeiros exemplos do conjunto de dados combinado e podemos ver que eles correspondem aos primeiros exemplos de cada um dos dois conjuntos de dados de origem.

Por fim, se vocÃª quiser transmitir o Pile em sua totalidade de 825 GB, poderÃ¡ pegar todos os arquivos preparados da seguinte maneira:

```py
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play â€œSurvival of the Tastiestâ€ on Android, and on the web...'}
```

> [!TIP]
> âœï¸ **Experimente!** Use um dos grandes corpora Common Crawl como [`mc4`](https://huggingface.co/datasets/mc4) ou [`oscar`](https://huggingface.co/datasets/oscar) para criar um conjunto de dados multilÃ­ngue de streaming que represente as proporÃ§Ãµes faladas de idiomas em um paÃ­s de sua escolha. Por exemplo, as quatro lÃ­nguas nacionais na SuÃ­Ã§a sÃ£o alemÃ£o, francÃªs, italiano e romanche, entÃ£o vocÃª pode tentar criar um corpus suÃ­Ã§o amostrando os subconjuntos do Oscar de acordo com sua proporÃ§Ã£o falada.

Agora vocÃª tem todas as ferramentas necessÃ¡rias para carregar e processar conjuntos de dados de todas as formas e tamanhos, mas, a menos que tenha muita sorte, chegarÃ¡ um ponto em sua jornada de PNL em que vocÃª terÃ¡ que criar um conjunto de dados para resolver o problema. problema em mÃ£os. Esse Ã© o tema da prÃ³xima seÃ§Ã£o!
