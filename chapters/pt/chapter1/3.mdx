# Transformers, o que eles podem fazer?

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter1/section3.ipynb"},
]} />

Nessa se√ß√£o, observaremos sobre o que os modelos Transformers podem fazer e usar nossa primeira ferramenta da biblioteca ü§ó Transformers: a fun√ß√£o `pipeline()` .

<Tip>
üëÄ T√° vendo o bot√£o <em>Open in Colab</em> no topo direito? Clique nele e abra um notebook Google Colab notebook  com todas as amostras de c√≥digos dessa se√ß√£o. Esse bot√£o estar√° presente em cada se√ß√£o contendo exemplos de c√≥digos. 

Se voc√™ deseja rodar os exemplos localmente, n√≥s recomendamos dar uma olhada no <a href="/course/chapter0">setup</a>.
</Tip>

## Transformers est√£o por toda parte!

Os modelos Transformers s√£o usados para resolver todos os tipos de tarefas de NLP, como algumas j√° mencionadas na se√ß√£o anterior. Aqui est√£o algumas empresas e organiza√ß√µes usando a Hugging Face e os modelos Transformers, que tamb√©m contribuem de volta para a comunidade compartilhando seus modelos:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Empresas usando a Hugging Face" width="100%">

A [biblioteca ü§ó Transformers](https://github.com/huggingface/transformers) oferece a funcionalidade para criar e usar esses modelos compartilhados. O [Model Hub](https://huggingface.co/models) cont√©m milhares de modelos pr√©-treinados que qualquer um pode baixar e usar. Voc√™ pode tamb√©m dar upload nos seus pr√≥prios modelos no Hub!

<Tip>
‚ö†Ô∏è O Hugging Face Hub n√£o √© limitado aos modelos Transformers. Qualquer um pode compartilhar quaisquer tipos de modelos ou datasets que quiserem! <a href="https://huggingface.co/join">Crie uma conta na huggingface.co</a> para se beneficiar de todos os recursos dispon√≠veis!
</Tip>

Antes de aprofundarmos sobre como os modelos Transformers funcionam por debaixo dos panos, vamos olhar alguns exemplos de como eles podem ser usados para solucionar alguns problemas de NLP interessantes.

## Trabalhando com pipelines

<Youtube id="tiZFewofSLM" />

O objeto mais b√°sico na biblioteca ü§ó Transformers √© a fun√ß√£o `pipeline()` . Ela conecta o modelo com seus passos necess√°rios de pr√© e p√≥s-processamento, permitindo-nos a diretamente inserir qualquer texto e obter uma resposta intelig√≠vel:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

N√≥s at√© podemos passar v√°rias senten√ßas!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Por padr√£o, esse pipeline seleciona particularmente um modelo pr√©-treinado que tem sido *ajustado* (fine-tuned) para an√°lise de sentimentos em Ingl√™s. O modelo √© baixado e cacheado quando voc√™ criar o objeto `classifier`. Se voc√™ rodar novamente o comando, o modelo cacheado ir√° ser usado no lugar e n√£o haver√° necessidade de baixar o modelo novamente.

H√° tr√™s principais passos envolvidos quando voc√™ passa algum texto para um pipeline:

1. O texto √© pr√©-processado para um formato que o modelo consiga entender.
2. As entradas (*inputs*) pr√©-processados s√£o passadas para o modelo.
3. As predi√ß√µes do modelo s√£o p√≥s-processadas, para que ent√£o voc√™ consiga atribuir sentido a elas.


Alguns dos [pipelines dispon√≠veis](https://huggingface.co/transformers/main_classes/pipelines.html) atualmente, s√£o:

- `feature-extraction` (pega a representa√ß√£o vetorial do texto)
- `fill-mask` (preenchimento de m√°scara)
- `ner` (reconhecimento de entidades nomeadas)
- `question-answering` (responder perguntas)
- `sentiment-analysis` (an√°lise de sentimentos)
- `summarization` (sumariza√ß√£o)
- `text-generation` (gera√ß√£o de texto)
- `translation` (tradu√ß√£o)
- `zero-shot-classification` (classifica√ß√£o "zero-shot")

Vamos dar uma olhada em alguns desses!

## Classifica√ß√£o Zero-shot 

N√≥s come√ßaremos abordando uma tarefa mais desafiadora da qual n√≥s precisamos classificar texto que n√£o tenham sido rotulados. Esse √© um cen√°rio comum nos projetos reais porque anotar texto geralmente consome bastante do nosso tempo e requer expertise no dom√≠nio. Para esse caso, o pipeline `zero-shot-classification` √© muito poderoso: permite voc√™ especificar quais r√≥tulos usar para a classifica√ß√£o, desse modo voc√™ n√£o precisa "confiar" nos r√≥tulos dos modelos pr√©-treinados. Voc√™ j√° viu como um modelo pode classificar uma senten√ßa como positiva ou negativa usando esses dois r√≥tulos - mas tamb√©m pode ser classificado usando qualquer outro conjunto de r√≥tulos que voc√™ quiser.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Esse pipeline √© chamado de _zero-shot_ porque voc√™ n√£o precisa fazer o ajuste fino do modelo nos dados que voc√™ o utiliza. Pode diretamente retornar scores de probabilidade para qualquer lista de r√≥tulos que voc√™ quiser!

<Tip>

‚úèÔ∏è **Experimente!** Brinque com suas pr√≥prias sequ√™ncias e r√≥tulos e veja como o modelo se comporta.

</Tip>


## Gera√ß√£o de Texto

Agora vamos ver como usar um pipeline para gerar uma por√ß√£o de texto. A principal ideia aqui √© que voc√™ coloque um peda√ßo de texto e o modelo ir√° autocomplet√°-lo ao gerar o texto restante. Isso √© similar ao recurso de predi√ß√£o textual que √© encontrado em in√∫meros celulares. A gera√ß√£o de texto envolve aleatoriedade, ent√£o √© normal se voc√™ n√£o obter o mesmo resultado obtido mostrado abaixo.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator(
    "In this course, we will teach you how to"
)  # nesse curso, n√≥s te mostraremos como voc√™
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows ‚Äî data flows of various types, as seen by the '
                    'HTTP'}] # nesse curso, n√≥s te mostraremos como voc√™ pode entender e usar o fluxo de dados e a troca de dados quando for lidar com dados do usu√°rio. N√≥s estaremos trabalhando com um ou um dos mais comuns fluxos de dados utilizados - fluxo de dados de v√°rios tipos, como visto pelo 'HTTP'
```

Voc√™ pode controlar qu√£o diferentes sequ√™ncias s√£o geradas com o argumento `num_return_sequences` e o tamanho total da sa√≠da de texto (*output*) com o argumento `max_length`.

<Tip>

‚úèÔ∏è **Experimente!** Use os argumentos `num_return_sequences` e `max_length` para gerar 2 textos com 15 palavras cada.

</Tip>


## Usando qualquer modelo do Hub em um pipeline

Nos exemplos passados, usamos o modelo padr√£o para a tarefa que executamos, mas voc√™ pode usar um modelo particular do Hub para us√°-lo no pipeline em uma tarefa espec√≠fica ‚Äî exemplo, gera√ß√£o de texto. V√° ao [Model Hub](https://huggingface.co/models) e clique na tag correspondente na esquerda para mostrar apenas os modelos suport√°veis para aquela tarefa. Voc√™ dever√° ir a uma p√°gina como [essa](https://huggingface.co/models?pipeline_tag=text-generation).

Vamos tentar o  modelo [`distilgpt2`](https://huggingface.co/distilgpt2)! Aqui est√° como carrega-lo no mesmo pipeline como antes:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

Voc√™ pode refinar sua pesquisa por um modelo clicando nas tags de linguagem, e pegando o modelo que gerar√° o texto em outra lingua. O Model Hub at√© mesmo cont√©m checkpoints para modelos multilinguais que suportem v√°rias linguas.

Uma vez que voc√™ seleciona o modelo clicando nele, voc√™ ir√° ver que h√° um widget que permite que voc√™ teste-o diretamente online. Desse modo voc√™ pode rapidamente testar as capacidades do modelo antes de baixa-lo.

<Tip>

‚úèÔ∏è **Experimente!** Use os filtros para encontrar um modelo de gera√ß√£o de texto em outra lingua. Fique √† vontade para brincar com o widget e usa-lo em um pipeline!

</Tip>

### A API de Infer√™ncia

Todos os modelos podem ser testados diretamente de seu navegador usando a API de Infer√™nciaI, que est√° dispon√≠vel no website da [Hugging Face](https://huggingface.co/). Voc√™ pode brincar com o modelo diretamente pela p√°gina colocando textos customizados e observando como o modelo processa os dados inseridos.

A API de Infer√™ncia que alimenta o widget tamb√©m est√° dispon√≠vel como um produto pago, que serve como uma "m√£o na roda" se voc√™ precisa dela para seus workflows. Olhe a [p√°gina de pre√ßos](https://huggingface.co/pricing) para mais detalhes.

## Preenchimento de m√°scara (*Mask filling*) 

O pr√≥ximo pipeline que voc√™ ir√° testar √© o `fill-mask`.  A ideia dessa tarefa √© preencher os espa√ßos em branco com um texto dado:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

O argumento `top_k` controla quantas possibilidades voc√™ quer que sejam geradas. Note que aqui o modelo preenche com uma palavra `<m√°scara>` especial, que √© frequentemente referida como *mask token*.  Outros modelos de preenchimento de m√°scara podem ter diferentes *mask tokens*, ent√£o √© sempre bom verificar uma palavra m√°scara apropriada quando explorar outros modelos. Um modo de checar isso √© olhando para a palavra m√°scara usada no widget.

<Tip>

‚úèÔ∏è **Experimente!**  Pesquise pelo modelo `bert-base-cased` no Hub e identifique suas palavras m√°scara no widget da API de infer√™ncia. O que esse modelo prediz para a senten√ßa em nosso `pipeline` no exemplo acima?

</Tip>

## Reconhecimento de entidades nomeadas

Reconhecimento de Entidades Nomeadas (NER) √© uma tarefa onde o modelo tem de achar quais partes do texto correspondem a entidades como pessoas, locais, organiza√ß√µes. Vamos olhar em um exemplo:

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

Aqui o modelo corretamente identificou que Sylvain √© uma pessoa (PER), Hugging Face √© uma organiza√ß√£o (ORG), e Brooklyn √© um local (LOC).

N√≥s passamos a op√ß√£o `grouped_entities=True` na cria√ß√£o da fun√ß√£o do pipelina para dize-lo para reagrupar juntos as partes da senten√ßa que correspondem √† mesma entidade: aqui o modelo agrupou corretamente "Hugging" e "Face" como √∫nica organiza√ß√£o, ainda que o mesmo nome consista em m√∫ltiplas palavras. Na verdade, como veremos no pr√≥ximo cap√≠tulo, o pr√©-processamento at√© mesmo divide algumas palavras em partes menores. Por exemplo, `Sylvain`  √© dividido em 4 peda√ßos: `S`, `##yl`, `##va`, e `##in`. No passo de p√≥s-processamento, o pipeline satisfatoriamente reagrupa esses peda√ßos.

<Tip>

‚úèÔ∏è **Experimente!** Procure no Model Hub por um modelo capaz de fazer o tageamento de partes do discurso (usualmente abreviado como POS) em ingl√™s. O que o modelo prediz para a senten√ßa no exemplo acima?

</Tip>

## Responder perguntas

O pipeline `question-answering` responde perguntas usando informa√ß√µes dado um contexto:

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Note que o pipeline funciona atrav√©s da extra√ß√£o da informa√ß√£o dado um contexto; n√£o gera uma resposta.

## Sumariza√ß√£o

Sumariza√ß√£o √© uma tarefa de reduzir um texto em um texto menor enquanto pega toda (ou boa parte) dos aspectos importantes do texto referenciado. Aqui um exemplo:

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

Como a gera√ß√£o de texto, voc√™ pode especificar o tamanho m√°ximo `max_length` ou m√≠nimo `min_length` para o resultado.


## Tradu√ß√£o

Para tradu√ß√£o, voc√™ pode usar o modelo default se voc√™ der um par de idiomas no nome da tarefa (tal como  `"translation_en_to_fr"`, para traduzir ingl√™s para franc√™s), mas a maneira mais f√°cil √© pegar o moddelo que voc√™ quiser e usa-lo no [Model Hub](https://huggingface.co/models). Aqui n√≥s iremos tentar traduzir do Franc√™s para o Ingl√™s:

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Como a gera√ß√£o de texto e a sumariza√ß√£o, voc√™ pode especificar o tamanho m√°ximo  `max_length`  e m√≠nimo `min_length` para o resultado.

<Tip>

‚úèÔ∏è **Experimente!** Pesquise por modelos de tradu√ß√£o em outras l√≠nguas e experimente traduzir a senten√ßa anterior em idiomas diferentes.

</Tip>

Os pipelines mostrados at√© agora s√£o em sua maioria para prop√≥sitos demonstrativos. Eles foram programados para tarefas espec√≠ficas e n√£o podem performar varia√ß√µes delas. No pr√≥ximo cap√≠tulo, voc√™ aprender√° o que est√° por dentro da fun√ß√£o `pipeline()` e como customizar seu comportamento.
