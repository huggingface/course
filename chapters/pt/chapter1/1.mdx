# Introdu√ß√£o

## Bem-vindo(a) ao Curso ü§ó!

<Youtube id="00GKzGyWFEs" />

Esse curso te ensinar√° sobre processamento de linguagem natural (PLN, ou NLP em ingl√™s) usando as bibliotecas do ecossistema [Hugging Face](https://huggingface.co/) ‚Äî [ü§ó Transformers](https://github.com/huggingface/transformers), [ü§ó Datasets](https://github.com/huggingface/datasets), [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) e [ü§ó Accelerate](https://github.com/huggingface/accelerate) ‚Äî assim como a [Hugging Face Hub](https://huggingface.co/models). √â completamente gratuito e sem an√∫ncios!


## O que esperar?

Aqui uma vis√£o geral do curso:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- Cap√≠tulos 1 ao 4 d√° uma introdu√ß√£o para os principais conceitos da biblioteca Transformers ü§ó. No final dessa parte do curso, voc√™ se tornar√° familiar sobre como os modelos Transformers funcionam, saber√° como usar o modelo da [Hugging Face Hub](https://huggingface.co/models), fazer um ajuste fino em um dataset e compartilhar os resultados no Hub!
- Cap√≠tulo 5 ao 8 ensina o b√°sico dos ü§ó Datasets e ü§ó Tokenizadores antes de mergulhar nas tarefas cl√°ssicas de NLP. Ao final dessa parte, voc√™ ser√° capaz de resolver por conta pr√≥pria os problemas mais comuns de NLP. Ao longo do caminho, voc√™ aprender√° como construir e compartilhar demonstra√ß√µes de seus modelos e otimiza-los para ambientes de produ√ß√£o. No final dessa parte, voc√™ ser√° capaz de aplicar a ü§ó Transformers para (quase) qualquer problema de aprendizagem de m√°quina!

Esse curso:
* Requer um bom conhecimento de Python
* √â melhor aproveitado depois de um curso de introdu√ß√£o ao deep learning (aprendizagem profunda), como o da [fast.ai](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/)  ou um dos programas desenvolvidos pela [DeepLearning.AI](https://www.deeplearning.ai/)
* N√£o se espera nenhum conhecimento pr√©vio em [PyTorch](https://pytorch.org/)  ou em [TensorFlow](https://www.tensorflow.org/) , ainda que certa familiaridade com ambas as ferramentas seja proveitoso

Depois de voc√™ ter completado esse curso, n√≥s recomendamos dar uma olhada na especializa√ß√£o da DeepLearning.AI de [Processamento de Linguagem Natural](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh), que cobre uma grande gama de modelos de NLP como naive Bayes e LSTMs que valem bastante a pena ter conhecimento!

## Quem n√≥s somos?

Sobre os autores:

**Matthew Carrigan** √© Engenheiro de Machine Learning na Hugging Face. Ele mora em Dublin na Irlanda e anteriormente trabalhou como Engenheiro de ML na Parse.ly e antes disso como pesquisador p√≥s-doc na Trinity College Dublin. Ele n√£o acredita que chegaremos a um rendimento anual bruto corrigido escalando as arquiteturas existentes, mas de qualquer forma ele tem grandes esperan√ßas na imortalidade das m√°quinas.

**Lysandre Debut** √© um Engenheiro de Machine Learning na Hugging Face e tem trabalhado para a biblioteca ü§ó Transformers desde seus est√°gios iniciais de desenvolvimento. Seu objetivo √© fazer com que a √°rea de NLP se torne acess√≠vel para qualquer pessoa atrav√©s do desenvolvimento de ferramentas com uma API simples.

**Sylvain Gugger** √© um Engenheiro Pesquisador na Hugging Face e um dos principais mantenedores da biblioteca ü§ó Transformers. Anteriormente ele foi Cientista Pesquisador na fast.ai, e co-escreveu _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ com Jeremy Howard. O principal foco de sua pesquisa est√° em fazer o deep learning mais acess√≠vel, atrav√©s de desenhos e t√©cnicas de aprimoramento que permitam que modelos sejam treinados mais r√°pidos e com recursos limitados.

**Merve Noyan** √© um desenvolvedor e evangelista na Hugging Face, trabalhando no desenvolvimento e constru√ß√£o de conte√∫dos envolta da tem√°tica de democratiza√ß√£o do Machine Learning para todas as pessoas.

**Lucile Saulnier** √© uma Engenheira de Machine Learning na Hugging Face, desenvolvendo e apoiando o uso de ferramentas de c√≥digo aberto. Ela tamb√©m √© ativamente envolvida em muitos projetos de pesquisa no campo do Processamento de Linguagem natural assim como em treinamentos colaborativos e BigScience.

**Lewis Tunstall**  √© um Engenheiro de Machine Learning na Hugging Face, focado no desenvolvimento de ferramentas open-source e em faz√™-las amplamente acess√≠veis pela comunidade. Ele tamb√©m √© co-autor do livro que est√° pra lan√ßar [O‚ÄôReilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/).

**Leandro von Werra**  √© um Engenheiro de Machine Learning no time de open-source na Hugging Face e tamb√©m co-autor do livro [O‚ÄôReilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/). Ele tem muitos anos de experi√™ncia na ind√∫stria trazendo projetos de NLP para produ√ß√£o trabalhando com v√°rias stacks de Machine Learning.

Est√° pronto para seguir? Nesse cap√≠tulo, voc√™ aprender√°:
* Como usar a fun√ß√£o `pipeline()`  para solucionar tarefas de NLP tais como gera√ß√£o de texto e classifica√ß√£o
* Sobre a arquitetura Transformer
* Como distinguir entre as arquiteturas encoder, decoder, encoder-decoder e seus casos de uso
