# Resumo

Nesse cap√≠tulo, voc√™ viu como abordar diferentes tarefas de NLP usando a fun√ß√£o de alto n√≠vel `pipeline()` da biblioteca ü§ó Transformers. Voc√™ tamb√©m viu como pesquisar e usar modelos no Hub, bem como usar a API de infer√™ncia para testar os modelos diretamente em seu navegador.

Discutimos como os modelos Transformers funcionam em alto n√≠vel e falamos sobre a import√¢ncia do aprendizado de transfer√™ncia (transfer learning) e do ajuste fino. Um aspecto chave √© que voc√™ pode usar a arquitetura completa ou apenas o codificador ou decodificador, dependendo do tipo de tarefa que voc√™ pretende resolver. A tabela a seguir resume isso:

| Modelo          | Exemplos                                   | Tarefas                                                                          |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Classifica√ß√£o de senten√ßas, reconhecimento de entidades nomeadas, Q&A            |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Gera√ß√£o de texto                                                                 |
| Encoder-decoder | BART, T5, Marian, mBART                    | Sumariza√ß√£o, tradu√ß√£o, perguntas e respostas gerativas                           |
