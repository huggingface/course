# Question√°rio de fim de cap√≠tulo

Este cap√≠tulo cobriu muito terreno! N√£o se preocupe se voc√™ n√£o entendeu todos os detalhes; os pr√≥ximos cap√≠tulos o ajudar√£o a entender como as coisas funcionam debaixo do cap√¥.

Primeiro, por√©m, vamos testar o que voc√™ aprendeu neste cap√≠tulo!

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#1-explore-the-hub-and-look-for-the-roberta-large-mnli-checkpoint-what-task-does-it-perform)1. Explore o Hub e olhe para o checkpoint¬†`roberta-large-mnli`¬†. Que tarefa ele executa?

<Question choices={[ { text: "Summariza√ß√£o", explica√ß√£o: "Olhe novamente na p√°gina <a href="[https://huggingface.co/roberta-large-mnli">roberta-large-mnli](https://huggingface.co/roberta-large-mnli%22%3Eroberta-large-mnli)." }, { text: "Classifica√ß√£o de texto", explica√ß√£o: "Mais precisamente, ele classifica se duas ou mais senten√ßas est√£o logicamente conectadas entre tr√™s r√≥tulos (contradi√ß√£o, neutro, vincula√ß√£o) ‚Äî uma tarefa tamb√©m chamada de¬†_infer√™ncia de linguagem natural_.", correct: true }, { text: "Gera√ß√£o de texto", explica√ß√£o: "Olhe novamente na p√°gina <a href="[https://huggingface.co/roberta-large-mnli">roberta-large-mnli](https://huggingface.co/roberta-large-mnli%22%3Eroberta-large-mnli)" } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#2-what-will-the-following-code-return)2. O que o c√≥digo a seguir retornar√°?
```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

<Question choices={[ { text: "Ele retornar√° pontua√ß√µes de classifica√ß√£o para esta frase, com r√≥tulos "positivo" ou "negativo".", explica√ß√£o: "Isso est√° incorreto - isso seria um pipeline de 'an√°lise de sentimentos'." }, { text: "Ele retornar√° um texto gerado completando esta frase.", explica√ß√£o: "Isso est√° incorreto - seria um pipeline de ``gera√ß√£o de texto`.", }, { text: "Ele retornar√° as palavras que representam pessoas, organiza√ß√µes ou locais.", explica√ß√£o: "Al√©m disso, com¬†`grouped_entities=True`, ele agrupar√° as palavras pertencentes √† mesma entidade, como "Hugging Face".", correct: true } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#3-what-should-replace--in-this-code-sample)3. O que dever√° substituir ... nesse trecho de c√≥digo?

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

<Question choices={[ { text: "Esta <mask> est√° esperando por voc√™.", explica√ß√£o: "Isso est√° incorreto. Confira o cart√£o modelo¬†`bert-base-cased`¬†e tente identificar seu erro." }, { text: "Esta [MASK] est√° esperando por voc√™.", explica√ß√£o: "Correto! O token de m√°scara deste modelo √© [MASK].", correct: true }, { text: "Este homem est√° esperando por voc√™.", explica√ß√£o: "Isso est√° incorreto. Esse pipeline preenche palavras mascaradas, portanto, precisa de um token de m√°scara em algum lugar." } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#4-why-will-this-code-fail)4. Por que esse c√≥digo ir√° dar erro?
```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

<Question choices={[ { text: "Esse pipeline exige que sejam fornecidos r√≥tulos para classificar esse texto.", explica√ß√£o: "Certo ‚Äî o c√≥digo correto precisa incluir¬†`candidate_labels=[...]`.", correct: true }, { text: "Esse pipeline requer v√°rias frases, n√£o apenas uma.", explica√ß√£o: "Isso √© incorreto, mas quando usado corretamente, esse pipeline pode levar uma lista de frases para processar (como todos os outros pipelines)." }, { text: "A biblioteca¬†ü§ó¬†Transformers est√° quebrada, como sempre.", explica√ß√£o: "N√£o vamos dignificar esta resposta com um coment√°rio!" }, { text: "Esse pipeline requer entradas mais longas; esta √© muito curta.", explica√ß√£o: "Isso est√° incorreto. Observe que um texto muito longo ser√° truncado quando processado por esse pipeline." } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#5-what-does-transfer-learning-mean)5. O que "transfer learning" significa?

<Question choices={[ { text: "Transferir o conhecimento de um modelo pr√©-treinado para um novo modelo treinando-o no mesmo conjunto de dados.", explica√ß√£o: "N√£o, seriam duas vers√µes do mesmo modelo." }, { text: "Transferir o conhecimento de um modelo pr√©-treinado para um novo modelo inicializando o segundo modelo com os pesos do primeiro modelo.", explica√ß√£o: "Correto: quando o segundo modelo √© treinado em uma nova tarefa, ele¬†_transfere_¬†o conhecimento do primeiro modelo.", correct: true }, { text: "Transferir o conhecimento de um modelo pr√©-treinado para um novo modelo construindo o segundo modelo com a mesma arquitetura do primeiro modelo", explica√ß√£o: "A arquitetura √© apenas a forma como o modelo √© constru√≠do; n√£o h√° conhecimento compartilhado ou transferido neste caso." } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#6-true-or-false-a-language-model-usually-does-not-need-labels-for-its-pretraining)6. Verdadeiro ou Falso? Um modelo de linguagem geralmente n√£o precisa de r√≥tulos para seu pr√©-treino.

<Question choices={[ { text: "True", explica√ß√£o: "The pretraining is usually¬†_self-supervised_, which means the labels are created automatically from the inputs (like predicting the next word or filling in some masked words).", correct: true }, { text: "False", explica√ß√£o: "This is not the correct answer." } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#7-select-the-sentence-that-best-describes-the-terms-model-architecture-and-weights)7. Selecione a senten√ßa que melhor descreve os termos "modelo", "arquitetura" e "pesos".

<Question choices={[ { text: "Se um modelo √© um edif√≠cio, sua arquitetura √© a planta e os pesos s√£o as pessoas que vivem dentro dele.", explica√ß√£o: "Seguindo essa met√°fora, os pesos seriam os tijolos e outros materiais utilizados na constru√ß√£o do edif√≠cio." }, { text: "Uma arquitetura √© um mapa para construir um modelo e seus pesos s√£o as cidades representadas no mapa.", explica√ß√£o: "O problema com essa met√°fora √© que um mapa geralmente representa uma realidade existente (h√° apenas uma cidade na Fran√ßa chamada Paris). Para uma determinada arquitetura, v√°rios pesos s√£o poss√≠veis." }, { text: "Uma arquitetura √© uma sucess√£o de fun√ß√µes matem√°ticas para construir um modelo e seus pesos s√£o os par√¢metros dessas fun√ß√µes.", explica√ß√£o: "O mesmo conjunto de fun√ß√µes matem√°ticas (arquitetura) pode ser usado para construir diferentes modelos usando diferentes par√¢metros (pesos).", correct: true } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#8-which-of-these-types-of-models-would-you-use-for-completing-prompts-with-generated-text)8. Quais desses tipos de modelos voc√™ usaria para completar comandos com textos gerados?

<Question choices={[ { text: "Um modelo de codificador", explica√ß√£o: "Um modelo de codificador gera uma representa√ß√£o de toda a frase que √© mais adequada para tarefas como classifica√ß√£o." }, { text: "Um modelo de decodificador", explica√ß√£o: "Os modelos de decodificadores s√£o perfeitamente adequados para gera√ß√£o de texto a partir de um prompt.", correct: true }, { text: "Um modelo de sequ√™ncia a sequ√™ncia", explica√ß√£o: "Os modelos de sequ√™ncia a sequ√™ncia s√£o mais adequados para tarefas em que voc√™ deseja gerar frases em rela√ß√£o √†s frases de entrada, n√£o a um determinado prompt." } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#9-which-of-those-types-of-models-would-you-use-for-summarizing-texts)9. Quais desses tipos de modelos voc√™ usaria para resumir textos?

<Question choices={[ { text: "Um modelo de codificador", explica√ß√£o: "Um modelo de codificador gera uma representa√ß√£o de toda a frase que √© mais adequada para tarefas como classifica√ß√£o." }, { text: "Um modelo de decodificador", explica√ß√£o: "Os modelos decodificadores s√£o bons para gerar texto de sa√≠da (como resumos), mas n√£o t√™m a capacidade de explorar um contexto como o texto inteiro para resumir.", explica√ß√£o: "Os modelos de sequ√™ncia a sequ√™ncia s√£o perfeitamente adequados para uma tarefa de sumariza√ß√£o.", correct: true } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#10-which-of-these-types-of-models-would-you-use-for-classifying-text-inputs-according-to-certain-labels)10. Quais desses tipos de modelos voc√™ usaria para classificar entradas de texto de acordo com determinados r√≥tulos?

<Question choices={[ { text: "Um modelo de codificador", explica√ß√£o: "Um modelo de codificador gera uma representa√ß√£o de toda a frase que √© perfeitamente adequada para uma tarefa como classifica√ß√£o.", correct: true }, { text: "Um modelo de decodificador", explica√ß√£o: "Os modelos decodificadores s√£o bons para gerar textos de sa√≠da, n√£o para extrair um r√≥tulo de uma frase." }, { text: "Um modelo de sequ√™ncia a sequ√™ncia", explica√ß√£o: "Os modelos de sequ√™ncia a sequ√™ncia s√£o mais adequados para tarefas em que voc√™ deseja gerar texto com base em uma frase de entrada, n√£o em um r√≥tulo.", } ]} />

### [](https://github.com/huggingface/course/blob/main/chapters/en/chapter1/10.mdx#11-what-possible-source-can-the-bias-observed-in-a-model-have)11. Que poss√≠vel fonte o vi√©s observado em um modelo pode ter?

<Question choices={[ { text: "O modelo √© uma vers√£o afinada de um modelo pr√©-treinado e pega seu vi√©s a partir dele.", explica√ß√£o: "Ao aplicar o Transfer Learning, o vi√©s no modelo pr√©-treinado usado persiste no modelo ajustado.", correct: true }, { text: "Os dados em que o modelo foi treinado s√£o enviesados.", explica√ß√£o: "Esta √© a fonte mais √≥bvia de vi√©s, mas n√£o a √∫nica.", correct: true }, { text: "A m√©trica para a qual o modelo estava otimizando √© enviesada.", explica√ß√£o: "Uma fonte menos √≥bvia de vi√©s √© a forma como o modelo √© treinado. Seu modelo ser√° otimizado cegamente para qualquer m√©trica que voc√™ escolher, sem pensar duas vezes.", correct: true } ]} />
