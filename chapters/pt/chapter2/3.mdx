<FrameworkSwitchCourse {fw} />

# Modelos

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
Nesta se√ß√£o, vamos analisar mais de perto a cria√ß√£o e a utiliza√ß√£o de um modelo. Vamos utilizar a classe `AutoModel`, que √© √∫til quando voc√™ quer instanciar qualquer modelo a partir de um checkpoint.

A classe `AutoModel` e todas as classes filhas s√£o na verdade simples wrapper sobre a grande variedade de modelos dispon√≠veis na biblioteca. √â um wrapper inteligente, pois pode automaticamente "adivinhar" a arquitetura apropriada do modelo para seu checkpoint, e ent√£o instancia um modelo com esta arquitetura.

{:else}
Nesta se√ß√£o, vamos analisar mais de perto a cria√ß√£o e a utiliza√ß√£o de um modelo. Vamos utilizar a classe `TFAutoModel`, que √© √∫til quando voc√™ quer instanciar qualquer modelo a partir de um checkpoint.

A classe `TFAutoModel` e todas as classes filhas s√£o na verdade simples  wrapper sobre a grande variedade de modelos dispon√≠veis na biblioteca. √â um wrapper inteligente, pois pode automaticamente "adivinhar" a arquitetura apropriada do modelo para seu checkpoint, e ent√£o instancia um modelo com esta arquitetura.

{/if}

Entretanto, se voc√™ conhece o tipo de modelo que deseja usar, pode usar diretamente a classe que define sua arquitetura. Vamos dar uma olhada em como isto funciona com um modelo BERT.

## Criando um Transformer

A primeira coisa que precisamos fazer para inicializar um modelo BERT √© carregar um objeto de configura√ß√£o:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Construindo a configura√ß√£o
config = BertConfig()

# Construindo o modelo a partir da configura√ß√£o
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Construindo a configura√ß√£o
config = BertConfig()

# Construindo o modelo a partir da configura√ß√£o
model = TFBertModel(config)
```
{/if}

A configura√ß√£o cont√©m muitos atributos que s√£o usados para construir o modelo:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

Embora voc√™ ainda n√£o tenha visto o que todos esses atributos fazem, voc√™ deve reconhecer alguns deles: o atributo `hidden_size` define o tamanho do vetor `hidden_states`, e o `num_hidden_layers`  define o n√∫mero de camadas que o Transformer possui.


### Diferentes m√©todos de inicializar o modelo

A cria√ß√£o de um modelo a partir da configura√ß√£o padr√£o o inicializa com valores aleat√≥rios:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# O modelo √© inicializado aleatoriamente!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# O modelo √© inicializado aleatoriamente!
```
{/if}

O modelo pode ser utilizado neste estado, mas produzir√° sa√≠das err√¥neas; ele precisa ser treinado primeiro. Poder√≠amos treinar o modelo a partir do zero na tarefa em m√£os, mas como voc√™ viu em [Cap√≠tulo 1](/course/pt/chapter1), isto exigiria muito tempo e muitos dados, e teria um impacto ambiental n√£o negligenci√°vel. Para evitar esfor√ßos desnecess√°rios e duplicados, normalmente √© poss√≠vel compartilhar e reutilizar modelos que j√° foram treinados.

Carregar um Transformer j√° treinado √© simples - podemos fazer isso utilizando o m√©todo `from_pretrained()`:

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

Como voc√™ viu anteriormente, poder√≠amos substituir o `BertModel` pela classe equivalente ao `AutoModel`. Faremos isto de agora em diante, pois isto produz um c√≥digo generalista a partir de um checkpoint; se seu c√≥digo funciona para checkpoint, ele deve funcionar perfeitamente com outro. Isto se aplica mesmo que a arquitetura seja diferente, desde que o checkpoint tenha sido treinado para uma tarefa semelhante (por exemplo, uma tarefa de an√°lise de sentimento).

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

Como voc√™ viu anteriormente, poder√≠amos substituir o `TFBertModel` pela classe equivalente ao `TFAutoModel`. Faremos isto de agora em diante, pois isto produz um c√≥digo generalista a partir de um checkpoint; se seu c√≥digo funciona para checkpoint, ele deve funcionar perfeitamente com outro. Isto se aplica mesmo que a arquitetura seja diferente, desde que o checkpoint tenha sido treinado para uma tarefa semelhante (por exemplo, uma tarefa de an√°lise de sentimento).


{/if}

No exemplo de c√≥digo acima n√£o utilizamos `BertConfig`, e em vez disso carregamos um modelo pr√©-treinado atrav√©s do identificador `bert-base-cased`. Este √© um checkpoint do modelo que foi treinado pelos pr√≥prios autores do BERT; voc√™ pode encontrar mais detalhes sobre ele em seu [model card](https://huggingface.co/bert-base-cased).

Este modelo agora √© inicializado com todos os pesos do checkpoint. Ele pode ser usado diretamente para infer√™ncia sobre as tarefas nas quais foi treinado, e tamb√©m pode ser *fine-tuned* (aperfei√ßoado) em uma nova tarefa. Treinando com pesos pr√©-treinados e n√£o do zero, podemos rapidamente alcan√ßar bons resultados.

Os pesos foram baixados e armazenados em cache (logo, para as futuras chamadas do m√©todo `from_pretrained()` n√£o ser√° realizado o download novamente) em sua respectiva pasta, que tem como padr√£o o path *~/.cache/huggingface/transformers*. Voc√™ pode personalizar sua pasta de cache definindo a vari√°vel de ambiente `HF_HOME`.

O identificador usado para carregar o modelo pode ser o identificador de qualquer modelo no Model Hub, desde que seja compat√≠vel com a arquitetura BERT. A lista completa dos checkpoints BERT dispon√≠veis podem ser encontrada [aqui].https://huggingface.co/models?filter=bert).

### M√©todos para salvar/armazenar o modelo

Salvar um modelo √© t√£o f√°cil quanto carregar um - utilizamos o m√©todo `save_pretrained()`, que √© an√°logo ao m√©todo `from_pretrained()`:

```py
model.save_pretrained("path_no_seu_computador")
```

Isto salva dois arquivos em seu disco:

{#if fw === 'pt'}
```
ls path_no_seu_computador

config.json pytorch_model.bin
```
{:else}
```
ls path_no_seu_computador

config.json tf_model.h5
```
{/if}

Se voc√™ der uma olhada no arquivo *config.json*, voc√™ reconhecer√° os atributos necess√°rios para construir a arquitetura modelo. Este arquivo tamb√©m cont√©m alguns metadados, como a origem do checkpoint e a vers√£o ü§ó Transformers que voc√™ estava usando quando salvou o checkpoint pela √∫ltima vez.

{#if fw === 'pt'}
O arquivo *pytorch_model.bin* √© conhecido como o *dicion√°rio de estado*; ele cont√©m todos os pesos do seu modelo. Os dois arquivos andam de m√£os dadas; a configura√ß√£o √© necess√°ria para conhecer a arquitetura de seu modelo, enquanto os pesos do modelo s√£o os par√¢metros de seu modelo.

{:else}
O arquivo *tf_model.h5* √© conhecido como o *dicion√°rio de estado*; ele cont√©m todos os pesos do seu modelo. Os dois arquivos andam de m√£os dadas; a configura√ß√£o √© necess√°ria para conhecer a arquitetura de seu modelo, enquanto os pesos do modelo s√£o os par√¢metros de seu modelo.

{/if}

## Usando um modelo de Transformer para infer√™ncia

Agora que voc√™ sabe como carregar e salvar um modelo, vamos tentar us√°-lo para fazer algumas predi√ß√µes. Os Transformers s√≥ podem processar n√∫meros - n√∫meros que o tokenizer gera. Mas antes de discutirmos os tokenizers, vamos explorar quais entradas o modelo aceita.

Os Tokenizers podem se encarregar de lan√ßar as entradas nos tensores da estrutura apropriada, mas para ajud√°-lo a entender o que est√° acontecendo, vamos dar uma r√°pida olhada no que deve ser feito antes de enviar as entradas para o modelo.

Digamos que temos um par de sequ√™ncias:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

O tokenizer os converte em √≠ndices de vocabul√°rio que s√£o normalmente chamados de *IDs de entrada*. Cada sequ√™ncia √© agora uma lista de n√∫meros! A sa√≠da resultante √©:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

Esta √© uma lista de sequ√™ncias codificadas: uma lista de listas. Os tensores s√≥ aceitam shapes (tamanhos) retangulares (pense em matrizes). Esta "matriz" j√° √© de forma retangular, portanto, convert√™-la em um tensor √© f√°cil:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### Usando os tensores como entradas para o modelo

Fazer uso dos tensores com o modelo √© extremamente simples - chamamos apenas o modelo com os inputs:

```py
output = model(model_inputs)
```

Embora o modelo aceite muitos argumentos diferentes, apenas os IDs de entrada s√£o necess√°rios. Explicaremos o que os outros argumentos fazem e quando eles s√£o necess√°rios mais tarde, mas primeiro precisamos olhar mais de perto os tokenizers que constroem as entradas que um Transformer pode compreender.
