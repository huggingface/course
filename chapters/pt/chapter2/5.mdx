<FrameworkSwitchCourse {fw} />

# Tratando sequ√™ncias m√∫ltiplas

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

Na se√ß√£o anterior, exploramos os casos mais simples de uso: fazer infer√™ncia sobre uma √∫nica sequ√™ncia de pequeno comprimento. No entanto, surgem algumas quest√µes:

- Como n√≥s tratamos diversas sequ√™ncias?
- Como n√≥s tratamos diversas sequ√™ncias *de diferentes tamanhos*?
- Os √≠ndices de vocabul√°rio s√£o as √∫nicas entradas que permitem que um modelo funcione bem?
- Existe uma sequ√™ncia muito longa?

Vamos ver que tipos de problemas estas quest√µes colocam, e como podemos resolv√™-los usando a API do ü§ó Transformers.

## Modelos esperam um batch de entradas

No exerc√≠cio anterior, voc√™ viu como as sequ√™ncias s√£o traduzidas em listas de n√∫meros. Vamos converter esta lista de n√∫meros em um tensor e envi√°-la para o modelo:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

Oh n√£o! Por que isso falhou? "Seguimos os passos do pipeline na se√ß√£o 2.

O problema √© que enviamos uma √∫nica sequ√™ncia para o modelo, enquanto que os ü§ó transformers esperam v√°rias senten√ßas por padr√£o. Aqui tentamos fazer tudo o que o tokenizer fez nos bastidores quando o aplicamos a uma `sequ√™ncia`, mas se voc√™ olhar com aten√ß√£o, ver√° que ele n√£o apenas converteu a lista de IDs de entrada em um tensor, mas acrescentou uma dimens√£o em cima dele:

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

Vamos tentar novamente e acrescentar uma nova dimens√£o:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

Printamos os IDs de entrada assim como os logits resultantes - aqui est√° a sa√≠da:

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

*Batching* √© o ato de enviar m√∫ltiplas senten√ßas atrav√©s do modelo, todas de uma s√≥ vez. Se voc√™ tiver apenas uma frase, voc√™ pode apenas construir um lote com uma √∫nica sequ√™ncia:

```
batched_ids = [ids, ids]
```

Este √© um lote de duas sequ√™ncias id√™nticas!

<Tip>

‚úèÔ∏è **Experimente!** Converta esta lista de `batched_ids` em um tensor e passe-a atrav√©s de seu modelo. Verifique se voc√™ obt√©m os mesmos logits que antes (mas duas vezes)!

</Tip>

O Batching permite que o modelo funcione quando voc√™ o alimenta com v√°rias frases. Usar v√°rias sequ√™ncias √© t√£o simples quanto construir um lote com uma √∫nica sequ√™ncia. H√° uma segunda quest√£o, no entanto. Quando voc√™ est√° tentando agrupar duas (ou mais) senten√ßas, elas podem ser de comprimentos diferentes. Se voc√™ j√° trabalhou com tensores antes, voc√™ sabe que eles precisam ser de forma retangular, ent√£o voc√™ n√£o ser√° capaz de converter a lista de IDs de entrada em um tensor diretamente. Para contornar este problema, normalmente realizamos uma *padroniza√ß√£o* (padding) nas entradas.


## Realizando padding nas entradas

A seguinte lista de listas n√£o pode ser convertida em um tensor:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Para contornar isso, usaremos *padding* para fazer com que nossos tensores tenham uma forma retangular. O padding garante que todas as nossas frases tenham o mesmo comprimento, acrescentando uma palavra especial chamada *padding token* √†s frases com menos valores. Por exemplo, se voc√™ tiver 10 frases com 10 palavras e 1 frase com 20 palavras, o padding garantir√° que todas as frases tenham 20 palavras. Em nosso exemplo, o tensor resultante se parece com isto:


```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

O padding do ID token pode ser encontrada em `tokenizer.pad_token_id`. Vamos utiliz√°-lo e enviar nossas duas frases atrav√©s do modelo individualmente e agrupadas em batches:

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

H√° algo errado com os logits em nossas predi√ß√µes em batches: a segunda fileira deveria ser a mesma que os logits para a segunda frase, mas temos valores completamente diferentes!

Isto porque a caracter√≠stica chave dos Transformer s√£o as camadas de aten√ß√£o que *contextualizam* cada token. Estes levar√£o em conta os tokens de padding, uma vez que atendem a todos os tokens de uma sequ√™ncia. Para obter o mesmo resultado ao passar frases individuais de diferentes comprimentos pelo modelo ou ao passar um batch com as mesmas frases e os paddings aplicados, precisamos dizer a essas camadas de aten√ß√£o para ignorar os tokens de padding. Isto √© feito com o uso de uma m√°scara de aten√ß√£o (*attention mask*).

## Attention masks

*Attention masks* s√£o tensores com a mesma forma exata do tensor de IDs de entrada, preenchidos com 0s e 1s: 1s indicam que os tokens correspondentes devem ser atendidas, e 0s indicam que os tokens correspondentes n√£o devem ser atendidas (ou seja, devem ser ignoradas pelas camadas de aten√ß√£o do modelo).

Vamos completar o exemplo anterior com uma m√°scara de aten√ß√£o:

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

Agora obtemos os mesmos logits para a segunda frase do batch.

Observe como o √∫ltimo valor da segunda sequ√™ncia √© um ID de padding, que √© um valor 0 na m√°scara de aten√ß√£o.

<Tip>

‚úèÔ∏è **Experimente!** Aplique a tokeniza√ß√£o manualmente nas duas frases usadas na se√ß√£o 2 ("I've been waiting for a HuggingFace course my whole life." e "I hate this so much!"). Passe-as atrav√©s do modelo e verifique se voc√™ obt√©m os mesmos logits que na se√ß√£o 2. Agora, agrupe-os usando o token de padding e depois crie a m√°scara de aten√ß√£o adequada. Verifique que voc√™ obtenha os mesmos resultados ao passar pelo modelo!

</Tip>

## Sequ√™ncias mais longas

Com os Transformer, h√° um limite para os comprimentos das sequ√™ncias, podemos passar os modelos. A maioria dos modelos manipula sequ√™ncias de at√© 512 ou 1024 tokens, e se chocar√° quando solicitados a processar sequ√™ncias mais longas. H√° duas solu√ß√µes para este problema:

- Use um modelo com suporte a um comprimento mais longo de sequ√™ncia.
- Trunque suas sequ√™ncias.

Os modelos t√™m diferentes comprimentos de sequ√™ncia suportados, e alguns s√£o especializados no tratamento de sequ√™ncias muito longas. O [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) √© um exemplo, e outro exemplo √© o [LED](https://huggingface.co/transformers/model_doc/led.html). Se voc√™ estiver trabalhando em uma tarefa que requer sequ√™ncias muito longas, recomendamos que voc√™ d√™ uma olhada nesses modelos.

Caso contr√°rio, recomendamos que voc√™ trunque suas sequ√™ncias, especificando o par√¢metro `max_sequence_length`:

```py
sequence = sequence[:max_sequence_length]
```
