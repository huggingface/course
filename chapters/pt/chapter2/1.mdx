# Introdu√ß√£o

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

Como voc√™ viu no [Capitulo 1](/course/pt/chapter1), normalmente modelos Transformers s√£o muito grandes. Com milh√µes a dezenas de *bilh√µes* de par√¢metros, o treinamento e o deploy destes modelos √© uma tarefa complicado. Al√©m disso, com novos modelos sendo lan√ßados quase diariamente e cada um tendo sua pr√≥pria implementa√ß√£o, experiment√°-los a todos n√£o √© tarefa f√°cil.

A biblioteca ü§ó Transformers foi criado para resolver este problema. Seu objetivo √© fornecer uma API √∫nica atrav√©s do qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter√≠sticas da biblioteca s√£o:

- **F√°cil de usar**: Baixar, carregar e usar um modelo de processamento natural de linguagem (PNL) de √∫ltima gera√ß√£o para infer√™ncia pode ser feito em apenas duas linhas de c√≥digo
- **Flexibilidade**: Em sua ess√™ncia, todos os modelos s√£o uma simples classe PyTorch `nn.Module` ou TensorFlow `tf.keras.Model` e podem ser tratadas como qualquer outro modelo em seus respectivos frameworks de machine learning (ML).
- **Simplicidade**: Quase nenhuma abstra√ß√£o √© feita em toda a biblioteca. O "Tudo em um arquivo" √© um conceito principal: o "passe para frente" de um modelo √© inteiramente definido em um √∫nico arquivo, de modo que o c√≥digo em si seja compreens√≠vel e hacke√°vel.

Esta √∫ltima caracter√≠stica torna ü§ó Transformers bem diferente de outras bibliotecas ML que modelos e/ou configura√ß√µes s√£o compartilhados entre arquivos; em vez disso, cada modelo tem suas pr√≥prias camadas. Al√©m de tornar os modelos mais acess√≠veis e compreens√≠veis, isto permite que voc√™ experimente facilmente um modelo sem afetar outros.

Este cap√≠tulo come√ßar√° com um exemplo de ponta a ponta onde usamos um modelo e um tokenizer juntos para replicar a fun√ß√£o `pipeline()` introduzida no [Capitulo 1](/course/pt/chapter1). A seguir, discutiremos o modelo da API: onde veremos profundamente as classes de modelo e configura√ß√£o, al√©m de mostrar como carregar um modelo e como ele processa as entradas num√©ricas para as previs√µes de sa√≠da. 

Depois veremos a API do tokenizer, que √© o outro componente principal da fun√ß√£o `pipeline()`. Os Tokenizers cuidam da primeira e da √∫ltima etapa do processamento, cuidando da convers√£o de texto para entradas num√©ricas para a rede neural, e da convers√£o de volta ao texto quando for necess√°rio. Por fim, mostraremos a voc√™ como lidar com o envio de v√°rias frases atrav√©s de um modelo em um batch preparado, depois olharemos tudo mais atentamente a fun√ß√£o de alto n√≠vel `tokenizer()`.


<Tip>
‚ö†Ô∏è Para beneficiar-se de todos os recursos dispon√≠veis com o Model Hub e ü§ó Transformers, recomendamos  <a href="https://huggingface.co/join"> criar uma conta</a>.
</Tip>