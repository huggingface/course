# IntroduÃ§Ã£o

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

Como vocÃª viu no [Capitulo 1](/course/pt/chapter1), normalmente modelos Transformers sÃ£o muito grandes. Com milhÃµes a dezenas de *bilhÃµes* de parÃ¢metros, o treinamento e o deploy destes modelos Ã© uma tarefa complicado. AlÃ©m disso, com novos modelos sendo lanÃ§ados quase diariamente e cada um tendo sua prÃ³pria implementaÃ§Ã£o, experimentÃ¡-los a todos nÃ£o Ã© tarefa fÃ¡cil.

A biblioteca ğŸ¤— Transformers foi criado para resolver este problema. Seu objetivo Ã© fornecer uma API Ãºnica atravÃ©s do qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracterÃ­sticas da biblioteca sÃ£o:

- **FÃ¡cil de usar**: Baixar, carregar e usar um modelo de processamento natural de linguagem (PNL) de Ãºltima geraÃ§Ã£o para inferÃªncia pode ser feito em apenas duas linhas de cÃ³digo
- **Flexibilidade**: Em sua essÃªncia, todos os modelos sÃ£o uma simples classe PyTorch `nn.Module` ou TensorFlow `tf.keras.Model` e podem ser tratadas como qualquer outro modelo em seus respectivos frameworks de machine learning (ML).
- **Simplicidade**: Quase nenhuma abstraÃ§Ã£o Ã© feita em toda a biblioteca. O "Tudo em um arquivo" Ã© um conceito principal: o "passe para frente" de um modelo Ã© inteiramente definido em um Ãºnico arquivo, de modo que o cÃ³digo em si seja compreensÃ­vel e hackeÃ¡vel.

Esta Ãºltima caracterÃ­stica torna ğŸ¤— Transformers bem diferente de outras bibliotecas ML que modelos e/ou configuraÃ§Ãµes sÃ£o compartilhados entre arquivos; em vez disso, cada modelo tem suas prÃ³prias camadas. AlÃ©m de tornar os modelos mais acessÃ­veis e compreensÃ­veis, isto permite que vocÃª experimente facilmente um modelo sem afetar outros.

Este capÃ­tulo comeÃ§arÃ¡ com um exemplo de ponta a ponta onde usamos um modelo e um tokenizer juntos para replicar a funÃ§Ã£o `pipeline()` introduzida no [Capitulo 1](/course/pt/chapter1). A seguir, discutiremos o modelo da API: onde veremos profundamente as classes de modelo e configuraÃ§Ã£o, alÃ©m de mostrar como carregar um modelo e como ele processa as entradas numÃ©ricas para as previsÃµes de saÃ­da. 

Depois veremos a API do tokenizer, que Ã© o outro componente principal da funÃ§Ã£o `pipeline()`. Os Tokenizers cuidam da primeira e da Ãºltima etapa do processamento, cuidando da conversÃ£o de texto para entradas numÃ©ricas para a rede neural, e da conversÃ£o de volta ao texto quando for necessÃ¡rio. Por fim, mostraremos a vocÃª como lidar com o envio de vÃ¡rias frases atravÃ©s de um modelo em um batch preparado, depois olharemos tudo mais atentamente a funÃ§Ã£o de alto nÃ­vel `tokenizer()`.


> [!TIP]
> âš ï¸ Para beneficiar-se de todos os recursos disponÃ­veis com o Model Hub e ğŸ¤— Transformers, recomendamos  <a href="https://huggingface.co/join"> criar uma conta</a>.