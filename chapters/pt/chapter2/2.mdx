<FrameworkSwitchCourse {fw} />

# Por dentro da fun√ß√£o pipeline

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
Esta √© a primeira se√ß√£o onde o conte√∫do √© ligeiramente diferente, dependendo se voc√™ usa PyTorch e TensorFlow. Para selecionar a plataforma que voc√™ prefere, basta alterar no bot√£o no topo.
</Tip>

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

Vamos come√ßar com um exemplo completo, dando uma olhada no que acontece dentro da fun√ß√£o quando executamos o seguinte c√≥digo no [Cap√≠tulo 1](/course/pt/chapter1):

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

tendo o resultado:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Como visto no [Cap√≠tulo 1](/course/pt/chapter1), este pipeline agrupa os tr√™s passos: o pr√©-processamento, passagem das entradas atrav√©s do modelo, e o p√≥s-processamento:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="O pipeline NLP completa: tokeniza√ß√£o do texto, convers√£o para IDs, e infer√™ncia atrav√©s do Transformer e pela 'cabe√ßa' do modelo."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="O pipeline NLP completa: tokeniza√ß√£o do texto, convers√£o para IDs, e infer√™ncia atrav√©s do Transformer e pela 'cabe√ßa' do modelo."/>
</div>

Vamos rever rapidamente cada um deles.

## Pr√©-processamento com o tokenizer

Como outras redes neurais, os Transformers n√£o podem processar texto bruto diretamente, portanto, o primeiro passo do nosso pipeline √© converter as entradas de texto em n√∫meros que o modelo possa fazer sentido. Para fazer isso, usamos um *tokenizer*, que ser√° respons√°vel por:

- Dividir a entrada em palavras, sub-palavras ou s√≠mbolos (como pontua√ß√£o) que s√£o chamados de *tokens*.
- Mapeando cada ficha para um n√∫mero inteiro
- Adicionando entradas adicionais que podem ser √∫teis ao modelo

Todo esse pr√©-processamento precisa ser feito exatamente da mesma forma que quando o modelo foi pr√©-treinado, ent√£o precisamos primeiro baixar essas informa√ß√µes do [Model Hub](https://huggingface.co/models). Para isso, utilizamos a classe `AutoTokenizer` e seu m√©todo `from_pretrained()`. Utilizando o nome do ponto de verifica√ß√£o de nosso modelo, ele ir√° buscar automaticamente os dados associados ao tokenizer do modelo e armazen√°-los em cache (portanto, ele s√≥ √© baixado na primeira vez que voc√™ executar o c√≥digo abaixo).

Desde que o checkpoint default do pipeline `sentiment-analysis` √© `distilbert-base-uncased-finetuned-sst-2-english` (voc√™ pode ver o card do modelo [aqui](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), ent√£o executamos o seguinte:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

Assim que tivermos o tokenizer, podemos passar diretamente nossas frases para ele e receberemos de volta um dicion√°rio que est√° pronto para alimentar nosso modelo! A √∫nica coisa que falta fazer √© converter a lista de identifica√ß√µes de entrada em tensores.

Voc√™ pode usar ü§ó Transformers sem ter que se preocupar com qual estrutura ML √© usada como backend; pode ser PyTorch ou TensorFlow, ou Flax para alguns modelos. Entretanto, os Transformers s√≥ aceitam *tensores* como entrada. Se esta √© a primeira vez que voc√™ ouve falar de tensores, voc√™ pode pensar neles como matrizes da NumPy. Uma matriz NumPy pode ser um escalar (0D), um vetor (1D), uma matriz (2D), ou ter mais dimens√µes. √â efetivamente um tensor; os tensores de outras estruturas ML comportam-se de forma semelhante, e geralmente s√£o t√£o simples de instanciar como os arrays da NumPy.

Para especificar o tipo de tensores que queremos recuperar (PyTorch, TensorFlow ou NumPy), utilizamos o argumento `return_tensors`:

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

N√£o se preocupe ainda com o truncamento e o padding; explicaremos isso mais tarde. As principais coisas a lembrar aqui s√£o que voc√™ pode passar uma frase ou uma lista de frases, bem como especificar o tipo de tensores que voc√™ quer recuperar (se nenhum tipo for passado, voc√™ receber√° uma lista de listas como resultado).

{#if fw === 'pt'}

Eis como s√£o os resultados como tensores PyTorch:

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

Eis como s√£o os resultados como tensores TensorFlow:

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

A sa√≠da em si √© um dicion√°rio contendo duas chaves, `input_ids' e `attention_mask'. O `input_ids' cont√©m duas linhas de inteiros (uma para cada frase) que s√£o os identificadores √∫nicos dos tokens em cada frase. Explicaremos o que √© a "m√°scara de aten√ß√£o" (attention mask) mais adiante neste cap√≠tulo. 

## Indo adianta pelo modelo

{#if fw === 'pt'}
Podemos baixar nosso modelo pr√©-treinado da mesma forma que fizemos com nosso tokenizer. ü§ó Transformers fornece uma classe `AutoModel` que tamb√©m tem um m√©todo `from_pretrained()`:

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
Podemos baixar nosso modelo pr√©-treinado da mesma forma que fizemos com nosso tokenizer. ü§ó Transformers fornece uma classe "TFAutoModel" que tamb√©m tem um m√©todo "from_pretrained":


```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

Neste trecho de c√≥digo, fizemos o download do mesmo checkpoint que usamos anteriormente em nosso pipeline (j√° deveria estar em cache) e instanciamos um modelo com ele.

Esta arquitetura cont√©m apenas o m√≥dulo base do Transformer: dadas algumas entradas, ele produz o que chamaremos de *hidden states* (estados ocultos), tamb√©m conhecidos como *features* (caracter√≠sticas). Para cada modelo de entrada, recuperaremos um vetor de alta dimensionalidade representando a **compreens√£o contextual dessa entrada pelo Transformer***.

Se isto n√£o faz sentido, n√£o se preocupe com isso. Explicaremos tudo isso mais tarde.

Embora estes hidden states possam ser √∫teis por si mesmos, eles geralmente s√£o entradas para outra parte do modelo, conhecida como *head* (cabe√ßa). No [Cap√≠tulo 1](/course/pt/chapter1), as diferentes tarefas poderiam ter sido realizadas com a mesma arquitetura, mas cada uma destas tarefas teria uma head diferente associada a ela.

### Um vetor de alta dimensionalidade?

A sa√≠da vetorial pelo m√≥dulo do Transformer √© geralmente grande. Geralmente tem tr√™s dimens√µes:

- **Tamanho do lote** (Batch size): O n√∫mero de sequ√™ncias processadas de cada vez (2 em nosso exemplo).
- **Tamanho da sequencia** (Sequence length): O comprimento da representa√ß√£o num√©rica da sequ√™ncia (16 em nosso exemplo).
- **Tamanho oculto** (Hidden size): A dimens√£o vetorial de cada modelo de entrada.

Diz-se que √© "de alta dimensionalidade" por causa do √∫ltimo valor. O tamanho oculto pode ser muito grande (768 √© comum para modelos menores, e em modelos maiores isto pode chegar a 3072 ou mais).

Podemos ver isso se alimentarmos os inputs que pr√©-processamos para nosso modelo:

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

Observe que as sa√≠das dos ü§ó Transformer se comportam como 'tuplas nomeadas' (namedtuple) ou dicion√°rios. Voc√™ pode acessar os elementos por atributos (como fizemos) ou por chave (`outputs["last_hidden_state"]`), ou mesmo por √≠ndice se voc√™ souber exatamente onde o que est√° procurando (`outputs[0]`).

### Cabe√ßa do modelo (model heads): Fazendo sentido a partir dos n√∫meros

As *heads* do modelo usam o vetor de alta dimensionalidade dos hidden states como entrada e os projetam em uma dimens√£o diferente. Eles s√£o geralmente compostos de uma ou algumas camadas lineares:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="Uma rede Transformer ao lado de sua head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="Uma rede Transformer ao lado de sua head."/>
</div>

A sa√≠da do Transformer √© enviada diretamente para a *head* do modelo a ser processado.

Neste diagrama, o modelo √© representado por sua camada de embeddings (vetores) e pelas camadas subsequentes. A camada de embeddings converte cada ID de entrada na entrada tokenizada em um vetor que representa o token associado. As camadas subsequentes manipulam esses vetores usando o mecanismo de aten√ß√£o para produzir a representa√ß√£o final das senten√ßas.

H√° muitas arquiteturas diferentes dispon√≠veis no ü§ó Transformers, com cada uma projetada em torno de uma tarefa espec√≠fica. Aqui est√° uma lista por **algumas** destas tarefas:

- `*Model` (recuperar os hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- e outros ü§ó

{#if fw === 'pt'}
Para nosso exemplo, precisaremos de um modelo com uma *head* de classifica√ß√£o em sequencia (para poder classificar as senten√ßas como positivas ou negativas). Portanto, n√£o utilizaremos a classe `AutoModel`, mas sim, a classe `AutoModelForSequenceClassification`:

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
Para nosso exemplo, precisaremos de um modelo com uma *head* de classifica√ß√£o em sequencia (para poder classificar as senten√ßas como positivas ou negativas). Portanto, n√£o utilizaremos a classe `TFAutoModel`, mas sim, a classe `TFAutoModelForSequenceClassification`:

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

Agora se observarmos o tamanho dos nossos inputs, a dimensionalidade ser√° muito menor: a *head* do modelo toma como entrada os vetores de alta dimensionalidade que vimos anteriormente, e os vetores de sa√≠da contendo dois valores (um por label):

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

Como temos apenas duas senten√ßas e duas labels, o resultado que obtemos de nosso modelo √© de tamanho 2 x 2.

## P√≥s-processamento da sa√≠da

Os valores que obtemos como resultado de nosso modelo n√£o fazem necessariamente sentido sozinhos. Vamos dar uma olhada:

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

Nosso modelo previu `[-1.5607, 1.6123]` para a primeira frase e `[ 4.1692, -3.3464]` para a segunda. Essas n√£o s√£o probabilidades, mas *logits*, a pontua√ß√£o bruta e n√£o normalizada produzida pela √∫ltima camada do modelo. Para serem convertidos em probabilidades, eles precisam passar por uma camada [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) (todas sa√≠das dos ü§ó Transformers produzem  *logits*, j√° que a fun√ß√£o de *loss* (perda) para treinamento geralmente fundir√° a √∫ltima fun√ß√£o de ativa√ß√£o, como SoftMax, com a fun√ß√£o de *loss* real, por exemplo a *cross entropy*):

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

Agora podemos ver que o modelo previu `[0.0402, 0.9598]` para a primeira frase e `[0.9995, 0.0005]` para a segunda. Estas s√£o pontua√ß√µes de probabilidade reconhec√≠veis.

Para obter as etiquetas correspondentes a cada posi√ß√£o, podemos inspecionar o atributo `id2label` da configura√ß√£o do modelo (mais sobre isso na pr√≥xima se√ß√£o):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

Agora podemos concluir que o modelo previu o seguinte:

- A primeira frase: NEGATIVE: 0.0402, POSITIVE: 0.9598
- Segunda frase: NEGATIVE: 0.9995, POSITIVE: 0.0005

Reproduzimos com sucesso as tr√™s etapas do pipeline: o pr√©-processamento, passagem das entradas atrav√©s do modelo, e o p√≥s-processamento! Agora, vamos levar algum tempo para mergulhar mais fundo em cada uma dessas etapas.

<Tip>

‚úèÔ∏è **Experimente!** Escolha duas (ou mais) textos pr√≥prios e passe-os atrav√©s do pipeline `sentiment-analysis`. Em seguida, replique as etapas que voc√™ mesmo viu aqui e verifique se voc√™ obt√©m os mesmos resultados!

</Tip>
