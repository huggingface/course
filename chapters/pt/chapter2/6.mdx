<FrameworkSwitchCourse {fw} />

# Colocando tudo junto

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter2/section6_tf.ipynb"},
]} />

{/if}

Nas √∫ltimas se√ß√µes, temos feito o nosso melhor para fazer a maior parte do trabalho √† m√£o. Exploramos como funcionam os tokenizers e analisamos a tokeniza√ß√£o, convers√£o para IDs de entrada, padding, truncagem e m√°scaras de aten√ß√£o.

Entretanto, como vimos na se√ß√£o 2, a API dos ü§ó Transformers pode tratar de tudo isso para n√≥s com uma fun√ß√£o de alto n√≠vel, na qual mergulharemos aqui. Quando voc√™ chama seu `tokenizer` diretamente na frase, voc√™ recebe de volta entradas que est√£o prontas para passar pelo seu modelo:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Aqui, a vari√°vel `model_inputs` cont√©m tudo o que √© necess√°rio para que um modelo funcione bem. Para DistilBERT, isso inclui os IDs de entrada, bem como a m√°scara de aten√ß√£o. Outros modelos que aceitam entradas adicionais tamb√©m ter√£o essas sa√≠das pelo objeto `tokenizer`.

Como veremos em alguns exemplos abaixo, este m√©todo √© muito poderoso. Primeiro, ele pode simbolizar uma √∫nica sequ√™ncia:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Tamb√©m lida com v√°rias sequ√™ncias de cada vez, sem nenhuma mudan√ßa na API:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

Ela pode ser aplicada de acordo com v√°rios objetivos:

```py
# Ir√° preencher as sequ√™ncias at√© o comprimento m√°ximo da sequ√™ncia
model_inputs = tokenizer(sequences, padding="longest")

# Ir√° preencher as sequ√™ncias at√© o comprimento m√°ximo do modelo
# (512 para o modelo BERT ou DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Ir√° preencher as sequ√™ncias at√© o comprimento m√°ximo especificado
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

Tamb√©m pode truncar sequ√™ncias:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Ir√° preencher as sequ√™ncias at√© o comprimento m√°ximo do modelo
# (512 para o modelo BERT ou DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Truncar√° as sequ√™ncias que s√£o mais longas do que o comprimento m√°ximo especificado
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

O objeto `tokenizer` pode lidar com a convers√£o para tensores de estrutura espec√≠ficos, que podem ent√£o ser enviados diretamente para o modelo. Por exemplo, na seguinte amostra de c√≥digo, estamos solicitando que o tokenizer retorne tensores de diferentes estruturas - `"pt"` retorna tensores PyTorch, `"tf"` retorna tensores TensorFlow, e `"np"` retorna arrays NumPy:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Retorna tensores PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Retorna tensores TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Retorna NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Tokens especiais

Se dermos uma olhada nos IDs de entrada devolvidos pelo tokenizer, veremos que eles s√£o um pouco diferentes do que t√≠nhamos anteriormente:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```


Um token ID foi adicionada no in√≠cio e uma no final. Vamos decodificar as duas sequ√™ncias de IDs acima para ver do que se trata:


```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

O tokenizer acrescentou a palavra especial `[CLS]` no in√≠cio e a palavra especial `[SEP]` no final. Isto porque o modelo foi pr√©-treinado com esses, ent√£o para obter os mesmos resultados para infer√™ncia, precisamos adicion√°-los tamb√©m. Note que alguns modelos n√£o acrescentam palavras especiais, ou acrescentam palavras diferentes; os modelos tamb√©m podem acrescentar estas palavras especiais apenas no in√≠cio, ou apenas no final. Em qualquer caso, o tokenizer sabe quais s√£o as palavras que s√£o esperadas e tratar√° disso para voc√™.


## Do tokenizer ao modelo

Agora que j√° vimos todos os passos individuais que o objeto `tokenizer` utiliza quando aplicado em textos, vamos ver uma √∫ltima vez como ele pode lidar com m√∫ltiplas sequ√™ncias (padding!), sequ√™ncias muito longas (truncagem!), e m√∫ltiplos tipos de tensores com seu API principal:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
