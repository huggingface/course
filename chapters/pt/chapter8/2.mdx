# O que fazer quando ocorrer um erro

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/pt/chapter8/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/pt/chapter8/section2.ipynb"},
]} />

Nesta se√ß√£o, veremos alguns erros comuns que podem ocorrer ao tentar gerar previs√µes de seu modelo Transformer rec√©m treinado. Isso ir√° prepar√°-lo para a [se√ß√£o 4](/course/chapter8/section4), onde exploraremos como debugar a pr√≥pria fase de treinamento.

<Youtube id="DQ-CpJn6Rc4"/>

Preparamos um [reposit√≥rio modelo](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) para esta se√ß√£o e, se voc√™ quiser executar o c√≥digo neste cap√≠tulo, Primeiro, voc√™ precisar√° copiar o modelo para sua conta no [Hugging Face Hub](https://huggingface.co). Para fazer isso, primeiro fa√ßa login executando o seguinte em um notebook Jupyter:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ou usando seu terminal favorito:

```bash
huggingface-cli login
```

Isso solicitar√° que voc√™ insira seu nome de usu√°rio e senha e salvar√° um token em *~/.cache/huggingface/*. Depois de fazer login, voc√™ pode copiar o reposit√≥rio de modelos com a seguinte fun√ß√£o:

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Clone the repo and extract the local path
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Create an empty repo on the Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Clone the empty repo
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copy files
    copy_tree(template_repo_dir, new_repo_dir)
    # Push to Hub
    repo.push_to_hub()
```

Agora, quando voc√™ chamar `copy_repository_template()`, ele criar√° uma c√≥pia do reposit√≥rio de modelos em sua conta.

## Debugando o pipeline de ü§ó Transformers

Para iniciar nossa jornada no maravilhoso mundo de debug de modelos Transformer, considere o seguinte cen√°rio: voc√™ est√° trabalhando com um colega em um projeto de resposta a perguntas para ajudar os clientes de um site de com√©rcio eletr√¥nico a encontrar respostas sobre produtos de consumo. Seu colega lhe envia uma mensagem como:

> Bom dia! Acabei de fazer um experimento usando as t√©cnicas do [Cap√≠tulo 7](/course/chapter7/7) do curso Hugging Face e obtive √≥timos resultados no SQuAD! Acho que podemos usar esse modelo como checkpoint para o nosso projeto. O ID do modelo no Hub √© "lewtun/distillbert-base-uncased-finetuned-squad-d5716d28". Fique a vontade para testar :)

e a primeira coisa que voc√™ pensa √© carregar o modelo usando o `pipeline` de ü§ó Transformers:

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Oh n√£o, algo parece ter dado errado! Se voc√™ √© novo em programa√ß√£o, esse tipo de erro pode parecer um pouco enigm√°tico no come√ßo (o que √© mesmo um `OSError`?!). O erro exibido aqui √© apenas a √∫ltima parte de um relat√≥rio de erros muito maior chamado _Python traceback_ (tamb√©m conhecido como **stack trace**). Por exemplo, se voc√™ estiver executando este c√≥digo no Google Colab, dever√° ver algo como a captura de tela a seguir:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png" alt="A Python traceback." width="100%"/>
</div>

H√° muitas informa√ß√µes contidas nesses relat√≥rios, ent√£o vamos percorrer as partes principais juntos. A primeira coisa a notar √© que os tracebacks devem ser lidos _de baixo para cima_. Isso pode soar estranho se voc√™ est√° acostumado a ler texto em ingl√™s de cima para baixo, mas reflete o fato de que o traceback mostra a sequ√™ncia de chamadas de fun√ß√£o que o `pipeline` faz ao baixar o modelo e o tokenizer. (Confira o [Cap√≠tulo 2](/course/chapter2) para mais detalhes sobre como o `pipeline` funciona nos bastidores.)

<Tip>

üö® Est√° vendo aquela caixa azul em torno de "6 frames" no traceback do Google Colab? Esse √© um recurso especial do Colab, que compacta o traceback em "quadros". Se voc√™ n√£o conseguir encontrar a fonte de um erro, certifique-se de expandir o rastreamento completo clicando nessas duas pequenas setas.

</Tip>

Isso significa que a √∫ltima linha do traceback indica a √∫ltima mensagem de erro e fornece o nome da exce√ß√£o que foi gerada. Nesse caso, o tipo de exce√ß√£o √© `OSError`, que indica um erro relacionado ao sistema. Se lermos a mensagem de erro que a acompanha, veremos que parece haver um problema com o arquivo *config.json* do modelo e recebemos duas sugest√µes para corrigi-lo:

```python out
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

<Tip>

üí° Se voc√™ encontrar uma mensagem de erro dif√≠cil de entender, basta copiar e colar a mensagem na barra de pesquisa do Google ou [Stack Overflow](https://stackoverflow.com/) (sim, s√©rio!). H√° uma boa chance de voc√™ n√£o ser a primeira pessoa a encontrar o erro, e essa √© uma boa maneira de encontrar solu√ß√µes que outras pessoas da comunidade postaram. Por exemplo, pesquisar por `OSError: Can't load config for` no Stack Overflow fornece v√°rios [hits](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+) que poderia ser usado como ponto de partida para resolver o problema.

</Tip>

A primeira sugest√£o √© nos pedir para verificar se o ID do modelo est√° realmente correto, ent√£o a primeira ordem do dia √© copiar o identificador e col√°-lo na barra de pesquisa do Hub:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png" alt="The wrong model name." width="100%"/>
</div>

Hmm, realmente parece que o modelo do nosso colega n√£o est√° no Hub... aha, mas h√° um erro de digita√ß√£o no nome do modelo! DistilBERT tem apenas um "l" em seu nome, ent√£o vamos corrigir isso e procurar por "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28":

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png" alt="The right model name." width="100%"/>
</div>

Ok, isso teve sucesso. Agora vamos tentar baixar o modelo novamente com o ID do modelo correto:

```python
model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Argh, frustrado novamente - bem-vindo ao cotidiano de um engenheiro de aprendizado de m√°quina! Como corrigimos o ID do modelo, o problema deve estar no pr√≥prio reposit√≥rio. Uma maneira r√°pida de acessar o conte√∫do de um reposit√≥rio no ü§ó Hub √© atrav√©s da fun√ß√£o `list_repo_files()` da biblioteca `huggingface_hub`:

```python
from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)
```

```python out
['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']
```

Interessante -- n√£o parece haver um arquivo *config.json* no reposit√≥rio! N√£o √© √† toa que nosso `pipeline` n√£o conseguiu carregar o modelo; nosso colega deve ter esquecido de enviar este arquivo para o Hub depois de ajust√°-lo. Nesse caso, o problema parece bem simples de corrigir: poder√≠amos pedir para adicionar o arquivo ou, como podemos ver no ID do modelo, que o modelo pr√©-treinado usado foi [`distilbert-base-uncased`](https:/ /huggingface.co/distilbert-base-uncased), podemos baixar a configura√ß√£o para este modelo e envi√°-la para nosso reposit√≥rio para ver se isso resolve o problema. Vamos tentar isso. Usando as t√©cnicas que aprendemos no [Cap√≠tulo 2](/course/chapter2), podemos baixar a configura√ß√£o do modelo com a classe `AutoConfig`:


```python
from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)
```

<Tip warning={true}>

üö® A abordagem que estamos tomando aqui n√£o √© infal√≠vel, j√° que nosso colega pode ter ajustado a configura√ß√£o de `distilbert-base-uncased` antes de ajustar o modelo. Na vida real, gostar√≠amos de verificar com eles primeiro, mas para os prop√≥sitos desta se√ß√£o, vamos supor que eles usaram a configura√ß√£o padr√£o.

</Tip>

Podemos ent√£o enviar isso para o nosso reposit√≥rio de modelos com a fun√ß√£o `push_to_hub()` da configura√ß√£o:

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

Agora podemos testar se funcionou carregando o modelo do √∫ltimo commit no branch `main`:

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)
```

```python out
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}
```

Uhuuul, funcionou! Vamos recapitular o que voc√™ acabou de aprender:

- As mensagens de erro em Python s√£o conhecidas como _tracebacks_ e s√£o lidas de baixo para cima. A √∫ltima linha da mensagem de erro geralmente cont√©m as informa√ß√µes necess√°rias para localizar a origem do problema.
- Se a √∫ltima linha n√£o contiver informa√ß√µes suficientes, suba o traceback e veja se voc√™ consegue identificar onde no c√≥digo-fonte o erro ocorre.
- Se nenhuma das mensagens de erro puder ajud√°-lo a debugar o problema, tente pesquisar online uma solu√ß√£o para um problema semelhante.
- O `huggingface_hub`
// ü§ó Hub?
esta biblioteca fornece um conjunto de ferramentas que voc√™ pode usar para interagir e debugar reposit√≥rios no Hub.

Agora que voc√™ sabe como debugar um pipeline, vamos dar uma olhada em um exemplo mais complicado no forward pass do pr√≥prio modelo.

## Debugando o forward pass do seu modelo

Embora o `pipeline` seja √≥timo para a maioria dos aplicativos em que voc√™ precisa gerar previs√µes rapidamente, √†s vezes voc√™ precisar√° acessar os logits do modelo (digamos, se voc√™ tiver algum p√≥s-processamento personalizado que gostaria de aplicar). Para ver o que pode dar errado neste caso, vamos primeiro pegar o modelo e o tokenizer do nosso `pipeline`:

```python
tokenizer = reader.tokenizer
model = reader.model
```

Em seguida, precisamos de uma pergunta, ent√£o vamos ver se nossos frameworks favoritos s√£o suportados:

```python
question = "Which frameworks can I use?"
```

Como vimos no [Cap√≠tulo 7](/course/chapter7), as etapas usuais que precisamos seguir s√£o tokenizar as entradas, extrair os logits dos tokens de in√≠cio e fim e, em seguida, decodificar o intervalo de resposta:

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""
```

Oxii, parece que temos um bug em nosso c√≥digo! Mas n√£o temos medo de debugar um pouco. Voc√™ pode usar o debugger do Python em um notebook:

<Youtube id="rSPyvPw0p9k"/>

ou em um terminal:

<Youtube id="5PkZ4rbHL6c"/>

Aqui, a leitura da mensagem de erro nos diz que o objeto `'list' n√£o tem atributo 'size'`, e podemos ver uma seta `-->` apontando para a linha onde o problema foi levantado em `model(**inputs) `. Voc√™ pode debugar isso interativamente usando o debugger Python, mas por enquanto vamos simplesmente imprimir uma fatia de `entradas` para ver o que temos:

```python
inputs["input_ids"][:5]
```

```python out
[101, 2029, 7705, 2015, 2064]
```

Isso certamente se parece com uma `lista` comum do Python, mas vamos verificar novamente o tipo:

```python
type(inputs["input_ids"])
```

```python out
list
```

Sim, isso √© uma `lista` do Python com certeza. Ent√£o o que deu errado? Lembre-se do [Cap√≠tulo 2](/course/chapter2) que as classes `AutoModelForXxx` em ü§ó Transformers operam em _tensors_ (em PyTorch ou TensorFlow), e uma opera√ß√£o comum √© extrair as dimens√µes de um tensor usando `Tensor.size( )` em, digamos, PyTorch. Vamos dar outra olhada no traceback, para ver qual linha acionou a exce√ß√£o:

```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

Parece que nosso c√≥digo tentou chamar `input_ids.size()`, mas isso claramente n√£o funcionar√° para uma `list` Python, que √© apenas um cont√™iner. Como podemos resolver este problema? Pesquisar a mensagem de erro no Stack Overflow fornece alguns [hits] relevantes (https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f). Clicar no primeiro exibe uma pergunta semelhante √† nossa, com a resposta mostrada na captura de tela abaixo:


<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png" alt="An answer from Stack Overflow." width="100%"/>
</div>

A resposta recomenda que adicionemos `return_tensors='pt'` ao tokenizer, ent√£o vamos ver se isso funciona para n√≥s:

```python out
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""
```

Legal, funcionou! Este √© um √≥timo exemplo de como o Stack Overflow pode ser √∫til: ao identificar um problema semelhante, pudemos nos beneficiar da experi√™ncia de outras pessoas da comunidade. No entanto, uma pesquisa como essa nem sempre produz uma resposta relevante, ent√£o o que voc√™ pode fazer nesses casos? Felizmente, h√° uma comunidade acolhedora de desenvolvedores nos [f√≥runs do Hugging Face](https://discuss.huggingface.co/) que pode ajud√°-lo! Na pr√≥xima se√ß√£o, veremos como voc√™ pode criar boas perguntas no f√≥rum que provavelmente ser√£o respondidas.