<FrameworkSwitchCourse {fw} />

# Processando os dados[[processando-os-dados]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
Continuando com o exemplo do [cap√≠tulo anterior](/course/chapter2), aqui est√° como treinar√≠amos um classificador de sequ√™ncia em um lote com PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# O mesmo de antes
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Isso √© novo
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Continuando com o exemplo do [cap√≠tulo anterior](/course/chapter2), aqui est√° como treinar√≠amos um classificador de sequ√™ncia em um lote com TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# O mesmo de antes
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Isso √© novo
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

Claro, treinar o modelo com apenas duas frases n√£o vai gerar bons resultados. Para obter melhores resultados, ser√° necess√°rio preparar um conjunto de dados maior.

Nesta se√ß√£o, usaremos como exemplo o conjunto de dados MRPC (Microsoft Research Paraphrase Corpus), introduzido em um [artigo](https://www.aclweb.org/anthology/I05-5002.pdf) por William B. Dolan e Chris Brockett. O conjunto de dados consiste em 5.801 pares de senten√ßas, com um r√≥tulo indicando se s√£o par√°frases ou n√£o (ou seja, se ambas as senten√ßas significam a mesma coisa). N√≥s o escolhemos para este cap√≠tulo porque √© um conjunto de dados pequeno, portanto √© f√°cil experimentar o processo de treinamento com ele.

### Carregando o dataset do Hub[[carregando-o-dataset-do-hub]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

O Hub n√£o cont√©m apenas modelos; ele tamb√©m possui v√°rios conjuntos de dados em diferentes idiomas. Voc√™ pode navegar pelos conjuntos de dados [aqui](https://huggingface.co/datasets), e recomendamos que tente carregar e processar um novo conjunto de dados ap√≥s concluir esta se√ß√£o (veja a documenta√ß√£o geral [aqui](https://huggingface.co/docs/datasets/loading)). Mas, por enquanto, vamos nos concentrar no conjunto de dados MRPC! Este √© um dos 10 conjuntos de dados que comp√µem o [GLUE benchmark](https://gluebenchmark.com/), um benchmark acad√™mico usado para medir o desempenho de modelos de Aprendizado de M√°quina em 10 tarefas de classifica√ß√£o de texto diferentes.

A biblioteca ü§ó Datasets fornece um comando muito simples para baixar e armazenar um conjunto de dados do Hub. Podemos baixar o conjunto de dados MRPC da seguinte maneira:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```
Como voc√™ pode ver, obtemos um objeto `DatasetDict` que cont√©m o conjunto de dados de treinamento, o conjunto de valida√ß√£o e o conjunto de teste. Cada um deles cont√©m v√°rias colunas (`sentence1`, `sentence2`, `label` e `idx`) e um n√∫mero vari√°vel de linhas, que s√£o o n√∫mero de elementos em cada conjunto (ent√£o, existem 3.668 pares de senten√ßas no conjunto de treinamento, 408 no conjunto de valida√ß√£o e 1.725 no conjunto de teste).

Este comando faz o download e armazena o conjunto de dados, por padr√£o em *~/.cache/huggingface/datasets*. Lembre-se do Cap√≠tulo 2 que voc√™ pode personalizar sua pasta de cache definindo a vari√°vel de ambiente `HF_HOME`.

Podemos acessar cada par de senten√ßas em nosso objeto `raw_datasets` por meio de indexa√ß√£o, como com um dicion√°rio:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

Podemos ver que os r√≥tulos j√° s√£o inteiros, ent√£o n√£o precisaremos fazer nenhum pr√©-processamento. Para saber qual inteiro corresponde a qual r√≥tulo, podemos inspecionar as `features` do nosso `raw_train_dataset`. Isso nos dir√° o tipo de cada coluna:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```
Por tr√°s das cortinas, `label` √© do tipo `ClassLabel`, e o mapeamento de inteiros para o nome do r√≥tulo √© armazenado em *names*. `0` corresponde a `not_equivalent` e `1` a `equivalent`.

<Tip>

‚úèÔ∏è **Experimente!** Olhe o elemento 15 do conjunto de treinamento e o elemento 87 do conjunto de valida√ß√£o. Quais s√£o seus r√≥tulos?

</Tip>

### Pr√©-processando o dataset[[pre-processando-o-dataset]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Para pr√©-processar o conjunto de dados precisamos converter o texto em n√∫meros que o modelo possa entender. Como voc√™ viu no [cap√≠tulo anterior](/course/chapter2), isso √© feito com um tokenizador. Podemos alimentar o tokenizador com uma senten√ßa ou uma lista de senten√ßas, ou seja, podemos tokenizar diretamente todas as primeiras senten√ßas e todas as segundas senten√ßas de cada par da seguinte maneira:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```
No entanto, n√£o podemos simplesmente passar duas sequ√™ncias para o modelo e obter uma previs√£o de se as duas s√£o par√°frases ou n√£o. Precisamos tratar as duas sequ√™ncias como um par e aplicar o pr√©-processamento apropriado. Felizmente, o tokenizador pode receber um par de sequ√™ncias e prepar√°-lo da maneira que nosso modelo BERT espera:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Discutimos os campos `input_ids` e `attention_mask` no [Cap√≠tulo 2](/course/chapter2), mas adiamos a conversa sobre `token_type_ids`. Neste exemplo, esse campo √© o que diz ao modelo qual parte da entrada √© a primeira senten√ßa e qual √© a segunda senten√ßa.

<Tip>

‚úèÔ∏è **Experimente!** Pegue o elemento 15 do conjunto de treinamento e tokenize as duas senten√ßas separadamente e como um par. Qual √© a diferen√ßa entre os dois resultados?

</Tip>

Se decodificarmos os IDs contidos em `input_ids` de volta para palavras:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

teremos:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

Portanto, vemos que o modelo espera que as entradas sejam da forma `[CLS] sentence1 [SEP] sentence2 [SEP]` quando h√° duas senten√ßas. Alinhar isso com `token_type_ids` nos d√°:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Como voc√™ pode ver, as partes da entrada correspondentes a `[CLS] sentence1 [SEP]` t√™m um ID de tipo de token `0`, enquanto as outras partes, correspondendo a `sentence2 [SEP]`, t√™m um ID de tipo de token `1`.

Note que se voc√™ selecionar um checkpoint diferente, voc√™ n√£o necessariamente ter√° o campo `token_type_ids` em suas entradas tokenizadas (por exemplo, eles n√£o s√£o retornados se voc√™ usar um modelo DistilBERT). Eles s√£o retornados apenas quando o modelo sabe o que fazer com eles, porque os viu durante seu pr√©-treinamento.

Aqui, o BERT √© pr√©-treinado com identificadores de tipo de token, e al√©m do objetivo de modelagem de linguagem mascarada, sobre o qual falamos no [Cap√≠tulo 1](/course/chapter1), ele possui um objetivo adicional chamado _previs√£o da pr√≥xima frase_. O objetivo com essa tarefa √© modelar a rela√ß√£o entre pares de senten√ßas.

Com a previs√£o da pr√≥xima senten√ßa, o modelo recebe pares de senten√ßas (com tokens mascarados aleatoriamente) e √© solicitado a prever se a segunda senten√ßa procede a primeira. Para tornar a tarefa n√£o trivial, metade do tempo as senten√ßas se precedem no documento original de onde foram extra√≠das, e a outra metade do tempo as duas senten√ßas v√™m de dois documentos diferentes.

Em geral, voc√™ n√£o precisa se preocupar se h√° ou n√£o `token_type_ids` em suas entradas tokenizadas: contanto que voc√™ use o mesmo checkpoint para o tokenizador e o modelo, tudo ficar√° bem, pois o tokenizador sabe o que fornecer ao seu modelo.

Agora que vimos como nosso tokenizador pode lidar com um par de senten√ßas, podemos us√°-lo para tokenizar todo o nosso conjunto de dados: como no [cap√≠tulo anterior](/course/chapter2), podemos alimentar o tokenizador com uma lista de pares de senten√ßas, fornecendo a lista de primeiras senten√ßas e, em seguida, a lista de segundas senten√ßas. Isso tamb√©m √© compat√≠vel com as op√ß√µes de padding e truncation que vimos no [Cap√≠tulo 2](/course/chapter2). Ent√£o, uma maneira de pr√©-processar o conjunto de dados de treinamento √©:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Isso funciona bem, mas tem a desvantagem de retornar um dicion√°rio (com nossas chaves, `input_ids`, `attention_mask` e `token_type_ids`, e valores que s√£o listas de listas). Isso s√≥ funcionar√° se voc√™ tiver RAM suficiente para armazenar todo o seu conjunto de dados durante a tokeniza√ß√£o (enquanto os conjuntos de dados da biblioteca ü§ó Datasets s√£o arquivos [Apache Arrow](https://arrow.apache.org/) armazenados no disco, ent√£o voc√™ s√≥ mant√©m as amostras que solicita carregadas na mem√≥ria).

Para manter os dados como um conjunto de dados, usaremos o m√©todo [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). Isso tamb√©m nos permite alguma flexibilidade extra se precisarmos de mais pr√©-processamento al√©m da tokeniza√ß√£o. O m√©todo `map()` funciona aplicando uma fun√ß√£o em cada elemento do conjunto de dados, ent√£o vamos definir uma fun√ß√£o que tokeniza nossas entradas:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Essa fun√ß√£o recebe um dicion√°rio (como os itens do nosso conjunto de dados) e retorna um novo dicion√°rio com as chaves `input_ids`, `attention_mask` e `token_type_ids`. Observe que tamb√©m funciona se o dicion√°rio `example` contiver v√°rias amostras (cada chave como uma lista de senten√ßas), j√° que o `tokenizer` funciona em listas de pares de senten√ßas, como visto antes. Isso nos permitir√° usar a op√ß√£o `batched=True` na nossa chamada para `map()`, o que acelerar√° muito a tokeniza√ß√£o. O `tokenizer` √© suportado por um tokenizador escrito em Rust da biblioteca [ü§ó Tokenizers](https://github.com/huggingface/tokenizers). Esse tokenizador pode ser muito r√°pido, mas apenas se fornecermos a ele muitas entradas de uma vez.

Observe que deixamos o argumento `padding` de fora na nossa fun√ß√£o de tokeniza√ß√£o por enquanto. Isso ocorre porque realizar o padding em todas as amostras at√© o comprimento m√°ximo n√£o √© eficiente: √© melhor preencher as amostras quando estamos construindo um lote, pois ent√£o s√≥ precisamos preencher at√© o comprimento m√°ximo nesse lote, e n√£o o comprimento m√°ximo em todo o conjunto de dados. Isso pode economizar muito tempo e energia de processamento quando as entradas t√™m tamanhos muito diferentes!

Aqui est√° como aplicamos a fun√ß√£o de tokeniza√ß√£o em todos os nossos conjuntos de dados de uma vez. Estamos usando `batched=True` na nossa chamada para `map`, ent√£o a fun√ß√£o √© aplicada a v√°rios elementos do nosso conjunto de dados de uma vez, e n√£o em cada elemento separadamente. Isso permite um pr√©-processamento mais r√°pido.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

A forma como a biblioteca ü§ó Datasets aplica esse processamento √© adicionando novos campos aos conjuntos de dados, um para cada chave no dicion√°rio retornado pela fun√ß√£o de pr√©-processamento:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Voc√™ pode at√© usar multiprocessamento ao aplicar sua fun√ß√£o de pr√©-processamento com `map()` passando o argumento `num_proc`. N√£o fizemos isso aqui porque a biblioteca ü§ó Tokenizers j√° usa m√∫ltiplas threads para tokenizar nossas amostras mais rapidamente, mas se voc√™ n√£o estiver usando um tokenizer r√°pido suportado por esta biblioteca, isso pode acelerar seu pr√©-processamento.

Nossa `tokenize_function` retorna um dicion√°rio com as chaves `input_ids`, `attention_mask` e `token_type_ids`, esses tr√™s campos s√£o adicionados a todas as divis√µes do nosso conjunto de dados. Observe que tamb√©m poder√≠amos ter alterado campos existentes se nossa fun√ß√£o de pr√©-processamento retornasse um novo valor para uma chave existente no conjunto de dados ao qual aplicamos `map()`.

A √∫ltima coisa que precisaremos fazer √© preencher todos os exemplos at√© o comprimento do elemento mais longo quando agrupamos os elementos juntos no lote ‚Äî uma t√©cnica que nos referimos como *padding din√¢mico*.

### Padding din√¢mico[[padding-dinamico]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
A fun√ß√£o respons√°vel por juntar amostras dentro de um lote √© chamada de *collate function* (fun√ß√£o de agrupamento). √â um argumento que voc√™ pode passar ao construir um `DataLoader`, sendo o padr√£o uma fun√ß√£o que apenas converte suas amostras para tensores PyTorch e as concatena (recursivamente se seus elementos forem listas, tuplas ou dicion√°rios). Isso n√£o ser√° poss√≠vel no nosso caso, j√° que as entradas que temos n√£o ser√£o todas do mesmo tamanho. Adiamos deliberadamente o padding para aplic√°-lo somente conforme necess√°rio em cada lote e evitar entradas excessivamente longas e com muito padding. Isso acelerar√° bastante o treinamento, mas observe que se voc√™ estiver treinando em uma TPU, isso pode causar problemas ‚Äî as TPUs preferem dimens√µes fixas, mesmo quando isso exige padding extra.

{:else}
A fun√ß√£o respons√°vel por juntar amostras dentro de um lote √© chamada de *collate function* (fun√ß√£o de agrupamento). O agrupador padr√£o √© uma fun√ß√£o que apenas converte suas amostras para tf.Tensor e as concatena (recursivamente se seus elementos forem listas, tuplas ou dicion√°rios). Isso n√£o ser√° poss√≠vel no nosso caso, j√° que as entradas que temos n√£o ser√£o todas do mesmo tamanho. Adiamos deliberadamente o padding para aplic√°-lo somente conforme necess√°rio em cada lote e evitar entradas excessivamente longas, com muito padding. Isso acelerar√° bastante o treinamento, mas observe que se voc√™ estiver treinando em uma TPU, isso pode causar problemas ‚Äî as TPUs preferem dimens√µes fixas, mesmo quando isso exige padding extra.

{/if}
Para fazer isso na pr√°tica, temos que definir uma fun√ß√£o de agrupamento que aplicar√° a quantidade correta de preenchimento aos itens do conjunto de dados que queremos agrupar. Felizmente, a biblioteca ü§ó Transformers nos fornece tal fun√ß√£o atrav√©s do `DataCollatorWithPadding`. Ela recebe um tokenizer ao ser instanciada (para saber qual token de preenchimento usar e se o modelo espera que o preenchimento esteja √† esquerda ou √† direita das entradas) e far√° tudo o que voc√™ precisa:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Para testar esse novo brinquedo vamos pegar algumas amostras do nosso conjunto de treinamento que gostar√≠amos de agrupar. Aqui, removemos as colunas `idx`, `sentence1` e `sentence2`, pois n√£o ser√£o necess√°rias e cont√™m strings (e n√£o podemos criar tensores com strings) e observamos os comprimentos de cada entrada no lote:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```
N√£o surpreendentemente, obtemos amostras de comprimentos variados, de 32 a 67. O padding din√¢mico significa que as amostras neste lote devem ser todas preenchidas at√© um comprimento de 67, o comprimento m√°ximo dentro do lote. Sem padding din√¢mico, todas as amostras teriam que ser preenchidas at√© o comprimento m√°ximo em todo o conjunto de dados ou o comprimento m√°ximo que o modelo pode aceitar. Vamos verificar se nosso `data_collator` est√° realizando o padding dinamicamente no lote corretamente:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Parece bom! Agora que passamos de texto bruto para lotes com os quais nosso modelo pode lidar, estamos prontos para ajust√°-lo!

{/if}

<Tip>

‚úèÔ∏è **Experimente!** Reproduza o pr√©-processamento no conjunto de dados GLUE SST-2. √â um pouco diferente, pois √© composto por senten√ßas √∫nicas em vez de pares, mas o resto do que fizemos deve parecer similar. Para um desafio maior, tente escrever uma fun√ß√£o de pr√©-processamento que funcione em qualquer uma das tarefas do GLUE.

</Tip>

{#if fw === 'tf'}

Agora que temos nosso conjunto de dados e um agrupador de dados, precisamos junt√°-los. Poder√≠amos carregar manualmente os lotes e agrup√°-los, mas isso d√° muito trabalho e provavelmente n√£o √© muito eficiente. Em vez disso, h√° um m√©todo simples que oferece uma solu√ß√£o eficiente para esse problema: `to_tf_dataset()`. Isso envolver√° um `tf.data.Dataset` em torno do seu dataset, com uma fun√ß√£o de agrupamento opcional. `tf.data.Dataset` √© um formato nativo do TensorFlow que o Keras pode usar para executar `model.fit()`, ent√£o esse √∫nico m√©todo converte imediatamente um ü§ó Dataset para um formato pronto para treinamento. Vamos v√™-lo em a√ß√£o com nosso dataset!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

√â isso! Podemos levar esses conjuntos de dados adiante na pr√≥xima aula, onde o treinamento ser√° agradavelmente direto ap√≥s todo o trabalho √°rduo de pr√©-processamento de dados.

{/if}
