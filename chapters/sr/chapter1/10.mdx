<!-- DISABLE-FRONTMATTER-SECTIONS -->

# Kviz za kraj poglavlja

<CourseFloatingBanner chapter={1} classNames="absolute z-10 right-0 top-0" />

Ovo poglavlje je pokrilo dosta toga! Nemojte da brinte ako niste zapamtili sve detalje; sledeće poglavlje će vam pomoći da razumete kako stvari rade ispod haube.

Ali pre svega, hajde da testiramo šta smo naučili u ovom poglavlju!

### 1. Istraži Hub and i pronađi `roberta-large-mnli` checkpoint. Koji zadatak on obavlja?

<Question
  choices={[
    {
      text: "Sumarizacija",
      explain:
        'Pogledaj opet <a href="https://huggingface.co/roberta-large-mnli">roberta-large-mnli stranicu</a>.',
    },
    {
      text: "Klasifikacija teksta",
      explain:
        "Preciznije, klasifikuje da li su dve rečenice logički povezane preko tri labele (contradiction, neutral, entailment) — zadatak koji se još zove <em>natural language inference</em>.",
      correct: tačno,
    },
    {
      text: "Generisanje teksta",
      explain:
        'Pogledaj opet <a href="https://huggingface.co/roberta-large-mnli">roberta-large-mnli stranicu</a>.',
    },
  ]}
/>

### 2. Šta će ovaj kod vratiti?

```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

<Question
  choices={[
    {
      text: 'Vratiće klasifikacione skorove, sa labelama "positive" or "negative".',
      explain:
        "Ovo nije tačno — ovo bi bio <code>sentiment-analysis</code> pipeline.",
    },
    {
      text: "Vratiće generisani tekst dopunjujići rečenicu.",
      explain:
        "Ovo nije tačno — ovo bi bio <code>text-generation</code> pipeline.",
    },
    {
      text: "Vratiće reči koje predstavljaju osobe, organizacije ili lokacije.",
      explain:
        'šta više, sa <code>grouped_entities=True</code>, grupisaće reči koje pripadaju istom entitetu, kao "Hugging Face".',
      correct: tačno,
    },
  ]}
/>

### 3. Šta bi trebalo da zameni ... u ovom primeru koda?

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

<Question
  choices={[
    {
      text: "This &#60;mask> has been waiting for you.",
      explain:
        "Ovo je netačno. Pogledaj <code>bert-base-cased</code> model card i probaj da uočiš grešku.",
    },
    {
      text: "This [MASK] has been waiting for you.",
      explain: "Ispravno! Mask token ovog modela je [MASK].",
      correct: tačno,
    },
    {
      text: "This man has been waiting for you.",
      explain:
        "Ovo je netačno. Ovaj pipline popunjava maskirane reči, tako da mu treba mask token negde.",
    },
  ]}
/>

### 4. Zašto ovaj kod neće raditi?

```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

<Question
  choices={[
    {
      text: "Ovaj pipline zahteva da mu se daju labele da bi klasifikovao ovaj tekst.",
      explain:
        "Tačno — tačan kod mora da uključi <code>candidate_labels=[...]</code>.",
      correct: tačno,
    },
    {
      text: "Ovaj pipline zahteva više od jedne rečenice.",
      explain:
        "Ovo je netačno, iako kada se ispravno koristi, ovaj pipline može da primi listu rečenica koje će procesirati (kao i svi drugi pipeline-i).",
    },
    {
      text: "🤗 Transformers biblioteka je pokvarena, kao i obično.",
      explain: "Nećemo udostojiti ovaj odgovor komentarom!",
    },
    {
      text: "Ovaj pipeline zahteva duže inpute; ovaj je prekratak.",
      explain:
        "Ovo je netačno. Setite se da će duži tekst biti skraćen kada se procesira koristeći pipline.",
    },
  ]}
/>

### 5. Šta znači "transfer learning"?

<Question
  choices={[
    {
      text: "Prenošenje znanja istreniranog modela na novi model treniranjem na isto setu podataka.",
      explain: "Ne, to bi bile dve verzije istog modela.",
    },
    {
      text: "Prenošenje znanja istreniranog modela na novi model inicijalizovanjem drugog modela sa težinama prvog modela.",
      explain:
        "Tačno: kada je drugi model treniran na novom zadatku, on *prenosi* znanje prvog modela.",
      correct: tačno,
    },
    {
      text: "Prenošenje znanja istreniranog modela na novi model praveći drugi model sa istom arhitekturom kao i prvi model.",
      explain:
        "Arhitektura je samo način na koji je model napravljen; nema znanja koje se deli ili penosi u ovom slučaja.",
    },
  ]}
/>

### 6. Tačno ili netačno? Veliki model uglavnom ne zahteva labele za pretraining fazu.

<Question
  choices={[
    {
      text: "Tačno",
      explain:
        "Pretraining faza je obično <em>self-supervised</em>, što znači da se labele automatski kreiraju iz ulaza (kao predikcija sledeće reči ili popunjavanje maskiranih reči).",
      correct: tačno,
    },
    {
      text: "Netačno",
      explain: "Ovo nije tačan odgovor.",
    },
  ]}
/>

### 7. Izaberi rečenice koje najbolje opisuje termine "model", "arhitektura", and "težine".

<Question
  choices={[
    {
      text: "Ako je model zgrada, arhitektura je projekat zgrade i težine su ljudi koje žive unutar.",
      explain:
        "Prateći ovu metaforu, težine bi bile cigle i ostali materijali koji se koriste za izgradnju zgrade.",
    },
    {
      text: "Arhitektura je mapa za izgradnju modela i težine su gradovi koji su predstavljeni na mapi.",
      explain:
        "Problem sa ovom metaforom je da mapa obično predstavlja realnost (postoji samo jedan grad u Francuskoj koji se zove Pariz). Za datu arhitekturu, moguće je više težina.",
    },
    {
      text: "Arhitektura je niz matematičkih funkcija za izgradnju modela, a težine su parametri tih funkcija.",
      explain:
        "Isti skup matematičkih funkcija (arhitektura) može se koristiti za izgradnju različitih modela primenom različitih parametara (težina).",
      correct: tačno,
    },
  ]}
/>

### 8. Koji tip modela bi koristili za dopunjavanje promptova generisanim tekstom?

<Question
  choices={[
    {
      text: "Enkoder model",
      explain:
        "Enkoder model generiše reprezentaciju cele rečenice koja je pogodna za zadatke kao što je klasifikacija.",
    },
    {
      text: "Dekoder model",
      explain:
        "Dekoder modeli su savršeni za generisanje teksta na osnovu prompta.",
      correct: tačno,
    },
    {
      text: "Sequence-to-sequence model",
      explain:
        "Sequence-to-sequence modeli koji su pogodniji za taskove gde želite da generišete rečenice povezane sa ulaznom rečenicom, ne dati prompt.",
    },
  ]}
/>

### 9. Koji od ovih tipova modela biste koristili za sumarizaciju teksta?

<Question
  choices={[
    {
      text: "Enkoder model",
      explain:
        "Enkoder model generiše reprezentaciju cele rečenice koja je pogodnija za zadatke kao što je klasifikacija",
    },
    {
      text: "Dekoder model",
      explain:
        "Dekoder modeli su dobri za generisanje izlaznog teksta (kratak rezime), ali nemaju sposobnost da iskoriste kontekst kao što je ceo tekst da bi napravili sumarizaciju.",
    },
    {
      text: "A sequence-to-sequence model",
      explain: "Sequence-to-sequence modeli su savršeni za sumarizaciju.",
      correct: tačno,
    },
  ]}
/>

### 10. Koji od ovih tipova modela biste koristili za klasifikaciju unesenog teksta na osnovu određenih labela?

<Question
  choices={[
    {
      text: "Enkoder model",
      explain:
        "Enkoder model generiše reprezentaciju cele rečenice što je jako pogodno za zadatak kao što je klasifikacija.",
      correct: tačno,
    },
    {
      text: "Dekoder model",
      explain:
        "Dekoder modeli su dobri za generisanje izlaznog teksta, ne za izvlačenje labele iz rečenice.",
    },
    {
      text: "A sequence-to-sequence model",
      explain:
        "Sequence-to-sequence modeli su pogodniji za zadatke gde želite da generišete tekst na osnovu ulazne rečenice, ne labele.",
    },
  ]}
/>

### 11. Šta može da bude izvor pristrasnosti koji se primeti kod modela?

<Question
  choices={[
    {
      text: "Model je fine-tuned verzija istreniranog modela i pokupio je njegove pristrasnosti.",
      explain:
        "Kada se primenjuje Transfer Learning, pristrasnost istreniranog modela koji se koristi ostaje prisutna u fine-tuned modelu.",
      correct: tačno,
    },
    {
      text: "Podaci na kojima je model treniran su pristrasni.",
      explain: "Ovo je najočigledniji izvor pristrastnosti, ali ne i jedini.",
      correct: tačno,
    },
    {
      text: "Metrika na osnovu koje je model optimizovan je pristrasna.",
      explain:
        "Manje očigledan izvor pristrasnosti je način na koji je model treniran. Vaš model će slepo pratiti optimizaciju za bilo koju metriku koju odaberete, bez ikakvog razmišljanja.",
      correct: tačno,
    },
  ]}
/>
