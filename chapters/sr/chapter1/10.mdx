<!-- DISABLE-FRONTMATTER-SECTIONS -->

# Kviz za kraj poglavlja

<CourseFloatingBanner chapter={1} classNames="absolute z-10 right-0 top-0" />

Ovo poglavlje je pokrilo dosta toga! Nemojte da brinte ako niste zapamtili sve detalje; sledeÄ‡e poglavlje Ä‡e vam pomoÄ‡i da razumete kako stvari rade ispod haube.

Ali pre svega, hajde da testiramo Å¡ta smo nauÄili u ovom poglavlju!

### 1. IstraÅ¾i Hub and i pronaÄ‘i `roberta-large-mnli` checkpoint. Koji zadatak on obavlja?

<Question
  choices={[
    {
      text: "Sumarizacija",
      explain:
        'Pogledaj opet <a href="https://huggingface.co/roberta-large-mnli">roberta-large-mnli stranicu</a>.',
    },
    {
      text: "Klasifikacija teksta",
      explain:
        "Preciznije, klasifikuje da li su dve reÄenice logiÄki povezane preko tri labele (contradiction, neutral, entailment) â€” zadatak koji se joÅ¡ zove <em>natural language inference</em>.",
      correct: taÄno,
    },
    {
      text: "Generisanje teksta",
      explain:
        'Pogledaj opet <a href="https://huggingface.co/roberta-large-mnli">roberta-large-mnli stranicu</a>.',
    },
  ]}
/>

### 2. Å ta Ä‡e ovaj kod vratiti?

```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

<Question
  choices={[
    {
      text: 'VratiÄ‡e klasifikacione skorove, sa labelama "positive" or "negative".',
      explain:
        "Ovo nije taÄno â€” ovo bi bio <code>sentiment-analysis</code> pipeline.",
    },
    {
      text: "VratiÄ‡e generisani tekst dopunjujiÄ‡i reÄenicu.",
      explain:
        "Ovo nije taÄno â€” ovo bi bio <code>text-generation</code> pipeline.",
    },
    {
      text: "VratiÄ‡e reÄi koje predstavljaju osobe, organizacije ili lokacije.",
      explain:
        'Å¡ta viÅ¡e, sa <code>grouped_entities=True</code>, grupisaÄ‡e reÄi koje pripadaju istom entitetu, kao "Hugging Face".',
      correct: taÄno,
    },
  ]}
/>

### 3. Å ta bi trebalo da zameni ... u ovom primeru koda?

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

<Question
  choices={[
    {
      text: "This &#60;mask> has been waiting for you.",
      explain:
        "Ovo je netaÄno. Pogledaj <code>bert-base-cased</code> model card i probaj da uoÄiÅ¡ greÅ¡ku.",
    },
    {
      text: "This [MASK] has been waiting for you.",
      explain: "Ispravno! Mask token ovog modela je [MASK].",
      correct: taÄno,
    },
    {
      text: "This man has been waiting for you.",
      explain:
        "Ovo je netaÄno. Ovaj pipline popunjava maskirane reÄi, tako da mu treba mask token negde.",
    },
  ]}
/>

### 4. ZaÅ¡to ovaj kod neÄ‡e raditi?

```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

<Question
  choices={[
    {
      text: "Ovaj pipline zahteva da mu se daju labele da bi klasifikovao ovaj tekst.",
      explain:
        "TaÄno â€” taÄan kod mora da ukljuÄi <code>candidate_labels=[...]</code>.",
      correct: taÄno,
    },
    {
      text: "Ovaj pipline zahteva viÅ¡e od jedne reÄenice.",
      explain:
        "Ovo je netaÄno, iako kada se ispravno koristi, ovaj pipline moÅ¾e da primi listu reÄenica koje Ä‡e procesirati (kao i svi drugi pipeline-i).",
    },
    {
      text: "ğŸ¤— Transformers biblioteka je pokvarena, kao i obiÄno.",
      explain: "NeÄ‡emo udostojiti ovaj odgovor komentarom!",
    },
    {
      text: "Ovaj pipeline zahteva duÅ¾e inpute; ovaj je prekratak.",
      explain:
        "Ovo je netaÄno. Setite se da Ä‡e duÅ¾i tekst biti skraÄ‡en kada se procesira koristeÄ‡i pipline.",
    },
  ]}
/>

### 5. Å ta znaÄi "transfer learning"?

<Question
  choices={[
    {
      text: "PrenoÅ¡enje znanja istreniranog modela na novi model treniranjem na isto setu podataka.",
      explain: "Ne, to bi bile dve verzije istog modela.",
    },
    {
      text: "PrenoÅ¡enje znanja istreniranog modela na novi model inicijalizovanjem drugog modela sa teÅ¾inama prvog modela.",
      explain:
        "TaÄno: kada je drugi model treniran na novom zadatku, on *prenosi* znanje prvog modela.",
      correct: taÄno,
    },
    {
      text: "PrenoÅ¡enje znanja istreniranog modela na novi model praveÄ‡i drugi model sa istom arhitekturom kao i prvi model.",
      explain:
        "Arhitektura je samo naÄin na koji je model napravljen; nema znanja koje se deli ili penosi u ovom sluÄaja.",
    },
  ]}
/>

### 6. TaÄno ili netaÄno? Veliki model uglavnom ne zahteva labele za pretraining fazu.

<Question
  choices={[
    {
      text: "TaÄno",
      explain:
        "Pretraining faza je obiÄno <em>self-supervised</em>, Å¡to znaÄi da se labele automatski kreiraju iz ulaza (kao predikcija sledeÄ‡e reÄi ili popunjavanje maskiranih reÄi).",
      correct: taÄno,
    },
    {
      text: "NetaÄno",
      explain: "Ovo nije taÄan odgovor.",
    },
  ]}
/>

### 7. Izaberi reÄenice koje najbolje opisuje termine "model", "arhitektura", and "teÅ¾ine".

<Question
  choices={[
    {
      text: "Ako je model zgrada, arhitektura je projekat zgrade i teÅ¾ine su ljudi koje Å¾ive unutar.",
      explain:
        "PrateÄ‡i ovu metaforu, teÅ¾ine bi bile cigle i ostali materijali koji se koriste za izgradnju zgrade.",
    },
    {
      text: "Arhitektura je mapa za izgradnju modela i teÅ¾ine su gradovi koji su predstavljeni na mapi.",
      explain:
        "Problem sa ovom metaforom je da mapa obiÄno predstavlja realnost (postoji samo jedan grad u Francuskoj koji se zove Pariz). Za datu arhitekturu, moguÄ‡e je viÅ¡e teÅ¾ina.",
    },
    {
      text: "Arhitektura je niz matematiÄkih funkcija za izgradnju modela, a teÅ¾ine su parametri tih funkcija.",
      explain:
        "Isti skup matematiÄkih funkcija (arhitektura) moÅ¾e se koristiti za izgradnju razliÄitih modela primenom razliÄitih parametara (teÅ¾ina).",
      correct: taÄno,
    },
  ]}
/>

### 8. Koji tip modela bi koristili za dopunjavanje promptova generisanim tekstom?

<Question
  choices={[
    {
      text: "Enkoder model",
      explain:
        "Enkoder model generiÅ¡e reprezentaciju cele reÄenice koja je pogodna za zadatke kao Å¡to je klasifikacija.",
    },
    {
      text: "Dekoder model",
      explain:
        "Dekoder modeli su savrÅ¡eni za generisanje teksta na osnovu prompta.",
      correct: taÄno,
    },
    {
      text: "Sequence-to-sequence model",
      explain:
        "Sequence-to-sequence modeli koji su pogodniji za taskove gde Å¾elite da generiÅ¡ete reÄenice povezane sa ulaznom reÄenicom, ne dati prompt.",
    },
  ]}
/>

### 9. Koji od ovih tipova modela biste koristili za sumarizaciju teksta?

<Question
  choices={[
    {
      text: "Enkoder model",
      explain:
        "Enkoder model generiÅ¡e reprezentaciju cele reÄenice koja je pogodnija za zadatke kao Å¡to je klasifikacija",
    },
    {
      text: "Dekoder model",
      explain:
        "Dekoder modeli su dobri za generisanje izlaznog teksta (kratak rezime), ali nemaju sposobnost da iskoriste kontekst kao Å¡to je ceo tekst da bi napravili sumarizaciju.",
    },
    {
      text: "A sequence-to-sequence model",
      explain: "Sequence-to-sequence modeli su savrÅ¡eni za sumarizaciju.",
      correct: taÄno,
    },
  ]}
/>

### 10. Koji od ovih tipova modela biste koristili za klasifikaciju unesenog teksta na osnovu odreÄ‘enih labela?

<Question
  choices={[
    {
      text: "Enkoder model",
      explain:
        "Enkoder model generiÅ¡e reprezentaciju cele reÄenice Å¡to je jako pogodno za zadatak kao Å¡to je klasifikacija.",
      correct: taÄno,
    },
    {
      text: "Dekoder model",
      explain:
        "Dekoder modeli su dobri za generisanje izlaznog teksta, ne za izvlaÄenje labele iz reÄenice.",
    },
    {
      text: "A sequence-to-sequence model",
      explain:
        "Sequence-to-sequence modeli su pogodniji za zadatke gde Å¾elite da generiÅ¡ete tekst na osnovu ulazne reÄenice, ne labele.",
    },
  ]}
/>

### 11. Å ta moÅ¾e da bude izvor pristrasnosti koji se primeti kod modela?

<Question
  choices={[
    {
      text: "Model je fine-tuned verzija istreniranog modela i pokupio je njegove pristrasnosti.",
      explain:
        "Kada se primenjuje Transfer Learning, pristrasnost istreniranog modela koji se koristi ostaje prisutna u fine-tuned modelu.",
      correct: taÄno,
    },
    {
      text: "Podaci na kojima je model treniran su pristrasni.",
      explain: "Ovo je najoÄigledniji izvor pristrastnosti, ali ne i jedini.",
      correct: taÄno,
    },
    {
      text: "Metrika na osnovu koje je model optimizovan je pristrasna.",
      explain:
        "Manje oÄigledan izvor pristrasnosti je naÄin na koji je model treniran. VaÅ¡ model Ä‡e slepo pratiti optimizaciju za bilo koju metriku koju odaberete, bez ikakvog razmiÅ¡ljanja.",
      correct: taÄno,
    },
  ]}
/>
