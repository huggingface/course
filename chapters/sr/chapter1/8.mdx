# Pristrastnost i ograničenja

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
]} />

Ako je vaša namera da koristite unapred istreniran model ili fine-tuned verziju u produkciji, imajte na umu da, iako su ovi modeli moćni alati, oni dolaze sa ograničenjima. Najveće od ovih ograničenja je to što, kako bi omogućili pretraining na velikim količinama podataka, istraživači često prikupljaju sav sadržaj koji mogu naći, uzimajući i najbolje i najgore od onoga što je dostupno na internetu.

Da bismo brzo ilustrovali ovaj problem, vratimo se na primer`fill-mask` pipeline-a sa BERT modelom:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

Kada se od modela traži da popuni nedostajuću reč u ove dve rečenice, model daje samo jedan odgovor bez rodne odrednice (waiter/waitress). stali odgovori su zanimanja koja se obično povezuju sa određenim polom — i da, "prostitute" je završila među top 5 mogućnosti koje model povezuje sa rečju "woman" i "work." Ovo se dešava iako je BERT jedan od retkih Transformer modela koji nije napravljen prikupljanjem podataka sa celog interneta, već korišćenjem naizgled neutralnih podataka (treniran je na [Engleskoj Vikipediji](https://huggingface.co/datasets/wikipedia) i [BookCorpus](https://huggingface.co/datasets/bookcorpus) datasets). 

Kada koristite ove alate, potrebno je imati na umu da originalni model koji koristite može vrlo lako generisati seksistički, rasistički ili homofobični sadržaj. Fino podešavanje modela na vašim podacima neće ukloniti ovu intrinzičnu pristrasnost.
