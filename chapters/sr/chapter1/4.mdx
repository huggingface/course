# Kako Transformeri rade?

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

U ovom delu Ä‡emo se na visokom nivou upoznati sa arhitekturom Transformer modela.

## Malo istorije Transformera

Evo nekoliko kljuÄnih taÄaka u (kratkoj) istoriji Transformer modela:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="Kratka hronologija Transformer modela.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="Kratka hronologija Transformer modela.">
</div>

[Transformer arhitektura](https://arxiv.org/abs/1706.03762) je predstavljena u junu 2017. godine. Fokus originalnog istraÅ¾ivanja bio je na zadacima prevoÄ‘enja. Nakon toga usledilo je predstavljanje nekoliko uticajnih modela, ukljuÄujuÄ‡:

- **Jun 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), prvi pretrained Transformer model, koriÅ¡Ä‡en za fine-tuning na raznim NLP zadacima, postigao je vrhunske rezultate

- **Oktobar 2018**: [BERT](https://arxiv.org/abs/1810.04805), joÅ¡ jedan veliki pretrained model, dizajniran da daje bolje saÅ¾etke reÄenica (viÅ¡e o tome u sledeÄ‡em poglavlju!)

- **Februar 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), poboljÅ¡ana (i veÄ‡a) verzija GPT-a koja nije odmah javno objavljena zbog etiÄkih pitanja

- **Oktobar 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108), destilovana verzija BERT-a koja je 60% brÅ¾a, 40% lakÅ¡a za memoriju, a i dalje zadrÅ¾ava 97% performansi BERT-a

- **Oktobar 2019**: [BART](https://arxiv.org/abs/1910.13461) i [T5](https://arxiv.org/abs/1910.10683), dva velika pretrained modela koji koriste istu arhitekturu kao originalni Transformer model (prvi koji su to uÄinili)

- **Maj 2020**, [GPT-3](https://arxiv.org/abs/2005.14165), joÅ¡ veÄ‡a verzija GPT-2 koja je sposobna da dobro obavlja razne zadatke bez potrebe za dodatnim fine-tuning-om (naziva se _zero-shot learning_)

Ova lista nije sveobuhvatna, veÄ‡ je zamiÅ¡ljena da istakne nekoliko razliÄitih vrsta Transformer modela. Generalno, mogu se grupisati u tri kategorije:

- GPT-like (nazivaju se joÅ¡ _auto-regresivni_ Transformer modeli)
- BERT-like (nazivaju se joÅ¡ _auto-enkoderski_ Transformer modeli) 
- BART/T5-like (nazivaju se joÅ¡ _sequence-to-sequence_ Transformer modeli)

Detaljnije Ä‡emo se baviti ovim porodicama modela kasnije.

## Transformeri su jeziÄki modeli

Svi gore pomenuti Transformer modeli (GPT, BERT, BART, T5, etc.) trenirani su kao *jeziÄki modeli*. o znaÄi da su trenirani na velikim koliÄinama sirovog teksta na self-supervised naÄin. Self-supervised uÄenje je tip treninga gde se cilj automatski izraÄunava iz ulaza modela. To znaÄi da ljudi nisu potrebni za obeleÅ¾avanje podataka!

Ovaj tip modela razvija statistiÄko razumevanje jezika na kojem je treniran, ali nije veoma koristan za specifiÄne praktiÄne zadatke. Zbog toga opÅ¡ti pretrained model prolazi kroz proces nazvan *uÄenje prenoÅ¡enjem*. Tokom ovog procesa, model se fine-tune-uje na nadgledan naÄin -- tj. koristeÄ‡i ljudski obeleÅ¾ene labele -- na datom zadatku.

Primer zadatka je predviÄ‘anje sledeÄ‡e reÄi u reÄenici nakon Äitanja *n* prethodnih reÄi. Ovo se naziva *kauzalno jeziÄko modelovanje* jer izlaz zavisi od proÅ¡lih i sadaÅ¡njih ulaza, ali ne i od buduÄ‡ih.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Primer kauzalnog jeziÄkog modelovanja gde se predviÄ‘a sledeÄ‡a reÄ u reÄenici.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Primer kauzalnog jeziÄkog modelovanja gde se predviÄ‘a sledeÄ‡a reÄ u reÄenici..">
</div>

JoÅ¡ jedan primer je *maskirano jeziÄko modelovanje*, gde model predviÄ‘a maskiranu reÄ u reÄenici.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Primer maskiranog jeziÄkog modelovanja gde se predviÄ‘a maskirana reÄ u reÄenici.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Primer maskiranog jeziÄkog modelovanja gde se predviÄ‘a maskirana reÄ u reÄenici.">
</div>

## Transformeri su veliki modeli

Osim nekoliko izuzetaka (poput DistilBERT-a), generalna strategija za postizanje boljih performansi je poveÄ‡anje veliÄine modela, kao i koliÄine podataka na kojima se treniraju.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Broj parametara nedavnih Transformer modela" width="90%">
</div>

NaÅ¾alost, treniranje modela, posebno velikog, zahteva veliku koliÄinu podataka. To postaje vrlo skupo u smislu vremena i resursa za raÄunanje. ÄŒak se prevodi i na uticaj na Å¾ivotnu sredinu, kao Å¡to se moÅ¾e videti na sledeÄ‡em grafikonu.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="Karbonski otisak velikog jeziÄkog modela.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="Karbonski otisak velikog jeziÄkog modela.">
</div>

<Youtube id="ftWlj4FBHTg"/>

Ovo pokazuje projekat za (vrlo veliki) model koji vodi tim koji svesno pokuÅ¡ava da smanji uticaj treniranja na Å¾ivotnu sredinu. Otisak koji bi proizveo veliki broj pokuÅ¡aja da se dobiju najbolji hiperparametri bio bi joÅ¡ veÄ‡i.

Zamislite da svaki put kada neki istraÅ¾ivaÄki tim, studentska organizacija ili kompanija Å¾ele da treniraju model, to rade od nule. To bi dovelo do ogromnih, nepotrebnih globalnih troÅ¡kova!

Zato je deljenje jeziÄkih modela kljuÄno: deljenje treniranih teÅ¾ina i izgradnja na veÄ‡ treniranim teÅ¾inama smanjuje ukupne troÅ¡kove i karbonski otisak zajednice.

InaÄe, moÅ¾ete proceniti karbonski otisak treniranja svojih modela kroz nekoliko alata. Na primer [ML CO2 Impact](https://mlco2.github.io/impact/) ili [Code Carbon]( https://codecarbon.io/) koji je integrisan u ğŸ¤— Transformers. Da biste saznali viÅ¡e o ovome, moÅ¾ete proÄitati ovaj [blog post](https://huggingface.co/blog/carbon-emissions-on-the-hub) koji Ä‡e vam pokazati kako da generiÅ¡ete `emissions.csv`  fajl sa procenom ugljeniÄnog otiska vaÅ¡eg treniranja, kao i [dokumentaciju](https://huggingface.co/docs/hub/model-cards-co2) iz ğŸ¤—Transformers koja se bavi ovom temom..


## UÄenje prenoÅ¡enjem

<Youtube id="BqqfQnyjmgg" />

*Pretraining* je Äin treniranja modela od nule: teÅ¾ine su nasumiÄno inicijalizovane, a treniranje poÄinje bez ikakvog predznanja.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="Pretraining jeziÄkog modela je skupo i u pogledu vremena i novca.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="Pretraining jeziÄkog modela je skupo i u pogledu vremena i novca.">
</div>

Pretraining se obiÄno radi na vrlo velikim koliÄinama podataka. Zbog toga je potrebno vrlo mnogo podataka, a treniranje moÅ¾e trajati i do nekoliko nedelja.

*Fine-tuning*, s druge strane, je treniranje koje se obavlja **nakon** Å¡to je model istreniran. Da biste sproveli fine-tuning, prvo pribavite unapred istrenirani jeziÄki model, a zatim obavite dodatno treniranje sa skupom podataka specifiÄnim za vaÅ¡ zadatak. ÄŒekajte -- zaÅ¡to jednostavno ne biste trenirali model za vaÅ¡u konaÄnu upotrebu od poÄetka (**od nule**)? Postoji nekoliko razloga:

*  Pretrained model je treniran na skupu podataka koji ima neke sliÄnosti sa skupom podataka za fine-tuning. Fine-tuning tako moÅ¾e da iskoristi znanje koje je poÄetni model stekao tokom pretraining-a (na primer, kod NLP problema, pretrained model Ä‡e imati neku vrstu statistiÄkog razumevanja jezika koji koristite za svoj zadatak). 
*  PoÅ¡to je pretrained model veÄ‡ treniran na mnogo podataka, fine-tuning zahteva mnogo manje podataka za postizanje pristojnih rezultata.
*  Iz istog razloga, koliÄina vremena i resursa potrebnih za postizanje dobrih rezultata je znatno manja.

Na primer, mogli biste iskoristiti unapred trenirani model treniran na engleskom jeziku i zatim ga fine-tune-ujete na arXiv korpusu, Äime biste dobili model zasnovan na nauci/istraÅ¾ivanju. Fine-tuning Ä‡e zahtevati samo ograniÄenu koliÄinu podataka: znanje koje je unapred trenirani model stekao se "prenosi, otud izraz *uÄenje prenoÅ¡enjem*.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="Fine-tuning jeziÄkog modela je jeftiniji u pogledu vremena i novca nego pretraining.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="Fine-tuning jeziÄkog modela je jeftiniji u pogledu vremena i novca nego pretraining">
</div>

Fine-tuning modela zato ima niÅ¾e troÅ¡kove u pogledu vremena, podataka, finansija i uticaja na Å¾ivotnu sredinu. TakoÄ‘e je brÅ¾e i lakÅ¡e iterirati razliÄite Å¡eme fine-tuning-a, jer treniranje nije toliko ograniÄavajuÄ‡e kao potpuni pretraining.

Ovaj proces Ä‡e takoÄ‘e postiÄ‡i bolje rezultate nego treniranje od nule (osim ako nemate mnogo podataka), zbog Äega uvek treba da pokuÅ¡ate da iskoristite unapred trenirani model -- onaj koji je Å¡to bliÅ¾i zadatku koji imate -- i da ga fine-tune-ujete.

## OpÅ¡ta arhitektura

U ovom delu Ä‡emo razmotriti opÅ¡tu arhitekturu Transformer modela. Ne brinite ako ne razumete neke od pojmova; postoje detaljni delovi kasnije koji pokrivaju svaki od komponenti.

<Youtube id="H39Z_720T5s" />

## Uvod

Model se prvenstveno sastoji od dva bloka:

* **Enkoder (levo)**: Enkoder prima ulaz i gradi njegovu reprezentaciju (njegove karakteristike). To znaÄi da je model optimizovan da stekne razumevanje iz ulaza.
* **Dekoder (desno)**: Dekoder koristi reprezentaciju enkodera (karakteristike) zajedno sa drugim ulazima da bi generisao ciljnu sekvencu. To znaÄi da je model optimizovan za generisanje izlaza.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Arhitektura Transformer modela">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Arhitektura Transformer modela">
</div>

Svaka od ovih komponenti moÅ¾e se koristiti nezavisno, u zavisnosti od zadatka:

* **Samo enkoder modeli**: Dobri za zadatke koji zahtevaju razumevanje ulaza, poput klasifikacije reÄenica i prepoznavanja entiteta.
* **Samo dekoder modeli**: Dobri za generativne zadatke poput generisanja teksta
* **Enkoder-dekoder modeli** ili **sequence-to-sequence modeli**: Dobri za generativne zadatke koji zahtevaju ulaz, poput prevoÄ‘enja ili sumarizacije.

Detaljnije Ä‡emo se baviti ovim arhitekturama kasnije u odeljcima.

## Slojevi paÅ¾nje

KljuÄna karakteristika Transformer modela je da su izgraÄ‘eni sa posebnim slojevima zvanim *slojevi paÅ¾nje*. Zapravo, naslov rada koji je predstavio Transformer arhitekturu bio je ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)! Detalje slojeva paÅ¾nje istraÅ¾iÄ‡emo kasnije u kursu; za sada je dovoljno da znate da Ä‡e ovaj sloj reÄ‡i modelu da posveti posebnu paÅ¾nju odreÄ‘enim reÄima u reÄenici koju mu prosledite (i viÅ¡e-manje ignoriÅ¡e ostale) kada se bavi reprezentacijom svake reÄi.

Da stavimo ovo u kontekst, razmotrimo zadatak prevoÄ‘enja teksta sa engleskog na francuski. S obzirom na ulaz "You like this course", model za prevoÄ‘enje moraÄ‡e da obrati paÅ¾nju na susednu reÄ "You" da bi dobio pravilan prevod za reÄ "like", jer se u francuskom glagol "like" konjugira drugaÄije u zavisnosti od subjekta. Ostatak reÄenice, meÄ‘utim, nije koristan za prevoÄ‘enje te reÄi. Na sliÄan naÄin, kada prevodite "this", model Ä‡e takoÄ‘e morati da obrati paÅ¾nju na reÄ "course", jer se "this" prevodi drugaÄije u zavisnosti od toga da li je pridruÅ¾ena imenica muÅ¡kog ili Å¾enskog roda. Opet, druge reÄi u reÄenici neÄ‡e biti vaÅ¾ne za prevoÄ‘enje "course". Sa sloÅ¾enijim reÄenicama (i sloÅ¾enijim gramatiÄkim pravilima), model bi morao da obrati posebnu paÅ¾nju na reÄi koje se mogu pojaviti dalje u reÄenici kako bi pravilno preveo svaku reÄ.

Isti koncept se primenjuje na svaki zadatak povezan sa prirodnim jezikom: reÄ sama po sebi ima znaÄenje, ali to znaÄenje je duboko pod uticajem konteksta, koji moÅ¾e biti bilo koja druga reÄ (ili reÄi) pre ili posle reÄi koja se prouÄava.

Sada kada imate predstavu o tome Å¡ta su slojevi paÅ¾nje, hajde da pogledamo Transformer arhitekturu izbliza.

## Originalna arhitektura

Transformer arhitektura je prvobitno dizajnirana za prevoÄ‘enje. Tokom treniranja, enkoder prima ulaze (reÄenice) na odreÄ‘enom jeziku, dok dekoder prima iste reÄenice na Å¾eljenom ciljnom jeziku. U enkoderu, slojevi paÅ¾nje mogu koristiti sve reÄi u reÄenici (jer, kao Å¡to smo upravo videli, prevoÄ‘enje odreÄ‘ene reÄi moÅ¾e zavisiti od toga Å¡ta se nalazi posle, kao i pre te reÄi u reÄenici). MeÄ‘utim, dekoder radi sekvencijalno i moÅ¾e obraÄ‡ati paÅ¾nju samo na reÄi u reÄenici koje je veÄ‡ preveo (dakle, samo reÄi pre one koja se trenutno generiÅ¡e). Na primer, kada smo predvideli prve tri reÄi prevedenog cilja, dajemo ih dekoderu koji zatim koristi sve ulaze enkodera kako bi pokuÅ¡ao da predvidi Äetvrtu reÄ.

Da bi se ubrzalo treniranje (kada model ima pristup ciljnim reÄenicama), dekoder se hrani celim ciljem, ali mu nije dozvoljeno da koristi buduÄ‡e reÄi (ako bi imao pristup reÄi na poziciji 2 dok pokuÅ¡ava da predvidi reÄ na poziciji 2, problem ne bi bio previÅ¡e teÅ¾ak!). Na primer, kada pokuÅ¡ava da predvidi Äetvrtu reÄ, sloj paÅ¾nje Ä‡e imati pristup samo reÄima na pozicijama od 1 do 3.

Originalna Transformer arhitektura izgledala je ovako, sa enkoderom sa leve strane i dekoderom sa desne:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Arhitektura Transformer modela">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Arhitektura Transformer modela">
</div>

Napomena da prvi sloj paÅ¾nje u dekoderskom bloku obraÄ‡a paÅ¾nju na sve (proÅ¡le) ulaze u dekoder, ali drugi sloj paÅ¾nje koristi izlaz enkodera. Tako moÅ¾e pristupiti celoj ulaznoj reÄenici kako bi najbolje predvideo trenutnu reÄ.Ovo je vrlo korisno jer razliÄiti jezici mogu imati gramatiÄka pravila koja postavljaju reÄi u razliÄite redoslede, ili neki kontekst, koji je pruÅ¾en kasnije u reÄenici, moÅ¾e biti od pomoÄ‡i u odreÄ‘ivanju najboljeg prevoda odreÄ‘ene reÄ.

*Maska paÅ¾nje* se takoÄ‘e moÅ¾e koristiti u enkoderu/dekoderu kako bi se spreÄilo da model obraÄ‡a paÅ¾nju na neke posebne reÄi -- na primer, posebnu reÄ za popunjavanje koja se koristi da bi svi ulazi bili iste duÅ¾ine kada se reÄenice grupiÅ¡u zajedno.

##  Arhitekture vs. kontrolne taÄke

Dok se budemo bavili Transformer modelima u ovom kursu, videÄ‡ete pominjanje *arhitektura* i *kontrolnih taÄaka* kao i *modela*. Ovi pojmovi imaju malo razliÄita znaÄenja: 

* **Arhitektura**: Ovo je skelet modela -- definicija svakog sloja i svake operacije koja se deÅ¡ava unutar modela.
* **Kontrolne taÄke**: Ovo su teÅ¾ine koje Ä‡e biti uÄitane u datu arhitekturu.
* **Model**: Ovo je krovni izraz koji nije toliko precizan kao "arhitektura" ili "kontrolna taÄka": moÅ¾e znaÄiti oba. Ovaj kurs Ä‡e specificirati *arhitekturu* ili *kontrolnu taÄku* kada je to vaÅ¾no kako bi se smanjila nejasnoÄ‡a.

Na primer, BERT je arhitektura, dok je bert-base-cased, skup teÅ¾ina treniran od strane Google tima za prvo izdanje BERT-a, kontrolna taÄka. MeÄ‘utim, moÅ¾e se reÄ‡i "BERT model" i "bert-base-cased model".
