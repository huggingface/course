# Kako Transformeri rade?

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

U ovom delu ćemo se na visokom nivou upoznati sa arhitekturom Transformer modela.

## Malo istorije Transformera

Evo nekoliko ključnih tačaka u (kratkoj) istoriji Transformer modela:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="Kratka hronologija Transformer modela.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="Kratka hronologija Transformer modela.">
</div>

[Transformer arhitektura](https://arxiv.org/abs/1706.03762) je predstavljena u junu 2017. godine. Fokus originalnog istraživanja bio je na zadacima prevođenja. Nakon toga usledilo je predstavljanje nekoliko uticajnih modela, uključujuć:

- **Jun 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), prvi pretrained Transformer model, korišćen za fine-tuning na raznim NLP zadacima, postigao je vrhunske rezultate

- **Oktobar 2018**: [BERT](https://arxiv.org/abs/1810.04805), još jedan veliki pretrained model, dizajniran da daje bolje sažetke rečenica (više o tome u sledećem poglavlju!)

- **Februar 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), poboljšana (i veća) verzija GPT-a koja nije odmah javno objavljena zbog etičkih pitanja

- **Oktobar 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108), destilovana verzija BERT-a koja je 60% brža, 40% lakša za memoriju, a i dalje zadržava 97% performansi BERT-a

- **Oktobar 2019**: [BART](https://arxiv.org/abs/1910.13461) i [T5](https://arxiv.org/abs/1910.10683), dva velika pretrained modela koji koriste istu arhitekturu kao originalni Transformer model (prvi koji su to učinili)

- **Maj 2020**, [GPT-3](https://arxiv.org/abs/2005.14165), još veća verzija GPT-2 koja je sposobna da dobro obavlja razne zadatke bez potrebe za dodatnim fine-tuning-om (naziva se _zero-shot learning_)

Ova lista nije sveobuhvatna, već je zamišljena da istakne nekoliko različitih vrsta Transformer modela. Generalno, mogu se grupisati u tri kategorije:

- GPT-like (nazivaju se još _auto-regresivni_ Transformer modeli)
- BERT-like (nazivaju se još _auto-enkoderski_ Transformer modeli) 
- BART/T5-like (nazivaju se još _sequence-to-sequence_ Transformer modeli)

Detaljnije ćemo se baviti ovim porodicama modela kasnije.

## Transformeri su jezički modeli

Svi gore pomenuti Transformer modeli (GPT, BERT, BART, T5, etc.) trenirani su kao *jezički modeli*. o znači da su trenirani na velikim količinama sirovog teksta na self-supervised način. Self-supervised učenje je tip treninga gde se cilj automatski izračunava iz ulaza modela. To znači da ljudi nisu potrebni za obeležavanje podataka!

Ovaj tip modela razvija statističko razumevanje jezika na kojem je treniran, ali nije veoma koristan za specifične praktične zadatke. Zbog toga opšti pretrained model prolazi kroz proces nazvan *učenje prenošenjem*. Tokom ovog procesa, model se fine-tune-uje na nadgledan način -- tj. koristeći ljudski obeležene labele -- na datom zadatku.

Primer zadatka je predviđanje sledeće reči u rečenici nakon čitanja *n* prethodnih reči. Ovo se naziva *kauzalno jezičko modelovanje* jer izlaz zavisi od prošlih i sadašnjih ulaza, ali ne i od budućih.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Primer kauzalnog jezičkog modelovanja gde se predviđa sledeća reč u rečenici.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Primer kauzalnog jezičkog modelovanja gde se predviđa sledeća reč u rečenici..">
</div>

Još jedan primer je *maskirano jezičko modelovanje*, gde model predviđa maskiranu reč u rečenici.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Primer maskiranog jezičkog modelovanja gde se predviđa maskirana reč u rečenici.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Primer maskiranog jezičkog modelovanja gde se predviđa maskirana reč u rečenici.">
</div>

## Transformeri su veliki modeli

Osim nekoliko izuzetaka (poput DistilBERT-a), generalna strategija za postizanje boljih performansi je povećanje veličine modela, kao i količine podataka na kojima se treniraju.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Broj parametara nedavnih Transformer modela" width="90%">
</div>

Nažalost, treniranje modela, posebno velikog, zahteva veliku količinu podataka. To postaje vrlo skupo u smislu vremena i resursa za računanje. Čak se prevodi i na uticaj na životnu sredinu, kao što se može videti na sledećem grafikonu.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="Karbonski otisak velikog jezičkog modela.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="Karbonski otisak velikog jezičkog modela.">
</div>

<Youtube id="ftWlj4FBHTg"/>

Ovo pokazuje projekat za (vrlo veliki) model koji vodi tim koji svesno pokušava da smanji uticaj treniranja na životnu sredinu. Otisak koji bi proizveo veliki broj pokušaja da se dobiju najbolji hiperparametri bio bi još veći.

Zamislite da svaki put kada neki istraživački tim, studentska organizacija ili kompanija žele da treniraju model, to rade od nule. To bi dovelo do ogromnih, nepotrebnih globalnih troškova!

Zato je deljenje jezičkih modela ključno: deljenje treniranih težina i izgradnja na već treniranim težinama smanjuje ukupne troškove i karbonski otisak zajednice.

Inače, možete proceniti karbonski otisak treniranja svojih modela kroz nekoliko alata. Na primer [ML CO2 Impact](https://mlco2.github.io/impact/) ili [Code Carbon]( https://codecarbon.io/) koji je integrisan u 🤗 Transformers. Da biste saznali više o ovome, možete pročitati ovaj [blog post](https://huggingface.co/blog/carbon-emissions-on-the-hub) koji će vam pokazati kako da generišete `emissions.csv`  fajl sa procenom ugljeničnog otiska vašeg treniranja, kao i [dokumentaciju](https://huggingface.co/docs/hub/model-cards-co2) iz 🤗Transformers koja se bavi ovom temom..


## Učenje prenošenjem

<Youtube id="BqqfQnyjmgg" />

*Pretraining* je čin treniranja modela od nule: težine su nasumično inicijalizovane, a treniranje počinje bez ikakvog predznanja.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="Pretraining jezičkog modela je skupo i u pogledu vremena i novca.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="Pretraining jezičkog modela je skupo i u pogledu vremena i novca.">
</div>

Pretraining se obično radi na vrlo velikim količinama podataka. Zbog toga je potrebno vrlo mnogo podataka, a treniranje može trajati i do nekoliko nedelja.

*Fine-tuning*, s druge strane, je treniranje koje se obavlja **nakon** što je model istreniran. Da biste sproveli fine-tuning, prvo pribavite unapred istrenirani jezički model, a zatim obavite dodatno treniranje sa skupom podataka specifičnim za vaš zadatak. Čekajte -- zašto jednostavno ne biste trenirali model za vašu konačnu upotrebu od početka (**od nule**)? Postoji nekoliko razloga:

*  Pretrained model je treniran na skupu podataka koji ima neke sličnosti sa skupom podataka za fine-tuning. Fine-tuning tako može da iskoristi znanje koje je početni model stekao tokom pretraining-a (na primer, kod NLP problema, pretrained model će imati neku vrstu statističkog razumevanja jezika koji koristite za svoj zadatak). 
*  Pošto je pretrained model već treniran na mnogo podataka, fine-tuning zahteva mnogo manje podataka za postizanje pristojnih rezultata.
*  Iz istog razloga, količina vremena i resursa potrebnih za postizanje dobrih rezultata je znatno manja.

Na primer, mogli biste iskoristiti unapred trenirani model treniran na engleskom jeziku i zatim ga fine-tune-ujete na arXiv korpusu, čime biste dobili model zasnovan na nauci/istraživanju. Fine-tuning će zahtevati samo ograničenu količinu podataka: znanje koje je unapred trenirani model stekao se "prenosi, otud izraz *učenje prenošenjem*.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="Fine-tuning jezičkog modela je jeftiniji u pogledu vremena i novca nego pretraining.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="Fine-tuning jezičkog modela je jeftiniji u pogledu vremena i novca nego pretraining">
</div>

Fine-tuning modela zato ima niže troškove u pogledu vremena, podataka, finansija i uticaja na životnu sredinu. Takođe je brže i lakše iterirati različite šeme fine-tuning-a, jer treniranje nije toliko ograničavajuće kao potpuni pretraining.

Ovaj proces će takođe postići bolje rezultate nego treniranje od nule (osim ako nemate mnogo podataka), zbog čega uvek treba da pokušate da iskoristite unapred trenirani model -- onaj koji je što bliži zadatku koji imate -- i da ga fine-tune-ujete.

## Opšta arhitektura

U ovom delu ćemo razmotriti opštu arhitekturu Transformer modela. Ne brinite ako ne razumete neke od pojmova; postoje detaljni delovi kasnije koji pokrivaju svaki od komponenti.

<Youtube id="H39Z_720T5s" />

## Uvod

Model se prvenstveno sastoji od dva bloka:

* **Enkoder (levo)**: Enkoder prima ulaz i gradi njegovu reprezentaciju (njegove karakteristike). To znači da je model optimizovan da stekne razumevanje iz ulaza.
* **Dekoder (desno)**: Dekoder koristi reprezentaciju enkodera (karakteristike) zajedno sa drugim ulazima da bi generisao ciljnu sekvencu. To znači da je model optimizovan za generisanje izlaza.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Arhitektura Transformer modela">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Arhitektura Transformer modela">
</div>

Svaka od ovih komponenti može se koristiti nezavisno, u zavisnosti od zadatka:

* **Samo enkoder modeli**: Dobri za zadatke koji zahtevaju razumevanje ulaza, poput klasifikacije rečenica i prepoznavanja entiteta.
* **Samo dekoder modeli**: Dobri za generativne zadatke poput generisanja teksta
* **Enkoder-dekoder modeli** ili **sequence-to-sequence modeli**: Dobri za generativne zadatke koji zahtevaju ulaz, poput prevođenja ili sumarizacije.

Detaljnije ćemo se baviti ovim arhitekturama kasnije u odeljcima.

## Slojevi pažnje

Ključna karakteristika Transformer modela je da su izgrađeni sa posebnim slojevima zvanim *slojevi pažnje*. Zapravo, naslov rada koji je predstavio Transformer arhitekturu bio je ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)! Detalje slojeva pažnje istražićemo kasnije u kursu; za sada je dovoljno da znate da će ovaj sloj reći modelu da posveti posebnu pažnju određenim rečima u rečenici koju mu prosledite (i više-manje ignoriše ostale) kada se bavi reprezentacijom svake reči.

Da stavimo ovo u kontekst, razmotrimo zadatak prevođenja teksta sa engleskog na francuski. S obzirom na ulaz "You like this course", model za prevođenje moraće da obrati pažnju na susednu reč "You" da bi dobio pravilan prevod za reč "like", jer se u francuskom glagol "like" konjugira drugačije u zavisnosti od subjekta. Ostatak rečenice, međutim, nije koristan za prevođenje te reči. Na sličan način, kada prevodite "this", model će takođe morati da obrati pažnju na reč "course", jer se "this" prevodi drugačije u zavisnosti od toga da li je pridružena imenica muškog ili ženskog roda. Opet, druge reči u rečenici neće biti važne za prevođenje "course". Sa složenijim rečenicama (i složenijim gramatičkim pravilima), model bi morao da obrati posebnu pažnju na reči koje se mogu pojaviti dalje u rečenici kako bi pravilno preveo svaku reč.

Isti koncept se primenjuje na svaki zadatak povezan sa prirodnim jezikom: reč sama po sebi ima značenje, ali to značenje je duboko pod uticajem konteksta, koji može biti bilo koja druga reč (ili reči) pre ili posle reči koja se proučava.

Sada kada imate predstavu o tome šta su slojevi pažnje, hajde da pogledamo Transformer arhitekturu izbliza.

## Originalna arhitektura

Transformer arhitektura je prvobitno dizajnirana za prevođenje. Tokom treniranja, enkoder prima ulaze (rečenice) na određenom jeziku, dok dekoder prima iste rečenice na željenom ciljnom jeziku. U enkoderu, slojevi pažnje mogu koristiti sve reči u rečenici (jer, kao što smo upravo videli, prevođenje određene reči može zavisiti od toga šta se nalazi posle, kao i pre te reči u rečenici). Međutim, dekoder radi sekvencijalno i može obraćati pažnju samo na reči u rečenici koje je već preveo (dakle, samo reči pre one koja se trenutno generiše). Na primer, kada smo predvideli prve tri reči prevedenog cilja, dajemo ih dekoderu koji zatim koristi sve ulaze enkodera kako bi pokušao da predvidi četvrtu reč.

Da bi se ubrzalo treniranje (kada model ima pristup ciljnim rečenicama), dekoder se hrani celim ciljem, ali mu nije dozvoljeno da koristi buduće reči (ako bi imao pristup reči na poziciji 2 dok pokušava da predvidi reč na poziciji 2, problem ne bi bio previše težak!). Na primer, kada pokušava da predvidi četvrtu reč, sloj pažnje će imati pristup samo rečima na pozicijama od 1 do 3.

Originalna Transformer arhitektura izgledala je ovako, sa enkoderom sa leve strane i dekoderom sa desne:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Arhitektura Transformer modela">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Arhitektura Transformer modela">
</div>

Napomena da prvi sloj pažnje u dekoderskom bloku obraća pažnju na sve (prošle) ulaze u dekoder, ali drugi sloj pažnje koristi izlaz enkodera. Tako može pristupiti celoj ulaznoj rečenici kako bi najbolje predvideo trenutnu reč.Ovo je vrlo korisno jer različiti jezici mogu imati gramatička pravila koja postavljaju reči u različite redoslede, ili neki kontekst, koji je pružen kasnije u rečenici, može biti od pomoći u određivanju najboljeg prevoda određene reč.

*Maska pažnje* se takođe može koristiti u enkoderu/dekoderu kako bi se sprečilo da model obraća pažnju na neke posebne reči -- na primer, posebnu reč za popunjavanje koja se koristi da bi svi ulazi bili iste dužine kada se rečenice grupišu zajedno.

##  Arhitekture vs. kontrolne tačke

Dok se budemo bavili Transformer modelima u ovom kursu, videćete pominjanje *arhitektura* i *kontrolnih tačaka* kao i *modela*. Ovi pojmovi imaju malo različita značenja: 

* **Arhitektura**: Ovo je skelet modela -- definicija svakog sloja i svake operacije koja se dešava unutar modela.
* **Kontrolne tačke**: Ovo su težine koje će biti učitane u datu arhitekturu.
* **Model**: Ovo je krovni izraz koji nije toliko precizan kao "arhitektura" ili "kontrolna tačka": može značiti oba. Ovaj kurs će specificirati *arhitekturu* ili *kontrolnu tačku* kada je to važno kako bi se smanjila nejasnoća.

Na primer, BERT je arhitektura, dok je bert-base-cased, skup težina treniran od strane Google tima za prvo izdanje BERT-a, kontrolna tačka. Međutim, može se reći "BERT model" i "bert-base-cased model".
