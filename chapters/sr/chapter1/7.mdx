# Sequence-to-sequence modeli

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

Enkoder-dekoder modeli (takođe poznati kao *sequence-to-sequence modeli*) koriste oba dela Transformer arhitekture. Na svakoj fazi, slojevi pažnje enkodera mogu pristupiti svim rečima u početnoj rečenici, dok slojevi pažnje dekodera mogu pristupiti samo rečima koje su pozicionirane pre određene reči u ulazu.

Pretraining faza ovih modela može se obaviti korišćenjem ciljeva enkoder ili dekoder modela, ali obično uključuje nešto malo složenije. Na primer, [T5](https://huggingface.co/t5-base) se u pretraning fazi trenira tako što nasumične delove teksta (koji mogu sadržati nekoliko reči) zamenjuje posebnom maskiranom rečju, a cilj je zatim predvideti tekst koji ta maskirana reč zamenjuje.

Sequence-to-sequence modeli su najpogodniji za zadatke koji se odnose na generisanje novih rečenica u zavisnosti od datog ulaza, kao što su sumarizacija, prevođenje ili generativno odgovaranje na pitanja.

Predstavnici ove porodice modela su:

- [BART](https://huggingface.co/transformers/model_doc/bart)
- [mBART](https://huggingface.co/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/transformers/model_doc/marian)
- [T5](https://huggingface.co/transformers/model_doc/t5)
