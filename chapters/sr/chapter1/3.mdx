# Transformeri, Å¡ta mogu da urade?

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
]} />

U ovom poglavlju, videÄ‡emo Å¡ta Transformer modeli mogu da urade i korisitiÄ‡emo prvu alatku iz ğŸ¤— Transformers biblioteke: `pipeline()` funkciju.

<Tip>
ğŸ‘€ Vidite li <em>Open in Colab</em> dugme gore desno? Kliknite na njega da biste otvorili Google Colab svesku sa svim primerima koda iz ovog poglavlja. Ovo dugme Ä‡e biti dostupno u svim poglavljima koja sadrÅ¾e primere koda.

Ako primer Å¾elite da pokrenete lokalno, savetujemo vam da pogledate <a href="/course/chapter0">podeÅ¡avanje</a>.
</Tip>

## Transformeri su svuda!

Transfomer modeli se korsite za reÅ¡avanje raznih vrsta NLP zadataka, od kojih su neki pomenuti u proÅ¡lom poglavlju. Ovo su neke od kompanija i organizacija koje koriste Hugging Face i Transfromer modele, koje takoÄ‘e doprinose zajednici deljenjem njihovih modela:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Kompanije koje koriste Hugging Face" width="100%">

[ğŸ¤— Transformers biblioteka](https://github.com/huggingface/transformers) pruÅ¾a moguÄ‡nosti kreiranja i koriÅ¡Ä‡enja tih deljenih modela.  [Model Hub](https://huggingface.co/models) sadrÅ¾i hiljade istreniranih modela koje bilo ko moÅ¾e da skine i koristi. Vi takoÄ‘e moÅ¾ete da okaÄite vaÅ¡ model na Hub!

<Tip>
âš ï¸ Hugging Face Hub nije samo ograniÄen na transformer modele. Svako moÅ¾e da podeli bilo koju vrstu modela ili dataset koji Å¾eli! <a href="https://huggingface.co/join">Napravite huggingface.co</a> nalog da biste u potpunosti iskoristili sve moguÄ‡nosti!
</Tip>

Pre nego Å¡to zavirimo ispod haube Transformer modela i vidimo kako oni rade, hajde da pogledamo par primera kako oni mogu da budu koriÅ¡Ä‡eni za reÅ¡avanje nekih interesantnih NLP problema.

## Rad sa pipeline-ovima

<Youtube id="tiZFewofSLM" />

Najosnovniji objekat iz ğŸ¤— Transformers biblioteke je `pipeline()` funkcija. Spaja model sa neophodnim koracima za pretproceiranje i postprocesiranje, dozvoljavajuÄ‡i nam da direktno unesemo tekst i dobijemo intelligible odgvor:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

ÄŒak moÅ¾emo da predamo i nekoliko reÄenica!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Po difoltu, pipline bira odreÄ‘eni model koji je veÄ‡ istreniran i fine-tuned za analizu sentimenta na engleskom. Model se skida i keÅ¡ira kada kreirate `classifier` objekat. Ako ponovo pokrenete komandu, keÅ¡irani model Ä‡e biti iskoriÅ¡Ä‡en bez potrebe za ponovnim skidanjem modela.

Postoje tri glavna koraka koja se izvrÅ¡avaju kada predate tekst u pipeline-u:

1. Tekst se preprocesira u format koji model moÅ¾e da razume.
2. Pretprocesirani tekst se predaje modelu.
3. Predikcije koje model vrati se postprocesiraju kako bi ti bile razumljive.


Neki od trenutno [dostupnih pipeline-a](https://huggingface.co/transformers/main_classes/pipelines) su:

- `feature-extraction` (vraÄ‡a vektorsku reprezentaciju teksta)
- `fill-mask`
- `ner` (prepoznavanje imenovanih entiteta)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

Hajde da pogledamo nekoliko!

## Zero-shot klasifikacija

PoÄeÄ‡emo sa zahtevnijim zadatkom gde je potrebno da klasifikujemo tekst koji nije oznaÄen. Ovo je Äest scenario u realnim projektima zato Å¡to anotacja teksta obiÄno oduzima puno vremena i zahteva domensku ekpertizu. Za ovaj sluÄ‡aj, `zero-shot-classification` pipeline predstavlja moÄ‡no reÅ¡enje: dozvoljava vam da odaberete Å¾eljene labele koje Ä‡e se koristiti za klasifikaciju, tako da ne morate da se oslanjate na labele istreniranog modela. VeÄ‡ ste videli kako model moÅ¾e da klasifikuje reÄenice na pozitivne i negativne koristeÄ‡i the dve labele -- ali takoÄ‘e moÅ¾e i da klasifikuje tekst na osnovu bilo kog skupa labela koji vi Å¾elite da koristite.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Pipeline se zove _zero-shot_ zato Å¡to nije potrebno da vi fine-tune-ujete model na vaÅ¡im podacima da biste ga koristili. On direktno moÅ¾e da vrati vrednosti verovatnoÄ‡a za svaku listu labela koju vi Å¾elite da koristite!

<Tip>

âœï¸ **Isprobajte!** Igrajte se sa vaÅ¡im sekvencama i labelama i vidite kako se model ponÅ¡a.

</Tip>


## Generisanje teksta

Hajde da vidimo kako se pipline koristi za generisanje teksta. Glavna ideja je da prosledite prompt i model Ä‡e ga dovrÅ¡iti generisanje ostatka teksta. Ovo je jako sliÄno predictive text feature-u koji je dostupan na mnogo telefona. Gnerisanje teksta ukljuÄuje dozu sluÄajnosti, tako da je potpuno normalno ako ne dobijete iste rezultate kao Å¡to su prikazani ispod.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows â€” data flows of various types, as seen by the '
                    'HTTP'}]
```

MoÅ¾ete da kontroliÅ¡ete koliko razliÄitih sekvenci Ä‡e se generisati koristeÄ‡i argument `num_return_sequences` i ukupnu duÅ¾inu izlaznog teksta koristeÄ‡i argument `max_length`.

<Tip>

âœï¸ **Isprobajte!** Koristite `num_return_sequences` i `max_length` argumente da generiÅ¡ete 2 reÄenice gde Ä‡e svaka od njih imati po 15 reÄi.

</Tip>


## KoriÅ¡Ä‡enje bilo kog modela sa Hub-a u pipline-u

Prethodni primeir koristili su difoltne modele za Å¾eljene taskove, ali vi takoÄ‘e moÅ¾ete izabrati odreÄ‘eni model sa Hub-a koji Å¾elite da koristite u pipline-u za reÅ¡avanje odreÄ‘enog zadatka -- recimo, generisanje teksta. Idite na [Model Hub](https://huggingface.co/models) i kliknite na odgovarajuÄ‡i tag koji se nalazi levo da bi prikazali samo podrÅ¾ane modele za taj zadatak. Trebali bi da doÄ‘ete do stranice kao Å¡to je [ova](https://huggingface.co/models?pipeline_tag=text-generation).

Hajde da isprobamo [`distilgpt2`](https://huggingface.co/distilgpt2) model! Evo kako da ga uÄitate koristeÄ‡i isti pipline kao i ranije:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

Dalje moÅ¾ete podeÅ¡avati vaÅ¡u pretragu tako Å¡to Ä‡ete kliknuti na language tag i izabrati model koji Ä‡e generisati tekst na drugom jeziku. Model Hub sadrÅ¾i i checkpoint-e za viÅ¡ejeziÄne modele koji imaju podrÅ¡ku za viÅ¡e jezika.

Jednom kada izaberete model kliknite na njega, videÄ‡ete vidÅ¾et koji dozvoljava da model probate direktno online. Na ovaj naÄin moÅ¾ete brzo da testirate sposobnosti modela pre nego Å¡to ga skinete.

<Tip>

âœï¸ **Isprobajte!** Koristite filtere da pronaÄ‘ete model za generisanje teksta na nekom drugom jeziku. Slobodno se poigrajte sa vidÅ¾etom i koristite model kroz pipline!

</Tip>

### Inference API[[the-inference-api]]

Svi modeli mogu da se testiraju direktno kroz vaÅ¡ pretraÅ¾ivaÄ koristeÄ‡i Inference API, koji je dostupan na Hugging Face [vebsajtu](https://huggingface.co/). Na ovoj stranici moÅ¾ete direktno da se poigrate sa modelom unoseÄ‡i tekst i gledajuÄ‡i kako model procesira ulazne podatke.

Inference API koji pokreÄ‡e vidÅ¾et je dostupan i kao plaÄ‡eni proizvod, gde dolazi do izraÄ‘aja u koliko vam je neophodan za vaÅ¡e workflows. Pogledajte [stranicu sa cenama](https://huggingface.co/pricing) za viÅ¡e detalja.

## Popunjavanje maske

SledeÄ‡i pipeline koji Ä‡ete probati je `fill-mask`. Ideja ovog zadatka je da se popune praznine u datom tekstu:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

`top_k` argument kontroliÅ¡e koliko Ä‡e se moguÄ‡ih opcija prikazati. Obratite paÅ¾nju da model ovde popunjava specijalnu `<mask>` reÄ, koja se Äesto naziva *mask token*. Drugi mask-filling modeli mogu da imaju drugaÄije mask tokene, tako da je dobra praksa da uvek proverite mask word kada istrÅ¾ujete druge modele. Jedan od naÄina da to uradite je da pogledate mask word koji se koristi u vidÅ¾etu.

<Tip>

âœï¸ **Isprobajte!** PotraÅ¾ite `bert-base-cased` model na Hub-u i identifikujte njegov mask word u Inference API vidÅ¾etu. Å ta ovaj model predviÄ‘a za naÅ¡u reÄenicu u `pipeline` primeru iznad?

</Tip>

## Prepoznavanje imenovanih entiteta

Prepoznavanje imenovanih entiteta (NER - Named Entity Recognition) je zadatak gde model treba da pronaÄ‘e koji delovi ulaznog teksta odgovaraju entitetima kao Å¡to su osoba, lokacija ili organizacija. Hajde da pogledamo primer:

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

Ovde je model ispravno identifikovao Sylvain kao personu (PER), Hugging Face kao organizaciju (ORG) i Brooklyn kao lokaciju (LOC).

Pradjemo opciju `grouped_entities=True` u pipeline funkciju kako bi rekli pipline-u da grupiÅ¡e delove reÄenice koji odgovaraju istom entiteu: ovde je model ispravno grupisao "Hugging" i "Face" u jedinstvenu organizaciju, iako se ime sastoji od viÅ¡e reÄi. Å ta viÅ¡e, u sledeÄ‡em poglavlju videÄ‡emo da, pretprocesiranje Äak deli neke reÄi na delove. Na primer, `Sylvain` je podeljen na Äetiri dela: `S`, `##yl`, `##va`, and `##in`. U postprocesiranju, pipeline je uspeÅ¡nog grupisao te delove.

<Tip>

âœï¸ **Isprobajte!** PretraÅ¾i Model Hub za model koji je sbosoban da uradi part-of-speech tagging (obiÅ¡no skraÄ‡eno kao POS) na Engleskom. Å ta ovaj model predviÄ‘a za reÄenicu iznad?

</Tip>

## Odgovaranje na pitanja

`question-answering` pipeline odgovara na pitanja koristeÄ‡i informacije iz datog konteksta:

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Obratite paÅ¾nju da ovaj pipeline radi tako Å¡to izvlaÄi informacije iz datog konteksta; ne generiÅ¡e odgovor.

## Sumarizacija

Sumarizacija je zadatak koji zahteva redukovanje teksta zadrÅ¾avajuÄ‡i sve (ili veÄ‡inu) najvaÅ¾nijih aspekata koji se pominju u tekstu. Evo ga primer:

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

Kao i kod generisanja teksta, moÅ¾ete da specificirate `max_length` ili `min_length` za Å¾eljeni rezultat.


## Prevod

Za prevod, moÅ¾ete da koristite difoltni model ako predate jeziÄki par u imenu zadatka (kao `"translation_en_to_fr"`), ali najlakÅ¡i naÄin je da izaberete model koji Å¾elite da koristite na [Model Hub](https://huggingface.co/models). ProbaÄ‡emo da prevod sa francuskog na engleski:

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Kao i kod generisanja teksta i sumarizacije, moÅ¾ete da specificirate `max_length` ili `min_length` za Å¾eljeni rezultat.

<Tip>

âœï¸ **Isprobajte!** PretraÅ¾ite modele za prevod na druge jezike i probajte da prevedete prethodnu reÄenicu na nekoliko drugih jezika.

</Tip>

Pipline-ovi prikazani do sada su uglavnom za demonstativne svrhe. Programirani su za odreÅ¾ene zadatke i ne mogu da da se izbore sa njihovim varijacijama. U sledeÄ‡em poglavlju Ä‡ete nauÄiti Å¡ta se nalazi unutar `pipeline()` funkcije i kako da je prilagodite za bolje ponaÅ¡anje.
