# Transformeri, šta mogu da urade?

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
]} />

U ovom poglavlju, videćemo šta Transformer modeli mogu da urade i korisitićemo prvu alatku iz 🤗 Transformers biblioteke: `pipeline()` funkciju.

<Tip>
👀 Vidite li <em>Open in Colab</em> dugme gore desno? Kliknite na njega da biste otvorili Google Colab svesku sa svim primerima koda iz ovog poglavlja. Ovo dugme će biti dostupno u svim poglavljima koja sadrže primere koda.

Ako primer želite da pokrenete lokalno, savetujemo vam da pogledate <a href="/course/chapter0">podešavanje</a>.
</Tip>

## Transformeri su svuda!

Transfomer modeli se korsite za rešavanje raznih vrsta NLP zadataka, od kojih su neki pomenuti u prošlom poglavlju. Ovo su neke od kompanija i organizacija koje koriste Hugging Face i Transfromer modele, koje takođe doprinose zajednici deljenjem njihovih modela:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Kompanije koje koriste Hugging Face" width="100%">

[🤗 Transformers biblioteka](https://github.com/huggingface/transformers) pruža mogućnosti kreiranja i korišćenja tih deljenih modela.  [Model Hub](https://huggingface.co/models) sadrži hiljade istreniranih modela koje bilo ko može da skine i koristi. Vi takođe možete da okačite vaš model na Hub!

<Tip>
⚠️ Hugging Face Hub nije samo ograničen na transformer modele. Svako može da podeli bilo koju vrstu modela ili dataset koji želi! <a href="https://huggingface.co/join">Napravite huggingface.co</a> nalog da biste u potpunosti iskoristili sve mogućnosti!
</Tip>

Pre nego što zavirimo ispod haube Transformer modela i vidimo kako oni rade, hajde da pogledamo par primera kako oni mogu da budu korišćeni za rešavanje nekih interesantnih NLP problema.

## Rad sa pipeline-ovima

<Youtube id="tiZFewofSLM" />

Najosnovniji objekat iz 🤗 Transformers biblioteke je `pipeline()` funkcija. Spaja model sa neophodnim koracima za pretproceiranje i postprocesiranje, dozvoljavajući nam da direktno unesemo tekst i dobijemo intelligible odgvor:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

Čak možemo da predamo i nekoliko rečenica!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Po difoltu, pipline bira određeni model koji je već istreniran i fine-tuned za analizu sentimenta na engleskom. Model se skida i kešira kada kreirate `classifier` objekat. Ako ponovo pokrenete komandu, keširani model će biti iskorišćen bez potrebe za ponovnim skidanjem modela.

Postoje tri glavna koraka koja se izvršavaju kada predate tekst u pipeline-u:

1. Tekst se preprocesira u format koji model može da razume.
2. Pretprocesirani tekst se predaje modelu.
3. Predikcije koje model vrati se postprocesiraju kako bi ti bile razumljive.


Neki od trenutno [dostupnih pipeline-a](https://huggingface.co/transformers/main_classes/pipelines) su:

- `feature-extraction` (vraća vektorsku reprezentaciju teksta)
- `fill-mask`
- `ner` (prepoznavanje imenovanih entiteta)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

Hajde da pogledamo nekoliko!

## Zero-shot klasifikacija

Počećemo sa zahtevnijim zadatkom gde je potrebno da klasifikujemo tekst koji nije označen. Ovo je čest scenario u realnim projektima zato što anotacja teksta obično oduzima puno vremena i zahteva domensku ekpertizu. Za ovaj slućaj, `zero-shot-classification` pipeline predstavlja moćno rešenje: dozvoljava vam da odaberete željene labele koje će se koristiti za klasifikaciju, tako da ne morate da se oslanjate na labele istreniranog modela. Već ste videli kako model može da klasifikuje rečenice na pozitivne i negativne koristeći the dve labele -- ali takođe može i da klasifikuje tekst na osnovu bilo kog skupa labela koji vi želite da koristite.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Pipeline se zove _zero-shot_ zato što nije potrebno da vi fine-tune-ujete model na vašim podacima da biste ga koristili. On direktno može da vrati vrednosti verovatnoća za svaku listu labela koju vi želite da koristite!

<Tip>

✏️ **Isprobajte!** Igrajte se sa vašim sekvencama i labelama i vidite kako se model ponša.

</Tip>


## Generisanje teksta

Hajde da vidimo kako se pipline koristi za generisanje teksta. Glavna ideja je da prosledite prompt i model će ga dovršiti generisanje ostatka teksta. Ovo je jako slično predictive text feature-u koji je dostupan na mnogo telefona. Gnerisanje teksta uključuje dozu slučajnosti, tako da je potpuno normalno ako ne dobijete iste rezultate kao što su prikazani ispod.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows — data flows of various types, as seen by the '
                    'HTTP'}]
```

Možete da kontrolišete koliko različitih sekvenci će se generisati koristeći argument `num_return_sequences` i ukupnu dužinu izlaznog teksta koristeći argument `max_length`.

<Tip>

✏️ **Isprobajte!** Koristite `num_return_sequences` i `max_length` argumente da generišete 2 rečenice gde će svaka od njih imati po 15 reči.

</Tip>


## Korišćenje bilo kog modela sa Hub-a u pipline-u

Prethodni primeir koristili su difoltne modele za željene taskove, ali vi takođe možete izabrati određeni model sa Hub-a koji želite da koristite u pipline-u za rešavanje određenog zadatka -- recimo, generisanje teksta. Idite na [Model Hub](https://huggingface.co/models) i kliknite na odgovarajući tag koji se nalazi levo da bi prikazali samo podržane modele za taj zadatak. Trebali bi da dođete do stranice kao što je [ova](https://huggingface.co/models?pipeline_tag=text-generation).

Hajde da isprobamo [`distilgpt2`](https://huggingface.co/distilgpt2) model! Evo kako da ga učitate koristeći isti pipline kao i ranije:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

Dalje možete podešavati vašu pretragu tako što ćete kliknuti na language tag i izabrati model koji će generisati tekst na drugom jeziku. Model Hub sadrži i checkpoint-e za višejezične modele koji imaju podršku za više jezika.

Jednom kada izaberete model kliknite na njega, videćete vidžet koji dozvoljava da model probate direktno online. Na ovaj način možete brzo da testirate sposobnosti modela pre nego što ga skinete.

<Tip>

✏️ **Isprobajte!** Koristite filtere da pronađete model za generisanje teksta na nekom drugom jeziku. Slobodno se poigrajte sa vidžetom i koristite model kroz pipline!

</Tip>

### Inference API[[the-inference-api]]

Svi modeli mogu da se testiraju direktno kroz vaš pretraživač koristeći Inference API, koji je dostupan na Hugging Face [vebsajtu](https://huggingface.co/). Na ovoj stranici možete direktno da se poigrate sa modelom unoseći tekst i gledajući kako model procesira ulazne podatke.

Inference API koji pokreće vidžet je dostupan i kao plaćeni proizvod, gde dolazi do izrađaja u koliko vam je neophodan za vaše workflows. Pogledajte [stranicu sa cenama](https://huggingface.co/pricing) za više detalja.

## Popunjavanje maske

Sledeći pipeline koji ćete probati je `fill-mask`. Ideja ovog zadatka je da se popune praznine u datom tekstu:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

`top_k` argument kontroliše koliko će se mogućih opcija prikazati. Obratite pažnju da model ovde popunjava specijalnu `<mask>` reč, koja se često naziva *mask token*. Drugi mask-filling modeli mogu da imaju drugačije mask tokene, tako da je dobra praksa da uvek proverite mask word kada istržujete druge modele. Jedan od načina da to uradite je da pogledate mask word koji se koristi u vidžetu.

<Tip>

✏️ **Isprobajte!** Potražite `bert-base-cased` model na Hub-u i identifikujte njegov mask word u Inference API vidžetu. Šta ovaj model predviđa za našu rečenicu u `pipeline` primeru iznad?

</Tip>

## Prepoznavanje imenovanih entiteta

Prepoznavanje imenovanih entiteta (NER - Named Entity Recognition) je zadatak gde model treba da pronađe koji delovi ulaznog teksta odgovaraju entitetima kao što su osoba, lokacija ili organizacija. Hajde da pogledamo primer:

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

Ovde je model ispravno identifikovao Sylvain kao personu (PER), Hugging Face kao organizaciju (ORG) i Brooklyn kao lokaciju (LOC).

Pradjemo opciju `grouped_entities=True` u pipeline funkciju kako bi rekli pipline-u da grupiše delove rečenice koji odgovaraju istom entiteu: ovde je model ispravno grupisao "Hugging" i "Face" u jedinstvenu organizaciju, iako se ime sastoji od više reči. Šta više, u sledećem poglavlju videćemo da, pretprocesiranje čak deli neke reči na delove. Na primer, `Sylvain` je podeljen na četiri dela: `S`, `##yl`, `##va`, and `##in`. U postprocesiranju, pipeline je uspešnog grupisao te delove.

<Tip>

✏️ **Isprobajte!** Pretraži Model Hub za model koji je sbosoban da uradi part-of-speech tagging (obišno skraćeno kao POS) na Engleskom. Šta ovaj model predviđa za rečenicu iznad?

</Tip>

## Odgovaranje na pitanja

`question-answering` pipeline odgovara na pitanja koristeći informacije iz datog konteksta:

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Obratite pažnju da ovaj pipeline radi tako što izvlači informacije iz datog konteksta; ne generiše odgovor.

## Sumarizacija

Sumarizacija je zadatak koji zahteva redukovanje teksta zadržavajući sve (ili većinu) najvažnijih aspekata koji se pominju u tekstu. Evo ga primer:

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

Kao i kod generisanja teksta, možete da specificirate `max_length` ili `min_length` za željeni rezultat.


## Prevod

Za prevod, možete da koristite difoltni model ako predate jezički par u imenu zadatka (kao `"translation_en_to_fr"`), ali najlakši način je da izaberete model koji želite da koristite na [Model Hub](https://huggingface.co/models). Probaćemo da prevod sa francuskog na engleski:

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Kao i kod generisanja teksta i sumarizacije, možete da specificirate `max_length` ili `min_length` za željeni rezultat.

<Tip>

✏️ **Isprobajte!** Pretražite modele za prevod na druge jezike i probajte da prevedete prethodnu rečenicu na nekoliko drugih jezika.

</Tip>

Pipline-ovi prikazani do sada su uglavnom za demonstativne svrhe. Programirani su za odrežene zadatke i ne mogu da da se izbore sa njihovim varijacijama. U sledećem poglavlju ćete naučiti šta se nalazi unutar `pipeline()` funkcije i kako da je prilagodite za bolje ponašanje.
