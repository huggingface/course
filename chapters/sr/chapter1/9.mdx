# Rezime

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

U ovom poglavlju ste videli kako da pristupite različitim NLP zadacima koristeći visok nivo apstrakcije koji nudi pipeline() funkcija iz 🤗 Transformers biblioteke. Takođe ste naučili kako da pretražujete i koristite modele na Hub-u, kao i kako da koristite Inference API za testiranje modela direktno u vašem pretraživču.

Razgovarali smo o tome kako Transformer modeli funkcionišu na visokom nivou i o važnosti prensenog učenja i fine-tuning-a. Ključni aspekt je da možete koristiti celu arhitekturu ili samo enkoder ili dekoder, u zavisnosti od vrste zadatka koji želite da rešite. Sledeća tabela sumira ovo:

| Model           | Primer                                     | Zadatak                                                                                        |
|-----------------|--------------------------------------------|------------------------------------------------------------------------------------------------|
| Enkoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | SKlasifikacija rečenica, prepoznavanje imenovanih entiteta, ekstraktivno odgovaranje na pitanja|
| Dekoder         | CTRL, GPT, GPT-2, Transformer XL           | Generisanje teskta                                                                             |
| Enkoder-decoder | BART, T5, Marian, mBART                    | Sumarizacija, prevod, generativno odgovaranje na pitanja                                       |
