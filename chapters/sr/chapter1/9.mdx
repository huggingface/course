# Rezime

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

U ovom poglavlju ste videli kako da pristupite razliÄitim NLP zadacima koristeÄ‡i visok nivo apstrakcije koji nudi pipeline() funkcija iz ğŸ¤— Transformers biblioteke. TakoÄ‘e ste nauÄili kako da pretraÅ¾ujete i koristite modele na Hub-u, kao i kako da koristite Inference API za testiranje modela direktno u vaÅ¡em pretraÅ¾ivÄu.

Razgovarali smo o tome kako Transformer modeli funkcioniÅ¡u na visokom nivou i o vaÅ¾nosti prensenog uÄenja i fine-tuning-a. KljuÄni aspekt je da moÅ¾ete koristiti celu arhitekturu ili samo enkoder ili dekoder, u zavisnosti od vrste zadatka koji Å¾elite da reÅ¡ite. SledeÄ‡a tabela sumira ovo:

| Model           | Primer                                     | Zadatak                                                                                        |
|-----------------|--------------------------------------------|------------------------------------------------------------------------------------------------|
| Enkoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | SKlasifikacija reÄenica, prepoznavanje imenovanih entiteta, ekstraktivno odgovaranje na pitanja|
| Dekoder         | CTRL, GPT, GPT-2, Transformer XL           | Generisanje teskta                                                                             |
| Enkoder-decoder | BART, T5, Marian, mBART                    | Sumarizacija, prevod, generativno odgovaranje na pitanja                                       |
