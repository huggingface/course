# Dekoder modeli

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="d_ixlCubqQw" />

Dekoder modeli koriste samo dekoder deo Transformer modela. U svakoj fazi, za određenu reč slojevi pažnje mogu pristupiti samo rečima koje se nalaze pre nje u rečenici. Ovi modeli se često nazivaju *autoregresivni modeli*.

Pretraining faza dekoder modela obično se zasniva na predviđanju sledeće reči u rečenici.

Ovi modeli su najpogodniji za zadatke koji uključuju generisanje teksta.

Predstavnici ove porodice modela su:

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl)
- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transfo-xl)
