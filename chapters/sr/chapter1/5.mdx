# Enkoder modeli

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="MUqNwgPjJvQ" />

Enkoder modeli koriste samo enkoder Transformer modela. Encoder models use only the encoder of a Transformer model. U svakoj fazi, slojevi pažnje mogu pristupiti svim rečima u početnoj rečenici. Ovi modeli se često opisuju kao modeli sa "dvosmernom" pažnjom, i često se nazivaju *auto-enkoderskim modelima*.

Pretraining faza ovih modela obično se zasniva na nekakvom korumpiranju date rečenice (na primer, maskiranjem nasumičnih reči u njoj) i zadavanju zadatka modelu da pronađe ili rekonstruiše početnu rečenicu.

Enkoder modeli su najpogodniji za zadatke koji zahtevaju razumevanje cele rečenice, kao što su klasifikacija rečenica, prepoznavanje imenovanih entiteta (i generalno klasifikacija reči) i ekstraktivno odgovaranje na pitanja.

Predstavnici of porodice modela su:

- [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)
- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
