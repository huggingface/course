<FrameworkSwitchCourse {fw} />

# FAISS Ú©Û’ Ø³Ø§ØªÚ¾ semantic search[[semantic-search-with-faiss]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
]} />

{/if}

[Ø¨Ø§Ø¨ 5](/course/chapter5/5) Ù…ÛŒÚºØŒ ÛÙ… Ù†Û’ ğŸ¤— Datasets repository Ø³Û’ GitHub issues Ø§ÙˆØ± comments Ú©Ø§ Ø§ÛŒÚ© dataset ØªÛŒØ§Ø± Ú©ÛŒØ§ ØªÚ¾Ø§Û” Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº ÛÙ… Ø§Ø³ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ search engine ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº Ú¯Û’ Ø¬Ùˆ ÛÙ…ÛŒÚº library Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº ÛÙ…Ø§Ø±Û’ Ø³Ø¨ Ø³Û’ Ø§ÛÙ… Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ø¯Û’ Ø³Ú©Û’!

<Youtube id="OATCgQtNX2o"/>

## semantic search Ú©Û’ Ù„ÛŒÛ’ embeddings Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„[[using-embeddings-for-semantic-search]]

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ [Ø¨Ø§Ø¨ 1](/course/chapter1) Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ØŒ Transformer-based Ø²Ø¨Ø§Ù† Ú©Û’ Ù…Ø§ÚˆÙ„Ø² Ù…ØªÙ† Ú©Û’ Ø§ÛŒÚ© Ø­ØµÛ’ Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± token Ú©Ùˆ Ø§ÛŒÚ© _embedding vector_ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø¸Ø§ÛØ± Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØ§ Ú©Û Ø§Ù†ÙØ±Ø§Ø¯ÛŒ embeddings Ú©Ùˆ "pool" Ú©Ø± Ú©Û’ Ù¾ÙˆØ±Û’ Ø¬Ù…Ù„ÙˆÚºØŒ Ù¾ÛŒØ±Ø§Ú¯Ø±Ø§ÙØŒ ÛŒØ§ (Ú©Ú†Ú¾ ØµÙˆØ±ØªÙˆÚº Ù…ÛŒÚº) Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©ÛŒ vector representation Ø¨Ù†Ø§Ø¦ÛŒ Ø¬Ø§ Ø³Ú©ØªÛŒ ÛÛ’Û” Ø§Ù† embeddings Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ corpus Ù…ÛŒÚº Ù…Ù„ØªÛ’ Ø¬Ù„ØªÛ’ documents ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº dot-product similarity (ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø§ÙˆØ± similarity metric) Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§ Ú©Ø± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…ÛŒÙ„ Ú©Ú¾Ø§Ù†Û’ ÙˆØ§Ù„Û’ documents ÙˆØ§Ù¾Ø³ Ú©ÛŒÛ’ Ø¬Ø§ØªÛ’ ÛÛŒÚºÛ”

Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº ÛÙ… embeddings Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§ÛŒÚ© semantic search engine ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø§ÛŒØ³Û’ search engines Ø±ÙˆØ§ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚ÙˆÚº Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ú©Ø¦ÛŒ ÙÙˆØ§Ø¦Ø¯ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ùˆ ØµØ±Ù query Ø§ÙˆØ± documents Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ keywords Ú©Ùˆ match Ú©Ø±Ù†Û’ Ù¾Ø± Ù…Ù†Ø­ØµØ± ÛÙˆØªÛ’ ÛÛŒÚºÛ”

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Semantic search."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Semantic search."/>
</div>

## ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ú©Ùˆ Ù„ÙˆÚˆ Ø§ÙˆØ± ØªÛŒØ§Ø± Ú©Ø±Ù†Ø§[[loading-and-preparing-the-dataset]]

Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº GitHub issues Ú©Ø§ dataset download Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ØŒ ØªÙˆ Ø¢Ø¦ÛŒÚº `load_dataset()` ÙÙ†Ú©Ø´Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

ÛŒÛØ§Úº ÛÙ… Ù†Û’ `train` split Ú©Ùˆ ÚˆÛŒÙØ§Ù„Ù¹ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± specify Ú©ÛŒØ§ ÛÛ’ØŒ Ø¬Ø³ Ø³Û’ Ø§ÛŒÚ© `Dataset` ÙˆØ§Ù¾Ø³ Ø¢ØªØ§ ÛÛ’ Ù†Û Ú©Û `DatasetDict`Û” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº pull requests Ú©Ùˆ ÙÙ„Ù¹Ø± Ú©Ø±Ù†Ø§ ÛÛ’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÙˆÛ Ø¹Ù…ÙˆÙ…Ø§Ù‹ user queries Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙ†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù†ÛÛŒÚº ÛÙˆØªÛ’ Ø§ÙˆØ± ÛÙ…Ø§Ø±Û’ search engine Ù…ÛŒÚº noise Ù¾ÛŒØ¯Ø§ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø¬ÛŒØ³Û’ Ú©Û Ø§Ø¨ ØªÚ© Ø¢Ù¾ Ø¬Ø§Ù†ØªÛ’ ÛÛŒÚºØŒ ÛÙ… `Dataset.filter()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ø§Ù† rows Ú©Ùˆ Ù†Ú©Ø§Ù„ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø³Ø§ØªÚ¾ ÛÛŒØŒ ÛÙ… Ø§Ù† rows Ú©Ùˆ Ø¨Ú¾ÛŒ ÙÙ„Ù¹Ø± Ú©Ø± Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ù† Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ comments Ù…ÙˆØ¬ÙˆØ¯ Ù†Û ÛÙˆÚº:

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

ÛÙ…Ø§Ø±Û’ dataset Ù…ÛŒÚº Ú©Ø§ÙÛŒ Ú©Ø§Ù„Ù…Ø² Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚº Ø¬Ù† Ù…ÛŒÚº Ø³Û’ Ø²ÛŒØ§Ø¯Û ØªØ± ÙˆÛ Ù†ÛÛŒÚº ÛÛŒÚº Ø¬Ù† Ú©ÛŒ ÛÙ…ÛŒÚº search engine Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û” search Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ Ø³Ø¨ Ø³Û’ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÛŒ Ú©Ø§Ù„Ù…Ø² `title`, `body`, Ø§ÙˆØ± `comments` ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û `html_url` source issue Ú©Ø§ Ù„Ù†Ú© ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û” Ø¢Ø¦ÛŒÚº `Dataset.remove_columns()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ø¨Ø§Ù‚ÛŒ Ú©Ø§Ù„Ù…Ø² Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒÚº:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

embeddings ØªØ®Ù„ÛŒÙ‚ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… ÛØ± comment Ú©Û’ Ø³Ø§ØªÚ¾ issue Ú©Ø§ title Ø§ÙˆØ± body Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ ÙÛŒÙ„ÚˆØ² context ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛŒÚºÛ” Ú†ÙˆÙ†Ú©Û ÛÙ…Ø§Ø±Ø§ `comments` Ú©Ø§Ù„Ù… ÙÛŒ Ø§Ù„Ø­Ø§Ù„ ÛØ± issue Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© list ÛÛ’ØŒ ÛÙ…ÛŒÚº Ø§Ø³Û’ "explode" Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ ØªØ§Ú©Û ÛØ± row Ø§ÛŒÚ© `(html_url, title, body, comment)` tuple Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÙˆÛ” Pandas Ù…ÛŒÚº ÛÙ… ÛŒÛ [`DataFrame.explode()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ù¾ÛÙ„Û’ ÛÙ… Ø§Ù¾Ù†Û’ dataset Ú©Ùˆ Pandas `DataFrame` Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

Ø§Ú¯Ø± ÛÙ… Ø§Ø³ DataFrame Ú©ÛŒ Ù¾ÛÙ„ÛŒ row Ø¯ÛŒÚ©Ú¾ÛŒÚº ØªÙˆ Ù¾ØªÛ Ú†Ù„Û’ Ú¯Ø§ Ú©Û Ø§Ø³ issue Ú©Û’ Ø³Ø§ØªÚ¾ Ú†Ø§Ø± comments Ù…Ù†Ø³Ù„Ú© ÛÛŒÚº:

```py
df["comments"][0].tolist()
```

```python
['the bug code locate in ï¼š\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Ø¬Ø¨ ÛÙ… DataFrame Ú©Ùˆ explode Ú©Ø±ØªÛ’ ÛÛŒÚº ØªÙˆ ÛÙ…ÛŒÚº ÛØ± comment Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© row Ù…Ù„Ù†ÛŒ Ú†Ø§ÛÛŒÛ’:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in ï¼š\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

Ø²Ø¨Ø±Ø¯Ø³ØªØŒ Ø§Ø¨ ÛÙ… Ø¯ÛŒÚ©Ú¾ Ú†Ú©Û’ ÛÛŒÚº Ú©Û rows explode ÛÙˆ Ú¯Ø¦ÛŒ ÛÛŒÚº Ø§ÙˆØ± `comments` Ú©Ø§Ù„Ù… Ù…ÛŒÚº ÛØ± comment Ø§Ù„Ú¯ Ø³Û’ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û” Ø¬Ø¨ ÛÙ… Ø§Ù¾Ù†ÛŒ Pandas analysis Ù…Ú©Ù…Ù„ Ú©Ø± Ù„ÛŒÚº ØªÙˆ ÛÙ… Ø§Ø³Û’ Ø¯ÙˆØ¨Ø§Ø±Û `Dataset` Ù…ÛŒÚº convert Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

Ø§Ø¨ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ú†Ù†Ø¯ ÛØ²Ø§Ø± comments Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚº!

<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº Ú©Û `Dataset.map()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ `comments` Ú©Ø§Ù„Ù… Ú©Ùˆ explode Ú©Ø±ÛŒÚº Ø¨ØºÛŒØ± Pandas Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒÛ’Û” ÛŒÛ ØªÚ¾ÙˆÚ‘Ø§ Ù…Ø´Ú©Ù„ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’Ø› Ø§Ø³ Ú©Ø§Ù… Ú©Û’ Ù„ÛŒÛ’ ["Batch mapping"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) Ø³ÛŒÚ©Ø´Ù† Ù…Ù„Ø§Ø­Ø¸Û Ú©Ø±ÛŒÚºÛ”

</Tip>

Ø§Ø¨ Ø¬Ø¨ Ú©Û ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ ÛØ± row Ù…ÛŒÚº Ø§ÛŒÚ© comment ÛÛ’ØŒ Ø¢Ø¦ÛŒÚº Ø§ÛŒÚ© Ù†ÛŒØ§ `comment_length` Ú©Ø§Ù„Ù… Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº Ø¬Ùˆ ÛØ± comment Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø¨ØªØ§ØªØ§ ÛÛ’:

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

ÛÙ… Ø§Ø³ Ù†Ø¦Û’ Ú©Ø§Ù„Ù… Ú©Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú†Ú¾ÙˆÙ¹Û’ comments Ú©Ùˆ ÙÙ„Ù¹Ø± Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ú©ÛŒÙˆÙ†Ú©Û Ø§ÛŒØ³Û’ comments Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± "cc @lewtun" ÛŒØ§ "Thanks!" Ø¬ÛŒØ³Û’ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬Ùˆ ÛÙ…Ø§Ø±Û’ search engine Ú©Û’ Ù„ÛŒÛ’ ØºÛŒØ± Ù…ØªØ¹Ù„Ù‚Û ÛÛŒÚºÛ” ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 15 Ø§Ù„ÙØ§Ø¸ Ø§ÛŒÚ© Ø§Ú†Ú¾Ø§ Ù…Ø¹ÛŒØ§Ø± ÛÛ’:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

Ø§Ø¨ Ø¬Ø¨ Ú©Û ÛÙ… Ù†Û’ dataset Ú©Ùˆ ØµØ§Ù Ú©Ø± Ù„ÛŒØ§ ÛÛ’ØŒ Ø¢Ø¦ÛŒÚº issue Ú©Û’ titleØŒ body Ø§ÙˆØ± comments Ú©Ùˆ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© Ù†ÛŒØ§ `text` Ú©Ø§Ù„Ù… ØªØ®Ù„ÛŒÙ‚ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }

comments_dataset = comments_dataset.map(concatenate_text)
```

ÛÙ… Ø§Ø¨ embeddings Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚº!

## text embeddings ØªØ®Ù„ÛŒÙ‚ Ú©Ø±Ù†Ø§[[creating-text-embeddings]]

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ [Ø¨Ø§Ø¨ 2](/course/chapter2) Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ Ú©Û Transformer-based Ù…Ø§ÚˆÙ„Ø² ÛØ± token Ú©Ùˆ Ø§ÛŒÚ© _embedding vector_ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø¸Ø§ÛØ± Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø§Ù†ÙØ±Ø§Ø¯ÛŒ embeddings Ú©Ùˆ pool Ú©Ø± Ú©Û’ Ù¾ÙˆØ±Û’ Ø¬Ù…Ù„ÙˆÚºØŒ Ù¾ÛŒØ±Ø§Ú¯Ø±Ø§Ù ÛŒØ§ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©ÛŒ vector representation ØªÛŒØ§Ø± Ú©ÛŒ Ø¬Ø§ Ø³Ú©ØªÛŒ ÛÛ’Û” ÛŒÛ embeddings corpus Ù…ÛŒÚº Ù…Ù„ØªÛ’ Ø¬Ù„ØªÛ’ documents ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛŒÚºØŒ Ø¬Ùˆ Ú©Û dot-product similarity (ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø§ÙˆØ± similarity metric) Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§ Ú©Ø± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…ÛŒÙ„ Ú©Ú¾Ø§Ù†Û’ ÙˆØ§Ù„Û’ documents ÙˆØ§Ù¾Ø³ Ú©Ø±ØªÛŒ ÛÛŒÚºÛ”

Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚºØŒ ÛÙ… embeddings Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§ÛŒÚ© semantic search engine ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø§ÛŒØ³Û’ search engines Ø±ÙˆØ§ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚ÙˆÚº Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ú©Ø¦ÛŒ ÙÙˆØ§Ø¦Ø¯ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ú©Û ØµØ±Ù keywords match Ú©Ø±Ù†Û’ Ù¾Ø± Ù…Ù†Ø­ØµØ± ÛÙˆØªÛ’ ÛÛŒÚºÛ”

## FAISS Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…Ø¤Ø«Ø± similarity search[[using-faiss-for-efficient-similarity-search]]

Ø§Ø¨ Ø¬Ø¨Ú©Û ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ embeddings Ú©Ø§ dataset Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ØŒ ÛÙ…ÛŒÚº Ø§Ù† Ù…ÛŒÚº ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ú©Ø§ Ø§ÛŒÚ© Ø·Ø±ÛŒÙ‚Û Ú†Ø§ÛÛŒÛ’Û” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… ğŸ¤— Datasets Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ FAISS index Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” [FAISS](https://faiss.ai/) (Facebook AI Similarity Search) Ø§ÛŒÚ© Ø§ÛŒØ³ÛŒ Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ ÛÛ’ Ø¬Ùˆ embeddings Ú©ÛŒ ØªÛŒØ²ÛŒ Ø³Û’ ØªÙ„Ø§Ø´ Ø§ÙˆØ± clustering Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¤Ø«Ø± Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù…Ø² ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’Û”

FAISS Ú©Ø§ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø®ÛŒØ§Ù„ ÛŒÛ ÛÛ’ Ú©Û Ø§ÛŒÚ© Ø®Ø§Øµ data structure ÛŒØ¹Ù†ÛŒ _index_ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§Ø¦Û’ Ø¬Ùˆ input embedding Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ù…Ù„ØªÛ’ Ø¬Ù„ØªÛ’ embeddings ØªÙ„Ø§Ø´ Ú©Ø±Û’Û” ğŸ¤— Datasets Ù…ÛŒÚº FAISS index Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… `Dataset.add_faiss_index()` ÙÙ†Ú©Ø´Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ specify Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ…Ø§Ø±Û’ dataset Ú©Ø§ Ú©ÙˆÙ† Ø³Ø§ column index Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

Ø§Ø¨ ÛÙ… Ø§Ø³ index Ù¾Ø± query Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº `Dataset.get_nearest_examples()` ÙÙ†Ú©Ø´Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’Û” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛÙ… Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ú©Ø§ embedding ØªØ®Ù„ÛŒÙ‚ Ú©Ø±ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

Ø¨Ø§Ù„Ú©Ù„ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©ÛŒ Ø·Ø±Ø­ØŒ Ø§Ø¨ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ 768-Ø§Ø¨Ø¹Ø§Ø¯ÛŒ ÙˆÛŒÚ©Ù¹Ø± ÛÛ’ Ø¬Ùˆ Ø§Ø³ØªÙØ³Ø§Ø± (query) Ú©ÛŒ Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ø³Û’ ÛÙ… Ù¾ÙˆØ±Û’ Ú©Ø§Ø±Ù¾Ø³ (corpus) Ú©Û’ Ø®Ù„Ø§Ù Ù…ÙˆØ§Ø²Ù†Û Ú©Ø±Ú©Û’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ø´Ø§Ø¨ÛØª Ø±Ú©Ú¾Ù†Û’ ÙˆØ§Ù„ÛŒ Ø§ÛŒÙ…Ø¨ÛŒÚˆÙ†Ú¯Ø² ØªÙ„Ø§Ø´ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

`Dataset.get_nearest_examples()` ÙÙ†Ú©Ø´Ù† Ø§ÛŒÚ© Ù¹ÛŒÙˆÙ¾Ù„ (tuple) ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº Ø§Ø³Ú©ÙˆØ± Ø´Ø§Ù…Ù„ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø§Ø³ØªÙØ³Ø§Ø± (query) Ø§ÙˆØ± Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ù…Ù…Ø§Ø«Ù„Øª Ú©Ùˆ Ø¯Ø±Ø¬Û Ø¨Ù†Ø¯ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ØªØ¹Ù„Ù‚Û Ù†Ù…ÙˆÙ†ÙˆÚº Ú©Ø§ Ø§ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Û Ø¨Ú¾ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ (ÛŒÛØ§ÚºØŒ 5 Ø¨ÛØªØ±ÛŒÙ† Ù…Ù…Ø§Ø«Ù„Ø§Øª)Û”  

Ø¢Ø¦ÛŒÛ’ Ø§Ù† Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ Ø§ÛŒÚ© `pandas.DataFrame` Ù…ÛŒÚº Ø¬Ù…Ø¹ Ú©Ø±ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û ÛÙ… Ø§Ù†ÛÛŒÚº Ø¢Ø³Ø§Ù†ÛŒ Ø³Û’ ØªØ±ØªÛŒØ¨ Ø¯Û’ Ø³Ú©ÛŒÚº:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Ø§Ø¨ ÛÙ… Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ú†Ù†Ø¯ Ù‚Ø·Ø§Ø±ÙˆÚº Ù¾Ø± ØªÚ©Ø±Ø§Ø± (iterate) Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û Ø¯ÛŒÚ©Ú¾ Ø³Ú©ÛŒÚº Ú©Û ÛÙ…Ø§Ø±Ø§ Ø§Ø³ØªÙØ³Ø§Ø± (query) Ø¯Ø³ØªÛŒØ§Ø¨ ØªØ¨ØµØ±ÙˆÚº (comments) Ø³Û’ Ú©Ø³ Ø­Ø¯ ØªÚ© Ù…Ø·Ø§Ø¨Ù‚Øª Ø±Ú©Ú¾ØªØ§ ÛÛ’:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out

"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
```
python
datasets = load_dataset("text", data_files=data_files)
```

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
```
python
load_dataset("./my_dataset")
```
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Not bad! Our second hit seems to match the query.

<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ø§Ù¾Ù†Ø§ query Ø¨Ù†Ø§Ø¦ÛŒÚº Ø§ÙˆØ± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú©Û Ú©ÛŒØ§ retrieved documents Ù…ÛŒÚº Ø³Û’ Ú©ÙˆØ¦ÛŒ Ø¢Ù¾ Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒØªØ§ ÛÛ’Û” ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ Ø¢Ù¾ Ú©Ùˆ search Ú©Ùˆ ÙˆØ³ÛŒØ¹ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ `k` parameter Ø¨Ú‘Ú¾Ø§Ù†Ø§ Ù¾Ú‘Û’Û”

</Tip>