# وقت آ گیا ہے "سلائس اینڈ ڈائس" کرنے کا[[time-to-slice-and-dice]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section3.ipynb"},
]} />

اکثر اوقات، جو ڈیٹا آپ کے پاس ہوتا ہے وہ ماڈلز کی تربیت کے لیے پوری طرح تیار نہیں ہوتا۔ اس سیکشن میں ہم ان مختلف خصوصیات کا جائزہ لیں گے جو 🤗 Datasets آپ کے ڈیٹاسیٹس کو صاف کرنے کے لیے فراہم کرتا ہے۔

<Youtube id="tqfSFcPMgOI"/>

## اپنے ڈیٹا کو سلائس اور ڈائس کرنا[[slicing-and-dicing-our-data]]

Pandas کی طرح، 🤗 Datasets متعدد ایسے فنکشنز فراہم کرتا ہے جن کی مدد سے آپ `Dataset` اور `DatasetDict` آبجیکٹس کے مندرجات کو منیپولیٹ کر سکتے ہیں۔ ہم پہلے ہی [باب 3](/course/chapter3) میں `Dataset.map()` میتھڈ کا استعمال دیکھ چکے ہیں، اور اس سیکشن میں ہم اپنی دسترس میں موجود دیگر فنکشنز کو دریافت کریں گے۔

اس مثال کے لیے ہم [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) استعمال کریں گے، جو [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) پر ہوسٹ ہے۔ اس میں مختلف دوائیں، علاج کی جانے والی حالت اور مریض کی اطمینان کے 10 ستارے کے ریٹنگ کے ساتھ مریضوں کے ریویوز شامل ہیں۔

سب سے پہلے ہمیں ڈیٹا ڈاؤن لوڈ اور ایکسٹریکٹ کرنا ہوگا، جو کہ `wget` اور `unzip` کمانڈز کے ذریعے کیا جا سکتا ہے:

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

چونکہ TSV صرف CSV کا ایک مختلف ورژن ہے جس میں علیحدگی کے لیے کاماز کی بجائے tabs استعمال ہوتے ہیں، اس لیے ہم ان فائلوں کو `csv` لوڈنگ اسکرپٹ اور `load_dataset()` فنکشن میں `delimiter` آرگومنٹ فراہم کر کے لوڈ کر سکتے ہیں:

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t Python میں tab کردار ہے
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

ڈیٹا اینالیسس کرتے وقت ایک اچھی مشق یہ ہے کہ آپ ایک چھوٹا رینڈم نمونہ حاصل کریں تاکہ آپ کو فوری طور پر یہ اندازہ ہو سکے کہ آپ کس قسم کے ڈیٹا کے ساتھ کام کر رہے ہیں۔ 🤗 Datasets میں ہم `Dataset.shuffle()` اور `Dataset.select()` فنکشنز کو چین کر کے ایک رینڈم نمونہ بنا سکتے ہیں:

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# پہلے چند مثالیں دیکھیں
drug_sample[:3]
```

```python
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

نوٹ کریں کہ ہم نے reproducibility کے لیے `Dataset.shuffle()` میں seed مقرر کیا ہے۔ `Dataset.select()` ایک iterable (مثلاً `range(1000)`) کی توقع کرتا ہے تاکہ شفل شدہ ڈیٹاسیٹ سے پہلی 1,000 مثالیں حاصل کی جا سکیں۔ اس نمونے سے ہمیں پہلے ہی اپنے ڈیٹاسیٹ کی چند خامیاں نظر آ رہی ہیں:

* `Unnamed: 0` کالم ہر مریض کے لیے ایک انانومائزڈ ID کی طرح لگتا ہے۔
* `condition` کالم میں uppercase اور lowercase دونوں لیبلز شامل ہیں۔
* ریویوز مختلف لمبائی کے ہیں اور ان میں Python کے لائن سیپرٹر (`\r\n`) کے ساتھ ساتھ HTML character codes جیسے `&\#039;` بھی موجود ہیں۔

آئیں دیکھتے ہیں کہ ہم 🤗 Datasets کا استعمال کر کے ان مسائل کو کیسے حل کر سکتے ہیں۔ `Unnamed: 0` کالم کے لیے مریض ID کی مفروضہ کی تصدیق کرنے کے لیے ہم `Dataset.unique()` فنکشن استعمال کر کے دیکھ سکتے ہیں کہ ID کی تعداد ہر سپلٹ میں موجود قطاروں کی تعداد سے ملتی ہے یا نہیں:

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

یہ ہماری مفروضہ کی تصدیق کرتا ہے، لہٰذا آئیں ڈیٹاسیٹ کو صاف کرتے ہوئے `Unnamed: 0` کالم کا نام تبدیل کر کے اسے زیادہ سمجھ آنے والا بنا دیتے ہیں۔ ہم `DatasetDict.rename_column()` فنکشن کا استعمال کر کے دونوں سپلٹس میں ایک ساتھ کالم کا نام تبدیل کر سکتے ہیں:

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<Tip>

✏️ **آزمائیں!** `Dataset.unique()` فنکشن کا استعمال کرتے ہوئے ٹریننگ اور ٹیسٹ سیٹس میں منفرد دواؤں اور حالات کی تعداد معلوم کریں۔

</Tip>

اگلا مرحلہ `condition` لیبلز کو normalize کرنا ہے، جس کے لیے ہم `Dataset.map()` کا استعمال کریں گے۔ جیسے کہ ہم نے [باب 3](/course/chapter3) میں tokenization کے لیے کیا تھا، ہم ایک سادہ فنکشن ڈیفائن کر سکتے ہیں جو ہر مثال پر لاگو ہو:

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}
```

```py
drug_dataset.map(lowercase_condition)
```

```python
AttributeError: 'NoneType' object has no attribute 'lower'
```

اوہ نہیں! ہمارا map فنکشن ایک مسئلے سے گزر گیا ہے! اس غلطی سے ہمیں پتہ چلتا ہے کہ `condition` کالم کی کچھ entries `None` ہیں، جنہیں lowercasing نہیں کیا جا سکتا کیونکہ وہ strings نہیں ہیں۔ ان rows کو ہٹانے کے لیے ہم `Dataset.filter()` کا استعمال کریں گے، جو `Dataset.map()` کی طرح کام کرتا ہے اور ایک فنکشن کی توقع رکھتا ہے جو ایک مثال وصول کرے۔ بجائے اس کے کہ ہم ایک واضح فنکشن لکھیں جیسے:

```py
def filter_nones(x):
    return x["condition"] is not None
```

اور پھر `drug_dataset.filter(filter_nones)` چلائیں، ہم ایک لائن میں lambda فنکشن استعمال کر سکتے ہیں:

```
lambda <arguments> : <expression>
```

جہاں `lambda` پائتھون کے خاص [کی ورڈز](https://docs.python.org/3/reference/lexical_analysis.html#keywords) میں سے ایک ہے، `<arguments>` ایک فہرست یا سیٹ ہوتی ہے جو کاما سے جدا کیے گئے اقدار پر مشتمل ہوتی ہے اور فنکشن کے ان پٹ کو بیان کرتی ہے، اور `<expression>` وہ آپریشنز ہوتے ہیں جو آپ انجام دینا چاہتے ہیں۔ مثال کے طور پر، ہم ایک سادہ `lambda` فنکشن تعریف کر سکتے ہیں جو کسی نمبر کا اسکوائر کرتا ہے:

```
lambda x : x * x
```

اس فنکشن کو کسی ان پٹ پر لاگو کرنے کے لیے، ہمیں اسے اور ان پٹ کو قوسین `()` میں لپیٹنا ہوگا:

```py
(lambda x: x * x)(3)
```

```python out
9
```

اسی طرح، ہم متعدد دلائل (arguments) کے ساتھ `lambda` فنکشنز کی تعریف کر سکتے ہیں، جہاں ہر دلیل کو کاما سے الگ کیا جاتا ہے۔ مثال کے طور پر، ہم مثلث کا رقبہ اس طرح حاصل کر سکتے ہیں:


```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

`lambda` فنکشنز مفید ہوتے ہیں جب آپ چھوٹے اور ایک بار استعمال ہونے والے فنکشنز بنانا چاہتے ہیں۔ (مزید معلومات کے لیے، ہم اینڈری برگاڈ کا شاندار [Real Python tutorial](https://realpython.com/python-lambda/) پڑھنے کی سفارش کرتے ہیں)۔  

🤗 Datasets کے تناظر میں، ہم `lambda` فنکشنز کو سادہ `map` اور `filter` آپریشنز کے لیے استعمال کر سکتے ہیں۔ تو آئیے اس تکنیک کا استعمال کرکے اپنے ڈیٹا سیٹ میں سے `None` اندراجات کو ختم کرتے ہیں:


```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```





اب `None` entries ہٹ چکی ہیں، ہم `condition` کالم کو normalize کر سکتے ہیں:

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# چیک کریں کہ lowercasing ہو گیا
drug_dataset["train"]["condition"][:3]
```

```python
['left ventricular dysfunction', 'adhd', 'birth control']
```




یہ کام کر گیا! اب جب کہ ہم نے لیبلز صاف کر لیے ہیں، آئیں ریویوز کو صاف کرنے پر توجہ دیں۔

## نئی کالمز تخلیق کرنا[[creating-new-columns]]

جب آپ کسٹمر ریویوز کے ساتھ کام کر رہے ہوں، ایک اچھی مشق یہ ہے کہ آپ ہر ریویو میں الفاظ کی تعداد چیک کریں۔ ایک ریویو صرف "Great!" جیسے ایک لفظ پر مشتمل ہو سکتا ہے یا ہزاروں الفاظ پر مشتمل ایک مکمل مضمون ہو سکتا ہے، اور استعمال کے معاملے کے مطابق آپ کو ان انتہاؤں کو مختلف طریقے سے سنبھالنے کی ضرورت ہوگی۔ ہر ریویو میں الفاظ کی تعداد معلوم کرنے کے لیے ہم whitespace پر مبنی تقسیم کے ایک اندازے کا استعمال کریں گے۔

آئیں ایک سادہ فنکشن ڈیفائن کریں جو ہر ریویو میں موجود الفاظ کی تعداد گنتا ہے:

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

ہمارے `lowercase_condition()` فنکشن کے برعکس، `compute_review_length()` ایک ایسی ڈکشنری واپس کرتا ہے جس کی key ڈیٹاسیٹ کے کسی موجودہ کالم کے مطابق نہیں ہوتی۔ اس صورت میں، جب `compute_review_length()` کو `Dataset.map()` میں پاس کیا جاتا ہے، تو یہ ڈیٹاسیٹ کی تمام rows پر لاگو ہو کر ایک نیا `review_length` کالم تخلیق کر دیتا ہے:

```py
drug_dataset = drug_dataset.map(compute_review_length)
# پہلے ٹریننگ مثال کو دیکھیں
drug_dataset["train"][0]
```

```python
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

جیسا کہ توقع کی گئی، ہمارے ٹریننگ سیٹ میں `review_length` کالم شامل ہو گیا ہے۔ ہم `Dataset.sort()` کا استعمال کر کے اس نئے کالم کو sort کر کے دیکھ سکتے ہیں کہ انتہاؤں میں کیا حالات ہیں:

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

جیسا کہ ہم نے مشاہدہ کیا، کچھ ریویوز صرف ایک لفظ کے ہیں، جو کہ sentiment analysis کے لیے ٹھیک ہو سکتا ہے لیکن اگر ہم condition کی پیش گوئی کرنا چاہتے ہیں تو معلوماتی نہیں ہوں گے۔

<Tip>

🙋 ڈیٹاسیٹ میں نئی کالمز شامل کرنے کا ایک متبادل طریقہ `Dataset.add_column()` ہے۔ اس سے آپ Python list یا NumPy array فراہم کر کے کالم شامل کر سکتے ہیں، جو ان صورتوں میں مفید ہوتا ہے جب `Dataset.map()` آپ کی تجزیے کے لیے مناسب نہ ہو۔

</Tip>

آئیں `Dataset.filter()` فنکشن کا استعمال کرتے ہوئے ایسے ریویوز کو ہٹا دیں جن میں 30 سے کم الفاظ ہیں۔ ویسے ہی جیسے ہم نے `condition` کالم کے ساتھ کیا، ہم انتہائی چھوٹے ریویوز کو فلٹر کر سکتے ہیں:

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python
{'train': 138514, 'test': 46108}
```

جیسا کہ آپ دیکھ سکتے ہیں، اس سے ہمارے اصل ٹریننگ اور ٹیسٹ سیٹس سے تقریباً 15% ریویوز ہٹ گئے ہیں۔

<Tip>

✏️ **آزمائیں!** `Dataset.sort()` فنکشن کا استعمال کرتے ہوئے ان ریویوز کو دیکھیں جن میں سب سے زیادہ الفاظ ہیں۔ دیکھنے کے لیے [دستاویزات](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.sort) میں بیان کردہ آرگومنٹ کا استعمال کریں تاکہ ریویوز کو descending order میں sort کیا جا سکے۔

</Tip>

آخری چیز جسے ہمیں سنبھالنے کی ضرورت ہے وہ ہے HTML character codes کا ریویوز میں موجود ہونا۔ ہم Python کے `html` module کا استعمال کرتے ہوئے ان حروف کو unescape کر سکتے ہیں:

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python
"I'm a transformer called BERT"
```

ہم `Dataset.map()` کا استعمال کرتے ہوئے اپنے corpus میں موجود تمام HTML characters کو unescape کر دیں گے:

```py
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

جیسا کہ آپ دیکھ سکتے ہیں، `Dataset.map()` میتھڈ ڈیٹا پراسیسنگ کے لیے کافی کارآمد ہے — اور یہ ابھی تک اس کی تمام صلاحیتوں کا احاطہ نہیں کرتا!

## `map()` میتھڈ کی سپر پاورز[[the-map-methods-superpowers]]

`Dataset.map()` میتھڈ ایک `batched` آرگومنٹ لیتا ہے جو اگر `True` پر سیٹ ہو تو ایک ساتھ متعدد مثالیں map فنکشن کو بھیجتا ہے (بیچ سائز کنفیگریبل ہے مگر ڈیفالٹ 1,000 ہے)۔ مثال کے طور پر، وہ map فنکشن جو تمام HTML characters کو unescape کرتا تھا، اسے چلانے میں تھوڑا وقت لگ رہا تھا (آپ progress bars سے وقت دیکھ سکتے ہیں)۔ ہم اس کی کارکردگی کو بہتر بنانے کے لیے ایک ساتھ کئی عناصر کو list comprehension کے ذریعے پراسیس کر سکتے ہیں۔

جب آپ `batched=True` سیٹ کرتے ہیں تو فنکشن کو ڈیٹاسیٹ کے فیلڈز کے ساتھ ایک ڈکشنری موصول ہوتی ہے، لیکن اب ہر ویلیو ایک _list of values_ ہوتی ہے، نہ کہ ایک واحد ویلیو۔ `Dataset.map()` کا ریٹرن ویلیو بھی ایسی ہی ہونی چاہیے: ایک ڈکشنری جس میں وہ فیلڈز شامل ہوں جنہیں ہم اپڈیٹ یا نیا شامل کرنا چاہتے ہیں، اور ان کی ویلیوز کی لسٹ۔ مثال کے طور پر، یہاں ایک اور طریقہ ہے جس سے تمام HTML characters کو unescape کیا جا سکتا ہے، مگر `batched=True` کے ساتھ:

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

اگر آپ یہ کوڈ نوٹ بُک میں چلا رہے ہیں تو آپ دیکھیں گے کہ یہ کمانڈ پچھلی کمانڈ کے مقابلے میں بہت تیزی سے چلتی ہے۔ اور یہ اس لیے نہیں کہ ہمارے ریویوز پہلے ہی unescape ہو چکے ہیں — اگر آپ پچھلے سیکشن کی ہدایات کو دوبارہ (بغیر `batched=True` کے) چلائیں تو اسے پہلے جیسا ہی وقت لگے گا۔ اس کی وجہ یہ ہے کہ list comprehensions عموماً for loop کے مقابلے میں تیز ہوتی ہیں، اور اس کے علاوہ ایک ساتھ بہت سے عناصر تک رسائی سے بھی کارکردگی میں بہتری آتی ہے۔

`Dataset.map()` کے ساتھ `batched=True` کا استعمال "فاسٹ" ٹوکنائزرز کی رفتار کو ان لاک کرنے کے لیے ضروری ہوگا جن کا ہم [باب 6](/course/chapter6) میں سامنا کریں گے، جو بڑی لسٹوں کے texts کو تیزی سے tokenize کر سکتے ہیں۔ مثال کے طور پر، اگر ہم تمام drug ریویوز کو ایک fast tokenizer کے ساتھ tokenize کرنا چاہیں تو ہم ایسا فنکشن استعمال کر سکتے ہیں:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

جیسا کہ آپ نے [باب 3](/course/chapter3) میں دیکھا، ہم ٹوکنائزر کو ایک یا کئی مثالیں پاس کر سکتے ہیں، لہٰذا ہم اس فنکشن کو `batched=True` کے ساتھ یا بغیر استعمال کر سکتے ہیں۔ آئیں اس موقع کا فائدہ اٹھاتے ہوئے مختلف اختیارات کی کارکردگی کا موازنہ کرتے ہیں۔ نوٹ بُک میں، آپ `%time` شامل کر کے ایک لائن کے کوڈ کی کارکردگی جانچ سکتے ہیں:

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

آپ پورے سیل کو بھی `%%time` کے ساتھ ٹائم کر سکتے ہیں۔ جس ہارڈویئر پر ہم نے یہ آزمایا، اس نے "Wall time" کے بعد 10.8s دکھایا۔

<Tip>

✏️ **آزمائیں!** وہی ہدایت `batched=True` کے ساتھ اور بغیر چلائیں، پھر ایک slow tokenizer (AutoTokenizer.from_pretrained() میں `use_fast=False` شامل کریں) کے ساتھ آزمائیں تاکہ آپ دیکھ سکیں کہ آپ کے ہارڈویئر پر کیا نتائج ملتے ہیں۔

</Tip>

یہاں وہ نتائج ہیں جو ہم نے batching کے ساتھ اور بغیر، fast اور slow tokenizer کے ساتھ حاصل کیے:

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

اس کا مطلب ہے کہ fast tokenizer کو `batched=True` کے آپشن کے ساتھ استعمال کرنا slow tokenizer کے بغیر batching کے مقابلے میں 30 گنا تیز ہے — یہ واقعی حیران کن ہے! یہی وجہ ہے کہ fast tokenizers `AutoTokenizer` کے استعمال میں ڈیفالٹ ہوتے ہیں (اور انہیں "fast" کہا جاتا ہے)۔ وہ یہ رفتار اس لیے حاصل کر پاتے ہیں کیونکہ ان کے اندرونی کوڈ Rust میں چلایا جاتا ہے، جو کہ کوڈ کی parallelization کو آسان بناتا ہے۔

parallelization کی بدولت fast tokenizer batching کے ساتھ تقریباً 6x تیزی حاصل کرتا ہے: آپ ایک واحد tokenization آپریشن کو parallelize نہیں کر سکتے، لیکن جب آپ بیک وقت بہت سے texts کو tokenize کرنا چاہتے ہیں تو آپ execution کو مختلف processes میں تقسیم کر سکتے ہیں، جن میں سے ہر ایک اپنی texts کے لیے ذمہ دار ہوتا ہے۔

`Dataset.map()` میں بھی parallelization کی کچھ صلاحیتیں موجود ہیں۔ چونکہ یہ Rust سے backed نہیں ہے، اس لیے یہ slow tokenizer کو fast tokenizer کے برابر نہیں لا سکتا، مگر یہ اب بھی مفید ثابت ہو سکتا ہے (خاص طور پر اگر آپ ایسے tokenizer استعمال کر رہے ہیں جس کا fast ورژن موجود نہ ہو)۔ multiprocessing فعال کرنے کے لیے `num_proc` آرگومنٹ استعمال کریں اور اپنی `Dataset.map()` کال میں استعمال ہونے والے processes کی تعداد بتائیں:

```python
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

آپ تھوڑا تجربہ کر کے timing سے معلوم کر سکتے ہیں کہ processes کی تعداد کیا ہونی چاہیے؛ ہمارے کیس میں 8 سب سے بہتر نتیجہ دے رہا تھا۔ یہاں وہ اعداد و شمار ہیں جو ہمیں multiprocessing کے ساتھ اور بغیر ملے:

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s
`batched=True`, `num_proc=8`  | 6.52s          | 41.3s
`batched=False`, `num_proc=8` | 9.49s          | 45.2s

یہ slow tokenizer کے لیے کافی معقول نتائج ہیں، لیکن fast tokenizer کی کارکردگی بھی نمایاں طور پر بہتر ہوئی۔ البتہ، یہ ہمیشہ ایسا نہیں ہوگا — 8 کے علاوہ دیگر `num_proc` قدروں کے لیے ہمارے ٹیسٹ نے یہ دکھایا کہ `batched=True` کو بغیر multiprocessing کے استعمال کرنا زیادہ تیز تھا۔ عمومی طور پر، ہم fast tokenizers کے لیے `batched=True` کے ساتھ Python multiprocessing استعمال کرنے کی سفارش نہیں کرتے۔

<Tip>

اپنی processing کی رفتار بڑھانے کے لیے `num_proc` کا استعمال عام طور پر اچھا خیال ہے، بشرطیکہ آپ جو فنکشن استعمال کر رہے ہیں وہ پہلے سے ہی کسی قسم کی multiprocessing نہ کر رہا ہو۔

</Tip>

یہ تمام خصوصیات ایک ہی میتھڈ میں سمٹ کر کافی حیران کن ہیں، مگر ابھی اور بھی بہت کچھ ہے! `Dataset.map()` اور `batched=True` کے ساتھ آپ اپنے ڈیٹاسیٹ میں موجود عناصر کی تعداد کو بھی تبدیل کر سکتے ہیں۔ یہ بہت سی صورتوں میں مفید ہے جہاں آپ ایک مثال سے متعدد تربیتی فیچرز تخلیق کرنا چاہتے ہیں، اور ہمیں یہ کئی NLP کاموں کی preprocessing میں کرنے کی ضرورت ہوگی جو ہم [باب 7](/course/chapter7) میں کریں گے۔

<Tip>

💡 مشین لرننگ میں، ایک _example_ عام طور پر وہ فیچرز کا مجموعہ ہوتا ہے جو ہم ماڈل کو دیتے ہیں۔ کچھ مواقع پر یہ فیچرز `Dataset` کے کالمز کا مجموعہ ہوتے ہیں، مگر بعض مواقع (جیسے یہاں اور سوال جواب کے لیے) ایک ہی مثال سے متعدد فیچرز نکالے جا سکتے ہیں جو ایک ہی کالم میں شامل ہو سکتے ہیں۔

</Tip>

آئیں دیکھتے ہیں کہ یہ کیسے کام کرتا ہے! یہاں ہم اپنی مثالوں کو tokenize کریں گے اور انہیں زیادہ سے زیادہ 128 tokens تک truncate کریں گے، مگر ہم ٹوکنائزر سے یہ چاہیں گے کہ وہ texts کے *تمام* حصے واپس کرے نہ کہ صرف پہلا۔ یہ `return_overflowing_tokens=True` کے ذریعے کیا جا سکتا ہے:

```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

آئیں اسے پورے ڈیٹاسیٹ پر `Dataset.map()` استعمال کرنے سے پہلے ایک مثال پر ٹیسٹ کرتے ہیں:

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python
[128, 49]
```

تو، ہماری پہلی مثال جو ٹریننگ سیٹ میں تھی، دو فیچرز میں تقسیم ہو گئی کیونکہ اسے tokenize کرتے وقت 128 tokens سے زیادہ tokens بن گئے: پہلا 128 tokens کا اور دوسرا 49 tokens کا۔ اب آئیں اسے پورے ڈیٹاسیٹ پر کریں!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

اوہ نہیں! یہ کام نہیں کر رہا! کیوں؟ غلطی کے پیغام کو دیکھ کر ہمیں سراغ ملتا ہے کہ ایک کالم کی لمبائی میں میل نہیں کھا رہی: ایک کالم کی لمبائی 1,463 ہے جبکہ دوسرے کی 1,000 ہے۔ اگر آپ نے `Dataset.map()` کی [دستاویزات](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) دیکھی ہو تو آپ کو یاد ہوگا کہ جس تعداد کی مثالیں فنکشن کو دی جاتی ہیں، وہی تعداد نئے ڈیٹا کی ہونی چاہیے؛ یہاں 1,000 مثالیں 1,463 نئے فیچرز میں تبدیل ہو گئی ہیں، جس سے shape error آ گیا۔

مسئلہ یہ ہے کہ ہم دو مختلف سائز کے ڈیٹاسیٹس کو ملا رہے ہیں: `drug_dataset` کے کالمز میں ایک مخصوص تعداد کی مثالیں (ہمارے error میں 1,000) ہوں گی، مگر `tokenized_dataset` میں زیادہ (error پیغام میں 1,463) کیونکہ ہم لمبے ریویوز کو `return_overflowing_tokens=True` کے ذریعے ایک سے زیادہ مثالوں میں تقسیم کر رہے ہیں۔ یہ ایک `Dataset` کے لیے کام نہیں کرتا، لہٰذا ہمیں یا تو پرانے ڈیٹاسیٹ کے کالمز کو ہٹا دینا ہوگا یا انہیں نئے ڈیٹاسیٹ کے سائز کے برابر کرنا ہوگا۔ ہم پہلے طریقے کو `remove_columns` آرگومنٹ کے ذریعے کر سکتے ہیں:

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

اب یہ بغیر کسی غلطی کے کام کرتا ہے۔ ہم ڈیٹا سیٹ کی لمبائیوں کا موازنہ کرکے تصدیق کر سکتے ہیں کہ ہمارے نئے ڈیٹا سیٹ میں اصل ڈیٹا سیٹ کے مقابلے میں زیادہ عناصر موجود ہیں:

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

ہم نے ذکر کیا تھا کہ ہم غیر مماثل لمبائی کے مسئلے سے اس طرح بھی نمٹ سکتے ہیں کہ پرانے کالمز کو نئے کالمز کے برابر بنا دیں۔ اس کے لیے ہمیں `overflow_to_sample_mapping` فیلڈ کی ضرورت ہوگی، جو کہ `tokenizer` اس وقت واپس کرتا ہے جب ہم `return_overflowing_tokens=True` سیٹ کرتے ہیں۔  

یہ ہمیں ایک نیا فیچر انڈیکس اس نمونے (sample) کے انڈیکس سے منسلک کرنے کی اجازت دیتا ہے جہاں سے یہ نکلا تھا۔ اس کی مدد سے، ہم اپنے اصل ڈیٹا سیٹ میں موجود ہر کلید (key) کو درست سائز کی ویلیوز کی فہرست سے جوڑ سکتے ہیں، یعنی ہر مثال کی قدریں اتنی بار دہرائی جاتی ہیں جتنی بار یہ نئے فیچرز تخلیق کرتی ہے:


```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extract mapping between new and old indices
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

ہم دیکھ سکتے ہیں کہ یہ `Dataset.map()` کے ساتھ بغیر پرانے کالمز کو ہٹائے کام کرتا ہے:


```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

ہمیں وہی تعداد میں ٹریننگ فیچرز ملتی ہیں جیسا کہ پہلے، لیکن یہاں ہم نے تمام پرانے فیلڈز کو برقرار رکھا ہے۔ اگر آپ کو اپنے ماڈل کو لاگو کرنے کے بعد کسی پوسٹ پروسیسنگ کے لیے ان فیلڈز کی ضرورت ہو، تو آپ اس طریقے کو استعمال کرنا چاہیں گے۔  

اب آپ نے دیکھ لیا کہ 🤗 Datasets کو مختلف طریقوں سے ڈیٹا سیٹ کی پری پروسیسنگ کے لیے کیسے استعمال کیا جا سکتا ہے۔ اگرچہ 🤗 Datasets کی پروسیسنگ فنکشنز زیادہ تر ماڈل ٹریننگ کی ضروریات کو پورا کر لیتی ہیں، لیکن کچھ مواقع پر آپ کو مزید طاقتور فیچرز تک رسائی کے لیے Pandas کی طرف جانا پڑ سکتا ہے، جیسے `DataFrame.groupby()` یا ڈیٹا کی بصری نمائندگی کے لیے ہائی لیول APIs۔  

خوش قسمتی سے، 🤗 Datasets کو Pandas، NumPy، PyTorch، TensorFlow، اور JAX جیسی لائبریریوں کے ساتھ کام کرنے کے لیے ڈیزائن کیا گیا ہے۔ آئیے دیکھتے ہیں کہ یہ کیسے کام کرتا ہے۔  

## `Dataset` سے `DataFrame` اور واپس

<Youtube id="tfcY1067A5Q"/>

مختلف تھرڈ پارٹی لائبریریوں کے درمیان کنورژن کو ممکن بنانے کے لیے، 🤗 Datasets ایک `Dataset.set_format()` فنکشن فراہم کرتا ہے۔ یہ فنکشن صرف ڈیٹا سیٹ کا **آؤٹ پٹ فارمیٹ** تبدیل کرتا ہے، لہذا آپ آسانی سے کسی دوسرے فارمیٹ پر سوئچ کر سکتے ہیں بغیر بنیادی **ڈیٹا فارمیٹ** (جو کہ Apache Arrow ہے) کو متاثر کیے۔  

یہ فارمیٹنگ **اصل ڈیٹا سیٹ پر براہ راست** لاگو ہوتی ہے۔ آئیے ایک مثال کے ذریعے دیکھتے ہیں کہ ہم اپنے ڈیٹا سیٹ کو Pandas میں کیسے تبدیل کر سکتے ہیں:


```py
drug_dataset.set_format("pandas")
```

اب جب ہم ڈیٹا سیٹ کے عناصر تک رسائی حاصل کرتے ہیں، تو ہمیں لغت (dictionary) کی بجائے ایک `pandas.DataFrame` ملتا ہے:

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

آئیں پورے ٹریننگ سیٹ کا `pandas.DataFrame` بناتے ہیں:

```py
train_df = drug_dataset["train"][:]
```

<Tip>

🚨 دراصل، `Dataset.set_format()` ڈیٹاسیٹ کے `__getitem__()` dunder method کے output format کو تبدیل کرتا ہے۔ اس کا مطلب ہے کہ جب ہم `drug_dataset["train"]` کو `"pandas"` فارمیٹ میں استعمال کرتے ہیں، تب بھی اس کا اصل ٹائپ `Dataset` ہی رہتا ہے۔ لیکن slicing کرنے پر ایک `pandas.DataFrame` حاصل ہوتا ہے۔

</Tip>

اب ہم Pandas کی تمام خصوصیات استعمال کر سکتے ہیں۔ مثال کے طور پر، ہم chaining استعمال کر کے `condition` entries میں class distribution معلوم کر سکتے ہیں:

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>

اور جب آپ اپنی Pandas تجزیہ کاری مکمل کر لیں، آپ `Dataset.from_pandas()` فنکشن کا استعمال کر کے ایک نیا `Dataset` آبجیکٹ بھی بنا سکتے ہیں:

```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<Tip>

✏️ **آزمائیں!** ہر دوا کی اوسط ریٹنگ معلوم کریں اور نتیجہ کو ایک نئے `Dataset` میں اسٹور کریں۔

</Tip>

یہ ہماری 🤗 Datasets کے ذریعے ڈیٹا پری پراسیسنگ کی مختلف تکنیکوں کا جائزہ ختم کرتا ہے۔ اس سیکشن کو ختم کرنے کے لیے، آئیں ایک validation set تخلیق کرتے ہیں تاکہ classifier کی تربیت کے لیے ڈیٹاسیٹ تیار ہو جائے۔ اس سے پہلے کہ ہم ایسا کریں، ہم اپنے `drug_dataset` کا output format `"pandas"` سے `"arrow"` میں واپس تبدیل کر دیتے ہیں:

```py
drug_dataset.reset_format()
```

## ایک validation set تخلیق کرنا[[creating-a-validation-set]]

اگرچہ ہمارے پاس جانچ (test) set موجود ہے، بہتر یہ ہے کہ اسے بغیر چھوئے ایک علیحدہ validation set بنایا جائے تاکہ ترقی کے دوران ماڈل کو جانچا جا سکے۔ جب آپ validation set پر ماڈل کی کارکردگی سے مطمئن ہو جائیں، تب آپ test set پر حتمی جانچ کر سکتے ہیں۔ اس عمل سے یہ یقینی بنتا ہے کہ آپ test set پر overfit نہ ہو جائیں اور حقیقی دنیا کے ڈیٹا پر ماڈل اچھی کارکردگی دکھائے۔

🤗 Datasets `Dataset.train_test_split()` فنکشن فراہم کرتا ہے جو scikit-learn کی مشہور فعالیت پر مبنی ہے۔ آئیں اسے استعمال کر کے اپنے training set کو `train` اور `validation` سپلٹس میں تقسیم کرتے ہیں (reproducibility کے لیے seed سیٹ کریں):

```py
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# ڈیفالٹ "test" سپلٹ کا نام تبدیل کر کے "validation" کر دیں
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# "test" سیٹ کو ہمارے DatasetDict میں شامل کریں
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

زبردست، اب ہمارے پاس ایک ایسا ڈیٹاسیٹ تیار ہے جو ماڈلز کی تربیت کے لیے استعمال کیا جا سکتا ہے! [باب 5](/course/chapter5/5) میں ہم دکھائیں گے کہ کس طرح ڈیٹاسیٹس کو Hugging Face Hub پر اپلوڈ کیا جا سکتا ہے، مگر ابھی ہم اپنے تجزیے کو ختم کرتے ہوئے یہ دیکھیں گے کہ آپ اپنے مقامی مشین پر ڈیٹاسیٹس کو کس طرح محفوظ کر سکتے ہیں۔

## ایک ڈیٹاسیٹ محفوظ کرنا[[saving-a-dataset]]

<Youtube id="blF9uxYcKHo"/>

اگرچہ 🤗 Datasets ہر ڈاؤن لوڈ شدہ ڈیٹاسیٹ اور اس پر کی جانے والی تمام operations کو cache کر دیتا ہے، بعض اوقات آپ کو ڈیٹاسیٹ کو ڈسک پر محفوظ کرنے کی ضرورت پیش آ سکتی ہے (مثلاً، اگر cache ڈیلیٹ ہو جائے)۔ جیسا کہ نیچے دی گئی ٹیبل میں دکھایا گیا ہے، 🤗 Datasets تین اہم فنکشنز فراہم کرتا ہے تاکہ آپ اپنے ڈیٹاسیٹ کو مختلف فارمیٹس میں محفوظ کر سکیں:

|  ڈیٹا فارمیٹ  |        فنکشن         |
| :------------: | :-------------------: |
|    Arrow     | `Dataset.save_to_disk()` |
|     CSV      |    `Dataset.to_csv()`    |
|    JSON      |   `Dataset.to_json()`    |

مثال کے طور پر، آئیں اپنے صاف شدہ ڈیٹاسیٹ کو Arrow فارمیٹ میں محفوظ کرتے ہیں:

```py
drug_dataset_clean.save_to_disk("drug-reviews")
```

یہ ایک directory تخلیق کرے گا جس کی ساخت کچھ اس طرح ہوگی:

```
drug-reviews/
├── dataset_dict.json
├── test
│   ├── dataset.arrow
│   ├── dataset_info.json
│   └── state.json
├── train
│   ├── dataset.arrow
│   ├── dataset_info.json
│   ├── indices.arrow
│   └── state.json
└── validation
    ├── dataset.arrow
    ├── dataset_info.json
    ├── indices.arrow
    └── state.json
```

جہاں ہر سپلٹ اپنے متعلقہ *dataset.arrow* ٹیبل اور کچھ metadata (*dataset_info.json* اور *state.json*) کے ساتھ محفوظ ہے۔ آپ Arrow فارمیٹ کو ایک جدید ٹیبل سمجھ سکتے ہیں جو بڑی ڈیٹاسیٹس کی تیز پراسیسنگ اور ٹرانسپورٹ کے لیے موزوں ہے۔

ایک بار ڈیٹاسیٹ محفوظ ہوجائے، آپ `load_from_disk()` فنکشن کا استعمال کر کے اسے دوبارہ لوڈ کر سکتے ہیں:

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

CSV اور JSON فارمیٹس کے لیے، ہر سپلٹ کو ایک علیحدہ فائل کے طور پر محفوظ کرنا پڑتا ہے۔ ایسا کرنے کے لیے آپ `DatasetDict` پر iterate کر سکتے ہیں:

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

یہ ہر سپلٹ کو [JSON Lines format](https://jsonlines.org) میں محفوظ کرتا ہے، جہاں ہر row ایک JSON لائن میں اسٹور ہوتا ہے۔ مثال کے طور پر، پہلی لائن یوں دکھ سکتی ہے:

```py
!head -n 1 drug-reviews-train.jsonl
```

```python
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

آپ پھر [باب 2](/course/chapter5/2) کی تکنیکوں کا استعمال کر کے JSON فائلوں کو لوڈ کر سکتے ہیں:

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

یہ ہماری ڈیٹا وارنگ کا سفر ختم کرتا ہے۔ اب جب کہ ہمارے پاس ایک صاف ڈیٹاسیٹ ہے جس پر ماڈل کی تربیت کی جا سکتی ہے، یہاں چند تجاویز ہیں جنہیں آپ آزما سکتے ہیں:

1. [باب 3](/course/chapter3) کی تکنیکوں کا استعمال کرتے ہوئے ایک classifier تربیت کریں جو drug review کی بنیاد پر مریض کی حالت کی پیش گوئی کر سکے۔
2. [باب 1](/course/chapter1) سے summarization pipeline کا استعمال کرتے ہوئے ریویوز کا خلاصہ تیار کریں۔

آخر میں، ہم دیکھیں گے کہ کس طرح 🤗 Datasets آپ کو بہت بڑے ڈیٹاسیٹس کے ساتھ کام کرنے کی اجازت دیتا ہے بغیر آپ کے لیپ ٹاپ کی میموری بھرنے کے۔

