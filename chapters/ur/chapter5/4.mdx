# Ø¨Ú‘Ø§ ÚˆÛŒÙ¹Ø§ØŸ ğŸ¤— Datasets Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Û’ Ù„ÛŒÛ’ Ø­Ø§Ø¶Ø± ÛÛŒÚº![[big-data-datasets-to-the-rescue]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
]} />

Ø¢Ø¬ Ú©Ù„ Ø§ÛŒØ³Ø§ Ú©ÙˆØ¦ÛŒ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨Ø§Øª Ù†ÛÛŒÚº Ø±ÛÛŒ Ú©Û Ø¢Ù¾ Ú©Ùˆ Ù…Ù„Ù¹ÛŒ Ú¯ÛŒÚ¯Ø§Ø¨Ø§Ø¦Ù¹ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ú©Ø±Ù†Ø§ Ù¾Ú‘Û’ØŒ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø§Ú¯Ø± Ø¢Ù¾ BERT ÛŒØ§ GPT-2 Ø¬ÛŒØ³Û’ transformer Ú©Ùˆ scratch Ø³Û’ pretrain Ú©Ø±Ù†Û’ Ú©Ø§ Ø§Ø±Ø§Ø¯Û Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºÛ” Ø§Ù† ØµÙˆØ±ØªÙˆÚº Ù…ÛŒÚºØŒ ÛŒÛØ§Úº ØªÚ© Ú©Û ÚˆÛŒÙ¹Ø§ _Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§_ Ø¨Ú¾ÛŒ Ø§ÛŒÚ© Ú†ÛŒÙ„Ù†Ø¬ Ø¨Ù† Ø³Ú©ØªØ§ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ GPT-2 Ú©ÛŒ pretraining Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„Ø§ WebText corpus 8 Ù…Ù„ÛŒÙ† Ø³Û’ Ø²Ø§Ø¦Ø¯ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ø§ÙˆØ± 40 GB Ù…ØªÙ† Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÛ’ â€” Ø§Ø³Û’ Ø§Ù¾Ù†Û’ Ù„ÛŒÙ¾ Ù¹Ø§Ù¾ Ú©ÛŒ RAM Ù…ÛŒÚº Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§ Ø§Ø³ Ú©ÛŒ RAM Ú©Ùˆ Ø¯Ú¾Ú†Ú©Û Ø¯Û’ Ø³Ú©ØªØ§ ÛÛ’!

Ø®ÙˆØ´ Ù‚Ø³Ù…ØªÛŒ Ø³Û’ØŒ ğŸ¤— Datasets Ú©Ùˆ Ø§Ù† Ù¾Ø§Ø¨Ù†Ø¯ÛŒÙˆÚº Ú©Ùˆ Ø¯ÙˆØ± Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” ÛŒÛ Ø¢Ù¾ Ú©Ùˆ memory management Ú©Û’ Ù…Ø³Ø§Ø¦Ù„ Ø³Û’ Ø¢Ø²Ø§Ø¯ Ú©Ø±ØªØ§ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Ùˆ _memory-mapped_ ÙØ§Ø¦Ù„ÙˆÚº Ú©ÛŒ Ø·Ø±Ø­ Ù¹Ø±ÛŒÙ¹ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± hard drive Ú©ÛŒ Ø­Ø¯ÙˆØ¯ Ø³Û’ _streaming_ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Û’ entries ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û”

<Youtube id="JwISwTCPPWo"/>

Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº ÛÙ… ğŸ¤— Datasets Ú©ÛŒ Ø§Ù† Ø®ØµÙˆØµÛŒØ§Øª Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÛŒÚ© Ø¨ÛØª Ø¨Ú‘Û’ 825 GB corpusØŒ Ø¬Ø³Û’ [the Pile](https://pile.eleuther.ai) Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ú©Û’ Ø³Ø§ØªÚ¾Û” Ú†Ù„ÛŒÚº Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÛŒÚº!

## The Pile Ú©ÛŒØ§ ÛÛ’ØŸ[[what-is-the-pile]]

The Pile Ø§ÛŒÚ© Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ØªÙ† Ú©Ø§ corpus ÛÛ’ Ø¬Ùˆ [EleutherAI](https://www.eleuther.ai) Ù†Û’ Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ø²Ø¨Ø§Ù† Ú©Û’ Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ú©Û’ Ù„ÛŒÛ’ Ø¨Ù†Ø§ÛŒØ§ ÛÛ’Û” Ø§Ø³ Ù…ÛŒÚº Ù…Ø®ØªÙ„Ù ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ø´Ø§Ù…Ù„ ÛÛŒÚºØŒ Ø¬Ù† Ù…ÛŒÚº Ø³Ø§Ø¦Ù†Ø³ÛŒ Ù…Ø¶Ø§Ù…ÛŒÙ†ØŒ GitHub Ú©ÙˆÚˆ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒØ²ØŒ Ø§ÙˆØ± ÙÙ„Ù¹Ø±Úˆ ÙˆÛŒØ¨ Ù¹ÛŒÚ©Ø³Ù¹ Ø´Ø§Ù…Ù„ ÛÛ’Û” ØªØ±Ø¨ÛŒØªÛŒ corpus [14 GB chunks](https://the-eye.eu/public/AI/pile/) Ù…ÛŒÚº Ø¯Ø³ØªÛŒØ§Ø¨ ÛÛ’ØŒ Ø§ÙˆØ± Ø¢Ù¾ [Ø§Ù†ÙØ±Ø§Ø¯ÛŒ Ø§Ø¬Ø²Ø§Ø¡](https://the-eye.eu/public/AI/pile_preliminary_components/) Ú©Ùˆ Ø¨Ú¾ÛŒ ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø¢Ø¦ÛŒÛ’ PubMed Abstracts dataset Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ [PubMed](https://pubmed.ncbi.nlm.nih.gov/) Ù¾Ø± 15 Ù…Ù„ÛŒÙ† Ø¨Ø§ÛŒÙˆÙ…ÛŒÚˆÛŒÚ©Ù„ Ø§Ø´Ø§Ø¹ØªÙˆÚº Ú©Û’ abstracts Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÛ’Û” ÛŒÛ dataset [JSON Lines format](https://jsonlines.org) Ù…ÛŒÚº ÛÛ’ Ø§ÙˆØ± `zstandard` Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©Ù…Ù¾Ø±ÛŒØ³ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº Ø§Ø³Û’ Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§:

```py
!pip install zstandard
```

Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ØŒ ÛÙ… [section 2](/course/chapter5/2) Ù…ÛŒÚº Ø³ÛŒÚ©Ú¾ÛŒ Ú¯Ø¦ÛŒ Ø¯ÙˆØ± Ø¯Ø±Ø§Ø² ÙØ§Ø¦Ù„ÙˆÚº Ú©Ùˆ Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©ÛŒ ØªÚ©Ù†ÛŒÚ© Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ dataset Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
from datasets import load_dataset

# ÛŒÛ Ú©Ù…Ø§Ù†Úˆ Ú†Ù„Ø§Ù†Û’ Ù…ÛŒÚº Ú†Ù†Ø¯ Ù…Ù†Ù¹ Ù„Ú¯ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ú†Ø§Ø¦Û’ ÛŒØ§ Ú©Ø§ÙÛŒ Ù„Û’ Ú©Ø± Ø¨ÛŒÙ¹Ú¾ Ø¬Ø§Ø¦ÛŒÚº :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ…Ø§Ø±Û’ dataset Ù…ÛŒÚº 15,518,009 rows Ø§ÙˆØ± 2 columns Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚº â€” Ø¨ÛØª Ø³Ø§ ÚˆÛŒÙ¹Ø§!

<Tip>

âœ ÚˆÛŒÙØ§Ù„Ù¹ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ğŸ¤— Datasets ÙØ§Ø¦Ù„ÙˆÚº Ú©Ùˆ decompress Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’ Ø¬Ø¨ dataset Ù„ÙˆÚˆ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ hard drive space Ø¨Ú†Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ `download_config` Ø¢Ø±Ú¯ÙˆÙ…Ù†Ù¹ Ù…ÛŒÚº `DownloadConfig(delete_extracted=True)` Ù¾Ø§Ø³ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ù…Ø²ÛŒØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Û’ Ù„ÛŒÛ’ [Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig) Ø¯ÛŒÚ©Ú¾ÛŒÚºÛ”

</Tip>

Ø¢Ø¦ÛŒÚº Ù¾ÛÙ„Û’ example Ú©Û’ Ù…Ù†Ø¯Ø±Ø¬Ø§Øª Ú©Ø§ Ù…Ø¹Ø§Ø¦Ù†Û Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
pubmed_dataset[0]
```

```python
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

ÛŒÛ Ø§ÛŒÚ© Ù…ÛŒÚˆÛŒÚ©Ù„ Ø¢Ø±Ù¹ÛŒÚ©Ù„ Ú©Ø§ abstract Ù„Ú¯ØªØ§ ÛÛ’Û” Ø§Ø¨ Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û dataset Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ ÛÙ…Ø§Ø±ÛŒ RAM Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ØªÙ†Ø§ ÛÙˆØ§ ÛÛ’!

## Memory Mapping Ú©Ø§ Ø¬Ø§Ø¯Ùˆ[[the-magic-of-memory-mapping]]

Python Ù…ÛŒÚº memory usage Ù…Ø§Ù¾Ù†Û’ Ú©Ø§ Ø§ÛŒÚ© Ø¢Ø³Ø§Ù† Ø·Ø±ÛŒÙ‚Û [`psutil`](https://psutil.readthedocs.io/en/latest/) Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÛ’ØŒ Ø¬Ø³Û’ `pip` Ø³Û’ Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’:

```py
!pip install psutil
```

ÛŒÛ Ø§ÛŒÚ© `Process` Ú©Ù„Ø§Ø³ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ ÛÙ…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯Û process Ú©Û’ memory usage Ú©Ùˆ Ø¬Ø§Ù†Ú†Ù†Û’ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªØ§ ÛÛ’:

```py
import psutil

# Process.memory_info Ø¨Ø§Ø¦Ù¹Ø³ Ù…ÛŒÚº ÛÙˆØªØ§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø§Ø³Û’ Ù…ÛŒÚ¯Ø§ Ø¨Ø§Ø¦Ù¹Ø³ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python
RAM used: 5678.33 MB
```

ÛŒÛØ§Úº `rss` attribute _resident set size_ Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø±ØªØ§ ÛÛ’ØŒ ÛŒØ¹Ù†ÛŒ ÙˆÛ Ø­ØµÛ Ø¬Ùˆ process RAM Ù…ÛŒÚº ÙˆØ§Ù‚Ø¹ ÛÛ’Û” Ø§Ø³ measurement Ù…ÛŒÚº Python interpreter Ø§ÙˆØ± Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒØ² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ ÛÙˆØªØ§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø§ØµÙ„ dataset Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„ÛŒ RAM ØªÚ¾ÙˆÚ‘ÛŒ Ú©Ù… ÛÙˆ Ø³Ú©ØªÛŒ ÛÛ’Û” Ù…ÙˆØ§Ø²Ù†Û Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û dataset on disk Ú©ØªÙ†Ø§ Ø¨Ú‘Ø§ ÛÛ’ØŒ `dataset_size` attribute Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’Û” Ú†ÙˆÙ†Ú©Û Ù†ØªÛŒØ¬Û Ø¨Ø§Ø¦Ù¹Ø³ Ù…ÛŒÚº Ø¸Ø§ÛØ± ÛÙˆØªØ§ ÛÛ’ØŒ Ø§Ø³Û’ manually Ú¯ÛŒÚ¯Ø§Ø¨Ø§Ø¦Ù¹Ø³ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§:

```py
print(f"Dataset size in bytes: {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python
Dataset size in bytes : 20979437051
Dataset size (cache file) : 19.54 GB
```

Ø¨ÛØª Ø®ÙˆØ¨ â€” ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 20 GB Ø¨Ú‘Û’ dataset Ú©Û’ Ø¨Ø§ÙˆØ¬ÙˆØ¯ ÛÙ… Ø§Ø³Û’ Ø¨ÛØª Ú©Ù… RAM Ú©Û’ Ø³Ø§ØªÚ¾ Ù„ÙˆÚˆ Ú©Ø± Ù¾Ø§Ø¦Û’ ÛÛŒÚº!

<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Pile Ú©Û’ [subsets](https://the-eye.eu/public/AI/pile_preliminary_components/) Ù…ÛŒÚº Ø³Û’ Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ Ú†Ù†ÛŒÚº Ø¬Ùˆ Ø¢Ù¾ Ú©Û’ Ù„ÛŒÙ¾ Ù¹Ø§Ù¾ ÛŒØ§ ÚˆÛŒØ³Ú© Ù¹Ø§Ù¾ Ú©ÛŒ RAM Ø³Û’ Ø¨Ú‘Ø§ ÛÙˆØŒ Ø§Ø³Û’ ğŸ¤— Datasets Ú©Û’ Ø³Ø§ØªÚ¾ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚºØŒ Ø§ÙˆØ± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø´Ø¯Û RAM Ú©ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ù…Ø§Ù¾ÛŒÚºÛ” Ø¯Ø±Ø³Øª measurement Ú©Û’ Ù„ÛŒÛ’ØŒ Ø§ÛŒÚ© Ù†Ø¦Û’ process Ù…ÛŒÚº ÛŒÛ Ø¢Ø²Ù…Ø§Ø¦ÛŒÚºÛ” Ø¢Ù¾ [the Pile paper](https://arxiv.org/abs/2101.00027) Ú©ÛŒ Table 1 Ù…ÛŒÚº ÛØ± subset Ú©Ø§ decompressed size Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

</Tip>

Ø§Ú¯Ø± Ø¢Ù¾ Pandas Ø³Û’ ÙˆØ§Ù‚Ù ÛÛŒÚº ØªÙˆ ÛŒÛ Ù†ØªÛŒØ¬Û Ø­ÛŒØ±Ø§Ù† Ú©Ù† Ù„Ú¯ Ø³Ú©ØªØ§ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û Wes Kinney Ú©Ø§ Ù…Ø´ÛÙˆØ± [rule of thumb](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) Ú©Û Ø¢Ù¾ Ú©Ùˆ Ø¹Ù…ÙˆÙ…Ø§Ù‹ Ø§Ù¾Ù†Û’ dataset Ú©Û’ size Ú©Ø§ 5 Ø³Û’ 10 Ú¯Ù†Ø§ RAM Ø¯Ø±Ú©Ø§Ø± ÛÙˆØªØ§ ÛÛ’Û” ØªÙˆ ğŸ¤— Datasets Ù†Û’ ÛŒÛ memory management Ú©Ø§ Ù…Ø³Ø¦Ù„Û Ú©Ø³ Ø·Ø±Ø­ Ø­Ù„ Ú©ÛŒØ§ØŸ ğŸ¤— Datasets ÛØ± dataset Ú©Ùˆ [memory-mapped file](https://en.wikipedia.org/wiki/Memory-mapped_file) Ú©Û’ Ø·ÙˆØ± Ù¾Ø± treat Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ RAM Ø§ÙˆØ± filesystem storage Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† mapping ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± library Ú©Ùˆ dataset Ú©Û’ elements ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ø¯ÛŒØªØ§ ÛÛ’ Ø¨ØºÛŒØ± Ø§Ø³Û’ Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± memory Ù…ÛŒÚº load Ú©ÛŒÛ’Û”

Memory-mapped files Ú©Ùˆ Ù…ØªØ¹Ø¯Ø¯ processes Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† share Ø¨Ú¾ÛŒ Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ø³Û’ `Dataset.map()` Ø¬ÛŒØ³Û’ ÙÙ†Ú©Ø´Ù†Ø² parallelize ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¨ØºÛŒØ± dataset Ú©Ùˆ copy Ú©ÛŒÛ’Û” Ø§Ù† ØªÙ…Ø§Ù… ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ù¾ÛŒÚ†Ú¾Û’ [Apache Arrow](https://arrow.apache.org) memory format Ø§ÙˆØ± [`pyarrow`](https://arrow.apache.org/docs/python/index.html) Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ ÛÛŒÚºØŒ Ø¬Ùˆ data loading Ø§ÙˆØ± processing Ú©Ùˆ Ø¨ÛØª ØªÛŒØ² Ø¨Ù†Ø§ Ø¯ÛŒØªÛŒ ÛÛŒÚºÛ” (Ù…Ø²ÛŒØ¯ ØªÙØµÛŒÙ„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ [Dejan Simic Ú©ÛŒ Ø¨Ù„Ø§Ú¯ Ù¾ÙˆØ³Ù¹](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a) Ù…Ù„Ø§Ø­Ø¸Û Ú©Ø±ÛŒÚºÛ”) Ø§Ø³Û’ Ø¹Ù…Ù„ÛŒ Ø¬Ø§Ù…Û Ù¾ÛÙ†Ø§ØªÛ’ ÛÙˆØ¦Û’ØŒ Ø¢Ø¦ÛŒÚº Ø§ÛŒÚ© speed test Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ… PubMed Abstracts dataset Ù¾Ø± Ú©ØªÙ†ÛŒ ØªÛŒØ²ÛŒ Ø³Û’ iterate Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

ÛŒÛØ§Úº ÛÙ… Ù†Û’ Python Ú©Û’ `timeit` module Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ `code_snippet` Ú©ÛŒ execution time Ù†Ø§Ù¾ÛŒÛ” Ø¹Ù…ÙˆÙ…Ø§Ù‹ Ø¢Ù¾ dataset Ú©Ùˆ iterate Ú©Ø±Ù†Û’ Ú©ÛŒ Ø±ÙØªØ§Ø± Ú©Ú†Ú¾ Ø¯Ø³ÙˆÛŒÚº GB/s Ø³Û’ Ú©Ø¦ÛŒ GB/s ØªÚ© ÛÙˆØªÛŒ ÛÛ’Û” ÛŒÛ Ø²ÛŒØ§Ø¯Û ØªØ± applications Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ±ÛŒÙ† ÛÛ’ØŒ Ù…Ú¯Ø± Ú©Ø¨Ú¾ÛŒ Ú©Ø¨Ú¾Ø§Ø± Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒØ³Ø§ dataset Ù…Ù„ Ø³Ú©ØªØ§ ÛÛ’ Ø¬Ùˆ Ø¢Ù¾ Ú©Û’ Ù„ÛŒÙ¾ Ù¹Ø§Ù¾ Ú©Û’ hard drive Ù¾Ø± Ø¨Ú¾ÛŒ store Ù†Û ÛÙˆ Ø³Ú©Û’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± ÛÙ… Ù¾ÙˆØ±ÛŒ Pile dataset ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº ØªÙˆ 825 GB free disk space Ú†Ø§ÛÛŒÛ’ ÛÙˆÚ¯Ø§! Ø§Ù† ØµÙˆØ±ØªÙˆÚº Ø³Û’ Ù†Ù…Ù¹Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ğŸ¤— Datasets Ø§ÛŒÚ© streaming feature ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ø¢Ù¾ Ú©Ùˆ on-the-fly elements download Ø§ÙˆØ± access Ú©Ø±Ù†Û’ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªØ§ ÛÛ’ Ø¨ØºÛŒØ± Ù¾ÙˆØ±ÛŒ dataset download Ú©ÛŒÛ’Û”

<Tip>

ğŸ’¡ Jupyter Ù†ÙˆÙ¹ Ø¨ÙÚ©Ø³ Ù…ÛŒÚº Ø¢Ù¾ [`%%timeit` magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) Ú©Ø§ Ø¨Ú¾ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ cells Ú©Ø§ ÙˆÙ‚Øª Ù†Ø§Ù¾ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

</Tip>

## Streaming datasets[[streaming-datasets]]

Dataset streaming Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù¾ Ú©Ùˆ ØµØ±Ù `load_dataset()` ÙÙ†Ú©Ø´Ù† Ù…ÛŒÚº `streaming=True` Ø¢Ø±Ú¯ÙˆÙ…Ù†Ù¹ Ù¾Ø§Ø³ Ú©Ø±Ù†Ø§ ÛÙˆØªØ§ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø¢Ø¦ÛŒÚº PubMed Abstracts dataset Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ù„ÙˆÚˆ Ú©Ø±ØªÛ’ ÛÛŒÚº Ù„ÛŒÚ©Ù† Ø§Ø³ Ø¨Ø§Ø± streaming mode Ù…ÛŒÚº:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Ø§Ø¨ ÙˆÛ familiar `Dataset` Ú©ÛŒ Ø¨Ø¬Ø§Ø¦Û’ Ø§ÛŒÚ© `IterableDataset` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ ÙˆØ§Ù¾Ø³ ÛÙˆØªØ§ ÛÛ’Û” Ø¬ÛŒØ³Ø§ Ú©Û Ù†Ø§Ù… Ø³Û’ Ø¸Ø§ÛØ± ÛÛ’ØŒ `IterableDataset` Ú©Û’ elements ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº iterate Ú©Ø±Ù†Ø§ Ù¾Ú‘ØªØ§ ÛÛ’Û” ÛÙ… Ø§Ù¾Ù†Û’ streamed dataset Ú©Ø§ Ù¾ÛÙ„Ø§ element ÛŒÙˆÚº Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
next(iter(pubmed_dataset_streamed))
```

```python
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Streamed dataset Ú©Û’ elements Ú©Ùˆ on the fly process Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’ `IterableDataset.map()` Ú©Û’ Ø°Ø±ÛŒØ¹Û’ØŒ Ø¬Ùˆ training Ú©Û’ Ø¯ÙˆØ±Ø§Ù† inputs tokenize Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…ÙÛŒØ¯ ÛÛ’Û” Ø¹Ù…Ù„ Ø¨Ø§Ù„Ú©Ù„ ÙˆÛÛŒ ÛÛ’ Ø¬Ùˆ ÛÙ… Ù†Û’ [Ø¨Ø§Ø¨ 3](/course/chapter3) Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ØŒ ØµØ±Ù ÙØ±Ù‚ ÛŒÛ ÛÛ’ Ú©Û outputs Ø§ÛŒÚ© Ø§ÛŒÚ© Ú©Ø± Ú©Û’ ÙˆØ§Ù¾Ø³ ÛÙˆØªÛ’ ÛÛŒÚº:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<Tip>

ğŸ’¡ Streaming Ú©Û’ Ø³Ø§ØªÚ¾ tokenization Ú©Ùˆ ØªÛŒØ² Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù¾ `batched=True` Ù¾Ø§Ø³ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ù¾Ú†Ú¾Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§Û” ÛŒÛ examples Ú©Ùˆ batch-by-batch process Ú©Ø±ØªØ§ ÛÛ’Ø› ÚˆÛŒÙØ§Ù„Ù¹ batch size 1,000 ÛÛ’ Ø§ÙˆØ± `batch_size` Ø¢Ø±Ú¯ÙˆÙ…Ù†Ù¹ Ø³Û’ ØªØ¨Ø¯ÛŒÙ„ Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’Û”

</Tip>

Ø¢Ù¾ `IterableDataset.shuffle()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ streamed dataset Ú©Ùˆ Ø¨Ú¾ÛŒ shuffle Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù…Ú¯Ø± `Dataset.shuffle()` Ú©Û’ Ø¨Ø±Ø¹Ú©Ø³ ÛŒÛ ØµØ±Ù Ø§ÛŒÚ© Ù…Ù‚Ø±Ø±Û `buffer_size` Ú©Û’ Ø§Ù†Ø¯Ø± Ø¹Ù†Ø§ØµØ± Ú©Ùˆ shuffle Ú©Ø±ØªØ§ ÛÛ’:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

Ø§Ø³ Ù…Ø«Ø§Ù„ Ù…ÛŒÚºØŒ ÛÙ… Ù†Û’ buffer Ú©Û’ Ù¾ÛÙ„Û’ 10,000 examples Ù…ÛŒÚº Ø³Û’ Ø§ÛŒÚ© random example Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§Û” Ø§ÛŒÚ© Ø¨Ø§Ø± Ø¬Ø¨ Ú©ÙˆØ¦ÛŒ example access ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’ ØªÙˆ Ø§Ø³ Ú©ÛŒ Ø¬Ú¯Û buffer Ù…ÛŒÚº Ø§Ú¯Ù„Ø§ example Ø¢ Ø¬Ø§ØªØ§ ÛÛ’ (ÛŒØ¹Ù†ÛŒØŒ 10,001 ÙˆØ§Úº example)Û” Ø¢Ù¾ `IterableDataset.take()` Ø§ÙˆØ± `IterableDataset.skip()` ÙÙ†Ú©Ø´Ù†Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø¨Ú¾ÛŒ streamed dataset Ø³Û’ elements Ù…Ù†ØªØ®Ø¨ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ú©Û `Dataset.select()` Ú©Û’ Ø¬ÛŒØ³Û’ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ PubMed Abstracts dataset Ú©Û’ Ù¾ÛÙ„Û’ 5 examples Ù…Ù†ØªØ®Ø¨ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

Ø§Ø³ÛŒ Ø·Ø±Ø­ØŒ `IterableDataset.skip()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ø¢Ù¾ Ø§ÛŒÚ© shuffled dataset Ø³Û’ training Ø§ÙˆØ± validation splits ØªØ®Ù„ÛŒÙ‚ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
# Ù¾ÛÙ„Û’ 1,000 examples Ú†Ú¾ÙˆÚ‘ Ø¯ÛŒÚº Ø§ÙˆØ± Ø¨Ø§Ù‚ÛŒ Ú©Ùˆ training set Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
train_dataset = shuffled_dataset.skip(1000)
# Ù¾ÛÙ„Û’ 1,000 examples Ú©Ùˆ validation set Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº
validation_dataset = shuffled_dataset.take(1000)
```

Ø¢Ø¦ÛŒÚº Ø§Ø¨ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û Ú©Ø³ Ø·Ø±Ø­ Ù…ØªØ¹Ø¯Ø¯ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Ùˆ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© corpus Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’Û” ğŸ¤— Datasets `interleave_datasets()` ÙÙ†Ú©Ø´Ù† ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ú©Û Ø§ÛŒÚ© list of `IterableDataset` objects Ú©Ùˆ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ `IterableDataset` Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº Ù†Ø¦Û’ dataset Ú©Û’ elements source examples Ù…ÛŒÚº alternation Ø³Û’ Ø­Ø§ØµÙ„ ÛÙˆØªÛ’ ÛÛŒÚºÛ” ÛŒÛ ÙÙ†Ú©Ø´Ù† Ø¨ÛØª Ø¨Ú‘Û’ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Ùˆ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© corpus Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ù…ÙÛŒØ¯ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø¢Ø¦ÛŒÚº FreeLaw subset of the Pile Ú©Ùˆ stream Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ú©Û 51 GB Ú©Ø§ Ø§ÛŒÚ© dataset ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº US Ø¹Ø¯Ø§Ù„ØªÙˆÚº Ú©Û’ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø±Ø§Ø¦Û’ Ø´Ø§Ù…Ù„ ÛÛŒÚº:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

ÛŒÛ dataset Ø§ØªÙ†Ø§ Ø¨Ú‘Ø§ ÛÛ’ Ú©Û Ø²ÛŒØ§Ø¯Û ØªØ± Ù„ÛŒÙ¾ Ù¹Ø§Ù¾Ø³ Ú©ÛŒ RAM Ú©Ùˆ Ú†ÛŒÙ„Ù†Ø¬ Ø¯Û’ Ø¯Û’ Ú¯Ø§ØŒ Ù„ÛŒÚ©Ù† ÛÙ… Ø§Ø³Û’ Ø¨Ø¢Ø³Ø§Ù†ÛŒ Ù„ÙˆÚˆ Ú©Ø± Ù¾Ø§ØªÛ’ ÛÛŒÚº! Ø§Ø¨ FreeLaw Ø§ÙˆØ± PubMed Abstracts Ú©Û’ examples Ú©Ùˆ `interleave_datasets()` ÙÙ†Ú©Ø´Ù† Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© combined dataset Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

ÛŒÛØ§Úº ÛÙ… Ù†Û’ Python Ú©ÛŒ `itertools.islice()` ÙÙ†Ú©Ø´Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ combined dataset Ú©Û’ Ù¾ÛÙ„Û’ Ø¯Ùˆ examples Ù…Ù†ØªØ®Ø¨ Ú©ÛŒÛ’ØŒ Ø§ÙˆØ± ÛŒÛ Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº source datasets Ú©Û’ Ù¾ÛÙ„Û’ examples Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛÛŒÚºÛ”

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ Ø§Ú¯Ø± Ø¢Ù¾ Ù¾ÙˆØ±Û’ 825 GB Ú©Û’ Pile Ú©Ùˆ stream Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ ØªÙ…Ø§Ù… ØªÛŒØ§Ø± Ø´Ø¯Û ÙØ§Ø¦Ù„ÙˆÚº Ú©Ùˆ Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play â€œSurvival of the Tastiestâ€ on Android, and on the web...'}
```

<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** [the-eye.eu/public/AI/pile_preliminary_components/] Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ú©Ø³ÛŒ Ø¨Ú‘Û’ Common Crawl corpus Ø¬ÛŒØ³Û’ [`mc4`](https://huggingface.co/datasets/mc4) ÛŒØ§ [`oscar`](https://huggingface.co/datasets/oscar) Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§ÛŒÚ© streaming multilingual dataset ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº Ø¬Ùˆ Ø¢Ù¾ Ú©Û’ Ù…Ù†ØªØ®Ø¨ Ù…Ù„Ú© Ú©ÛŒ Ø¨ÙˆÙ„ÛŒ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„ÛŒ Ø²Ø¨Ø§Ù†ÙˆÚº Ú©Û’ ØªÙ†Ø§Ø³Ø¨ Ú©ÛŒ Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Ø±ØªØ§ ÛÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø³ÙˆØ¦Ù¹Ø²Ø±Ù„ÛŒÙ†Úˆ Ù…ÛŒÚº Ú†Ø§Ø± Ù‚ÙˆÙ…ÛŒ Ø²Ø¨Ø§Ù†ÛŒÚº ÛÛŒÚº: Ø¬Ø±Ù…Ù†ØŒ ÙØ±Ø§Ù†Ø³ÛŒØ³ÛŒØŒ Ø§Ø·Ø§Ù„ÙˆÛŒØŒ Ø§ÙˆØ± RomanshØŒ ØªÙˆ Ø¢Ù¾ Oscar Ú©Û’ subsets Ú©Ùˆ Ø§Ù† Ú©Û’ Ø¨ÙˆÙ„Ù†Û’ Ú©Û’ ØªÙ†Ø§Ø³Ø¨ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ sample Ú©Ø± Ú©Û’ Ø§ÛŒÚ© Ø³ÙˆØ¦Ø³ corpus Ø¨Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

</Tip>

Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ Ø§Ø¨ ÛØ± Ù‚Ø³Ù… Ú©Û’ shapes Ø§ÙˆØ± sizes Ú©Û’ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Ùˆ Ù„ÙˆÚˆ Ø§ÙˆØ± Ù¾Ø±Ø§Ø³ÛŒØ³ Ú©Ø±Ù†Û’ Ú©Û’ ØªÙ…Ø§Ù… Ø§ÙˆØ²Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚº â€” Ù„ÛŒÚ©Ù† Ø¬Ø¨ ØªÚ© Ø¢Ù¾ Ø¨ÛØª Ø®ÙˆØ´ Ù†ØµÛŒØ¨ Ù†ÛÛŒÚº ÛÙˆØªÛ’ØŒ Ø¢Ù¾ Ú©Û’ NLP Ø³ÙØ± Ù…ÛŒÚº Ø§ÛŒØ³Ø§ Ù„Ù…Ø­Û Ø¶Ø±ÙˆØ± Ø¢Ø¦Û’ Ú¯Ø§ Ø¬Ø¨ Ø¢Ù¾ Ú©Ùˆ Ú©Ø³ÛŒ Ù…Ø³Ø¦Ù„Û’ Ú©Û’ Ø­Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø®ÙˆØ¯ Ø§Ù¾Ù†Ø§ dataset ØªØ®Ù„ÛŒÙ‚ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” ÛŒÛÛŒ Ø§Ú¯Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ú©Ø§ Ù…ÙˆØ¶ÙˆØ¹ ÛÛ’!