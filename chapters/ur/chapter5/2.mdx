# اگر میرا ڈیٹاسیٹ ہب پر نہیں ہے تو کیا؟[[what-if-my-dataset-isnt-on-the-hub]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

آپ جانتے ہیں کہ [Hugging Face Hub](https://huggingface.co/datasets) کا استعمال کرتے ہوئے ڈیٹاسیٹس کیسے ڈاؤن لوڈ کیے جاتے ہیں، لیکن اکثر آپ ایسے ڈیٹا کے ساتھ کام کریں گے جو یا تو آپ کے لیپ ٹاپ پر محفوظ ہے یا کسی دور دراز سرور پر۔ اس سیکشن میں ہم آپ کو دکھائیں گے کہ کس طرح 🤗 Datasets کا استعمال کرتے ہوئے ایسے ڈیٹاسیٹس لوڈ کیے جا سکتے ہیں جو Hugging Face Hub پر دستیاب نہیں ہیں۔

<Youtube id="HyQgpJTkRdE"/>

## مقامی اور ریموٹ ڈیٹاسیٹس کے ساتھ کام کرنا[[working-with-local-and-remote-datasets]]

🤗 Datasets لوڈنگ اسکرپٹس فراہم کرتا ہے جو مقامی اور ریموٹ ڈیٹاسیٹس کو لوڈ کرنے میں مدد دیتے ہیں۔ یہ کئی عام ڈیٹا فارمیٹس کی حمایت کرتا ہے، جیسے کہ:

|    ڈیٹا فارمیٹ     | لوڈنگ اسکرپٹ |                         مثال                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Text files     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

جیسا کہ ٹیبل میں دکھایا گیا ہے، ہر ڈیٹا فارمیٹ کے لیے ہمیں صرف `load_dataset()` فنکشن میں لوڈنگ اسکرپٹ کی قسم اور `data_files` آرگومنٹ فراہم کرنا ہوتا ہے جو ایک یا زیادہ فائلوں کا راستہ بتاتا ہے۔ آئیے پہلے مقامی فائلوں سے ایک ڈیٹاسیٹ لوڈ کرنے سے شروع کرتے ہیں؛ بعد میں ہم دیکھیں گے کہ دور دراز فائلوں کے ساتھ ایسا کیسے کیا جاتا ہے۔

## مقامی ڈیٹاسیٹ لوڈ کرنا[[loading-a-local-dataset]]

اس مثال کے لیے ہم [SQuAD-it dataset](https://github.com/crux82/squad-it/) کا استعمال کریں گے، جو اطالوی زبان میں سوال جواب کے لیے ایک بڑے پیمانے پر ڈیٹاسیٹ ہے۔

ٹریننگ اور ٹیسٹ سپلٹس GitHub پر ہوسٹ کیے گئے ہیں، اس لیے ہم انہیں ایک سادہ `wget` کمانڈ کے ذریعے ڈاؤن لوڈ کر سکتے ہیں:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

یہ دو کمپریسڈ فائلیں ڈاؤن لوڈ کرے گا جن کے نام *SQuAD_it-train.json.gz* اور *SQuAD_it-test.json.gz* ہیں، جنہیں ہم Linux کے `gzip` کمانڈ کے ذریعے ڈی کمپریس کر سکتے ہیں:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

ہم دیکھ سکتے ہیں کہ کمپریسڈ فائلیں _SQuAD_it-train.json_ اور _SQuAD_it-test.json_ سے بدل گئیں ہیں، اور ڈیٹا JSON فارمیٹ میں محفوظ ہے۔

<Tip>

✎ اگر آپ سوچ رہے ہیں کہ اوپر دی گئی شیل کمانڈز میں `!` کردار کیوں ہے، تو یہ اس لیے ہے کیونکہ ہم انہیں Jupyter نوٹ بُک میں چلا رہے ہیں۔ اگر آپ ٹرمینل میں ڈیٹاسیٹ ڈاؤن لوڈ اور ان زپ کرنا چاہتے ہیں تو بس اس پرِفکس کو ہٹا دیں۔

</Tip>

ایک JSON فائل کو `load_dataset()` فنکشن کے ساتھ لوڈ کرنے کے لیے، ہمیں صرف یہ جاننے کی ضرورت ہے کہ ہم عام JSON (جو ایک نیسٹڈ ڈکشنری کی طرح ہوتا ہے) سے کام کر رہے ہیں یا JSON Lines (لائن سے علیحدہ JSON) سے۔ بہت سے سوال جواب ڈیٹاسیٹس کی طرح، SQuAD-it نیسٹڈ فارمیٹ استعمال کرتا ہے، جس میں تمام متن `data` فیلڈ میں محفوظ ہوتا ہے۔ اس کا مطلب ہے کہ ہم `field` آرگومنٹ فراہم کر کے ڈیٹاسیٹ لوڈ کر سکتے ہیں جیسے کہ:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

ڈیفالٹ کے طور پر، مقامی فائلیں لوڈ کرنے سے ایک `DatasetDict` آبجیکٹ بنتا ہے جس میں ایک `train` سپلٹ شامل ہوتا ہے۔ ہم اسے `squad_it_dataset` آبجیکٹ کو دیکھ کر جان سکتے ہیں:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

یہ ہمیں ٹریننگ سیٹ سے وابستہ قطاروں کی تعداد اور کالم کے نام دکھاتا ہے۔ ہم درج ذیل طریقے سے `train` سپلٹ میں سے ایک مثال دیکھ سکتے ہیں:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si è verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

زبردست، ہم نے اپنا پہلا مقامی ڈیٹاسیٹ لوڈ کر لیا ہے! لیکن جبکہ یہ ٹریننگ سیٹ کے لیے کام کیا، دراصل ہم چاہتے ہیں کہ `train` اور `test` دونوں سپلٹس کو ایک ہی `DatasetDict` آبجیکٹ میں شامل کیا جائے تاکہ ہم ایک ساتھ دونوں سپلٹس پر `Dataset.map()` فنکشنز لاگو کر سکیں۔ ایسا کرنے کے لیے، ہم `data_files` آرگومنٹ میں ایک ڈکشنری فراہم کر سکتے ہیں جو ہر سپلٹ کے نام کو اس سپلٹ سے وابستہ فائل کے ساتھ میپ کرتی ہے:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

یہ بالکل وہی ہے جو ہم چاہتے تھے۔ اب، ہم مختلف پری پروسیسنگ تکنیکوں کو لاگو کر سکتے ہیں تاکہ ڈیٹا کو صاف کیا جا سکے، ریویوز کو ٹوکنائز کیا جا سکے، وغیرہ۔

<Tip>

`load_dataset()` فنکشن کا `data_files` آرگومنٹ کافی لچکدار ہے اور یہ یا تو ایک فائل کا راستہ، فائل راستوں کی فہرست، یا ایک ڈکشنری ہو سکتا ہے جو سپلٹ کے ناموں کو فائل راستوں کے ساتھ میپ کرتا ہے۔ آپ یونکس شیل کے استعمال شدہ اصولوں کے مطابق مخصوص پیٹرن سے میل کھاتی فائلوں کو بھی گلوب کر سکتے ہیں (مثلاً، آپ `data_files="*.json"` سیٹ کر کے کسی ڈائریکٹری کی تمام JSON فائلوں کو ایک سپلٹ کے طور پر گلوب کر سکتے ہیں)۔ مزید تفصیلات کے لیے 🤗 Datasets کی [دستاویزات](https://huggingface.co/docs/datasets/loading#local-and-remote-files) دیکھیں۔

</Tip>

🤗 Datasets کے لوڈنگ اسکرپٹس دراصل ان پٹ فائلوں کی خودکار ڈی کمپریشن کی حمایت کرتے ہیں، لہٰذا ہم `data_files` آرگومنٹ کو براہ راست کمپریسڈ فائلوں کی طرف پوائنٹ کر کے `gzip` کے استعمال کو چھوڑ سکتے تھے:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

یہ اس صورت میں مفید ہو سکتا ہے اگر آپ بہت سی GZIP فائلوں کو دستی طور پر ڈی کمپریس نہیں کرنا چاہتے۔ خودکار ڈی کمپریشن دیگر عام فارمیٹس جیسے ZIP اور TAR پر بھی لاگو ہوتا ہے، لہٰذا آپ کو صرف `data_files` کو کمپریسڈ فائلوں کی طرف پوائنٹ کرنا ہے اور آپ تیار ہیں!

اب جب کہ آپ جان چکے ہیں کہ اپنے لیپ ٹاپ یا ڈیسک ٹاپ پر مقامی فائلیں کیسے لوڈ کی جاتی ہیں، آئیں دور دراز فائلیں لوڈ کرنے کا طریقہ دیکھتے ہیں۔

## دور دراز ڈیٹاسیٹ لوڈ کرنا[[loading-a-remote-dataset]]

اگر آپ کسی کمپنی میں ڈیٹا سائنسدان یا کوڈر کے طور پر کام کر رہے ہیں، تو اچھا امکان ہے کہ جن ڈیٹاسیٹس کا آپ تجزیہ کرنا چاہتے ہیں وہ کسی دور دراز سرور پر محفوظ ہوں۔ خوش قسمتی سے، دور دراز فائلیں لوڈ کرنا مقامی فائلیں لوڈ کرنے کے جتنا ہی آسان ہے! مقامی فائلوں کا راستہ فراہم کرنے کی بجائے، ہم `load_dataset()` کے `data_files` آرگومنٹ کو ان ایک یا زیادہ URLs کی طرف پوائنٹ کرتے ہیں جہاں دور دراز فائلیں محفوظ ہوتی ہیں۔ مثال کے طور پر، GitHub پر ہوسٹ کیے گئے SQuAD-it ڈیٹاسیٹ کے لیے، ہم صرف `data_files` کو _SQuAD_it-*.json.gz_ URLs کی طرف پوائنٹ کر سکتے ہیں جیسے کہ:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

یہ وہی `DatasetDict` آبجیکٹ واپس کرتا ہے جو اوپر حاصل کیا گیا تھا، لیکن ہمیں _SQuAD_it-*.json.gz_ فائلوں کو دستی طور پر ڈاؤن لوڈ اور ڈی کمپریس کرنے کا مرحلہ بچا لیتا ہے۔ یہ ہماری Hugging Face Hub پر نہ ہونے والے ڈیٹاسیٹس کو لوڈ کرنے کے مختلف طریقوں میں ایک چھوٹا سا جائزہ ہے۔ اب جب کہ ہمارے پاس تجربہ کرنے کے لیے ایک ڈیٹاسیٹ ہے، آئیں مختلف ڈیٹا وارنگ کے تکنیکوں کے ساتھ ہاتھ آزمائیں!

<Tip>

✏️ **آزمائیں!** GitHub یا [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) پر ہوسٹ کیے گئے کسی اور ڈیٹاسیٹ کو منتخب کریں اور اوپر بیان کردہ تکنیکوں کا استعمال کرتے ہوئے اسے مقامی اور دور دراز دونوں طریقوں سے لوڈ کرنے کی کوشش کریں۔ اضافی پوائنٹس کے لیے، CSV یا ٹیکسٹ فارمیٹ میں محفوظ ڈیٹاسیٹ لوڈ کرنے کی کوشش کریں (مزید معلومات کے لیے [دستاویزات](https://huggingface.co/docs/datasets/loading#local-and-remote-files) دیکھیں)۔

</Tip>