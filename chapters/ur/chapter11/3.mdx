<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/en/chapter11/section3.ipynb"},
]} />

# سپروائزڈ فائن-ٹیوننگ

سپروائزڈ فائن-ٹیوننگ (SFT) ایک ایسا عمل ہے جسے بنیادی طور پر پہلے سے تربیت یافتہ زبان کے ماڈلز کو ہدایات پر عمل کرنے، گفتگو میں شامل ہونے، اور مخصوص آؤٹ پٹ فارمیٹس استعمال کرنے کے قابل بنانے کے لیے استعمال کیا جاتا ہے۔ اگرچہ پہلے سے تربیت یافتہ ماڈلز کی عمومی صلاحیتیں متاثر کن ہوتی ہیں، لیکن SFT انہیں اسسٹنٹ نما ماڈلز میں تبدیل کرنے میں مدد کرتا ہے جو صارف کے پرامپٹس کو بہتر طور پر سمجھ کر ان کا جواب دے سکیں۔ عام طور پر یہ عمل انسانی تحریر کردہ گفتگو اور ہدایات کے ڈیٹاسیٹس پر تربیت دے کر انجام دیا جاتا ہے۔

یہ صفحہ [`deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) ماڈل کو [`SFTTrainer`](https://huggingface.co/docs/trl/en/sft_trainer) کا استعمال کرتے ہوئے فائن-ٹیون کرنے کے لیے مرحلہ وار رہنما فراہم کرتا ہے۔ ان مراحل پر عمل کر کے، آپ ماڈل کو مخصوص کاموں کو زیادہ مؤثر طریقے سے انجام دینے کے قابل بنا سکتے ہیں۔

## SFT کب استعمال کریں

تعمیراتی عمل میں غوطہ لگانے سے پہلے، یہ سمجھنا ضروری ہے کہ آپ کے پروجیکٹ کے لیے SFT کب درست انتخاب ہے۔ پہلا قدم یہ سوچنا چاہیے کہ آیا پہلے سے ہدایت شدہ ماڈل کو عمدہ تیار کردہ پرامپٹس کے ساتھ استعمال کرنا آپ کی ضروریات کو پورا کر سکتا ہے یا نہیں۔ SFT میں نمایاں کمپیوٹیشنل وسائل اور انجینئرنگ کی محنت شامل ہوتی ہے، لہٰذا اسے صرف تب ہی اختیار کیا جانا چاہیے جب موجودہ ماڈلز کو پرامپٹ کر کے مطلوبہ نتائج حاصل نہ ہو سکیں۔

<Tip>
صرف تب SFT پر غور کریں اگر:
- آپ کو پرامپٹنگ سے حاصل ہونے والی کارکردگی سے زیادہ کارکردگی کی ضرورت ہو
- آپ کا کوئی ایسا مخصوص استعمال کا معاملہ ہو جہاں بڑے عام مقصد کے ماڈل کے استعمال کی قیمت ایک چھوٹے ماڈل کو فائن-ٹیون کرنے کی قیمت سے زیادہ ہو
- آپ کو مخصوص آؤٹ پٹ فارمیٹس یا ڈومین مخصوص علم کی ضرورت ہو جس میں موجودہ ماڈلز کو دشواری ہو
</Tip>

اگر آپ یہ نتیجہ اخذ کر لیں کہ SFT ضروری ہے، تو آگے بڑھنے کا فیصلہ دو بنیادی عوامل پر منحصر ہے:

### ٹیمپلیٹ کنٹرول
SFT ماڈل کے آؤٹ پٹ ڈھانچے پر درست کنٹرول فراہم کرتا ہے۔ یہ خاص طور پر اس وقت قیمتی ہوتا ہے جب آپ کو ماڈل سے درج ذیل کی توقع ہو:
1. ایک مخصوص چیٹ ٹیمپلیٹ فارمیٹ میں جوابات تیار کرنا
2. سخت آؤٹ پٹ اسکیما کی پیروی کرنا
3. جوابات میں مستقل اسٹائل برقرار رکھنا

### ڈومین ایڈاپٹیشن
جب مخصوص ڈومینز میں کام کیا جائے تو SFT ماڈل کو ڈومین مخصوص تقاضوں کے مطابق ڈھالنے میں مدد کرتا ہے:
1. ڈومین کی اصطلاحات اور تصورات کی تعلیم دینا
2. پیشہ ورانہ معیارات کو نافذ کرنا
3. تکنیکی سوالات کا مناسب طریقے سے جواب دینا
4. صنعت کے مخصوص رہنما اصولوں کی پیروی کرنا

<Tip>
SFT شروع کرنے سے پہلے یہ جائزہ لیں کہ آیا آپ کے استعمال کے معاملے میں:
- درست آؤٹ پٹ فارمیٹنگ کی ضرورت ہے
- ڈومین مخصوص علم درکار ہے
- مستقل جواب دینے کے پیٹرنز کی ضرورت ہے
- مخصوص رہنما اصولوں کی پابندی ضروری ہے

یہ تجزیہ اس بات کا فیصلہ کرنے میں مدد کرے گا کہ آیا SFT آپ کی ضروریات کے لیے درست طریقہ ہے یا نہیں۔
</Tip>

## ڈیٹاسیٹ کی تیاری

سپروائزڈ فائن-ٹیوننگ کے عمل کے لیے ایک مخصوص کام کا ڈیٹاسیٹ درکار ہوتا ہے جسے ان پٹ-آؤٹ پٹ جوڑوں کے ساتھ ترتیب دیا گیا ہو۔ ہر جوڑے میں شامل ہونا چاہیے:
1. ایک ان پٹ پرامپٹ
2. متوقع ماڈل کا جواب
3. کوئی اضافی سیاق و سباق یا میٹا ڈیٹا

آپ کے تربیتی ڈیٹا کا معیار کامیاب فائن-ٹیوننگ کے لیے انتہائی اہم ہے۔ آئیں دیکھتے ہیں کہ آپ اپنے ڈیٹاسیٹ کو کیسے تیار اور تصدیق کر سکتے ہیں:

<iframe
  src="https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0"
  frameborder="0"
  width="100%"
  height="360px"
></iframe>

## تربیتی ترتیب

آپ کی فائن-ٹیوننگ کی کامیابی کا دارومدار درست تربیتی پیرامیٹرز کے انتخاب پر بہت زیادہ ہے۔ آئیں ان اہم پیرامیٹرز کا جائزہ لیتے ہیں اور ان کو مؤثر طریقے سے ترتیب دینے کا طریقہ سمجھتے ہیں:

SFTTrainer کی ترتیب کے لیے کئی ایسے پیرامیٹرز پر غور کرنا ضروری ہے جو تربیتی عمل کو کنٹرول کرتے ہیں:

1. **تربیتی دورانیہ کے پیرامیٹرز**:
   - `num_train_epochs`: کل تربیتی دورانیہ کو کنٹرول کرتا ہے
   - `max_steps`: ایپوک کے متبادل کے طور پر، تربیتی اقدامات کی زیادہ سے زیادہ تعداد مقرر کرتا ہے
   - زیادہ ایپوک بہتر سیکھنے کی اجازت دیتے ہیں مگر زیادہ ایپوک سے اوورفٹنگ کا خطرہ بھی ہوتا ہے

2. **بیچ سائز کے پیرامیٹرز**:
   - `per_device_train_batch_size`: میموری کے استعمال اور تربیتی استحکام کا تعین کرتا ہے
   - `gradient_accumulation_steps`: بڑے مؤثر بیچ سائز کی اجازت دیتا ہے
   - بڑے بیچ زیادہ مستحکم گریڈینٹس فراہم کرتے ہیں مگر زیادہ میموری کی ضرورت ہوتی ہے

3. **لرننگ ریٹ کے پیرامیٹرز**:
   - `learning_rate`: وزن کی اپڈیٹس کا سائز کنٹرول کرتا ہے
   - `warmup_ratio`: تربیت کے ابتدائی حصے میں لرننگ ریٹ کو گرم کرنے کا تناسب
   - بہت زیادہ لرننگ ریٹ عدم استحکام پیدا کر سکتا ہے، جبکہ بہت کم سست سیکھنے کا سبب بنتا ہے

4. **مانیٹرنگ کے پیرامیٹرز**:
   - `logging_steps`: میٹرکس لاگ کرنے کی تعدد
   - `eval_steps`: ویلیڈیشن ڈیٹا پر کتنا بار جانچ کرنا ہے
   - `save_steps`: ماڈل کے چیک پوائنٹس محفوظ کرنے کی تعدد

<Tip>
محتاط اقدار سے آغاز کریں اور مانیٹرنگ کی بنیاد پر ایڈجسٹ کریں:
- 1-3 ایپوک سے آغاز کریں
- ابتدائی طور پر چھوٹے بیچ سائز استعمال کریں
- ویلیڈیشن میٹرکس کو قریبی نگرانی کریں
- اگر تربیت میں عدم استحکام ہو تو لرننگ ریٹ کو ایڈجسٹ کریں
</Tip>

## TRL کے ساتھ امپلیمنٹیشن

اب جبکہ ہم اہم اجزاء کو سمجھ چکے ہیں، آئیں مناسب تصدیق اور مانیٹرنگ کے ساتھ تربیت کا نفاذ کرتے ہیں۔ ہم `transformers` لائبریری پر مبنی Transformers Reinforcement Learning (TRL) لائبریری کے `SFTTrainer` کلاس کا استعمال کریں گے۔ یہاں TRL لائبریری کا استعمال کرتے ہوئے ایک مکمل مثال دی گئی ہے:

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer
import torch

# Set device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load dataset
dataset = load_dataset("HuggingFaceTB/smoltalk", "all")

# Configure trainer
training_args = SFTConfig(
    output_dir="./sft_output",
    max_steps=1000,
    per_device_train_batch_size=4,
    learning_rate=5e-5,
    logging_steps=10,
    save_steps=100,
    eval_strategy="steps",
    eval_steps=50,
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
)

# Start training
trainer.train()
```

<Tip>
اگر آپ ایسے ڈیٹاسیٹ کا استعمال کر رہے ہیں جس میں "messages" فیلڈ موجود ہو (جیسا کہ اوپر کی مثال میں ہے)، تو SFTTrainer خود بخود ماڈل کا چیٹ ٹیمپلیٹ لاگو کر دیتا ہے، جو ہب سے حاصل کیا جاتا ہے۔ اس کا مطلب ہے کہ چیٹ اسٹائل گفتگو کو سنبھالنے کے لیے آپ کو کوئی اضافی ترتیب دینے کی ضرورت نہیں—ٹرینر ماڈل کی توقع کے مطابق پیغامات کو فارمیٹ کر دے گا۔
</Tip>

## ڈیٹاسیٹ کی پیکنگ

SFTTrainer تربیت کی کارکردگی کو بہتر بنانے کے لیے مثالوں کی پیکنگ کی حمایت کرتا ہے۔ اس خصوصیت کی مدد سے متعدد چھوٹی مثالوں کو ایک ہی ان پٹ سیکوئنس میں پیک کیا جا سکتا ہے، جس سے تربیت کے دوران GPU کی مکمل صلاحیت استعمال کی جا سکتی ہے۔ پیکنگ کو فعال کرنے کے لیے، SFTConfig کنسٹرکٹر میں `packing=True` سیٹ کریں۔ پیک کیے گئے ڈیٹاسیٹس کے ساتھ `max_steps` استعمال کرتے وقت اس بات کا خیال رکھیں کہ آپ اپنی پیکنگ کی ترتیب کے مطابق زیادہ ایپوک کی تربیت کر سکتے ہیں۔ آپ ایک فارمیٹنگ فنکشن کا استعمال کر کے مثالوں کو کس طرح ملایا جائے اس کو کسٹمائز بھی کر سکتے ہیں—خاص طور پر ان ڈیٹاسیٹس کے ساتھ جو سوال-جواب کے جوڑوں جیسے متعدد فیلڈز رکھتے ہیں۔ ویلیڈیشن ڈیٹاسیٹس کے لیے، آپ SFTConfig میں `eval_packing=False` سیٹ کر کے پیکنگ کو غیر فعال کر سکتے ہیں۔ یہاں پیکنگ کی ترتیب کو کسٹمائز کرنے کی ایک بنیادی مثال دی گئی ہے:

```python
# Configure packing
training_args = SFTConfig(packing=True)

trainer = SFTTrainer(model=model, train_dataset=dataset, args=training_args)

trainer.train()
```

جب ایک سے زیادہ فیلڈز والے ڈیٹاسیٹ کو پیک کیا جائے، تو آپ ایک کسٹم فارمیٹنگ فنکشن ڈیفائن کر سکتے ہیں جو فیلڈز کو ایک واحد ان پٹ سیکوئنس میں ملا دے۔ اس فنکشن کو مثالوں کی ایک فہرست لینا چاہیے اور پیک شدہ ان پٹ سیکوئنس کے ساتھ ایک ڈکشنری واپس کرنی چاہیے۔ یہاں ایک کسٹم فارمیٹنگ فنکشن کی مثال ہے:

```python
def formatting_func(example):
    text = f"### Question: {example['question']}\n ### Answer: {example['answer']}"
    return text


training_args = SFTConfig(packing=True)
trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
    formatting_func=formatting_func,
)
```

## تربیت کی پیش رفت کی مانیٹرنگ

کامیاب فائن-ٹیوننگ کے لیے مؤثر مانیٹرنگ ضروری ہے۔ آئیں دیکھتے ہیں کہ تربیت کے دوران کن چیزوں پر توجہ دینی چاہیے:

### لاس پیٹرنز کو سمجھنا

تربیتی لاس عموماً تین مختلف مراحل سے گزرتا ہے:
1. ابتدائی تیز کمی: نئے ڈیٹا تقسیم کے مطابق تیزی سے ڈھلنا
2. بتدریج استحکام: ماڈل کے فائن-ٹیون ہونے کے ساتھ لرننگ ریٹ کا دھیرے دھیرے کم ہونا
3. کنورجنس: لاس کی قیمتوں کا مستحکم ہونا، جو تربیت کی تکمیل کی نشاندہی کرتا ہے

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/nlp_course_sft_loss_graphic.png" alt="SFTTrainer Training" />

### مانیٹر کرنے کے میٹرکس

مؤثر مانیٹرنگ میں مقداری میٹرکس کے ساتھ ساتھ معیاری (qualitative) میٹرکس کی جانچ شامل ہے۔ دستیاب میٹرکس میں شامل ہیں:

- تربیتی لاس
- ویلیڈیشن لاس
- لرننگ ریٹ کی پیش رفت
- گریڈینٹ نورمز

<Tip warning={true}>
تربیت کے دوران مندرجہ ذیل وارننگ سائنز پر نظر رکھیں:
1. جب ویلیڈیشن لاس بڑھتا ہے جبکہ تربیتی لاس کم ہوتا رہتا ہے (اوورفٹنگ)
2. لاس کی قیمتوں میں نمایاں بہتری نہ ہونا (انڈر فٹنگ)
3. انتہائی کم لاس کی قیمتیں (ممکنہ یادداشت)
4. آؤٹ پٹ فارمیٹنگ میں غیر مطابقت (ٹیمپلیٹ لرننگ کے مسائل)
</Tip>

### کنورجنس کا راستہ

جیسے جیسے تربیت آگے بڑھتی ہے، لاس کا گراف بتدریج مستحکم ہونا چاہیے۔ صحت مند تربیت کی کلیدی نشانی یہ ہے کہ تربیتی اور ویلیڈیشن لاس کے درمیان فاصلہ کم ہو، جو اس بات کی نشاندہی کرتا ہے کہ ماڈل مخصوص مثالوں کو یاد رکھنے کی بجائے عام پیٹرنز سیکھ رہا ہے۔ مطلق لاس کی قیمتیں آپ کے کام اور ڈیٹاسیٹ پر منحصر ہوں گی۔

### تربیت کی پیش رفت کی مانیٹرنگ

اوپر دیا گیا گراف ایک عام تربیتی پیش رفت کو ظاہر کرتا ہے۔ نوٹ کریں کہ ابتدا میں تربیتی اور ویلیڈیشن لاس دونوں تیزی سے کم ہوتے ہیں، پھر بتدریج لیول آف ہو جاتے ہیں۔ یہ پیٹرن اس بات کی نشاندہی کرتا ہے کہ ماڈل مؤثر طریقے سے سیکھ رہا ہے جبکہ جنرلائزیشن کی صلاحیت کو برقرار رکھ رہا ہے۔

### وارننگ سائنز جن پر نظر رکھیں

لاس کے گراف میں کئی پیٹرنز ممکنہ مسائل کی نشاندہی کر سکتے ہیں۔ ذیل میں ہم عام وارننگ سائنز اور ان کے ممکنہ حل بیان کرتے ہیں۔

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_1.png" alt="SFTTrainer Training" />

اگر ویلیڈیشن لاس تربیتی لاس کے مقابلے میں نمایاں طور پر سست رفتار سے کم ہوتا ہے، تو آپ کا ماڈل تربیتی ڈیٹا کے لیے اوورفٹنگ کا شکار ہو سکتا ہے۔ غور کریں:
- تربیتی اقدامات کو کم کریں
- ڈیٹاسیٹ کا سائز بڑھائیں
- ڈیٹاسیٹ کے معیار اور تنوع کی تصدیق کریں

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_2.png" alt="SFTTrainer Training" />

اگر لاس میں نمایاں بہتری نہیں آتی تو ماڈل ہو سکتا ہے:
- بہت آہستہ سیکھ رہا ہو (لرننگ ریٹ بڑھانے کی کوشش کریں)
- کام میں دشواری کا سامنا کر رہا ہو (ڈیٹا کے معیار اور کام کی پیچیدگی چیک کریں)
- ماڈل کی حدود سے تجاوز کر رہا ہو (کسی مختلف ماڈل پر غور کریں)

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_3.png" alt="SFTTrainer Training" />

انتہائی کم لاس کی قیمتیں یادداشت (memorization) کی نشاندہی کر سکتی ہیں بجائے اس کے کہ ماڈل واقعی سیکھ رہا ہو۔ یہ خاص طور پر تشویشناک ہے اگر:
- ماڈل نئے، مشابہ مثالوں پر خراب کارکردگی دکھائے
- آؤٹ پٹس میں تنوع نہ ہو
- جوابات تربیتی مثالوں سے بہت زیادہ مشابہ ہوں

<Tip warning={true}>
تربیت کے دوران لاس کی قیمتوں اور ماڈل کے حقیقی آؤٹ پٹس دونوں کی مانیٹرنگ کریں۔ بعض اوقات لاس تو ٹھیک دکھتا ہے لیکن ماڈل میں غیر مطلوبہ رویے پیدا ہو جاتے ہیں۔ ماڈل کے جوابات کی باقاعدہ معیاری جانچ سے ان مسائل کا پتا چل سکتا ہے جو صرف میٹرکس سے نظر نہیں آتے۔
</Tip>

ہم یہ نوٹ کرنا چاہیں گے کہ یہاں بیان کی گئی لاس کی قیمتوں کی تشریح عام کیس کے لیے ہے، اور دراصل، لاس کی قیمتیں ماڈل، ڈیٹاسیٹ، تربیتی پیرامیٹرز وغیرہ پر منحصر مختلف طریقوں سے برتاؤ کر سکتی ہیں۔ اگر آپ مذکورہ پیٹرنز کے بارے میں مزید جاننا چاہتے ہیں تو [Fast AI](https://www.fast.ai/posts/2023-09-04-learning-jumps/) کی اس بلاگ پوسٹ کو دیکھیں۔

## SFT کے بعد کی جانچ

سیکشن [11.4](/en/chapter11/4) میں ہم بینچ مارک ڈیٹاسیٹس کا استعمال کرتے ہوئے ماڈل کی جانچ کرنا سیکھیں گے۔ فی الحال، ہم ماڈل کی معیاری (qualitative) جانچ پر توجہ مرکوز کریں گے۔

SFT مکمل کرنے کے بعد، درج ذیل فالو اپ اقدامات پر غور کریں:

1. ماڈل کا held-out ٹیسٹ ڈیٹا پر مکمل طور پر تجزیہ کریں
2. مختلف ان پٹس پر ٹیمپلیٹ کی پابندی کی تصدیق کریں
3. ڈومین مخصوص علم کی برقراری کی جانچ کریں
4. حقیقی دنیا کے کارکردگی میٹرکس کی نگرانی کریں

<Tip>
اپنے تربیتی عمل کی دستاویز بندی کریں، جس میں شامل ہو:
- ڈیٹاسیٹ کی خصوصیات
- تربیتی پیرامیٹرز
- کارکردگی کے میٹرکس
- معلوم خامیاں
یہ دستاویز بندی مستقبل میں ماڈل کی بہتری کے لیے قیمتی ثابت ہوگی۔
</Tip>

## کوئز

### 1. SFT میں تربیتی دورانیہ کو کن پیرامیٹرز سے کنٹرول کیا جاتا ہے؟

<Question
	choices={[
		{
			text: "num_train_epochs اور max_steps",
			explain: "درست! یہ پیرامیٹرز یہ تعین کرتے ہیں کہ ماڈل کتنے عرصے تک تربیت پائے گا، یا تو ایپوک کی تعداد سے یا کل اقدامات سے.",
			correct: true
		},
		{
			text: "batch_size اور learning_rate",
			explain: "اگرچہ یہ تربیت پر اثر انداز ہوتے ہیں، لیکن براہ راست دورانیہ کو کنٹرول نہیں کرتے."
		},
		{
			text: "gradient_checkpointing اور warmup_ratio",
			explain: "یہ پیرامیٹرز تربیتی کارکردگی اور استحکام پر اثر ڈالتے ہیں، نہ کہ دورانیہ پر."
		}
	]}
/>

### 2. لاس کے گراف میں کون سا پیٹرن ممکنہ اوورفٹنگ کی نشاندہی کرتا ہے؟

<Question
    choices={[
        {
            text: "ویلیڈیشن لاس بڑھتا ہے جبکہ تربیتی لاس کم ہوتا جاتا ہے",
            explain: "درست! تربیتی اور ویلیڈیشن لاس کے درمیان یہ انحراف اوورفٹنگ کی کلاسیکی علامت ہے.",
            correct: true
        },
        {
            text: "تربیتی اور ویلیڈیشن لاس دونوں مسلسل کم ہوتے ہیں",
            explain: "یہ پیٹرن دراصل صحت مند تربیت کی نشاندہی کرتا ہے."
        },
        {
            text: "تربیتی لاس مستقل رہتا ہے جبکہ ویلیڈیشن لاس کم ہوتا ہے",
            explain: "یہ ایک غیر معمولی پیٹرن ہوگا اور اوورفٹنگ کی نشاندہی نہیں کرتا."
        }
    ]}
/>

### 3. gradient_accumulation_steps کا استعمال کس لیے کیا جاتا ہے؟

<Question
    choices={[
        {
            text: "زیادہ میموری استعمال کیے بغیر مؤثر بیچ سائز میں اضافہ کرنے کے لیے",
            explain: "درست! یہ متعدد فارورڈ پاسز کے دوران گریڈینٹس کو جمع کرتا ہے اس سے پہلے کہ وزن اپڈیٹ کیے جائیں.",
            correct: true
        },
        {
            text: "تربیت کے دوران چیک پوائنٹس محفوظ کرنے کے لیے",
            explain: "یہ save_steps اور save_strategy پیرامیٹرز کے ذریعے ہینڈل ہوتا ہے."
        },
        {
            text: "لرننگ ریٹ کے شیڈول کو کنٹرول کرنے کے لیے",
            explain: "لرننگ ریٹ شیڈولنگ کو learning_rate اور warmup_ratio کنٹرول کرتے ہیں."
        }
    ]}
/>

### 4. SFT تربیت کے دوران آپ کو کیا مانیٹر کرنا چاہیے؟

<Question
    choices={[
        {
            text: "مقداری میٹرکس اور معیاری آؤٹ پٹس دونوں",
            explain: "درست! دونوں اقسام کے میٹرکس کی نگرانی سے تمام ممکنہ مسائل کا پتا چلتا ہے.",
            correct: true
        },
        {
            text: "صرف تربیتی لاس",
            explain: "صرف تربیتی لاس ماڈل کی اچھی کارکردگی کو یقینی بنانے کے لیے کافی نہیں ہے."
        },
        {
            text: "صرف ماڈل کے آؤٹ پٹ کا معیار",
            explain: "اگرچہ اہم ہے، لیکن صرف معیاری جانچ تربیتی حرکیات کو نظر انداز کر دیتی ہے."
        }
    ]}
/>

### 5. تربیت کے دوران صحت مند کنورجنس کی نشاندہی کیا کرتی ہے؟

<Question
    choices={[
        {
            text: "تربیتی اور ویلیڈیشن لاس کے درمیان کم فاصلہ",
            explain: "درست! یہ ظاہر کرتا ہے کہ ماڈل ایسے عمومی پیٹرنز سیکھ رہا ہے جو نئے ڈیٹا پر بھی لاگو ہو سکیں.",
            correct: true
        },
        {
            text: "تربیتی لاس کا صفر تک پہنچ جانا",
            explain: "انتہائی کم لاس کی قیمتیں یادداشت کی نشاندہی کر سکتی ہیں بجائے حقیقی سیکھنے کے."
        },
        {
            text: "ویلیڈیشن لاس کا تربیتی لاس سے کم ہونا",
            explain: "یہ غیر معمولی ہوگا اور ممکنہ طور پر ویلیڈیشن سیٹ کے مسائل کی نشاندہی کرتا ہے."
        }
    ]}
/>

## 💐 شاندار کام!

آپ نے SFT کا استعمال کرتے ہوئے ماڈلز کو فائن-ٹیون کرنے کا طریقہ سیکھ لیا ہے! اپنی تعلیم جاری رکھنے کے لیے:
1. مختلف پیرامیٹرز کے ساتھ نوٹ بک آزما کر دیکھیں
2. دیگر ڈیٹاسیٹس کے ساتھ تجربہ کریں
3. کورس مواد میں بہتری کے لیے اپنا حصہ ڈالیں

## اضافی وسائل

- [TRL دستاویزات](https://huggingface.co/docs/trl)
- [SFT مثالوں کی ریپوزٹری](https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py)
- [فائن-ٹیوننگ بہترین طریقے](https://huggingface.co/docs/transformers/training)