<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/en/chapter11/section4.ipynb"},
]} />

# جانچ

فائن-ٹیون کیے گئے ماڈل، چاہے SFT کے ذریعے ہو یا LoRA SFT کے ذریعے، کو معیاری بینچ مارکس پر جانچنا ضروری ہے۔ ایک مشین لرننگ انجینئر کے طور پر آپ کو اپنے ہدفی ڈومین کے لیے متعلقہ جانچ کے آلات کا ایک مجموعہ برقرار رکھنا چاہیے۔ اس صفحہ میں، ہم کچھ عام بینچ مارکس کو دیکھیں گے اور یہ سمجھیں گے کہ اپنے ماڈل کی جانچ کے لیے انہیں کیسے استعمال کیا جائے۔ ہم یہ بھی دیکھیں گے کہ کس طرح اپنے مخصوص استعمال کے معاملے کے لیے کسٹم بینچ مارکس بنائے جائیں۔

## خودکار بینچ مارکس

خودکار بینچ مارکس مختلف کاموں اور صلاحیتوں کے لیے زبان کے ماڈلز کی جانچ کے معیاری آلات کے طور پر کام کرتے ہیں۔ اگرچہ یہ ماڈل کی کارکردگی کو سمجھنے کے لیے ایک مفید آغاز فراہم کرتے ہیں، لیکن یہ یاد رکھنا ضروری ہے کہ یہ ایک جامع جانچ حکمت عملی کا صرف ایک جزو ہیں۔

## خودکار بینچ مارکس کو سمجھنا

خودکار بینچ مارکس عموماً پہلے سے ترتیب دیے گئے ڈیٹاسیٹس پر مشتمل ہوتے ہیں جن میں پہلے سے طے شدہ کام اور جانچ کے میٹرکس شامل ہوتے ہیں۔ ان بینچ مارکس کا مقصد ماڈل کی صلاحیت کے مختلف پہلوؤں کا اندازہ لگانا ہے، بنیادی زبان کی سمجھ بوجھ سے لے کر پیچیدہ منطق تک۔ خودکار بینچ مارکس استعمال کرنے کا کلیدی فائدہ ان کی معیاری شکل ہے — یہ مختلف ماڈلز کے درمیان مسلسل موازنہ کی اجازت دیتے ہیں اور نتائج کو دوبارہ حاصل کرنے کے قابل بناتے ہیں۔

تاہم، یہ سمجھنا ضروری ہے کہ بینچ مارک کی کارکردگی ہمیشہ براہ راست حقیقی دنیا کی مؤثریت میں ترجمہ نہیں ہوتی۔ ایک ماڈل جو تعلیمی بینچ مارکس میں شاندار کارکردگی دکھاتا ہے وہ مخصوص ڈومین ایپلیکیشنز یا عملی استعمال کے معاملات میں مشکل پیش کر سکتا ہے۔

## عمومی علم کے بینچ مارکس

[MMLU](https://huggingface.co/datasets/cais/mmlu) (Massive Multitask Language Understanding) 57 مضامین میں علم کی جانچ کرتا ہے، جیسے کہ سائنس سے لے کر علوم انسانی تک۔ اگرچہ یہ جامع ہے، لیکن یہ مخصوص ڈومینز میں مطلوبہ مہارت کی گہرائی کی عکاسی نہیں کرتا۔ TruthfulQA ماڈل کے عمومی غلط فہمیوں کو دہرانے کے رجحان کا اندازہ لگاتا ہے، حالانکہ یہ تمام اقسام کی غلط معلومات کو نہیں پکڑ سکتا۔

## منطق کے بینچ مارکس

[BBH](https://huggingface.co/datasets/lukaemon/bbh) (Big Bench Hard) اور [GSM8K](https://huggingface.co/datasets/openai/gsm8k) پیچیدہ منطق کے کاموں پر مرکوز ہیں۔ BBH منطقی سوچ اور منصوبہ بندی کو پرکھتا ہے، جبکہ GSM8K خاص طور پر ریاضیاتی مسائل کے حل پر توجہ دیتا ہے۔ یہ بینچ مارکس تجزیاتی صلاحیتوں کا اندازہ لگانے میں مدد دیتے ہیں، لیکن حقیقی دنیا کے منطق کے باریک پہلوؤں کو مکمل طور پر نہیں پکڑ سکتے۔

## زبان کی سمجھ بوجھ

[HELM](https://github.com/stanford-crfm/helm) ایک جامع جانچ کا فریم ورک فراہم کرتا ہے۔ HELM جیسے بینچ مارکس زبان کی پروسیسنگ کی صلاحیتوں کے بارے میں بصیرت فراہم کرتے ہیں، جیسے کہ عمومی فہم، عالمی علم، اور منطق۔ لیکن یہ قدرتی گفتگو کی پیچیدگی یا مخصوص ڈومین کی اصطلاحات کی مکمل نمائندگی نہیں کر سکتے۔

## ڈومین مخصوص بینچ مارکس

آئیں چند بینچ مارکس کو دیکھتے ہیں جو ریاضی، کوڈنگ اور چیٹ جیسے مخصوص ڈومینز پر توجہ دیتے ہیں۔

[MATH بینچ مارک](https://huggingface.co/papers/2103.03874) ریاضیاتی منطق کے لیے ایک اہم جانچ کا آلہ ہے۔ یہ 12,500 مسائل پر مشتمل ہے جو ریاضی کے مقابلوں سے لیے گئے ہیں، جن میں الجبرا، جیومیٹری، نمبر تھیوری، شمار، احتمال اور مزید شامل ہیں۔ MATH کو خاص طور پر چیلنجنگ اس لیے بنایا گیا ہے کیونکہ یہ متعدد مراحل پر مشتمل منطق، رسمی ریاضیاتی نوٹیشن کی سمجھ بوجھ، اور مرحلہ وار حل پیدا کرنے کی صلاحیت کا مطالبہ کرتا ہے۔ آسان حسابی کاموں کے برعکس، MATH کے مسائل اکثر پیچیدہ مسئلہ حل کرنے کی حکمت عملیوں اور ریاضیاتی تصورات کے استعمال کا تقاضا کرتے ہیں۔

[HumanEval بینچ مارک](https://github.com/openai/human-eval) ایک کوڈنگ پر مرکوز جانچ ڈیٹاسیٹ ہے جس میں 164 پروگرامنگ مسائل شامل ہیں۔ یہ بینچ مارک ماڈل کی اس قابلِ پیمائش صلاحیت کا اندازہ لگاتا ہے کہ وہ دیے گئے پروگرامنگ کاموں کو حل کرنے کے لیے فنکشنلی درست Python کوڈ پیدا کر سکتا ہے۔ HumanEval کی خصوصیت یہ ہے کہ یہ صرف ریفرنس حل سے مماثلت تک محدود نہیں بلکہ حقیقی ٹیسٹ کیسز کے ذریعے کوڈ کی درستگی کو بھی پرکھتا ہے۔ مسائل بنیادی سٹرنگ مینپولیشن سے لے کر مزید پیچیدہ الگورتھمز اور ڈیٹا سٹرکچرز تک پھیلے ہوئے ہیں۔

[Alpaca Eval](https://tatsu-lab.github.io/alpaca_eval/) ایک خودکار جانچ کا فریم ورک ہے جو ہدایت پر عمل کرنے والے زبان کے ماڈلز کی کوالٹی کا اندازہ لگانے کے لیے ڈیزائن کیا گیا ہے۔ یہ GPT-4 کو ایک جج کے طور پر استعمال کرتا ہے تاکہ ماڈل کے جوابات کو مددگار، دیانتدار اور بے ضرر کے مختلف پہلوؤں پر پرکھا جا سکے۔ یہ فریم ورک 805 احتیاط سے منتخب کردہ پرامپٹس پر مشتمل ڈیٹاسیٹ کا استعمال کرتا ہے اور Claude، GPT-4، اور دیگر جیسے متعدد ریفرنس ماڈلز کے مقابلے میں جوابات کا موازنہ کرتا ہے۔ Alpaca Eval کی خاص بات یہ ہے کہ یہ بغیر انسانی تشریح کاروں کے مسلسل اور پیمانے پر جانچ فراہم کرتا ہے، جبکہ روایتی میٹرکس سے چھپے ہوئے باریک پہلوؤں کو بھی پکڑ لیتا ہے۔

## متبادل جانچ کے طریقے

بہت سی تنظیموں نے روایتی بینچ مارکس کی حدود کو دور کرنے کے لیے متبادل جانچ کے طریقے تیار کیے ہیں:

### LLM-as-Judge

ایک زبان کے ماڈل کو دوسرے کے جوابات کی جانچ کے لیے استعمال کرنا ایک مقبول طریقہ بن چکا ہے۔ یہ طریقہ روایتی میٹرکس کے مقابلے میں زیادہ باریک بینی کا فیڈ بیک فراہم کر سکتا ہے، اگرچہ اس کے اپنے تعصب اور حدود بھی ہیں۔

### Evaluation Arenas

[Chatbot Arena](https://lmarena.ai/) جیسے جانچ کے میدان ایک منفرد طریقہ پیش کرتے ہیں جہاں صارفین گمنام "لڑائیوں" میں دو LLMs کے درمیان سوالات پوچھتے ہیں اور یہ فیصلہ کرتے ہیں کہ کون سا ماڈل بہتر جواب دیتا ہے۔ یہ طریقہ حقیقی دنیا کے استعمال کے نمونوں اور ترجیحات کو متنوع اور چیلنجنگ سوالات کے ذریعے پکڑتا ہے، اور مطالعوں سے پتہ چلتا ہے کہ ہجوم کی رائے ماہر تجزیہ کاروں کی جانچ سے میل کھاتی ہے۔ اگرچہ طاقتور ہے، ان پلیٹ فارمز کی حدود میں ممکنہ صارفین کے تعصب، پرامپٹ کی تقسیم کا ایک جھکاو اور بنیادی طور پر مددگار ہونے پر توجہ شامل ہیں بجائے حفاظت کے پہلوؤں کے۔

### کسٹم بینچ مارک سوئٹس

تنظیمیں اکثر اپنے مخصوص استعمال کے معاملات کے مطابق اندرونی بینچ مارک سوئٹس تیار کرتی ہیں۔ ان میں ڈومین مخصوص علم کے امتحانات یا ایسے جائزہ منظرنامے شامل ہو سکتے ہیں جو حقیقی ڈپلائمنٹ کی شرائط کی عکاسی کرتے ہوں۔

## کسٹم جانچ

اگرچہ معیاری بینچ مارکس ایک مفید بنیاد فراہم کرتے ہیں، لیکن یہ آپ کا واحد جانچ کا طریقہ نہیں ہونا چاہیے۔ یہاں ایک جامع طریقہ کار تیار کرنے کا طریقہ ہے:

1. متعلقہ معیاری بینچ مارکس سے آغاز کریں تاکہ ایک بنیاد قائم ہو اور دوسرے ماڈلز کے ساتھ موازنہ ممکن ہو۔

2. اپنے استعمال کے معاملے کی مخصوص ضروریات اور چیلنجز کی نشاندہی کریں۔ آپ کا ماڈل کن کاموں کو انجام دے گا؟ کن غلطیوں کا ہونا سب سے زیادہ نقصان دہ ہوگا؟

3. اپنے حقیقی استعمال کے معاملے کی عکاسی کرنے والے کسٹم جانچ ڈیٹاسیٹس تیار کریں۔ اس میں شامل ہو سکتے ہیں:
   - آپ کے ڈومین سے حقیقی صارفین کے سوالات
   - آپ کو درپیش عام نادر کیسز
   - خاص طور پر چیلنجنگ منظرناموں کی مثالیں

4. ایک کثیر الطبقات جانچ کی حکمت عملی پر غور کریں:
   - فوری فیڈ بیک کے لیے خودکار میٹرکس
   - باریک بینی سے سمجھ بوجھ کے لیے انسانی جانچ
   - خصوصی ایپلیکیشنز کے لیے ڈومین کے ماہر کی جانچ
   - کنٹرول شدہ ماحول میں A/B ٹیسٹنگ

## کسٹم جانچ کو نافذ کرنا

اس حصے میں، ہم اپنے فائن-ٹیون ماڈل کی جانچ کا نفاذ کریں گے۔ ہم [`lighteval`](https://github.com/huggingface/lighteval) کا استعمال کرتے ہوئے اپنے فائن-ٹیون ماڈل کو معیاری بینچ مارکس پر جانچیں گے، جس میں لائبریری میں متعدد کام شامل ہیں۔ ہمیں صرف ان کاموں کو تعریف کرنا ہے جن کی جانچ کرنی ہے اور جانچ کے پیرامیٹرز کو ترتیب دینا ہے۔

LightEval کام ایک مخصوص فارمیٹ استعمال کرتے ہوئے بیان کیے جاتے ہیں:

```
{suite}|{task}|{num_few_shot}|{auto_reduce}
```

| پیرامیٹر      | تفصیل |
|----------------|-------------|
| `suite`        | بینچ مارک سوئٹ (مثلاً، 'mmlu', 'truthfulqa') |
| `task`         | سوئٹ کے اندر مخصوص کام (مثلاً، 'abstract_algebra') |
| `num_few_shot` | پرامپٹ میں شامل کرنے کے لیے مثالوں کی تعداد (0 کا مطلب zero-shot) |
| `auto_reduce`  | اگر پرامپٹ بہت طویل ہو تو few-shot مثالوں کو خودکار طور پر کم کرنا (0 یا 1) |

مثال: `"mmlu|abstract_algebra|0|0"` MMLU کے abstract_algebra کام پر zero-shot انفرنس کے ساتھ جانچ کرتا ہے۔

## مثال جانچ پائپ لائن

آئیں اپنے فائن-ٹیون ماڈل کے لیے ایک جانچ پائپ لائن ترتیب دیتے ہیں۔ ہم اپنے ماڈل کو میڈیسن کے ڈومین سے متعلق چند ضمنی کاموں پر جانچیں گے۔

یہاں VLLM بیک اینڈ کے ساتھ Lighteval استعمال کرتے ہوئے خودکار بینچ مارکس پر جانچ کی ایک مکمل مثال دی گئی ہے:

```bash
lighteval accelerate \
    "pretrained=your-model-name" \
    "mmlu|anatomy|0|0" \
    "mmlu|high_school_biology|0|0" \
    "mmlu|high_school_chemistry|0|0" \
    "mmlu|professional_medicine|0|0" \
    --max_samples 40 \
    --batch_size 1 \
    --output_path "./results" \
    --save_generations true
```

نتائج ایک ٹیبل کی شکل میں ظاہر ہوتی ہیں جس میں شامل ہوتا ہے:

```
|                  Task                  |Version|Metric|Value |   |Stderr|
|----------------------------------------|------:|------|-----:|---|-----:|
|all                                     |       |acc   |0.3333|±  |0.1169|
|leaderboard:mmlu:_average:5             |       |acc   |0.3400|±  |0.1121|
|leaderboard:mmlu:anatomy:5              |      0|acc   |0.4500|±  |0.1141|
|leaderboard:mmlu:high_school_biology:5  |      0|acc   |0.1500|±  |0.0819|
```

Lighteval مزید تفصیلی جانچ کے کاموں کے لیے Python API بھی فراہم کرتا ہے، جو نتائج کو زیادہ لچکدار طریقے سے منیج کرنے کے لیے مفید ہے۔ مزید معلومات کے لیے [Lighteval دستاویزات](https://huggingface.co/docs/lighteval/using-the-python-api) ملاحظہ کریں۔

<Tip>

✏️ **آزمائیں!** اپنے فائن-ٹیون ماڈل کو lighteval میں کسی مخصوص کام پر جانچیں۔

</Tip>

# باب کے آخر کا کوئز[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={11}
    classNames="absolute z-10 right-0 top-0"
/>

### 1. ماڈل کی جانچ کے لیے خودکار بینچ مارکس استعمال کرنے کے کلیدی فوائد کیا ہیں؟

<Question
	choices={[
		{
			text: "یہ حقیقی دنیا کی کارکردگی کے کامل میٹرکس فراہم کرتے ہیں",
			explain: "غلط! اگرچہ خودکار بینچ مارکس مفید ہیں، لیکن یہ ہمیشہ حقیقی دنیا کی کارکردگی میں براہ راست ترجمہ نہیں ہوتے۔"
		},
		{
			text: "یہ ماڈلز کے درمیان معیاری موازنہ کی اجازت دیتے ہیں اور قابلِ تکرار نتائج فراہم کرتے ہیں",
			explain: "درست! یہ خودکار بینچ مارکس کے کلیدی فوائد میں سے ایک ہے۔",
			correct: true
		},
		{
			text: "یہ کسی بھی دوسرے جانچ کے طریقے کی ضرورت کو ختم کر دیتے ہیں",
			explain: "غلط! خودکار بینچ مارکس کو جامع جانچ حکمت عملی کا ایک حصہ ہونا چاہیے، نہ کہ واحد طریقہ۔"
		}
	]}
/>

### 2. وہ کون سا بینچ مارک ہے جو 57 مختلف مضامین میں علم کی جانچ کرتا ہے؟

<Question
	choices={[
		{
			text: "BBH (Big Bench Hard)",
			explain: "غلط! BBH پیچیدہ منطق کے کاموں پر مرکوز ہے، نہ کہ وسیع مضامین کی جانچ پر۔"
		},
		{
			text: "GSM8K",
			explain: "غلط! GSM8K خاص طور پر ریاضیاتی مسائل کے حل پر مرکوز ہے۔"
		},
		{
			text: "MMLU",
			explain: "درست! MMLU (Massive Multitask Language Understanding) 57 مضامین میں علم کی جانچ کرتا ہے، جیسے کہ سائنس سے لے کر علوم انسانی تک۔",
			correct: true
		}
	]}
/>

### 3. LLM-as-Judge کیا ہے؟

<Question
	choices={[
		{
			text: "ایک زبان کے ماڈل کو دوسرے کے جوابات کی جانچ کے لیے استعمال کرنا",
			explain: "درست! یہ ایک متبادل جانچ کا طریقہ ہے جو زیادہ باریک بینی کا فیڈ بیک فراہم کر سکتا ہے۔",
			correct: true
		},
		{
			text: "ایک بینچ مارک جو عدالتی منطق کی جانچ کرتا ہے",
			explain: "غلط! LLM-as-Judge کا مطلب ہے کہ ایک ماڈل کو دوسرے کے جوابات کی جانچ کے لیے استعمال کرنا، نہ کہ عدالتی منطق کی جانچ کرنا۔"
		},
		{
			text: "ماڈلز کو قانونی ڈیٹاسیٹس پر تربیت دینے کا ایک طریقہ",
			explain: "غلط! یہ قانونی ڈیٹاسیٹس پر تربیت دینے سے متعلق نہیں ہے بلکہ ایک ماڈل کو دوسرے کے جوابات کی جانچ کے لیے استعمال کرنے سے متعلق ہے۔"
		}
	]}
/>

### 4. جامع جانچ حکمت عملی میں کیا شامل ہونا چاہیے؟

<Question
	choices={[
		{
			text: "صرف معیاری بینچ مارکس",
			explain: "غلط! جامع حکمت عملی میں متعدد جانچ کے طریقے شامل ہونے چاہئیں۔"
		},
		{
			text: "معیاری بینچ مارکس، کسٹم جانچ ڈیٹاسیٹس، اور ڈومین مخصوص ٹیسٹنگ",
			explain: "درست! جامع حکمت عملی میں متعدد سطحوں کی جانچ شامل ہونی چاہیے۔",
			correct: true
		},
		{
			text: "صرف آپ کے استعمال کے معاملے سے متعلق کسٹم ڈیٹاسیٹس",
			explain: "غلط! اگرچہ کسٹم ڈیٹاسیٹس اہم ہیں، لیکن انہیں واحد جانچ کا طریقہ نہیں ہونا چاہیے۔"
		}
	]}
/>

### 5. خودکار بینچ مارکس کی ایک محدودیت کیا ہے؟

<Question
	choices={[
		{
			text: "یہ چلانے کے لیے بہت مہنگے ہیں",
			explain: "غلط! لاگت عموماً خودکار بینچ مارکس کی بنیادی محدودیت نہیں ہے۔"
		},
		{
			text: "بینچ مارک کی کارکردگی ہمیشہ حقیقی دنیا کی مؤثریت میں براہ راست ترجمہ نہیں ہوتی",
			explain: "درست! یہ ایک اہم محدودیت ہے جس کا خیال رکھنا ضروری ہے جب خودکار بینچ مارکس استعمال کیے جائیں۔",
			correct: true
		},
		{
			text: "یہ صرف چھوٹے ماڈلز کی جانچ کر سکتے ہیں",
			explain: "غلط! خودکار بینچ مارکس مختلف سائز کے ماڈلز کی جانچ کے لیے استعمال کیے جا سکتے ہیں۔"
		}
	]}
/>

### 6. کسٹم جانچ ڈیٹاسیٹس بنانے کا مقصد کیا ہے؟

<Question
	choices={[
		{
			text: "اپنے مخصوص استعمال کے معاملے کی عکاسی کرنے اور اپنے ڈومین سے حقیقی صارفین کے سوالات شامل کرنے کے لیے",
			explain: "درست! کسٹم ڈیٹاسیٹس اس بات کو یقینی بناتے ہیں کہ جانچ آپ کی مخصوص ضروریات کے مطابق ہو۔",
			correct: true
		},
		{
			text: "معیاری بینچ مارکس کو مکمل طور پر تبدیل کرنے کے لیے",
			explain: "غلط! کسٹم ڈیٹاسیٹس کو معیاری بینچ مارکس کے ساتھ مل کر استعمال کرنا چاہیے، نہ کہ انہیں مکمل طور پر تبدیل کرنا چاہیے۔"
		},
		{
			text: "جانچ کو آسان بنانے کے لیے",
			explain: "غلط! کسٹم ڈیٹاسیٹس تیار کرنا اضافی محنت کا تقاضا کرتا ہے لیکن یہ زیادہ متعلقہ جانچ فراہم کرتے ہیں۔"
		}
	]}
/>