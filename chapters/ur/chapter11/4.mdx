<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/en/chapter11/section4.ipynb"},
]} />

# LoRA (لو-رینک ایڈاپٹیشن)

بڑے زبان کے ماڈلز کی فائن-ٹیوننگ ایک وسائل سے بھرپور عمل ہے۔ LoRA ایک ایسی تکنیک ہے جو ہمیں کم تعداد میں پیرامیٹرز کے ساتھ بڑے زبان کے ماڈلز کی فائن-ٹیوننگ کرنے کی اجازت دیتی ہے۔ یہ طریقہ توجہ کے وزن میں چھوٹی میٹرکس شامل کر کے اور ان کو بہتر بنا کر کام کرتا ہے، جس سے تربیتی پیرامیٹرز تقریباً 90% تک کم ہو جاتے ہیں۔

## LoRA کو سمجھنا

LoRA (لو-رینک ایڈاپٹیشن) ایک پیرامیٹر موثر فائن-ٹیوننگ تکنیک ہے جو پہلے سے تربیت یافتہ ماڈل کے وزن کو منجمد کر کے ماڈل کی تہوں میں تربیتی رینک ڈیکمپوزیشن میٹرکس شامل کر دیتی ہے۔ پورے ماڈل کے پیرامیٹرز کو تربیت دینے کی بجائے، LoRA وزن کی اپڈیٹس کو کم-رینک ڈیکمپوزیشن کے ذریعے چھوٹی میٹرکس میں تقسیم کر دیتی ہے، جس سے تربیتی پیرامیٹرز کی تعداد نمایاں طور پر کم ہو جاتی ہے اور ماڈل کی کارکردگی برقرار رہتی ہے۔ مثال کے طور پر، جب GPT-3 175B پر لاگو کیا گیا تو LoRA نے مکمل فائن-ٹیوننگ کے مقابلے میں تربیتی پیرامیٹرز کو 10,000x اور GPU میموری کی ضروریات کو 3x کم کر دیا۔ آپ [LoRA پیپر](https://arxiv.org/pdf/2106.09685) میں مزید پڑھ سکتے ہیں۔

LoRA ٹرانسفارمر لیئرز میں رینک ڈیکمپوزیشن میٹرکس کے جوڑے شامل کر کے کام کرتا ہے، جو عموماً توجہ کے وزن پر مرکوز ہوتا ہے۔ استنباط کے دوران، ان اڈاپٹر وزن کو بیس ماڈل کے ساتھ مرج کیا جا سکتا ہے، جس سے کوئی اضافی تاخیر پیدا نہیں ہوتی۔ LoRA خاص طور پر بڑے زبان کے ماڈلز کو مخصوص کاموں یا ڈومینز کے مطابق ڈھالنے کے لیے مفید ہے جبکہ وسائل کی ضروریات کو قابلِ انتظام رکھا جاتا ہے۔

## LoRA کے کلیدی فوائد

1. **میموری ایفیشنسی**:
   - صرف اڈاپٹر پیرامیٹرز GPU میموری میں محفوظ ہوتے ہیں
   - بیس ماڈل کے وزن منجمد رہتے ہیں اور کم پریسینشن میں لوڈ کیے جا سکتے ہیں
   - صارفین کے GPU پر بڑے ماڈلز کی فائن-ٹیوننگ کو ممکن بناتا ہے

2. **تربیتی خصوصیات**:
   - کم سیٹ اپ کے ساتھ نیٹو PEFT/LoRA انٹیگریشن
   - بہتر میموری ایفیشنسی کے لیے QLoRA (Quantized LoRA) کی حمایت

3. **اڈاپٹر مینجمنٹ**:
   - چیک پوائنٹس کے دوران اڈاپٹر وزن کی سیونگ
   - بیس ماڈل میں اڈاپٹرز کو مرج کرنے کی خصوصیات

## PEFT کے ساتھ LoRA اڈاپٹرز کو لوڈ کرنا

[PEFT](https://github.com/huggingface/peft) ایک ایسی لائبریری ہے جو PEFT طریقوں کو لوڈ اور مینج کرنے کے لیے ایک متحدہ انٹرفیس فراہم کرتی ہے، جس میں LoRA بھی شامل ہے۔ یہ آپ کو مختلف PEFT طریقوں کو آسانی سے لوڈ اور تبدیل کرنے کی اجازت دیتی ہے، جس سے مختلف فائن-ٹیوننگ تکنیکوں کے ساتھ تجربہ کرنا آسان ہو جاتا ہے۔

اڈاپٹرز کو `load_adapter()` کے ذریعے پہلے سے تربیت یافتہ ماڈل پر لوڈ کیا جا سکتا ہے، جو کہ مختلف ایسے اڈاپٹرز آزمانے کے لیے مفید ہے جن کے وزن مرج نہیں کیے گئے۔ `set_adapter()` فنکشن کے ذریعے فعال اڈاپٹر وزن سیٹ کریں۔ بیس ماڈل پر واپس جانے کے لیے، آپ `unload()` استعمال کر سکتے ہیں تاکہ تمام LoRA ماڈیولز کو ان لوڈ کیا جا سکے۔ اس سے مختلف ٹاسک مخصوص وزن کے درمیان آسانی سے تبدیلی ممکن ہو جاتی ہے۔  

```python
from peft import PeftModel, PeftConfig

config = PeftConfig.from_pretrained("ybelkada/opt-350m-lora")
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
lora_model = PeftModel.from_pretrained(model, "ybelkada/opt-350m-lora")
```

![lora_load_adapter](https://github.com/huggingface/smol-course/raw/main/3_parameter_efficient_finetuning/images/lora_adapter.png)

## LoRA کے ساتھ `trl` اور `SFTTrainer` کا استعمال کرتے ہوئے LLM کو فائن-ٹیون کرنا

`trl` کی [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) LoRA اڈاپٹرز کے ساتھ PEFT لائبریری کے ذریعے انٹیگریشن فراہم کرتی ہے۔ اس کا مطلب یہ ہے کہ ہم SFT کے طریقے سے ماڈل کو فائن-ٹیون کر سکتے ہیں، لیکن تربیت کے لیے ضروری پیرامیٹرز کی تعداد کو کم کرنے کے لیے LoRA استعمال کر سکتے ہیں۔

ہم اپنے مثال میں PEFT کی `LoRAConfig` کلاس استعمال کریں گے۔ سیٹ اپ کے لیے صرف چند کنفیگریشن اقدامات کی ضرورت ہوتی ہے:

1. LoRA کنفیگریشن کو ڈیفائن کریں (رینک، الفا، ڈراپ آؤٹ)
2. PEFT کنفیگریشن کے ساتھ SFTTrainer بنائیں
3. اڈاپٹر وزن کو تربیت دیں اور محفوظ کریں

## LoRA کنفیگریشن

آئیں LoRA کنفیگریشن اور کلیدی پیرامیٹرز کا جائزہ لیتے ہیں۔

| پیرامیٹر | تفصیل |
|-----------|-------------|
| `r` (رینک) | وہ جہت جس میں وزن کی اپڈیٹس کے لیے کم-رینک میٹرکس استعمال کیے جاتے ہیں۔ عموماً 4 سے 32 کے درمیان۔ کم قدریں زیادہ کمپریشن فراہم کرتی ہیں مگر ممکنہ طور پر اظہاریت کم ہو سکتی ہے۔ |
| `lora_alpha` | LoRA لیئرز کے لیے اسکیلنگ فیکٹر، عموماً رینک کی قیمت کے 2 گنا کے طور پر سیٹ کیا جاتا ہے۔ زیادہ قدریں زیادہ مضبوط ایڈاپٹیشن کے اثرات دیتی ہیں۔ |
| `lora_dropout` | LoRA لیئرز کے لیے ڈراپ آؤٹ پروببلٹی، عموماً 0.05-0.1۔ زیادہ قدریں تربیت کے دوران اوورفٹنگ کو روکنے میں مدد کرتی ہیں۔ |
| `bias` | بائس ٹرمز کی تربیت کو کنٹرول کرتا ہے۔ اختیارات "none", "all", یا "lora_only" ہیں۔ "none" میموری ایفیشنسی کے لیے سب سے عام ہے۔ |
| `target_modules` | وہ ماڈل ماڈیولز جن پر LoRA لاگو کیا جائے۔ "all-linear" یا مخصوص ماڈیولز جیسے "q_proj,v_proj" ہو سکتے ہیں۔ زیادہ ماڈیولز زیادہ ایڈاپٹیشن کی اجازت دیتے ہیں مگر میموری کے استعمال میں اضافہ کرتے ہیں۔ |

<Tip>
PEFT طریقے نافذ کرتے وقت، LoRA کے لیے چھوٹے رینک قدریں (4-8) سے آغاز کریں اور تربیتی لاس کی نگرانی کریں۔ اوورفٹنگ سے بچنے کے لیے ویلیڈیشن سیٹ کا استعمال کریں اور ممکن ہو تو مکمل فائن-ٹیوننگ بیس لائن کے ساتھ نتائج کا موازنہ کریں۔ مختلف طریقے کام کی نوعیت کے مطابق مختلف ہو سکتے ہیں، اس لیے تجربہ کرنا کلیدی ہے۔
</Tip>

## PEFT کے ساتھ TRL کا استعمال

PEFT طریقوں کو TRL کے ساتھ ملایا جا سکتا ہے تاکہ میموری کی ضروریات کو کم کیا جا سکے۔ ہم ماڈل کو لوڈ کرتے وقت `LoraConfig` کو پاس کر سکتے ہیں۔  

```python
from peft import LoraConfig

# TODO: LoRA پیرامیٹرز کی ترتیب دیں
# r: LoRA اپڈیٹ میٹرکس کے لیے رینک ڈائمینشن (چھوٹا = زیادہ کمپریشن)
rank_dimension = 6
# lora_alpha: LoRA لیئرز کے لیے اسکیلنگ فیکٹر (زیادہ = مضبوط ایڈاپٹیشن)
lora_alpha = 8
# lora_dropout: LoRA لیئرز کے لیے ڈراپ آؤٹ پروببلٹی (اوورفٹنگ سے بچاؤ میں مددگار)
lora_dropout = 0.05

peft_config = LoraConfig(
    r=rank_dimension,  # رینک ڈائمینشن - عموماً 4-32 کے درمیان
    lora_alpha=lora_alpha,  # LoRA اسکیلنگ فیکٹر - عموماً رینک کا 2 گنا
    lora_dropout=lora_dropout,  # LoRA لیئرز کے لیے ڈراپ آؤٹ پروببلٹی
    bias="none",  # LoRA کے لیے بائس ٹائپ۔ متعلقہ بائسز تربیت کے دوران اپڈیٹ ہوں گے۔
    target_modules="all-linear",  # جن ماڈیولز پر LoRA لاگو کیا جائے
    task_type="CAUSAL_LM",  # ماڈل آرکیٹیکچر کے لیے ٹاسک ٹائپ
)
```

اوپر، ہم نے `device_map="auto"` استعمال کیا تاکہ ماڈل کو خودکار طور پر درست ڈیوائس پر اسائن کیا جا سکے۔ آپ ماڈل کو مخصوص ڈیوائس پر دستی طور پر بھی اسائن کر سکتے ہیں `device_map={"": device_index}` استعمال کر کے۔

ہمیں LoRA کنفیگریشن کے ساتھ `SFTTrainer` کو بھی ڈیفائن کرنا ہوگا۔

```python
# LoRA کنفیگریشن کے ساتھ SFTTrainer بنائیں
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    peft_config=peft_config,  # LoRA کنفیگریشن
    max_seq_length=max_seq_length,  # زیادہ سے زیادہ سیکوئنس کی لمبائی
    processing_class=tokenizer,
)
```

<Tip>

✏️ **آزمائیں!** پچھلے سیکشن سے اپنے فائن-ٹیون شدہ ماڈل پر تعمیر کریں، لیکن اسے LoRA کے ساتھ فائن-ٹیون کریں۔ `HuggingFaceTB/smoltalk` ڈیٹاسیٹ کا استعمال کرتے ہوئے `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` ماڈل کو LoRA کنفیگریشن کے ساتھ فائن-ٹیون کریں جیسا کہ اوپر بیان کیا گیا ہے۔

</Tip>

## LoRA اڈاپٹرز کو مرج کرنا

LoRA کے ساتھ تربیت کے بعد، آپ بیس ماڈل میں اڈاپٹر وزن کو مرج کر کے اسے آسان ڈپلائمنٹ کے لیے ایک واحد ماڈل بنا سکتے ہیں۔ اس سے آپ کو استنباط کے دوران الگ سے اڈاپٹرز لوڈ کرنے کی ضرورت ختم ہو جاتی ہے۔

مرجنگ کے عمل میں میموری مینجمنٹ اور پریسینشن کا خیال رکھنا ضروری ہے۔ چونکہ آپ کو بیس ماڈل اور اڈاپٹر وزن دونوں کو بیک وقت لوڈ کرنا ہوگا، اس لیے یقینی بنائیں کہ کافی GPU/CPU میموری دستیاب ہو۔ `transformers` میں `device_map="auto"` استعمال کرنے سے آپ کے ہارڈویئر کی بنیاد پر ماڈل کے لیے درست ڈیوائس مل جائے گی۔

عمل کے دوران مستقل پریسینشن (مثلاً، float16) برقرار رکھیں، جو تربیت کے دوران استعمال شدہ پریسینشن سے میل کھاتی ہو اور مرج شدہ ماڈل کو اسی فارمیٹ میں سیو کریں تاکہ ڈپلائمنٹ میں آسانی ہو۔

## مرجنگ امپلیمنٹیشن

LoRA اڈاپٹر کی تربیت کے بعد، آپ اڈاپٹر وزن کو بیس ماڈل میں مرج کر سکتے ہیں۔ یہ کیسے کرنا ہے، ذیل میں دیکھیں:

```python
import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel

# 1. بیس ماڈل کو لوڈ کریں
base_model = AutoModelForCausalLM.from_pretrained(
    "base_model_name", torch_dtype=torch.float16, device_map="auto"
)

# 2. اڈاپٹر کے ساتھ PEFT ماڈل کو لوڈ کریں
peft_model = PeftModel.from_pretrained(
    base_model, "path/to/adapter", torch_dtype=torch.float16
)

# 3. بیس ماڈل کے ساتھ اڈاپٹر وزن کو مرج کریں
merged_model = peft_model.merge_and_unload()
```

اگر آپ کو محفوظ شدہ ماڈل میں سائز کے اختلافات کا سامنا ہو تو اس بات کو یقینی بنائیں کہ آپ ٹوکنائزر بھی سیو کر رہے ہیں:

```python
# ماڈل اور ٹوکنائزر دونوں کو سیو کریں
tokenizer = AutoTokenizer.from_pretrained("base_model_name")
merged_model.save_pretrained("path/to/save/merged_model")
tokenizer.save_pretrained("path/to/save/merged_model")
```

<Tip>

✏️ **آزمائیں!** اڈاپٹر وزن کو بیس ماڈل میں مرج کریں۔ `HuggingFaceTB/smoltalk` ڈیٹاسیٹ کا استعمال کرتے ہوئے `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` ماڈل کو LoRA کنفیگریشن کے ساتھ فائن-ٹیون کریں جیسا کہ اوپر بیان کیا گیا ہے۔

</Tip>


# Resources

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [Hugging Face blog post on PEFT](https://huggingface.co/blog/peft)