# ØºÙ„Ø·ÛŒ Ø¢Ù†Û’ Ù¾Ø± Ú©ÛŒØ§ Ú©Ø±ÛŒÚº[[what-to-do-when-you-get-an-error]]

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
]} />

Ø§Ø³ Ø­ØµÛ’ Ù…ÛŒÚº ÛÙ… Ø§ÙÙ† Ø¹Ø§Ù… ØºÙ„Ø·ÛŒÙˆÚº Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„ÛŒÚº Ú¯Û’ Ø¬Ùˆ Ø¢Ù¾ Ú©Û’ Ø­Ø§Ù„ ÛÛŒ Ù…ÛŒÚº ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ú©ÛŒÛ’ Ú¯Ø¦Û’ Transformer Ù…Ø§ÚˆÙ„ Ø³Û’ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº (predictions) Ø¬Ù†Ø±ÛŒÙ¹ Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ø³Ø§Ù…Ù†Û’ Ø¢ Ø³Ú©ØªÛŒ ÛÛŒÚºÛ” Ø§Ø³ Ø³Û’ Ø¢Ù¾ Ú©Ùˆ [section 4](/course/chapter8/section4) Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŒ Ø¬ÛØ§Úº ÛÙ… ØªØ±Ø¨ÛŒØªÛŒ Ù…Ø±Ø­Ù„Û’ (training phase) Ú©ÛŒ ÚˆÛŒØ¨Ú¯Ù†Ú¯ (debugging) Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº Ú¯Û’Û”

<Youtube id="DQ-CpJn6Rc4"/>

ÛÙ… Ù†Û’ Ø§Ø³ Ø­ØµÛ’ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© [template model repository](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) ØªÛŒØ§Ø± Ú©ÛŒØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø§Ú¯Ø± Ø¢Ù¾ Ø§Ø³ Ø¨Ø§Ø¨ Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ú©ÙˆÚˆ Ú©Ùˆ Ú†Ù„Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø¢Ù¾ Ú©Ùˆ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø§Ù¾Ù†Û’ Ø§Ú©Ø§Ø¤Ù†Ù¹ Ù¾Ø± [Hugging Face Hub](https://huggingface.co) Ù¾Ø± Ú©Ø§Ù¾ÛŒ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§ÛŒØ³Ø§ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Jupyter Ù†ÙˆÙ¹ Ø¨ÙÚ© Ù…ÛŒÚº Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ù…ÛŒÚº Ø³Û’ Ú©ÙˆØ¦ÛŒ Ø§ÛŒÚ© Ú©Ù…Ø§Ù†Úˆ Ú†Ù„Ø§Ø¦ÛŒÚº:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ÛŒØ§ Ø§Ù¾Ù†Û’ Ù¾Ø³Ù†Ø¯ÛŒØ¯Û Ù¹Ø±Ù…ÛŒÙ†Ù„ Ù…ÛŒÚº:

```bash
huggingface-cli login
```

ÛŒÛ Ø¢Ù¾ Ø³Û’ Ø¢Ù¾ Ú©Ø§ ÛŒÙˆØ²Ø±Ù†ÛŒÙ… Ø§ÙˆØ± Ù¾Ø§Ø³ ÙˆØ±Úˆ Ù…Ø§Ù†Ú¯Û’ Ú¯Ø§ØŒ Ø§ÙˆØ± *~/.cache/huggingface/* Ú©Û’ ØªØ­Øª Ø§ÛŒÚ© Ù¹ÙˆÚ©Ù† Ù…Ø­ÙÙˆØ¸ Ú©Ø± Ø¯Û’ Ú¯Ø§Û” Ø§ÛŒÚ© Ø¨Ø§Ø± Ù„Ø§Ú¯ Ø§Ù† ÛÙˆ Ø¬Ø§Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ØŒ Ø¢Ù¾ Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ ÙÙ†Ú©Ø´Ù† Ú©Û’ Ø°Ø±ÛŒØ¹Û’ template repository Ú©Ùˆ Ú©Ø§Ù¾ÛŒ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Clone the repo and extract the local path
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Create an empty repo on the Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Clone the empty repo
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copy files
    copy_tree(template_repo_dir, new_repo_dir)
    # Push to Hub
    repo.push_to_hub()
```

Ø§Ø¨ Ø¬Ø¨ Ø¢Ù¾ `copy_repository_template()` Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ ØªÙˆ ÛŒÛ Ø¢Ù¾ Ú©Û’ Ø§Ú©Ø§Ø¤Ù†Ù¹ Ú©Û’ ØªØ­Øª template repository Ú©ÛŒ Ø§ÛŒÚ© Ú©Ø§Ù¾ÛŒ Ø¨Ù†Ø§ Ø¯Û’ Ú¯Ø§Û”

## ğŸ¤— Transformers Ø³Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©ÛŒ ÚˆÛŒØ¨Ú¯Ù†Ú¯[[debugging-the-pipeline-from-transformers]]

Transformer Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ ÚˆÛŒØ¨Ú¯Ù†Ú¯ Ú©ÛŒ Ø´Ø§Ù†Ø¯Ø§Ø± Ø¯Ù†ÛŒØ§ Ù…ÛŒÚº Ø¯Ø§Ø®Ù„ ÛÙˆÙ†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø°ÛŒÙ„ Ú©Ø§ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…Û ØºÙˆØ± Ú©Ø±ÛŒÚº: Ø¢Ù¾ Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ø¬ÙˆØ§Ø¨ (question answering) Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù¾Ø± Ø§Ù¾Ù†Û’ Ø³Ø§ØªÚ¾ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº ØªØ§Ú©Û Ø§ÛŒÚ© Ø§ÛŒ-Ú©Ø§Ù…Ø±Ø³ ÙˆÛŒØ¨ Ø³Ø§Ø¦Ù¹ Ú©Û’ ØµØ§Ø±ÙÛŒÙ† Ú©Ùˆ ØµØ§Ø±ÙÛŒÙ† Ú©ÛŒ Ù…ØµÙ†ÙˆØ¹Ø§Øª Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨Ø§Øª ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ù…Ù„Û’Û” Ø¢Ù¾ Ú©Û’ Ø³Ø§ØªÚ¾ÛŒ Ù†Û’ Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒÚ© Ù¾ÛŒØºØ§Ù… Ø¨Ú¾ÛŒØ¬Ø§:

> G'day! I just ran an experiment using the techniques in [Chapter 7](/course/chapter7/7) of the Hugging Face course and got some great results on SQuAD! I think we can use this model as a starting point for our project. The model ID on the Hub is "lewtun/distillbert-base-uncased-finetuned-squad-d5716d28". Feel free to test it out :)

Ø§ÙˆØ± Ø¢Ù¾ Ú©Ø§ Ù¾ÛÙ„Ø§ Ø®ÛŒØ§Ù„ ÛŒÛ ÛÙˆØªØ§ ÛÛ’ Ú©Û Ø¢Ù¾ `pipeline` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº:

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Ø§ÙˆÛ Ù†ÛÛŒÚºØŒ Ú©Ú†Ú¾ ØºÙ„Ø· ÛÙˆ Ú¯ÛŒØ§ ÛÛ’! Ø§Ú¯Ø± Ø¢Ù¾ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù†Ú¯ Ù…ÛŒÚº Ù†Ø¦Û’ ÛÛŒÚºØŒ ØªÙˆ Ø§Ø³ Ù‚Ø³Ù… Ú©ÛŒ ØºÙ„Ø·ÛŒØ§Úº Ø´Ø±ÙˆØ¹ Ù…ÛŒÚº ØªÚ¾ÙˆÚ‘ÛŒ Ù…Ø¨ÛÙ… (cryptic) Ù„Ú¯ Ø³Ú©ØªÛŒ ÛÛŒÚº (Ù…Ø«Ù„Ø§Ù‹ `OSError` Ú©ÛŒØ§ ÛÛ’ØŸ)Û” ÛŒÛØ§Úº Ø¯Ú©Ú¾Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ ØºÙ„Ø·ÛŒ Ø§ÛŒÚ© Ø¨ÛØª Ø¨Ú‘ÛŒ Ø§ÛŒØ±Ø± Ø±Ù¾ÙˆØ±Ù¹ (error report) Ú©Ø§ Ø¢Ø®Ø±ÛŒ Ø­ØµÛ ÛÛ’ Ø¬Ø³Û’ _Python traceback_ (ÛŒØ§ stack trace) Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± Ø¢Ù¾ ÛŒÛ Ú©ÙˆÚˆ Google Colab Ù¾Ø± Ú†Ù„Ø§ Ø±ÛÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ú©Ú†Ú¾ Ø§Ø³ Ø·Ø±Ø­ Ú©Ø§ Ø§Ø³Ú©Ø±ÛŒÙ† Ø´Ø§Ù¹ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png" alt="Ø§ÛŒÚ© Ù¾Ø§Ø¦ØªÚ¾ÙˆÙ† Ù¹Ø±ÛŒØ³ Ø¨ÛŒÚ©." width="100%"/>
</div>

Ø§Ù† Ø±Ù¾ÙˆØ±Ù¹Ø³ Ù…ÛŒÚº Ø¨ÛØª Ø³ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø´Ø§Ù…Ù„ ÛÙˆØªÛŒ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ø¦ÛŒÛ’ Ù…Ù„ Ú©Ø± Ø§ÛÙ… Ù†Ú©Ø§Øª Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„ØªÛ’ ÛÛŒÚºÛ” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø¨Ø§Øª ÛŒÛ ÛÛ’ Ú©Û tracebacks Ú©Ùˆ _Ù†ÛŒÚ†Û’ Ø³Û’ Ø§ÙˆÙ¾Ø±_ Ù¾Ú‘Ú¾Ù†Ø§ Ú†Ø§ÛÛŒÛ’Û” Ø§Ú¯Ø±Ú†Û ÛŒÛ Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ØªÙ† Ú©Ùˆ Ø§ÙˆÙ¾Ø± Ø³Û’ Ù†ÛŒÚ†Û’ Ù¾Ú‘Ú¾Ù†Û’ Ú©Û’ Ù…Ø¹Ù…ÙˆÙ„ Ø³Û’ Ù…Ø®ØªÙ„Ù Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØªØ§ ÛÛ’ØŒ Ù…Ú¯Ø± ÛŒÛ Ø§Ø³ Ø­Ù‚ÛŒÙ‚Øª Ú©ÛŒ Ø¹Ú©Ø§Ø³ÛŒ Ú©Ø±ØªØ§ ÛÛ’ Ú©Û traceback Ø§Ù† ÙÙ†Ú©Ø´Ù† Ú©Ø§Ù„Ø² (function calls) Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ø¯Ú©Ú¾Ø§ØªØ§ ÛÛ’ Ø¬Ùˆ `pipeline` Ù…Ø§ÚˆÙ„ Ø§ÙˆØ± tokenizer Ú©Ùˆ ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ú©Ø±ØªØ§ ÛÛ’Û” (Ù…Ø²ÛŒØ¯ ØªÙØµÛŒÙ„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ [Chapter 2](/course/chapter2) Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú©Û `pipeline` Ú©Ø³ Ø·Ø±Ø­ Ø§Ù†Ø¯Ø±ÙˆÙ†ÛŒ Ø·ÙˆØ± Ù¾Ø± Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’Û”)

<Tip>
ğŸš¨ Ú©ÛŒØ§ Ø¢Ù¾ Ù†Û’ Google Colab Ú©Û’ traceback Ù…ÛŒÚº "6 frames" Ú©Û’ Ú¯Ø±Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒÙ„Û’ Ø¨Ø§Ú©Ø³ Ú©Ùˆ Ø¯ÛŒÚ©Ú¾Ø§ØŸ ÛŒÛ Colab Ú©ÛŒ Ø§ÛŒÚ© Ø®Ø§Øµ Ø®ØµÙˆØµÛŒØª ÛÛ’ Ø¬Ùˆ traceback Ú©Ùˆ "frames" Ù…ÛŒÚº Ú©Ù…Ù¾Ø±ÛŒØ³ Ú©Ø± Ø¯ÛŒØªÛŒ ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒØ±Ø± Ú©Ø§ Ù…Ø§Ø®Ø° (source) Ù…Ø¹Ù„ÙˆÙ… Ù†ÛÛŒÚº ÛÙˆ Ø±ÛØ§ØŒ ØªÙˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ø¦ÛŒÚº Ú©Û Ø¢Ù¾ Ù…Ú©Ù…Ù„ traceback Ú©Ùˆ Ø§Ù† Ø¯Ùˆ Ú†Ú¾ÙˆÙ¹Û’ ØªÛŒØ± (arrows) Ù¾Ø± Ú©Ù„Ú© Ú©Ø± Ú©Û’ Ú©Ú¾ÙˆÙ„ÛŒÚºÛ”
</Tip>

Ø§Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛŒÛ ÛÛ’ Ú©Û traceback Ú©ÛŒ Ø¢Ø®Ø±ÛŒ Ù„Ø§Ø¦Ù† Ø¢Ø®Ø±ÛŒ Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù… Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø±ØªÛŒ ÛÛ’ Ø§ÙˆØ± Ø§ÙØ³ Ù…ÛŒÚº Ø§ÙØªÙ†ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÛÙˆØªÛŒ ÛÛŒÚº Ú©Û Ø¢Ù¾ Ù…Ø³Ø¦Ù„Û’ Ú©Ø§ Ù…Ø§Ø®Ø° ØªÙ„Ø§Ø´ Ú©Ø± Ø³Ú©ÛŒÚºÛ” Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚºØŒ exception Ú©ÛŒ Ù‚Ø³Ù… `OSError` ÛÛ’ØŒ Ø¬Ùˆ Ú©Û Ø³Ø³Ù¹Ù… Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø§ÛŒØ±Ø± Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø±ØªØ§ ÛÛ’Û” Ø§Ú¯Ø± ÛÙ… Ø§Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¯ÛŒØ§ Ú¯ÛŒØ§ Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù… Ù¾Ú‘Ú¾ÛŒÚº ØªÙˆ ÛÙ…ÛŒÚº Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØªØ§ ÛÛ’ Ú©Û Ù…Ø§ÚˆÙ„ Ú©Û’ *config.json* ÙØ§Ø¦Ù„ Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ Ù…Ø³Ø¦Ù„Û ÛÛ’ØŒ Ø§ÙˆØ± ÛÙ…ÛŒÚº Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¯Ùˆ ØªØ¬Ø§ÙˆÛŒØ² Ø¯ÛŒ Ú¯Ø¦ÛŒ ÛÛŒÚº:

```python out
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

<Tip warning={true}>
ğŸš¨ ÛŒÛØ§Úº ÛÙ… Ø¬Ùˆ Ø·Ø±ÛŒÙ‚Û Ø§Ø®ØªÛŒØ§Ø± Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº ÙˆÛ Ú©Ø§Ù…Ù„ (foolproof) Ù†ÛÛŒÚº ÛÛ’ØŒ Ú©ÛŒÙˆÙ†Ú©Û Ø¢Ù¾ Ú©Û’ Ø³Ø§ØªÚ¾ÛŒ Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ú©Ø±Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ `distilbert-base-uncased` Ú©ÛŒ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ÛŒ (tweak) Ú©Ø± Ø¯ÛŒ ÛÙˆ Ø³Ú©ØªÛŒ ÛÛ’Û” Ø­Ù‚ÛŒÙ‚ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒÚºØŒ ÛÙ…ÛŒÚº Ù¾ÛÙ„Û’ Ø§Ù† Ø³Û’ Ú†ÛŒÚ© Ú©Ø±Ù†Ø§ Ú†Ø§ÛÛŒÛ’ØŒ Ù…Ú¯Ø± Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ú©Û’ Ù„ÛŒÛ’ ÛÙ… ÙØ±Ø¶ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø§Ù†ÛÙˆÚº Ù†Û’ ÚˆÛŒÙØ§Ù„Ù¹ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒÛ”
</Tip>

ÛÙ… Ù¾Ú¾Ø± Ø§Ø³ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ú©Ùˆ Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ repository Ù…ÛŒÚº `push_to_hub()` ÙÙ†Ú©Ø´Ù† Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¨Ú¾ÛŒØ¬ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

Ø§Ø¨ ÛÙ… ÛŒÛ Ù¹ÛŒØ³Ù¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ø¢ÛŒØ§ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ú¯ÛŒØ§ ÛÛ’ ÛŒØ§ Ù†ÛÛŒÚºØŒ `main` Ø¨Ø±Ø§Ù†Ú† Ú©Û’ ØªØ§Ø²Û ØªØ±ÛŒÙ† commit Ø³Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ù„ÙˆÚˆ Ú©Ø± Ú©Û’:

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

ğŸ¤— Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)
```

```python out
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}
```

Ø²Ø¨Ø±Ø¯Ø³ØªØŒ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ú¯ÛŒØ§! Ø§Ø¨ Ø¢Ø¦ÛŒÚº Ø§Ø³ Ø¨Ø§Øª Ú©Ø§ Ø®Ù„Ø§ØµÛ (recap) Ú©Ø±ÛŒÚº Ú©Û Ø¢Ù¾ Ù†Û’ Ú©ÛŒØ§ Ø³ÛŒÚ©Ú¾Ø§:

- Python Ù…ÛŒÚº Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù…Ø§Øª Ú©Ùˆ _tracebacks_ Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø§Ù†ÛÛŒÚº Ù†ÛŒÚ†Û’ Ø³Û’ Ø§ÙˆÙ¾Ø± Ù¾Ú‘Ú¾Ø§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù… Ú©ÛŒ Ø¢Ø®Ø±ÛŒ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº Ø¹Ù…ÙˆÙ…Ø§Ù‹ ÙˆÛ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÛÙˆØªÛŒ ÛÛŒÚº Ø¬Ù† Ø³Û’ Ù…Ø³Ø¦Ù„Û’ Ú©Ø§ Ù…Ø§Ø®Ø° Ù…Ø¹Ù„ÙˆÙ… Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’Û”
- Ø§Ú¯Ø± Ø¢Ø®Ø±ÛŒ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº Ú©Ø§ÙÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Û ÛÙˆÚº ØªÙˆ traceback Ú©Û’ Ø§ÙˆÙ¾Ø± Ú©ÛŒ Ù„Ø§Ø¦Ù†ÙˆÚº Ú©Ùˆ Ù¾Ú‘Ú¾ÛŒÚº ØªØ§Ú©Û ÛŒÛ Ù¾ØªÛ Ú†Ù„ Ø³Ú©Û’ Ú©Û Ø³ÙˆØ±Ø³ Ú©ÙˆÚˆ Ú©Û’ Ú©Ø³ Ø­ØµÛ’ Ù…ÛŒÚº Ø§ÛŒØ±Ø± Ø¢ Ø±ÛØ§ ÛÛ’Û”
- Ø§Ú¯Ø± Ú©ÙˆØ¦ÛŒ Ø¨Ú¾ÛŒ Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù… Ù…Ø³Ø¦Ù„Û Ø­Ù„ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ù†Û Ø¯Û’ ØªÙˆ Ù…Ù„ØªÛŒ Ø¬Ù„ØªÛŒ Ù…Ø³Ø¦Ù„Û’ Ú©Û’ Ø­Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù† Ù„Ø§Ø¦Ù† Ø³Ø±Ú† Ú©Ø±ÛŒÚºÛ”
- `huggingface_hub` Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§ÛŒØ³Û’ Ù¹ÙˆÙ„Ø² Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹Û ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’ Ø¬Ù†ÛÛŒÚº Ø¢Ù¾ Hub Ù¾Ø± Ù…ÙˆØ¬ÙˆØ¯ repositories Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¹Ø§Ù…Ù„ Ø§ÙˆØ± ÚˆÛŒØ¨Ú¯Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

Ø§Ø¨ Ø¬Ø¨ Ø¢Ù¾ Ø¬Ø§Ù† Ú†Ú©Û’ ÛÛŒÚº Ú©Û Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Ùˆ Ú©ÛŒØ³Û’ ÚˆÛŒØ¨Ú¯ Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ØŒ ØªÙˆ Ø¢Ø¦ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©Û’ ÙØ§Ø±ÙˆØ±Úˆ Ù¾Ø§Ø³ (forward pass) Ù…ÛŒÚº Ø§ÛŒÚ© Ù…Ø´Ú©Ù„ Ù…Ø«Ø§Ù„ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºÛ”

## Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ÙØ§Ø±ÙˆØ±Úˆ Ù¾Ø§Ø³ Ú©ÛŒ ÚˆÛŒØ¨Ú¯Ù†Ú¯[[debugging-the-forward-pass-of-your-model]]

Ø§Ú¯Ø±Ú†Û `pipeline` ØªÛŒØ²ÛŒ Ø³Û’ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº Ø¬Ù†Ø±ÛŒÙ¹ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØª Ù…ÙÛŒØ¯ ÛÛ’ØŒ Ø¨Ø¹Ø¶ Ø§ÙˆÙ‚Ø§Øª Ø¢Ù¾ Ú©Ùˆ Ù…Ø§ÚˆÙ„ Ú©Û’ logits ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù¾Ú‘ Ø³Ú©ØªÛŒ ÛÛ’ (Ù…Ø«Ù„Ø§Ù‹ Ø§Ú¯Ø± Ø¢Ù¾ Ú©ÙˆØ¦ÛŒ Ú©Ø³Ù¹Ù… Ù¾ÙˆØ³Ù¹-Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ (post-processing) Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº)Û” Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº Ú©ÛŒØ§ Ù…Ø³Ø¦Ù„Û Ù¾ÛŒØ¯Ø§ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ØŒ ÛŒÛ Ø¬Ø§Ù†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛÙ„Û’ ÛÙ… Ø§Ù¾Ù†Û’ `pipeline` Ø³Û’ Ù…Ø§ÚˆÙ„ Ø§ÙˆØ± tokenizer Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
tokenizer = reader.tokenizer
model = reader.model
```

Ø§Ø¨ ÛÙ…ÛŒÚº Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’ØŒ ØªÙˆ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û Ú©ÛŒØ§ ÛÙ…Ø§Ø±Û’ Ù¾Ø³Ù†Ø¯ÛŒØ¯Û frameworks Ø³Ù¾ÙˆØ±Ù¹ Ú©ÛŒÛ’ Ø¬Ø§ØªÛ’ ÛÛŒÚº:

```python
question = "Which frameworks can I use?"
```

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ [Chapter 7](/course/chapter7) Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ØŒ Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± ÛÙ…ÛŒÚº Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ù…Ø±Ø§Ø­Ù„ Ø§Ø®ØªÛŒØ§Ø± Ú©Ø±Ù†Û’ ÛÙˆØªÛ’ ÛÛŒÚº: Ø§Ù† Ù¾Ù¹Ø³ Ú©Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² Ú©Ø±Ù†Ø§ØŒ Ø¢ØºØ§Ø² Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ logits Ù†Ú©Ø§Ù„Ù†Ø§ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø­ØµÛ’ Ú©Ùˆ ÚˆÛŒ Ú©ÙˆÚˆ Ú©Ø±Ù†Ø§:

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], [] 

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724 
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,
    
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], [] 

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]
    
AttributeError: 'list' object has no attribute 'size'
"""
 
Ø§ÙØ³ÙˆØ³ØŒ Ø§ÛŒØ³Ø§ Ù„Ú¯ØªØ§ ÛÛ’ Ú©Û ÛÙ…Ø§Ø±Û’ Ú©ÙˆÚˆ Ù…ÛŒÚº Ø¨Ú¯ ÛÛ’! Ù…Ú¯Ø± ÛÙ… ØªÚ¾ÙˆÚ‘ÛŒ ÚˆÛŒØ¨Ú¯Ù†Ú¯ Ø³Û’ Ú¯Ú¾Ø¨Ø±Ø§ØªÛ’ Ù†ÛÛŒÚºÛ” Ø¢Ù¾ Ù†ÙˆÙ¹ Ø¨ÙÚ© Ù…ÛŒÚº Python debugger Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

<Youtube id="rSPyvPw0p9k"/>

ÛŒØ§ Ù¹Ø±Ù…ÛŒÙ†Ù„ Ù…ÛŒÚº:

<Youtube id="5PkZ4rbHL6c"/>

ÛŒÛØ§ÚºØŒ Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù… Ù¾Ú‘Ú¾ Ú©Ø± ÛÙ…ÛŒÚº Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØªØ§ ÛÛ’ Ú©Û `'list' object has no attribute 'size'`ØŒ Ø§ÙˆØ± Ø§ÛŒÚ© `-->` ØªÛŒØ± Ø§Ø³ Ù„Ø§Ø¦Ù† Ú©ÛŒ Ø·Ø±Ù Ø§Ø´Ø§Ø±Û Ú©Ø± Ø±ÛØ§ ÛÛ’ Ø¬ÛØ§Úº `model(**inputs)` Ù…ÛŒÚº Ù…Ø³Ø¦Ù„Û Ù¾ÛŒØ¯Ø§ ÛÙˆØ§Û” Ø¢Ù¾ Ø§Ø³ Ú©ÙˆÚˆ Ú©Ùˆ Python debugger Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø§Ù†Ù¹Ø±Ø§ÛŒÚ©Ù¹Ùˆ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ ÚˆÛŒØ¨Ú¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù…Ú¯Ø± Ø§Ø¨Ú¾ÛŒ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… Ø¨Ø³ `inputs` Ú©Ø§ Ø§ÛŒÚ© Ø­ØµÛ Ù¾Ø±Ù†Ù¹ Ú©Ø± Ú©Û’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ú©ÛŒØ§ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’:

```python
inputs["input_ids"][:5]
```

```python out
[101, 2029, 7705, 2015, 2064]
```

ÛŒÛ ÛŒÙ‚ÛŒÙ†ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø§ÛŒÚ© Ø¹Ø§Ù… Python `list` Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØªØ§ ÛÛ’ØŒ Ù…Ú¯Ø± Ø¢Ø¦ÛŒÚº Ø§Ø³ Ú©ÛŒ Ù‚Ø³Ù… (type) Ø¨Ú¾ÛŒ Ú†ÛŒÚ© Ú©Ø± Ù„ÛŒØªÛ’ ÛÛŒÚº:

```python
type(inputs["input_ids"])
```

```python out
list
```

Ø¬ÛŒ ÛØ§ÚºØŒ ÛŒÛ ÙˆØ§Ù‚Ø¹ÛŒ Ø§ÛŒÚ© Python `list` ÛÛ’Û” ØªÙˆ Ù…Ø³Ø¦Ù„Û Ú©ÛØ§Úº ÛÛ’ØŸ ÛŒØ§Ø¯ Ø±Ú©Ú¾ÛŒÚº Ú©Û [Chapter 2](/course/chapter2) Ø³Û’ ÛÙ… Ù†Û’ Ø³ÛŒÚ©Ú¾Ø§ Ú©Û ğŸ¤— Transformers Ú©ÛŒ `AutoModelForXxx` Ú©Ù„Ø§Ø³Ø² _tensors_ (PyTorch ÛŒØ§ TensorFlow Ù…ÛŒÚº) Ù¾Ø± Ú©Ø§Ù… Ú©Ø±ØªÛŒ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± tensor Ú©ÛŒ Ø¬ÛØª Ù…Ø¹Ù„ÙˆÙ… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ `Tensor.size()` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø¢Ø¦ÛŒÚº Ø¯ÙˆØ¨Ø§Ø±Û traceback Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºØŒ ØªØ§Ú©Û Ù¾ØªÛ Ú†Ù„ Ø³Ú©Û’ Ú©Û Ú©Ø³ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº exception Ù¾ÛŒØ¯Ø§ ÛÙˆØ§:

```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]
```

Ø§ÛŒØ³Ø§ Ù„Ú¯ØªØ§ ÛÛ’ Ú©Û ÛÙ…Ø§Ø±Û’ Ú©ÙˆÚˆ Ù†Û’ `input_ids.size()` Ú©Ø§Ù„ Ú©ÛŒØŒ Ø¬Ùˆ Ú©Û Ø§ÛŒÚ© Python `list` Ù¾Ø± Ú©Ø§Ù… Ù†ÛÛŒÚº Ú©Ø±ØªØ§ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ ØµØ±Ù Ø§ÛŒÚ© Ú©Ù†Ù¹ÛŒÙ†Ø± ÛÛ’Û” Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Ùˆ Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ§ Ú©Ø±ÛŒÚºØŸ Stack Overflow Ù¾Ø± Ø§Ø³ Ø§ÛŒØ±Ø± Ù¾ÛŒØºØ§Ù… Ú©ÛŒ ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ø³Û’ Ú©Ø¦ÛŒ Ù…ØªØ¹Ù„Ù‚Û [hits](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f) Ù…Ù„ØªÛ’ ÛÛŒÚºÛ” Ù¾ÛÙ„Û’ ÛÙ¹ Ù¾Ø± Ú©Ù„Ú© Ú©Ø±Ù†Û’ Ø³Û’ Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’ Ø¬Ùˆ ÛÙ…Ø§Ø±Û’ Ù…Ø³Ø¦Ù„Û’ Ø³Û’ Ù…Ù„ØªØ§ Ø¬Ù„ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ù†ÛŒÚ†Û’ Ø¯ÛŒ Ú¯Ø¦ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ† Ø´Ø§Ù¹ Ù…ÛŒÚº Ø¯Ú©Ú¾Ø§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png" alt="Stack Overflow Ú©Ø§ Ø§ÛŒÚ© Ø¬ÙˆØ§Ø¨." width="100%"/>
</div>

Ø¬ÙˆØ§Ø¨ Ù…ÛŒÚº ØªØ¬ÙˆÛŒØ² Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ Ú©Û tokenizer Ù…ÛŒÚº `return_tensors='pt'` Ø´Ø§Ù…Ù„ Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ØŒ ØªÙˆ Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û Ø¢ÛŒØ§ ÛŒÛ ÛÙ…Ø§Ø±Û’ Ù„ÛŒÛ’ Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’:

```python out
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""
```

Ø¨ÛØª Ø§Ú†Ú¾Ø§ØŒ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ú¯ÛŒØ§! ÛŒÛ Ø§ÛŒÚ© Ø´Ø§Ù†Ø¯Ø§Ø± Ù…Ø«Ø§Ù„ ÛÛ’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ Stack Overflow Ù…ÙÛŒØ¯ Ø«Ø§Ø¨Øª ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’: Ù…Ù„ØªÛ’ Ø¬Ù„ØªÛ’ Ù…Ø³Ø¦Ù„Û’ Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø± Ú©Û’ØŒ ÛÙ… Ù†Û’ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ú©Û’ ØªØ¬Ø±Ø¨Û’ Ø³Û’ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ÛŒØ§Û” ØªØ§ÛÙ…ØŒ Ø§ÛŒØ³Ø§ Ø³Ø±Ú† ÛÙ…ÛŒØ´Û Ù…ØªØ¹Ù„Ù‚Û Ø¬ÙˆØ§Ø¨ ÙØ±Ø§ÛÙ… Ù†ÛÛŒÚº Ú©Ø±ØªØ§ØŒ ØªÙˆ Ø§ÛŒØ³ÛŒ ØµÙˆØ±Øª Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒØ§ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŸ Ø®ÙˆØ´ Ù‚Ø³Ù…ØªÛŒ Ø³Û’ØŒ [Hugging Face forums](https://discuss.huggingface.co/) Ù¾Ø± ÚˆÙˆÛŒÙ„Ù¾Ø±Ø² Ú©ÛŒ Ø§ÛŒÚ© Ø®ÙˆØ´ Ø¢Ø¦Ù†Ø¯ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ Ø¬Ùˆ Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛŒ ÛÛ’! Ø§Ú¯Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚºØŒ ÛÙ… Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ø¢Ù¾ Ø§Ú†Ú¾Û’ forum Ø³ÙˆØ§Ù„Ø§Øª Ú©ÛŒØ³Û’ Ø¨Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ù† Ú©Û’ Ø¬ÙˆØ§Ø¨ Ù…Ù„Ù†Û’ Ú©Û’ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø²ÛŒØ§Ø¯Û ÛÙˆÚºÛ”