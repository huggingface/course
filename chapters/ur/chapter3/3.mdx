```mdx
<FrameworkSwitchCourse {fw} />

# Trainer API کے ساتھ ماڈل کی فائن ٹیوننگ[[fine-tuning-a-model-with-the-trainer-api]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

🤗 Transformers آپ کے ڈیٹاسیٹ پر فراہم کردہ پری ٹرینڈ ماڈلز کو فائن ٹیون کرنے میں مدد کے لیے ایک `Trainer` کلاس فراہم کرتا ہے۔ جب آپ نے پچھلے سیکشن میں تمام ڈیٹا پری پروسیسنگ کا کام مکمل کر لیا ہو، تو `Trainer` کی تعریف کرنے کے لیے صرف چند قدم باقی رہ جاتے ہیں۔ سب سے مشکل حصہ غالباً `Trainer.train()` کو چلانے کے لیے ماحول تیار کرنا ہوگا، کیونکہ یہ CPU پر بہت آہستہ چلتا ہے۔ اگر آپ کے پاس GPU سیٹ اپ نہیں ہے، تو آپ [Google Colab](https://colab.research.google.com/) پر مفت GPU یا TPU تک رسائی حاصل کر سکتے ہیں۔

نیچے دی گئی کوڈ کی مثالیں یہ فرض کرتی ہیں کہ آپ نے پچھلے سیکشن کی مثالیں پہلے ہی چلا لی ہیں۔ یہاں ایک مختصر خلاصہ پیش کیا گیا ہے کہ آپ کو کیا درکار ہے:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### تربیت[[training]]

`Trainer` کی تعریف کرنے سے پہلے پہلا قدم یہ ہے کہ ایک `TrainingArguments` کلاس کی تعریف کی جائے جو تربیت اور جائزہ کے لیے `Trainer` کے استعمال کردہ تمام ہائپر پیرامیٹرز کو شامل کرے۔ آپ کو صرف ایک ایسی ڈائریکٹری فراہم کرنی ہے جہاں تربیت یافتہ ماڈل اور راستے میں بننے والے چیک پوائنٹس محفوظ کیے جائیں۔ باقی سب کے لیے، آپ ڈیفالٹس ہی چھوڑ سکتے ہیں، جو بنیادی فائن ٹیوننگ کے لیے کافی اچھے کام کرتے ہیں۔

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<Tip>

💡 اگر آپ تربیت کے دوران اپنے ماڈل کو خود بخود Hub پر اپ لوڈ کرنا چاہتے ہیں، تو `TrainingArguments` میں `push_to_hub=True` پاس کریں۔ ہم اس کے بارے میں مزید [Chapter 4](/course/chapter4/3) میں جانیں گے۔

</Tip>

دوسرا قدم ہمارے ماڈل کی تعریف ہے۔ جیسا کہ [پچھلے باب](/course/chapter2) میں بیان کیا گیا، ہم `AutoModelForSequenceClassification` کلاس استعمال کریں گے، جس میں دو لیبل شامل ہوں گے:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

آپ نوٹ کریں گے کہ [Chapter 2](/course/chapter2) کے برخلاف، اس پری ٹرینڈ ماڈل کی انسٹینشی ایشن کے بعد وارننگ ظاہر ہوتی ہے۔ اس کی وجہ یہ ہے کہ BERT کو جملوں کے جوڑوں کی کلاسفیکیشن کے لیے پری ٹرینڈ نہیں کیا گیا ہے، لہٰذا پری ٹرینڈ ماڈل کا ہیڈ نکال دیا گیا ہے اور اس کی جگہ سیکوئنس کلاسفیکیشن کے لیے موزوں نیا ہیڈ شامل کیا گیا ہے۔ وارننگز ظاہر کرتی ہیں کہ کچھ وزن استعمال نہیں کیے گئے (جو ڈراپ کیے گئے پری ٹریننگ ہیڈ سے متعلق ہیں) اور کچھ وزن تصادفی طور پر initialize کیے گئے (جو نئے ہیڈ کے لیے ہیں)۔ یہ آخر میں آپ کو ماڈل کو ٹرین کرنے کی ترغیب دیتی ہیں، جو کہ بالکل وہی ہے جو ہم اب کرنے جا رہے ہیں۔

ایک بار جب ہمارا ماڈل تیار ہو جائے، تو ہم اب تک تشکیل دی گئی تمام چیزوں — `model`, `training_args`, تربیتی اور ویلیڈیشن ڈیٹاسیٹس، ہمارا `data_collator`، اور ہمارا `tokenizer` — کو پاس کر کے ایک `Trainer` کی تعریف کر سکتے ہیں:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

نوٹ کریں کہ جب آپ یہاں `tokenizer` کو پاس کرتے ہیں، تو `Trainer` کی جانب سے استعمال ہونے والا ڈیفالٹ `data_collator` ایک `DataCollatorWithPadding` ہوگا جیسا کہ پہلے بیان کیا گیا ہے، لہٰذا آپ اس کال میں `data_collator=data_collator` کی لائن کو چھوڑ بھی سکتے ہیں۔ تاہم، یہ سیکشن 2 میں اس پراسیسنگ کا حصہ دکھانا ضروری تھا!

ہمارے ڈیٹاسیٹ پر ماڈل کو فائن ٹیون کرنے کے لیے، ہمیں صرف اپنے `Trainer` کے `train()` میتھڈ کو کال کرنا ہے:

```py
trainer.train()
```

اس سے فائن ٹیوننگ شروع ہو جائے گی (جو کہ GPU پر چند منٹ لگنے چاہئیں) اور ہر 500 قدم پر ٹریننگ لاس رپورٹ ہوگی۔ تاہم، یہ آپ کو یہ نہیں بتائے گی کہ آپ کا ماڈل کیسا (یا برا) کام کر رہا ہے۔ اس کی وجوہات درج ذیل ہیں:

1. ہم نے تربیت کے دوران `Trainer` کو جائزہ لینے کے لیے یہ سیٹ نہیں کیا کہ `evaluation_strategy` کو یا تو `"steps"` (ہر `eval_steps` پر جائزہ) یا `"epoch"` (ہر ایپوک کے آخر میں جائزہ) پر سیٹ کیا جائے۔
2. ہم نے `Trainer` کو میٹرک کیلکولیشن کے لیے `compute_metrics()` فنکشن فراہم نہیں کیا (ورنہ جائزہ صرف لاس پرنٹ کرتا، جو کہ کوئی بہت مفید عدد نہیں ہے)۔

### جائزہ[[evaluation]]

آئیے دیکھتے ہیں کہ ہم ایک مفید `compute_metrics()` فنکشن کیسے بنا سکتے ہیں اور اسے اگلی بار تربیت میں کیسے استعمال کر سکتے ہیں۔ یہ فنکشن ایک `EvalPrediction` آبجیکٹ لیتا ہے (جو کہ ایک نامزد tuple ہے جس میں `predictions` اور `label_ids` شامل ہیں) اور ایک ایسی ڈکشنری واپس کرتا ہے جس میں strings (میٹرکس کے نام) کو floats (ان کی قدریں) کے ساتھ map کیا گیا ہو۔

ہمارے ماڈل سے پیش گوئیاں حاصل کرنے کے لیے، ہم `Trainer.predict()` کمانڈ استعمال کر سکتے ہیں:

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

`predict()` میتھڈ کا آؤٹ پٹ ایک اور نامزد tuple ہے جس میں تین فیلڈز شامل ہیں: `predictions`, `label_ids`, اور `metrics`۔ `metrics` فیلڈ صرف فراہم کردہ ڈیٹاسیٹ پر لاس اور کچھ وقت کے میٹرکس (کل اور اوسطاً کتنا وقت لگا) شامل کرے گی۔ ایک بار جب ہم اپنا `compute_metrics()` فنکشن مکمل کر کے `Trainer` کو پاس کر دیں گے، تو یہ فیلڈ `compute_metrics()` کے ذریعے واپس کردہ میٹرکس بھی شامل کرے گی۔

جیسا کہ آپ دیکھ سکتے ہیں، `predictions` ایک دو جہتی array ہے جس کا shape 408 x 2 ہے (408 وہ تعداد ہے جو ہم نے ڈیٹاسیٹ کے عناصر کے لیے استعمال کی تھی)۔ یہ ہر عنصر کے لیے logits ہیں جو کہ ہم نے `predict()` کو پاس کیا (جیسا کہ آپ نے [پچھلے باب](/course/chapter2) میں دیکھا، تمام Transformer ماڈلز logits واپس کرتے ہیں)۔ انہیں ایسی پیش گوئیوں میں تبدیل کرنے کے لیے جن کا موازنہ ہم اپنے لیبلز سے کر سکیں، ہمیں دوسرے محور پر سب سے زیادہ قدر والے انڈیکس کو لینا ہوگا:

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

اب ہم ان `preds` کا لیبلز سے موازنہ کر سکتے ہیں۔ ہمارا `compute_metrics()` فنکشن بنانے کے لیے، ہم 🤗 [Evaluate](https://github.com/huggingface/evaluate/) لائبریری کے میٹرکس پر انحصار کریں گے۔ ہم MRPC ڈیٹاسیٹ سے متعلق میٹرکس کو اتنی آسانی سے لوڈ کر سکتے ہیں جتنی آسانی سے ہم نے ڈیٹاسیٹ کو لوڈ کیا تھا، اس بار `evaluate.load()` فنکشن کے ذریعے۔ واپس ملنے والا آبجیکٹ ایک `compute()` میتھڈ رکھتا ہے جسے ہم میٹرک کیلکولیشن کے لیے استعمال کر سکتے ہیں:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ممکن ہے کہ آپ کو حاصل ہونے والے نتائج مختلف ہوں، کیونکہ ماڈل ہیڈ کی تصادفی initialization میٹرکس کو بدل سکتی ہے۔ یہاں ہم دیکھ سکتے ہیں کہ ہمارے ماڈل کی validation سیٹ پر درستگی 85.78% ہے اور F1 اسکور 89.97 ہے۔ یہ وہ دو میٹرکس ہیں جو GLUE بینچ مارک کے لیے MRPC ڈیٹاسیٹ پر نتائج کی جانچ کے لیے استعمال ہوتے ہیں۔ [BERT پیپر](https://arxiv.org/pdf/1810.04805.pdf) کی ٹیبل میں بیس ماڈل کے لیے F1 اسکور 88.9 رپورٹ کیا گیا تھا۔ وہ `uncased` ماڈل تھا جبکہ ہم اس وقت `cased` ماڈل استعمال کر رہے ہیں، جو بہتر نتیجہ کی وضاحت کرتا ہے۔

سب کچھ یکجا کرتے ہوئے، ہمارا `compute_metrics()` فنکشن کچھ یوں ہوگا:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

اور ہر ایپوک کے آخر میں میٹرکس رپورٹ کرنے کے لیے اسے ایکشن میں دیکھنے کے لیے، یہاں ہم ایک نئے `Trainer` کی تعریف کرتے ہیں جس میں یہ `compute_metrics()` فنکشن شامل ہو:

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

نوٹ کریں کہ ہم ایک نیا `TrainingArguments` بناتے ہیں جس کی `evaluation_strategy` کو `"epoch"` پر سیٹ کیا گیا ہے اور ایک نیا ماڈل استعمال کرتے ہیں — ورنہ ہم صرف اسی ماڈل کی تربیت جاری رکھتے جو پہلے ہی ٹرین ہو چکا ہے۔ ایک نئی تربیتی رن شروع کرنے کے لیے، ہم یہ execute کرتے ہیں:

```py
trainer.train()
```

اس بار، یہ ہر ایپوک کے آخر میں تربیتی لاس کے علاوہ validation لاس اور میٹرکس رپورٹ کرے گا۔ ایک بار پھر، حاصل کردہ درستگی/F1 اسکور تھوڑا مختلف ہو سکتا ہے کیونکہ ماڈل کے ہیڈ کی تصادفی initialization کے باعث، لیکن یہ تقریباً اسی حدود میں ہونا چاہیے۔

`Trainer` بغیر کسی اضافی سیٹ اپ کے متعدد GPUs یا TPUs پر کام کرتا ہے اور بہت سے آپشنز فراہم کرتا ہے، جیسے کہ mixed-precision training (اپنے training arguments میں `fp16 = True` استعمال کریں)۔ ہم باب 10 میں اس کی تمام خصوصیات پر غور کریں گے۔

یہ `Trainer` API کا استعمال کرتے ہوئے فائن ٹیوننگ کے تعارف کا اختتام کرتا ہے۔ زیادہ عام NLP ٹاسکس کے لیے اس کی ایک مثال [Chapter 7](/course/chapter7) میں دی جائے گی، لیکن فی الحال آئیے دیکھتے ہیں کہ خالص PyTorch میں یہی کام کیسے کیا جاتا ہے۔

<Tip>

✏️ **Try it out!** GLUE SST-2 ڈیٹاسیٹ پر ماڈل کی فائن ٹیوننگ کرنے کی کوشش کریں، جس ڈیٹا پروسیسنگ کو آپ نے سیکشن 2 میں انجام دیا تھا۔

</Tip>
```