```mdx
# Ù…Ú©Ù…Ù„ ØªØ±Ø¨ÛŒØª[[a-full-training]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

Ø§Ø¨ ÛÙ… Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ù¾Ú†Ú¾Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº `Trainer` Ú©Ù„Ø§Ø³ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒÛ’ Ø¨ØºÛŒØ± ÙˆÛÛŒ Ù†ØªØ§Ø¦Ø¬ Ú©ÛŒØ³Û’ Ø­Ø§ØµÙ„ Ú©ÛŒÛ’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø¯ÙˆØ¨Ø§Ø±ÛØŒ ÛÙ… ÙØ±Ø¶ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø¢Ù¾ Ù†Û’ Ø³ÛŒÚ©Ø´Ù† 2 Ù…ÛŒÚº ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù…Ú©Ù…Ù„ Ú©Ø± Ù„ÛŒ ÛÛ’Û” ÛŒÛØ§Úº Ø§ÛŒÚ© Ù…Ø®ØªØµØ± Ø®Ù„Ø§ØµÛ Ù¾ÛŒØ´ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ø´Ø§Ù…Ù„ ÛÛ’ Ø¬Ùˆ Ø¢Ù¾ Ú©Ùˆ Ø¯Ø±Ú©Ø§Ø± ÛÙˆÚ¯Ø§:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### ØªØ±Ø¨ÛŒØª Ú©ÛŒ ØªÛŒØ§Ø±ÛŒ[[prepare-for-training]]

Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ù„ÙˆÙ¾ Ù„Ú©Ú¾Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛÙ…ÛŒÚº Ú†Ù†Ø¯ objects Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆÚ¯ÛŒÛ” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÙˆÛ dataloaders ÛÛŒÚº Ø¬Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙ… Ø¨ÛŒÚ†Ø² Ù¾Ø± iteration Ú©Û’ Ù„ÛŒÛ’ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ù„ÛŒÚ©Ù† Ø§Ù† dataloaders Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ `tokenized_datasets` Ù¾Ø± Ú©Ú†Ú¾ postprocessing Ú©Ø±Ù†ÛŒ ÛÙˆÚ¯ÛŒ ØªØ§Ú©Û Ø§Ù† Ú†ÛŒØ²ÙˆÚº Ú©Ø§ Ø®ÛŒØ§Ù„ Ø±Ú©Ú¾Ø§ Ø¬Ø§ Ø³Ú©Û’ Ø¬Ùˆ `Trainer` Ù†Û’ Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ ÛÙ…Ø§Ø±Û’ Ù„ÛŒÛ’ Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÚºÛ” Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛÙ…ÛŒÚº Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§:

- Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªÙˆÙ‚Ø¹Ø§Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ù†Û ÛÙˆÙ†Û’ ÙˆØ§Ù„ÛŒ ÙˆÛŒÙ„ÛŒÙˆØ² ÙˆØ§Ù„Û’ Ú©Ø§Ù„Ù…Ø² Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒÚº (Ø¬ÛŒØ³Û’ Ú©Û `sentence1` Ø§ÙˆØ± `sentence2` Ú©Ø§Ù„Ù…Ø²).
- Ú©Ø§Ù„Ù… `label` Ú©Ø§ Ù†Ø§Ù… ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ú©Û’ `labels` Ø±Ú©Ú¾ÛŒÚº (Ú©ÛŒÙˆÙ†Ú©Û Ù…Ø§ÚˆÙ„ ØªÙˆÙ‚Ø¹ Ú©Ø±ØªØ§ ÛÛ’ Ú©Û argument Ú©Ø§ Ù†Ø§Ù… `labels` ÛÙˆ).
- ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹Ø³ Ú©Ø§ ÙØ§Ø±Ù…ÛŒÙ¹ Ø§Ø³ Ø·Ø±Ø­ Ø³ÛŒÙ¹ Ú©Ø±ÛŒÚº Ú©Û ÙˆÛ Ù„Ø³Ù¹Ø³ Ú©ÛŒ Ø¨Ø¬Ø§Ø¦Û’ PyTorch tensors ÙˆØ§Ù¾Ø³ Ú©Ø±ÛŒÚº.

ÛÙ…Ø§Ø±Û’ `tokenized_datasets` Ù…ÛŒÚº Ø§Ù† Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ù…ÛŒÚº Ø³Û’ ÛØ± Ø§ÛŒÚ© Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© method Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Ù¾Ú¾Ø± ÛÙ… Ú†ÛŒÚ© Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ù†ØªÛŒØ¬Û’ Ù…ÛŒÚº ØµØ±Ù ÙˆÛÛŒ Ú©Ø§Ù„Ù… Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚº Ø¬Ùˆ ÛÙ…Ø§Ø±Ø§ Ù…Ø§ÚˆÙ„ Ù‚Ø¨ÙˆÙ„ Ú©Ø±Û’ Ú¯Ø§:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Ø§Ø¨ Ø¬Ø¨ ÛŒÛ Ø³Ø¨ ÛÙˆ Ú†Ú©Ø§ ÛÛ’ØŒ ØªÙˆ ÛÙ… Ø¢Ø³Ø§Ù†ÛŒ Ø³Û’ Ø§Ù¾Ù†Û’ dataloaders Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù…ÛŒÚº Ú©Ø³ÛŒ ØºÙ„Ø·ÛŒ Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… Ø§Ø³ Ø·Ø±Ø­ Ø§ÛŒÚ© Ø¨ÛŒÚ† Ú©Ø§ Ù…Ø¹Ø§Ø¦Ù†Û Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û Ø§ØµÙ„ shapes Ø´Ø§ÛŒØ¯ Ø¢Ù¾ Ú©Û’ Ù„ÛŒÛ’ ØªÚ¾ÙˆÚ‘ÛŒ Ù…Ø®ØªÙ„Ù ÛÙˆÚº Ú©ÛŒÙˆÙ†Ú©Û ÛÙ… Ù†Û’ training dataloader Ú©Û’ Ù„ÛŒÛ’ `shuffle=True` Ø³ÛŒÙ¹ Ú©ÛŒØ§ ÛÛ’ Ø§ÙˆØ± ÛÙ… Ø¨ÛŒÚ† Ú©Û’ Ø§Ù†Ø¯Ø± Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© padding Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºÛ”

Ø§Ø¨ Ø¬Ø¨Ú©Û ÛÙ… ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù…Ú©Ù…Ù„ Ú©Ø± Ú†Ú©Û’ ÛÛŒÚº (Ø¬Ùˆ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ ML practitioner Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© ØªØ³Ù„ÛŒ Ø¨Ø®Ø´ Ù…Ú¯Ø± Ù…Ø´Ú©Ù„ Ù…Ù‚ØµØ¯ ÛÛ’)ØŒ Ø¢Ø¦ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ø·Ø±Ù Ø±Ø¬ÙˆØ¹ Ú©Ø±ÛŒÚºÛ” ÛÙ… Ø§Ø³Û’ Ø¨Ø§Ù„Ú©Ù„ Ø§Ø³ÛŒ Ø·Ø±Ø­ instantiate Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Ø§ Ú©Û Ù¾Ú†Ú¾Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ¾Ø§:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ÛŒÛ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Û ØªØ±Ø¨ÛŒØª Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø³Ø¨ Ú©Ú†Ú¾ Ø¨Ø®ÙˆØ¨ÛŒ Ú†Ù„Û’ØŒ ÛÙ… Ø§Ù¾Ù†Ø§ Ø¨ÛŒÚ† Ø§Ø³ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ù¾Ø§Ø³ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ØªÙ…Ø§Ù… ğŸ¤— Transformers Ù…Ø§ÚˆÙ„Ø² `labels` ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Û’ Ù¾Ø± loss ÙˆØ§Ù¾Ø³ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø³Ø§ØªÚ¾ ÛÛŒ ÛÙ…ÛŒÚº logits Ø¨Ú¾ÛŒ Ù…Ù„ØªÛ’ ÛÛŒÚº (ÛØ± Ø§Ù† Ù¾Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ø¯ÙˆØŒ ÛŒØ¹Ù†ÛŒ 8 x 2 Ú©Ø§ tensor).

ÛÙ… ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø§Ù¾Ù†ÛŒ training loop Ù„Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚº! ÛÙ…ÛŒÚº ØµØ±Ù Ø¯Ùˆ Ú†ÛŒØ²ÛŒÚº Ø¯Ø±Ú©Ø§Ø± ÛÛŒÚº: Ø§ÛŒÚ© optimizer Ø§ÙˆØ± Ø§ÛŒÚ© learning rate schedulerÛ” Ú†ÙˆÙ†Ú©Û ÛÙ… Ú©ÙˆØ´Ø´ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº Ú©Û ÙˆÛÛŒ replicate Ú©Ø±ÛŒÚº Ø¬Ùˆ `Trainer` ÛØ§ØªÚ¾ Ø³Û’ Ú©Ø± Ø±ÛØ§ ØªÚ¾Ø§ØŒ Ù„ÛÙ°Ø°Ø§ ÛÙ… ÙˆÛÛŒ defaults Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” `Trainer` Ø¬Ùˆ optimizer Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ ÙˆÛ ÛÛ’ `AdamW`ØŒ Ø¬Ùˆ Ú©Û Adam Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÛ’ØŒ Ù…Ú¯Ø± weight decay regularization Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© twist Ú©Û’ Ø³Ø§ØªÚ¾ (Ø¯ÛŒÚ©Ú¾ÛŒÚº ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) Ø§Ø² Ilya Loshchilov Ø§ÙˆØ± Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÚˆÛŒÙØ§Ù„Ù¹ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„Ø§ learning rate scheduler ØµØ±Ù Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û ÙˆÛŒÙ„ÛŒÙˆ (5e-5) Ø³Û’ 0 ØªÚ© Ú©Ø§ linear decay ÛÛ’Û” Ø§Ø³Û’ ØµØ­ÛŒØ­ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ define Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº ÛŒÛ Ø¬Ø§Ù†Ù†Ø§ ÛÙˆÚ¯Ø§ Ú©Û ÛÙ… Ú©ØªÙ†Û’ training steps Ù„ÛŒÚº Ú¯Û’ØŒ Ø¬Ùˆ Ú©Û ÙˆÛ epochs Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÛÛ’ Ø¬Ùˆ ÛÙ… Ú†Ù„Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº multiplied by training batches Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ (Ø¬Ùˆ Ú©Û ÛÙ…Ø§Ø±Û’ training dataloader Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ ÛÛ’)Û” `Trainer` ÚˆÛŒÙØ§Ù„Ù¹ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± ØªÛŒÙ† epochs Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛÙ… Ø§Ø³ÛŒ Ù¾ÛŒØ±ÙˆÛŒ Ú©Ø±ÛŒÚº Ú¯Û’:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### ØªØ±Ø¨ÛŒØªÛŒ Ù„ÙˆÙ¾[[the-training-loop]]

Ø§ÛŒÚ© Ø¢Ø®Ø±ÛŒ Ø¨Ø§Øª: Ø§Ú¯Ø± ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ GPU Ø¯Ø³ØªÛŒØ§Ø¨ ÛÙˆ ØªÙˆ ÛÙ… Ø§Ø³Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛÛŒÚº Ú¯Û’ (CPU Ù¾Ø± ØªØ±Ø¨ÛŒØª Ú†Ù†Ø¯ Ù…Ù†Ù¹ÙˆÚº Ú©ÛŒ Ø¨Ø¬Ø§Ø¦Û’ Ú©Ø¦ÛŒ Ú¯Ú¾Ù†Ù¹Û’ Ù„Û’ Ø³Ú©ØªÛŒ ÛÛ’)Û” Ø§ÛŒØ³Ø§ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… Ø§ÛŒÚ© `device` Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ø³ Ù¾Ø± ÛÙ… Ø§Ù¾Ù†Ø§ Ù…Ø§ÚˆÙ„ Ø§ÙˆØ± Ø¨ÛŒÚ†Ø² Ø±Ú©Ú¾ÛŒÚº Ú¯Û’:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

Ø§Ø¨ ÛÙ… ØªØ±Ø¨ÛŒØª Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚº! ÛŒÛ Ø¬Ø§Ù†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Û ØªØ±Ø¨ÛŒØª Ú©Ø¨ Ø®ØªÙ… ÛÙˆÚ¯ÛŒØŒ ÛÙ… `tqdm` Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ù¾Ù†Û’ training steps Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø± Ø§ÛŒÚ© progress bar Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Ø¢Ù¾ Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û training loop Ú©Ø§ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø­ØµÛ Ø§Ù†Ù¹Ø±ÙˆÚˆÚ©Ø´Ù† Ø¬ÛŒØ³Ø§ ÛÛŒ ÛÛ’Û” Ú†ÙˆÙ†Ú©Û ÛÙ… Ù†Û’ Ú©ÙˆØ¦ÛŒ reporting Ø´Ø§Ù…Ù„ Ù†ÛÛŒÚº Ú©ÛŒØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛŒÛ training loop ÛÙ…ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ú©Ú†Ú¾ Ù†ÛÛŒÚº Ø¨ØªØ§Ø¦Û’ Ú¯Ø§Û” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº Ø§ÛŒÚ© evaluation loop Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û”

### Ø¬Ø§Ø¦Ø²Û Ù„ÙˆÙ¾[[the-evaluation-loop]]

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ Ú©ÛŒØ§ØŒ ÛÙ… ğŸ¤— Evaluate Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÙØ±Ø§ÛÙ… Ú©Ø±Ø¯Û metric Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ… Ù¾ÛÙ„Û’ ÛÛŒ `metric.compute()` Ù…ÛŒØªÚ¾Úˆ Ø¯ÛŒÚ©Ú¾ Ú†Ú©Û’ ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† metrics Ø­Ù‚ÛŒÙ‚Øª Ù…ÛŒÚº `add_batch()` Ù…ÛŒØªÚ¾Úˆ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ prediction loop Ù…ÛŒÚº Ø¨ÛŒÚ†Ø² Ú©Ùˆ Ø¬Ù…Ø¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§ÛŒÚ© Ø¨Ø§Ø± Ø¬Ø¨ ÛÙ… ØªÙ…Ø§Ù… Ø¨ÛŒÚ†Ø² Ø¬Ù…Ø¹ Ú©Ø± Ù„ÛŒÚºØŒ ØªÙˆ ÛÙ… `metric.compute()` Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø­ØªÙ…ÛŒ Ù†ØªÛŒØ¬Û Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” ÛŒÛ Ø±ÛØ§ Ú©Û Ø§Ø³ Ø³Ø¨ Ú©Ùˆ Ø§ÛŒÚ© evaluation loop Ù…ÛŒÚº Ú©ÛŒØ³Û’ implement Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Ø§ÛŒÚ© Ø¨Ø§Ø± Ù¾Ú¾Ø±ØŒ Ø¢Ù¾ Ú©Û’ Ù†ØªØ§Ø¦Ø¬ Ù…Ø§ÚˆÙ„ ÛÛŒÚˆ Ú©ÛŒ ØªØµØ§Ø¯ÙÛŒ initialization Ø§ÙˆØ± data shuffling Ú©ÛŒ ÙˆØ¬Û Ø³Û’ ØªÚ¾ÙˆÚ‘Û’ Ù…Ø®ØªÙ„Ù ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† ÛŒÛ Ø§ÛŒÚ© ÛÛŒ Ø­Ø¯ÙˆØ¯ Ù…ÛŒÚº ÛÙˆÙ†Û’ Ú†Ø§ÛØ¦ÛŒÚºÛ”

<Tip>

âœï¸ **Try it out!** Ù¾Ú†Ú¾Ù„Û’ training loop Ù…ÛŒÚº ØªØ±Ù…ÛŒÙ… Ú©Ø±ÛŒÚº ØªØ§Ú©Û Ø¢Ù¾ Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ SST-2 ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ù¾Ø± ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ú©Ø± Ø³Ú©ÛŒÚºÛ”

</Tip>

### ğŸ¤— Accelerate Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù¾Ù†Û’ training loop Ú©Ùˆ Ø³Ù¾Ø±Ú†Ø§Ø±Ø¬ Ú©Ø±ÛŒÚº[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

Ø¬Ùˆ training loop ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ ØªØ¹Ø±ÛŒÙ Ú©ÛŒØ§ ØªÚ¾Ø§ ÙˆÛ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ CPU ÛŒØ§ GPU Ù¾Ø± Ù¹Ú¾ÛŒÚ© Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’Û” Ù„ÛŒÚ©Ù† [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate) Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ØŒ Ú†Ù†Ø¯ ØªØ¨Ø¯ÛŒÙ„ÛŒÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙ… Ù…ØªØ¹Ø¯Ø¯ GPUs ÛŒØ§ TPUs Ù¾Ø± distributed training Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” training Ø§ÙˆØ± validation dataloaders Ú©ÛŒ ØªØ®Ù„ÛŒÙ‚ Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ØŒ ÛŒÛ ÛÛ’ ÛÙ…Ø§Ø±Ø§ manual training loop Ú©ÛŒØ³Ø§ Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Ø§ÙˆØ± ÛŒÛØ§Úº ØªØ¨Ø¯ÛŒÙ„ÛŒØ§Úº ÛÛŒÚº:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

Ø§Ø¶Ø§ÙÛ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛÙ„ÛŒ Ù„Ø§Ø¦Ù† import Ù„Ø§Ø¦Ù† ÛÛ’Û” Ø¯ÙˆØ³Ø±ÛŒ Ù„Ø§Ø¦Ù† Ø§ÛŒÚ© `Accelerator` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ instantiate Ú©Ø±ØªÛŒ ÛÛ’ Ø¬Ùˆ Ù…Ø§Ø­ÙˆÙ„ Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ú©Ø± Ù…ÙˆØ²ÙˆÚº distributed setup initialize Ú©Ø±Û’ Ú¯ÛŒÛ” ğŸ¤— Accelerate Ø¢Ù¾ Ú©Û’ Ù„ÛŒÛ’ device placement Ú©Ùˆ Ø³Ù†Ø¨Ú¾Ø§Ù„ØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ Ø¢Ù¾ ÙˆÛ Ù„Ø§Ø¦Ù†ÛŒÚº ÛÙ¹Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©Ùˆ device Ù¾Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº (ÛŒØ§ Ø§Ú¯Ø± Ø¢Ù¾ Ú†Ø§ÛÛŒÚº ØªÙˆ Ø§Ù†ÛÛŒÚº `accelerator.device` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº).

Ù¾Ú¾Ø± Ú©Ø§Ù… Ú©Ø§ Ù…Ø±Ú©Ø²ÛŒ Ø­ØµÛ Ø§Ù† Ù„Ø§Ø¦Ù†ÙˆÚº Ù…ÛŒÚº Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø¬Ùˆ dataloadersØŒ Ù…Ø§ÚˆÙ„ Ø§ÙˆØ± optimizer Ú©Ùˆ `accelerator.prepare()` Ú©Ùˆ Ø¨Ú¾ÛŒØ¬ØªÛŒ ÛÛŒÚºÛ” ÛŒÛ Ø§Ù† Ø§Ø´ÛŒØ§Ø¡ Ú©Ùˆ Ù…ÙˆØ²ÙˆÚº container Ù…ÛŒÚº Ù„Ù¾ÛŒÙ¹ Ø¯Û’ Ú¯Ø§ ØªØ§Ú©Û Ø¢Ù¾ Ú©ÛŒ distributed training Ù…Ø·Ù„ÙˆØ¨Û Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ú©Ø§Ù… Ú©Ø±Û’Û” Ø¨Ø§Ù‚ÛŒ Ø±Û Ø¬Ø§Ù†Û’ ÙˆØ§Ù„ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒØ§Úº ÛŒÛ ÛÛŒÚº Ú©Û ÙˆÛ Ù„Ø§Ø¦Ù† ÛÙ¹Ø§ Ø¯ÛŒÚº Ø¬Ùˆ Ø¨ÛŒÚ† Ú©Ùˆ `device` Ù¾Ø± Ø±Ú©Ú¾ØªÛŒ ÛÛ’ (Ø§Ú¯Ø± Ø¢Ù¾ Ø§Ø³Û’ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ Ø§Ø³Û’ `accelerator.device` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº) Ø§ÙˆØ± `loss.backward()` Ú©Ùˆ `accelerator.backward(loss)` Ø³Û’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒÚºÛ”

<Tip>
âš ï¸ Cloud TPUs Ú©ÛŒ Ø·Ø±Ù Ø³Û’ ÙØ±Ø§ÛÙ… Ú©ÛŒ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„ÛŒ Ø±ÙØªØ§Ø± Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ø³Û’ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… ØªØ¬ÙˆÛŒØ² Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø¢Ù¾ Ø§Ù¾Ù†Û’ samples Ú©Ùˆ Ø§ÛŒÚ© Ù…Ù‚Ø±Ø±Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© padding Ú©Ø±ÛŒÚºØŒ `padding="max_length"` Ø§ÙˆØ± `max_length` Ø¢Ø±Ú¯ÛŒÙˆÙ…Ù†Ù¹Ø³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’Û”
</Tip>

Ø§Ú¯Ø± Ø¢Ù¾ Ú©Ø§Ù¾ÛŒ Ø§ÙˆØ± Ù¾ÛŒØ³Ù¹ Ú©Ø± Ú©Û’ Ø¢Ø²Ù…Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ ØªÙˆ ÛŒÛØ§Úº Ù…Ú©Ù…Ù„ training loop ÛÛ’ Ø¬Ùˆ ğŸ¤— Accelerate Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒØ³Ø§ Ø¯Ú©Ú¾ØªØ§ ÛÛ’:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Ø§Ø³Û’ Ø§ÛŒÚ© `train.py` Ø§Ø³Ú©Ø±Ù¾Ù¹ Ù…ÛŒÚº Ø±Ú©Ú¾Ù†Û’ Ø³Û’ ÛŒÛ Ø§Ø³Ú©Ø±Ù¾Ù¹ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù‚Ø³Ù… Ú©Û’ distributed setup Ù¾Ø± Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§Û” Ø§Ù¾Ù†Û’ distributed setup Ù…ÛŒÚº Ø§Ø³Û’ Ø¢Ø²Ù…Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ú©Ù…Ø§Ù†Úˆ Ú†Ù„Ø§Ø¦ÛŒÚº:

```bash
accelerate config
```

ÛŒÛ Ø¢Ù¾ Ø³Û’ Ú†Ù†Ø¯ Ø³ÙˆØ§Ù„Ø§Øª Ù¾ÙˆÚ†Ú¾Û’ Ú¯ÛŒ Ø§ÙˆØ± Ø¢Ù¾ Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Ùˆ Ø§ÛŒÚ© configuration file Ù…ÛŒÚº dump Ú©Ø± Ø¯Û’ Ú¯ÛŒ Ø¬Ùˆ Ø§Ø³ Ú©Ù…Ø§Ù†Úˆ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªÛŒ ÛÛ’:

```
accelerate launch train.py
```

Ø¬Ùˆ distributed training Ú©Ùˆ launch Ú©Ø± Ø¯Û’ Ú¯ÛŒÛ”

Ø§Ú¯Ø± Ø¢Ù¾ ÛŒÛ Notebook Ù…ÛŒÚº Ø¢Ø²Ù…Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº (Ù…Ø«Ù„Ø§Ù‹ Colab Ù¾Ø± TPUs Ú©Û’ Ø³Ø§ØªÚ¾ Ù¹ÛŒØ³Ù¹ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’)ØŒ ØªÙˆ ØµØ±Ù Ú©ÙˆÚˆ Ú©Ùˆ Ø§ÛŒÚ© `training_function()` Ù…ÛŒÚº Ù¾ÛŒØ³Ù¹ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¢Ø®Ø±ÛŒ cell Ù…ÛŒÚº ÛŒÛ Ú†Ù„Ø§Ø¦ÛŒÚº:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

Ø¢Ù¾ Ù…Ø²ÛŒØ¯ Ù…Ø«Ø§Ù„ÛŒÚº [ğŸ¤— Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples) Ù…ÛŒÚº ØªÙ„Ø§Ø´ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº.
```