```mdx
<FrameworkSwitchCourse {fw} />

# ڈیٹا کی پراسیسنگ[[processing-the-data]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
پچھلے باب کی مثال کو جاری رکھتے ہوئے، یہاں ہم PyTorch میں ایک بیچ پر سیکوئنس کلاسفائر کو ٹرین کرنے کا طریقہ پیش کرتے ہیں:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
پچھلے باب کی مثال کو جاری رکھتے ہوئے، یہاں ہم TensorFlow میں ایک بیچ پر سیکوئنس کلاسفائر کو ٹرین کرنے کا طریقہ پیش کرتے ہیں:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

ظاہر ہے، صرف دو جملوں پر ماڈل کو ٹرین کرنا بہت اچھے نتائج فراہم نہیں کرے گا۔ بہتر نتائج حاصل کرنے کے لیے، آپ کو ایک بڑا ڈیٹاسیٹ تیار کرنا ہوگا۔

اس سیکشن میں ہم مثال کے طور پر MRPC (Microsoft Research Paraphrase Corpus) ڈیٹاسیٹ کا استعمال کریں گے، جس کا تعارف [ایک مقالے](https://www.aclweb.org/anthology/I05-5002.pdf) میں William B. Dolan اور Chris Brockett نے کرایا تھا۔ یہ ڈیٹاسیٹ 5,801 جملوں کے جوڑوں پر مشتمل ہے، جن کے ساتھ ایک لیبل بھی ہوتا ہے جو بتاتا ہے کہ آیا یہ پیرا فریز ہیں یا نہیں (یعنی، اگر دونوں جملوں کا مطلب ایک ہی ہے)۔ ہم نے اسے اس باب کے لیے منتخب کیا ہے کیونکہ یہ ایک چھوٹا ڈیٹاسیٹ ہے، اس لیے اس پر ٹریننگ کے تجربات کرنا آسان ہے۔

### Hub سے ڈیٹاسیٹ لوڈ کرنا[[loading-a-dataset-from-the-hub]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub میں صرف ماڈلز نہیں ہوتے؛ بلکہ اس میں مختلف زبانوں میں متعدد ڈیٹاسیٹس بھی موجود ہیں۔ آپ ڈیٹاسیٹس کو [یہاں](https://huggingface.co/datasets) دیکھ سکتے ہیں، اور ہم تجویز کرتے ہیں کہ آپ اس سیکشن سے گزرنے کے بعد ایک نیا ڈیٹاسیٹ لوڈ اور پروسیس کرنے کی کوشش کریں (عام دستاویزات کے لیے [یہاں](https://huggingface.co/docs/datasets/loading) ملاحظہ کریں)۔ لیکن فی الحال، آئیں MRPC ڈیٹاسیٹ پر توجہ مرکوز کریں! یہ [GLUE benchmark](https://gluebenchmark.com/) کے 10 ڈیٹاسیٹس میں سے ایک ہے، جو کہ ایک علمی بینچ مارک ہے اور ML ماڈلز کی کارکردگی کو 10 مختلف ٹیکسٹ کلاسفکیشن ٹاسکس میں ناپنے کے لیے استعمال ہوتا ہے۔

🤗 Datasets لائبریری Hub پر ڈیٹاسیٹ ڈاؤن لوڈ اور کیش کرنے کے لیے ایک بہت سادہ کمانڈ فراہم کرتی ہے۔ ہم MRPC ڈیٹاسیٹ کو اس طرح ڈاؤن لوڈ کر سکتے ہیں:

<Tip>
⚠️ **انتباہ** اس بات کو یقینی بنائیں کہ `datasets` انسٹال ہے، `pip install datasets` چلا کر۔ پھر، MRPC ڈیٹاسیٹ کو لوڈ کریں اور پرنٹ کریں تاکہ آپ دیکھ سکیں کہ اس میں کیا شامل ہے۔
</Tip> 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

جیسا کہ آپ دیکھ سکتے ہیں، ہمیں ایک `DatasetDict` آبجیکٹ ملتا ہے جس میں ٹریننگ سیٹ، ویلیڈیشن سیٹ، اور ٹیسٹ سیٹ شامل ہیں۔ ان میں سے ہر ایک میں کئی کالمز (`sentence1`, `sentence2`, `label`, اور `idx`) شامل ہیں اور قطاروں کی تعداد متغیر ہوتی ہے، جو ہر سیٹ میں عناصر کی تعداد کو ظاہر کرتی ہے (یعنی، ٹریننگ سیٹ میں 3,668 جملوں کے جوڑے، ویلیڈیشن سیٹ میں 408 اور ٹیسٹ سیٹ میں 1,725 جملے کے جوڑے موجود ہیں)۔

یہ کمانڈ ڈیٹاسیٹ کو ڈاؤن لوڈ اور کیش کرتی ہے، ڈیفالٹ کے طور پر *~/.cache/huggingface/datasets* میں۔ باب 2 سے یاد رکھیں کہ آپ `HF_HOME` انوائرمنٹ ویریبل سیٹ کر کے اپنی کیش فولڈر کو حسب ضرورت بنا سکتے ہیں۔

ہم اپنے `raw_datasets` آبجیکٹ میں موجود ہر جملے کے جوڑے تک انڈیکسنگ کے ذریعے رسائی حاصل کر سکتے ہیں، جیسے کہ ایک ڈکشنری کے ساتھ:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence.'}
```

ہم دیکھ سکتے ہیں کہ لیبلز پہلے سے ہی انٹیجر ہیں، لہٰذا ہمیں وہاں کوئی پری پروسیسنگ کرنے کی ضرورت نہیں ہوگی۔ یہ جاننے کے لیے کہ کون سا انٹیجر کس لیبل سے متعلق ہے، ہم اپنے `raw_train_dataset` کے `features` کا معائنہ کر سکتے ہیں۔ یہ ہمیں ہر کالم کی قسم بتائے گا:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

پردہ کے پیچھے، `label` کی قسم `ClassLabel` ہے، اور انٹیجرز سے لیبل نیم کی میپنگ *names* فولڈر میں محفوظ کی گئی ہے۔ `0` کا تعلق `not_equivalent` سے ہے، اور `1` کا تعلق `equivalent` سے ہے۔

<Tip>

✏️ **Try it out!** تربیتی سیٹ کے عنصر 15 اور ویلیڈیشن سیٹ کے عنصر 87 کو دیکھیں۔ ان کے لیبلز کیا ہیں؟

</Tip>

### ڈیٹاسیٹ کی پری پروسیسنگ[[preprocessing-a-dataset]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ڈیٹاسیٹ کی پری پروسیسنگ کے لیے، ہمیں ٹیکسٹ کو نمبرز میں تبدیل کرنا ہوگا جسے ماڈل سمجھ سکے۔ جیسا کہ آپ نے [پچھلے باب](/course/chapter2) میں دیکھا، یہ ٹوکنائزر کے ذریعے کیا جاتا ہے۔ ہم ٹوکنائزر کو ایک جملہ یا جملوں کی فہرست فراہم کر سکتے ہیں، لہٰذا ہم ہر جوڑے کے پہلے جملوں اور دوسرے جملوں کو اس طرح براہ راست ٹوکنائز کر سکتے ہیں:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

تاہم، ہم صرف دو سیکوئنسز کو ماڈل میں پاس کر کے یہ پیش گوئی نہیں کر سکتے کہ آیا یہ دونوں جملے پیرا فریز ہیں یا نہیں۔ ہمیں دونوں سیکوئنسز کو ایک جوڑے کے طور پر ہینڈل کرنا ہوگا، اور مناسب پری پروسیسنگ لگانی ہوگی۔ خوش قسمتی سے، ٹوکنائزر ایک جوڑے کے سیکوئنسز کو بھی اسی انداز میں تیار کر سکتا ہے جیسا کہ ہمارا BERT ماڈل توقع کرتا ہے: 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

ہم نے [باب 2](/course/chapter2) میں `input_ids` اور `attention_mask` کی بات کی تھی، لیکن `token_type_ids` پر بات کرنے کو ملتوی کر دیا گیا تھا۔ اس مثال میں، یہی ماڈل کو بتاتا ہے کہ ان پٹ کا کون سا حصہ پہلا جملہ ہے اور کون سا دوسرا۔

<Tip>

✏️ **Try it out!** تربیتی سیٹ کے عنصر 15 کو لیں اور دونوں جملوں کو علیحدہ اور ایک جوڑے کے طور پر ٹوکنائز کریں۔ دونوں نتائج میں کیا فرق ہے؟

</Tip>

اگر ہم `input_ids` کے اندر موجود IDs کو الفاظ میں واپس ڈی کوڈ کریں:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

تو ہمیں حاصل ہوگا:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

یوں ہم دیکھتے ہیں کہ ماڈل توقع کرتا ہے کہ ان پٹ کا فارمیٹ `[CLS] sentence1 [SEP] sentence2 [SEP]` ہو جب دو جملے ہوں۔ `token_type_ids` کے ساتھ ہم اس کا موازنہ کرتے ہیں:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

جیسا کہ آپ دیکھ سکتے ہیں، `[CLS] sentence1 [SEP]` سے متعلق ان پٹ کے حصوں کا ٹوکن ٹائپ ID `0` ہوتا ہے، جبکہ دوسرے حصوں، جو کہ `sentence2 [SEP]` سے متعلق ہیں، ان کا ٹوکن ٹائپ ID `1` ہوتا ہے۔

نوٹ کریں کہ اگر آپ کوئی مختلف checkpoint منتخب کرتے ہیں، تو ضروری نہیں کہ آپ کے tokenized inputs میں `token_type_ids` موجود ہوں (مثلاً، اگر آپ DistilBERT ماڈل استعمال کرتے ہیں تو یہ واپس نہیں کیے جاتے)۔ یہ صرف اس صورت میں واپس کیے جاتے ہیں جب ماڈل کو ان کا استعمال معلوم ہو، کیونکہ ماڈل نے اپنی پری ٹریننگ کے دوران انہیں دیکھا ہوتا ہے۔ 

یہاں، BERT کو token type IDs کے ساتھ پری ٹرین کیا گیا ہے، اور ماسکڈ لینگویج ماڈلنگ آبجیکٹو کے علاوہ جس کا ہم نے [باب 1](/course/chapter1) میں ذکر کیا، اس کا ایک اضافی مقصد ہے جسے _next sentence prediction_ کہتے ہیں۔ اس کام کا مقصد جملوں کے جوڑوں کے درمیان تعلق کو ماڈل کرنا ہے۔

Next sentence prediction میں، ماڈل کو جملوں کے جوڑے فراہم کیے جاتے ہیں (جس میں تصادفی طور پر ماسک کیے گئے ٹوکن ہوتے ہیں) اور اسے پیش گوئی کرنے کو کہا جاتا ہے کہ آیا دوسرا جملہ پہلے کے بعد آتا ہے یا نہیں۔ کام کو غیر معمولی بنانے کے لیے، آدھے وقت جملے اصل دستاویز سے ایک دوسرے کے پیچھے ہوتے ہیں، اور باقی آدھے وقت دونوں جملے دو مختلف دستاویزات سے آتے ہیں۔

عمومًا، آپ کو اس بات کی فکر کرنے کی ضرورت نہیں کہ آپ کے tokenized inputs میں `token_type_ids` ہیں یا نہیں: جب تک آپ ٹوکنائزر اور ماڈل کے لیے ایک ہی checkpoint استعمال کرتے ہیں، سب کچھ ٹھیک رہے گا کیونکہ ٹوکنائزر کو معلوم ہوتا ہے کہ ماڈل کو کیا فراہم کرنا ہے۔

اب جبکہ ہم نے دیکھا کہ ہمارا ٹوکنائزر ایک جملے کے جوڑے کو کیسے ہینڈل کرتا ہے، ہم اسے اپنے پورے ڈیٹاسیٹ کو ٹوکنائز کرنے کے لیے استعمال کر سکتے ہیں: جیسے کہ [پچھلے باب](/course/chapter2) میں، ہم ٹوکنائزر کو جملوں کے جوڑوں کی فہرست دے کر، پہلے جملوں کی فہرست اور پھر دوسرے جملوں کی فہرست دے سکتے ہیں۔ یہ padding اور truncation کے آپشنز کے ساتھ بھی مطابقت رکھتا ہے جو ہم نے [باب 2](/course/chapter2) میں دیکھے تھے۔ لہٰذا، ٹریننگ ڈیٹاسیٹ کی پری پروسیسنگ کا ایک طریقہ یہ ہے:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

یہ طریقہ ٹھیک کام کرتا ہے، لیکن اس کا ایک نقصان یہ ہے کہ یہ ایک ڈکشنری واپس کرتا ہے (ہمارے keys جیسے `input_ids`, `attention_mask`, اور `token_type_ids` کے ساتھ، اور ان کی ویلیوز جو کہ لسٹس کی لسٹس ہوتی ہیں)۔ یہ صرف اس صورت میں کام کرے گا اگر آپ کے پاس پورے ڈیٹاسیٹ کو ٹوکنائز کرنے کے دوران اسے اسٹور کرنے کے لیے کافی RAM موجود ہو (جبکہ 🤗 Datasets لائبریری کے ڈیٹاسیٹس [Apache Arrow](https://arrow.apache.org/) فائلز کی صورت میں ڈسک پر محفوظ ہوتے ہیں، اس لیے آپ صرف وہی سیمپلز لوڈ کرتے ہیں جن کی آپ نے درخواست کی ہوتی ہے)۔

ڈیٹا کو ایک ڈیٹاسیٹ کی صورت میں برقرار رکھنے کے لیے، ہم [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) میتھڈ استعمال کریں گے۔ یہ اضافی لچک بھی فراہم کرتا ہے، اگر ہمیں صرف ٹوکنائزیشن سے زیادہ پری پروسیسنگ کی ضرورت ہو۔ `map()` میتھڈ ہر عنصر پر ایک فنکشن لگانے سے کام کرتا ہے، لہٰذا آئیے ایک ایسا فنکشن ڈیفائن کرتے ہیں جو ہمارے ان پٹس کو ٹوکنائز کرے:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

یہ فنکشن ایک ڈکشنری لیتا ہے (جیسا کہ ہمارے ڈیٹاسیٹ کے آئٹمز) اور ایک نئی ڈکشنری واپس کرتا ہے جس میں keys `input_ids`, `attention_mask`, اور `token_type_ids` شامل ہوتے ہیں۔ نوٹ کریں کہ اگر `example` ڈکشنری میں کئی سیمپلز شامل ہوں (ہر key کے ساتھ جملوں کی لسٹ) تو بھی یہ کام کرتا ہے کیونکہ `tokenizer` پہلے جیسا کہ دیکھا گیا ہے، جملوں کے جوڑوں کی لسٹ پر کام کرتا ہے۔ یہ ہمیں `map()` کال میں `batched=True` آپشن استعمال کرنے کی اجازت دے گا، جو ٹوکنائزیشن کو بہت تیز کر دے گا۔ `tokenizer` کو [🤗 Tokenizers](https://github.com/huggingface/tokenizers) لائبریری کی طرف سے Rust میں لکھے گئے ایک ٹوکنائزر سے سپورٹ ملتی ہے۔ یہ ٹوکنائزر بہت تیز ہو سکتا ہے، لیکن صرف اس صورت میں جب ہم اسے بیک وقت بہت سے ان پٹس فراہم کریں۔

نوٹ کریں کہ ہم نے ابھی اپنے ٹوکنائزیشن فنکشن میں `padding` آرگیومنٹ چھوڑ دیا ہے۔ اس کی وجہ یہ ہے کہ تمام سیمپلز کو زیادہ سے زیادہ لمبائی تک پیڈ کرنا مؤثر نہیں ہے: بہتر ہے کہ سیمپلز کو بیچ بناتے وقت پیڈ کریں، کیونکہ اس سے ہمیں صرف اسی بیچ کی زیادہ سے زیادہ لمبائی تک پیڈ کرنا پڑے گا، نہ کہ پورے ڈیٹاسیٹ کی زیادہ سے زیادہ لمبائی تک۔ یہ ان پٹس کی متغیر لمبائیوں کے ساتھ بہت سا وقت اور پروسیسنگ پاور بچا سکتا ہے!

یہاں یہ طریقہ ہے کہ ہم اپنے تمام ڈیٹاسیٹس پر ایک ساتھ ٹوکنائزیشن فنکشن کیسے لگاتے ہیں۔ ہم `map` کال میں `batched=True` استعمال کر رہے ہیں تاکہ فنکشن ایک ساتھ ڈیٹاسیٹ کے متعدد عناصر پر لگایا جائے، نہ کہ ہر ایک پر الگ الگ۔ اس سے پری پروسیسنگ تیز ہو جاتی ہے۔

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

🤗 Datasets لائبریری یہ پروسیسنگ اس طرح کرتی ہے کہ ڈیٹاسیٹس میں نئے فیلڈز شامل کیے جاتے ہیں، ہر ایک اس ڈکشنری کی کلید کے لیے جو پری پروسیسنگ فنکشن واپس کرتا ہے:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

آپ اپنے پری پروسیسنگ فنکشن کو `map()` کے ذریعے لگاتے وقت `num_proc` آرگیومنٹ پاس کر کے ملٹی پروسیسنگ بھی استعمال کر سکتے ہیں۔ ہم نے یہاں ایسا نہیں کیا کیونکہ 🤗 Tokenizers لائبریری پہلے ہی متعدد تھریڈز استعمال کرتی ہے تاکہ ہمارے سیمپلز کو تیزی سے ٹوکنائز کیا جا سکے، لیکن اگر آپ اس لائبریری کے بیکڈ تیز ٹوکنائزر کا استعمال نہیں کر رہے ہیں تو یہ آپ کی پری پروسیسنگ کو تیز کر سکتا ہے۔

ہمارا `tokenize_function` ایک ڈکشنری واپس کرتا ہے جس میں keys `input_ids`, `attention_mask`, اور `token_type_ids` شامل ہیں، لہٰذا یہ تینوں فیلڈز ہمارے ڈیٹاسیٹ کے تمام سپلٹس میں شامل ہو جاتے ہیں۔ نوٹ کریں کہ اگر ہمارا پری پروسیسنگ فنکشن کسی موجودہ key کے لیے نئی ویلیو واپس کرتا، تو ہم موجودہ فیلڈز کو بھی تبدیل کر سکتے تھے۔

آخری چیز جو ہمیں کرنی ہے وہ یہ ہے کہ جب ہم عناصر کو ایک ساتھ بیچ بناتے ہیں تو تمام مثالوں کو سب سے لمبے عنصر کی لمبائی تک پیڈ کیا جائے — جس تکنیک کو ہم *dynamic padding* کہتے ہیں۔

### ڈائنامک پیڈنگ[[dynamic-padding]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
بیچ کے اندر سیمپلز کو اکٹھا کرنے کے ذمہ دار فنکشن کو *collate function* کہا جاتا ہے۔ یہ ایک آرگیومنٹ ہے جو آپ `DataLoader` بناتے وقت پاس کر سکتے ہیں، ڈیفالٹ میں ایسا فنکشن ہوتا ہے جو صرف آپ کے سیمپلز کو PyTorch tensors میں تبدیل کر کے ان کو concatenate کر دیتا ہے (اگر آپ کے عناصر لسٹس، tuples، یا dictionaries ہوں تو recursively)۔ ہمارے کیس میں یہ ممکن نہیں ہوگا کیونکہ ہمارے پاس موجود ان پٹس سب ایک ہی سائز کے نہیں ہوں گے۔ ہم نے جان بوجھ کر padding کو ملتوی کیا ہے، تاکہ اسے ہر بیچ پر ضرورت کے مطابق ہی لگایا جائے اور بہت زیادہ padding والے over-long inputs سے بچا جائے۔ اس سے ٹریننگ کافی تیز ہو جائے گی، لیکن نوٹ کریں کہ اگر آپ TPU پر ٹریننگ کر رہے ہیں تو یہ مسائل پیدا کر سکتا ہے — TPU کو fixed shapes پسند ہیں، چاہے اس کے لیے اضافی padding کی ضرورت کیوں نہ ہو۔
{:else}
بیچ کے اندر سیمپلز کو اکٹھا کرنے کے ذمہ دار فنکشن کو *collate function* کہا جاتا ہے۔ ڈیفالٹ collator ایک ایسا فنکشن ہے جو صرف آپ کے سیمپلز کو tf.Tensor میں تبدیل کر کے ان کو concatenate کر دیتا ہے (اگر آپ کے عناصر لسٹس، tuples، یا dictionaries ہوں تو recursively)۔ ہمارے کیس میں یہ ممکن نہیں ہوگا کیونکہ ہمارے پاس موجود ان پٹس سب ایک ہی سائز کے نہیں ہوں گے۔ ہم نے جان بوجھ کر padding کو ملتوی کیا ہے، تاکہ اسے ہر بیچ پر ضرورت کے مطابق ہی لگایا جائے اور بہت زیادہ padding والے over-long inputs سے بچا جائے۔ اس سے ٹریننگ کافی تیز ہو جائے گی، لیکن نوٹ کریں کہ اگر آپ TPU پر ٹریننگ کر رہے ہیں تو یہ مسائل پیدا کر سکتا ہے — TPU کو fixed shapes پسند ہیں، چاہے اس کے لیے اضافی padding کی ضرورت کیوں نہ ہو۔
{/if}

عملی طور پر ایسا کرنے کے لیے، ہمیں ایک collate function ڈیفائن کرنا ہوگا جو ہمارے ڈیٹاسیٹ کے آئٹمز پر صحیح مقدار میں padding لگائے جو ہم ایک ساتھ بیچ کرنا چاہتے ہیں۔ خوش قسمتی سے، 🤗 Transformers لائبریری ہمیں `DataCollatorWithPadding` کے ذریعے ایسا فنکشن فراہم کرتی ہے۔ یہ ٹوکنائزر لیتا ہے جب آپ اسے انسٹینشی ایٹ کرتے ہیں (یہ جاننے کے لیے کہ کون سا padding token استعمال کرنا ہے، اور ماڈل کو ان پٹس کے بائیں یا دائیں padding کی توقع ہے) اور آپ کی تمام ضروریات پوری کر دیتا ہے:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

اس نئے ٹول کو ٹیسٹ کرنے کے لیے، آئیے اپنے ٹریننگ سیٹ سے چند سیمپلز لیں جنہیں ہم ایک ساتھ بیچ کرنا چاہتے ہیں۔ یہاں، ہم `idx`, `sentence1`, اور `sentence2` کالمز کو ہٹا دیتے ہیں کیونکہ ان کی ضرورت نہیں ہوگی اور یہ strings پر مشتمل ہیں (اور ہم strings کے ساتھ tensors نہیں بنا سکتے) اور بیچ میں ہر انٹری کی لمبائی دیکھتے ہیں:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

کسی قسم کا حیران کن بات نہیں، ہمیں مختلف لمبائی کے سیمپلز ملتے ہیں، 32 سے 67 تک۔ ڈائنامک پیڈنگ کا مطلب ہے کہ اس بیچ کے تمام سیمپلز کو 67 کی لمبائی تک پیڈ کیا جانا چاہیے، جو کہ بیچ میں سب سے زیادہ ہے۔ ڈائنامک پیڈنگ کے بغیر، تمام سیمپلز کو پورے ڈیٹاسیٹ کی زیادہ سے زیادہ لمبائی یا ماڈل کی قبول کردہ زیادہ سے زیادہ لمبائی تک پیڈ کرنا پڑتا۔ آئیے اس بات کی دوبارہ تصدیق کریں کہ ہمارا `data_collator` بیچ کو درست طریقے سے ڈائنامک پیڈنگ کر رہا ہے:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

بہت اچھا لگ رہا ہے! اب جبکہ ہم نے خام ٹیکسٹ سے بیچز میں تبدیلی کر لی ہے جنہیں ہمارا ماڈل سنبھال سکتا ہے، ہم اسے fine-tune کرنے کے لیے تیار ہیں!
{/if}

<Tip>

✏️ **Try it out!** GLUE SST-2 ڈیٹاسیٹ پر بھی پری پروسیسنگ کو دہرائیں۔ یہ تھوڑا مختلف ہے کیونکہ یہ جوڑوں کی بجائے ایک جملوں پر مشتمل ہے، لیکن جو کچھ ہم نے کیا ہے وہ باقی سب کچھ ویسا ہی ہونا چاہیے۔ ایک مزید مشکل چیلنج کے لیے، ایسی پری پروسیسنگ فنکشن لکھنے کی کوشش کریں جو GLUE کے کسی بھی ٹاسک پر کام کرے۔

</Tip>

{#if fw === 'tf'}

اب جبکہ ہمارے پاس ڈیٹاسیٹ اور ایک data collator موجود ہے، ہمیں ان کو ایک ساتھ ملانا ہے۔ ہم دستی طور پر بیچز لوڈ کر کے انہیں collate کر سکتے ہیں، لیکن یہ بہت زیادہ کام ہے، اور شاید کارکردگی میں بھی کم ہو۔ اس کے بجائے، ایک سادہ طریقہ ہے جو اس مسئلے کا موثر حل پیش کرتا ہے: `to_tf_dataset()`۔ یہ آپ کے ڈیٹاسیٹ کے گرد ایک `tf.data.Dataset` لپیٹ دے گا، ایک اختیاری collation فنکشن کے ساتھ۔ `tf.data.Dataset` ایک مقامی TensorFlow فارمیٹ ہے جسے Keras `model.fit()` کے لیے استعمال کر سکتا ہے، اس لیے یہ ایک طریقہ فوراً ایک 🤗 Dataset کو تربیت کے لیے تیار فارمیٹ میں تبدیل کر دیتا ہے۔ آئیے اسے اپنے ڈیٹاسیٹ کے ساتھ عمل میں دیکھتے ہیں!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

اور بس! ہم ان ڈیٹاسیٹس کو اگلے لیکچر میں لے جا سکتے ہیں، جہاں ڈیٹا پری پروسیسنگ کے سارے سخت کام کے بعد ٹریننگ کافی آسان ہو جائے گی!

{/if}
```