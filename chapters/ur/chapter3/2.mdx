```mdx
<FrameworkSwitchCourse {fw} />

# ฺูนุง ฺฉ ูพุฑุงุณุณูฺฏ[[processing-the-data]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
ูพฺฺพู ุจุงุจ ฺฉ ูุซุงู ฺฉู ุฌุงุฑ ุฑฺฉฺพุช ูุฆุ ุงฺบ ู PyTorch ูฺบ ุงฺฉ ุจฺ ูพุฑ ุณฺฉูุฆูุณ ฺฉูุงุณูุงุฆุฑ ฺฉู ูนุฑู ฺฉุฑู ฺฉุง ุทุฑู ูพุด ฺฉุฑุช ฺบ:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
ูพฺฺพู ุจุงุจ ฺฉ ูุซุงู ฺฉู ุฌุงุฑ ุฑฺฉฺพุช ูุฆุ ุงฺบ ู TensorFlow ูฺบ ุงฺฉ ุจฺ ูพุฑ ุณฺฉูุฆูุณ ฺฉูุงุณูุงุฆุฑ ฺฉู ูนุฑู ฺฉุฑู ฺฉุง ุทุฑู ูพุด ฺฉุฑุช ฺบ:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

ุธุงุฑ ุ ุตุฑู ุฏู ุฌูููฺบ ูพุฑ ูุงฺู ฺฉู ูนุฑู ฺฉุฑูุง ุจุช ุงฺฺพ ูุชุงุฆุฌ ูุฑุงู ูฺบ ฺฉุฑ ฺฏุง ุจุชุฑ ูุชุงุฆุฌ ุญุงุตู ฺฉุฑู ฺฉ ูุ ุขูพ ฺฉู ุงฺฉ ุจฺุง ฺูนุงุณูน ุชุงุฑ ฺฉุฑูุง ูฺฏุง

ุงุณ ุณฺฉุดู ูฺบ ู ูุซุงู ฺฉ ุทูุฑ ูพุฑ MRPC (Microsoft Research Paraphrase Corpus) ฺูนุงุณูน ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏุ ุฌุณ ฺฉุง ุชุนุงุฑู [ุงฺฉ ููุงู](https://www.aclweb.org/anthology/I05-5002.pdf) ูฺบ William B. Dolan ุงูุฑ Chris Brockett ู ฺฉุฑุงุง ุชฺพุง  ฺูนุงุณูน 5,801 ุฌูููฺบ ฺฉ ุฌูฺูฺบ ูพุฑ ูุดุชูู ุ ุฌู ฺฉ ุณุงุชฺพ ุงฺฉ ูุจู ุจฺพ ูุชุง  ุฌู ุจุชุงุชุง  ฺฉ ุขุง  ูพุฑุง ูุฑุฒ ฺบ ุง ูฺบ (ุนูุ ุงฺฏุฑ ุฏูููฺบ ุฌูููฺบ ฺฉุง ูุทูุจ ุงฺฉ  ) ู ู ุงุณ ุงุณ ุจุงุจ ฺฉ ู ููุชุฎุจ ฺฉุง  ฺฉููฺฉ  ุงฺฉ ฺฺพููนุง ฺูนุงุณูน ุ ุงุณ ู ุงุณ ูพุฑ ูนุฑููฺฏ ฺฉ ุชุฌุฑุจุงุช ฺฉุฑูุง ุขุณุงู 

### Hub ุณ ฺูนุงุณูน ููฺ ฺฉุฑูุง[[loading-a-dataset-from-the-hub]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub ูฺบ ุตุฑู ูุงฺูุฒ ูฺบ ูุชุ ุจูฺฉ ุงุณ ูฺบ ูุฎุชูู ุฒุจุงููฺบ ูฺบ ูุชุนุฏุฏ ฺูนุงุณูนุณ ุจฺพ ููุฌูุฏ ฺบ ุขูพ ฺูนุงุณูนุณ ฺฉู [ุงฺบ](https://huggingface.co/datasets) ุฏฺฉฺพ ุณฺฉุช ฺบุ ุงูุฑ ู ุชุฌูุฒ ฺฉุฑุช ฺบ ฺฉ ุขูพ ุงุณ ุณฺฉุดู ุณ ฺฏุฒุฑู ฺฉ ุจุนุฏ ุงฺฉ ูุง ฺูนุงุณูน ููฺ ุงูุฑ ูพุฑูุณุณ ฺฉุฑู ฺฉ ฺฉูุดุด ฺฉุฑฺบ (ุนุงู ุฏุณุชุงูุฒุงุช ฺฉ ู [ุงฺบ](https://huggingface.co/docs/datasets/loading) ููุงุญุธ ฺฉุฑฺบ) ูฺฉู ู ุงูุญุงูุ ุขุฆฺบ MRPC ฺูนุงุณูน ูพุฑ ุชูุฌ ูุฑฺฉูุฒ ฺฉุฑฺบ!  [GLUE benchmark](https://gluebenchmark.com/) ฺฉ 10 ฺูนุงุณูนุณ ูฺบ ุณ ุงฺฉ ุ ุฌู ฺฉ ุงฺฉ ุนูู ุจูฺ ูุงุฑฺฉ  ุงูุฑ ML ูุงฺูุฒ ฺฉ ฺฉุงุฑฺฉุฑุฏฺฏ ฺฉู 10 ูุฎุชูู ูนฺฉุณูน ฺฉูุงุณูฺฉุดู ูนุงุณฺฉุณ ูฺบ ูุงูพู ฺฉ ู ุงุณุชุนูุงู ูุชุง 

๐ค Datasets ูุงุฆุจุฑุฑ Hub ูพุฑ ฺูนุงุณูน ฺุงุคู ููฺ ุงูุฑ ฺฉุด ฺฉุฑู ฺฉ ู ุงฺฉ ุจุช ุณุงุฏ ฺฉูุงูฺ ูุฑุงู ฺฉุฑุช  ู MRPC ฺูนุงุณูน ฺฉู ุงุณ ุทุฑุญ ฺุงุคู ููฺ ฺฉุฑ ุณฺฉุช ฺบ:

<Tip>
โ๏ธ **ุงูุชุจุง** ุงุณ ุจุงุช ฺฉู ูู ุจูุงุฆฺบ ฺฉ `datasets` ุงูุณูนุงู ุ `pip install datasets` ฺูุง ฺฉุฑ ูพฺพุฑุ MRPC ฺูนุงุณูน ฺฉู ููฺ ฺฉุฑฺบ ุงูุฑ ูพุฑููน ฺฉุฑฺบ ุชุงฺฉ ุขูพ ุฏฺฉฺพ ุณฺฉฺบ ฺฉ ุงุณ ูฺบ ฺฉุง ุดุงูู 
</Tip> 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

ุฌุณุง ฺฉ ุขูพ ุฏฺฉฺพ ุณฺฉุช ฺบุ ูฺบ ุงฺฉ `DatasetDict` ุขุจุฌฺฉูน ููุชุง  ุฌุณ ูฺบ ูนุฑููฺฏ ุณูนุ ููฺุดู ุณูนุ ุงูุฑ ูนุณูน ุณูน ุดุงูู ฺบ ุงู ูฺบ ุณ ุฑ ุงฺฉ ูฺบ ฺฉุฆ ฺฉุงููุฒ (`sentence1`, `sentence2`, `label`, ุงูุฑ `idx`) ุดุงูู ฺบ ุงูุฑ ูุทุงุฑูฺบ ฺฉ ุชุนุฏุงุฏ ูุชุบุฑ ูุช ุ ุฌู ุฑ ุณูน ูฺบ ุนูุงุตุฑ ฺฉ ุชุนุฏุงุฏ ฺฉู ุธุงุฑ ฺฉุฑุช  (ุนูุ ูนุฑููฺฏ ุณูน ูฺบ 3,668 ุฌูููฺบ ฺฉ ุฌูฺุ ููฺุดู ุณูน ูฺบ 408 ุงูุฑ ูนุณูน ุณูน ูฺบ 1,725 ุฌูู ฺฉ ุฌูฺ ููุฌูุฏ ฺบ)

 ฺฉูุงูฺ ฺูนุงุณูน ฺฉู ฺุงุคู ููฺ ุงูุฑ ฺฉุด ฺฉุฑุช ุ ฺูุงููน ฺฉ ุทูุฑ ูพุฑ *~/.cache/huggingface/datasets* ูฺบ ุจุงุจ 2 ุณ ุงุฏ ุฑฺฉฺพฺบ ฺฉ ุขูพ `HF_HOME` ุงููุงุฆุฑูููน ูุฑุจู ุณูน ฺฉุฑ ฺฉ ุงูพู ฺฉุด ูููฺุฑ ฺฉู ุญุณุจ ุถุฑูุฑุช ุจูุง ุณฺฉุช ฺบ

ู ุงูพู `raw_datasets` ุขุจุฌฺฉูน ูฺบ ููุฌูุฏ ุฑ ุฌูู ฺฉ ุฌูฺ ุชฺฉ ุงูฺฺฉุณูฺฏ ฺฉ ุฐุฑุน ุฑุณุงุฆ ุญุงุตู ฺฉุฑ ุณฺฉุช ฺบุ ุฌุณ ฺฉ ุงฺฉ ฺฺฉุดูุฑ ฺฉ ุณุงุชฺพ:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence.'}
```

ู ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ ูุจูุฒ ูพู ุณ  ุงููนุฌุฑ ฺบุ ููฐุฐุง ูฺบ ูุงฺบ ฺฉูุฆ ูพุฑ ูพุฑูุณุณูฺฏ ฺฉุฑู ฺฉ ุถุฑูุฑุช ูฺบ ูฺฏ  ุฌุงูู ฺฉ ู ฺฉ ฺฉูู ุณุง ุงููนุฌุฑ ฺฉุณ ูุจู ุณ ูุชุนูู ุ ู ุงูพู `raw_train_dataset` ฺฉ `features` ฺฉุง ูุนุงุฆู ฺฉุฑ ุณฺฉุช ฺบ  ูฺบ ุฑ ฺฉุงูู ฺฉ ูุณู ุจุชุงุฆ ฺฏุง:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

ูพุฑุฏ ฺฉ ูพฺฺพุ `label` ฺฉ ูุณู `ClassLabel` ุ ุงูุฑ ุงููนุฌุฑุฒ ุณ ูุจู ูู ฺฉ ููพูฺฏ *names* ูููฺุฑ ูฺบ ูุญููุธ ฺฉ ฺฏุฆ  `0` ฺฉุง ุชุนูู `not_equivalent` ุณ ุ ุงูุฑ `1` ฺฉุง ุชุนูู `equivalent` ุณ 

<Tip>

โ๏ธ **Try it out!** ุชุฑุจุช ุณูน ฺฉ ุนูุตุฑ 15 ุงูุฑ ููฺุดู ุณูน ฺฉ ุนูุตุฑ 87 ฺฉู ุฏฺฉฺพฺบ ุงู ฺฉ ูุจูุฒ ฺฉุง ฺบุ

</Tip>

### ฺูนุงุณูน ฺฉ ูพุฑ ูพุฑูุณุณูฺฏ[[preprocessing-a-dataset]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ฺูนุงุณูน ฺฉ ูพุฑ ูพุฑูุณุณูฺฏ ฺฉ ูุ ูฺบ ูนฺฉุณูน ฺฉู ููุจุฑุฒ ูฺบ ุชุจุฏู ฺฉุฑูุง ูฺฏุง ุฌุณ ูุงฺู ุณูุฌฺพ ุณฺฉ ุฌุณุง ฺฉ ุขูพ ู [ูพฺฺพู ุจุงุจ](/course/chapter2) ูฺบ ุฏฺฉฺพุงุ  ูนูฺฉูุงุฆุฒุฑ ฺฉ ุฐุฑุน ฺฉุง ุฌุงุชุง  ู ูนูฺฉูุงุฆุฒุฑ ฺฉู ุงฺฉ ุฌูู ุง ุฌูููฺบ ฺฉ ูุฑุณุช ูุฑุงู ฺฉุฑ ุณฺฉุช ฺบุ ููฐุฐุง ู ุฑ ุฌูฺ ฺฉ ูพู ุฌูููฺบ ุงูุฑ ุฏูุณุฑ ุฌูููฺบ ฺฉู ุงุณ ุทุฑุญ ุจุฑุง ุฑุงุณุช ูนูฺฉูุงุฆุฒ ฺฉุฑ ุณฺฉุช ฺบ:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ุชุงูุ ู ุตุฑู ุฏู ุณฺฉูุฆูุณุฒ ฺฉู ูุงฺู ูฺบ ูพุงุณ ฺฉุฑ ฺฉ  ูพุด ฺฏูุฆ ูฺบ ฺฉุฑ ุณฺฉุช ฺฉ ุขุง  ุฏูููฺบ ุฌูู ูพุฑุง ูุฑุฒ ฺบ ุง ูฺบ ูฺบ ุฏูููฺบ ุณฺฉูุฆูุณุฒ ฺฉู ุงฺฉ ุฌูฺ ฺฉ ุทูุฑ ูพุฑ ูฺู ฺฉุฑูุง ูฺฏุงุ ุงูุฑ ููุงุณุจ ูพุฑ ูพุฑูุณุณูฺฏ ูฺฏุงู ูฺฏ ุฎูุด ูุณูุช ุณุ ูนูฺฉูุงุฆุฒุฑ ุงฺฉ ุฌูฺ ฺฉ ุณฺฉูุฆูุณุฒ ฺฉู ุจฺพ ุงุณ ุงูุฏุงุฒ ูฺบ ุชุงุฑ ฺฉุฑ ุณฺฉุชุง  ุฌุณุง ฺฉ ูุงุฑุง BERT ูุงฺู ุชููุน ฺฉุฑุชุง : 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

ู ู [ุจุงุจ 2](/course/chapter2) ูฺบ `input_ids` ุงูุฑ `attention_mask` ฺฉ ุจุงุช ฺฉ ุชฺพุ ูฺฉู `token_type_ids` ูพุฑ ุจุงุช ฺฉุฑู ฺฉู ููุชู ฺฉุฑ ุฏุง ฺฏุง ุชฺพุง ุงุณ ูุซุงู ูฺบุ  ูุงฺู ฺฉู ุจุชุงุชุง  ฺฉ ุงู ูพูน ฺฉุง ฺฉูู ุณุง ุญุต ูพูุง ุฌูู  ุงูุฑ ฺฉูู ุณุง ุฏูุณุฑุง

<Tip>

โ๏ธ **Try it out!** ุชุฑุจุช ุณูน ฺฉ ุนูุตุฑ 15 ฺฉู ูฺบ ุงูุฑ ุฏูููฺบ ุฌูููฺบ ฺฉู ุนูุญุฏ ุงูุฑ ุงฺฉ ุฌูฺ ฺฉ ุทูุฑ ูพุฑ ูนูฺฉูุงุฆุฒ ฺฉุฑฺบ ุฏูููฺบ ูุชุงุฆุฌ ูฺบ ฺฉุง ูุฑู ุ

</Tip>

ุงฺฏุฑ ู `input_ids` ฺฉ ุงูุฏุฑ ููุฌูุฏ IDs ฺฉู ุงููุงุธ ูฺบ ูุงูพุณ ฺ ฺฉูฺ ฺฉุฑฺบ:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ุชู ูฺบ ุญุงุตู ูฺฏุง:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

ูฺบ ู ุฏฺฉฺพุช ฺบ ฺฉ ูุงฺู ุชููุน ฺฉุฑุชุง  ฺฉ ุงู ูพูน ฺฉุง ูุงุฑููน `[CLS] sentence1 [SEP] sentence2 [SEP]` ู ุฌุจ ุฏู ุฌูู ูฺบ `token_type_ids` ฺฉ ุณุงุชฺพ ู ุงุณ ฺฉุง ููุงุฒู ฺฉุฑุช ฺบ:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ุฌุณุง ฺฉ ุขูพ ุฏฺฉฺพ ุณฺฉุช ฺบุ `[CLS] sentence1 [SEP]` ุณ ูุชุนูู ุงู ูพูน ฺฉ ุญุตูฺบ ฺฉุง ูนูฺฉู ูนุงุฆูพ ID `0` ูุชุง ุ ุฌุจฺฉ ุฏูุณุฑ ุญุตูฺบุ ุฌู ฺฉ `sentence2 [SEP]` ุณ ูุชุนูู ฺบุ ุงู ฺฉุง ูนูฺฉู ูนุงุฆูพ ID `1` ูุชุง 

ูููน ฺฉุฑฺบ ฺฉ ุงฺฏุฑ ุขูพ ฺฉูุฆ ูุฎุชูู checkpoint ููุชุฎุจ ฺฉุฑุช ฺบุ ุชู ุถุฑูุฑ ูฺบ ฺฉ ุขูพ ฺฉ tokenized inputs ูฺบ `token_type_ids` ููุฌูุฏ ูฺบ (ูุซูุงูุ ุงฺฏุฑ ุขูพ DistilBERT ูุงฺู ุงุณุชุนูุงู ฺฉุฑุช ฺบ ุชู  ูุงูพุณ ูฺบ ฺฉ ุฌุงุช)  ุตุฑู ุงุณ ุตูุฑุช ูฺบ ูุงูพุณ ฺฉ ุฌุงุช ฺบ ุฌุจ ูุงฺู ฺฉู ุงู ฺฉุง ุงุณุชุนูุงู ูุนููู ูุ ฺฉููฺฉ ูุงฺู ู ุงูพู ูพุฑ ูนุฑููฺฏ ฺฉ ุฏูุฑุงู ุงูฺบ ุฏฺฉฺพุง ูุชุง  

ุงฺบุ BERT ฺฉู token type IDs ฺฉ ุณุงุชฺพ ูพุฑ ูนุฑู ฺฉุง ฺฏุง ุ ุงูุฑ ูุงุณฺฉฺ ููฺฏูุฌ ูุงฺููฺฏ ุขุจุฌฺฉูนู ฺฉ ุนูุงู ุฌุณ ฺฉุง ู ู [ุจุงุจ 1](/course/chapter1) ูฺบ ุฐฺฉุฑ ฺฉุงุ ุงุณ ฺฉุง ุงฺฉ ุงุถุงู ููุตุฏ  ุฌุณ _next sentence prediction_ ฺฉุช ฺบ ุงุณ ฺฉุงู ฺฉุง ููุตุฏ ุฌูููฺบ ฺฉ ุฌูฺูฺบ ฺฉ ุฏุฑูุงู ุชุนูู ฺฉู ูุงฺู ฺฉุฑูุง 

Next sentence prediction ูฺบุ ูุงฺู ฺฉู ุฌูููฺบ ฺฉ ุฌูฺ ูุฑุงู ฺฉ ุฌุงุช ฺบ (ุฌุณ ูฺบ ุชุตุงุฏู ุทูุฑ ูพุฑ ูุงุณฺฉ ฺฉ ฺฏุฆ ูนูฺฉู ูุช ฺบ) ุงูุฑ ุงุณ ูพุด ฺฏูุฆ ฺฉุฑู ฺฉู ฺฉุง ุฌุงุชุง  ฺฉ ุขุง ุฏูุณุฑุง ุฌูู ูพู ฺฉ ุจุนุฏ ุขุชุง  ุง ูฺบ ฺฉุงู ฺฉู ุบุฑ ูุนููู ุจูุงู ฺฉ ูุ ุขุฏฺพ ููุช ุฌูู ุงุตู ุฏุณุชุงูุฒ ุณ ุงฺฉ ุฏูุณุฑ ฺฉ ูพฺฺพ ูุช ฺบุ ุงูุฑ ุจุงู ุขุฏฺพ ููุช ุฏูููฺบ ุฌูู ุฏู ูุฎุชูู ุฏุณุชุงูุฒุงุช ุณ ุขุช ฺบ

ุนููููุงุ ุขูพ ฺฉู ุงุณ ุจุงุช ฺฉ ูฺฉุฑ ฺฉุฑู ฺฉ ุถุฑูุฑุช ูฺบ ฺฉ ุขูพ ฺฉ tokenized inputs ูฺบ `token_type_ids` ฺบ ุง ูฺบ: ุฌุจ ุชฺฉ ุขูพ ูนูฺฉูุงุฆุฒุฑ ุงูุฑ ูุงฺู ฺฉ ู ุงฺฉ  checkpoint ุงุณุชุนูุงู ฺฉุฑุช ฺบุ ุณุจ ฺฉฺฺพ ูนฺพฺฉ ุฑ ฺฏุง ฺฉููฺฉ ูนูฺฉูุงุฆุฒุฑ ฺฉู ูุนููู ูุชุง  ฺฉ ูุงฺู ฺฉู ฺฉุง ูุฑุงู ฺฉุฑูุง 

ุงุจ ุฌุจฺฉ ู ู ุฏฺฉฺพุง ฺฉ ูุงุฑุง ูนูฺฉูุงุฆุฒุฑ ุงฺฉ ุฌูู ฺฉ ุฌูฺ ฺฉู ฺฉุณ ูฺู ฺฉุฑุชุง ุ ู ุงุณ ุงูพู ูพูุฑ ฺูนุงุณูน ฺฉู ูนูฺฉูุงุฆุฒ ฺฉุฑู ฺฉ ู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ: ุฌุณ ฺฉ [ูพฺฺพู ุจุงุจ](/course/chapter2) ูฺบุ ู ูนูฺฉูุงุฆุฒุฑ ฺฉู ุฌูููฺบ ฺฉ ุฌูฺูฺบ ฺฉ ูุฑุณุช ุฏ ฺฉุฑุ ูพู ุฌูููฺบ ฺฉ ูุฑุณุช ุงูุฑ ูพฺพุฑ ุฏูุณุฑ ุฌูููฺบ ฺฉ ูุฑุณุช ุฏ ุณฺฉุช ฺบ  padding ุงูุฑ truncation ฺฉ ุขูพุดูุฒ ฺฉ ุณุงุชฺพ ุจฺพ ูุทุงุจูุช ุฑฺฉฺพุชุง  ุฌู ู ู [ุจุงุจ 2](/course/chapter2) ูฺบ ุฏฺฉฺพ ุชฺพ ููฐุฐุงุ ูนุฑููฺฏ ฺูนุงุณูน ฺฉ ูพุฑ ูพุฑูุณุณูฺฏ ฺฉุง ุงฺฉ ุทุฑู  :

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

 ุทุฑู ูนฺพฺฉ ฺฉุงู ฺฉุฑุชุง ุ ูฺฉู ุงุณ ฺฉุง ุงฺฉ ููุตุงู   ฺฉ  ุงฺฉ ฺฺฉุดูุฑ ูุงูพุณ ฺฉุฑุชุง  (ูุงุฑ keys ุฌุณ `input_ids`, `attention_mask`, ุงูุฑ `token_type_ids` ฺฉ ุณุงุชฺพุ ุงูุฑ ุงู ฺฉ ูููุฒ ุฌู ฺฉ ูุณูนุณ ฺฉ ูุณูนุณ ูุช ฺบ)  ุตุฑู ุงุณ ุตูุฑุช ูฺบ ฺฉุงู ฺฉุฑ ฺฏุง ุงฺฏุฑ ุขูพ ฺฉ ูพุงุณ ูพูุฑ ฺูนุงุณูน ฺฉู ูนูฺฉูุงุฆุฒ ฺฉุฑู ฺฉ ุฏูุฑุงู ุงุณ ุงุณูนูุฑ ฺฉุฑู ฺฉ ู ฺฉุงู RAM ููุฌูุฏ ู (ุฌุจฺฉ ๐ค Datasets ูุงุฆุจุฑุฑ ฺฉ ฺูนุงุณูนุณ [Apache Arrow](https://arrow.apache.org/) ูุงุฆูุฒ ฺฉ ุตูุฑุช ูฺบ ฺุณฺฉ ูพุฑ ูุญููุธ ูุช ฺบุ ุงุณ ู ุขูพ ุตุฑู ู ุณููพูุฒ ููฺ ฺฉุฑุช ฺบ ุฌู ฺฉ ุขูพ ู ุฏุฑุฎูุงุณุช ฺฉ ูุช )

ฺูนุง ฺฉู ุงฺฉ ฺูนุงุณูน ฺฉ ุตูุฑุช ูฺบ ุจุฑูุฑุงุฑ ุฑฺฉฺพู ฺฉ ูุ ู [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) ูุชฺพฺ ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ  ุงุถุงู ูฺฺฉ ุจฺพ ูุฑุงู ฺฉุฑุชุง ุ ุงฺฏุฑ ูฺบ ุตุฑู ูนูฺฉูุงุฆุฒุดู ุณ ุฒุงุฏ ูพุฑ ูพุฑูุณุณูฺฏ ฺฉ ุถุฑูุฑุช ู `map()` ูุชฺพฺ ุฑ ุนูุตุฑ ูพุฑ ุงฺฉ ููฺฉุดู ูฺฏุงู ุณ ฺฉุงู ฺฉุฑุชุง ุ ููฐุฐุง ุขุฆ ุงฺฉ ุงุณุง ููฺฉุดู ฺูุงุฆู ฺฉุฑุช ฺบ ุฌู ูุงุฑ ุงู ูพูนุณ ฺฉู ูนูฺฉูุงุฆุฒ ฺฉุฑ:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

 ููฺฉุดู ุงฺฉ ฺฺฉุดูุฑ ูุชุง  (ุฌุณุง ฺฉ ูุงุฑ ฺูนุงุณูน ฺฉ ุขุฆูนูุฒ) ุงูุฑ ุงฺฉ ูุฆ ฺฺฉุดูุฑ ูุงูพุณ ฺฉุฑุชุง  ุฌุณ ูฺบ keys `input_ids`, `attention_mask`, ุงูุฑ `token_type_ids` ุดุงูู ูุช ฺบ ูููน ฺฉุฑฺบ ฺฉ ุงฺฏุฑ `example` ฺฺฉุดูุฑ ูฺบ ฺฉุฆ ุณููพูุฒ ุดุงูู ูฺบ (ุฑ key ฺฉ ุณุงุชฺพ ุฌูููฺบ ฺฉ ูุณูน) ุชู ุจฺพ  ฺฉุงู ฺฉุฑุชุง  ฺฉููฺฉ `tokenizer` ูพู ุฌุณุง ฺฉ ุฏฺฉฺพุง ฺฏุง ุ ุฌูููฺบ ฺฉ ุฌูฺูฺบ ฺฉ ูุณูน ูพุฑ ฺฉุงู ฺฉุฑุชุง   ูฺบ `map()` ฺฉุงู ูฺบ `batched=True` ุขูพุดู ุงุณุชุนูุงู ฺฉุฑู ฺฉ ุงุฌุงุฒุช ุฏ ฺฏุงุ ุฌู ูนูฺฉูุงุฆุฒุดู ฺฉู ุจุช ุชุฒ ฺฉุฑ ุฏ ฺฏุง `tokenizer` ฺฉู [๐ค Tokenizers](https://github.com/huggingface/tokenizers) ูุงุฆุจุฑุฑ ฺฉ ุทุฑู ุณ Rust ูฺบ ูฺฉฺพ ฺฏุฆ ุงฺฉ ูนูฺฉูุงุฆุฒุฑ ุณ ุณูพูุฑูน ููุช   ูนูฺฉูุงุฆุฒุฑ ุจุช ุชุฒ ู ุณฺฉุชุง ุ ูฺฉู ุตุฑู ุงุณ ุตูุฑุช ูฺบ ุฌุจ ู ุงุณ ุจฺฉ ููุช ุจุช ุณ ุงู ูพูนุณ ูุฑุงู ฺฉุฑฺบ

ูููน ฺฉุฑฺบ ฺฉ ู ู ุงุจฺพ ุงูพู ูนูฺฉูุงุฆุฒุดู ููฺฉุดู ูฺบ `padding` ุขุฑฺฏููููน ฺฺพูฺ ุฏุง  ุงุณ ฺฉ ูุฌ   ฺฉ ุชูุงู ุณููพูุฒ ฺฉู ุฒุงุฏ ุณ ุฒุงุฏ ููุจุงุฆ ุชฺฉ ูพฺ ฺฉุฑูุง ูุคุซุฑ ูฺบ : ุจุชุฑ  ฺฉ ุณููพูุฒ ฺฉู ุจฺ ุจูุงุช ููุช ูพฺ ฺฉุฑฺบุ ฺฉููฺฉ ุงุณ ุณ ูฺบ ุตุฑู ุงุณ ุจฺ ฺฉ ุฒุงุฏ ุณ ุฒุงุฏ ููุจุงุฆ ุชฺฉ ูพฺ ฺฉุฑูุง ูพฺ ฺฏุงุ ู ฺฉ ูพูุฑ ฺูนุงุณูน ฺฉ ุฒุงุฏ ุณ ุฒุงุฏ ููุจุงุฆ ุชฺฉ  ุงู ูพูนุณ ฺฉ ูุชุบุฑ ููุจุงุฆูฺบ ฺฉ ุณุงุชฺพ ุจุช ุณุง ููุช ุงูุฑ ูพุฑูุณุณูฺฏ ูพุงูุฑ ุจฺุง ุณฺฉุชุง !

ุงฺบ  ุทุฑู  ฺฉ ู ุงูพู ุชูุงู ฺูนุงุณูนุณ ูพุฑ ุงฺฉ ุณุงุชฺพ ูนูฺฉูุงุฆุฒุดู ููฺฉุดู ฺฉุณ ูฺฏุงุช ฺบ ู `map` ฺฉุงู ูฺบ `batched=True` ุงุณุชุนูุงู ฺฉุฑ ุฑ ฺบ ุชุงฺฉ ููฺฉุดู ุงฺฉ ุณุงุชฺพ ฺูนุงุณูน ฺฉ ูุชุนุฏุฏ ุนูุงุตุฑ ูพุฑ ูฺฏุงุง ุฌุงุฆุ ู ฺฉ ุฑ ุงฺฉ ูพุฑ ุงูฺฏ ุงูฺฏ ุงุณ ุณ ูพุฑ ูพุฑูุณุณูฺฏ ุชุฒ ู ุฌุงุช 

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

๐ค Datasets ูุงุฆุจุฑุฑ  ูพุฑูุณุณูฺฏ ุงุณ ุทุฑุญ ฺฉุฑุช  ฺฉ ฺูนุงุณูนุณ ูฺบ ูุฆ ููฺุฒ ุดุงูู ฺฉ ุฌุงุช ฺบุ ุฑ ุงฺฉ ุงุณ ฺฺฉุดูุฑ ฺฉ ฺฉูุฏ ฺฉ ู ุฌู ูพุฑ ูพุฑูุณุณูฺฏ ููฺฉุดู ูุงูพุณ ฺฉุฑุชุง :

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

ุขูพ ุงูพู ูพุฑ ูพุฑูุณุณูฺฏ ููฺฉุดู ฺฉู `map()` ฺฉ ุฐุฑุน ูฺฏุงุช ููุช `num_proc` ุขุฑฺฏููููน ูพุงุณ ฺฉุฑ ฺฉ ูููน ูพุฑูุณุณูฺฏ ุจฺพ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ู ู ุงฺบ ุงุณุง ูฺบ ฺฉุง ฺฉููฺฉ ๐ค Tokenizers ูุงุฆุจุฑุฑ ูพู  ูุชุนุฏุฏ ุชฺพุฑฺุฒ ุงุณุชุนูุงู ฺฉุฑุช  ุชุงฺฉ ูุงุฑ ุณููพูุฒ ฺฉู ุชุฒ ุณ ูนูฺฉูุงุฆุฒ ฺฉุง ุฌุง ุณฺฉุ ูฺฉู ุงฺฏุฑ ุขูพ ุงุณ ูุงุฆุจุฑุฑ ฺฉ ุจฺฉฺ ุชุฒ ูนูฺฉูุงุฆุฒุฑ ฺฉุง ุงุณุชุนูุงู ูฺบ ฺฉุฑ ุฑ ฺบ ุชู  ุขูพ ฺฉ ูพุฑ ูพุฑูุณุณูฺฏ ฺฉู ุชุฒ ฺฉุฑ ุณฺฉุชุง 

ูุงุฑุง `tokenize_function` ุงฺฉ ฺฺฉุดูุฑ ูุงูพุณ ฺฉุฑุชุง  ุฌุณ ูฺบ keys `input_ids`, `attention_mask`, ุงูุฑ `token_type_ids` ุดุงูู ฺบุ ููฐุฐุง  ุชููฺบ ููฺุฒ ูุงุฑ ฺูนุงุณูน ฺฉ ุชูุงู ุณูพููนุณ ูฺบ ุดุงูู ู ุฌุงุช ฺบ ูููน ฺฉุฑฺบ ฺฉ ุงฺฏุฑ ูุงุฑุง ูพุฑ ูพุฑูุณุณูฺฏ ููฺฉุดู ฺฉุณ ููุฌูุฏ key ฺฉ ู ูุฆ ููู ูุงูพุณ ฺฉุฑุชุงุ ุชู ู ููุฌูุฏ ููฺุฒ ฺฉู ุจฺพ ุชุจุฏู ฺฉุฑ ุณฺฉุช ุชฺพ

ุขุฎุฑ ฺุฒ ุฌู ูฺบ ฺฉุฑู  ู   ฺฉ ุฌุจ ู ุนูุงุตุฑ ฺฉู ุงฺฉ ุณุงุชฺพ ุจฺ ุจูุงุช ฺบ ุชู ุชูุงู ูุซุงููฺบ ฺฉู ุณุจ ุณ ููุจ ุนูุตุฑ ฺฉ ููุจุงุฆ ุชฺฉ ูพฺ ฺฉุง ุฌุงุฆ โ ุฌุณ ุชฺฉูฺฉ ฺฉู ู *dynamic padding* ฺฉุช ฺบ

### ฺุงุฆูุงูฺฉ ูพฺูฺฏ[[dynamic-padding]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
ุจฺ ฺฉ ุงูุฏุฑ ุณููพูุฒ ฺฉู ุงฺฉูนฺพุง ฺฉุฑู ฺฉ ุฐู ุฏุงุฑ ููฺฉุดู ฺฉู *collate function* ฺฉุง ุฌุงุชุง   ุงฺฉ ุขุฑฺฏููููน  ุฌู ุขูพ `DataLoader` ุจูุงุช ููุช ูพุงุณ ฺฉุฑ ุณฺฉุช ฺบุ ฺูุงููน ูฺบ ุงุณุง ููฺฉุดู ูุชุง  ุฌู ุตุฑู ุขูพ ฺฉ ุณููพูุฒ ฺฉู PyTorch tensors ูฺบ ุชุจุฏู ฺฉุฑ ฺฉ ุงู ฺฉู concatenate ฺฉุฑ ุฏุชุง  (ุงฺฏุฑ ุขูพ ฺฉ ุนูุงุตุฑ ูุณูนุณุ tuplesุ ุง dictionaries ูฺบ ุชู recursively) ูุงุฑ ฺฉุณ ูฺบ  ููฺฉู ูฺบ ูฺฏุง ฺฉููฺฉ ูุงุฑ ูพุงุณ ููุฌูุฏ ุงู ูพูนุณ ุณุจ ุงฺฉ  ุณุงุฆุฒ ฺฉ ูฺบ ูฺบ ฺฏ ู ู ุฌุงู ุจูุฌฺพ ฺฉุฑ padding ฺฉู ููุชู ฺฉุง ุ ุชุงฺฉ ุงุณ ุฑ ุจฺ ูพุฑ ุถุฑูุฑุช ฺฉ ูุทุงุจู  ูฺฏุงุง ุฌุงุฆ ุงูุฑ ุจุช ุฒุงุฏ padding ูุงู over-long inputs ุณ ุจฺุง ุฌุงุฆ ุงุณ ุณ ูนุฑููฺฏ ฺฉุงู ุชุฒ ู ุฌุงุฆ ฺฏุ ูฺฉู ูููน ฺฉุฑฺบ ฺฉ ุงฺฏุฑ ุขูพ TPU ูพุฑ ูนุฑููฺฏ ฺฉุฑ ุฑ ฺบ ุชู  ูุณุงุฆู ูพุฏุง ฺฉุฑ ุณฺฉุชุง  โ TPU ฺฉู fixed shapes ูพุณูุฏ ฺบุ ฺุง ุงุณ ฺฉ ู ุงุถุงู padding ฺฉ ุถุฑูุฑุช ฺฉูฺบ ู ู
{:else}
ุจฺ ฺฉ ุงูุฏุฑ ุณููพูุฒ ฺฉู ุงฺฉูนฺพุง ฺฉุฑู ฺฉ ุฐู ุฏุงุฑ ููฺฉุดู ฺฉู *collate function* ฺฉุง ุฌุงุชุง  ฺูุงููน collator ุงฺฉ ุงุณุง ููฺฉุดู  ุฌู ุตุฑู ุขูพ ฺฉ ุณููพูุฒ ฺฉู tf.Tensor ูฺบ ุชุจุฏู ฺฉุฑ ฺฉ ุงู ฺฉู concatenate ฺฉุฑ ุฏุชุง  (ุงฺฏุฑ ุขูพ ฺฉ ุนูุงุตุฑ ูุณูนุณุ tuplesุ ุง dictionaries ูฺบ ุชู recursively) ูุงุฑ ฺฉุณ ูฺบ  ููฺฉู ูฺบ ูฺฏุง ฺฉููฺฉ ูุงุฑ ูพุงุณ ููุฌูุฏ ุงู ูพูนุณ ุณุจ ุงฺฉ  ุณุงุฆุฒ ฺฉ ูฺบ ูฺบ ฺฏ ู ู ุฌุงู ุจูุฌฺพ ฺฉุฑ padding ฺฉู ููุชู ฺฉุง ุ ุชุงฺฉ ุงุณ ุฑ ุจฺ ูพุฑ ุถุฑูุฑุช ฺฉ ูุทุงุจู  ูฺฏุงุง ุฌุงุฆ ุงูุฑ ุจุช ุฒุงุฏ padding ูุงู over-long inputs ุณ ุจฺุง ุฌุงุฆ ุงุณ ุณ ูนุฑููฺฏ ฺฉุงู ุชุฒ ู ุฌุงุฆ ฺฏุ ูฺฉู ูููน ฺฉุฑฺบ ฺฉ ุงฺฏุฑ ุขูพ TPU ูพุฑ ูนุฑููฺฏ ฺฉุฑ ุฑ ฺบ ุชู  ูุณุงุฆู ูพุฏุง ฺฉุฑ ุณฺฉุชุง  โ TPU ฺฉู fixed shapes ูพุณูุฏ ฺบุ ฺุง ุงุณ ฺฉ ู ุงุถุงู padding ฺฉ ุถุฑูุฑุช ฺฉูฺบ ู ู
{/if}

ุนูู ุทูุฑ ูพุฑ ุงุณุง ฺฉุฑู ฺฉ ูุ ูฺบ ุงฺฉ collate function ฺูุงุฆู ฺฉุฑูุง ูฺฏุง ุฌู ูุงุฑ ฺูนุงุณูน ฺฉ ุขุฆูนูุฒ ูพุฑ ุตุญุญ ููุฏุงุฑ ูฺบ padding ูฺฏุงุฆ ุฌู ู ุงฺฉ ุณุงุชฺพ ุจฺ ฺฉุฑูุง ฺุงุช ฺบ ุฎูุด ูุณูุช ุณุ ๐ค Transformers ูุงุฆุจุฑุฑ ูฺบ `DataCollatorWithPadding` ฺฉ ุฐุฑุน ุงุณุง ููฺฉุดู ูุฑุงู ฺฉุฑุช   ูนูฺฉูุงุฆุฒุฑ ูุชุง  ุฌุจ ุขูพ ุงุณ ุงูุณูนูุด ุงูน ฺฉุฑุช ฺบ ( ุฌุงูู ฺฉ ู ฺฉ ฺฉูู ุณุง padding token ุงุณุชุนูุงู ฺฉุฑูุง ุ ุงูุฑ ูุงฺู ฺฉู ุงู ูพูนุณ ฺฉ ุจุงุฆฺบ ุง ุฏุงุฆฺบ padding ฺฉ ุชููุน ) ุงูุฑ ุขูพ ฺฉ ุชูุงู ุถุฑูุฑุงุช ูพูุฑ ฺฉุฑ ุฏุชุง :

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

ุงุณ ูุฆ ูนูู ฺฉู ูนุณูน ฺฉุฑู ฺฉ ูุ ุขุฆ ุงูพู ูนุฑููฺฏ ุณูน ุณ ฺูุฏ ุณููพูุฒ ูฺบ ุฌูฺบ ู ุงฺฉ ุณุงุชฺพ ุจฺ ฺฉุฑูุง ฺุงุช ฺบ ุงฺบุ ู `idx`, `sentence1`, ุงูุฑ `sentence2` ฺฉุงููุฒ ฺฉู ูนุง ุฏุช ฺบ ฺฉููฺฉ ุงู ฺฉ ุถุฑูุฑุช ูฺบ ูฺฏ ุงูุฑ  strings ูพุฑ ูุดุชูู ฺบ (ุงูุฑ ู strings ฺฉ ุณุงุชฺพ tensors ูฺบ ุจูุง ุณฺฉุช) ุงูุฑ ุจฺ ูฺบ ุฑ ุงููนุฑ ฺฉ ููุจุงุฆ ุฏฺฉฺพุช ฺบ:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

ฺฉุณ ูุณู ฺฉุง ุญุฑุงู ฺฉู ุจุงุช ูฺบุ ูฺบ ูุฎุชูู ููุจุงุฆ ฺฉ ุณููพูุฒ ููุช ฺบุ 32 ุณ 67 ุชฺฉ ฺุงุฆูุงูฺฉ ูพฺูฺฏ ฺฉุง ูุทูุจ  ฺฉ ุงุณ ุจฺ ฺฉ ุชูุงู ุณููพูุฒ ฺฉู 67 ฺฉ ููุจุงุฆ ุชฺฉ ูพฺ ฺฉุง ุฌุงูุง ฺุงุ ุฌู ฺฉ ุจฺ ูฺบ ุณุจ ุณ ุฒุงุฏ  ฺุงุฆูุงูฺฉ ูพฺูฺฏ ฺฉ ุจุบุฑุ ุชูุงู ุณููพูุฒ ฺฉู ูพูุฑ ฺูนุงุณูน ฺฉ ุฒุงุฏ ุณ ุฒุงุฏ ููุจุงุฆ ุง ูุงฺู ฺฉ ูุจูู ฺฉุฑุฏ ุฒุงุฏ ุณ ุฒุงุฏ ููุจุงุฆ ุชฺฉ ูพฺ ฺฉุฑูุง ูพฺุชุง ุขุฆ ุงุณ ุจุงุช ฺฉ ุฏูุจุงุฑ ุชุตุฏู ฺฉุฑฺบ ฺฉ ูุงุฑุง `data_collator` ุจฺ ฺฉู ุฏุฑุณุช ุทุฑู ุณ ฺุงุฆูุงูฺฉ ูพฺูฺฏ ฺฉุฑ ุฑุง :

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

ุจุช ุงฺฺพุง ูฺฏ ุฑุง ! ุงุจ ุฌุจฺฉ ู ู ุฎุงู ูนฺฉุณูน ุณ ุจฺุฒ ูฺบ ุชุจุฏู ฺฉุฑ ู  ุฌูฺบ ูุงุฑุง ูุงฺู ุณูุจฺพุงู ุณฺฉุชุง ุ ู ุงุณ fine-tune ฺฉุฑู ฺฉ ู ุชุงุฑ ฺบ!
{/if}

<Tip>

โ๏ธ **Try it out!** GLUE SST-2 ฺูนุงุณูน ูพุฑ ุจฺพ ูพุฑ ูพุฑูุณุณูฺฏ ฺฉู ุฏุฑุงุฆฺบ  ุชฺพูฺุง ูุฎุชูู  ฺฉููฺฉ  ุฌูฺูฺบ ฺฉ ุจุฌุงุฆ ุงฺฉ ุฌูููฺบ ูพุฑ ูุดุชูู ุ ูฺฉู ุฌู ฺฉฺฺพ ู ู ฺฉุง  ู ุจุงู ุณุจ ฺฉฺฺพ ูุณุง  ููุง ฺุง ุงฺฉ ูุฒุฏ ูุดฺฉู ฺููุฌ ฺฉ ูุ ุงุณ ูพุฑ ูพุฑูุณุณูฺฏ ููฺฉุดู ูฺฉฺพู ฺฉ ฺฉูุดุด ฺฉุฑฺบ ุฌู GLUE ฺฉ ฺฉุณ ุจฺพ ูนุงุณฺฉ ูพุฑ ฺฉุงู ฺฉุฑ

</Tip>

{#if fw === 'tf'}

ุงุจ ุฌุจฺฉ ูุงุฑ ูพุงุณ ฺูนุงุณูน ุงูุฑ ุงฺฉ data collator ููุฌูุฏ ุ ูฺบ ุงู ฺฉู ุงฺฉ ุณุงุชฺพ ููุงูุง  ู ุฏุณุช ุทูุฑ ูพุฑ ุจฺุฒ ููฺ ฺฉุฑ ฺฉ ุงูฺบ collate ฺฉุฑ ุณฺฉุช ฺบุ ูฺฉู  ุจุช ุฒุงุฏ ฺฉุงู ุ ุงูุฑ ุดุงุฏ ฺฉุงุฑฺฉุฑุฏฺฏ ูฺบ ุจฺพ ฺฉู ู ุงุณ ฺฉ ุจุฌุงุฆุ ุงฺฉ ุณุงุฏ ุทุฑู  ุฌู ุงุณ ูุณุฆู ฺฉุง ููุซุฑ ุญู ูพุด ฺฉุฑุชุง : `to_tf_dataset()`  ุขูพ ฺฉ ฺูนุงุณูน ฺฉ ฺฏุฑุฏ ุงฺฉ `tf.data.Dataset` ููพูน ุฏ ฺฏุงุ ุงฺฉ ุงุฎุชุงุฑ collation ููฺฉุดู ฺฉ ุณุงุชฺพ `tf.data.Dataset` ุงฺฉ ููุงู TensorFlow ูุงุฑููน  ุฌุณ Keras `model.fit()` ฺฉ ู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุชุง ุ ุงุณ ู  ุงฺฉ ุทุฑู ููุฑุงู ุงฺฉ ๐ค Dataset ฺฉู ุชุฑุจุช ฺฉ ู ุชุงุฑ ูุงุฑููน ูฺบ ุชุจุฏู ฺฉุฑ ุฏุชุง  ุขุฆ ุงุณ ุงูพู ฺูนุงุณูน ฺฉ ุณุงุชฺพ ุนูู ูฺบ ุฏฺฉฺพุช ฺบ!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

ุงูุฑ ุจุณ! ู ุงู ฺูนุงุณูนุณ ฺฉู ุงฺฏู ูฺฉฺุฑ ูฺบ ู ุฌุง ุณฺฉุช ฺบุ ุฌุงฺบ ฺูนุง ูพุฑ ูพุฑูุณุณูฺฏ ฺฉ ุณุงุฑ ุณุฎุช ฺฉุงู ฺฉ ุจุนุฏ ูนุฑููฺฏ ฺฉุงู ุขุณุงู ู ุฌุงุฆ ฺฏ!

{/if}
```