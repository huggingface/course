```mdx
<FrameworkSwitchCourse {fw} />

# Keras ฺฉ ุณุงุชฺพ ูุงฺู ฺฉ ูุงุฆู ูนูููฺฏ[[fine-tuning-a-model-with-keras]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
]} />

ุฌุจ ุขูพ ู ูพฺฺพู ุณฺฉุดู ูฺบ ุชูุงู ฺูนุง ูพุฑ ูพุฑูุณุณูฺฏ ฺฉุง ฺฉุงู ูฺฉูู ฺฉุฑ ูุง ูุ ุชู ูุงฺู ฺฉู ูนุฑู ฺฉุฑู ฺฉ ู ุตุฑู ฺูุฏ ูุฏู ุจุงู ุฑ ุฌุงุช ฺบ ุชุงู ูููน ฺฉุฑฺบ ฺฉ `model.fit()` ฺฉูุงูฺ CPU ูพุฑ ุจุช ุขุณุช ฺู ฺฏ ุงฺฏุฑ ุขูพ ฺฉ ูพุงุณ GPU ุณูน ุงูพ ูฺบ ุ ุชู ุขูพ [Google Colab](https://colab.research.google.com/) ูพุฑ ููุช GPU ุง TPU ุชฺฉ ุฑุณุงุฆ ุญุงุตู ฺฉุฑ ุณฺฉุช ฺบ

ูฺ ุฏ ฺฏุฆ ฺฉูฺ ฺฉ ูุซุงูฺบ  ูุฑุถ ฺฉุฑุช ฺบ ฺฉ ุขูพ ูพู  ูพฺฺพู ุณฺฉุดู ฺฉ ูุซุงูฺบ ฺูุง ฺฺฉ ฺบ ุงฺบ ุงฺฉ ูุฎุชุตุฑ ุฎูุงุต  ุฌู ุจุชุงุชุง  ฺฉ ุขูพ ฺฉู ฺฉุง ุฏุฑฺฉุงุฑ :

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### ุชุฑุจุช[[training]]

TensorFlow ูุงฺูุฒ ุฌู ๐ค Transformers ุณ ุฏุฑุขูุฏ ฺฉ ุฌุงุช ฺบุ ูพู ุณ  Keras ูุงฺูุฒ ฺบ ุงฺบ Keras ฺฉุง ุงฺฉ ูุฎุชุตุฑ ุชุนุงุฑู ูพุด ฺฉุง ฺฏุง 

<Youtube id="rnTGBy2ax1c"/>

ุงุณ ฺฉุง ูุทูุจ  ฺฉ ุฌุณ  ูุงุฑ ูพุงุณ ฺูนุง ุข ุฌุงุฆุ ุงุณ ูพุฑ ูนุฑููฺฏ ุดุฑูุน ฺฉุฑู ฺฉ ู ุจุช ฺฉู ฺฉุงู ุจุงู ุฑ ุฌุงุชุง 

<Youtube id="AUozVp78dhk"/>

ุฌุณุง ฺฉ [ูพฺฺพู ุจุงุจ](/course/chapter2) ูฺบ ุจุงู ฺฉุง ฺฏุง ุ ู `TFAutoModelForSequenceClassification` ฺฉูุงุณ ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏุ ุฌุณ ูฺบ ุฏู ูุจู ุดุงูู ฺบ:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ุขูพ ูููน ฺฉุฑฺบ ฺฏ ฺฉ [ุจุงุจ 2](/course/chapter2) ฺฉ ุจุฑุฎูุงูุ ุงุณ ูพุฑ ูนุฑูฺ ูุงฺู ฺฉู ุงูุณูนูุด ุงูน ฺฉุฑู ฺฉ ุจุนุฏ ุงฺฉ ูุงุฑููฺฏ ุธุงุฑ ูุช  ุงุณ ฺฉ ูุฌ   ฺฉ BERT ฺฉู ุฌูููฺบ ฺฉ ุฌูฺ ฺฉ ฺฉูุงุณูฺฉุดู ฺฉ ู ูพุฑ ูนุฑูฺ ูฺบ ฺฉุง ฺฏุง ุ ููฐุฐุง ูพุฑ ูนุฑูฺ ูุงฺู ฺฉุง ฺ ูฺฉุงู ุฏุง ฺฏุง  ุงูุฑ ุงุณ ฺฉ ุจุฌุงุฆ ุณฺฉูุฆูุณ ฺฉูุงุณูฺฉุดู ฺฉ ู ููุฒูฺบ ุงฺฉ ูุง ฺ ุดุงูู ฺฉุง ฺฏุง  ูุงุฑููฺฏุฒ ุธุงุฑ ฺฉุฑุช ฺบ ฺฉ ฺฉฺฺพ ูุฒู ุงุณุชุนูุงู ูฺบ ฺฉ ฺฏุฆ (ุฌู ฺฉ ฺุฑุงูพ ฺฉ ฺฏุฆ ูพุฑ ูนุฑููฺฏ ฺ ุณ ูุชุนูู ฺบ) ุงูุฑ ฺฉฺฺพ ูุฒู ุชุตุงุฏู ุทูุฑ ูพุฑ initialize ฺฉ ฺฏุฆ (ุฌู ูุฆ ฺ ฺฉ ู ฺบ)  ูุงุฑููฺฏุฒ ุขุฎุฑ ูฺบ ุขูพ ฺฉู ูุงฺู ฺฉู ูนุฑู ฺฉุฑู ฺฉ ุชุฑุบุจ ุฏุช ฺบุ ุฌู ฺฉ ู ุงุจ ฺฉุฑู ุฌุง ุฑ ฺบ

ุงูพู ฺูนุงุณูน ูพุฑ ูุงฺู ฺฉ ูุงุฆู ูนูููฺฏ ฺฉุฑู ฺฉ ูุ ูฺบ ุตุฑู ุงูพู ูุงฺู ฺฉู `compile()` ฺฉุฑูุง ูฺฏุง ุงูุฑ ูพฺพุฑ ุงูพูุง ฺูนุง `fit()` ูุชฺพฺ ูฺบ ูพุงุณ ฺฉุฑูุง ูฺฏุง ุงุณ ุณ ูุงุฆู ูนูููฺฏ ฺฉุง ุนูู ุดุฑูุน ู ุฌุงุฆ ฺฏุง (ุฌู ฺฉ GPU ูพุฑ ฺูุฏ ูููนูฺบ ูฺบ ูฺฉูู ููุง ฺุง) ุงูุฑ ุฑ ุงูพูฺฉ ฺฉ ุขุฎุฑ ูฺบ ูนุฑููฺฏ ูุงุณ ฺฉ ุณุงุชฺพ ุณุงุชฺพ ููฺุดู ูุงุณ ุจฺพ ุฑูพูุฑูน ูฺฏ

<Tip>

ูููน ฺฉุฑฺบ ฺฉ ๐ค Transformers ูุงฺูุฒ ูฺบ ุงฺฉ ุฎุงุต ุตูุงุญุช ููุฌูุฏ  ุฌู ุฒุงุฏ ุชุฑ Keras ูุงฺูุฒ ูฺบ ูฺบ ูุช โ  ุฎูุฏฺฉุงุฑ ุทูุฑ ูพุฑ ุงฺฉ ููุงุณุจ ูุงุณ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ุฌุณ ู ุงูุฏุฑูู ุทูุฑ ูพุฑ ุญุณุงุจ ฺฉุฑุช ฺบ ุงฺฏุฑ ุขูพ `compile()` ูฺบ ูุงุณ ุขุฑฺฏููููน ุณูน ู ฺฉุฑฺบ ุชู  ฺูุงููน ุทูุฑ ูพุฑ ุงุณ ูุงุณ ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ุงูุฏุฑูู ูุงุณ ฺฉู ุงุณุชุนูุงู ฺฉุฑู ฺฉ ู ุขูพ ฺฉู ุงูพู ูุจูุฒ ฺฉู ุงู ูพูน ฺฉุง ุญุต ฺฉ ุทูุฑ ูพุฑ ูพุงุณ ฺฉุฑูุง ูฺฏุงุ ุงูฺฏ ุณ ูุจู ฺฉ ุทูุฑ ูพุฑ ูฺบ โ ุฌู ฺฉ Keras ูุงฺูุฒ ฺฉ ุณุงุชฺพ ูุจูุฒ ุงุณุชุนูุงู ฺฉุฑู ฺฉุง ูุนููู ฺฉุง ุทุฑู  ุขูพ ุงุณ ฺฉ ูุซุงูฺบ ฺฉูุฑุณ ฺฉ ุญุต 2 ูฺบ ุฏฺฉฺพฺบ ฺฏุ ุฌุงฺบ ุฏุฑุณุช ูุงุณ ููฺฉุดู ฺฉ ุชุนุฑู ูุดฺฉู ู ุณฺฉุช  ุณฺฉูุฆูุณ ฺฉูุงุณูฺฉุดู ฺฉ ูุ ุงฺฉ ูุนุงุฑ Keras ูุงุณ ููฺฉุดู ุจุงูฺฉู ูนฺพฺฉ ฺฉุงู ฺฉุฑุชุง ุ ููฐุฐุง ู ุงฺบ ุงุณ ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

ุงฺบ ุงฺฉ ุนุงู ุบูุท ูููน ฺฉุฑฺบ โ ุขูพ Keras ฺฉู ูุงุณ ฺฉุง ูุงู ุจุทูุฑ string ูพุงุณ ฺฉุฑ ุณฺฉุช ฺบุ ูฺฉู ฺูุงููน ฺฉ ุทูุฑ ูพุฑ Keras ูุฑุถ ฺฉุฑ ูุชุง  ฺฉ ุขูพ ู ูพู  ุงูพู ุขุคูน ูพูน ูพุฑ softmax ูุงฺฏู ฺฉุฑ ุฏุง  ุชุงูุ ุจุช ุณ ูุงฺูุฒ ู ูููุฒ ูุฑุงู ฺฉุฑุช ฺบ ุฌู softmax ูุงฺฏู ูู ุณ ูพู ฺฉ ูุช ฺบุ ุฌูฺบ *logits* ฺฉุง ุฌุงุชุง  ูฺบ ูุงุณ ููฺฉุดู ฺฉู ุจุชุงูุง ูฺฏุง ฺฉ ูุงุฑุง ูุงฺู ุงุณุง  ฺฉุฑุชุง ุ ุงูุฑ ุงุณ ฺฉุง ูุงุญุฏ ุทุฑู   ฺฉ ุงุณ ุจุฑุง ุฑุงุณุช ฺฉุงู ฺฉุง ุฌุงุฆุ ู ฺฉ string ฺฉ ูุงู ุณ

</Tip>

### ุชุฑุจุช ฺฉุงุฑฺฉุฑุฏฺฏ ูฺบ ุจุชุฑ[[improving-training-performance]]

<Youtube id="cpzq6ESSM5c"/>

ุงฺฏุฑ ุขูพ ุงููพุฑ ุฏุง ฺฏุง ฺฉูฺ ฺูุงุช ฺบ ุชู  ูู ุทูุฑ ูพุฑ ฺูุชุง ุ ูฺฉู ุขูพ ุฏฺฉฺพฺบ ฺฏ ฺฉ ูุงุณ ุตุฑู ุขุณุช ุง ฺฉุจฺพ ฺฉุจฺพุงุฑ ฺฉู ูุช  ุงุณ ฺฉ ุจูุงุฏ ูุฌ *learning rate*  ุฌุณ ฺฉ ูุงุณ ฺฉ ุณุงุชฺพ ูุชุง ุ ุฌุจ ู Keras ฺฉู optimizer ฺฉุง ูุงู ุจุทูุฑ string ูพุงุณ ฺฉุฑุช ฺบุ ุชู Keras ุงุณ optimizer ฺฉู ุชูุงู ูพุฑุงููนุฑุฒ ฺฉ ู ฺูุงููน ูููุฒ ฺฉ ุณุงุชฺพ initialize ฺฉุฑุชุง ุ ุจุดููู learning rate ฺฉ ุทูู ุชุฌุฑุจ ุณ ู ุฌุงูุช ฺบ ฺฉ transformer ูุงฺูุฒ ฺฉู Adam ฺฉ ฺูุงููน ุณ ฺฉฺบ ฺฉู learning rate ุณ ูุงุฆุฏ ูุชุง ุ ุฌู ฺฉ 1e-3 (ุนู 10^-3 ุง 0.001)  5e-5 (0.00005)ุ ุฌู ุชูุฑุจุงู ุจุณ ฺฏูุง ฺฉู ุ ุงฺฉ ุจุชุฑ ุงุจุชุฏุงุฆ ููุท 

learning rate ฺฉู ฺฉู ฺฉุฑู ฺฉ ุนูุงูุ ูุงุฑ ูพุงุณ ุงฺฉ ุงูุฑ ุชุฏุจุฑ ุจฺพ : ู training ฺฉ ุฏูุฑุงู learning rate ฺฉู ุขุณุช ุขุณุช ฺฉู ฺฉุฑ ุณฺฉุช ฺบ ููนุฑฺุฑ ูฺบุ ุขูพ ฺฉู ุงุณ ฺฉุจฺพ ฺฉุจฺพุงุฑ *decaying* ุง *annealing* learning rate ฺฉ ุทูุฑ ูพุฑ ุฏฺฉฺพุง ุฌุงุฆ ฺฏุง Keras ูฺบุ ุงุณ ฺฉุง ุจุชุฑู ุทุฑู *learning rate scheduler* ุงุณุชุนูุงู ฺฉุฑูุง  ุงฺฉ ุงฺฺพุง ุงูุชุฎุงุจ `PolynomialDecay`  โ ูุงู ฺฉ ุจุงูุฌูุฏุ ฺูุงููน ุณูนูฺฏุฒ ฺฉ ุณุงุชฺพ  ุจุณ learning rate ฺฉู ุงุจุชุฏุงุฆ ููู ุณ ุขุฎุฑ ููู ุชฺฉ ูฺฉุฑ ุทูุฑ ูพุฑ ฺฉู ฺฉุฑ ุฏุชุง ุ ุฌู ฺฉ ุจุงูฺฉู ู  ุฌู ูฺบ ฺุง ุชุงูุ scheduler ฺฉู ุฏุฑุณุช ุทุฑู ุณ ุงุณุชุนูุงู ฺฉุฑู ฺฉ ูุ ูฺบ ุงุณ  ุจุชุงูุง ูฺฏุง ฺฉ training ฺฉุชู ุฏุฑ ุชฺฉ ุฌุงุฑ ุฑ ฺฏ ู ุงุณ ูฺ `num_train_steps` ฺฉ ุทูุฑ ูพุฑ ุญุณุงุจ ฺฉุฑุช ฺบ

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

๐ค Transformers ูุงุฆุจุฑุฑ ูฺบ ุงฺฉ `create_optimizer()` ููฺฉุดู ุจฺพ ููุฌูุฏ  ุฌู learning rate decay ฺฉ ุณุงุชฺพ ุงฺฉ `AdamW` optimizer ุชุฎูู ฺฉุฑุชุง   ุงฺฉ ุขุณุงู ุดุงุฑูน ฺฉูน  ุฌุณ ุขูพ ุขุฆูุฏ ุณฺฉุดูุฒ ูฺบ ุชูุตู ุณ ุฏฺฉฺพฺบ ฺฏ

</Tip>

ุงุจ ูุงุฑ ูพุงุณ ูุงุฑุง ูุง optimizer ููุฌูุฏ ุ ุงูุฑ ู ุงุณ ฺฉ ุณุงุชฺพ ูนุฑููฺฏ ฺฉุฑู ฺฉ ฺฉูุดุด ฺฉุฑ ุณฺฉุช ฺบ ูพูุ ุขุฆ ูุงฺู ฺฉู ุฏูุจุงุฑ ููฺ ฺฉุฑฺบ ุชุงฺฉ ุญุงู  ูฺบ ฺฉ ฺฏุฆ ูนุฑููฺฏ ุฑู ฺฉ ุฏูุฑุงู ูุฒู ูฺบ ูู ูุงู ุชุจุฏูุงฺบ ุฑ ุณูน ู ุฌุงุฆฺบุ ุงูุฑ ูพฺพุฑ ู ุงุณ ูุฆ optimizer ฺฉ ุณุงุชฺพ compile ฺฉุฑุช ฺบ:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ุงุจุ ู ุฏูุจุงุฑ fit ฺฉุฑุช ฺบ:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

๐ก ุงฺฏุฑ ุขูพ ูนุฑููฺฏ ฺฉ ุฏูุฑุงู ุงูพู ูุงฺู ฺฉู ุฎูุฏฺฉุงุฑ ุทูุฑ ูพุฑ Hub ูพุฑ ุงูพููฺ ฺฉุฑูุง ฺุงุช ฺบุ ุชู ุขูพ `model.fit()` ูุชฺพฺ ูฺบ `PushToHubCallback` ูพุงุณ ฺฉุฑ ุณฺฉุช ฺบ ู ุงุณ ฺฉ ุจุงุฑ ูฺบ ูุฒุฏ [ุจุงุจ 4](/course/chapter4/3) ูฺบ ุฌุงูฺบ ฺฏ

</Tip>

### ูุงฺู ฺฉ ูพุด ฺฏูุฆุงฺบ[[model-predictions]]

<Youtube id="nx10eh4CoOs"/>

ูนุฑููฺฏ ุงูุฑ ูุงุณ ฺฉ ฺฉู ูุช ุฏฺฉฺพูุง ุจุช ุงฺฺพุง ุ ูฺฉู ุงฺฏุฑ ู ุชุฑุจุช ุงูุช ูุงฺู ุณ ุญูู ูุชุงุฆุฌ ุญุงุตู ฺฉุฑูุง ฺุงฺบ โ ฺุง ููนุฑฺฉุณ ฺฉููพููน ฺฉุฑู ฺฉ ู ูฺบ ุง ูุงฺู ฺฉู ูพุฑูฺฺฉุดู ูฺบ ุงุณุชุนูุงู ฺฉุฑู ฺฉ ู โ ุชู ู ุจุณ `predict()` ูุชฺพฺ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ  ูุงฺู ฺฉ ุขุคูน ูพูน ฺ ุณ *logits* ูุงูพุณ ฺฉุฑ ฺฏุงุ ุฌู ฺฉ ุฑ ฺฉูุงุณ ฺฉ ู ุงฺฉ ูุชุง 

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

ู ุงู logits ฺฉู ูุงฺู ฺฉ ฺฉูุงุณ ูพุด ฺฏูุฆูฺบ ูฺบ ุชุจุฏู ฺฉุฑู ฺฉ ู `argmax` ฺฉุง ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ุชุงฺฉ ุณุจ ุณ ุฒุงุฏ logit ุชูุงุด ฺฉุง ุฌุง ุณฺฉุ ุฌู ฺฉ ุณุจ ุณ ุฒุงุฏ ููฺฉู ฺฉูุงุณ ุณ ูุทุงุจูุช ุฑฺฉฺพุชุง :

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

ุงุจุ ุขุฆ ุงู `preds` ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ฺฉฺฺพ ููนุฑฺฉุณ ฺฉููพููน ฺฉุฑุช ฺบ! ู MRPC ฺูนุงุณูน ุณ ููุณูฺฉ ููนุฑฺฉุณ ฺฉู ุงุชู ุขุณุงู ุณ ููฺ ฺฉุฑ ุณฺฉุช ฺบ ุฌุชู ุขุณุงู ุณ ู ู ฺูนุงุณูน ฺฉู ููฺ ฺฉุง ุชฺพุงุ ุงุณ ุจุงุฑ `evaluate.load()` ููฺฉุดู ฺฉ ุฐุฑุน ูุงูพุณ ููู ูุงู ุขุจุฌฺฉูน ูฺบ ุงฺฉ `compute()` ูุชฺพฺ ููุฌูุฏ  ุฌุณ ู ููนุฑฺฉ ฺฉูฺฉููุดู ฺฉ ู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ุขูพ ฺฉู ุญุงุตู ูู ูุงู ูุชุงุฆุฌ ูุฎุชูู ู ุณฺฉุช ฺบุ ฺฉููฺฉ ูุงฺู ฺ ฺฉ ุชุตุงุฏู initialization ุงุณ ฺฉ ุญุงุตู ฺฉุฑุฏ ููนุฑฺฉุณ ฺฉู ุจุฏู ุณฺฉุช  ุงฺบุ ู ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ ูุงุฑ ูุงฺู ฺฉ validation ุณูน ูพุฑ ุฏุฑุณุชฺฏ 85.78%  ุงูุฑ F1 ุงุณฺฉูุฑ 89.97   ู ุฏู ููนุฑฺฉุณ ฺบ ุฌู GLUE ุจูฺ ูุงุฑฺฉ ฺฉ ู MRPC ฺูนุงุณูน ูพุฑ ูุชุงุฆุฌ ฺฉุง ุชุฌุฒ ฺฉุฑู ฺฉ ู ุงุณุชุนูุงู ูุช ฺบ [BERT ูพูพุฑ](https://arxiv.org/pdf/1810.04805.pdf) ฺฉ ูนุจู ู ุจุณ ูุงฺู ฺฉ ู F1 ุงุณฺฉูุฑ 88.9 ุฑูพูุฑูน ฺฉุง ุชฺพุง ู `uncased` ูุงฺู ุชฺพุง ุฌุจฺฉ ู ุงุณ ููุช `cased` ูุงฺู ุงุณุชุนูุงู ฺฉุฑ ุฑ ฺบุ ุฌู ุจุชุฑ ูุชุฌ ฺฉ ูุถุงุญุช ฺฉุฑุชุง 

 Keras API ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ูุงุฆู ูนูููฺฏ ฺฉ ุชุนุงุฑู ฺฉุง ุงุฎุชุชุงู ฺฉุฑุชุง  ุฒุงุฏ ุนุงู NLP ูนุงุณฺฉุณ ฺฉ ู ุงุณ ฺฉุง ุงฺฉ ูุซุงู [ุจุงุจ 7](/course/chapter7) ูฺบ ุฏ ุฌุงุฆ ฺฏ ุงฺฏุฑ ุขูพ Keras API ูพุฑ ุงูพู ูุงุฑุช ุจฺฺพุงูุง ฺุงุช ฺบุ ุชู GLUE SST-2 ฺูนุงุณูน ูพุฑ ูุงฺู ฺฉ ูุงุฆู ูนูููฺฏ ฺฉุฑู ฺฉ ฺฉูุดุด ฺฉุฑฺบุ ุฌุณ ฺูนุง ูพุฑูุณุณูฺฏ ฺฉู ุขูพ ู ุณฺฉุดู 2 ูฺบ ุงูุฌุงู ุฏุง ุชฺพุง
```