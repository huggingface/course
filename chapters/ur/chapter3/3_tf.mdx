```mdx
<FrameworkSwitchCourse {fw} />

# Keras کے ساتھ ماڈل کی فائن ٹیوننگ[[fine-tuning-a-model-with-keras]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3_tf.ipynb"},
]} />

جب آپ نے پچھلے سیکشن میں تمام ڈیٹا پری پروسیسنگ کا کام مکمل کر لیا ہو، تو ماڈل کو ٹرین کرنے کے لیے صرف چند قدم باقی رہ جاتے ہیں۔ تاہم نوٹ کریں کہ `model.fit()` کمانڈ CPU پر بہت آہستہ چلے گی۔ اگر آپ کے پاس GPU سیٹ اپ نہیں ہے، تو آپ [Google Colab](https://colab.research.google.com/) پر مفت GPU یا TPU تک رسائی حاصل کر سکتے ہیں۔

نیچے دیے گئے کوڈ کے مثالیں یہ فرض کرتی ہیں کہ آپ پہلے ہی پچھلے سیکشن کی مثالیں چلا چکے ہیں۔ یہاں ایک مختصر خلاصہ ہے جو بتاتا ہے کہ آپ کو کیا درکار ہے:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### تربیت[[training]]

TensorFlow ماڈلز جو 🤗 Transformers سے درآمد کیے جاتے ہیں، پہلے سے ہی Keras ماڈلز ہیں۔ یہاں Keras کا ایک مختصر تعارف پیش کیا گیا ہے۔

<Youtube id="rnTGBy2ax1c"/>

اس کا مطلب ہے کہ جیسے ہی ہمارے پاس ڈیٹا آ جائے، اس پر ٹریننگ شروع کرنے کے لیے بہت کم کام باقی رہ جاتا ہے۔

<Youtube id="AUozVp78dhk"/>

جیسا کہ [پچھلے باب](/course/chapter2) میں بیان کیا گیا ہے، ہم `TFAutoModelForSequenceClassification` کلاس استعمال کریں گے، جس میں دو لیبل شامل ہیں:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

آپ نوٹ کریں گے کہ [باب 2](/course/chapter2) کے برخلاف، اس پری ٹرینڈ ماڈل کو انسٹینشی ایٹ کرنے کے بعد ایک وارننگ ظاہر ہوتی ہے۔ اس کی وجہ یہ ہے کہ BERT کو جملوں کے جوڑے کی کلاسفیکیشن کے لیے پری ٹرینڈ نہیں کیا گیا ہے، لہٰذا پری ٹرینڈ ماڈل کا ہیڈ نکال دیا گیا ہے اور اس کے بجائے سیکوئنس کلاسفیکیشن کے لیے موزوں ایک نیا ہیڈ شامل کیا گیا ہے۔ وارننگز ظاہر کرتی ہیں کہ کچھ وزن استعمال نہیں کیے گئے (جو کہ ڈراپ کیے گئے پری ٹریننگ ہیڈ سے متعلق ہیں) اور کچھ وزن تصادفی طور پر initialize کیے گئے (جو نئے ہیڈ کے لیے ہیں)۔ یہ وارننگز آخر میں آپ کو ماڈل کو ٹرین کرنے کی ترغیب دیتی ہیں، جو کہ ہم اب کرنے جا رہے ہیں۔

اپنے ڈیٹاسیٹ پر ماڈل کی فائن ٹیوننگ کرنے کے لیے، ہمیں صرف اپنے ماڈل کو `compile()` کرنا ہوگا اور پھر اپنا ڈیٹا `fit()` میتھڈ میں پاس کرنا ہوگا۔ اس سے فائن ٹیوننگ کا عمل شروع ہو جائے گا (جو کہ GPU پر چند منٹوں میں مکمل ہونا چاہیے) اور ہر ایپوک کے آخر میں ٹریننگ لاس کے ساتھ ساتھ ویلیڈیشن لاس بھی رپورٹ ہوگی۔

<Tip>

نوٹ کریں کہ 🤗 Transformers ماڈلز میں ایک خاص صلاحیت موجود ہے جو زیادہ تر Keras ماڈلز میں نہیں ہوتی — یہ خودکار طور پر ایک مناسب لاس استعمال کر سکتے ہیں جسے وہ اندرونی طور پر حساب کرتے ہیں۔ اگر آپ `compile()` میں لاس آرگیومنٹ سیٹ نہ کریں تو یہ ڈیفالٹ طور پر اسی لاس کا استعمال کریں گے۔ اندرونی لاس کو استعمال کرنے کے لیے آپ کو اپنے لیبلز کو ان پٹ کا حصہ کے طور پر پاس کرنا ہوگا، الگ سے لیبل کے طور پر نہیں — جو کہ Keras ماڈلز کے ساتھ لیبلز استعمال کرنے کا معمول کا طریقہ ہے۔ آپ اس کی مثالیں کورس کے حصہ 2 میں دیکھیں گے، جہاں درست لاس فنکشن کی تعریف مشکل ہو سکتی ہے۔ سیکوئنس کلاسفیکیشن کے لیے، ایک معیاری Keras لاس فنکشن بالکل ٹھیک کام کرتا ہے، لہٰذا ہم یہاں اسی کا استعمال کریں گے۔

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

یہاں ایک عام غلطی نوٹ کریں — آپ Keras کو لاس کا نام بطور string پاس کر سکتے ہیں، لیکن ڈیفالٹ کے طور پر Keras فرض کر لیتا ہے کہ آپ نے پہلے ہی اپنے آؤٹ پٹ پر softmax لاگو کر دیا ہے۔ تاہم، بہت سے ماڈلز وہ ویلیوز فراہم کرتے ہیں جو softmax لاگو ہونے سے پہلے کی ہوتی ہیں، جنہیں *logits* کہا جاتا ہے۔ ہمیں لاس فنکشن کو بتانا ہوگا کہ ہمارا ماڈل ایسا ہی کرتا ہے، اور اس کا واحد طریقہ یہ ہے کہ اسے براہ راست کال کیا جائے، نہ کہ string کے نام سے۔

</Tip>

### تربیتی کارکردگی میں بہتری[[improving-training-performance]]

<Youtube id="cpzq6ESSM5c"/>

اگر آپ اوپر دیا گیا کوڈ چلاتے ہیں تو یہ یقینی طور پر چلتا ہے، لیکن آپ دیکھیں گے کہ لاس صرف آہستہ یا کبھی کبھار کم ہوتی ہے۔ اس کی بنیادی وجہ *learning rate* ہے۔ جیسے کہ لاس کے ساتھ ہوتا ہے، جب ہم Keras کو optimizer کا نام بطور string پاس کرتے ہیں، تو Keras اس optimizer کو تمام پیرامیٹرز کے لیے ڈیفالٹ ویلیوز کے ساتھ initialize کرتا ہے، بشمول learning rate کے۔ طویل تجربے سے ہم جانتے ہیں کہ transformer ماڈلز کو Adam کے ڈیفالٹ سے کہیں کم learning rate سے فائدہ ہوتا ہے، جو کہ 1e-3 (یعنی 10^-3 یا 0.001) ہے۔ 5e-5 (0.00005)، جو تقریباً بیس گنا کم ہے، ایک بہتر ابتدائی نقطہ ہے۔

learning rate کو کم کرنے کے علاوہ، ہمارے پاس ایک اور تدبیر بھی ہے: ہم training کے دوران learning rate کو آہستہ آہستہ کم کر سکتے ہیں۔ لٹریچر میں، آپ کو اسے کبھی کبھار *decaying* یا *annealing* learning rate کے طور پر دیکھا جائے گا۔ Keras میں، اس کا بہترین طریقہ *learning rate scheduler* استعمال کرنا ہے۔ ایک اچھا انتخاب `PolynomialDecay` ہے — نام کے باوجود، ڈیفالٹ سیٹنگز کے ساتھ یہ بس learning rate کو ابتدائی ویلیو سے آخری ویلیو تک لکیری طور پر کم کر دیتا ہے، جو کہ بالکل وہی ہے جو ہمیں چاہیے۔ تاہم، scheduler کو درست طریقے سے استعمال کرنے کے لیے، ہمیں اسے یہ بتانا ہوگا کہ training کتنی دیر تک جاری رہے گی۔ ہم اسے نیچے `num_train_steps` کے طور پر حساب کرتے ہیں۔

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

🤗 Transformers لائبریری میں ایک `create_optimizer()` فنکشن بھی موجود ہے جو learning rate decay کے ساتھ ایک `AdamW` optimizer تخلیق کرتا ہے۔ یہ ایک آسان شارٹ کٹ ہے جسے آپ آئندہ سیکشنز میں تفصیل سے دیکھیں گے۔

</Tip>

اب ہمارے پاس ہمارا نیا optimizer موجود ہے، اور ہم اس کے ساتھ ٹریننگ کرنے کی کوشش کر سکتے ہیں۔ پہلے، آئیے ماڈل کو دوبارہ لوڈ کریں تاکہ حال ہی میں کی گئی ٹریننگ رن کے دوران وزن میں ہونے والی تبدیلیاں ری سیٹ ہو جائیں، اور پھر ہم اسے نئے optimizer کے ساتھ compile کرتے ہیں:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

اب، ہم دوبارہ fit کرتے ہیں:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

💡 اگر آپ ٹریننگ کے دوران اپنے ماڈل کو خودکار طور پر Hub پر اپلوڈ کرنا چاہتے ہیں، تو آپ `model.fit()` میتھڈ میں `PushToHubCallback` پاس کر سکتے ہیں۔ ہم اس کے بارے میں مزید [باب 4](/course/chapter4/3) میں جانیں گے۔

</Tip>

### ماڈل کی پیش گوئیاں[[model-predictions]]

<Youtube id="nx10eh4CoOs"/>

ٹریننگ اور لاس کے کم ہوتے دیکھنا بہت اچھا ہے، لیکن اگر ہم تربیت یافتہ ماڈل سے حقیقی نتائج حاصل کرنا چاہیں — چاہے میٹرکس کمپیوٹ کرنے کے لیے ہوں یا ماڈل کو پروڈکشن میں استعمال کرنے کے لیے — تو ہم بس `predict()` میتھڈ استعمال کر سکتے ہیں۔ یہ ماڈل کے آؤٹ پٹ ہیڈ سے *logits* واپس کرے گا، جو کہ ہر کلاس کے لیے ایک ہوتا ہے۔

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

ہم ان logits کو ماڈل کی کلاس پیش گوئیوں میں تبدیل کرنے کے لیے `argmax` کا استعمال کر سکتے ہیں تاکہ سب سے زیادہ logit تلاش کیا جا سکے، جو کہ سب سے زیادہ ممکنہ کلاس سے مطابقت رکھتا ہے:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

اب، آئیے ان `preds` کا استعمال کرتے ہوئے کچھ میٹرکس کمپیوٹ کرتے ہیں! ہم MRPC ڈیٹاسیٹ سے منسلک میٹرکس کو اتنی آسانی سے لوڈ کر سکتے ہیں جتنی آسانی سے ہم نے ڈیٹاسیٹ کو لوڈ کیا تھا، اس بار `evaluate.load()` فنکشن کے ذریعے۔ واپس ملنے والے آبجیکٹ میں ایک `compute()` میتھڈ موجود ہے جسے ہم میٹرک کیلکولیشن کے لیے استعمال کر سکتے ہیں:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

آپ کو حاصل ہونے والے نتائج مختلف ہو سکتے ہیں، کیونکہ ماڈل ہیڈ کی تصادفی initialization اس کے حاصل کردہ میٹرکس کو بدل سکتی ہے۔ یہاں، ہم دیکھ سکتے ہیں کہ ہمارے ماڈل کی validation سیٹ پر درستگی 85.78% ہے اور F1 اسکور 89.97 ہے۔ یہ وہ دو میٹرکس ہیں جو GLUE بینچ مارک کے لیے MRPC ڈیٹاسیٹ پر نتائج کا تجزیہ کرنے کے لیے استعمال ہوتے ہیں۔ [BERT پیپر](https://arxiv.org/pdf/1810.04805.pdf) کی ٹیبل نے بیس ماڈل کے لیے F1 اسکور 88.9 رپورٹ کیا تھا۔ وہ `uncased` ماڈل تھا جبکہ ہم اس وقت `cased` ماڈل استعمال کر رہے ہیں، جو بہتر نتیجہ کی وضاحت کرتا ہے۔

یہ Keras API کا استعمال کرتے ہوئے فائن ٹیوننگ کے تعارف کا اختتام کرتا ہے۔ زیادہ عام NLP ٹاسکس کے لیے اس کا ایک مثال [باب 7](/course/chapter7) میں دی جائے گی۔ اگر آپ Keras API پر اپنی مہارت بڑھانا چاہتے ہیں، تو GLUE SST-2 ڈیٹاسیٹ پر ماڈل کی فائن ٹیوننگ کرنے کی کوشش کریں، جس ڈیٹا پروسیسنگ کو آپ نے سیکشن 2 میں انجام دیا تھا۔
```