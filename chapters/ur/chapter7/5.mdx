<FrameworkSwitchCourse {fw} />

# خلاصہ نگاری[[summarization]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
]} />

{/if}

اس سیکشن میں ہم دیکھیں گے کہ Transformer ماڈلز کو کس طرح استعمال کر کے لمبی دستاویزات کو مختصر خلاصوں میں بدلا جا سکتا ہے، جسے _متنی خلاصہ نگاری_ کہا جاتا ہے۔ یہ NLP کے سب سے مشکل کاموں میں سے ایک ہے کیونکہ اس کے لیے لمبے پیراگرافز کو سمجھنا اور ایسا مربوط متن تیار کرنا ضروری ہوتا ہے جو دستاویز کے اہم موضوعات کو اجاگر کرے۔ تاہم، جب یہ اچھی طرح انجام دیا جائے تو متنی خلاصہ نگاری ایک ایسا طاقتور آلہ ہے جو مختلف کاروباری عملوں کی رفتار بڑھا سکتا ہے، کیونکہ یہ ماہرین پر لمبی دستاویزات تفصیل سے پڑھنے کا بوجھ کم کر دیتا ہے۔

<Youtube id="yHnr5Dk2zCI"/>

اگرچہ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads) پر خلاصہ نگاری کے لیے پہلے سے fine-tune شدہ مختلف ماڈلز موجود ہیں، مگر ان میں سے تقریباً تمام صرف انگریزی دستاویزات کے لیے موزوں ہیں۔ لہٰذا، اس سیکشن میں ایک نئی جہت شامل کرنے کے لیے، ہم انگریزی اور ہسپانوی دونوں زبانوں کے لیے ایک دو لسانی ماڈل کی تربیت کریں گے۔ اس سیکشن کے اختتام تک، آپ کے پاس ایک [ماڈل](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) موجود ہوگا جو صارفین کے جائزوں کا خلاصہ فراہم کر سکے گا جیسا کہ نیچے دکھایا گیا ہے:

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

جیسا کہ ہم دیکھیں گے، یہ خلاصے مختصر ہوتے ہیں کیونکہ یہ صارفین کی مصنوعات کے جائزوں میں فراہم کردہ عنوانات سے سیکھے جاتے ہیں۔ آئیے اس کام کے لیے ایک موزوں دو لسانی مجموعہ تیار کرنے سے آغاز کرتے ہیں۔

## ایک کثیر لسانی مجموعہ تیار کرنا[[preparing-a-multilingual-corpus]]

ہم اپنے دو لسانی خلاصہ نگار کی تخلیق کے لیے [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) استعمال کریں گے۔ یہ مجموعہ چھ زبانوں میں Amazon مصنوعات کے جائزوں پر مشتمل ہے اور عموماً کثیر لسانی classifiers کی جانچ کے لیے استعمال کیا جاتا ہے۔ تاہم، چونکہ ہر جائزے کے ساتھ ایک مختصر عنوان بھی شامل ہوتا ہے، لہٰذا ہم ان عنوانات کو اپنے ماڈل کے لیے ہدف خلاصے کے طور پر استعمال کر سکتے ہیں! شروع کرنے کے لیے، آئیے Hugging Face Hub سے انگریزی اور ہسپانوی ذیلی مجموعے ڈاؤن لوڈ کرتے ہیں:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

جیسا کہ آپ دیکھ سکتے ہیں، ہر زبان کے لیے `train` سپلٹ میں 200,000 جائزے اور `validation` اور `test` سپلٹس میں 5,000 جائزے موجود ہیں۔ جو جائزے کی معلومات ہمیں درکار ہیں وہ `review_body` اور `review_title` کالمز میں موجود ہیں۔ آئیے [Chapter 5](/course/chapter5) میں سیکھی گئی تکنیکوں کا استعمال کرتے ہوئے training set سے random sample لیتے ہوئے چند مثالیں دیکھتے ہیں:

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

✏️ **آزما کر دیکھیں!** `Dataset.shuffle()` کمانڈ میں random seed کو تبدیل کریں تاکہ مجموعے میں موجود دیگر جائزوں کا معائنہ کیا جا سکے۔ اگر آپ ہسپانوی بولنے والے ہیں، تو `spanish_dataset` کے کچھ جائزے دیکھیں تاکہ یہ معلوم ہو سکے کہ کیا ان کے عنوانات بھی معقول خلاصے معلوم ہوتے ہیں۔

</Tip>

یہ نمونہ ان جائزوں کی تنوع ظاہر کرتا ہے جو عموماً آن لائن ملتے ہیں، جو مثبت سے منفی (اور درمیان میں سب کچھ) تک پھیلے ہوئے ہیں! اگرچہ "meh" عنوان والا نمونہ بہت معلوماتی نہیں ہے، باقی عنوانات جائزوں کے لیے معقول خلاصے معلوم ہوتے ہیں۔ تمام 400,000 جائزوں پر خلاصہ نگاری ماڈل کی تربیت ایک ہی GPU پر بہت زیادہ وقت لے گی، لہٰذا ہم ایک مخصوص مصنوعات کے ڈومین کے لیے خلاصے تیار کرنے پر توجہ مرکوز کریں گے۔ آئیں دیکھتے ہیں کہ ہم کون سے ڈومینز منتخب کر سکتے ہیں؛ اس کے لیے ہم `english_dataset` کو ایک `pandas.DataFrame` میں تبدیل کر کے ہر مصنوعات کے زمرے کے لیے جائزوں کی تعداد کا حساب لگائیں گے:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

انگریزی ڈیٹاسیٹ میں سب سے زیادہ مقبول مصنوعات گھریلو اشیاء، کپڑے اور وائرلیس الیکٹرانکس سے متعلق ہیں۔ تاہم، Amazon تھیم کے ساتھ قائم رہتے ہوئے، آئیے کتابوں کے جائزوں کا خلاصہ کرنے پر توجہ دیں — آخرکار، یہی وہ چیز ہے جس پر کمپنی کی بنیاد رکھی گئی تھی! ہم دیکھ سکتے ہیں کہ دو مصنوعات کے زمرے (`book` اور `digital_ebook_purchase`) اس معیار پر پورا اترتے ہیں، لہٰذا آئیے دونوں زبانوں کے ڈیٹاسیٹس کو صرف ان مصنوعات کے لیے filter کریں۔ جیسا کہ ہم نے [Chapter 5](/course/chapter5) میں دیکھا، `Dataset.filter()` فنکشن ہمیں ایک ڈیٹاسیٹ کو مؤثر طریقے سے چھانٹنے کی اجازت دیتا ہے، تو ہم اس کام کے لیے ایک سادہ فنکشن تعریف کر سکتے ہیں:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

اب جب ہم اس فنکشن کو `english_dataset` اور `spanish_dataset` پر لاگو کرتے ہیں، تو نتیجے میں صرف وہی قطاریں شامل ہوں گی جو کتابوں کے زمروں سے متعلق ہوں۔ filter لاگو کرنے سے پہلے، آئیے `english_dataset` کا format `"pandas"` سے `"arrow"` میں تبدیل کر دیتے ہیں:

```python
english_dataset.reset_format()
```

پھر ہم filter فنکشن کو لاگو کر کے، ایک جانچ کے طور پر، یہ دیکھتے ہیں کہ آیا جائزے واقعی کتابوں سے متعلق ہیں:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

ٹھیک ہے، ہم دیکھ سکتے ہیں کہ جائزے سختی سے کتابوں کے بارے میں نہیں ہیں اور ممکنہ طور پر کیلنڈر یا OneNote جیسے الیکٹرانک اطلاقات کا حوالہ دیتے ہیں۔ بہرحال، یہ ڈومین خلاصہ نگاری ماڈل کی تربیت کے لیے مناسب معلوم ہوتا ہے۔ آخری مرحلہ یہ ہے کہ ہم انگریزی اور ہسپانوی جائزوں کو ایک واحد `DatasetDict` آبجیکٹ میں یکجا کریں۔ 🤗 Datasets ایک مفید `concatenate_datasets()` فنکشن فراہم کرتا ہے جو (جیسا کہ نام سے ظاہر ہے) دو `Dataset` آبجیکٹس کو ایک دوسرے کے اوپر stack کر دیتا ہے۔ لہٰذا، اپنے دو لسانی ڈیٹاسیٹ کو بنانے کے لیے، ہم ہر split پر loop چلائیں گے، ان ڈیٹاسیٹس کو concatenate کریں گے، اور نتیجے کو shuffle کریں گے تاکہ ہمارا ماڈل کسی ایک زبان پر overfit نہ ہو جائے:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÑADO'
'>> Review: Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

یہ یقیناً انگریزی اور ہسپانوی جائزوں کا ایک مرکب معلوم ہوتا ہے! اب جب کہ ہمارا تربیتی مجموعہ تیار ہو چکا ہے، ایک آخری چیز جس کی جانچ کرنی ہے وہ ہے جائزوں اور ان کے عنوانات میں الفاظ کی تقسیم۔ یہ خاص طور پر خلاصہ نگاری کے کاموں کے لیے اہم ہے، کیونکہ ڈیٹا میں موجود مختصر حوالہ خلاصے ماڈل کو generated خلاصوں میں صرف ایک یا دو الفاظ نکالنے کے لیے متعصب کر سکتے ہیں۔ نیچے دی گئی تصاویر الفاظ کی تقسیم دکھاتی ہیں، اور ہم دیکھ سکتے ہیں کہ عنوانات صرف 1-2 الفاظ کی طرف شدید جھکے ہوئے ہیں:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

اس مسئلے سے نمٹنے کے لیے، ہم ان مثالوں کو filter کر دیں گے جن کے عنوانات بہت مختصر ہوں تاکہ ہمارا ماڈل زیادہ دلچسپ خلاصے پیدا کر سکے۔ چونکہ ہم انگریزی اور ہسپانوی متون سے نمٹ رہے ہیں، ہم ایک عمومی اندازہ لگا کر عنوانات کو whitespace پر تقسیم کر کے پھر ہمارے قابل اعتماد `Dataset.filter()` میتھڈ کا استعمال کر سکتے ہیں:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

اب جب کہ ہم نے اپنا مجموعہ تیار کر لیا ہے، آئیے چند ایسے Transformer ماڈلز کو دیکھتے ہیں جنہیں اس پر fine-tune کیا جا سکتا ہے!

## متنی خلاصہ نگاری کے لیے ماڈلز[[models-for-text-summarization]]

اگر آپ غور کریں تو متنی خلاصہ نگاری دراصل مشین ترجمے کے مشابہ ایک کام ہے: ہمارے پاس ایک ایسا متن ہوتا ہے جیسے کہ ایک جائزہ جسے ہم ایک مختصر ورژن میں "ترجمہ" کرنا چاہتے ہیں جو ان پٹ کی نمایاں خصوصیات کو اجاگر کرے۔ اسی بنا پر، زیادہ تر Transformer ماڈلز خلاصہ نگاری کے لیے encoder-decoder architecture اپناتے ہیں، جس کا ہمیں پہلے [Chapter 1](/course/chapter1) میں سامنا ہوا تھا، اگرچہ کچھ استثنائی ماڈلز جیسے کہ GPT فیملی few-shot سیٹنگز میں بھی خلاصہ نگاری کے لیے استعمال ہو سکتے ہیں۔ نیچے دی گئی جدول میں چند مقبول pretrained ماڈلز کی فہرست ہے جنہیں خلاصہ نگاری کے لیے fine-tune کیا جا سکتا ہے۔

| Transformer ماڈل | تفصیل | کثیر لسانی؟ |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
| [GPT-2](https://huggingface.co/gpt2-xl) | اگرچہ اسے auto-regressive language model کے طور پر تربیت دی گئی ہے، آپ GPT-2 سے خلاصے generate کروا سکتے ہیں اگر آپ input text کے آخر میں "TL;DR" شامل کریں۔ | ❌ |
| [PEGASUS](https://huggingface.co/google/pegasus-large) | یہ multi-sentence متون میں masked جملوں کی پیش گوئی کے لیے pretraining objective استعمال کرتا ہے۔ یہ pretraining objective vanilla language modeling سے زیادہ خلاصہ نگاری کے قریب ہے اور معروف benchmarks پر اعلیٰ اسکور حاصل کرتا ہے۔ | ❌ |
| [T5](https://huggingface.co/t5-base) | ایک یونیورسل Transformer architecture جو تمام کاموں کو text-to-text framework میں ترتیب دیتا ہے؛ مثلاً، کسی دستاویز کا خلاصہ کرنے کے لیے ماڈل کا input format `summarize: ARTICLE` ہوتا ہے۔ | ❌ |
| [mT5](https://huggingface.co/google/mt5-base) | T5 کا ایک کثیر لسانی ورژن، جو multilingual Common Crawl corpus (mC4) پر تربیت یافتہ ہے، اور 101 زبانوں کا احاطہ کرتا ہے۔ | ✅ |
| [BART](https://huggingface.co/facebook/bart-base) | ایک نیا Transformer architecture جس میں encoder اور decoder دونوں کا stack شامل ہے، جو خراب شدہ input کو دوبارہ بنانے کے لیے تربیت یافتہ ہے اور BERT اور GPT-2 کی pretraining schemes کو ملا کر کام کرتا ہے۔ | ❌ |
| [mBART-50](https://huggingface.co/facebook/mbart-large-50) | BART کا ایک کثیر لسانی ورژن، جو 50 زبانوں پر تربیت یافتہ ہے۔ | ✅ |

جیسا کہ اس جدول سے ظاہر ہے، خلاصہ نگاری کے لیے زیادہ تر Transformer ماڈلز (اور درحقیقت بیشتر NLP کام) ایک لسانی ہوتے ہیں۔ یہ ان صورتوں میں بہتر ہے جب آپ کا کام "ہائی ریسورس" زبان جیسے کہ انگریزی یا جرمن میں ہو، لیکن دنیا بھر کی ہزاروں دیگر زبانوں کے لیے کم موزوں۔ خوش قسمتی سے، کثیر لسانی Transformer ماڈلز کی ایک ایسی کلاس موجود ہے، جیسے کہ mT5 اور mBART، جو اس مسئلے کا حل پیش کرتے ہیں۔ یہ ماڈلز language modeling کے ذریعے pretrained کیے جاتے ہیں، مگر ایک موڑ کے ساتھ: ایک زبان کے corpus پر تربیت دینے کے بجائے، انہیں بیک وقت 50 سے زائد زبانوں کے متون پر مشترکہ طور پر تربیت دی جاتی ہے!

ہم mT5 پر توجہ مرکوز کریں گے، جو T5 پر مبنی ایک دلچسپ architecture ہے اور اسے text-to-text framework میں pretrained کیا گیا تھا۔ T5 میں، ہر NLP کام کو ایک prompt prefix جیسے `summarize:` کے ذریعے ترتیب دیا جاتا ہے جو ماڈل کو generated متن کو prompt کے مطابق ڈھالنے کے لیے condition کرتا ہے۔ جیسا کہ نیچے دی گئی شکل میں دکھایا گیا ہے، اس سے T5 انتہائی ورسٹائل ہو جاتا ہے، کیونکہ ایک ہی ماڈل کے ساتھ بہت سے کام حل کیے جا سکتے ہیں!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

mT5 prefixes استعمال نہیں کرتا، مگر T5 کی بہت سی ورسٹائل خصوصیات کو شیئر کرتا ہے اور کثیر لسانی ہونے کا فائدہ بھی رکھتا ہے۔ اب جب کہ ہم نے ایک ماڈل منتخب کر لیا ہے، آئیے تربیت کے لیے اپنے ڈیٹا کی تیاری دیکھتے ہیں۔

<Tip>

✏️ **آزما کر دیکھیں!** جب آپ اس سیکشن کو مکمل کر لیں، تو دیکھیں کہ mT5 mBART کے مقابلے میں کیسا ہے اگر آپ mBART کو انہی تکنیکوں کے ساتھ fine-tune کریں۔ اضافی پوائنٹس کے لیے، آپ صرف انگریزی جائزوں پر T5 کو fine-tune کرنے کی کوشش بھی کر سکتے ہیں۔ چونکہ T5 میں ایک خاص prefix prompt ہوتا ہے، لہٰذا آپ کو نیچے دیے گئے preprocessing مراحل میں input مثالوں سے پہلے `summarize:` شامل کرنا ہوگا۔

</Tip>

## ڈیٹا کی پیش کاری[[preprocessing-the-data]]

<Youtube id="1m7BerpSq8A"/>

ہمارا اگلا کام ہمارے جائزوں اور ان کے عنوانات کو tokenize اور encode کرنا ہے۔ ہمیشہ کی طرح، ہم pretrained ماڈل checkpoint سے منسلک tokenizer لوڈ کر کے شروع کرتے ہیں۔ ہم `mt5-small` کو اپنا checkpoint استعمال کریں گے تاکہ ہم ماڈل کو مناسب وقت میں fine-tune کر سکیں:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

💡 NLP منصوبوں کے ابتدائی مراحل میں، ایک اچھا طریقہ یہ ہے کہ "small" ماڈلز کی ایک کلاس کو محدود ڈیٹا کے نمونے پر تربیت دیا جائے۔ اس سے آپ کو end-to-end workflow کی طرف جلدی debug اور iterate کرنے میں مدد ملتی ہے۔ ایک بار جب آپ نتائج پر مطمئن ہو جائیں، تو آپ ماڈل checkpoint کو تبدیل کر کے ماڈل کو آسانی سے بڑے پیمانے پر منتقل کر سکتے ہیں!

</Tip>

آئیے mT5 tokenizer کو ایک چھوٹے مثال پر آزما کر دیکھتے ہیں:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

یہاں ہم ان معروف `input_ids` اور `attention_mask` دیکھ سکتے ہیں جن کا ہمیں [Chapter 3](/course/chapter3) میں پہلے fine-tuning تجربات میں سامنا ہوا تھا۔ آئیے tokenizer کے `convert_ids_to_tokens()` فنکشن کا استعمال کرتے ہوئے ان input IDs کو decode کریں تاکہ معلوم ہو سکے کہ ہم کس قسم کے tokenizer سے نمٹ رہے ہیں:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '</s>']
```

خاص Unicode کردار `▁` اور end-of-sequence token `</s>` یہ ظاہر کرتے ہیں کہ ہم SentencePiece tokenizer سے نمٹ رہے ہیں، جو Unigram segmentation algorithm پر مبنی ہے جس کا ذکر [Chapter 6](/course/chapter6) میں کیا گیا تھا۔ Unigram خاص طور پر کثیر لسانی corpora کے لیے مفید ہے کیونکہ یہ SentencePiece کو لہجے، نقطہ وار علامات، اور اس حقیقت سے بے خبر رکھتا ہے کہ بہت سی زبانوں، جیسے کہ جاپانی، میں whitespace characters موجود نہیں ہوتے۔

ہمارے مجموعے کو tokenize کرنے کے لیے، ہمیں خلاصہ نگاری سے متعلق ایک باریک پہلو سے نمٹنا ہوگا: چونکہ ہمارے labels بھی متن ہیں، یہ ممکن ہے کہ وہ ماڈل کی زیادہ سے زیادہ context size سے تجاوز کر جائیں۔ اس کا مطلب ہے کہ ہمیں یقینی بنانے کے لیے جائزوں اور ان کے عنوانات دونوں پر truncation لاگو کرنا ہوگا تاکہ ہم اپنے ماڈل کو غیر ضروری طویل inputs نہ بھیجیں۔ 🤗 Transformers کے tokenizers ایک شاندار `text_target` آرگیومنٹ فراہم کرتے ہیں جو آپ کو inputs کے ساتھ ساتھ labels کو بھی parallel طور پر tokenize کرنے کی اجازت دیتا ہے۔ یہاں mT5 کے لیے inputs اور targets کو کس طرح process کیا جاتا ہے کی ایک مثال ہے:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

آئیے اس کوڈ کا جائزہ لیتے ہیں تاکہ سمجھ سکیں کہ کیا ہو رہا ہے۔ سب سے پہلے، ہم نے `max_input_length` اور `max_target_length` کی قدریں متعین کی ہیں، جو ہمارے جائزوں اور عنوانات کی زیادہ سے زیادہ لمبائی کو مقرر کرتی ہیں۔ چونکہ review body عموماً عنوان سے کافی بڑی ہوتی ہے، ہم نے ان قدروں کو اسی حساب سے scale کیا ہے۔

اب `preprocess_function()` کے ساتھ، پورے مجموعے کو tokenize کرنا ایک آسان عمل ہو جاتا ہے، جیسا کہ ہم نے اس کورس کے دوران `Dataset.map()` فنکشن کا وسیع استعمال کیا ہے:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

اب جبکہ مجموعہ preprocess ہو چکا ہے، آئیے کچھ ایسے میٹرکس دیکھتے ہیں جو عام طور پر خلاصہ نگاری کے لیے استعمال ہوتے ہیں۔ جیسا کہ ہم دیکھیں گے، مشینی طور پر تیار کردہ متن کے معیار کو ناپنے کے لیے کوئی جادوئی حل موجود نہیں ہے۔

<Tip>

💡 آپ نے نوٹ کیا ہوگا کہ ہم نے اوپر `Dataset.map()` فنکشن میں `batched=True` استعمال کیا۔ اس سے مثالیں ڈیفالٹ کے مطابق 1,000 کے بیچز میں encode ہوتی ہیں اور آپ کو 🤗 Transformers کے تیز tokenizers کی multithreading صلاحیتوں کا بھرپور فائدہ اٹھانے میں مدد ملتی ہے۔ جہاں ممکن ہو، `batched=True` کا استعمال کرنے کی کوشش کریں!

</Tip>

## متنی خلاصہ نگاری کے لیے میٹرکس[[metrics-for-text-summarization]]

<Youtube id="TMshhnrEXlg"/>

اس کورس کے زیادہ تر دیگر کاموں کے مقابلے میں، summarization یا translation جیسے text generation کاموں کی کارکردگی کو ناپنا اتنا سیدھا نہیں ہے۔ مثال کے طور پر، اگر ایک جائزہ ہو "I loved reading the Hunger Games"، تو اس کے لیے متعدد درست خلاصے ہو سکتے ہیں، جیسے کہ "I loved the Hunger Games" یا "Hunger Games is a great read"۔ واضح ہے کہ generated summary اور label کے درمیان کسی قسم کا exact match لاگو کرنا مناسب حل نہیں ہے — یہاں تک کہ انسان بھی ایسے میٹرکس کے تحت کم کارکردگی دکھائیں گے کیونکہ ہر کسی کا اپنا تحریری انداز ہوتا ہے۔

خلاصہ نگاری کے لیے، سب سے زیادہ استعمال ہونے والا میٹرک [ROUGE score](https://en.wikipedia.org/wiki/ROUGE_(metric)) ہے (جسے Recall-Oriented Understudy for Gisting Evaluation کہا جاتا ہے)۔ اس میٹرک کے پیچھے بنیادی خیال یہ ہے کہ generated summary کا موازنہ ایک ایسے reference summaries کے سیٹ سے کیا جائے جو عموماً انسانوں کی جانب سے تیار کیے جاتے ہیں۔ اسے مزید واضح کرنے کے لیے، فرض کریں کہ ہم مندرجہ ذیل دو خلاصوں کا موازنہ کرنا چاہتے ہیں:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

انہیں موازنہ کرنے کا ایک طریقہ یہ ہو سکتا ہے کہ overlapping الفاظ کی تعداد گنی جائے، جو اس مثال میں 6 ہوگی۔ تاہم، یہ طریقہ تھوڑا سادہ سا ہے، لہٰذا ROUGE کو overlapping کے لیے _precision_ اور _recall_ اسکورز کی بنیاد پر تیار کیا گیا ہے۔

<Tip>

🙋 اگر یہ آپ کا پہلا تجربہ ہے کہ آپ نے precision اور recall کے بارے میں سنا ہے تو فکر نہ کریں — ہم کچھ واضح مثالوں کے ذریعے اس کو مکمل طور پر واضح کریں گے۔ یہ میٹرکس عموماً classification کاموں میں دیکھے جاتے ہیں، لہٰذا اگر آپ اس سیاق و سباق میں precision اور recall کی تعریف سمجھنا چاہتے ہیں تو ہم `scikit-learn` کے [guides](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) دیکھنے کی سفارش کرتے ہیں۔

</Tip>

ROUGE کے لیے، recall یہ ماپتا ہے کہ reference summary کا کتنا حصہ generated summary نے حاصل کیا ہے۔ اگر ہم صرف الفاظ کا موازنہ کریں تو recall کو درج ذیل فارمولا کے مطابق حساب کیا جا سکتا ہے:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

ہمارے سادہ مثال کے لیے، اس فارمولا سے 6/6 = 1 کا مکمل recall ملتا ہے؛ یعنی، reference summary کے تمام الفاظ ماڈل نے پیدا کیے ہیں۔ یہ شاندار لگتا ہے، مگر تصور کریں اگر ہمارا generated summary "I really really loved reading the Hunger Games all night" ہوتا۔ ایسی صورت میں بھی مکمل recall ملتا، مگر یہ خلاصہ دراصل بدتر ہو سکتا ہے کیونکہ یہ غیر ضروری لمبا ہے۔ ایسے حالات سے نمٹنے کے لیے ہم precision بھی حساب کرتے ہیں، جو ROUGE کے تناظر میں یہ ماپتا ہے کہ generated summary کا کتنا حصہ متعلقہ ہے:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

اس کو ہمارے verbose خلاصے پر لاگو کرنے سے 6/10 = 0.6 کا precision ملتا ہے، جو کہ ہمارے مختصر خلاصے سے حاصل ہونے والے 6/7 = 0.86 کے precision سے کافی کم ہے۔ عملی طور پر، precision اور recall دونوں کو حساب کیا جاتا ہے، اور پھر F1-score (precision اور recall کا harmonic mean) رپورٹ کیا جاتا ہے۔ ہم یہ کام 🤗 Datasets میں آسانی سے کر سکتے ہیں، پہلے `rouge_score` پیکیج کو انسٹال کر کے:

```python
!pip install rouge_score
```

اور پھر ROUGE میٹرک کو درج ذیل طریقے سے لوڈ کریں:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

پھر ہم `rouge_score.compute()` فنکشن کا استعمال کر کے تمام میٹرکس کو ایک ساتھ حساب کر سکتے ہیں:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

واہ، اس output میں بہت ساری معلومات ہیں — اب یہ سب کیا مطلب رکھتی ہیں؟ سب سے پہلے، 🤗 Datasets دراصل precision، recall، اور F1-score کے لیے confidence intervals بھی حساب کرتا ہے؛ یہ وہ `low`، `mid`، اور `high` attributes ہیں جو آپ یہاں دیکھ سکتے ہیں۔ مزید برآں، 🤗 Datasets مختلف قسم کے ROUGE scores بھی حساب کرتا ہے جو generated اور reference summaries کے موازنہ میں متن کی باریکیوں پر مبنی ہوتے ہیں۔ `rouge1` variant unigrams کا overlap ہے — یہ محض الفاظ کے overlap کو کہنے کا ایک شاندار طریقہ ہے اور بالکل وہی میٹرک ہے جس کا ہم نے اوپر ذکر کیا۔ اس کی تصدیق کرنے کے لیے، آئیے اپنے scores کی `mid` value نکالتے ہیں:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

زبردست، precision اور recall کی قدریں میل کھاتی ہیں! اب ان دیگر ROUGE scores کا کیا؟ `rouge2` bigrams (الفاظ کے جوڑوں) کے overlap کو ماپتا ہے، جبکہ `rougeL` اور `rougeLsum` generated اور reference summaries میں سب سے لمبی مشترکہ الفاظ کی قطاروں کو تلاش کر کے ان کا موازنہ کرتے ہیں۔ `rougeLsum` میں "sum" اس بات کی طرف اشارہ کرتا ہے کہ یہ میٹرک پورے summary پر حساب کیا جاتا ہے، جبکہ `rougeL` کو انفرادی جملوں کے اوسط کے طور پر نکالا جاتا ہے۔

<Tip>

✏️ **آزما کر دیکھیں!** اپنی ایک مثال بنائیں جس میں ایک generated اور ایک reference summary ہو اور دیکھیں کہ resulting ROUGE scores precision اور recall کے فارمولوں پر مبنی manual حساب سے کتنے متفق ہیں۔ اضافی پوائنٹس کے لیے، متن کو bigrams میں تقسیم کریں اور `rouge2` میٹرک کے لیے precision اور recall کا موازنہ کریں۔

</Tip>

ہم ان ROUGE scores کا استعمال اپنے ماڈل کی کارکردگی کو ٹریک کرنے کے لیے کریں گے، مگر اس سے پہلے کہ ہم ایسا کریں، آئیے وہ کام کرتے ہیں جو ہر اچھے NLP practitioner کو کرنا چاہیے: ایک مضبوط مگر سادہ baseline تخلیق کریں!

### ایک مضبوط baseline تخلیق کرنا[[creating-a-strong-baseline]]

متنی خلاصہ نگاری کے لیے ایک عام baseline یہ ہے کہ کسی مضمون کی صرف پہلی تین جملے لیے جائیں، جسے عموماً _lead-3_ baseline کہا جاتا ہے۔ ہم جملوں کی حد بندی معلوم کرنے کے لیے full stops کا استعمال کر سکتے ہیں، مگر یہ "U.S." یا "U.N." جیسے مخففات پر ناکام ہو جائے گا — لہٰذا اس کی بجائے ہم `nltk` لائبریری استعمال کریں گے، جس میں ان معاملات کو سنبھالنے کے لیے بہتر الگورتھم شامل ہے۔ آپ اس پیکیج کو `pip` کے ذریعے درج ذیل طریقے سے انسٹال کر سکتے ہیں:

```python
!pip install nltk
```

اور پھر رموزِ اوقاف (punctuation) کے قواعد ڈاؤن لوڈ کریں:

```python
import nltk

nltk.download("punkt")
```

اگلا مرحلہ یہ ہے کہ ہم `nltk` سے جملہ ٹوکنائزر درآمد کریں اور ایک سادہ فنکشن بنائیں جو کسی جائزے (review) میں سے پہلی تین سزائیں نکالے۔ ٹیکسٹ سمریائزیشن میں یہ رواج ہے کہ ہر خلاصے کو ایک نئی لائن میں الگ کیا جائے، تو آئیے اس کو بھی شامل کریں اور اسے ایک تربیتی مثال پر آزمائیں:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

یہ طریقہ کارگر معلوم ہوتا ہے، لہذا اب آئیے ایک فنکشن نافذ کریں جو کسی ڈیٹا سیٹ سے یہ "خلاصے" نکالے اور بیس لائن (baseline) کے لیے ROUGE اسکورز کا حساب لگائے۔

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

پھر ہم اس فنکشن کو استعمال کرکے تصدیقی سیٹ (validation set) پر ROUGE اسکورز کا حساب لگا سکتے ہیں اور انہیں Pandas کی مدد سے مزید خوبصورت بنا سکتے ہیں۔

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

ہم دیکھ سکتے ہیں کہ `rouge2` اسکور باقی اسکورز کے مقابلے میں نمایاں طور پر کم ہے؛ یہ غالباً اس بات کی عکاسی کرتا ہے کہ جائزے کے عنوان عموماً مختصر ہوتے ہیں اور اس لیے lead-3 baseline بہت verbose ہے۔ اب جبکہ ہمارے پاس کام کرنے کے لیے ایک اچھا baseline موجود ہے، آئیں mT5 کو fine-tune کرنے کی طرف توجہ دیتے ہیں!

{#if fw === 'pt'}

## mT5 کو `Trainer` API کے ساتھ fine-tune کرنا[[fine-tuning-mt5-with-the-trainer-api]]

خلاصہ نگاری کے لیے ماڈل کو fine-tune کرنا اس باب میں شامل دیگر کاموں کی طرح ہی ہے۔ سب سے پہلے ہمیں `mt5-small` checkpoint سے pretrained ماڈل کو لوڈ کرنا ہے۔ چونکہ خلاصہ نگاری ایک sequence-to-sequence کام ہے، اس لیے ہم `AutoModelForSeq2SeqLM` کلاس کا استعمال کرتے ہوئے ماڈل کو لوڈ کر سکتے ہیں، جو خود بخود weights ڈاؤن لوڈ اور cache کر لے گا:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## Fine-tuning mT5 with Keras[[fine-tuning-mt5-with-keras]]

خلاصہ نگاری کے لیے ماڈل کو fine-tune کرنا اس باب میں شامل دیگر کاموں کی طرح ہی ہے۔ سب سے پہلی چیز جو ہمیں کرنی ہے وہ `mt5-small` checkpoint سے pretrained ماڈل کو لوڈ کرنا ہے۔ چونکہ خلاصہ نگاری ایک sequence-to-sequence کام ہے، اس لیے ہم `TFAutoModelForSeq2SeqLM` کلاس کا استعمال کرتے ہوئے ماڈل کو لوڈ کر سکتے ہیں، جو خود بخود weights ڈاؤن لوڈ اور cache کر لے گا:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

💡 اگر آپ یہ جاننا چاہتے ہیں کہ آپ کو downstream task پر ماڈل کو fine-tune کرتے وقت کوئی وارننگ کیوں نہیں ملتی، تو اس کی وجہ یہ ہے کہ sequence-to-sequence کاموں میں ہم نیٹ ورک کے تمام weights کو برقرار رکھتے ہیں۔ اس کا موازنہ [Chapter 3](/course/chapter3) میں ہمارے text classification ماڈل سے کریں، جہاں pretrained ماڈل کے head کو randomly initialized نیٹ ورک سے تبدیل کر دیا گیا تھا۔

</Tip>

اگلی چیز جو ہمیں کرنی ہے وہ Hugging Face Hub میں لاگ ان ہونا ہے۔ اگر آپ یہ کوڈ نوٹ بک میں چلا رہے ہیں، تو آپ مندرجہ ذیل utility function کے ذریعے لاگ ان ہو سکتے ہیں:

```python
from huggingface_hub import notebook_login

notebook_login()
```

جو ایک widget ظاہر کرے گا جہاں آپ اپنے credentials درج کر سکتے ہیں۔ متبادل کے طور پر، آپ یہ کمانڈ اپنے ٹرمینل میں چلا کر بھی لاگ ان ہو سکتے ہیں:

```
huggingface-cli login
```

{#if fw === 'pt'}

تربیت کے دوران ROUGE اسکورز کا حساب لگانے کے لیے ہمیں خلاصے generate کرنے ہوں گے۔ خوش قسمتی سے، 🤗 Transformers مخصوص `Seq2SeqTrainingArguments` اور `Seq2SeqTrainer` کلاسز فراہم کرتا ہے جو یہ کام خود بخود انجام دیتی ہیں! یہ دیکھنے کے لیے کہ یہ کیسے کام کرتا ہے، آئیں پہلے ہمارے تجربات کے لیے hyperparameters اور دیگر آرگیومنٹس کو متعین کرتے ہیں:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

یہاں، `predict_with_generate` آرگیومنٹ اس بات کی نشاندہی کرتا ہے کہ evaluation کے دوران ہمیں خلاصے generate کرنے چاہئیں تاکہ ہم ہر epoch کے لیے ROUGE اسکورز کا حساب لگا سکیں۔ جیسا کہ [Chapter 1](/course/chapter1) میں بیان کیا گیا ہے، decoder inference اس وقت کام کرتا ہے جب وہ tokens کو ایک ایک کر کے پیش گوئی کرتا ہے، اور یہ عمل ماڈل کی `generate()` میتھڈ کے ذریعے نافذ کیا جاتا ہے۔ `predict_with_generate=True` سیٹ کرنے سے `Seq2SeqTrainer` کو یہ ہدایت ملتی ہے کہ evaluation کے لیے اسی میتھڈ کا استعمال کریں۔ ہم نے کچھ default hyperparameters جیسے کہ learning rate، epochs کی تعداد، اور weight decay کو بھی ایڈجسٹ کیا ہے، اور `save_total_limit` آپشن کو اس طرح سیٹ کیا ہے کہ تربیت کے دوران صرف 3 checkpoints تک محفوظ ہوں – کیونکہ mT5 کا "small" ورژن تقریباً 1 GB ہارڈ ڈرائیو اسپیس استعمال کرتا ہے، اور محفوظ شدہ نقول کی تعداد کو محدود کر کے تھوڑی جگہ بچائی جا سکتی ہے۔

`push_to_hub=True` آرگیومنٹ ہمیں تربیت کے بعد ماڈل کو Hub پر push کرنے کی اجازت دیتا ہے؛ آپ کو output_dir کے ذریعہ متعین جگہ پر اپنی user profile کے تحت repository مل جائے گی۔ نوٹ کریں کہ آپ `hub_model_id` آرگیومنٹ کے ذریعے اس repository کا نام بھی مخصوص کر سکتے ہیں جسے آپ push کرنا چاہتے ہیں (خاص طور پر اگر آپ کسی organization میں push کر رہے ہیں تو آپ کو یہ آرگیومنٹ استعمال کرنا ہوگا)۔ مثال کے طور پر، جب ہم نے ماڈل کو [**huggingface-course** organization](https://huggingface.co/huggingface-course) پر push کیا، تو ہم نے `Seq2SeqTrainingArguments` میں `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` شامل کیا۔

اب اگلی چیز یہ ہے کہ ہم trainer کو ایک `compute_metrics()` فنکشن فراہم کریں تاکہ تربیت کے دوران ماڈل کا evaluation کیا جا سکے۔ خلاصہ نگاری کے لیے یہ عمل صرف ماڈل کی predictions پر `rouge_score.compute()` کال کرنے سے زیادہ پیچیدہ ہے کیونکہ ہمیں ROUGE اسکورز کا حساب لگانے سے پہلے outputs اور labels کو متن میں _decode_ کرنا ہوتا ہے۔ مندرجہ ذیل فنکشن بالکل یہی کام کرتا ہے، اور `nltk` کی `sent_tokenize()` فنکشن کا بھی استعمال کرتا ہے تاکہ خلاصے کی جملوں کو newline کے ساتھ الگ کیا جا سکے:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}


اگلا، ہمیں اپنے sequence-to-sequence کام کے لیے data collator متعین کرنے کی ضرورت ہے۔ چونکہ mT5 ایک encoder-decoder Transformer ماڈل ہے، اس لیے batches تیار کرتے وقت ایک نزاکت یہ ہے کہ decoding کے دوران ہمیں labels کو ایک جگہ دائیں کی طرف shift کرنا پڑتا ہے۔ یہ اس لیے ضروری ہے تاکہ decoder صرف پچھلے ground truth labels کو دیکھے نہ کہ موجودہ یا مستقبل کے، جو ماڈل کے لیے یاد رکھنا آسان ہوتا ہے۔ یہ ویسا ہی ہے جیسے کہ [causal language modeling](/course/chapter7/6) کے کام میں inputs پر masked self-attention لاگو کی جاتی ہے۔

خوش قسمتی سے، 🤗 Transformers ایک `DataCollatorForSeq2Seq` collator فراہم کرتا ہے جو ہمارے لیے inputs اور labels کو dynamically pad کر دیتا ہے۔ اس collator کو instantiate کرنے کے لیے ہمیں صرف `tokenizer` اور `model` فراہم کرنے کی ضرورت ہے:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

آئیں دیکھتے ہیں کہ یہ collator ایک چھوٹے بیچ کے نمونوں پر کیا پیدا کرتا ہے۔ سب سے پہلے، ہمیں ایسے کالمز کو ہٹانا ہوگا جن میں strings شامل ہیں، کیونکہ collator ان عناصر کو pad کرنے کا طریقہ نہیں جانتا۔

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

چونکہ collator ایک `dict` کی فہرست کی توقع کرتا ہے، جہاں ہر `dict` ڈیٹاسیٹ کی ایک واحد مثال کی نمائندگی کرتا ہے، اس لیے ہمیں data collator کو پاس کرنے سے پہلے ڈیٹا کو متوقع فارمیٹ میں ڈھالنا بھی ضروری ہے:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

یہاں سب سے اہم بات یہ ہے کہ پہلی مثال دوسری سے لمبی ہے، اس لیے دوسری مثال کے `input_ids` اور `attention_mask` کو دائیں جانب `[PAD]` ٹوکن (جس کا ID `0` ہے) کے ساتھ pad کیا گیا ہے۔ اسی طرح، ہم دیکھ سکتے ہیں کہ `labels` کو `-100` سے pad کیا گیا ہے تاکہ loss function کے دوران padding tokens کو نظرانداز کیا جائے۔ اور آخر میں، ہم ایک نیا `decoder_input_ids` دیکھ سکتے ہیں جس نے پہلا اندراج میں `[PAD]` ٹوکن شامل کر کے labels کو دائیں جانب منتقل کر دیا ہے۔

{#if fw === 'pt'}

آخرکار، ہمارے پاس تربیت کے لیے تمام اجزاء موجود ہیں! اب ہمیں صرف standard arguments کے ساتھ trainer کو instantiate کرنا ہے:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

اب ہم اپنی ٹریننگ کو لانچ کرنے کے لیے `trainer.train()` چلا سکتے ہیں۔

```python
trainer.train()
```

تربیت کے دوران، آپ کو دیکھنا چاہیے کہ training loss کم ہوتی جائے اور ہر epoch کے ساتھ ROUGE اسکورز میں اضافہ ہوتا جائے۔ تربیت مکمل ہونے کے بعد، آپ `Trainer.evaluate()` چلا کر حتمی ROUGE اسکورز دیکھ سکتے ہیں۔

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

اسکورز سے ظاہر ہوتا ہے کہ ہمارا ماڈل ہمارے lead-3 baseline سے بہتر کارکردگی کا مظاہرہ کر چکا ہے -- زبردست! آخری کام یہ ہے کہ ماڈل کے وزن کو Hub پر push کیا جائے، جیسا کہ درج ذیل ہے:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

یہ `output_dir` میں چیک پوائنٹ اور کنفیگریشن فائلیں محفوظ کر دے گا، اس سے پہلے کہ تمام فائلیں Hub پر اپلوڈ کی جائیں۔ `tags` آرگیومنٹ مخصوص کر کے، ہم یہ بھی یقینی بناتے ہیں کہ Hub پر موجود وجٹ خلاصہ سازی کے پائپ لائن کے لیے ہو نہ کہ mT5 آرکیٹیکچر سے منسلک ڈیفالٹ ٹیکسٹ جنریشن کے لیے (ماڈل ٹیگز کے بارے میں مزید معلومات کے لیے دیکھیں the [🤗 Hub documentation](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). `trainer.push_to_hub()` کا آؤٹ پٹ Git commit ہیش کے لیے ایک URL ہے، لہٰذا آپ آسانی سے ماڈل ریپوزیٹری میں کی جانے والی تبدیلیاں دیکھ سکتے ہیں!

اس حصے کو ختم کرنے کے لیے، آئیے دیکھتے ہیں کہ ہم 🤗 Accelerate کی فراہم کردہ نچلی سطح کی خصوصیات استعمال کرتے ہوئے mT5 کو کیسے fine-tune کر سکتے ہیں۔

{:else}

ہم تقریباً تربیت کے لیے تیار ہیں! ہمیں صرف اپنے datasets کو `tf.data.Dataset`s میں تبدیل کرنا ہے، جیسا کہ ہم نے اوپر data collator میں بیان کیا ہے، اور پھر ماڈل کو `compile()` اور `fit()` کرنا ہے۔ پہلے، datasets:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

اب، ہم اپنی تربیتی hyperparameters کو متعین کرتے ہیں اور compile کرتے ہیں:

```python
from transformers import create_optimizer
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

اور آخر میں، ہم ماڈل کو fit کرتے ہیں۔ ہم `PushToHubCallback` استعمال کرتے ہیں تاکہ ہر epoch کے بعد ماڈل کو Hub پر محفوظ کر لیا جائے، جس سے ہمیں بعد میں inference کے لیے استعمال کرنے میں سہولت ہوگی:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

ہمیں تربیت کے دوران کچھ loss ویلیوز ملیں، لیکن دراصل ہم وہ ROUGE میٹرکس دیکھنا چاہتے ہیں جو ہم نے پہلے حساب کیے تھے۔ ان میٹرکس کو حاصل کرنے کے لیے، ہمیں ماڈل سے outputs generate کرنے ہوں گے اور انہیں strings میں تبدیل کرنا ہوگا۔ آئیے ROUGE میٹرک کے لیے labels اور predictions کی کچھ فہرستیں بناتے ہیں تاکہ موازنہ کیا جا سکے (نوٹ کریں کہ اگر اس حصے میں import errors آتے ہیں، تو آپ کو `!pip install tqdm` کرنا پڑ سکتا ہے)۔ ہم ایک ایسا طریقہ بھی استعمال کرنے جا رہے ہیں جو کارکردگی میں ڈرامائی بہتری لاتا ہے - ہماری generation کوڈ کو [XLA](https://www.tensorflow.org/xla) کے ساتھ compile کرنا، جو کہ TensorFlow کا accelerated linear algebra compiler ہے۔ XLA ماڈل کے computation graph پر مختلف optimizations لگاتا ہے، اور اس سے رفتار اور میموری کے استعمال میں نمایاں بہتری آتی ہے۔ جیسا کہ Hugging Face کے [blog](https://huggingface.co/blog/tf-xla-generate) میں بیان کیا گیا ہے، XLA بہترین کارکردگی تب دکھاتا ہے جب ہمارے input shapes زیادہ مختلف نہ ہوں۔ اس مسئلے کو حل کرنے کے لیے، ہم اپنے inputs کو 128 کے multiples تک pad کریں گے، اور padding collator کے ساتھ ایک نیا dataset بنائیں گے، اور پھر ہم اپنی generation function پر `@tf.function(jit_compile=True)` decorator لگائیں گے، جو پورے function کو XLA کے ساتھ compilation کے لیے نشان زد کرتا ہے۔

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

اب جب کہ ہمارے پاس لیبل اور پیش گوئی کی گئی سٹرنگز کی فہرستیں موجود ہیں، تو ROUGE اسکور کا حساب لگانا آسان ہے۔

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}


## mT5 کو 🤗 Accelerate کے ساتھ فائن-ٹیون کرنا[[fine-tuning-mt5-with-accelerate]]

🤗 Accelerate کے ساتھ ہمارے ماڈل کو فائن-ٹیون کرنا [Chapter 3](/course/chapter3) میں پیش کیے گئے ٹیکسٹ کلاسفیکیشن مثال سے کافی ملتا جلتا ہے۔ اہم فرق یہ ہوگا کہ تربیت کے دوران ہمیں اپنی سمریز کو واضح طور پر جنریٹ کرنا ہوگا اور ROUGE اسکورز کو کیسے شمار کیا جائے یہ بھی متعین کرنا ہوگا (یاد رکھیں کہ `Seq2SeqTrainer` نے ہمارے لیے جنریشن کا انتظام کیا تھا)۔ آئیں دیکھتے ہیں کہ ہم 🤗 Accelerate کے اندر ان دونوں تقاضوں کو کیسے نافذ کر سکتے ہیں!

### تربیت کے لیے سب کچھ تیار کرنا[[preparing-everything-for-training]]

سب سے پہلی چیز جو ہمیں کرنی ہے وہ ہے ہمارے ہر سپلٹ کے لیے ایک `DataLoader` بنانا۔ چونکہ PyTorch کے DataLoader ٹینسرز کے بیچز کی توقع رکھتے ہیں، ہمیں اپنے datasets میں format کو `"torch"` پر سیٹ کرنا ہو گا:

```python
tokenized_datasets.set_format("torch")
```

اب جب کہ ہمارے پاس صرف ٹینسرز پر مشتمل datasets ہیں، اگلی چیز یہ ہے کہ دوبارہ `DataCollatorForSeq2Seq` کو instantiate کیا جائے۔ اس کے لیے ہمیں ماڈل کا ایک تازہ ورژن فراہم کرنا ہو گا، لہٰذا آئیں اسے دوبارہ ہمارے cache سے لوڈ کرتے ہیں:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

اس کے بعد ہم data collator کو instantiate کر سکتے ہیں اور اس کا استعمال کرتے ہوئے اپنے dataloaders کو define کر سکتے ہیں:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

اگلی چیز یہ ہے کہ ہم جس optimizer کو استعمال کرنا چاہتے ہیں اسے define کریں۔ ہماری دیگر مثالوں کی طرح، ہم `AdamW` کا استعمال کریں گے، جو زیادہ تر مسائل کے لیے اچھا کام کرتا ہے:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

آخر میں، ہم اپنے ماڈل، optimizer، اور dataloaders کو `accelerator.prepare()` method میں دیتے ہیں:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

🚨 اگر آپ TPU پر تربیت کر رہے ہیں، تو آپ کو اوپر دیا گیا تمام کوڈ ایک مخصوص training function میں منتقل کرنے کی ضرورت ہو گی۔ مزید تفصیلات کے لیے [Chapter 3](/course/chapter3) ملاحظہ کریں۔

</Tip>

اب جب کہ ہم نے اپنے objects تیار کر لیے ہیں، تین باقی چیزیں کرنا باقی ہیں:

* learning rate schedule کو define کرنا۔
* evaluation کے لیے سمریز کو post-process کرنے کا function نافذ کرنا۔
* Hub پر ایک repository بنانا جس پر ہم اپنا ماڈل push کر سکیں۔

learning rate schedule کے لیے، ہم پچھلے سیکشنز سے standard linear schedule کا استعمال کریں گے:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

پوسٹ پروسیسنگ کے لیے، ہمیں ایک فنکشن کی ضرورت ہے جو تیار کردہ خلاصوں کو جملوں میں تقسیم کرے، جنہیں نئی لائنوں سے الگ کیا جائے۔ یہ وہی فارمیٹ ہے جس کی ROUGE میٹرک توقع کرتی ہے۔ ہم اسے درج ذیل کوڈ سے حاصل کر سکتے ہیں:  

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

یہ آپ کو مانوس لگنا چاہیے اگر آپ کو یاد ہو کہ ہم نے `Seq2SeqTrainer` کے `compute_metrics()` فنکشن کو کیسے define کیا تھا۔

آخر میں، ہمیں Hugging Face Hub پر ایک ماڈل ریپوزٹری بنانے کی ضرورت ہے۔ اس کے لیے، ہم مناسب عنوان والی 🤗 Hub لائبریری استعمال کر سکتے ہیں۔ ہمیں صرف اپنی ریپوزٹری کے لیے ایک نام متعین کرنا ہے، اور لائبریری کے پاس ایک utility function ہے جو ریپوزٹری ID کو یوزر پروفائل کے ساتھ ملا دیتا ہے:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

اب ہم اس ریپوزٹری نام کو استعمال کر کے اپنی results directory میں ایک مقامی ورژن کلون کر سکتے ہیں جو training artifacts کو محفوظ کرے گی:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

یہ ہمیں training کے دوران `repo.push_to_hub()` میتھڈ کو کال کر کے artifacts کو Hub پر واپس push کرنے کی اجازت دے گا! آئیں اب اپنی تجزیہ کو ختم کرتے ہوئے training loop لکھتے ہیں۔

### Training loop[[training-loop]]

خلاصہ سازی کے لیے training loop دیگر 🤗 Accelerate مثالوں کے کافی مشابہ ہے جن کا ہمیں سامنا ہوا ہے اور اسے تقریباً چار بنیادی مراحل میں تقسیم کیا گیا ہے:

1. ہر epoch کے لیے `train_dataloader` میں موجود تمام مثالوں پر iterate کر کے ماڈل کو ٹرین کریں۔
2. ہر epoch کے اختتام پر ماڈل کی سمریز generate کریں، پہلے tokens generate کر کے اور پھر انہیں (اور reference سمریز کو) ٹیکسٹ میں decode کر کے۔
3. انہی تکنیکوں کا استعمال کرتے ہوئے ROUGE اسکورز compute کریں جو ہم نے پہلے دیکھی تھیں۔
4. چیک پوائنٹس کو محفوظ کریں اور سب کچھ Hub پر push کریں۔ یہاں ہم `Repository` آبجیکٹ کے `blocking=False` آرگیومنٹ پر انحصار کرتے ہیں تاکہ ہم ہر epoch کے بعد چیک پوائنٹس کو _asynchronously_ push کر سکیں۔ اس سے ہمیں training جاری رکھنے کی اجازت ملتی ہے بغیر اس کے کہ ہمیں GB سائز کے ماڈل کی upload کی نسبتا سست رفتار کا انتظار کرنا پڑے!

یہ مراحل درج ذیل کوڈ بلاک میں دیکھے جا سکتے ہیں:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

اور بس یہی ہے! ایک بار جب آپ اسے چلا لیں گے، تو آپ کے پاس ایک ایسا ماڈل اور نتائج ہوں گے جو `Trainer` سے حاصل کردہ نتائج کے کافی قریب ہوں گے.
{/if}

## اپنے فائن-ٹیونڈ ماڈل کا استعمال[[using-your-fine-tuned-model]]

ایک بار جب آپ ماڈل کو Hub پر push کر دیں، تو آپ اسے یا تو inference widget کے ذریعے یا `pipeline` آبجیکٹ کے ساتھ استعمال کر سکتے ہیں، جیسا کہ نیچے دیا گیا ہے:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

ہم ٹیسٹ سیٹ سے کچھ مثالیں (جنہیں ماڈل نے نہیں دیکھا) اپنے pipeline کو دے سکتے ہیں تاکہ سمریز کے معیار کا اندازہ ہو سکے۔ سب سے پہلے آئیں ایک سادہ فنکشن نافذ کرتے ہیں جو ریویو، ٹائٹل، اور generated summary کو ایک ساتھ دکھائے:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

آئیں انگریزی مثالوں میں سے ایک کو دیکھتے ہیں جو ہمیں ملتی ہے:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn’t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It’s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

یہ زیادہ خراب نہیں ہے! ہم دیکھ سکتے ہیں کہ ہمارا ماڈل _abstractive_ summarization انجام دینے میں کامیاب رہا ہے، جس میں ریویو کے کچھ حصوں کو نئے الفاظ کے ساتھ بڑھایا گیا ہے۔ اور شاید ہمارے ماڈل کا سب سے دلچسپ پہلو یہ ہے کہ یہ دو لسانی ہے، لہٰذا ہم اسپینیش ریویوز کی سمریز بھی generate کر سکتے ہیں:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

اس سمری کا ترجمہ انگریزی میں "Very easy to read" ہوتا ہے، جیسا کہ ہم اس صورت میں دیکھ سکتے ہیں کہ اسے براہ راست ریویو سے نکالا گیا تھا۔ بہرحال، یہ mT5 ماڈل کی وسعتِ قابلیت کو ظاہر کرتا ہے اور آپ کو ایک کثیر لسانی مجموعہ سے نمٹنے کا تجربہ فراہم کرتا ہے!

اگلے مرحلے میں، ہم اپنی توجہ ایک قدر زیادہ پیچیدہ کام کی طرف مرکوز کریں گے: ایک زبان ماڈل کو ابتداء سے تربیت دینا۔
