<FrameworkSwitchCourse {fw} />

# ترجمہ[[translation]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
]} />

{/if}

آئیے اب ترجمے میں ڈوبتے ہیں۔ یہ ایک اور [سیquence-to-sequence ٹاسک](/course/chapter1/7) ہے، جس کا مطلب ہے کہ مسئلہ کو ایک سیquence سے دوسری سیquence میں تبدیل کرنے کے مسئلے کی صورت میں پیش کیا جا سکتا ہے۔ اس لحاظ سے یہ مسئلہ [خلاصہ نگاری](/course/chapter7/6) کے بہت قریب ہے، اور آپ یہاں دیکھی جانے والی تکنیک کو دیگر sequence-to-sequence مسائل پر بھی اپن سکتے ہیں جیسے کہ:

- **اسٹائل ٹرانسفر**: ایسا ماڈل بنانا جو ایک مخصوص انداز میں لکھی گئی تحریروں کو دوسرے انداز میں *ترجمہ* کر دے (مثلاً، رسمی انداز سے غیر رسمی یا شیکسپیئرین انگریزی سے جدید انگریزی)
- **جنریٹو سوال و جواب**: ایسا ماڈل تیار کرنا جو کسی سیاق و سباق کو دیکھتے ہوئے سوالات کے جوابات تیار کرے

<Youtube id="1JvfrvZgi6c"/>

اگر آپ کے پاس دو (یا زیادہ) زبانوں میں کافی بڑی مقدار میں متون موجود ہوں، تو آپ شروع سے ایک نیا ترجمہ ماڈل تربیت دے سکتے ہیں جیسا کہ ہم [causal language modeling](/course/chapter7/6) کے حصے میں دیکھیں گے۔ تاہم، ایک موجودہ ترجمہ ماڈل کو fine-tune کرنا زیادہ تیز ہوگا، چاہے وہ mT5 یا mBART جیسے کثیر لسانی ماڈل ہوں جنہیں آپ مخصوص زبان جوڑے کے لیے fine-tune کرنا چاہتے ہوں، یا پھر ایک ایسا ماڈل جو ایک زبان سے دوسری زبان کے ترجمے کے لیے خاص ہو اور آپ اپنے مخصوص مجموعے کے لیے fine-tune کرنا چاہتے ہوں۔

اس سیکشن میں، ہم ایک Marian ماڈل کو fine-tune کریں گے جو پہلے سے انگریزی سے فرانسیسی میں ترجمہ کرنے کے لیے تربیت یافتہ ہے (کیونکہ بہت سے Hugging Face ملازمین دونوں زبانیں بولتے ہیں) [KDE4 dataset](https://huggingface.co/datasets/kde4) پر، جو [KDE ایپس](https://apps.kde.org/) کے مقامی فائلوں کا مجموعہ ہے۔ جس ماڈل کو ہم استعمال کریں گے اسے ایک بڑے فرانسیسی اور انگریزی متون کے مجموعے پر پہلے سے تربیت دی گئی ہے جو [Opus dataset](https://opus.nlpl.eu/) سے لیا گیا ہے، جس میں دراصل KDE4 dataset بھی شامل ہے۔ لیکن چاہے ہمارا استعمال کردہ pretrained ماڈل اپنی تربیت کے دوران اس ڈیٹا کو دیکھ چکا ہو، ہم دیکھیں گے کہ fine-tuning کے بعد اس کی کارکردگی بہتر ہو سکتی ہے۔

ایک دفعہ ہم کام مکمل کر لیں، ہمارے پاس ایسا ماڈل ہوگا جو اس طرح کی پیش گوئیاں کر سکے گا:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

پچھلے سیکشنز کی طرح، آپ اس اصلی ماڈل کو دیکھ سکتے ہیں جسے ہم تربیت دیں گے اور Hub پر اپلوڈ کریں گے، نیچے دیا گیا کوڈ استعمال کریں اور اس کی پیش گوئیوں کو [یہاں](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.) ڈبل چیک کریں۔

## ڈیٹا کی تیاری[[preparing-the-data]]

ایک ترجمہ ماڈل کو شروع سے تربیت دینے یا fine-tune کرنے کے لیے، ہمیں ایک ایسا ڈیٹاسیٹ چاہیے جو اس کام کے لیے موزوں ہو۔ جیسا کہ پہلے ذکر کیا گیا ہے، اس سیکشن میں ہم [KDE4 dataset](https://huggingface.co/datasets/kde4) استعمال کریں گے، لیکن آپ آسانی سے اپنے ڈیٹا کے ساتھ کوڈ کو ایڈجسٹ کر سکتے ہیں، بشرطیکہ آپ کے پاس ان دو زبانوں میں جملوں کے جوڑے موجود ہوں جن سے آپ ترجمہ کرنا چاہتے ہیں۔ اگر آپ کو اپنے کسٹم ڈیٹا کو `Dataset` میں لوڈ کرنے کا طریقہ یاد نہیں تو [Chapter 5](/course/chapter5) ملاحظہ کریں۔

### KDE4 ڈیٹاسیٹ[[the-kde4-dataset]]

ہمیشہ کی طرح، ہم `load_dataset()` فنکشن استعمال کرکے اپنا ڈیٹاسیٹ ڈاؤن لوڈ کرتے ہیں:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

اگر آپ کسی اور زبان کے جوڑے کے ساتھ کام کرنا چاہتے ہیں، تو آپ ان کے کوڈز کے ذریعے وضاحت کر سکتے ہیں۔ اس ڈیٹاسیٹ میں کل 92 زبانیں دستیاب ہیں؛ آپ ان سب کو اس کے [dataset card](https://huggingface.co/datasets/kde4) پر زبان ٹیگز کو وسعت دے کر دیکھ سکتے ہیں۔

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

آئیے ڈیٹاسیٹ کو دیکھتے ہیں:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

ہمارے پاس 210,173 جملوں کے جوڑے ہیں، لیکن یہ سب ایک ہی سپلٹ میں ہیں، لہٰذا ہمیں اپنا validation سیٹ بنانا ہوگا۔ جیسا کہ ہم نے [Chapter 5](/course/chapter5) میں دیکھا کہ ایک `Dataset` میں `train_test_split()` میتھڈ ہوتا ہے جو اس میں مدد کر سکتا ہے۔ ہم reproducibility کے لیے ایک seed فراہم کریں گے:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

ہم `"test"` کلید کو اس طرح `"validation"` میں تبدیل کر سکتے ہیں:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

اب آئیے ڈیٹاسیٹ کے ایک عنصر کو دیکھتے ہیں:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par défaut, développer les fils de discussion'}
```

ہمیں ایک ڈکشنری ملتی ہے جس میں دو جملے ہوتے ہیں جنہیں ہم نے مطلوبہ زبان کے جوڑے میں مانگا تھا۔ اس ڈیٹاسیٹ کی ایک خصوصیت جو تکنیکی کمپیوٹر سائنس کے اصطلاحات سے بھرا ہوا ہے یہ ہے کہ یہ مکمل طور پر فرانسیسی میں ترجمہ کیا گیا ہے۔ تاہم، فرانسیسی انجینئرز گفتگو میں زیادہ تر کمپیوٹر سائنس مخصوص الفاظ کو انگریزی میں ہی چھوڑ دیتے ہیں۔ یہاں، مثال کے طور پر، لفظ "threads" ایک فرانسیسی جملے میں ظاہر ہو سکتا ہے، خاص طور پر تکنیکی گفتگو میں؛ مگر اس ڈیٹاسیٹ میں اسے زیادہ درست "fils de discussion" میں ترجمہ کیا گیا ہے۔ ہمارا pretrained ماڈل، جسے زیادہ بڑے فرانسیسی اور انگریزی جملوں کے مجموعے پر تربیت دی گئی ہے، آسانی سے لفظ کو اسی طرح چھوڑ دیتا ہے:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par défaut pour les threads élargis'}]
```

اسی رویے کی ایک اور مثال لفظ "plugin" کے ساتھ دیکھی جا سکتی ہے، جو باضابطہ طور پر فرانسیسی لفظ نہیں ہے لیکن زیادہ تر مادری زبان بولنے والے اسے سمجھ لیتے ہیں اور ترجمہ کرنے کی زحمت نہیں کرتے۔
KDE4 ڈیٹاسیٹ میں اس لفظ کو فرانسیسی میں زیادہ باقاعدہ "module d'extension" میں ترجمہ کیا گیا ہے:

```py
split_datasets["train"][172]["translation"]
```

```python
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ہمارا pretrained ماڈل، تاہم، مختصر اور مانوس انگریزی لفظ کے ساتھ چپکا رہتا ہے:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

یہ دیکھنا دلچسپ ہوگا کہ کیا ہمارا fine-tuned ماڈل ڈیٹاسیٹ کی ان خصوصیات کو اپناتا ہے (چھوٹا سا اشارہ: یہ کرے گا)۔

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

✏️ **آپ کی باری!** ایک اور انگریزی لفظ جو فرانسیسی میں اکثر استعمال ہوتا ہے "email." تربیتی ڈیٹاسیٹ میں اس لفظ کا پہلا نمونہ تلاش کریں۔ اسے کیسے ترجمہ کیا گیا ہے؟ اور وہی انگریزی جملہ pretrained ماڈل کیسے ترجمہ کرتا ہے؟

</Tip>

### ڈیٹا کی پروسیسنگ[[processing-the-data]]

<Youtube id="XAR8jnZZuUs"/>

اب تک کی تربیت آپ کو معلوم ہو گئی ہوگی: تمام متون کو token IDs کے مجموعوں میں تبدیل کرنا ضروری ہے تاکہ ماڈل انہیں سمجھ سکے۔ اس کام کے لیے، ہمیں inputs اور targets دونوں کو tokenize کرنا ہوگا۔ ہمارا پہلا کام `tokenizer` آبجیکٹ تخلیق کرنا ہے۔ جیسا کہ پہلے بتایا گیا تھا، ہم Marian انگریزی سے فرانسیسی pretrained ماڈل استعمال کر رہے ہیں۔ اگر آپ یہ کوڈ کسی اور زبان کے جوڑے کے لیے استعمال کر رہے ہیں تو یقینی بنائیں کہ model checkpoint کو accordingly ایڈجسٹ کریں۔ [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) آرگنائزیشن متعدد زبانوں میں ہزاروں ماڈلز فراہم کرتی ہے۔

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

آپ `model_checkpoint` کو [Hub](https://huggingface.co/models) سے کسی اور ماڈل یا کسی مقامی فولڈر سے جہاں آپ نے pretrained ماڈل اور tokenizer محفوظ کیا ہو، سے بھی تبدیل کر سکتے ہیں۔

<Tip>

💡 اگر آپ mBART، mBART-50، یا M2M100 جیسے کثیر لسانی tokenizer استعمال کر رہے ہیں تو آپ کو اپنے inputs اور targets کے زبان کوڈز کو `tokenizer.src_lang` اور `tokenizer.tgt_lang` سیٹ کرکے درست طریقے سے متعین کرنا ہوگا۔

</Tip>

ہمارے ڈیٹا کی تیاری کافی سیدھی ہے۔ بس ایک بات یاد رکھنی ہے؛ آپ کو یہ یقینی بنانا ہے کہ tokenizer targets کو آؤٹ پٹ زبان (یہاں، فرانسیسی) میں process کرے۔ آپ یہ tokenizer کے `__call__` میتھڈ کے `text_target` آرگیومنٹ میں targets دے کر کر سکتے ہیں۔

یہ دیکھنے کے لیے کہ یہ کیسے کام کرتا ہے، آئیے تربیتی سیٹ کے ہر زبان کا ایک نمونہ process کرتے ہیں:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

جیسا کہ دیکھا جا سکتا ہے، آؤٹ پٹ میں انگریزی جملے سے وابستہ input IDs شامل ہیں، جبکہ فرانسیسی جملے کے IDs `labels` فیلڈ میں محفوظ ہیں۔ اگر آپ یہ بتانا بھول جائیں کہ آپ labels کو tokenize کر رہے ہیں، تو وہ input tokenizer کے ذریعے tokenize ہو جائیں گے، جو کہ Marian ماڈل کی صورت میں بالکل صحیح نہیں ہوگا:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']
['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']
```

جیسا کہ دیکھا جا سکتا ہے، انگریزی tokenizer کو فرانسیسی جملے کو process کرنے سے کہیں زیادہ tokens بن جاتے ہیں، کیونکہ tokenizer کو فرانسیسی الفاظ کا علم نہیں ہوتا (سوائے ان الفاظ کے جو انگریزی میں بھی موجود ہیں، جیسے "discussion")۔

چونکہ `inputs` ایک ڈکشنری ہے جس میں ہمارے معمول کے keys (input IDs، attention mask، وغیرہ) موجود ہیں، آخری قدم یہ ہے کہ ہم ڈیٹاسیٹس پر لگانے کے لیے preprocessing function تعریف کریں:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

نوٹ کریں کہ ہم نے inputs اور outputs دونوں کے لیے ایک ہی زیادہ سے زیادہ لمبائی (max_length) مقرر کی ہے۔ چونکہ ہمارے متون کافی چھوٹے نظر آتے ہیں، ہم 128 استعمال کرتے ہیں۔

<Tip>

💡 اگر آپ T5 ماڈل استعمال کر رہے ہیں (خاص طور پر `t5-xxx` چیک پوائنٹس میں سے کوئی)، تو ماڈل توقع کرے گا کہ text inputs میں کام کی نوعیت کی نشاندہی کرنے والا prefix شامل ہو، جیسے کہ `translate: English to French:`۔

</Tip>

<Tip warning={true}>

⚠️ ہم targets کے attention mask پر توجہ نہیں دیتے، کیونکہ ماڈل کو اس کی توقع نہیں ہوتی۔ بلکہ، padding token کے لیے labels کو `-100` پر سیٹ کرنا چاہیے تاکہ loss computation میں ان پڈنگ شدہ قدروں کو نظرانداز کیا جا سکے۔ یہ کام بعد میں ہمارے data collator کے ذریعے dynamic padding لگاتے وقت کیا جائے گا، لیکن اگر آپ یہاں padding استعمال کرتے ہیں تو preprocessing function کو اس طرح ایڈجسٹ کریں کہ padding token کے لیے تمام labels کو `-100` پر سیٹ کیا جائے۔

</Tip>

اب ہم اس preprocessing کو ایک ساتھ اپنے ڈیٹاسیٹ کے تمام splits پر لاگو کر سکتے ہیں:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

اب جبکہ ڈیٹا کو preprocess کر لیا گیا ہے، ہم اپنے pretrained ماڈل کو fine-tune کرنے کے لیے تیار ہیں!

{#if fw === 'pt'}

## `Trainer` API کے ساتھ ماڈل کی fine-tuning[[fine-tuning-the-model-with-the-trainer-api]]

اصل کوڈ جو `Trainer` استعمال کرتا ہے وہ پہلے جیسا ہی رہے گا، صرف ایک چھوٹا سا فرق یہ ہے کہ یہاں ہم [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) استعمال کرتے ہیں، جو `Trainer` کی ایک ذیلی کلاس ہے اور ہمیں evaluation کے دوران inputs سے outputs کی پیش گوئی کے لیے `generate()` میتھڈ استعمال کرنے کی سہولت دیتی ہے۔ ہم اس پر مزید تفصیل سے تب بات کریں گے جب ہم metric computation کی بات کریں گے۔

سب سے پہلے، ہمیں fine-tune کرنے کے لیے ایک اصلی ماڈل کی ضرورت ہے۔ ہم معمول کا `AutoModel` API استعمال کریں گے:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## Keras کے ساتھ ماڈل کی fine-tuning[[fine-tuning-the-model-with-keras]]

سب سے پہلے، ہمیں fine-tune کرنے کے لیے ایک اصلی ماڈل کی ضرورت ہے۔ ہم معمول کا `AutoModel` API استعمال کریں گے:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

💡 `Helsinki-NLP/opus-mt-en-fr` چیک پوائنٹ میں صرف PyTorch weights موجود ہیں، لہٰذا اگر آپ `from_pretrained()` میتھڈ میں `from_pt=True` آرگیومنٹ استعمال کیے بغیر ماڈل لوڈ کرنے کی کوشش کریں گے تو ایرر آئے گا۔ جب آپ `from_pt=True` استعمال کرتے ہیں، تو لائبریری خود بخود PyTorch weights کو ڈاؤن لوڈ اور convert کر دے گی۔ جیسا کہ آپ دیکھ سکتے ہیں، 🤗 Transformers میں frameworks کے درمیان سوئچ کرنا بہت آسان ہے!

</Tip>

{/if}

نوٹ کریں کہ اس بار ہم ایسے ماڈل کو استعمال کر رہے ہیں جسے ترجمہ کے کام پر تربیت دی گئی ہے اور اسے دراصل استعمال کیا جا سکتا ہے، لہٰذا weights کے گم ہونے یا نئے initialize ہونے کے بارے میں کوئی وارننگ نہیں آئے گی۔

### ڈیٹا کولیئشن[[data-collation]]

ہمیں dynamic batching کے لیے padding سے نمٹنے کے لیے data collator کی ضرورت ہوگی۔ ہم [Chapter 3](/course/chapter3) کی طرح صرف inputs (input IDs، attention mask، اور token type IDs) کو pad کرنے والا `DataCollatorWithPadding` استعمال نہیں کر سکتے۔ ہمارے labels کو بھی ان کی زیادہ سے زیادہ لمبائی تک pad کیا جانا چاہیے۔ اور، جیسا کہ پہلے ذکر کیا گیا، labels کو pad کرنے کے لیے استعمال ہونے والی padding value tokenizer کے padding token کی بجائے `-100` ہونی چاہیے، تاکہ loss computation میں ان padded قدروں کو نظرانداز کیا جا سکے۔

یہ سب ایک [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) کے ذریعے کیا جاتا ہے۔ `DataCollatorWithPadding` کی طرح، یہ بھی ان پٹس کو preprocess کرنے کے لیے استعمال ہونے والے `tokenizer` کو لیتا ہے، لیکن یہ `model` کو بھی لیتا ہے۔ ایسا اس لیے ہے کہ یہ data collator decoder input IDs تیار کرنے کا بھی ذمہ دار ہوگا، جو labels کے shifted ورژنز ہوتے ہیں جن کے شروع میں ایک خاص token ہوتا ہے۔ چونکہ یہ shift مختلف architectures کے لیے تھوڑا مختلف ہوتا ہے، اس لیے `DataCollatorForSeq2Seq` کو `model` آبجیکٹ کا علم ہونا ضروری ہے:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

کچھ نمونوں پر اسے آزمانے کے لیے، ہم اسے اپنے tokenized تربیتی سیٹ کے چند examples کی ایک فہرست پر کال کرتے ہیں:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

ہم یہ جانچ سکتے ہیں کہ ہمارے labels کو بیچ کی زیادہ سے زیادہ لمبائی تک pad کیا گیا ہے، اور padding value `-100` استعمال کی گئی ہے:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

اور ہم decoder input IDs کو بھی دیکھ سکتے ہیں، تاکہ یہ معلوم ہو سکے کہ وہ labels کے shifted ورژن ہیں:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

یہاں ہمارے ڈیٹاسیٹ کے پہلے اور دوسرے عناصر کے labels ہیں:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

ہم اس `data_collator` کو `Seq2SeqTrainer` کو پاس کریں گے۔ اب، آئیں metric پر نظر ڈالتے ہیں۔

{:else}

ہم اب اس `data_collator` کو استعمال کرتے ہوئے اپنے ہر ڈیٹاسیٹ کو ایک `tf.data.Dataset` میں تبدیل کر سکتے ہیں، جو تربیت کے لیے تیار ہے:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### میٹرکس[[metrics]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

وہ خصوصیت جو `Seq2SeqTrainer` اپنی سپر کلاس `Trainer` میں شامل کرتا ہے وہ یہ ہے کہ evaluation یا prediction کے دوران `generate()` میتھڈ استعمال کی جا سکتی ہے۔ تربیت کے دوران، ماڈل `decoder_input_ids` استعمال کرتا ہے اور ایک attention mask لگا کر یہ یقینی بناتا ہے کہ وہ اس token کے بعد والے tokens کا استعمال نہ کرے جس کی وہ پیش گوئی کر رہا ہے، تاکہ تربیت تیز ہو سکے۔ inference کے دوران چونکہ ہمارے پاس labels نہیں ہوں گے، اس لیے بہتر ہے کہ ہم اپنے ماڈل کی evaluation اسی ترتیب کے ساتھ کریں۔

جیسا کہ ہم نے [Chapter 1](/course/chapter1/6) میں دیکھا، decoder inference اس طرح کرتا ہے کہ ایک ایک token کی پیش گوئی کرتا ہے — جو کہ 🤗 Transformers میں `generate()` میتھڈ کے ذریعے پس پردہ عمل میں لایا جاتا ہے۔ اگر ہم `predict_with_generate=True` سیٹ کریں تو `Seq2SeqTrainer` ہمیں evaluation کے لیے اس میتھڈ کا استعمال کرنے دے گا۔

{/if}

ترجمے کے لیے روایتی metric [BLEU score](https://en.wikipedia.org/wiki/BLEU) ہے، جسے Kishore Papineni وغیرہ نے [2002 کے ایک مضمون](https://aclanthology.org/P02-1040.pdf) میں متعارف کرایا تھا۔ BLEU score اس بات کا اندازہ لگاتا ہے کہ ترجمے labels کے کتنے قریب ہیں۔ یہ ماڈل کے تیار کردہ outputs کی سمجھ بوجھ یا گرامر کی درستگی کو نہیں تولتا، بلکہ شماریاتی اصولوں کا استعمال کرتا ہے تاکہ یہ یقینی بنایا جا سکے کہ generated outputs کے تمام الفاظ targets میں بھی شامل ہوں۔ مزید برآں، ایسے اصول بھی ہیں جو ایک ہی لفظ کے بار بار دہرانے پر سزا دیتے ہیں اگر وہ targets میں بھی دہرایا نہ گیا ہو (تاکہ ماڈل "the the the the the" جیسے جملے نہ بنائے) اور ایسے جملے جن کی لمبائی targets سے کم ہو (تاکہ ماڈل "the" جیسے مختصر جملے نہ بنائے)۔

BLEU کا ایک مسئلہ یہ ہے کہ یہ توقع کرتا ہے کہ متن پہلے سے tokenize کیا جا چکا ہو، جس سے مختلف tokenizers استعمال کرنے والے ماڈلز کے scores کا موازنہ کرنا مشکل ہو جاتا ہے۔ لہٰذا، آج کل ترجمہ ماڈلز کی benchmarking کے لیے سب سے زیادہ استعمال ہونے والا metric [SacreBLEU](https://github.com/mjpost/sacrebleu) ہے، جو اس خامی (اور دیگر) کو دور کرتا ہے اور tokenization کے مرحلے کو standardize کرتا ہے۔ اس metric کو استعمال کرنے کے لیے، ہمیں سب سے پہلے SacreBLEU لائبریری انسٹال کرنی ہوگی:

```py
!pip install sacrebleu
```

پھر ہم اسے [Chapter 3](/course/chapter3) کی طرح `evaluate.load()` کے ذریعے لوڈ کرتے ہیں:

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

یہ metric texts کو inputs اور targets کے طور پر لیتا ہے۔ اسے متعدد قابل قبول targets قبول کرنے کے لیے ڈیزائن کیا گیا ہے، کیونکہ ایک ہی جملے کے متعدد قابل قبول تراجم اکثر ملتے ہیں — ہمارا ڈیٹاسیٹ صرف ایک فراہم کرتا ہے، لیکن NLP میں ایسا غیر معمولی نہیں کہ ڈیٹاسیٹس متعدد جملے labels کے طور پر دیں۔ لہٰذا، predictions کو جملوں کی فہرست ہونی چاہیے، جبکہ references کو جملوں کی فہرست کی فہرست ہونی چاہیے۔

آئیں ایک مثال آزماتے ہیں:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

اس سے BLEU score 46.75 حاصل ہوتا ہے، جو کہ کافی اچھا ہے — حوالہ کے لیے، "Attention Is All You Need" پیپر میں پیش کردہ اصلی Transformer ماڈل نے انگریزی سے فرانسیسی ترجمے کے ایک مشابہ ٹاسک پر 41.8 کا BLEU score حاصل کیا تھا! (مزید معلومات کے لیے جیسے کہ `counts` اور `bp` کے بارے میں، [SacreBLEU repository](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74) دیکھیں.) دوسری طرف، اگر ہم ترجمہ ماڈلز کے عموماً پیدا ہونے والے دو خراب prediction types (زیادہ تکرار یا بہت مختصر) آزمائیں تو ہمیں کافی برے BLEU scores ملیں گے:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

اسکور 0 سے 100 تک جا سکتا ہے، اور جتنا زیادہ ہوگا، اتنا بہتر۔

{#if fw === 'tf'}

ماڈل کے outputs کو ایسے متون میں تبدیل کرنے کے لیے جنہیں metric استعمال کر سکے، ہم `tokenizer.batch_decode()` میتھڈ استعمال کریں گے۔ ہمیں صرف labels میں موجود تمام `-100` کو صاف کرنا ہے؛ tokenizer padding token کے لیے بھی خودکار طریقے سے یہی کرے گا۔ آئیے ایک ایسا فنکشن تعریف کرتے ہیں جو ہمارے ماڈل اور ڈیٹاسیٹ کو لے کر ان پر میٹرکس کا حساب لگائے۔ ہم ایک ایسا طریقہ بھی استعمال کرنے جا رہے ہیں جو کارکردگی میں نمایاں اضافہ کرتا ہے - یعنی [XLA](https://www.tensorflow.org/xla) کے ساتھ ہماری generation کوڈ کو compile کرنا، جو کہ TensorFlow کا accelerated linear algebra compiler ہے۔ XLA ماڈل کے computation graph پر مختلف optimizations لگاتا ہے، جس سے رفتار اور memory کے استعمال میں بہتری آتی ہے۔ جیسا کہ Hugging Face کے [blog](https://huggingface.co/blog/tf-xla-generate) میں بیان کیا گیا ہے، XLA تب بہترین کام کرتا ہے جب ہمارے input shapes بہت زیادہ مختلف نہ ہوں۔ اس مسئلے سے نمٹنے کے لیے، ہم اپنے inputs کو 128 کے multiples تک pad کریں گے، padding collator کے ساتھ ایک نیا ڈیٹاسیٹ بنائیں گے، اور پھر اپنی generation function پر `@tf.function(jit_compile=True)` decorator لگائیں گے، جس سے پورے فنکشن کو XLA کے ساتھ compile کرنے کے لیے نشان زد کیا جائے گا۔

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []

    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

ماڈل کے outputs کو ایسے متون میں تبدیل کرنے کے لیے جنہیں metric استعمال کر سکے، ہم `tokenizer.batch_decode()` میتھڈ استعمال کریں گے۔ ہمیں صرف labels میں موجود تمام `-100` کو صاف کرنا ہے (tokenizer padding token کے لیے بھی خود بخود یہی کرے گا):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # اگر ماڈل prediction logits سے زیادہ کچھ لوٹائے تو
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # -100 کو labels میں تبدیل کریں کیونکہ ہم انہیں decode نہیں کر سکتے
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # کچھ سادہ post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

اب جبکہ یہ سب مکمل ہو چکا ہے، ہم اپنے ماڈل کو fine-tune کرنے کے لیے تیار ہیں!

### ماڈل کی fine-tuning[[fine-tuning-the-model]]

ماڈل fine-tune کرنے کا پہلا قدم Hugging Face میں لاگ ان ہونا ہے، تاکہ آپ اپنے نتائج Model Hub پر اپ لوڈ کر سکیں۔ نوٹ بک میں ایک آسان فنکشن دستیاب ہے جو اس کام میں آپ کی مدد کرتا ہے:

```python
from huggingface_hub import notebook_login

notebook_login()
```

یہ ایک widget ظاہر کرے گا جہاں آپ اپنے Hugging Face لاگ ان کریڈینشلز داخل کر سکتے ہیں۔

اگر آپ نوٹ بک میں کام نہیں کر رہے، تو اپنے ٹرمینل میں صرف یہ لائن ٹائپ کریں:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

شروع کرنے سے پہلے، آئیے دیکھتے ہیں کہ بغیر کسی تربیت کے ہمارے ماڈل سے کس قسم کے نتائج حاصل ہوتے ہیں:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

جب یہ ہو جائے، تو ہم وہ سب کچھ تیار کر لیتے ہیں جو ماڈل کو compile اور train کرنے کے لیے درکار ہے۔ نوٹ کریں کہ یہاں `tf.keras.mixed_precision.set_global_policy("mixed_float16")` استعمال کیا گیا ہے — یہ Keras کو بتائے گا کہ وہ float16 کا استعمال کرتے ہوئے تربیت کرے، جو ایسے GPUs (Nvidia 20xx/V100 یا اس سے جدید) پر نمایاں رفتار میں اضافہ کر سکتا ہے۔

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# تربیتی مراحل کی تعداد ڈیٹاسیٹ میں موجود samples کی تعداد، batch size سے تقسیم کر کے، پھر epochs کی تعداد سے ضرب دی جاتی ہے۔
# نوٹ کریں کہ tf_train_dataset یہاں ایک batched tf.data.Dataset ہے، نہ کہ اصلی Hugging Face Dataset، لہٰذا اس کی len() پہلے ہی num_samples // batch_size کے برابر ہے۔
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# mixed-precision float16 میں تربیت کریں
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

اس کے بعد، ہم `PushToHubCallback` تعریف کرتے ہیں تاکہ تربیت کے دوران ہمارا ماڈل Hub پر اپ لوڈ ہو جائے، جیسا کہ ہم نے [section 2](/course/chapter7/2) میں دیکھا تھا، اور پھر اس callback کے ساتھ ماڈل کو fit کر دیتے ہیں:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

نوٹ کریں کہ آپ `hub_model_id` آرگیومنٹ کے ذریعے repository کا نام مخصوص کر سکتے ہیں (خاص طور پر، اگر آپ کسی organization میں push کر رہے ہیں تو آپ کو یہ آرگیومنٹ استعمال کرنا ہوگا)۔ مثال کے طور پر، جب ہم نے ماڈل کو [`huggingface-course` organization](https://huggingface.co/huggingface-course) پر push کیا، تو ہم نے `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` `Seq2SeqTrainingArguments` میں شامل کیا۔ بذریعہ ڈیفالٹ، استعمال شدہ repository آپ کے namespace میں ہوگی اور output directory کے نام پر مبنی ہوگی، لہٰذا یہاں یہ `"sgugger/marian-finetuned-kde4-en-to-fr"` ہوگی (جو کہ ماڈل ہم نے اس سیکشن کے شروع میں لنک کیا تھا)۔

<Tip>

💡 اگر آپ کی output directory پہلے سے موجود ہے، تو یہ لازمی ہے کہ وہ repository کا ایک مقامی clone ہو جس پر آپ push کرنا چاہتے ہیں۔ اگر ایسا نہیں ہے، تو `model.fit()` کال کرنے پر ایرر آئے گا اور آپ کو نیا نام سیٹ کرنا ہوگا۔

</Tip>

آخر میں، آئیے دیکھتے ہیں کہ تربیت مکمل ہونے کے بعد ہمارے metrics کیسے ہیں:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

اس مرحلے پر، آپ Model Hub پر موجود inference widget کا استعمال کرتے ہوئے اپنے ماڈل کا تجربہ کر سکتے ہیں اور اسے اپنے دوستوں کے ساتھ شیئر کر سکتے ہیں۔ آپ نے کامیابی کے ساتھ ایک ترجمہ ٹاسک پر ماڈل fine-tune کر لیا ہے — مبارک ہو!

{:else}

ایک بار جب یہ سب ہو جائے، تو ہم اپنے `Seq2SeqTrainingArguments` تعریف کر سکتے ہیں۔ `Trainer` کی طرح، ہم `TrainingArguments` کی ایک ذیلی کلاس استعمال کرتے ہیں جس میں چند اضافی فیلڈز شامل ہیں:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

عام hyperparameters (جیسے learning rate، epochs کی تعداد، batch size، اور کچھ weight decay) کے علاوہ، یہاں چند تبدیلیاں ہیں جو پچھلے سیکشنز کے مقابلے میں مختلف ہیں:

- ہم باقاعدہ evaluation سیٹ نہیں کرتے، کیونکہ evaluation میں کافی وقت لگتا ہے؛ ہم صرف تربیت سے پہلے اور بعد میں evaluation کریں گے۔
- ہم `fp16=True` سیٹ کرتے ہیں، جو جدید GPUs پر تربیت کی رفتار بڑھاتا ہے۔
- ہم `predict_with_generate=True` سیٹ کرتے ہیں، جیسا کہ اوپر بیان کیا گیا ہے۔
- ہم `push_to_hub=True` سیٹ کرتے ہیں تاکہ ہر epoch کے آخر میں ماڈل Hub پر اپ لوڈ ہو جائے۔

نوٹ کریں کہ آپ `hub_model_id` آرگیومنٹ کے ذریعے repository کا مکمل نام بھی مخصوص کر سکتے ہیں (خاص طور پر، اگر آپ کسی organization میں push کر رہے ہیں تو آپ کو یہ آرگیومنٹ استعمال کرنا ہوگا)۔ مثال کے طور پر، جب ہم نے ماڈل کو [`huggingface-course` organization](https://huggingface.co/huggingface-course) پر push کیا، تو ہم نے `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` `Seq2SeqTrainingArguments` میں شامل کیا۔ بذریعہ ڈیفالٹ، استعمال شدہ repository آپ کے namespace میں ہوگی اور output directory کے نام پر مبنی ہوگی، لہٰذا ہمارے معاملے میں یہ `"sgugger/marian-finetuned-kde4-en-to-fr"` ہوگی (جو کہ وہ ماڈل ہے جس کا ہم نے اس سیکشن کے شروع میں ذکر کیا تھا)۔

<Tip>

💡 اگر آپ کی output directory پہلے سے موجود ہے، تو یہ لازمی ہے کہ وہ repository کا ایک مقامی clone ہو جس پر آپ push کرنا چاہتے ہیں۔ اگر ایسا نہیں ہے، تو `Seq2SeqTrainer` تعریف کرتے وقت ایرر آئے گا اور آپ کو نیا نام سیٹ کرنا ہوگا۔

</Tip>

آخر میں، ہم سب کچھ `Seq2SeqTrainer` کو پاس کر دیتے ہیں:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

تربیت شروع کرنے سے پہلے، ہم پہلے یہ دیکھتے ہیں کہ ہمارے ماڈل کو کیا اسکور ملتا ہے، تاکہ یہ یقینی بنایا جا سکے کہ ہم fine-tuning کے دوران چیزوں کو خراب نہیں کر رہے۔ یہ کمانڈ کچھ وقت لے گی، لہٰذا آپ کافی کا ایک کپ لے سکتے ہیں جب یہ execute ہو رہی ہو:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

BLEU score 39 برا نہیں ہے، جو اس بات کی عکاسی کرتا ہے کہ ہمارا ماڈل پہلے ہی انگریزی جملوں کو فرانسیسی میں ترجمہ کرنے میں اچھا ہے۔

اب تربیت کا مرحلہ شروع ہوتا ہے، جس میں بھی کچھ وقت لگے گا:

```python
trainer.train()
```

تربیت کے دوران، ہر بار جب ماڈل save ہوتا ہے (یہاں، ہر epoch میں) یہ بیک گراؤنڈ میں Hub پر اپ لوڈ ہو جاتا ہے۔ اس طرح، اگر ضرورت ہو تو آپ کسی اور مشین پر اپنی تربیت دوبارہ شروع کر سکتے ہیں۔

تربیت مکمل ہونے کے بعد، ہم دوبارہ اپنے ماڈل کا evaluation کرتے ہیں — امید ہے کہ BLEU score میں بہتری نظر آئے گی!

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

یہ تقریباً 14 پوائنٹس کی بہتری ہے، جو کہ بہت اچھی بات ہے۔

آخر میں، ہم `push_to_hub()` میتھڈ کا استعمال کرتے ہیں تاکہ یہ یقینی بنایا جا سکے کہ ماڈل کا تازہ ترین ورژن اپ لوڈ ہو چکا ہے۔ `Trainer` ایک model card بھی تیار کرتا ہے جس میں تمام evaluation کے نتائج شامل ہوتے ہیں اور اسے اپ لوڈ کر دیتا ہے۔ یہ model card metadata رکھتا ہے جو Model Hub کو inference demo widget منتخب کرنے میں مدد دیتا ہے۔ عموماً، اس کے لیے کچھ کہنے کی ضرورت نہیں ہوتی کیونکہ یہ ماڈل کلاس سے صحیح widget کا اندازہ لگا لیتا ہے، مگر اس صورت میں چونکہ ایک ہی ماڈل کلاس مختلف قسم کے sequence-to-sequence مسائل کے لیے استعمال ہو سکتی ہے، ہم واضح کرتے ہیں کہ یہ ایک ترجمہ ماڈل ہے:

```python
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

یہ کمانڈ اس commit کا URL لوٹاتی ہے جو ابھی کیا گیا، اگر آپ اسے دیکھنا چاہیں:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

اس مرحلے پر، آپ Model Hub پر موجود inference widget کا استعمال کرتے ہوئے اپنے ماڈل کا تجربہ کر سکتے ہیں اور اسے اپنے دوستوں کے ساتھ شیئر کر سکتے ہیں۔ آپ نے کامیابی کے ساتھ ایک ترجمہ ٹاسک پر ماڈل fine-tune کر لیا ہے — مبارک ہو!

اگر آپ تربیتی loop میں مزید گہرائی میں جانا چاہتے ہیں، تو اب ہم آپ کو یہ بھی دکھائیں گے کہ کس طرح اسی کام کو 🤗 Accelerate کا استعمال کرتے ہوئے کیا جا سکتا ہے۔

{/if}

{#if fw === 'pt'}

## ایک کسٹم تربیتی loop[[a-custom-training-loop]]

آئیے اب مکمل تربیتی loop کو دیکھتے ہیں، تاکہ آپ آسانی سے اپنی ضرورت کے مطابق حصے customize کر سکیں۔ یہ تقریباً ویسا ہی نظر آئے گا جیسا کہ ہم نے [section 2](/course/chapter7/2) اور [Chapter 3](/course/chapter3/4) میں کیا تھا۔

### تربیت کے لیے ہر چیز کی تیاری[[preparing-everything-for-training]]

آپ نے یہ سب کچھ کئی بار دیکھ لیا ہے، لہٰذا ہم کوڈ کو تیزی سے دیکھیں گے۔ سب سے پہلے ہم اپنے ڈیٹاسیٹس سے `DataLoader`s بناتے ہیں، اور ڈیٹاسیٹس کو `"torch"` فارمیٹ میں سیٹ کرتے ہیں تاکہ ہمیں PyTorch tensors مل سکیں:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

پھر ہم اپنے ماڈل کو دوبارہ instantiate کرتے ہیں، تاکہ یہ یقینی ہو سکے کہ ہم پچھلی fine-tuning کی حالت سے نہیں بلکہ pretrained ماڈل سے شروع کر رہے ہیں:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

اس کے بعد ہمیں ایک optimizer کی ضرورت ہوگی:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ایک بار جب ہمارے پاس یہ تمام چیزیں موجود ہوں، تو ہم انہیں `accelerator.prepare()` میتھڈ کو بھیج دیتے ہیں۔ یاد رہے کہ اگر آپ Colab نوٹ بک میں TPUs پر تربیت کرنا چاہتے ہیں تو آپ کو یہ کوڈ ایک training function میں ڈالنا ہوگا، اور ایسا کوئی بھی سیل execute نہیں کرنا چاہیے جو `Accelerator` instantiate کرے۔

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

اب چونکہ ہم نے اپنے `train_dataloader` کو `accelerator.prepare()` کو بھیج دیا ہے، ہم اس کی لمبائی کا استعمال کرتے ہوئے تربیتی مراحل کی تعداد کا حساب لگا سکتے ہیں۔ یاد رہے کہ ہمیں یہ ہمیشہ dataloader کو prepare کرنے کے بعد کرنا چاہیے، کیونکہ یہ میتھڈ DataLoader کی لمبائی کو تبدیل کر دیتا ہے۔ ہم learning rate کو 0 تک linear schedule استعمال کرتے ہیں:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

آخر میں، اپنے ماڈل کو Hub پر push کرنے کے لیے، ہمیں ایک `Repository` آبجیکٹ بنانا ہوگا۔ اگر آپ پہلے سے لاگ ان نہیں ہیں تو Hugging Face Hub میں لاگ ان کریں۔ ہم ماڈل کے ID سے repository کا نام متعین کریں گے (آپ آزادانہ طور پر `repo_name` تبدیل کر سکتے ہیں؛ بس یہ آپ کے username کو شامل کرے، جیسا کہ `get_full_repo_name()` فنکشن کرتا ہے):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

پھر ہم اس repository کو ایک مقامی فولڈر میں clone کر لیتے ہیں۔ اگر یہ پہلے سے موجود ہے تو اس مقامی فولڈر کو وہ repository ہونا چاہیے جس پر آپ کام کر رہے ہیں:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

اب ہم `output_dir` میں محفوظ کی جانے والی کسی بھی چیز کو `repo.push_to_hub()` میتھڈ کال کر کے اپ لوڈ کر سکتے ہیں۔ یہ ہمیں ہر epoch کے آخر میں intermediate ماڈلز اپ لوڈ کرنے میں مدد کرے گا۔

### تربیتی loop[[training-loop]]

اب ہم مکمل تربیتی loop لکھنے کے لیے تیار ہیں۔ evaluation کو آسان بنانے کے لیے، ہم ایک `postprocess()` فنکشن تعریف کرتے ہیں جو predictions اور labels کو ایسے string کی فہرست میں تبدیل کرتا ہے جیسا کہ ہمارا `metric` آبجیکٹ توقع رکھتا ہے:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # -100 کو labels میں تبدیل کریں کیونکہ ہم انہیں decode نہیں کر سکتے۔
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # کچھ سادہ post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

تربیتی loop تقریباً [section 2](/course/chapter7/2) اور [Chapter 3](/course/chapter3) جیسے loops کی طرح ہے، مگر evaluation حصے میں چند اختلافات ہیں — تو آئیے اس پر توجہ دیں!

سب سے پہلے بات یہ ہے کہ ہم predictions حاصل کرنے کے لیے `generate()` میتھڈ استعمال کرتے ہیں، مگر یہ ہمارے base model پر موجود ہے، نہ کہ وہ wrapped model جسے 🤗 Accelerate نے `prepare()` میتھڈ میں بنایا ہے۔ اس لیے ہم پہلے model کو unwrap کرتے ہیں، پھر یہ میتھڈ کال کرتے ہیں۔

دوسری بات یہ ہے کہ، جیسے کہ [token classification](/course/chapter7/2) میں ہوا، دو processes نے inputs اور labels کو مختلف shapes میں pad کیا ہو سکتا ہے، اس لیے ہم `accelerator.pad_across_processes()` استعمال کرتے ہیں تاکہ predictions اور labels کو ایک ہی shape میں لایا جا سکے اس سے پہلے کہ ہم `gather()` میتھڈ کال کریں۔ اگر ہم ایسا نہ کریں تو evaluation یا تو error دے گا یا ہمیشہ کے لیے رکا رہے گا۔

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # تربیت
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # ضروری ہے کہ predictions اور labels کو gather کرنے سے پہلے pad کیا جائے
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # محفوظ کریں اور اپ لوڈ کریں
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

ایک بار جب یہ سب ہو جائے، تو آپ کے پاس ایک ایسا ماڈل ہوگا جس کے نتائج تقریباً اسی طرح کے ہوں گے جیسے ہم نے `Seq2SeqTrainer` کے ساتھ تربیت کیا تھا۔ آپ اس ماڈل کو [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate) پر چیک کر سکتے ہیں۔ اور اگر آپ تربیتی loop میں کوئی تبدیلیاں آزمانا چاہتے ہیں، تو آپ اوپر دکھائے گئے کوڈ میں براہ راست تبدیلی کر سکتے ہیں!

{/if}

## fine-tuned ماڈل کا استعمال[[using-the-fine-tuned-model]]

ہم پہلے ہی آپ کو دکھا چکے ہیں کہ آپ Model Hub پر موجود inference widget کا استعمال کرتے ہوئے ہمارے fine-tuned ماڈل کو کیسے استعمال کر سکتے ہیں۔ لوکل طور پر `pipeline` میں استعمال کرنے کے لیے، ہمیں صرف صحیح ماڈل identifier مخصوص کرنا ہوگا:

```py
from transformers import pipeline

# اسے اپنے checkpoint سے تبدیل کریں
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par défaut, développer les fils de discussion'}]
```

جیسا کہ توقع کی جاتی ہے، ہمارا pretrained ماڈل جس corpus پر fine-tune کیا گیا اس کی معلومات کو اپنانے لگا ہے، اور اب وہ انگریزی لفظ "threads" کو ویسے ہی چھوڑنے کی بجائے، فرانسیسی کے باقاعدہ انداز میں ترجمہ کرتا ہے۔ "plugin" کے معاملے میں بھی یہی صورتحال ہے:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

یہ domain adaptation کی ایک اور بہترین مثال ہے!

<Tip>

✏️ **آپ کی باری!** اس نمونے پر جس میں "email" لفظ استعمال ہوا تھا، ماڈل کیا واپس کرتا ہے؟

</Tip>