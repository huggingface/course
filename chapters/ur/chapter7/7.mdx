<FrameworkSwitchCourse {fw} />

# Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª[[question-answering]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_tf.ipynb"},
]} />

{/if}

Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„Ù†Û’ Ú©Ø§ ÙˆÙ‚Øª Ø¢Ú¯ÛŒØ§ ÛÛ’! ÛŒÛ Ú©Ø§Ù… Ú©Ø¦ÛŒ Ø´Ú©Ù„ÙˆÚº Ù…ÛŒÚº Ø¢ØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† Ø§Ø³ Ø­ØµÛ’ Ù…ÛŒÚº Ø¬Ø³ Ù¾Ø± ÛÙ… ØªÙˆØ¬Û Ù…Ø±Ú©ÙˆØ² Ú©Ø±ÛŒÚº Ú¯Û’ Ø§Ø³Û’ *extractive* Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§Ø³ Ù…ÛŒÚº Ú©Ø³ÛŒ Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ù¾ÙˆÚ†Ú¾Ù†Ø§ Ø§ÙˆØ± Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Û’ Ø§Ù†Ø¯Ø± _Ù…ØªÙ† Ú©Û’ Ù¹Ú©Ú‘ÙˆÚº_ Ú©ÛŒ ØµÙˆØ±Øª Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨Ø§Øª Ú©ÛŒ Ø´Ù†Ø§Ø®Øª Ú©Ø±Ù†Ø§ Ø´Ø§Ù…Ù„ ÛÛ’Û”

<Youtube id="ajPx5LwJD-I"/>

ÛÙ… [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) Ù¾Ø± Ø§ÛŒÚ© BERT Ù…Ø§ÚˆÙ„ Ú©Ùˆ fine-tune Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø¬Ùˆ Ú©Û ÙˆÚ©ÛŒÙ¾ÛŒÚˆÛŒØ§ Ù…Ø¶Ø§Ù…ÛŒÙ† Ú©Û’ Ø³ÛŒÙ¹ Ù¾Ø± crowdworkers Ú©ÛŒ Ø¬Ø§Ù†Ø¨ Ø³Û’ Ù¾ÙˆÚ†Ú¾Û’ Ú¯Ø¦Û’ Ø³ÙˆØ§Ù„Ø§Øª Ù¾Ø± Ù…Ø¨Ù†ÛŒ ÛÛ’Û” Ø§Ø³ Ø³Û’ ÛÙ…ÛŒÚº Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ Ù…Ø§ÚˆÙ„ Ù…Ù„Û’ Ú¯Ø§ Ø¬Ùˆ Ø§Ø³ Ù‚Ø³Ù… Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº Ú©Ø± Ø³Ú©Û’ Ú¯Ø§:

<iframe src="https://course-demos-bert-finetuned-squad.hf.space" frameBorder="0" height="450" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ÛŒÛ Ø¯Ø±Ø§ØµÙ„ Ø§Ø³ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø± Ø±ÛØ§ ÛÛ’ Ø¬Ø³ Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº Ø¯Ú©Ú¾Ø§Ø¦Û’ Ú¯Ø¦Û’ Ú©ÙˆÚˆ Ú©ÛŒ Ù…Ø¯Ø¯ Ø³Û’ Ú©ÛŒ Ú¯Ø¦ÛŒ Ø§ÙˆØ± Hub Ù¾Ø± Ø§Ù¾Ù„ÙˆÚˆ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø¢Ù¾ Ø§Ø³Û’ [ÛŒÛØ§Úº](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F) Ú†ÛŒÚ© Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

<Tip>

ğŸ’¡ BERT Ø¬ÛŒØ³Û’ encoder-only Ù…Ø§ÚˆÙ„Ø² Ø¹Ù…ÙˆÙ…Ø§Ù‹ Ø§ÛŒØ³Û’ factoid Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ù†Ú©Ø§Ù„Ù†Û’ Ù…ÛŒÚº Ø¨ÛØª Ø§Ú†Ú¾Û’ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’ "Transformer architecture Ú©Ø³ Ù†Û’ Ø§ÛŒØ¬Ø§Ø¯ Ú©ÛŒØŸ" Ù„ÛŒÚ©Ù† Ø¬Ø¨ Ø§Ù†ÛÛŒÚº Ú©Ú¾Ù„Û’ Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¯ÛŒÛ’ Ø¬Ø§Ø¦ÛŒÚº Ø¬ÛŒØ³Û’ "Ø¢Ø³Ù…Ø§Ù† Ù†ÛŒÙ„Ø§ Ú©ÛŒÙˆÚº ÛÛ’ØŸ" ØªÙˆ ÛŒÛ Ø§ØªÙ†Ø§ Ù…Ø¤Ø«Ø± Ù†ÛÛŒÚº ÛÙˆØªÛ’Û” Ø§Ù† Ø²ÛŒØ§Ø¯Û Ù…Ø´Ú©Ù„ ØµÙˆØ±ØªÙˆÚº Ù…ÛŒÚºØŒ encoder-decoder Ù…Ø§ÚˆÙ„Ø² Ø¬ÛŒØ³Û’ T5 Ø§ÙˆØ± BART Ø¹Ù…ÙˆÙ…Ø§Ù‹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ùˆ synthesis Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ú©Û [text summarization](/course/chapter7/5) Ø³Û’ Ú©Ø§ÙÛŒ Ù…Ù„ØªÛ’ Ø¬Ù„ØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ø± Ø¢Ù¾ generative Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ù…ÛŒÚº Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºØŒ ØªÙˆ ÛÙ… Ø¢Ù¾ Ú©Ùˆ ÛÙ…Ø§Ø±ÛŒ [demo](https://yjernite.github.io/lfqa.html) Ø¯ÛŒÚ©Ú¾Ù†Û’ Ú©ÛŒ ØªØ¬ÙˆÛŒØ² Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ùˆ [ELI5 dataset](https://huggingface.co/datasets/eli5) Ù¾Ø± Ù…Ø¨Ù†ÛŒ ÛÛ’Û”

</Tip>

## ÚˆÛŒÙ¹Ø§ Ú©ÛŒ ØªÛŒØ§Ø±ÛŒ[[preparing-the-data]]

ÙˆÛ dataset Ø¬Ùˆ extractive Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© ØªØ¹Ù„ÛŒÙ…ÛŒ Ù…Ø¹ÛŒØ§Ø± Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’ ÙˆÛ [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛŒÛØ§Úº ÛÙ… Ø§Ø³ÛŒ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø§Ø³ Ú©Û’ Ø¹Ù„Ø§ÙˆÛ Ø§ÛŒÚ© Ø²ÛŒØ§Ø¯Û Ù…Ø´Ú©Ù„ [SQuAD v2](https://huggingface.co/datasets/squad_v2) benchmark Ø¨Ú¾ÛŒ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒØ³Û’ Ø³ÙˆØ§Ù„Ø§Øª Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ù† Ú©Û’ Ø¬ÙˆØ§Ø¨Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº ÛÙˆØªÛ’Û” Ø¬Ø¨ ØªÚ© Ú©Û Ø¢Ù¾ Ú©Û’ Ø§Ù¾Ù†Û’ dataset Ù…ÛŒÚº contextsØŒ questionsØŒ Ø§ÙˆØ± answers Ú©Û’ Ù„ÛŒÛ’ Ú©Ø§Ù„Ù… Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºØŒ Ø¢Ù¾ Ù†ÛŒÚ†Û’ Ø¯ÛŒÛ’ Ú¯Ø¦Û’ Ù…Ø±Ø§Ø­Ù„ Ú©Ùˆ Ø¢Ø³Ø§Ù†ÛŒ Ø³Û’ Ø§Ù¾Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

### SQuAD dataset[[the-squad-dataset]]

ÛÙ…ÛŒØ´Û Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ… `load_dataset()` Ú©ÛŒ Ø¨Ø¯ÙˆÙ„Øª ØµØ±Ù Ø§ÛŒÚ© Ù‚Ø¯Ù… Ù…ÛŒÚº dataset Ú©Ùˆ ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ø§ÙˆØ± cache Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
from datasets import load_dataset

raw_datasets = load_dataset("squad")
```

Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ØŒ ÛÙ… Ø§Ø³ Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ú©Ø± SQuAD dataset Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ù…Ø²ÛŒØ¯ Ø¬Ø§Ù† Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})
```

Ø§ÛŒØ³Ø§ Ù„Ú¯ØªØ§ ÛÛ’ Ú©Û ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ `context`, `question`, Ø§ÙˆØ± `answers` Ú©Û’ ÙÛŒÙ„ÚˆØ² Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ø¦ÛŒÛ’ Ø§Ù¾Ù†Û’ training set Ú©Û’ Ù¾ÛÙ„Û’ Ø¹Ù†ØµØ± Ú©Û’ Ù„ÛŒÛ’ Ø§Ù†ÛÛŒÚº Ù¾Ø±Ù†Ù¹ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
print("Context: ", raw_datasets["train"][0]["context"])
print("Question: ", raw_datasets["train"][0]["question"])
print("Answer: ", raw_datasets["train"][0]["answers"])
```

```python out
Context: 'Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'
Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'
Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
```

`context` Ø§ÙˆØ± `question` ÙÛŒÙ„ÚˆØ² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ø¨ÛØª Ø³ÛŒØ¯Ú¾Û’ Ø³Ø§Ø¯Û’ ÛÛŒÚºÛ” `answers` ÙÛŒÙ„Úˆ ØªÚ¾ÙˆÚ‘Ø§ Ù¾ÛŒÚ†ÛŒØ¯Û ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ Ø§ÛŒÚ© dictionary ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº Ø¯Ùˆ ÙÛŒÙ„ÚˆØ² Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ùˆ Ø¯ÙˆÙ†ÙˆÚº lists Ú©ÛŒ ØµÙˆØ±Øª Ù…ÛŒÚº ÛÛŒÚºÛ” ÛŒÛ ÙˆÛ ÙØ§Ø±Ù…ÛŒÙ¹ ÛÛ’ Ø¬Ø³ Ú©ÛŒ ØªÙˆÙ‚Ø¹ evaluation Ú©Û’ Ø¯ÙˆØ±Ø§Ù† `squad` metric Ú©Ø±Û’ Ú¯Ø§Ø› Ø§Ú¯Ø± Ø¢Ù¾ Ø§Ù¾Ù†Ø§ ÚˆÛŒÙ¹Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº ØªÙˆ Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛÛŒÚº Ú©Û Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Ùˆ Ø¨Ø§Ù„Ú©Ù„ Ø§Ø³ÛŒ ÙØ§Ø±Ù…ÛŒÙ¹ Ù…ÛŒÚº Ø±Ú©Ú¾ÛŒÚºÛ” `text` ÙÛŒÙ„Úˆ ÙˆØ§Ø¶Ø­ ÛÛ’ØŒ Ø§ÙˆØ± `answer_start` ÙÛŒÙ„Úˆ ÛØ± Ø¬ÙˆØ§Ø¨ Ú©Û’ context Ù…ÛŒÚº Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ú©Ø±Ø¯Ø§Ø± (character index) Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø±ØªÛŒ ÛÛ’Û”

ØªØ±Ø¨ÛŒØª Ú©Û’ Ø¯ÙˆØ±Ø§Ù†ØŒ ØµØ±Ù Ø§ÛŒÚ© Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨ ÛÙˆØªØ§ ÛÛ’Û” ÛÙ… `Dataset.filter()` Ù…ÛŒØªÚ¾Úˆ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ø³ Ø¨Ø§Øª Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
raw_datasets["train"].filter(lambda x: len(x["answers"]["text"]) != 1)
```

```python out
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
```

Ù„ÛŒÚ©Ù† evaluation Ú©Û’ Ø¯ÙˆØ±Ø§Ù†ØŒ ÛØ± sample Ú©Û’ Ù„ÛŒÛ’ Ù…ØªØ¹Ø¯Ø¯ Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ø§ÛŒÚ© Ø¬ÛŒØ³Û’ ÛŒØ§ Ù…Ø®ØªÙ„Ù ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
print(raw_datasets["validation"][0]["answers"])
print(raw_datasets["validation"][2]["answers"])
```

```python out
{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}
{'text': ['Santa Clara, California', "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], 'answer_start': [403, 355, 355]}
```

ÛÙ… evaluation script Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û ØªÙØµÛŒÙ„ Ù…ÛŒÚº Ù†ÛÛŒÚº Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’ Ú©ÛŒÙˆÙ†Ú©Û Ø§Ø³Û’ ÛÙ…Ø§Ø±Û’ Ù„ÛŒÛ’ ğŸ¤— Datasets metric Ù…ÛŒÚº Ù„Ù¾ÛŒÙ¹ Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† Ù…Ø®ØªØµØ± ÛŒÛ Ú©Û Ú©Ú†Ú¾ Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ú©Ø¦ÛŒ Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª ÛÙˆØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± ÛŒÛ script Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ø¬ÙˆØ§Ø¨ Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û ØªÙ…Ø§Ù… Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¬ÙˆØ§Ø¨Ø§Øª Ø³Û’ Ú©Ø±Û’ Ú¯Ø§ Ø§ÙˆØ± Ø¨ÛØªØ±ÛŒÙ† Ø§Ø³Ú©ÙˆØ± Ù„Û’ Ú¯Ø§Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± ÛÙ… index 2 Ú©Û’ sample Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ÛŒÚº:

```py
print(raw_datasets["validation"][2]["context"])
print(raw_datasets["validation"][2]["question"])
```

```python out
'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.'
'Where did Super Bowl 50 take place?'
```

ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ø¬ÙˆØ§Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ù…ÛŒÚº Ø§Ù† ØªÛŒÙ† Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ù…ÛŒÚº Ø³Û’ Ø§ÛŒÚ© ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ Ø¬Ùˆ ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ Ø¯ÛŒÚ©Ú¾Û’ ØªÚ¾Û’Û”

### ØªØ±Ø¨ÛŒØªÛŒ ÚˆÛŒÙ¹Ø§ Ú©ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯[[processing-the-training-data]]

<Youtube id="qgaM0weJHpA"/>

Ø¢Ø¦ÛŒÛ’ ØªØ±Ø¨ÛŒØªÛŒ ÚˆÛŒÙ¹Ø§ Ú©ÛŒ preprocessing Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø³Ø¨ Ø³Û’ Ù…Ø´Ú©Ù„ Ø­ØµÛ ÛŒÛ ÛÙˆÚ¯Ø§ Ú©Û Ø³ÙˆØ§Ù„ Ú©Û’ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ù„ÛŒÛ’ labels Ù¾ÛŒØ¯Ø§ Ú©ÛŒÛ’ Ø¬Ø§Ø¦ÛŒÚºØŒ Ø¬Ùˆ context Ú©Û’ Ø§Ù†Ø¯Ø± Ø¬ÙˆØ§Ø¨ Ú©Û’ tokens Ú©Û’ Ø´Ø±ÙˆØ¹ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… Ú©ÛŒ Ù¾ÙˆØ²ÛŒØ´Ù†Ø² ÛÙˆÚº Ú¯ÛŒÛ”

Ù„ÛŒÚ©Ù† Ø®ÙˆØ¯ Ú©Ùˆ Ø¬Ù„Ø¯ Ø¨Ø§Ø²ÛŒ Ù…ÛŒÚº Ù…Øª ÚˆØ§Ù„ÛŒÚºÛ” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛÙ…ÛŒÚº input Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù…ØªÙ† Ú©Ùˆ IDs Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’ ØªØ§Ú©Û Ù…Ø§ÚˆÙ„ Ø§Ø³Û’ Ø³Ù…Ø¬Ú¾ Ø³Ú©Û’ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… tokenizer Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```py
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Ø¬ÛŒØ³Ø§ Ú©Û Ù¾ÛÙ„Û’ Ø°Ú©Ø± Ú©ÛŒØ§ Ú¯ÛŒØ§ ØªÚ¾Ø§ØŒ ÛÙ… Ø§ÛŒÚ© BERT Ù…Ø§ÚˆÙ„ Ú©Ùˆ ÙØ§Ø¦Ù† Ù¹ÙˆÙ† Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ù„ÛŒÚ©Ù† Ø¢Ù¾ Ú©ÙˆØ¦ÛŒ Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ø¨Ú¾ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ø¨ ØªÚ© Ú©Û Ø§Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© ØªÛŒØ² Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù†Ø§ÙØ° ÛÙˆÛ” Ø¢Ù¾ [Ø§Ø³ Ø¨Ú‘Û’ Ø¬Ø¯ÙˆÙ„](https://huggingface.co/transformers/#supported-frameworks) Ù…ÛŒÚº ÙˆÛ ØªÙ…Ø§Ù… Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±Ø² Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ù† Ú©Û’ ØªÛŒØ² ÙˆØ±Ú˜Ù† Ø¯Ø³ØªÛŒØ§Ø¨ ÛÛŒÚºØŒ Ø§ÙˆØ± ÛŒÛ Ø¬Ø§Ù†Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Û Ø¢ÛŒØ§ Ø¢Ù¾ Ø¬Ùˆ `tokenizer` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº ÙˆÛ ÙˆØ§Ù‚Ø¹ÛŒ ğŸ¤— Tokenizers Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù…Ø¹Ø§ÙˆÙ†Øª ÛŒØ§ÙØªÛ ÛÛ’ØŒ Ø¢Ù¾ Ø§Ø³ Ú©ÛŒ `is_fast` Ø®ØµÙˆØµÛŒØª Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

```py
tokenizer.is_fast
```

```python out
True
```

ÛÙ… Ø§Ù¾Ù†Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ø¯Û’ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± ÛŒÛ Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ Ø®Ø§Øµ Ù¹ÙˆÚ©Ù†Ø² Ø¯Ø§Ø®Ù„ Ú©Ø±Û’ Ú¯Ø§ ØªØ§Ú©Û Ø§ÛŒÚ© Ø¬Ù…Ù„Û Ø§Ø³ Ø·Ø±Ø­ ØªØ´Ú©ÛŒÙ„ Ø¯ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’:

```
[CLS] question [SEP] context [SEP]
```

Ø¢Ø¦ÛŒÛ’ Ø¯ÙˆØ¨Ø§Ø±Û ØªØµØ¯ÛŒÙ‚ Ú©Ø±ÛŒÚº:

```py
context = raw_datasets["train"][0]["context"]
question = raw_datasets["train"][0]["question"]

inputs = tokenizer(question, context)
tokenizer.decode(inputs["input_ids"])
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '
'the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin '
'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '
'upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred '
'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '
'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '
'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '
'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

Ù„ÛŒØ¨Ù„Ø² Ù¾Ú¾Ø± Ø§Ø³ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø¢ØºØ§Ø² Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… ÙˆØ§Ù„Û’ tokens Ú©Û’ Ø§Ù†ÚˆÛŒÚ©Ø³ ÛÙˆÚº Ú¯Û’ØŒ Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ù¾Ø± ÛŒÛ Ø°Ù…Û Ø¯Ø§Ø±ÛŒ Ø¹Ø§Ø¦Ø¯ ÛÙˆÚ¯ÛŒ Ú©Û ÙˆÛ input Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± token Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø´Ø±ÙˆØ¹ Ø§ÙˆØ± Ø§ÛŒÚ© Ø§Ø®ØªØªØ§Ù…ÛŒ logit Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±Û’ØŒ Ø¬Ø¨Ú©Û Ù†Ø¸Ø±ÛŒØ§ØªÛŒ Ù„ÛŒØ¨Ù„Ø² Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÙˆÚº Ú¯Û’:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels-dark.svg" alt="One-hot encoded labels for question answering."/>
</div>

Ø§Ø³ ØµÙˆØ±ØªÙ Ø­Ø§Ù„ Ù…ÛŒÚº context Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§ Ù†ÛÛŒÚº ÛÛ’ØŒ Ù…Ú¯Ø± dataset Ú©ÛŒ Ú©Ú†Ú¾ Ù…Ø«Ø§Ù„ÙˆÚº Ù…ÛŒÚº Ø§ÛŒØ³Û’ Ø¨ÛØª Ø·ÙˆÛŒÙ„ context Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚº Ø¬Ùˆ ÛÙ… Ù†Û’ Ø¬Ùˆ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ù…Ù‚Ø±Ø± Ú©ÛŒ ÛÛ’ (Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº 384) Ø§Ø³Û’ ØªØ¬Ø§ÙˆØ² Ú©Ø± Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’Û” Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ [Chapter 6](/course/chapter6/4) Ù…ÛŒÚº `question-answering` pipeline Ú©Û’ Ø§Ù†Ø¯Ø±ÙˆÙ†ÛŒ Ø­ØµÙˆÚº Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒØ§ØŒ ÛÙ… Ø·ÙˆÛŒÙ„ context Ú©Û’ Ø³Ø§ØªÚ¾ Ù†Ù…Ù¹Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© ÛÛŒ sample Ø³Û’ Ù…ØªØ¹Ø¯Ø¯ training features ØªØ®Ù„ÛŒÙ‚ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø¬Ù† Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø§ÛŒÚ© sliding window ÛÙˆÚ¯Ø§Û”

Ù…ÙˆØ¬ÙˆØ¯Û Ù…Ø«Ø§Ù„ Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÙˆØ¦Û’ØŒ ÛÙ… Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Ùˆ 100 ØªÚ© Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± 50 tokens Ú©Ø§ sliding window Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” ÛŒØ§Ø¯Ø¯ÛØ§Ù†ÛŒ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛÙ… Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

- `max_length` ØªØ§Ú©Û Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ù…Ù‚Ø±Ø± Ú©ÛŒ Ø¬Ø§ Ø³Ú©Û’ (ÛŒÛØ§Úº 100)
- `truncation="only_second"` ØªØ§Ú©Û Ø¬Ø¨ Ø³ÙˆØ§Ù„ Ø§Ù¾Ù†Û’ context Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛØª Ù„Ù…Ø¨Ø§ ÛÙˆ Ø¬Ø§Ø¦Û’ ØªÙˆ context (Ø¬Ùˆ Ø¯ÙˆØ³Ø±Û’ Ù…Ù‚Ø§Ù… Ù¾Ø± ÛÛ’) Ú©Ùˆ truncate Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’
- `stride` ØªØ§Ú©Û Ø¯Ùˆ Ù…Ø³Ù„Ø³Ù„ Ø­ØµÙˆÚº Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† overlapping tokens Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø±Ø± Ú©ÛŒ Ø¬Ø§ Ø³Ú©Û’ (ÛŒÛØ§Úº 50)
- `return_overflowing_tokens=True` ØªØ§Ú©Û tokenizer Ú©Ùˆ Ø¨ØªØ§ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’ Ú©Û ÛÙ…ÛŒÚº overflow ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ tokens Ø¯Ø±Ú©Ø§Ø± ÛÛŒÚº

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ ÛÙ…Ø§Ø±ÛŒ Ù…Ø«Ø§Ù„ Ú©Ùˆ Ú†Ø§Ø± inputs Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø¬Ù† Ù…ÛŒÚº Ø³Û’ ÛØ± Ø§ÛŒÚ© Ù…ÛŒÚº Ø³ÙˆØ§Ù„ Ø§ÙˆØ± context Ú©Ø§ Ú©Ú†Ú¾ Ø­ØµÛ Ø´Ø§Ù…Ù„ ÛÛ’Û” Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ ("Bernadette Soubirous") ØµØ±Ù ØªÛŒØ³Ø±Û’ Ø§ÙˆØ± Ø¢Ø®Ø±ÛŒ inputs Ù…ÛŒÚº Ø¸Ø§ÛØ± ÛÙˆØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ Ø·ÙˆÛŒÙ„ context Ú©Û’ Ø§Ø³ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ù†Ù…Ù¹Ù†Û’ Ú©Û’ Ø¨Ø§Ø¹Ø« ÛÙ… Ú©Ú†Ú¾ Ø§ÛŒØ³Û’ training examples ØªØ®Ù„ÛŒÙ‚ Ú©Ø±ÛŒÚº Ú¯Û’ Ø¬Ù† Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ context Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ù†ÛÛŒÚº ÛÙˆÚ¯Ø§Û” Ø§Ù† Ù…Ø«Ø§Ù„ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ØŒ Ù„ÛŒØ¨Ù„Ø² `start_position = end_position = 0` ÛÙˆÚº Ú¯Û’ (ÛŒØ¹Ù†ÛŒ ÛÙ… `[CLS]` token Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±ÛŒÚº Ú¯Û’)Û” ÛÙ… Ø§Ù† Ø¨Ø¯Ù‚Ø³Ù…Øª Ù…Ø«Ø§Ù„ÙˆÚº Ù…ÛŒÚº Ø¨Ú¾ÛŒ ÛŒÛ Ù„ÛŒØ¨Ù„Ø² Ø³ÛŒÙ¹ Ú©Ø± Ø¯ÛŒÚº Ú¯Û’ Ø¬ÛØ§Úº Ø¬ÙˆØ§Ø¨ truncate ÛÙˆ Ú¯ÛŒØ§ ÛÙˆ Ø§ÙˆØ± ØµØ±Ù Ø§Ø³ Ú©Ø§ Ø¢ØºØ§Ø² (ÛŒØ§ Ø§Ø®ØªØªØ§Ù…) Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÛ” Ø§Ù† Ù…Ø«Ø§Ù„ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¬ÛØ§Úº Ø¬ÙˆØ§Ø¨ Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± context Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆØŒ Ù„ÛŒØ¨Ù„Ø² ÙˆÛ token index ÛÙˆÚº Ú¯Û’ Ø¬ÛØ§Úº Ø¬ÙˆØ§Ø¨ Ø´Ø±ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’ Ø§ÙˆØ± ÙˆÛ index Ø¬ÛØ§Úº Ø¬ÙˆØ§Ø¨ Ø®ØªÙ… ÛÙˆØªØ§ ÛÛ’Û”

dataset ÛÙ…ÛŒÚº context Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø¢ØºØ§Ø² Ú©Ø§ character index ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø¬ÙˆØ§Ø¨ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø´Ø§Ù…Ù„ Ú©Ø± Ú©Û’ ÛÙ… context Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø§Ø®ØªØªØ§Ù… Ú©Ø§ character Ù…Ø¹Ù„ÙˆÙ… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ù† Ú©Ùˆ token indices Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº offset mappings Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ ÛÙˆÚº Ú¯Û’ Ø¬Ù† Ú©Ø§ ÛÙ… Ù†Û’ [Chapter 6](/course/chapter6/4) Ù…ÛŒÚº Ù…Ø·Ø§Ù„Ø¹Û Ú©ÛŒØ§ ØªÚ¾Ø§Û” ÛÙ… Ø§Ù¾Ù†Û’ tokenizer Ú©Ùˆ ÛŒÛ ÙˆØ§Ù¾Ø³ÛŒ offset mappings Ø¯ÛŒÙ†Û’ Ú©Û’ Ù„ÛŒÛ’ `return_offsets_mapping=True` Ù¾Ø§Ø³ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
inputs.keys()
```

```python out
dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])
```

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºØŒ ÛÙ…ÛŒÚº Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ input IDsØŒ token type IDsØŒ Ø§ÙˆØ± attention mask Ú©Û’ Ø³Ø§ØªÚ¾ Ø³Ø§ØªÚ¾ Ù…Ø·Ù„ÙˆØ¨Û offset mapping Ø§ÙˆØ± Ø§ÛŒÚ© Ø§Ø¶Ø§ÙÛŒ key `overflow_to_sample_mapping` Ø¨Ú¾ÛŒ Ù…Ù„ Ú¯Ø¦ÛŒ ÛÛ’Û” Ù…ØªØ¹Ù„Ù‚Û value Ø§Ø³ ÙˆÙ‚Øª ÛÙ…Ø§Ø±Û’ Ú©Ø§Ù… Ø¢Ø¦Û’ Ú¯ÛŒ Ø¬Ø¨ ÛÙ… Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ú©Ø¦ÛŒ Ù…ØªÙ† tokenize Ú©Ø±ÛŒÚº Ú¯Û’ (Ø¬Ùˆ ÛÙ…ÛŒÚº Ø§Ø³ Ø¨Ø§Øª Ø³Û’ ÙØ§Ø¦Ø¯Û Ù¾ÛÙ†Ú†ØªØ§ ÛÛ’ Ú©Û ÛÙ…Ø§Ø±Ø§ tokenizer Rust Ø³Û’ Ú†Ù„ Ø±ÛØ§ ÛÛ’)Û” Ú†ÙˆÙ†Ú©Û Ø§ÛŒÚ© sample Ø³Û’ Ú©Ø¦ÛŒ features Ø¢ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ ÛŒÛ ÛØ± feature Ú©Ùˆ Ø§ÙØ³ Ù…Ø«Ø§Ù„ Ø³Û’ Ù…ÛŒÙ¾ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ø³ Ø³Û’ ÙˆÛ ÙˆØ¬ÙˆØ¯ Ù…ÛŒÚº Ø¢ÛŒØ§ ÛÛ’Û” Ú†ÙˆÙ†Ú©Û ÛŒÛØ§Úº ÛÙ… Ù†Û’ ØµØ±Ù Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ tokenize Ú©ÛŒ ÛÛ’ØŒ ÛÙ…ÛŒÚº `0` Ú©ÛŒ Ø§ÛŒÚ© list Ù…Ù„ØªÛŒ ÛÛ’:

```py
inputs["overflow_to_sample_mapping"]
```

```python out
[0, 0, 0, 0]
```

Ù„ÛŒÚ©Ù† Ø§Ú¯Ø± ÛÙ… Ù…Ø²ÛŒØ¯ Ù…Ø«Ø§Ù„ÛŒÚº tokenize Ú©Ø±ÛŒÚºØŒ ØªÙˆ ÛŒÛ Ø²ÛŒØ§Ø¯Û Ù…ÙÛŒØ¯ ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§:

```py
inputs = tokenizer(
    raw_datasets["train"][2:6]["question"],
    raw_datasets["train"][2:6]["context"],
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)

print(f"The 4 examples gave {len(inputs['input_ids'])} features.")
print(f"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.")
```

```python out
'The 4 examples gave 19 features.'
'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'
```

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ training set Ù…ÛŒÚº indices 2ØŒ 3ØŒ Ø§ÙˆØ± 4 Ù¾Ø± Ù…ÙˆØ¬ÙˆØ¯ Ù¾ÛÙ„ÛŒ ØªÛŒÙ† Ù…Ø«Ø§Ù„ÛŒÚº ÛØ± Ø§ÛŒÚ© Ú†Ø§Ø± features Ø¯ÛŒØªÛŒ ÛÛŒÚº Ø§ÙˆØ± Ø¢Ø®Ø±ÛŒ Ù…Ø«Ø§Ù„ (index 5 Ù¾Ø±) 7 features Ø¯ÛŒØªÛŒ ÛÛ’Û”

ÛŒÛ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÛØ± feature Ú©Ùˆ Ø§Ø³ Ú©Û’ Ù…ØªØ¹Ù„Ù‚Û label Ø³Û’ Ù…ÛŒÙ¾ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…ÙÛŒØ¯ Ø«Ø§Ø¨Øª ÛÙˆÚ¯ÛŒÛ” Ø¬ÛŒØ³Ø§ Ú©Û Ù¾ÛÙ„Û’ Ø¨ÛŒØ§Ù† Ú©ÛŒØ§ Ú¯ÛŒØ§ØŒ ÛŒÛ labels Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÛŒÚº:

- `(0, 0)` Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ Ù…ØªØ¹Ù„Ù‚Û context Ú©Û’ span Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†Û ÛÙˆ
- `(start_position, end_position)` Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ Ù…ØªØ¹Ù„Ù‚Û context Ú©Û’ span Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆØŒ Ø¬ÛØ§Úº `start_position` token index (input IDs Ù…ÛŒÚº) ÛÛ’ Ø¬ÛØ§Úº Ø¬ÙˆØ§Ø¨ Ø´Ø±ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’ Ø§ÙˆØ± `end_position` token index (input IDs Ù…ÛŒÚº) ÛÛ’ Ø¬ÛØ§Úº Ø¬ÙˆØ§Ø¨ Ø®ØªÙ… ÛÙˆØªØ§ ÛÛ’

ÛŒÛ Ø·Û’ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Û Ø§Ù† Ù…ÛŒÚº Ø³Û’ Ú©ÙˆÙ† Ø³Ø§ Ú©ÛŒØ³ ÛÛ’ Ø§ÙˆØ±ØŒ Ø§Ú¯Ø± Ø¶Ø±ÙˆØ±ÛŒ ÛÙˆØŒ tokens Ú©ÛŒ Ù¾ÙˆØ²ÛŒØ´Ù†Ø² Ù…Ø¹Ù„ÙˆÙ… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ… input IDs Ù…ÛŒÚº context Ú©Û’ Ø¢ØºØ§Ø² Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… ÙˆØ§Ù„Û’ indices ØªÙ„Ø§Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” ÛÙ… ÛŒÛ Ú©Ø§Ù… token type IDs Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ú©Ø± Ø³Ú©ØªÛ’ ØªÚ¾Û’ØŒ Ù„ÛŒÚ©Ù† Ú†ÙˆÙ†Ú©Û ÛŒÛ ÛØ± Ù…Ø§ÚˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛÛŒÚº ÛÙˆØªÛ’ (Ù…Ø«Ù„Ø§Ù‹ DistilBERT Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±Ú©Ø§Ø± Ù†ÛÛŒÚº)ØŒ ÛÙ… Ø§Ø³ Ú©ÛŒ Ø¨Ø¬Ø§Ø¦Û’ ÛÙ…Ø§Ø±Û’ tokenizer Ú©Û’ Ø°Ø±ÛŒØ¹Û ÙˆØ§Ù¾Ø³ÛŒ ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ `BatchEncoding` Ú©Û’ `sequence_ids()` Ù…ÛŒØªÚ¾Úˆ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”

Ø§ÛŒÚ© Ø¨Ø§Ø± Ø¬Ø¨ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ ÛŒÛ token indices ÛÙˆ Ø¬Ø§Ø¦ÛŒÚºØŒ ÛÙ… Ù…ØªØ¹Ù„Ù‚Û offsets Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ø¯Ùˆ integers Ú©ÛŒ tuples ÛÙˆØªÛŒ ÛÛŒÚº Ø¬Ùˆ Ø§ØµÙ„ context Ú©Û’ Ø§Ù†Ø¯Ø± Ú©Ø±Ø¯Ø§Ø± (characters) Ú©Û’ span Ú©ÛŒ Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Ø±ØªÛŒ ÛÛŒÚºÛ” Ø§Ø³ Ø·Ø±Ø­ ÛÙ… ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ø§Ø³ feature Ù…ÛŒÚº context Ú©Ø§ ÙˆÛ Ø­ØµÛ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø¨Ø¹Ø¯ Ø´Ø±ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’ ÛŒØ§ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø´Ø±ÙˆØ¹ ÛÙˆÙ†Û’ Ø³Û’ Ù¾ÛÙ„Û’ Ø®ØªÙ… ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’ (Ø¬Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº label `(0, 0)` ÛÙˆÚ¯Ø§)Û” Ø§Ú¯Ø± Ø§ÛŒØ³Ø§ Ù†Û ÛÙˆØŒ ØªÙˆ ÛÙ… loop Ú©Ø± Ú©Û’ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ù¾ÛÙ„Û’ Ø§ÙˆØ± Ø¢Ø®Ø±ÛŒ token Ú©Ùˆ ØªÙ„Ø§Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
answers = raw_datasets["train"][2:6]["answers"]
start_positions = []
end_positions = []

for i, offset in enumerate(inputs["offset_mapping"]):
    sample_idx = inputs["overflow_to_sample_mapping"][i]
    answer = answers[sample_idx]
    start_char = answer["answer_start"][0]
    end_char = answer["answer_start"][0] + len(answer["text"][0])
    sequence_ids = inputs.sequence_ids(i)

    # Find the start and end of the context
    idx = 0
    while sequence_ids[idx] != 1:
        idx += 1
    context_start = idx
    while sequence_ids[idx] == 1:
        idx += 1
    context_end = idx - 1

    # If the answer is not fully inside the context, label is (0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # Otherwise it's the start and end token positions
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python out
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

Ø¢Ø¦ÛŒÛ’ Ú©Ú†Ú¾ Ù†ØªØ§Ø¦Ø¬ Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„ÛŒÚº ØªØ§Ú©Û ØªØµØ¯ÛŒÙ‚ Ú©Ø± Ø³Ú©ÛŒÚº Ú©Û ÛÙ…Ø§Ø±Ø§ Ø·Ø±ÛŒÙ‚Û Ø¯Ø±Ø³Øª ÛÛ’Û” Ù¾ÛÙ„Û’ ÙÛŒÚ†Ø± Ú©Û’ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº `(83, 85)` Ø¨Ø·ÙˆØ± Ù„ÛŒØ¨Ù„ Ù…Ù„ØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ø¦ÛŒÛ’ Ù†Ø¸Ø±ÛŒØ§ØªÛŒ Ø¬ÙˆØ§Ø¨ Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û 83 Ø³Û’ 85 (Ø´Ø§Ù…Ù„) ØªÚ© Ú©Û’ Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ ÚˆÛŒ Ú©ÙˆÚˆ Ø´Ø¯Û Ø§Ø³Ù¾ÛŒÙ† Ø³Û’ Ú©Ø±ÛŒÚº:

```py
idx = 0
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs["input_ids"][idx][start : end + 1])

print(f"Theoretical answer: {answer}, labels give: {labeled_answer}")
```

```python out
'Theoretical answer: the Main Building, labels give: the Main Building'
```
ØªÙˆ ÛŒÛ Ù…ÛŒÙ„ Ú©Ú¾Ø§ØªØ§ ÛÛ’! Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ Ø§Ù†ÚˆÛŒÚ©Ø³ 4 Ú©Ùˆ Ú†ÛŒÚ© Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛØ§Úº ÛÙ… Ù†Û’ Ù„ÛŒØ¨Ù„Ø² Ú©Ùˆ `(0, 0)` Ù¾Ø± Ø³ÛŒÙ¹ Ú©ÛŒØ§ ÛÛ’ØŒ Ø¬Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛÛ’ Ú©Û Ø§Ø³ ÙÛŒÚ†Ø± Ú©Û’ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ ÙˆØ§Ù„Û’ Ø­ØµÛ’ Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº ÛÛ’Û”

```py
idx = 4
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

decoded_example = tokenizer.decode(inputs["input_ids"][idx])
print(f"Theoretical answer: {answer}, decoded example: {decoded_example}")
```

```python out
'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'
```

ÙˆØ§Ù‚Ø¹ÛŒØŒ ÛÙ…ÛŒÚº Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ø§Ù†Ø¯Ø± Ø¬ÙˆØ§Ø¨ Ù†Ø¸Ø± Ù†ÛÛŒÚº Ø¢ØªØ§Û”  

<Tip>  

âœï¸ **Ø§Ø¨ Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Ø±ÛŒ!** Ø¬Ø¨ XLNet Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ ØªÙˆ Ù¾ÛŒÚˆÙ†Ú¯ Ø¨Ø§Ø¦ÛŒÚº Ø·Ø±Ù Ù„Ú¯Ø§Ø¦ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’ Ø§ÙˆØ± Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ø¨Ø¯Ù„ Ø¬Ø§ØªÛŒ ÛÛ’Û” Ø³Ø§Ø±Ø§ Ú©ÙˆÚˆ Ø¬Ùˆ ÛÙ… Ù†Û’ Ø§Ø¨Ú¾ÛŒ Ø¯ÛŒÚ©Ú¾Ø§ ÛÛ’ Ø§Ø³Û’ XLNet Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÚˆÚ¾Ø§Ù„ÛŒÚº (Ø§ÙˆØ± `padding=True` Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº)Û” ÛŒØ§Ø¯ Ø±Ú©Ú¾ÛŒÚº Ú©Û Ù¾ÛŒÚˆÙ†Ú¯ Ù„Ú¯Ø§Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ `[CLS]` Ù¹ÙˆÚ©Ù† Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛÛŒÚº Ú©Û 0 Ù¾ÙˆØ²ÛŒØ´Ù† Ù¾Ø± ÛÙˆÛ”  

</Tip>  

Ø§Ø¨ Ø¬Ø¨ Ú©Û ÛÙ… Ù†Û’ Ù‚Ø¯Ù… Ø¨Û Ù‚Ø¯Ù… Ø¯ÛŒÚ©Ú¾Ø§ Ú©Û Ø§Ù¾Ù†Û’ Ù¹Ø±ÛŒÙ†Ù†Ú¯ ÚˆÛŒÙ¹Ø§ Ú©Ùˆ Ú©ÛŒØ³Û’ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±Ù†Ø§ ÛÛ’ØŒ ÛÙ… Ø§Ø³Û’ Ø§ÛŒÚ© ÙÙ†Ú©Ø´Ù† Ù…ÛŒÚº Ú¯Ø±ÙˆÙ¾ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ø³Û’ ÛÙ… Ù¾ÙˆØ±Û’ Ù¹Ø±ÛŒÙ†Ù†Ú¯ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ù¾Ø± Ù„Ø§Ú¯Ùˆ Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ… ÛØ± ÙÛŒÚ†Ø± Ú©Ùˆ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© Ù¾ÛŒÚˆ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û Ø²ÛŒØ§Ø¯Û ØªØ± Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø·ÙˆÛŒÙ„ ÛÙˆÚº Ú¯Û’ (Ø§ÙˆØ± Ø§Ù† Ú©Û’ Ù…ØªØ¹Ù„Ù‚Û Ù†Ù…ÙˆÙ†Û’ Ú©Ø¦ÛŒ ÙÛŒÚ†Ø±Ø² Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… ÛÙˆ Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’)ØŒ Ù„ÛÙ°Ø°Ø§ ÛŒÛØ§Úº ÚˆØ§Ø¦Ù†Ø§Ù…Ú© Ù¾ÛŒÚˆÙ†Ú¯ Ù„Ø§Ú¯Ùˆ Ú©Ø±Ù†Û’ Ú©Ø§ Ú©ÙˆØ¦ÛŒ Ø®Ø§Øµ ÙØ§Ø¦Ø¯Û Ù†ÛÛŒÚº ÛÛ’Û”

```py
max_length = 384
stride = 128


def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label is (0, 0)
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs
```

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û ÛÙ… Ù†Û’ Ø¯Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø§Ù‚Ø¯Ø§Ø± (constants) Ù…ØªØ¹ÛŒÙ† Ú©ÛŒ ÛÛŒÚº ØªØ§Ú©Û Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø§ÙˆØ± Ø³Ù„Ø§Ø¦ÛŒÚˆÙ†Ú¯ ÙˆÙ†ÚˆÙˆ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Ø§ ØªØ¹ÛŒÙ† Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’ØŒ Ø§ÙˆØ± ÛÙ… Ù†Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ø³Û’ Ù¾ÛÙ„Û’ ØªÚ¾ÙˆÚ‘ÛŒ Ø³ÛŒ ØµÙØ§Ø¦ÛŒ Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ Ú©ÛŒ ÛÛ’: SQuAD ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ù…ÛŒÚº Ú©Ú†Ú¾ Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø´Ø±ÙˆØ¹ ÛŒØ§ Ø¢Ø®Ø± Ù…ÛŒÚº ØºÛŒØ± Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø®Ù„Ø§ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ú©Ø³ÛŒ Ù‚Ø³Ù… Ú©ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙØ±Ø§ÛÙ… Ù†ÛÛŒÚº Ú©Ø±ØªÛ’ (Ø§ÙˆØ± Ø§Ú¯Ø± Ø¢Ù¾ RoBERTa Ø¬ÛŒØ³Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº ØªÙˆ ÛŒÛ ØºÛŒØ± Ø¶Ø±ÙˆØ±ÛŒ Ø¬Ú¯Û Ù„ÛŒØªÛ’ ÛÛŒÚº)ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… Ù†Û’ Ø§Ù† Ø§Ø¶Ø§ÙÛŒ Ø®Ù„Ø§ Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒØ§Û”  

Ù¾ÙˆØ±Û’ Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ø³ÛŒÙ¹ Ù¾Ø± Ø§Ø³ ÙÙ†Ú©Ø´Ù† Ú©Ø§ Ø§Ø·Ù„Ø§Ù‚ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… `Dataset.map()` Ù…ÛŒØªÚ¾Úˆ Ú©Ùˆ `batched=True` ÙÙ„ÛŒÚ¯ Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” ÛŒÛ ÛŒÛØ§Úº Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ… ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº (Ú©ÛŒÙˆÙ†Ú©Û Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Ø¦ÛŒ Ù¹Ø±ÛŒÙ†Ù†Ú¯ ÙÛŒÚ†Ø±Ø² Ø¯Û’ Ø³Ú©ØªÛŒ ÛÛ’):

```py
train_dataset = raw_datasets["train"].map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
len(raw_datasets["train"]), len(train_dataset)
```

```python out
(87599, 88729)
```

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù†Û’ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 1,000 ÙÛŒÚ†Ø±Ø² Ú©Ø§ Ø§Ø¶Ø§ÙÛ Ú©ÛŒØ§ ÛÛ’Û” ÛÙ…Ø§Ø±Ø§ Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ø³ÛŒÙ¹ Ø§Ø¨ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛ’â€”Ø¢Ø¦ÛŒÛ’ Ø§Ø¨ ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù† Ø³ÛŒÙ¹ Ú©ÛŒ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù¾Ø± Ú©Ø§Ù… Ú©Ø±ÛŒÚº!  

### ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù† ÚˆÛŒÙ¹Ø§ Ú©ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯[[processing-the-validation-data]]  

ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù† ÚˆÛŒÙ¹Ø§ Ú©ÛŒ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù‚Ø¯Ø±Û’ Ø¢Ø³Ø§Ù† ÛÙˆÚ¯ÛŒ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ…ÛŒÚº Ù„ÛŒØ¨Ù„Ø² Ø¨Ù†Ø§Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº (Ø¬Ø¨ ØªÚ© Ú©Û ÛÙ… ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù† Ù„Ø§Ø³ Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù†Û Ù„Ú¯Ø§Ù†Ø§ Ú†Ø§ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† ÙˆÛ Ù†Ù…Ø¨Ø± ÛÙ…ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†ÛÛŒÚº Ø¯Û’ Ú¯Ø§)Û” Ø§ØµÙ„ Ú©Ø§Ù… Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒÙˆÚº Ú©Ùˆ Ø§ØµÙ„ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ø§Ø³Ù¾ÛŒÙ†Ø² Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº ØµØ±Ù Ø¢ÙØ³ÛŒÙ¹ Ù…ÛŒÙ¾Ù†Ú¯Ø² Ú©Ùˆ Ù…Ø­ÙÙˆØ¸ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ Ø§ÙˆØ± Ú©ÙˆØ¦ÛŒ Ø§ÛŒØ³Ø§ Ø·Ø±ÛŒÙ‚Û Ø§Ù¾Ù†Ø§Ù†Ø§ ÛÙˆÚ¯Ø§ Ø¬Ø³ Ø³Û’ ÛØ± ØªØ®Ù„ÛŒÙ‚ Ø´Ø¯Û ÙÛŒÚ†Ø± Ú©Ùˆ Ø§Ø³ Ú©ÛŒ Ø§ØµÙ„ Ù…Ø«Ø§Ù„ Ø³Û’ Ø¬ÙˆÚ‘Ø§ Ø¬Ø§ Ø³Ú©Û’Û” Ú†ÙˆÙ†Ú©Û Ø§ØµÙ„ ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ù…ÛŒÚº Ø§ÛŒÚ© ID Ú©Ø§Ù„Ù… Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ØŒ ÛÙ… Ø§Ø³ÛŒ ID Ú©Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”  

ÛŒÛØ§Úº ÛÙ… Ø§ÛŒÚ© Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø³ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ø¢ÙØ³ÛŒÙ¹ Ù…ÛŒÙ¾Ù†Ú¯Ø² Ú©ÛŒ ØµÙØ§Ø¦ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø§Ù† Ù…ÛŒÚº Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ù„ÛŒÛ’ Ø¢ÙØ³ÛŒÙ¹Ø³ Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÚº Ú¯Û’ØŒ Ù„ÛŒÚ©Ù† Ø¬Ø¨ ÛÙ… Ù¾ÙˆØ³Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù…Ø±Ø­Ù„Û’ Ù…ÛŒÚº Ù¾ÛÙ†Ú†ÛŒÚº Ú¯Û’ ØªÙˆ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø±Ù†Û’ Ú©Ø§ Ú©ÙˆØ¦ÛŒ Ø·Ø±ÛŒÙ‚Û Ù†ÛÛŒÚº ÛÙˆÚ¯Ø§ Ú©Û Ø§Ù† Ù¾Ù¹ IDs Ú©Û’ Ú©ÙˆÙ† Ø³Û’ Ø­ØµÛ’ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø³Û’ Ù…Ø·Ø§Ø¨Ù‚Øª Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ú©ÙˆÙ† Ø³Û’ Ø³ÙˆØ§Ù„ Ø³Û’ (Ú©ÛŒÙˆÙ†Ú©Û `sequence_ids()` Ù…ÛŒØªÚ¾Úˆ ØµØ±Ù Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ø¢Ø¤Ù¹ Ù¾Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø³ØªÛŒØ§Ø¨ ÛÛ’)Û” Ø§Ø³ Ù„ÛŒÛ’ØŒ ÛÙ… Ø§Ù† Ø¢ÙØ³ÛŒÙ¹Ø³ Ú©Ùˆ `None` Ù¾Ø± Ø³ÛŒÙ¹ Ú©Ø± Ø¯ÛŒÚº Ú¯Û’ Ø¬Ùˆ Ø³ÙˆØ§Ù„ Ø³Û’ Ù…Ø·Ø§Ø¨Ù‚Øª Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºÛ”

```py
def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs
```
ÛÙ… Ø§Ø³ ÙÙ†Ú©Ø´Ù† Ú©Ùˆ Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ Ù¾ÙˆØ±Û’ ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù† ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ù¾Ø± Ù„Ø§Ú¯Ùˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
validation_dataset = raw_datasets["validation"].map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
len(raw_datasets["validation"]), len(validation_dataset)
```

```python out
(10570, 10822)
```

Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº ÛÙ… Ù†Û’ ØµØ±Ù Ú†Ù†Ø¯ Ø³Ùˆ Ù†Ù…ÙˆÙ†Û’ Ø´Ø§Ù…Ù„ Ú©ÛŒÛ’ ÛÛŒÚºØŒ Ù„ÛÙ°Ø°Ø§ Ø§ÛŒØ³Ø§ Ù„Ú¯ØªØ§ ÛÛ’ Ú©Û validation dataset Ú©Û’ contexts Ú©Ú†Ú¾ Ú†Ú¾ÙˆÙ¹Û’ ÛÛŒÚºÛ”

Ø§Ø¨ Ø¬Ø¨Ú©Û ÛÙ… Ù†Û’ ØªÙ…Ø§Ù… ÚˆÛŒÙ¹Ø§ Ú©ÛŒ preprocessing Ú©Ø± Ù„ÛŒ ÛÛ’ØŒ ÛÙ… ØªØ±Ø¨ÛŒØª (training) Ú©ÛŒ Ø¬Ø§Ù†Ø¨ Ø¨Ú‘Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

{#if fw === 'pt'}

## `Trainer` API Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø§ÚˆÙ„ Ú©ÛŒ fine-tuning[[fine-tuning-the-model-with-the-trainer-api]]

Ø§Ø³ Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ø¨ÛŒØªÛŒ Ú©ÙˆÚˆ Ù¾Ú†Ú¾Ù„Û’ Ø­ØµÙˆÚº Ú©Û’ Ú©ÙˆÚˆ Ø¬ÛŒØ³Ø§ ÛÛŒ Ù„Ú¯Û’ Ú¯Ø§ â€“ Ø³Ø¨ Ø³Û’ Ù…Ø´Ú©Ù„ Ú©Ø§Ù… `compute_metrics()` ÙÙ†Ú©Ø´Ù† Ù„Ú©Ú¾Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ú†ÙˆÙ†Ú©Û ÛÙ… Ù†Û’ ØªÙ…Ø§Ù… Ù†Ù…ÙˆÙ†ÙˆÚº Ú©Ùˆ Ù…Ù‚Ø±Ø± Ú©Ø±Ø¯Û Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© pad Ú©Ø± Ø¯ÛŒØ§ ÛÛ’ØŒ Ú©ÙˆØ¦ÛŒ data collator ØªØ¹Ø±ÛŒÙ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛŒÛ metric computation Ø¯Ø±Ø§ØµÙ„ ÙˆØ§Ø­Ø¯ Ú†ÛŒØ² ÛÛ’ Ø¬Ø³ Ú©ÛŒ ÛÙ…ÛŒÚº ÙÚ©Ø± ÛÛ’Û” Ù…Ø´Ú©Ù„ Ø­ØµÛ Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒÙˆÚº Ú©Ùˆ Ø§ØµÙ„ Ù…Ø«Ø§Ù„ÙˆÚº Ú©Û’ Ù…ØªÙ† (spans) Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Ø› Ø§ÛŒÚ© Ø¨Ø§Ø± Ø¬Ø¨ ÛÙ… ÛŒÛ Ú©Ø± Ù„ÛŒÚº Ú¯Û’ØŒ ğŸ¤— Datasets Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Ø§ metric Ø¨Ø§Ù‚ÛŒ Ø²ÛŒØ§Ø¯Û Ú©Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ Ú¯Ø§Û”

{:else}

## Keras Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø§ÚˆÙ„ Ú©ÛŒ fine-tuning[[fine-tuning-the-model-with-keras]]

Ø§Ø³ Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ø¨ÛŒØªÛŒ Ú©ÙˆÚˆ Ù¾Ú†Ú¾Ù„Û’ Ø­ØµÙˆÚº Ú©Û’ Ú©ÙˆÚˆ Ø¬ÛŒØ³Ø§ ÛÛŒ Ù„Ú¯Û’ Ú¯Ø§ØŒ Ù…Ú¯Ø± metrics Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§Ù†Ø§ Ù…Ù†ÙØ±Ø¯ Ø·ÙˆØ± Ù¾Ø± Ú†ÛŒÙ„Ù†Ø¬Ù†Ú¯ ÛÙˆÚ¯Ø§Û” Ú†ÙˆÙ†Ú©Û ÛÙ… Ù†Û’ ØªÙ…Ø§Ù… Ù†Ù…ÙˆÙ†ÙˆÚº Ú©Ùˆ Ù…Ù‚Ø±Ø± Ú©Ø±Ø¯Û Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© pad Ú©Ø± Ø¯ÛŒØ§ ÛÛ’ØŒ Ú©ÙˆØ¦ÛŒ data collator ØªØ¹Ø±ÛŒÙ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛŒÛ metric computation Ø¯Ø±Ø§ØµÙ„ ÙˆØ§Ø­Ø¯ Ú†ÛŒØ² ÛÛ’ Ø¬Ø³ Ú©ÛŒ ÛÙ…ÛŒÚº ÙÚ©Ø± ÛÛ’Û” Ù…Ø´Ú©Ù„ Ø­ØµÛ Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒÙˆÚº Ú©Ùˆ Ø§ØµÙ„ Ù…Ø«Ø§Ù„ÙˆÚº Ú©Û’ Ù…ØªÙ† (spans) Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Ø› Ø§ÛŒÚ© Ø¨Ø§Ø± Ø¬Ø¨ ÛÙ… ÛŒÛ Ú©Ø± Ù„ÛŒÚº Ú¯Û’ØŒ ğŸ¤— Datasets Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Ø§ metric Ø¨Ø§Ù‚ÛŒ Ø²ÛŒØ§Ø¯Û Ú©Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ Ú¯Ø§Û”

{/if}

### Ù¾ÙˆØ³Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯[[post-processing]]

{#if fw === 'pt'}

<Youtube id="BNy08iIWVJM"/>

{:else}

<Youtube id="VN67ZpN33Ss"/>

{/if}

Ù…Ø§ÚˆÙ„ input IDs Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø¢ØºØ§Ø² Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… Ú©ÛŒ Ù¾ÙˆØ²ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ logits output Ú©Ø±Û’ Ú¯Ø§ØŒ Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ [`question-answering` pipeline](/course/chapter6/3b) Ú©ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø¯ÛŒÚ©Ú¾Ø§ ØªÚ¾Ø§Û” Ø¨Ø¹Ø¯ Ø§Ø² Ø¹Ù…Ù„ Ú©Ø§Ø±ÛŒ (post-processing) Ú©Ø§ Ù…Ø±Ø­Ù„Û ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÙˆÛÛŒ ÛÙˆÚ¯Ø§ Ø¬Ùˆ ÛÙ… Ù†Û’ ÙˆÛØ§Úº Ú©ÛŒØ§ ØªÚ¾Ø§ØŒ Ù„ÛÙ°Ø°Ø§ ÛŒÛØ§Úº ÛÙ… Ù†Û’ Ø¬Ùˆ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©ÛŒÛ’ ØªÚ¾Û’ Ø§Ù† Ú©ÛŒ Ø§ÛŒÚ© Ù…Ø®ØªØµØ± ÛŒØ§Ø¯ Ø¯ÛØ§Ù†ÛŒ Ù¾ÛŒØ´ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’:

- ÛÙ… Ù†Û’ context Ø³Û’ Ø¨Ø§ÛØ± Ù…ÙˆØ¬ÙˆØ¯ tokens Ú©Û’ Ù„ÛŒÛ’ Ø´Ø±ÙˆØ¹ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù…ÛŒ logits Ú©Ùˆ mask Ú©Ø± Ø¯ÛŒØ§Û”
- Ù¾Ú¾Ø± ÛÙ… Ù†Û’ softmax Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø´Ø±ÙˆØ¹ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù…ÛŒ logits Ú©Ùˆ probabilities Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒØ§Û”
- ÛÙ… Ù†Û’ ÛØ± `(start_token, end_token)` Ø¬ÙˆÚ‘Û’ Ú©Ùˆ Ù…ØªØ¹Ù„Ù‚Û Ø¯ÙˆÙ†ÙˆÚº probabilities Ú©Û’ Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ø³Û’ Ø§ÛŒÚ© Ø§Ø³Ú©ÙˆØ± Ø¯ÛŒØ§Û”
- ÛÙ… Ù†Û’ Ø§ÛŒØ³Û’ Ø¬ÙˆÚ‘Û’ Ú©ÛŒ ØªÙ„Ø§Ø´ Ú©ÛŒ Ø¬Ø³ Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÙˆ Ø§ÙˆØ± Ø¬Ùˆ Ø§ÛŒÚ© valid Ø¬ÙˆØ§Ø¨ ÙØ±Ø§ÛÙ… Ú©Ø±Û’ (Ù…Ø«Ù„Ø§Ù‹ØŒ Ø§ÛŒØ³Ø§ `start_token` Ø¬Ùˆ `end_token` Ø³Û’ Ú©Ù… ÛÙˆ)Û”

ÛŒÛØ§Úº ÛÙ… Ø§Ø³ Ø¹Ù…Ù„ Ú©Ùˆ ØªÚ¾ÙˆÚ‘Ø§ Ø³Ø§ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒÚº Ú¯Û’ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ…ÛŒÚº Ø­Ù‚ÛŒÙ‚ÛŒ Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº (ØµØ±Ù Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±Ø¯Û Ø¬ÙˆØ§Ø¨ Ø¯Ø±Ú©Ø§Ø± ÛÛ’)Û” Ø§Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛÛ’ Ú©Û ÛÙ… softmax Ù…Ø±Ø­Ù„Û’ Ú©Ùˆ Ú†Ú¾ÙˆÚ‘ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” ØªÛŒØ²ÛŒ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û `(start_token, end_token)` Ø¬ÙˆÚ‘ÙˆÚº Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ù†ÛÛŒÚº Ø¯ÛŒÚº Ú¯Û’ØŒ Ø¨Ù„Ú©Û ØµØ±Ù Ø§Ù† Ø¬ÙˆÚ‘ÙˆÚº Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø¯ÛŒÚº Ú¯Û’ Ø¬Ùˆ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û `n_best` logits (Ø¬ÛØ§Úº `n_best=20` ÛÛ’) Ø³Û’ Ù…ØªØ¹Ù„Ù‚ ÛÙˆÚºÛ” Ú†ÙˆÙ†Ú©Û ÛÙ… softmax Ú©Ùˆ Ú†Ú¾ÙˆÚ‘ Ø±ÛÛ’ ÛÛŒÚºØŒ Ø§Ù† Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ùˆ logit scores Ú©ÛØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŒ Ø§ÙˆØ± Ø§Ù†ÛÛŒÚº Ø´Ø±ÙˆØ¹ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù…ÛŒ logits Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹Û Ù„Û’ Ú©Ø± Ø­Ø§ØµÙ„ Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ (Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ú©ÛŒ Ø¨Ø¬Ø§Ø¦Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û \\(\log(ab) = \log(a) + \log(b)\\) Ú©Û’ Ø§ØµÙˆÙ„ Ú©ÛŒ ÙˆØ¬Û Ø³Û’)Û”

Ø§Ø³ Ø³Ø¨ Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº Ú©Ú†Ú¾ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒÙˆÚº (predictions) Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆÚ¯ÛŒÛ” Ú†ÙˆÙ†Ú©Û ÛÙ… Ù†Û’ Ø§Ø¨Ú¾ÛŒ ØªÚ© Ø§Ù¾Ù†Ø§ Ù…Ø§ÚˆÙ„ ØªØ±Ø¨ÛŒØª Ù†ÛÛŒÚº Ø¯ÛŒØ§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… QA pipeline Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒÙØ§Ù„Ù¹ Ù…Ø§ÚˆÙ„ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ validation set Ú©Û’ Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹Û’ Ø­ØµÛ’ Ù¾Ø± Ú©Ú†Ú¾ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ… Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ Ø§Ø³ÛŒ processing function Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØ› Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ global constant `tokenizer` Ù¾Ø± Ù…Ù†Ø­ØµØ± ÛÛ’ØŒ ÛÙ…ÛŒÚº ØµØ±Ù Ø¹Ø§Ø±Ø¶ÛŒ Ø·ÙˆØ± Ù¾Ø± tokenizer Ú©Ùˆ Ø§ÙØ³ Ù…Ø§ÚˆÙ„ Ú©Û’ tokenizer Ø³Û’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’ Ø¬Ø³Û’ ÛÙ… Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº:

```python
small_eval_set = raw_datasets["validation"].select(range(100))
trained_checkpoint = "distilbert-base-cased-distilled-squad"

tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)
eval_set = small_eval_set.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
```

Ø§Ø¨ Ø¬Ø¨Ú©Û preprocessing Ù…Ú©Ù…Ù„ ÛÙˆ Ú†Ú©Ø§ ÛÛ’ØŒ ÛÙ… tokenizer Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ø§Ø³ÛŒ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ùˆ ÛÙ… Ù†Û’ Ø§ØµÙ„ Ù…ÛŒÚº Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§ ØªÚ¾Ø§:

```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Ù¾Ú¾Ø± ÛÙ… Ø§Ù¾Ù†Û’ `eval_set` Ú©Û’ Ø§ÙÙ† columns Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªÙˆÙ‚Ø¹ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ù†ÛÛŒÚº ÛÛŒÚºØŒ Ø§Ø³ Ú†Ú¾ÙˆÙ¹Û’ validation set Ú©Ø§ Ø§ÛŒÚ© batch ØªÛŒØ§Ø± Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø§Ø³Û’ Ù…Ø§ÚˆÙ„ Ø³Û’ Ú¯Ø²Ø§Ø± Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ø± Ú©ÙˆØ¦ÛŒ GPU Ø¯Ø³ØªÛŒØ§Ø¨ ÛÙˆØŒ ØªÙˆ ÛÙ… ØªÛŒØ²ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}

```python
import torch
from transformers import AutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}
trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(
    device
)

with torch.no_grad():
    outputs = trained_model(**batch)
```

Ú†ÙˆÙ†Ú©Û `Trainer` ÛÙ…ÛŒÚº Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº NumPy arrays Ú©ÛŒ Ø´Ú©Ù„ Ù…ÛŒÚº Ø¯Û’ Ú¯Ø§ØŒ ÛÙ… Ø§Ø³Ù¹Ø§Ø±Ù¹ Ø§ÙˆØ± Ø§ÛŒÙ†Úˆ Ù„Ø§Ø¬Ù¹Ø³ Ø­Ø§ØµÙ„ Ú©Ø±Ú©Û’ Ø§Ù†ÛÛŒÚº Ø§Ø³ÛŒ ÙØ§Ø±Ù…ÛŒÙ¹ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()
```

{:else}

```python
import tensorflow as tf
from transformers import TFAutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("numpy")

batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}
trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)

outputs = trained_model(**batch)
```

Ø¢Ø³Ø§Ù†ÛŒ Ø³Û’ ØªØ¬Ø±Ø¨Ø§Øª Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¢Ø¦ÛŒÛ’ Ø§Ù† Ø¢Ø¤Ù¹ Ù¾Ù¹Ø³ Ú©Ùˆ NumPy arrays Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº:

```python
start_logits = outputs.start_logits.numpy()
end_logits = outputs.end_logits.numpy()
```

{/if}

Ø§Ø¨ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ `small_eval_set` Ù…ÛŒÚº ÛØ± Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ù…ØªÙˆÙ‚Ø¹ Ø¬ÙˆØ§Ø¨ ØªÙ„Ø§Ø´ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ `eval_set` Ù…ÛŒÚº Ú©Ø¦ÛŒ ÙÛŒÚ†Ø±Ø² Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… ÛÙˆ Ø³Ú©ØªÛŒ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ù¾ÛÙ„Ø§ Ù‚Ø¯Ù… ÛŒÛ ÛÛ’ Ú©Û `small_eval_set` Ù…ÛŒÚº ÛØ± Ù…Ø«Ø§Ù„ Ú©Ùˆ `eval_set` Ù…ÛŒÚº Ø§Ø³ Ú©Û’ Ù…ØªØ¹Ù„Ù‚Û ÙÛŒÚ†Ø±Ø² Ø³Û’ Ø¬ÙˆÚ‘ÛŒÚº:

```python
import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(eval_set):
    example_to_features[feature["example_id"]].append(idx)
```

Ø§Ø¨ ÛÙ… Ø§ØµÙ„ Ú©Ø§Ù… Ø´Ø±ÙˆØ¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº: ØªÙ…Ø§Ù… Ù…Ø«Ø§Ù„ÙˆÚº Ù¾Ø± Ù„ÙˆÙ¾ Ú†Ù„Ø§Ø¦ÛŒÚº Ú¯Û’ Ø§ÙˆØ± ÛØ± Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ Ø³Û’ Ù…ØªØ¹Ù„Ù‚ ØªÙ…Ø§Ù… ÙÛŒÚ†Ø±Ø² Ù¾Ø± Ø¨Ú¾ÛŒ Ù„ÙˆÙ¾ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ Ø°Ú©Ø± Ú©ÛŒØ§ØŒ ÛÙ… `n_best` Ø§Ø³Ù¹Ø§Ø±Ù¹ Ù„Ø§Ø¬Ù¹Ø³ Ø§ÙˆØ± Ø§ÛŒÙ†Úˆ Ù„Ø§Ø¬Ù¹Ø³ Ú©Û’ Ù„Ø§Ø¬Ù¹ Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ØŒ Ù„ÛŒÚ©Ù† Ø§Ù† Ù¾ÙˆØ²ÛŒØ´Ù†Ø² Ú©Ùˆ Ø®Ø§Ø±Ø¬ Ú©Ø± Ø¯ÛŒÚº Ú¯Û’ Ø¬Ùˆ:  

- Ø§ÛŒØ³Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒØªÛŒ ÛÛŒÚº Ø¬Ùˆ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ø§Ù†Ø¯Ø± Ù†ÛÛŒÚº Ø¢ØªØ§  
- Ø§ÛŒØ³Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒØªÛŒ ÛÛŒÚº Ø¬Ø³ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ù…Ù†ÙÛŒ ÛÙˆ  
- Ø§ÛŒØ³Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒØªÛŒ ÛÛŒÚº Ø¬Ùˆ Ø¨ÛØª Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§ ÛÙˆ (ÛÙ… Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ `max_answer_length=30` Ù¾Ø± Ù…Ø­Ø¯ÙˆØ¯ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº)  

Ø¬Ø¨ ÛÙ… Ú©Ø³ÛŒ Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Û’ ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Û’ Ø§Ø³Ú©ÙˆØ± Ø­Ø§ØµÙ„ Ú©Ø± Ù„ÛŒÚº Ú¯Û’ØŒ ØªÙˆ ÛÙ… Ø³Ø¨ Ø³Û’ Ø¨ÛØªØ±ÛŒÙ† Ù„Ø§Ø¬Ù¹ Ø§Ø³Ú©ÙˆØ± ÙˆØ§Ù„Ø§ Ø¬ÙˆØ§Ø¨ Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº Ú¯Û’Û”

```python
import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in small_eval_set:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = eval_set["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Skip answers that are not fully in the context
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Skip answers with a length that is either < 0 or > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
```

Ù…ØªÙˆÙ‚Ø¹ Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Ø§ Ø­ØªÙ…ÛŒ ÙØ§Ø±Ù…ÛŒÙ¹ ÙˆÛÛŒ ÛÙˆÚ¯Ø§ Ø¬Ùˆ Ø§Ø³ Ù…ÛŒÙ¹Ø±Ú© Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±Ú©Ø§Ø± ÛÛ’ Ø¬Ø³Û’ ÛÙ… Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø­Ø³Ø¨ Ù…Ø¹Ù…ÙˆÙ„ØŒ ÛÙ… Ø§Ø³Û’ ğŸ¤— Evaluate Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©ÛŒ Ù…Ø¯Ø¯ Ø³Û’ Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
import evaluate

metric = evaluate.load("squad")
```

ÛŒÛ Ù…ÛŒÙ¹Ø±Ú© Ù…ØªÙˆÙ‚Ø¹ Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Ùˆ Ø§Ø³ ÙØ§Ø±Ù…ÛŒÙ¹ Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ù†Û’ Ú©ÛŒ ØªÙˆÙ‚Ø¹ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ ÛÙ… Ù†Û’ Ø§ÙˆÙ¾Ø± Ø¯ÛŒÚ©Ú¾Ø§ ØªÚ¾Ø§ (Ù„ØºØ§Øª Ú©ÛŒ Ø§ÛŒÚ© ÙÛØ±Ø³Øª Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© Ú©Ù„ÛŒØ¯ Ù…Ø«Ø§Ù„ Ú©ÛŒ ID Ú©Û’ Ù„ÛŒÛ’ Ø§ÙˆØ± Ø§ÛŒÚ© Ú©Ù„ÛŒØ¯ Ù…ØªÙˆÙ‚Ø¹ Ù…ØªÙ† Ú©Û’ Ù„ÛŒÛ’ ÛÙˆ)Û” Ø¬Ø¨Ú©Û Ù†Ø¸Ø±ÛŒØ§ØªÛŒ Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÙØ§Ø±Ù…ÛŒÙ¹ Ù…ÛŒÚº ÛÙˆÚº Ú¯Û’ (Ù„ØºØ§Øª Ú©ÛŒ Ø§ÛŒÚ© ÙÛØ±Ø³Øª Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© Ú©Ù„ÛŒØ¯ Ù…Ø«Ø§Ù„ Ú©ÛŒ ID Ú©Û’ Ù„ÛŒÛ’ Ø§ÙˆØ± Ø§ÛŒÚ© Ú©Ù„ÛŒØ¯ Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Û’ Ù„ÛŒÛ’ ÛÙˆ):

```python
theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]} for ex in small_eval_set
]
```

Ø§Ø¨ ÛÙ… Ø¯ÙˆÙ†ÙˆÚº ÙÛØ±Ø³ØªÙˆÚº Ú©Û’ Ù¾ÛÙ„Û’ Ø¹Ù†ØµØ± Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ú©Ø± ØªØµØ¯ÛŒÙ‚ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ…ÛŒÚº Ù…Ø¹Ù‚ÙˆÙ„ Ù†ØªØ§Ø¦Ø¬ Ù…Ù„ Ø±ÛÛ’ ÛÛŒÚº:

```python
print(predicted_answers[0])
print(theoretical_answers[0])
```

```python out
{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}
```

Ø§ØªÙ†Ø§ Ø¨Ø±Ø§ Ù†ÛÛŒÚº! Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ Ù…ÛŒÙ¹Ø±Ú© ÛÙ…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³Ú©ÙˆØ± Ø¯ÛŒØªØ§ ÛÛ’:

```python
metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

Ù¾Ú¾Ø± Ø¨Ú¾ÛŒØŒ ÛŒÛ Ú©Ø§ÙÛŒ Ø§Ú†Ú¾Ø§ ÛÛ’ Ø§Ú¯Ø± ØºÙˆØ± Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú©Û [its paper](https://arxiv.org/abs/1910.01108v2) Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ SQuAD Ù¾Ø± fine-tuned DistilBERT Ù¾ÙˆØ±Û’ dataset Ù¾Ø± Ø§Ù† Ø§Ø³Ú©ÙˆØ±Ø² Ú©Û’ Ù„ÛŒÛ’ 79.1 Ø§ÙˆØ± 86.9 Ø­Ø§ØµÙ„ Ú©Ø±ØªØ§ ÛÛ’Û”

{#if fw === 'pt'}

Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ø§ÛŒÚ© `compute_metrics()` ÙÙ†Ú©Ø´Ù† Ù…ÛŒÚº ÚˆØ§Ù„ Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ø³Û’ ÛÙ… `Trainer` Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø±ØŒ ÙˆÛ `compute_metrics()` ÙÙ†Ú©Ø´Ù† ØµØ±Ù Ø§ÛŒÚ© tuple `eval_preds` Ù„ÛŒØªØ§ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº logits Ø§ÙˆØ± labels Ø´Ø§Ù…Ù„ ÛÙˆØªÛ’ ÛÛŒÚºÛ” ÛŒÛØ§Úº ÛÙ…ÛŒÚº ØªÚ¾ÙˆÚ‘Ø§ Ø§ÙˆØ± Ø¯Ø±Ú©Ø§Ø± ÛÙˆÚ¯Ø§ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ…ÛŒÚº features Ú©Û’ dataset Ù…ÛŒÚº offset Ú©Ùˆ Ø¯ÛŒÚ©Ú¾Ù†Ø§ ÛÛ’ Ø§ÙˆØ± examples Ú©Û’ dataset Ù…ÛŒÚº Ø§ØµÙ„ contexts Ú©Ùˆ Ø¨Ú¾ÛŒ Ø¯ÛŒÚ©Ú¾Ù†Ø§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… ØªØ±Ø¨ÛŒØª Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø§Ø³ ÙÙ†Ú©Ø´Ù† Ú©Ùˆ Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ evaluation Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù†ÛÛŒÚº Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’Û” ÛÙ… Ø§Ø³Û’ ØµØ±Ù ØªØ±Ø¨ÛŒØª Ú©Û’ Ø§Ø®ØªØªØ§Ù… Ù¾Ø± Ù†ØªØ§Ø¦Ø¬ Ú†ÛŒÚ© Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”

`compute_metrics()` ÙÙ†Ú©Ø´Ù† Ù¾ÛÙ„Û’ Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ùˆ ÛŒÚ©Ø¬Ø§ Ú©Ø±ØªØ§ ÛÛ’Ø› ÛÙ… ØµØ±Ù Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹ÛŒ Ø¬Ø§Ù†Ú† Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø§Ú¯Ø± ÛÙ…ÛŒÚº Ú©ÙˆØ¦ÛŒ valid Ø¬ÙˆØ§Ø¨ Ù†Û Ù…Ù„Û’ (Ø¬Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº ÛÙ… Ø§ÛŒÚ© Ø®Ø§Ù„ÛŒ string Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±ÛŒÚº Ú¯Û’)Û”

{:else}

Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ø§ÛŒÚ© `compute_metrics()` ÙÙ†Ú©Ø´Ù† Ù…ÛŒÚº ÚˆØ§Ù„ Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ø³Û’ ÛÙ… Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ú©Û’ Ø¨Ø¹Ø¯ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ…ÛŒÚº ØµØ±Ù output logits Ø³Û’ Ø²ÛŒØ§Ø¯Û Ú©Ú†Ú¾ Ù¾Ø§Ø³ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆÚ¯ÛŒØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ…ÛŒÚº features Ú©Û’ dataset Ù…ÛŒÚº offset Ú©Ùˆ Ø¯ÛŒÚ©Ú¾Ù†Ø§ ÛÛ’ Ø§ÙˆØ± examples Ú©Û’ dataset Ù…ÛŒÚº Ø§ØµÙ„ contexts Ú©Ùˆ Ø¨Ú¾ÛŒ Ø¯ÛŒÚ©Ú¾Ù†Ø§ ÛÛ’:

{/if}

```python
from tqdm.auto import tqdm


def compute_metrics(start_logits, end_logits, features, examples):
    example_to_features = collections.defaultdict(list)
    for idx, feature in enumerate(features):
        example_to_features[feature["example_id"]].append(idx)

    predicted_answers = []
    for example in tqdm(examples):
        example_id = example["id"]
        context = example["context"]
        answers = []

        # Loop through all features associated with that example
        for feature_index in example_to_features[example_id]:
            start_logit = start_logits[feature_index]
            end_logit = end_logits[feature_index]
            offsets = features[feature_index]["offset_mapping"]

            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Skip answers that are not fully in the context
                    if offsets[start_index] is None or offsets[end_index] is None:
                        continue
                    # Skip answers with a length that is either < 0 or > max_answer_length
                    if (
                        end_index < start_index
                        or end_index - start_index + 1 > max_answer_length
                    ):
                        continue

                    answer = {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                    answers.append(answer)

        # Select the answer with the best score
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append(
                {"id": example_id, "prediction_text": best_answer["text"]}
            )
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    theoretical_answers = [{"id": ex["id"], "answers": ex["answers"]} for ex in examples]
    return metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

ÛÙ… Ø§Ù¾Ù†ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒÙˆÚº Ù¾Ø± Ø§Ø³Û’ Ú†Ù„Ø§ Ú©Ø± ØªØµØ¯ÛŒÙ‚ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ ØµØ­ÛŒØ­ Ú©Ø§Ù… Ú©Ø± Ø±ÛØ§ ÛÛ’:

```python
compute_metrics(start_logits, end_logits, eval_set, small_eval_set)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

Looking good! Now let's use this to fine-tune our model.

### Ù…Ø§ÚˆÙ„ Ú©ÛŒ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ†Ù†Ú¯[[fine-tuning-the-model]]

{#if fw === 'pt'}

Ø§Ø¨ ÛÙ… Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚºÛ” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø§Ø³Û’ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛŒØ³Û’ Ù¾ÛÙ„Û’ Ú©ÛŒØ§ ØªÚ¾Ø§ØŒ `AutoModelForQuestionAnswering` Ú©Ù„Ø§Ø³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’:

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{:else}

Ø§Ø¨ ÛÙ… Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚºÛ” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø§Ø³Û’ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛŒØ³Û’ Ù¾ÛÙ„Û’ Ú©ÛŒØ§ ØªÚ¾Ø§ØŒ `TFAutoModelForQuestionAnswering` Ú©Ù„Ø§Ø³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’:

```python
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{/if}

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ…ÛŒØ´Û Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ…ÛŒÚº Ø§ÛŒÚ© ÙˆØ§Ø±Ù†Ù†Ú¯ Ù…Ù„ØªÛŒ ÛÛ’ Ú©Û Ú©Ú†Ú¾ ÙˆØ²Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù†ÛÛŒÚº ÛÙˆØ¦Û’ (Ù¾Ø±ÛŒ Ù¹Ø±ÛŒÙ†Ù†Ú¯ ÛÛŒÚˆ Ú©Û’ ÙˆØ²Ù†) Ø§ÙˆØ± Ú©Ú†Ú¾ ÙˆØ²Ù† Ø¨Û’ ØªØ±ØªÛŒØ¨ Ø·ÙˆØ± Ù¾Ø± initialize ÛÙˆØ¦Û’ ÛÛŒÚº (Ø³ÙˆØ§Ù„ Ø¬ÙˆØ§Ø¨ ÛÛŒÚˆ Ú©Û’ ÙˆØ²Ù†)Û” Ø§Ø¨ ØªÚ© Ø¢Ù¾ Ø§Ø³ Ú©Û’ Ø¹Ø§Ø¯ÛŒ ÛÙˆ Ú†Ú©Û’ ÛÙˆÚº Ú¯Û’ØŒ Ù…Ú¯Ø± Ø§Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛŒÛ ÛÛ’ Ú©Û ÛŒÛ Ù…Ø§ÚˆÙ„ Ø§Ø¨Ú¾ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ù†ÛÛŒÚº ÛÛ’ Ø§ÙˆØ± Ø§Ø³Û’ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’ â€“ Ø®ÛŒØ±ØŒ Ø§Ø¨Ú¾ÛŒ ÛÙ… ÛŒÛ Ú©Ø±Ù†Û’ Ø¬Ø§ Ø±ÛÛ’ ÛÛŒÚº!

Ù…Ø§ÚˆÙ„ Ú©Ùˆ Hub Ù¾Ø± Ø¯Ú¾Ú©ÛŒÙ„Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº Hugging Face Ù…ÛŒÚº Ù„Ø§Ú¯ Ø§Ù† Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§Ú¯Ø± Ø¢Ù¾ ÛŒÛ Ú©ÙˆÚˆ Ù†ÙˆÙ¹ Ø¨Ú© Ù…ÛŒÚº Ú†Ù„Ø§ Ø±ÛÛ’ ÛÛŒÚº ØªÙˆ Ø¢Ù¾ Ù†ÛŒÚ†Û’ Ø¯ÛŒ Ú¯Ø¦ÛŒ utility ÙÙ†Ú©Ø´Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ø§ÛŒÚ© widget Ø¯Ú©Ú¾Ø§ØªØ§ ÛÛ’ Ø¬ÛØ§Úº Ø¢Ù¾ Ø§Ù¾Ù†Û’ Ù„Ø§Ú¯ Ø§Ù† Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ø¯Ø§Ø®Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Ø§Ú¯Ø± Ø¢Ù¾ Ù†ÙˆÙ¹ Ø¨Ú© Ù…ÛŒÚº Ú©Ø§Ù… Ù†ÛÛŒÚº Ú©Ø± Ø±ÛÛ’ØŒ ØªÙˆ Ø§Ù¾Ù†Û’ Ù¹Ø±Ù…ÛŒÙ†Ù„ Ù…ÛŒÚº ØµØ±Ù ÛŒÛ Ù„Ø§Ø¦Ù† Ù¹Ø§Ø¦Ù¾ Ú©Ø±ÛŒÚº:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Ø¬Ø¨ ÛŒÛ Ø¹Ù…Ù„ Ù…Ú©Ù…Ù„ ÛÙˆ Ø¬Ø§Ø¦Û’ØŒ ØªÙˆ ÛÙ… Ø§Ù¾Ù†Û’ `TrainingArguments` Ú©Ùˆ ÚˆÛŒÙØ§Ø¦Ù† Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø¬ÛŒØ³Û’ Ú©Û ÛÙ… Ù†Û’ metric Ú©Ù…Ù¾ÛŒÙˆÙ¹ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ù¾Ù†Û’ ÙÙ†Ú©Ø´Ù† Ú©Ùˆ ÚˆÛŒÙØ§Ø¦Ù† Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ú©ÛØ§ ØªÚ¾Ø§ØŒ ÛÙ… `compute_metrics()` ÙÙ†Ú©Ø´Ù† Ú©Û’ signature Ú©ÛŒ ÙˆØ¬Û Ø³Û’ Ø§ÛŒÚ© Ù…Ø¹Ù…ÙˆÙ„ Ú©Ø§ evaluation loop Ù†ÛÛŒÚº Ú†Ù„Ø§ Ø³Ú©ÛŒÚº Ú¯Û’Û” ÛÙ… Ø§Ù¾Ù†Ø§ Ø°Ø§ØªÛŒ subclass `Trainer` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø¨Ú¾ÛŒ Ú©Ø± Ø³Ú©ØªÛ’ ØªÚ¾Û’ (Ø¬ÛŒØ³Û’ Ú©Û [question answering example script](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py) Ù…ÛŒÚº Ø¯Ú©Ú¾Ø§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’)ØŒ Ù…Ú¯Ø± ÛŒÛ Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ú©Û’ Ù„ÛŒÛ’ Ú©Ø§ÙÛŒ Ø·ÙˆÛŒÙ„ ÛÙˆÚ¯Ø§Û” Ø§Ø³ Ú©Û’ Ø¨Ø¬Ø§Ø¦Û’ØŒ ÛÙ… ÛŒÛØ§Úº ØµØ±Ù ØªØ±Ø¨ÛŒØª Ú©Û’ Ø§Ø®ØªØªØ§Ù… Ù¾Ø± Ù…Ø§ÚˆÙ„ Ú©Ø§ evaluation Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÙˆØ± "A custom training loop" Ù…ÛŒÚº Ø§ÛŒÚ© Ù…Ø¹Ù…ÙˆÙ„ Ú©Ø§ evaluation Ø¯Ú©Ú¾Ø§Ù†Û’ Ú©Ø§ Ø·Ø±ÛŒÙ‚Û Ø¨ØªØ§Ø¦ÛŒÚº Ú¯Û’Û”

ÛŒÛÛŒ ÙˆÛ Ø¬Ú¯Û ÛÛ’ Ø¬ÛØ§Úº `Trainer` API Ú©ÛŒ Ø­Ø¯ÙˆØ¯ Ø³Ø§Ù…Ù†Û’ Ø¢ØªÛŒ ÛÛŒÚº Ø§ÙˆØ± ğŸ¤— Accelerate Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§Ù¾Ù†ÛŒ Ø¬Ú¯Û Ú†Ù…Ú©ØªÛŒ ÛÛ’: Ú©Ø³ÛŒ Ù…Ø®ØµÙˆØµ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ú©Ù„Ø§Ø³ Ú©Ùˆ Ú©Ø³Ù¹Ù…Ø§Ø¦Ø² Ú©Ø±Ù†Ø§ Ù…Ø´Ú©Ù„ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ØŒ Ù…Ú¯Ø± Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± exposed training loop Ú©Ùˆ tweak Ú©Ø±Ù†Ø§ Ø¢Ø³Ø§Ù† ÛÛ’Û”

Ø¢Ø¦ÛŒÛ’ Ø§Ù¾Ù†Û’ `TrainingArguments` Ù¾Ø± Ø§ÛŒÚ© Ù†Ø¸Ø± ÚˆØ§Ù„ØªÛ’ ÛÛŒÚº:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-squad",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)
```

ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ ÛÛŒ Ø§Ù† Ù…ÛŒÚº Ø³Û’ Ø²ÛŒØ§Ø¯Û ØªØ± Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ù„ÛŒØ§ ÛÛ’: ÛÙ… Ù†Û’ Ú©Ú†Ú¾ hyperparameters (Ø¬ÛŒØ³Û’ learning rateØŒ epochs Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ØŒ Ø§ÙˆØ± weight decay) Ø³ÛŒÙ¹ Ú©ÛŒÛ’ ÛÛŒÚº Ø§ÙˆØ± ÛŒÛ Ø¨ØªØ§ Ø¯ÛŒØ§ ÛÛ’ Ú©Û ÛÙ… ÛØ± epoch Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©Ùˆ save Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ evaluation Ú†Ú¾ÙˆÚ‘Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø§Ù¾Ù†Û’ Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ Model Hub Ù¾Ø± Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºÛ” ÛÙ… Ù†Û’ `fp16=True` Ú©Û’ Ø°Ø±ÛŒØ¹Û’ mixed-precision training Ø¨Ú¾ÛŒ ÙØ¹Ø§Ù„ Ú©Ø± Ø¯ÛŒ ÛÛ’ØŒ Ø¬Ùˆ Ø¬Ø¯ÛŒØ¯ GPU Ù¾Ø± ØªØ±Ø¨ÛŒØª Ú©ÛŒ Ø±ÙØªØ§Ø± Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§ Ø³Ú©ØªÛŒ ÛÛ’Û”

{:else}

Ø§Ø¨ ÛŒÛ Ú©Ø§Ù… Ù…Ú©Ù…Ù„ ÛÙˆ Ú¯ÛŒØ§ ÛÛ’ØŒ ÛÙ… Ø§Ù¾Ù†Û’ TF Datasets ØªØ®Ù„ÛŒÙ‚ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ø¨Ø§Ø± ÛÙ… Ø³Ø§Ø¯Û default data collator Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

Ø§ÙˆØ± Ø§Ø¨ ÛÙ… datasets Ú©Ùˆ Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº:

```python
tf_train_dataset = model.prepare_tf_dataset(
    train_dataset,
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)
tf_eval_dataset = model.prepare_tf_dataset(
    validation_dataset,
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ØŒ ÛÙ… Ø§Ù¾Ù†Û’ training hyperparameters Ø³ÛŒÙ¹ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ú©Ùˆ compile Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# ØªØ±Ø¨ÛŒØªÛŒ Ù…Ø±Ø§Ø­Ù„ Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯: dataset Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†Ù…ÙˆÙ†ÙˆÚº Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ùˆ Ø¨ÛŒÚ† Ø³Ø§Ø¦Ø² Ø³Û’ ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ú©Û’ Ø§ÙˆØ± Ù¾Ú¾Ø± epochs Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø³Û’ Ø¶Ø±Ø¨ Ø¯ÛŒÚºÛ”
# Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û ÛŒÛØ§Úº tf_train_dataset Ø§ÛŒÚ© batched tf.data.Dataset ÛÛ’ØŒ Ø§ØµÙ„ Hugging Face Dataset Ù†ÛÛŒÚºØŒ Ù„ÛÙ°Ø°Ø§ Ø§Ø³ Ú©ÛŒ len() Ù¾ÛÙ„Û’ ÛÛŒ num_samples // batch_size Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÛ’Û”
num_train_epochs = 3
num_train_steps = len(tf_train_dataset) * num_train_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# mixed-precision float16 Ù…ÛŒÚº ØªØ±Ø¨ÛŒØª Ú©Ø±ÛŒÚº
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… `model.fit()` Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ØªØ±Ø¨ÛŒØª Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” ÛÙ… Ø§ÛŒÚ© `PushToHubCallback` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û ÛØ± epoch Ú©Û’ Ø¨Ø¹Ø¯ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Hub Ù¾Ø± Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ÛŒÚºÛ”

{/if}

By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be in `"sgugger/bert-finetuned-squad"`. We can override this by passing a `hub_model_id`; for instance, to push the model to the `huggingface_course` organization we used `hub_model_id="huggingface_course/bert-finetuned-squad"` (which is the model we linked to at the beginning of this section).

{#if fw === 'pt'}

<Tip>

ğŸ’¡ Ø§Ú¯Ø± Ø¢Ù¾ Ø¬Ùˆ output directory Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº ÙˆÛ Ù¾ÛÙ„Û’ Ø³Û’ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ØŒ ØªÙˆ Ø§Ø³Û’ Ø§ÙØ³ repository Ú©Ø§ Ø§ÛŒÚ© Ù„ÙˆÚ©Ù„ Ú©Ù„ÙˆÙ† ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’ Ø¬Ø³Û’ Ø¢Ù¾ push Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº (Ø§Ú¯Ø± `Trainer` Ú©Ùˆ ÚˆÛŒÙØ§Ø¦Ù† Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ú©ÙˆØ¦ÛŒ error Ø¢Ø¦Û’ ØªÙˆ Ù†ÛŒØ§ Ù†Ø§Ù… Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº)Û”

</Tip>

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… Ø³Ø¨ Ú©Ú†Ú¾ `Trainer` Ú©Ù„Ø§Ø³ Ú©Ùˆ Ù¾Ø§Ø³ Ú©Ø± Ú©Û’ ØªØ±Ø¨ÛŒØª Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

{:else}

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-squad", tokenizer=tokenizer)

# Ú†ÙˆÙ†Ú©Û ÛÙ… Ø¨Ø¹Ø¯ Ù…ÛŒÚº validation Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø¯Ø±Ù…ÛŒØ§Ù† Ù…ÛŒÚº validation Ù†ÛÛŒÚº Ú©Ø± Ø±ÛÛ’
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)
```

{/if}

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û Ø¬Ø¨ ØªØ±Ø¨ÛŒØª Ø¬Ø§Ø±ÛŒ ÛÙˆØªÛŒ ÛÛ’ØŒ ÛØ± Ø¨Ø§Ø± Ø¬Ø¨ Ù…Ø§ÚˆÙ„ save ÛÙˆØªØ§ ÛÛ’ (ÛŒÛØ§ÚºØŒ ÛØ± epoch) ÙˆÛ Ù¾Ø³ Ù…Ù†Ø¸Ø± Ù…ÛŒÚº Hub Ù¾Ø± Ø§Ù¾Ù„ÙˆÚˆ ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§Ø³ Ø·Ø±Ø­ØŒ Ø§Ú¯Ø± Ø¶Ø±ÙˆØ±Øª ÛÙˆ ØªÙˆ Ø¢Ù¾ Ø§Ù¾Ù†ÛŒ ØªØ±Ø¨ÛŒØª Ú©Ùˆ Ú©Ø³ÛŒ Ø¯ÙˆØ³Ø±ÛŒ Ù…Ø´ÛŒÙ† Ù¾Ø± resume Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’Û” Ù¾ÙˆØ±ÛŒ ØªØ±Ø¨ÛŒØª Ù…ÛŒÚº Ú©Ø§ÙÛŒ ÙˆÙ‚Øª Ù„Ú¯ØªØ§ ÛÛ’ (Titan RTX Ù¾Ø± ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø§ÛŒÚ© Ú¯Ú¾Ù†Ù¹Û’ Ø³Û’ Ú©Ú†Ú¾ Ø²ÛŒØ§Ø¯Û)ØŒ ØªÙˆ Ø¢Ù¾ Ø§ÛŒÚ© Ú©Ø§ÙÛŒ Ú©Ø§ Ú©Ù¾ Ù„Û’ Ø³Ú©ØªÛ’ ÛÛŒÚº ÛŒØ§ Ú©ÙˆØ±Ø³ Ú©Û’ Ø§ÙÙ† Ø­ØµÙˆÚº Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ù¾Ú‘Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø¢Ù¾ Ú©Ùˆ Ù…Ø´Ú©Ù„ Ù„Ú¯Û’Û” Ù…Ø²ÛŒØ¯ ÛŒÛ Ú©Û Ø¬ÛŒØ³Û’ ÛÛŒ Ù¾ÛÙ„Ø§ epoch Ù…Ú©Ù…Ù„ ÛÙˆØªØ§ ÛÛ’ØŒ Ø¢Ù¾ Hub Ù¾Ø± Ú©Ú†Ú¾ weights Ø§Ù¾Ù„ÙˆÚˆ ÛÙˆØªÛ’ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ø¢Ù¾ Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Û’ ØµÙØ­Û’ Ù¾Ø± Ø§Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¬Ø±Ø¨Ø§Øª Ø´Ø±ÙˆØ¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

{#if fw === 'pt'}

ØªØ±Ø¨ÛŒØª Ù…Ú©Ù…Ù„ ÛÙˆÙ†Û’ Ú©Û’ Ø¨Ø¹Ø¯ØŒ ÛÙ… Ø¢Ø®Ø± Ú©Ø§Ø± Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ evaluation Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº (Ø§ÙˆØ± Ø¯Ø¹Ø§ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ… Ù†Û’ Ø³Ø§Ø±Ø§ compute time Ø¶Ø§Ø¦Ø¹ Ù†Û Ú©ÛŒØ§ ÛÙˆ)Û” `Trainer` Ú©Ø§ `predict()` Ù…ÛŒØªÚ¾Úˆ Ø§ÛŒÚ© tuple ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ø³ Ú©Û’ Ù¾ÛÙ„Û’ Ø¹Ù†Ø§ØµØ± Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº ÛÙˆØªÛŒ ÛÛŒÚº (ÛŒÛØ§ÚºØŒ Ø¢ØºØ§Ø² Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… Ú©Û’ logits Ú©Ø§ Ø¬ÙˆÚ‘Ø§)Û” ÛÙ… Ø§Ø³Û’ Ø§Ù¾Ù†Û’ `compute_metrics()` ÙÙ†Ú©Ø´Ù† Ú©Ùˆ Ø¨Ú¾ÛŒØ¬ Ø¯ÛŒØªÛ’ ÛÛŒÚº:

```python
predictions, _, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets["validation"])
```

{:else}

ØªØ±Ø¨ÛŒØª Ù…Ú©Ù…Ù„ ÛÙˆÙ†Û’ Ú©Û’ Ø¨Ø¹Ø¯ØŒ ÛÙ… Ø¢Ø®Ø± Ú©Ø§Ø± Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ evaluation Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº (Ø§ÙˆØ± Ø¯Ø¹Ø§ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ… Ù†Û’ Ø³Ø§Ø±Ø§ compute time Ø¶Ø§Ø¦Ø¹ Ù†Û Ú©ÛŒØ§ ÛÙˆ)Û” ÛÙ…Ø§Ø±Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ `predict()` Ù…ÛŒØªÚ¾Úˆ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ù… Ú©Ø±Û’ Ú¯Ø§ØŒ Ø§ÙˆØ± Ú†ÙˆÙ†Ú©Û ÛÙ… Ù¾ÛÙ„Û’ ÛÛŒ Ø§ÛŒÚ© `compute_metrics()` ÙÙ†Ú©Ø´Ù† ÚˆÛŒÙØ§Ø¦Ù† Ú©Ø± Ú†Ú©Û’ ÛÛŒÚºØŒ ÛÙ… Ø§ÛŒÚ© ÛÛŒ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº Ø§Ù¾Ù†Û’ Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
predictions = model.predict(tf_eval_dataset)
compute_metrics(
    predictions["start_logits"],
    predictions["end_logits"],
    validation_dataset,
    raw_datasets["validation"],
)
```

{/if}

```python out
{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}
```

Ø¨ÛØª Ø¹Ù…Ø¯Û! Ù…ÙˆØ§Ø²Ù†Û Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ BERT Ø¢Ø±Ù¹ÛŒÚ©Ù„ Ù…ÛŒÚº Ø§Ø³ Ù…Ø§ÚˆÙ„ Ú©Û’ Ù„ÛŒÛ’ baseline scores 80.8 Ø§ÙˆØ± 88.5 Ø±Ù¾ÙˆØ±Ù¹ Ú©ÛŒÛ’ Ú¯Ø¦Û’ ÛÛŒÚºØŒ Ù„ÛÙ°Ø°Ø§ ÛÙ… Ø¯Ø±Ø³Øª Ù…Ù‚Ø§Ù… Ù¾Ø± ÛÛŒÚºÛ”

{#if fw === 'pt'}

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… `push_to_hub()` Ù…ÛŒØªÚ¾Úˆ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û ÛŒÛ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’ Ú©Û Ù…Ø§ÚˆÙ„ Ú©Ø§ ØªØ§Ø²Û ØªØ±ÛŒÙ† ÙˆØ±Ú˜Ù† Ø§Ù¾Ù„ÙˆÚˆ ÛÙˆ Ø¬Ø§Ø¦Û’:

```py
trainer.push_to_hub(commit_message="Training complete")
```

ÛŒÛ Ø§Ø³ commit Ú©Ø§ URL ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ø§Ø¨Ú¾ÛŒ Ú©ÛŒØ§ Ú¯ÛŒØ§ØŒ Ø§Ú¯Ø± Ø¢Ù¾ Ø§Ø³Û’ inspect Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº:

```python out
'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68'
```

`Trainer` Ø§ÛŒÚ© model card Ø¨Ú¾ÛŒ ØªÛŒØ§Ø± Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº ØªÙ…Ø§Ù… evaluation Ù†ØªØ§Ø¦Ø¬ Ø´Ø§Ù…Ù„ ÛÙˆØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§Ø³Û’ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’Û”

{/if}

At this stage, you can use the inference widget on the Model Hub to test the model and share it with your friends, family, and favorite pets. You have successfully fine-tuned a model on a question answering task -- congratulations!

<Tip>

âœï¸ **Your turn!** Try another model architecture to see if it performs better on this task!

</Tip>

{#if fw === 'pt'}

Ø§Ú¯Ø± Ø¢Ù¾ training loop Ù…ÛŒÚº Ù…Ø²ÛŒØ¯ Ú¯ÛØ±Ø§Ø¦ÛŒ Ù…ÛŒÚº Ø¬Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø§Ø¨ ÛÙ… Ø¢Ù¾ Ú©Ùˆ ÙˆÛÛŒ Ú©Ø§Ù… ğŸ¤— Accelerate Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©Ø±Ù†Û’ Ú©Ø§ Ø·Ø±ÛŒÙ‚Û Ø¯Ú©Ú¾Ø§ØªÛ’ ÛÛŒÚºÛ”

## Ø§ÛŒÚ© Ú©Ø³Ù¹Ù… Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ù„ÙˆÙ¾[[a-custom-training-loop]]

Ø¢Ø¦ÛŒÛ’ Ø§Ø¨ Ù¾ÙˆØ±Û’ training loop Ù¾Ø± Ø§ÛŒÚ© Ù†Ø¸Ø± ÚˆØ§Ù„ØªÛ’ ÛÛŒÚºØŒ ØªØ§Ú©Û Ø¢Ù¾ Ø¢Ø³Ø§Ù†ÛŒ Ø³Û’ Ø§ÙÙ† Ø­ØµÙˆÚº Ú©Ùˆ Ú©Ø³Ù¹Ù…Ø§Ø¦Ø² Ú©Ø± Ø³Ú©ÛŒÚº Ø¬Ù† Ú©ÛŒ Ø¢Ù¾ Ú©Ùˆ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û” ÛŒÛ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ [Chapter 3](/course/chapter3/4) Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ training loop Ø¬ÛŒØ³Ø§ ÛÛŒ Ù„Ú¯Û’ Ú¯Ø§ØŒ Ø³ÙˆØ§Ø¦Û’ evaluation loop Ú©Û’Û” Ø§Ø¨ Ú†ÙˆÙ†Ú©Û ÛÙ… `Trainer` Ú©Ù„Ø§Ø³ Ú©ÛŒ Ù‚ÛŒØ¯ Ù…ÛŒÚº Ù†ÛÛŒÚº ÛÛŒÚºØŒ ÛÙ… Ø¨Ø§Ù‚Ø§Ø¹Ø¯Ú¯ÛŒ Ø³Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ evaluation Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’Û”

### ØªØ±Ø¨ÛŒØª Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ú©Ú†Ú¾ ØªÛŒØ§Ø± Ú©Ø±Ù†Ø§[[preparing-everything-for-training]]

Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ datasets Ø³Û’ `DataLoader`s Ø¨Ù†Ø§Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û” ÛÙ… Ø§ÙÙ† datasets Ú©Ø§ format `"torch"` Ù¾Ø± Ø³ÛŒÙ¹ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± validation set Ù…ÛŒÚº Ø§ÙÙ† columns Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù†ÛÛŒÚº Ú©Ø±ØªØ§Û” Ù¾Ú¾Ø±ØŒ ÛÙ… Transformers Ú©ÛŒ Ø·Ø±Ù Ø³Û’ ÙØ±Ø§ÛÙ… Ú©Ø±Ø¯Û `default_data_collator` Ú©Ùˆ `collate_fn` Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± training set Ú©Ùˆ shuffle Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù…Ú¯Ø± validation set Ú©Ùˆ Ù†ÛÛŒÚº:

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_dataset.set_format("torch")
validation_set = validation_dataset.remove_columns(["example_id", "offset_mapping"])
validation_set.set_format("torch")

train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=default_data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    validation_set, collate_fn=default_data_collator, batch_size=8
)
```

Ù¾Ú¾Ø± ÛÙ… Ø§Ù¾Ù†Ø§ Ù…Ø§ÚˆÙ„ Ø¯ÙˆØ¨Ø§Ø±Û instantiate Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ØªØ§Ú©Û ÛŒÛ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’ Ú©Û ÛÙ… Ù¾Ú†Ú¾Ù„ÛŒ fine-tuning Ø¬Ø§Ø±ÛŒ Ù†ÛÛŒÚº Ø±Ú©Ú¾ Ø±ÛÛ’ Ø¨Ù„Ú©Û BERT Ú©Û’ Ù¾Ø±ÛŒ Ù¹Ø±ÛŒÙ†Úˆ Ù…Ø§ÚˆÙ„ Ø³Û’ Ø¯ÙˆØ¨Ø§Ø±Û Ø´Ø±ÙˆØ¹ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº:

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

Ù¾Ú¾Ø± ÛÙ…ÛŒÚº Ø§ÛŒÚ© Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²Ø± Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆÚ¯ÛŒÛ” Ø­Ø³Ø¨ Ù…Ø¹Ù…ÙˆÙ„ØŒ ÛÙ… Ú©Ù„Ø§Ø³Ú© `AdamW` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø¬Ùˆ Ø§ÛŒÚˆÙ… Ú©ÛŒ Ø·Ø±Ø­ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† Ø§Ø³ Ù…ÛŒÚº ÙˆÛŒÙ¹ ÚˆÛŒÚ©Û’ Ú©Û’ Ø§Ø·Ù„Ø§Ù‚ Ú©Û’ Ø·Ø±ÛŒÙ‚Û’ Ù…ÛŒÚº Ø§ÛŒÚ© Ø§ØµÙ„Ø§Ø­ Ø´Ø§Ù…Ù„ ÛÛ’:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Ø¬Ø¨ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ ÛŒÛ ØªÙ…Ø§Ù… Ø¢Ø¨Ø¬ÛŒÚ©Ù¹Ø³ ÛÙˆÚºØŒ ØªÙˆ ÛÙ… Ø§Ù†ÛÛŒÚº `accelerator.prepare()` Ù…ÛŒØªÚ¾Úˆ Ù…ÛŒÚº Ø¨Ú¾ÛŒØ¬ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” ÛŒØ§Ø¯ Ø±Ú©Ú¾ÛŒÚº Ú©Û Ø§Ú¯Ø± Ø¢Ù¾ Colab Ù†ÙˆÙ¹ Ø¨Ú© Ù…ÛŒÚº TPUs Ù¾Ø± Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ ÛŒÛ Ø³Ø§Ø±Ø§ Ú©ÙˆÚˆ Ø§ÛŒÚ© Ù¹Ø±ÛŒÙ†Ù†Ú¯ ÙÙ†Ú©Ø´Ù† Ù…ÛŒÚº Ù…Ù†ØªÙ‚Ù„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ØŒ Ø§ÙˆØ± Ø§ÛŒØ³Ø§ Ú©ÙˆØ¦ÛŒ Ø³ÛŒÙ„ Ù†ÛÛŒÚº Ú†Ù„Ø§Ù†Ø§ Ú†Ø§ÛÛŒÛ’ Ø¬Ùˆ `Accelerator` Ú©Ùˆ Ø§Ù†Ø³Ù¹ÛŒÙ¹ÛŒÙˆÙ¹ Ú©Ø±Û’Û” ÛÙ… `Accelerator` Ù…ÛŒÚº `fp16=True` Ù¾Ø§Ø³ Ú©Ø± Ú©Û’ Ù…Ú©Ø³Úˆ-Ù¾Ø±ÛŒØ³ÛŒÚ˜Ù† Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº (ÛŒØ§ Ø§Ú¯Ø± Ø¢Ù¾ Ú©ÙˆÚˆ Ú©Ùˆ Ø§Ø³Ú©Ø±Ù¾Ù¹ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ú†Ù„Ø§ Ø±ÛÛ’ ÛÛŒÚºØŒ ØªÙˆ ğŸ¤— Accelerate `config` Ú©Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ø³ÛŒÙ¹ Ú©Ø± Ù„ÛŒÚº)Û”

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

Ø¬ÛŒØ³Ø§ Ú©Û Ø¢Ù¾ Ù¾Ú†Ú¾Ù„Û’ Ø­ØµÙˆÚº Ø³Û’ Ø¬Ø§Ù†ØªÛ’ ÛÛŒÚºØŒ ÛÙ… `train_dataloader` Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Ùˆ ØµØ±Ù Ø§Ø³ÛŒ ÙˆÙ‚Øª Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ø§Ø³Ù¹ÛŒÙ¾Ø³ Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ú©Ø§Ù„Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ø¨ ÛŒÛ `accelerator.prepare()` Ù…ÛŒØªÚ¾Úˆ Ø³Û’ Ú¯Ø²Ø± Ú†Ú©Ø§ ÛÙˆÛ” ÛÙ… ÙˆÛÛŒ Ù„Ú©ÛŒØ±ÛŒ Ø´ÛŒÚˆÙˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù¾Ú†Ú¾Ù„Û’ Ø­ØµÙˆÚº Ù…ÛŒÚº ØªÚ¾Ø§Û”

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```
ÛÙ…Ø§Ø±Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ ÛØ¨ Ù¾Ø± Ø§Ù¾ Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº Ø§ÛŒÚ© `Repository` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ú©Ùˆ ÙˆØ±Ú©Ù†Ú¯ ÙÙˆÙ„ÚˆØ± Ù…ÛŒÚº Ø¨Ù†Ø§Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§Ú¯Ø± Ø¢Ù¾ Ù¾ÛÙ„Û’ Ø³Û’ Ù„Ø§Ú¯ Ø§Ù† Ù†ÛÛŒÚº ÛÛŒÚºØŒ ØªÙˆ Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Hugging Face Hub Ù…ÛŒÚº Ù„Ø§Ú¯ Ø§Ù† Ú©Ø±ÛŒÚºÛ” ÛÙ… Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ Ú©Ø§ Ù†Ø§Ù… Ø§Ø³ Ù…Ø§ÚˆÙ„ ID Ø³Û’ Ø·Û’ Ú©Ø±ÛŒÚº Ú¯Û’ Ø¬Ùˆ ÛÙ… Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø¯ÛŒÙ†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº (Ø¢Ù¾ `repo_name` Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ù¾Ø³Ù†Ø¯ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø¯Ù„ Ø³Ú©ØªÛ’ ÛÛŒÚºØ› Ø¨Ø³ Ø§Ø³ Ù…ÛŒÚº Ø¢Ù¾ Ú©Ø§ ÛŒÙˆØ²Ø± Ù†ÛŒÙ… Ø´Ø§Ù…Ù„ ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’ØŒ Ø¬Ùˆ Ú©Û `get_full_repo_name()` ÙÙ†Ú©Ø´Ù† Ú©Ø±ØªØ§ ÛÛ’)Û”

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-squad-accelerate'
```

Ù¾Ú¾Ø± ÛÙ… Ø§Ø³ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ Ú©Ùˆ Ø§ÛŒÚ© Ù…Ù‚Ø§Ù…ÛŒ ÙÙˆÙ„ÚˆØ± Ù…ÛŒÚº Ú©Ù„ÙˆÙ† Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ø± ÛŒÛ Ù¾ÛÙ„Û’ Ø³Û’ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ØŒ ØªÙˆ ÛŒÛ Ù…Ù‚Ø§Ù…ÛŒ ÙÙˆÙ„ÚˆØ± Ø§Ø³ Ø±ÛŒÙ¾ÙˆØ²Ù¹Ø±ÛŒ Ú©ÛŒ Ø§ÛŒÚ© Ú©Ù„ÙˆÙ† ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’ Ø¬Ø³ Ù¾Ø± ÛÙ… Ú©Ø§Ù… Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºÛ”

```py
output_dir = "bert-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ÛÙ… Ø§Ø¨ `output_dir` Ù…ÛŒÚº Ù…Ø­ÙÙˆØ¸ Ú©ÛŒ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„ÛŒ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ú†ÛŒØ² Ú©Ùˆ `repo.push_to_hub()` Ù…ÛŒØªÚ¾Úˆ Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø± Ú©Û’ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ø³Û’ ÛÙ…ÛŒÚº ÛØ± epoch Ú©Û’ Ø§Ø®ØªØªØ§Ù… Ù¾Ø± intermediate Ù…Ø§ÚˆÙ„Ø² Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ù…Ù„Û’ Ú¯ÛŒÛ”

## ØªØ±Ø¨ÛŒØªÛŒ Ù„ÙˆÙ¾[[training-loop]]

Ø§Ø¨ ÛÙ… Ù…Ú©Ù…Ù„ ØªØ±Ø¨ÛŒØªÛŒ Ù„ÙˆÙ¾ Ù„Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚºÛ” progress bar Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Û’ Ø¨Ø¹Ø¯ØŒ Ù„ÙˆÙ¾ Ù…ÛŒÚº ØªÛŒÙ† Ø­ØµÛ’ Ø´Ø§Ù…Ù„ ÛÛŒÚº:

- **ØªØ±Ø¨ÛŒØª:** Ø¬Ùˆ Ú©Û `train_dataloader` Ù¾Ø± Ú©Ù„Ø§Ø³Ú© iterationØŒ Ù…Ø§ÚˆÙ„ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ forward passØŒ Ù¾Ú¾Ø± backward pass Ø§ÙˆØ± optimizer step Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÛ’Û”
- **Ø¬Ø§Ù†Ú† (Evaluation):** Ø¬Ø³ Ù…ÛŒÚº ÛÙ… ØªÙ…Ø§Ù… `start_logits` Ø§ÙˆØ± `end_logits` Ú©ÛŒ Ù‚Ø¯Ø±ÛŒÚº Ø§Ú©Ù¹Ú¾ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ù¾Ú¾Ø± Ø§Ù†ÛÛŒÚº NumPy arrays Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø¬Ø¨ evaluation loop Ø®ØªÙ… ÛÙˆ Ø¬Ø§Ø¦Û’ ØªÙˆ ÛÙ… ØªÙ…Ø§Ù… Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ concatenate Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û ÛÙ…ÛŒÚº truncate Ú©Ø±Ù†Ø§ Ù¾Ú‘ØªØ§ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û `Accelerator` Ù†Û’ ÛØ± process Ù…ÛŒÚº Ø§ÛŒÚ© Ø¬ÛŒØ³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…ÛŒÚº examples Ø±Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¢Ø®Ø± Ù…ÛŒÚº Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Û’ Ø´Ø§Ù…Ù„ Ú©Ø± Ø¯Ø¦ÛŒÛ’ ÛÙˆØªÛ’ ÛÛŒÚºÛ”
- **Ù…Ø­ÙÙˆØ¸ Ú©Ø±Ù†Ø§ Ø§ÙˆØ± Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§:** Ø¬ÛØ§Úº Ù¾ÛÙ„Û’ ÛÙ… Ù…Ø§ÚˆÙ„ Ø§ÙˆØ± tokenizer Ú©Ùˆ Ù…Ø­ÙÙˆØ¸ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ù¾Ú¾Ø± `repo.push_to_hub()` Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø¬ÛŒØ³Ø§ Ú©Û Ù¾ÛÙ„Û’ Ú©ÛŒØ§ Ú¯ÛŒØ§ØŒ ÛÙ… `blocking=False` Ø¢Ø±Ú¯ÛŒÙˆÙ…Ù†Ù¹ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û ğŸ¤— Hub Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ùˆ asynchronous process Ù…ÛŒÚº Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ØŒ Ø¬Ø³ Ø³Û’ ØªØ±Ø¨ÛŒØª Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø¬Ø§Ø±ÛŒ Ø±ÛØªÛŒ ÛÛ’ Ø§ÙˆØ± ÛŒÛ (Ø·ÙˆÛŒÙ„) Ø¹Ù…Ù„ Ù¾Ø³ Ù…Ù†Ø¸Ø± Ù…ÛŒÚº Ú†Ù„ØªØ§ Ø±ÛØªØ§ ÛÛ’Û”

Ø°ÛŒÙ„ Ù…ÛŒÚº ØªØ±Ø¨ÛŒØªÛŒ Ù„ÙˆÙ¾ Ú©Ø§ Ù…Ú©Ù…Ù„ Ú©ÙˆÚˆ Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # ØªØ±Ø¨ÛŒØª
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Ø¬Ø§Ù†Ú† (Evaluation)
    model.eval()
    start_logits = []
    end_logits = []
    accelerator.print("Evaluation!")
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())
        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())

    start_logits = np.concatenate(start_logits)
    end_logits = np.concatenate(end_logits)
    start_logits = start_logits[: len(validation_dataset)]
    end_logits = end_logits[: len(validation_dataset)]

    metrics = compute_metrics(
        start_logits, end_logits, validation_dataset, raw_datasets["validation"]
    )
    print(f"epoch {epoch}:", metrics)

    # Ù…Ø­ÙÙˆØ¸ Ú©Ø±Ù†Ø§ Ø§ÙˆØ± Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

Ø§Ú¯Ø± ÛŒÛ Ø¢Ù¾ Ú©ÛŒ Ù¾ÛÙ„ÛŒ Ø¨Ø§Ø± ÛÛ’ Ú©Û Ø¢Ù¾ Ù†Û’ ğŸ¤— Accelerate Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø­ÙÙˆØ¸ Ú©ÛŒØ§ ÛÙˆØ§ Ù…Ø§ÚˆÙ„ Ø¯ÛŒÚ©Ú¾Ø§ ÛÛ’ØŒ ØªÙˆ Ø¢Ø¦ÛŒÛ’ Ø§Ù† ØªÛŒÙ† Ù„Ø§Ø¦Ù†ÙˆÚº Ú©Ùˆ ØºÙˆØ± Ø³Û’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

- **Ù¾ÛÙ„ÛŒ Ù„Ø§Ø¦Ù†:** Ø³ÛŒØ¯Ú¾ÛŒ Ø³Ø§Ø¯ÛŒ Ø¨Ø§Øª ÛÛ’: ÛŒÛ ØªÙ…Ø§Ù… processes Ú©Ùˆ ÛŒÛ Ø¨ØªØ§ØªÛŒ ÛÛ’ Ú©Û ÙˆÛ Ø§Ø³ Ù…Ù‚Ø§Ù… ØªÚ© Ù¾ÛÙ†Ú† Ø¬Ø§Ø¦ÛŒÚº Ø¬Ø¨ ØªÚ© Ú©Û Ø³Ø¨ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ù†Û ÛÙˆÚºØŒ ØªØ§Ú©Û Ù…Ø­ÙÙˆØ¸ Ú©Ø±Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ ÛØ± process Ù…ÛŒÚº Ø§ÛŒÚ© ÛÛŒ Ù…Ø§ÚˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÛ”
- **Ø¯ÙˆØ³Ø±ÛŒ Ù„Ø§Ø¦Ù†:** ÛŒÛØ§Úº ÛÙ… `unwrapped_model` Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ú©Û ÙˆÛ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù…Ø§ÚˆÙ„ ÛÛ’ Ø¬Ùˆ ÛÙ… Ù†Û’ ØªØ¹Ø±ÛŒÙ Ú©ÛŒØ§ ØªÚ¾Ø§Û” `accelerator.prepare()` Ù…Ø§ÚˆÙ„ Ú©Ùˆ distributed training Ú©Û’ Ù„ÛŒÛ’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ø³Û’ Ø§Ø¨ Ù…Ø§ÚˆÙ„ Ù…ÛŒÚº `save_pretrained()` Ù…ÛŒØªÚ¾Úˆ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº ÛÙˆØªØ§Ø› `accelerator.unwrap_model()` Ù…ÛŒØªÚ¾Úˆ Ø§Ø³ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Ùˆ ÙˆØ§Ù¾Ø³ Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’Û”
- **ØªÛŒØ³Ø±ÛŒ Ù„Ø§Ø¦Ù†:** Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… `save_pretrained()` Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ù„ÛŒÚ©Ù† Ø§Ø³ Ø¨Ø§Øª Ú©ÛŒ ÛØ¯Ø§ÛŒØª Ø¯ÛŒØªÛ’ ÛÛŒÚº Ú©Û ÙˆÛ `accelerator.save()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Û’ Ø¨Ø¬Ø§Ø¦Û’ `torch.save()` Ú©Û’Û”

Ø§ÛŒÚ© Ø¨Ø§Ø± ÛŒÛ Ø¹Ù…Ù„ Ù…Ú©Ù…Ù„ ÛÙˆ Ø¬Ø§Ø¦Û’ØŒ ØªÙˆ Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ Ù…Ø§ÚˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÚ¯Ø§ Ø¬Ùˆ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø§Ø³ÛŒ Ø·Ø±Ø­ Ù†ØªØ§Ø¦Ø¬ Ù¾ÛŒØ¯Ø§ Ú©Ø±ØªØ§ ÛÛ’ Ø¬ÛŒØ³Û’ `Trainer` Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ±Ø¨ÛŒØª Ø¯ÛŒØ§ Ú¯ÛŒØ§ Ù…Ø§ÚˆÙ„Û” Ø¢Ù¾ [*huggingface-course/bert-finetuned-squad-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate) Ù¾Ø± Ø§Ø³ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ú†ÛŒÚ© Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§ÙˆØ± Ø§Ú¯Ø± Ø¢Ù¾ ØªØ±Ø¨ÛŒØªÛŒ Ù„ÙˆÙ¾ Ù…ÛŒÚº Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù‚Ø³Ù… Ú©ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒØ§Úº Ø¢Ø²Ù…Ø§Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ø§ÙˆÙ¾Ø± Ø¯Ú©Ú¾Ø§Ø¦Û’ Ú¯Ø¦Û’ Ú©ÙˆÚˆ Ù…ÛŒÚº Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª ØªØ±Ù…ÛŒÙ… Ú©Ø± Ú©Û’ ÛŒÛ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº!

## ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ú©ÛŒØ§ ÛÙˆØ§ Ù…Ø§ÚˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§[[using-the-fine-tuned-model]]

ÛÙ… Ù¾ÛÙ„Û’ ÛÛŒ Ø¢Ù¾ Ú©Ùˆ Ø¯Ú©Ú¾Ø§ Ú†Ú©Û’ ÛÛŒÚº Ú©Û Ø¢Ù¾ Model Hub Ù¾Ø± inference widget Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø§Ù¾Ù†Û’ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ú©ÛŒØ³Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³Û’ Ù„ÙˆÚ©Ù„ Ø³Ø·Ø­ Ù¾Ø± `pipeline` Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¢Ù¾ Ú©Ùˆ ØµØ±Ù Ù…Ø§ÚˆÙ„ Ú©Ø§ identifier ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Ø§ ÛÙˆØªØ§ ÛÛ’:

```py
from transformers import pipeline

# Ø§Ø³ Ú©Ùˆ Ø§Ù¾Ù†Û’ checkpoint Ú©Û’ Ø³Ø§ØªÚ¾ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº
model_checkpoint = "huggingface-course/bert-finetuned-squad"
question_answerer = pipeline("question-answering", model=model_checkpoint)

context = """
ğŸ¤— Transformers Ú©Ùˆ ØªÛŒÙ† Ø³Ø¨ Ø³Û’ Ù…Ø´ÛÙˆØ± deep learning libraries â€” Jax, PyTorch Ø§ÙˆØ± TensorFlow â€” Ú©ÛŒ Ø­Ù…Ø§ÛŒØª Ø­Ø§ØµÙ„ ÛÛ’ØŒ Ø¬Ù† Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø§ÛŒÚ© seamless Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù† Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û”
Ø§ÛŒÚ© Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ú©Ø±Ù†Ø§ Ø§ÙˆØ± Ø¯ÙˆØ³Ø±Û’ Ú©Û’ Ù„ÛŒÛ’ inference Ú©Û’ Ù„ÛŒÛ’ Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§ Ø¨ÛØª Ø³ÛŒØ¯Ú¾Ø§ Ø³Ø§Ø¯Û ÛÛ’Û”
"""
question = "ğŸ¤— Transformers Ú©ÛŒ Ø­Ù…Ø§ÛŒØª Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ deep learning libraries Ú©ÙˆÙ† Ø³ÛŒ ÛÛŒÚºØŸ"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.9979003071784973,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Ø²Ø¨Ø±Ø¯Ø³Øª! ÛÙ…Ø§Ø±Ø§ Ù…Ø§ÚˆÙ„ Ø§Ø³ÛŒ Ø·Ø±Ø­ Ú©Ø§Ù… Ú©Ø± Ø±ÛØ§ ÛÛ’ Ø¬ÛŒØ³Û’ Ø§Ø³ pipeline Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒÙØ§Ù„Ù¹ Ù…Ø§ÚˆÙ„ Ú©Ø±ØªØ§ ÛÛ’!