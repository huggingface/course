<FrameworkSwitchCourse {fw} />

# سوالات کے جوابات[[question-answering]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section7_tf.ipynb"},
]} />

{/if}

سوالات کے جوابات پر نظر ڈالنے کا وقت آگیا ہے! یہ کام کئی شکلوں میں آتا ہے، لیکن اس حصے میں جس پر ہم توجہ مرکوز کریں گے اسے *extractive* سوالات کے جوابات کہا جاتا ہے۔ اس میں کسی دستاویز کے بارے میں سوالات پوچھنا اور دستاویز کے اندر _متن کے ٹکڑوں_ کی صورت میں جوابات کی شناخت کرنا شامل ہے۔

<Youtube id="ajPx5LwJD-I"/>

ہم [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) پر ایک BERT ماڈل کو fine-tune کریں گے، جو کہ وکیپیڈیا مضامین کے سیٹ پر crowdworkers کی جانب سے پوچھے گئے سوالات پر مبنی ہے۔ اس سے ہمیں ایک ایسا ماڈل ملے گا جو اس قسم کی پیش گوئیاں کر سکے گا:

<iframe src="https://course-demos-bert-finetuned-squad.hf.space" frameBorder="0" height="450" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

یہ دراصل اس ماڈل کو ظاہر کر رہا ہے جس کی تربیت اس سیکشن میں دکھائے گئے کوڈ کی مدد سے کی گئی اور Hub پر اپلوڈ کیا گیا ہے۔ آپ اسے [یہاں](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F) چیک کر سکتے ہیں۔

<Tip>

💡 BERT جیسے encoder-only ماڈلز عموماً ایسے factoid سوالات کے جوابات نکالنے میں بہت اچھے ہوتے ہیں جیسے "Transformer architecture کس نے ایجاد کی؟" لیکن جب انہیں کھلے اختیاری سوالات دیے جائیں جیسے "آسمان نیلا کیوں ہے؟" تو یہ اتنا مؤثر نہیں ہوتے۔ ان زیادہ مشکل صورتوں میں، encoder-decoder ماڈلز جیسے T5 اور BART عموماً معلومات کو synthesis کرنے کے لیے استعمال ہوتے ہیں، جو کہ [text summarization](/course/chapter7/5) سے کافی ملتے جلتے ہیں۔ اگر آپ generative سوالات کے جوابات میں دلچسپی رکھتے ہیں، تو ہم آپ کو ہماری [demo](https://yjernite.github.io/lfqa.html) دیکھنے کی تجویز دیتے ہیں جو [ELI5 dataset](https://huggingface.co/datasets/eli5) پر مبنی ہے۔

</Tip>

## ڈیٹا کی تیاری[[preparing-the-data]]

وہ dataset جو extractive سوالات کے جوابات کے لیے ایک تعلیمی معیار کے طور پر سب سے زیادہ استعمال ہوتا ہے وہ [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) ہے، لہٰذا یہاں ہم اسی کا استعمال کریں گے۔ اس کے علاوہ ایک زیادہ مشکل [SQuAD v2](https://huggingface.co/datasets/squad_v2) benchmark بھی موجود ہے، جس میں ایسے سوالات شامل ہیں جن کے جوابات موجود نہیں ہوتے۔ جب تک کہ آپ کے اپنے dataset میں contexts، questions، اور answers کے لیے کالم موجود ہیں، آپ نیچے دیے گئے مراحل کو آسانی سے اپنا سکتے ہیں۔

### SQuAD dataset[[the-squad-dataset]]

ہمیشہ کی طرح، ہم `load_dataset()` کی بدولت صرف ایک قدم میں dataset کو ڈاؤن لوڈ اور cache کر سکتے ہیں:

```py
from datasets import load_dataset

raw_datasets = load_dataset("squad")
```

اس کے بعد، ہم اس آبجیکٹ کو دیکھ کر SQuAD dataset کے بارے میں مزید جان سکتے ہیں:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})
```

ایسا لگتا ہے کہ ہمارے پاس `context`, `question`, اور `answers` کے فیلڈز موجود ہیں، تو آئیے اپنے training set کے پہلے عنصر کے لیے انہیں پرنٹ کرتے ہیں:

```py
print("Context: ", raw_datasets["train"][0]["context"])
print("Question: ", raw_datasets["train"][0]["question"])
print("Answer: ", raw_datasets["train"][0]["answers"])
```

```python out
Context: 'Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'
Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'
Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
```

`context` اور `question` فیلڈز استعمال کرنے میں بہت سیدھے سادے ہیں۔ `answers` فیلڈ تھوڑا پیچیدہ ہے کیونکہ یہ ایک dictionary ہے جس میں دو فیلڈز شامل ہیں جو دونوں lists کی صورت میں ہیں۔ یہ وہ فارمیٹ ہے جس کی توقع evaluation کے دوران `squad` metric کرے گا؛ اگر آپ اپنا ڈیٹا استعمال کر رہے ہیں تو ضروری نہیں کہ جوابات کو بالکل اسی فارمیٹ میں رکھیں۔ `text` فیلڈ واضح ہے، اور `answer_start` فیلڈ ہر جواب کے context میں ابتدائی کردار (character index) کو ظاہر کرتی ہے۔

تربیت کے دوران، صرف ایک ممکنہ جواب ہوتا ہے۔ ہم `Dataset.filter()` میتھڈ کا استعمال کرتے ہوئے اس بات کی تصدیق کر سکتے ہیں:

```py
raw_datasets["train"].filter(lambda x: len(x["answers"]["text"]) != 1)
```

```python out
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
```

لیکن evaluation کے دوران، ہر sample کے لیے متعدد ممکنہ جوابات ہو سکتے ہیں، جو ایک جیسے یا مختلف ہو سکتے ہیں:

```py
print(raw_datasets["validation"][0]["answers"])
print(raw_datasets["validation"][2]["answers"])
```

```python out
{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}
{'text': ['Santa Clara, California', "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], 'answer_start': [403, 355, 355]}
```

ہم evaluation script میں زیادہ تفصیل میں نہیں جائیں گے کیونکہ اسے ہمارے لیے 🤗 Datasets metric میں لپیٹ دیا گیا ہے، لیکن مختصر یہ کہ کچھ سوالات کے کئی ممکنہ جوابات ہوتے ہیں، اور یہ script پیش گوئی کیے گئے جواب کا موازنہ تمام قابل قبول جوابات سے کرے گا اور بہترین اسکور لے گا۔ مثال کے طور پر، اگر ہم index 2 کے sample کو دیکھیں:

```py
print(raw_datasets["validation"][2]["context"])
print(raw_datasets["validation"][2]["question"])
```

```python out
'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.'
'Where did Super Bowl 50 take place?'
```

ہم دیکھ سکتے ہیں کہ جواب واقعی میں ان تین ممکنہ جوابات میں سے ایک ہو سکتا ہے جو ہم نے پہلے دیکھے تھے۔

### تربیتی ڈیٹا کی پروسیسنگ[[processing-the-training-data]]

<Youtube id="qgaM0weJHpA"/>

آئیے تربیتی ڈیٹا کی preprocessing سے شروع کرتے ہیں۔ سب سے مشکل حصہ یہ ہوگا کہ سوال کے جواب کے لیے labels پیدا کیے جائیں، جو context کے اندر جواب کے tokens کے شروع اور اختتام کی پوزیشنز ہوں گی۔

لیکن خود کو جلد بازی میں مت ڈالیں۔ سب سے پہلے، ہمیں input میں موجود متن کو IDs میں تبدیل کرنا ہے تاکہ ماڈل اسے سمجھ سکے، اور اس کے لیے ہم tokenizer کا استعمال کریں گے:

```py
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

جیسا کہ پہلے ذکر کیا گیا تھا، ہم ایک BERT ماڈل کو فائن ٹون کریں گے، لیکن آپ کوئی اور ماڈل بھی استعمال کر سکتے ہیں جب تک کہ اس میں ایک تیز ٹوکنائزر نافذ ہو۔ آپ [اس بڑے جدول](https://huggingface.co/transformers/#supported-frameworks) میں وہ تمام آرکیٹیکچرز دیکھ سکتے ہیں جن کے تیز ورژن دستیاب ہیں، اور یہ جانچنے کے لیے کہ آیا آپ جو `tokenizer` آبجیکٹ استعمال کر رہے ہیں وہ واقعی 🤗 Tokenizers کے ذریعے معاونت یافتہ ہے، آپ اس کی `is_fast` خصوصیت کو دیکھ سکتے ہیں۔

```py
tokenizer.is_fast
```

```python out
True
```

ہم اپنے ٹوکنائزر کو سوال اور سیاق و سباق ایک ساتھ دے سکتے ہیں، اور یہ خود بخود خاص ٹوکنز داخل کرے گا تاکہ ایک جملہ اس طرح تشکیل دیا جا سکے:

```
[CLS] question [SEP] context [SEP]
```

آئیے دوبارہ تصدیق کریں:

```py
context = raw_datasets["train"][0]["context"]
question = raw_datasets["train"][0]["question"]

inputs = tokenizer(question, context)
tokenizer.decode(inputs["input_ids"])
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '
'the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin '
'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '
'upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred '
'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '
'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '
'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '
'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

لیبلز پھر اس جواب کے آغاز اور اختتام والے tokens کے انڈیکس ہوں گے، اور ماڈل پر یہ ذمہ داری عائد ہوگی کہ وہ input میں موجود ہر token کے لیے ایک شروع اور ایک اختتامی logit کی پیش گوئی کرے، جبکہ نظریاتی لیبلز درج ذیل ہوں گے:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels-dark.svg" alt="One-hot encoded labels for question answering."/>
</div>

اس صورتِ حال میں context زیادہ لمبا نہیں ہے، مگر dataset کی کچھ مثالوں میں ایسے بہت طویل context موجود ہیں جو ہم نے جو زیادہ سے زیادہ لمبائی مقرر کی ہے (اس صورت میں 384) اسے تجاوز کر جائیں گے۔ جیسا کہ ہم نے [Chapter 6](/course/chapter6/4) میں `question-answering` pipeline کے اندرونی حصوں کا جائزہ لیا، ہم طویل context کے ساتھ نمٹنے کے لیے ایک ہی sample سے متعدد training features تخلیق کریں گے، جن کے درمیان ایک sliding window ہوگا۔

موجودہ مثال کو دیکھتے ہوئے، ہم لمبائی کو 100 تک محدود کر سکتے ہیں اور 50 tokens کا sliding window استعمال کر سکتے ہیں۔ یاددہانی کے طور پر، ہم استعمال کرتے ہیں:

- `max_length` تاکہ زیادہ سے زیادہ لمبائی مقرر کی جا سکے (یہاں 100)
- `truncation="only_second"` تاکہ جب سوال اپنے context کے ساتھ بہت لمبا ہو جائے تو context (جو دوسرے مقام پر ہے) کو truncate کیا جا سکے
- `stride` تاکہ دو مسلسل حصوں کے درمیان overlapping tokens کی تعداد مقرر کی جا سکے (یہاں 50)
- `return_overflowing_tokens=True` تاکہ tokenizer کو بتایا جا سکے کہ ہمیں overflow ہونے والے tokens درکار ہیں

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

جیسا کہ ہم دیکھ سکتے ہیں، ہماری مثال کو چار inputs میں تقسیم کر دیا گیا ہے، جن میں سے ہر ایک میں سوال اور context کا کچھ حصہ شامل ہے۔ نوٹ کریں کہ سوال کا جواب ("Bernadette Soubirous") صرف تیسرے اور آخری inputs میں ظاہر ہوتا ہے، لہٰذا طویل context کے اس طریقے سے نمٹنے کے باعث ہم کچھ ایسے training examples تخلیق کریں گے جن میں جواب context میں شامل نہیں ہوگا۔ ان مثالوں کے لیے، لیبلز `start_position = end_position = 0` ہوں گے (یعنی ہم `[CLS]` token کی پیش گوئی کریں گے)۔ ہم ان بدقسمت مثالوں میں بھی یہ لیبلز سیٹ کر دیں گے جہاں جواب truncate ہو گیا ہو اور صرف اس کا آغاز (یا اختتام) موجود ہو۔ ان مثالوں کے لیے جہاں جواب مکمل طور پر context میں موجود ہو، لیبلز وہ token index ہوں گے جہاں جواب شروع ہوتا ہے اور وہ index جہاں جواب ختم ہوتا ہے۔

dataset ہمیں context میں جواب کے آغاز کا character index فراہم کرتا ہے، اور جواب کی لمبائی شامل کر کے ہم context میں جواب کے اختتام کا character معلوم کر سکتے ہیں۔ ان کو token indices میں تبدیل کرنے کے لیے، ہمیں offset mappings استعمال کرنے ہوں گے جن کا ہم نے [Chapter 6](/course/chapter6/4) میں مطالعہ کیا تھا۔ ہم اپنے tokenizer کو یہ واپسی offset mappings دینے کے لیے `return_offsets_mapping=True` پاس کر سکتے ہیں:

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
inputs.keys()
```

```python out
dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])
```

جیسا کہ ہم دیکھتے ہیں، ہمیں معمول کے مطابق input IDs، token type IDs، اور attention mask کے ساتھ ساتھ مطلوبہ offset mapping اور ایک اضافی key `overflow_to_sample_mapping` بھی مل گئی ہے۔ متعلقہ value اس وقت ہمارے کام آئے گی جب ہم ایک ساتھ کئی متن tokenize کریں گے (جو ہمیں اس بات سے فائدہ پہنچتا ہے کہ ہمارا tokenizer Rust سے چل رہا ہے)۔ چونکہ ایک sample سے کئی features آ سکتے ہیں، یہ ہر feature کو اُس مثال سے میپ کرتا ہے جس سے وہ وجود میں آیا ہے۔ چونکہ یہاں ہم نے صرف ایک مثال tokenize کی ہے، ہمیں `0` کی ایک list ملتی ہے:

```py
inputs["overflow_to_sample_mapping"]
```

```python out
[0, 0, 0, 0]
```

لیکن اگر ہم مزید مثالیں tokenize کریں، تو یہ زیادہ مفید ہو جائے گا:

```py
inputs = tokenizer(
    raw_datasets["train"][2:6]["question"],
    raw_datasets["train"][2:6]["context"],
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)

print(f"The 4 examples gave {len(inputs['input_ids'])} features.")
print(f"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.")
```

```python out
'The 4 examples gave 19 features.'
'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'
```

جیسا کہ ہم دیکھ سکتے ہیں، training set میں indices 2، 3، اور 4 پر موجود پہلی تین مثالیں ہر ایک چار features دیتی ہیں اور آخری مثال (index 5 پر) 7 features دیتی ہے۔

یہ معلومات ہر feature کو اس کے متعلقہ label سے میپ کرنے کے لیے مفید ثابت ہوگی۔ جیسا کہ پہلے بیان کیا گیا، یہ labels درج ذیل ہیں:

- `(0, 0)` اگر جواب متعلقہ context کے span میں موجود نہ ہو
- `(start_position, end_position)` اگر جواب متعلقہ context کے span میں موجود ہو، جہاں `start_position` token index (input IDs میں) ہے جہاں جواب شروع ہوتا ہے اور `end_position` token index (input IDs میں) ہے جہاں جواب ختم ہوتا ہے

یہ طے کرنے کے لیے کہ ان میں سے کون سا کیس ہے اور، اگر ضروری ہو، tokens کی پوزیشنز معلوم کرنے کے لیے، سب سے پہلے ہم input IDs میں context کے آغاز اور اختتام والے indices تلاش کرتے ہیں۔ ہم یہ کام token type IDs استعمال کر کے کر سکتے تھے، لیکن چونکہ یہ ہر ماڈل کے لیے ضروری نہیں ہوتے (مثلاً DistilBERT کے لیے درکار نہیں)، ہم اس کی بجائے ہمارے tokenizer کے ذریعہ واپسی ہونے والے `BatchEncoding` کے `sequence_ids()` میتھڈ کا استعمال کریں گے۔

ایک بار جب ہمارے پاس یہ token indices ہو جائیں، ہم متعلقہ offsets دیکھتے ہیں، جو دو integers کی tuples ہوتی ہیں جو اصل context کے اندر کردار (characters) کے span کی نمائندگی کرتی ہیں۔ اس طرح ہم یہ معلوم کر سکتے ہیں کہ اس feature میں context کا وہ حصہ جواب کے بعد شروع ہوتا ہے یا جواب کے شروع ہونے سے پہلے ختم ہو جاتا ہے (جس صورت میں label `(0, 0)` ہوگا)۔ اگر ایسا نہ ہو، تو ہم loop کر کے جواب کے پہلے اور آخری token کو تلاش کرتے ہیں:

```py
answers = raw_datasets["train"][2:6]["answers"]
start_positions = []
end_positions = []

for i, offset in enumerate(inputs["offset_mapping"]):
    sample_idx = inputs["overflow_to_sample_mapping"][i]
    answer = answers[sample_idx]
    start_char = answer["answer_start"][0]
    end_char = answer["answer_start"][0] + len(answer["text"][0])
    sequence_ids = inputs.sequence_ids(i)

    # Find the start and end of the context
    idx = 0
    while sequence_ids[idx] != 1:
        idx += 1
    context_start = idx
    while sequence_ids[idx] == 1:
        idx += 1
    context_end = idx - 1

    # If the answer is not fully inside the context, label is (0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # Otherwise it's the start and end token positions
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python out
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

آئیے کچھ نتائج پر نظر ڈالیں تاکہ تصدیق کر سکیں کہ ہمارا طریقہ درست ہے۔ پہلے فیچر کے لیے ہمیں `(83, 85)` بطور لیبل ملتے ہیں، تو آئیے نظریاتی جواب کا موازنہ 83 سے 85 (شامل) تک کے ٹوکنز کے ڈی کوڈ شدہ اسپین سے کریں:

```py
idx = 0
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs["input_ids"][idx][start : end + 1])

print(f"Theoretical answer: {answer}, labels give: {labeled_answer}")
```

```python out
'Theoretical answer: the Main Building, labels give: the Main Building'
```
تو یہ میل کھاتا ہے! اب آئیے انڈیکس 4 کو چیک کرتے ہیں، جہاں ہم نے لیبلز کو `(0, 0)` پر سیٹ کیا ہے، جس کا مطلب ہے کہ اس فیچر کے سیاق و سباق والے حصے میں جواب موجود نہیں ہے۔

```py
idx = 4
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

decoded_example = tokenizer.decode(inputs["input_ids"][idx])
print(f"Theoretical answer: {answer}, decoded example: {decoded_example}")
```

```python out
'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'
```

واقعی، ہمیں سیاق و سباق کے اندر جواب نظر نہیں آتا۔  

<Tip>  

✏️ **اب آپ کی باری!** جب XLNet آرکیٹیکچر استعمال کیا جاتا ہے، تو پیڈنگ بائیں طرف لگائی جاتی ہے اور سوال اور سیاق و سباق کی ترتیب بدل جاتی ہے۔ سارا کوڈ جو ہم نے ابھی دیکھا ہے اسے XLNet آرکیٹیکچر کے مطابق ڈھالیں (اور `padding=True` شامل کریں)۔ یاد رکھیں کہ پیڈنگ لگانے کے بعد `[CLS]` ٹوکن ضروری نہیں کہ 0 پوزیشن پر ہو۔  

</Tip>  

اب جب کہ ہم نے قدم بہ قدم دیکھا کہ اپنے ٹریننگ ڈیٹا کو کیسے پری پروسیس کرنا ہے، ہم اسے ایک فنکشن میں گروپ کر سکتے ہیں جسے ہم پورے ٹریننگ ڈیٹاسیٹ پر لاگو کریں گے۔ ہم ہر فیچر کو زیادہ سے زیادہ لمبائی تک پیڈ کریں گے، کیونکہ زیادہ تر سیاق و سباق طویل ہوں گے (اور ان کے متعلقہ نمونے کئی فیچرز میں تقسیم ہو جائیں گے)، لہٰذا یہاں ڈائنامک پیڈنگ لاگو کرنے کا کوئی خاص فائدہ نہیں ہے۔

```py
max_length = 384
stride = 128


def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label is (0, 0)
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs
```

نوٹ کریں کہ ہم نے دو مستقل اقدار (constants) متعین کی ہیں تاکہ زیادہ سے زیادہ لمبائی اور سلائیڈنگ ونڈو کی لمبائی کا تعین کیا جا سکے، اور ہم نے ٹوکنائزیشن سے پہلے تھوڑی سی صفائی بھی شامل کی ہے: SQuAD ڈیٹاسیٹ میں کچھ سوالات کے شروع یا آخر میں غیر ضروری اضافی خلا ہوتے ہیں جو کسی قسم کی معلومات فراہم نہیں کرتے (اور اگر آپ RoBERTa جیسے ماڈل کا استعمال کریں تو یہ غیر ضروری جگہ لیتے ہیں)، اس لیے ہم نے ان اضافی خلا کو ہٹا دیا۔  

پورے ٹریننگ سیٹ پر اس فنکشن کا اطلاق کرنے کے لیے، ہم `Dataset.map()` میتھڈ کو `batched=True` فلیگ کے ساتھ استعمال کرتے ہیں۔ یہ یہاں ضروری ہے کیونکہ ہم ڈیٹاسیٹ کی لمبائی کو تبدیل کر رہے ہیں (کیونکہ ایک مثال کئی ٹریننگ فیچرز دے سکتی ہے):

```py
train_dataset = raw_datasets["train"].map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
len(raw_datasets["train"]), len(train_dataset)
```

```python out
(87599, 88729)
```

جیسا کہ ہم دیکھ سکتے ہیں، پری پروسیسنگ نے تقریباً 1,000 فیچرز کا اضافہ کیا ہے۔ ہمارا ٹریننگ سیٹ اب استعمال کے لیے تیار ہے—آئیے اب ویلیڈیشن سیٹ کی پری پروسیسنگ پر کام کریں!  

### ویلیڈیشن ڈیٹا کی پروسیسنگ[[processing-the-validation-data]]  

ویلیڈیشن ڈیٹا کی پری پروسیسنگ قدرے آسان ہوگی کیونکہ ہمیں لیبلز بنانے کی ضرورت نہیں (جب تک کہ ہم ویلیڈیشن لاس کا حساب نہ لگانا چاہیں، لیکن وہ نمبر ہمیں ماڈل کی کارکردگی کے بارے میں زیادہ معلومات نہیں دے گا)۔ اصل کام ماڈل کی پیش گوئیوں کو اصل سیاق و سباق کے اسپینز میں تبدیل کرنا ہوگا۔ اس کے لیے ہمیں صرف آفسیٹ میپنگز کو محفوظ کرنا ہوگا اور کوئی ایسا طریقہ اپنانا ہوگا جس سے ہر تخلیق شدہ فیچر کو اس کی اصل مثال سے جوڑا جا سکے۔ چونکہ اصل ڈیٹاسیٹ میں ایک ID کالم موجود ہے، ہم اسی ID کو استعمال کریں گے۔  

یہاں ہم ایک معمولی سی تبدیلی آفسیٹ میپنگز کی صفائی کے لیے شامل کریں گے۔ ان میں سوال اور سیاق و سباق کے لیے آفسیٹس موجود ہوں گے، لیکن جب ہم پوسٹ پروسیسنگ مرحلے میں پہنچیں گے تو ہمارے پاس یہ معلوم کرنے کا کوئی طریقہ نہیں ہوگا کہ ان پٹ IDs کے کون سے حصے سیاق و سباق سے مطابقت رکھتے ہیں اور کون سے سوال سے (کیونکہ `sequence_ids()` میتھڈ صرف ٹوکنائزر کے آؤٹ پٹ کے لیے دستیاب ہے)۔ اس لیے، ہم ان آفسیٹس کو `None` پر سیٹ کر دیں گے جو سوال سے مطابقت رکھتے ہیں۔

```py
def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs
```
ہم اس فنکشن کو پہلے کی طرح پورے ویلیڈیشن ڈیٹاسیٹ پر لاگو کر سکتے ہیں:

```py
validation_dataset = raw_datasets["validation"].map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
len(raw_datasets["validation"]), len(validation_dataset)
```

```python out
(10570, 10822)
```

اس صورت میں ہم نے صرف چند سو نمونے شامل کیے ہیں، لہٰذا ایسا لگتا ہے کہ validation dataset کے contexts کچھ چھوٹے ہیں۔

اب جبکہ ہم نے تمام ڈیٹا کی preprocessing کر لی ہے، ہم تربیت (training) کی جانب بڑھ سکتے ہیں۔

{#if fw === 'pt'}

## `Trainer` API کے ساتھ ماڈل کی fine-tuning[[fine-tuning-the-model-with-the-trainer-api]]

اس مثال کے لیے تربیتی کوڈ پچھلے حصوں کے کوڈ جیسا ہی لگے گا – سب سے مشکل کام `compute_metrics()` فنکشن لکھنا ہوگا۔ چونکہ ہم نے تمام نمونوں کو مقرر کردہ زیادہ سے زیادہ لمبائی تک pad کر دیا ہے، کوئی data collator تعریف کرنے کی ضرورت نہیں ہے، لہٰذا یہ metric computation دراصل واحد چیز ہے جس کی ہمیں فکر ہے۔ مشکل حصہ ماڈل کی پیش گوئیوں کو اصل مثالوں کے متن (spans) میں تبدیل کرنا ہوگا؛ ایک بار جب ہم یہ کر لیں گے، 🤗 Datasets لائبریری کا metric باقی زیادہ کام انجام دے گا۔

{:else}

## Keras کے ساتھ ماڈل کی fine-tuning[[fine-tuning-the-model-with-keras]]

اس مثال کے لیے تربیتی کوڈ پچھلے حصوں کے کوڈ جیسا ہی لگے گا، مگر metrics کا حساب لگانا منفرد طور پر چیلنجنگ ہوگا۔ چونکہ ہم نے تمام نمونوں کو مقرر کردہ زیادہ سے زیادہ لمبائی تک pad کر دیا ہے، کوئی data collator تعریف کرنے کی ضرورت نہیں ہے، لہٰذا یہ metric computation دراصل واحد چیز ہے جس کی ہمیں فکر ہے۔ مشکل حصہ ماڈل کی پیش گوئیوں کو اصل مثالوں کے متن (spans) میں تبدیل کرنا ہوگا؛ ایک بار جب ہم یہ کر لیں گے، 🤗 Datasets لائبریری کا metric باقی زیادہ کام انجام دے گا۔

{/if}

### پوسٹ پروسیسنگ[[post-processing]]

{#if fw === 'pt'}

<Youtube id="BNy08iIWVJM"/>

{:else}

<Youtube id="VN67ZpN33Ss"/>

{/if}

ماڈل input IDs میں جواب کے آغاز اور اختتام کی پوزیشنز کے لیے logits output کرے گا، جیسا کہ ہم نے [`question-answering` pipeline](/course/chapter6/3b) کی تحقیق کے دوران دیکھا تھا۔ بعد از عمل کاری (post-processing) کا مرحلہ تقریباً وہی ہوگا جو ہم نے وہاں کیا تھا، لہٰذا یہاں ہم نے جو اقدامات کیے تھے ان کی ایک مختصر یاد دہانی پیش کی جاتی ہے:

- ہم نے context سے باہر موجود tokens کے لیے شروع اور اختتامی logits کو mask کر دیا۔
- پھر ہم نے softmax استعمال کرتے ہوئے شروع اور اختتامی logits کو probabilities میں تبدیل کر دیا۔
- ہم نے ہر `(start_token, end_token)` جوڑے کو متعلقہ دونوں probabilities کے حاصل ضرب سے ایک اسکور دیا۔
- ہم نے ایسے جوڑے کی تلاش کی جس کا اسکور سب سے زیادہ ہو اور جو ایک valid جواب فراہم کرے (مثلاً، ایسا `start_token` جو `end_token` سے کم ہو)۔

یہاں ہم اس عمل کو تھوڑا سا تبدیل کر دیں گے کیونکہ ہمیں حقیقی اسکورز کا حساب لگانے کی ضرورت نہیں (صرف پیش گوئی کردہ جواب درکار ہے)۔ اس کا مطلب ہے کہ ہم softmax مرحلے کو چھوڑ سکتے ہیں۔ تیزی کے لیے، ہم تمام ممکنہ `(start_token, end_token)` جوڑوں کا اسکور نہیں دیں گے، بلکہ صرف ان جوڑوں کا اسکور دیں گے جو سب سے زیادہ `n_best` logits (جہاں `n_best=20` ہے) سے متعلق ہوں۔ چونکہ ہم softmax کو چھوڑ رہے ہیں، ان اسکورز کو logit scores کہا جائے گا، اور انہیں شروع اور اختتامی logits کا مجموعہ لے کر حاصل کیا جائے گا (حاصل ضرب کی بجائے، کیونکہ \\(\log(ab) = \log(a) + \log(b)\\) کے اصول کی وجہ سے)۔

اس سب کو ظاہر کرنے کے لیے، ہمیں کچھ پیش گوئیوں (predictions) کی ضرورت ہوگی۔ چونکہ ہم نے ابھی تک اپنا ماڈل تربیت نہیں دیا ہے، اس لیے ہم QA pipeline کے لیے ڈیفالٹ ماڈل کا استعمال کرتے ہوئے validation set کے ایک چھوٹے حصے پر کچھ پیش گوئیاں حاصل کریں گے۔ ہم پہلے کی طرح اسی processing function کا استعمال کر سکتے ہیں؛ کیونکہ یہ global constant `tokenizer` پر منحصر ہے، ہمیں صرف عارضی طور پر tokenizer کو اُس ماڈل کے tokenizer سے تبدیل کرنا ہے جسے ہم استعمال کرنا چاہتے ہیں:

```python
small_eval_set = raw_datasets["validation"].select(range(100))
trained_checkpoint = "distilbert-base-cased-distilled-squad"

tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)
eval_set = small_eval_set.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
```

اب جبکہ preprocessing مکمل ہو چکا ہے، ہم tokenizer کو دوبارہ اسی میں تبدیل کر دیتے ہیں جو ہم نے اصل میں منتخب کیا تھا:

```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

پھر ہم اپنے `eval_set` کے اُن columns کو ہٹا دیتے ہیں جو ماڈل کی توقع کے مطابق نہیں ہیں، اس چھوٹے validation set کا ایک batch تیار کرتے ہیں، اور اسے ماڈل سے گزار دیتے ہیں۔ اگر کوئی GPU دستیاب ہو، تو ہم تیزی کے لیے اس کا استعمال کرتے ہیں:

{#if fw === 'pt'}

```python
import torch
from transformers import AutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}
trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(
    device
)

with torch.no_grad():
    outputs = trained_model(**batch)
```

چونکہ `Trainer` ہمیں پیش گوئیاں NumPy arrays کی شکل میں دے گا، ہم اسٹارٹ اور اینڈ لاجٹس حاصل کرکے انہیں اسی فارمیٹ میں تبدیل کرتے ہیں:

```python
start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()
```

{:else}

```python
import tensorflow as tf
from transformers import TFAutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("numpy")

batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}
trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)

outputs = trained_model(**batch)
```

آسانی سے تجربات کرنے کے لیے، آئیے ان آؤٹ پٹس کو NumPy arrays میں تبدیل کریں:

```python
start_logits = outputs.start_logits.numpy()
end_logits = outputs.end_logits.numpy()
```

{/if}

اب ہمیں اپنے `small_eval_set` میں ہر مثال کے لیے متوقع جواب تلاش کرنا ہوگا۔ ایک مثال `eval_set` میں کئی فیچرز میں تقسیم ہو سکتی ہے، اس لیے پہلا قدم یہ ہے کہ `small_eval_set` میں ہر مثال کو `eval_set` میں اس کے متعلقہ فیچرز سے جوڑیں:

```python
import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(eval_set):
    example_to_features[feature["example_id"]].append(idx)
```

اب ہم اصل کام شروع کر سکتے ہیں: تمام مثالوں پر لوپ چلائیں گے اور ہر مثال کے لیے اس سے متعلق تمام فیچرز پر بھی لوپ کریں گے۔ جیسا کہ ہم نے پہلے ذکر کیا، ہم `n_best` اسٹارٹ لاجٹس اور اینڈ لاجٹس کے لاجٹ اسکورز کو دیکھیں گے، لیکن ان پوزیشنز کو خارج کر دیں گے جو:  

- ایسا جواب دیتی ہیں جو سیاق و سباق کے اندر نہیں آتا  
- ایسا جواب دیتی ہیں جس کی لمبائی منفی ہو  
- ایسا جواب دیتی ہیں جو بہت زیادہ لمبا ہو (ہم زیادہ سے زیادہ لمبائی `max_answer_length=30` پر محدود رکھتے ہیں)  

جب ہم کسی ایک مثال کے تمام ممکنہ جوابات کے اسکور حاصل کر لیں گے، تو ہم سب سے بہترین لاجٹ اسکور والا جواب منتخب کریں گے۔

```python
import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in small_eval_set:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = eval_set["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Skip answers that are not fully in the context
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Skip answers with a length that is either < 0 or > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
```

متوقع جوابات کا حتمی فارمیٹ وہی ہوگا جو اس میٹرک کے لیے درکار ہے جسے ہم استعمال کریں گے۔ حسب معمول، ہم اسے 🤗 Evaluate لائبریری کی مدد سے لوڈ کر سکتے ہیں:

```python
import evaluate

metric = evaluate.load("squad")
```

یہ میٹرک متوقع جوابات کو اس فارمیٹ میں دیکھنے کی توقع کرتا ہے جو ہم نے اوپر دیکھا تھا (لغات کی ایک فہرست جس میں ایک کلید مثال کی ID کے لیے اور ایک کلید متوقع متن کے لیے ہو)۔ جبکہ نظریاتی جوابات درج ذیل فارمیٹ میں ہوں گے (لغات کی ایک فہرست جس میں ایک کلید مثال کی ID کے لیے اور ایک کلید ممکنہ جوابات کے لیے ہو):

```python
theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]} for ex in small_eval_set
]
```

اب ہم دونوں فہرستوں کے پہلے عنصر کو دیکھ کر تصدیق کر سکتے ہیں کہ ہمیں معقول نتائج مل رہے ہیں:

```python
print(predicted_answers[0])
print(theoretical_answers[0])
```

```python out
{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}
```

اتنا برا نہیں! اب آئیے دیکھتے ہیں کہ یہ میٹرک ہمیں کیا اسکور دیتا ہے:

```python
metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

پھر بھی، یہ کافی اچھا ہے اگر غور کیا جائے کہ [its paper](https://arxiv.org/abs/1910.01108v2) کے مطابق SQuAD پر fine-tuned DistilBERT پورے dataset پر ان اسکورز کے لیے 79.1 اور 86.9 حاصل کرتا ہے۔

{#if fw === 'pt'}

اب آئیے وہ سب کچھ ایک `compute_metrics()` فنکشن میں ڈال دیتے ہیں جسے ہم `Trainer` میں استعمال کریں گے۔ عام طور پر، وہ `compute_metrics()` فنکشن صرف ایک tuple `eval_preds` لیتا ہے جس میں logits اور labels شامل ہوتے ہیں۔ یہاں ہمیں تھوڑا اور درکار ہوگا، کیونکہ ہمیں features کے dataset میں offset کو دیکھنا ہے اور examples کے dataset میں اصل contexts کو بھی دیکھنا ہے، اس لیے ہم تربیت کے دوران اس فنکشن کو معمول کے evaluation نتائج حاصل کرنے کے لیے استعمال نہیں کر سکیں گے۔ ہم اسے صرف تربیت کے اختتام پر نتائج چیک کرنے کے لیے استعمال کریں گے۔

`compute_metrics()` فنکشن پہلے کیے گئے اقدامات کو یکجا کرتا ہے؛ ہم صرف ایک چھوٹی جانچ شامل کرتے ہیں کہ اگر ہمیں کوئی valid جواب نہ ملے (جس صورت میں ہم ایک خالی string کی پیش گوئی کریں گے)۔

{:else}

اب آئیے وہ سب کچھ ایک `compute_metrics()` فنکشن میں ڈال دیتے ہیں جسے ہم اپنے ماڈل کی تربیت کے بعد استعمال کریں گے۔ ہمیں صرف output logits سے زیادہ کچھ پاس کرنے کی ضرورت ہوگی، کیونکہ ہمیں features کے dataset میں offset کو دیکھنا ہے اور examples کے dataset میں اصل contexts کو بھی دیکھنا ہے:

{/if}

```python
from tqdm.auto import tqdm


def compute_metrics(start_logits, end_logits, features, examples):
    example_to_features = collections.defaultdict(list)
    for idx, feature in enumerate(features):
        example_to_features[feature["example_id"]].append(idx)

    predicted_answers = []
    for example in tqdm(examples):
        example_id = example["id"]
        context = example["context"]
        answers = []

        # Loop through all features associated with that example
        for feature_index in example_to_features[example_id]:
            start_logit = start_logits[feature_index]
            end_logit = end_logits[feature_index]
            offsets = features[feature_index]["offset_mapping"]

            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Skip answers that are not fully in the context
                    if offsets[start_index] is None or offsets[end_index] is None:
                        continue
                    # Skip answers with a length that is either < 0 or > max_answer_length
                    if (
                        end_index < start_index
                        or end_index - start_index + 1 > max_answer_length
                    ):
                        continue

                    answer = {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                    answers.append(answer)

        # Select the answer with the best score
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append(
                {"id": example_id, "prediction_text": best_answer["text"]}
            )
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    theoretical_answers = [{"id": ex["id"], "answers": ex["answers"]} for ex in examples]
    return metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

ہم اپنی پیش گوئیوں پر اسے چلا کر تصدیق کر سکتے ہیں کہ یہ صحیح کام کر رہا ہے:

```python
compute_metrics(start_logits, end_logits, eval_set, small_eval_set)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

Looking good! Now let's use this to fine-tune our model.

### ماڈل کی فائن ٹیوننگ[[fine-tuning-the-model]]

{#if fw === 'pt'}

اب ہم اپنے ماڈل کی تربیت کے لیے تیار ہیں۔ سب سے پہلے اسے بناتے ہیں، جیسے پہلے کیا تھا، `AutoModelForQuestionAnswering` کلاس کا استعمال کرتے ہوئے:

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{:else}

اب ہم اپنے ماڈل کی تربیت کے لیے تیار ہیں۔ سب سے پہلے اسے بناتے ہیں، جیسے پہلے کیا تھا، `TFAutoModelForQuestionAnswering` کلاس کا استعمال کرتے ہوئے:

```python
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{/if}

جیسا کہ ہمیشہ کی طرح، ہمیں ایک وارننگ ملتی ہے کہ کچھ وزن استعمال نہیں ہوئے (پری ٹریننگ ہیڈ کے وزن) اور کچھ وزن بے ترتیب طور پر initialize ہوئے ہیں (سوال جواب ہیڈ کے وزن)۔ اب تک آپ اس کے عادی ہو چکے ہوں گے، مگر اس کا مطلب یہ ہے کہ یہ ماڈل ابھی استعمال کے لیے تیار نہیں ہے اور اسے فائن ٹیون کرنے کی ضرورت ہے – خیر، ابھی ہم یہ کرنے جا رہے ہیں!

ماڈل کو Hub پر دھکیلنے کے قابل بنانے کے لیے، ہمیں Hugging Face میں لاگ ان کرنا ہوگا۔ اگر آپ یہ کوڈ نوٹ بک میں چلا رہے ہیں تو آپ نیچے دی گئی utility فنکشن کا استعمال کر سکتے ہیں، جو ایک widget دکھاتا ہے جہاں آپ اپنے لاگ ان کی تفصیلات داخل کر سکتے ہیں:

```python
from huggingface_hub import notebook_login

notebook_login()
```

اگر آپ نوٹ بک میں کام نہیں کر رہے، تو اپنے ٹرمینل میں صرف یہ لائن ٹائپ کریں:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

جب یہ عمل مکمل ہو جائے، تو ہم اپنے `TrainingArguments` کو ڈیفائن کر سکتے ہیں۔ جیسے کہ ہم نے metric کمپیوٹ کرنے کے لیے اپنے فنکشن کو ڈیفائن کرتے وقت کہا تھا، ہم `compute_metrics()` فنکشن کے signature کی وجہ سے ایک معمول کا evaluation loop نہیں چلا سکیں گے۔ ہم اپنا ذاتی subclass `Trainer` کا استعمال بھی کر سکتے تھے (جیسے کہ [question answering example script](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py) میں دکھایا گیا ہے)، مگر یہ اس سیکشن کے لیے کافی طویل ہوگا۔ اس کے بجائے، ہم یہاں صرف تربیت کے اختتام پر ماڈل کا evaluation کریں گے اور "A custom training loop" میں ایک معمول کا evaluation دکھانے کا طریقہ بتائیں گے۔

یہی وہ جگہ ہے جہاں `Trainer` API کی حدود سامنے آتی ہیں اور 🤗 Accelerate لائبریری اپنی جگہ چمکتی ہے: کسی مخصوص استعمال کے لیے کلاس کو کسٹمائز کرنا مشکل ہو سکتا ہے، مگر ایک مکمل طور پر exposed training loop کو tweak کرنا آسان ہے۔

آئیے اپنے `TrainingArguments` پر ایک نظر ڈالتے ہیں:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-squad",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)
```

ہم نے پہلے ہی ان میں سے زیادہ تر کو دیکھ لیا ہے: ہم نے کچھ hyperparameters (جیسے learning rate، epochs کی تعداد، اور weight decay) سیٹ کیے ہیں اور یہ بتا دیا ہے کہ ہم ہر epoch کے آخر میں ماڈل کو save کرنا چاہتے ہیں، evaluation چھوڑنا چاہتے ہیں، اور اپنے نتائج کو Model Hub پر اپلوڈ کرنا چاہتے ہیں۔ ہم نے `fp16=True` کے ذریعے mixed-precision training بھی فعال کر دی ہے، جو جدید GPU پر تربیت کی رفتار کو بہتر بنا سکتی ہے۔

{:else}

اب یہ کام مکمل ہو گیا ہے، ہم اپنے TF Datasets تخلیق کر سکتے ہیں۔ اس بار ہم سادہ default data collator استعمال کر سکتے ہیں:

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

اور اب ہم datasets کو معمول کے مطابق بناتے ہیں:

```python
tf_train_dataset = model.prepare_tf_dataset(
    train_dataset,
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)
tf_eval_dataset = model.prepare_tf_dataset(
    validation_dataset,
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

اس کے بعد، ہم اپنے training hyperparameters سیٹ کرتے ہیں اور ماڈل کو compile کرتے ہیں:

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# تربیتی مراحل کی تعداد: dataset میں موجود نمونوں کی تعداد کو بیچ سائز سے تقسیم کر کے اور پھر epochs کی تعداد سے ضرب دیں۔
# نوٹ کریں کہ یہاں tf_train_dataset ایک batched tf.data.Dataset ہے، اصل Hugging Face Dataset نہیں، لہٰذا اس کی len() پہلے ہی num_samples // batch_size کے برابر ہے۔
num_train_epochs = 3
num_train_steps = len(tf_train_dataset) * num_train_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# mixed-precision float16 میں تربیت کریں
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

آخر میں، ہم `model.fit()` کے ذریعے تربیت شروع کرتے ہیں۔ ہم ایک `PushToHubCallback` کا استعمال کرتے ہیں تاکہ ہر epoch کے بعد ماڈل کو Hub پر اپلوڈ کر سکیں۔

{/if}

By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be in `"sgugger/bert-finetuned-squad"`. We can override this by passing a `hub_model_id`; for instance, to push the model to the `huggingface_course` organization we used `hub_model_id="huggingface_course/bert-finetuned-squad"` (which is the model we linked to at the beginning of this section).

{#if fw === 'pt'}

<Tip>

💡 اگر آپ جو output directory استعمال کر رہے ہیں وہ پہلے سے موجود ہے، تو اسے اُس repository کا ایک لوکل کلون ہونا چاہیے جسے آپ push کرنا چاہتے ہیں (اگر `Trainer` کو ڈیفائن کرتے وقت کوئی error آئے تو نیا نام استعمال کریں)۔

</Tip>

آخر میں، ہم سب کچھ `Trainer` کلاس کو پاس کر کے تربیت شروع کرتے ہیں:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

{:else}

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-squad", tokenizer=tokenizer)

# چونکہ ہم بعد میں validation کریں گے، اس لیے درمیان میں validation نہیں کر رہے
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)
```

{/if}

نوٹ کریں کہ جب تربیت جاری ہوتی ہے، ہر بار جب ماڈل save ہوتا ہے (یہاں، ہر epoch) وہ پس منظر میں Hub پر اپلوڈ ہو جاتا ہے۔ اس طرح، اگر ضرورت ہو تو آپ اپنی تربیت کو کسی دوسری مشین پر resume کر سکیں گے۔ پوری تربیت میں کافی وقت لگتا ہے (Titan RTX پر تقریباً ایک گھنٹے سے کچھ زیادہ)، تو آپ ایک کافی کا کپ لے سکتے ہیں یا کورس کے اُن حصوں کو دوبارہ پڑھ سکتے ہیں جو آپ کو مشکل لگے۔ مزید یہ کہ جیسے ہی پہلا epoch مکمل ہوتا ہے، آپ Hub پر کچھ weights اپلوڈ ہوتے دیکھیں گے اور آپ اپنے ماڈل کے صفحے پر اس کے ساتھ تجربات شروع کر سکتے ہیں۔

{#if fw === 'pt'}

تربیت مکمل ہونے کے بعد، ہم آخر کار اپنے ماڈل کا evaluation کر سکتے ہیں (اور دعا کرتے ہیں کہ ہم نے سارا compute time ضائع نہ کیا ہو)۔ `Trainer` کا `predict()` میتھڈ ایک tuple واپس کرتا ہے جس کے پہلے عناصر ماڈل کی پیش گوئیاں ہوتی ہیں (یہاں، آغاز اور اختتام کے logits کا جوڑا)۔ ہم اسے اپنے `compute_metrics()` فنکشن کو بھیج دیتے ہیں:

```python
predictions, _, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets["validation"])
```

{:else}

تربیت مکمل ہونے کے بعد، ہم آخر کار اپنے ماڈل کا evaluation کر سکتے ہیں (اور دعا کرتے ہیں کہ ہم نے سارا compute time ضائع نہ کیا ہو)۔ ہمارے ماڈل کا `predict()` میتھڈ پیش گوئیاں حاصل کرنے کا انتظام کرے گا، اور چونکہ ہم پہلے ہی ایک `compute_metrics()` فنکشن ڈیفائن کر چکے ہیں، ہم ایک ہی لائن میں اپنے نتائج حاصل کر سکتے ہیں:

```python
predictions = model.predict(tf_eval_dataset)
compute_metrics(
    predictions["start_logits"],
    predictions["end_logits"],
    validation_dataset,
    raw_datasets["validation"],
)
```

{/if}

```python out
{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}
```

بہت عمدہ! موازنہ کے طور پر، BERT آرٹیکل میں اس ماڈل کے لیے baseline scores 80.8 اور 88.5 رپورٹ کیے گئے ہیں، لہٰذا ہم درست مقام پر ہیں۔

{#if fw === 'pt'}

آخر میں، ہم `push_to_hub()` میتھڈ کا استعمال کرتے ہیں تاکہ یہ یقینی بنایا جا سکے کہ ماڈل کا تازہ ترین ورژن اپلوڈ ہو جائے:

```py
trainer.push_to_hub(commit_message="Training complete")
```

یہ اس commit کا URL واپس کرتا ہے جو ابھی کیا گیا، اگر آپ اسے inspect کرنا چاہتے ہیں:

```python out
'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68'
```

`Trainer` ایک model card بھی تیار کرتا ہے جس میں تمام evaluation نتائج شامل ہوتے ہیں اور اسے اپلوڈ کر دیتا ہے۔

{/if}

At this stage, you can use the inference widget on the Model Hub to test the model and share it with your friends, family, and favorite pets. You have successfully fine-tuned a model on a question answering task -- congratulations!

<Tip>

✏️ **Your turn!** Try another model architecture to see if it performs better on this task!

</Tip>

{#if fw === 'pt'}

اگر آپ training loop میں مزید گہرائی میں جانا چاہتے ہیں، تو اب ہم آپ کو وہی کام 🤗 Accelerate کا استعمال کرتے ہوئے کرنے کا طریقہ دکھاتے ہیں۔

## ایک کسٹم ٹریننگ لوپ[[a-custom-training-loop]]

آئیے اب پورے training loop پر ایک نظر ڈالتے ہیں، تاکہ آپ آسانی سے اُن حصوں کو کسٹمائز کر سکیں جن کی آپ کو ضرورت ہے۔ یہ تقریباً [Chapter 3](/course/chapter3/4) میں موجود training loop جیسا ہی لگے گا، سوائے evaluation loop کے۔ اب چونکہ ہم `Trainer` کلاس کی قید میں نہیں ہیں، ہم باقاعدگی سے ماڈل کا evaluation کر سکیں گے۔

### تربیت کے لیے سب کچھ تیار کرنا[[preparing-everything-for-training]]

سب سے پہلے ہمیں اپنے datasets سے `DataLoader`s بنانے کی ضرورت ہے۔ ہم اُن datasets کا format `"torch"` پر سیٹ کرتے ہیں، اور validation set میں اُن columns کو ہٹا دیتے ہیں جو ماڈل استعمال نہیں کرتا۔ پھر، ہم Transformers کی طرف سے فراہم کردہ `default_data_collator` کو `collate_fn` کے طور پر استعمال کر سکتے ہیں اور training set کو shuffle کر سکتے ہیں، مگر validation set کو نہیں:

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_dataset.set_format("torch")
validation_set = validation_dataset.remove_columns(["example_id", "offset_mapping"])
validation_set.set_format("torch")

train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=default_data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    validation_set, collate_fn=default_data_collator, batch_size=8
)
```

پھر ہم اپنا ماڈل دوبارہ instantiate کرتے ہیں، تاکہ یہ یقینی بنایا جا سکے کہ ہم پچھلی fine-tuning جاری نہیں رکھ رہے بلکہ BERT کے پری ٹرینڈ ماڈل سے دوبارہ شروع کر رہے ہیں:

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

پھر ہمیں ایک آپٹیمائزر کی ضرورت ہوگی۔ حسب معمول، ہم کلاسک `AdamW` استعمال کریں گے، جو ایڈم کی طرح ہے، لیکن اس میں ویٹ ڈیکے کے اطلاق کے طریقے میں ایک اصلاح شامل ہے:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

جب ہمارے پاس یہ تمام آبجیکٹس ہوں، تو ہم انہیں `accelerator.prepare()` میتھڈ میں بھیج سکتے ہیں۔ یاد رکھیں کہ اگر آپ Colab نوٹ بک میں TPUs پر ٹریننگ کرنا چاہتے ہیں، تو آپ کو یہ سارا کوڈ ایک ٹریننگ فنکشن میں منتقل کرنا ہوگا، اور ایسا کوئی سیل نہیں چلانا چاہیے جو `Accelerator` کو انسٹیٹیوٹ کرے۔ ہم `Accelerator` میں `fp16=True` پاس کر کے مکسڈ-پریسیژن ٹریننگ کو فعال کر سکتے ہیں (یا اگر آپ کوڈ کو اسکرپٹ کے طور پر چلا رہے ہیں، تو 🤗 Accelerate `config` کو مناسب طریقے سے سیٹ کر لیں)۔

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

جیسا کہ آپ پچھلے حصوں سے جانتے ہیں، ہم `train_dataloader` کی لمبائی کو صرف اسی وقت ٹریننگ اسٹیپس کی تعداد نکالنے کے لیے استعمال کر سکتے ہیں جب یہ `accelerator.prepare()` میتھڈ سے گزر چکا ہو۔ ہم وہی لکیری شیڈول استعمال کرتے ہیں جو پچھلے حصوں میں تھا۔

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```
ہمارے ماڈل کو ہب پر اپ لوڈ کرنے کے لیے، ہمیں ایک `Repository` آبجیکٹ کو ورکنگ فولڈر میں بنانا ہوگا۔ اگر آپ پہلے سے لاگ ان نہیں ہیں، تو سب سے پہلے Hugging Face Hub میں لاگ ان کریں۔ ہم ریپوزٹری کا نام اس ماڈل ID سے طے کریں گے جو ہم اپنے ماڈل کو دینا چاہتے ہیں (آپ `repo_name` کو اپنی پسند کے مطابق بدل سکتے ہیں؛ بس اس میں آپ کا یوزر نیم شامل ہونا چاہیے، جو کہ `get_full_repo_name()` فنکشن کرتا ہے)۔

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-squad-accelerate'
```

پھر ہم اس ریپوزٹری کو ایک مقامی فولڈر میں کلون کر سکتے ہیں۔ اگر یہ پہلے سے موجود ہے، تو یہ مقامی فولڈر اس ریپوزٹری کی ایک کلون ہونا چاہیے جس پر ہم کام کر رہے ہیں۔

```py
output_dir = "bert-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ہم اب `output_dir` میں محفوظ کی جانے والی کسی بھی چیز کو `repo.push_to_hub()` میتھڈ کو کال کر کے اپلوڈ کر سکتے ہیں۔ اس سے ہمیں ہر epoch کے اختتام پر intermediate ماڈلز اپلوڈ کرنے میں مدد ملے گی۔

## تربیتی لوپ[[training-loop]]

اب ہم مکمل تربیتی لوپ لکھنے کے لیے تیار ہیں۔ progress bar کی تعریف کے بعد، لوپ میں تین حصے شامل ہیں:

- **تربیت:** جو کہ `train_dataloader` پر کلاسک iteration، ماڈل کے ذریعے forward pass، پھر backward pass اور optimizer step پر مشتمل ہے۔
- **جانچ (Evaluation):** جس میں ہم تمام `start_logits` اور `end_logits` کی قدریں اکٹھی کرتے ہیں، پھر انہیں NumPy arrays میں تبدیل کرتے ہیں۔ جب evaluation loop ختم ہو جائے تو ہم تمام نتائج کو concatenate کرتے ہیں۔ نوٹ کریں کہ ہمیں truncate کرنا پڑتا ہے کیونکہ `Accelerator` نے ہر process میں ایک جیسی تعداد میں examples رکھنے کے لیے آخر میں چند نمونے شامل کر دئیے ہوتے ہیں۔
- **محفوظ کرنا اور اپلوڈ کرنا:** جہاں پہلے ہم ماڈل اور tokenizer کو محفوظ کرتے ہیں، پھر `repo.push_to_hub()` کو کال کرتے ہیں۔ جیسا کہ پہلے کیا گیا، ہم `blocking=False` آرگیومنٹ استعمال کرتے ہیں تاکہ 🤗 Hub لائبریری اپلوڈ کو asynchronous process میں انجام دے، جس سے تربیت معمول کے مطابق جاری رہتی ہے اور یہ (طویل) عمل پس منظر میں چلتا رہتا ہے۔

ذیل میں تربیتی لوپ کا مکمل کوڈ دیا گیا ہے:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # تربیت
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # جانچ (Evaluation)
    model.eval()
    start_logits = []
    end_logits = []
    accelerator.print("Evaluation!")
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())
        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())

    start_logits = np.concatenate(start_logits)
    end_logits = np.concatenate(end_logits)
    start_logits = start_logits[: len(validation_dataset)]
    end_logits = end_logits[: len(validation_dataset)]

    metrics = compute_metrics(
        start_logits, end_logits, validation_dataset, raw_datasets["validation"]
    )
    print(f"epoch {epoch}:", metrics)

    # محفوظ کرنا اور اپلوڈ کرنا
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

اگر یہ آپ کی پہلی بار ہے کہ آپ نے 🤗 Accelerate کے ساتھ محفوظ کیا ہوا ماڈل دیکھا ہے، تو آئیے ان تین لائنوں کو غور سے دیکھتے ہیں:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

- **پہلی لائن:** سیدھی سادی بات ہے: یہ تمام processes کو یہ بتاتی ہے کہ وہ اس مقام تک پہنچ جائیں جب تک کہ سب ایک ساتھ نہ ہوں، تاکہ محفوظ کرنے سے پہلے ہر process میں ایک ہی ماڈل موجود ہو۔
- **دوسری لائن:** یہاں ہم `unwrapped_model` حاصل کرتے ہیں، جو کہ وہ بنیادی ماڈل ہے جو ہم نے تعریف کیا تھا۔ `accelerator.prepare()` ماڈل کو distributed training کے لیے تبدیل کر دیتا ہے، جس سے اب ماڈل میں `save_pretrained()` میتھڈ موجود نہیں ہوتا؛ `accelerator.unwrap_model()` میتھڈ اس تبدیلی کو واپس کر دیتا ہے۔
- **تیسری لائن:** آخر میں، ہم `save_pretrained()` کو کال کرتے ہیں لیکن اس بات کی ہدایت دیتے ہیں کہ وہ `accelerator.save()` کا استعمال کرے بجائے `torch.save()` کے۔

ایک بار یہ عمل مکمل ہو جائے، تو آپ کے پاس ایک ایسا ماڈل موجود ہوگا جو تقریباً اسی طرح نتائج پیدا کرتا ہے جیسے `Trainer` کے ساتھ تربیت دیا گیا ماڈل۔ آپ [*huggingface-course/bert-finetuned-squad-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate) پر اس ماڈل کو چیک کر سکتے ہیں۔ اور اگر آپ تربیتی لوپ میں کسی بھی قسم کی تبدیلیاں آزمانا چاہتے ہیں، تو آپ اوپر دکھائے گئے کوڈ میں براہ راست ترمیم کر کے یہ کر سکتے ہیں!

## فائن ٹیون کیا ہوا ماڈل استعمال کرنا[[using-the-fine-tuned-model]]

ہم پہلے ہی آپ کو دکھا چکے ہیں کہ آپ Model Hub پر inference widget کے ذریعے اپنے فائن ٹیون ماڈل کو کیسے استعمال کر سکتے ہیں۔ اسے لوکل سطح پر `pipeline` میں استعمال کرنے کے لیے، آپ کو صرف ماڈل کا identifier فراہم کرنا ہوتا ہے:

```py
from transformers import pipeline

# اس کو اپنے checkpoint کے ساتھ تبدیل کریں
model_checkpoint = "huggingface-course/bert-finetuned-squad"
question_answerer = pipeline("question-answering", model=model_checkpoint)

context = """
🤗 Transformers کو تین سب سے مشہور deep learning libraries — Jax, PyTorch اور TensorFlow — کی حمایت حاصل ہے، جن کے درمیان ایک seamless انٹیگریشن موجود ہے۔
ایک کے ساتھ ماڈلز کی تربیت کرنا اور دوسرے کے لیے inference کے لیے لوڈ کرنا بہت سیدھا سادہ ہے۔
"""
question = "🤗 Transformers کی حمایت کرنے والی deep learning libraries کون سی ہیں؟"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.9979003071784973,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

زبردست! ہمارا ماڈل اسی طرح کام کر رہا ہے جیسے اس pipeline کے لیے ڈیفالٹ ماڈل کرتا ہے!