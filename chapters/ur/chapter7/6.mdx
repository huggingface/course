<FrameworkSwitchCourse {fw} />

# ุงุจุชุฏุงุก ุณ ุงฺฉ ุงุณุจุงุจ ุฒุจุงู ูุงฺู ฺฉ ุชุฑุจุช[[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

ุงุจ ุชฺฉุ ู ู ุฒุงุฏ ุชุฑ ูพู ุณ ุชุฑุจุช ุงูุช ูุงฺูุฒ ฺฉุง ุงุณุชุนูุงู ฺฉุง  ุงูุฑ ูุฆ ุงุณุชุนูุงูุงุช ฺฉ ู ุงูฺบ ูุงุฆู ูนูู ฺฉุง ุ ุฌุณ ูฺบ ูพุฑ ูนุฑููฺฏ ฺฉ ูุฒู ฺฉู ุฏูุจุงุฑ ุงุณุชุนูุงู ฺฉุง ุฌุงุชุง  ุฌุณุง ฺฉ ู ู [Chapter 1](/course/chapter1) ูฺบ ุฏฺฉฺพุงุ ุงุณ ุนุงู ุทูุฑ ูพุฑ _transfer learning_ ฺฉุง ุฌุงุชุง ุ ุงูุฑ  ุงู ุฒุงุฏ ุชุฑ ุญูู ุฏูุง ฺฉ ุงุณุชุนูุงูุงุช ฺฉ ู ุงฺฉ ุจุช ฺฉุงูุงุจ ุญฺฉูุช ุนูู  ุฌุงฺบ ูุจู ุดุฏ ฺูนุง ฺฉู ูุชุง  ุงุณ ุจุงุจ ูฺบุ ู ุงฺฉ ูุฎุชูู ุทุฑู ุงูพูุงุฆฺบ ฺฏ ุงูุฑ ุจุงูฺฉู ูุง ูุงฺู ุงุจุชุฏุงุก ุณ ุชุฑุจุช ุฏฺบ ฺฏ ุงฺฏุฑ ุขูพ ฺฉ ูพุงุณ ุจุช ุณุง ฺูนุง ููุฌูุฏ  ุงูุฑ ู ุฏุณุชุงุจ ูุงฺูุฒ ฺฉ ู ุงุณุชุนูุงู ุดุฏ ูพุฑ ูนุฑููฺฏ ฺูนุง ุณ ฺฉุงู ูุฎุชูู ุ ุชู  ุงฺฉ ุงฺฺพุง ููุท ูุธุฑ  ุชุงูุ ุงฺฉ ุฒุจุงู ูุงฺู ฺฉู ูพุฑ ูนุฑู ฺฉุฑู ฺฉ ููุงุจู ูฺบ ุตุฑู ููุฌูุฏ ูุงฺู ฺฉู ูุงุฆู ูนูู ฺฉุฑู ฺฉ ู ฺฉฺบ ุฒุงุฏ ฺฉููพููน ูุณุงุฆู ุฏุฑฺฉุงุฑ ูุช ฺบ ุงุณ ฺูนุงุณูนุณ ุฌู ูฺบ ููุณู ฺฉ ูููนุณุ ูุงูฺฉููุฑ ุณูุณู ุฌุณ DNAุ ุง ูพุฑูฺฏุฑุงููฺฏ ุฒุจุงูฺบ ุดุงูู ูฺบุ ฺฉ ู ูุง ูุงฺู ุชุฑุจุช ุฏูุง ูุนููู ุซุงุจุช ู ุณฺฉุชุง  ุญุงู  ูฺบุ ูพุฑูฺฏุฑุงููฺฏ ุฒุจุงููฺบ ู ููุงุงฺบ ุชูุฌ ุญุงุตู ฺฉ ุ ุฎุงุต ุทูุฑ ูพุฑ TabNine ุงูุฑ GitHub ฺฉ Copilot ุฌุณ ูนููุฒ ฺฉ ุจุฏููุชุ ุฌู OpenAI ฺฉ Codex ูุงฺู ฺฉ ุทุงูุช ุณ ุทูู ฺฉูฺ ฺฉ ุชุณูุณู ูพุฏุง ฺฉุฑ ุณฺฉุช ฺบ ูุชู ฺฉ ุชุฎูู ฺฉุง  ฺฉุงู auto-regressive ุง ุงุณุจุงุจ ุฒุจุงู ูุงฺูุฒ ุฌุณ GPT-2 ฺฉ ุฐุฑุน ุจุชุฑู ุทุฑู ุณ ุญู ฺฉุง ุฌุง ุณฺฉุชุง 

ุงุณ ุญุต ูฺบ ู ฺฉูฺ ุฌูุฑุดู ูุงฺู ฺฉุง ุงฺฉ ูุฎุชุตุฑ ูุฑฺู ุจูุงุฆฺบ ฺฏ: ู ูฺฉูู ููฺฉุดูุฒ ุง ฺฉูุงุณุฒ ฺฉ ุจุฌุงุฆ ุงฺฉ ูุงุฆู ฺฉ ุชฺฉูู ูพุฑ ุชูุฌ ูุฑฺฉูุฒ ฺฉุฑฺบ ฺฏุ ุฌุณ ฺฉ ู Python ฺฉูฺ ฺฉุง ุงฺฉ ุฐู ูุฌููุน ุงุณุชุนูุงู ฺฉุง ุฌุงุฆ ฺฏุง ุฌุจ ุขูพ Python ูฺบ ฺูนุง ูพุฑ ฺฉุงู ฺฉุฑ ุฑ ูุช ฺบ ุชู ุขูพ ุงฺฉุซุฑ Python ฺูนุง ุณุงุฆูุณ ุงุณูนฺฉ ฺฉ ุฑุงุจุท ูฺบ ุฑุช ฺบุ ุฌุณ ูฺบ `matplotlib`, `seaborn`, `pandas`, ุงูุฑ `scikit-learn` ูุงุฆุจุฑุฑุฒ ุดุงูู ฺบ ุงู ูุฑู ูุฑฺฉุณ ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ููุช ูุฎุตูุต ฺฉูุงูฺุฒ ุชูุงุด ฺฉุฑูุง ุนุงู ุจุงุช ุ ุงุณ ู  ุจุช ููุฏ ูฺฏุง ุงฺฏุฑ ู ุงฺฉ ูุงฺู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉฺบ ุฌู ูุงุฑ ฺฉุงูุฒ ฺฉ ุชฺฉูู ฺฉุฑ ุฏ

<Youtube id="Vpjb1lu0MDk"/>

[Chapter 6](/course/chapter6) ูฺบ ู ู Python ุณูุฑุณ ฺฉูฺ ฺฉู ูพุฑุงุณุณ ฺฉุฑู ฺฉ ู ุงฺฉ ููุซุฑ tokenizer ุชุงุฑ ฺฉุงุ ูฺฉู ูฺบ ุงุจฺพ ุจฺพ ุงฺฉ ุจฺ ูพูุงู ฺฉุง ฺูนุงุณูน ุฏุฑฺฉุงุฑ  ุฌุณ ูพุฑ ูุงฺู ฺฉู ูพุฑ ูนุฑู ฺฉุง ุฌุง ุณฺฉ ุงฺบุ ู ุงูพู tokenizer ฺฉู GitHub ุฑูพูุฒูนุฑุฒ ุณ ุญุงุตู ฺฉุฑุฏ Python ฺฉูฺ ฺฉ ูุฌููุน ูพุฑ ูุงฺฏู ฺฉุฑฺบ ฺฏ ุงุณ ฺฉ ุจุนุฏ ู ูุงฺู ฺฉ ุชุฑุจุช ฺฉ ู `Trainer` API ุงูุฑ ๐ค Accelerate ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ฺู ุดุฑูุน ฺฉุฑุช ฺบ!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

 ุฏุฑุงุตู ุงุณ ูุงฺู ฺฉู ุธุงุฑ ฺฉุฑ ุฑุง  ุฌุณ ฺฉ ุชุฑุจุช ฺฉ ฺฏุฆ ุชฺพ ุงูุฑ ุงุณ Hub ูพุฑ ุงูพููฺ ฺฉุง ฺฏุง ุ ุฌุณุง ฺฉ ุงุณ ุญุต ูฺบ ุฏฺฉฺพุงุง ฺฏุง ฺฉูฺ ุงุณุชุนูุงู ฺฉุง ฺฏุง  ุขูพ ุงุณ [ุงฺบ](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28) ุฏฺฉฺพ ุณฺฉุช ฺบ ูููน ฺฉุฑฺบ ฺฉ ฺููฺฉ ูุชู ฺฉ ุชุฎูู ูฺบ ฺฉฺฺพ ุจ ุชุฑุชุจ ููุฌูุฏ ุ ุงุณ ู ุขูพ ฺฉู ููฺฉู ุทูุฑ ูพุฑ ุชฺพูฺุง ูุฎุชูู ูุชุฌ ูู ุณฺฉุชุง .

## ฺูนุง ุงฺฉูนฺพุง ฺฉุฑูุง[[gathering-the-data]]

Python ฺฉูฺ GitHub ุฌุณ ฺฉูฺ ุฑูพูุฒูนุฑุฒ ุณ ูุงูุฑ ููุฏุงุฑ ูฺบ ุฏุณุชุงุจ ุ ุฌุณ ฺฉุง ุงุณุชุนูุงู ู ุฑ Python ุฑูพูุฒูนุฑ ฺฉ ุณฺฉุฑูพูฺฏ ฺฉุฑ ฺฉ ุงฺฉ ฺูนุงุณูน ุจูุงู ฺฉ ู ฺฉุฑ ุณฺฉุช ฺบ  ู ุทุฑู ุชฺพุง ุฌู [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) ูฺบ ุงุณุชุนูุงู ฺฉุง ฺฏุง ุชฺพุง ุชุงฺฉ ุงฺฉ ุจฺ GPT-2 ูุงฺู ฺฉู ูพุฑ ูนุฑู ฺฉุง ุฌุง ุณฺฉ `codeparrot` ูุงู ุชูุฑุจุงู 180 GB ฺฉ GitHub ฺููพ ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆุ ุฌุณ ูฺบ ุชูุฑุจุงู 20 ููู Python ูุงุฆูฺบ ุดุงูู ุชฺพฺบุ ูุตููู ู ุงฺฉ ฺูนุงุณูน ุชุงุฑ ฺฉุง ุฌุณ ุงููฺบ ู ุจุนุฏ ูฺบ [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot) ูพุฑ ุดุฆุฑ ฺฉุง

ุชุงูุ ูฺฉูู ูุฌููุน ูพุฑ ุชุฑุจุช ุฏูุง ููุช ุงูุฑ ฺฉููพููนูฺฏ ูุณุงุฆู ฺฉุง ุจุช ุฒุงุฏ ูุทุงูุจ ฺฉุฑุชุง ุ ุงูุฑ ูฺบ ุตุฑู ุงุณ ฺูนุงุณูน ฺฉุง ู ุญุต ฺุง ุฌู Python ฺูนุง ุณุงุฆูุณ ุงุณูนฺฉ ุณ ูุชุนูู ู ุชูุ ุขุฆ `codeparrot` ฺูนุงุณูน ฺฉู ูููนุฑ ฺฉุฑูุง ุดุฑูุน ฺฉุฑุช ฺบ ุชุงฺฉ ุงู ุชูุงู ูุงุฆููฺบ ฺฉู ููุชุฎุจ ฺฉุง ุฌุง ุณฺฉ ุฌู ูฺบ ุงุณ ุงุณูนฺฉ ฺฉ ฺฉูุฆ ูุงุฆุจุฑุฑ ุดุงูู ู ฺูนุงุณูน ฺฉ ุญุฌู ฺฉ ูุฌ ุณุ ู ุงุณ ฺุงุคู ููฺ ฺฉุฑู ุณ ุจฺูุง ฺุงุช ฺบุ ุงุณ ฺฉ ุจุฌุงุฆุ ู ุณูนุฑููฺฏ ูฺุฑ ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ุงุณ ูููนุฑ ฺฉุฑฺบ ฺฏ ุงู ูุงุฆุจุฑุฑุฒ ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ฺฉูฺ ูููููฺบ ฺฉู ูููนุฑ ฺฉุฑู ูฺบ ูุงุฑ ูุฏุฏ ฺฉ ูุ ู ููุฏุฑุฌ ุฐู ููฺฉุดู ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ:


```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

ุขุฆ ุงุณ ุฏู ูุซุงููฺบ ูพุฑ ุขุฒูุงุฆฺบ:


```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

ู ุงุณ ุงฺฉ ููฺฉุดู ุจูุงู ฺฉ ู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ุฌู ฺูนุงุณูน ฺฉู ุงุณูนุฑู ฺฉุฑ ฺฏุง ุงูุฑ ุงู ุนูุงุตุฑ ฺฉู ูููนุฑ ฺฉุฑ ฺฏุง ุฌูฺบ ู ฺุงุช ฺบ:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```
ูพฺพุฑ ู ุจุณ ุงุณ ููฺฉุดู ฺฉู ุงุณูนุฑููฺฏ ฺูนุงุณูน ูพุฑ ูุงฺฏู ฺฉุฑ ุณฺฉุช ฺบ

```py
# This cell will take a very long time to execute, so you should skip it and go to
# the next one!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

 ูฺบ ุงุตู ฺูนุงุณูน ฺฉุง ุชูุฑุจุงู 3ูช ฺฺพูฺ ุฏุชุง ุ ุฌู ุงุจ ุจฺพ ฺฉุงู ุจฺุง  -- ูุชุฌ ุฎุฒ ฺูนุงุณูน 6 ุฌ ุจ ฺฉุง  ุงูุฑ ุงุณ ูฺบ 600,000 ูพุงุฆุชฺพูู ุงุณฺฉุฑูพูนุณ ุดุงูู ฺบ!  

ูฺฉูู ฺูนุงุณูน ฺฉู ูููนุฑ ฺฉุฑู ูฺบ ุขูพ ฺฉ ูุดู ุงูุฑ ุจูฺูุชฺพ ูพุฑ ููุญุตุฑ ฺฉุฑุช ูุฆ 2-3 ฺฏฺพููน ูฺฏ ุณฺฉุช ฺบ ุงฺฏุฑ ุขูพ ุฎูุฏ ุงุณ ุทูู ุนูู ุณ ูฺบ ฺฏุฒุฑูุง ฺุงุช ุชู ู ุขูพ ฺฉ ู ุจ ูพุฑ ูููนุฑ ุดุฏ ฺูนุงุณูน ูุฑุงู ฺฉุฑุช ฺบ ุชุงฺฉ ุขูพ ุงุณ ฺุงุคู ููฺ ฺฉุฑ ุณฺฉฺบ

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>  

ููฺฏูุฌ ูุงฺู ฺฉ ูพุฑ ูนุฑููฺฏ ูฺบ ฺฉฺฺพ ููุช ูฺฏ ฺฏุง ู ุชุฌูุฒ ฺฉุฑุช ฺบ ฺฉ ุขูพ ูพู ฺูนุง ฺฉ ุงฺฉ ูููู ูพุฑ ูนุฑููฺฏ ูููพ ฺูุง ฺฉุฑ ุขุฒูุงุฆฺบุ ุงู ุฏู ุฌุฒู ูุงุฆููฺบ ฺฉู ุงู ฺฉููููน ฺฉุฑ ฺฉุ ุงูุฑ ูู ุจูุงุฆฺบ ฺฉ ูนุฑููฺฏ ฺฉุงูุงุจ ุณ ูฺฉูู ู ุฑ  ุงูุฑ ูุงฺูุฒ ูุญููุธ ู ุฑ ฺบ ูนุฑููฺฏ ฺฉ ุขุฎุฑ ูุฑุญู ูฺบ ุตุฑู ุงุณ ู ูุงฺฉุงู ููุง ฺฉ ุขูพ ู ูููฺุฑ ุจูุงูุง ุจฺพูู ฺฏุฆ ุง ูนุฑููฺฏ ูููพ ฺฉ ุขุฎุฑ ูฺบ ฺฉูุฆ ูนุงุฆูพู ุฑ ฺฏุงุ ุงูุชุงุฆ ูุงูุณ ฺฉู ู ุณฺฉุชุง !  

</Tip>  

ุขุฆ ฺูนุงุณูน ุณ ุงฺฉ ูุซุงู ุฏฺฉฺพุช ฺบ ู ุตุฑู ุฑ ููฺ ฺฉ ูพู 200 ฺฉุฑฺฉูนุฑุฒ ุฏฺฉฺพุงุฆฺบ ฺฏ:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

ู ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ `content` ููฺ ูฺบ ู ฺฉูฺ ููุฌูุฏ  ุฌุณ ูพุฑ ู ุงูพูุง ูุงฺู ุชุฑุจุช ุฏูุง ฺุงุช ฺบ ุงุจ ุฌุจฺฉ ูุงุฑ ูพุงุณ ฺูนุงุณูน ููุฌูุฏ ุ ูฺบ ูุชูู ฺฉู ุงุณ ูุงุฑููน ูฺบ ุชุงุฑ ฺฉุฑู ฺฉ ุถุฑูุฑุช  ุฌู ูพุฑ ูนุฑููฺฏ ฺฉ ู ููุฒูฺบ ู

## ฺูนุงุณูน ฺฉ ุชุงุฑ[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

ูพูุง ูุฏู ฺูนุง ฺฉู tokenize ฺฉุฑูุง ูฺฏุง ุชุงฺฉ ู ุงุณ ุชุฑุจุช ฺฉ ู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉฺบ ฺููฺฉ ูุงุฑุง ููุตุฏ ุจูุงุฏ ุทูุฑ ูพุฑ ูุฎุชุตุฑ ููฺฉุดู ฺฉุงูุฒ ฺฉู ุขูนู ฺฉููพููน ฺฉุฑูุง ุ ู context size ฺฉู ูุณุจุชุงู ฺฺพููนุง ุฑฺฉฺพ ุณฺฉุช ฺบ ุงุณ ฺฉุง ูุงุฆุฏ   ฺฉ ู ูุงฺู ฺฉู ุจุช ุชุฒ ุณ ุชุฑุจุช ุฏ ุณฺฉุช ฺบ ุงูุฑ ุงุณ ฺฉ ู ููุงุงฺบ ุทูุฑ ูพุฑ ฺฉู ูููุฑ ุฏุฑฺฉุงุฑ ูฺฏ ุงฺฏุฑ ุขูพ ฺฉ ุงูพูฺฉุดู ฺฉ ู ุฒุงุฏ context ููุง ุถุฑูุฑ  (ูุซูุงู ุงฺฏุฑ ุขูพ ฺุงุช ฺบ ฺฉ ูุงฺู ฺฉุณ ููฺฉุดู ฺฉ ุชุนุฑู ูุงู ูุงุฆู ฺฉ ุจูุงุฏ ูพุฑ ูููน ูนุณูน ูฺฉฺพ)ุ ุชู ุงุณ ุชุนุฏุงุฏ ูฺบ ุงุถุงู ฺฉุฑูุง ูู ุจูุงุฆฺบุ ูฺฏุฑ  ุจฺพ ุฐู ูฺบ ุฑฺฉฺพฺบ ฺฉ ุงุณ ุณ GPU ูููุฑ ฺฉุง ุจูุฌฺพ ุจฺพ ุจฺฺพ ุฌุงุฆ ฺฏุง ู ุงูุญุงูุ ุขุฆ context size ฺฉู 128 tokens ูพุฑ ููุฑุฑ ฺฉุฑ ุฏุช ฺบุ ุฌุจฺฉ GPT-2 ุง GPT-3 ูฺบ ุจุงูุชุฑุชุจ 1,024 ุง 2,048 tokens ุงุณุชุนูุงู ูุช ฺบ

ุฒุงุฏ ุชุฑ ุฏุณุชุงูุฒุงุช ูฺบ 128 tokens ุณ ฺฉฺบ ุฒุงุฏ ููุงุฏ ูุชุง ุ ููฐุฐุง ุตุฑู inputs ฺฉู ุฒุงุฏ ุณ ุฒุงุฏ ููุจุงุฆ ุชฺฉ truncate ฺฉุฑู ุณ ูุงุฑ ฺูนุงุณูน ฺฉุง ุงฺฉ ุจฺุง ุญุต ุฎุชู ู ุฌุงุฆ ฺฏุง ุงุณ ฺฉ ุจุฌุงุฆุ ู `return_overflowing_tokens` ุขูพุดู ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ุชุงฺฉ ูพูุฑ input ฺฉู tokenize ฺฉุง ุฌุง ุณฺฉ ุงูุฑ ุงุณ ฺฉุฆ ุญุตูฺบ ูฺบ ุชูุณู ฺฉุง ุฌุง ุณฺฉุ ุฌุณุง ฺฉ ู ู [Chapter 6](/course/chapter6/4) ูฺบ ฺฉุง ุชฺพุง ู `return_length` ุขูพุดู ฺฉุง ุจฺพ ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ุชุงฺฉ ุฑ ุจูุงุฆ ฺฏุฆ ูนฺฉฺ ฺฉ ููุจุงุฆ ุฎูุฏ ุจุฎูุฏ ูุงูพุณ ูู ุฌุงุฆ ุงฺฉุซุฑ ุขุฎุฑ ูนฺฉฺุง context size ุณ ฺฺพููนุง ูฺฏุงุ ุงูุฑ ู padding ฺฉ ูุณุงุฆู ุณ ุจฺู ฺฉ ู ุงู ุญุตูฺบ ฺฉู ูนุง ุฏฺบ ฺฏุ ูฺบ ูุงูุน ุงู ฺฉ ุถุฑูุฑุช ูฺบ  ฺฉููฺฉ ูุงุฑ ูพุงุณ ฺฉุงู ฺูนุง ููุฌูุฏ 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

ุขุฆ ุฏฺฉฺพุช ฺบ ฺฉ  ฺฉุณ ฺฉุงู ฺฉุฑุชุง ุ ูพู ุฏู ูุซุงููฺบ ฺฉู ุฏฺฉฺพ ฺฉุฑ:


```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ู ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ ุงู ุฏู ูุซุงููฺบ ุณ ูฺบ ฺฉู 34 ุณฺฏูููนุณ ููุช ฺบ ุงฺฏุฑ ู ฺูฺฉ ฺฉ ููุจุงุฆูฺบ ฺฉู ุฏฺฉฺพฺบ ุชู ูุนููู ูุชุง  ฺฉ ุฏูููฺบ ฺุงฺฉููููนุณ ฺฉ ุขุฎุฑ ูฺบ ููุฌูุฏ ฺูฺฉุณ ูฺบ 128 ุณ ฺฉู ูนูฺฉูุฒ ฺบ (ุจุงูุชุฑุชุจ 117 ุงูุฑ 41)  ูุงุฑ ฺฉู ฺูฺฉุณ ฺฉุง ุงฺฉ ฺฺพููนุง ุณุง ุญุต ฺบุ ุงุณ ู ู ุงูฺบ ูุญููุธ ุทุฑู ุณ ุฎุงุฑุฌ ฺฉุฑ ุณฺฉุช ฺบ `overflow_to_sample_mapping` ููฺ ฺฉ ูุฏุฏ ุณ ู  ุจฺพ ูุนููู ฺฉุฑ ุณฺฉุช ฺบ ฺฉ ฺฉูู ุณ ฺูฺฉุณ ฺฉุณ ุงู ูพูน ุณููพู ุณ ุชุนูู ุฑฺฉฺพุช ุชฺพ  

ุงุณ ุขูพุฑุดู ฺฉ ุฏูุฑุงู ู ๐ค Datasets ฺฉ `Dataset.map()` ููฺฉุดู ฺฉ ุงฺฉ ููุฏ ุฎุตูุตุช ุงุณุชุนูุงู ฺฉุฑ ุฑ ฺบุ ุฌู   ฺฉ ุงุณ ุงฺฉ ุณ ุงฺฉ ููพูฺฏ ฺฉ ุถุฑูุฑุช ูฺบ ูุชุ ุฌุณุง ฺฉ ู ู [ุณฺฉุดู 3](/course/chapter7/3) ูฺบ ุฏฺฉฺพุงุ ู ุงู ูพูน ุจฺ ุณ ุฒุงุฏ ุง ฺฉู ุนูุงุตุฑ ฺฉ ุณุงุชฺพ ุจฺุฒ ุจูุง ุณฺฉุช ฺบ  ุฎุงุตุช ฺูนุง ุขฺฏูููนุดู ุง ฺูนุง ูููนุฑูฺฏ ุฌุณ ุขูพุฑุดูุฒ ูฺบ ููุฏ ูุช ุ ุฌุงฺบ ุนูุงุตุฑ ฺฉ ุชุนุฏุงุฏ ุชุจุฏู ูุช  ูุงุฑ ูุนุงูู ูฺบุ ุฌุจ ู ุฑ ุนูุตุฑ ฺฉู ูุฎุตูุต ฺฉุงููนฺฉุณูน ุณุงุฆุฒ ฺฉ ฺูฺฉุณ ูฺบ ูนูฺฉูุงุฆุฒ ฺฉุฑุช ฺบุ ุชู ุฑ ฺุงฺฉููููน ุณ ฺฉุฆ ุณููพูุฒ ุจู ุฌุงุช ฺบ ูฺบ ุจุณ  ูู ุจูุงูุง ูุชุง  ฺฉ ููุฌูุฏ ฺฉุงููุฒ ฺฉู ุญุฐู ฺฉุฑ ุฏฺบุ ฺฉููฺฉ ุงู ฺฉ ุณุงุฆุฒ ูฺบ ุชุถุงุฏ ู ุณฺฉุชุง  ุงฺฏุฑ ู ุงูฺบ ุจุฑูุฑุงุฑ ุฑฺฉฺพูุง ฺุงุช ุชู ุงูฺบ ููุงุณุจ ุทุฑู ุณ ุฏุฑุงุง ุฌุง ุณฺฉุชุง ุชฺพุง ุงูุฑ `Dataset.map()` ฺฉุงู ฺฉ ุงูุฏุฑ ูุงูพุณ ฺฉุง ุฌุง ุณฺฉุชุง ุชฺพุง


```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```
ุงุจ ูุงุฑ ูพุงุณ 16.7 ููู ูุซุงูฺบ ฺบุ ุฌู ูฺบ ุณ ุฑ ุงฺฉ ูฺบ 128 ูนูฺฉูุฒ ฺบุ ุฌู ฺฉู ููุง ฺฉุฑ ุชูุฑุจุงู 2.1 ุจูู ูนูฺฉูุฒ ุจูุช ฺบ ุญูุงู ฺฉ ูุ OpenAI ฺฉ GPT-3 ุงูุฑ Codex ูุงฺูุฒ ุจุงูุชุฑุชุจ 300 ุงูุฑ 100 ุจูู ูนูฺฉูุฒ ูพุฑ ุชุฑุจุช ุงูุช ฺบุ ุฌุงฺบ Codex ูุงฺูุฒ ฺฉู GPT-3 ฺฺฉ ูพูุงุฆููนุณ ุณ ุงูุดูุงุฆุฒ ฺฉุง ฺฏุง  ุงุณ ุณฺฉุดู ูฺบ ูุงุฑุง ููุตุฏ ุงู ูุงฺูุฒ ฺฉุง ููุงุจู ฺฉุฑูุง ูฺบ ุ ุฌู ุทููุ ูุฑุจูุท ูุชู ุชุฎูู ฺฉุฑ ุณฺฉุช ฺบุ ุจูฺฉ ฺูนุง ุณุงุฆูุณุฏุงููฺบ ฺฉ ู ุงฺฉ ฺฺพููน ูพูุงู ูพุฑุ ุชุฒ ุขูนู ฺฉููพููน ููฺฉุดู ุจูุงูุง   

ุงุจ ุฌุจ ฺฉ ูุงุฑุง ฺูนุงุณูน ุชุงุฑ ุ ุขุฆ ูุงฺู ฺฉู ุณูน ุงูพ ฺฉุฑฺบ!  

<Tip>  

โ๏ธ **ุขุฒูุง ฺฉุฑ ุฏฺฉฺพฺบ!** ุงู ุชูุงู ฺูฺฉุณ ฺฉู ูนุงูุง ุฌู ฺฉุงููนฺฉุณูน ุณุงุฆุฒ ุณ ฺฺพููน ฺบุ ุงฺบ ฺฉูุฆ ุจฺุง ูุณุฆู ูฺบ ุชฺพุง ฺฉููฺฉ ู ฺฺพููน ฺฉุงููนฺฉุณูน ููฺูุฒ ุงุณุชุนูุงู ฺฉุฑ ุฑ ฺบ ุฌุณ ุฌุณ ุขูพ ฺฉุงููนฺฉุณูน ุณุงุฆุฒ ุจฺฺพุงุฆฺบ ฺฏ (ุง ุงฺฏุฑ ุขูพ ฺฉ ูพุงุณ ูุฎุชุตุฑ ฺุงฺฉููููนุณ ฺฉุง ฺฉุงุฑูพุณ )ุ ุชู ุงู ฺูฺฉุณ ฺฉุง ุชูุงุณุจ ุจฺพ ุจฺฺพ ฺฏุง ุฌู ุถุงุฆุน ฺฉ ุฌุง ุฑ ฺบ ฺูนุง ุชุงุฑ ฺฉุฑู ฺฉุง ุงฺฉ ุฒุงุฏ ูุคุซุฑ ุทุฑู   ฺฉ ุชูุงู ูนูฺฉูุงุฆุฒฺ ุณููพูุฒ ฺฉู ุงฺฉ ุจฺ ูฺบ `eos_token_id` ูนูฺฉู ฺฉ ุณุงุชฺพ ุฌูฺฺบุ ุงูุฑ ูพฺพุฑ ฺฉูฺฉูนููนฺ ุณฺฉููุณุฒ ูพุฑ ฺูฺฉูฺฏ ฺฉุฑฺบ ุงฺฉ ูุดู ฺฉ ุทูุฑ ูพุฑุ `tokenize()` ููฺฉุดู ูฺบ ุชุฑูู ฺฉุฑฺบ ุชุงฺฉ ู ุงุณ ุทุฑู ฺฉู ุงูพูุงุฆ ุงุฏ ุฑฺฉฺพฺบ ฺฉ ุขูพ ฺฉู `truncation=False` ุณูน ฺฉุฑูุง ูฺฏุง ุงูุฑ ูนูฺฉูุงุฆุฒุฑ ุณ ุฏฺฏุฑ ุขุฑฺฏููููนุณ ูนุงู ูฺบ ฺฏ ุชุงฺฉ ูฺฉูู ุณฺฉููุณ ุขู ูนูฺฉู ุขุฆ ฺุฒ ุญุงุตู ู ุณฺฉ  

</Tip>  

## ูุง ูุงฺู ุงูุดูุงุฆุฒ ฺฉุฑูุง[[initializing-a-new-model]]  

ูุงุฑุง ูพูุง ูุฏู ุงฺฉ ูุง GPT-2 ูุงฺู ุงูุดูุงุฆุฒ ฺฉุฑูุง  ู ุงูพู ูุงฺู ฺฉ ู ู ฺฉููฺฏุฑุดู ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ุฌู ฺฺพููน GPT-2 ูุงฺู ฺฉ ู ุ ุงุณ ู ู ูพู ุณ ุชุฑุจุช ุงูุช ฺฉููฺฏุฑุดู ููฺ ฺฉุฑฺบ ฺฏุ ูู ุจูุงุฆฺบ ฺฏ ฺฉ ูนูฺฉูุงุฆุฒุฑ ุณุงุฆุฒ ูุงฺู ฺฉ ูฺฉุจูุฑ ุณุงุฆุฒ ุณ ูู ฺฉฺพุงุชุง ุ ุงูุฑ `bos` ุงูุฑ `eos` (ุณฺฉููุณ ฺฉ ุขุบุงุฒ ุงูุฑ ุงุฎุชุชุงู) ฺฉ ูนูฺฉู ุขุฆ ฺุฒ ูพุงุณ ฺฉุฑฺบ ฺฏ


{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ุงุณ ฺฉููฺฏุฑุดู ฺฉ ุณุงุชฺพุ ู ุงฺฉ ูุง ูุงฺู ููฺ ฺฉุฑ ุณฺฉุช ฺบ ูููน ฺฉุฑฺบ ฺฉ  ูพูุง ูููุน  ุฌุจ ู `from_pretrained()` ููฺฉุดู ุงุณุชุนูุงู ูฺบ ฺฉุฑ ุฑุ ฺฉููฺฉ ู ุฏุฑุญููุช ุฎูุฏ ุณ ุงฺฉ ูุงฺู ุงูุดูุงุฆุฒ ฺฉุฑ ุฑ ฺบ:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ุงุณ ฺฉููฺฏุฑุดู ฺฉ ุณุงุชฺพุ ู ุงฺฉ ูุง ูุงฺู ููฺ ฺฉุฑ ุณฺฉุช ฺบ ูููน ฺฉุฑฺบ ฺฉ  ูพูุง ูููุน  ุฌุจ ู `from_pretrained()` ููฺฉุดู ุงุณุชุนูุงู ูฺบ ฺฉุฑ ุฑุ ฺฉููฺฉ ู ุญููุช ูฺบ ุฎูุฏ ุณ ุงฺฉ ูุงฺู ุงูุดูุงุฆุฒ ฺฉุฑ ุฑ ฺบ:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

ูุงุฑ ูุงฺู ูฺบ 124 ููู ูพุฑุงููนุฑุฒ ฺบ ุฌูฺบ ูฺบ ูนูู ฺฉุฑูุง ูฺฏุง ุชุฑุจุช ุดุฑูุน ฺฉุฑู ุณ ูพูุ ูฺบ ุงฺฉ ฺูนุง ฺฉูููนุฑ ุณูน ุงูพ ฺฉุฑูุง ูฺฏุง ุฌู ุจฺุฒ ุจูุงู ฺฉุง ุฎุงู ุฑฺฉฺพ ฺฏุง ู `DataCollatorForLanguageModeling` ฺฉูููนุฑ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบุ ุฌู ุฎุงุต ุทูุฑ ูพุฑ ููฺฏูุฌ ูุงฺููฺฏ ฺฉ ู ฺุฒุงุฆู ฺฉุง ฺฏุง  (ุฌุณุง ฺฉ ูุงู ุณ ุธุงุฑ ูุชุง )  ุจฺุฒ ฺฉู ุงุณูนฺฉ ุงูุฑ ูพฺ ฺฉุฑู ฺฉ ุนูุงู ููฺฏูุฌ ูุงฺู ฺฉ ูุจูุฒ ุจูุงู ฺฉุง ุจฺพ ุฎุงู ุฑฺฉฺพุชุง  โ ฺฉููฺฉ ฺฉุงุฒูู ููฺฏูุฌ ูุงฺููฺฏ ูฺบ ุงู ูพูนุณ  ูุจูุฒ ฺฉ ุทูุฑ ูพุฑ ฺฉุงู ฺฉุฑุช ฺบ (ุจุณ ุงฺฉ ุนูุตุฑ ุณ ุดููน ฺฉุฑ ฺฉ)ุ ุงูุฑ  ฺูนุง ฺฉูููนุฑ ุงูฺบ ุชุฑุจุช ฺฉ ุฏูุฑุงู ุฎูุฏฺฉุงุฑ ุทูุฑ ูพุฑ ุจูุงุชุง ุ ููฐุฐุง ูฺบ `input_ids` ฺฉู ฺูพูฺฉูน ฺฉุฑู ฺฉ ุถุฑูุฑุช ูฺบ ูุช  

ูููน ฺฉุฑฺบ ฺฉ `DataCollatorForLanguageModeling` ูุงุณฺฉฺ ููฺฏูุฌ ูุงฺููฺฏ (MLM) ุงูุฑ ฺฉุงุฒูู ููฺฏูุฌ ูุงฺููฺฏ (CLM) ุฏูููฺบ ฺฉู ุณูพูุฑูน ฺฉุฑุชุง  ฺูุงููน ฺฉ ุทูุฑ ูพุฑ  ฺูนุง ฺฉู MLM ฺฉ ู ุชุงุฑ ฺฉุฑุชุง ุ ูฺฉู ู `mlm=False` ุณูน ฺฉุฑ ฺฉ CLM ูฺบ ุชุจุฏู ฺฉุฑ ุณฺฉุช ฺบ:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

ฺูฺบ ุงฺฉ ูุซุงู ูพุฑ ูุธุฑ ฺุงูุช ฺบ:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

ู ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ ูุซุงูฺบ ุงุณูนฺฉ ฺฉ ฺฏุฆ ฺบ ุงูุฑ ุชูุงู ูนูุณุฑุฒ ฺฉ ุดฺฉู ุงฺฉ ุฌุณ 

{#if fw === 'tf'}

ุงุจ ู `prepare_tf_dataset()` ูุชฺพฺ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ุชุงฺฉ ุงูพู ฺูนุงุณูนุณ ฺฉู TensorFlow ฺูนุงุณูนุณ ูฺบ ุชุจุฏู ฺฉุฑฺบุ ุงุณ ฺูนุง ฺฉูููนุฑ ฺฉ ุณุงุชฺพ ุฌู ู ู ุงููพุฑ ุจูุงุง ุชฺพุง:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

โ๏ธ ุงู ูพูนุณ ุงูุฑ ูุจูุฒ ฺฉู ุณุฏฺพ ูฺบ ูุงู ฺฉ ู ุดููน ฺฉุฑูุง ูุงฺู ฺฉ ุงูุฏุฑ ูุชุง ุ ููฐุฐุง ฺูนุง ฺฉูููนุฑ ุตุฑู ุงู ูพูนุณ ฺฉู ฺฉุงูพ ฺฉุฑ ฺฉ ูุจูุฒ ุจูุงุชุง 

</Tip>

ุงุจ ูุงุฑ ูพุงุณ ุณุจ ฺฉฺฺพ ุชุงุฑ  ุชุงฺฉ ู ุญููุช ูฺบ ุงูพู ูุงฺู ฺฉู ูนุฑู ฺฉุฑ ุณฺฉฺบ โ ุขุฎุฑฺฉุงุฑ  ุฒุงุฏ ูุดฺฉู ฺฉุงู ูฺบ ุชฺพุง! ุชุฑุจุช ุดุฑูุน ฺฉุฑู ุณ ูพูุ ูฺบ Hugging Face ูฺบ ูุงฺฏ ุงู ฺฉุฑูุง ฺุง ุงฺฏุฑ ุขูพ ูููน ุจฺฉ ูฺบ ฺฉุงู ฺฉุฑ ุฑ ฺบุ ุชู ุขูพ ุฏุฑุฌ ุฐู ููนููน ููฺฉุดู ุณ ุงุณุง ฺฉุฑ ุณฺฉุช ฺบ:

```python
from huggingface_hub import notebook_login

notebook_login()
```

 ุงฺฉ ูุฌูน ุฏฺฉฺพุงุฆ ฺฏุง ุฌุงฺบ ุขูพ ุงูพู Hugging Face ูุงฺฏ ุงู ุงุณูุงุฏ ุฏุฑุฌ ฺฉุฑ ุณฺฉุช ฺบ  

ุงฺฏุฑ ุขูพ ูููน ุจฺฉ ูฺบ ฺฉุงู ูฺบ ฺฉุฑ ุฑุ ุชู ุจุณ ุงูพู ูนุฑููู ูฺบ ุฏุฑุฌ ุฐู ูุงุฆู ูนุงุฆูพ ฺฉุฑฺบ:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

ุงุจ ุตุฑู ูนุฑููฺฏ ฺฉ ุขุฑฺฏููููนุณ ฺฉู ฺฉููฺฏุฑ ฺฉุฑูุง ุงูุฑ `Trainer` ฺฉู ุดุฑูุน ฺฉุฑูุง ุจุงู  ู ุงฺฉ ฺฉูุณุงุฆู ูุฑููฺฏ ุฑูน ุดฺูู ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ุฌุณ ูฺบ ฺฉฺฺพ ูุงุฑู ุงูพ ูฺฏุง ุงูุฑ ุงฺฉ ูุคุซุฑ ุจฺ ุณุงุฆุฒ 256 ูฺฏุง (`per_device_train_batch_size` * `gradient_accumulation_steps`)  

ฺฏุฑฺููน ุงฺฉููููุดู ุงุณ ููุช ุงุณุชุนูุงู ูุช  ุฌุจ ุงฺฉ ูุงุญุฏ ุจฺ ูููุฑ ูฺบ ููน ูฺบ ูุชุงุ ุงูุฑ  ฺฉุฆ ูุงุฑูุฑฺ/ุจฺฉ ูุฑฺ ูพุงุณุฒ ฺฉ ุฐุฑุน ฺฏุฑฺููน ฺฉู ุจุชุฏุฑุฌ ุจูุงุช  ู ุงุณ ุนูู ุทูุฑ ูพุฑ ุฏฺฉฺพฺบ ฺฏ ุฌุจ ู ๐ค Accelerate ฺฉ ุณุงุชฺพ ูนุฑููฺฏ ูููพ ุจูุงุฆฺบ ฺฏ


```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ุงุจ ู ุจุณ `Trainer` ฺฉู ุดุฑูุน ฺฉุฑ ุณฺฉุช ฺบ ุงูุฑ ุชุฑุจุช ฺฉ ูฺฉูู ูู ฺฉุง ุงูุชุธุงุฑ ฺฉุฑ ุณฺฉุช ฺบ  ุงุณ ุจุงุช ูพุฑ ููุญุตุฑ  ฺฉ ุขูพ ุงุณ ูพูุฑ ุชุฑุจุช ุณูน ูพุฑ ฺูุงุช ฺบ ุง ุงุณ ฺฉ ฺฉุณ ุฐู ุญุต ูพุฑุ ุฌุณ ฺฉ ูุทุงุจู  20 ุง 2 ฺฏฺพููน ู ฺฏุงุ ููฐุฐุง ฺูุฏ ฺฉุงู ฺฉุง ฺฉูพ ุงูุฑ ุงฺฉ ุงฺฺพ ฺฉุชุงุจ ุงูนฺพุง ูฺบ!

```py
trainer.train()
```

ุชุฑุจุช ูฺฉูู ูู ฺฉ ุจุนุฏุ ู ูุงฺู ุงูุฑ tokenizer ฺฉู Hub ูพุฑ ุฏฺพฺฉู ุณฺฉุช ฺบ:

```py
trainer.push_to_hub()
```

{:else}

ุงุจ ุตุฑู ุงุชูุง ฺฉุฑูุง ุจุงู ุฑ ฺฏุง  ฺฉ ุชุฑุจุช ุงุฆูพุฑูพุฑุง ููนุฑุฒ ฺฉู ุชุฑุชุจ ุฏฺบ ุงูุฑ `compile()` ุงูุฑ `fit()` ฺฉู ฺฉุงู ฺฉุฑฺบ ู ุชุฑุจุช ฺฉ ุงุณุชุญฺฉุงู ฺฉู ุจุชุฑ ุจูุงู ฺฉ ู ฺฉฺฺพ warmup ฺฉ ุณุงุชฺพ learning rate schedule ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ุงุจ ู ุจุณ `model.fit()` ฺฉู ฺฉุงู ฺฉุฑ ุณฺฉุช ฺบ ุงูุฑ ุชุฑุจุช ฺฉ ูฺฉูู ูู ฺฉุง ุงูุชุธุงุฑ ฺฉุฑ ุณฺฉุช ฺบ  ุงุณ ุจุงุช ูพุฑ ููุญุตุฑ  ฺฉ ุขูพ ุงุณ ูพูุฑ ุชุฑุจุช ุณูน ูพุฑ ฺูุงุช ฺบ ุง ุงุณ ฺฉ ฺฉุณ ุฐู ุญุต ูพุฑุ ุฌุณ ฺฉ ูุทุงุจู  20 ุง 2 ฺฏฺพููน ู ฺฏุงุ ููฐุฐุง ฺูุฏ ฺฉุงู ฺฉุง ฺฉูพ ุงูุฑ ุงฺฉ ุงฺฺพ ฺฉุชุงุจ ุงูนฺพุง ูฺบ! ุชุฑุจุช ูฺฉูู ูู ฺฉ ุจุนุฏ ู ูุงฺู ุงูุฑ tokenizer ฺฉู Hub ูพุฑ ุฏฺพฺฉู ุณฺฉุช ฺบ:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

โ๏ธ **ุขุฒูุงุฆฺบ!** ฺฉฺ ูุชูู ุณ GPT-2 ฺฉ ุชุฑุจุช ุชฺฉ ูพูฺู ฺฉ ู ูฺบ `TrainingArguments` ฺฉ ุนูุงู ุชูุฑุจุงู 30 ฺฉูฺ ูุงุฆูุฒ ฺฉ ุถุฑูุฑุช ูพฺ ุงุณ ุงูพู ฺูนุงุณูน ฺฉ ุณุงุชฺพ ุขุฒูุงุฆฺบ ุงูุฑ ุฏฺฉฺพฺบ ฺฉ ฺฉุง ุขูพ ุงฺฺพ ูุชุงุฆุฌ ุญุงุตู ฺฉุฑ ุณฺฉุช ฺบ!

</Tip>

<Tip>

{#if fw === 'pt'}

๐ก ุงฺฏุฑ ุขูพ ฺฉ ูพุงุณ ุงฺฉ ุงุณ ูุดู ุชฺฉ ุฑุณุงุฆ  ุฌุณ ูฺบ ูุชุนุฏุฏ GPUs ููุฌูุฏ ูฺบุ ุชู ฺฉูฺ ฺฉู ูุงฺบ ฺูุงู ฺฉ ฺฉูุดุด ฺฉุฑฺบ `Trainer` ุฎูุฏ ุจุฎูุฏ ูุชุนุฏุฏ ูุดููฺบ ฺฉุง ุงูุชุธุงู ฺฉุฑุชุง ุ ุงูุฑ ุงุณ ุณ ุชุฑุจุช ฺฉ ุฑูุชุงุฑ ูฺบ ููุงุงฺบ ุงุถุงู ู ุณฺฉุชุง 

{:else}

๐ก ุงฺฏุฑ ุขูพ ฺฉ ูพุงุณ ุงฺฉ ุงุณ ูุดู ุชฺฉ ุฑุณุงุฆ  ุฌุณ ูฺบ ูุชุนุฏุฏ GPUs ููุฌูุฏ ูฺบุ ุชู ุขูพ `MirroredStrategy` context ุงุณุชุนูุงู ฺฉุฑู ฺฉ ฺฉูุดุด ฺฉุฑ ุณฺฉุช ฺบ ุชุงฺฉ ุชุฑุจุช ฺฉ ุฑูุชุงุฑ ูฺบ ุฎุงุทุฑ ุฎูุง ุงุถุงู ู ุขูพ ฺฉู ุงฺฉ `tf.distribute.MirroredStrategy` ุขุจุฌฺฉูน ุจูุงูุง ูฺฏุงุ ุงูุฑ ุงุณ ุจุงุช ฺฉู ูู ุจูุงูุง ูฺฏุง ฺฉ ฺฉูุฆ ุจฺพ `to_tf_dataset()` ุง `prepare_tf_dataset()` ูุชฺพฺุฒุ ุณุงุชฺพ  ูุงฺู ฺฉ ุชุฎูู ุงูุฑ `fit()` ฺฉุงูุ ุงุณ ฺฉ `scope()` context ูฺบ ฺูุงุฆ ุฌุงุฆฺบ ุขูพ ุงุณ ฺฉ ุฏุณุชุงูุฒุงุช [ุงฺบ](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit) ุฏฺฉฺพ ุณฺฉุช ฺบ

{/if}

</Tip>

## ูพุงุฆูพ ูุงุฆู ฺฉ ุณุงุชฺพ ฺฉูฺ ุฌูุฑุดู[[code-generation-with-a-pipeline]]

ุงุจ ุณฺ ฺฉุง ููุญ ุข ฺฏุง : ุขุฆ ุฏฺฉฺพุช ฺบ ฺฉ ุชุฑุจุช ุงูุช ูุงฺู ูุงูุน ฺฉุชูุง ุงฺฺพุง ฺฉุงู ฺฉุฑุชุง ! ู ูุงฺฏุฒ ูฺบ ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ ููุตุงู (loss) ูุณูุณู ฺฉู ู ุฑุง ุ ูฺฉู ูุงฺู ฺฉู ุขุฒูุงุฆุด ูฺบ ฺุงูู ฺฉ ู ุขุฆ ุฏฺฉฺพุช ฺบ ฺฉ  ฺฉฺฺพ prompts ูพุฑ ฺฉุชูุง ูุคุซุฑ ฺฉุงู ฺฉุฑุชุง  ุงุณุง ฺฉุฑู ฺฉ ู ู ูุงฺู ฺฉู ุงฺฉ text generation `pipeline` ูฺบ ููพูนฺบ ฺฏุ ุงูุฑ ุงฺฏุฑ ฺฉูุฆ GPU ุฏุณุชุงุจ ู ุชู ุชุฒ ุฌูุฑุดู ฺฉ ู ุงุณ ุงุณ ูพุฑ ููุชูู ฺฉุฑ ุฏฺบ ฺฏ:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

ฺูฺบุ ุณุจ ุณ ูพู ุงฺฉ ุณุงุฏ ุงุณฺฉูนุฑ ูพูุงูน ุจูุงู ฺฉุง ฺฉุงู ุดุฑูุน ฺฉุฑุช ฺบ:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

ูุชุฌ ุฏุฑุณุช ูุธุฑ ุข ุฑุง  ฺฉุง  `pandas` ุขูพุฑุดู ฺฉ ู ุจฺพ ฺฉุงู ฺฉุฑุชุง ุ ุขุฆ ุฏฺฉฺพุช ฺบ ฺฉ ฺฉุง ู ุฏู arrays ุณ ุงฺฉ `DataFrame` ุจูุง ุณฺฉุช ฺบ:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

ุจุชุฑู!  ุฏุฑุณุช ุฌูุงุจ  โ ุงฺฏุฑฺ  ุฏูุจุงุฑ `x` ฺฉุงูู ุฏุงุฎู ฺฉุฑ ุฏุชุง  ฺููฺฉ ุชุงุฑ ฺฉุฑุฏ ูนูฺฉูุฒ ฺฉ ุชุนุฏุงุฏ ูุญุฏูุฏ ุ ุงุณ ู ุฏุฑุฌ ุฐู `for` ูููพ ฺฉูน ฺฏุง  ุขุฆ ุฏฺฉฺพุช ฺบ ฺฉ ฺฉุง ู ฺฉฺฺพ ุฒุงุฏ ูพฺุฏ ฺฉุฑ ุณฺฉุช ฺบ ุงูุฑ ูุงฺู ฺฉ ูุฏุฏ ุณ `groupby` ุขูพุฑุดู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

ุจุฑุง ูฺบุ  ุตุญุญ ุทุฑู  ุขุฎุฑ ูฺบุ ุขุฆ ุฏฺฉฺพุช ฺบ ฺฉ ฺฉุง ู ุงุณ `scikit-learn` ฺฉ ู ุจฺพ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ุงูุฑ ุงฺฉ ุฑูฺู ูุงุฑุณูน ูุงฺู ุณูน ุงูพ ฺฉุฑ ุณฺฉุช ฺบ:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

 ุธุงุฑ ูุชุง  ฺฉ ูุงฺู ู Python ฺูนุง ุณุงุฆูุณ ุงุณูนฺฉ ฺฉ ฺฉฺฺพ ุณููนฺฉุณ ุณฺฉฺพ ู  ุชุงูุ ุญูู ุฏูุง ูฺบ ุงุณ ฺูพูุงุฆ ฺฉุฑู ุณ ูพู ูฺบ ุงุณ ูุฒุฏ ุชูุตู ุณ ุฌุงูฺู ฺฉ ุถุฑูุฑุช ูฺฏ ุจุนุถ ุงููุงุช ูุทููุจ ฺฉุงุฑฺฉุฑุฏฺฏ ุญุงุตู ฺฉุฑู ฺฉ ู ูุงฺู ูนุฑููฺฏ ูฺบ ูุฒุฏ ุชุฎุตุต ฺฉ ุถุฑูุฑุช ูุช ุ ุฌุณ ฺฉ ุจฺ ุณุงุฆุฒ ฺฉู ูุชุญุฑฺฉ ุทูุฑ ูพุฑ ุงูพ ฺูน ฺฉุฑูุง ุง ุฎุฑุงุจ ูุซุงููฺบ ฺฉู ููุฑ ุทูุฑ ูพุฑ ฺฺพูฺ ุฏูุง  

ุงุณ ุตูุฑุชุญุงู ูฺบุ `Trainer` ฺฉู ุณุจ ฺฉูุงุณ ฺฉุฑ ฺฉ ุถุฑูุฑ ุชุจุฏูุงฺบ ุดุงูู ฺฉุฑูุง ุงฺฉ ุขูพุดู ู ุณฺฉุชุง ุ ูฺฉู ุจุนุถ ุงููุงุช ุตูุฑ ุณ ุงูพูุง ูนุฑููฺฏ ูููพ ูฺฉฺพูุง ุฒุงุฏ ุขุณุงู ูุชุง   ู ุฌฺฏ  ุฌุงฺบ ๐ค Accelerate ูุฏุฏฺฏุงุฑ ุซุงุจุช ูุชุง   

### ๐ค Accelerate ฺฉ ุณุงุชฺพ ูนุฑููฺฏ  

ู ู `Trainer` ฺฉุง ุงุณุชุนูุงู ฺฉุฑ ฺฉ ูุงฺู ูนุฑููฺฏ ุฏฺฉฺพุ ุฌู ฺฉฺฺพ ุญุฏ ุชฺฉ ุชุฎุตุต ฺฉ ุงุฌุงุฒุช ุฏุชุง  ูฺฉู ุจุนุถ ุงููุงุช ูฺบ ูฺฉูู ฺฉููนุฑูู ฺฉ ุถุฑูุฑุช ูุช ุ ุง ู ฺฉูุฆ ูููุฑุฏ ุชุจุฏูุงฺบ ฺฉุฑูุง ฺุงุช ฺบ ุงุณ ุตูุฑุช ูฺบุ ๐ค Accelerate ุงฺฉ ุจุชุฑู ุงูุชุฎุงุจ   

ฺููฺฉ ูุงุฑุง ุจูุงุฏ ููุตุฏ ฺูนุง ุณุงุฆูุณ ูุงุฆุจุฑุฑูฺบ ฺฉ ู ููุงุณุจ ุขูนู ฺฉููพูุดู ูุฑุงู ฺฉุฑูุง ุ ุงุณ ู ุงู ูููููฺบ ฺฉู ุฒุงุฏ ูุฒู ุฏูุง ุจุชุฑ ูฺฏุง ุฌู ูฺบ `matplotlib.pyplot`, `pandas`, ุงูุฑ `sklearn` ุฌุณ ูุงุฆุจุฑุฑูฺบ ฺฉุง ุฒุงุฏ ุงุณุชุนูุงู ูุชุง  ู ุนุงู ุทูุฑ ูพุฑ ุงุณุชุนูุงู ูู ูุงู ุงููุงุธ ุฌุณ `plt`, `pd`, `sk`, `fit`, ุงูุฑ `predict` ฺฉ ุฐุฑุน ุงู ูููููฺบ ฺฉู ุขุณุงู ุณ ุดูุงุฎุช ฺฉุฑ ุณฺฉุช ฺบ


```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

ุฒุจุฑุฏุณุชุ  ุงฺฺพ ุทุฑุญ ฺฉุงู ฺฉุฑ ุฑุง ! ุงุจ ู ุงฺฉ ฺฉุณูนู ููุตุงู (loss) ููฺฉุดู ูฺฉฺพ ุณฺฉุช ฺบ ุฌู ุงู ูพูน ุณฺฉููุณุ logitsุ ุงูุฑ ู key tokens ฺฉู ุจุทูุฑ ุงู ูพูน ูุชุง  ุฌู ู ู ุงุจฺพ ููุชุฎุจ ฺฉ ฺบ ุณุจ ุณ ูพู ูฺบ logits ุงูุฑ inputs ฺฉู ู ุขูฺฏ (align) ฺฉุฑูุง : input ุณฺฉููุณ ุฌุณ ุงฺฉ token ุฏุงุฆฺบ ุดููน ฺฉุง ฺฏุง ุ ูุจูุฒ ุจูุงุชุง ุ ฺฉููฺฉ ุงฺฏูุง token ููุฌูุฏ token ฺฉุง ูุจู ูุชุง  ู  ุงุณ ุทุฑุญ ุญุงุตู ฺฉุฑ ุณฺฉุช ฺบ ฺฉ ูุจูุฒ ฺฉู input ุณฺฉููุณ ฺฉ ุฏูุณุฑ token ุณ ุดุฑูุน ฺฉุฑฺบุ ฺฉููฺฉ ูุงฺู ูพู token ฺฉ ู ูพุด ฺฏูุฆ ูฺบ ฺฉุฑุชุง ูพฺพุฑ ู ุขุฎุฑ logit ฺฉู ฺฉุงูน ุฏุช ฺบุ ฺฉููฺฉ ูุงุฑ ูพุงุณ ูฺฉูู input ุณฺฉููุณ ฺฉ ุจุนุฏ ุขู ูุงู token ฺฉ ู ฺฉูุฆ ูุจู ูฺบ ูุชุง ุงุณ ฺฉ ุณุงุชฺพ ู ุฑ sample ูพุฑ loss ุญุณุงุจ ฺฉุฑ ุณฺฉุช ฺบ ุงูุฑ ุฑ sample ูฺบ ุชูุงู keywords ฺฉ ูููุน ูพุฐุฑ ูู (occurrences) ฺฉู ฺฏู ุณฺฉุช ฺบ ุขุฎุฑ ูฺบุ ู ุงู occurrences ฺฉู weights ฺฉ ุทูุฑ ูพุฑ ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ุชูุงู samples ฺฉุง weighted average ุญุณุงุจ ฺฉุฑุช ฺบ ฺููฺฉ ู ุงู ุชูุงู samples ฺฉู ุถุงุฆุน ูฺบ ฺฉุฑูุง ฺุงุช ุฌู ูฺบ ฺฉูุฆ keywords ูฺบ ฺบุ ุงุณ ู ู weights ูฺบ 1 ุดุงูู ฺฉุฑ ุฏุช ฺบ:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

ุงุณ ุดุงูุฏุงุฑ ูุฆ ููุตุงู (loss) ููฺฉุดู ฺฉ ุณุงุชฺพ ุชุฑุจุช ุดุฑูุน ฺฉุฑู ุณ ูพูุ ูฺบ ฺูุฏ ฺุฒฺบ ุชุงุฑ ฺฉุฑู ฺฉ ุถุฑูุฑุช :

- ูฺบ ฺูนุง ฺฉู ุจฺุฒ ูฺบ ููฺ ฺฉุฑู ฺฉ ู dataloaders ฺฉ ุถุฑูุฑุช 
- ูฺบ weight decay ูพุฑุงููนุฑุฒ ุชุฑุชุจ ุฏู ฺบ
- ฺฉุจฺพ ฺฉุจฺพุงุฑ ูฺบ ูุงฺู ฺฉ ุฌุงูฺ (evaluation) ฺฉุฑู ูุช ุ ููฐุฐุง evaluation ฺฉูฺ ฺฉู ุงฺฉ ููฺฉุดู ูฺบ ููพูนูุง ูุนููู 

ุขุฆ ุณุจ ุณ ูพู dataloaders ุณ ุขุบุงุฒ ฺฉุฑุช ฺบ ูฺบ ุตุฑู ฺูนุงุณูน ฺฉ ูุงุฑููน ฺฉู `"torch"` ูพุฑ ุณูน ฺฉุฑูุง ุ ุงูุฑ ูพฺพุฑ ู ุงุณ ููุงุณุจ ุจฺ ุณุงุฆุฒ ฺฉ ุณุงุชฺพ PyTorch `DataLoader` ฺฉู ุฏ ุณฺฉุช ฺบ:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

ุงฺฏูุง ูุฑุญู   ฺฉ ู ูพุฑุงููนุฑุฒ ฺฉู ฺฏุฑููพ ฺฉุฑฺบ ุชุงฺฉ optimizer ฺฉู ูุนููู ู ฺฉ ฺฉูู ุณ ูพุฑุงููนุฑุฒ ูพุฑ ุงุถุงู weight decay ูฺฏุงุง ุฌุงุฆ ฺฏุง ุนูููุงูุ ุชูุงู bias ุงูุฑ LayerNorm ูุฒู ูุงู ูพุฑุงููนุฑุฒ ฺฉู ุงุณ ุณ ูุณุชุซููฐ ุฑฺฉฺพุง ุฌุงุชุง ุ  ุฑุง ุทุฑู:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

ฺููฺฉ ู ุชุฑุจุช ฺฉ ุฏูุฑุงู validation ุณูน ูพุฑ ูุงฺู ฺฉุง ุจุงูุงุนุฏฺฏ ุณ evaluation ฺฉุฑูุง ฺุงุช ฺบุ ุงุณ ู ุขุฆ ุงุณ ฺฉ ู ุงฺฉ ููฺฉุดู ุจฺพ ูฺฉฺพุช ฺบ  ุตุฑู evaluation dataloader ฺฉ ุฐุฑุน ฺูุชุง  ุงูุฑ ุชูุงู processes ุณ losses ฺฉู ุฌูุน ฺฉุฑุชุง :

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

`evaluate()` ููฺฉุดู ฺฉ ูุฏุฏ ุณ ู ุจุงูุงุนุฏฺฏ ุณ loss ุงูุฑ [perplexity](/course/chapter7/3) ฺฉ ุฑูพูุฑูน ฺฉุฑ ุณฺฉุช ฺบ ุงฺฏูุงุ ู ุงูพูุง ูุงฺู ุฏูุจุงุฑ ุชุนุฑู ฺฉุฑุช ฺบ ุชุงฺฉ  ูู ุจูุงุง ุฌุง ุณฺฉ ฺฉ ู ุฏูุจุงุฑ ุงุจุชุฏุงุก ุณ ุชุฑุจุช ฺฉุฑ ุฑ ฺบ:

```py
model = GPT2LMHeadModel(config)
```

ูพฺพุฑ ู ุงูพู optimizer ฺฉู ุชุนุฑู ฺฉุฑุช ฺบุ ูพู ูุงู ููฺฉุดู ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ ูุฒู ฺฉู (weight decay) ฺฉ ู ูพุฑุงููนุฑุฒ ฺฉู ุชูุณู ฺฉุฑู ฺฉ ู:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ุงุจ ุขุฆ ูุงฺูุ optimizerุ ุงูุฑ dataloaders ฺฉู ุชุงุฑ ฺฉุฑุช ฺบ ุชุงฺฉ ู ุชุฑุจุช ุดุฑูุน ฺฉุฑ ุณฺฉฺบ:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

๐จ ุงฺฏุฑ ุขูพ TPU ูพุฑ ุชุฑุจุช ฺฉุฑ ุฑ ฺบุ ุชู ุขูพ ฺฉู ุงููพุฑ ูุงู ุณู ุณ ุดุฑูุน ูู ูุงูุง ุชูุงู ฺฉูฺ ุงฺฉ ูุฎุตูุต ุชุฑุจุช ููฺฉุดู ูฺบ ููุชูู ฺฉุฑูุง ูฺฏุง ูุฒุฏ ุชูุตูุงุช ฺฉ ู [Chapter 3](/course/chapter3) ุฏฺฉฺพฺบ

</Tip>

ุงุจ ุฌุจฺฉ ู ู ุงูพู `train_dataloader` ฺฉู `accelerator.prepare()` ูฺบ ุจฺพุฌ ุฏุง ุ ู ุงุณ ฺฉ ููุจุงุฆ ุงุณุชุนูุงู ฺฉุฑ ฺฉ ุชุฑุจุช ูุฑุงุญู ฺฉ ุชุนุฏุงุฏ ุญุณุงุจ ฺฉุฑ ุณฺฉุช ฺบ ุงุฏ ุฑฺฉฺพฺบ ฺฉ  ุนูู ูุด dataloader ฺฉ ุชุงุฑ ฺฉ ุจุนุฏ ฺฉุง ุฌุงูุง ฺุงุ ฺฉููฺฉ ุงุณ ุทุฑู ฺฉุงุฑ ุณ ุงุณ ฺฉ ููุจุงุฆ ุชุจุฏู ู ุฌุงุฆ ฺฏ ู learning rate ุณ 0 ุชฺฉ ุงฺฉ ฺฉูุงุณฺฉ linear schedule ุงุณุชุนูุงู ฺฉุฑุช ฺบ:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

ุขุฎุฑ ูฺบุ ุงูพู ูุงฺู ฺฉู Hub ูพุฑ ุฏฺพฺฉูู ฺฉ ูุ ูฺบ ุงฺฉ working folder ูฺบ `Repository` ุขุจุฌฺฉูน ุจูุงู ฺฉ ุถุฑูุฑุช ูฺฏ ุงฺฏุฑ ุขูพ ูพู ุณ ูุงฺฏ ุงู ูฺบ ฺบ ุชู ูพู Hugging Face Hub ูฺบ ูุงฺฏ ุงู ฺฉุฑฺบ ู ุงูพู ูุงฺู ฺฉู ุฏู ูุงู model ID ุณ repository ฺฉุง ูุงู ุท ฺฉุฑฺบ ฺฏ (ุขูพ ุงูพู ุงุฎุชุงุฑ ุณ `repo_name` ฺฉู ุชุจุฏู ฺฉุฑ ุณฺฉุช ฺบุ ุงุณ ูฺบ ุตุฑู ุขูพ ฺฉุง username ุดุงูู ููุง ฺุงุ ุฌุณุง ฺฉ `get_full_repo_name()` ููฺฉุดู ฺฉุฑุชุง ):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ูพฺพุฑ ู ุงุณ repository ฺฉู ุงฺฉ ููฺฉู ูููฺุฑ ูฺบ ฺฉููู ฺฉุฑ ุณฺฉุช ฺบ ุงฺฏุฑ  ูพู ุณ ููุฌูุฏ ุ ุชู  ููฺฉู ูููฺุฑ ุงูุณ repository ฺฉุง ุงฺฉ ููุฌูุฏ ฺฉููู ููุง ฺุง ุฌุณ ูพุฑ ู ฺฉุงู ฺฉุฑ ุฑ ฺบ:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ู ุงุจ `output_dir` ูฺบ ูุญููุธ ฺฉ ุฌุงู ูุงู ฺฉุณ ุจฺพ ฺุฒ ฺฉู `repo.push_to_hub()` ูุชฺพฺ ฺฉู ฺฉุงู ฺฉุฑ ฺฉ ุงูพููฺ ฺฉุฑ ุณฺฉุช ฺบ  ูฺบ ุฑ epoch ฺฉ ุขุฎุฑ ูฺบ intermediate models ุงูพููฺ ฺฉุฑู ูฺบ ูุฏุฏ ุฏ ฺฏุง

ุชุฑุจุช ุดุฑูุน ฺฉุฑู ุณ ูพูุ ุขุฆ ุงฺฉ ููุฑ ุฌุงูฺ ฺูุงุฆฺบ ุชุงฺฉ  ูุนููู ู ุณฺฉ ฺฉ evaluation ููฺฉุดู ุตุญุญ ฺฉุงู ฺฉุฑ ุฑุง :

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

 loss ุงูุฑ perplexity ฺฉ ูุฏุฑฺบ ุจุช ุฒุงุฏ ฺบุ ูฺฉู ฺููฺฉ ู ู ุงุจฺพ ูุงฺู ฺฉ ุชุฑุจุช ูฺบ ฺฉ ุ ุงุณ ู  ุญุฑุงู ฺฉู ุจุงุช ูฺบ  ุงุณ ฺฉ ุณุงุชฺพุ ูุงุฑ ูพุงุณ training script ฺฉ core ุญุต ฺฉู ูฺฉฺพู ฺฉ ู ุณุจ ฺฉฺฺพ ุชุงุฑ : training loop training loop ูฺบ ู dataloader ูพุฑ iterate ฺฉุฑุช ฺบ ุงูุฑ batches ฺฉู ูุงฺู ฺฉู ูพุงุณ ฺฉุฑุช ฺบ logits ฺฉ ูุฏุฏ ุณุ ู ูพฺพุฑ ุงูพู custom loss ููฺฉุดู ฺฉุง ุญุณุงุจ ูฺฏุง ุณฺฉุช ฺบ ู loss ฺฉู gradient accumulation steps ฺฉ ุชุนุฏุงุฏ ุณ scale ฺฉุฑุช ฺบ ุชุงฺฉ ุงุถุงู steps ฺฉู aggregate ฺฉุฑู ุณ loss ุจุช ุฒุงุฏ ู ู ุฌุงุฆ Optimize ฺฉุฑู ุณ ูพูุ ู ุจุชุฑ convergence ฺฉ ู gradients ฺฉู clip ุจฺพ ฺฉุฑุช ฺบ ุขุฎุฑ ูฺบุ ุฑ ฺูุฏ steps ูพุฑ ู ุงูพู ูุฆ `evaluate()` ููฺฉุดู ฺฉ ูุฏุฏ ุณ evaluation set ูพุฑ ูุงฺู ฺฉุง ุฌุงุฆุฒ ูุช ฺบ:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

ุงูุฑ ุจุณ   -- ุงุจ ุขูพ ฺฉ ูพุงุณ causal language models ุฌุณ GPT-2 ฺฉ ู ุงูพูุง custom training loop ููุฌูุฏ  ุฌุณ ุขูพ ุงูพู ุถุฑูุฑุงุช ฺฉ ูุทุงุจู ูุฒุฏ ุชุฎุตุต ฺฉุฑ ุณฺฉุช ฺบ 

<Tip>

โ๏ธ **ุขุฒูุงุฆฺบ!** ุง ุชู ุงูพู ูุฑุถ ฺฉุง custom loss ููฺฉุดู ุจูุงุฆฺบ ุฌู ุขูพ ฺฉ use case ฺฉ ูุทุงุจู ูุ ุง training loop ูฺบ ฺฉูุฆ ุงูุฑ custom step ุดุงูู ฺฉุฑฺบ

</Tip>

<Tip>

โ๏ธ **ุขุฒูุงุฆฺบ!** ุฌุจ ุทูู training experiments ฺู ุฑ ูฺบ ุชู ุงู metrics ฺฉู log ฺฉุฑูุง ุงฺฉ ุงฺฺพุง ุฎุงู ุ ุฌุณ TensorBoard ุง Weights & Biases ุฌุณ tools ุงุณุชุนูุงู ฺฉุฑ ฺฉ training loop ูฺบ ููุงุณุจ logging ุดุงูู ฺฉุฑฺบ ุชุงฺฉ ุขูพ ูุด ุฌุงู ุณฺฉฺบ ฺฉ training ฺฉุณ ุฌุง ุฑ 

</Tip>

{/if}

































