# سیکوئنس-ٹو-سیکوئنس ماڈلز [[sequence-to-sequence-models]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

اینکوڈر-ڈیکوڈر ماڈلز (جنہیں *سیکوئنس-ٹو-سیکوئنس ماڈلز* بھی کہا جاتا ہے) ٹرانسفارمر آرکیٹیکچر کے دونوں حصے استعمال کرتے ہیں۔ ہر مرحلے پر، اینکوڈر کی اٹینشن لیئرز مکمل جملے کے تمام الفاظ تک رسائی حاصل کر سکتی ہیں، جبکہ ڈیکوڈر کی اٹینشن لیئرز صرف ان الفاظ تک رسائی حاصل کر سکتی ہیں جو کسی مخصوص لفظ سے پہلے آ چکے ہوں۔

ان ماڈلز کی پری ٹریننگ اینکوڈر یا ڈیکوڈر ماڈلز کے اصولوں پر کی جا سکتی ہے، لیکن عام طور پر یہ زیادہ پیچیدہ ہوتی ہے۔ مثال کے طور پر، [T5](https://huggingface.co/t5-base) کی پری ٹریننگ اس طرح کی جاتی ہے کہ متن کے کچھ حصوں (جو کئی الفاظ پر مشتمل ہو سکتے ہیں) کو ایک خاص ماسک لفظ سے بدل دیا جاتا ہے، اور ماڈل کا ہدف یہ ہوتا ہے کہ وہ اصل متن کی پیشن گوئی کرے جو اس ماسک کی جگہ پر تھا۔

سیکوئنس-ٹو-سیکوئنس ماڈلز ایسے کاموں کے لیے سب سے زیادہ موزوں ہوتے ہیں جن میں دیے گئے ان پٹ پر منحصر ہو کر نیا جملہ بنایا جاتا ہے، جیسے کہ خلاصہ نکالنا (summarization)، ترجمہ (translation)، یا تخلیقی سوالات کے جوابات دینا (generative question answering)۔

اس خاندان کے مشہور ماڈلز میں شامل ہیں:

- [BART](https://huggingface.co/transformers/model_doc/bart)
- [mBART](https://huggingface.co/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/transformers/model_doc/marian)
- [T5](https://huggingface.co/transformers/model_doc/t5)
