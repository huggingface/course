```mdx
# Transformers کیسے کام کرتے ہیں؟[[how-do-transformers-work]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

اس سیکشن میں، ہم Transformer ماڈلز کی آرکیٹیکچر کا ایک اعلیٰ سطحی جائزہ لیں گے۔

## Transformer کی تاریخ کا ایک جھلک[[a-bit-of-transformer-history]]

یہاں Transformer ماڈلز کی (مختصر) تاریخ کے کچھ اہم لمحات پیش کیے گئے ہیں:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

[Transformer آرکیٹیکچر](https://arxiv.org/abs/1706.03762) کو جون 2017 میں متعارف کرایا گیا تھا۔ اصل تحقیق کا مقصد ترجمہ کے ٹاسکس پر تھا۔ اس کے بعد کئی اثرانداز ماڈلز متعارف کرائے گئے، جن میں شامل ہیں:

- **جون 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) — پہلا پری ٹرینڈ Transformer ماڈل، جسے مختلف NLP ٹاسکس پر fine-tune کر کے state-of-the-art نتائج حاصل کیے گئے۔
- **اکتوبر 2018**: [BERT](https://arxiv.org/abs/1810.04805) — ایک اور بڑا پری ٹرینڈ ماڈل، جسے جملوں کا بہتر خلاصہ پیش کرنے کے لیے ڈیزائن کیا گیا (اگلے باب میں مزید تفصیل میں جائیں گے)۔
- **فروری 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) — GPT کا بہتر (اور بڑا) ورژن، جسے اخلاقی خدشات کی وجہ سے فوراً عوامی سطح پر جاری نہیں کیا گیا۔
- **اکتوبر 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108) — BERT کا distilled ورژن جو 60% تیز، 40% ہلکا ہے اور پھر بھی BERT کی کارکردگی کا 97% برقرار رکھتا ہے۔
- **اکتوبر 2019**: [BART](https://arxiv.org/abs/1910.13461) اور [T5](https://arxiv.org/abs/1910.10683) — دو بڑے پری ٹرینڈ ماڈلز جو اصل Transformer آرکیٹیکچر (پہلے سے متعارف شدہ آرکیٹیکچر) استعمال کرتے ہیں۔
- **مئی 2020**: [GPT-3](https://arxiv.org/abs/2005.14165) — GPT-2 کا ایک اور بڑا ورژن جو fine-tuning کی ضرورت کے بغیر مختلف ٹاسکس پر اچھے نتائج پیش کرتا ہے (جسے _zero-shot learning_ کہا جاتا ہے)۔

یہ فہرست مکمل نہیں ہے بلکہ مختلف قسم کے Transformer ماڈلز میں سے چند کی نشاندہی کرتی ہے۔ مجموعی طور پر، انہیں تین زمروں میں تقسیم کیا جا سکتا ہے:

- GPT-جیسی (جنہیں _auto-regressive_ Transformer ماڈلز بھی کہا جاتا ہے)
- BERT-جیسی (جنہیں _auto-encoding_ Transformer ماڈلز بھی کہا جاتا ہے)
- BART/T5-جیسی (جنہیں _sequence-to-sequence_ Transformer ماڈلز بھی کہا جاتا ہے)

ہم ان خاندانوں میں بعد میں مزید تفصیل سے جائیں گے۔

## Transformers زبان ماڈلز ہیں[[transformers-are-language-models]]

اوپر بیان کیے گئے تمام Transformer ماڈلز (GPT, BERT, BART, T5 وغیرہ) کو *language models* کے طور پر تربیت دی گئی ہے۔ اس کا مطلب ہے کہ انہیں بے شمار کچے متن پر self-supervised انداز میں ٹرینڈ کیا گیا ہے۔ Self-supervised learning ایک ایسا طریقہ ہے جس میں ماڈل کے ان پٹس سے objective خودکار طور پر شمار کیا جاتا ہے، یعنی انسانی لیبلرز کی ضرورت نہیں پڑتی!

ایسے ماڈلز اس زبان کا شماریاتی فہم پیدا کر لیتے ہیں جس پر وہ ٹرینڈ کیے گئے ہیں، مگر یہ مخصوص عملی ٹاسکس کے لیے زیادہ مفید نہیں ہوتے۔ اسی وجہ سے، عام پری ٹرینڈ ماڈل کو بعد میں *transfer learning* کے عمل سے گزارا جاتا ہے۔ اس عمل کے دوران، ماڈل کو supervised طریقے سے (یعنی انسانی لیبلز کا استعمال کرتے ہوئے) کسی مخصوص ٹاسک پر fine-tune کیا جاتا ہے۔

مثال کے طور پر، ایک ٹاسک ہو سکتا ہے کہ جملے میں پچھلے *n* الفاظ کے بعد اگلا لفظ پیش کیا جائے۔ اسے *causal language modeling* کہتے ہیں کیونکہ output ماضی اور موجودہ ان پٹس پر منحصر ہوتا ہے، لیکن مستقبل کے الفاظ پر نہیں۔

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

ایک اور مثال *masked language modeling* ہے، جس میں ماڈل جملے میں موجود کسی masked لفظ کی پیش گوئی کرتا ہے۔

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## Transformers بڑے ماڈلز ہیں[[transformers-are-big-models]]

چند استثنیٰ (جیسے DistilBERT) کے علاوہ، بہتر کارکردگی حاصل کرنے کی عام حکمت عملی ماڈل کا سائز بڑھانا اور ان پر پری ٹریننگ کے لیے استعمال ہونے والے ڈیٹا کی مقدار بڑھانا ہے۔

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

بدقسمتی سے، خاص طور پر بڑے ماڈلز کی تربیت کے لیے بہت زیادہ ڈیٹا کی ضرورت ہوتی ہے۔ اس کا مطلب ہے کہ تربیت کے لیے درکار وقت اور compute وسائل بہت زیادہ ہوتے ہیں۔ اس کا ماحولیاتی اثر بھی ہوتا ہے، جیسا کہ نیچے دیے گئے گراف میں دکھایا گیا ہے۔

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>

<Youtube id="ftWlj4FBHTg"/>

یہاں ایک ایسے پروجیکٹ کو دکھایا گیا ہے جس میں (بہت بڑے) ماڈل کی تربیت کی گئی ہے، جس کی ٹیم نے پری ٹریننگ کے ماحولیاتی اثر کو کم کرنے کی کوشش کی ہے۔ بہترین hyperparameters حاصل کرنے کے لیے بہت سی آزمائشیں چلانے کا خرچ اور بھی زیادہ ہوتا۔

سوچیں اگر ہر بار کوئی تحقیقاتی ٹیم، اسٹوڈنٹ آرگنائزیشن، یا کمپنی ماڈل کو scratch سے ٹرین کرے تو اس سے عالمی سطح پر غیر ضروری اخراجات بڑھ جائیں گے!

اسی لیے زبان ماڈلز کو شیئر کرنا بہت اہم ہے: پہلے سے تربیت یافتہ weights کو شیئر کر کے اور انہی پر مزید کام کر کے مجموعی compute لاگت اور carbon footprint کو کم کیا جا سکتا ہے۔

ویسے، آپ اپنے ماڈلز کی تربیت کے carbon footprint کا اندازہ لگانے کے لیے مختلف tools استعمال کر سکتے ہیں۔ مثال کے طور پر [ML CO2 Impact](https://mlco2.github.io/impact/) یا [Code Carbon](https://codecarbon.io/) جو کہ 🤗 Transformers میں integrated ہے۔ اس بارے میں مزید جاننے کے لیے آپ [blog post](https://huggingface.co/blog/carbon-emissions-on-the-hub) پڑھ سکتے ہیں جو آپ کو ایک `emissions.csv` فائل تیار کرنے کا طریقہ دکھائے گی، نیز [documentation](https://huggingface.co/docs/hub/model-cards-co2) بھی ملاحظہ کریں۔

## Transfer Learning[[transfer-learning]]

<Youtube id="BqqfQnyjmgg" />

*Pretraining* کا مطلب ہے کہ ماڈل کو scratch سے ٹرین کرنا: weights کو random initialize کیا جاتا ہے اور تربیت بغیر کسی پہلے سے علم کے شروع کی جاتی ہے۔

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

یہ pretraining عموماً بہت بڑے ڈیٹا پر کی جاتی ہے۔ لہٰذا، اس کے لیے ایک بہت بڑا ڈیٹا corpus درکار ہوتا ہے، اور تربیت میں کئی ہفتے لگ سکتے ہیں۔

*Fine-tuning* اس کے برعکس وہ تربیت ہے جو ماڈل کو پری ٹرینڈ ہونے کے بعد کی جاتی ہے۔ Fine-tuning کرنے کے لیے پہلے ایک پری ٹرینڈ زبان ماڈل حاصل کیا جاتا ہے، پھر ایک مخصوص ٹاسک کے لیے اضافی تربیت کی جاتی ہے۔ ارے — کیوں نہ شروع سے ہی ماڈل کو **scratch** سے آپ کے استعمال کے لیے ٹرین کیا جائے؟ اس کی چند وجوہات ہیں:

*  پری ٹرینڈ ماڈل پہلے ہی ایسے ڈیٹا پر ٹرین کیا گیا ہوتا ہے جس میں fine-tuning ڈیٹاسیٹ کے ساتھ کچھ مماثلت ہوتی ہے۔ اس طرح fine-tuning کے عمل میں ماڈل کو پہلے سے حاصل کردہ علم کا فائدہ اٹھانے کا موقع ملتا ہے (مثلاً NLP مسائل میں، پری ٹرینڈ ماڈل کو آپ کی زبان کا شماریاتی فہم حاصل ہو جاتا ہے).
*  چونکہ پری ٹرینڈ ماڈل کو پہلے ہی بہت زیادہ ڈیٹا پر ٹرین کیا گیا ہوتا ہے، fine-tuning کے لیے کم ڈیٹا کافی ہوتا ہے۔
*  اسی وجہ سے، اچھے نتائج حاصل کرنے کے لیے درکار وقت اور وسائل بھی کم ہوتے ہیں۔

مثال کے طور پر، ایک پری ٹرینڈ ماڈل کو انگریزی زبان پر ٹرین کیا جا سکتا ہے اور پھر اسے arXiv corpus پر fine-tune کیا جا سکتا ہے، جس سے ایک سائنس/ریسرچ بیسڈ ماڈل حاصل ہوگا۔ Fine-tuning کے لیے صرف محدود ڈیٹا درکار ہوگا: پری ٹرینڈ ماڈل نے جو علم حاصل کیا ہے وہ "transfer" ہو جاتا ہے، اسی لیے اسے *transfer learning* کہا جاتا ہے۔

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

Fine-tuning کے عمل کی وجہ سے وقت، ڈیٹا، مالی اور ماحولیاتی اخراجات کم ہوتے ہیں۔ اس کے ساتھ ہی مختلف fine-tuning schemes پر تجربہ کرنا بھی تیز اور آسان ہو جاتا ہے، کیونکہ full pretraining کے مقابلے میں تربیت کم پابند ہوتی ہے۔

یہ عمل scratch سے ٹریننگ کرنے سے بہتر نتائج بھی فراہم کرتا ہے (جب تک کہ آپ کے پاس بہت زیادہ ڈیٹا نہ ہو)، اسی لیے آپ کو ہمیشہ ایک پری ٹرینڈ ماڈل کا فائدہ اٹھانا چاہیے — اور جو آپ کے ٹاسک کے قریب ہو — اور اسے fine-tune کرنا چاہیے۔

## عمومی آرکیٹیکچر[[general-architecture]]

اس سیکشن میں، ہم Transformer ماڈل کے عمومی آرکیٹیکچر کا جائزہ لیں گے۔ فکر نہ کریں اگر کچھ تصورات آپ کو ابھی سمجھ نہ آئیں؛ بعد میں ہر جزو کے بارے میں تفصیل سے بتایا جائے گا۔

<Youtube id="H39Z_720T5s" />

## تعارف[[introduction]]

ماڈل بنیادی طور پر دو بلاکس پر مشتمل ہے:

* **Encoder (بائیں طرف)**: Encoder ان پٹس (جملوں) کو وصول کرتا ہے اور ان کی نمائندگی (features) تیار کرتا ہے۔ یعنی ماڈل ان پٹس سے سمجھ بوجھ حاصل کرنے کے لیے optimize کیا گیا ہے۔
* **Decoder (دائیں طرف)**: Decoder encoder کی نمائندگی (features) کے ساتھ ساتھ دیگر ان پٹس کا استعمال کر کے target sequence generate کرتا ہے۔ یعنی ماڈل output generate کرنے کے لیے optimize کیا گیا ہے۔

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

ان میں سے ہر ایک حصے کو کام کے مطابق آزادانہ طور پر استعمال کیا جا سکتا ہے:

* **Encoder-only ماڈلز**: وہ ٹاسکس جو ان پٹس کی سمجھ بوجھ کا تقاضا کرتے ہیں، جیسے کہ sentence classification اور named entity recognition.
* **Decoder-only ماڈلز**: وہ جنریٹو ٹاسکس کے لیے جیسے کہ text generation.
* **Encoder-decoder ماڈلز** یا **sequence-to-sequence ماڈلز**: وہ جنریٹو ٹاسکس جن میں input بھی درکار ہو، جیسے کہ translation یا summarization.

ہم بعد میں ان آرکیٹیکچرز میں الگ الگ تفصیل سے جائیں گے۔

## Attention layers[[attention-layers]]

Transformer ماڈلز کی ایک کلیدی خصوصیت یہ ہے کہ یہ خاص layers یعنی *attention layers* سے بنے ہوتے ہیں۔ دراصل، Transformer آرکیٹیکچر کو متعارف کرانے والے پیپر کا عنوان ہی "Attention Is All You Need" تھا! ہم بعد میں اس کورس میں attention layers کی تفصیلات کو دریافت کریں گے؛ فی الحال، آپ کو اتنا جان لینا چاہیے کہ یہ layer ماڈل کو بتاتی ہے کہ آپ کے فراہم کردہ جملے میں کون سے الفاظ پر خاص توجہ دی جائے (اور باقی کو نسبتا نظر انداز کر دیا جائے) جب کہ ہر لفظ کی نمائندگی تیار کی جا رہی ہو۔

سیاق و سباق میں رکھنے کے لیے، مثال کے طور پر انگریزی سے فرانسیسی ترجمے کا کام لیں۔ جب آپ "You like this course" ان پٹ فراہم کرتے ہیں، تو ترجمہ ماڈل کو "like" کے صحیح ترجمے کے لیے یہ جاننا ضروری ہوگا کہ "You" بھی موجود ہے، کیونکہ فرانسیسی میں "like" کے لیے فعل کا صیغہ مختلف ہوتا ہے۔ اسی طرح، "this" کا ترجمہ بھی اس بات پر منحصر ہوتا ہے کہ متعلقہ اسم مذکر ہے یا مؤنث۔ مزید پیچیدہ جملوں اور گرامر کے قوانین کی صورت میں، ماڈل کو صحیح ترجمے کے لیے جملے کے دور دراز الفاظ پر بھی توجہ دینی پڑے گی۔

اسی تصور کا اطلاق کسی بھی قدرتی زبان کے کام پر ہوتا ہے: ایک لفظ کا اپنا مطلب ہوتا ہے، مگر اس کا مطلب سیاق و سباق سے گہرائی میں متاثر ہوتا ہے۔

اب جبکہ آپ کو attention layers کا خیال آ گیا ہے، آئیں Transformer آرکیٹیکچر کو قریب سے دیکھتے ہیں۔

## اصلی آرکیٹیکچر[[the-original-architecture]]

Transformer آرکیٹیکچر کو اصل میں ترجمہ کے لیے ڈیزائن کیا گیا تھا۔ تربیت کے دوران، encoder ایک خاص زبان میں جملے وصول کرتا ہے، جبکہ decoder کو وہی جملے مطلوبہ target زبان میں ملتے ہیں۔ Encoder میں، attention layers جملے کے تمام الفاظ کو استعمال کر سکتی ہیں (کیونکہ جیسا کہ ہم نے دیکھا کہ کسی لفظ کا ترجمہ اس کے پہلے اور بعد کے الفاظ پر منحصر ہو سکتا ہے)۔ Decoder، تاہم، sequential طریقے سے کام کرتا ہے اور صرف انہی الفاظ پر توجہ دے سکتا ہے جو پہلے ہی ترجمہ ہو چکے ہوں (یعنی موجودہ لفظ سے پہلے والے الفاظ)۔ مثال کے طور پر، جب ہم translated target کے پہلے تین الفاظ پیش کر چکے ہوں، تو decoder کو یہ تین الفاظ دیے جاتے ہیں تاکہ وہ چوتھے لفظ کی پیش گوئی کر سکے۔

تربیت کو تیز کرنے کے لیے (جب ماڈل کو target جملوں تک رسائی حاصل ہو) decoder کو پورا target دیا جاتا ہے، مگر اسے مستقبل کے الفاظ استعمال کرنے کی اجازت نہیں ہوتی (اگر اسے پوزیشن 2 کا لفظ پوزیشن 2 کی پیش گوئی کے لیے مل جائے تو مسئلہ بہت آسان ہو جائے گا)! مثال کے طور پر، جب چوتھے لفظ کی پیش گوئی کرنے کی کوشش کی جاتی ہے تو attention layer کو صرف پوزیشن 1 سے 3 تک کے الفاظ دستیاب ہوتے ہیں۔

اصل Transformer آرکیٹیکچر اس طرح دکھائی دیتا تھا، جس میں encoder بائیں جانب اور decoder دائیں جانب ہوتا تھا:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

نوٹ کریں کہ decoder بلاک کے پہلے attention layer میں decoder کے تمام (ماضی کے) ان پٹس پر توجہ دی جاتی ہے، مگر دوسرے attention layer میں encoder کا output استعمال ہوتا ہے۔ اس طرح ماڈل کو موجودہ لفظ کی پیش گوئی کے لیے پورے input جملے تک رسائی حاصل ہوتی ہے۔ یہ بہت مفید ہے کیونکہ مختلف زبانوں کے گرامر کے قوانین ایسے ہو سکتے ہیں کہ الفاظ مختلف ترتیب میں آتے ہیں، یا جملے کے آخر میں موجود کچھ context کسی لفظ کے بہترین ترجمے کا تعین کرنے میں مددگار ثابت ہوتا ہے۔

attention mask کو encoder/decoder میں بھی استعمال کیا جا سکتا ہے تاکہ ماڈل کو کچھ خاص الفاظ (مثلاً padding token) پر توجہ دینے سے روکا جا سکے۔

## آرکیٹیکچرز بمقابلہ checkpoints[[architecture-vs-checkpoints]]

جب ہم اس کورس میں Transformer ماڈلز میں گھومتے ہیں، تو آپ کو *آرکیٹیکچرز* اور *checkpoints* کے ساتھ ساتھ *ماڈلز* کا ذکر ملے گا۔ یہ اصطلاحیں ذرا مختلف معنی رکھتی ہیں:

* **آرکیٹیکچر**: ماڈل کا ڈھانچہ — ہر layer کی تعریف اور وہ تمام آپریشنز جو ماڈل کے اندر ہوتے ہیں۔
* **Checkpoints**: وہ weights جو ایک مخصوص آرکیٹیکچر میں لوڈ کیے جاتے ہیں۔
* **ماڈل**: یہ ایک عمومی اصطلاح ہے جو "آرکیٹیکچر" یا "checkpoint" دونوں کو ظاہر کر سکتی ہے۔ اس کورس میں ہم وضاحت کے لیے *آرکیٹیکچر* یا *checkpoint* کا استعمال کریں گے تاکہ ابہام کم ہو۔

مثال کے طور پر، BERT ایک آرکیٹیکچر ہے جبکہ `bert-base-cased`، جو Google ٹیم نے BERT کے پہلے ورژن کے لیے ٹرین کیا تھا، ایک checkpoint ہے۔ تاہم، ایک شخص کہہ سکتا ہے "BERT ماڈل" اور "the `bert-base-cased` ماڈل"۔

```