```mdx
# Transformers ฺฉุณ ฺฉุงู ฺฉุฑุช ฺบุ[[how-do-transformers-work]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

ุงุณ ุณฺฉุดู ูฺบุ ู Transformer ูุงฺูุฒ ฺฉ ุขุฑฺฉูนฺฉฺุฑ ฺฉุง ุงฺฉ ุงุนููฐ ุณุทุญ ุฌุงุฆุฒ ูฺบ ฺฏ

## Transformer ฺฉ ุชุงุฑุฎ ฺฉุง ุงฺฉ ุฌฺพูฺฉ[[a-bit-of-transformer-history]]

ุงฺบ Transformer ูุงฺูุฒ ฺฉ (ูุฎุชุตุฑ) ุชุงุฑุฎ ฺฉ ฺฉฺฺพ ุงู ููุญุงุช ูพุด ฺฉ ฺฏุฆ ฺบ:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

[Transformer ุขุฑฺฉูนฺฉฺุฑ](https://arxiv.org/abs/1706.03762) ฺฉู ุฌูู 2017 ูฺบ ูุชุนุงุฑู ฺฉุฑุงุง ฺฏุง ุชฺพุง ุงุตู ุชุญูู ฺฉุง ููุตุฏ ุชุฑุฌู ฺฉ ูนุงุณฺฉุณ ูพุฑ ุชฺพุง ุงุณ ฺฉ ุจุนุฏ ฺฉุฆ ุงุซุฑุงูุฏุงุฒ ูุงฺูุฒ ูุชุนุงุฑู ฺฉุฑุงุฆ ฺฏุฆุ ุฌู ูฺบ ุดุงูู ฺบ:

- **ุฌูู 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) โ ูพูุง ูพุฑ ูนุฑูฺ Transformer ูุงฺูุ ุฌุณ ูุฎุชูู NLP ูนุงุณฺฉุณ ูพุฑ fine-tune ฺฉุฑ ฺฉ state-of-the-art ูุชุงุฆุฌ ุญุงุตู ฺฉ ฺฏุฆ
- **ุงฺฉุชูุจุฑ 2018**: [BERT](https://arxiv.org/abs/1810.04805) โ ุงฺฉ ุงูุฑ ุจฺุง ูพุฑ ูนุฑูฺ ูุงฺูุ ุฌุณ ุฌูููฺบ ฺฉุง ุจุชุฑ ุฎูุงุต ูพุด ฺฉุฑู ฺฉ ู ฺุฒุงุฆู ฺฉุง ฺฏุง (ุงฺฏู ุจุงุจ ูฺบ ูุฒุฏ ุชูุตู ูฺบ ุฌุงุฆฺบ ฺฏ)
- **ูุฑูุฑ 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) โ GPT ฺฉุง ุจุชุฑ (ุงูุฑ ุจฺุง) ูุฑฺูุ ุฌุณ ุงุฎูุงู ุฎุฏุดุงุช ฺฉ ูุฌ ุณ ููุฑุงู ุนูุงู ุณุทุญ ูพุฑ ุฌุงุฑ ูฺบ ฺฉุง ฺฏุง
- **ุงฺฉุชูุจุฑ 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108) โ BERT ฺฉุง distilled ูุฑฺู ุฌู 60% ุชุฒุ 40% ูฺฉุง  ุงูุฑ ูพฺพุฑ ุจฺพ BERT ฺฉ ฺฉุงุฑฺฉุฑุฏฺฏ ฺฉุง 97% ุจุฑูุฑุงุฑ ุฑฺฉฺพุชุง 
- **ุงฺฉุชูุจุฑ 2019**: [BART](https://arxiv.org/abs/1910.13461) ุงูุฑ [T5](https://arxiv.org/abs/1910.10683) โ ุฏู ุจฺ ูพุฑ ูนุฑูฺ ูุงฺูุฒ ุฌู ุงุตู Transformer ุขุฑฺฉูนฺฉฺุฑ (ูพู ุณ ูุชุนุงุฑู ุดุฏ ุขุฑฺฉูนฺฉฺุฑ) ุงุณุชุนูุงู ฺฉุฑุช ฺบ
- **ูุฆ 2020**: [GPT-3](https://arxiv.org/abs/2005.14165) โ GPT-2 ฺฉุง ุงฺฉ ุงูุฑ ุจฺุง ูุฑฺู ุฌู fine-tuning ฺฉ ุถุฑูุฑุช ฺฉ ุจุบุฑ ูุฎุชูู ูนุงุณฺฉุณ ูพุฑ ุงฺฺพ ูุชุงุฆุฌ ูพุด ฺฉุฑุชุง  (ุฌุณ _zero-shot learning_ ฺฉุง ุฌุงุชุง )

 ูุฑุณุช ูฺฉูู ูฺบ  ุจูฺฉ ูุฎุชูู ูุณู ฺฉ Transformer ูุงฺูุฒ ูฺบ ุณ ฺูุฏ ฺฉ ูุดุงูุฏ ฺฉุฑุช  ูุฌููุน ุทูุฑ ูพุฑุ ุงูฺบ ุชู ุฒูุฑูฺบ ูฺบ ุชูุณู ฺฉุง ุฌุง ุณฺฉุชุง :

- GPT-ุฌุณ (ุฌูฺบ _auto-regressive_ Transformer ูุงฺูุฒ ุจฺพ ฺฉุง ุฌุงุชุง )
- BERT-ุฌุณ (ุฌูฺบ _auto-encoding_ Transformer ูุงฺูุฒ ุจฺพ ฺฉุง ุฌุงุชุง )
- BART/T5-ุฌุณ (ุฌูฺบ _sequence-to-sequence_ Transformer ูุงฺูุฒ ุจฺพ ฺฉุง ุฌุงุชุง )

ู ุงู ุฎุงูุฏุงููฺบ ูฺบ ุจุนุฏ ูฺบ ูุฒุฏ ุชูุตู ุณ ุฌุงุฆฺบ ฺฏ

## Transformers ุฒุจุงู ูุงฺูุฒ ฺบ[[transformers-are-language-models]]

ุงููพุฑ ุจุงู ฺฉ ฺฏุฆ ุชูุงู Transformer ูุงฺูุฒ (GPT, BERT, BART, T5 ูุบุฑ) ฺฉู *language models* ฺฉ ุทูุฑ ูพุฑ ุชุฑุจุช ุฏ ฺฏุฆ  ุงุณ ฺฉุง ูุทูุจ  ฺฉ ุงูฺบ ุจ ุดูุงุฑ ฺฉฺ ูุชู ูพุฑ self-supervised ุงูุฏุงุฒ ูฺบ ูนุฑูฺ ฺฉุง ฺฏุง  Self-supervised learning ุงฺฉ ุงุณุง ุทุฑู  ุฌุณ ูฺบ ูุงฺู ฺฉ ุงู ูพูนุณ ุณ objective ุฎูุฏฺฉุงุฑ ุทูุฑ ูพุฑ ุดูุงุฑ ฺฉุง ุฌุงุชุง ุ ุนู ุงูุณุงู ูุจูุฑุฒ ฺฉ ุถุฑูุฑุช ูฺบ ูพฺุช!

ุงุณ ูุงฺูุฒ ุงุณ ุฒุจุงู ฺฉุง ุดูุงุฑุงุช ูู ูพุฏุง ฺฉุฑ ูุช ฺบ ุฌุณ ูพุฑ ู ูนุฑูฺ ฺฉ ฺฏุฆ ฺบุ ูฺฏุฑ  ูุฎุตูุต ุนูู ูนุงุณฺฉุณ ฺฉ ู ุฒุงุฏ ููุฏ ูฺบ ูุช ุงุณ ูุฌ ุณุ ุนุงู ูพุฑ ูนุฑูฺ ูุงฺู ฺฉู ุจุนุฏ ูฺบ *transfer learning* ฺฉ ุนูู ุณ ฺฏุฒุงุฑุง ุฌุงุชุง  ุงุณ ุนูู ฺฉ ุฏูุฑุงูุ ูุงฺู ฺฉู supervised ุทุฑู ุณ (ุนู ุงูุณุงู ูุจูุฒ ฺฉุง ุงุณุชุนูุงู ฺฉุฑุช ูุฆ) ฺฉุณ ูุฎุตูุต ูนุงุณฺฉ ูพุฑ fine-tune ฺฉุง ุฌุงุชุง 

ูุซุงู ฺฉ ุทูุฑ ูพุฑุ ุงฺฉ ูนุงุณฺฉ ู ุณฺฉุชุง  ฺฉ ุฌูู ูฺบ ูพฺฺพู *n* ุงููุงุธ ฺฉ ุจุนุฏ ุงฺฏูุง ููุธ ูพุด ฺฉุง ุฌุงุฆ ุงุณ *causal language modeling* ฺฉุช ฺบ ฺฉููฺฉ output ูุงุถ ุงูุฑ ููุฌูุฏ ุงู ูพูนุณ ูพุฑ ููุญุตุฑ ูุชุง ุ ูฺฉู ูุณุชูุจู ฺฉ ุงููุงุธ ูพุฑ ูฺบ

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

ุงฺฉ ุงูุฑ ูุซุงู *masked language modeling* ุ ุฌุณ ูฺบ ูุงฺู ุฌูู ูฺบ ููุฌูุฏ ฺฉุณ masked ููุธ ฺฉ ูพุด ฺฏูุฆ ฺฉุฑุชุง 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## Transformers ุจฺ ูุงฺูุฒ ฺบ[[transformers-are-big-models]]

ฺูุฏ ุงุณุชุซููฐ (ุฌุณ DistilBERT) ฺฉ ุนูุงูุ ุจุชุฑ ฺฉุงุฑฺฉุฑุฏฺฏ ุญุงุตู ฺฉุฑู ฺฉ ุนุงู ุญฺฉูุช ุนูู ูุงฺู ฺฉุง ุณุงุฆุฒ ุจฺฺพุงูุง ุงูุฑ ุงู ูพุฑ ูพุฑ ูนุฑููฺฏ ฺฉ ู ุงุณุชุนูุงู ูู ูุงู ฺูนุง ฺฉ ููุฏุงุฑ ุจฺฺพุงูุง 

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

ุจุฏูุณูุช ุณุ ุฎุงุต ุทูุฑ ูพุฑ ุจฺ ูุงฺูุฒ ฺฉ ุชุฑุจุช ฺฉ ู ุจุช ุฒุงุฏ ฺูนุง ฺฉ ุถุฑูุฑุช ูุช  ุงุณ ฺฉุง ูุทูุจ  ฺฉ ุชุฑุจุช ฺฉ ู ุฏุฑฺฉุงุฑ ููุช ุงูุฑ compute ูุณุงุฆู ุจุช ุฒุงุฏ ูุช ฺบ ุงุณ ฺฉุง ูุงุญููุงุช ุงุซุฑ ุจฺพ ูุชุง ุ ุฌุณุง ฺฉ ูฺ ุฏ ฺฏุฆ ฺฏุฑุงู ูฺบ ุฏฺฉฺพุงุง ฺฏุง 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>

<Youtube id="ftWlj4FBHTg"/>

ุงฺบ ุงฺฉ ุงุณ ูพุฑูุฌฺฉูน ฺฉู ุฏฺฉฺพุงุง ฺฏุง  ุฌุณ ูฺบ (ุจุช ุจฺ) ูุงฺู ฺฉ ุชุฑุจุช ฺฉ ฺฏุฆ ุ ุฌุณ ฺฉ ูนู ู ูพุฑ ูนุฑููฺฏ ฺฉ ูุงุญููุงุช ุงุซุฑ ฺฉู ฺฉู ฺฉุฑู ฺฉ ฺฉูุดุด ฺฉ  ุจุชุฑู hyperparameters ุญุงุตู ฺฉุฑู ฺฉ ู ุจุช ุณ ุขุฒูุงุฆุดฺบ ฺูุงู ฺฉุง ุฎุฑฺ ุงูุฑ ุจฺพ ุฒุงุฏ ูุชุง

ุณูฺฺบ ุงฺฏุฑ ุฑ ุจุงุฑ ฺฉูุฆ ุชุญููุงุช ูนูุ ุงุณูนูฺููน ุขุฑฺฏูุงุฆุฒุดูุ ุง ฺฉููพู ูุงฺู ฺฉู scratch ุณ ูนุฑู ฺฉุฑ ุชู ุงุณ ุณ ุนุงูู ุณุทุญ ูพุฑ ุบุฑ ุถุฑูุฑ ุงุฎุฑุงุฌุงุช ุจฺฺพ ุฌุงุฆฺบ ฺฏ!

ุงุณ ู ุฒุจุงู ูุงฺูุฒ ฺฉู ุดุฆุฑ ฺฉุฑูุง ุจุช ุงู : ูพู ุณ ุชุฑุจุช ุงูุช weights ฺฉู ุดุฆุฑ ฺฉุฑ ฺฉ ุงูุฑ ุงู ูพุฑ ูุฒุฏ ฺฉุงู ฺฉุฑ ฺฉ ูุฌููุน compute ูุงฺฏุช ุงูุฑ carbon footprint ฺฉู ฺฉู ฺฉุง ุฌุง ุณฺฉุชุง 

ูุณุ ุขูพ ุงูพู ูุงฺูุฒ ฺฉ ุชุฑุจุช ฺฉ carbon footprint ฺฉุง ุงูุฏุงุฒ ูฺฏุงู ฺฉ ู ูุฎุชูู tools ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ูุซุงู ฺฉ ุทูุฑ ูพุฑ [ML CO2 Impact](https://mlco2.github.io/impact/) ุง [Code Carbon](https://codecarbon.io/) ุฌู ฺฉ ๐ค Transformers ูฺบ integrated  ุงุณ ุจุงุฑ ูฺบ ูุฒุฏ ุฌุงูู ฺฉ ู ุขูพ [blog post](https://huggingface.co/blog/carbon-emissions-on-the-hub) ูพฺฺพ ุณฺฉุช ฺบ ุฌู ุขูพ ฺฉู ุงฺฉ `emissions.csv` ูุงุฆู ุชุงุฑ ฺฉุฑู ฺฉุง ุทุฑู ุฏฺฉฺพุงุฆ ฺฏุ ูุฒ [documentation](https://huggingface.co/docs/hub/model-cards-co2) ุจฺพ ููุงุญุธ ฺฉุฑฺบ

## Transfer Learning[[transfer-learning]]

<Youtube id="BqqfQnyjmgg" />

*Pretraining* ฺฉุง ูุทูุจ  ฺฉ ูุงฺู ฺฉู scratch ุณ ูนุฑู ฺฉุฑูุง: weights ฺฉู random initialize ฺฉุง ุฌุงุชุง  ุงูุฑ ุชุฑุจุช ุจุบุฑ ฺฉุณ ูพู ุณ ุนูู ฺฉ ุดุฑูุน ฺฉ ุฌุงุช 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

 pretraining ุนูููุงู ุจุช ุจฺ ฺูนุง ูพุฑ ฺฉ ุฌุงุช  ููฐุฐุงุ ุงุณ ฺฉ ู ุงฺฉ ุจุช ุจฺุง ฺูนุง corpus ุฏุฑฺฉุงุฑ ูุชุง ุ ุงูุฑ ุชุฑุจุช ูฺบ ฺฉุฆ ูุช ูฺฏ ุณฺฉุช ฺบ

*Fine-tuning* ุงุณ ฺฉ ุจุฑุนฺฉุณ ู ุชุฑุจุช  ุฌู ูุงฺู ฺฉู ูพุฑ ูนุฑูฺ ูู ฺฉ ุจุนุฏ ฺฉ ุฌุงุช  Fine-tuning ฺฉุฑู ฺฉ ู ูพู ุงฺฉ ูพุฑ ูนุฑูฺ ุฒุจุงู ูุงฺู ุญุงุตู ฺฉุง ุฌุงุชุง ุ ูพฺพุฑ ุงฺฉ ูุฎุตูุต ูนุงุณฺฉ ฺฉ ู ุงุถุงู ุชุฑุจุช ฺฉ ุฌุงุช  ุงุฑ โ ฺฉูฺบ ู ุดุฑูุน ุณ  ูุงฺู ฺฉู **scratch** ุณ ุขูพ ฺฉ ุงุณุชุนูุงู ฺฉ ู ูนุฑู ฺฉุง ุฌุงุฆุ ุงุณ ฺฉ ฺูุฏ ูุฌูุงุช ฺบ:

*  ูพุฑ ูนุฑูฺ ูุงฺู ูพู  ุงุณ ฺูนุง ูพุฑ ูนุฑู ฺฉุง ฺฏุง ูุชุง  ุฌุณ ูฺบ fine-tuning ฺูนุงุณูน ฺฉ ุณุงุชฺพ ฺฉฺฺพ ููุงุซูุช ูุช  ุงุณ ุทุฑุญ fine-tuning ฺฉ ุนูู ูฺบ ูุงฺู ฺฉู ูพู ุณ ุญุงุตู ฺฉุฑุฏ ุนูู ฺฉุง ูุงุฆุฏ ุงูนฺพุงู ฺฉุง ูููุน ููุชุง  (ูุซูุงู NLP ูุณุงุฆู ูฺบุ ูพุฑ ูนุฑูฺ ูุงฺู ฺฉู ุขูพ ฺฉ ุฒุจุงู ฺฉุง ุดูุงุฑุงุช ูู ุญุงุตู ู ุฌุงุชุง ).
*  ฺููฺฉ ูพุฑ ูนุฑูฺ ูุงฺู ฺฉู ูพู  ุจุช ุฒุงุฏ ฺูนุง ูพุฑ ูนุฑู ฺฉุง ฺฏุง ูุชุง ุ fine-tuning ฺฉ ู ฺฉู ฺูนุง ฺฉุงู ูุชุง 
*  ุงุณ ูุฌ ุณุ ุงฺฺพ ูุชุงุฆุฌ ุญุงุตู ฺฉุฑู ฺฉ ู ุฏุฑฺฉุงุฑ ููุช ุงูุฑ ูุณุงุฆู ุจฺพ ฺฉู ูุช ฺบ

ูุซุงู ฺฉ ุทูุฑ ูพุฑุ ุงฺฉ ูพุฑ ูนุฑูฺ ูุงฺู ฺฉู ุงูฺฏุฑุฒ ุฒุจุงู ูพุฑ ูนุฑู ฺฉุง ุฌุง ุณฺฉุชุง  ุงูุฑ ูพฺพุฑ ุงุณ arXiv corpus ูพุฑ fine-tune ฺฉุง ุฌุง ุณฺฉุชุง ุ ุฌุณ ุณ ุงฺฉ ุณุงุฆูุณ/ุฑุณุฑฺ ุจุณฺ ูุงฺู ุญุงุตู ูฺฏุง Fine-tuning ฺฉ ู ุตุฑู ูุญุฏูุฏ ฺูนุง ุฏุฑฺฉุงุฑ ูฺฏุง: ูพุฑ ูนุฑูฺ ูุงฺู ู ุฌู ุนูู ุญุงุตู ฺฉุง  ู "transfer" ู ุฌุงุชุง ุ ุงุณ ู ุงุณ *transfer learning* ฺฉุง ุฌุงุชุง 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

Fine-tuning ฺฉ ุนูู ฺฉ ูุฌ ุณ ููุชุ ฺูนุงุ ูุงู ุงูุฑ ูุงุญููุงุช ุงุฎุฑุงุฌุงุช ฺฉู ูุช ฺบ ุงุณ ฺฉ ุณุงุชฺพ  ูุฎุชูู fine-tuning schemes ูพุฑ ุชุฌุฑุจ ฺฉุฑูุง ุจฺพ ุชุฒ ุงูุฑ ุขุณุงู ู ุฌุงุชุง ุ ฺฉููฺฉ full pretraining ฺฉ ููุงุจู ูฺบ ุชุฑุจุช ฺฉู ูพุงุจูุฏ ูุช 

 ุนูู scratch ุณ ูนุฑููฺฏ ฺฉุฑู ุณ ุจุชุฑ ูุชุงุฆุฌ ุจฺพ ูุฑุงู ฺฉุฑุชุง  (ุฌุจ ุชฺฉ ฺฉ ุขูพ ฺฉ ูพุงุณ ุจุช ุฒุงุฏ ฺูนุง ู ู)ุ ุงุณ ู ุขูพ ฺฉู ูุด ุงฺฉ ูพุฑ ูนุฑูฺ ูุงฺู ฺฉุง ูุงุฆุฏ ุงูนฺพุงูุง ฺุง โ ุงูุฑ ุฌู ุขูพ ฺฉ ูนุงุณฺฉ ฺฉ ูุฑุจ ู โ ุงูุฑ ุงุณ fine-tune ฺฉุฑูุง ฺุง

## ุนููู ุขุฑฺฉูนฺฉฺุฑ[[general-architecture]]

ุงุณ ุณฺฉุดู ูฺบุ ู Transformer ูุงฺู ฺฉ ุนููู ุขุฑฺฉูนฺฉฺุฑ ฺฉุง ุฌุงุฆุฒ ูฺบ ฺฏ ูฺฉุฑ ู ฺฉุฑฺบ ุงฺฏุฑ ฺฉฺฺพ ุชุตูุฑุงุช ุขูพ ฺฉู ุงุจฺพ ุณูุฌฺพ ู ุขุฆฺบุ ุจุนุฏ ูฺบ ุฑ ุฌุฒู ฺฉ ุจุงุฑ ูฺบ ุชูุตู ุณ ุจุชุงุง ุฌุงุฆ ฺฏุง

<Youtube id="H39Z_720T5s" />

## ุชุนุงุฑู[[introduction]]

ูุงฺู ุจูุงุฏ ุทูุฑ ูพุฑ ุฏู ุจูุงฺฉุณ ูพุฑ ูุดุชูู :

* **Encoder (ุจุงุฆฺบ ุทุฑู)**: Encoder ุงู ูพูนุณ (ุฌูููฺบ) ฺฉู ูุตูู ฺฉุฑุชุง  ุงูุฑ ุงู ฺฉ ููุงุฆูุฏฺฏ (features) ุชุงุฑ ฺฉุฑุชุง  ุนู ูุงฺู ุงู ูพูนุณ ุณ ุณูุฌฺพ ุจูุฌฺพ ุญุงุตู ฺฉุฑู ฺฉ ู optimize ฺฉุง ฺฏุง 
* **Decoder (ุฏุงุฆฺบ ุทุฑู)**: Decoder encoder ฺฉ ููุงุฆูุฏฺฏ (features) ฺฉ ุณุงุชฺพ ุณุงุชฺพ ุฏฺฏุฑ ุงู ูพูนุณ ฺฉุง ุงุณุชุนูุงู ฺฉุฑ ฺฉ target sequence generate ฺฉุฑุชุง  ุนู ูุงฺู output generate ฺฉุฑู ฺฉ ู optimize ฺฉุง ฺฏุง 

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

ุงู ูฺบ ุณ ุฑ ุงฺฉ ุญุต ฺฉู ฺฉุงู ฺฉ ูุทุงุจู ุขุฒุงุฏุงู ุทูุฑ ูพุฑ ุงุณุชุนูุงู ฺฉุง ุฌุง ุณฺฉุชุง :

* **Encoder-only ูุงฺูุฒ**: ู ูนุงุณฺฉุณ ุฌู ุงู ูพูนุณ ฺฉ ุณูุฌฺพ ุจูุฌฺพ ฺฉุง ุชูุงุถุง ฺฉุฑุช ฺบุ ุฌุณ ฺฉ sentence classification ุงูุฑ named entity recognition.
* **Decoder-only ูุงฺูุฒ**: ู ุฌูุฑูนู ูนุงุณฺฉุณ ฺฉ ู ุฌุณ ฺฉ text generation.
* **Encoder-decoder ูุงฺูุฒ** ุง **sequence-to-sequence ูุงฺูุฒ**: ู ุฌูุฑูนู ูนุงุณฺฉุณ ุฌู ูฺบ input ุจฺพ ุฏุฑฺฉุงุฑ ูุ ุฌุณ ฺฉ translation ุง summarization.

ู ุจุนุฏ ูฺบ ุงู ุขุฑฺฉูนฺฉฺุฑุฒ ูฺบ ุงูฺฏ ุงูฺฏ ุชูุตู ุณ ุฌุงุฆฺบ ฺฏ

## Attention layers[[attention-layers]]

Transformer ูุงฺูุฒ ฺฉ ุงฺฉ ฺฉูุฏ ุฎุตูุตุช   ฺฉ  ุฎุงุต layers ุนู *attention layers* ุณ ุจู ูุช ฺบ ุฏุฑุงุตูุ Transformer ุขุฑฺฉูนฺฉฺุฑ ฺฉู ูุชุนุงุฑู ฺฉุฑุงู ูุงู ูพูพุฑ ฺฉุง ุนููุงู  "Attention Is All You Need" ุชฺพุง! ู ุจุนุฏ ูฺบ ุงุณ ฺฉูุฑุณ ูฺบ attention layers ฺฉ ุชูุตูุงุช ฺฉู ุฏุฑุงูุช ฺฉุฑฺบ ฺฏุ ู ุงูุญุงูุ ุขูพ ฺฉู ุงุชูุง ุฌุงู ููุง ฺุง ฺฉ  layer ูุงฺู ฺฉู ุจุชุงุช  ฺฉ ุขูพ ฺฉ ูุฑุงู ฺฉุฑุฏ ุฌูู ูฺบ ฺฉูู ุณ ุงููุงุธ ูพุฑ ุฎุงุต ุชูุฌ ุฏ ุฌุงุฆ (ุงูุฑ ุจุงู ฺฉู ูุณุจุชุง ูุธุฑ ุงูุฏุงุฒ ฺฉุฑ ุฏุง ุฌุงุฆ) ุฌุจ ฺฉ ุฑ ููุธ ฺฉ ููุงุฆูุฏฺฏ ุชุงุฑ ฺฉ ุฌุง ุฑ ู

ุณุงู ู ุณุจุงู ูฺบ ุฑฺฉฺพู ฺฉ ูุ ูุซุงู ฺฉ ุทูุฑ ูพุฑ ุงูฺฏุฑุฒ ุณ ูุฑุงูุณุณ ุชุฑุฌู ฺฉุง ฺฉุงู ูฺบ ุฌุจ ุขูพ "You like this course" ุงู ูพูน ูุฑุงู ฺฉุฑุช ฺบุ ุชู ุชุฑุฌู ูุงฺู ฺฉู "like" ฺฉ ุตุญุญ ุชุฑุฌู ฺฉ ู  ุฌุงููุง ุถุฑูุฑ ูฺฏุง ฺฉ "You" ุจฺพ ููุฌูุฏ ุ ฺฉููฺฉ ูุฑุงูุณุณ ูฺบ "like" ฺฉ ู ูุนู ฺฉุง ุตุบ ูุฎุชูู ูุชุง  ุงุณ ุทุฑุญุ "this" ฺฉุง ุชุฑุฌู ุจฺพ ุงุณ ุจุงุช ูพุฑ ููุญุตุฑ ูุชุง  ฺฉ ูุชุนูู ุงุณู ูุฐฺฉุฑ  ุง ูุคูุซ ูุฒุฏ ูพฺุฏ ุฌูููฺบ ุงูุฑ ฺฏุฑุงูุฑ ฺฉ ููุงูู ฺฉ ุตูุฑุช ูฺบุ ูุงฺู ฺฉู ุตุญุญ ุชุฑุฌู ฺฉ ู ุฌูู ฺฉ ุฏูุฑ ุฏุฑุงุฒ ุงููุงุธ ูพุฑ ุจฺพ ุชูุฌ ุฏู ูพฺ ฺฏ

ุงุณ ุชุตูุฑ ฺฉุง ุงุทูุงู ฺฉุณ ุจฺพ ูุฏุฑุช ุฒุจุงู ฺฉ ฺฉุงู ูพุฑ ูุชุง : ุงฺฉ ููุธ ฺฉุง ุงูพูุง ูุทูุจ ูุชุง ุ ูฺฏุฑ ุงุณ ฺฉุง ูุทูุจ ุณุงู ู ุณุจุงู ุณ ฺฏุฑุงุฆ ูฺบ ูุชุงุซุฑ ูุชุง 

ุงุจ ุฌุจฺฉ ุขูพ ฺฉู attention layers ฺฉุง ุฎุงู ุข ฺฏุง ุ ุขุฆฺบ Transformer ุขุฑฺฉูนฺฉฺุฑ ฺฉู ูุฑุจ ุณ ุฏฺฉฺพุช ฺบ

## ุงุตู ุขุฑฺฉูนฺฉฺุฑ[[the-original-architecture]]

Transformer ุขุฑฺฉูนฺฉฺุฑ ฺฉู ุงุตู ูฺบ ุชุฑุฌู ฺฉ ู ฺุฒุงุฆู ฺฉุง ฺฏุง ุชฺพุง ุชุฑุจุช ฺฉ ุฏูุฑุงูุ encoder ุงฺฉ ุฎุงุต ุฒุจุงู ูฺบ ุฌูู ูุตูู ฺฉุฑุชุง ุ ุฌุจฺฉ decoder ฺฉู ู ุฌูู ูุทููุจ target ุฒุจุงู ูฺบ ููุช ฺบ Encoder ูฺบุ attention layers ุฌูู ฺฉ ุชูุงู ุงููุงุธ ฺฉู ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ (ฺฉููฺฉ ุฌุณุง ฺฉ ู ู ุฏฺฉฺพุง ฺฉ ฺฉุณ ููุธ ฺฉุง ุชุฑุฌู ุงุณ ฺฉ ูพู ุงูุฑ ุจุนุฏ ฺฉ ุงููุงุธ ูพุฑ ููุญุตุฑ ู ุณฺฉุชุง ) Decoderุ ุชุงูุ sequential ุทุฑู ุณ ฺฉุงู ฺฉุฑุชุง  ุงูุฑ ุตุฑู ุงู ุงููุงุธ ูพุฑ ุชูุฌ ุฏ ุณฺฉุชุง  ุฌู ูพู  ุชุฑุฌู ู ฺฺฉ ูฺบ (ุนู ููุฌูุฏ ููุธ ุณ ูพู ูุงู ุงููุงุธ) ูุซุงู ฺฉ ุทูุฑ ูพุฑุ ุฌุจ ู translated target ฺฉ ูพู ุชู ุงููุงุธ ูพุด ฺฉุฑ ฺฺฉ ูฺบุ ุชู decoder ฺฉู  ุชู ุงููุงุธ ุฏ ุฌุงุช ฺบ ุชุงฺฉ ู ฺูุชฺพ ููุธ ฺฉ ูพุด ฺฏูุฆ ฺฉุฑ ุณฺฉ

ุชุฑุจุช ฺฉู ุชุฒ ฺฉุฑู ฺฉ ู (ุฌุจ ูุงฺู ฺฉู target ุฌูููฺบ ุชฺฉ ุฑุณุงุฆ ุญุงุตู ู) decoder ฺฉู ูพูุฑุง target ุฏุง ุฌุงุชุง ุ ูฺฏุฑ ุงุณ ูุณุชูุจู ฺฉ ุงููุงุธ ุงุณุชุนูุงู ฺฉุฑู ฺฉ ุงุฌุงุฒุช ูฺบ ูุช (ุงฺฏุฑ ุงุณ ูพูุฒุดู 2 ฺฉุง ููุธ ูพูุฒุดู 2 ฺฉ ูพุด ฺฏูุฆ ฺฉ ู ูู ุฌุงุฆ ุชู ูุณุฆู ุจุช ุขุณุงู ู ุฌุงุฆ ฺฏุง)! ูุซุงู ฺฉ ุทูุฑ ูพุฑุ ุฌุจ ฺูุชฺพ ููุธ ฺฉ ูพุด ฺฏูุฆ ฺฉุฑู ฺฉ ฺฉูุดุด ฺฉ ุฌุงุช  ุชู attention layer ฺฉู ุตุฑู ูพูุฒุดู 1 ุณ 3 ุชฺฉ ฺฉ ุงููุงุธ ุฏุณุชุงุจ ูุช ฺบ

ุงุตู Transformer ุขุฑฺฉูนฺฉฺุฑ ุงุณ ุทุฑุญ ุฏฺฉฺพุงุฆ ุฏุชุง ุชฺพุงุ ุฌุณ ูฺบ encoder ุจุงุฆฺบ ุฌุงูุจ ุงูุฑ decoder ุฏุงุฆฺบ ุฌุงูุจ ูุชุง ุชฺพุง:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

ูููน ฺฉุฑฺบ ฺฉ decoder ุจูุงฺฉ ฺฉ ูพู attention layer ูฺบ decoder ฺฉ ุชูุงู (ูุงุถ ฺฉ) ุงู ูพูนุณ ูพุฑ ุชูุฌ ุฏ ุฌุงุช ุ ูฺฏุฑ ุฏูุณุฑ attention layer ูฺบ encoder ฺฉุง output ุงุณุชุนูุงู ูุชุง  ุงุณ ุทุฑุญ ูุงฺู ฺฉู ููุฌูุฏ ููุธ ฺฉ ูพุด ฺฏูุฆ ฺฉ ู ูพูุฑ input ุฌูู ุชฺฉ ุฑุณุงุฆ ุญุงุตู ูุช   ุจุช ููุฏ  ฺฉููฺฉ ูุฎุชูู ุฒุจุงููฺบ ฺฉ ฺฏุฑุงูุฑ ฺฉ ููุงูู ุงุณ ู ุณฺฉุช ฺบ ฺฉ ุงููุงุธ ูุฎุชูู ุชุฑุชุจ ูฺบ ุขุช ฺบุ ุง ุฌูู ฺฉ ุขุฎุฑ ูฺบ ููุฌูุฏ ฺฉฺฺพ context ฺฉุณ ููุธ ฺฉ ุจุชุฑู ุชุฑุฌู ฺฉุง ุชุนู ฺฉุฑู ูฺบ ูุฏุฏฺฏุงุฑ ุซุงุจุช ูุชุง 

attention mask ฺฉู encoder/decoder ูฺบ ุจฺพ ุงุณุชุนูุงู ฺฉุง ุฌุง ุณฺฉุชุง  ุชุงฺฉ ูุงฺู ฺฉู ฺฉฺฺพ ุฎุงุต ุงููุงุธ (ูุซูุงู padding token) ูพุฑ ุชูุฌ ุฏู ุณ ุฑูฺฉุง ุฌุง ุณฺฉ

## ุขุฑฺฉูนฺฉฺุฑุฒ ุจููุงุจู checkpoints[[architecture-vs-checkpoints]]

ุฌุจ ู ุงุณ ฺฉูุฑุณ ูฺบ Transformer ูุงฺูุฒ ูฺบ ฺฏฺพููุช ฺบุ ุชู ุขูพ ฺฉู *ุขุฑฺฉูนฺฉฺุฑุฒ* ุงูุฑ *checkpoints* ฺฉ ุณุงุชฺพ ุณุงุชฺพ *ูุงฺูุฒ* ฺฉุง ุฐฺฉุฑ ูู ฺฏุง  ุงุตุทูุงุญฺบ ุฐุฑุง ูุฎุชูู ูุนู ุฑฺฉฺพุช ฺบ:

* **ุขุฑฺฉูนฺฉฺุฑ**: ูุงฺู ฺฉุง ฺฺพุงูฺ โ ุฑ layer ฺฉ ุชุนุฑู ุงูุฑ ู ุชูุงู ุขูพุฑุดูุฒ ุฌู ูุงฺู ฺฉ ุงูุฏุฑ ูุช ฺบ
* **Checkpoints**: ู weights ุฌู ุงฺฉ ูุฎุตูุต ุขุฑฺฉูนฺฉฺุฑ ูฺบ ููฺ ฺฉ ุฌุงุช ฺบ
* **ูุงฺู**:  ุงฺฉ ุนููู ุงุตุทูุงุญ  ุฌู "ุขุฑฺฉูนฺฉฺุฑ" ุง "checkpoint" ุฏูููฺบ ฺฉู ุธุงุฑ ฺฉุฑ ุณฺฉุช  ุงุณ ฺฉูุฑุณ ูฺบ ู ูุถุงุญุช ฺฉ ู *ุขุฑฺฉูนฺฉฺุฑ* ุง *checkpoint* ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ ฺฏ ุชุงฺฉ ุงุจุงู ฺฉู ู

ูุซุงู ฺฉ ุทูุฑ ูพุฑุ BERT ุงฺฉ ุขุฑฺฉูนฺฉฺุฑ  ุฌุจฺฉ `bert-base-cased`ุ ุฌู Google ูนู ู BERT ฺฉ ูพู ูุฑฺู ฺฉ ู ูนุฑู ฺฉุง ุชฺพุงุ ุงฺฉ checkpoint  ุชุงูุ ุงฺฉ ุดุฎุต ฺฉ ุณฺฉุชุง  "BERT ูุงฺู" ุงูุฑ "the `bert-base-cased` ูุงฺู"

```