```mdx
# Transformers, وہ کیا کر سکتے ہیں؟[[transformers-what-can-they-do]]

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
]} />

اس سیکشن میں، ہم دیکھیں گے کہ Transformer ماڈلز کیا کچھ کر سکتے ہیں اور 🤗 Transformers لائبریری کے پہلے ٹول `pipeline()` فنکشن کا استعمال کیسے کرتے ہیں۔

<Tip>
👀 دیکھیں کہ اوپر دائیں جانب "Open in Colab" بٹن ہے؟ اس پر کلک کریں تاکہ آپ ایک Google Colab نوٹ بک کھول سکیں جس میں اس سیکشن کے تمام کوڈ سیمپلز موجود ہیں۔ یہ بٹن ہر ایسے سیکشن میں موجود ہوگا جس میں کوڈ سیمپلز شامل ہوں۔

اگر آپ مثالیں لوکل چلانا چاہتے ہیں، تو ہم تجویز کرتے ہیں کہ آپ <a href="/course/chapter0">setup</a> پر ایک نظر ڈالیں۔
</Tip>

## Transformers ہر جگہ موجود ہیں![[transformers-are-everywhere]]

Transformer ماڈلز مختلف NLP ٹاسکس کو حل کرنے کے لیے استعمال کیے جاتے ہیں، جیسے کہ پچھلے سیکشن میں بیان کیے گئے ٹاسکس۔ یہاں کچھ کمپنیوں اور تنظیموں کی ایک مثال دی گئی ہے جو Hugging Face اور Transformer ماڈلز کا استعمال کرتی ہیں، اور کمیونٹی کو اپنا ماڈل شیئر کر کے بھی واپس دیتی ہیں:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Companies using Hugging Face" width="100%">

[🤗 Transformers لائبریری](https://github.com/huggingface/transformers) آپ کو وہی شیئر کیے گئے ماڈلز بنانے اور استعمال کرنے کی سہولت فراہم کرتی ہے۔ [Model Hub](https://huggingface.co/models) میں ہزاروں پری ٹرینڈ ماڈلز موجود ہیں جنہیں کوئی بھی ڈاؤن لوڈ کر کے استعمال کر سکتا ہے۔ آپ اپنے ماڈلز کو Hub پر اپ لوڈ بھی کر سکتے ہیں!

<Tip>
⚠️ Hugging Face Hub صرف Transformer ماڈلز تک محدود نہیں ہے۔ کوئی بھی اپنی مرضی کے ماڈلز یا ڈیٹاسیٹس شیئر کر سکتا ہے! تمام دستیاب خصوصیات سے فائدہ اٹھانے کے لیے <a href="https://huggingface.co/join">huggingface.co</a> اکاؤنٹ بنائیں!
</Tip>

Transformer ماڈلز کس طرح کام کرتے ہیں اس میں گہرائی میں جانے سے پہلے، آئیں دیکھتے ہیں کہ انہیں استعمال کر کے ہم کچھ دلچسپ NLP مسائل کو کس طرح حل کر سکتے ہیں۔

## pipelines کے ساتھ کام کرنا[[working-with-pipelines]]

<Youtube id="tiZFewofSLM" />

🤗 Transformers لائبریری کا سب سے بنیادی object `pipeline()` فنکشن ہے۔ یہ ایک ماڈل کو اس کی ضروری preprocessing اور postprocessing steps کے ساتھ جوڑ دیتا ہے، جس سے ہم سیدھے ٹیکسٹ ان پٹ کر کے ایک سمجھ میں آنے والا جواب حاصل کر سکتے ہیں:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

ہم متعدد جملے بھی پاس کر سکتے ہیں!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

ڈیفالٹ کے طور پر، یہ pipeline ایک خاص پری ٹرینڈ ماڈل منتخب کرتا ہے جسے انگریزی میں sentiment analysis کے لیے fine-tune کیا گیا ہے۔ ماڈل کو `classifier` object بنانے پر ڈاؤن لوڈ کر کے کیش کیا جاتا ہے۔ اگر آپ دوبارہ یہ کمانڈ چلائیں تو کیش کیا گیا ماڈل استعمال ہوگا اور ماڈل کو دوبارہ ڈاؤن لوڈ کرنے کی ضرورت نہیں پڑے گی۔

جب آپ کسی ٹیکسٹ کو pipeline میں پاس کرتے ہیں تو تین بنیادی مراحل شامل ہوتے ہیں:

1. ٹیکسٹ کو ایسی شکل میں preprocess کیا جاتا ہے جو ماڈل سمجھ سکے۔
2. Preprocessed inputs کو ماڈل کو پاس کیا جاتا ہے۔
3. ماڈل کی پیش گوئیوں کو post-process کیا جاتا ہے تاکہ آپ ان کا مطلب سمجھ سکیں۔

فی الحال دستیاب pipelines میں سے کچھ یہ ہیں:

- `feature-extraction` (کسی ٹیکسٹ کی ویکٹر representation حاصل کرنا)
- `fill-mask`
- `ner` (named entity recognition)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

آئیں ان میں سے چند کی مثالیں دیکھتے ہیں!

## Zero-shot classification[[zero-shot-classification]]

آئیں ایک زیادہ چیلنجنگ ٹاسک سے شروع کرتے ہیں جہاں ہمیں بغیر لیبل کیے ہوئے texts کی درجہ بندی کرنی ہوتی ہے۔ حقیقی دنیا کے پروجیکٹس میں اس صورتحال کا سامنا کرنا عام بات ہے کیونکہ متن کو annotate کرنا عموماً وقت طلب ہوتا ہے اور ماہرین کا کام ہوتا ہے۔ اس استعمال کے لیے، `zero-shot-classification` pipeline بہت طاقتور ہے: یہ آپ کو درجہ بندی کے لیے لیبلز مہیا کرنے کی اجازت دیتا ہے، اس طرح آپ پری ٹرینڈ ماڈل کے لیبلز پر انحصار نہیں کرتے۔ آپ نے پہلے ہی دیکھا کہ ماڈل دو لیبلز (مثلاً positive یا negative) کے ساتھ ایک جملے کی درجہ بندی کر سکتا ہے — لیکن یہ کسی بھی دوسرے لیبل کے سیٹ کے ساتھ بھی ٹیکسٹ کی درجہ بندی کر سکتا ہے۔

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

یہ pipeline _zero-shot_ کہلاتا ہے کیونکہ آپ کو اسے اپنے ڈیٹا پر fine-tune کرنے کی ضرورت نہیں پڑتی — یہ سیدھے ہی آپ کے فراہم کردہ لیبلز کے لیے probability scores واپس کر دیتا ہے!

<Tip>
✏️ **Try it out!** اپنے جملے اور لیبلز کے ساتھ تجربہ کریں اور دیکھیں کہ ماڈل کیسے ردعمل ظاہر کرتا ہے۔
</Tip>

## Text generation[[text-generation]]

اب دیکھتے ہیں کہ کس طرح ایک pipeline کا استعمال کر کے متن generate کیا جا سکتا ہے۔ بنیادی خیال یہ ہے کہ آپ ایک prompt فراہم کرتے ہیں اور ماڈل باقی متن کو auto-complete کر دیتا ہے۔ یہ ویسے ہی ہے جیسے بہت سے فونز میں predictive text فیچر ہوتا ہے۔ چونکہ text generation میں randomness شامل ہوتی ہے، اس لیے یہ عام بات ہے کہ آپ کو وہی نتائج نہ ملیں جو نیچے دکھائے گئے ہیں۔

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows — data flows of various types, as seen by the '
                    'HTTP'}]
```

آپ `num_return_sequences` آرگیومنٹ کے ساتھ یہ کنٹرول کر سکتے ہیں کہ کتنی مختلف sequences generate کی جائیں اور `max_length` آرگیومنٹ کے ساتھ output text کی کل لمبائی کو بھی کنٹرول کر سکتے ہیں۔

<Tip>
✏️ **Try it out!** `num_return_sequences` اور `max_length` آرگیومنٹس کا استعمال کر کے 15 الفاظ پر مشتمل دو جملے generate کریں۔
</Tip>

## Model Hub سے کسی بھی ماڈل کو pipeline میں استعمال کرنا[[using-any-model-from-the-hub-in-a-pipeline]]

پچھلی مثالوں میں، ڈیفالٹ ماڈل استعمال کیا گیا تھا جو مخصوص ٹاسک کے لیے fine-tune کیا گیا ہے، مگر آپ Hub سے کوئی مخصوص ماڈل بھی منتخب کر سکتے ہیں تاکہ pipeline میں استعمال کیا جا سکے — مثلاً text generation کے لیے۔ [Model Hub](https://huggingface.co/models) پر جائیں اور بائیں جانب موجود tag پر کلک کریں تاکہ صرف اس ٹاسک کے لیے سپورٹ شدہ ماڈلز ظاہر ہوں۔ آپ کو ایک ایسا صفحہ نظر آئے گا جیسے [یہ](https://huggingface.co/models?pipeline_tag=text-generation)۔

آئیں [`distilgpt2`](https://huggingface.co/distilgpt2) ماڈل آزما کر دیکھتے ہیں! پچھلی مثال کی طرح اسی pipeline میں اسے لوڈ کرنے کا طریقہ یہ ہے:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

آپ ماڈل کو مزید refine کرنے کے لیے language tags پر کلک کر کے تلاش کر سکتے ہیں اور کوئی ایسا ماڈل منتخب کر سکتے ہیں جو کسی دوسری زبان میں متن generate کرے۔ Model Hub میں تو multilingual ماڈلز کے checkpoints بھی شامل ہیں جو متعدد زبانوں کی حمایت کرتے ہیں۔

ایک بار جب آپ کسی ماڈل پر کلک کر لیتے ہیں، تو آپ کو ایک widget دکھائی دیتا ہے جس کے ذریعے آپ اسے آن لائن آزما سکتے ہیں۔ اس طرح آپ ماڈل کی صلاحیتوں کو جلدی سے test کر سکتے ہیں، اس سے پہلے کہ آپ ماڈل کو ڈاؤن لوڈ کریں۔

<Tip>
✏️ **Try it out!** فلٹرز کا استعمال کر کے کسی دوسری زبان کے لیے text generation ماڈل تلاش کریں۔ widget کے ساتھ تجربہ کریں اور اسے pipeline میں استعمال کریں!
</Tip>

### Inference API[[the-inference-api]]

تمام ماڈلز کو آپ براہ راست اپنے براؤزر میں Inference API کے ذریعے ٹیسٹ کر سکتے ہیں، جو Hugging Face کی [website](https://huggingface.co/) پر دستیاب ہے۔ آپ اس صفحے پر ماڈل کو اپنا کسٹم ٹیکسٹ فراہم کر کے دیکھ سکتے ہیں کہ ماڈل ان پٹ ڈیٹا کو کس طرح process کرتا ہے۔

وہ Inference API جو widget کو چلانے کے لیے استعمال ہوتی ہے، اسے ایک paid product کے طور پر بھی حاصل کیا جا سکتا ہے، جو کہ آپ کے workflows کے لیے مفید ثابت ہو سکتی ہے۔ مزید تفصیلات کے لیے [pricing page](https://huggingface.co/pricing) ملاحظہ کریں۔

## Mask filling[[mask-filling]]

اگلا pipeline جسے آپ آزما کر دیکھیں گے وہ ہے `fill-mask`۔ اس ٹاسک کا خیال یہ ہے کہ کسی دیے گئے متن میں خالی جگہ کو بھرنا:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

`top_k` آرگیومنٹ کنٹرول کرتا ہے کہ کتنی ممکنہ تجاویز دکھائی جائیں۔ نوٹ کریں کہ یہاں ماڈل خاص `<mask>` لفظ کو بھر رہا ہے، جسے اکثر *mask token* کہا جاتا ہے۔ دیگر mask-filling ماڈلز میں مختلف mask tokens ہو سکتے ہیں، لہٰذا دوسرے ماڈلز کا جائزہ لیتے وقت صحیح mask word کی تصدیق کرنا ضروری ہے۔ ایک طریقہ یہ ہے کہ widget میں استعمال شدہ mask word کو دیکھا جائے۔

<Tip>
✏️ **Try it out!** Hub پر `bert-base-cased` ماڈل تلاش کریں اور Inference API widget میں اس کا mask word دیکھیں۔ یہ ماڈل ہمارے pipeline مثال کے لیے جملے کا کیا پیش گوئی کرتا ہے؟
</Tip>

## Named entity recognition[[named-entity-recognition]]

Named entity recognition (NER) ایک ایسا ٹاسک ہے جس میں ماڈل کو یہ تلاش کرنا ہوتا ہے کہ ان پٹ ٹیکسٹ کے کون سے حصے ایسی entities سے متعلق ہیں جیسے کہ افراد، مقامات یا تنظیمیں۔ آئیے ایک مثال دیکھتے ہیں:

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

یہاں ماڈل نے درست طور پر شناخت کیا کہ "Sylvain" ایک شخص (PER) ہے، "Hugging Face" ایک تنظیم (ORG) ہے، اور "Brooklyn" ایک مقام (LOC) ہے۔

ہم pipeline تخلیق کرتے وقت `grouped_entities=True` کا انتخاب پاس کرتے ہیں تاکہ pipeline ان الفاظ کو جو ایک ہی entity سے متعلق ہوں، ایک ساتھ گروپ کر دے: یہاں ماڈل نے صحیح طور پر "Hugging" اور "Face" کو ایک تنظیم کے طور پر گروپ کر دیا، حالانکہ نام متعدد الفاظ پر مشتمل ہے۔ درحقیقت، جیسا کہ اگلے باب میں دیکھیں گے، preprocessing کے دوران کچھ الفاظ کو چھوٹے حصوں میں تقسیم کیا جاتا ہے۔ مثال کے طور پر، "Sylvain" کو چار حصوں میں تقسیم کیا جاتا ہے: `S`, `##yl`, `##va`, اور `##in`. Post-processing میں، pipeline نے ان حصوں کو کامیابی سے دوبارہ جوڑ دیا۔

<Tip>
✏️ **Try it out!** Model Hub پر ایک ایسا ماڈل تلاش کریں جو انگریزی میں part-of-speech tagging (عام طور پر POS سے مختصر) کر سکے۔ یہ ماڈل اوپر دی گئی مثال کے جملے کے لیے کیا پیش گوئی کرتا ہے؟
</Tip>

## Question answering[[question-answering]]

`question-answering` pipeline ایک دیے گئے سیاق و سباق سے سوالات کا جواب نکالتا ہے:

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

نوٹ کریں کہ یہ pipeline فراہم کردہ سیاق و سباق سے معلومات نکال کر جواب دیتا ہے؛ یہ جواب generate نہیں کرتا۔

## Summarization[[summarization]]

Summarization کا ٹاسک کسی متن کو مختصر شکل میں پیش کرنا ہے جبکہ متن کے اہم پہلوؤں کو برقرار رکھا جائے۔ یہاں ایک مثال ہے:

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

جیسا کہ text generation میں ہوتا ہے، آپ نتیجے کے لیے `max_length` یا `min_length` آرگیومنٹس بھی سیٹ کر سکتے ہیں۔

## Translation[[translation]]

ترجمہ کے لیے، اگر آپ task name میں language pair فراہم کریں (جیسے `"translation_en_to_fr"`)، تو آپ ڈیفالٹ ماڈل استعمال کر سکتے ہیں، مگر آسان ترین طریقہ یہ ہے کہ آپ [Model Hub](https://huggingface.co/models) سے کوئی ماڈل منتخب کریں۔ یہاں ہم French سے English میں ترجمہ آزمانے جا رہے ہیں:

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

text generation اور summarization کی طرح، آپ نتیجے کے لیے `max_length` یا `min_length` بھی سیٹ کر سکتے ہیں۔

<Tip>
✏️ **Try it out!** دوسرے زبانوں میں translation ماڈلز تلاش کریں اور پچھلا جملہ مختلف زبانوں میں ترجمہ کرنے کی کوشش کریں۔
</Tip>

اب تک دکھائے گئے pipelines زیادہ تر مظاہرے کے لیے ہیں۔ انہیں مخصوص ٹاسکس کے لیے پروگرام کیا گیا ہے اور وہ ان میں کسی قسم کی تبدیلی کرنے کے قابل نہیں ہیں۔ اگلے باب میں، آپ سیکھیں گے کہ `pipeline()` فنکشن کے اندر کیا ہوتا ہے اور آپ اس کے رویے کو کس طرح حسب ضرورت بنا سکتے ہیں۔
```