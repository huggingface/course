# خلاصہ [[summary]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

اس باب میں، آپ نے مختلف NLP ٹاسکس کو حل کرنے کے لیے 🤗 Transformers کی `pipeline()` فنکشن کا استعمال سیکھا۔ آپ نے یہ بھی دیکھا کہ ماڈلز کو ہب میں کیسے تلاش کیا جائے اور Inference API کے ذریعے براہِ راست براؤزر میں کیسے آزمایا جائے۔

ہم نے ٹرانسفارمر ماڈلز کی بنیادی کام کرنے کی صلاحیت پر بات کی اور ٹرانسفر لرننگ اور فائن ٹیوننگ کی اہمیت کو اجاگر کیا۔ ایک اہم نکتہ یہ ہے کہ آپ مکمل ماڈل آرکیٹیکچر، صرف اینکوڈر، یا صرف ڈیکوڈر استعمال کر سکتے ہیں، اس پر منحصر ہے کہ آپ کس قسم کا ٹاسک حل کرنا چاہتے ہیں۔ درج ذیل جدول اس کا خلاصہ پیش کرتا ہے:

| ماڈل            | مثالیں                                      | کاموں کی اقسام                                                               |
|-----------------|--------------------------------------------|------------------------------------------------------------------------------|
| اینکوڈر        | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | جملے کی درجہ بندی، نام کی پہچان، ایکسٹریکٹیو سوال جواب                      |
| ڈیکوڈر         | CTRL, GPT, GPT-2, Transformer XL           | ٹیکسٹ جنریشن (متن کی تخلیق)                                                  |
| اینکوڈر-ڈیکوڈر | BART, T5, Marian, mBART                    | خلاصہ سازی، ترجمہ، جنریٹیو سوال جواب                                        |

