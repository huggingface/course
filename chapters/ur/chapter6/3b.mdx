<FrameworkSwitchCourse {fw} />

# Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ (QA) Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº ÙØ§Ø³Ù¹ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø²[[fast-tokenizers-in-the-qa-pipeline]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_tf.ipynb"},
]} />

{/if}

Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº ÛÙ… `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº ØºÙˆØ·Û Ù„Ú¯Ø§Ø¦ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ offset mapping Ú©Ø§ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ Ú©Ø± ÛÙ… context Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ø¬ÙˆØ§Ø¨ Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¨Ø§Ù„Ú©Ù„ Ø§Ø³ÛŒ Ø·Ø±Ø­ Ø¬ÛŒØ³Û’ ÛÙ… Ù†Û’ Ù¾Ú†Ú¾Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº grouped entities Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ§ ØªÚ¾Ø§Û” Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ ÛÙ… ÛŒÛ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ø¨ÛØª Ø·ÙˆÛŒÙ„ context Ú©Û’ Ø³Ø§ØªÚ¾ Ú©ÛŒØ³Û’ Ù†Ù…Ù¹Ø§ Ø¬Ø§Ø¦Û’ Ø¬Ùˆ truncate ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ Ø³ÙˆØ§Ù„ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ú©Ø§Ù… Ù…ÛŒÚº Ø¯Ù„Ú†Ø³Ù¾ÛŒ Ù†ÛÛŒÚº Ø±Ú©Ú¾ØªÛ’ ØªÙˆ Ø¢Ù¾ Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ú©Ùˆ Ú†Ú¾ÙˆÚ‘ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

{#if fw === 'pt'}

<Youtube id="_wxyB3j3mk4"/>

{:else}

<Youtube id="b3u8RzBCX9Y"/>

{/if}

## `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„[[using-the-question-answering-pipeline]]

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ [Ø¨Ø§Ø¨ 1](/course/chapter1) Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ØŒ ÛÙ… `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©Ø³ÛŒ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Ø±ÙˆØ§ÛŒØªÛŒ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù†Ø² Ú©Û’ Ø¨Ø±Ø¹Ú©Ø³ØŒ Ø¬Ùˆ Ú©Û Ø§ÛŒØ³Û’ texts Ú©Ùˆ truncate Ø§ÙˆØ± split Ù†ÛÛŒÚº Ú©Ø± Ø³Ú©ØªÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û length Ø³Û’ Ù„Ù…Ø¨Û’ ÛÙˆÚºØŒ ÛŒÛ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ø¨ÛØª Ø·ÙˆÛŒÙ„ context Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨Ú¾ÛŒ Ù†Ù…Ù¹ Ø³Ú©ØªÛŒ ÛÛ’ Ø§ÙˆØ± Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ ÙˆØ§Ù¾Ø³ Ú©Ø± Ø¯ÛŒØªÛŒ ÛÛ’ Ú†Ø§ÛÛ’ ÙˆÛ context Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº Ú©ÛŒÙˆÚº Ù†Û ÛÙˆ:

```py
long_context = """
ğŸ¤— Transformers: State of the Art NLP

ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ Ú©ÛŒØ³Û’ Ú©Ø±ØªØ§ ÛÛ’!

## Ù…Ø§ÚˆÙ„ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø³ÙˆØ§Ù„ Ø¬ÙˆØ§Ø¨[[using-a-model-for-question-answering]]

Ø¬ÛŒØ³Û’ Ú©Û Ú©Ø³ÛŒ Ø¯ÙˆØ³Ø±Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Û’ Ø³Ø§ØªÚ¾ØŒ ÛÙ… Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø§Ù¾Ù†Û’ input Ú©Ùˆ tokenize Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø§Ø³Û’ model Ø³Û’ Ú¯Ø²Ø§Ø±ÛŒÚº Ú¯Û’Û” `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒÙØ§Ù„Ù¹ checkpoint [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) ÛÛ’ (Ø§Ø³ Ú©Ø§ "squad" Ø§Ø³ dataset Ø³Û’ Ù…ØªØ¹Ù„Ù‚ ÛÛ’ Ø¬Ø³ Ù¾Ø± Ù…Ø§ÚˆÙ„ fine-tune ÛÙˆØ§ ØªÚ¾Ø§Ø› ÛÙ… [Ø¨Ø§Ø¨ 7](/course/chapter7/7) Ù…ÛŒÚº SQuAD dataset Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ù…Ø²ÛŒØ¯ Ø¨Ø§Øª Ú©Ø±ÛŒÚº Ú¯Û’):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)
```

{/if}

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û ÛÙ… Ø³ÙˆØ§Ù„ Ø§ÙˆØ± context Ú©Ùˆ Ø§ÛŒÚ© Ø¬ÙˆÚ‘Û’ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± tokenize Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø¬Ø³ Ù…ÛŒÚº Ø³ÙˆØ§Ù„ Ù¾ÛÙ„Û’ Ø¢ØªØ§ ÛÛ’Û”

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

Question Answering Ù…Ø§ÚˆÙ„Ø² Ø¯ÙˆØ³Ø±Û’ Ù…Ø§ÚˆÙ„Ø² Ø³Û’ ØªÚ¾ÙˆÚ‘Û’ Ù…Ø®ØªÙ„Ù Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ model Ú©Ùˆ ØªØ±Ø¨ÛŒØª Ø¯ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’ Ú©Û ÙˆÛ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø´Ø±ÙˆØ¹ Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… Ú©Û’ tokens Ú©ÛŒ indices predict Ú©Ø±Û’ (ÛŒÛØ§ÚºØŒ Ø´Ø±ÙˆØ¹ index 21 Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… index 24 ÛÛŒÚº)Û” Ø§Ø³ÛŒ Ù„ÛŒÛ’ Ø§ÛŒØ³Û’ Ù…Ø§ÚˆÙ„Ø² Ø¯Ùˆ tensors return Ú©Ø±ØªÛ’ ÛÛŒÚº: Ø§ÛŒÚ© start_logits Ú©Û’ Ù„ÛŒÛ’ Ø§ÙˆØ± Ø§ÛŒÚ© end_logits Ú©Û’ Ù„ÛŒÛ’Û” Ú†ÙˆÙ†Ú©Û ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ ØµØ±Ù 66 tokens Ù¾Ø± Ù…Ø´ØªÙ…Ù„ Ø§ÛŒÚ© input ÛÛ’ØŒ ØªÙˆ ÛÙ…ÛŒÚº:

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

{:else}

```python out
(1, 66) (1, 66)
```

{/if}

Ø§Ø¨ ÛÙ… Ù¾ÛÙ„Û’ ÙˆÛ tokens mask Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ù† Ù¾Ø± ÛÙ… prediction Ù†ÛÛŒÚº Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ (Ø³ÙˆØ§Ù„ Ø§ÙˆØ± [SEP] token)Ø› [CLS] token Ú©Ùˆ ÛÙ… Ø§Ù†-masked Ú†Ú¾ÙˆÚ‘ØªÛ’ ÛÛŒÚº Ú©ÛŒÙˆÙ†Ú©Û Ú©Ú†Ú¾ Ù…Ø§ÚˆÙ„Ø² Ø§Ø³Û’ ÛŒÛ Ø¸Ø§ÛØ± Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø¬ÙˆØ§Ø¨ context Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº:

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
# ØµØ±Ù context Ú©Û’ tokens Ú©Û’ Ø¹Ù„Ø§ÙˆÛ Ø³Ø¨ mask Ú©Ø±ÛŒÚº
mask = [i != 1 for i in sequence_ids]
# [CLS] token Ú©Ùˆ unmask Ú©Ø±ÛŒÚº
mask[0] = False
mask = torch.tensor(mask)[None]
start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# ØµØ±Ù context Ú©Û’ tokens Ú©Û’ Ø¹Ù„Ø§ÙˆÛ Ø³Ø¨ mask Ú©Ø±ÛŒÚº
mask = [i != 1 for i in sequence_ids]
# [CLS] token Ú©Ùˆ unmask Ú©Ø±ÛŒÚº
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Ø§Ø¨ ÛÙ… softmax Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ logits Ú©Ùˆ probabilities Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

Ø§Ú¯Ù„Ø§ Ù…Ø±Ø­Ù„Û Ø¨Ø§Ù„Ú©Ù„ Ù¾ÛÙ„Û’ Ø¬ÛŒØ³Ø§ ÛÛ’ Ù…Ú¯Ø± Ø§Ø¨ Ø¯Ùˆ chunks Ú©Û’ Ù„ÛŒÛ’ Ø¯ÛØ±Ø§ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§Û” ÛÙ… ÛØ± chunk Ù…ÛŒÚº Ù…Ù…Ú©Ù†Û `start_index` Ø§ÙˆØ± `end_index` Ú©Û’ Ù„ÛŒÛ’ probability Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§Ø¦ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø¨ÛØªØ±ÛŒÙ† score ÙˆØ§Ù„ÛŒ span Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº Ú¯Û’:


```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

{#if fw === 'pt'}

Ù¾Ú¾Ø± ÛÙ… Ø§Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ú©Ùˆ Ù…Ø§Ø³Ú© (mask) Ú©Ø±ÛŒÚº Ú¯Û’ Ø¬ÛØ§Úº `start_index > end_index` ÛÙˆØŒ ÛŒØ¹Ù†ÛŒ Ø§Ù†ÛÛŒÚº `0` Ù¾Ø± Ø³ÛŒÙ¹ Ú©Ø± Ø¯ÛŒÚº Ú¯Û’ (Ú©ÛŒÙˆÙ†Ú©Û Ø¨Ø§Ù‚ÛŒ ØªÙ…Ø§Ù… Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø«Ø¨Øª Ø§Ø¹Ø¯Ø§Ø¯ ÛÙˆØªÛ’ ÛÛŒÚº)Û”  

`torch.triu()` ÙÙ†Ú©Ø´Ù† Ú©Ø³ÛŒ 2D Ù¹ÛŒÙ†Ø³Ø± (tensor) Ú©Û’ Ø§ÙˆÙ¾Ø±ÛŒ ØªÚ©ÙˆÙ†ÛŒ (upper triangular) Ø­ØµÛ’ Ú©Ùˆ ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ù„ÛØ°Ø§ ÛŒÛ Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± ÛÙ…Ø§Ø±ÛŒ Ù…Ø·Ù„ÙˆØ¨Û Ù…Ø§Ø³Ú©Ù†Ú¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ Ø¯Û’ Ú¯Ø§:

```py
scores = torch.triu(scores)
```

{:else}

Ù¾Ú¾Ø± ÛÙ… Ø§Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ú©Ùˆ Ù…Ø§Ø³Ú© (mask) Ú©Ø±ÛŒÚº Ú¯Û’ Ø¬ÛØ§Úº `start_index > end_index` ÛÙˆØŒ ÛŒØ¹Ù†ÛŒ Ø§Ù†ÛÛŒÚº `0` Ù¾Ø± Ø³ÛŒÙ¹ Ú©Ø± Ø¯ÛŒÚº Ú¯Û’ (Ú©ÛŒÙˆÙ†Ú©Û Ø¨Ø§Ù‚ÛŒ ØªÙ…Ø§Ù… Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø«Ø¨Øª Ø§Ø¹Ø¯Ø§Ø¯ ÛÙˆØªÛ’ ÛÛŒÚº)Û”  

`np.triu()` ÙÙ†Ú©Ø´Ù† Ú©Ø³ÛŒ 2D Ù¹ÛŒÙ†Ø³Ø± (tensor) Ú©Û’ Ø§ÙˆÙ¾Ø±ÛŒ ØªÚ©ÙˆÙ†ÛŒ (upper triangular) Ø­ØµÛ’ Ú©Ùˆ ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛŒÛ Ù…Ø§Ø³Ú©Ù†Ú¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ Ú¯Ø§:

```py
import numpy as np

scores = np.triu(scores)
```

{/if}

Ø§Ø¨ ÛÙ…ÛŒÚº ØµØ±Ù Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù‚Ø¯Ø§Ø± Ú©Û’ Ø§Ù†ÚˆÛŒÚ©Ø³ (index) Ú©Ùˆ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Ø§ ÛÛ’Û” Ú†ÙˆÙ†Ú©Û PyTorch ÙÙ„ÛŒÙ¹ (flattened) Ù¹ÛŒÙ†Ø³Ø± Ù…ÛŒÚº Ø§Ù†ÚˆÛŒÚ©Ø³ ÙˆØ§Ù¾Ø³ Ú©Ø±Û’ Ú¯Ø§ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº `start_index` Ø§ÙˆØ± `end_index` Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÙÙ„ÙˆØ± ÚˆÙˆÛŒÚ˜Ù† (`//`) Ø§ÙˆØ± Ù…Ø§ÚˆÛŒÙˆÙ„Ø³ (`%`) Ø¢Ù¾Ø±ÛŒØ´Ù†Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§:

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

ÛÙ… Ø§Ø¨Ú¾ÛŒ Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± ÙØ§Ø±Øº Ù†ÛÛŒÚº ÛÙˆØ¦Û’ØŒ Ù„ÛŒÚ©Ù† Ú©Ù… Ø§Ø² Ú©Ù… ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ø¬ÙˆØ§Ø¨ Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±Ø³Øª Ø§Ø³Ú©ÙˆØ± Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ (Ø¢Ù¾ Ø§Ø³Û’ Ù¾Ú†Ú¾Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ú©Û’ Ù¾ÛÙ„Û’ Ù†ØªÛŒØ¬Û’ Ø³Û’ Ù…ÙˆØ§Ø²Ù†Û Ú©Ø±Ú©Û’ Ú†ÛŒÚ© Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº):

```python out
0.97773
```

<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ù¾Ø§Ù†Ú† Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Û’ Ù„ÛŒÛ’ `start_index` Ø§ÙˆØ± `end_index` Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§Ø¦ÛŒÚºÛ”

</Tip>

ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ø¬ÙˆØ§Ø¨ Ú©Û’ `start_index` Ø§ÙˆØ± `end_index` Ù¹ÙˆÚ©Ù†Ø² Ú©ÛŒ Ø´Ú©Ù„ Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºØŒ Ø§Ø¨ ÛÙ…ÛŒÚº Ø§Ù†ÛÛŒÚº Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ù…ÛŒÚº Ú©Ø±ÛŒÚ©Ù¹Ø± Ø§Ù†ÚˆÛŒÚ©Ø³Ø² Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’Û”  

ÛŒÛØ§Úº Ø¢ÙØ³ÛŒÙ¹Ø³ (offsets) Ø¨ÛØª Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø«Ø§Ø¨Øª ÛÙˆÚº Ú¯Û’Û” ÛÙ… Ø§Ù†ÛÛŒÚº Ø­Ø§ØµÙ„ Ú©Ø±Ú©Û’ Ø§Ø³ÛŒ Ø·Ø±Ø­ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’ ÛÙ… Ù†Û’ Ù¹ÙˆÚ©Ù† Ú©Ù„Ø§Ø³ÛŒÙÛŒÚ©ÛŒØ´Ù† Ù¹Ø§Ø³Ú© Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ¾Ø§:


```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

Ø§Ø¨ ÛÙ…ÛŒÚº ØµØ±Ù ÛØ± Ú†ÛŒØ² Ú©Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø´Ú©Ù„ Ù…ÛŒÚº ØªØ±ØªÛŒØ¨ Ø¯ÛŒÙ†Ø§ ÛÛ’ ØªØ§Ú©Û ÛÙ…Ø§Ø±Ø§ Ù†ØªÛŒØ¬Û Ø­Ø§ØµÙ„ ÛÙˆ Ø¬Ø§Ø¦Û’:

```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

Ø¹Ù…Ø¯Û! ÛŒÛ Ø¨Ø§Ù„Ú©Ù„ ÛÙ…Ø§Ø±Û’ Ù¾ÛÙ„Û’ Ù…Ø«Ø§Ù„ Ø¬ÛŒØ³Ø§ ÛÛŒ ÛÛ’!

<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ù¾ÛÙ„Û’ Ø³Û’ Ø­Ø§ØµÙ„ Ú©Ø±Ø¯Û Ø¨ÛØªØ±ÛŒÙ† Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾Ø§Ù†Ú† Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚºÛ”  

Ø§Ù¾Ù†Û’ Ù†ØªØ§Ø¦Ø¬ Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛÙ„Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù¾Ø± ÙˆØ§Ù¾Ø³ Ø¬Ø§Ø¦ÛŒÚº Ø§ÙˆØ± Ø§Ø³Û’ Ú©Ø§Ù„ Ú©Ø±ØªÛ’ ÙˆÙ‚Øª `top_k=5` Ù¾Ø§Ø³ Ú©Ø±ÛŒÚºÛ”

</Tip>

## Ø·ÙˆÛŒÙ„ Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©Ùˆ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±Ù†Ø§ [[handling-long-contexts]]  

Ø§Ú¯Ø± ÛÙ… Ù¾ÛÙ„Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø·ÙˆÛŒÙ„ Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚºØŒ ØªÙˆ ÛÙ…ÛŒÚº 384 Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù¹ÙˆÚ©Ù†Ø² Ù…Ù„ÛŒÚº Ú¯Û’ØŒ Ø¬Ùˆ Ú©Û `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ÛÛ’Û”


```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

Ù„ÛØ°Ø§ØŒ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ Ø§Ù† Ù¾Ù¹Ø³ Ú©Ùˆ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ù¾Ø± Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§Ø³ Ú©Û’ Ú©Ø¦ÛŒ Ø·Ø±ÛŒÙ‚Û’ ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† ÛÙ… Ø³ÙˆØ§Ù„ Ú©Ùˆ Ù…Ø®ØªØµØ± Ù†ÛÛŒÚº Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ØŒ ØµØ±Ù Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©ÙˆÛ” Ú†ÙˆÙ†Ú©Û Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ø¯ÙˆØ³Ø±Ø§ Ø¬Ù…Ù„Û ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… `"only_second"` Ù¹Ø±Ù†Ú©ÛŒØ´Ù† Ø§Ø³Ù¹Ø±ÛŒÙ¹ÛŒØ¬ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”  

ÛŒÛØ§Úº Ø§ÛŒÚ© Ù…Ø³Ø¦Ù„Û ÛŒÛ Ù¾ÛŒØ¯Ø§ ÛÙˆØªØ§ ÛÛ’ Ú©Û Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ù…Ù…Ú©Ù† ÛÛ’ Ú©Û Ù…Ø®ØªØµØ± Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†Û ÛÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛÙ… Ù†Û’ Ø§ÛŒØ³Ø§ Ø³ÙˆØ§Ù„ Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§ ÛÛ’ Ø¬Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº ÛÛ’ØŒ Ø§ÙˆØ± Ø¬Ø¨ ÛÙ… Ø§Ø³Û’ Ù…Ø®ØªØµØ± Ú©Ø±ØªÛ’ ÛÛŒÚº ØªÙˆ ÙˆÛ Ø¬ÙˆØ§Ø¨ ØºØ§Ø¦Ø¨ ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’Û”


```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""
```

Ø§Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛÛ’ Ú©Û Ù…Ø§ÚˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨ ØªÙ„Ø§Ø´ Ú©Ø±Ù†Ø§ Ù…Ø´Ú©Ù„ ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§Û” Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Ùˆ Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† ÛÙ…ÛŒÚº Ú©Ø§Ù†Ù¹ÛŒÚ©Ø³Ù¹ Ú©Ùˆ Ú†Ú¾ÙˆÙ¹Û’ Ø­ØµÙˆÚº Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ù†Û’ Ú©ÛŒ Ø³ÛÙˆÙ„Øª Ø¯ÛŒØªÛŒ ÛÛ’ØŒ Ø¬ÛØ§Úº ÛÙ… Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ (max length) Ù…ØªØ¹ÛŒÙ† Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”  

ØªØ§Ú©Û Ø¬ÙˆØ§Ø¨ Ú©Ø³ÛŒ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ù…Ù‚Ø§Ù… Ù¾Ø± Ú©Ù¹ Ù†Û Ø¬Ø§Ø¦Û’ØŒ ÛŒÛ ÛØ± Ø­ØµÛ’ Ù…ÛŒÚº Ú©Ú†Ú¾ Ø§ÙˆÙˆØ±Ù„ÛŒÙ¾ (overlap) Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛŒ ÛÛ’Û”  

ÛÙ… Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± (Ú†Ø§ÛÛ’ ÙØ§Ø³Ù¹ ÛÙˆ ÛŒØ§ Ø³Ù„Ùˆ) Ú©Ùˆ ÛŒÛ Ú©Ø§Ù… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ `return_overflowing_tokens=True` Ø³ÛŒÙ¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± `stride` Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø± Ú©ÛŒ Ù…Ø¯Ø¯ Ø³Û’ Ø§ÙˆÙˆØ±Ù„ÛŒÙ¾ Ú©ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ú©Ùˆ Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”  

ÛŒÛØ§Úº Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ø¯ÛŒ Ú¯Ø¦ÛŒ ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹Û’ Ø¬Ù…Ù„Û’ Ù¾Ø± Ø¹Ù…Ù„ Ø¯Ø±Ø¢Ù…Ø¯ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’:


```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```


Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ù…Ù„Û Ø§ÛŒØ³Û’ Ø­ØµÙˆÚº Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… ÛÙˆ Ú¯ÛŒØ§ ÛÛ’ Ú©Û `inputs["input_ids"]` Ù…ÛŒÚº ÛØ± Ø§Ù†Ù¹Ø±ÛŒ Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 6 Ù¹ÙˆÚ©Ù†Ø² ÛÛŒÚºÛ” (Ø¢Ø®Ø±ÛŒ Ø§Ù†Ù¹Ø±ÛŒ Ú©Ùˆ Ø¨Ø§Ù‚ÛŒÙˆÚº Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛŒÚˆÙ†Ú¯ Ø´Ø§Ù…Ù„ Ú©Ø±Ù†ÛŒ Ù¾Ú‘Û’ Ú¯ÛŒ)Û” ÛØ± Ø§Ù†Ù¹Ø±ÛŒ Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† 2 Ù¹ÙˆÚ©Ù†Ø² Ú©Ø§ Ø§ÙˆÙˆØ±Ù„ÛŒÙ¾ Ø¨Ú¾ÛŒ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û”  

Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ØŒ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Û’ Ù†ØªÛŒØ¬Û’ Ú©Ùˆ ØªÙØµÛŒÙ„ Ø³Û’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº:

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

Ø¬ÛŒØ³Ø§ Ú©Û ØªÙˆÙ‚Ø¹ Ú©ÛŒ Ø¬Ø§ Ø±ÛÛŒ ØªÚ¾ÛŒØŒ ÛÙ…ÛŒÚº **input IDs** Ø§ÙˆØ± **attention mask** Ù…Ù„Û’ ÛÛŒÚºÛ”  
Ø¢Ø®Ø±ÛŒ Ú©Ù„ÛŒØ¯ØŒ **`overflow_to_sample_mapping`**ØŒ Ø§ÛŒÚ© Ù…ÛŒÙ¾ ÛÛ’ Ø¬Ùˆ ÛÙ…ÛŒÚº Ø¨ØªØ§ØªÛŒ ÛÛ’ Ú©Û ÛØ± Ù†ØªÛŒØ¬Û Ú©Ø³ Ø¬Ù…Ù„Û’ Ø³Û’ Ù…Ø·Ø§Ø¨Ù‚Øª Ø±Ú©Ú¾ØªØ§ ÛÛ’Û” ÛŒÛØ§Úº ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ **7 Ù†ØªØ§Ø¦Ø¬** ÛÛŒÚº Ø¬Ùˆ Ø³Ø¨ **(ÙˆØ§Ø­Ø¯) Ø¬Ù…Ù„Û’** Ø³Û’ Ø¢Ø¦Û’ ÛÛŒÚº Ø¬Ùˆ ÛÙ… Ù†Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø¯ÛŒØ§ ØªÚ¾Ø§Û”


```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

ÛŒÛ Ø²ÛŒØ§Ø¯Û Ù…ÙÛŒØ¯ ÛÙˆØªØ§ ÛÛ’ Ø¬Ø¨ ÛÙ… Ú©Ø¦ÛŒ Ø¬Ù…Ù„Û’ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛŒÛ:

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

...ÛŒÛ ÛÙ…ÛŒÚº Ø§Ø³ Ø¨Ø§Øª Ú©Ø§ Ù¾ØªÛ Ø±Ú©Ú¾Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ø¯ÛŒØªØ§ ÛÛ’ Ú©Û Ú©ÙˆÙ† Ø³Ø§ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² Ø´Ø¯Û Ø­ØµÛ Ø§ØµÙ„ Ø¬Ù…Ù„Û’ Ú©Û’ Ú©Ø³ Ø­ØµÛ’ Ø³Û’ ØªØ¹Ù„Ù‚ Ø±Ú©Ú¾ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ø³Û’ Ù…Ú©Ù…Ù„ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Ùˆ Ø¨Ø¹Ø¯ Ù…ÛŒÚº Ø¯ÙˆØ¨Ø§Ø±Û ØªØ±ØªÛŒØ¨ Ø¯ÛŒÙ†Ø§ Ø¢Ø³Ø§Ù† ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’Û”

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

gets us:

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

...Ø¬Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛÛ’ Ú©Û Ù¾ÛÙ„Ø§ Ø¬Ù…Ù„Û Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ 7 Ø­ØµÙˆÚº Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… ÛÙˆØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø§Ú¯Ù„Û’ 4 Ø­ØµÛ’ Ø¯ÙˆØ³Ø±Û’ Ø¬Ù…Ù„Û’ Ø³Û’ Ø¢ØªÛ’ ÛÛŒÚºÛ”  

Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ Ø§Ù¾Ù†Û’ Ø·ÙˆÛŒÙ„ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©ÛŒ Ø·Ø±Ù ÙˆØ§Ù¾Ø³ Ú†Ù„ØªÛ’ ÛÛŒÚºÛ” Ù¾ÛÙ„Û’ Ø³Û’ Ø·Û’ Ø´Ø¯Û Ø·ÙˆØ± Ù¾Ø±ØŒ `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ 384 Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ Ø°Ú©Ø± Ú©ÛŒØ§ØŒ Ø§ÙˆØ± Ø§ÛŒÚ© `stride` 128ØŒ Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©Û’ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ† ÛÙˆÙ†Û’ Ú©Û’ Ø·Ø±ÛŒÙ‚Û’ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛÛ’ (Ø¢Ù¾ Ø§Ù† Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©Ùˆ `max_seq_len` Ø§ÙˆØ± `stride` Ø¯Ù„Ø§Ø¦Ù„ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø§ÛŒÚˆØ¬Ø³Ù¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº)Û” Ù„ÛØ°Ø§ ÛÙ… Ø§Ù† Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ… Ù¾ÛŒÚˆÙ†Ú¯ Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ (ØªØ§Ú©Û ØªÙ…Ø§Ù… Ù†Ù…ÙˆÙ†Û’ Ø§ÛŒÚ© ÛÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Û’ ÛÙˆÚºØŒ ØªØ§Ú©Û ÛÙ… Ù¹ÛŒÙ†Ø³Ø± Ø¨Ù†Ø§ Ø³Ú©ÛŒÚº) Ø§ÙˆØ± Ø¢ÙØ³ÛŒÙ¹Ø³ Ø¨Ú¾ÛŒ Ø·Ù„Ø¨ Ú©Ø±ÛŒÚº Ú¯Û’Û”


```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

ÛŒÛ `inputs` Ø§Ù† Ù¾Ù¹ IDs Ø§ÙˆØ± ØªÙˆØ¬Û Ú©Û’ Ù…Ø§Ø³Ú© Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÙˆÚº Ú¯Û’ Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªÙˆÙ‚Ø¹Ø§Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛÙˆÚº Ú¯Û’ØŒ Ù†ÛŒØ² Ø¢ÙØ³ÛŒÙ¹Ø³ Ø§ÙˆØ± `overflow_to_sample_mapping` Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ ÛÙˆÚº Ú¯Û’ Ø¬Ù† Ú©Ø§ ÛÙ… Ù†Û’ Ø§Ø¨Ú¾ÛŒ Ø°Ú©Ø± Ú©ÛŒØ§Û” Ú†ÙˆÙ†Ú©Û ÛŒÛ Ø¯Ùˆ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ù…Ø§ÚˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù†ÛÛŒÚº ÛÙˆØªÛ’ØŒ ÛÙ… Ø§Ù†ÛÛŒÚº `inputs` Ø³Û’ ÛÙ¹Ø§ Ø¯ÛŒÚº Ú¯Û’ (Ø§ÙˆØ± ÛÙ… Ø§Ø³ Ù…ÛŒÙ¾ Ú©Ùˆ Ù…Ø­ÙÙˆØ¸ Ù†ÛÛŒÚº Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ ÛŒÛØ§Úº Ù…ÙÛŒØ¯ Ù†ÛÛŒÚº ÛÛ’) Ø§Ø³ Ø³Û’ Ù¾ÛÙ„Û’ Ú©Û ÛÙ… Ø§Ø³Û’ Ù¹ÛŒÙ†Ø³Ø± Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚºÛ”

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python out
(2, 384)
```

{/if}


ÛÙ…Ø§Ø±Û’ Ø·ÙˆÛŒÙ„ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ (long context) Ú©Ùˆ Ø¯Ùˆ Ø­ØµÙˆÚº Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø¬Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛÛ’ Ú©Û Ø¬Ø¨ ÛŒÛ Ù…Ø§ÚˆÙ„ Ø³Û’ Ú¯Ø²Ø±Û’ Ú¯Ø§ØŒ ØªÙˆ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ø´Ø±ÙˆØ¹ (start) Ø§ÙˆØ± Ø§Ø®ØªØªØ§Ù… (end) Ù„Ø§Ø¬Ù¹Ø³ Ú©Û’ Ø¯Ùˆ Ø§Ù„Ú¯ Ø³ÛŒÙ¹ ÛÙˆÚº Ú¯Û’Û”

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python out
(2, 384) (2, 384)
```

{/if}

Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ… Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø§Ù† Ù¹ÙˆÚ©Ù†Ø² Ú©Ùˆ Ù…Ø§Ø³Ú© Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ (context) Ú©Ø§ Ø­ØµÛ Ù†ÛÛŒÚº ÛÛŒÚºØŒ Ø§Ø³ Ø³Û’ Ù¾ÛÙ„Û’ Ú©Û ÛÙ… Ø³Ø§ÙÙ¹ Ù…ÛŒÚ©Ø³ (softmax) Ù„ÛŒÚºÛ” ÛÙ… ØªÙ…Ø§Ù… Ù¾ÛŒÚˆÙ†Ú¯ Ù¹ÙˆÚ©Ù†Ø² Ú©Ùˆ Ø¨Ú¾ÛŒ Ù…Ø§Ø³Ú© Ú©Ø±ØªÛ’ ÛÛŒÚº (Ø¬Ùˆ Ú©Û Ø§Ù¹ÛŒÙ†Ø´Ù† Ù…Ø§Ø³Ú© Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù†Ø´Ø§Ù† Ø²Ø¯ ÛÙˆØªÛ’ ÛÛŒÚº)Û”

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Ù¾Ú¾Ø± ÛÙ… Ø³Ø§ÙÙ¹ Ù…ÛŒÚ©Ø³ (softmax) Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ù¾Ù†Û’ Ù„Ø§Ø¬Ù¹Ø³ (logits) Ú©Ùˆ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª (probabilities) Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()
```

{/if}


Ø§Ú¯Ù„Ø§ Ù…Ø±Ø­Ù„Û Ø§Ø³ÛŒ Ø·Ø±Ø­ ÛÛ’ Ø¬ÛŒØ³Û’ ÛÙ… Ù†Û’ Ú†Ú¾ÙˆÙ¹Û’ Ù…ØªÙ† Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ§ ØªÚ¾Ø§ØŒ Ù„ÛŒÚ©Ù† Ø§Ø¨ ÛÙ… Ø§Ø³Û’ Ø§Ù¾Ù†Û’ Ø¯ÙˆÙ†ÙˆÚº Ø­ØµÙˆÚº (chunks) Ú©Û’ Ù„ÛŒÛ’ Ø¯ÛØ±Ø§Ø¦ÛŒÚº Ú¯Û’Û” ÛÙ… ÛØ± Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨ Ú©Û’ Ø­ØµÛ’ (span) Ú©Ùˆ Ø§ÛŒÚ© Ø§Ø³Ú©ÙˆØ± ØªÙÙˆÛŒØ¶ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ù¾Ú¾Ø± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø§Ø³Ú©ÙˆØ± ÙˆØ§Ù„Û’ Ø­ØµÛ’ Ú©Ùˆ Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº Ú¯Û’Û”

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº Ø§Ù…ÛŒØ¯ÙˆØ§Ø± (candidates) ÙˆÛ Ø¨ÛØªØ±ÛŒÙ† Ø¬ÙˆØ§Ø¨Ø§Øª ÛÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ù†Û’ ÛØ± Ø­ØµÛ’ (chunk) Ù…ÛŒÚº ØªÙ„Ø§Ø´ Ú©ÛŒÛ’ ÛÛŒÚºÛ” Ù…Ø§ÚˆÙ„ Ú©Ø§ÙÛŒ Ø²ÛŒØ§Ø¯Û Ù¾Ø±Ø§Ø¹ØªÙ…Ø§Ø¯ ÛÛ’ Ú©Û Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨ Ø¯ÙˆØ³Ø±Û’ Ø­ØµÛ’ Ù…ÛŒÚº ÛÛ’ØŒ Ø¬Ùˆ Ø§ÛŒÚ© Ù…Ø«Ø¨Øª Ø¹Ù„Ø§Ù…Øª ÛÛ’! Ø§Ø¨ ÛÙ…ÛŒÚº Ø¨Ø³ Ø§Ù† Ø¯ÙˆÙ†ÙˆÚº Ù¹ÙˆÚ©Ù† Ø§Ø³Ù¾ÛŒÙ†Ø² (token spans) Ú©Ùˆ Ù…ØªÙ† Ù…ÛŒÚº Ú©Ø±ÛŒÚ©Ù¹Ø± Ø§Ø³Ù¾ÛŒÙ†Ø² (character spans) Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’Û” (ÛÙ…ÛŒÚº ØµØ±Ù Ø¯ÙˆØ³Ø±Û’ ÙˆØ§Ù„Û’ Ú©Ùˆ Ù…ÛŒÙ¾ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ ØªØ§Ú©Û Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨ Ø­Ø§ØµÙ„ ÛÙˆØŒ Ù„ÛŒÚ©Ù† ÛŒÛ Ø¯ÛŒÚ©Ú¾Ù†Ø§ Ø¯Ù„Ú†Ø³Ù¾ ÛÙˆÚ¯Ø§ Ú©Û Ù…Ø§ÚˆÙ„ Ù†Û’ Ù¾ÛÙ„Û’ Ø­ØµÛ’ Ù…ÛŒÚº Ú©ÛŒØ§ Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§Û”)


<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ø§ÙˆÙ¾Ø± Ø¯ÛŒÛ’ Ú¯Ø¦Û’ Ú©ÙˆÚˆ Ú©Ùˆ Ø§ÛŒÚˆØ¬Ø³Ù¹ Ú©Ø±ÛŒÚº ØªØ§Ú©Û ÛŒÛ Ú©Ù„ Ù¾Ø§Ù†Ú† Ø³Ø¨ Ø³Û’ Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Û’ Ø§Ø³Ú©ÙˆØ± Ø§ÙˆØ± Ø§Ø³Ù¾ÛŒÙ† ÙˆØ§Ù¾Ø³ Ú©Ø±Û’ (ÛØ± Ø­ØµÛ Ø§Ù„Ú¯ Ø§Ù„Ú¯ Ù†ÛÛŒÚºØŒ Ø¨Ù„Ú©Û Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø·ÙˆØ± Ù¾Ø±)Û”

</Tip>

Ø¬Ùˆ `offsets` ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ Ø­Ø§ØµÙ„ Ú©ÛŒÛ’ ØªÚ¾Û’ØŒ ÙˆÛ Ø¯Ø±Ø­Ù‚ÛŒÙ‚Øª Ø¢ÙØ³ÛŒÙ¹Ø³ Ú©ÛŒ Ø§ÛŒÚ© ÙÛØ±Ø³Øª ÛÛ’ØŒ Ø¬ÛØ§Úº ÛØ± Ù¹Ú©Ú‘Û’ (chunk) Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø§Ù„Ú¯ ÙÛØ±Ø³Øª Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û”


```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python out
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

Ø§Ú¯Ø± ÛÙ… Ù¾ÛÙ„Û’ Ù†ØªÛŒØ¬Û’ Ú©Ùˆ Ù†Ø¸Ø± Ø§Ù†Ø¯Ø§Ø² Ú©Ø±ÛŒÚºØŒ ØªÙˆ ÛÙ…ÛŒÚº ÙˆÛÛŒ Ù†ØªÛŒØ¬Û Ù…Ù„ØªØ§ ÛÛ’ Ø¬Ùˆ ÛÙ…Ø§Ø±Û’ `pipeline` Ù†Û’ Ø§Ø³ Ù„Ù…Ø¨Û’ Ù…ØªÙ† Ú©Û’ Ù„ÛŒÛ’ Ø¯ÛŒØ§ ØªÚ¾Ø§ â€” Ø²Ø¨Ø±Ø¯Ø³Øª! ğŸ‰


<Tip>

âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ù¾ÛÙ„Û’ Ø­Ø§ØµÙ„ Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ø¨ÛØªØ±ÛŒÙ† Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾ÙˆØ±Û’ Ù…ØªÙ† Ú©Û’ Ù„ÛŒÛ’ Ù¾Ø§Ù†Ú† Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº (ÛØ± Ø­ØµÛ’ Ú©Û’ Ù„ÛŒÛ’ Ù†ÛÛŒÚºØŒ Ø¨Ù„Ú©Û Ù…Ú©Ù…Ù„ Ù…ØªÙ† Ú©Û’ Ù„ÛŒÛ’)Û”  
Ø§Ù¾Ù†Û’ Ù†ØªØ§Ø¦Ø¬ Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛÙ„Û’ `pipeline` Ù¾Ø± ÙˆØ§Ù¾Ø³ Ø¬Ø§Ø¦ÛŒÚº Ø§ÙˆØ± `top_k=5` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ø³Û’ Ú©Ø§Ù„ Ú©Ø±ÛŒÚºÛ”

</Tip>

ÛŒÛ ÛÙ…Ø§Ø±Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ ØªÙØµÛŒÙ„ÛŒ Ø¬Ø§Ø¦Ø²Û’ Ú©Ø§ Ø§Ø®ØªØªØ§Ù… ÛÛ’Û” ÛÙ… Ø§Ú¯Ù„Û’ Ø¨Ø§Ø¨ Ù…ÛŒÚº Ø§Ù† ØªÙ…Ø§Ù… ØªØµÙˆØ±Ø§Øª Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø¬ÛØ§Úº ÛÙ… Ø¢Ù¾ Ú©Ùˆ Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ú¯Û’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ Ú©Ø³ÛŒ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø¹Ø§Ù… NLP Ù¹Ø§Ø³Ú©Ø³ Ù¾Ø± **ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ†** Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’Û”
























































```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
import numpy as np

candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº candidates ÛÙ…ÛŒÚº ÛØ± chunk Ú©Û’ Ø¨ÛØªØ±ÛŒÙ† Ø¬ÙˆØ§Ø¨ Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ø§Ø¨ start Ø§ÙˆØ± end indices ÛÛŒÚºØŒ Ø¬Ù†ÛÛŒÚº ÛÙ…ÛŒÚº character indices Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’Û” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ offsets Ø¨ÛØª Ù…ÙÛŒØ¯ Ø«Ø§Ø¨Øª ÛÙˆÚº Ú¯Û’Û” ÛÙ… offsets Ú©Ùˆ Ø§Ø³ÛŒ Ø·Ø±Ø­ Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Ø§ Ú©Û Ù¾ÛÙ„Û’:

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… Ø§Ù¾Ù†Û’ result Ú©Ùˆ format Ú©Ø± Ú©Û’ output Ø¯ÛŒØªÛ’ ÛÛŒÚº:

```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

Ø²Ø¨Ø±Ø¯Ø³Øª! ÛŒÛ ÙˆÛÛŒ Ù†ØªÛŒØ¬Û ÛÛ’ Ø¬Ùˆ ÛÙ…ÛŒÚº Ù¾ÛÙ„Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ø³Û’ Ù…Ù„Ø§ ØªÚ¾Ø§Û”

<Tip>
âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ø§Ù¾Ù†Û’ computed scores Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾ÙˆØ±ÛŒ context Ù…ÛŒÚº Ø³Û’ top 5 Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚºÛ” Ù†ØªØ§Ø¦Ø¬ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛÙ„Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº `top_k=5` Ù¾Ø§Ø³ Ú©Ø± Ú©Û’ Ø¯ÛŒÚ©Ú¾ÛŒÚºÛ”
</Tip>

## Ø·ÙˆÛŒÙ„ contexts Ú©Û’ Ø³Ø§ØªÚ¾ Ù†Ù…Ù¹Ù†Ø§[[handling-long-contexts]]

Ø§Ú¯Ø± ÛÙ… question Ø§ÙˆØ± Ø·ÙˆÛŒÙ„ context Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚºØŒ ØªÙˆ ÛÙ…ÛŒÚº model Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û length (Ù…Ø«Ù„Ø§Ù‹ 384 tokens) Ø³Û’ Ø²ÛŒØ§Ø¯Û tokens Ù…Ù„ Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’:

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python
461
```

Ù„ÛÙ°Ø°Ø§ØŒ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ inputs Ú©Ùˆ truncate Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ù…Ú¯Ø± ÛÙ… Ø³ÙˆØ§Ù„ Ú©Ùˆ truncate Ù†ÛÛŒÚº Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ØŒ ØµØ±Ù context Ú©ÙˆÛ” Ú†ÙˆÙ†Ú©Û context Ø¯ÙˆØ³Ø±Ø§ Ø­ØµÛ ÛÛ’ØŒ ÛÙ… `"only_second"` truncation strategy Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ù…Ø³Ø¦Ù„Û ÛŒÛ ÛÛ’ Ú©Û Ø¬ÙˆØ§Ø¨ truncate Ø´Ø¯Û context Ù…ÛŒÚº Ù†Û ÛÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ context Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº ÛÛ’ ØªÙˆ truncate Ú©Ø±Ù†Û’ Ø³Û’ ÙˆÛ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº Ø±ÛÛ’ Ú¯Ø§:

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
...
"""
```

Ø§Ø³ Ø³Û’ model Ú©Ùˆ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨ Ù†Ú©Ø§Ù„Ù†Û’ Ù…ÛŒÚº Ù…Ø´Ú©Ù„ Ù¾ÛŒØ´ Ø¢Ø¦Û’ Ú¯ÛŒÛ” Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Ùˆ Ø­Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† context Ú©Ùˆ Ú†Ú¾ÙˆÙ¹Û’ Ú†Ú¾ÙˆÙ¹Û’ chunks Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ú©Û’ overlap Ø¨Ú¾ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’Û” Ø¢Ù¾ fast ÛŒØ§ slow tokenizer Ø³Û’ `return_overflowing_tokens=True` Ú©Û’ Ø³Ø§ØªÚ¾ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± `stride` Ø¢Ø±Ú¯ÙˆÙ…Ù†Ù¹ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ overlap specify Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹Û’ Ø¬Ù…Ù„Û’ Ú©Û’ Ù„ÛŒÛ’:

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

Ø¬ÛŒØ³Ø§ Ú©Û Ø¯ÛŒÚ©Ú¾Ø§ Ú¯ÛŒØ§ØŒ ÛØ± chunk Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 6 tokens ÛÛŒÚº Ø§ÙˆØ± ÛØ± chunk Ù…ÛŒÚº 2 tokens Ú©Ø§ overlap Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û”

Ø¢Ø¦ÛŒÚº ØªÙØµÛŒÙ„ Ø³Û’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û tokenize Ú©Ø±Ù†Û’ Ú©Ø§ Ù†ØªÛŒØ¬Û Ú©ÛŒØ³Ø§ Ø¢ØªØ§ ÛÛ’:

```py
print(inputs.keys())
```

```python
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

`overflow_to_sample_mapping` Ø¨ØªØ§ØªØ§ ÛÛ’ Ú©Û ÛØ± Ù†ØªÛŒØ¬Û Ú©Ø³ sample Ø³Û’ Ø¢ÛŒØ§ ÛÛ’Ø› ÛŒÛØ§Úº ØªÙ…Ø§Ù… 7 Ù†ØªØ§Ø¦Ø¬ Ø§ÛŒÚ© ÛÛŒ sentence Ø³Û’ ÛÛŒÚºÛ”

Ø§Ú¯Ø± ÛÙ… Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ú©Ø¦ÛŒ Ø¬Ù…Ù„ÙˆÚº Ú©Ùˆ tokenize Ú©Ø±ÛŒÚº:

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

```python
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

ÛŒØ¹Ù†ÛŒ Ù¾ÛÙ„Û’ sentence Ú©Û’ 7 chunks Ø§ÙˆØ± Ø¯ÙˆØ³Ø±Û’ Ú©Û’ 4 chunksÛ”

Ø§Ø¨ ÛÙ… Ø§Ù¾Ù†Û’ Ø·ÙˆÛŒÙ„ context Ú©ÛŒ Ø·Ø±Ù ÙˆØ§Ù¾Ø³ Ø¢ØªÛ’ ÛÛŒÚºÛ” ÚˆÛŒÙØ§Ù„Ù¹ `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† 384 tokens Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø§ÙˆØ± 128 Ú©Ø§ stride Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’ (Ø¬Ø³ Ù¾Ø± Ù…Ø§ÚˆÙ„ fine-tune ÛÙˆØ§ ØªÚ¾Ø§)Û” Ù„ÛÙ°Ø°Ø§ ÛÙ… Ø§Ù† parameters Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

ÛŒÛ `inputs` ÙˆÛ IDs Ø§ÙˆØ± attention masks Ø¯ÛŒÚº Ú¯Û’ Ø¬Ùˆ model Ú©Ùˆ Ú†Ø§ÛÛŒÛ’ØŒ Ø§ÙˆØ± Ø³Ø§ØªÚ¾ ÛÛŒ offsets Ø§ÙˆØ± overflow mapping Ø¨Ú¾ÛŒ Ø¯ÛŒÚº Ú¯Û’Û” Ú†ÙˆÙ†Ú©Û ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº model Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±Ú©Ø§Ø± Ù†ÛÛŒÚºØŒ ÛÙ… Ø§Ù†ÛÛŒÚº `pop()` Ú©Ø± Ø¯ÛŒØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python
(2, 384)
```

{/if}

ÛÙ…Ø§Ø±Ø§ Ø·ÙˆÛŒÙ„ context Ø¯Ùˆ chunks Ù…ÛŒÚº split ÛÙˆ Ú¯ÛŒØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ model Ø³Û’ ÛÙ…ÛŒÚº Ø¯Ùˆ Ø³ÛŒÙ¹ÙˆÚº Ú©Û’ start Ø§ÙˆØ± end logits Ù…Ù„ÛŒÚº Ú¯Û’:

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python
(2, 384) (2, 384)
```

{/if}

Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ… Ù¾ÛÙ„Û’ Ø§Ù† tokens Ú©Ùˆ mask Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ù† Ù¾Ø± ÛÙ… prediction Ù†ÛÛŒÚº Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ (question Ø§ÙˆØ± padding tokens):

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
mask = [i != 1 for i in sequence_ids]
mask[0] = False
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
mask = [i != 1 for i in sequence_ids]
mask[0] = False
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Ø§Ø¨ ÛÙ… softmax Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

Ø§Ø¨ ÛÙ… ÛØ± chunk Ú©Û’ Ù„ÛŒÛ’ ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û spans Ú©Û’ scores calculate Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ø¨ÛØªØ±ÛŒÙ† span Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº Ú¯Û’:

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº candidates ÛØ± chunk Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒÙ† Ø¬ÙˆØ§Ø¨ Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø§Ø¨ ÛÙ…ÛŒÚº Ø§Ù† token spans Ú©Ùˆ context Ú©Û’ character spans Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’Û” `offsets` Ø§ÛŒÚ© list ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº ÛØ± chunk Ú©Û’ Ù„ÛŒÛ’ offsets Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºÛ” ÛÙ… ÛØ± candidate Ú©Û’ Ù„ÛŒÛ’ offsets Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

Ø§Ú¯Ø± ÛÙ… Ù¾ÛÙ„Û’ Ù†ØªÛŒØ¬Û’ Ú©Ùˆ Ù†Ø¸Ø± Ø§Ù†Ø¯Ø§Ø² Ú©Ø± Ø¯ÛŒÚº ØªÙˆ ÛÙ…ÛŒÚº ÙˆÛÛŒ Ø¬ÙˆØ§Ø¨ Ù…Ù„ØªØ§ ÛÛ’ Ø¬Ùˆ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ø³Û’ Ø¢ÛŒØ§ ØªÚ¾Ø§ â€” Ø²Ø¨Ø±Ø¯Ø³Øª!

<Tip>
âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ù¾ÛÙ„Û’ Ø³Û’ computed scores Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾ÙˆØ±ÛŒ context Ú©Û’ Ù„ÛŒÛ’ top 5 Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚºÛ” Ø§Ù¾Ù†Û’ Ù†ØªØ§Ø¦Ø¬ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛÙ„Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº `top_k=5` Ù¾Ø§Ø³ Ú©Ø±ÛŒÚºÛ”
</Tip>

ÛŒÛØ§Úº ÛÙ… Ù†Û’ token spans Ú©Ùˆ character spans Ù…ÛŒÚº convert Ú©Ø± Ú©Û’ final result ØªÛŒØ§Ø± Ú©Ø± Ù„ÛŒØ§ ÛÛ’Û” Ø§Ú¯Ù„Û’ Ø¨Ø§Ø¨ Ù…ÛŒÚº ÛÙ… Ù…Ø²ÛŒØ¯ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù†Ø²ØŒ Ø¬ÛŒØ³Û’ question answeringØŒ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº ØªÙØµÛŒÙ„ Ø³Û’ Ø¨Ø§Øª Ú©Ø±ÛŒÚº Ú¯Û’Û”

## Ø·ÙˆÛŒÙ„ contexts Ú©Û’ Ø³Ø§ØªÚ¾ Ù†Ù…Ù¹Ù†Ø§[[handling-long-contexts]]

Ø§Ú¯Ø± ÛÙ… question Ø§ÙˆØ± Ø§Ø³ Ø·ÙˆÛŒÙ„ context Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚºØŒ ØªÙˆ ÛÙ…ÛŒÚº model Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û length (Ù…Ø«Ù„Ø§Ù‹ 384 tokens) Ø³Û’ Ø²ÛŒØ§Ø¯Û tokens Ù…Ù„ Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’:

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python
461
```

Ù„ÛÙ°Ø°Ø§ ÛÙ…ÛŒÚº inputs Ú©Ùˆ truncate Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ØŒ Ù…Ú¯Ø± ØµØ±Ù context Ú©ÙˆØ› Ø³ÙˆØ§Ù„ truncate Ù†ÛÛŒÚº Ú©Ø±Ù†Ø§Û” Ú†ÙˆÙ†Ú©Û context Ø¯ÙˆØ³Ø±Ø§ Ø­ØµÛ ÛÛ’ØŒ ÛÙ… `"only_second"` strategy Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ù…Ø³Ø¦Ù„Û ÛŒÛ ÛÛ’ Ú©Û truncate Ú©Ø±Ù†Û’ Ù¾Ø± Ø¬ÙˆØ§Ø¨ context Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†Û ÛÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ context Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº ÛÙˆ ØªÙˆ truncate Ú©Ø±Ù†Û’ Ø³Û’ ÙˆÛ ØºØ§Ø¦Ø¨ ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯Ø§:

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides pretrained models to perform tasks like classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
...
"""
```

Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©Ùˆ ØµØ­ÛŒØ­ Ø¬ÙˆØ§Ø¨ Ù†Ú©Ø§Ù„Ù†Û’ Ù…ÛŒÚº Ø¯Ø´ÙˆØ§Ø±ÛŒ Ù¾ÛŒØ´ Ø¢Ø¦Û’ Ú¯ÛŒÛ” Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Ø§ Ø­Ù„ ÛŒÛ ÛÛ’ Ú©Û `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† context Ú©Ùˆ Ú†Ú¾ÙˆÙ¹Û’ Ú†Ú¾ÙˆÙ¹Û’ chunks Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ø¯ÛŒØªÛŒ ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº ÛØ± chunk Ù…ÛŒÚº Ú©Ú†Ú¾ overlap Ø¨Ú¾ÛŒ ÛÙˆØªØ§ ÛÛ’Û” Ø¢Ù¾ fast ÛŒØ§ slow tokenizer Ú©Û’ Ø³Ø§ØªÚ¾ `return_overflowing_tokens=True` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ ÛŒÛ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± `stride` Ø¢Ø±Ú¯ÙˆÙ…Ù†Ù¹ Ø³Û’ overlap specify Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹Û’ Ø¬Ù…Ù„Û’ Ú©Û’ Ù„ÛŒÛ’:

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

Ø¬ÛŒØ³Ø§ Ú©Û Ø¯ÛŒÚ©Ú¾Ø§ Ú¯ÛŒØ§ØŒ ÛØ± chunk Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 6 tokens ÛÛŒÚº Ø§ÙˆØ± ÛØ± chunk Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† 2 tokens Ú©Ø§ overlap ÛÛ’Û” Ù…Ø²ÛŒØ¯ ØªÙØµÛŒÙ„ Ø³Û’ØŒ ÛÙ… Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û tokenize Ú©Û’ Ù†ØªÛŒØ¬Û’ Ù…ÛŒÚº ÛÙ…ÛŒÚº `overflow_to_sample_mapping` Ø¨Ú¾ÛŒ Ù…Ù„ØªØ§ ÛÛ’ Ø¬Ùˆ Ø¨ØªØ§ØªØ§ ÛÛ’ Ú©Û ÛØ± Ù†ØªÛŒØ¬Û Ú©Ø³ sample Ø³Û’ Ø¢ÛŒØ§ ÛÛ’Û” Ø§Ú¯Ø± Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ú©Ø¦ÛŒ Ø¬Ù…Ù„ÙˆÚº Ú©Ùˆ tokenize Ú©Ø±ÛŒÚº ØªÙˆ:

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

```python
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

ÛŒØ¹Ù†ÛŒ Ù¾ÛÙ„Ø§ sentence 7 chunks Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… ÛÙˆ Ú¯ÛŒØ§ Ø§ÙˆØ± Ø¯ÙˆØ³Ø±Ø§ 4 chunks Ù…ÛŒÚºÛ”

Ø§Ø¨ ÙˆØ§Ù¾Ø³ ÛÙ…Ø§Ø±Û’ Ø·ÙˆÛŒÙ„ context Ú©ÛŒ Ø·Ø±ÙÛ” ÚˆÛŒÙØ§Ù„Ù¹ `question-answering` Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† 384 tokens Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø§ÙˆØ± 128 Ú©Ø§ stride Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ø¬Ùˆ fine-tuning Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØ§ ØªÚ¾Ø§Û” ÛÙ… Ø§Ù†ÛÛŒ parameters Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø³Ø§ØªÚ¾ ÛÛŒ padding Ø§ÙˆØ± offsets Ø¨Ú¾ÛŒ:

```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

Ú†ÙˆÙ†Ú©Û ÛŒÛ `inputs` model Ú©Û’ Ù„ÛŒÛ’ input IDs Ø§ÙˆØ± attention masks Ú©Û’ Ø¹Ù„Ø§ÙˆÛ offsets Ø§ÙˆØ± overflow mapping Ø¨Ú¾ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ ÛÙ… Ø§Ù†ÛÛŒÚº model Ú©Û’ Ù„ÛŒÛ’ convert Ú©Ø±Ù†Û’ Ø³Û’ Ù¾ÛÙ„Û’ remove Ú©Ø± Ø¯ÛŒØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python
(2, 384)
```

{/if}

ÛÙ…Ø§Ø±Ø§ Ø·ÙˆÛŒÙ„ context Ø¯Ùˆ chunks Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… ÛÙˆ Ú¯ÛŒØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ model Ø³Û’ ÛÙ…ÛŒÚº Ø¯Ùˆ Ø³ÛŒÙ¹ÙˆÚº Ú©Û’ start Ø§ÙˆØ± end logits Ù…Ù„ÛŒÚº Ú¯Û’:

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python
(2, 384) (2, 384)
```

{/if}

Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ… mask Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø¬Ù† tokens Ù¾Ø± prediction Ù†ÛÛŒÚº Ú©Ø±Ù†ÛŒØŒ Ø§Ù† Ú©Û’ logits Ú©Ùˆ -10000 Ø³Û’ replace Ú©Ø±ÛŒÚºØŒ Ù¾Ú¾Ø± softmax Ù„Ú¯Ø§Ø¦ÛŒÚº:

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
mask = [i != 1 for i in sequence_ids]
mask[0] = False
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
mask = [i != 1 for i in sequence_ids]
mask[0] = False
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Ø§Ø¨ softmax Ù„Ú¯Ø§Ø¦ÛŒÚº:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

Ø§Ø¨ ÛØ± chunk Ú©Û’ Ù„ÛŒÛ’ Ù…Ù…Ú©Ù†Û spans Ú©Û’ scores calculate Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¨ÛØªØ±ÛŒÙ† span Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº:

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
import numpy as np

candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº candidates ÛØ± chunk Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒÙ† Ø¬ÙˆØ§Ø¨ Ø¸Ø§ÛØ± Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø§Ø¨ ÛÙ…ÛŒÚº Ø§Ù† token spans Ú©Ùˆ context Ú©Û’ character spans Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ÛÛ’ØŒ Ø¬Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… Ù¾ÛÙ„Û’ Ø³Û’ Ø­Ø§ØµÙ„ Ú©ÛŒÛ’ Ú¯Ø¦Û’ `offsets` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

Ø§Ú¯Ø± ÛÙ… Ù¾ÛÙ„Û’ Ù†ØªÛŒØ¬Û’ Ú©Ùˆ Ù†Ø¸Ø± Ø§Ù†Ø¯Ø§Ø² Ú©Ø± Ø¯ÛŒÚº ØªÙˆ ÛÙ…ÛŒÚº ÙˆÛÛŒ Ø¬ÙˆØ§Ø¨ Ù…Ù„ØªØ§ ÛÛ’ Ø¬Ùˆ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ø³Û’ Ø¢ÛŒØ§ ØªÚ¾Ø§ â€” Ø²Ø¨Ø±Ø¯Ø³Øª!

<Tip>
âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ø§Ù¾Ù†Û’ computed scores Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾ÙˆØ±ÛŒ context Ú©Û’ Ù„ÛŒÛ’ top 5 Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨Ø§Øª ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚºÛ” Ø§Ù¾Ù†Û’ Ù†ØªØ§Ø¦Ø¬ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛÙ„Û’ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº `top_k=5` Ù¾Ø§Ø³ Ú©Ø±ÛŒÚºÛ”
</Tip>

ÛŒÛ ÛÙ…Ø§Ø±ÛŒ deep dive Ø®ØªÙ… Ú©Ø±ØªÛŒ ÛÛ’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ fast tokenizers Ú©Û’ Ø§Ø¶Ø§ÙÛŒ ÙÛŒÚ†Ø±Ø² Ø¬ÛŒØ³Û’ offset mapping Ø§ÙˆØ± batch encoding Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ ÛÙ… question-answering Ú©Û’ Ú©Ø§Ù… Ú©Ùˆ manually re-implement Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ù„Û’ Ø¨Ø§Ø¨ Ù…ÛŒÚº ÛÙ… Ø§Ù† ØªÙ…Ø§Ù… concepts Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§ÛŒÚ© language model Ú©Ùˆ fine-tune Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø± Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº Ú¯Û’Û”