# Byte-Pair Encoding tokenization[[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

Byte-Pair Encoding (BPE) Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ø·ÙˆØ± Ù¾Ø± texts Ú©Ùˆ compress Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Û’ Ø·ÙˆØ± Ù¾Ø± ØªÛŒØ§Ø± Ú©ÛŒØ§ Ú¯ÛŒØ§ ØªÚ¾Ø§ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± OpenAI Ù†Û’ GPT Ù…Ø§ÚˆÙ„ Ú©ÛŒ pretraining Ú©Û’ Ø¯ÙˆØ±Ø§Ù† tokenization Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§Û” Ø§Ø³Û’ Ø¨ÛØª Ø³Û’ Transformer Ù…Ø§ÚˆÙ„Ø² Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ø¬Ù† Ù…ÛŒÚº GPTØŒ GPT-2ØŒ RoBERTaØŒ BARTØŒ Ø§ÙˆØ± DeBERTa Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”

<Youtube id="HEikzVL-lZU"/>

<Tip>
ğŸ’¡ ÛŒÛ Ø³ÛŒÚ©Ø´Ù† BPE Ú©Ùˆ ØªÙØµÛŒÙ„ Ø³Û’ cover Ú©Ø±ØªØ§ ÛÛ’ØŒ ÛŒÛØ§Úº ØªÚ© Ú©Û Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ implementation Ø¯Ú©Ú¾Ø§ØªØ§ ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ ØµØ±Ù tokenization Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Ø§ Ø¹Ù…ÙˆÙ…ÛŒ Ø¬Ø§Ø¦Ø²Û Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ Ø¢Ù¾ Ø¢Ø®Ø± Ù…ÛŒÚº jump Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”
</Tip>

## Training algorithm[[training-algorithm]]

BPE Ú©ÛŒ training corpus Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ù†ÙØ±Ø¯ Ø§Ù„ÙØ§Ø¸ Ú©Ø§ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§ Ú©Ø± Ø´Ø±ÙˆØ¹ ÛÙˆØªÛŒ ÛÛ’ (normalization Ø§ÙˆØ± pre-tokenization Ú©Û’ Ù…Ø±Ø§Ø­Ù„ Ù…Ú©Ù…Ù„ ÛÙˆÙ†Û’ Ú©Û’ Ø¨Ø¹Ø¯)ØŒ Ù¾Ú¾Ø± Ø§Ù† Ø§Ù„ÙØ§Ø¸ Ú©Ùˆ Ù„Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ ØªÙ…Ø§Ù… symbols Ú©Ùˆ Ù„Û’ Ú©Ø± vocabulary Ø¨Ù†Ø§Ø¦ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û” Ø§ÛŒÚ© Ø¨ÛØª Ø¢Ø³Ø§Ù† Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û ÛÙ…Ø§Ø±Û’ corpus Ù…ÛŒÚº ÛŒÛ Ù¾Ø§Ù†Ú† Ø§Ù„ÙØ§Ø¸ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªÛ’ ÛÛŒÚº:

```
"hug", "pug", "pun", "bun", "hugs"
```

ØªØ¨ base vocabulary Ø¨Ù† Ø¬Ø§Ø¦Û’ Ú¯ÛŒ `["b", "g", "h", "n", "p", "s", "u"]`. Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ÛŒÚºØŒ Ø§Ø³ base vocabulary Ù…ÛŒÚº Ú©Ù… Ø§Ø² Ú©Ù… ØªÙ…Ø§Ù… ASCII characters Ø§ÙˆØ± Ø´Ø§ÛŒØ¯ Ú©Ú†Ú¾ Unicode characters Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ ÛÙˆÚº Ú¯Û’Û” Ø§Ú¯Ø± Ø¢Ù¾ tokenize Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ú©ÙˆØ¦ÛŒ Ø§ÛŒØ³Ø§ character Ø¢ØªØ§ ÛÛ’ Ø¬Ùˆ training corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†Û ÛÙˆ ØªÙˆ Ø§Ø³Û’ unknown token Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” ÛŒÛÛŒ ÙˆØ¬Û ÛÛ’ Ú©Û Ø¨ÛØª Ø³Û’ NLP Ù…Ø§ÚˆÙ„Ø² emojis Ø¬ÛŒØ³Û’ Ø¹Ù†Ø§ØµØ± Ú©Ùˆ analyze Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù†Ø§Ú©Ø§Ù… Ø±ÛØªÛ’ ÛÛŒÚºÛ”

<Tip>
GPT-2 Ø§ÙˆØ± RoBERTa tokenizers (Ø¬Ùˆ Ú©Ø§ÙÛŒ Ù…Ù…Ø§Ø«Ù„ ÛÛŒÚº) Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Ø§ Ø§ÛŒÚ© Ú†Ø§Ù„Ø§Ú© Ø­Ù„ Ù¾ÛŒØ´ Ú©Ø±ØªÛ’ ÛÛŒÚº: ÙˆÛ Ø§Ù„ÙØ§Ø¸ Ú©Ùˆ Unicode characters Ú©ÛŒ Ø¬Ú¯Û bytes Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ø·Ø±Ø­ base vocabulary Ú©Ø§ Ø³Ø§Ø¦Ø² Ú†Ú¾ÙˆÙ¹Ø§ (256) Ø±ÛØªØ§ ÛÛ’ØŒ Ù…Ú¯Ø± ÛØ± character Ø´Ø§Ù…Ù„ ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’ Ø§ÙˆØ± unknown token Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ù†ÛÛŒÚº ÛÙˆØªØ§Û” Ø§Ø³ Ù¹Ø±Ú© Ú©Ùˆ *byte-level BPE* Ú©ÛØªÛ’ ÛÛŒÚºÛ”
</Tip>

Ø§Ø³ base vocabulary Ú©Ùˆ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ØŒ ÛÙ… Ù†Ø¦Û’ tokens Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº ÛŒÛØ§Úº ØªÚ© Ú©Û Ù…Ø·Ù„ÙˆØ¨Û vocabulary size Ø­Ø§ØµÙ„ ÛÙˆ Ø¬Ø§Ø¦Û’ØŒ merge rules Ø³ÛŒÚ©Ú¾ Ú©Ø± Ø¬Ùˆ Ù…ÙˆØ¬ÙˆØ¯Û vocabulary Ú©Û’ Ø¯Ùˆ Ø¹Ù†Ø§ØµØ± Ú©Ùˆ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© Ù†ÛŒØ§ token Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§Ø¨ØªØ¯Ø§ Ù…ÛŒÚº ÛŒÛ merges Ø¯Ùˆ characters Ú©Û’ tokens Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’ØŒ Ø§ÙˆØ± Ø¬ÛŒØ³Û’ Ø¬ÛŒØ³Û’ training Ø¨Ú‘Ú¾ØªÛŒ Ø¬Ø§Ø¦Û’ Ú¯ÛŒØŒ ÛŒÛ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Û’ subwords Ø¨Ù†Ø§ØªÛ’ Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’Û”

Tokenzier training Ú©Û’ Ø¯ÙˆØ±Ø§Ù†ØŒ BPE Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… ÛØ± Ù‚Ø¯Ù… Ù¾Ø± Ù…ÙˆØ¬ÙˆØ¯ tokens Ú©Û’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent pair (ÛŒÛØ§Úº "pair" Ø³Û’ Ù…Ø±Ø§Ø¯ Ø§ÛŒÚ© Ù„ÙØ¸ Ù…ÛŒÚº Ù„Ú¯Ø§ØªØ§Ø± Ø¯Ùˆ tokens ÛÛ’) ØªÙ„Ø§Ø´ Ú©Ø±ØªØ§ ÛÛ’Û” ÙˆÛÛŒ pair merge Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø§Ú¯Ù„Û’ Ù‚Ø¯Ù… Ú©Û’ Ù„ÛŒÛ’ Ø¹Ù…Ù„ Ú©Ùˆ Ø¯ÛØ±Ø§ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”

Ù¾Ú†Ú¾Ù„ÛŒ Ù…Ø«Ø§Ù„ Ú©ÛŒ Ø·Ø±Ù ÙˆØ§Ù¾Ø³ Ú†Ù„ØªÛ’ ÛÛŒÚºØŒ ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ frequencies Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÛŒÚº:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ÛŒØ¹Ù†ÛŒ `"hug"` corpus Ù…ÛŒÚº 10 Ø¨Ø§Ø±ØŒ `"pug"` 5 Ø¨Ø§Ø±ØŒ `"pun"` 12 Ø¨Ø§Ø±ØŒ `"bun"` 4 Ø¨Ø§Ø±ØŒ Ø§ÙˆØ± `"hugs"` 5 Ø¨Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û” Training Ú©ÛŒ Ø´Ø±ÙˆØ¹Ø§Øª Ù…ÛŒÚºØŒ ÛØ± Ù„ÙØ¸ Ú©Ùˆ characters Ù…ÛŒÚº split Ú©Ø± Ú©Û’ Ø¯ÛŒÚ©Ú¾Ø§ Ø¬Ø§ØªØ§ ÛÛ’:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

Ù¾Ú¾Ø± ÛÙ… pairs Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ pair `("h", "u")` `"hug"` Ø§ÙˆØ± `"hugs"` Ù…ÛŒÚº 15 Ø¨Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’Û” Ù…Ú¯Ø± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent pair `("u", "g")` ÛÛ’ØŒ Ø¬Ùˆ `"hug"`, `"pug"`, Ø§ÙˆØ± `"hugs"` Ù…ÛŒÚº 20 Ø¨Ø§Ø± Ø¢ØªØ§ ÛÛ’Û”

Ù„ÛÙ°Ø°Ø§ØŒ tokenizer Ú©ÛŒ Ù¾ÛÙ„ÛŒ merge rule ÛÛ’ `("u", "g") -> "ug"`, ÛŒØ¹Ù†ÛŒ Ú©Û `"ug"` Ú©Ùˆ vocabulary Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŒ Ø§ÙˆØ± corpus Ú©Û’ ØªÙ…Ø§Ù… Ø§Ù„ÙØ§Ø¸ Ù…ÛŒÚº Ø§Ø³ pair Ú©Ùˆ merge Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§Û” Ø§Ø³ Ù…Ø±Ø­Ù„Û’ Ú©Û’ Ø¨Ø¹Ø¯ vocabulary Ø§ÙˆØ± corpus Ú©Ú†Ú¾ ÛŒÙˆÚº Ù†Ø¸Ø± Ø¢Ø¦ÛŒÚº Ú¯Û’:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

Ø§Ø¨ ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ú©Ú†Ú¾ Ø§ÛŒØ³Û’ pairs Ø¨Ú¾ÛŒ ÛÛŒÚº Ø¬Ùˆ Ø¯Ùˆ Ø­Ø±ÙˆÙ Ø³Û’ Ø·ÙˆÛŒÙ„ token Ø¯ÛŒØªÛ’ ÛÛŒÚº: Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ pair `("h", "ug")` (Ø¬Ùˆ corpus Ù…ÛŒÚº 15 Ø¨Ø§Ø± Ø¢ØªØ§ ÛÛ’)Û” Ù…Ú¯Ø± Ø§Ø³ Ù…Ø±Ø­Ù„Û’ Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent pair `("u", "n")` ÛÛ’ØŒ Ø¬Ùˆ corpus Ù…ÛŒÚº 16 Ø¨Ø§Ø± Ø¢ØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ Ø¯ÙˆØ³Ø±ÛŒ merge rule ÛÛ’ `("u", "n") -> "un"`. Ø§Ø³ Ú©Ùˆ vocabulary Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø± Ú©Û’ corpus Ú©Ùˆ merge Ú©Ø±Ù†Û’ Ø³Û’ ÛÙ…ÛŒÚº Ù…Ù„ØªØ§ ÛÛ’:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

Ø§Ø¨ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent pair `("h", "ug")` ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ merge rule `("h", "ug") -> "hug"` Ø³ÛŒÚ©Ú¾Ø§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ø³Û’ ÛÙ…ÛŒÚº Ù¾ÛÙ„Ø§ ØªÛŒÙ† Ø­Ø±ÙÛŒ token Ù…Ù„ØªØ§ ÛÛ’Û” Merge Ú©Ø±Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ corpus ÛŒÙˆÚº Ù†Ø¸Ø± Ø¢Ø¦Û’ Ú¯Ø§:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

Ø§ÙˆØ± Ø§Ø³ÛŒ Ø·Ø±Ø­ ÛÙ… Ø¬Ø§Ø±ÛŒ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº Ø¬Ø¨ ØªÚ© Ú©Û Ù…Ø·Ù„ÙˆØ¨Û vocabulary size Ø­Ø§ØµÙ„ Ù†Û ÛÙˆ Ø¬Ø§Ø¦Û’Û”

<Tip>
âœï¸ **Ø§Ø¨ Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Ø±ÛŒ!** Ø¢Ù¾ Ú©Û’ Ø®ÛŒØ§Ù„ Ù…ÛŒÚº Ø§Ú¯Ù„ÛŒ merge rule Ú©ÛŒØ§ ÛÙˆÚ¯ÛŒØŸ
</Tip>

## Tokenization algorithm[[tokenization-algorithm]]

Tokenization ØªØ±Ø¨ÛŒØªÛŒ Ø¹Ù…Ù„ Ú©Û’ Ù‚Ø±ÛŒØ¨ Ù‚Ø±ÛŒØ¨ follow Ú©Ø±ØªÛŒ ÛÛ’ØŒ ÛŒØ¹Ù†ÛŒ Ú©Û Ù†Ø¦Û’ inputs Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ù…Ø±Ø§Ø­Ù„ Ø§Ù¾Ù†Ø§Ø¦Û’ Ø¬Ø§ØªÛ’ ÛÛŒÚº:

1. Normalization  
2. Pre-tokenization  
3. Ø§Ù„ÙØ§Ø¸ Ú©Ùˆ individual characters Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ù†Ø§  
4. Ø§Ù† splits Ù¾Ø± ØªØ±Ø¨ÛŒØª Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø³ÛŒÚ©Ú¾Û’ Ú¯Ø¦Û’ merge rules Ú©Ùˆ ØªØ±ØªÛŒØ¨ ÙˆØ§Ø± Ù„Ø§Ú¯Ùˆ Ú©Ø±Ù†Ø§

ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û ÛÙ… Ù†Û’ training Ú©Û’ Ø¯ÙˆØ±Ø§Ù† ØªÛŒÙ† merge rules Ø³ÛŒÚ©Ú¾Û’:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

ØªÙˆ Ù„ÙØ¸ `"bug"` tokenize ÛÙˆÚ¯Ø§ `["b", "ug"]`. Ù…Ú¯Ø± `"mug"` tokenize ÛÙˆÚ¯Ø§ `["[UNK]", "ug"]` Ú©ÛŒÙˆÙ†Ú©Û Ø­Ø±Ù `"m"` base vocabulary Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚºÛ” Ø§Ø³ÛŒ Ø·Ø±Ø­ØŒ `"thug"` tokenize ÛÙˆÚ¯Ø§ `["[UNK]", "hug"]`: Ø­Ø±Ù `"t"` Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛÛŒÚº Ø§ÙˆØ± merge rules Ú©Û’ ØªØ­Øª Ù¾ÛÙ„Û’ `"u"` Ø§ÙˆØ± `"g"` merge ÛÙˆ Ø¬Ø§ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ù¾Ú¾Ø± `"h"` Ø§ÙˆØ± `"ug"` merge ÛÙˆ Ø¬Ø§ØªÛ’ ÛÛŒÚºÛ”

<Tip>
âœï¸ **Ø§Ø¨ Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Ø±ÛŒ!** Ø¢Ù¾ Ú©Û’ Ø®ÛŒØ§Ù„ Ù…ÛŒÚº Ù„ÙØ¸ `"unhug"` Ú©Ùˆ Ú©ÛŒØ³Û’ tokenize Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŸ
</Tip>

## Implementing BPE[[implementing-bpe]]

Ø§Ø¨ Ø¢Ø¦ÛŒÚº BPE Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©ÛŒ Ø§ÛŒÚ© implementation Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºÛ” ÛŒÛ Ú©ÙˆØ¦ÛŒ optimized ÙˆØ±Ú˜Ù† Ù†ÛÛŒÚº ÛÛ’ Ø¬Ùˆ Ø¨Ú‘Û’ corpus Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆ Ø³Ú©Û’ Ú¯Ø§Ø› ÛÙ…Ø§Ø±Ø§ Ù…Ù‚ØµØ¯ ØµØ±Ù Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Ùˆ Ø¨ÛØªØ± Ø³Ù…Ø¬Ú¾Ù†Ø§ ÛÛ’Û”

Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº Ø§ÛŒÚ© corpus Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’ØŒ ØªÙˆ Ø¢Ø¦ÛŒÚº Ú†Ù†Ø¯ Ø¬Ù…Ù„ÙˆÚº Ù¾Ø± Ù…Ø´ØªÙ…Ù„ Ø§ÛŒÚ© Ø¢Ø³Ø§Ù† corpus ØªØ®Ù„ÛŒÙ‚ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Ù¾Ú¾Ø± ÛÙ…ÛŒÚº Ø§Ø³ corpus Ú©Ùˆ pre-tokenize Ú©Ø± Ú©Û’ Ø§Ù„ÙØ§Ø¸ Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ú†ÙˆÙ†Ú©Û ÛÙ… BPE tokenizer (Ø¬ÛŒØ³Û’ GPT-2) Ú©ÛŒ Ù†Ù‚Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… `gpt2` tokenizer Ú©Ø§ pre-tokenization Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Ø§Ø¨ ÛÙ… corpus Ù…ÛŒÚº ÛØ± Ù„ÙØ¸ Ú©ÛŒ frequencies compute Ú©Ø±ØªÛ’ ÛÛŒÚº pre-tokenization Ú©Û’ Ø¯ÙˆØ±Ø§Ù†:

```py
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

Ø§Ø¨ base vocabulary Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Ø§ Ù…Ø±Ø­Ù„Û Ø¢ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ corpus Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ ØªÙ…Ø§Ù… characters Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÙˆØªØ§ ÛÛ’:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python
[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
 't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

ÛÙ… Ø§Ø³ alphabet Ù…ÛŒÚº model Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø´Ø¯Û special tokens Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” GPT-2 Ú©Û’ Ù…Ø¹Ø§Ù…Ù„Û’ Ù…ÛŒÚºØŒ ÙˆØ§Ø­Ø¯ special token `<|endoftext|>` ÛÛ’:

```py
vocab = ["<|endoftext|>"] + alphabet.copy()
```

Ø§Ø¨ ÛÙ… ÛØ± Ù„ÙØ¸ Ú©Ùˆ Ø§Ù†ÙØ±Ø§Ø¯ÛŒ characters Ù…ÛŒÚº split Ú©Ø± Ù„ÛŒØªÛ’ ÛÛŒÚº ØªØ§Ú©Û ØªØ±Ø¨ÛŒØª Ø´Ø±ÙˆØ¹ Ú©ÛŒ Ø¬Ø§ Ø³Ú©Û’:

```py
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

Ø§Ø¨ training Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚºÛ” Ø§Ú¯Ù„Ø§ Ù…Ø±Ø­Ù„Û ÛŒÛ ÛÛ’ Ú©Û ÛÙ… ÛØ± possible pair Ú©ÛŒ frequency compute Ú©Ø±ÛŒÚºÛ” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© function Ù„Ú©Ú¾ØªÛ’ ÛÛŒÚº:

```py
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

Ø¢Ø¦ÛŒÚº Ø§Ø³ dictionary Ú©Û’ Ú©Ú†Ú¾ entries Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº:

```py
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

Ø§Ø¨ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent pair ØªÙ„Ø§Ø´ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’:

```py
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python
('Ä ', 't') 7
```

ÛŒØ¹Ù†ÛŒ Ù¾ÛÙ„ÛŒ merge rule `('Ä ', 't') -> 'Ä t'` ÛÙˆÚ¯ÛŒØŒ Ø§ÙˆØ± ÛÙ… `'Ä t'` Ú©Ùˆ vocabulary Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```py
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

Ø§Ø¨ ÛÙ…ÛŒÚº splits dictionary Ù…ÛŒÚº Ø§Ø³ merge Ú©Ùˆ apply Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” Ø§ÛŒÚ© function Ù„Ú©Ú¾ØªÛ’ ÛÛŒÚº:

```py
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

Ø§Ø¨ Ù¾ÛÙ„ÛŒ merge Ú©Ø§ Ù†ØªÛŒØ¬Û Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº:

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```python
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

Ø§Ø¨ ÛÙ… vocabulary Ú©Ø§ size 50 ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ loop Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

Ø§Ø³ Ú©Û’ Ù†ØªÛŒØ¬Û’ Ù…ÛŒÚºØŒ ÛÙ… Ù†Û’ 19 merge rules Ø³ÛŒÚ©Ú¾ Ù„ÛŒ (Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary size 31 ØªÚ¾ÛŒ â€” 30 alphabet characters Ø§ÙˆØ± special token Ø´Ø§Ù…Ù„ ØªÚ¾Û’):

```py
print(merges)
```

```python
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd',
 ('Ä is',): 'Ä is', ('Ä th', 'e'): 'Ä the', ('in',): 'in', ('Ä ab',): 'Ä ab', ('Ä tokeni',): 'Ä tokeni'}
```

Ø§ÙˆØ± vocabulary:

```py
print(vocab)
```

```python
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

<Tip>
ğŸ’¡ `train_new_from_iterator()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ Ø³Û’ Ø§Ø³ÛŒ corpus Ù¾Ø± ØªØ±Ø¨ÛŒØª Ø¯ÛŒØ§ Ú¯ÛŒØ§ tokenizer Ø¹ÛŒÙ† Ø§Ø³ÛŒ vocabulary Ù¾Ø± Ù†ÛÛŒÚº Ù¾ÛÙ†Ú†ØªØ§ Ú©ÛŒÙˆÙ†Ú©Û Ø¬Ø¨ Ø²ÛŒØ§Ø¯Û frequent pair Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ ÛÙˆØªØ§ ÛÛ’ ØªÙˆ ÛÙ… Ù¾ÛÙ„ÛŒ occurrence Ù…Ù†ØªØ®Ø¨ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§Ù¾Ù†Û’ Ø§Ù†Ø¯Ø±ÙˆÙ†ÛŒ IDs Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ù¾ÛÙ„ÛŒ occurrence Ù…Ù†ØªØ®Ø¨ Ú©Ø±ØªÛŒ ÛÛ’Û”
</Tip>

Ù†Ø¦Û’ text Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛÙ„Û’ pre-tokenization Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’ØŒ Ù¾Ú¾Ø± split Ú©ÛŒÛ’ Ú¯Ø¦Û’ characters Ù¾Ø± Ø³ÛŒÚ©Ú¾Û’ Ú¯Ø¦Û’ merge rules Ù„Ø§Ú¯Ùˆ Ú©ÛŒÛ’ Ø¬Ø§ØªÛ’ ÛÛŒÚº:

```py
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

Ø§Ø³Û’ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ text Ù¾Ø± Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº:

```py
tokenize("This is not a token.")
```

```python
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

<Tip warning={true}>
âš ï¸ ÛÙ…Ø§Ø±ÛŒ implementation unknown character Ú©ÛŒ ØµÙˆØ±Øª Ù…ÛŒÚº error Ø¯Û’ Ú¯ÛŒ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ… Ù†Û’ Ø§Ù† Ú©Ùˆ handle Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Ú†Ú¾ Ù†ÛÛŒÚº Ú©ÛŒØ§Û” GPT-2 Ú©Ø§ Ú©ÙˆØ¦ÛŒ unknown token Ù†ÛÛŒÚº ÛÙˆØªØ§ (byte-level BPE Ú©ÛŒ ÙˆØ¬Û Ø³Û’) Ù„ÛŒÚ©Ù† ÛŒÛØ§Úº Ø§ÛŒØ³Ø§ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û ÛÙ… Ù†Û’ Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary Ù…ÛŒÚº ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û bytes Ø´Ø§Ù…Ù„ Ù†ÛÛŒÚº Ú©ÛŒÛ’Û”
</Tip>

Ø§Ø¨ BPE Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº ÛÙ…Ø§Ø±Ø§ Ø¬Ø§Ø¦Ø²Û Ø®ØªÙ… ÛÙˆØªØ§ ÛÛ’! Ø§Ú¯Ù„Û’ Ø­ØµÛ’ Ù…ÛŒÚº ÛÙ… WordPiece Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„ÛŒÚº Ú¯Û’Û”