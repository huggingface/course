# تعارف[[introduction]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

[باب 3](/course/chapter3) میں، ہم نے اس بات کا جائزہ لیا کہ کسی مخصوص کام پر ماڈل کو fine-tune کیسے کیا جاتا ہے۔ جب ہم ایسا کرتے ہیں تو ہم وہی tokenizer استعمال کرتے ہیں جس پر ماڈل کو پہلے سے تربیت دی گئی ہوتی ہے — لیکن جب ہم صفر سے ماڈل کی تربیت کرنا چاہتے ہیں تو کیا کریں؟ ایسی صورتوں میں، کسی دوسرے ڈومین یا زبان کے corpus پر پہلے سے تربیت یافتہ tokenizer کا استعمال عموماً غیر مؤثر ثابت ہوتا ہے۔ مثال کے طور پر، اگر ایک tokenizer کو انگریزی corpus پر تربیت دی گئی ہو تو وہ جاپانی texts کے corpus پر خراب کارکردگی دکھائے گا کیونکہ دونوں زبانوں میں spaces اور punctuation کا استعمال کافی مختلف ہوتا ہے۔

اس باب میں، آپ سیکھیں گے کہ کس طرح ایک نیا tokenizer texts کے corpus پر تربیت دی جائے، تاکہ اسے language model کی pretraining کے لیے استعمال کیا جا سکے۔ یہ سب کام [🤗 Tokenizers](https://github.com/huggingface/tokenizers) لائبریری کی مدد سے کیا جائے گا، جو [🤗 Transformers](https://github.com/huggingface/transformers) لائبریری میں "فاسٹ" tokenizers فراہم کرتی ہے۔ ہم اس لائبریری کی فراہم کردہ خصوصیات کا قریب سے جائزہ لیں گے اور یہ دریافت کریں گے کہ فاسٹ tokenizers "سلو" ورژنز سے کیسے مختلف ہیں۔

ہم درج ذیل موضوعات پر بات کریں گے:

* کس طرح ایک نیا tokenizer تربیت دیں جو کسی دیے گئے checkpoint کے استعمال کردہ tokenizer کے مماثل ہو لیکن نئے texts کے corpus پر ہو
* فاسٹ tokenizers کی خاص خصوصیات
* NLP میں آج استعمال ہونے والے تین اہم subword tokenization algorithms کے درمیان فرق
* 🤗 Tokenizers لائبریری کا استعمال کرتے ہوئے صفر سے tokenizer بنانا اور اسے ڈیٹا پر تربیت دینا

اس باب میں متعارف کرائی جانے والی تکنیکیں آپ کو [باب 7](/course/chapter7/6) کے اس حصے کے لیے تیار کریں گی جہاں ہم Python source code کے لیے language model تخلیق کرنے پر غور کریں گے۔ آئیں پہلے یہ دیکھتے ہیں کہ tokenizer کو "train" کرنے کا کیا مطلب ہے۔