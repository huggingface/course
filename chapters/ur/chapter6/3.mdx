<FrameworkSwitchCourse {fw} />

# ÙØ§Ø³Ù¹ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©ÛŒ Ø®Ø§Øµ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº[[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}

Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº ÛÙ… ğŸ¤— Transformers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ø§ ØªÙØµÛŒÙ„ Ø³Û’ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº Ú¯Û’Û” Ø§Ø¨ ØªÚ© ÛÙ… Ù†Û’ ØµØ±Ù inputs Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ ÛŒØ§ IDs Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û text Ù…ÛŒÚº decode Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ ÛÛ’ØŒ Ù…Ú¯Ø± Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² â€” Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± ÙˆÛ Ø¬Ùˆ ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø³Û’ Ù…Ø±Ø¨ÙˆØ· ÛÛŒÚº â€” Ø¨ÛØª Ú©Ú†Ú¾ Ø§ÙˆØ± Ø¨Ú¾ÛŒ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ù† Ø§Ø¶Ø§ÙÛŒ Ø®ØµÙˆØµÛŒØ§Øª Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… ÛŒÛ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ ÛÙ… token-classification (Ø¬Ø³Û’ ÛÙ… Ù†Û’ NER Ú©ÛØ§ ØªÚ¾Ø§) Ø§ÙˆØ± question-answering pipelines Ú©Û’ Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ù† Ú©Ø§ ÛÙ… Ù†Û’ Ù¾ÛÙ„ÛŒ Ø¨Ø§Ø± [Ø¨Ø§Ø¨ 1](/course/chapter1) Ù…ÛŒÚº Ù…Ø´Ø§ÛØ¯Û Ú©ÛŒØ§ ØªÚ¾Ø§Û”

<Youtube id="g8quOxoqhHQ"/>

ÛÙ…Ø§Ø±ÛŒ Ú¯ÙØªÚ¯Ùˆ Ù…ÛŒÚºØŒ ÛÙ… Ø§Ú©Ø«Ø± "Ø³Ù„Ùˆ" Ø§ÙˆØ± "ÙØ§Ø³Ù¹" Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† ØªÙØ±ÛŒÙ‚ Ú©Ø±ÛŒÚº Ú¯Û’Û” Ø³Ù„Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² ÙˆÛ ÛÛŒÚº Ø¬Ùˆ ğŸ¤— Transformers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ù…ÛŒÚº Python Ù…ÛŒÚº Ù„Ú©Ú¾Û’ Ú¯Ø¦Û’ ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û ÙØ§Ø³Ù¹ ÙˆØ±Ú˜Ù† ÙˆÛ ÛÛŒÚº Ø¬Ùˆ ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø³Û’ ÙØ±Ø§ÛÙ… Ú©ÛŒÛ’ Ø¬Ø§ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Rust Ù…ÛŒÚº Ù„Ú©Ú¾Û’ Ú¯Ø¦Û’ ÛÛŒÚºÛ” Ø§Ú¯Ø± Ø¢Ù¾ Ù†Û’ [Ø¨Ø§Ø¨ 5](/course/chapter5/3) Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ table Ø¯ÛŒÚ©Ú¾ÛŒ ÛÙˆ Ú©Û Ú©Ø³ Ø·Ø±Ø­ fast Ø§ÙˆØ± slow Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† tokenization Ú©ÛŒ Ø±ÙØªØ§Ø± Ù…ÛŒÚº ÙØ±Ù‚ ÛÛ’ØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ Ø§Ù†Ø¯Ø§Ø²Û ÛÙˆ Ø¬Ø§Ù†Ø§ Ú†Ø§ÛÛŒÛ’ Ú©Û Ø§Ù†ÛÛŒÚº ÙØ§Ø³Ù¹ Ø§ÙˆØ± Ø³Ù„Ùˆ Ú©ÛŒÙˆÚº Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’:

|               | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>
âš ï¸ Ø¬Ø¨ Ø¢Ù¾ Ø§ÛŒÚ© Ø¬Ù…Ù„Û’ Ú©Ùˆ tokenize Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ ÛÙ…ÛŒØ´Û slow Ø§ÙˆØ± fast ÙˆØ±Ú˜Ù†Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø±ÙØªØ§Ø± Ú©Ø§ ÙØ±Ù‚ Ù†ÛÛŒÚº Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’Û” Ø¯Ø±Ø§ØµÙ„ØŒ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ Ø¬Ù…Ù„Û’ Ú©Û’ Ù„ÛŒÛ’ ÙØ§Ø³Ù¹ ÙˆØ±Ú˜Ù† Ø³Ù„Ùˆ ÙˆØ±Ú˜Ù† Ø³Û’ Ø¢ÛØ³ØªÛ Ø¨Ú¾ÛŒ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’! ÙØ±Ù‚ ØªØ¨ ÙˆØ§Ø¶Ø­ ÛÙˆØªØ§ ÛÛ’ Ø¬Ø¨ Ø¢Ù¾ Ø¨ÛŒÚ© ÙˆÙ‚Øª Ø¨ÛØª Ø³Û’ texts Ú©Ùˆ parallel Ù…ÛŒÚº tokenize Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”
</Tip>

## Ø¨ÛŒÚ† Ø§Ù†Ú©ÙˆÚˆÙ†Ú¯[[batch-encoding]]

<Youtube id="3umI3tm27Vw"/>

Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ø§ output ØµØ±Ù Ø§ÛŒÚ© Ø³Ø§Ø¯Û Python ÚˆÚ©Ø´Ù†Ø±ÛŒ Ù†ÛÛŒÚº ÛÙˆØªØ§Ø› Ø¬Ùˆ ÛÙ… Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº ÙˆÛ Ø§ÛŒÚ© Ø®Ø§Øµ `BatchEncoding` object ÛÙˆØªØ§ ÛÛ’Û” ÛŒÛ Ø§ÛŒÚ© ÚˆÚ©Ø´Ù†Ø±ÛŒ Ú©Ø§ Ø³Ø¨ Ú©Ù„Ø§Ø³ ÛÛ’ (Ø§Ø³ÛŒ Ù„ÛŒÛ’ ÛÙ… Ø¨ØºÛŒØ± Ú©Ø³ÛŒ Ù…Ø³Ø¦Ù„Û’ Ú©Û’ Ø§Ø³ Ú©Ùˆ index Ú©Ø± Ù¾Ø§Ø¦Û’ ØªÚ¾Û’)ØŒ Ù…Ú¯Ø± Ø§Ø³ Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛŒ Ù…ÛŒØªÚ¾ÚˆØ² Ø´Ø§Ù…Ù„ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø²ÛŒØ§Ø¯Û ØªØ± ÙØ§Ø³Ù¹ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

Parallelization Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ø¹Ù„Ø§ÙˆÛØŒ ÙØ§Ø³Ù¹ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø®ØµÙˆØµÛŒØª ÛŒÛ ÛÛ’ Ú©Û ÙˆÛ ÛÙ…ÛŒØ´Û Ø§Ø³ original text Ú©Û’ span Ú©Ø§ Ù¾ØªÛ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº Ø¬Ø³ Ø³Û’ final tokens Ø¢Ø¦Û’ ÛÛŒÚº â€” Ø§Ø³ Ø®ØµÙˆØµÛŒØª Ú©Ùˆ ÛÙ… *offset mapping* Ú©ÛØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ø³Û’ ÛÙ…ÛŒÚº ÛŒÛ ÙØ§Ø¦Ø¯Û ÛÙˆØªØ§ ÛÛ’ Ú©Û ÛÙ… ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø± Ø³Ú©ÛŒÚº Ú©Û ÛØ± word Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆÙ† Ø³Û’ tokens generate ÛÙˆØ¦Û’ ÛÛŒÚº ÛŒØ§ ÛØ± character Ú©Ø§ ØªØ¹Ù„Ù‚ Ú©Ø³ token Ø³Û’ ÛÛ’ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø¨Ø±Ø¹Ú©Ø³ Ø¨Ú¾ÛŒÛ”

Ø¢Ø¦ÛŒÛ’ Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

```python
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

Ú©ÛŒÙˆÙ†Ú©Û `AutoTokenizer` ÚˆÛŒÙØ§Ù„Ù¹ Ø·ÙˆØ± Ù¾Ø± Ø§ÛŒÚ© ÙØ§Ø³Ù¹ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù…Ù†ØªØ®Ø¨ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… Ø§Ø¶Ø§ÙÛŒ Ù…ÛŒØªÚ¾ÚˆØ² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” ÛÙ… Ø¯Ùˆ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ø¬Ø§Ù† Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ø¢ÛŒØ§ ÛÙ…Ø§Ø±Ø§ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± ÙØ§Ø³Ù¹ ÛÛ’ ÛŒØ§ Ø³Ù„Ùˆ:

```py
tokenizer.is_fast
```

```python
True
```

ÛŒØ§ `encoding` object Ú©ÛŒ `is_fast` attribute Ú†ÛŒÚ© Ú©Ø± Ú©Û’:

```py
encoding.is_fast
```

```python
True
```

ÙØ§Ø³Ù¹ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± ÛÙ…ÛŒÚº ÛŒÛ Ø¨Ú¾ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªØ§ ÛÛ’ Ú©Û ÛÙ… tokens Ú©Ùˆ IDs Ù…ÛŒÚº convert Ú©ÛŒÛ’ Ø¨ØºÛŒØ± Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª tokens ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ÛŒÚº:

```py
encoding.tokens()
```

```python
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛŒÛØ§Úº token index 5 Ù¾Ø± `##yl` ÛÛ’ Ø¬Ùˆ Ø§ØµÙ„ Ù…ÛŒÚº "Sylvain" Ú©Û’ Ø§ÛŒÚ© Ø­ØµÛ’ Ø³Û’ Ø¢ÛŒØ§ ÛÛ’Û” ÛÙ… `word_ids()` Ù…ÛŒØªÚ¾Úˆ Ú©Ø§ Ø¨Ú¾ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û Ù…Ø¹Ù„ÙˆÙ… ÛÙˆ Ø³Ú©Û’ Ú©Û ÛØ± token Ú©Ø³ word Ø³Û’ Ø¢ÛŒØ§ ÛÛ’:

```py
encoding.word_ids()
```

```python
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

ÛÙ… Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ø®Ø§Øµ tokens `[CLS]` Ø§ÙˆØ± `[SEP]` Ú©Ùˆ `None` map Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± ÛØ± token Ú©Ùˆ Ø§Ø³ word Ú©Ø§ index Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ Ø¬Ø³ Ø³Û’ ÙˆÛ Ø¢ÛŒØ§ ÛÛ’Û” ÛŒÛ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ Ø¨Ø§Øª Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ Ù…ÛŒÚº Ù…ÙÛŒØ¯ ÛÛ’ Ú©Û Ú©ÙˆØ¦ÛŒ token word Ú©Û’ Ø´Ø±ÙˆØ¹ Ù…ÛŒÚº ÛÛ’ ÛŒØ§ Ø¯Ùˆ tokens Ø§ÛŒÚ© ÛÛŒ word Ú©Û’ ÛÛŒÚºÛ” Ø§Ú¯Ø±Ú†Û ÛÙ… `##` prefix Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ ÛŒÛ ØµØ±Ù BERT Ø¬ÛŒØ³Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ù¾Ø± Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’Ø› ÛŒÛ Ø·Ø±ÛŒÙ‚Û Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ù„ÛŒÛ’ Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’ Ø¨Ø´Ø±Ø·ÛŒÚ©Û ÙˆÛ fast ÛÙˆÛ”

Ø§Ø³ÛŒ Ø·Ø±Ø­ØŒ Ø§ÛŒÚ© `sentence_ids()` Ù…ÛŒØªÚ¾Úˆ Ø¨Ú¾ÛŒ Ù…ÙˆØ¬ÙˆØ¯ ÛÛ’ Ø¬Ø³ Ø³Û’ ÛÙ… ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ú©ÙˆØ¦ÛŒ token Ú©Ø³ sentence Ø³Û’ Ø¢ÛŒØ§ ÛÛ’ (Ø­Ø§Ù„Ø§Ù†Ú©Û Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚº tokenizer Ú©Û’ return Ú©Ø±Ø¯Û `token_type_ids` Ø¨Ú¾ÛŒ ÛŒÛÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙØ±Ø§ÛÙ… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº)Û”

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… `word_to_chars()` ÛŒØ§ `token_to_chars()` Ø§ÙˆØ± `char_to_word()` ÛŒØ§ `char_to_token()` Ù…ÛŒØªÚ¾ÚˆØ² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ word ÛŒØ§ token Ú©Ùˆ original text Ú©Û’ characters Ú©Û’ Ø³Ø§ØªÚ¾ map Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ `word_ids()` ÛÙ…ÛŒÚº Ø¨ØªØ§ØªØ§ ÛÛ’ Ú©Û token index 5 (`##yl`) word index 3 Ú©Ø§ Ø­ØµÛ ÛÛ’ØŒ Ù…Ú¯Ø± Ø§ØµÙ„ Ù…ÛŒÚº ÛŒÛ Ú©Ø³ word Ú©Ø§ Ø­ØµÛ ÛÛ’ØŸ ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python
Sylvain
```

Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ Ø°Ú©Ø± Ú©ÛŒØ§ØŒ ÛŒÛ Ø³Ø¨ fast Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ø§Ø³ fact Ú©ÛŒ Ø¨Ø¯ÙˆÙ„Øª Ù…Ù…Ú©Ù† ÛÙˆØ§ ÛÛ’ Ú©Û ÙˆÛ ÛØ± token Ú©Û’ origin text Ú©Ø§ span Ø±Ú©Ú¾ØªØ§ ÛÛ’ØŒ Ø¬Ø³Û’ ÛÙ… offsets Ú©ÛØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº ÛÙ… Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ú¯Û’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ ÛÙ… manually token-classification pipeline Ú©Û’ Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ Ø¯ÙˆØ¨Ø§Ø±Û Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

<Tip>
âœï¸ **Ø¢Ø²Ù…Ø§Ø¦ÛŒÚº!** Ø§Ù¾Ù†Ø§ Ú©ÙˆØ¦ÛŒ example text Ø¨Ù†Ø§Ø¦ÛŒÚº Ø§ÙˆØ± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú©Û Ú©ÙˆÙ† Ø³Û’ tokens Ú©Ø³ word Ú©Û’ Ø³Ø§ØªÚ¾ Ø¬Ú‘Û’ ÛÙˆØ¦Û’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø§ÛŒÚ© word Ú©Û’ Ù„ÛŒÛ’ character spans Ú©ÛŒØ³Û’ extract Ú©ÛŒÛ’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ø¶Ø§ÙÛŒ Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø¯Ùˆ Ø¬Ù…Ù„ÙˆÚº Ú©Ø§ input Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú©Û sentence IDs Ø³Ù…Ø¬Ú¾ Ù…ÛŒÚº Ø¢ØªÛ’ ÛÛŒÚº ÛŒØ§ Ù†ÛÛŒÚºÛ”
</Tip>

## token-classification pipeline Ú©Û’ Ø§Ù†Ø¯Ø±[[inside-the-token-classification-pipeline]]

[Ø¨Ø§Ø¨ 1](/course/chapter1) Ù…ÛŒÚº ÛÙ…ÛŒÚº NER Ú©Ø§ Ù¾ÛÙ„Ø§ ØªØ¬Ø±Ø¨Û ÛÙˆØ§ ØªÚ¾Ø§ â€“ ÛŒØ¹Ù†ÛŒ Ú©Û text Ú©Û’ Ø§Ù† Ø­ØµÙˆÚº Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø±Ù†Ø§ Ø¬Ùˆ entities Ø¬ÛŒØ³Û’ Ú©Û personsØŒ locationsØŒ ÛŒØ§ organizations Ø³Û’ Ù…ØªØ¹Ù„Ù‚ ÛÙˆÚº â€“ ğŸ¤— Transformers Ú©Û’ `pipeline()` ÙÙ†Ú©Ø´Ù† Ú©Û’ Ø°Ø±ÛŒØ¹Û’Û” Ù¾Ú¾Ø± [Ø¨Ø§Ø¨ 2](/course/chapter2) Ù…ÛŒÚº ÛÙ… Ù†Û’ Ø¯ÛŒÚ©Ú¾Ø§ Ú©Û Ú©Ø³ Ø·Ø±Ø­ Ø§ÛŒÚ© pipeline raw text Ø³Û’ predictions ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Û’ ØªÛŒÙ† Ù…Ø±Ø§Ø­Ù„ Ú©Ùˆ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ø¬ÙˆÚ‘ØªØ§ ÛÛ’: tokenizationØŒ model Ú©Û’ Ø°Ø±ÛŒØ¹Û’ inputs pass Ú©Ø±Ù†Ø§ØŒ Ø§ÙˆØ± post-processingÛ” token-classification pipeline Ú©Û’ Ù¾ÛÙ„Û’ Ø¯Ùˆ Ù…Ø±Ø§Ø­Ù„ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ø¯ÙˆØ³Ø±Û’ pipeline Ú©ÛŒ Ø·Ø±Ø­ ÛÙˆØªÛ’ ÛÛŒÚºØŒ Ù…Ú¯Ø± post-processing ØªÚ¾ÙˆÚ‘Ø§ Ù¾ÛŒÚ†ÛŒØ¯Û ÛÙˆØªØ§ ÛÛ’ â€“ Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©ÛŒØ³Û’!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### pipeline Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Ø§[[getting-the-base-results-with-the-pipeline]]

Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ØŒ Ø¢Ø¦ÛŒÚº Ø§ÛŒÚ© token classification pipeline Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û ÛÙ… Ù†ØªØ§Ø¦Ø¬ Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û Ø¯Ø³ØªÛŒ Ø·ÙˆØ± Ù¾Ø± Ú©Ø± Ø³Ú©ÛŒÚºÛ” ÚˆÛŒÙØ§Ù„Ù¹ model [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english) Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’Ø› ÛŒÛ Ø¬Ù…Ù„ÙˆÚº Ù¾Ø± NER Ú©Ø±ØªØ§ ÛÛ’:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Ù…Ø§ÚˆÙ„ Ù†Û’ ØµØ­ÛŒØ­ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ "Sylvain" Ú©Û’ tokens Ú©Ùˆ person Ø§ÙˆØ± "Hugging Face" Ú©Û’ tokens Ú©Ùˆ organizationØŒ Ø§ÙˆØ± "Brooklyn" Ú©Ùˆ location classify Ú©ÛŒØ§Û” ÛÙ… pipeline Ú©Ùˆ Ø§Ø³ Ø·Ø±Ø­ Ø¨Ú¾ÛŒ Ú©ÛÛ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÙˆÛ Ø§ÛŒÚ© ÛÛŒ entity Ú©Û’ tokens Ú©Ùˆ group Ú©Ø± Ø¯Û’:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Ù…Ù†ØªØ®Ø¨ Ú©Ø±Ø¯Û `aggregation_strategy` ÛØ± Ú¯Ø±ÙˆÙ¾ Ø´Ø¯Û ÛØ³ØªÛŒ (entity) Ú©Û’ Ù„ÛŒÛ’ Ø­Ø³Ø§Ø¨ Ú©Ø±Ø¯Û Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Û’ Ú¯Ø§Û”  

- `"simple"` Ù…ÛŒÚºØŒ Ú©Ø³ÛŒ ÛØ³ØªÛŒ Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø§Ø³ Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ØªÙ…Ø§Ù… Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ø§ Ø§ÙˆØ³Ø· ÛÙˆØªØ§ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ **"Sylvain"** Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø§Ù†ÙØ±Ø§Ø¯ÛŒ Ù¹ÙˆÚ©Ù†Ø² `S`, `##yl`, `##va`, Ø§ÙˆØ± `##in` Ú©Û’ Ø§ÙˆØ³Ø· Ø§Ø³Ú©ÙˆØ± Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÙˆÚ¯Ø§Û”  
- `"first"` Ù…ÛŒÚºØŒ ÛØ³ØªÛŒ Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø§Ø³ Ú©Û’ Ù¾ÛÙ„Û’ Ù¹ÙˆÚ©Ù† Ú©Û’ Ø§Ø³Ú©ÙˆØ± Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÙˆØªØ§ ÛÛ’ØŒ Ø¬ÛŒØ³Û’ **"Sylvain"** Ú©Û’ Ù„ÛŒÛ’ ÛŒÛ `S` Ú©Ø§ Ø§Ø³Ú©ÙˆØ± (0.993828) ÛÙˆÚ¯Ø§Û”  
- `"max"` Ù…ÛŒÚºØŒ ÛØ³ØªÛŒ Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø§Ø³ Ù…ÛŒÚº Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… Ù¹ÙˆÚ©Ù†Ø² Ù…ÛŒÚº Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø§Ø³Ú©ÙˆØ± Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÙˆØªØ§ ÛÛ’ØŒ Ø¬ÛŒØ³Û’ **"Hugging Face"** Ú©Û’ Ù„ÛŒÛ’ "Face" Ú©Û’ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø§Ø³Ú©ÙˆØ± (0.98879766) Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÙˆÚ¯Ø§Û”  
- `"average"` Ù…ÛŒÚºØŒ Ú©Ø³ÛŒ ÛØ³ØªÛŒ Ú©Ø§ Ø§Ø³Ú©ÙˆØ± Ø§Ø³ Ú©Û’ ØªÙ…Ø§Ù… Ø§Ù„ÙØ§Ø¸ Ú©Û’ Ø§Ø³Ú©ÙˆØ±Ø² Ú©Ø§ Ø§ÙˆØ³Ø· ÛÙˆØªØ§ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ **"Sylvain"** Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³Ú©ÙˆØ± **"simple"** Ø·Ø±ÛŒÙ‚Û Ø³Û’ Ù…Ø®ØªÙ„Ù Ù†ÛÛŒÚº ÛÙˆÚ¯Ø§ØŒ Ù„ÛŒÚ©Ù† **"Hugging Face"** Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³Ú©ÙˆØ± (0.9819) ÛÙˆÚ¯Ø§ØŒ Ø¬Ùˆ "Hugging" (0.975) Ø§ÙˆØ± "Face" (0.98879) Ú©Û’ Ø§ÙˆØ³Ø· Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÛ’Û”  

Ø§Ø¨ Ø¢Ø¦ÛŒÛ’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û `pipeline()` ÙÙ†Ú©Ø´Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒÛ’ Ø¨ØºÛŒØ± Ø§Ù† Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ Ú©ÛŒØ³Û’ Ø­Ø§ØµÙ„ Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’!  

### Ø§Ù† Ù¾Ù¹ Ø³Û’ Ù¾ÛŒØ´Ú¯ÙˆØ¦ÛŒ ØªÚ©

{#if fw === 'pt'}

Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ input Ú©Ùˆ tokenize Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ Ø§ÙˆØ± model Ú©Û’ Ø°Ø±ÛŒØ¹Û’ pass Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” ÛŒÛ ÙˆÛÛŒ Ø·Ø±ÛŒÙ‚Û ÛÛ’ Ø¬ÛŒØ³Ø§ Ú©Û [Ø¨Ø§Ø¨ 2](/course/chapter2) Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ Ú¯ÛŒØ§Ø› ÛÙ… `AutoTokenizer` Ø§ÙˆØ± `AutoModelForTokenClassification` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

Ú†ÙˆÙ†Ú©Û ÛÙ… `AutoModelForTokenClassification` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŒ ÛÙ…ÛŒÚº ÛØ± token Ú©Û’ Ù„ÛŒÛ’ logits Ù…Ù„ØªÛ’ ÛÛŒÚº:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ Ø§Ù† Ù¾Ù¹ Ú©Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² (tokenize) Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ø³Û’ Ú¯Ø²Ø§Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û” ÛŒÛ Ø¹Ù…Ù„ Ø¨Ø§Ù„Ú©Ù„ [Ø¨Ø§Ø¨ 2](/course/chapter2) Ù…ÛŒÚº Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ø·Ø±ÛŒÙ‚Û’ Ú©ÛŒ Ø·Ø±Ø­ ÛÙˆØªØ§ ÛÛ’Û”  

ÛÙ… `TFAutoXxx` Ú©Ù„Ø§Ø³Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø§Ù†Ø³Ù¹ÛŒÙ¹ÛŒÙˆÙ¹ (instantiate) Ú©Ø±ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø§Ù†ÛÛŒÚº Ø§Ù¾Ù†ÛŒ Ù…Ø«Ø§Ù„ Ù¾Ø± Ù„Ø§Ú¯Ùˆ Ú©Ø±ÛŒÚº Ú¯Û’:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

Ú†ÙˆÙ†Ú©Û ÛÙ… ÛŒÛØ§Úº `TFAutoModelForTokenClassification` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº Ø§Ù† Ù¾Ù¹ ØªØ±ØªÛŒØ¨ Ù…ÛŒÚº ÛØ± Ù¹ÙˆÚ©Ù† Ú©Û’ Ù„ÛŒÛ’ Ø§Ù„Ú¯ Ø§Ù„Ú¯ Ù„Ø§Ø¬Ù¹Ø³ (logits) Ú©Ø§ Ø§ÛŒÚ© Ø³ÛŒÙ¹ Ù…Ù„ØªØ§ ÛÛ’:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python
(1, 19)
(1, 19, 9)
```

{/if}

ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ 1 sequence ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº 19 tokens ÛÛŒÚº Ø§ÙˆØ± model Ú©Û’ Ù¾Ø§Ø³ 9 labels ÛÛŒÚºØŒ Ù„ÛÙ°Ø°Ø§ output Ú©ÛŒ shape 1 x 19 x 9 ÛÛ’Û” ÛÙ… softmax function Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ logits Ú©Ùˆ probabilities Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± argmax Ù„Û’ Ú©Ø± predictions Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº (Ú†ÙˆÙ†Ú©Û softmax ØªØ±ØªÛŒØ¨ ØªØ¨Ø¯ÛŒÙ„ Ù†ÛÛŒÚº Ú©Ø±ØªØ§):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

`model.config.id2label` attribute ÛÙ…ÛŒÚº ÙˆÛ mapping Ø¯ÛŒØªØ§ ÛÛ’ Ø¬Ø³ Ø³Û’ ÛÙ… predictions Ú©Ùˆ Ø³Ù…Ø¬Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
model.config.id2label
```

```python
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

ÛŒÛØ§Úº 9 labels ÛÛŒÚº: `O` Ø§Ù† tokens Ú©Û’ Ù„ÛŒÛ’ Ø¬Ù†ÛÛŒÚº Ú©Ø³ÛŒ entity Ø³Û’ ÙˆØ§Ø¨Ø³ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ Ú¯ÛŒØ§ (ÛŒØ¹Ù†ÛŒ "outside")ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± ÛØ± entity (miscØŒ personØŒ organizationØŒ Ø§ÙˆØ± location) Ú©Û’ Ù„ÛŒÛ’ Ø¯Ùˆ labelsÛ” `B-XXX` entity Ú©Û’ Ø¢ØºØ§Ø² Ú©Ùˆ Ø¸Ø§ÛØ± Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± `I-XXX` Ø§Ù†Ø¯Ø±ÙˆÙ†ÛŒ tokens Ú©ÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛÙ… ØªÙˆÙ‚Ø¹ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û "Sylvain" Ú©Û’ tokens Ú©Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø·ÙˆØ± Ù¾Ø± classify Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’Û” Ù…Ø§ÚˆÙ„ Ù†Û’ ØªÙ…Ø§Ù… tokens Ú©Ùˆ `I-PER` Ø¯ÛŒØ§ØŒ Ø­Ø§Ù„Ø§Ù†Ú©Û Ø¢Ù¾ Ø³ÙˆÚ† Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û Ù¾ÛÙ„Ø§ token `B-PER` ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’Ø› Ù…Ú¯Ø± ÛŒÛ Ø¯Ø±Ø³Øª ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û Ù…Ø®ØªÙ„Ù formats (IOB1 Ø§ÙˆØ± IOB2) Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºÛ” IOB1 Ù…ÛŒÚº adjacent entities Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† `B-` ØµØ±Ù Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’ Ø¬Ø¨Ú©Û IOB2 Ù…ÛŒÚº ÛØ± entity Ú©Û’ Ø¢ØºØ§Ø² Ù¾Ø± `B-` Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’Û” ÛÙ…Ø§Ø±Ø§ model IOB1 format Ù¾Ø± fine-tuned ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÙˆÛ ØªÙ…Ø§Ù… tokens Ú©Ùˆ `I-PER` ÛÛŒ label Ú©Ø±ØªØ§ ÛÛ’Û”

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

Ø§Ø¨ ÛÙ… base results Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÛŒÚºÛ” ÛÙ… ØµØ±Ù Ø§Ù† tokens Ú©Ø§ score Ø§ÙˆØ± label Ù„ÛŒÚº Ú¯Û’ Ø¬Ù†ÛÛŒÚº `O` Ú©Û’ Ø¹Ù„Ø§ÙˆÛ classify Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

ÛŒÛ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÙˆÛÛŒ ÛÛ’ Ø¬Ùˆ Ù¾ÛÙ„Û’ pipeline Ø³Û’ Ù…Ù„ØªØ§ ÛÛ’Û” pipeline Ù†Û’ additionally ÛØ± entity Ú©Û’ `start` Ø§ÙˆØ± `end` offsets Ø¨Ú¾ÛŒ Ø¯ÛŒÛ’ ØªÚ¾Û’Û” ÛŒÛ offsets fast tokenizer Ú©Û’ offset mapping Ú©ÛŒ Ø¨Ø¯ÙˆÙ„Øª Ù…Ù…Ú©Ù† ÛÙˆØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ø± ÛÙ… tokenizer Ú©Ùˆ `return_offsets_mapping=True` Ø³ÛŒÙ¹ Ú©Ø±ÛŒÚº ØªÙˆ:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

ÛØ± tuple ÙˆÛ span ÛÛ’ Ø¬Ùˆ ÛØ± token Ø³Û’ Ù…ØªØ¹Ù„Ù‚ ÛÛ’Ø› `(0, 0)` Ø®Ø§Øµ tokens Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØªØµ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ token index 5 `##yl` Ú©Û’ offsets `(12, 14)` ÛÛŒÚºØ› Ø§Ú¯Ø± ÛÙ… original example Ú©Û’ corresponding slice Ø¯ÛŒÚ©Ú¾ÛŒÚº:

```py
example[12:14]
```

```python
'yl'
```

Ø§Ø³ Ø·Ø±Ø­ ÛÙ… results Ù…Ú©Ù…Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ÛŒÛÛŒ ÙˆÛÛŒ Ù†ØªØ§Ø¦Ø¬ ÛÛŒÚº Ø¬Ùˆ ÛÙ…ÛŒÚº Ù¾ÛÙ„Û’ pipeline Ø³Û’ Ù…Ù„Û’ ØªÚ¾Û’Û”

### entities Ú©Ùˆ group Ú©Ø±Ù†Ø§[[grouping-entities]]

offsets Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ tokens Ú©Ùˆ group Ú©Ø±Ù†Ø§ Ø¨ÛØª Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø«Ø§Ø¨Øª ÛÙˆØªØ§ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û Ø§Ø³ Ø³Û’ ÛÙ…ÛŒÚº custom rules Ù„Ú©Ú¾Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº Ø±ÛØªÛŒÛ” Ù…Ø«Ù„Ø§Ù‹ØŒ Ø§Ú¯Ø± ÛÙ… tokens `Hu`, `##gging`, Ø§ÙˆØ± `Face` Ú©Ùˆ group Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ ÛÙ…ÛŒÚº ØµØ±Ù original text Ú©Û’ span Ú©Ùˆ Ù†Ú©Ø§Ù„Ù†Ø§ ÛÛ’ Ø¬Ùˆ index 33 Ø³Û’ Ø´Ø±ÙˆØ¹ ÛÙˆ Ú©Ø± index 45 Ù¾Ø± Ø®ØªÙ… ÛÙˆØªØ§ ÛÙˆ:

```py
example[33:45]
```

```python
'Hugging Face'
```

grouping Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… Ø§ÛŒØ³ÛŒ logic Ù„Ú©Ú¾ÛŒÚº Ú¯Û’ Ø¬Ùˆ Ù…Ø³Ù„Ø³Ù„ Ø§ÛŒØ³Û’ tokens Ú©Ùˆ group Ú©Ø± Ø¯Û’ Ø¬Ùˆ `I-XXX` Ú©Û’ Ø³Ø§ØªÚ¾ label Ú©ÛŒÛ’ Ú¯Ø¦Û’ ÛÙˆÚºØŒ Ø¬Ø¨Ú©Û Ù¾ÛÙ„ÛŒ token `B-XXX` ÛŒØ§ `I-XXX` ÛÙˆ Ø³Ú©ØªÛŒ ÛÛ’Û” ÛÙ… grouping ØªØ¨ Ø±ÙˆÚ© Ø¯ÛŒÚº Ú¯Û’ Ø¬Ø¨ ÛÙ…ÛŒÚº `O`ØŒ Ú©Ø³ÛŒ Ù†Ø¦Û’ Ù‚Ø³Ù… Ú©Ø§ entityØŒ ÛŒØ§ `B-XXX` Ù…Ù„Û’ Ø¬Ø³ Ø³Û’ Ø§Ø³ÛŒ type Ú©Ø§ Ù†ÛŒØ§ entity Ø´Ø±ÙˆØ¹ ÛÙˆ Ø±ÛØ§ ÛÙˆ:

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # B- ÛŒØ§ I- Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒÚº
        label = label[2:]
        start, _ = offsets[idx]

        # ØªÙ…Ø§Ù… tokens Ø¬Ù† Ú©Ø§ label I-label ÛÙˆ Ø§Ù†ÛÛŒÚº Ø¬Ù…Ø¹ Ú©Ø±ÛŒÚº
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # score ÛØ± token Ú©Û’ score Ú©Ø§ mean ÛÙˆØªØ§ ÛÛ’
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

Ø§ÙˆØ± ÛÙ…ÛŒÚº ÙˆÛÛŒ Ù†ØªØ§Ø¦Ø¬ Ù…Ù„ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’ Ú©Û Ù¾ÛÙ„Û’:

```python
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Ø§ÛŒÚ© Ø§ÙˆØ± Ù…Ø«Ø§Ù„ Ø¬ÛØ§Úº ÛŒÛ offsets Ø¨ÛØª Ù…ÙÛŒØ¯ Ø«Ø§Ø¨Øª ÛÙˆØªÛ’ ÛÛŒÚº ÙˆÛ question answering ÛÛ’ØŒ Ø¬Ø³ Ù¾Ø± Ø§Ú¯Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚº Ø¨Ø§Øª ÛÙˆÚ¯ÛŒÛ” Ø§Ø³ Ø·Ø±Ø­ ÛÙ… fast tokenizers Ú©ÛŒ ÙˆÛ ØªÙ…Ø§Ù… Ø®ØµÙˆØµÛŒØ§Øª Ø¯ÛŒÚ©Ú¾ Ú†Ú©Û’ ÛÛŒÚº Ø¬Ùˆ token classification Ø§ÙˆØ± question answering Ø¬ÛŒØ³Û’ Ú©Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø«Ø§Ø¨Øª ÛÙˆØªÛŒ ÛÛŒÚºÛ”