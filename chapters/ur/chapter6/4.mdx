# Normalization اور pre-tokenization[[normalization-and-pre-tokenization]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
    notebooks={[
      {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
      {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

Transformer ماڈلز کے ساتھ سب سے زیادہ استعمال ہونے والے subword tokenization الگورتھمز (BPE, WordPiece, اور Unigram) میں سے پہلے ہم اس preprocessing پر نظر ڈالیں گے جو ہر tokenizer text پر لاگو کرتا ہے۔ یہاں tokenization pipeline کے اقدامات کا ایک high-level overview پیش کیا گیا ہے:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

text کو subtokens میں تقسیم کرنے سے پہلے tokenizer دو اقدامات انجام دیتا ہے: _normalization_ اور _pre-tokenization_۔

## Normalization[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

Normalization مرحلے میں عام صفائی جیسے کہ غیر ضروری whitespace کو ہٹانا، lowercasing کرنا، اور accents کو ہٹانا شامل ہوتا ہے۔ اگر آپ [Unicode normalization](http://www.unicode.org/reports/tr15/) (جیسے NFC یا NFKC) سے واقف ہیں تو یہ وہی عمل ہے جو tokenizer لاگو کر سکتا ہے۔

🤗 Transformers tokenizer کا ایک attribute ہے جسے `backend_tokenizer` کہتے ہیں، جو کہ 🤗 Tokenizers لائبریری کا underlying tokenizer فراہم کرتا ہے:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python
<class 'tokenizers.Tokenizer'>
```

`normalizer` attribute کے ذریعے ہم دیکھ سکتے ہیں کہ normalization کیسے انجام پاتا ہے:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python
'hello how are u?'
```

اس مثال میں چونکہ ہم نے `bert-base-uncased` checkpoint منتخب کیا ہے، normalization lowercasing کرتا ہے اور accents ہٹا دیتا ہے۔

<Tip>
✏️ **آزمائیں!** `bert-base-cased` checkpoint سے ایک tokenizer لوڈ کریں اور اسی مثال کو پاس کریں۔ آپ کو cased اور uncased ورژنز میں کیا فرق نظر آتا ہے؟
</Tip>

## Pre-tokenization[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

جیسا کہ ہم اگلے حصوں میں دیکھیں گے، tokenizer کو صرف خام text پر تربیت نہیں دی جا سکتی۔ اس کے بجائے، پہلے ہمیں texts کو چھوٹے حصوں (مثلاً الفاظ) میں تقسیم کرنا ہوتا ہے۔ یہی وہ مرحلہ ہے جسے pre-tokenization کہتے ہیں۔ [باب 2](/course/chapter2) میں ہم نے دیکھا کہ word-based tokenizer whitespace اور punctuation پر تقسیم کرتا ہے، اور یہ الفاظ tokenizer کے training کے دوران subtokens کی حدود بن جاتے ہیں۔

فاسٹ ٹوکنائزرز کس طرح pre-tokenization انجام دیتے ہیں، یہ دیکھنے کے لیے ہم `pre_tokenize_str()` میتھڈ استعمال کر سکتے ہیں جو `pre_tokenizer` attribute میں موجود ہوتا ہے:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

نوٹ کریں کہ ٹوکنائزر offsets کا بھی خیال رکھ رہا ہے، جو ہمیں پچھلے سیکشن میں offset mapping فراہم کرنے میں مدد دیتا ہے۔ یہاں ٹوکنائزر دو spaces کو نظرانداز کر کے ایک space لیتا ہے، مگر `are` اور `you` کے درمیان offset jump کر کے اس کا حساب رکھتا ہے۔

چونکہ ہم BERT tokenizer استعمال کر رہے ہیں، pre-tokenization whitespace اور punctuation پر تقسیم پر مبنی ہوتی ہے۔ دیگر ٹوکنائزرز کے لیے اس کی rules مختلف ہو سکتی ہیں۔ مثال کے طور پر، اگر ہم GPT-2 tokenizer استعمال کریں:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

اس کا output ہوگا:

```python
[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)), ('?', (19, 20))]
```

یہاں GPT-2 tokenizer whitespace اور punctuation دونوں پر تقسیم کرتا ہے، مگر وہ spaces کو `Ġ` symbol کے ساتھ رکھتا ہے، جس سے ہمیں decoding میں اصل spaces واپس مل جاتے ہیں۔ نیز، اس tokenizer نے double space کو نظرانداز نہیں کیا۔

ایک آخری مثال کے طور پر، T5 tokenizer، جو SentencePiece algorithm پر مبنی ہے، کو دیکھتے ہیں:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python
[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]
```

GPT-2 tokenizer کی طرح، T5 tokenizer spaces کو مخصوص token (`▁`) سے ظاہر کرتا ہے، مگر T5 tokenizer صرف whitespace پر تقسیم کرتا ہے نہ کہ punctuation پر۔ نیز، یہ پہلے ہی sentence کے شروع میں ایک space شامل کر دیتا ہے (جو `▁Hello,` میں ظاہر ہے) اور double space کو نظرانداز کرتا ہے۔

اب جبکہ ہم نے دیکھا کہ مختلف ٹوکنائزرز کس طرح text کو process کرتے ہیں، ہم ان کے underlying الگورتھمز کو explore کرنا شروع کر سکتے ہیں۔ ہم سب سے پہلے broadly استعمال ہونے والے SentencePiece کا مختصر جائزہ لیں گے؛ پھر اگلے تین حصوں میں ہم subword tokenization کے تین اہم الگورتھمز (BPE, WordPiece، اور Unigram) پر بات کریں گے۔

## SentencePiece[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) ایک tokenization الگورتھم ہے جو text preprocessing کے لیے استعمال کیا جاتا ہے اور اسے آپ کسی بھی ماڈل کے ساتھ استعمال کر سکتے ہیں۔ یہ text کو Unicode characters کے سلسلے کے طور پر دیکھتا ہے، اور spaces کو ایک خاص character `▁` سے تبدیل کر دیتا ہے۔ Unigram الگورتھم (جو [باب 7](/course/chapter7/7) میں استعمال ہوتا ہے) کے ساتھ مل کر یہ pre-tokenization کی ضرورت کو ختم کر دیتا ہے، جو ایسی زبانوں کے لیے بہت مفید ہے جہاں space character استعمال نہیں ہوتا (مثلاً چینی یا جاپانی)۔

## Algorithm overview[[algorithm-overview]]

اگلے حصوں میں ہم تین اہم subword tokenization الگورتھمز پر تفصیل سے بات کریں گے: BPE (جو GPT-2 وغیرہ میں استعمال ہوتا ہے)، WordPiece (جیسے BERT میں) اور Unigram (جو T5 وغیرہ میں استعمال ہوتا ہے)۔ شروع کرنے سے پہلے، یہاں ان کے کام کرنے کا ایک مختصر overview دیا گیا ہے۔ اگر یہ ابھی واضح نہ ہو تو اگلے حصوں کے بعد دوبارہ اس table کو دیکھیں۔

Model | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Training | چھوٹے vocabulary سے شروع کر کے tokens merge کرنے کے قواعد سیکھتا ہے | چھوٹے vocabulary سے شروع کر کے tokens merge کرنے کے قواعد سیکھتا ہے | بڑے vocabulary سے شروع کر کے tokens کو remove کرنے کے قواعد سیکھتا ہے
Training step | سب سے زیادہ عام pair کو merge کرتا ہے | pair کی frequency اور انفرادی tokens کی کم frequency کو ترجیح دیتا ہے | پورے corpus پر loss کو کم سے کم کرنے کے لیے tokens کو remove کرتا ہے
Learns | Merge rules اور vocabulary | صرف vocabulary | ایک vocabulary جس میں ہر token کا score شامل ہو
Encoding | ایک word کو characters میں تقسیم کرتا ہے اور training کے دوران merge کیے گئے rules کو لاگو کرتا ہے | سب سے طویل subword تلاش کرتا ہے جو vocabulary میں موجود ہو، پھر باقی کے لیے دہرایا جاتا ہے | سب سے زیادہ موزوں split تلاش کرتا ہے، جو training کے دوران سیکھے گئے scores پر مبنی ہوتا ہے

اب آئیں BPE میں ڈوبیں!