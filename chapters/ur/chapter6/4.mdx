# Normalization ุงูุฑ pre-tokenization[[normalization-and-pre-tokenization]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
    notebooks={[
      {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
      {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

Transformer ูุงฺูุฒ ฺฉ ุณุงุชฺพ ุณุจ ุณ ุฒุงุฏ ุงุณุชุนูุงู ูู ูุงู subword tokenization ุงูฺฏูุฑุชฺพูุฒ (BPE, WordPiece, ุงูุฑ Unigram) ูฺบ ุณ ูพู ู ุงุณ preprocessing ูพุฑ ูุธุฑ ฺุงูฺบ ฺฏ ุฌู ุฑ tokenizer text ูพุฑ ูุงฺฏู ฺฉุฑุชุง  ุงฺบ tokenization pipeline ฺฉ ุงูุฏุงูุงุช ฺฉุง ุงฺฉ high-level overview ูพุด ฺฉุง ฺฏุง :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

text ฺฉู subtokens ูฺบ ุชูุณู ฺฉุฑู ุณ ูพู tokenizer ุฏู ุงูุฏุงูุงุช ุงูุฌุงู ุฏุชุง : _normalization_ ุงูุฑ _pre-tokenization_

## Normalization[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

Normalization ูุฑุญู ูฺบ ุนุงู ุตูุงุฆ ุฌุณ ฺฉ ุบุฑ ุถุฑูุฑ whitespace ฺฉู ูนุงูุงุ lowercasing ฺฉุฑูุงุ ุงูุฑ accents ฺฉู ูนุงูุง ุดุงูู ูุชุง  ุงฺฏุฑ ุขูพ [Unicode normalization](http://www.unicode.org/reports/tr15/) (ุฌุณ NFC ุง NFKC) ุณ ูุงูู ฺบ ุชู  ู ุนูู  ุฌู tokenizer ูุงฺฏู ฺฉุฑ ุณฺฉุชุง 

๐ค Transformers tokenizer ฺฉุง ุงฺฉ attribute  ุฌุณ `backend_tokenizer` ฺฉุช ฺบุ ุฌู ฺฉ ๐ค Tokenizers ูุงุฆุจุฑุฑ ฺฉุง underlying tokenizer ูุฑุงู ฺฉุฑุชุง :

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python
<class 'tokenizers.Tokenizer'>
```

`normalizer` attribute ฺฉ ุฐุฑุน ู ุฏฺฉฺพ ุณฺฉุช ฺบ ฺฉ normalization ฺฉุณ ุงูุฌุงู ูพุงุชุง :

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("Hรฉllรฒ hรดw are รผ?"))
```

```python
'hello how are u?'
```

ุงุณ ูุซุงู ูฺบ ฺููฺฉ ู ู `bert-base-uncased` checkpoint ููุชุฎุจ ฺฉุง ุ normalization lowercasing ฺฉุฑุชุง  ุงูุฑ accents ูนุง ุฏุชุง 

<Tip>
โ๏ธ **ุขุฒูุงุฆฺบ!** `bert-base-cased` checkpoint ุณ ุงฺฉ tokenizer ููฺ ฺฉุฑฺบ ุงูุฑ ุงุณ ูุซุงู ฺฉู ูพุงุณ ฺฉุฑฺบ ุขูพ ฺฉู cased ุงูุฑ uncased ูุฑฺูุฒ ูฺบ ฺฉุง ูุฑู ูุธุฑ ุขุชุง ุ
</Tip>

## Pre-tokenization[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

ุฌุณุง ฺฉ ู ุงฺฏู ุญุตูฺบ ูฺบ ุฏฺฉฺพฺบ ฺฏุ tokenizer ฺฉู ุตุฑู ุฎุงู text ูพุฑ ุชุฑุจุช ูฺบ ุฏ ุฌุง ุณฺฉุช ุงุณ ฺฉ ุจุฌุงุฆุ ูพู ูฺบ texts ฺฉู ฺฺพููน ุญุตูฺบ (ูุซูุงู ุงููุงุธ) ูฺบ ุชูุณู ฺฉุฑูุง ูุชุง   ู ูุฑุญู  ุฌุณ pre-tokenization ฺฉุช ฺบ [ุจุงุจ 2](/course/chapter2) ูฺบ ู ู ุฏฺฉฺพุง ฺฉ word-based tokenizer whitespace ุงูุฑ punctuation ูพุฑ ุชูุณู ฺฉุฑุชุง ุ ุงูุฑ  ุงููุงุธ tokenizer ฺฉ training ฺฉ ุฏูุฑุงู subtokens ฺฉ ุญุฏูุฏ ุจู ุฌุงุช ฺบ

ูุงุณูน ูนูฺฉูุงุฆุฒุฑุฒ ฺฉุณ ุทุฑุญ pre-tokenization ุงูุฌุงู ุฏุช ฺบุ  ุฏฺฉฺพู ฺฉ ู ู `pre_tokenize_str()` ูุชฺพฺ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ ุฌู `pre_tokenizer` attribute ูฺบ ููุฌูุฏ ูุชุง :

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

ูููน ฺฉุฑฺบ ฺฉ ูนูฺฉูุงุฆุฒุฑ offsets ฺฉุง ุจฺพ ุฎุงู ุฑฺฉฺพ ุฑุง ุ ุฌู ูฺบ ูพฺฺพู ุณฺฉุดู ูฺบ offset mapping ูุฑุงู ฺฉุฑู ูฺบ ูุฏุฏ ุฏุชุง  ุงฺบ ูนูฺฉูุงุฆุฒุฑ ุฏู spaces ฺฉู ูุธุฑุงูุฏุงุฒ ฺฉุฑ ฺฉ ุงฺฉ space ูุชุง ุ ูฺฏุฑ `are` ุงูุฑ `you` ฺฉ ุฏุฑูุงู offset jump ฺฉุฑ ฺฉ ุงุณ ฺฉุง ุญุณุงุจ ุฑฺฉฺพุชุง 

ฺููฺฉ ู BERT tokenizer ุงุณุชุนูุงู ฺฉุฑ ุฑ ฺบุ pre-tokenization whitespace ุงูุฑ punctuation ูพุฑ ุชูุณู ูพุฑ ูุจู ูุช  ุฏฺฏุฑ ูนูฺฉูุงุฆุฒุฑุฒ ฺฉ ู ุงุณ ฺฉ rules ูุฎุชูู ู ุณฺฉุช ฺบ ูุซุงู ฺฉ ุทูุฑ ูพุฑุ ุงฺฏุฑ ู GPT-2 tokenizer ุงุณุชุนูุงู ฺฉุฑฺบ:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

ุงุณ ฺฉุง output ูฺฏุง:

```python
[('Hello', (0, 5)), (',', (5, 6)), ('ฤhow', (6, 10)), ('ฤare', (10, 14)), ('ฤ', (14, 15)), ('ฤyou', (15, 19)), ('?', (19, 20))]
```

ุงฺบ GPT-2 tokenizer whitespace ุงูุฑ punctuation ุฏูููฺบ ูพุฑ ุชูุณู ฺฉุฑุชุง ุ ูฺฏุฑ ู spaces ฺฉู `ฤ` symbol ฺฉ ุณุงุชฺพ ุฑฺฉฺพุชุง ุ ุฌุณ ุณ ูฺบ decoding ูฺบ ุงุตู spaces ูุงูพุณ ูู ุฌุงุช ฺบ ูุฒุ ุงุณ tokenizer ู double space ฺฉู ูุธุฑุงูุฏุงุฒ ูฺบ ฺฉุง

ุงฺฉ ุขุฎุฑ ูุซุงู ฺฉ ุทูุฑ ูพุฑุ T5 tokenizerุ ุฌู SentencePiece algorithm ูพุฑ ูุจู ุ ฺฉู ุฏฺฉฺพุช ฺบ:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python
[('โHello,', (0, 6)), ('โhow', (7, 10)), ('โare', (11, 14)), ('โyou?', (16, 20))]
```

GPT-2 tokenizer ฺฉ ุทุฑุญุ T5 tokenizer spaces ฺฉู ูุฎุตูุต token (`โ`) ุณ ุธุงุฑ ฺฉุฑุชุง ุ ูฺฏุฑ T5 tokenizer ุตุฑู whitespace ูพุฑ ุชูุณู ฺฉุฑุชุง  ู ฺฉ punctuation ูพุฑ ูุฒุ  ูพู  sentence ฺฉ ุดุฑูุน ูฺบ ุงฺฉ space ุดุงูู ฺฉุฑ ุฏุชุง  (ุฌู `โHello,` ูฺบ ุธุงุฑ ) ุงูุฑ double space ฺฉู ูุธุฑุงูุฏุงุฒ ฺฉุฑุชุง 

ุงุจ ุฌุจฺฉ ู ู ุฏฺฉฺพุง ฺฉ ูุฎุชูู ูนูฺฉูุงุฆุฒุฑุฒ ฺฉุณ ุทุฑุญ text ฺฉู process ฺฉุฑุช ฺบุ ู ุงู ฺฉ underlying ุงูฺฏูุฑุชฺพูุฒ ฺฉู explore ฺฉุฑูุง ุดุฑูุน ฺฉุฑ ุณฺฉุช ฺบ ู ุณุจ ุณ ูพู broadly ุงุณุชุนูุงู ูู ูุงู SentencePiece ฺฉุง ูุฎุชุตุฑ ุฌุงุฆุฒ ูฺบ ฺฏุ ูพฺพุฑ ุงฺฏู ุชู ุญุตูฺบ ูฺบ ู subword tokenization ฺฉ ุชู ุงู ุงูฺฏูุฑุชฺพูุฒ (BPE, WordPieceุ ุงูุฑ Unigram) ูพุฑ ุจุงุช ฺฉุฑฺบ ฺฏ

## SentencePiece[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) ุงฺฉ tokenization ุงูฺฏูุฑุชฺพู  ุฌู text preprocessing ฺฉ ู ุงุณุชุนูุงู ฺฉุง ุฌุงุชุง  ุงูุฑ ุงุณ ุขูพ ฺฉุณ ุจฺพ ูุงฺู ฺฉ ุณุงุชฺพ ุงุณุชุนูุงู ฺฉุฑ ุณฺฉุช ฺบ  text ฺฉู Unicode characters ฺฉ ุณูุณู ฺฉ ุทูุฑ ูพุฑ ุฏฺฉฺพุชุง ุ ุงูุฑ spaces ฺฉู ุงฺฉ ุฎุงุต character `โ` ุณ ุชุจุฏู ฺฉุฑ ุฏุชุง  Unigram ุงูฺฏูุฑุชฺพู (ุฌู [ุจุงุจ 7](/course/chapter7/7) ูฺบ ุงุณุชุนูุงู ูุชุง ) ฺฉ ุณุงุชฺพ ูู ฺฉุฑ  pre-tokenization ฺฉ ุถุฑูุฑุช ฺฉู ุฎุชู ฺฉุฑ ุฏุชุง ุ ุฌู ุงุณ ุฒุจุงููฺบ ฺฉ ู ุจุช ููุฏ  ุฌุงฺบ space character ุงุณุชุนูุงู ูฺบ ูุชุง (ูุซูุงู ฺู ุง ุฌุงูพุงู)

## Algorithm overview[[algorithm-overview]]

ุงฺฏู ุญุตูฺบ ูฺบ ู ุชู ุงู subword tokenization ุงูฺฏูุฑุชฺพูุฒ ูพุฑ ุชูุตู ุณ ุจุงุช ฺฉุฑฺบ ฺฏ: BPE (ุฌู GPT-2 ูุบุฑ ูฺบ ุงุณุชุนูุงู ูุชุง )ุ WordPiece (ุฌุณ BERT ูฺบ) ุงูุฑ Unigram (ุฌู T5 ูุบุฑ ูฺบ ุงุณุชุนูุงู ูุชุง ) ุดุฑูุน ฺฉุฑู ุณ ูพูุ ุงฺบ ุงู ฺฉ ฺฉุงู ฺฉุฑู ฺฉุง ุงฺฉ ูุฎุชุตุฑ overview ุฏุง ฺฏุง  ุงฺฏุฑ  ุงุจฺพ ูุงุถุญ ู ู ุชู ุงฺฏู ุญุตูฺบ ฺฉ ุจุนุฏ ุฏูุจุงุฑ ุงุณ table ฺฉู ุฏฺฉฺพฺบ

Model | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Training | ฺฺพููน vocabulary ุณ ุดุฑูุน ฺฉุฑ ฺฉ tokens merge ฺฉุฑู ฺฉ ููุงุนุฏ ุณฺฉฺพุชุง  | ฺฺพููน vocabulary ุณ ุดุฑูุน ฺฉุฑ ฺฉ tokens merge ฺฉุฑู ฺฉ ููุงุนุฏ ุณฺฉฺพุชุง  | ุจฺ vocabulary ุณ ุดุฑูุน ฺฉุฑ ฺฉ tokens ฺฉู remove ฺฉุฑู ฺฉ ููุงุนุฏ ุณฺฉฺพุชุง 
Training step | ุณุจ ุณ ุฒุงุฏ ุนุงู pair ฺฉู merge ฺฉุฑุชุง  | pair ฺฉ frequency ุงูุฑ ุงููุฑุงุฏ tokens ฺฉ ฺฉู frequency ฺฉู ุชุฑุฌุญ ุฏุชุง  | ูพูุฑ corpus ูพุฑ loss ฺฉู ฺฉู ุณ ฺฉู ฺฉุฑู ฺฉ ู tokens ฺฉู remove ฺฉุฑุชุง 
Learns | Merge rules ุงูุฑ vocabulary | ุตุฑู vocabulary | ุงฺฉ vocabulary ุฌุณ ูฺบ ุฑ token ฺฉุง score ุดุงูู ู
Encoding | ุงฺฉ word ฺฉู characters ูฺบ ุชูุณู ฺฉุฑุชุง  ุงูุฑ training ฺฉ ุฏูุฑุงู merge ฺฉ ฺฏุฆ rules ฺฉู ูุงฺฏู ฺฉุฑุชุง  | ุณุจ ุณ ุทูู subword ุชูุงุด ฺฉุฑุชุง  ุฌู vocabulary ูฺบ ููุฌูุฏ ูุ ูพฺพุฑ ุจุงู ฺฉ ู ุฏุฑุงุง ุฌุงุชุง  | ุณุจ ุณ ุฒุงุฏ ููุฒูฺบ split ุชูุงุด ฺฉุฑุชุง ุ ุฌู training ฺฉ ุฏูุฑุงู ุณฺฉฺพ ฺฏุฆ scores ูพุฑ ูุจู ูุชุง 

ุงุจ ุขุฆฺบ BPE ูฺบ ฺูุจฺบ!