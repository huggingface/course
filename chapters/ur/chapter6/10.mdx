# End-of-chapter quiz[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

Let's test what you learned in this chapter!

### 1. نیا tokenizer کب تربیت دینا چاہیے؟

<Question
	choices={[
		{
			text: "جب آپ کا dataset ایک موجودہ pretrained model کے استعمال کیے گئے dataset سے مماثل ہو، اور آپ ایک نیا model pretrain کرنا چاہتے ہوں",
			explain: "اس صورت میں، وقت اور compute وسائل بچانے کے لیے بہتر یہی ہوگا کہ آپ اسی tokenizer کا استعمال کریں جو pretrained model نے استعمال کیا ہو اور اس model کو fine-tune کریں۔"
		},
		{
			text: "جب آپ کا dataset ایک موجودہ pretrained model کے استعمال کیے گئے dataset سے مماثل ہو، اور آپ اس pretrained model کا استعمال کرتے ہوئے ایک نیا model fine-tune کرنا چاہتے ہوں",
			explain: "Pretrained model سے fine-tuning کے لیے ہمیشہ وہی tokenizer استعمال کریں۔"
		},
		{
			text: "جب آپ کا dataset ایک موجودہ pretrained model کے استعمال کیے گئے dataset سے مختلف ہو، اور آپ ایک نیا model pretrain کرنا چاہتے ہوں",
			explain: "صحیح! اس صورت میں ایک ہی tokenizer استعمال کرنے کا کوئی فائدہ نہیں ہوتا۔",
            correct: true
		},
        {
			text: "جب آپ کا dataset ایک موجودہ pretrained model کے استعمال کیے گئے dataset سے مختلف ہو، لیکن آپ اس pretrained model کا استعمال کرتے ہوئے ایک نیا model fine-tune کرنا چاہتے ہوں",
			explain: "Pretrained model سے fine-tuning کے لیے ہمیشہ وہی tokenizer استعمال کریں۔"
		}
	]}
/>

### 2. `train_new_from_iterator()` استعمال کرتے وقت، texts کی lists کا generator استعمال کرنے کا کیا فائدہ ہے، بجائے texts کی lists کی لسٹ کے؟

<Question
	choices={[
		{
			text: "یہ واحد قسم ہے جو method `train_new_from_iterator()` قبول کرتا ہے۔",
			explain: "texts کی lists کی لسٹ، texts کی lists کا ایک خاص قسم کا generator ہے، لہٰذا method اسے بھی قبول کرے گا۔ دوبارہ کوشش کریں!"
		},
		{
			text: "آپ پورا dataset ایک ساتھ memory میں load ہونے سے بچ جاتے ہیں۔",
			explain: "صحیح! ہر batch کے texts کو iterate کرنے کے بعد memory سے آزاد کر دیا جاتا ہے، اور یہ فائدہ خاص طور پر اس صورت میں واضح ہوتا ہے جب آپ 🤗 Datasets کو texts ذخیرہ کرنے کے لیے استعمال کریں۔",
            correct: true
		},
		{
			text: "اس سے 🤗 Tokenizers لائبریری multiprocessing استعمال کر سکتی ہے۔",
			explain: "نہیں، دونوں صورتوں میں multiprocessing استعمال ہوتی ہے۔"
		},
        {
			text: "تربیت دیا گیا tokenizer بہتر texts generate کرے گا۔",
			explain: "Tokenizer text generate نہیں کرتا — کیا آپ اسے language model سے غلط سمجھ رہے ہیں؟"
		}
	]}
/>

### 3. "فاسٹ" tokenizer استعمال کرنے کے کیا فائدے ہیں؟

<Question
	choices={[
		{
			text: "یہ بیک وقت بہت سے inputs کو batch کرنے پر سلو tokenizer کے مقابلے میں زیادہ تیزی سے process کر سکتا ہے۔",
			explain: "صحیح! Rust میں parallelism کی بدولت، یہ batches پر تیزی سے کام کرتا ہے۔ آپ اور کیا فائدہ سوچ سکتے ہیں؟",
            correct: true
		},
		{
			text: "فاسٹ tokenizers ہمیشہ اپنے سلو counterparts سے تیزی سے tokenize کرتے ہیں۔",
			explain: "ایک فاسٹ tokenizer ایک یا بہت کم texts پر کام کرتے وقت سلو tokenizer سے آہستہ ہو سکتا ہے کیونکہ وہ parallelism استعمال نہیں کر پاتا۔"
		},
		{
			text: "یہ padding اور truncation لاگو کر سکتا ہے۔",
			explain: "سچ ہے، مگر سلو tokenizers بھی یہ کام کرتے ہیں۔"
		},
        {
			text: "اس میں اضافی خصوصیات شامل ہیں جو آپ کو tokens کو ان کے original text کے span سے map کرنے کی اجازت دیتی ہیں۔",
			explain: "واقعی — یہ خصوصیات offset mappings کہلاتی ہیں۔",
            correct: true
		}
	]}
/>

### 4. `token-classification` پائپ لائن ایسے entities کو کیسے ہینڈل کرتی ہے جو کئی tokens پر محیط ہوں؟

<Question
	choices={[
		{
			text: "ایک ہی label والے entities کو ملا کر ایک entity میں تبدیل کر دیا جاتا ہے۔",
			explain: "یہ تھوڑا oversimplify کر رہا ہے۔ دوبارہ کوشش کریں!"
		},
		{
			text: "ایک entity کے آغاز اور اس کے جاری رہنے کے لیے مختلف labels ہوتے ہیں۔",
			explain: "صحیح!",
            correct: true
		},
		{
			text: "کسی word میں، جب تک کہ پہلا token entity کا label رکھتا ہے، پورا word اسی entity کے تحت شمار کیا جاتا ہے۔",
			explain: "یہ entities کو ہینڈل کرنے کی ایک حکمت عملی ہے۔",
            correct: true
		},
        {
			text: "جب کسی token کو کسی entity کا label دیا جائے، تو مسلسل آنے والے دوسرے tokens کو اسی entity کا حصہ سمجھا جاتا ہے، جب تک کہ وہ نئے entity کے آغاز کا نشان نہ دیں۔",
			explain: "یہ entities کو گروپ کرنے کا سب سے عام طریقہ ہے — مگر یہ واحد صحیح جواب نہیں ہے۔",
            correct: true
		}
	]}
/>

### 5. `question-answering` پائپ لائن طویل contexts کو کیسے ہینڈل کرتی ہے؟

<Question
	choices={[
		{
			text: "یہ دراصل طویل context کو model کی زیادہ سے زیادہ لمبائی پر truncate کر دیتی ہے۔",
			explain: "ایک چالاک طریقہ موجود ہے جس سے طویل contexts کو ہینڈل کیا جا سکتا ہے۔ کیا آپ کو یاد ہے وہ کیا ہے؟"
		},
		{
			text: "یہ context کو کئی حصوں میں تقسیم کر کے نتائج کا average لیتی ہے۔",
			explain: "نہیں، results کا average لینا معقول نہیں کیونکہ context کے کچھ حصوں میں جواب موجود نہیں ہوگا۔"
		},
		{
			text: "یہ context کو overlap کے ساتھ کئی حصوں میں تقسیم کرتی ہے اور ہر حصے میں جواب کے لیے maximum score تلاش کرتی ہے۔",
			explain: "یہ صحیح جواب ہے!",
            correct: true
		},
        {
			text: "یہ context کو بغیر overlap کے کئی حصوں میں تقسیم کرتی ہے اور ہر حصے میں جواب کے لیے maximum score تلاش کرتی ہے۔",
			explain: "نہیں، جواب کو دو حصوں میں تقسیم ہونے سے بچانے کے لیے یہ overlap شامل کرتی ہے۔"
		}
	]}
/>

### 6. Normalization کیا ہے؟

<Question
	choices={[
		{
			text: "یہ وہ صفائی ہے جو tokenizer ابتدائی مراحل میں texts پر کرتا ہے۔",
			explain: "یہ درست ہے — مثال کے طور پر، یہ accents یا whitespace کو ہٹا سکتا ہے یا inputs کو lower case میں تبدیل کر سکتا ہے۔",
            correct: true
		},
		{
			text: "یہ ایک data augmentation تکنیک ہے جس میں text کو زیادہ 'normal' بنانے کے لیے rare الفاظ کو ہٹایا جاتا ہے۔",
			explain: "یہ غلط ہے! دوبارہ کوشش کریں۔"
		},
		{
			text: "یہ آخری post-processing مرحلہ ہے جہاں tokenizer special tokens شامل کرتا ہے۔",
			explain: "یہ مرحلہ post-processing کہلاتا ہے۔"
		},
        {
			text: "یہ وہ عمل ہے جب embeddings کو mean 0 اور standard deviation 1 کے ساتھ normalize کیا جاتا ہے، یعنی mean کو subtract کر کے اور std سے تقسیم کر کے۔",
			explain: "یہ عمل عموماً computer vision میں pixel values کو normalize کرنے کے لیے استعمال ہوتا ہے، مگر NLP میں normalization کا یہی مطلب نہیں ہوتا۔"
		}
	]}
/>

### 7. سب ورڈ ٹوکنائزر کے لیے pre-tokenization کیا ہے؟

<Question
	choices={[
		{
			text: "یہ وہ مرحلہ ہے جو tokenization سے پہلے آتا ہے، جہاں data augmentation (جیسے random masking) لاگو کی جاتی ہے۔",
			explain: "نہیں، یہ مرحلہ preprocessing کا حصہ ہے۔"
		},
		{
			text: "یہ وہ مرحلہ ہے جو tokenization سے پہلے آتا ہے، جہاں text پر مطلوبہ صفائی (cleanup) کی جاتی ہے۔",
			explain: "نہیں، یہ normalization کا مرحلہ ہے۔"
		},
		{
			text: "یہ وہ مرحلہ ہے جو tokenizer model کو لاگو کرنے سے پہلے آتا ہے، تاکہ input کو الفاظ میں تقسیم کیا جا سکے۔",
			explain: "یہ درست جواب ہے!",
            correct: true
		},
        {
			text: "یہ وہ مرحلہ ہے جو tokenizer model کو لاگو کرنے سے پہلے آتا ہے، تاکہ input کو tokens میں تقسیم کیا جا سکے۔",
			explain: "نہیں، tokens میں تقسیم کرنا tokenizer model کا کام ہے۔"
		}
	]}
/>

### 8. ان جملوں کو منتخب کریں جو BPE ماڈل کے لیے درست ہیں:

<Question
	choices={[
		{
			text: "BPE ایک سب ورڈ ٹوکنائزیشن الگورتھم ہے جو ایک چھوٹی vocabulary سے شروع ہوتا ہے اور merge rules سیکھتا ہے۔",
			explain: "یہ بالکل درست ہے!",
            correct: true
		},
		{
			text: "BPE ایک سب ورڈ ٹوکنائزیشن الگورتھم ہے جو ایک بڑی vocabulary سے شروع ہوتا ہے اور بتدریج tokens کو حذف کرتا ہے۔",
			explain: "نہیں، یہ طریقہ کار ایک مختلف ٹوکنائزیشن الگورتھم کا ہے۔"
		},
		{
			text: "BPE tokenizers merge rules سب سے زیادہ frequent pair کو merge کر کے سیکھتے ہیں۔",
			explain: "یہ درست ہے!",
            correct: true
		},
        {
			text: "ایک BPE tokenizer وہ pair merge کرتا ہے جو ایک ایسے score کو maximize کرتا ہے جو کم frequent individual حصوں کو ترجیح دیتا ہے۔",
			explain: "نہیں، یہ حکمت عملی کسی اور ٹوکنائزیشن الگورتھم کی ہے۔"
		},
		{
			text: "BPE الفاظ کو subwords میں tokenize کرتا ہے، پہلے انہیں characters میں تقسیم کرتا ہے اور پھر merge rules apply کرتا ہے۔",
			explain: "یہ درست ہے!",
            correct: true
		},
        {
			text: "BPE الفاظ کو subwords میں tokenize کرتا ہے، سب سے طویل subword تلاش کر کے جو vocabulary میں موجود ہو، پھر باقی متن کے لیے یہی عمل دہرایا جاتا ہے۔",
			explain: "نہیں، یہ ایک مختلف ٹوکنائزیشن الگورتھم کا طریقہ ہے۔"
		}
	]}
/>

### 9. ان جملوں کو منتخب کریں جو WordPiece ماڈل کے لیے درست ہیں:

<Question
	choices={[
		{
			text: "WordPiece ایک سب ورڈ ٹوکنائزیشن الگورتھم ہے جو ایک چھوٹی vocabulary سے شروع ہوتا ہے اور merge rules سیکھتا ہے۔",
			explain: "یہ بالکل درست ہے!",
            correct: true
		},
		{
			text: "WordPiece ایک سب ورڈ ٹوکنائزیشن الگورتھم ہے جو ایک بڑی vocabulary سے شروع ہوتا ہے اور بتدریج tokens کو حذف کرتا ہے۔",
			explain: "نہیں، یہ طریقہ کار ایک مختلف ٹوکنائزیشن الگورتھم کا ہے۔"
		},
		{
			text: "WordPiece tokenizers سب سے زیادہ frequent pair کو merge کر کے merge rules سیکھتے ہیں۔",
			explain: "نہیں، یہ حکمت عملی دوسرے ٹوکنائزیشن الگورتھم کی ہے۔"
		},
        {
			text: "ایک WordPiece tokenizer وہ merge rule سیکھتا ہے جو ایسے pair کو merge کرتا ہے جو ایک ایسے score کو maximize کرتا ہے جو کم frequent individual حصوں کے ساتھ frequent pairs کو ترجیح دیتا ہے۔",
			explain: "یہ درست ہے!",
            correct: true
		},
		{
			text: "WordPiece الفاظ کو subwords میں tokenize کرتا ہے، ماڈل کے مطابق سب سے زیادہ ممکنہ segmentation تلاش کر کے۔",
			explain: "نہیں، یہ ایک اور ٹوکنائزیشن الگورتھم کا طریقہ ہے۔"
		},
        {
			text: "WordPiece الفاظ کو subwords میں tokenize کرتا ہے، سب سے طویل subword تلاش کر کے جو vocabulary میں موجود ہو، پھر باقی متن کے لیے یہ عمل دہرایا جاتا ہے۔",
			explain: "ہاں، یہ WordPiece encoding کا طریقہ ہے۔",
            correct: true
		}
	]}
/>

### 10. ان جملوں کو منتخب کریں جو Unigram ماڈل کے لیے درست ہیں:

<Question
	choices={[
		{
			text: "Unigram ایک سب ورڈ ٹوکنائزیشن الگورتھم ہے جو ایک چھوٹی vocabulary سے شروع ہوتا ہے اور merge rules سیکھتا ہے۔",
			explain: "نہیں، یہ طریقہ کار ایک مختلف ٹوکنائزیشن الگورتھم کا ہے۔"
		},
		{
			text: "Unigram ایک سب ورڈ ٹوکنائزیشن الگورتھم ہے جو ایک بڑی vocabulary سے شروع ہوتا ہے اور بتدریج tokens کو حذف کرتا ہے۔",
			explain: "یہ درست ہے!",
            correct: true
		},
		{
			text: "Unigram اپنی vocabulary کو minimize ہونے والے loss کے ذریعے اپڈیٹ کرتا ہے جو پورے corpus پر compute ہوتا ہے۔",
			explain: "یہ درست ہے!",
            correct: true
		},
		{
			text: "Unigram اپنی vocabulary کو سب سے زیادہ frequent subwords کو رکھ کر اپڈیٹ کرتا ہے۔",
			explain: "نہیں، یہ درست نہیں ہے۔"
		},
        {
			text: "Unigram الفاظ کو subwords میں tokenize کرتا ہے کہ وہ ماڈل کے مطابق سب سے زیادہ ممکنہ segmentation تلاش کرے۔",
			explain: "یہ درست ہے!",
            correct: true
		},
		{
			text: "Unigram الفاظ کو subwords میں tokenize کرتا ہے، پہلے انہیں characters میں تقسیم کر کے پھر merge rules apply کرتا ہے۔",
			explain: "نہیں، یہ طریقہ کار ایک مختلف ٹوکنائزیشن الگورتھم کا ہے۔"
		}
	]}
/>

