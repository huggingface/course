# Unigram tokenization[[unigram-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
]} />

Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ø§Ú©Ø«Ø± SentencePiece Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Ú©Û Ø§Ù† tokenization Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù…Ø² Ù…ÛŒÚº Ø³Û’ Ø§ÛŒÚ© ÛÛ’ Ø¬Ùˆ AlBERT, T5, mBART, Big Bird, Ø§ÙˆØ± XLNet Ø¬ÛŒØ³Û’ Ù…Ø§ÚˆÙ„Ø² Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’Û”

<Youtube id="TGZfZVuF9Yc"/>

<Tip>
ğŸ’¡ ÛŒÛ Ø³ÛŒÚ©Ø´Ù† Unigram Ú©Ùˆ ØªÙØµÛŒÙ„ Ø³Û’ cover Ú©Ø±ØªØ§ ÛÛ’ØŒ ÛŒÛØ§Úº ØªÚ© Ú©Û Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ implementation Ø¯Ú©Ú¾Ø§ØªØ§ ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ ØµØ±Ù tokenization Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Ø§ Ø¹Ù…ÙˆÙ…ÛŒ Ø¬Ø§Ø¦Ø²Û Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ Ø¢Ù¾ Ø¢Ø®Ø± Ù…ÛŒÚº jump Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”
</Tip>

## Training algorithm[[training-algorithm]]

BPE Ø§ÙˆØ± WordPiece Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚºØŒ Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ø§Ù„Ù¹Ø§ Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’: ÛŒÛ Ø§ÛŒÚ© Ø¨Ú‘ÛŒ vocabulary Ø³Û’ Ø´Ø±ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’ Ø§ÙˆØ± Ù…Ø·Ù„ÙˆØ¨Û vocabulary size ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ tokens Ú©Ùˆ Ú©Ù… Ú©Ø±ØªØ§ ÛÛ’Û” Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù¾ Ú©Ø¦ÛŒ Ø·Ø±ÛŒÙ‚Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº: Ù…Ø«Ù„Ø§Ù‹ pre-tokenized Ø§Ù„ÙØ§Ø¸ Ù…ÛŒÚº Ø³Û’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent substrings Ú©Ùˆ Ù„ÛŒÙ†Ø§ ÛŒØ§ BPE Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ø§ÛŒÚ© Ø¨Ú‘ÛŒ vocabulary size Ú©Û’ Ø³Ø§ØªÚ¾Û”

ÛØ± Ù…Ø±Ø­Ù„Û’ Ù¾Ø± Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… corpus Ù¾Ø± Ù…ÙˆØ¬ÙˆØ¯ Ù…ÙˆØ¬ÙˆØ¯Û vocabulary Ú©Û’ Ø³Ø§ØªÚ¾ loss compute Ú©Ø±ØªØ§ ÛÛ’Û” Ù¾Ú¾Ø±ØŒ ÛØ± symbol Ú©Û’ Ù„ÛŒÛ’ Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… ÛŒÛ Ø­Ø³Ø§Ø¨ Ù„Ú¯Ø§ØªØ§ ÛÛ’ Ú©Û Ø§Ú¯Ø± ÙˆÛ symbol Ø­Ø°Ù Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ overall loss Ù…ÛŒÚº Ú©ØªÙ†ÛŒ Ø²ÛŒØ§Ø¯ØªÛŒ ÛÙˆÚ¯ÛŒØŒ Ø§ÙˆØ± ÙˆÛ symbols Ø¬Ù† Ø³Û’ loss Ù…ÛŒÚº Ú©Ù… Ø§Ø¶Ø§ÙÛ ÛÙˆØªØ§ ÛÛ’ØŒ Ø§Ù†ÛÛŒÚº Ø­Ø°Ù Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”

ÛŒÛ Ø¹Ù…Ù„ Ú©Ø§ÙÛŒ Ù…ÛÙ†Ú¯Ø§ (computationally expensive) ÛÙˆØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ Ø¹Ù…ÙˆÙ…Ø§Ù‹ ØµØ±Ù \\(p\\) ÙÛŒØµØ¯ (Ø¬ÛØ§Úº \\(p\\) Ø§ÛŒÚ© hyperparameter ÛÛ’ØŒ Ø¹Ù…ÙˆÙ…Ø§Ù‹ 10 ÛŒØ§ 20) symbols Ú©Ùˆ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ø­Ø°Ù Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø¬Ù† Ú©Û’ loss Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ú©Ù… Ø§Ø¶Ø§ÙÛ ÛÙˆØªØ§ ÛÛ’Û” ÛŒÛ Ø¹Ù…Ù„ Ø¯ÙˆØ¨Ø§Ø±Û Ø¯ÛØ±Ø§ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø¬Ø¨ ØªÚ© Ú©Û vocabulary Ù…Ø·Ù„ÙˆØ¨Û size ØªÚ© Ù†Û Ù¾ÛÙ†Ú† Ø¬Ø§Ø¦Û’Û”

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û base characters Ú©Ùˆ Ú©Ø¨Ú¾ÛŒ Ø­Ø°Ù Ù†ÛÛŒÚº Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªØ§Ú©Û ÛØ± word Ú©Ùˆ tokenize Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’Û”

Ø§Ø¨ Ø¨Ú¾ÛŒ ÛŒÛ Ø¨Ø§Øª Ú©Ú†Ú¾ Ù…Ø¨ÛÙ… ÛÛ’: Ø§ØµÙ„ Ø¬Ø² Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ù…ÛŒÚº corpus Ù¾Ø± loss compute Ú©Ø±Ù†Ø§ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø¯ÛŒÚ©Ú¾Ù†Ø§ Ú©Û Ø§Ú¯Ø± Ú©Ø³ÛŒ token Ú©Ùˆ Ø­Ø°Ù Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ loss Ù…ÛŒÚº Ú©ØªÙ†Ø§ ÙØ±Ù‚ Ø¢ØªØ§ ÛÛ’Û” ÛŒÛ Ù…Ø±Ø­Ù„Û Unigram Ù…Ø§ÚˆÙ„ Ú©Û’ tokenization Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ù¾Ø± Ù…Ù†Ø­ØµØ± ÛÛ’ØŒ Ø¬Ø³ Ù¾Ø± ÛÙ… Ø¢Ú¯Û’ ØªÙØµÛŒÙ„ Ø³Û’ Ø¨Ø§Øª Ú©Ø±ÛŒÚº Ú¯Û’Û”

ÛÙ… Ø§Ù¾Ù†ÛŒ Ù¾Ú†Ú¾Ù„ÛŒ Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

Ø§ÙˆØ± Ø§Ø³ Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary Ú©Û’ Ù„ÛŒÛ’ ØªÙ…Ø§Ù… strict substrings Ù„ÛŒÚº Ú¯Û’:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Tokenization algorithm[[tokenization-algorithm]]

Unigram Ù…Ø§ÚˆÙ„ Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ language model ÛÛ’ Ø¬Ùˆ ÛØ± token Ú©Ùˆ Ø§Ø³ Ú©Û’ Ø³Ø§Ø¨Ù‚Û tokens Ø³Û’ Ø¢Ø²Ø§Ø¯ Ø³Ù…Ø¬Ú¾ØªØ§ ÛÛ’Û” ÛŒÛ Ø³Ø¨ Ø³Û’ Ø¢Ø³Ø§Ù† language model ÛÛ’ØŒ Ú©ÛŒÙˆÙ†Ú©Û token X Ú©Û’ probability Ú©Ø§ Ù…Ø·Ù„Ø¨ ØµØ±Ù X Ú©ÛŒ corpus Ù…ÛŒÚº frequency ÛÛ’Û” ÛŒØ¹Ù†ÛŒ Ú©Û Ø§Ú¯Ø± ÛÙ… Unigram language model Ø³Û’ text generate Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº ØªÙˆ ÛÙ…ÛŒØ´Û Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent token Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ ÛÙˆÚ¯ÛŒÛ”

Ú©Ø³ÛŒ token Ú©ÛŒ probability Ø§Ø³ Ú©ÛŒ frequency (corpus Ù…ÛŒÚº Ø¸Ø§ÛØ± ÛÙˆÙ†Û’ Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯) Ú©Ùˆ ØªÙ…Ø§Ù… tokens Ú©ÛŒ frequencies Ú©Û’ Ù…Ø¬Ù…ÙˆØ¹Û’ Ø³Û’ ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ú©Û’ Ø­Ø§ØµÙ„ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ `"ug"` Ù„ÙØ¸ `"hug"`, `"pug"`, Ø§ÙˆØ± `"hugs"` Ù…ÛŒÚº Ø¸Ø§ÛØ± ÛÙˆØªØ§ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ Ø§Ø³ Ú©ÛŒ frequency 20 ÛÛ’Û”

ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± Ù…Ù…Ú©Ù† subword Ú©ÛŒ frequencies Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÛŒÚº:

```
("h", 15), ("u", 36), ("g", 20), ("hu", 15), ("ug", 20), ("p", 17), ("pu", 17), ("n", 16),
("un", 16), ("b", 4), ("bu", 4), ("s", 5), ("hug", 15), ("gs", 5), ("ugs", 5)
```

Ù„ÛÙ°Ø°Ø§ ØªÙ…Ø§Ù… frequencies Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹Û 210 ÛÛ’ØŒ Ø§ÙˆØ± subword `"ug"` Ú©ÛŒ probability ÛÙˆÚ¯ÛŒ 20/210.

<Tip>
âœï¸ **Ø§Ø¨ Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Ø±ÛŒ!** Ú©ÙˆÚˆ Ù„Ú©Ú¾ Ú©Ø± Ø§ÙˆÙ¾Ø± Ø¯ÛŒ Ú¯Ø¦ÛŒ frequencies compute Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ù†ØªØ§Ø¦Ø¬ Ø¯Ø±Ø³Øª ÛÛŒÚº Ø§ÙˆØ± Ù…Ø¬Ù…ÙˆØ¹ÛŒ sum Ø¨Ú¾ÛŒ Ù¹Ú¾ÛŒÚ© ÛÛ’Û”
</Tip>

Ø§Ø¨ØŒ Ú©Ø³ÛŒ word Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… word Ú©Û’ ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û segmentations Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Unigram model Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛØ± segmentation Ú©ÛŒ probability compute Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ú†ÙˆÙ†Ú©Û ØªÙ…Ø§Ù… tokens Ú©Ùˆ Ø¢Ø²Ø§Ø¯ Ø³Ù…Ø¬Ú¾Ø§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ probability ÛØ± token Ú©ÛŒ probability Ú©Ø§ Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ ÛÙˆÚ¯ÛŒÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ word `"pug"` Ú©Û’ Ù„ÛŒÛ’ segmentation `["p", "u", "g"]` Ú©ÛŒ probability:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

Ø¬Ø¨Ú©Û segmentation `["p", "ug"]` Ú©ÛŒ probability ÛÙˆÚ¯ÛŒ:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

Ø§Ø³ Ø·Ø±Ø­ØŒ segmentation Ø¬Ø³ Ù…ÛŒÚº Ú©Ù… tokens ÛÙˆÚº Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± Ø²ÛŒØ§Ø¯Û probability Ø±Ú©Ú¾ØªÛŒ ÛÛ’ØŒ Ø¬Ùˆ Ú©Û ÛÙ…Ø§Ø±ÛŒ ÙØ·Ø±ÛŒ ØªÙˆÙ‚Ø¹ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛÛ’: Ú©Ø³ÛŒ word Ú©Ùˆ Ø¬ØªÙ†Ø§ Ù…Ù…Ú©Ù† ÛÙˆ Ø§ÛŒÚ© ÛÛŒ token Ù…ÛŒÚº split Ú©Ø±Ù†Ø§ Ø¨ÛØªØ± ÛÛ’Û”

Ø§Ø³ word Ú©Û’ Ù„ÛŒÛ’ Unigram model Ú©ÛŒ tokenization ÙˆÛ segmentation ÛÙˆÚ¯ÛŒ Ø¬Ø³ Ú©ÛŒ probability Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ `"pug"` Ú©Û’ Ù„ÛŒÛ’ Ù…Ù…Ú©Ù† segmentation:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

Ù„ÛÙ°Ø°Ø§ `"pug"` Ú©Ùˆ `["p", "ug"]` ÛŒØ§ `["pu", "g"]` tokenize Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŒ Ø§Ø³ Ø¨Ø§Øª Ù¾Ø± Ù…Ù†Ø­ØµØ± Ú©Û Ú©ÙˆÙ† Ø³Ø§ segmentation Ù¾ÛÙ„Û’ Ù…Ù„ØªØ§ ÛÛ’ (Ø¨Ú‘Û’ corpus Ù…ÛŒÚº Ø§ÛŒØ³Û’ equality Ú©Û’ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø´Ø§Ø° Ùˆ Ù†Ø§Ø¯Ø± ÛÙˆØªÛ’ ÛÛŒÚº)Û”

Ø§Ø³ ØµÙˆØ±Øª Ù…ÛŒÚºØŒ ÛŒÛ Ú©Ø§Ù… Ø¢Ø³Ø§Ù† ÛÛ’ Ú©Û ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û segmentations ØªÙ„Ø§Ø´ Ú©ÛŒÛ’ Ø¬Ø§Ø¦ÛŒÚº Ø§ÙˆØ± Ø§Ù† Ú©ÛŒ probabilities compute Ú©ÛŒ Ø¬Ø§Ø¦ÛŒÚºØŒ Ù…Ú¯Ø± Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± ÛŒÛ Ø¹Ù…Ù„ ØªÚ¾ÙˆÚ‘Ø§ Ù…Ø´Ú©Ù„ ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ú©Ù„Ø§Ø³ÛŒÚ©ÛŒ Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… ÛÛ’ Ø¬Ø³Û’ *Viterbi algorithm* Ú©ÛØªÛ’ ÛÛŒÚºÛ” Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø·ÙˆØ± Ù¾Ø±ØŒ ÛÙ… Ø§ÛŒÚ© graph Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº Ø¬Ø³ Ù…ÛŒÚº ÛŒÛ Ø¸Ø§ÛØ± ÛÙˆØªØ§ ÛÛ’ Ú©Û word Ú©Û’ Ù…Ø®ØªÙ„Ù Ø­ØµÙˆÚº Ú©Ùˆ Ú©Ø³ Ø·Ø±Ø­ segmentation Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± ÛØ± branch Ú©Ùˆ Ø§Ø³ subword Ú©ÛŒ probability Ø¯ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û”

Viterbi algorithm Ù¾Ú¾Ø± Ø§Ø³ graph Ù…ÛŒÚº Ø³Û’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û probability ÙˆØ§Ù„Û’ path Ú©Ùˆ ØªÙ„Ø§Ø´ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Ú©Û Ù…Ú©Ù…Ù„ word Ú©ÛŒ best segmentation ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ word `"unhug"` Ú©Û’ Ù„ÛŒÛ’ØŒ ÛØ± position Ù¾Ø± best segmentation Ú©Û’ scores Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

Ø§Ø³ Ø·Ø±Ø­ `"unhug"` tokenize ÛÙˆ Ú©Ø± `["un", "hug"]` Ø¨Ù†ØªØ§ ÛÛ’Û”

<Tip>
âœï¸ **Ø§Ø¨ Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Ø±ÛŒ!** Ù„ÙØ¸ `"huggun"` Ú©ÛŒ tokenization Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ score Ù†Ú©Ø§Ù„ Ú©Ø± Ø¯ÛŒÚ©Ú¾ÛŒÚºÛ”
</Tip>

## Back to training[[back-to-training]]

Ø§Ø¨ Ø¬Ø¨Ú©Û ÛÙ… Ù†Û’ tokenization Ú©Ø§ Ø¹Ù…Ù„ Ø¯ÛŒÚ©Ú¾ Ù„ÛŒØ§ ÛÛ’ØŒ ÛÙ… ØªÚ¾ÙˆÚ‘Ø§ Ø³Ø§ loss Ù¾Ø± Ø¨Ú¾ÛŒ ØºÙˆØ± Ú©Ø±ÛŒÚº Ú¯Û’ Ø¬Ùˆ training Ú©Û’ Ø¯ÙˆØ±Ø§Ù† compute Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù…Ø±Ø­Ù„Û’ Ù¾Ø± ÛŒÛ loss corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± word Ú©Ùˆ tokenize Ú©Ø± Ú©Û’ØŒ Ù…ÙˆØ¬ÙˆØ¯Û vocabulary Ø§ÙˆØ± Unigram model Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± compute Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”

ÛØ± word Ú©Ø§ Ø§ÛŒÚ© score ÛÙˆØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± loss Ø§Ù† scores Ú©Ø§ negative log likelihood ÛÙˆØªØ§ ÛÛ’ â€“ ÛŒØ¹Ù†ÛŒ corpus Ú©Û’ ØªÙ…Ø§Ù… words Ú©Û’ Ù„ÛŒÛ’ \\(-\log(P(word))\\) Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹ÛÛ”

ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙØ§Ø¸ Ø§ÙˆØ± Ø§Ù† Ú©ÛŒ frequencies Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÛŒÚº:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ÛØ± Ù„ÙØ¸ Ú©ÛŒ tokenization Ø§ÙˆØ± Ø§Ù† Ú©Û’ Ù…ØªØ¹Ù„Ù‚Û Ø§Ø³Ú©ÙˆØ± Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÛŒÚº:

```
"hug": ["hug"] (score 0.071428)  (frequency 10)
"pug": ["pu", "g"] (score 0.007710) (frequency 5)
"pun": ["pu", "n"] (score 0.006168) (frequency 12)
"bun": ["bu", "n"] (score 0.001451) (frequency 4)
"hugs": ["hug", "s"] (score 0.001701) (frequency 5)
```

ØªÙˆ loss ÛÙˆÚ¯ÛŒ:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Ø§Ø¨ ÛÙ…ÛŒÚº ÛŒÛ Ø¯ÛŒÚ©Ú¾Ù†Ø§ ÛÙˆÚ¯Ø§ Ú©Û ÛØ± token Ú©Ùˆ Ø­Ø°Ù Ú©Ø±Ù†Û’ Ø³Û’ loss Ù…ÛŒÚº Ú©ØªÙ†Ø§ Ø§Ø¶Ø§ÙÛ ÛÙˆØªØ§ ÛÛ’Û” ÛŒÛ Ú©Ø§Ù… Ú©Ø§ÙÛŒ Ù…Ø­Ù†Øª Ø·Ù„Ø¨ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛÙ… ÛŒÛØ§Úº ØµØ±Ù Ø¯Ùˆ tokens Ú©Û’ Ù„ÛŒÛ’ Ø¯Ú©Ú¾Ø§ØªÛ’ ÛÛŒÚºÛ” Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± Ø¯Ùˆ equivalent tokenizations ÛÙˆÚº (Ø¬ÛŒØ³Ø§ Ú©Û `"pug"` Ú©Û’ Ù„ÛŒÛ’ `["p", "ug"]` Ø§ÙˆØ± `["pu", "g"]` Ø¯ÙˆÙ†ÙˆÚº Ú©Û’ scores Ø¨Ø±Ø§Ø¨Ø± ÛÙˆÚº) ØªÙˆ `"pu"` Ú©Ùˆ Ø­Ø°Ù Ú©Ø±Ù†Û’ Ø³Û’ loss ÙˆÛÛŒ Ø±ÛÛ’ Ú¯Ø§Û”

Ø¯ÙˆØ³Ø±ÛŒ Ø·Ø±ÙØŒ Ø§Ú¯Ø± `"hug"` Ú©Ùˆ Ø­Ø°Ù Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ loss Ø¨Ú‘Ú¾ Ø¬Ø§Ø¦Û’ Ú¯ÛŒ Ú©ÛŒÙˆÙ†Ú©Û `"hug"` Ø§ÙˆØ± `"hugs"` Ú©ÛŒ tokenization Ø¨Ø¯Ù„ Ø¬Ø§Ø¦Û’ Ú¯ÛŒ:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Ø§Ù† ØªØ¨Ø¯ÛŒÙ„ÛŒÙˆÚº Ø³Û’ loss Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ ÛÙˆÚ¯Ø§:

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

Ù„ÛÙ°Ø°Ø§ØŒ ØºØ§Ù„Ø¨Ø§Ù‹ `"pu"` Ú©Ùˆ vocabulary Ø³Û’ ÛÙ¹Ø§ Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŒ Ù…Ú¯Ø± `"hug"` Ú©Ùˆ Ù†ÛÛŒÚºÛ”

## Implementing Unigram[[implementing-unigram]]

Ø§Ø¨ ÛÙ… Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©ÛŒ Ù¾ÙˆØ±ÛŒ implementation Ú©Ùˆ code Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºÛ” Ø¬ÛŒØ³Ø§ Ú©Û BPE Ø§ÙˆØ± WordPiece Ú©Û’ Ù„ÛŒÛ’ØŒ ÛŒÛ implementation Ù…Ø¤Ø«Ø± Ù†ÛÛŒÚº ÛÛ’ (Ø¨Ù„Ú©Û Ø¨ÛØª Ø³Ø³Øª ÛÛ’) Ù…Ú¯Ø± ÛÙ…Ø§Ø±Ø§ Ù…Ù‚ØµØ¯ ØµØ±Ù Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Ùˆ Ø¨ÛØªØ± Ø³Ù…Ø¬Ú¾Ù†Ø§ ÛÛ’Û”

ÛÙ… ÙˆÛÛŒ corpus Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Ø§Ø³ Ø¨Ø§Ø± ÛÙ… `xlnet-base-cased` Ù…Ø§ÚˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

BPE Ø§ÙˆØ± WordPiece Ú©ÛŒ Ø·Ø±Ø­ØŒ Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± word Ú©ÛŒ occurrences count Ú©Ø±ÛŒÚº Ú¯Û’:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Ø§Ø¨ ÛÙ…ÛŒÚº Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary initialize Ú©Ø±Ù†ÛŒ ÛÛ’ Ø¬Ùˆ Ø§ÛŒÚ© Ø¨Ú‘ÛŒ vocabulary ÛÙˆØŒ Ù…Ú¯Ø± ÛÙ… Ø¨Ø¹Ø¯ Ù…ÛŒÚº Ø§Ø³ Ø³Û’ Ø³Ø¨ Ø³Û’ common substrings Ø±Ú©Ú¾ÛŒÚº Ú¯Û’Û” ÛÙ… ØªÙ…Ø§Ù… Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø­Ø±ÙˆÙ (base characters) Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ù„ÛŒÚ©Ù† Ø¨Ú‘Û’ substrings Ú©Ùˆ Ø§Ù† Ú©ÛŒ frequency Ú©Û’ Ø­Ø³Ø§Ø¨ Ø³Û’ sort Ú©Ø±ÛŒÚº Ú¯Û’:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Ù„Ù…Ø¨Ø§Ø¦ÛŒ 2 ÛŒØ§ Ø§Ø³ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ú©Û’ subwords
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Frequency Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ subwords Ú©Ùˆ sort Ú©Ø±ÛŒÚº
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

ÛÙ… characters Ø§ÙˆØ± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent subwords Ú©Ùˆ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© initial vocabulary ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº Ú¯Û’ØŒ ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û ÛÙ… size 300 Ú†Ø§ÛØªÛ’ ÛÛŒÚº:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>
ğŸ’¡ SentencePiece Ø§ÛŒÚ© Ø²ÛŒØ§Ø¯Û Ù…ÙˆØ«Ø± Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Enhanced Suffix Array (ESA) Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ ØªØ§Ú©Û Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary ØªÛŒØ§Ø± Ú©ÛŒ Ø¬Ø§ Ø³Ú©Û’Û”
</Tip>

Ø§Ø¨ØŒ ØªÙ…Ø§Ù… frequencies Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹Û compute Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ù†ÛÛŒÚº probabilities Ù…ÛŒÚº convert Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ logarithm Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº (Ú©ÛŒÙˆÙ†Ú©Û Ú†Ú¾ÙˆÙ¹Û’ Ù†Ù…Ø¨Ø±Ø² Ú©Ø§ Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¨Ø¬Ø§Ø¦Û’ logarithms Ú©Ùˆ Ø¬Ù…Ø¹ Ú©Ø±Ù†Ø§ numerically Ù…Ø³ØªØ­Ú©Ù… ÛÙˆØªØ§ ÛÛ’):

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Ø§Ø¨ word Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Viterbi algorithm Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºÛ” ÛŒÛ algorithm word Ú©Û’ ÛØ± substring Ú©Û’ Ù„ÛŒÛ’ best segmentation compute Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ø³Û’ ÛÙ… `best_segmentations` Ù…ÛŒÚº store Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛØ± position (0 Ø³Û’ word Ú©ÛŒ length ØªÚ©) Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© dictionary ÛÙˆÚ¯ÛŒ Ø¬Ø³ Ù…ÛŒÚº Ø¯Ùˆ keys ÛÙˆÚº Ú¯Û’: Ø¢Ø®Ø±ÛŒ token Ú©ÛŒ Ø´Ø±ÙˆØ¹Ø§Øª Ú©Ø§ index Ø§ÙˆØ± best segmentation Ú©Ø§ scoreÛ” Ø§Ø³ index Ú©ÛŒ Ù…Ø¯Ø¯ Ø³Û’ ÛÙ… Ù¾ÙˆØ±Ø§ segmentation ÙˆØ§Ù¾Ø³ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’Û”

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}
    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        return ["<unk>"], None
    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

ÛÙ… Ù¾ÛÙ„Û’ ÛÛŒ Ø§Ù¾Ù†Û’ initial model Ú©Ùˆ Ú©Ú†Ú¾ Ø§Ù„ÙØ§Ø¸ Ù¾Ø± Ø¢Ø²Ù…Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Ø§Ø¨ corpus Ù¾Ø± model Ú©Ø§ loss compute Ú©Ø±ÛŒÚº:

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±:

```python
compute_loss(model)
```

```python
413.10377642940875
```

Ø§Ø¨ ÛØ± token Ú©Ùˆ Ø­Ø°Ù Ú©Ø±Ù†Û’ Ø³Û’ loss Ù…ÛŒÚº Ú©ÛŒØ§ ÙØ±Ù‚ Ø¢ØªØ§ ÛÛ’ØŒ ÛŒÛ compute Ú©Ø±ÛŒÚº:

```python
import copy

def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

ÛÙ… Ø§Ø³Û’ Ú©Ø³ÛŒ Ø¯ÛŒÛ’ Ú¯Ø¦Û’ **token** Ù¾Ø± Ø¢Ø²Ù…Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

Ú†ÙˆÙ†Ú©Û `"ll"` Ú©Ùˆ `"Hopefully"` Ú©ÛŒ tokenization Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø§Ø³Û’ ÛÙ¹Ø§Ù†Û’ Ø³Û’ Ø´Ø§ÛŒØ¯ ÛÙ…ÛŒÚº `"l"` Ú©Ùˆ Ø¯Ùˆ Ø¨Ø§Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ù¾Ú‘Û’ØŒ ÛÙ… ØªÙˆÙ‚Ø¹ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø§Ø³ Ú©Ø§ **Ù…Ø«Ø¨Øª Ù†Ù‚ØµØ§Ù† (positive loss)** ÛÙˆÚ¯Ø§Û”  

Ø¯ÙˆØ³Ø±ÛŒ Ø·Ø±ÙØŒ `"his"` ØµØ±Ù Ù„ÙØ¸ `"This"` Ú©Û’ Ø§Ù†Ø¯Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Ø®ÙˆØ¯ Ø§Ù¾Ù†ÛŒ Ù…Ú©Ù…Ù„ Ø´Ú©Ù„ Ù…ÛŒÚº tokenized ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛÙ… ØªÙˆÙ‚Ø¹ Ú©Ø±ØªÛ’ ÛÛŒÚº Ú©Û Ø§Ø³ Ú©Ø§ **Ù†Ù‚ØµØ§Ù† ØµÙØ± (zero loss)** ÛÙˆÚ¯Ø§Û”  

ÛŒÛØ§Úº Ù†ØªØ§Ø¦Ø¬ Ø¯ÛŒÛ’ Ú¯Ø¦Û’ ÛÛŒÚº:

```python
6.376412403623874
0.0
```

<Tip>
ğŸ’¡ ÛŒÛ Ø·Ø±ÛŒÙ‚Û Ø¨ÛØª ØºÛŒØ± Ù…Ø¤Ø«Ø± ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ SentencePiece loss Ú©Ùˆ approximate Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø§Ù†Ø¯Ø§Ø²Û Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’Û”
</Tip>

Ø§Ø¨ Ø¢Ø®Ø±ÛŒ Ù…Ø±Ø­Ù„Û ÛŒÛ ÛÛ’ Ú©Û special tokens Ú©Ùˆ vocabulary Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚºØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± loop Ú©Ø±ÛŒÚº Ø¬Ø¨ ØªÚ© Ú©Û vocabulary Ú©Ùˆ Ù…Ø·Ù„ÙˆØ¨Û size ØªÚ© prune Ù†Û Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])
    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Ø§Ø¨ text Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛÙ„Û’ pre-tokenization Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ù¾Ú¾Ø± `encode_word()` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])

tokenize("This is the Hugging Face course.", model)
```

```python
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

ÛŒÛÛŒ ÛÛ’ Unigram Ú©Ø§ Ø¬Ø§Ø¦Ø²Û! Ø§Ø¨ Ø§Ú¯Ù„Û’ Ø­ØµÛ’ Ù…ÛŒÚº ÛÙ… ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Û’ building blocks Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ØŒ Ø§ÙˆØ± Ú©ÛŒØ³Û’ Ø§Ù¾Ù†Ø§ tokenizer ØªÛŒØ§Ø± Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§Û”