# Unigram tokenization[[unigram-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
]} />


Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ù¾Ú†Ú¾Ù„Û’ Ø­ØµÙˆÚº Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ØŒ **tokenization** Ú©Ø¦ÛŒ Ù…Ø±Ø§Ø­Ù„ Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÙˆØªÛŒ ÛÛ’:

- **Normalization** (Ù…ØªÙ† Ú©ÛŒ ØµÙØ§Ø¦ÛŒ Ø¬ÛŒØ³Û’ Ú©Û Ø§Ø¶Ø§ÙÛŒ Ø§Ø³Ù¾ÛŒØ³ ÛŒØ§ Ø§ÛŒÚ©Ø³Ù†Ù¹ ÛÙ¹Ø§Ù†Ø§ØŒ ÛŒÙˆÙ†ÛŒÚ©ÙˆÚˆ Ù†Ø§Ø±Ù…Ù„Ø§Ø¦Ø²ÛŒØ´Ù† ÙˆØºÛŒØ±Û)  
- **Pre-tokenization** (Ù…ØªÙ† Ú©Ùˆ Ø§Ù„ÙØ§Ø¸ Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ù†Ø§)  
- **Ù…Ø§ÚˆÙ„ Ù¾Ø± Ú†Ù„Ø§Ù†Ø§** (Ù¾ÛÙ„Û’ Ø³Û’ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Û Ø§Ù„ÙØ§Ø¸ Ú©Ùˆ **tokens** Ú©ÛŒ ØªØ±ØªÛŒØ¨ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§)  
- **Post-processing** (Ø§Ø³Ù¾ÛŒØ´Ù„ **tokens** Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Ø§ØŒ **attention mask** Ø§ÙˆØ± **token type IDs** Ø¨Ù†Ø§Ù†Ø§)  

ÛŒØ§Ø¯ Ø¯ÛØ§Ù†ÛŒ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛŒÛØ§Úº **tokenization pipeline** Ú©Ø§ Ø§ÛŒÚ© Ø§ÙˆØ± Ø¬Ø§Ø¦Ø²Û Ù„ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’:  

<div class="flex justify-center">  
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="Tokenization Pipeline">  
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="Tokenization Pipeline">  
</div>  

**ğŸ¤— Tokenizers** Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§Ù† ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØªÙ„Ù Ø§Ø®ØªÛŒØ§Ø±Ø§Øª ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ø¬Ù†ÛÛŒÚº Ø¢Ù¾ Ø§Ù¾Ù†ÛŒ Ø¶Ø±ÙˆØ±Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ØªØ±ØªÛŒØ¨ Ø¯Û’ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚºØŒ ÛÙ… **Ù†Ø¦Û’ Ø³Ø±Û’ Ø³Û’** Ø§ÛŒÚ© **tokenizer** Ø¨Ù†Ø§Ù†Ø§ Ø³ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ØŒ Ø¨Ø¬Ø§Ø¦Û’ Ø§Ø³ Ú©Û’ Ú©Û Ú©Ø³ÛŒ Ù¾Ø±Ø§Ù†Û’ **tokenizer** Ú©Ùˆ Ø§Ù¾ÚˆÛŒÙ¹ Ú©Ø±ÛŒÚºØŒ Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ **[Ø³ÛŒÚ©Ø´Ù† 2](/course/chapter6/2)** Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ¾Ø§Û” Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ØŒ Ø¢Ù¾ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù‚Ø³Ù… Ú©Ø§ **tokenizer** Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ ÛÙˆÚº Ú¯Û’!

<Youtube id="MR8tZm5ViWU"/>

**ğŸ¤— Tokenizers** Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø§ÛŒÚ© Ù…Ø±Ú©Ø²ÛŒ `Tokenizer` Ú©Ù„Ø§Ø³ Ú©Û’ Ú¯Ø±Ø¯ Ø¨Ù†ÛŒ ÛÛ’ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ù…Ø®ØªÙ„Ù Ù…Ø±Ø§Ø­Ù„ Ú©Ùˆ Ù…Ø®ØªÙ„Ù **Ø°ÛŒÙ„ÛŒ Ù…Ø§ÚˆÛŒÙˆÙ„Ø²** Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’:  

### **Ø§ÛÙ… Ø°ÛŒÙ„ÛŒ Ù…Ø§ÚˆÛŒÙˆÙ„Ø²**:
- **`normalizers`** Ù…ÛŒÚº ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û Ø§Ù‚Ø³Ø§Ù… Ú©Û’ `Normalizer` Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ù†ÛÛŒÚº Ø¢Ù¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ([Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª](https://huggingface.co/docs/tokenizers/api/normalizers))Û”  
- **`pre_tokenizers`** Ù…ÛŒÚº ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û Ø§Ù‚Ø³Ø§Ù… Ú©Û’ `PreTokenizer` Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ù†ÛÛŒÚº Ø¢Ù¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ([Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª](https://huggingface.co/docs/tokenizers/api/pre-tokenizers))Û”  
- **`models`** Ù…ÛŒÚº Ù…Ø®ØªÙ„Ù Ø§Ù‚Ø³Ø§Ù… Ú©Û’ `Model` Ø´Ø§Ù…Ù„ ÛÛŒÚºØŒ Ø¬ÛŒØ³Û’ Ú©Û `BPE`, `WordPiece`, Ø§ÙˆØ± `Unigram` ([Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª](https://huggingface.co/docs/tokenizers/api/models))Û”  
- **`trainers`** Ù…ÛŒÚº Ù…Ø®ØªÙ„Ù Ø§Ù‚Ø³Ø§Ù… Ú©Û’ `Trainer` Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ù†ÛÛŒÚº Ø¢Ù¾ Ø§Ù¾Ù†Û’ Ú©Ø§Ø±Ù¾Ø³ Ù¾Ø± Ù…Ø§ÚˆÙ„ Ù¹Ø±ÛŒÙ† Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº (ÛØ± Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ù‚Ø³Ù… Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø¹Ù„ÛŒØ­Ø¯Û `Trainer` ÛÛ’Ø› [Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª](https://huggingface.co/docs/tokenizers/api/trainers))Û”  
- **`post_processors`** Ù…ÛŒÚº Ù…Ø®ØªÙ„Ù Ø§Ù‚Ø³Ø§Ù… Ú©Û’ `PostProcessor` Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ù†ÛÛŒÚº Ø¢Ù¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ([Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª](https://huggingface.co/docs/tokenizers/api/post-processors))Û”  
- **`decoders`** Ù…ÛŒÚº Ù…Ø®ØªÙ„Ù Ø§Ù‚Ø³Ø§Ù… Ú©Û’ `Decoder` Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ù†ÛÛŒÚº Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Û’ Ù†ØªØ§Ø¦Ø¬ Ú©Ùˆ ÚˆÛŒ Ú©ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ ([Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª](https://huggingface.co/docs/tokenizers/components#decoders))Û”  

ØªÙ…Ø§Ù… **Ø§Ø¬Ø²Ø§Ø¡** Ú©ÛŒ Ù…Ú©Ù…Ù„ ÙÛØ±Ø³Øª [ÛŒÛØ§Úº](https://huggingface.co/docs/tokenizers/components) Ø¯Ø³ØªÛŒØ§Ø¨ ÛÛ’Û”  

## **Ú©Ø§Ø±Ù¾Ø³ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Ø§**  

Ø§Ù¾Ù†Ø§ Ù†ÛŒØ§ **tokenizer** Ù¹Ø±ÛŒÙ† Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… Ø§ÛŒÚ© **Ú†Ú¾ÙˆÙ¹Û’ Ú©Ø§Ø±Ù¾Ø³** Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ (ØªØ§Ú©Û Ù…Ø«Ø§Ù„ÛŒÚº ØªÛŒØ²ÛŒ Ø³Û’ Ú†Ù„ Ø³Ú©ÛŒÚº)Û”  
Ú©Ø§Ø±Ù¾Ø³ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª ÙˆÛÛŒ ÛÛŒÚº Ø¬Ùˆ ÛÙ… Ù†Û’ **[Ø§Ø³ Ø¨Ø§Ø¨ Ú©Û’ Ø¢ØºØ§Ø² Ù…ÛŒÚº](/course/chapter6/2)** Ú©ÛŒÛ’ ØªÚ¾Û’ØŒ Ù„ÛŒÚ©Ù† Ø§Ø³ Ø¨Ø§Ø± ÛÙ… **[WikiText-2](https://huggingface.co/datasets/wikitext)** ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”


```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

The function `get_training_corpus()` Ø§ÛŒÚ© Ø¬Ù†Ø±ÛŒÙ¹Ø± ÛÛ’ Ø¬Ùˆ 1,000 Ù…ØªÙ† Ú©Û’ Ø¨ÛŒÚ†Ø² ÙØ±Ø§ÛÙ… Ú©Ø±Û’ Ú¯Ø§ØŒ Ø¬Ù†ÛÛŒÚº ÛÙ… tokenizer Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”

ğŸ¤— Tokenizers Ú©Ùˆ text files Ù¾Ø± Ø¨Ú¾ÛŒ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª ØªØ±Ø¨ÛŒØª Ø¯ÛŒ Ø¬Ø§ Ø³Ú©ØªÛŒ ÛÛ’Û” ÛŒÛØ§Úº ÛŒÛ Ø¯Ú©Ú¾Ø§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ Ú©Û ÛÙ… WikiText-2 Ú©Û’ ØªÙ…Ø§Ù… texts/inputs Ú©Ùˆ Ø§ÛŒÚ© text file Ù…ÛŒÚº Ú©ÛŒØ³Û’ generate Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ØªØ§Ú©Û Ø§Ø³Û’ Ù…Ù‚Ø§Ù…ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```
Next we'll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block. That will give us an example of each of the three main tokenization algorithms: WordPiece, BPE, and Unigram. Let's start with BERT!

Ø¢Ú¯Û’ ÛÙ… Ø¢Ù¾ Ú©Ùˆ Ø¨ØªØ§Ø¦ÛŒÚº Ú¯Û’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ Ø¢Ù¾ Ø§Ù¾Ù†ÛŒ Ù…Ø±Ø¶ÛŒ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ BERTØŒ GPT-2ØŒ Ø§ÙˆØ± XLNet Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©Ùˆ Ø¨Ù„Ø§Ú© Ø¨Ø§Ø¦ÛŒ Ø¨Ù„Ø§Ú© ØªÛŒØ§Ø± Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ø³Û’ ÛÙ…ÛŒÚº ØªÛŒÙ† Ø§ÛÙ… Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù…Ø² â€” WordPiece, BPE, Ø§ÙˆØ± Unigram â€” Ú©ÛŒ Ù…Ø«Ø§Ù„ Ù…Ù„Û’ Ú¯ÛŒÛ” Ø¢Ø¦ÛŒÛ’ BERT Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÛŒÚº!

## Building a WordPiece tokenizer from scratch[[building-a-wordpiece-tokenizer-from-scratch]]

ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒÚ© Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø§ÛŒÚ© `Tokenizer` object instantiate Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© `model` Ø´Ø§Ù…Ù„ ÛÙˆØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø§Ø³ Ú©Û’ `normalizer`, `pre_tokenizer`, `post_processor`, Ø§ÙˆØ± `decoder` attributes Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ù…Ø±Ø¶ÛŒ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø³ÛŒÙ¹ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

Ø§Ø³ Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… WordPiece model Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒÚ© `Tokenizer` Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

We have to specify the `unk_token` so the model knows what to return when it encounters characters it hasn't seen before. Other arguments we can set here include the `vocab` of our model (we're going to train the model, so we don't need to set this) and `max_input_chars_per_word`, which specifies a maximum length for each word (words longer than the value passed will be split).

ÛÙ…ÛŒÚº `unk_token` Ú©Ùˆ Ù…Ø®ØµÙˆØµ Ú©Ø±Ù†Ø§ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ ØªØ§Ú©Û Ù…Ø§ÚˆÙ„ Ú©Ùˆ ÛŒÛ Ù…Ø¹Ù„ÙˆÙ… ÛÙˆ Ú©Û Ø¬Ø¨ Ø§Ø³Û’ Ø§ÛŒØ³Û’ Ø­Ø±ÙˆÙ Ù…Ù„ÛŒÚº Ø¬Ùˆ Ù¾ÛÙ„Û’ Ú©Ø¨Ú¾ÛŒ Ù†ÛÛŒÚº Ø¯ÛŒÚ©Ú¾Û’ Ú¯Ø¦Û’ØŒ ØªÙˆ Ú©ÛŒØ§ ÙˆØ§Ù¾Ø³ Ú©Ø±Ù†Ø§ ÛÛ’Û” ÛŒÛØ§Úº Ø¯ÛŒÚ¯Ø± Ø¢Ø±Ú¯ÙˆÙ…Ù†Ù¹Ø³ Ø¨Ú¾ÛŒ Ø³ÛŒÙ¹ Ú©ÛŒÛ’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’ Ú©Û ÛÙ…Ø§Ø±Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ `vocab` (Ú†ÙˆÙ†Ú©Û ÛÙ… Ù…Ø§ÚˆÙ„ Ú©Ùˆ ØªØ±Ø¨ÛŒØª Ø¯ÛŒÙ†Û’ Ø¬Ø§ Ø±ÛÛ’ ÛÛŒÚºØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø§Ø³Û’ Ø³ÛŒÙ¹ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº) Ø§ÙˆØ± `max_input_chars_per_word`ØŒ Ø¬Ùˆ ÛØ± Ù„ÙØ¸ Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ù…ØªØ¹ÛŒÙ† Ú©Ø±ØªØ§ ÛÛ’ (Ø§Ú¯Ø± Ù„ÙØ¸ Ø§Ø³ Ù‚Ø¯Ø± Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§ ÛÙˆ ØªÙˆ Ø§Ø³Û’ ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§)Û”

The first step of tokenization is normalization, so let's begin with that.

Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Ø§ Ù¾ÛÙ„Ø§ Ù…Ø±Ø­Ù„Û normalization ÛÛ’ØŒ ØªÙˆ Ø¢Ø¦ÛŒÚº Ø§Ø³ Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

Since BERT is widely used, there is a `BertNormalizer` with the classic options we can set for BERT: `lowercase` and `strip_accents`, which are self-explanatory; `clean_text` to remove all control characters and replace repeating spaces with a single one; and `handle_chinese_chars`, which places spaces around Chinese characters. To replicate the `bert-base-uncased` tokenizer, we can just set this normalizer:

Ú†ÙˆÙ†Ú©Û BERT ÙˆØ³ÛŒØ¹ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØªØ§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø§ÛŒÚ© `BertNormalizer` Ø¯Ø³ØªÛŒØ§Ø¨ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº ÙˆÛ Ú©Ù„Ø§Ø³ÛŒÚ©ÛŒ Ø§Ø®ØªÛŒØ§Ø±Ø§Øª Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ùˆ BERT Ú©Û’ Ù„ÛŒÛ’ Ø³ÛŒÙ¹ Ú©ÛŒÛ’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº: `lowercase` Ø§ÙˆØ± `strip_accents` (Ø¬Ùˆ Ø¨Ø°Ø§Øª Ø®ÙˆØ¯ ÙˆØ§Ø¶Ø­ ÛÛŒÚº)Ø› `clean_text` Ø¬Ùˆ ØªÙ…Ø§Ù… Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ø±ÛŒÚ©Ù¹Ø±Ø² Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø§ÙˆØ± Ø¨Ø§Ø± Ø¨Ø§Ø± Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ Ø¬Ú¯ÛÙˆÚº Ú©Ùˆ Ø§ÛŒÚ© Ø¬Ú¯Û Ù…ÛŒÚº Ø¨Ø¯Ù„ Ø¯ÛŒØªØ§ ÛÛ’Ø› Ø§ÙˆØ± `handle_chinese_chars`ØŒ Ø¬Ùˆ Ú†ÛŒÙ†ÛŒ Ø­Ø±ÙˆÙ Ú©Û’ Ø§Ø±Ø¯Ú¯Ø±Ø¯ space Ù„Ú¯Ø§ Ø¯ÛŒØªØ§ ÛÛ’Û” `bert-base-uncased` tokenizer Ú©ÛŒ Ù†Ù‚Ù„ Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… Ø¨Ø³ ÛŒÛ normalizer Ø³ÛŒÙ¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

Generally speaking, however, when building a new tokenizer you won't have access to such a handy normalizer already implemented in the ğŸ¤— Tokenizers library -- so let's see how to create the BERT normalizer by hand. The library provides a `Lowercase` normalizer and a `StripAccents` normalizer, and you can compose several normalizers using a `Sequence`:

Ø¹Ù…ÙˆÙ…ÛŒ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø¬Ø¨ ÛÙ… Ø§ÛŒÚ© Ù†ÛŒØ§ tokenizer ØªÛŒØ§Ø± Ú©Ø±ØªÛ’ ÛÛŒÚº ØªÙˆ Ø¢Ù¾ Ú©Ùˆ ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ù…ÛŒÚº Ù¾ÛÙ„Û’ Ø³Û’ Ù…ÙˆØ¬ÙˆØ¯ Ø§ØªÙ†Ø§ Ù…ÙÛŒØ¯ normalizer Ø¯Ø³ØªÛŒØ§Ø¨ Ù†ÛÛŒÚº ÛÙˆØªØ§ â€” ØªÙˆ Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û ÛÙ… BERT normalizer Ú©Ùˆ Ø¯Ø³ØªÛŒ Ø·ÙˆØ± Ù¾Ø± Ú©ÛŒØ³Û’ Ø¨Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§ÛŒÚ© `Lowercase` normalizer Ø§ÙˆØ± Ø§ÛŒÚ© `StripAccents` normalizer ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛŒ ÛÛ’ØŒ Ø§ÙˆØ± Ø¢Ù¾ `Sequence` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©Ø¦ÛŒ normalizers Ú©Ùˆ ÛŒÚ©Ø¬Ø§ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

We're also using an `NFD` Unicode normalizer, as otherwise the `StripAccents` normalizer won't properly recognize the accented characters and thus won't strip them out.

ÛÙ… Ø§ÛŒÚ© `NFD` Unicode normalizer Ø¨Ú¾ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŒ Ú©ÛŒÙˆÙ†Ú©Û ÙˆØ±Ù†Û `StripAccents` normalizer accented Ø­Ø±ÙˆÙ Ú©Ùˆ Ø¯Ø±Ø³Øª Ø·ÙˆØ± Ù¾Ø± Ù¾ÛÚ†Ø§Ù† Ù†ÛÛŒÚº Ù¾Ø§Ø¦Û’ Ú¯Ø§ Ø§ÙˆØ± Ø§Ù†ÛÛŒÚº ÛÙ¹Ø§ Ù†ÛÛŒÚº Ù¾Ø§Ø¦Û’ Ú¯Ø§Û”

As we've seen before, we can use the `normalize_str()` method of the `normalizer` to check out the effects it has on a given text:

Ø¬ÛŒØ³Ø§ Ú©Û Ù¾ÛÙ„Û’ Ø¯ÛŒÚ©Ú¾Ø§ Ú¯ÛŒØ§ØŒ ÛÙ… `normalizer` Ú©Û’ `normalize_str()` method Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ Ú©Ø³ÛŒ Ù…ØªÙ† Ù¾Ø± Ú©ÛŒØ§ Ø§Ø«Ø± ÚˆØ§Ù„ Ø±ÛØ§ ÛÛ’:

```python
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python
hello how are u?
```
<Tip>

**Ù…Ø²ÛŒØ¯ Ø¢Ú¯Û’ Ø¨Ú‘Ú¾ÛŒÚº** Ø§Ú¯Ø± Ø¢Ù¾ Ù¾Ú†Ú¾Ù„Û’ normalizers Ú©Û’ Ø¯ÙˆÙ†ÙˆÚº ÙˆØ±Ú˜Ù†Ø² Ú©Ùˆ Ø§ÛŒØ³Û’ string Ù¾Ø± test Ú©Ø±ÛŒÚº Ø¬Ø³ Ù…ÛŒÚº unicode character `u"\u0085"` Ø´Ø§Ù…Ù„ ÛÙˆØŒ ØªÙˆ Ø¢Ù¾ ÛŒÙ‚ÛŒÙ†ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û ÛŒÛ Ø¯ÙˆÙ†ÙˆÚº normalizers Ø¹ÛŒÙ† Ù…Ø·Ø§Ø¨Ù‚ Ù†ÛÛŒÚº ÛÛŒÚºÛ”  
`normalizers.Sequence` ÙˆØ§Ù„Û’ ÙˆØ±Ú˜Ù† Ú©Ùˆ Ø²ÛŒØ§Ø¯Û Ù¾ÛŒÚ†ÛŒØ¯Û Ú©ÛŒÛ’ Ø¨ØºÛŒØ± ÛÙ… Ù†Û’ Regex replacements Ø´Ø§Ù…Ù„ Ù†ÛÛŒÚº Ú©ÛŒÛ’ Ø¬Ùˆ `BertNormalizer` Ú©Ùˆ `clean_text=True` Ú©Û’ Ø³Ø§ØªÚ¾ Ø¯Ø±Ú©Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº â€” Ø¬Ùˆ Ú©Û ÚˆÛŒÙØ§Ù„Ù¹ Ø¨Ø±ØªØ§Ø¤ ÛÛ’Û” Ù„ÛŒÚ©Ù† ÙÚ©Ø± Ù†Û Ú©Ø±ÛŒÚº: Ø¢Ù¾ Ø¯Ùˆ `normalizers.Replace` Ú©Ùˆ normalizers sequence Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø± Ú©Û’ Ø¨ØºÛŒØ± `BertNormalizer` Ú©Û’ Ø¨Ø§Ù„Ú©Ù„ ÙˆÛÛŒ normalization Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

</Tip>

Next is the pre-tokenization step. Again, there is a prebuilt `BertPreTokenizer` that we can use:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

ÛŒØ§ ÛÙ… Ø§Ø³Û’ zero Ø³Û’ Ø¨Ù†Ø§ Ø¨Ú¾ÛŒ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û `Whitespace` pre-tokenizer whitespace Ø§ÙˆØ± Ø§ÙÙ† ØªÙ…Ø§Ù… characters Ú©Ùˆ split Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ lettersØŒ digitsØŒ ÛŒØ§ underscore Ù†ÛÛŒÚº ÛÛŒÚºØŒ Ø§Ø³ Ù„ÛŒÛ’ ÛŒÛ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø·ÙˆØ± Ù¾Ø± whitespace Ø§ÙˆØ± punctuation Ø¯ÙˆÙ†ÙˆÚº Ù¾Ø± ØªÙ‚Ø³ÛŒÙ… Ú©Ø±ØªØ§ ÛÛ’:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

Ø§Ú¯Ø± Ø¢Ù¾ ØµØ±Ù whitespace Ù¾Ø± ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ `WhitespaceSplit` pre-tokenizer Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛÛŒÛ’:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

Ø¬ÛŒØ³Û’ Ú©Û normalizers Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙˆØªØ§ ÛÛ’ØŒ Ø¢Ù¾ `Sequence` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…ØªØ¹Ø¯Ø¯ pre-tokenizers Ú©Ùˆ Ù…Ù„Ø§ Ø¨Ú¾ÛŒ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Ø§ Ø§Ú¯Ù„Ø§ Ù…Ø±Ø­Ù„Û ÛŒÛ ÛÛ’ Ú©Û inputs Ú©Ùˆ model Ø³Û’ Ú¯Ø²Ø§Ø±Ø§ Ø¬Ø§Ø¦Û’Û” ÛÙ… Ù†Û’ Ù¾ÛÙ„Û’ ÛÛŒ initialization Ù…ÛŒÚº Ø§Ù¾Ù†Ø§ model specify Ú©Ø± Ø¯ÛŒØ§ ÛÛ’ØŒ Ù…Ú¯Ø± Ø§Ø¨ Ø¨Ú¾ÛŒ ÛÙ…ÛŒÚº Ø§Ø³Û’ ØªØ±Ø¨ÛŒØª Ø¯ÛŒÙ†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’ØŒ Ø¬Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© `WordPieceTrainer` Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆÚ¯ÛŒÛ” ğŸ¤— Tokenizers Ù…ÛŒÚº trainer instantiate Ú©Ø±ØªÛ’ ÙˆÙ‚Øª ÛŒØ§Ø¯ Ø±Ú©Ú¾Ù†Ø§ ÛÛ’ Ú©Û Ø¢Ù¾ Ú©Ùˆ ØªÙ…Ø§Ù… ÙˆÛ special tokens ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Û’ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬Ù† Ú©Ø§ Ø¢Ù¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº â€” ÙˆØ±Ù†Û training corpus Ù…ÛŒÚº ÛŒÛ Ø´Ø§Ù…Ù„ Ù†ÛÛŒÚº ÛÙˆØªÛ’:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

`vocab_size` Ø§ÙˆØ± `special_tokens` Ú©Û’ Ø¹Ù„Ø§ÙˆÛØŒ Ø¢Ù¾ `min_frequency` (Ø¬ØªÙ†ÛŒ Ø¨Ø§Ø± Ú©Ø³ÛŒ token Ú©Ø§ Ø¸Ø§ÛØ± ÛÙˆÙ†Ø§ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ Ú©Û Ø§Ø³Û’ vocabulary Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’) ÛŒØ§ `continuing_subword_prefix` (Ø§Ú¯Ø± Ø¢Ù¾ `##` Ú©Û’ Ø¹Ù„Ø§ÙˆÛ Ú©ÙˆØ¦ÛŒ Ø§ÙˆØ± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº) Ø³ÛŒÙ¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

ÛÙ… Ø§Ù¾Ù†Û’ Ù¾ÛÙ„Û’ Ø³Û’ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Û iterator Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ model Ú©Ùˆ ØªØ±Ø¨ÛŒØª Ø¯ÛŒÙ†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ø³ ÛŒÛ Ú©Ù…Ø§Ù†Úˆ Ú†Ù„Ø§Ø¦ÛŒÚº Ú¯Û’:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Ø¢Ù¾ text files Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø¨Ú¾ÛŒ Ú©Ø± Ú©Û’ Ø§Ù¾Ù†Û’ tokenizer Ú©Ùˆ ØªØ±Ø¨ÛŒØª Ø¯Û’ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ú©Ú†Ú¾ ÛŒÙˆÚº Ù†Ø¸Ø± Ø¢Ø¦Û’ Ú¯Ø§ (Ø§Ø³ Ø³Û’ Ù¾ÛÙ„Û’ ÛÙ… model Ú©Ùˆ Ø§ÛŒÚ© Ø®Ø§Ù„ÛŒ `WordPiece` Ú©Û’ Ø³Ø§ØªÚ¾ reinitialize Ú©Ø± Ø¯ÛŒÚº Ú¯Û’):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Ø¯ÙˆÙ†ÙˆÚº ØµÙˆØ±ØªÙˆÚº Ù…ÛŒÚºØŒ Ø¢Ù¾ Ù¾Ú¾Ø± `encode()` method Ú©Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ú©Û’ Ú©Ø³ÛŒ text Ù¾Ø± tokenizer Ú©Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

Ø­Ø§ØµÙ„ Ø´Ø¯Û `encoding` Ø§ÛŒÚ© `Encoding` object ÛÛ’ØŒ Ø¬Ø³ Ù…ÛŒÚº tokenizer Ú©Û’ ØªÙ…Ø§Ù… Ø¶Ø±ÙˆØ±ÛŒ outputs Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆØªÛ’ ÛÛŒÚº Ø¬ÛŒØ³Û’: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, Ø§ÙˆØ± `overflowing`.

Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ú©Ø§ Ø¢Ø®Ø±ÛŒ Ù…Ø±Ø­Ù„Û post-processing ÛÛ’Û” ÛÙ…ÛŒÚº `[CLS]` token Ú©Ùˆ Ø´Ø±ÙˆØ¹Ø§Øª Ù…ÛŒÚº Ø§ÙˆØ± `[SEP]` token Ú©Ùˆ Ø¢Ø®Ø± Ù…ÛŒÚº (ÛŒØ§ Ø§Ú¯Ø± Ø¬Ù…Ù„ÙˆÚº Ú©Ø§ Ø¬ÙˆÚ‘Ø§ ÛÙˆ ØªÙˆ ÛØ± Ø¬Ù…Ù„Û’ Ú©Û’ Ø¨Ø¹Ø¯) Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Ø§ ÛÙˆØªØ§ ÛÛ’Û” Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… `TemplateProcessor` Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ù…Ú¯Ø± Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº vocabulary Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ `[CLS]` Ø§ÙˆØ± `[SEP]` tokens Ú©ÛŒ IDs Ù…Ø¹Ù„ÙˆÙ… Ú©Ø±Ù†ÛŒ ÛÙˆÚº Ú¯ÛŒ:


```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

To write the template for the `TemplateProcessor`, we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon.

TemplateProcessor Ú©Û’ Ù„ÛŒÛ’ template Ù„Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº ÛŒÛ Ø¨ØªØ§Ù†Ø§ ÛÙˆØªØ§ ÛÛ’ Ú©Û Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ Ø¬Ù…Ù„Û Ø§ÙˆØ± Ø¬Ù…Ù„ÙˆÚº Ú©Û’ Ø¬ÙˆÚ‘Û’ Ú©Ùˆ Ú©ÛŒØ³Û’ handle Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’Û” Ø¯ÙˆÙ†ÙˆÚº ØµÙˆØ±ØªÙˆÚº Ù…ÛŒÚºØŒ ÛÙ… ÙˆÛ Ø®Ø§Øµ tokens Ù„Ú©Ú¾ØªÛ’ ÛÛŒÚº Ø¬Ù† Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚºØ› Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ (ÛŒØ§ single) Ø¬Ù…Ù„Û’ Ú©Ùˆ `$A` Ø³Û’ Ø¸Ø§ÛØ± Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ø¬Ø¨Ú©Û Ø¬Ù…Ù„ÙˆÚº Ú©Û’ Ø¬ÙˆÚ‘Û’ Ù…ÛŒÚº Ø¯ÙˆØ³Ø±Û’ Ø¬Ù…Ù„Û’ Ú©Ùˆ `$B` Ø³Û’ Ø¸Ø§ÛØ± Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û” Ø§Ù† Ú©Û’ Ù„ÛŒÛ’ (Ø®Ø§Øµ tokens Ø§ÙˆØ± Ø¬Ù…Ù„Û’) ÛÙ… colon Ú©Û’ Ø¨Ø¹Ø¯ Ù…ØªØ¹Ù„Ù‚Û token type ID Ø¨Ú¾ÛŒ Ù…ØªØ¹ÛŒÙ† Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

The classic BERT template is thus defined as follows:

Ø±ÙˆØ§ÛŒØªÛŒ BERT template Ú©Ùˆ Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ define Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û ÛÙ…ÛŒÚº **Ø§Ø³Ù¾ÛŒØ´Ù„ Ù¹ÙˆÚ©Ù†Ø²** Ú©Û’ **IDs** Ø¨Ú¾ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Û’ ÛÙˆÚº Ú¯Û’ ØªØ§Ú©Û **Ù¹ÙˆÚ©ÛŒÙ†Ø§Ø¦Ø²Ø±** Ø§Ù†ÛÛŒÚº Ø¯Ø±Ø³Øª Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ **ID** Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø³Ú©Û’Û”  

Ø¬Ø¨ ÛÙ… ÛŒÛ Ø´Ø§Ù…Ù„ Ú©Ø± Ù„ÛŒÚº Ú¯Û’ØŒ ØªÙˆ ÛÙ…Ø§Ø±Ø§ Ù¾Ú†Ú¾Ù„Ø§ **Ù…Ø«Ø§Ù„** Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ù†ØªÛŒØ¬Û Ø¯Û’ Ú¯Ø§:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

Ø§ÙˆØ± Ø¬Ø¨ ÛÙ… Ø¯Ùˆ Ø¬Ù…Ù„ÙˆÚº Ú©Û’ Ø¬ÙˆÚ‘Û’ Ù¾Ø± Ø¢Ø²Ù…Ø§ØªÛ’ ÛÛŒÚºØŒ ØªÙˆ ÛÙ…ÛŒÚº Ø¯Ø±Ø³Øª Ù†ØªÛŒØ¬Û Ù…Ù„ØªØ§ ÛÛ’:


```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

ÛÙ… Ù†Û’ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÛ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø´Ø±ÙˆØ¹ Ø³Û’ Ø¨Ù†Ø§ Ù„ÛŒØ§ ÛÛ’â€”Ø¢Ø®Ø±ÛŒ Ù…Ø±Ø­Ù„Û Ø§ÛŒÚ© ÚˆÛŒÚ©ÙˆÚˆØ± Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Ø§ ÛÛ’:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

Ø¢Ø¦ÛŒÛ’ Ø§Ø³Û’ Ø§Ù¾Ù†Û’ Ù¾Ú†Ú¾Ù„Û’ `encoding` Ù¾Ø± Ø¢Ø²Ù…Ø§ØªÛ’ ÛÛŒÚº:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

Ø¨ÛØª Ø®ÙˆØ¨! ÛÙ… Ø§Ù¾Ù†Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ JSON ÙØ§Ø¦Ù„ Ù…ÛŒÚº Ø§Ø³ Ø·Ø±Ø­ Ù…Ø­ÙÙˆØ¸ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
tokenizer.save("tokenizer.json")
```

Ù¾Ú¾Ø± ÛÙ… Ø§Ø³ ÙØ§Ø¦Ù„ Ú©Ùˆ `Tokenizer` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ù…ÛŒÚº `from_file()` Ù…ÛŒØªÚ¾Úˆ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¯ÙˆØ¨Ø§Ø±Û Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

ğŸ¤— Transformers Ù…ÛŒÚº Ø§Ø³ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº Ø§Ø³Û’ `PreTrainedTokenizerFast` Ù…ÛŒÚº Ù„Ù¾ÛŒÙ¹Ù†Ø§ ÛÙˆÚ¯Ø§Û” ÛÙ… ÛŒØ§ ØªÙˆ Ø¬Ù†ÛŒØ±Ú© Ú©Ù„Ø§Ø³ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº ÛŒØ§ØŒ Ø§Ú¯Ø± ÛÙ…Ø§Ø±Ø§ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ø³ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ù…Ø§ÚˆÙ„ Ø³Û’ Ù…Ø·Ø§Ø¨Ù‚Øª Ø±Ú©Ú¾ØªØ§ ÛÛ’ØŒ ØªÙˆ Ø§Ø³ Ú©Ù„Ø§Ø³ Ú©Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº (ÛŒÛØ§ÚºØŒ `BertTokenizerFast`)Û” Ø§Ú¯Ø± Ø¢Ù¾ ÛŒÛ Ø³Ø¨Ù‚ Ú©Ø³ÛŒ Ø¨Ø§Ù„Ú©Ù„ Ù†Ø¦Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ù„ÛŒÛ’ Ù„Ø§Ú¯Ùˆ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ Ù¾ÛÙ„Ø§ Ø¢Ù¾Ø´Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§Û”  

`PreTrainedTokenizerFast` Ù…ÛŒÚº Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ù„Ù¾ÛŒÙ¹Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… ÛŒØ§ ØªÙˆ `tokenizer_object` Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø§Ù¾Ù†Ø§ Ø¨Ù†Ø§ÛŒØ§ ÛÙˆØ§ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø¯Û’ Ø³Ú©ØªÛ’ ÛÛŒÚº ÛŒØ§ Ù…Ø­ÙÙˆØ¸ Ú©Ø±Ø¯Û Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± ÙØ§Ø¦Ù„ Ú©Ùˆ `tokenizer_file` Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø¯Û’ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§ÛÙ… Ø¨Ø§Øª ÛŒÛ ÛÛ’ Ú©Û ÛÙ…ÛŒÚº ØªÙ…Ø§Ù… Ø®Ø§Øµ Ù¹ÙˆÚ©Ù†Ø² (special tokens) Ú©Ùˆ Ø¯Ø³ØªÛŒ Ø·ÙˆØ± Ù¾Ø± Ø³ÛŒÙ¹ Ú©Ø±Ù†Ø§ ÛÙˆÚ¯Ø§ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ Ú©Ù„Ø§Ø³ Ø®ÙˆØ¯ Ù†ÛÛŒÚº Ø¬Ø§Ù† Ø³Ú©ØªÛŒ Ú©Û Ù…Ø§Ø³Ú© Ù¹ÙˆÚ©Ù†ØŒ `[CLS]` Ù¹ÙˆÚ©Ù†ØŒ ÙˆØºÛŒØ±Û Ú©ÙˆÙ† Ø³Û’ ÛÛŒÚºÛ”

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

Ø§Ú¯Ø± Ø¢Ù¾ Ú©Ø³ÛŒ Ù…Ø®ØµÙˆØµ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ù„Ø§Ø³ (Ø¬ÛŒØ³Û’ `BertTokenizerFast`) Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ ØµØ±Ù ÙˆÛ Ø®Ø§Øµ Ù¹ÙˆÚ©Ù†Ø² (special tokens) Ù…ØªØ¹ÛŒÙ† Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆÚ¯ÛŒ Ø¬Ùˆ ÚˆÛŒÙØ§Ù„Ù¹ Ø³Û’ Ù…Ø®ØªÙ„Ù ÛÙˆÚº (ÛŒÛØ§ÚºØŒ Ú©ÙˆØ¦ÛŒ Ø¨Ú¾ÛŒ Ù†ÛÛŒÚº):


```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

Ø§Ø¨ Ø¢Ù¾ Ø§Ø³ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ø¯ÙˆØ³Ø±Û’ ğŸ¤— Transformers Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©ÛŒ Ø·Ø±Ø­ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø¢Ù¾ Ø§Ø³Û’ `save_pretrained()` Ù…ÛŒØªÚ¾Úˆ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù…Ø­ÙÙˆØ¸ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ ÛŒØ§ `push_to_hub()` Ù…ÛŒØªÚ¾Úˆ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÛØ¨ Ù¾Ø± Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”  

Ø§Ø¨ Ø¬Ø¨ Ú©Û ÛÙ… Ù†Û’ Ø¯ÛŒÚ©Ú¾Ø§ Ú©Û ÙˆØ±ÚˆÙ¾ÛŒØ³ (WordPiece) Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©ÛŒØ³Û’ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ø¢Ø¦ÛŒÛ’ Ø¨ÛŒ Ù¾ÛŒ Ø§ÛŒ (BPE) Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ù„ÛŒÛ’ Ø¨Ú¾ÛŒ ÛŒÛÛŒ Ø¹Ù…Ù„ Ø¯ÛØ±Ø§Ø¦ÛŒÚºÛ” ÛÙ… Ø§Ø³ Ø¨Ø§Ø± ØªÛŒØ² Ø¬Ø§Ø¦ÛŒÚº Ú¯Û’ Ú©ÛŒÙˆÙ†Ú©Û Ø¢Ù¾ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¬Ø§Ù† Ú†Ú©Û’ ÛÛŒÚºØŒ Ø§ÙˆØ± ØµØ±Ù ÙØ±Ù‚ Ú©Ùˆ Ù†Ù…Ø§ÛŒØ§Úº Ú©Ø±ÛŒÚº Ú¯Û’Û”  

## Ø¨ÛŒ Ù¾ÛŒ Ø§ÛŒ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø´Ø±ÙˆØ¹ Ø³Û’ Ø¨Ù†Ø§Ù†Ø§ [[building-a-bpe-tokenizer-from-scratch]]  

Ø§Ø¨ ÛÙ… Ø§ÛŒÚ© GPT-2 Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’Û” Ø¨ÛŒ Ø§ÛŒ Ø¢Ø± Ù¹ÛŒ (BERT) Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ… Ø¨Ú¾ÛŒ Ø§ÛŒÚ© `Tokenizer` Ú©Ùˆ Ø¨ÛŒ Ù¾ÛŒ Ø§ÛŒ Ù…Ø§ÚˆÙ„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù†ÛŒØ´ÛŒØ¦Ù„Ø§Ø¦Ø² (initialize) Ú©Ø± Ú©Û’ Ø´Ø±ÙˆØ¹Ø§Øª Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
tokenizer = Tokenizer(models.BPE())
```

Ø§Ø³ÛŒ Ø·Ø±Ø­ Ø¬ÛŒØ³Û’ Ø¨ÛŒ Ø§ÛŒ Ø¢Ø± Ù¹ÛŒ (BERT) Ú©Û’ Ù„ÛŒÛ’ØŒ Ø§Ú¯Ø± ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ù¾ÛÙ„Û’ Ø³Û’ Ø§ÛŒÚ© Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ ÙÛØ±Ø³Øª (vocabulary) ÛÙˆØªÛŒØŒ ØªÙˆ ÛÙ… Ø§Ø³ Ù…Ø§ÚˆÙ„ Ú©Ùˆ `vocab` Ø§ÙˆØ± `merges` Ù¾Ø§Ø³ Ú©Ø± Ú©Û’ Ø§Ù†ÛŒØ´ÛŒØ¦Ù„Ø§Ø¦Ø² Ú©Ø± Ø³Ú©ØªÛ’ ØªÚ¾Û’Û” Ù„ÛŒÚ©Ù† Ú†ÙˆÙ†Ú©Û ÛÙ… Ø§Ø³Û’ Ø´Ø±ÙˆØ¹ Ø³Û’ ØªØ±Ø¨ÛŒØª Ø¯ÛŒÚº Ú¯Û’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø§Ø³ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚºÛ” ÛÙ…ÛŒÚº `unk_token` Ø¨Ú¾ÛŒ Ù…ØªØ¹ÛŒÙ† Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº Ú©ÛŒÙˆÙ†Ú©Û GPT-2 Ø¨Ø§Ø¦Ù¹-Ù„ÛŒÙˆÙ„ Ø¨ÛŒ Ù¾ÛŒ Ø§ÛŒ (byte-level BPE) Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Ø§Ø³ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº Ø±Ú©Ú¾ØªØ§Û”  

GPT-2 Ú©Ø³ÛŒ Ù†Ø§Ø±Ù…Ù„Ø§Ø¦Ø²Ø± (normalizer) Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù†ÛÛŒÚº Ú©Ø±ØªØ§ØŒ Ù„ÛÙ°Ø°Ø§ ÛÙ… Ø§Ø³ Ù…Ø±Ø­Ù„Û’ Ú©Ùˆ Ú†Ú¾ÙˆÚ‘ Ú©Ø± Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ù¾Ø±ÛŒ-Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† (pre-tokenization) Ù¾Ø± Ø¬Ø§ØªÛ’ ÛÛŒÚº:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

ÛÙ… Ù†Û’ ÛŒÛØ§Úº `ByteLevel` Ù…ÛŒÚº Ø¬Ùˆ Ø¢Ù¾Ø´Ù† Ø´Ø§Ù…Ù„ Ú©ÛŒØ§ ÛÛ’ØŒ ÙˆÛ Ø¬Ù…Ù„Û’ Ú©Û’ Ø´Ø±ÙˆØ¹ Ù…ÛŒÚº Ø§Ø³Ù¾ÛŒØ³ Ø´Ø§Ù…Ù„ Ù†Û Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÛÛ’ (Ø¬Ùˆ Ú©Û Ø¨ØµÙˆØ±Øª Ø¯ÛŒÚ¯Ø± ÚˆÛŒÙØ§Ù„Ù¹ ÛÙˆØªØ§ ÛÛ’)Û” ÛÙ… Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ú©Ø³ÛŒ Ù…ØªÙ† Ú©ÛŒ Ù¾Ø±ÛŒ-Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛŒØ³Û’ Ù¾ÛÙ„Û’ Ú©ÛŒØ§ ØªÚ¾Ø§:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

Ø§Ú¯Ù„Ø§ Ù…Ø±Ø­Ù„Û Ù…Ø§ÚˆÙ„ ÛÛ’ØŒ Ø¬Ø³Û’ Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’Û” GPT-2 Ú©Û’ Ù„ÛŒÛ’ ÙˆØ§Ø­Ø¯ Ø®Ø§Øµ Ù¹ÙˆÚ©Ù† "Ø§Ø®ØªØªØ§Ù…Ù Ù…ØªÙ†" (end-of-text) Ù¹ÙˆÚ©Ù† ÛÛ’:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Ø¬ÛŒØ³Û’ Ú©Û `WordPieceTrainer` Ù…ÛŒÚºØŒ ÛŒÛØ§Úº Ø¨Ú¾ÛŒ ÛÙ… `vocab_size` Ø§ÙˆØ± `special_tokens` Ú©Û’ Ø³Ø§ØªÚ¾ `min_frequency` Ú©Ùˆ Ù…ØªØ¹ÛŒÙ† Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§Ú¯Ø± Ø¶Ø±ÙˆØ±Øª ÛÙˆÛ” Ø§Ú¯Ø± ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ú©Ø³ÛŒ Ù„ÙØ¸ Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ Ù…Ø®ØµÙˆØµ Ù„Ø§Ø­Ù‚Û (suffix) ÛÙˆ (Ø¬ÛŒØ³Û’ `</w>`)ØŒ ØªÙˆ ÛÙ… Ø§Ø³Û’ `end_of_word_suffix` Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø³ÛŒÙ¹ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”  

ÛŒÛ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù¹ÛŒÚ©Ø³Ù¹ ÙØ§Ø¦Ù„ÙˆÚº Ù¾Ø± Ø¨Ú¾ÛŒ Ù¹Ø±ÛŒÙ† Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Ø¢Ø¦ÛŒÛ’ Ú©Ø³ÛŒ Ù†Ù…ÙˆÙ†Û Ù…ØªÙ† Ú©ÛŒ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒØªÛ’ ÛÛŒÚº:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

ÛÙ… GPT-2 Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Û’ Ù„ÛŒÛ’ Ø¨Ø§Ø¦Ù¹-Ù„ÛŒÙˆÙ„ Ù¾ÙˆØ³Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Ùˆ Ø¯Ø±Ø¬ Ø°ÛŒÙ„ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ù„Ø§Ú¯Ùˆ Ú©Ø±ØªÛ’ ÛÛŒÚº:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

`trim_offsets = False` Ø¢Ù¾Ø´Ù† Ù¾ÙˆØ³Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ø± Ú©Ùˆ Ø¨ØªØ§ØªØ§ ÛÛ’ Ú©Û ÙˆÛ Ø§Ù† Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ Ø¢ÙØ³ÛŒÙ¹Ø³ Ú©Ùˆ Ø¬ÙˆÚº Ú©Ø§ ØªÙˆÚº Ú†Ú¾ÙˆÚ‘ Ø¯Û’ Ø¬Ùˆ 'Ä ' Ø³Û’ Ø´Ø±ÙˆØ¹ ÛÙˆØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛŒÛ ÛÛ’ Ú©Û Ø¢ÙØ³ÛŒÙ¹Ø³ Ú©Ø§ Ø¢ØºØ§Ø² Ø§Ø³ Ù„ÙØ¸ Ø³Û’ Ù¾ÛÙ„Û’ Ú©ÛŒ Ø§Ø³Ù¾ÛŒØ³ Ú©ÛŒ Ø·Ø±Ù Ø§Ø´Ø§Ø±Û Ú©Ø±Û’ Ú¯Ø§ØŒ Ù†Û Ú©Û Ù„ÙØ¸ Ú©Û’ Ù¾ÛÙ„Û’ Ø­Ø±Ù Ú©ÛŒ Ø·Ø±Ù (Ú©ÛŒÙˆÙ†Ú©Û Ø§Ø³Ù¾ÛŒØ³ ØªÚ©Ù†ÛŒÚ©ÛŒ Ø·ÙˆØ± Ù¾Ø± Ù¹ÙˆÚ©Ù† Ú©Ø§ Ø­ØµÛ ÛÛ’)Û” Ø¢Ø¦ÛŒÛ’ ÛÙ… Ø§Ø¨Ú¾ÛŒ Ø§Ù†Ú©ÙˆÚˆ Ú©ÛŒÛ’ Ú¯Ø¦Û’ Ù¹ÛŒÚ©Ø³Ù¹ Ú©Û’ Ù†ØªÛŒØ¬Û’ Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛØ§Úº `'Ä test'` Ø§Ù†ÚˆÛŒÚ©Ø³ 4 Ù¾Ø± ÙˆØ§Ù‚Ø¹ Ù¹ÙˆÚ©Ù† ÛÛ’:


```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… Ø§ÛŒÚ© Ø¨Ø§Ø¦Ù¹-Ù„ÛŒÙˆÙ„ ÚˆÛŒÚ©ÙˆÚˆØ± Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:


```python
tokenizer.decoder = decoders.ByteLevel()
```

Ø§ÙˆØ± ÛÙ… Ø¯ÙˆØ¨Ø§Ø±Û Ú†ÛŒÚ© Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ Ø¯Ø±Ø³Øª Ú©Ø§Ù… Ú©Ø± Ø±ÛØ§ ÛÛ’:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

Ø²Ø¨Ø±Ø¯Ø³Øª! Ø§Ø¨ Ø¬Ø¨ Ú©Û ÛÙ… Ù…Ú©Ù…Ù„ Ú©Ø± Ú†Ú©Û’ ÛÛŒÚºØŒ ÛÙ… Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ù…Ø­ÙÙˆØ¸ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§Ú¯Ø± ÛÙ… Ø§Ø³Û’ ğŸ¤— Transformers Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ Ø§Ø³Û’ `PreTrainedTokenizerFast` ÛŒØ§ `GPT2TokenizerFast` Ù…ÛŒÚº Ù„Ù¾ÛŒÙ¹ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```
ÛŒØ§:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

Ø¬ÛŒØ³Ø§ Ú©Û Ø¢Ø®Ø±ÛŒ Ù…Ø«Ø§Ù„ Ù…ÛŒÚºØŒ ÛÙ… Ø¢Ù¾ Ú©Ùˆ Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ú¯Û’ Ú©Û ÛŒÙˆÙ†ÙÚ¯Ø±Ø§Ù… Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø´Ø±ÙˆØ¹ Ø³Û’ Ú©ÛŒØ³Û’ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§Ø¦Û’Û”  

## ÛŒÙˆÙ†ÙÚ¯Ø±Ø§Ù… Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ø´Ø±ÙˆØ¹ Ø³Û’ Ø¨Ù†Ø§Ù†Ø§ [[building-a-unigram-tokenizer-from-scratch]]  

Ø§Ø¨ ÛÙ… Ø§ÛŒÚ© **XLNet** Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’Û” Ù¾Ú†Ú¾Ù„Û’ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©ÛŒ Ø·Ø±Ø­ØŒ ÛÙ… Ø§ÛŒÚ© **`Tokenizer`** Ú©Ùˆ **Unigram** Ù…Ø§ÚˆÙ„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹ Ú©Ø±Ú©Û’ Ø¢ØºØ§Ø² Ú©Ø±ÛŒÚº Ú¯Û’:


```python
tokenizer = Tokenizer(models.Unigram())
```

Ø§Ø³ÛŒ Ø·Ø±Ø­ØŒ Ø§Ú¯Ø± ÛÙ…Ø§Ø±Û’ Ù¾Ø§Ø³ Ø§ÛŒÚ© ÙˆÚ©ÛŒØ¨Ù„Ø±ÛŒ ÛÙˆØªÛŒ ØªÙˆ ÛÙ… Ø§Ø³ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø§Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨Ú¾ÛŒ Ø´Ø±ÙˆØ¹ Ú©Ø± Ø³Ú©ØªÛ’ ØªÚ¾Û’Û”  

**Normalization** Ú©Û’ Ù„ÛŒÛ’ØŒ **XLNet** Ú©Ú†Ú¾ Ø±ÛŒÙ¾Ù„ÛŒØ³Ù…Ù†Ù¹Ø³ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ (Ø¬Ùˆ **SentencePiece** Ø³Û’ Ø¢ØªÛŒ ÛÛŒÚº):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

ÛŒÛ <code>``</code> Ø§ÙˆØ± <code>''</code> Ú©Ùˆ <code>"</code> Ø³Û’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø± Ø¯ÛŒØªØ§ ÛÛ’ Ø§ÙˆØ± Ø¯Ùˆ ÛŒØ§ Ø§Ø³ Ø³Û’ Ø²ÛŒØ§Ø¯Û spaces Ú©ÛŒ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ sequence Ú©Ùˆ Ø§ÛŒÚ© single space Ø³Û’ Ø¨Ø¯Ù„ Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ø³Ø§ØªÚ¾ ÛÛŒ tokenize Ú©ÛŒÛ’ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„Û’ texts Ø³Û’ accents Ú©Ùˆ Ø¨Ú¾ÛŒ ÛÙ¹Ø§ Ø¯ÛŒØªØ§ ÛÛ’Û”

SentencePiece tokenizer Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆÙ†Û’ ÙˆØ§Ù„Ø§ pre-tokenizer `Metaspace` ÛÛ’:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

ÛÙ… Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ Ú©Ø³ÛŒ example text Ú©ÛŒ pre-tokenization Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº:



```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("â–Let's", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', (14, 29))]
```

Ø§Ú¯Ù„Ø§ Ù…Ø±Ø­Ù„Û Ù…Ø§ÚˆÙ„ ÛÛ’ØŒ Ø¬Ø³Û’ ØªØ±Ø¨ÛŒØª (training) Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’Û” **XLNet** Ù…ÛŒÚº Ú©Ø§ÙÛŒ Ø²ÛŒØ§Ø¯Û **special tokens** Ø´Ø§Ù…Ù„ ÛÙˆØªÛ’ ÛÛŒÚº:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```


**`UnigramTrainer`** Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø¨ÛØª Ø§ÛÙ… Ø¯Ù„ÛŒÙ„ **`unk_token`** ÛÛ’ØŒ Ø¬Ø³Û’ Ø¨Ú¾ÙˆÙ„Ù†Ø§ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’Û” ÛÙ… Ø¯ÛŒÚ¯Ø± Ø¯Ù„Ø§Ø¦Ù„ Ø¨Ú¾ÛŒ Ù¾Ø§Ø³ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø¬Ùˆ **Unigram** Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØµÙˆØµ ÛÛŒÚºØŒ Ø¬ÛŒØ³Û’ Ú©Û **`shrinking_factor`** (ÛØ± Ù…Ø±Ø­Ù„Û’ Ù…ÛŒÚº Ù¹ÙˆÚ©Ù†Ø² Ú©Ùˆ ÛÙ¹Ø§Ù†Û’ Ú©Ø§ ØªÙ†Ø§Ø³Ø¨ØŒ Ø¬Ùˆ Ú©Û ÚˆÛŒÙØ§Ù„Ù¹ Ù…ÛŒÚº **0.75** ÛÙˆØªØ§ ÛÛ’) ÛŒØ§ **`max_piece_length`** (Ú©Ø³ÛŒ Ù…Ø®ØµÙˆØµ Ù¹ÙˆÚ©Ù† Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒØŒ Ø¬Ùˆ Ú©Û ÚˆÛŒÙØ§Ù„Ù¹ Ù…ÛŒÚº **16** ÛÙˆØªÛŒ ÛÛ’)Û”  

ÛŒÛ **tokenizer** Ù¹ÛŒÚ©Ø³Ù¹ ÙØ§Ø¦Ù„Ø² Ù¾Ø± Ø¨Ú¾ÛŒ ØªØ±Ø¨ÛŒØª Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:


```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```


Ø¢Ø¦ÛŒÛ’ Ø§ÛŒÚ© Ù†Ù…ÙˆÙ†Û Ù…ØªÙ† Ú©ÛŒ **tokenization** Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„ØªÛ’ ÛÛŒÚº:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']
```


XLNet Ú©ÛŒ Ø§ÛŒÚ© Ø®Ø§Øµ Ø¨Ø§Øª ÛŒÛ ÛÛ’ Ú©Û ÛŒÛ `<cls>` Ù¹ÙˆÚ©Ù† Ú©Ùˆ Ø¬Ù…Ù„Û’ Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº Ø±Ú©Ú¾ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ú©Ø§ **type ID** `2` ÛÙˆØªØ§ ÛÛ’ (ØªØ§Ú©Û Ø§Ø³Û’ Ø¯ÙˆØ³Ø±Û’ Ù¹ÙˆÚ©Ù†Ø² Ø³Û’ Ø§Ù„Ú¯ Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’)Û” Ø§Ø³ Ú©Ø§ **padding** Ø¨Ø§Ø¦ÛŒÚº Ø·Ø±Ù ÛÙˆØªØ§ ÛÛ’Û”  

ÛÙ… ØªÙ…Ø§Ù… **special tokens** Ø§ÙˆØ± **token type IDs** Ú©Ùˆ Ø§ÛŒÚ© **template** Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø³Ù†Ø¨Ú¾Ø§Ù„ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø¬ÛŒØ³Û’ Ú©Û **BERT** Ú©Û’ Ù„ÛŒÛ’ØŒ Ù„ÛŒÚ©Ù† Ù¾ÛÙ„Û’ ÛÙ…ÛŒÚº `<cls>` Ø§ÙˆØ± `<sep>` Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ **IDs** Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ ÛÙˆÚº Ú¯Û’:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

Ù¹ÛŒÙ…Ù¾Ù„ÛŒÙ¹ Ø§Ø³ Ø·Ø±Ø­ Ø¯Ú©Ú¾Ø§Ø¦ÛŒ Ø¯ÛŒØªØ§ ÛÛ’:


```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

Ø§ÙˆØ± ÛÙ… Ø§Ø³Û’ Ø§ÛŒÚ© Ø¬ÙˆÚ‘Û’ Ø¬Ù…Ù„ÙˆÚº Ú©Ùˆ Ø§Ù†Ú©ÙˆÚˆ Ú©Ø±Ú©Û’ Ø¢Ø²Ù…Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ ÛÙ… Ø§ÛŒÚ© `Metaspace` ÚˆÛŒÚ©ÙˆÚˆØ± Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:


```python
tokenizer.decoder = decoders.Metaspace()
```

Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙ…Ø§Ø±Ø§ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù…Ú©Ù…Ù„ ÛÙˆ Ú¯ÛŒØ§! ÛÙ… Ø§Ø³Û’ Ù¾ÛÙ„Û’ Ú©ÛŒ Ø·Ø±Ø­ Ù…Ø­ÙÙˆØ¸ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§Ú¯Ø± ÛÙ… Ø§Ø³Û’ ğŸ¤— Transformers Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛÛŒÚº ØªÙˆ `PreTrainedTokenizerFast` ÛŒØ§ `XLNetTokenizerFast` Ù…ÛŒÚº Ù„Ù¾ÛŒÙ¹ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§ÛŒÚ© Ø¨Ø§Øª ÛŒØ§Ø¯ Ø±Ú©Ú¾Ù†Û’ Ú©ÛŒ ÛÛ’ Ú©Û `PreTrainedTokenizerFast` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÙˆÙ‚ØªØŒ Ø®Ø§Øµ Ù¹ÙˆÚ©Ù†Ø² Ú©Û’ Ø¹Ù„Ø§ÙˆÛØŒ ÛÙ…ÛŒÚº ğŸ¤— Transformers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Ùˆ ÛŒÛ Ø¨Ú¾ÛŒ Ø¨ØªØ§Ù†Ø§ ÛÙˆÚ¯Ø§ Ú©Û Ù¾ÛŒÚˆÙ†Ú¯ Ø¨Ø§Ø¦ÛŒÚº Ø¬Ø§Ù†Ø¨ ÛÙˆÚ¯ÛŒ:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

ÛŒØ§ Ù…ØªØ¨Ø§Ø¯Ù„ Ø·ÙˆØ± Ù¾Ø±:


```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

Ø§Ø¨ Ø¬Ø¨ Ú©Û Ø¢Ù¾ Ù†Û’ Ø¯ÛŒÚ©Ú¾Ø§ Ú©Û Ù…Ø®ØªÙ„Ù ØªØ¹Ù…ÛŒØ±Ø§ØªÛŒ Ø¨Ù„Ø§Ú©Ø³ Ú©Ùˆ Ù…ÙˆØ¬ÙˆØ¯Û Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ³Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ú©Ùˆ Ù„Ú©Ú¾Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’ Ø§ÙˆØ± Ø§Ø³Û’ ğŸ¤— Transformers Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ú†Ø§ÛÛŒÛ’Û”














<Youtube id="TGZfZVuF9Yc"/>

<Tip>

ğŸ’¡ ÛŒÛ Ø³ÛŒÚ©Ø´Ù† Unigram Ú©Ùˆ ØªÙØµÛŒÙ„ Ø³Û’ cover Ú©Ø±ØªØ§ ÛÛ’ØŒ ÛŒÛØ§Úº ØªÚ© Ú©Û Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ implementation Ø¯Ú©Ú¾Ø§ØªØ§ ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ ØµØ±Ù Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Ø§ Ø¹Ù…ÙˆÙ…ÛŒ Ø¬Ø§Ø¦Ø²Û Ú†Ø§ÛØªÛ’ ÛÛŒÚº ØªÙˆ Ø¢Ù¾ Ø¢Ø®Ø± Ù…ÛŒÚº jump Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

</Tip>

## Training algorithm[[training-algorithm]]

BPE Ø§ÙˆØ± WordPiece Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚºØŒ Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ø§ÛŒÚ© Ø¨Ø§Ù„Ú©Ù„ Ù…Ø®ØªÙ„Ù Ø±Ø® Ø§Ø®ØªÛŒØ§Ø± Ú©Ø±ØªØ§ ÛÛ’: ÛŒÛ Ø§ÛŒÚ© Ø¨Ú‘ÛŒ vocabulary Ø³Û’ Ø´Ø±ÙˆØ¹ ÛÙˆØªØ§ ÛÛ’ Ø§ÙˆØ± Ù…Ø·Ù„ÙˆØ¨Û vocabulary size ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ tokens Ú©Ùˆ Ø­Ø°Ù Ú©Ø±ØªØ§ ÛÛ’Û” Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¢Ù¾ Ù…Ø®ØªÙ„Ù Ø·Ø±ÛŒÙ‚Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù…Ø«Ù„Ø§Ù‹ pre-tokenized Ø§Ù„ÙØ§Ø¸ Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent substrings Ù„ÛŒÙ†Ø§ØŒ ÛŒØ§ Ù¾Ú¾Ø± Ø§ÛŒÚ© Ø¨Ú‘ÛŒ vocabulary Ù¾Ø± BPE Ú†Ù„Ø§Ù†Ø§Û”

Training Ú©Û’ ÛØ± Ù…Ø±Ø­Ù„Û’ Ù¾Ø± Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ù…ÙˆØ¬ÙˆØ¯Û vocabulary Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± corpus Ú©Ø§ loss compute Ú©Ø±ØªØ§ ÛÛ’Û” Ù¾Ú¾Ø±ØŒ ÛØ± symbol Ú©Û’ Ù„ÛŒÛ’ Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… ÛŒÛ ØªØ¹ÛŒÙ† Ú©Ø±ØªØ§ ÛÛ’ Ú©Û Ø§Ú¯Ø± ÙˆÛ symbol Ø­Ø°Ù Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ Ù…Ø¬Ù…ÙˆØ¹ÛŒ loss Ù…ÛŒÚº Ú©ØªÙ†Ø§ Ø§Ø¶Ø§ÙÛ ÛÙˆÚ¯Ø§Û” Ø¬Ù† tokens Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹ÛŒ loss Ù¾Ø± Ú©Ù… Ø§Ø«Ø± Ù¾Ú‘ØªØ§ ÛÛ’ Ø§Ù†ÛÛŒÚº "Ú©Ù… Ø¶Ø±ÙˆØ±ÛŒ" Ø³Ù…Ø¬Ú¾ Ú©Ø± Ø­Ø°Ù Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û”

ÛŒÛ Ø¹Ù…Ù„ Ú©Ø§ÙÛŒ Ù…ÛÙ†Ú¯Ø§ (computationally expensive) ÛÙˆØªØ§ ÛÛ’ØŒ Ø§Ø³ Ù„ÛŒÛ’ Ø¹Ù…ÙˆÙ…Ø§Ù‹ ØµØ±Ù \\(p\\) ÙÛŒØµØ¯ (Ø¬ÛØ§Úº \\(p\\) Ø§ÛŒÚ© hyperparameter ÛÛ’ØŒ Ø¹Ù…ÙˆÙ…Ø§Ù‹ 10 ÛŒØ§ 20) tokens Ú©Ùˆ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ø­Ø°Ù Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø¬Ù† Ø³Û’ loss Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ú©Ù… Ø§Ø¶Ø§ÙÛ ÛÙˆØªØ§ ÛÛ’Û” ÛŒÛ Ø¹Ù…Ù„ Ø§Ø³ÛŒ Ø·Ø±Ø­ Ø¯ÛØ±Ø§ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø¬Ø¨ ØªÚ© Ú©Û vocabulary Ù…Ø·Ù„ÙˆØ¨Û size ØªÚ© Ù†Û Ù¾ÛÙ†Ú† Ø¬Ø§Ø¦Û’Û”

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û Ø¨Ù†ÛŒØ§Ø¯ÛŒ characters (ÛŒØ¹Ù†ÛŒ ÙˆÛ Ø¬Ùˆ Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ alphabet Ú©Ø§ Ø­ØµÛ ÛÛŒÚº) Ú©Ùˆ Ú©Ø¨Ú¾ÛŒ Ø­Ø°Ù Ù†ÛÛŒÚº Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªØ§Ú©Û Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ word Ú©Ùˆ tokenize Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’Û”

Ø§Ø¨ Ø¨Ú¾ÛŒ ÛŒÛ Ø¨ÛŒØ§Ù† Ú©Ú†Ú¾ Ù…Ø¨ÛÙ… ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’: Ø§ØµÙ„ Ù…Ø±Ø­Ù„Û corpus Ù¾Ø± loss compute Ú©Ø±Ù†Ø§ Ø§ÙˆØ± Ù¾Ú¾Ø± Ø¯ÛŒÚ©Ú¾Ù†Ø§ Ú©Û Ø§Ú¯Ø± Ú©Ø³ÛŒ token Ú©Ùˆ Ø­Ø°Ù Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ loss Ù…ÛŒÚº Ú©ØªÙ†Ø§ ÙØ±Ù‚ Ø¢ØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† ÛÙ… Ø§Ø¨Ú¾ÛŒ ØªÚ© Ø§Ø³ Ø¨Ø§Øª Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ù†ÛÛŒÚº Ú©ÛŒ ÛÛ’ Ú©Û ÛŒÛ Ú©ÛŒØ³Û’ Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§Û” ÛŒÛ Ù…Ø±Ø­Ù„Û Unigram Ù…Ø§ÚˆÙ„ Ú©Û’ tokenization Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ù¾Ø± Ù…Ù†Ø­ØµØ± ÛÛ’ØŒ Ø¬Ø³ Ù¾Ø± ÛÙ… Ø¢Ú¯Û’ ØªÙØµÛŒÙ„ Ø³Û’ Ø¨Ø§Øª Ú©Ø±ÛŒÚº Ú¯Û’Û”

ÛÙ… Ø§Ù¾Ù†ÛŒ Ù¾Ú†Ú¾Ù„ÛŒ Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

Ø§ÙˆØ± Ø§Ø³ Ù…Ø«Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ ÛÙ… Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary Ú©Û’ Ù„ÛŒÛ’ ØªÙ…Ø§Ù… strict substrings Ù„ÛŒÚº Ú¯Û’:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Tokenization algorithm[[tokenization-algorithm]]

Unigram Ù…Ø§ÚˆÙ„ Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ language model ÛÛ’ Ø¬Ùˆ ÙØ±Ø¶ Ú©Ø±ØªØ§ ÛÛ’ Ú©Û ÛØ± token Ù¾Ú†Ú¾Ù„Û’ tokens Ø³Û’ Ø¢Ø²Ø§Ø¯ ÛÛ’Û” ÛŒÛ Ø³Ø¨ Ø³Û’ Ø¢Ø³Ø§Ù† language model ØªØµÙˆØ± Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ Ú©ÛŒÙˆÙ†Ú©Û Ú©Ø³ÛŒ token X Ú©ÛŒ probability ØµØ±Ù X Ú©ÛŒ corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ frequency Ù¾Ø± Ù…Ø¨Ù†ÛŒ ÛÙˆØªÛŒ ÛÛ’Û” Ø§Ú¯Ø± ÛÙ… Unigram Ù…Ø§ÚˆÙ„ Ø³Û’ text generate Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº ØªÙˆ ÛÙ… ÛÙ…ÛŒØ´Û Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent token Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±ÛŒÚº Ú¯Û’Û”

Ú©Ø³ÛŒ token Ú©ÛŒ probability Ø§Ø³ Ú©ÛŒ frequency (corpus Ù…ÛŒÚº Ø¸Ø§ÛØ± ÛÙˆÙ†Û’ Ú©ÛŒ ØªØ¹Ø¯Ø§Ø¯) Ú©Ùˆ ØªÙ…Ø§Ù… tokens Ú©ÛŒ frequencies Ú©Û’ Ù…Ø¬Ù…ÙˆØ¹Û’ Ø³Û’ ØªÙ‚Ø³ÛŒÙ… Ú©Ø± Ú©Û’ Ø­Ø§ØµÙ„ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ `"ug"` Ú©Ùˆ Ù„ÙØ¸ `"hug"`, `"pug"`, Ø§ÙˆØ± `"hugs"` Ù…ÛŒÚº Ø¸Ø§ÛØ± ÛÙˆÙ†Û’ Ú©ÛŒ ÙˆØ¬Û Ø³Û’ frequency 20 Ù…Ù„ØªÛŒ ÛÛ’Û”

ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± Ù…Ù…Ú©Ù† subword Ú©ÛŒ frequencies Ø¯Ø±Ø¬ Ø°ÛŒÙ„ ÛÛŒÚº:

```
("h", 15), ("u", 36), ("g", 20), ("hu", 15), ("ug", 20), ("p", 17), ("pu", 17), ("n", 16),
("un", 16), ("b", 4), ("bu", 4), ("s", 5), ("hug", 15), ("gs", 5), ("ugs", 5)
```

Ú©Ù„ frequencies Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹Û 210 ÛÙˆÚ¯Ø§ØŒ Ø§ÙˆØ± subword `"ug"` Ú©ÛŒ probability ÛÙˆÚ¯ÛŒ 20/210.

<Tip>

âœï¸ **Ø§Ø¨ Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Ø±ÛŒ!** Ú©ÙˆÚˆ Ù„Ú©Ú¾ Ú©Ø± Ø§ÙˆÙ¾Ø± Ø¨ÛŒØ§Ù† Ú©Ø±Ø¯Û frequencies compute Ú©Ø±ÛŒÚº Ø§ÙˆØ± ÛŒÛ Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ù†ØªØ§Ø¦Ø¬ Ø¯Ø±Ø³Øª ÛÛŒÚº Ø§ÙˆØ± Ù…Ø¬Ù…ÙˆØ¹ÛŒ sum Ø¨Ú¾ÛŒ Ø¯Ø±Ø³Øª ÛÛ’Û”

</Tip>

Ø§Ø¨ØŒ Ú©Ø³ÛŒ word Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ… ØªÙ…Ø§Ù… Ù…Ù…Ú©Ù†Û segmentations ØªÙ„Ø§Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Unigram model Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÛØ± segmentation Ú©ÛŒ probability compute Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ú†ÙˆÙ†Ú©Û ØªÙ…Ø§Ù… tokens Ú©Ùˆ Ø¢Ø²Ø§Ø¯ Ø³Ù…Ø¬Ú¾Ø§ Ø¬Ø§ØªØ§ ÛÛ’ØŒ ÛŒÛ probability ÛØ± token Ú©ÛŒ probability Ú©Û’ Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ú©Û’ Ø¨Ø±Ø§Ø¨Ø± ÛÙˆÚ¯ÛŒÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ word `"pug"` Ú©Û’ Ù„ÛŒÛ’ segmentation `["p", "u", "g"]` Ú©ÛŒ probability ÛÙˆÚ¯ÛŒ:

$$P(["p", "u", "g"]) = P("p") \times P("u") \times P("g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

Ø¬Ø¨Ú©Û segmentation `["p", "ug"]` Ú©ÛŒ probability ÛÙˆÚ¯ÛŒ:

$$P(["p", "ug"]) = P("p") \times P("ug") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

Ø§Ø³ Ø·Ø±Ø­ØŒ ÙˆÛ segmentation Ø¬Ø³ Ù…ÛŒÚº Ú©Ù… tokens ÛÙˆÚº Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± Ø²ÛŒØ§Ø¯Û probability Ø±Ú©Ú¾ØªÛŒ ÛÛ’ (Ú©ÛŒÙˆÙ†Ú©Û ÛØ± token Ú©Û’ Ù„ÛŒÛ’ division Ø¯ÙˆÛØ±Ø§ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’) â€” Ø§ÙˆØ± ÛŒÛ ÙˆÛÛŒ ÛÛ’ Ø¬Ùˆ ÛÙ… ÙØ·Ø±ÛŒ Ø·ÙˆØ± Ù¾Ø± Ú†Ø§ÛØªÛ’ ÛÛŒÚº: Ø§ÛŒÚ© word Ú©Ùˆ Ø¬ØªÙ†Ø§ Ù…Ù…Ú©Ù† ÛÙˆ Ø§ÛŒÚ© ÛÛŒ token Ù…ÛŒÚº ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ù†Ø§Û”

Unigram model Ú©Û’ ØªØ­Øª word Ú©ÛŒ tokenization ÙˆÛ segmentation ÛÙˆÚ¯ÛŒ Ø¬Ø³ Ú©ÛŒ probability Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÙˆÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ word `"pug"` Ú©Û’ Ù„ÛŒÛ’ Ù…Ù…Ú©Ù†Û segmentation Ø§ÙˆØ± Ø§Ù† Ú©ÛŒ probabilities:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

Ù„ÛÙ°Ø°Ø§ØŒ `"pug"` Ú©Ùˆ `["p", "ug"]` ÛŒØ§ `["pu", "g"]` tokenize Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§ØŒ Ø§Ø³ Ø¨Ø§Øª Ù¾Ø± Ù…Ù†Ø­ØµØ± Ú©Û Ú©ÙˆÙ† Ø³Ø§ segmentation Ù¾ÛÙ„Û’ Ù…Ù„ØªØ§ ÛÛ’ (Ø¨Ú‘Û’ corpus Ù…ÛŒÚº Ø§ÛŒØ³Û’ equality Ú©Û’ Ù…Ø¹Ø§Ù…Ù„Û’ Ø´Ø§Ø° Ùˆ Ù†Ø§Ø¯Ø± ÛÙˆÚº Ú¯Û’)Û”

## Back to training[[back-to-training]]

Ø§Ø¨ Ø¬Ø¨Ú©Û ÛÙ… Ù†Û’ tokenization Ú©Ø§ Ø¹Ù…Ù„ Ø³Ù…Ø¬Ú¾ Ù„ÛŒØ§ ÛÛ’ØŒ ÛÙ… training Ú©Û’ Ø¯ÙˆØ±Ø§Ù† compute ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ loss Ù¾Ø± Ø¨Ú¾ÛŒ Ø¨Ø§Øª Ú©Ø±ÛŒÚº Ú¯Û’Û” Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ Ù…Ø±Ø­Ù„Û’ Ù¾Ø±ØŒ corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± word Ú©Ùˆ tokenize Ú©Ø± Ú©Û’ØŒ Ù…ÙˆØ¬ÙˆØ¯Û vocabulary Ø§ÙˆØ± Unigram model Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± loss compute Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”

ÛØ± word Ú©Ø§ Ø§ÛŒÚ© score ÛÙˆØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ù…Ø¬Ù…ÙˆØ¹ÛŒ loss ÙˆÛ negative log likelihood ÛÛ’ â€” ÛŒØ¹Ù†ÛŒ corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± word Ú©Û’ Ù„ÛŒÛ’ \\(-\log(P(word))\\) Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹ÛÛ”

Ø¢Ø¦ÛŒÚº Ø§Ù¾Ù†ÛŒ Ù¾Ú†Ú¾Ù„ÛŒ Ù…Ø«Ø§Ù„ Ù¾Ø± ÙˆØ§Ù¾Ø³ Ú†Ù„ÛŒÚº:

```
"hug": ["hug"] (score 0.071428)   (frequency 10)
"pug": ["pu", "g"] (score 0.007710)  (frequency 5)
"pun": ["pu", "n"] (score 0.006168)  (frequency 12)
"bun": ["bu", "n"] (score 0.001451)  (frequency 4)
"hugs": ["hug", "s"] (score 0.001701) (frequency 5)
```

ØªÙˆ loss ÛÙˆÚ¯ÛŒ:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Ø§Ø¨ ÛÙ…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ù†Ø§ ÛÛ’ Ú©Û ÛØ± token Ú©Ùˆ Ø­Ø°Ù Ú©Ø±Ù†Û’ Ø³Û’ loss Ù…ÛŒÚº Ú©ØªÙ†Ø§ Ø§Ø¶Ø§ÙÛ ÛÙˆØªØ§ ÛÛ’Û” ÛŒÛ Ú©Ø§Ù… Ú©Ø§ÙÛŒ Ù…Ø­Ù†Øª Ø·Ù„Ø¨ ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ ÛÙ… ÛŒÛØ§Úº ØµØ±Ù Ø¯Ùˆ tokens Ú©Û’ Ù„ÛŒÛ’ Ø¨ØªØ§ØªÛ’ ÛÛŒÚºÛ” ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û `"pug"` Ú©Û’ Ù„ÛŒÛ’ Ø¯Ùˆ equivalent tokenizations ÛÛŒÚº (Ø¬ÛŒØ³Ø§ Ú©Û `["p", "ug"]` Ø§ÙˆØ± `["pu", "g"]` Ø¯ÙˆÙ†ÙˆÚº Ú©Û’ scores Ø¨Ø±Ø§Ø¨Ø± ÛÛŒÚº) ØªÙˆ `"pu"` Ú©Ùˆ Ø­Ø°Ù Ú©Ø±Ù†Û’ Ø³Û’ loss Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ ÙØ±Ù‚ Ù†ÛÛŒÚº Ø¢Ø¦Û’ Ú¯Ø§Û”

Ø¯ÙˆØ³Ø±ÛŒ Ø·Ø±ÙØŒ Ø§Ú¯Ø± `"hug"` Ú©Ùˆ Ø­Ø°Ù Ú©Ø± Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ ØªÙˆ loss Ø¨Ú‘Ú¾Û’ Ú¯ÛŒØŒ Ú©ÛŒÙˆÙ†Ú©Û `"hug"` Ø§ÙˆØ± `"hugs"` Ú©ÛŒ tokenization ÛŒÙˆÚº ÛÙˆ Ø¬Ø§Ø¦Û’ Ú¯ÛŒ:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Ø§Ù† ØªØ¨Ø¯ÛŒÙ„ÛŒÙˆÚº Ø³Û’ loss Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ ÛÙˆÚ¯Ø§:

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

Ø§Ø³ Ù„ÛŒÛ’ Ø§Ù…Ú©Ø§Ù† ÛÛ’ Ú©Û vocabulary Ø³Û’ `"pu"` Ú©Ùˆ ÛÙ¹Ø§ Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ØŒ Ù…Ú¯Ø± `"hug"` Ú©Ùˆ Ù†ÛÛŒÚºÛ”

## Implementing Unigram[[implementing-unigram]]

Ø§Ø¨ ÛÙ… Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©ÛŒ Ù¾ÙˆØ±ÛŒ implementation Ú©Ùˆ code Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚºÛ” Ø¬ÛŒØ³Ø§ Ú©Û BPE Ø§ÙˆØ± WordPiece Ú©Û’ Ù„ÛŒÛ’ØŒ ÛŒÛ implementation ØªØ¹Ù„ÛŒÙ…ÛŒ Ù…Ù‚ØµØ¯ Ú©Û’ Ù„ÛŒÛ’ ÛÛ’ Ø§ÙˆØ± Ø¨Ú‘Û’ corpus Ù¾Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ²ÙˆÚº Ù†ÛÛŒÚº ÛÙˆÚ¯ÛŒØŒ Ù…Ú¯Ø± Ø§Ø³ Ø³Û’ Ø¢Ù¾ Ú©Ùˆ Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©ÛŒ Ø³Ù…Ø¬Ú¾ Ø¨ÛØªØ± ÛÙˆÚ¯ÛŒÛ”

ÛÙ… ÙˆÛÛŒ corpus Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Ø§Ø³ Ø¨Ø§Ø± ÛÙ… `xlnet-base-cased` Ù…Ø§ÚˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

BPE Ø§ÙˆØ± WordPiece Ú©ÛŒ Ø·Ø±Ø­ØŒ Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ corpus Ù…ÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛØ± word Ú©ÛŒ occurrences count Ú©Ø±ÛŒÚº Ú¯Û’:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Ø§Ø¨ ÛÙ…ÛŒÚº Ø§Ù¾Ù†ÛŒ vocabulary initialize Ú©Ø±Ù†ÛŒ ÛÛ’ Ø¬Ùˆ Ø¢Ø®Ø±ÛŒ vocabulary size Ø³Û’ Ø¨Ú‘ÛŒ ÛÙˆÛ” ÛÙ…ÛŒÚº ØªÙ…Ø§Ù… Ø¨Ù†ÛŒØ§Ø¯ÛŒ characters Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Û’ ÛÛŒÚº (ØªØ§Ú©Û ÛØ± word tokenize ÛÙˆ Ø³Ú©Û’) Ø§ÙˆØ± Ø¨Ú‘Û’ substrings Ú©Ùˆ Ø§Ù† Ú©ÛŒ frequency Ú©Û’ Ø­Ø³Ø§Ø¨ Ø³Û’ sort Ú©Ø±Ù†Ø§ ÛÛ’:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

ÛÙ… characters Ø§ÙˆØ± Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û frequent subwords Ú©Ùˆ Ù…Ù„Ø§ Ú©Ø± Ø§ÛŒÚ© initial vocabulary ØªÛŒØ§Ø± Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ÙØ±Ø¶ Ú©Ø±ÛŒÚº Ú©Û ÛÙ… vocabulary size 300 Ú†Ø§ÛØªÛ’ ÛÛŒÚº:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>
ğŸ’¡ SentencePiece Ø§ÛŒÚ© Ø²ÛŒØ§Ø¯Û Ù…Ø¤Ø«Ø± Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Enhanced Suffix Array (ESA) Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ ØªØ§Ú©Û Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ vocabulary ØªÛŒØ§Ø± Ú©ÛŒ Ø¬Ø§ Ø³Ú©Û’Û”
</Tip>

Ø§Ø¨ØŒ ØªÙ…Ø§Ù… frequencies Ú©Ø§ Ù…Ø¬Ù…ÙˆØ¹Û compute Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ù†ÛÛŒÚº probabilities Ù…ÛŒÚº convert Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ logarithm Ù„ÛŒÚº (Ú©ÛŒÙˆÙ†Ú©Û Ú†Ú¾ÙˆÙ¹Û’ numbers Ú©Ø§ Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ú©Ø±Ù†Û’ Ú©Û’ Ø¨Ø¬Ø§Ø¦Û’ logarithms Ú©Ùˆ Ø¬Ù…Ø¹ Ú©Ø±Ù†Ø§ numerical Ø·ÙˆØ± Ù¾Ø± Ø²ÛŒØ§Ø¯Û Ù…Ø³ØªØ­Ú©Ù… ÛÛ’):

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Ø§Ø¨ Viterbi algorithm Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©Ø³ÛŒ word Ú©Ùˆ tokenize Ú©Ø±ÛŒÚºÛ” ÛŒÛ algorithm word Ú©Û’ ÛØ± substring Ú©Û’ Ù„ÛŒÛ’ best segmentation ØªÙ„Ø§Ø´ Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø§Ø³Û’ `best_segmentations` Ù…ÛŒÚº store Ú©Ø±ØªØ§ ÛÛ’Û” ÛØ± position (0 Ø³Û’ word Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ©) Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© dictionary ÛÙˆÚ¯ÛŒ Ø¬Ø³ Ù…ÛŒÚº Ø¯Ùˆ keys ÛÙˆÚº Ú¯ÛŒ: Ø¢Ø®Ø±ÛŒ token Ú©ÛŒ Ø´Ø±ÙˆØ¹Ø§Øª Ú©Ø§ index Ø§ÙˆØ± best segmentation Ú©Ø§ scoreÛ” Ø§Ø³ index Ú©ÛŒ Ù…Ø¯Ø¯ Ø³Û’ ÛÙ… Ù¾ÙˆØ±Ø§ segmentation Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’Û”

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}
    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        return ["<unk>"], None
    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

ÛÙ… Ø§Ù¾Ù†Û’ initial model Ú©Ùˆ Ú©Ú†Ú¾ Ø§Ù„ÙØ§Ø¸ Ù¾Ø± Ø¢Ø²Ù…Ø§ Ú©Ø± Ø¯ÛŒÚ©Ú¾ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Ø§Ø¨ corpus Ù¾Ø± model Ú©Ø§ loss compute Ú©Ø±ÛŒÚº:

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±:

```python
compute_loss(model)
```

```python
413.10377642940875
```

Ø§Ø¨ ÛØ± token Ú©Ùˆ Ø­Ø°Ù Ú©Ø±Ù†Û’ Ø³Û’ loss Ù…ÛŒÚº Ú©ÛŒØ§ ÙØ±Ù‚ Ø¢ØªØ§ ÛÛ’ØŒ ÛŒÛ compute Ú©Ø±ÛŒÚº:

```python
import copy

def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

Ù…Ø«Ù„Ø§Ù‹:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

```python
6.376412403623874
0.0
```

<Tip>
ğŸ’¡ ÛŒÛ Ø·Ø±ÛŒÙ‚Û Ú©Ø§ÙÛŒ ØºÛŒØ± Ù…Ø¤Ø«Ø± ÛÛ’ØŒ Ù„ÛÙ°Ø°Ø§ SentencePiece loss Ú©Ùˆ approximate Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø§Ù†Ø¯Ø§Ø²Û Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’Û”
</Tip>

Ø§Ø¨ Ø¢Ø®Ø±ÛŒ Ù…Ø±Ø­Ù„Û ÛŒÛ ÛÛ’ Ú©Û special tokens Ú©Ùˆ vocabulary Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚºØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± loop Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ vocabulary Ú©Ùˆ prune Ú©Ø±ÛŒÚº Ø¬Ø¨ ØªÚ© Ú©Û Ù…Ø·Ù„ÙˆØ¨Û size Ù†Û ÛÙˆ Ø¬Ø§Ø¦Û’:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])
    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Ø¢Ø®Ø± Ù…ÛŒÚºØŒ text Ú©Ùˆ tokenize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛÙ„Û’ pre-tokenize Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ù¾Ú¾Ø± `encode_word()` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])

tokenize("This is the Hugging Face course.", model)
```

```python
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

ÛŒÛÛŒ ÛÛ’ Unigram Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ú©Ø§ Ø¬Ø§Ø¦Ø²Û! Ø§Ù…ÛŒØ¯ ÛÛ’ Ú©Û Ø§Ø¨ Ø¢Ù¾ ØªÙ…Ø§Ù… tokenizers Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ù…Ø§ÛØ± Ù…Ø­Ø³ÙˆØ³ Ú©Ø± Ø±ÛÛ’ ÛÙˆÚº Ú¯Û’Û” Ø§Ú¯Ù„Û’ Ø­ØµÛ’ Ù…ÛŒÚº ÛÙ… ğŸ¤— Tokenizers Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ú©Û’ building blocks Ú©Ùˆ explore Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø§ÙˆØ± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ú©Ø³ Ø·Ø±Ø­ Ø¢Ù¾ Ø§Ù¾Ù†ÛŒ Ù…Ø±Ø¶ÛŒ Ú©Ø§ tokenizer Ø¨Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”