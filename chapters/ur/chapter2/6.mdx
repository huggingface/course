<FrameworkSwitchCourse {fw} />

# سب کچھ اکٹھا کرنا[[putting-it-all-together]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
]} />

{/if}

پچھلے چند سیکشنز میں، ہم نے دستی طور پر زیادہ تر کام کرنے کی کوشش کی ہے۔ ہم نے اس بات کا جائزہ لیا کہ ٹوکنائزرز کیسے کام کرتے ہیں اور ٹوکنائزیشن، input IDs میں تبدیلی، padding، truncation اور attention masks کا جائزہ لیا۔

تاہم، جیسا کہ ہم نے سیکشن 2 میں دیکھا، 🤗 Transformers API ہمارے لیے ان سب کو ایک اعلی سطحی فنکشن کے ذریعے ہینڈل کر سکتی ہے جس میں ہم یہاں تفصیل سے داخل ہوں گے۔ جب آپ اپنے `tokenizer` کو براہ راست جملے پر کال کرتے ہیں، تو آپ کو ایسے inputs ملتے ہیں جو ماڈل کو پاس کرنے کے لیے تیار ہوتے ہیں:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

یہاں، `model_inputs` ویری ایبل میں وہ سب کچھ شامل ہے جو ماڈل کے درست طریقے سے کام کرنے کے لیے ضروری ہے۔ DistilBERT کے لیے، اس میں input IDs کے ساتھ ساتھ attention mask بھی شامل ہوتا ہے۔ دیگر ماڈلز جو اضافی inputs قبول کرتے ہیں، ان کے لیے بھی وہی چیز tokenizer کی طرف سے آجاتی ہے۔

نیچے دی گئی کچھ مثالوں میں ہم دیکھیں گے کہ یہ طریقہ کتنا طاقتور ہے۔ سب سے پہلے، یہ ایک واحد تسلسل کو ٹوکنائز کر سکتا ہے:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

یہ ایک ساتھ متعدد تسلسلوں کو بھی ہینڈل کرتا ہے، بغیر API میں کسی تبدیلی کے:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

یہ مختلف اہداف کے مطابق padding بھی کر سکتا ہے:

```py
# تسلسلوں کو ان کی زیادہ سے زیادہ لمبائی تک padding کرے گا
model_inputs = tokenizer(sequences, padding="longest")

# تسلسلوں کو ماڈل کی زیادہ سے زیادہ لمبائی تک padding کرے گا
# (BERT یا DistilBERT کے لیے 512)
model_inputs = tokenizer(sequences, padding="max_length")

# تسلسلوں کو مخصوص زیادہ سے زیادہ لمبائی تک padding کرے گا
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

یہ تسلسلوں کو truncate بھی کر سکتا ہے:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# ان تسلسلوں کو truncate کرے گا جو ماڈل کی زیادہ سے زیادہ لمبائی (مثلاً 512) سے زیادہ ہوں
model_inputs = tokenizer(sequences, truncation=True)

# ان تسلسلوں کو truncate کرے گا جو مخصوص زیادہ سے زیادہ لمبائی سے زیادہ ہوں
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` آبجیکٹ مخصوص framework tensors میں تبدیل کرنے کو بھی ہینڈل کر سکتا ہے، جنہیں پھر براہ راست ماڈل کو بھیجا جا سکتا ہے۔ مثال کے طور پر، نیچے دیے گئے کوڈ نمونے میں ہم tokenizer کو مختلف frameworks کے tensors واپس کرنے کے لیے کہہ رہے ہیں — `"pt"` PyTorch tensors دیتا ہے، `"tf"` TensorFlow tensors دیتا ہے، اور `"np"` NumPy arrays دیتا ہے:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch tensors واپس کرتا ہے
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# TensorFlow tensors واپس کرتا ہے
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# NumPy arrays واپس کرتا ہے
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## خاص ٹوکنز[[special-tokens]]

اگر ہم tokenizer کی طرف سے واپس کیے گئے input IDs کو دیکھیں تو ہمیں پتا چلے گا کہ وہ پہلے والی مثالوں سے تھوڑے مختلف ہیں:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

ایک token ID ابتدا میں اور ایک آخر میں شامل کیا گیا ہے۔ آئیے اوپر دی گئی دونوں ID کی فہرستوں کو decode کر کے دیکھتے ہیں کہ اس کا کیا مطلب ہے:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

ٹوکنائزر نے آغاز میں خاص لفظ `[CLS]` اور آخر میں خاص لفظ `[SEP]` شامل کیا۔ ایسا اس لیے ہے کیونکہ ماڈل کی پری ٹریننگ کے دوران یہ الفاظ استعمال ہوئے تھے، لہٰذا inference کے لیے وہی نتائج حاصل کرنے کے لیے ہمیں انہیں شامل کرنا ضروری ہے۔ نوٹ کریں کہ کچھ ماڈلز خاص الفاظ شامل نہیں کرتے یا مختلف شامل کرتے ہیں؛ بعض ماڈلز صرف آغاز میں یا صرف آخر میں یہ الفاظ شامل کرتے ہیں۔ بہرحال، tokenizer کو معلوم ہوتا ہے کہ کون سے الفاظ متوقع ہیں اور وہ خود بخود ان کا خیال رکھتا ہے۔

## اختتام: tokenizer سے ماڈل تک[[wrapping-up-from-tokenizer-to-model]]

اب جبکہ ہم نے انفرادی مراحل کو دیکھ لیا ہے جو `tokenizer` آبجیکٹ متن پر لگاتا ہے، آئیں آخری بار دیکھیں کہ یہ کس طرح متعدد تسلسلوں (padding!)، بہت لمبے تسلسل (truncation!)، اور مختلف قسم کے tensors کو اپنی مین API کے ذریعے ہینڈل کر سکتا ہے:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}