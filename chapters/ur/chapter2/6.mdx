<FrameworkSwitchCourse {fw} />

# Ø³Ø¨ Ú©Ú†Ú¾ Ø§Ú©Ù¹Ú¾Ø§ Ú©Ø±Ù†Ø§[[putting-it-all-together]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
]} />

{/if}

Ù¾Ú†Ú¾Ù„Û’ Ú†Ù†Ø¯ Ø³ÛŒÚ©Ø´Ù†Ø² Ù…ÛŒÚºØŒ ÛÙ… Ù†Û’ Ø¯Ø³ØªÛŒ Ø·ÙˆØ± Ù¾Ø± Ø²ÛŒØ§Ø¯Û ØªØ± Ú©Ø§Ù… Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©ÛŒ ÛÛ’Û” ÛÙ… Ù†Û’ Ø§Ø³ Ø¨Ø§Øª Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒØ§ Ú©Û Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø±Ø² Ú©ÛŒØ³Û’ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù†ØŒ input IDs Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ÛŒØŒ paddingØŒ truncation Ø§ÙˆØ± attention masks Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒØ§Û”

ØªØ§ÛÙ…ØŒ Ø¬ÛŒØ³Ø§ Ú©Û ÛÙ… Ù†Û’ Ø³ÛŒÚ©Ø´Ù† 2 Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾Ø§ØŒ ğŸ¤— Transformers API ÛÙ…Ø§Ø±Û’ Ù„ÛŒÛ’ Ø§Ù† Ø³Ø¨ Ú©Ùˆ Ø§ÛŒÚ© Ø§Ø¹Ù„ÛŒ Ø³Ø·Ø­ÛŒ ÙÙ†Ú©Ø´Ù† Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÛÛŒÙ†ÚˆÙ„ Ú©Ø± Ø³Ú©ØªÛŒ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº ÛÙ… ÛŒÛØ§Úº ØªÙØµÛŒÙ„ Ø³Û’ Ø¯Ø§Ø®Ù„ ÛÙˆÚº Ú¯Û’Û” Ø¬Ø¨ Ø¢Ù¾ Ø§Ù¾Ù†Û’ `tokenizer` Ú©Ùˆ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ø¬Ù…Ù„Û’ Ù¾Ø± Ú©Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ØªÙˆ Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒØ³Û’ inputs Ù…Ù„ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ù¾Ø§Ø³ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± ÛÙˆØªÛ’ ÛÛŒÚº:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ÛŒÛØ§ÚºØŒ `model_inputs` ÙˆÛŒØ±ÛŒ Ø§ÛŒØ¨Ù„ Ù…ÛŒÚº ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ø´Ø§Ù…Ù„ ÛÛ’ Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©Û’ Ø¯Ø±Ø³Øª Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ú©Ø§Ù… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’Û” DistilBERT Ú©Û’ Ù„ÛŒÛ’ØŒ Ø§Ø³ Ù…ÛŒÚº input IDs Ú©Û’ Ø³Ø§ØªÚ¾ Ø³Ø§ØªÚ¾ attention mask Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ ÛÙˆØªØ§ ÛÛ’Û” Ø¯ÛŒÚ¯Ø± Ù…Ø§ÚˆÙ„Ø² Ø¬Ùˆ Ø§Ø¶Ø§ÙÛŒ inputs Ù‚Ø¨ÙˆÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§Ù† Ú©Û’ Ù„ÛŒÛ’ Ø¨Ú¾ÛŒ ÙˆÛÛŒ Ú†ÛŒØ² tokenizer Ú©ÛŒ Ø·Ø±Ù Ø³Û’ Ø¢Ø¬Ø§ØªÛŒ ÛÛ’Û”

Ù†ÛŒÚ†Û’ Ø¯ÛŒ Ú¯Ø¦ÛŒ Ú©Ú†Ú¾ Ù…Ø«Ø§Ù„ÙˆÚº Ù…ÛŒÚº ÛÙ… Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û ÛŒÛ Ø·Ø±ÛŒÙ‚Û Ú©ØªÙ†Ø§ Ø·Ø§Ù‚ØªÙˆØ± ÛÛ’Û” Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛŒÛ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ ØªØ³Ù„Ø³Ù„ Ú©Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø² Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ÛŒÛ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ù…ØªØ¹Ø¯Ø¯ ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ Ø¨Ú¾ÛŒ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø¨ØºÛŒØ± API Ù…ÛŒÚº Ú©Ø³ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Û’:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

ÛŒÛ Ù…Ø®ØªÙ„Ù Ø§ÛØ¯Ø§Ù Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ padding Ø¨Ú¾ÛŒ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:

```py
# ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ Ø§Ù† Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© padding Ú©Ø±Û’ Ú¯Ø§
model_inputs = tokenizer(sequences, padding="longest")

# ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© padding Ú©Ø±Û’ Ú¯Ø§
# (BERT ÛŒØ§ DistilBERT Ú©Û’ Ù„ÛŒÛ’ 512)
model_inputs = tokenizer(sequences, padding="max_length")

# ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ Ù…Ø®ØµÙˆØµ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ ØªÚ© padding Ú©Ø±Û’ Ú¯Ø§
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

ÛŒÛ ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ truncate Ø¨Ú¾ÛŒ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Ø§Ù† ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ truncate Ú©Ø±Û’ Ú¯Ø§ Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ (Ù…Ø«Ù„Ø§Ù‹ 512) Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÙˆÚº
model_inputs = tokenizer(sequences, truncation=True)

# Ø§Ù† ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ truncate Ú©Ø±Û’ Ú¯Ø§ Ø¬Ùˆ Ù…Ø®ØµÙˆØµ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÙˆÚº
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ù…Ø®ØµÙˆØµ framework tensors Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú©Ùˆ Ø¨Ú¾ÛŒ ÛÛŒÙ†ÚˆÙ„ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø¬Ù†ÛÛŒÚº Ù¾Ú¾Ø± Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø¨Ú¾ÛŒØ¬Ø§ Ø¬Ø§ Ø³Ú©ØªØ§ ÛÛ’Û” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ù†ÛŒÚ†Û’ Ø¯ÛŒÛ’ Ú¯Ø¦Û’ Ú©ÙˆÚˆ Ù†Ù…ÙˆÙ†Û’ Ù…ÛŒÚº ÛÙ… tokenizer Ú©Ùˆ Ù…Ø®ØªÙ„Ù frameworks Ú©Û’ tensors ÙˆØ§Ù¾Ø³ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛÛ Ø±ÛÛ’ ÛÛŒÚº â€” `"pt"` PyTorch tensors Ø¯ÛŒØªØ§ ÛÛ’ØŒ `"tf"` TensorFlow tensors Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± `"np"` NumPy arrays Ø¯ÛŒØªØ§ ÛÛ’:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch tensors ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# TensorFlow tensors ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# NumPy arrays ÙˆØ§Ù¾Ø³ Ú©Ø±ØªØ§ ÛÛ’
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Ø®Ø§Øµ Ù¹ÙˆÚ©Ù†Ø²[[special-tokens]]

Ø§Ú¯Ø± ÛÙ… tokenizer Ú©ÛŒ Ø·Ø±Ù Ø³Û’ ÙˆØ§Ù¾Ø³ Ú©ÛŒÛ’ Ú¯Ø¦Û’ input IDs Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ÛŒÚº ØªÙˆ ÛÙ…ÛŒÚº Ù¾ØªØ§ Ú†Ù„Û’ Ú¯Ø§ Ú©Û ÙˆÛ Ù¾ÛÙ„Û’ ÙˆØ§Ù„ÛŒ Ù…Ø«Ø§Ù„ÙˆÚº Ø³Û’ ØªÚ¾ÙˆÚ‘Û’ Ù…Ø®ØªÙ„Ù ÛÛŒÚº:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

Ø§ÛŒÚ© token ID Ø§Ø¨ØªØ¯Ø§ Ù…ÛŒÚº Ø§ÙˆØ± Ø§ÛŒÚ© Ø¢Ø®Ø± Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø¢Ø¦ÛŒÛ’ Ø§ÙˆÙ¾Ø± Ø¯ÛŒ Ú¯Ø¦ÛŒ Ø¯ÙˆÙ†ÙˆÚº ID Ú©ÛŒ ÙÛØ±Ø³ØªÙˆÚº Ú©Ùˆ decode Ú©Ø± Ú©Û’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û Ø§Ø³ Ú©Ø§ Ú©ÛŒØ§ Ù…Ø·Ù„Ø¨ ÛÛ’:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù†Û’ Ø¢ØºØ§Ø² Ù…ÛŒÚº Ø®Ø§Øµ Ù„ÙØ¸ `[CLS]` Ø§ÙˆØ± Ø¢Ø®Ø± Ù…ÛŒÚº Ø®Ø§Øµ Ù„ÙØ¸ `[SEP]` Ø´Ø§Ù…Ù„ Ú©ÛŒØ§Û” Ø§ÛŒØ³Ø§ Ø§Ø³ Ù„ÛŒÛ’ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ù¾Ø±ÛŒ Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Û’ Ø¯ÙˆØ±Ø§Ù† ÛŒÛ Ø§Ù„ÙØ§Ø¸ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆØ¦Û’ ØªÚ¾Û’ØŒ Ù„ÛÙ°Ø°Ø§ inference Ú©Û’ Ù„ÛŒÛ’ ÙˆÛÛŒ Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÛÙ…ÛŒÚº Ø§Ù†ÛÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Ø§ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’Û” Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û Ú©Ú†Ú¾ Ù…Ø§ÚˆÙ„Ø² Ø®Ø§Øµ Ø§Ù„ÙØ§Ø¸ Ø´Ø§Ù…Ù„ Ù†ÛÛŒÚº Ú©Ø±ØªÛ’ ÛŒØ§ Ù…Ø®ØªÙ„Ù Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºØ› Ø¨Ø¹Ø¶ Ù…Ø§ÚˆÙ„Ø² ØµØ±Ù Ø¢ØºØ§Ø² Ù…ÛŒÚº ÛŒØ§ ØµØ±Ù Ø¢Ø®Ø± Ù…ÛŒÚº ÛŒÛ Ø§Ù„ÙØ§Ø¸ Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” Ø¨ÛØ±Ø­Ø§Ù„ØŒ tokenizer Ú©Ùˆ Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØªØ§ ÛÛ’ Ú©Û Ú©ÙˆÙ† Ø³Û’ Ø§Ù„ÙØ§Ø¸ Ù…ØªÙˆÙ‚Ø¹ ÛÛŒÚº Ø§ÙˆØ± ÙˆÛ Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ Ø§Ù† Ú©Ø§ Ø®ÛŒØ§Ù„ Ø±Ú©Ú¾ØªØ§ ÛÛ’Û”

## Ø§Ø®ØªØªØ§Ù…: tokenizer Ø³Û’ Ù…Ø§ÚˆÙ„ ØªÚ©[[wrapping-up-from-tokenizer-to-model]]

Ø§Ø¨ Ø¬Ø¨Ú©Û ÛÙ… Ù†Û’ Ø§Ù†ÙØ±Ø§Ø¯ÛŒ Ù…Ø±Ø§Ø­Ù„ Ú©Ùˆ Ø¯ÛŒÚ©Ú¾ Ù„ÛŒØ§ ÛÛ’ Ø¬Ùˆ `tokenizer` Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ù…ØªÙ† Ù¾Ø± Ù„Ú¯Ø§ØªØ§ ÛÛ’ØŒ Ø¢Ø¦ÛŒÚº Ø¢Ø®Ø±ÛŒ Ø¨Ø§Ø± Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú©Û ÛŒÛ Ú©Ø³ Ø·Ø±Ø­ Ù…ØªØ¹Ø¯Ø¯ ØªØ³Ù„Ø³Ù„ÙˆÚº (padding!)ØŒ Ø¨ÛØª Ù„Ù…Ø¨Û’ ØªØ³Ù„Ø³Ù„ (truncation!)ØŒ Ø§ÙˆØ± Ù…Ø®ØªÙ„Ù Ù‚Ø³Ù… Ú©Û’ tensors Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ù…ÛŒÙ† API Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ÛÛŒÙ†ÚˆÙ„ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}