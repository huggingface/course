<FrameworkSwitchCourse {fw} />

# ماڈلز[[models]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
اس حصے میں ہم ایک ماڈل بنانے اور اسے استعمال کرنے کے طریقے پر تفصیلی نظر ڈالیں گے۔ ہم `AutoModel` کلاس کا استعمال کریں گے، جو کہ کسی بھی ماڈل کو ایک checkpoint سے انسٹینشیئٹ کرنے کے لیے نہایت مفید ہے۔

`AutoModel` کلاس اور اس کے تمام متعلقین دراصل لائبریری میں دستیاب مختلف ماڈلز پر سادہ wrappers ہیں۔ یہ ایک ذہین wrapper ہے کیونکہ یہ خود بخود آپ کے checkpoint کے لیے مناسب ماڈل آرکیٹیکچر کا اندازہ لگا لیتا ہے، اور پھر اسی آرکیٹیکچر کے ساتھ ماڈل کو انسٹینشیئٹ کر دیتا ہے۔

{:else}
اس حصے میں ہم ایک ماڈل بنانے اور اسے استعمال کرنے کے طریقے پر تفصیلی نظر ڈالیں گے۔ ہم `TFAutoModel` کلاس کا استعمال کریں گے، جو کہ کسی بھی ماڈل کو ایک checkpoint سے انسٹینشیئٹ کرنے کے لیے نہایت مفید ہے۔

`TFAutoModel` کلاس اور اس کے تمام متعلقین دراصل لائبریری میں دستیاب مختلف ماڈلز پر سادہ wrappers ہیں۔ یہ ایک ذہین wrapper ہے کیونکہ یہ خود بخود آپ کے checkpoint کے لیے مناسب ماڈل آرکیٹیکچر کا اندازہ لگا لیتا ہے، اور پھر اسی آرکیٹیکچر کے ساتھ ماڈل کو انسٹینشیئٹ کر دیتا ہے۔

{/if}

تاہم، اگر آپ کو معلوم ہے کہ آپ کس قسم کے ماڈل کا استعمال کرنا چاہتے ہیں، تو آپ براہ راست وہ کلاس استعمال کر سکتے ہیں جو اس کی آرکیٹیکچر کو متعین کرتی ہے۔ آئیے دیکھتے ہیں کہ یہ BERT ماڈل کے ساتھ کیسے کام کرتا ہے۔

## ایک ٹرانسفارمر تخلیق کرنا[[creating-a-transformer]]

BERT ماڈل کو initialize کرنے کے لیے سب سے پہلے ہمیں ایک configuration object لوڈ کرنا ہوگا:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```
{/if}

Configuration میں بہت سی ایسی خصوصیات شامل ہوتی ہیں جو ماڈل بنانے کے لیے استعمال ہوتی ہیں:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

اگرچہ آپ نے ابھی تک یہ نہیں دیکھا کہ ان تمام خصوصیات کا کیا کردار ہے، لیکن آپ کو ان میں سے کچھ کی پہچان ہو جانی چاہیے: `hidden_size` خصوصیت `hidden_states` ویکٹر کا سائز متعین کرتی ہے، اور `num_hidden_layers` ٹرانسفارمر ماڈل کی لیئرز کی تعداد کو ظاہر کرتی ہے۔

### مختلف لوڈنگ کے طریقے[[different-loading-methods]]

ڈیفالٹ configuration سے ماڈل تخلیق کرنے پر اسے random اقدار کے ساتھ initialize کیا جاتا ہے:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}

ماڈل کو اس حالت میں استعمال کیا جا سکتا ہے، لیکن یہ بے معنی نتائج دے گا؛ اسے پہلے ٹرین کرنا ضروری ہے۔ ہم ماڈل کو نئے سرے سے بھی ٹرین کر سکتے ہیں، لیکن جیسا کہ آپ نے [Chapter 1](/course/chapter1) میں دیکھا، اس کے لیے کافی وقت اور بہت سا ڈیٹا درکار ہوگا، اور اس کا ماحولیاتی اثر بھی غیر معمولی ہوگا۔ غیر ضروری اور دہرائے گئے کام سے بچنے کے لیے، پہلے سے تربیت شدہ ماڈلز کو شیئر اور دوبارہ استعمال کرنا بہت اہم ہے۔

پہلے سے تربیت شدہ Transformer ماڈل کو لوڈ کرنا نہایت آسان ہے — ہم یہ `from_pretrained()` میتھڈ کا استعمال کرتے ہوئے کر سکتے ہیں:

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

جیسا کہ آپ نے پہلے دیکھا، ہم `BertModel` کو متبادل `AutoModel` کلاس سے بدل سکتے ہیں۔ ہم اب سے یہی طریقہ استعمال کریں گے کیونکہ اس سے checkpoint-agnostic کوڈ تیار ہوتا ہے؛ اگر آپ کا کوڈ ایک checkpoint پر کام کرتا ہے، تو یہ بغیر کسی رکاوٹ کے دوسرے پر بھی کام کرے گا۔ یہ اس صورت بھی لاگو ہوتا ہے اگر آرکیٹیکچر مختلف ہو، بشرطیکہ checkpoint کو کسی مشابہہ ٹاسک (مثلاً، sentiment analysis task) کے لیے تربیت دی گئی ہو۔

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

جیسا کہ آپ نے پہلے دیکھا، ہم `TFBertModel` کو متبادل `TFAutoModel` کلاس سے بدل سکتے ہیں۔ ہم اب سے یہی طریقہ استعمال کریں گے کیونکہ اس سے checkpoint-agnostic کوڈ تیار ہوتا ہے؛ اگر آپ کا کوڈ ایک checkpoint پر کام کرتا ہے، تو یہ بغیر کسی رکاوٹ کے دوسرے پر بھی کام کرے گا۔ یہ اس صورت بھی لاگو ہوتا ہے اگر آرکیٹیکچر مختلف ہو، بشرطیکہ checkpoint کو کسی مشابہہ ٹاسک (مثلاً، sentiment analysis task) کے لیے تربیت دی گئی ہو۔

{/if}

اوپر دیے گئے کوڈ نمونے میں ہم نے `BertConfig` کا استعمال نہیں کیا، بلکہ `bert-base-cased` identifier کے ذریعے ایک pretrained ماڈل لوڈ کیا۔ یہ ایک ایسا ماڈل checkpoint ہے جس کی تربیت BERT کے مصنفین نے خود کی تھی؛ آپ اس کی مزید تفصیلات اس کے [model card](https://huggingface.co/bert-base-cased) میں دیکھ سکتے ہیں.

یہ ماڈل اب checkpoint کے تمام weights کے ساتھ initialize ہو چکا ہے۔ اسے براہ راست ان ٹاسکس پر inference کے لیے استعمال کیا جا سکتا ہے جن پر اس کی تربیت کی گئی تھی، اور اسے نئے ٹاسک پر fine-tune بھی کیا جا سکتا ہے۔ نئے سرے سے تربیت دینے کے بجائے pretrained weights کے ساتھ تربیت دے کر ہم تیزی سے اچھے نتائج حاصل کر سکتے ہیں.

Weights کو ڈاؤن لوڈ کر کے cache کر لیا گیا ہے (تاکہ مستقبل میں `from_pretrained()` میتھڈ کے کالز میں انہیں دوبارہ ڈاؤن لوڈ نہ کیا جائے) cache فولڈر میں، جو ڈیفالٹ طور پر *~/.cache/huggingface/transformers* ہے. آپ `HF_HOME` environment variable سیٹ کر کے اپنے cache فولڈر کو اپنی مرضی کے مطابق بنا سکتے ہیں.

ماڈل کو لوڈ کرنے کے لیے استعمال ہونے والا identifier کسی بھی ماڈل کا identifier ہو سکتا ہے جو Model Hub پر دستیاب ہو، بشرطیکہ وہ BERT آرکیٹیکچر کے ساتھ مطابقت رکھتا ہو. دستیاب تمام BERT checkpoints کی فہرست [یہاں](https://huggingface.co/models?filter=bert) مل سکتی ہے.

### سیونگ کے طریقے[[saving-methods]]

ماڈل کو سیو کرنا اسے لوڈ کرنے کے برابر آسان ہے — ہم `save_pretrained()` میتھڈ کا استعمال کرتے ہیں، جو کہ `from_pretrained()` میتھڈ کے مترادف ہے:

```py
model.save_pretrained("directory_on_my_computer")
```

یہ آپ کی ڈسک پر دو فائلیں سیو کرتا ہے:

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

اگر آپ *config.json* فائل کو دیکھیں تو آپ کو وہ خصوصیات معلوم ہوں گی جو ماڈل کی آرکیٹیکچر بنانے کے لیے ضروری ہیں. یہ فائل کچھ metadata بھی رکھتی ہے، جیسے کہ checkpoint کہاں سے آیا اور آخری بار checkpoint کو سیو کرتے وقت آپ نے کون سا 🤗 Transformers ورژن استعمال کیا تھا.

{#if fw === 'pt'}
*pytorch_model.bin* فائل کو *state dictionary* کہا جاتا ہے؛ اس میں آپ کے ماڈل کے تمام weights شامل ہوتے ہیں. یہ دونوں فائلیں ایک دوسرے کے ساتھ مربوط ہوتی ہیں؛ configuration آپ کے ماڈل کی آرکیٹیکچر جاننے کے لیے ضروری ہے، جبکہ ماڈل کے weights آپ کے ماڈل کے parameters ہیں.

{:else}
*tf_model.h5* فائل کو *state dictionary* کہا جاتا ہے؛ اس میں آپ کے ماڈل کے تمام weights شامل ہوتے ہیں. یہ دونوں فائلیں ایک دوسرے کے ساتھ مربوط ہوتی ہیں؛ configuration آپ کے ماڈل کی آرکیٹیکچر جاننے کے لیے ضروری ہے، جبکہ ماڈل کے weights آپ کے ماڈل کے parameters ہیں.

{/if}

## inference کے لیے Transformer ماڈل کا استعمال[[using-a-transformer-model-for-inference]]

اب جبکہ آپ کو ماڈل کو لوڈ اور سیو کرنے کا طریقہ معلوم ہو گیا ہے، آئیے اسے کچھ پیش گوئیاں کرنے کے لیے استعمال کرتے ہیں. Transformer ماڈلز صرف اعداد کو ہی پروسیس کر سکتے ہیں — وہ اعداد جو tokenizer پیدا کرتا ہے. لیکن tokenizer پر بات کرنے سے پہلے، آئیے دیکھتے ہیں کہ ماڈل کون سے inputs قبول کرتا ہے.

Tokenizers مناسب framework کے tensors میں inputs کو cast کرنے کا خیال رکھتے ہیں، لیکن آپ کو سمجھنے میں مدد کے لیے، ہم ایک مختصر نظر ڈالیں گے کہ ماڈل کو inputs بھیجنے سے پہلے کیا کرنا ضروری ہے.

فرض کریں ہمارے پاس چند sequences ہیں:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

Tokenizer ان کو vocabulary indices میں تبدیل کرتا ہے جنہیں عموماً *input IDs* کہا جاتا ہے. اب ہر sequence نمبروں کی ایک فہرست بن چکی ہے! نتیجے کے طور پر output یہ ہے:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

یہ encoded sequences کی فہرست ہے: فہرستوں کی ایک فہرست. Tensors صرف rectangular shapes قبول کرتے ہیں (مثلاً matrices). چونکہ یہ "array" پہلے ہی rectangular shape کا ہے، لہٰذا اسے tensor میں تبدیل کرنا آسان ہے:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### ماڈل میں inputs کے طور پر tensors کا استعمال[[using-the-tensors-as-inputs-to-the-model]]

ماڈل کے ساتھ tensors کا استعمال انتہائی آسان ہے — ہم صرف ماڈل کو inputs کے ساتھ کال کرتے ہیں:

```py
output = model(model_inputs)
```

اگرچہ ماڈل بہت سے مختلف arguments قبول کرتا ہے، صرف input IDs ہی ضروری ہیں. ہم بعد میں وضاحت کریں گے کہ باقی arguments کیا کرتے ہیں اور کب درکار ہوتے ہیں، لیکن سب سے پہلے ہمیں ان tokenizers پر تفصیلی نظر ڈالنی ہوگی جو Transformer ماڈل کے سمجھنے کے قابل inputs تیار کرتے ہیں.