<FrameworkSwitchCourse {fw} />

# Ù…ØªØ¹Ø¯Ø¯ ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±Ù†Ø§[[handling-multiple-sequences]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

Ù¾Ú†Ú¾Ù„Û’ Ø³ÛŒÚ©Ø´Ù† Ù…ÛŒÚºØŒ ÛÙ… Ù†Û’ Ø³Ø¨ Ø³Û’ Ø³Ø§Ø¯Û Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ú©ÛŒØ³ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒØ§: Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹Û’ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Û’ Ø§ÛŒÚ© ØªØ³Ù„Ø³Ù„ Ù¾Ø± Ø§Ù†ÙØ±Ù†Ø³ Ú©Ø±Ù†Ø§Û” ØªØ§ÛÙ…ØŒ Ú©Ú†Ú¾ Ø³ÙˆØ§Ù„Ø§Øª ÙÙˆØ±Ø§Ù‹ Ø³Ø§Ù…Ù†Û’ Ø¢ØªÛ’ ÛÛŒÚº:

- ÛÙ… Ù…ØªØ¹Ø¯Ø¯ ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ Ú©ÛŒØ³Û’ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±ÛŒÚºØŸ
- ÛÙ… Ù…Ø®ØªÙ„Ù Ù„Ù…Ø¨Ø§Ø¦ÛŒÙˆÚº Ú©Û’ Ù…ØªØ¹Ø¯Ø¯ ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ùˆ Ú©ÛŒØ³Û’ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±ÛŒÚºØŸ
- Ú©ÛŒØ§ ÙˆÚ©ÛŒØ¨Ù„Ø±ÛŒ Ø§Ù†ÚˆÛŒÚ©Ø³Ø² ÛÛŒ ÙˆÛ ÙˆØ§Ø­Ø¯ inputs ÛÛŒÚº Ø¬Ùˆ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø§Ú†Ú¾Ø§ Ú©Ø§Ù… Ú©Ø±Ù†Û’ Ø¯ÛŒØªÛ’ ÛÛŒÚºØŸ
- Ú©ÛŒØ§ Ú©Ø¨Ú¾ÛŒ ØªØ³Ù„Ø³Ù„ Ø¨ÛØª Ù„Ù…Ø¨Ø§ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ØŸ

Ø¢Ø¦ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û ÛŒÛ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ù…Ø³Ø§Ø¦Ù„ Ù¾ÛŒØ¯Ø§ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± ÛÙ… Ø§Ù†ÛÛŒÚº ğŸ¤— Transformers API Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©ÛŒØ³Û’ Ø­Ù„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

## Ù…Ø§ÚˆÙ„Ø² Ø§ÛŒÚ© Ø¨ÛŒÚ† (batch) Ú©Û’ inputs Ú©ÛŒ ØªÙˆÙ‚Ø¹ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº[[models-expect-a-batch-of-inputs]]

Ù¾Ú†Ú¾Ù„Û’ Ù…Ø´Ù‚ Ù…ÛŒÚº Ø¢Ù¾ Ù†Û’ Ø¯ÛŒÚ©Ú¾Ø§ Ú©Û Ú©Ø³ Ø·Ø±Ø­ ØªØ³Ù„Ø³Ù„ Ù†Ù…Ø¨Ø±ÙˆÚº Ú©ÛŒ ÙÛØ±Ø³Øª Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ ÛÙˆ Ø¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§Ø¨ Ø§Ø³ ÙÛØ±Ø³Øª Ú©Ùˆ tensor Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø¨Ú¾ÛŒØ¬ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

Ø§ÙˆÛ Ù†ÛÛŒÚº! ÛŒÛ Ú©ÛŒÙˆÚº Ù†Ø§Ú©Ø§Ù… ÛÙˆØ§ØŸ ÛÙ… Ù†Û’ Ø³ÛŒÚ©Ø´Ù† 2 Ú©Û’ Ù…Ø±Ø§Ø­Ù„ Ú©Ùˆ ÙØ§Ù„Ùˆ Ú©ÛŒØ§ ØªÚ¾Ø§Û”

Ù…Ø³Ø¦Ù„Û ÛŒÛ ÛÛ’ Ú©Û ÛÙ… Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ ØªØ³Ù„Ø³Ù„ Ø¨Ú¾ÛŒØ¬Ø§ØŒ Ø¬Ø¨Ú©Û ğŸ¤— Transformers Ù…Ø§ÚˆÙ„Ø² Ø¨Ø°Ø§Øª Ø®ÙˆØ¯ Ù…ØªØ¹Ø¯Ø¯ Ø¬Ù…Ù„ÙˆÚº Ú©ÛŒ ØªÙˆÙ‚Ø¹ Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºÛ” ÛŒÛØ§Úº ÛÙ… Ù†Û’ ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©ÛŒ Ø¬Ùˆ Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù†Û’ `sequence` Ù¾Ø± Ù„Ú¯Ø§ØªÛ’ ÛÙˆØ¦Û’ Ø¨ÛŒÚ© Ú¯Ø±Ø§Ø¤Ù†Úˆ Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ¾Ø§Û” Ù„ÛŒÚ©Ù† Ø§Ú¯Ø± ØºÙˆØ± Ø³Û’ Ø¯ÛŒÚ©Ú¾ÛŒÚº ØªÙˆ Ø¢Ù¾ Ù¾Ø§Ø¦ÛŒÚº Ú¯Û’ Ú©Û Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²Ø± Ù†Û’ ØµØ±Ù input IDs Ú©ÛŒ ÙÛØ±Ø³Øª Ú©Ùˆ tensor Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ù†ÛÛŒÚº Ú©ÛŒØ§ Ø¨Ù„Ú©Û Ø§Ø³ Ù…ÛŒÚº Ø§ÛŒÚ© Ø§Ø¶Ø§ÙÛŒ Ø¬ÛØª Ø¨Ú¾ÛŒ Ø´Ø§Ù…Ù„ Ú©Ø± Ø¯ÛŒÛ”

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

Ø¢Ø¦ÛŒÚº Ø¯ÙˆØ¨Ø§Ø±Û Ú©ÙˆØ´Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§ÛŒÚ© Ù†Ø¦ÛŒ Ø¬ÛØª Ø´Ø§Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

ÛÙ… input IDs Ø§ÙˆØ± Ø­Ø§ØµÙ„ Ø´Ø¯Û logits Ø¯ÙˆÙ†ÙˆÚº Ú©Ùˆ Ù¾Ø±Ù†Ù¹ Ú©Ø±ØªÛ’ ÛÛŒÚº â€” ÛŒÛØ§Úº output ÛÛ’:

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

*Ø¨ÛŒÚ†Ù†Ú¯* Ú©Ø§ Ù…Ø·Ù„Ø¨ ÛÛ’ Ú©Û Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ Ù…ØªØ¹Ø¯Ø¯ Ø¬Ù…Ù„Û’ Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø¨Ú¾ÛŒØ¬Ù†Ø§Û” Ø§Ú¯Ø± Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ ØµØ±Ù Ø§ÛŒÚ© Ø¬Ù…Ù„Û ÛÛ’ØŒ ØªÙˆ Ø¢Ù¾ Ø§ÛŒÚ© ÙˆØ§Ø­Ø¯ ØªØ³Ù„Ø³Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛŒÚ† Ø¨Ù†Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚº:

```
batched_ids = [ids, ids]
```

ÛŒÛ Ø¯Ùˆ ÛŒÚ©Ø³Ø§Úº ØªØ³Ù„Ø³Ù„ÙˆÚº Ú©Ø§ Ø¨ÛŒÚ† ÛÛ’!

<Tip>

âœï¸ **Try it out!** Ø§Ø³ `batched_ids` ÙÛØ±Ø³Øª Ú©Ùˆ tensor Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ù¾Ù†Û’ Ù…Ø§ÚˆÙ„ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù¾Ø§Ø³ Ú©Ø±ÛŒÚºÛ” Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ø¢Ù¾ Ú©Ùˆ Ù¾ÛÙ„Û’ Ø¬ÛŒØ³Ø§ ÛÛŒ logits Ø­Ø§ØµÙ„ ÛÙˆ Ø±ÛÛ’ ÛÛŒÚº (Ù„ÛŒÚ©Ù† Ø¯Ùˆ Ø¨Ø§Ø±)!

</Tip>

Ø¨ÛŒÚ†Ù†Ú¯ Ø§Ø³ Ù„ÛŒÛ’ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’ Ú©Û Ø¬Ø¨ Ø¢Ù¾ Ù…ØªØ¹Ø¯Ø¯ Ø¬Ù…Ù„Û’ ÙÛŒÚˆ Ú©Ø±ÛŒÚº ØªÙˆ Ù…Ø§ÚˆÙ„ ØµØ­ÛŒØ­ Ú©Ø§Ù… Ú©Ø±Û’Û” Ù…ØªØ¹Ø¯Ø¯ ØªØ³Ù„Ø³Ù„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±Ù†Ø§ Ø§ÛŒÚ© ÛÛŒ ØªØ³Ù„Ø³Ù„ Ú©Û’ Ø¨ÛŒÚ† Ø¨Ù†Ø§Ù†Û’ Ø¬ØªÙ†Ø§ ÛÛŒ Ø¢Ø³Ø§Ù† ÛÛ’Û” Ù„ÛŒÚ©Ù† Ø§ÛŒÚ© Ø¯ÙˆØ³Ø±Ø§ Ù…Ø³Ø¦Ù„Û Ø¨Ú¾ÛŒ ÛÛ’Û” Ø¬Ø¨ Ø¢Ù¾ Ø¯Ùˆ (ÛŒØ§ Ø²ÛŒØ§Ø¯Û) Ø¬Ù…Ù„ÙˆÚº Ú©Ùˆ Ø¨ÛŒÚ† Ù…ÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚº ØªÙˆ ÙˆÛ Ù…Ø®ØªÙ„Ù Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Û’ ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø§Ú¯Ø± Ø¢Ù¾ Ù†Û’ Ù¾ÛÙ„Û’ tensor Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ú©ÛŒØ§ ÛÛ’ ØªÙˆ Ø¢Ù¾ Ø¬Ø§Ù†ØªÛ’ ÛÛŒÚº Ú©Û ÙˆÛ rectanglar Ø´Ú©Ù„ Ú©Û’ ÛÙˆÙ†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŒ Ù„ÛÙ°Ø°Ø§ Ø¢Ù¾ input IDs Ú©ÛŒ ÙÛØ±Ø³Øª Ú©Ùˆ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª tensor Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ù†ÛÛŒÚº Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’Û” Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Ø§ Ø­Ù„ ÛŒÛ ÛÛ’ Ú©Û ÛÙ… Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± inputs Ú©Ùˆ *padding* Ú©Ø±ÛŒÚºÛ”

## Inputs Ú©Ùˆ Padding Ú©Ø±Ù†Ø§[[padding-the-inputs]]

Ù†ÛŒÚ†Û’ Ø¯ÛŒ Ú¯Ø¦ÛŒ ÙÛØ±Ø³Øª Ú©Ùˆ tensor Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ù†ÛÛŒÚº Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©ØªØ§:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Ø§Ø³ Ú©Ø§ Ø­Ù„ ÛŒÛ ÛÛ’ Ú©Û ÛÙ… *padding* Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº ØªØ§Ú©Û ÛÙ…Ø§Ø±Û’ tensors rectangular shape Ø§Ø®ØªÛŒØ§Ø± Ú©Ø± Ø³Ú©ÛŒÚºÛ” Padding Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ØªØ§ ÛÛ’ Ú©Û ÛÙ…Ø§Ø±Û’ ØªÙ…Ø§Ù… Ø¬Ù…Ù„ÙˆÚº Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø§ÛŒÚ© Ø¬ÛŒØ³ÛŒ ÛÙˆ Ø¬Ø§Ø¦Û’ØŒ Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒØ³Û’ Ø¬Ù…Ù„ÙˆÚº Ù…ÛŒÚº Ø§ÛŒÚ© Ø®Ø§Øµ Ù„ÙØ¸ Ø´Ø§Ù…Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ø¬Ø³Û’ *padding token* Ú©ÛØªÛ’ ÛÛŒÚºÛ” Ù…Ø«Ø§Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø±ØŒ Ø§Ú¯Ø± Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ 10 Ø¬Ù…Ù„Û’ ÛÛŒÚº Ø¬Ù† Ù…ÛŒÚº 10 Ø§Ù„ÙØ§Ø¸ ÛÛŒÚº Ø§ÙˆØ± 1 Ø¬Ù…Ù„Û ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº 20 Ø§Ù„ÙØ§Ø¸ ÛÛŒÚºØŒ ØªÙˆ padding Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ø¦Û’ Ú¯Ø§ Ú©Û ØªÙ…Ø§Ù… Ø¬Ù…Ù„ÙˆÚº Ù…ÛŒÚº 20 Ø§Ù„ÙØ§Ø¸ ÛÙˆÚºÛ” ÛÙ…Ø§Ø±Û’ Ù…Ø«Ø§Ù„ Ù…ÛŒÚºØŒ resultant tensor Ú©Ú†Ú¾ ÛŒÙˆÚº Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’:

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

Padding token ID `tokenizer.pad_token_id` Ù…ÛŒÚº Ù…Ù„ Ø³Ú©ØªÛŒ ÛÛ’Û” Ø¢Ø¦ÛŒÚº Ø§Ø³Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ø§ÙˆØ± ÛÙ…Ø§Ø±Û’ Ø¯Ùˆ Ø¬Ù…Ù„ÙˆÚº Ú©Ùˆ Ø§Ù†ÙØ±Ø§Ø¯ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø§ÙˆØ± Ù¾Ú¾Ø± Ø¨ÛŒÚ† Ù…ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù¾Ø§Ø³ Ú©Ø±ÛŒÚº:

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

ÛŒÛØ§Úº ÛÙ…Ø§Ø±Û’ Ø¨ÛŒÚ†Úˆ Ù¾ÛŒØ´Ù† Ú¯ÙˆØ¦ÛŒÙˆÚº (logits) Ù…ÛŒÚº Ú©Ú†Ú¾ Ù…Ø³Ø¦Ù„Û ÛÛ’: Ø¯ÙˆØ³Ø±Û’ ØµÙ Ú©Û’ logits ÙˆÛÛŒ ÛÙˆÙ†Û’ Ú†Ø§ÛØ¦ÛŒÚº Ø¬Ùˆ Ø¯ÙˆØ³Ø±Û’ Ø¬Ù…Ù„Û’ Ú©Û’ Ù„ÛŒÛ’ ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† ÛÙ…ÛŒÚº Ù…Ú©Ù…Ù„ Ù…Ø®ØªÙ„Ù Ø§Ù‚Ø¯Ø§Ø± Ù…Ù„ Ø±ÛÛŒ ÛÛŒÚº!

Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛŒÛ ÛÛ’ Ú©Û Transformer Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ø§ÛÙ… Ø®ØµÙˆØµÛŒØª attention layers ÛÛŒÚº Ø¬Ùˆ ÛØ± token Ú©Ùˆ context Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛŒ ÛÛŒÚºÛ” ÛŒÛ padding tokens Ú©Ùˆ Ø¨Ú¾ÛŒ Ù…Ø¯Ù†Ø¸Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº Ú©ÛŒÙˆÙ†Ú©Û ÙˆÛ Ø§ÛŒÚ© ØªØ³Ù„Ø³Ù„ Ú©Û’ ØªÙ…Ø§Ù… tokens Ù¾Ø± Ø¯Ú¾ÛŒØ§Ù† Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ” Ø§Ù†ÙØ±Ø§Ø¯ÛŒ Ø¬Ù…Ù„ÙˆÚº ÛŒØ§ Ù…Ø®ØªÙ„Ù Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©Û’ Ø¬Ù…Ù„ÙˆÚº Ú©Ùˆ Ø¨ÛŒÚ† Ù…ÛŒÚº Ø¨Ú¾ÛŒØ¬Ù†Û’ Ù¾Ø± Ø§ÛŒÚ© Ø¬ÛŒØ³Ø§ Ù†ØªÛŒØ¬Û Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛÙ…ÛŒÚº Ø§Ù† attention layers Ú©Ùˆ ÛŒÛ Ø¨ØªØ§Ù†Ø§ ÛÙˆÚ¯Ø§ Ú©Û padding tokens Ú©Ùˆ Ù†Ø¸Ø±Ø§Ù†Ø¯Ø§Ø² Ú©Ø±ÛŒÚºÛ” ÛŒÛ Ú©Ø§Ù… attention mask Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”

## Attention masks[[attention-masks]]

*Attention masks* Ø§ÛŒØ³Û’ tensors ÛÙˆØªÛ’ ÛÛŒÚº Ø¬Ù† Ú©ÛŒ Ø´Ú©Ù„ input IDs tensor Ø¬ÛŒØ³ÛŒ ÛÛŒ ÛÙˆØªÛŒ ÛÛ’ØŒ Ø§ÙˆØ± ÛŒÛ 0s Ø§ÙˆØ± 1s Ø³Û’ Ø¨Ú¾Ø±ÛŒ ÛÙˆØªÛŒ ÛÛŒÚº: 1 Ø§Ø³ Ø¨Ø§Øª Ú©ÛŒ Ù†Ø´Ø§Ù†Ø¯ÛÛŒ Ú©Ø±ØªÛŒ ÛÛ’ Ú©Û Ù…ØªØ¹Ù„Ù‚Û token Ù¾Ø± Ø¯Ú¾ÛŒØ§Ù† Ø¯ÛŒØ§ Ø¬Ø§Ù†Ø§ Ú†Ø§ÛÛŒÛ’ØŒ Ø§ÙˆØ± 0 Ø§Ø³ Ø¨Ø§Øª Ú©ÛŒ Ú©Û Ù…ØªØ¹Ù„Ù‚Û token Ú©Ùˆ Ù†Ø¸Ø±Ø§Ù†Ø¯Ø§Ø² Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ (ÛŒØ¹Ù†ÛŒØŒ Ù…Ø§ÚˆÙ„ Ú©ÛŒ attention layers Ø§Ø³Û’ ignore Ú©Ø± Ø¯ÛŒÚº)Û”

Ø¢Ø¦ÛŒÚº Ù¾Ú†Ú¾Ù„Û’ Ù…Ø«Ø§Ù„ Ú©Ùˆ Ø§ÛŒÚ© attention mask Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ú©Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº:

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

Ø§Ø¨ ÛÙ…ÛŒÚº Ø¨ÛŒÚ† Ù…ÛŒÚº Ø¯ÙˆØ³Ø±Û’ Ø¬Ù…Ù„Û’ Ú©Û’ Ù„ÛŒÛ’ ÙˆÛÛŒ logits Ù…Ù„ Ø±ÛÛ’ ÛÛŒÚºÛ”

Ù†ÙˆÙ¹ Ú©Ø±ÛŒÚº Ú©Û Ø¯ÙˆØ³Ø±Û’ ØªØ³Ù„Ø³Ù„ Ú©ÛŒ Ø¢Ø®Ø±ÛŒ Ù‚Ø¯Ø± Ø§ÛŒÚ© padding ID ÛÛ’ØŒ Ø¬Ùˆ Ú©Û attention mask Ù…ÛŒÚº 0 Ú©ÛŒ Ù‚Ø¯Ø± Ø±Ú©Ú¾ØªÛŒ ÛÛ’Û”

<Tip>

âœï¸ **Try it out!** Ø³ÛŒÚ©Ø´Ù† 2 Ù…ÛŒÚº Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø´Ø¯Û Ø¯ÙˆÙ†ÙˆÚº Ø¬Ù…Ù„ÙˆÚº ("I've been waiting for a HuggingFace course my whole life." Ø§ÙˆØ± "I hate this so much!") Ù¾Ø± Ù¹ÙˆÚ©Ù†Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Ùˆ manually Ø§Ù¾Ù„Ø§Ø¦ÛŒ Ú©Ø±ÛŒÚºÛ” Ø§Ù†ÛÛŒÚº Ù…Ø§ÚˆÙ„ Ø³Û’ Ú¯Ø²Ø§Ø±ÛŒÚº Ø§ÙˆØ± Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ø¢Ù¾ Ú©Ùˆ Ø³ÛŒÚ©Ø´Ù† 2 Ø¬ÛŒØ³Ø§ ÛÛŒ logits Ù…Ù„ Ø±ÛÛ’ ÛÛŒÚºÛ” Ø§Ø¨ Ø§Ù†ÛÛŒÚº padding token Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø¨ÛŒÚ† Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ù…Ù†Ø§Ø³Ø¨ attention mask Ø¨Ù†Ø§Ø¦ÛŒÚºÛ” Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ø¢Ù¾ Ù…Ø§ÚˆÙ„ Ø³Û’ Ú¯Ø²Ø±ØªÛ’ ÙˆÙ‚Øª ÙˆÛÛŒ Ù†ØªØ§Ø¦Ø¬ Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚº!

</Tip>

## Ù„Ù…Ø¨Û’ ØªØ³Ù„Ø³Ù„[[longer-sequences]]

Transformer Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ØŒ ØªØ³Ù„Ø³Ù„ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ú©ÛŒ Ø§ÛŒÚ© Ø­Ø¯ ÛÙˆØªÛŒ ÛÛ’ Ø¬Ø³Û’ ÛÙ… Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ù¾Ø§Ø³ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” Ø²ÛŒØ§Ø¯Û ØªØ± Ù…Ø§ÚˆÙ„Ø² 512 ÛŒØ§ 1024 tokens ØªÚ© Ú©Û’ ØªØ³Ù„Ø³Ù„ Ú©Ùˆ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø¬Ø¨ Ø§Ù†ÛÛŒÚº Ø§Ø³ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù„Ù…Ø¨Û’ ØªØ³Ù„Ø³Ù„ Ø¯ÛŒÙ†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’ ØªÙˆ ÙˆÛ Ú©Ø±ÛŒØ´ Ú©Ø± Ø¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§Ø³ Ù…Ø³Ø¦Ù„Û’ Ú©Û’ Ø¯Ùˆ Ø­Ù„ ÛÛŒÚº:

- Ø§ÛŒØ³Û’ Ù…Ø§ÚˆÙ„ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ø¬Ø³ Ú©ÛŒ Ø³Ù¾ÙˆØ±Ù¹Úˆ ØªØ³Ù„Ø³Ù„ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒ Ø²ÛŒØ§Ø¯Û ÛÙˆÛ”
- Ø§Ù¾Ù†Û’ ØªØ³Ù„Ø³Ù„ Ú©Ùˆ truncate Ú©Ø±ÛŒÚºÛ”

Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ø³Ù¾ÙˆØ±Ù¹Úˆ ØªØ³Ù„Ø³Ù„ Ú©ÛŒ Ù„Ù…Ø¨Ø§Ø¦ÛŒØ§Úº Ù…Ø®ØªÙ„Ù ÛÙˆØªÛŒ ÛÛŒÚºØŒ Ø§ÙˆØ± Ú©Ú†Ú¾ Ø¨ÛØª Ù„Ù…Ø¨Û’ ØªØ³Ù„Ø³Ù„ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…ÛØ§Ø±Øª Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºÛ” [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ ÛÛ’ØŒ Ø§ÙˆØ± Ø¯ÙˆØ³Ø±Ø§ [LED](https://huggingface.co/docs/transformers/model_doc/led) ÛÛ’Û” Ø§Ú¯Ø± Ø¢Ù¾ Ø§ÛŒØ³Û’ Ú©Ø§Ù… Ù¾Ø± Ú©Ø§Ù… Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº Ø¬Ø³ Ù…ÛŒÚº Ø¨ÛØª Ù„Ù…Ø¨Û’ ØªØ³Ù„Ø³Ù„ Ø¯Ø±Ú©Ø§Ø± ÛÙˆÚº ØªÙˆ ÛÙ… Ø¢Ù¾ Ú©Ùˆ Ø§Ù† Ù…Ø§ÚˆÙ„Ø² Ù¾Ø± Ù†Ø¸Ø± ÚˆØ§Ù„Ù†Û’ Ú©Ø§ Ù…Ø´ÙˆØ±Û Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ”

ÙˆØ±Ù†ÛØŒ ÛÙ… Ø¢Ù¾ Ú©Ùˆ Ù…Ø´ÙˆØ±Û Ø¯ÛŒØªÛ’ ÛÛŒÚº Ú©Û Ø¢Ù¾ Ø§Ù¾Ù†Û’ ØªØ³Ù„Ø³Ù„ Ú©Ùˆ truncate Ú©Ø±ÛŒÚºØŒ `max_sequence_length` parameter Ù…Ø®ØµÙˆØµ Ú©Ø± Ú©Û’:

```py
sequence = sequence[:max_sequence_length]
```