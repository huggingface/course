<FrameworkSwitchCourse {fw} />

# باب کے اختتام کا کوئز[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

### 1. زبان ماڈلنگ پائپ لائن کا ترتیب کیا ہے؟

<Question
	choices={[
		{
			text: "سب سے پہلے ماڈل، جو متن کو ہینڈل کرتا ہے اور خام پیش گوئیاں دیتا ہے۔ پھر ٹوکنائزر ان پیش گوئیوں کو سمجھ کر ضرورت پڑنے پر انہیں دوبارہ متن میں تبدیل کرتا ہے۔",
			explain: "ماڈل متن کو نہیں سمجھ سکتا! ٹوکنائزر کو پہلے متن کو ٹوکنائز کرنا اور اسے IDs میں تبدیل کرنا ضروری ہے تاکہ ماڈل کے لیے سمجھ میں آئے۔"
		},
		{
			text: "سب سے پہلے ٹوکنائزر، جو متن کو ہینڈل کرتا ہے اور IDs دیتا ہے۔ ماڈل ان IDs کو ہینڈل کرتا ہے اور ایک پیش گوئی دیتا ہے، جو کچھ متن ہو سکتی ہے۔",
			explain: "ماڈل کی پیش گوئی سیدھے متن نہیں ہو سکتی۔ پیش گوئی کو دوبارہ متن میں تبدیل کرنے کے لیے ٹوکنائزر کا استعمال کرنا ضروری ہے!"
		},
		{
			text: "ٹوکنائزر متن کو ہینڈل کرتا ہے اور IDs دیتا ہے۔ ماڈل ان IDs کو ہینڈل کرتا ہے اور ایک پیش گوئی دیتا ہے۔ پھر ٹوکنائزر کا دوبارہ استعمال کر کے ان پیش گوئیوں کو دوبارہ متن میں تبدیل کیا جا سکتا ہے۔",
			explain: "صحیح! ٹوکنائزر کو ٹوکنائزنگ اور ڈی-ٹوکنائزنگ دونوں کے لیے استعمال کیا جا سکتا ہے۔",
            correct: true
		}
	]}
/>

### 2. بنیادی Transformer ماڈل کی طرف سے آؤٹ پٹ ہونے والا tensor کتنی جہات کا ہوتا ہے اور وہ کیا ہیں؟

<Question
	choices={[
		{
			text: "2: تسلسل کی لمبائی اور بیچ سائز",
			explain: "غلط! ماڈل کی طرف سے آؤٹ پٹ tensor میں تیسری جہت ہوتی ہے: hidden size."
		},
		{
			text: "2: تسلسل کی لمبائی اور hidden size",
			explain: "غلط! تمام Transformer ماڈلز بیچز کو ہینڈل کرتے ہیں، چاہے صرف ایک تسلسل ہو؛ اس صورت میں بیچ سائز 1 ہوگا!"
		},
		{
			text: "3: تسلسل کی لمبائی، بیچ سائز، اور hidden size",
			explain: "صحیح!",
            correct: true
		}
	]}
/>

### 3. مندرجہ ذیل میں سے کون سا سبورڈ ٹوکنائزیشن کی مثال ہے؟

<Question
	choices={[
		{
			text: "WordPiece",
			explain: "ہاں، یہ سبورڈ ٹوکنائزیشن کی ایک مثال ہے!",
            correct: true
		},
		{
			text: "Character-based tokenization",
			explain: "کریکٹر پر مبنی ٹوکنائزیشن سبورڈ ٹوکنائزیشن کی قسم نہیں ہے."
		},
		{
			text: "Splitting on whitespace and punctuation",
			explain: "یہ لفظ پر مبنی ٹوکنائزیشن سکیم ہے!"
		},
		{
			text: "BPE",
			explain: "ہاں، یہ سبورڈ ٹوکنائزیشن کی ایک مثال ہے!",
            correct: true
        },
		{
			text: "Unigram",
			explain: "ہاں، یہ سبورڈ ٹوکنائزیشن کی ایک مثال ہے!",
            correct: true
        },
		{
			text: "None of the above",
			explain: "غلط!"
        }
	]}
/>

### 4. ماڈل ہیڈ کیا ہے؟

<Question
	choices={[
		{
			text: "بنیادی Transformer نیٹ ورک کا ایک جزو جو tensors کو ان کی درست layers کی طرف بھیجتا ہے",
			explain: "غلط! ایسا کوئی جزو موجود نہیں ہے."
		},
		{
			text: "جو کہ self-attention mechanism کے نام سے بھی جانا جاتا ہے، یہ ایک token کی نمائندگی کو باقی tokens کے مطابق ڈھالتا ہے",
			explain: "غلط! self-attention layer میں attention \"heads\" شامل ہوتے ہیں، مگر یہ adaptation heads نہیں ہوتے."
		},
		{
			text: "ایک اضافی جزو، جو عموماً ایک یا چند layers پر مشتمل ہوتا ہے، تاکہ transformer کی پیش گوئیوں کو کسی مخصوص کام کے لیے آؤٹ پٹ میں تبدیل کیا جا سکے",
			explain: "بالکل صحیح۔ Adaptation heads، جنہیں صرف heads بھی کہا جاتا ہے، مختلف شکلوں میں آتے ہیں: language modeling heads، question answering heads، sequence classification heads وغیرہ۔",
			correct: true
		} 
	]}
/>

{#if fw === 'pt'}
### 5. AutoModel کیا ہے؟

<Question
	choices={[
		{
			text: "ایک ماڈل جو خود بخود آپ کے ڈیٹا پر ٹرین ہو جاتا ہے",
			explain: "غلط۔ کیا آپ اس کو ہمارے <a href='https://huggingface.co/autotrain'>AutoTrain</a> پروڈکٹ کے ساتھ مغالطہ کر رہے ہیں؟"
		},
		{
			text: "ایک ایسا آبجیکٹ جو checkpoint کی بنیاد پر درست architecture واپس کرتا ہے",
			explain: "بالکل: <code>AutoModel</code> کو صرف یہ جاننے کی ضرورت ہوتی ہے کہ کس checkpoint سے initialize کرنا ہے تاکہ درست architecture واپس کیا جا سکے.",
			correct: true
		},
		{
			text: "ایک ماڈل جو خود بخود اس کی inputs میں استعمال شدہ زبان کا پتہ لگا کر درست weights لوڈ کرتا ہے",
			explain: "غلط؛ اگرچہ کچھ checkpoints اور ماڈلز متعدد زبانوں کو ہینڈل کرنے کے قابل ہوتے ہیں، مگر زبان کے مطابق خودکار checkpoint selection کے لیے کوئی built-in tools موجود نہیں ہیں۔ آپ کو اپنے کام کے لیے بہترین checkpoint تلاش کرنے کے لیے <a href='https://huggingface.co/models'>Model Hub</a> پر جانا چاہیے!"
		} 
	]}
/>

{:else}
### 5. TFAutoModel کیا ہے؟

<Question
	choices={[
		{
			text: "ایک ماڈل جو خود بخود آپ کے ڈیٹا پر ٹرین ہو جاتا ہے",
			explain: "غلط۔ کیا آپ اس کو ہمارے <a href='https://huggingface.co/autotrain'>AutoTrain</a> پروڈکٹ کے ساتھ مغالطہ کر رہے ہیں؟"
		},
		{
			text: "ایک ایسا آبجیکٹ جو checkpoint کی بنیاد پر درست architecture واپس کرتا ہے",
			explain: "بالکل: <code>TFAutoModel</code> کو صرف یہ جاننے کی ضرورت ہوتی ہے کہ کس checkpoint سے initialize کرنا ہے تاکہ درست architecture واپس کیا جا سکے.",
			correct: true
		},
		{
			text: "ایک ماڈل جو خود بخود اس کی inputs میں استعمال شدہ زبان کا پتہ لگا کر درست weights لوڈ کرتا ہے",
			explain: "غلط؛ اگرچہ کچھ checkpoints اور ماڈلز متعدد زبانوں کو ہینڈل کرنے کے قابل ہوتے ہیں، مگر زبان کے مطابق خودکار checkpoint selection کے لیے کوئی built-in tools موجود نہیں ہیں۔ آپ کو اپنے کام کے لیے بہترین checkpoint تلاش کرنے کے لیے <a href='https://huggingface.co/models'>Model Hub</a> پر جانا چاہیے!"
		} 
	]}
/>

{/if}

### 6. مختلف لمبائیوں کے تسلسل کو ایک ساتھ بیچ میں رکھنے کے دوران کن تکنیکوں کا خیال رکھنا چاہیے؟

<Question
	choices={[
		{
			text: "Truncating",
			explain: "جی ہاں، truncate کرنا تسلسل کو مستطیلی شکل میں فٹ کرنے کا درست طریقہ ہے۔ کیا یہ واحد طریقہ ہے، بہرحال؟",
			correct: true
		},
		{
			text: "Returning tensors",
			explain: "جبکہ دیگر تکنیکیں آپ کو مستطیلی شکل کے ٹینسرز واپس کرنے کی اجازت دیتی ہیں، ٹینسرز واپس کرنا تسلسل کو بیچ کرنے میں مددگار نہیں ہے."
		},
		{
			text: "Padding",
			explain: "جی ہاں، padding تسلسل کو مستطیلی شکل میں فٹ کرنے کا صحیح طریقہ ہے۔ کیا یہ واحد طریقہ ہے، بہرحال؟",
			correct: true
		}, 
		{
			text: "Attention masking",
			explain: "بالکل! Attention masks مختلف لمبائیوں کے تسلسل کو ہینڈل کرنے میں بنیادی اہمیت کے حامل ہوتے ہیں۔ تاہم، یہ واحد تکنیک نہیں ہے جس کا خیال رکھنا چاہیے.",
			correct: true
		} 
	]}
/>

### 7. sequence classification ماڈل کی طرف سے آؤٹ پٹ کیے گئے logits پر SoftMax فنکشن لگانے کا مقصد کیا ہے؟

<Question
	choices={[
		{
			text: "یہ logits کو نرم کر دیتا ہے تاکہ وہ زیادہ قابل اعتماد ہوں.",
			explain: "نہیں، SoftMax فنکشن نتائج کی قابل اعتمادیت پر اثر انداز نہیں ہوتا."
		},
		{
			text: "یہ ایک نچلا اور اوپری حد لگاتا ہے تاکہ وہ قابل فہم ہوں.",
			explain: "صحیح! حاصل شدہ اقدار 0 اور 1 کے درمیان محدود ہوتی ہیں۔ تاہم، یہ SoftMax فنکشن استعمال کرنے کا واحد سبب نہیں ہے.",
            correct: true
		},
		{
			text: "آؤٹ پٹ کا مجموعی حاصل 1 ہو جاتا ہے، جس سے ممکنہ طور پر احتمالی تشریح سامنے آتی ہے.",
			explain: "صحیح! تاہم، یہ SoftMax فنکشن استعمال کرنے کا واحد سبب نہیں ہے.",
            correct: true
		}
	]}
/>

### 8. tokenizer API کا زیادہ تر مرکز کس میتھڈ کے گرد ہے؟

<Question
	choices={[
		{
			text: "<code>encode</code>, کیونکہ یہ متن کو IDs میں اور IDs کو پیش گوئیوں میں تبدیل کر سکتا ہے",
			explain: "غلط! اگرچہ tokenizers پر <code>encode</code> میتھڈ موجود ہے، لیکن یہ ماڈلز پر موجود نہیں ہوتا."
		},
		{
			text: "براہ راست tokenizer آبجیکٹ کو کال کرنا.",
			explain: "بالکل صحیح! tokenizer کا <code>__call__</code> میتھڈ بہت طاقتور ہے اور تقریباً ہر چیز کو سنبھال سکتا ہے۔ یہ وہی میتھڈ ہے جو ماڈل سے پیش گوئیاں حاصل کرنے کے لیے استعمال ہوتا ہے.",
			correct: true
		},
		{
			text: "<code>pad</code>",
			explain: "غلط! padding بہت مفید ہے، لیکن یہ صرف tokenizer API کا ایک حصہ ہے."
		},
		{
			text: "<code>tokenize</code>",
			explain: "<code>tokenize</code> میتھڈ شاید سب سے زیادہ مفید میتھڈز میں سے ایک ہے، لیکن یہ tokenizer API کا مرکزی حصہ نہیں ہے."
		}
	]}
/>

### 9. اس کوڈ سیمپل میں `result` ویری ایبل میں کیا ہوتا ہے؟

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```

<Question
	choices={[
		{
			text: "strings کی ایک فہرست، جس میں ہر string ایک token ہے",
			explain: "بالکل! اسے IDs میں تبدیل کریں، اور ماڈل کو بھیج دیں!",
            correct: true
		},
		{
			text: "IDs کی ایک فہرست",
			explain: "غلط؛ یہ وہی کام ہے جو <code>__call__</code> یا <code>convert_tokens_to_ids</code> میتھڈ کرتا ہے!"
		},
		{
			text: "ایک string جس میں تمام tokens شامل ہوں",
			explain: "یہ کم مؤثر ہوگا، کیونکہ مقصد string کو متعدد tokens میں تقسیم کرنا ہے."
		}
	]}
/>

{#if fw === 'pt'}
### 10. کیا درج ذیل کوڈ میں کچھ غلط ہے؟

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "نہیں، یہ درست معلوم ہوتا ہے۔",
			explain: "بدقسمتی سے، ایک مختلف checkpoint کے ساتھ تربیت شدہ tokenizer کے ساتھ ماڈل کو جوڑنا عام طور پر اچھا خیال نہیں ہے۔ ماڈل کو اس tokenizer کے آؤٹ پٹ سے سمجھ بوجھ حاصل کرنے کے لیے تربیت نہیں دی گئی تھی، لہٰذا ماڈل کا آؤٹ پٹ (اگر چل بھی سکے) کوئی معنی نہیں رکھے گا."
		},
		{
			text: "ٹوکنائزر اور ماڈل ہمیشہ ایک ہی checkpoint سے ہونے چاہئیں۔",
			explain: "بالکل درست!",
            correct: true
		},
		{
			text: "یہ اچھی بات ہے کہ ہر ان پٹ کو بیچ بنانے کے لیے ٹوکنائزر کے ساتھ padding اور truncation کی جائے۔",
			explain: "یہ درست ہے کہ ہر ماڈل ان پٹ کو بیچ ہونا چاہیے۔ تاہم، اس ایک تسلسل کو truncate یا pad کرنا ضروری نہیں کہ معنی خیز ہو کیونکہ یہ صرف ایک ہی ہے، اور یہ تکنیکیں متعدد جملوں کو بیچ میں رکھنے کے لیے استعمال ہوتی ہیں."
		}
	]}
/>

{:else}
### 10. کیا درج ذیل کوڈ میں کچھ غلط ہے؟

```py
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "نہیں، یہ درست معلوم ہوتا ہے۔",
			explain: "بدقسمتی سے، ایک مختلف checkpoint کے ساتھ تربیت شدہ tokenizer کے ساتھ ماڈل کو جوڑنا عام طور پر اچھا خیال نہیں ہے۔ ماڈل کو اس tokenizer کے آؤٹ پٹ سے سمجھ بوجھ حاصل کرنے کے لیے تربیت نہیں دی گئی تھی، لہٰذا ماڈل کا آؤٹ پٹ (اگر چل بھی سکے) کوئی معنی نہیں رکھے گا."
		},
		{
			text: "ٹوکنائزر اور ماڈل ہمیشہ ایک ہی checkpoint سے ہونے چاہئیں۔",
			explain: "بالکل درست!",
            correct: true
		},
		{
			text: "یہ اچھی بات ہے کہ ہر ماڈل ان پٹ کو بیچ بنانے کے لیے ٹوکنائزر کے ساتھ padding اور truncation کی جائے۔",
			explain: "یہ درست ہے کہ ہر ماڈل ان پٹ کو بیچ ہونا چاہیے۔ تاہم، اس ایک تسلسل کو truncate یا pad کرنا ضروری نہیں کہ معنی خیز ہو کیونکہ یہ صرف ایک ہی ہے، اور یہ تکنیکیں متعدد جملوں کو بیچ میں رکھنے کے لیے استعمال ہوتی ہیں."
		}
	]}
/>

{/if}