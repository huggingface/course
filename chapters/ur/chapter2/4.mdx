<FrameworkSwitchCourse {fw} />

# ٹوکنائزرز[[tokenizers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
]} />

{/if}

<Youtube id="VFp38yj8h3A"/>

ٹوکنائزرز این ایل پی پائپ لائن کے بنیادی اجزاء میں سے ایک ہیں۔ ان کا واحد مقصد یہ ہے کہ متن کو ایسے ڈیٹا میں تبدیل کریں جسے ماڈل پراسیس کر سکے۔ چونکہ ماڈلز صرف اعداد کو ہی پراسیس کر سکتے ہیں، اس لیے ٹوکنائزرز کو ہمارے متن کے ان پٹس کو عددی ڈیٹا میں تبدیل کرنا ہوتا ہے۔ اس حصے میں، ہم یہ دریافت کریں گے کہ ٹوکنائزیشن پائپ لائن میں در حقیقت کیا ہوتا ہے۔

این ایل پی کے کاموں میں، عموماً جو ڈیٹا پراسیس کیا جاتا ہے وہ کچا متن ہوتا ہے۔ یہاں ایسے متن کی ایک مثال پیش کی جاتی ہے:

```
Jim Henson was a puppeteer
```

تاہم، چونکہ ماڈلز صرف اعداد کو ہی پراسیس کر سکتے ہیں، ہمیں کچے متن کو اعداد میں تبدیل کرنے کا طریقہ تلاش کرنا ہوگا۔ یہی کام ٹوکنائزرز انجام دیتے ہیں، اور اس کے لیے کئی طریقے موجود ہیں۔ مقصد یہ ہے کہ سب سے زیادہ معنی خیز نمائندگی حاصل کی جائے — یعنی وہ جو ماڈل کے لیے سب سے زیادہ معقول ہو — اور، اگر ممکن ہو تو، سب سے چھوٹی نمائندگی بھی۔

آئیں ٹوکنائزیشن الگوردمز کی کچھ مثالوں پر نظر ڈالتے ہیں اور ان سوالات کے جوابات تلاش کرنے کی کوشش کرتے ہیں جو آپ کو ٹوکنائزیشن کے بارے میں ہو سکتے ہیں۔

## لفظ پر مبنی[[word-based]]

<Youtube id="nhJxYji1aho"/>

سب سے پہلی قسم کا ٹوکنائزر جو ذہن میں آتا ہے وہ _لفظ پر مبنی_ ہے۔ اسے سیٹ اپ کرنا اور صرف چند اصولوں کے ساتھ استعمال کرنا عموماً بہت آسان ہوتا ہے، اور یہ اکثر معقول نتائج فراہم کرتا ہے۔ مثال کے طور پر، نیچے دی گئی تصویر میں، مقصد یہ ہے کہ کچے متن کو الفاظ میں تقسیم کیا جائے اور ہر لفظ کی عددی نمائندگی حاصل کی جائے:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" alt="An example of word-based tokenization."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg" alt="An example of word-based tokenization."/>
</div>

متن کو تقسیم کرنے کے مختلف طریقے موجود ہیں۔ مثال کے طور پر، ہم Python کے `split()` فنکشن کا استعمال کرتے ہوئے whitespace کے ذریعہ متن کو الفاظ میں ٹوکنائز کر سکتے ہیں:

```py
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

لفظ پر مبنی ٹوکنائزرز کی ایسی اقسام بھی ہیں جن میں رموزِ اوقاف کے لیے اضافی اصول شامل ہوتے ہیں۔ اس قسم کے ٹوکنائزر کے ساتھ، ہم کافی بڑی "وکیبلریز" حاصل کر سکتے ہیں، جہاں وکیبلری کا تعین ہمارے کورپس میں موجود آزاد ٹوکنز کی کل تعداد سے کیا جاتا ہے۔

ہر لفظ کو ایک ID تفویض کی جاتی ہے، جو 0 سے شروع ہو کر وکیبلری کے سائز تک جاتی ہے۔ ماڈل ان IDs کا استعمال ہر لفظ کی شناخت کے لیے کرتا ہے۔

اگر ہم کسی زبان کو مکمل طور پر لفظ پر مبنی ٹوکنائزر کے ساتھ کور کرنا چاہیں، تو ہمیں زبان کے ہر لفظ کے لیے ایک منفرد شناختی نمبر کی ضرورت ہوگی، جس سے بے شمار ٹوکنز پیدا ہوں گے۔ مثال کے طور پر، انگریزی زبان میں 500,000 سے زیادہ الفاظ موجود ہیں، لہٰذا ہر لفظ کو input ID کے ساتھ نقشہ بنانے کے لیے ہمیں اتنے IDs کا خیال رکھنا ہوگا۔ مزید برآں، جیسے "dog" اور "dogs" مختلف طریقے سے پیش کیے جاتے ہیں، اور ابتدائی طور پر ماڈل کے پاس یہ معلوم کرنے کا کوئی طریقہ نہیں ہوتا کہ "dog" اور "dogs" ایک جیسے ہیں: یہ دونوں الفاظ کو غیر متعلق سمجھتا ہے۔ یہی معاملہ دیگر مشابہ الفاظ جیسے "run" اور "running" کا بھی ہے، جنہیں ابتدائی طور پر ماڈل ایک جیسا نہیں دیکھتا۔

آخر میں، ہمیں ایک کسٹم ٹوکن کی ضرورت ہوتی ہے تاکہ ان الفاظ کی نمائندگی کی جا سکے جو ہماری وکیبلری میں شامل نہیں ہیں۔ اسے "unknown" ٹوکن کہا جاتا ہے، جسے عموماً "[UNK]" یا "&lt;unk&gt;" کے طور پر پیش کیا جاتا ہے۔ اگر آپ دیکھیں کہ ٹوکنائزر بہت سے ایسے ٹوکنز پیدا کر رہا ہے تو یہ عام طور پر منفی علامت ہے، کیونکہ اس نے کسی لفظ کی معقول نمائندگی حاصل نہیں کی اور آپ راستے میں معلومات کھو رہے ہیں۔ وکیبلری تیار کرتے وقت مقصد یہ ہے کہ ٹوکنائزر کم سے کم الفاظ کو unknown ٹوکن میں تبدیل کرے۔

unknown ٹوکنز کی تعداد کم کرنے کا ایک طریقہ یہ ہے کہ ایک قدم مزید آگے بڑھ کر _کریکٹر پر مبنی_ ٹوکنائزر استعمال کیا جائے۔

## کریکٹر پر مبنی[[character-based]]

<Youtube id="ssLq_EK2jLE"/>

کریکٹر پر مبنی ٹوکنائزرز متن کو الفاظ کی بجائے کریکٹرز میں تقسیم کرتے ہیں۔ اس کے دو بنیادی فائدے ہیں:

- وکیبلری بہت چھوٹی ہوتی ہے۔
- out-of-vocabulary (unknown) ٹوکنز کی تعداد بہت کم ہوتی ہے، کیونکہ ہر لفظ کریکٹرز سے تشکیل دیا جا سکتا ہے۔

لیکن یہاں بھی spaces اور punctuation کے حوالے سے کچھ سوالات پیدا ہوتے ہیں:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" alt="An example of character-based tokenization."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg" alt="An example of character-based tokenization."/>
</div>

یہ طریقہ بھی مکمل طور پر درست نہیں ہے۔ چونکہ اب نمائندگی الفاظ کی بجائے کریکٹرز پر مبنی ہے، کوئی دلیل دے سکتا ہے کہ بدیہی طور پر یہ کم معنی خیز ہے: ہر کریکٹر اکیلا زیادہ معنی نہیں رکھتا، جبکہ الفاظ کے ساتھ ایسا ہوتا ہے۔ تاہم، یہ زبان کے حساب سے مختلف ہو سکتا ہے؛ مثال کے طور پر، چینی زبان میں ہر کریکٹر ایک لاطینی زبان کے کریکٹر کی نسبت زیادہ معلومات رکھتا ہے۔

ایک اور بات جو غور طلب ہے وہ یہ ہے کہ ہمارے ماڈل کو پراسیس کرنے کے لیے بے شمار ٹوکنز پیدا ہوں گے: جہاں ایک لفظ صرف ایک ٹوکن ہوتا ہے لفظ پر مبنی ٹوکنائزر کے ساتھ، وہ کریکٹرز میں تبدیل ہونے پر باآسانی 10 یا زیادہ ٹوکنز میں بدل سکتا ہے۔

دونوں طریقوں کے بہترین پہلو حاصل کرنے کے لیے، ہم ایک تیسرا طریقہ استعمال کر سکتے ہیں جو دونوں طریقوں کو یکجا کرتا ہے: *سبورڈ ٹوکنائزیشن*.

## سبورڈ ٹوکنائزیشن[[subword-tokenization]]

<Youtube id="zHvTiHr506c"/>

سبورڈ ٹوکنائزیشن الگوردمز اس اصول پر انحصار کرتے ہیں کہ کثرت سے استعمال ہونے والے الفاظ کو چھوٹے سبورڈز میں تقسیم نہیں کیا جانا چاہیے، بلکہ نایاب الفاظ کو معانی خیز سبورڈز میں تقسیم کیا جانا چاہیے۔

مثال کے طور پر، "annoyingly" کو ایک نایاب لفظ سمجھا جا سکتا ہے اور اسے "annoying" اور "ly" میں تقسیم کیا جا سکتا ہے۔ یہ دونوں ممکنہ طور پر اکیلے سبورڈز کے طور پر زیادہ بار ظاہر ہوں گے، جبکہ ساتھ ہی "annoyingly" کا معنی "annoying" اور "ly" کے مشترکہ معنی سے برقرار رہے گا۔

یہاں ایک مثال پیش کی گئی ہے کہ ایک سبورڈ ٹوکنائزیشن الگوردم "Let's do tokenization!" کو کیسے ٹوکنائز کرے گا:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" alt="A subword tokenization algorithm."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg" alt="A subword tokenization algorithm."/>
</div>

یہ سبورڈز بہت زیادہ معنی خیز معلومات فراہم کرتے ہیں: مثال کے طور پر، اوپر دی گئی مثال میں "tokenization" کو "token" اور "ization" میں تقسیم کیا گیا، دو ٹوکنز جو کہ معنی خیز ہیں اور جگہ کی بچت کرتے ہیں (ایک لمبے لفظ کی نمائندگی کے لیے صرف دو ٹوکنز درکار ہوتے ہیں)۔ اس سے ہمیں چھوٹی وکیبلریز کے ساتھ نسبتاََ اچھا احاطہ حاصل ہوتا ہے، اور تقریباً کوئی unknown ٹوکنز نہیں ہوتے۔

یہ طریقہ خاص طور پر ایسے لسانوں میں مفید ہے جہاں الفاظ آپس میں ملحق ہو سکتے ہیں، جیسے ترکی، جہاں آپ سبورڈز کو جوڑ کر (تقریباً) کسی بھی حد تک طویل پیچیدہ الفاظ بنا سکتے ہیں۔

### اور بھی بہت کچھ![[and-more]]

حیران کن بات نہیں کہ وہاں اور بھی بہت سی تکنیکیں موجود ہیں۔ چند کا ذکر کرتے ہیں:

- Byte-level BPE, as used in GPT-2
- WordPiece, as used in BERT
- SentencePiece or Unigram, as used in several multilingual models

اب آپ کے پاس اتنی معلومات ہونی چاہئیں کہ آپ سمجھ سکیں کہ ٹوکنائزرز کیسے کام کرتے ہیں اور API کے ساتھ کام شروع کر سکیں۔

## لوڈنگ اور سیونگ[[loading-and-saving]]

ٹوکنائزرز کو لوڈ اور سیو کرنا ماڈلز کی طرح ہی آسان ہے۔ دراصل، یہ انہی دو طریقوں پر مبنی ہے: `from_pretrained()` اور `save_pretrained()`. یہ طریقے ٹوکنائزر میں استعمال ہونے والے الگوردم (جو کہ ماڈل کی *architecture* کی طرح ہے) کے ساتھ ساتھ اس کی وکیبلری (جو کہ ماڈل کے *weights* کی طرح ہے) کو لوڈ یا سیو کرتے ہیں۔

BERT کے لیے ٹرین کیے گئے BERT ٹوکنائزر کو لوڈ کرنا ماڈل کو لوڈ کرنے کے طریقے جیسا ہی ہے، سوائے اس کے کہ ہم `BertTokenizer` کلاس کا استعمال کرتے ہیں:

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

{#if fw === 'pt'}
`AutoTokenizer` کلاس بالکل اسی طرح کام کرتی ہے جیسے `AutoModel`؛ یہ لائبریری میں checkpoint کے نام کی بنیاد پر مناسب ٹوکنائزر کلاس کو پکڑ لیتی ہے، اور اسے کسی بھی checkpoint کے ساتھ براہ راست استعمال کیا جا سکتا ہے:
{:else}
`AutoTokenizer` کلاس بالکل اسی طرح کام کرتی ہے جیسے `TFAutoModel`؛ یہ لائبریری میں checkpoint کے نام کی بنیاد پر مناسب ٹوکنائزر کلاس کو پکڑ لیتی ہے، اور اسے کسی بھی checkpoint کے ساتھ براہ راست استعمال کیا جا سکتا ہے:
{/if}

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ہم اب ٹوکنائزر کو اس طرح استعمال کر سکتے ہیں جیسا کہ پچھلے حصے میں دکھایا گیا ہے:

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ٹوکنائزر کو سیو کرنا ماڈل کو سیو کرنے کے مترادف ہے:

```py
tokenizer.save_pretrained("directory_on_my_computer")
```

ہم [Chapter 3](/course/chapter3) میں `token_type_ids` کے بارے میں مزید بات کریں گے، اور تھوڑا بعد `attention_mask` key کی وضاحت کریں گے۔ پہلے، آئیے دیکھتے ہیں کہ `input_ids` کیسے پیدا کیے جاتے ہیں۔ اس کے لیے، ہمیں ٹوکنائزر کے درمیانی مراحل پر نظر ڈالنی ہوگی۔

## انکوڈنگ[[encoding]]

<Youtube id="Yffk5aydLzg"/>

متن کو اعداد میں تبدیل کرنے کو _انکوڈنگ_ کہا جاتا ہے۔ انکوڈنگ ایک دو مرحلہ عمل ہے: ٹوکنائزیشن، اور پھر input IDs میں تبدیلی۔

جیسا کہ ہم نے دیکھا، پہلا مرحلہ یہ ہے کہ متن کو الفاظ (یا الفاظ کے حصے، رموزِ اوقاف وغیرہ) میں تقسیم کیا جائے، جنہیں عموماً *tokens* کہا جاتا ہے۔ اس عمل کو کنٹرول کرنے کے لیے متعدد اصول موجود ہیں، اسی لیے ہمیں ٹوکنائزر کو ماڈل کے نام کے ساتھ انسٹینشیئٹ کرنا ضروری ہے تاکہ ہم وہی اصول استعمال کریں جو ماڈل کی پری ٹریننگ کے وقت استعمال ہوئے تھے۔

دوسرا مرحلہ ان tokens کو اعداد میں تبدیل کرنا ہے، تاکہ ہم ان سے ایک tensor بنا سکیں اور ماڈل کو فیڈ کر سکیں۔ ایسا کرنے کے لیے، ٹوکنائزر کے پاس ایک *vocabulary* ہوتی ہے، جو وہ حصہ ہے جو ہم `from_pretrained()` میتھڈ کے ذریعے انسٹینشیئٹ کرتے وقت ڈاؤن لوڈ کرتے ہیں۔ دوبارہ، ہمیں وہی vocabulary استعمال کرنی ہوتی ہے جو ماڈل کی پری ٹریننگ کے وقت استعمال ہوئی تھی۔

ان دونوں مرحلوں کی بہتر تفہیم کے لیے، ہم انہیں علیحدہ علیحدہ دیکھیں گے۔ نوٹ کریں کہ ہم کچھ ایسے طریقے استعمال کریں گے جو ٹوکنائزیشن پائپ لائن کے کچھ حصے علیحدہ علیحدہ انجام دیتے ہیں تاکہ آپ کو ان مراحل کے درمیانی نتائج دکھا سکیں، لیکن عملی طور پر، آپ کو اپنے inputs پر براہ راست ٹوکنائزر کال کرنا چاہیے (جیسا کہ سیکشن 2 میں دکھایا گیا ہے)۔

### ٹوکنائزیشن[[tokenization]]

ٹوکنائزیشن کا عمل ٹوکنائزر کے `tokenize()` میتھڈ کے ذریعے کیا جاتا ہے:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

اس میتھڈ کا آؤٹ پٹ strings کی ایک فہرست، یا tokens ہوتا ہے:

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

یہ ٹوکنائزر ایک سبورڈ ٹوکنائزر ہے: یہ الفاظ کو اس وقت تک تقسیم کرتا ہے جب تک کہ اسے ایسے tokens نہ مل جائیں جو اس کی vocabulary میں شامل ہو سکیں۔ یہی یہاں `transformer` کے ساتھ ہوتا ہے، جسے دو tokens میں تقسیم کیا گیا ہے: `transform` اور `##er`.

### tokens سے input IDs تک[[from-tokens-to-input-ids]]

tokens کو input IDs میں تبدیل کرنے کا کام ٹوکنائزر کے `convert_tokens_to_ids()` میتھڈ کے ذریعے انجام دیا جاتا ہے:

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

یہ آؤٹ پٹ، جب مناسب framework کے tensor میں تبدیل کر دیے جائیں، تو اس باب کے شروع میں دئے گئے طریقے کے مطابق ماڈل کے inputs کے طور پر استعمال کیے جا سکتے ہیں۔

<Tip>

✏️ **Try it out!** سیکشن 2 میں استعمال کیے گئے input جملوں ("I've been waiting for a HuggingFace course my whole life." اور "I hate this so much!") پر ٹوکنائزیشن اور input IDs میں تبدیل کرنے کے آخری دو مراحل کو دہرائیں۔ چیک کریں کہ آپ کو وہی input IDs مل رہے ہیں جو پہلے ملے تھے!

</Tip>

## ڈی کوڈنگ[[decoding]]

*ڈی کوڈنگ* الٹ سمت جانے کا عمل ہے: vocabulary indices سے ہمیں ایک string حاصل کرنی ہے۔ یہ `decode()` میتھڈ کے ذریعے یوں کیا جا سکتا ہے:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

نوٹ کریں کہ `decode` میتھڈ نہ صرف indices کو tokens میں تبدیل کرتا ہے بلکہ ایک ہی لفظ کے حصے کے tokens کو اکٹھا کر کے ایک قابل مطالعہ جملہ بھی بناتا ہے۔ یہ عمل ان ماڈلز کے ساتھ بے حد مفید ثابت ہوگا جو نیا متن پیش گوئی کرتے ہیں (یا تو کسی prompt سے پیدا کردہ متن، یا ترجمہ یا خلاصہ جیسے sequence-to-sequence مسائل کے لیے)۔

اب تک آپ کو یہ سمجھ آ جانا چاہیے کہ ٹوکنائزر کن بنیادی آپریشنز کو انجام دے سکتا ہے: ٹوکنائزیشن، IDs میں تبدیلی، اور IDs کو دوبارہ string میں تبدیل کرنا۔ تاہم، ہم نے ابھی صرف برف کے ٹکڑے کا اوپر کا حصہ چھوا ہے۔ اگلے حصے میں، ہم اپنے طریقہ کار کی حدوں تک جائیں گے اور دیکھیں گے کہ ان کا حل کیسے نکالا جا سکتا ہے۔