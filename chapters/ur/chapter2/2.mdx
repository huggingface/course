<FrameworkSwitchCourse {fw} />

# پائپ لائن کے پیچھے[[behind-the-pipeline]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
یہ پہلا سیکشن ہے جہاں مواد تھوڑا مختلف ہے اس بات پر منحصر کہ آپ PyTorch استعمال کر رہے ہیں یا TensorFlow. عنوان کے اوپر موجود سوئچ کو ٹوگل کریں تاکہ آپ اپنی پسند کا پلیٹ فارم منتخب کر سکیں!
</Tip>

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

آئیے ایک مکمل مثال سے آغاز کرتے ہیں اور دیکھتے ہیں کہ [Chapter 1](/course/chapter1) میں درج ذیل کوڈ چلانے کے دوران پردے کے پیچھے کیا ہوا:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

اور حاصل کیا:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

جیسا کہ ہم نے [Chapter 1](/course/chapter1) میں دیکھا، یہ پائپ لائن تین مراحل کو ایک ساتھ گروپ کرتی ہے: پری پروسیسنگ، ماڈل کے ذریعے ان پٹس کو پاس کرنا، اور پوسٹ پروسیسنگ:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
</div>

آئیے ان مراحل کا مختصر جائزہ لیتے ہیں۔

## ٹوکنائزر کے ساتھ پری پروسیسنگ[[preprocessing-with-a-tokenizer]]

دیگر نیورل نیٹ ورکس کی طرح، ٹرانسفارمر ماڈلز کچے متن کو براہ راست پروسیس نہیں کر سکتے، لہٰذا ہماری پائپ لائن کا پہلا مرحلہ یہ ہے کہ متن کے ان پٹس کو نمبروں میں تبدیل کیا جائے تاکہ ماڈل انہیں سمجھ سکے. اس کے لیے ہم ایک *ٹوکنائزر* استعمال کرتے ہیں، جو درج ذیل ذمہ داریاں انجام دے گا:

- ان پٹس کو الفاظ، سب ورڈز، یا علامات (جیسے رموزِ اوقاف) میں تقسیم کرنا جنہیں *ٹوکنز* کہا جاتا ہے
- ہر ٹوکن کو ایک عدد کے ساتھ میپ کرنا
- اضافی ان پٹس شامل کرنا جو ماڈل کے لیے مفید ہو سکتے ہیں

یہ تمام پری پروسیسنگ عین اسی طریقے سے کی جانی چاہیے جیسا کہ ماڈل کی پری ٹریننگ کے وقت کی گئی تھی، لہٰذا ہمیں سب سے پہلے [Model Hub](https://huggingface.co/models) سے وہ معلومات ڈاؤن لوڈ کرنی ہوں گی. اس کے لیے ہم `AutoTokenizer` کلاس اور اس کے `from_pretrained()` میتھڈ کا استعمال کرتے ہیں. ماڈل کے چیک پوائنٹ کے نام کا استعمال کرتے ہوئے، یہ خود بخود ماڈل کے ٹوکنائزر سے متعلق ڈیٹا حاصل کر کے اسے کیشے کر لے گا (یعنی کوڈ پہلی بار چلانے پر ہی ڈاؤن لوڈ ہوگا).

چونکہ `sentiment-analysis` پائپ لائن کا ڈیفالٹ چیک پوائنٹ `distilbert-base-uncased-finetuned-sst-2-english` ہے (آپ اس کا ماڈل کارڈ [یہاں](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) دیکھ سکتے ہیں)، ہم مندرجہ ذیل کوڈ چلاتے ہیں:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ایک بار جب ہمارے پاس ٹوکنائزر آ جائے، ہم براہ راست اپنے جملوں کو پاس کر سکتے ہیں اور ایک ایسی ڈکشنری حاصل کر لیتے ہیں جو ماڈل کو فیڈ کرنے کے لیے تیار ہوتی ہے! باقی صرف اتنا کرنا ہے کہ ان پٹ آئی ڈیز کی فہرست کو ٹینسرز میں تبدیل کیا جائے.

آپ 🤗 Transformers کو اس فکر کے بغیر استعمال کر سکتے ہیں کہ بیک اینڈ کے لیے کون سا ML فریم ورک استعمال ہو رہا ہے؛ یہ PyTorch، TensorFlow یا بعض ماڈلز کے لیے Flax ہو سکتا ہے. تاہم، ٹرانسفارمر ماڈلز صرف *ٹینسرز* کو بطور ان پٹ قبول کرتے ہیں. اگر یہ آپ کا پہلا تجربہ ہے کہ ٹینسرز کیا ہوتے ہیں، تو آپ انہیں NumPy ارریز کے طور پر سمجھ سکتے ہیں. ایک NumPy اررے ایک scalar (0D)، ویکٹر (1D)، میٹرکس (2D)، یا مزید جہتوں کا حامل ہو سکتا ہے. دراصل، یہ ایک ٹینسر ہی ہے؛ دیگر ML فریم ورکس کے ٹینسرز بھی اسی طرح کے ہوتے ہیں، اور انہیں بنانا NumPy ارریز بنانے جتنا ہی آسان ہوتا ہے.

ٹینسرز کی مطلوبہ قسم (PyTorch، TensorFlow، یا سادہ NumPy) کی وضاحت کرنے کے لیے ہم `return_tensors` آرگیومنٹ کا استعمال کرتے ہیں:

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

Padding اور truncation کی فکر فی الحال نہ کریں؛ ہم بعد میں ان کی وضاحت کریں گے. یہاں بنیادی بات یہ ہے کہ آپ ایک جملہ یا جملوں کی فہرست پاس کر سکتے ہیں، اور ساتھ ہی ٹینسرز کی قسم کی وضاحت کر سکتے ہیں جو آپ واپس چاہتے ہیں (اگر کوئی قسم مخصوص نہ کی جائے، تو آپ کو فہرست کی فہرست ملے گی).

{#if fw === 'pt'}

یہاں PyTorch ٹینسرز کی صورت میں نتائج کچھ یوں نظر آتے ہیں:
```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}
یہاں TensorFlow ٹینسرز کی صورت میں نتائج کچھ یوں نظر آتے ہیں:
```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

خروجی بذات خود ایک ڈکشنری ہے جس میں دو keys شامل ہیں: `input_ids` اور `attention_mask`. `input_ids` میں دو قطاریں ہوتی ہیں (ہر جملے کے لیے ایک) جو ہر جملے میں موجود ٹوکنز کے منفرد شناخت کنندگان ہیں. ہم بعد میں اس باب میں `attention_mask` کی وضاحت کریں گے.

## ماڈل سے گزرنا[[going-through-the-model]]

{#if fw === 'pt'}
ہم اپنے پری ٹرینڈ ماڈل کو اسی طریقے سے ڈاؤن لوڈ کر سکتے ہیں جیسے ہم نے اپنے ٹوکنائزر کے ساتھ کیا تھا. 🤗 Transformers ایک `AutoModel` کلاس فراہم کرتا ہے جس میں `from_pretrained()` میتھڈ بھی شامل ہے:
```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
ہم اپنے پری ٹرینڈ ماڈل کو اسی طریقے سے ڈاؤن لوڈ کر سکتے ہیں جیسے ہم نے اپنے ٹوکنائزر کے ساتھ کیا تھا. 🤗 Transformers ایک `TFAutoModel` کلاس فراہم کرتا ہے جس میں `from_pretrained` میتھڈ بھی شامل ہے:
```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

اس کوڈ ٹکڑے میں، ہم نے وہی چیک پوائنٹ ڈاؤن لوڈ کیا ہے جو پہلے پائپ لائن میں استعمال ہوا تھا (جو کہ پہلے ہی کیشے ہو چکا ہوگا) اور اس سے ایک ماڈل کی مثال تیار کی ہے.

اس فن تعمیر میں صرف بنیادی Transformer ماڈیول شامل ہے: کچھ ان پٹس دیے جانے پر، یہ وہ *hidden states* (چھپی ہوئی حالتیں) پیدا کرتا ہے جنہیں *features* بھی کہا جاتا ہے. ہر ماڈل ان پٹ کے لیے، ہم ایک اعلیٰ جہتی ویکٹر حاصل کرتے ہیں جو **Transformer ماڈل کی جانب سے اس ان پٹ کی سیاق و سباق کی تفہیم** کی نمائندگی کرتا ہے.

اگر یہ سمجھ نہ آئے تو فکر نہ کریں. ہم بعد میں سب کچھ وضاحت سے بیان کریں گے.

اگرچہ یہ hidden states بذات خود مفید ہو سکتے ہیں، لیکن عموماً یہ ماڈل کے ایک دوسرے حصے کے ان پٹس ہوتے ہیں، جسے *ہیڈ* کہا جاتا ہے. [Chapter 1](/course/chapter1) میں مختلف کام ایک ہی فن تعمیر کے ذریعے انجام دیے جا سکتے تھے، لیکن ہر کام کا اپنا الگ ہیڈ ہوتا ہے.

### ایک اعلیٰ جہتی ویکٹر؟[[a-high-dimensional-vector]]

Transformer ماڈیول کی جانب سے پیدا کردہ ویکٹر عموماً بڑا ہوتا ہے. اس کے عموماً تین جہتیں ہوتی ہیں:

- **بیچ سائز**: ایک وقت میں عمل میں لائی جانے والی تسلسلوں کی تعداد (ہماری مثال میں 2).
- **تسلسل کی لمبائی**: تسلسل کی عددی نمائندگی کی لمبائی (ہماری مثال میں 16).
- **ہیڈن سائز**: ہر ماڈل ان پٹ کا ویکٹر ڈائمینشن.

اسے "اعلیٰ جہتی" کہا جاتا ہے کیونکہ آخری قدر بہت بڑی ہوتی ہے. ہیڈن سائز بہت زیادہ ہو سکتا ہے (چھوٹے ماڈلز کے لیے 768 عام ہے، اور بڑے ماڈلز میں یہ 3072 یا اس سے بھی زیادہ ہو سکتا ہے).

ہم یہ دیکھ سکتے ہیں اگر ہم اپنے پہلے سے تیار شدہ ان پٹس کو ماڈل میں فیڈ کریں:

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

نوٹ کریں کہ 🤗 Transformers ماڈلز کے outputs `namedtuple`s یا ڈکشنریز کی طرح برتاؤ کرتے ہیں. آپ عناصر تک ان کی خصوصیات کے ذریعے (جیسا کہ ہم نے کیا) یا key (`outputs["last_hidden_state"]`) کے ذریعے، یا یہاں تک کہ انڈیکس کے ذریعے بھی رسائی حاصل کر سکتے ہیں اگر آپ کو بالکل معلوم ہو کہ جس چیز کی آپ تلاش کر رہے ہیں وہ کہاں ہے (`outputs[0]`).

### ماڈل ہیڈز: نمبرز میں معنی پیدا کرنا[[model-heads-making-sense-out-of-numbers]]

ماڈل ہیڈز hidden states کے اعلیٰ جہتی ویکٹر کو بطور ان پٹ لیتے ہیں اور انہیں ایک مختلف جہت پر پروجیکٹ کرتے ہیں. عموماً یہ ایک یا چند linear layers پر مشتمل ہوتے ہیں:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

Transformer ماڈل کا آؤٹ پٹ براہ راست ماڈل ہیڈ کو بھیجا جاتا ہے تاکہ اسے پروسیس کیا جا سکے.

اس خاکے میں، ماڈل کو اس کی embeddings layer اور بعد کی layers کے ذریعے ظاہر کیا گیا ہے. Embeddings layer ٹوکنائزڈ ان پٹ میں موجود ہر input ID کو ایک ویکٹر میں تبدیل کرتی ہے جو متعلقہ ٹوکن کی نمائندگی کرتا ہے. بعد کی layers ان ویکٹرز کو attention mechanism کا استعمال کرتے ہوئے تبدیل کرتی ہیں تاکہ جملوں کی حتمی نمائندگی حاصل ہو سکے.

🤗 Transformers میں کئی مختلف فن تعمیر دستیاب ہیں، جن میں سے ہر ایک کو مخصوص کام کو حل کرنے کے لیے ڈیزائن کیا گیا ہے. یہاں ایک غیر جامع فہرست ہے:

- `*Model` (hidden states حاصل کرنے کے لیے)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- اور دیگر 🤗

{#if fw === 'pt'}
ہمارے مثال کے لیے، ہمیں ایک ایسا ماڈل درکار ہوگا جس کا sequence classification ہیڈ ہو (تاکہ جملوں کو مثبت یا منفی کے طور پر درجہ بندی کی جا سکے). لہٰذا، ہم دراصل `AutoModel` کلاس استعمال نہیں کریں گے، بلکہ `AutoModelForSequenceClassification` استعمال کریں گے:
```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
ہمارے مثال کے لیے، ہمیں ایک ایسا ماڈل درکار ہوگا جس کا sequence classification ہیڈ ہو (تاکہ جملوں کو مثبت یا منفی کے طور پر درجہ بندی کی جا سکے). لہٰذا، ہم دراصل `TFAutoModel` کلاس استعمال نہیں کریں گے، بلکہ `TFAutoModelForSequenceClassification` استعمال کریں گے:
```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

اب اگر ہم اپنے outputs کے shape کو دیکھیں، تو جہتی معیار بہت کم ہوگا: ماڈل ہیڈ نے بطور ان پٹ وہ اعلیٰ جہتی ویکٹر لیے جنہیں ہم نے پہلے دیکھا، اور دو اقدار (ہر لیبل کے لیے ایک) پر مشتمل ویکٹرز آؤٹ پٹ کیے:

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

چونکہ ہمارے پاس صرف دو جملے اور دو لیبل ہیں، لہٰذا ماڈل سے ملنے والا نتیجہ 2 x 2 شکل کا ہے.

## آؤٹ پٹ کی پوسٹ پروسیسنگ[[postprocessing-the-output]]

ماڈل سے حاصل کردہ آؤٹ پٹ کی قدریں بذات خود ضروری نہیں کہ معنی خیز ہوں. آئیے ایک نظر ڈالتے ہیں:

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

ہمارے ماڈل نے پہلے جملے کے لیے `[-1.5607, 1.6123]` اور دوسرے جملے کے لیے `[ 4.1692, -3.3464]` کی پیش گوئی کی. یہ احتمالات نہیں بلکہ *logits* ہیں، یعنی ماڈل کی آخری تہہ سے نکلے ہوئے کچے، غیر معمولی سکورز. احتمالات میں تبدیل کرنے کے لیے، انہیں [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) پرت سے گزارنا پڑتا ہے (تمام 🤗 Transformers ماڈلز logits آؤٹ پٹ کرتے ہیں کیونکہ تربیت کے loss function عام طور پر آخری activation function، جیسے SoftMax، کو اصل loss function، جیسے cross entropy، کے ساتھ ملا دیتا ہے).

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

اب ہم دیکھ سکتے ہیں کہ ماڈل نے پہلے جملے کے لیے `[0.0402, 0.9598]` اور دوسرے جملے کے لیے `[0.9995, 0.0005]` کی پیش گوئی کی. یہ پہچانے جانے والے احتمال سکورز ہیں.

ہر پوزیشن سے متعلق لیبلز حاصل کرنے کے لیے، ہم ماڈل کنفیگریشن کی `id2label` خصوصیت کا معائنہ کر سکتے ہیں (اس کے بارے میں اگلے حصے میں مزید وضاحت کی جائے گی):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

اب ہم نتیجہ اخذ کر سکتے ہیں کہ ماڈل نے مندرجہ ذیل پیش گوئی کی:
 
- پہلا جملہ: NEGATIVE: 0.0402, POSITIVE: 0.9598
- دوسرا جملہ: NEGATIVE: 0.9995, POSITIVE: 0.0005

ہم نے کامیابی سے پائپ لائن کے تین مراحل (ٹوکنائزرز کے ساتھ پری پروسیسنگ، ماڈل کے ذریعے ان پٹس کو پاس کرنا، اور پوسٹ پروسیسنگ) کو دہرایا ہے! اب آئیے ان مراحل میں سے ہر ایک کو مزید تفصیل سے دیکھتے ہیں.

<Tip>

✏️ **Try it out!** دو (یا زیادہ) اپنے متن منتخب کریں اور انہیں `sentiment-analysis` پائپ لائن کے ذریعے چلائیں. پھر یہاں دیکھے گئے مراحل کو خود دہرائیں اور چیک کریں کہ آپ کو وہی نتائج مل رہے ہیں!

</Tip>