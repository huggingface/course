```mdx
<FrameworkSwitchCourse {fw} />

# پری ٹرینڈ ماڈلز کا استعمال[[using-pretrained-models]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={4}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter4/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter4/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={4}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter4/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter4/section2_tf.ipynb"},
]} />

{/if}

Hugging Face Hub کی بدولت مناسب ماڈل کا انتخاب بہت آسان ہو جاتا ہے، اور اسے کسی بھی downstream لائبریری میں استعمال کرنے کے لیے صرف چند لائنوں کا کوڈ لکھنا کافی ہوتا ہے۔ آئیں دیکھتے ہیں کہ ان میں سے ایک ماڈل کو حقیقی طور پر کیسے استعمال کیا جاتا ہے اور کمیونٹی کو کس طرح واپس حصہ ڈالا جا سکتا ہے۔

فرض کریں کہ ہم ایک ایسا ماڈل تلاش کر رہے ہیں جو فرانسیسی زبان پر مبنی ہو اور mask filling کا کام کر سکے۔

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/camembert.gif" alt="Selecting the Camembert model." width="80%"/>
</div>

ہم `camembert-base` checkpoint منتخب کرتے ہیں تاکہ اسے آزمایا جا سکے۔ identifier `camembert-base` ہی کافی ہے اسے استعمال شروع کرنے کے لیے! جیسا کہ آپ نے پچھلے ابواب میں دیکھا، ہم اسے `pipeline()` فنکشن کا استعمال کرتے ہوئے instantiate کر سکتے ہیں:

```python
from transformers import pipeline

camembert_fill_mask = pipeline("fill-mask", model="camembert-base")
results = camembert_fill_mask("Le camembert est <mask> :)")
```

```python out
[
  {'sequence': 'Le camembert est délicieux :)', 'score': 0.49091005325317383, 'token': 7200, 'token_str': 'délicieux'}, 
  {'sequence': 'Le camembert est excellent :)', 'score': 0.1055697426199913, 'token': 2183, 'token_str': 'excellent'}, 
  {'sequence': 'Le camembert est succulent :)', 'score': 0.03453313186764717, 'token': 26202, 'token_str': 'succulent'}, 
  {'sequence': 'Le camembert est meilleur :)', 'score': 0.0330314114689827, 'token': 528, 'token_str': 'meilleur'}, 
  {'sequence': 'Le camembert est parfait :)', 'score': 0.03007650189101696, 'token': 1654, 'token_str': 'parfait'}
]
```

جیسا کہ آپ دیکھ سکتے ہیں، pipeline کے اندر ماڈل لوڈ کرنا بہت آسان ہے۔ صرف یہ بات ذہن میں رکھیں کہ منتخب کردہ checkpoint وہی ٹاسک کے لیے موزوں ہو جس کے لیے آپ اسے استعمال کرنا چاہتے ہیں۔ مثال کے طور پر، یہاں ہم `camembert-base` checkpoint کو `fill-mask` pipeline میں لوڈ کر رہے ہیں، جو کہ بالکل درست ہے۔ لیکن اگر ہم اس checkpoint کو `text-classification` pipeline میں لوڈ کریں تو نتائج معنی نہیں رکھیں گے کیونکہ `camembert-base` کا head اس ٹاسک کے لیے موزوں نہیں ہے! ہم تجویز کرتے ہیں کہ Hugging Face Hub کے interface میں task selector کا استعمال کر کے مناسب checkpoint منتخب کریں:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png" alt="The task selector on the web interface." width="80%"/>
</div>

آپ ماڈل آرکیٹیکچر کو براہ راست استعمال کر کے بھی checkpoint instantiate کر سکتے ہیں:

{#if fw === 'pt'}
```python
from transformers import CamembertTokenizer, CamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = CamembertForMaskedLM.from_pretrained("camembert-base")
```

تاہم، ہم [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto?highlight=auto#auto-classes) کا استعمال کرنے کی تجویز دیتے ہیں، کیونکہ یہ اپنی design کی وجہ سے آرکیٹیکچر-agnostic ہوتے ہیں۔ جبکہ پچھلا کوڈ sample صارفین کو صرف Camembert آرکیٹیکچر کے لیے قابل لوڈ checkpoints تک محدود کر دیتا ہے، `Auto*` classes کا استعمال کرنے سے checkpoint تبدیل کرنا بہت آسان ہو جاتا ہے:

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = AutoModelForMaskedLM.from_pretrained("camembert-base")
```
{:else}
```python
from transformers import CamembertTokenizer, TFCamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = TFCamembertForMaskedLM.from_pretrained("camembert-base")
```

تاہم، ہم [`TFAuto*` classes](https://huggingface.co/transformers/model_doc/auto?highlight=auto#auto-classes) کا استعمال کرنے کی تجویز دیتے ہیں، کیونکہ یہ بھی design کی وجہ سے آرکیٹیکچر-agnostic ہوتے ہیں۔ جبکہ پچھلا کوڈ sample صارفین کو صرف Camembert آرکیٹیکچر کے لیے قابل لوڈ checkpoints تک محدود کر دیتا ہے، `TFAuto*` classes کا استعمال کرنے سے checkpoint تبدیل کرنا بہت آسان ہو جاتا ہے:

```python
from transformers import AutoTokenizer, TFAutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = TFAutoModelForMaskedLM.from_pretrained("camembert-base")
```
{/if}

<Tip>
Pretrained ماڈل استعمال کرتے وقت، اس بات کو یقینی بنائیں کہ اسے کس طرح ٹرین کیا گیا، کون سے ڈیٹاسیٹس پر ٹرین ہوا، اس کی حدود کیا ہیں اور اس میں کون سی bias موجود ہے۔ یہ تمام معلومات اس کے model card پر ظاہر ہونی چاہیے۔
</Tip>
```