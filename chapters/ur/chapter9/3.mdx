```mdx
# انٹرفیس کلاس کو سمجھنا[[understanding-the-interface-class]]

<CourseFloatingBanner chapter={9}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section3.ipynb"},
]} />

اس حصے میں، ہم `Interface` کلاس کا قریب سے جائزہ لیں گے، اور اُن اہم پیرامیٹرز کو سمجھیں گے جو ایک انٹرفیس بنانے میں استعمال ہوتے ہیں۔

## ایک انٹرفیس کیسے بنایا جائے[[how-to-create-an-interface]]

آپ دیکھیں گے کہ `Interface` کلاس کے 3 ضروری پیرامیٹرز ہیں:

`Interface(fn, inputs, outputs, ...)`

یہ پیرامیٹرز درج ذیل ہیں:

  - `fn`: وہ پیش گوئی فنکشن جسے Gradio انٹرفیس میں لپیٹ دیا گیا ہے۔ یہ فنکشن ایک یا زیادہ پیرامیٹرز لے سکتا ہے اور ایک یا زیادہ ویلیوز واپس کر سکتا ہے۔
  - `inputs`: ان پٹ جزو کی قسم(یں)۔ Gradio کئی پہلے سے تیار کردہ اجزاء فراہم کرتا ہے جیسے `"image"` یا `"mic"`۔
  - `outputs`: آؤٹ پٹ جزو کی قسم(یں)۔ ایک بار پھر، Gradio بہت سے پہلے سے تیار کردہ اجزاء فراہم کرتا ہے، مثلاً `"image"` یا `"label"`۔

اجزاء کی مکمل فہرست کے لیے، [Gradio docs دیکھیں](https://gradio.app/docs)۔ ہر پہلے سے تیار کردہ جزو کو اُس سے متعلق کلاس کا انسٹینس بنا کر حسب ضرورت بنایا جا سکتا ہے۔

مثال کے طور پر، جیسا کہ ہم نے [پچھلے حصے](/course/chapter9/2) میں دیکھا،
`inputs` پیرامیٹر میں `"textbox"` دینے کی بجائے، آپ ایک `Textbox(lines=7, label="Prompt")` جزو دے سکتے ہیں تاکہ 7 لائنوں والا اور ایک لیبل والا ٹیکسٹ باکس بنایا جا سکے۔

آئیں ایک اور مثال دیکھتے ہیں، اس بار ایک `Audio` جزو کے ساتھ۔

## آڈیو کے ساتھ ایک سادہ مثال[[a-simple-example-with-audio]]

جیسا کہ پہلے ذکر کیا گیا، Gradio مختلف ان پٹ اور آؤٹ پٹ فراہم کرتا ہے۔
تو چلیں ایک ایسا `Interface` بناتے ہیں جو آڈیو کے ساتھ کام کرتا ہے۔

اس مثال میں، ہم ایک آڈیو-ٹو-آڈیو فنکشن بنائیں گے جو ایک
آڈیو فائل لیتا ہے اور اسے صرف ریورس کر دیتا ہے۔

ہم ان پٹ کے لیے `Audio` جزو استعمال کریں گے۔ جب `Audio` جزو استعمال کیا جاتا ہے،
تو آپ یہ وضاحت کر سکتے ہیں کہ آپ چاہتے ہیں کہ آڈیو کا `source` وہ فائل ہو جو صارف اپلوڈ کرے
یا وہ مائیکروفون ہو جس سے صارف اپنی آواز ریکارڈ کرتا ہے۔ اس معاملے میں، آئیے اسے
`"microphone"` پر سیٹ کر دیتے ہیں۔ مزید تفریح کے لیے، ہم اپنے `Audio` میں ایک لیبل شامل کریں گے جو کہتا ہے
"Speak here..."۔

اس کے علاوہ، ہم آڈیو کو numpy array کی صورت میں وصول کرنا چاہتے ہیں تاکہ ہم آسانی سے
اُسے "ریورس" کر سکیں۔ لہٰذا ہم `"type"` کو `"numpy"` پر سیٹ کریں گے، جو ان پٹ
ڈیٹا کو ایک tuple (`sample_rate`, `data`) کی صورت میں ہماری فنکشن میں پاس کرتا ہے۔

ہم آؤٹ پٹ کے لیے بھی `Audio` جزو استعمال کریں گے جو خود بخود
ایک tuple (sample rate اور numpy array) کو قابل پلے آڈیو فائل میں رینڈر کر سکتا ہے۔
اس معاملے میں، ہمیں کسی بھی تخصیص کی ضرورت نہیں، لہٰذا ہم اس کے لیے سٹرنگ شارٹ کٹ
`"audio"` استعمال کریں گے۔

```py
import numpy as np
import gradio as gr


def reverse_audio(audio):
    sr, data = audio
    reversed_audio = (sr, np.flipud(data))
    return reversed_audio


mic = gr.Audio(source="microphone", type="numpy", label="Speak here...")
gr.Interface(reverse_audio, mic, "audio").launch()
```

اوپر دیا گیا کوڈ ایک ایسا انٹرفیس پیدا کرے گا جیسا کہ نیچے دکھایا گیا ہے (اگر آپ کا براؤزر
آپ سے مائیکروفون کی اجازت نہ مانگے، <a href="https://huggingface.co/spaces/course-demos/audio-reverse" target="_blank">ڈیمو کو ایک الگ ٹیب میں کھولیں</a>.) 

<iframe src="https://course-demos-audio-reverse.hf.space" frameBorder="0" height="250" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

آپ اب اپنی آواز ریکارڈ کر کے سن سکتے ہیں کہ آپ کی آواز الٹ (reverse) ہو گئی ہے - کچھ خوفناک 👻!

## متعدد ان پٹ اور آؤٹ پٹ کو ہینڈل کرنا[[handling-multiple-inputs-and-outputs]]

فرض کریں کہ ہمارے پاس ایک پیچیدہ فنکشن ہے، جس میں متعدد ان پٹ اور آؤٹ پٹ شامل ہیں۔
نیچے دی گئی مثال میں، ہمارے پاس ایک فنکشن ہے جو ایک ڈراپ ڈاؤن انڈیکس، ایک سلائیڈر ویلیو، اور ایک نمبر لیتا ہے،
اور ایک آڈیو سیمپل (موسیقی کی دھن) واپس کرتا ہے۔

دھیان دیں کہ ہم ان پٹ اور آؤٹ پٹ اجزاء کی ایک فہرست کیسے پاس کرتے ہیں،
اور دیکھیں کہ آپ سمجھ سکتے ہیں کہ کیا ہو رہا ہے۔

اہم بات یہ ہے کہ جب آپ:
* ان پٹ اجزاء کی ایک فہرست پاس کرتے ہیں، تو ہر جزو ترتیب وار ایک پیرامیٹر سے میل کھاتا ہے۔
* آؤٹ پٹ اجزاء کی ایک فہرست پاس کرتے ہیں، تو ہر جزو ایک واپسی ویلیو سے میل کھاتا ہے۔

نیچے دیا گیا کوڈ دکھاتا ہے کہ تین ان پٹ اجزاء کیسے `generate_tone()` فنکشن کے تین دلائل سے مطابقت رکھتے ہیں:

```py
import numpy as np
import gradio as gr

notes = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]


def generate_tone(note, octave, duration):
    sr = 48000
    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)
    frequency = a4_freq * 2 ** (tones_from_a4 / 12)
    duration = int(duration)
    audio = np.linspace(0, duration, duration * sr)
    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)
    return (sr, audio)


gr.Interface(
    generate_tone,
    [
        gr.Dropdown(notes, type="index"),
        gr.Slider(minimum=4, maximum=6, step=1),
        gr.Number(value=1, label="Duration in seconds"),
    ],
    "audio",
).launch()
```

<iframe src="https://course-demos-generate-tone.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

### `launch()` میتھڈ[[the-launch-method]]

اب تک، ہم نے `launch()` میتھڈ کا استعمال کر کے انٹرفیس لانچ کیا ہے، لیکن ہم نے اس کی تفصیلات پر بات نہیں کی۔

ڈیفالٹ کے مطابق، `launch()` میتھڈ ایک ویب سرور میں ڈیمو لانچ کرتا ہے جو مقامی طور پر چل رہا ہوتا ہے۔ اگر آپ اپنا کوڈ Jupyter یا Colab نوٹ بک میں چلا رہے ہیں، تو Gradio ڈیمو GUI کو نوٹ بک میں ہی ایمبیڈ کر دیتا ہے تاکہ آپ اسے آسانی سے استعمال کر سکیں۔

آپ `launch()` کے برتاؤ کو مختلف پیرامیٹرز کے ذریعے حسب ضرورت بنا سکتے ہیں:

  - `inline` - کہ آیا انٹرفیس کو Python نوٹ بکس میں ان لائن دکھانا ہے۔
  - `inbrowser` - کہ آیا خود بخود انٹرفیس کو ڈیفالٹ براؤزر میں ایک نئے ٹیب میں لانچ کرنا ہے۔
  - `share` - کہ آیا آپ کے کمپیوٹر سے انٹرفیس کے لیے عوامی طور پر شیئر کرنے کے قابل لنک بنانا ہے۔ بالکل Google Drive لنک کی طرح!

ہم اگلے حصے میں `share` پیرامیٹر کی بہت تفصیل سے بات کریں گے!

## ✏️ آئیے اسے عملی جامہ پہناتے ہیں![[lets-apply-it]]

آئیے ایک ایسا انٹرفیس بناتے ہیں جو آپ کو **speech-recognition** ماڈل کا ڈیمو فراہم کرتا ہے۔
دلچسپ بنانے کے لیے، ہم *یا تو* مائیک ان پٹ یا اپلوڈ کی گئی فائل قبول کریں گے۔

ہمیشہ کی طرح، ہم اپنے speech recognition ماڈل کو 🤗 Transformers کی `pipeline()` فنکشن کے ذریعے لوڈ کریں گے۔
اگر آپ کو ایک مختصر ریفریشر کی ضرورت ہو تو آپ [باب 1 کے اس حصے](/course/chapter1/3) پر واپس جا سکتے ہیں۔  
اس کے بعد، ہم ایک `transcribe_audio()` فنکشن نافذ کریں گے جو آڈیو کو پروسیس کرتا ہے اور ٹرانسکرپشن واپس کرتا ہے۔ آخر میں، ہم اس فنکشن کو ایک `Interface` میں لپیٹ دیں گے جس میں ان پٹ کے لیے `Audio` اجزاء اور آؤٹ پٹ کے لیے صرف ٹیکسٹ شامل ہوگا۔ مجموعی طور پر، اس ایپلیکیشن کا کوڈ درج ذیل ہے:

```py
from transformers import pipeline
import gradio as gr

model = pipeline("automatic-speech-recognition")


def transcribe_audio(audio):
    transcription = model(audio)["text"]
    return transcription


gr.Interface(
    fn=transcribe_audio,
    inputs=gr.Audio(type="filepath"),
    outputs="text",
).launch()
```

اگر آپ کا براؤزر آپ سے مائیکروفون کی اجازت نہ مانگے، <a href="https://huggingface.co/spaces/course-demos/audio-reverse" target="_blank">ڈیمو کو ایک الگ ٹیب میں کھولیں</a>۔

<iframe src="https://course-demos-asr.hf.space" frameBorder="0" height="550" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

بس یہی ہے! اب آپ اس انٹرفیس کو استعمال کر کے آڈیو کو ٹرانسکرائب کر سکتے ہیں۔ نوٹ کریں کہ یہاں `optional` پیرامیٹر کو `True` پاس کرنے سے صارف کو یا تو مائیکروفون فراہم کرنے یا آڈیو فائل اپلوڈ کرنے کی اجازت ملتی ہے (یا دونوں، لیکن اگر کچھ نہ دیا جائے تو غلطی کا پیغام واپس آئے گا)۔

آگے پڑھتے رہیں تاکہ آپ دیکھ سکیں کہ دوسروں کے ساتھ اپنا انٹرفیس کیسے شیئر کریں!
```