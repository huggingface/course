<FrameworkSwitchCourse {fw} />

# Models

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}

ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” ëª¨ë¸ì„ ìƒì„±í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ë° ìœ ìš©í•œ `AutoModel` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

ì´ `AutoModel` í´ë˜ìŠ¤ì™€ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ì€ ì‹¤ì œë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìˆëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ ê°ì‹¸ê³  ìˆëŠ” ê°„ë‹¨í•œ ë˜í¼ì…ë‹ˆë‹¤. ì´ ë˜í¼ëŠ” ì²´í¬í¬ì¸íŠ¸ì— ì í•©í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¸¡í•˜ê³ , ì´ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ê²ƒë„ ë˜‘ë˜‘í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

{:else}
ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” ëª¨ë¸ì„ ìƒì„±í•˜ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ë° ìœ ìš©í•œ `TFAutoModel` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

ì´ `TFAutoModel` í´ë˜ìŠ¤ì™€ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ì€ ì‹¤ì œë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìˆëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ ê°ì‹¸ê³  ìˆëŠ” ê°„ë‹¨í•œ ë˜í¼ì…ë‹ˆë‹¤. ì´ ë˜í¼ëŠ” ì²´í¬í¬ì¸íŠ¸ì— ì í•©í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¸¡í•˜ê³ , ì´ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ê²ƒë„ ë˜‘ë˜‘í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

{/if}

í•˜ì§€ë§Œ, ë§Œì•½ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì§ì ‘ ì •ì˜í•˜ê³  ì‹¶ë‹¤ë©´, í•´ë‹¹ ëª¨ë¸ì˜ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. BERT ëª¨ë¸ì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

## Creating a Transformer (Transformer ìƒì„±í•˜ê¸°)

BERT ëª¨ë¸ì„ ì´ˆê¸°í™” í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ëª¨ë¸ì˜ í™˜ê²½ì„¤ì •ì„ ë¡œë“œí•´ì•¼ í•©ë‚˜ë‹¤.

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```
{/if}

ì´ í™˜ê²½ì„¤ì •ì€ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë§ì€ ì†ì„±ë“¤ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

ì•„ì§ ì´ ì†ì„±ë“¤ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ, ëª‡ëª‡ì€ ìµìˆ™í•  ê²ƒì…ë‹ˆë‹¤: `hidden_size` ì†ì„±ì€ `hidden_states` ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì •ì˜í•˜ê³ , `num_hidden_layers`ëŠ” Transformer ëª¨ë¸ì´ ê°€ì§€ê³  ìˆëŠ” ë ˆì´ì–´ì˜ ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

### Different loading methods (ë‹¤ë¥¸ ë¡œë”© ë°©ë²•)

ë¬´ì‘ìœ„ ê°’ì„ í†µí•œ ê¸°ë³¸ í™˜ê²½ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}

ì´ ëª¨ë¸ì€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ì•„ì§ì€ ì•„ë¬´ëŸ° ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ”, ë¨¼ì € í›ˆë ¨ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ë°”ë‹¥ë¶€í„° í•™ìŠµì„ í•  ìˆ˜ ìˆì§€ë§Œ, ì´ ê³¼ì •ì€ [Chapter 1](/course/chapter1)ì—ì„œ í™•ì¸ í–ˆë“¯ì´, ë§ì€ ì‹œê°„ê³¼ ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ë©°, í•™ìŠµ í™˜ê²½ì—ë„ í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë¶ˆí•„ìš”í•œ ì¤‘ë³µëœ ë…¸ë ¥ì„ í”¼í•˜ê¸° ìœ„í•´ì„œ, ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì„ ê³µìœ í•˜ê³  ì¬ì‚¬ìš©í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

í›ˆë ¨ëœ Transformer ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì€ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤ - `from_pretrained` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

ì´ì „ì— ë´¤ë“¯ì´, `BertModel` ëŒ€ì‹  `AutoModel` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ì œë¶€í„°ëŠ” ì²´í¬í¬ì¸íŠ¸ì— ë…ë¦½ì ì¸ ì½”ë“œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ `AutoModel`ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ì½”ë“œê°€ í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì˜ ë™ì‘í•œë‹¤ë©´, ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œë„ ì˜ ë™ì‘í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì²´í¬í¬ì¸íŠ¸ì˜ ì•„í‚¤í…ì²˜ê°€ ë‹¤ë¥´ë”ë¼ë„, ë¹„ìŠ·í•œ ì‘ì—…(ì˜ˆë¥¼ ë“¤ì–´, ê°ì„± ë¶„ì„ ì‘ì—…)ì„ ìœ„í•´ í›ˆë ¨ëœ ê²½ìš°ì—ë„ ì ìš©ë©ë‹ˆë‹¤.

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

ì´ì „ì— ë´¤ë“¯ì´, `TFBertModel` ëŒ€ì‹  `TFAutoModel` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ì œë¶€í„°ëŠ” ì²´í¬í¬ì¸íŠ¸ì— ë…ë¦½ì ì¸ ì½”ë“œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ `TFAutoModel`ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ì½”ë“œê°€ í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì˜ ë™ì‘í•œë‹¤ë©´, ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œë„ ì˜ ë™ì‘í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì²´í¬í¬ì¸íŠ¸ì˜ ì•„í‚¤í…ì²˜ê°€ ë‹¤ë¥´ë”ë¼ë„, ë¹„ìŠ·í•œ ì‘ì—…(ì˜ˆë¥¼ ë“¤ì–´, ê°ì„± ë¶„ì„ ì‘ì—…)ì„ ìœ„í•´ í›ˆë ¨ëœ ê²½ìš°ì—ë„ ì ìš©ë©ë‹ˆë‹¤.

{/if}

ì´ ì½”ë“œ ìƒ˜í”Œì—ì„œëŠ” `BertConfig`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ê³ , ëŒ€ì‹  `bert-base-cased` ì‹ë³„ìë¥¼ í†µí•´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ì´ëŠ” BERTì˜ ì €ìë“¤ì´ ì§ì ‘ í›ˆë ¨ì‹œí‚¨ ì²´í¬í¬ì¸íŠ¸ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/bert-base-cased)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.

The weights have been downloaded and cached (so future calls to the `from_pretrained()` method won't re-download them) in the cache folder, which defaults to *~/.cache/huggingface/transformers*. You can customize your cache folder by setting the `HF_HOME` environment variable.

The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found [here](https://huggingface.co/models?filter=bert).

### Saving methods

Saving a model is as easy as loading one â€” we use the `save_pretrained()` method, which is analogous to the `from_pretrained()` method:

```py
model.save_pretrained("directory_on_my_computer")
```

This saves two files to your disk:

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

If you take a look at the *config.json* file, you'll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what ğŸ¤— Transformers version you were using when you last saved the checkpoint.

{#if fw === 'pt'}
The *pytorch_model.bin* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.

{:else}
The *tf_model.h5* file is known as the *state dictionary*; it contains all your model's weights. The two files go hand in hand; the configuration is necessary to know your model's architecture, while the model weights are your model's parameters.

{/if}

## Using a Transformer model for inference

Now that you know how to load and save a model, let's try using it to make some predictions. Transformer models can only process numbers â€” numbers that the tokenizer generates. But before we discuss tokenizers, let's explore what inputs the model accepts.

Tokenizers can take care of casting the inputs to the appropriate framework's tensors, but to help you understand what's going on, we'll take a quick look at what must be done before sending the inputs to the model.

Let's say we have a couple of sequences:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

The tokenizer converts these to vocabulary indices which are typically called *input IDs*. Each sequence is now a list of numbers! The resulting output is:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This "array" is already of rectangular shape, so converting it to a tensor is easy:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### Using the tensors as inputs to the model

Making use of the tensors with the model is extremely simple â€” we just call the model with the inputs:

```py
output = model(model_inputs)
```

While the model accepts a lot of different arguments, only the input IDs are necessary. We'll explain what the other arguments do and when they are required later, 
but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.
