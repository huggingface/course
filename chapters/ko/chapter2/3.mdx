<FrameworkSwitchCourse {fw} />

# 모델[[models]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
이 장에서는 모델을 생성하고 사용하는 방법에 대해 자세히 알아볼 것입니다. 체크포인트를 이용해 어떤 모델이든 편리하게 인스턴스화할 수 있는 `AutoModel` 클래스를 사용할 것입니다.

`AutoModel` 클래스와 이와 관련된 모든 항목들은 라이브러리에서 이용할 수 있는 다양한 모델에 대한 간단한 래퍼입니다. 당신이 선택한 체크포인트로부터 자동으로 모델 구조를 추론하고 그 구조로 모델을 인스턴스화하기 때문에 영리한 래퍼라고 할 수 있습니다.

{:else}
이 장에서는 모델을 생성하고 사용하는 방법에 대해 자세히 알아볼 것입니다. 체크포인트를 이용해 어떤 모델이든 편리하게 인스턴스화할 수 있는 `TFAutoModel` 클래스를 사용할 것입니다.

`TFAutoModel` 클래스와 이와 관련된 모든 항목들은 라이브러리에서 이용할 수 있는 다양한 모델에 대한 간단한 래퍼입니다. 당신이 선택한 체크포인트로부터 자동으로 모델 구조를 추론하고 그 구조로 모델을 인스턴스화하기 때문에 영리한 래퍼라고 할 수 있습니다.

{/if}

하지만, 사용하고자 하는 모델의 구조를 알고 있다면 해당 구조를 직접 정의하는 클래스를 사용할 수도 있습니다. BERT 모델을 통해 동작 과정을 알아봅시다.

## Transformer 모델 생성[[creating-a-transformer]]

BERT 모델을 초기화하기 위해 가장 먼저 해야 할 일은 환경 설정 객체를 불러오는 것입니다.

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# 환경 설정 생성
config = BertConfig()

# 환경 설정을 이용한 모델 생성
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# 환경 설정 생성
config = BertConfig()

# 환경 설정을 이용한 모델 생성
model = TFBertModel(config)
```
{/if}

환경 설정은 모델 생성에 사용되는 많은 속성을 포함하고 있습니다.

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

이 속성들이 뭘 하는지는 보지 못했지만 몇 가지는 짚고 넘어가야 합니다. `hidden_size` 속성은 `hidden_states` 벡터의 크기를 결정하고, `num_hidden_layers` 속성은 Transformer 모델이 가지는 레이어 수를 결정합니다.

### 다른 로딩 메서드[[different-loading-methods]]

기본 환경 설정으로 모델을 생성하면 임의의 값으로 초기화됩니다.

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# 모델은 임의의 값으로 초기화되었습니다!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# 모델은 임의의 값으로 초기화되었습니다!
```
{/if}

모델은 이 상태로 사용될 수 있지만 모델의 출력 결과를 보면 횡설수설할 것입니다. 먼저 모델은 학습되어야 합니다. 우리는 직면한 과제에 대해 처음부터 모델을 학습시킬 수 있지만, [제1단원](/course/chapter1)에서 봤듯이 이 과정은 긴 시간과 많은 데이터를 필요로 하며 환경에 무시할 수 없는 영향을 미칠 것입니다. 불필요하고 중복되는 노력을 피하기 위해서는 이미 학습된 모델을 재사용하고 공유할 수 있어야 합니다.

학습된 Transformer 모델을 불러오는 것은 간단합니다 - `from_pretrained()` 메서드를 이용할 수 있습니다.

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

앞서 본 것처럼 `BertModel`을 동일한 `AutoModel` 클래스로 대체할 수 있습니다. 이렇게 하면 체크포인트에 구애받지 않는 코드를 구현할 수 있기 때문에 지금부터 `AutoModel` 클래스를 사용할 것입니다. 만약 특정 체크포인트에서 코드가 작동한다면 다른 체크포인트에서도 작동이 원활해야 합니다. 체크포인트가 유사한 태스크(예를 들면 감성분석 태스크)에 대해 학습되는 한 모델 구조가 다르더라도 적용됩니다.

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

앞서 본 것처럼 `BertModel`을 동일한 `AutoModel` 클래스로 대체할 수 있습니다. 이렇게 하면 체크포인트에 구애받지 않는 코드를 구현할 수 있기 때문에 지금부터 `AutoModel` 클래스를 사용할 것입니다. 만약 특정 체크포인트에서 코드가 작동한다면 다른 체크포인트에서도 작동이 원활해야 합니다. 체크포인트가 유사한 태스크(예를 들면 감성분석 태스크)에 대해 학습되는 한 모델 구조가 다르더라도 적용됩니다.

{/if}

위의 샘플 코드에서 `BertConfig`를 사용하지 않고 `bert-base-cased` 식별자를 통해 사전학습 모델을 불러왔습니다. 이 체크포인트의 BERT 저자들이 직접 학습시킨 모델 체크포인트입니다. [모델 카드](https://huggingface.co/bert-base-cased)에서 더 자세한 내용을 확인할 수 있습니다.

이제 이 모델은 체크포인트의 모든 가중치로 초기화됩니다. 학습된 태스크에 대한 추론 시 바로 사용할 수 있으며 새로운 태스크로 파인튜닝할 수 있습니다. 처음부터 직접 학습시키기보다 사전학습된 가중치를 사용하면 좋은 결과를 빨리 얻을 수 있습니다.

가중치가 다운로드되고 기본 경로가 *~/.cache/huggingface/transformers*인 캐시 폴더에 저장되었기 때문에 이후 `from_pretrained()` 메서드 호출 시 가중치를 다시 다운로드 하지 않습니다. 캐시 폴더는 `HF_HOME` 환경 변수 설정을 통해 변경할 수 있습니다.

BERT 구조와 호환되는 모델이라면, 모델 허브에 올라온 어떤 모델이라도 식별자로 사용될 수 있습니다. 이용 가능한 BERT 체크포인트 전체 리스트는 [이 곳](https://huggingface.co/models?filter=bert)에서 확인할 수 있습니다.

### 저장 메서드[[saving-methods]]

모델 저장은 모델을 불러오는 것만큼 간단합니다 - `from_pretrained()` 메서드와 유사한 `save_pretrained()` 메서드를 사용합니다.

```py
model.save_pretrained("directory_on_my_computer")
```

이 메서드는 당신의 디스크에 두 가지 파일을 저장합니다.

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

*config.json* 파일을 자세히 들여다보면, 속성들이 모델 구조 생성에 중요하다는 것을 알게 될 것입니다. 또한, 이 파일에는 체크포인트가 시작된 위치와 체크포인트를 마지막으로 저장할 때 사용한 🤗 Transformers 버전과 같은 메타데이터가 포함되어 있습니다.

{#if fw === 'pt'}
*pytorch_model.bin* 파일은 *state dictionary*로 알려져 있습니다. 이 파일은 모델의 모든 가중치를 저장하고 있으며 두 개의 파일이 서로 연결되어 있습니다. 모델 가중치는 모델의 파라미터이고 환경 설정은 모델 구조 파악에 중요한 역할을 합니다.

{:else}
*tf_model.h5* 파일은 *state dictionary*로 알려져 있습니다. 이 파일은 모델의 모든 가중치를 저장하고 있으며 두 개의 파일은 서로 관련이 있습니다. 모델 가중치는 모델의 파라미터이고 환경 설정은 모델 구조 파악에 중요한 역할을 합니다.

{/if}

## Transformer 모델을 이용하여 추론하기[[using-a-transformer-model-for-inference]]

모델을 불러오고 저장하는 방법을 알았기 때문에 이제 모델을 활용하여 몇 가지 예측을 해봅시다. Transformer 모델은 토크나이저가 생성한 숫자만 처리할 수 있는데 토크나이저에 대해 이야기하기 전 모델이 허락하는 입력 형태에 대해 알아보도록 하겠습니다.

토크나이저는 적절한 프레임워크의 텐서로 입력을 보낼 수 있지만 동작 과정에 대한 이해를 돕기 위해 입력을 모델로 보내기 전 수행해야 할 작업을 간단히 살펴보겠습니다.

몇 개의 시퀀스가 있다고 가정해봅시다.

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

토크나이저는 이 시퀀스들을 *입력 ID*라고 하는 단어 인덱스로 변환합니다. 각 시퀀스는 이제 숫자 리스트입니다! 결과는 다음과 같이 출력됩니다.

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

이 결과는 인코딩된 시퀀스들로, 리스트 안의 리스트 형태입니다. 텐서는 직사각형 형태(행렬을 생각해보세요)만 사용할 수 있습니다. 인코딩된 시퀀스 "배열"은 이미 직사각형 형태이기 때문에 이 배열을 텐서로 변환하는 것은 쉽습니다.

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### 텐서를 모델 입력으로 사용하기[[using-the-tensors-as-inputs-to-the-model]]

텐서를 모델과 합께 사용하는 것은 매우 간단합니다 - 입력과 함께 모델을 호출합니다.

```py
output = model(model_inputs)
```

모델은 다양한 인자를 입력받지만 여기서는 입력 ID만 필요합니다. 다른 인자들의 역할이나 다른 인자들이 필요한 시점에 대한 설명을 듣기 전에, Transformer 모델이 이해할 수 있는 입력을 생성하는 토크나이저를 자세히 살펴봐야 합니다.
