<FrameworkSwitchCourse {fw} />

# ë‹¤ì¤‘ ì‹œí€€ìŠ¤ ì²˜ë¦¬[[handling-multiple-sequences]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

ì´ì „ ì„¹ì…˜ì—ì„œ, ìš°ë¦¬ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì‹œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê¸¸ì´ê°€ ì§§ì€ ë‹¨ì¼ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì¶”ë¡ ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¯¸ ëª‡ ê°€ì§€ ê¶ê¸ˆì¦ì´ ìƒê¹ë‹ˆë‹¤.

- ì—¬ëŸ¬ ê°œì˜ ì‹œí€€ìŠ¤ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ë‚˜ìš”?
- *ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´*ë¥¼ ê°–ëŠ” ë‹¤ì¤‘ ì‹œí€€ìŠ¤ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ë‚˜ìš”?
- ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ê°€ ëª¨ë¸ì´ ì˜ ì‘ë™í•˜ê²Œ í•˜ëŠ” ìœ ì¼í•œ ì…ë ¥ì¸ê°€ìš”?
- ì—„ì²­ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?

ì´ëŸ¬í•œ ì§ˆë¬¸ë“¤ì´ ì œê¸°í•˜ëŠ” ë¬¸ì œì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ğŸ¤— Transformers APIë¥¼ ì´ìš©í•´ ì´ ë¬¸ì œë“¤ì„ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤.

## ëª¨ë¸ì€ ë°°ì¹˜ ì…ë ¥ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤[[models-expect-a-batch-of-inputs]]

ìš°ë¦¬ëŠ” ì´ì „ ì‹¤ìŠµì—ì„œ ì‹œí€€ìŠ¤ê°€ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¡œ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë°”ê¾¸ê³  ëª¨ë¸ë¡œ ë³´ë‚´ë´…ì‹œë‹¤.:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# ì´ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# ì´ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

ì´ëŸ°! 2ì¥ì˜ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ë”°ë¼í–ˆëŠ”ë° ì™œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ê±¸ê¹Œìš”?

ğŸ¤— Transformers ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë° í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë§Œì„ ëª¨ë¸ì— ë„˜ê²¨ì¤¬ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ `ì‹œí€€ìŠ¤`ì— ì ìš©í–ˆì„ ë•Œ ë’¤ì—ì„œ ì¼ì–´ë‚˜ê³  ìˆëŠ” ëª¨ë“  ì¼ì„ ìˆ˜í–‰í•˜ë ¤ê³  í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìì„¸íˆ ë³´ë©´, í† í¬ë‚˜ì´ì €ê°€ ì…ë ¥ ID ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë°”ê¿¨ì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì°¨ì›ë„ ì¶”ê°€í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

ìƒˆë¡œìš´ ì°¨ì›ì„ ì¶”ê°€í•´ì„œ ë‹¤ì‹œ ì‹œë„í•´ë´…ì‹œë‹¤.

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

ì…ë ¥ IDì™€ ê²°ê³¼ ë¡œì§“ì„ ì¶œë ¥í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

*ë°°ì¹˜*ëŠ” í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ ëª¨ë¸ë¡œ ë³´ë‚´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§Œì•½ ë‹¨ í•œ ê°œì˜ ë¬¸ì¥ì„ ê°€ì§€ê³  ìˆë‹¤ë©´ í•œ ë¬¸ì¥ì„ ìœ„í•œ ë°°ì¹˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```
batched_ids = [ids, ids]
```

ë™ì¼í•œ ë¬¸ì¥ 2ê°œë¡œ ë§Œë“  ë°°ì¹˜ì…ë‹ˆë‹¤!

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ì´ `batched_ids` ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  ëª¨ë¸ë¡œ ì „ë‹¬í•´ë³´ì„¸ìš”. ì´ì „ì— ì–»ì€ ë¡œì§“ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”. (ê°œìˆ˜ëŠ” ë‘ ê°œì—¬ì•¼ í•©ë‹ˆë‹¤!)

</Tip>

ë°°ì¹˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ ëª¨ë¸ë¡œ ë„˜ê²¼ì„ ë•Œë„ ëª¨ë¸ì´ ì‘ë™í•˜ê²Œ í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì‹œí€€ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë‹¨ì¼ ì‹œí€€ìŠ¤ë¡œ ë°°ì¹˜ë¥¼ ë§Œë“œëŠ” ê²ƒë§Œí¼ ê°„ë‹¨í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‘ ë²ˆì§¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‘ ê°œ ì´ìƒì˜ ë¬¸ì¥ì„ ë°°ì¹˜ë¡œ ë§Œë“œë ¤ê³  í•  ë•Œ, ê·¸ ë¬¸ì¥ë“¤ì€ ì•„ë§ˆ ë‹¤ë¥¸ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì´ì „ì— í…ì„œë¥¼ ë‹¤ë¤„ë³¸ ì‚¬ëŒì´ë¼ë©´, í…ì„œì˜ í˜•íƒœê°€ ì§ì‚¬ê°í˜•ì´ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ì¥ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì…ë ¥ ID ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë°”ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì¼ë°˜ì ìœ¼ë¡œ ì…ë ¥ì— *íŒ¨ë“œ*ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

## ì…ë ¥ì— íŒ¨ë”© ì¶”ê°€í•˜ê¸°[[padding-the-inputs]]

ì•„ë˜ ë³´ì´ëŠ” ë¦¬ìŠ¤íŠ¸ëŠ” í…ì„œë¡œ ë³€í™˜ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

*íŒ¨ë”©*ì„ ì´ìš©í•´ í…ì„œê°€ ì§ì‚¬ê°í˜• í˜•íƒœë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê²Œ í•˜ë©´ ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒ¨ë”©ì€ ê¸¸ì´ê°€ ì§§ì€ ë¬¸ì¥ì— *íŒ¨ë”© í† í°*ì´ë¼ê³  ë¶ˆë¦¬ëŠ” íŠ¹ë³„í•œ í† í°ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ëª¨ë“  ë¬¸ì¥ì´ ê°™ì€ ê¸¸ì´ë¥¼ ê°–ê²Œ í•©ë‹ˆë‹¤. 10ê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ 10ê°œì™€ 20ê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ 1ê°œë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •í•œë‹¤ë©´, íŒ¨ë”©ì€ ëª¨ë“  ë¬¸ì¥ì´ 20ê°œì˜ ë‹¨ì–´ë¥¼ ê°–ê²Œ í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œì—ì„œ ê²°ê³¼ í…ì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

íŒ¨ë”© í† í°ì˜ IDëŠ” `tokenizer.pad_token_id` ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê±¸ ì‚¬ìš©í•´ì„œ ë‘ ë¬¸ì¥ì„ ê°ê° ëª¨ë¸ë¡œ ë³´ë‚´ê³  í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

ë°°ì¹˜ë˜ì–´ ìˆëŠ” ì˜ˆì¸¡ ê²°ê³¼ì˜ ë¡œì§“ì´ ë­”ê°€ ì˜ëª»ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‘ ë²ˆì§¸ í–‰ì€ ë‘ ë²ˆì§¸ ë¬¸ì¥ì˜ ë¡œì§“ê³¼ ê°’ì´ ê°™ì•„ì•¼ í•˜ëŠ”ë° ì™„ì „íˆ ë‹¤ë¥¸ ê°’ì„ ì–»ì—ˆìŠµë‹ˆë‹¤!

ì´ê²ƒì€ Transformer ëª¨ë¸ì˜ í•µì‹¬ ê¸°ëŠ¥ì´ ê° í† í°ì„ *ë¬¸ë§¥í™”*í•˜ëŠ” ì–´í…ì…˜ ë ˆì´ì–´ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì–´í…ì…˜ ë ˆì´ì–´ëŠ” ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  í† í°ì„ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— íŒ¨ë”© í† í°ë„ ê³ ë ¤í•©ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ë¥¼ ê°€ì§€ëŠ” ë¬¸ì¥ ê°ê°ì„ ëª¨ë¸ë¡œ ì „ë‹¬í–ˆì„ ë•Œì™€ íŒ¨ë”©ì´ ì¶”ê°€ë˜ì–´ ê¸¸ì´ê°€ ê°™ì•„ì§„ ë¬¸ì¥ë“¤ì„ ë°°ì¹˜ë¡œ ì „ë‹¬í–ˆì„ ë•Œì˜ ê²°ê³¼ê°€ ê°™ê¸° ìœ„í•´ì„œëŠ” ì´ ì–´í…ì…˜ ë ˆì´ì–´ë“¤ì—ê²Œ íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ë¼ê³  ì•Œë ¤ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ì—­í• ì„ ì–´í…ì…˜ ë§ˆìŠ¤í¬ê°€ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ì–´í…ì…˜ ë§ˆìŠ¤í¬[[attention-masks]]

*ì–´í…ì…˜ ë§ˆìŠ¤í¬*ëŠ” ì…ë ¥ ID í…ì„œì™€ ê°™ì€ í¬ê¸°ë¥¼ ê°™ëŠ” í…ì„œë¡œ, 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. 1ì€ í•´ë‹¹ í† í°ì„ ì£¼ì˜ ê¹Šê²Œ ë´ì•¼í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³  0ì€ í•´ë‹¹ í† í°ì„ ì‹ ê²½ ì“°ì§€ ì•Šì•„ë„ ëœë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (ë‹¤ì‹œ ë§í•´, 0ì— í•´ë‹¹í•˜ëŠ” í† í°ì€ ëª¨ë¸ì˜ ì–´í…ì…˜ ë ˆì´ì–´ì—ì„œ ë¬´ì‹œë˜ì–´ì•¼ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.)

ì–´í…ì…˜ ë§ˆìŠ¤í¬ì™€ í•¨ê»˜ ì´ì „ ì˜ˆì‹œë¥¼ ì™„ì„±í•´ë´…ì‹œë‹¤.

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

ì´ì œ ë°°ì¹˜ ë‚´ ë‘ ë²ˆì§¸ ë¬¸ì¥ê³¼ ë™ì¼í•œ ë¡œì§“ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.

ë‘ ë²ˆì§¸ ë¬¸ì¥ì˜ ì–´í…ì…˜ ë§ˆìŠ¤í¬ì—ì„œ ë§ˆì§€ë§‰ ê°’ì¸ 0ì€ íŒ¨ë”© IDë¼ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** 2ì¥ì—ì„œ ì‚¬ìš©í•œ ë‘ ê°œì˜ ë¬¸ì¥("I've been waiting for a HuggingFace course my whole life." and "I hate this so much!")ì„ ì´ìš©í•´ ì§ì ‘ í† í°í™”ë¥¼ ì ìš©í•´ë³´ì„¸ìš”. í† í°í™” ê²°ê³¼ë¥¼ ëª¨ë¸ì— ë„˜ê¸°ê³  2ì¥ì—ì„œ ì–»ì€ ê²ƒê³¼ ë™ì¼í•œ ë¡œì§“ì„ ì–»ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”. ì´ì œ Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model!

</Tip>

## ê¸¸ì´ê°€ ê¸´ ì‹œí€€ìŠ¤[[longer-sequences]]

Transformer ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ, ëª¨ë¸ì— ë„˜ê²¨ì¤„ ìˆ˜ ìˆëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ì— ì œí•œì´ ìˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ìµœëŒ€ 512ê°œë‚˜ 1024ê°œì˜ í† í°ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ë” ê¸´ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•´ë‹¬ë¼ëŠ” ìš”ì²­ì„ ë°›ìœ¼ë©´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë¬¸ì œì— ëŒ€í•œ í•´ê²° ë°©ë²•ì€ 2ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

- ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ì˜ë¼ë‚´ì„¸ìš”.

ëª¨ë¸ë³„ë¡œ ì§€ì›í•˜ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” ë‹¤ë¥´ê³  ëª‡ ê°œì˜ íŠ¹ë³„í•œ ëª¨ë¸ì€ ì—„ì²­ë‚˜ê²Œ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) ê°€ ê·¸ ì¤‘ í•˜ë‚˜ì´ë©°, [LED](https://huggingface.co/transformers/model_doc/led.html)ë„ í•´ë‹¹í•©ë‹ˆë‹¤. ë§Œì•½ ë§¤ìš° ê¸´ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ë¤„ì•¼ í•˜ëŠ” íƒœìŠ¤í¬ë¥¼ ì§„í–‰í•˜ê³  ìˆë‹¤ë©´, ë‘ ëª¨ë¸ì„ ì‚´í´ë³´ì„¸ìš”.

ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ `max_sequence_length` íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì˜ë¼ë‚´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

```py
sequence = sequence[:max_sequence_length]
```
