<FrameworkSwitchCourse {fw} />

# í•œ ë²ˆì— ì‹¤í–‰í•˜ê¸°[[putting-it-all-together]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
]} />

{/if}

ì§€ë‚œ ì„¹ì…˜ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì˜ ê³¼ì •ì„ í•˜ë‚˜ì”© ìˆ˜í–‰í•´ì™”ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì˜ ì‘ë™ ë°©ì‹ì„ ì‚´í´ë³´ê³  í† í°í™”, ì…ë ¥ IDë¡œì˜ ë³€í™˜, íŒ¨ë”©, ì˜ë¼ë‚´ê¸° ê·¸ë¦¬ê³  ì–´í…ì…˜ ë§ˆìŠ¤í¬ì— ëŒ€í•´ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ 2ì¥ì—ì„œ ë³´ì•˜ë“¯ì´ ìš°ë¦¬ëŠ” ğŸ¤— Transformers APIì˜ ê³ ìˆ˜ì¤€ í•¨ìˆ˜ë¡œ ì´ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì¥ì„ ì´ìš©í•´ `tokenizer`ë¥¼ í˜¸ì¶œí•˜ë©´ ëª¨ë¸ë¡œ ë„˜ê²¨ì¤„ ìˆ˜ ìˆëŠ” ì…ë ¥ì„ ì–»ê²Œ ë©ë‹ˆë‹¤.

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ì´ì œ `model_inputs` ë³€ìˆ˜ëŠ” ëª¨ë¸ì´ ì˜ ë™ì‘í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. DistilBERTëŠ” ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¿ë§Œ ì•„ë‹ˆë¼ ì…ë ¥ IDë„ í¬í•¨í•©ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì…ë ¥ì„ ë°›ëŠ” ë‹¤ë¥¸ ëª¨ë¸ë“¤ë„ `tokenizer` ê°ì²´ì— ì˜í•´ ìƒê¸°ëŠ” ê²°ê³¼ë¬¼ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´ tokenizer ë©”ì„œë“œëŠ” ë§¤ìš° ê°•ë ¥í•©ë‹ˆë‹¤. ë¨¼ì €, ì´ ë©”ì„œë“œëŠ” ë‹¨ì¼ ì‹œí€€ìŠ¤ë¥¼ í† í°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ë˜í•œ APIì˜ ë³€ê²½ ì—†ì´ ì—¬ëŸ¬ ê°œì˜ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

ì›í•˜ëŠ”ëŒ€ë¡œ íŒ¨ë”©ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
# ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
model_inputs = tokenizer(sequences, padding="longest")

# ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
# (BERTë‚˜ DistilBERTì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 512)
model_inputs = tokenizer(sequences, padding="max_length")

# ì§€ì •í•œ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì˜ë¼ë‚¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ê²Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì˜ë¼ëƒ…ë‹ˆë‹¤.
# (BERTë‚˜ DistilBERTì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 512)
model_inputs = tokenizer(sequences, truncation=True)

# ì§€ì •í•œ ìµœëŒ€ ê¸¸ì´ì— ë§ê²Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì˜ë¼ëƒ…ë‹ˆë‹¤.
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` ê°ì²´ë¥¼ ì´ìš©í•´ ê²°ê³¼ë¥¼ íŠ¹ì • í”„ë ˆì„ì›Œí¬ì˜ í…ì„œë¡œ ë³€í™˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì— ë°”ë¡œ ë³´ë‚´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ ì½”ë“œ ì˜ˆì‹œì—ì„œ í† í¬ë‚˜ì´ì €ê°€ í”„ë ˆì„ì›Œí¬ì— ë”°ë¼ ë‹¤ë¥¸ í…ì„œë¥¼ ë°˜í™˜í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤ - `"pt"`ëŠ” PyTorch í…ì„œë¥¼ ë°˜í™˜í•˜ê³  `"tf"`ëŠ” TensorFlow í…ì„œë¥¼ ë°˜í™˜í•˜ë©°, `"np"`ëŠ” NumPy ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# TensorFlow í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# NumPy ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## íŠ¹ìˆ˜ í† í°[[special-tokens]]

í† í¬ë‚˜ì´ì €ê°€ ë°˜í™˜í•œ ì…ë ¥ IDë¥¼ ìì„¸íˆ ì‚´í´ë³´ë©´ ì´ì „ì— ë´¤ë˜ ê²°ê³¼ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

ì‹œì‘ê³¼ ëì— ì¶”ê°€ëœ í† í° IDê°€ ìˆìŠµë‹ˆë‹¤. ë‘ ì‹œí€€ìŠ¤ì˜ IDê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë””ì½”ë”©í•´ë³´ê² ìŠµë‹ˆë‹¤.

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

í† í¬ë‚˜ì´ì €ëŠ” ë¬¸ì¥ì´ ì‹œì‘í•  ë–„ `[CLS]`ë¼ëŠ” íŠ¹ë³„í•œ í† í°ì„ ë¶™ì´ê³ , ëë‚  ë•ŒëŠ” `[SEP]` í† í°ì„ ë¶™ì…ë‹ˆë‹¤. ì´ëŸ° íŠ¹ë³„í•œ í† í°ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ëª¨ë¸ì´ ì‚¬ì „í•™ìŠµë  ë•Œ ì´ í† í°ë“¤ì„ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ì¶”ë¡  ì‹œ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì°¸ê³ ë¡œ ëª‡ëª‡ ëª¨ë¸ì€ íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•˜ì§€ ì•Šì•„ë„ ë˜ê³ , ì–´ë–¤ ëª¨ë¸ì€ ë‹¤ë¥¸ í† í°ì„ ì¶”ê°€í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ë˜í•œ, ì´ëŸ¬í•œ íŠ¹ìˆ˜ í† í°ì„ ì‹œì‘ ë¶€ë¶„ì´ë‚˜ ë ë¶€ë¶„ì—ë§Œ ì¶”ê°€í•˜ëŠ” ëª¨ë¸ë„ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ê²½ìš°ë“  í† í¬ë‚˜ì´ì €ëŠ” í† í¬ë‚˜ì´ì €ë¡œ ì–´ë–¤ ë‚´ìš©ì´ ë“¤ì–´ì˜¬ì§€ ì•Œê³  ìˆê³  ì´ ë‚´ìš©ì„ ì²˜ë¦¬í•´ì¤„ ê²ƒì…ë‹ˆë‹¤.

## ë§ˆë¬´ë¦¬: í† í¬ë‚˜ì´ì €ì—ì„œ ëª¨ë¸ê¹Œì§€[[wrapping-up-from-tokenizer-to-model]]

ì§€ê¸ˆê¹Œì§€ `tokenizer` ê°ì²´ê°€ í…ìŠ¤íŠ¸ì— ì ìš©ë  ë•Œ ê±°ì¹˜ëŠ” ê°œë³„ì ì¸ ë‹¨ê³„ë¥¼ ëª¨ë‘ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ ê°ì²´ê°€ íŒ¨ë”©ì„ ì´ìš©í•´ ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€, ì˜ë¼ë‚´ê¸°ë¥¼ í†µí•´ ë§¤ìš° ê¸´ ë¬¸ì¥ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€, ì£¼ìš” APIì— ë”°ë¼ ë‹¤ì–‘í•œ í…ì„œë¥¼ ë‹¤ë£¨ëŠ” ë²•ì„ ì•Œì•„ë´…ì‹œë‹¤.

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
