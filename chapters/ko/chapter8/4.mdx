<FrameworkSwitchCourse {fw} />

# í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section4_pt.ipynb"},
]} />

[ë‹¨ì› 7](/course/chapter7)ì˜ ì¡°ì–¸ì„ ì¶©ì‹¤íˆ ë”°ë¼ ì£¼ì–´ì§„ ì‘ì—…ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜ íŒŒì¸íŠœë‹ í•˜ëŠ” ì•„ë¦„ë‹¤ìš´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ `trainer.train()` ëª…ë ¹ì„ ì‹¤í–‰í•˜ë©´ ë”ì°í•œ ì¼ì´ ë°œìƒí•©ë‹ˆë‹¤. ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤ ğŸ˜±! ë˜ëŠ” ë” ë‚˜ìœ ê²ƒì€ ëª¨ë“  ê²ƒì´ ì •ìƒì¸ ê²ƒì²˜ëŸ¼ ë³´ì´ê³  í•™ìŠµì´ ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ë˜ì§€ë§Œ ê²°ê³¼ ëª¨ë¸ì€ ì—‰ë§ì…ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ì´ëŸ¬í•œ ì¢…ë¥˜ì˜ ë¬¸ì œë¥¼ ë””ë²„ê·¸í•˜ê¸° ìœ„í•´ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

## í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…

<Youtube id="L-WSwUWde1U"/>

`trainer.train()`ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œì˜ ë¬¸ì œëŠ” `Trainer`ê°€ ì¼ë°˜ì ìœ¼ë¡œ ë§ì€ ê²ƒì„ ê²°í•©í•˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ë¥¼ ë°ì´í„° ë¡œë”ë¡œ ë³€í™˜í•˜ë¯€ë¡œ ë°ì´í„° ì„¸íŠ¸ì— ë¬¸ì œê°€ ìˆê±°ë‚˜ ë°ì´í„° ì„¸íŠ¸ì˜ ìš”ì†Œë¥¼ í•¨ê»˜ ì¼ê´„ ì²˜ë¦¬í•˜ë ¤ê³  í•  ë•Œ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë°ì´í„° ë°°ì¹˜ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ê³µê¸‰í•˜ë¯€ë¡œ ë¬¸ì œê°€ ëª¨ë¸ ì½”ë“œì— ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³  ìµœì í™” ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ë¯€ë¡œ ë¬¸ì œê°€ ì˜µí‹°ë§ˆì´ì €ì—ë„ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ê²ƒì´ í•™ìŠµì— ì í•©í•˜ë”ë¼ë„ í‰ê°€ í•¨ìˆ˜ì— ë¬¸ì œê°€ ìˆìœ¼ë©´ í‰ê°€ ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`trainer.train()`ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ ë””ë²„ê·¸í•˜ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ ì´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‚´í´ë³´ê³  ë¬¸ì œê°€ ë°œìƒí•œ ë¶€ë¶„ì„ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ê¸°ê°€ ë§¤ìš° ì‰¬ìš´ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

ì´ë¥¼ ì‹œì—°í•˜ê¸° ìœ„í•´ [MNLI ë°ì´í„° ì„¸íŠ¸](https://huggingface.co/datasets/glue)ì—ì„œ DistilBERT ëª¨ë¸ì„ íŒŒì¸íŠœë‹(ì‹œë„í•˜ëŠ”)í•˜ëŠ” ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.:

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ê³  í•˜ë©´ ë‹¤ì†Œ ì‹ ë¹„í•œ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.:

```python out
'ValueError: You have to specify either input_ids or inputs_embeds'
```

### ë°ì´í„° í™•ì¸

ë§í•  í•„ìš”ë„ ì—†ì´, ë°ì´í„°ê°€ ì†ìƒë˜ë©´ `Trainer`ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ë¬¼ë¡  ë°°ì¹˜ë¥¼ í˜•ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë¨¼ì € í•™ìŠµ ì„¸íŠ¸ ë‚´ë¶€ì— ë¬´ì—‡ì´ ìˆëŠ”ì§€ ì‚´í´ë³´ì•„ì•¼ í•©ë‹ˆë‹¤.

ë²„ê·¸ì˜ ì›ì¸ì´ ì•„ë‹Œ ê²ƒì„ ìˆ˜ì •í•˜ëŠ” ë° ìˆ˜ë§ì€ ì‹œê°„ì„ ì†Œë¹„í•˜ì§€ ì•Šìœ¼ë ¤ë©´ ì²´í¬í•  ë•Œ `trainer.train_dataset`ì„ ì‚¬ìš©í•˜ê³  ë‹¤ë¥¸ ê²ƒì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ í•´ë³´ì„¸ìš”.:

```py
trainer.train_dataset[0]
```

```python out
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```
ë­”ê°€ ì˜ëª»ëœ ê²ƒì„ ëˆˆì¹˜ì±„ì…¨ë‚˜ìš”? `input_ids`ê°€ ëˆ„ë½ë˜ì—ˆë‹¤ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìê°€ ì•„ë‹ˆë¼ í…ìŠ¤íŠ¸ë¼ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•„ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì›ë˜ ì—ëŸ¬ëŠ” `Trainer`ê°€ ëª¨ë¸ì˜ ì…ë ¥ íŒŒë¼ë¯¸í„°(ì¦‰, ëª¨ë¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” ì¸ìˆ˜)ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ì—´ì„ ìë™ìœ¼ë¡œ ì œê±°í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì´ ì˜ˆì‹œì—ì„œëŠ” ë ˆì´ë¸”ì„ ì œì™¸í•œ ëª¨ë“  ê²ƒì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë°°ì¹˜ë¥¼ ìƒì„±í•œ ë‹¤ìŒ ëª¨ë¸ë¡œ ë³´ë‚´ëŠ” ë° ë¬¸ì œê°€ ì—†ì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ëª¨ë¸ì€ ì ì ˆí•œ ì…ë ¥ì„ ë°›ì§€ ëª»í–ˆë‹¤ê³  ë¶ˆí‰í•œ ê²ƒì…ë‹ˆë‹¤.

ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ê° ìƒ˜í”Œì— í† í¬ë‚˜ì´ì €ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ ë°ì´í„°ì…‹ì— `Dataset.map()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì½”ë“œë¥¼ ìì„¸íˆ ë³´ë©´ í›ˆë ¨ ë° í‰ê°€ ì„¸íŠ¸ë¥¼ `Trainer`ì— ì „ë‹¬í•  ë•Œ ì‹¤ìˆ˜ í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `tokenized_datasets` ëŒ€ì‹  'raw_datasets' ğŸ¤¦ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ìˆ˜ì • í•´ë´…ì‹œë‹¤!

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```
ìƒˆë¡œìš´ ì½”ë“œëŠ” ì´ì œ ë‹¤ë¥¸ ì—ëŸ¬(ì–´ì¨Œë“  ì§„ì „!)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.:

```python out
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```
tracebackì„ ë³´ë©´ ë°ì´í„° ì •ë ¬ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.:

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

ì´ì œ ë¬¸ì œê°€ ìˆëŠ” ê³³ìœ¼ë¡œ ì´ë™í•´ì•¼í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ì „ì— ë°ì´í„° ê²€ì‚¬ë¥¼ ë§ˆì³ì„œ ë°ì´í„°ê°€ 100% ì •í™•í•œì§€ í™•ì¸í•©ì‹œë‹¤.

í•™ìŠµ ì„¸ì…˜ì„ ë””ë²„ê¹…í•  ë•Œ í•­ìƒ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” í•œ ê°€ì§€ëŠ” ëª¨ë¸ì˜ ì…ë ¥ê°’ì„ ë””ì½”ë”©í•´ì„œ ì‚´í´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì—ê²Œ ì…ë ¥í•˜ëŠ” ìˆ«ìë¥¼ ì´í•´í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í•´ë‹¹ ìˆ«ìê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì‚´í´ë´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì»´í“¨í„° ë¹„ì „ì—ì„œ ì´ëŠ” ì „ë‹¬í•œ í”½ì…€ì˜ ë””ì½”ë”©ëœ ê·¸ë¦¼ì„ ë³´ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³ , ìŒì„±ì—ì„œëŠ” ë””ì½”ë”©ëœ ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ë“£ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì—¬ê¸° NLP ì˜ˆì œì˜ ê²½ìš° ì…ë ¥ì„ ë””ì½”ë”©í•˜ê¸° ìœ„í•´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.:

```py
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python out
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

í™•ì¸í•´ë³´ë‹ˆ ë§ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì˜ ëª¨ë“  í‚¤ì— ëŒ€í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.:

```py
trainer.train_dataset[0].keys()
```

```python out
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

ëª¨ë¸ì—ì„œ í—ˆìš©í•˜ì§€ ì•ŠëŠ” í‚¤ê°’ì€ ìë™ìœ¼ë¡œ íê¸°ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” `input_ids`, `attention_mask` ë° `label`(ì´ë¦„ì´ `labels`ë¡œ ë³€ê²½ë¨)ë§Œ ìœ ì§€í•©ë‹ˆë‹¤. ëª¨ë¸ ì •ë³´ë¥¼ í™•ì‹¤í•˜ê²Œ í™•ì¸í•˜ë ¤ë©´ ëª¨ë¸ì˜ í´ë˜ìŠ¤ë¥¼ ì¶œë ¥í•´ë³¸ ë‹¤ìŒ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.:

```py
type(trainer.model)
```

```python out
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

ìœ„ ì½”ë“œì˜ ê²½ìš° [ì´ í˜ì´ì§€](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification)ì—ì„œ í—ˆìš©ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Trainer`ëŠ” ë²„ë¦¬ëŠ” ì—´ë„ ê¸°ë¡í•©ë‹ˆë‹¤.

input IDsë¥¼ ë””ì½”ë”©í•˜ì—¬ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ `attention_mask` ì°¨ë¡€ ì…ë‹ˆë‹¤.:

```py
trainer.train_dataset[0]["attention_mask"]
```

```python out
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```
ì „ì²˜ë¦¬ì—ì„œ íŒ¨ë”©ì„ ì ìš©í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ìœ„ ê°’ì€ ì™„ë²½í•˜ê²Œ ìì—°ìŠ¤ëŸ¬ì›Œ ë³´ì…ë‹ˆë‹¤. attention maskì— ë¬¸ì œê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ input IDs ì™€ ê¸¸ì´ê°€ ê°™ì€ì§€ í™•ì¸í•©ë‹ˆë‹¤.:

```py
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python out
True
```

ì¢‹ìŠµë‹ˆë‹¤! ë§ˆì§€ë§‰ìœ¼ë¡œ ë ˆì´ë¸”ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.:

```py
trainer.train_dataset[0]["label"]
```

```python out
1
```

input IDs ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ìˆ«ìë§Œìœ¼ë¡œëŠ” ì˜ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ì „ì— ë³´ì•˜ë“¯ì´ ì •ìˆ˜ì™€ ë ˆì´ë¸” ì´ë¦„ ì‚¬ì´ì˜ ë§µì€ ë°ì´í„°ì„¸íŠ¸ì˜ í•´ë‹¹ *feature*ì˜ `names` ì†ì„± ë‚´ë¶€ì— ì €ì¥ë©ë‹ˆë‹¤.:

```py
trainer.train_dataset.features["label"].names
```

```python out
['entailment', 'neutral', 'contradiction']
```

ë”°ë¼ì„œ `1`ì€ `ì¤‘ë¦½`ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ìœ„ì—ì„œ ë³¸ ë‘ ë¬¸ì¥ì´ ëª¨ìˆœë˜ì§€ ì•Šê³  ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤!

ì—¬ê¸°ì—ëŠ” token type IDs ê°€ ì—†ìŠµë‹ˆë‹¤. DistilBERTëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. token type IDsë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ ê²½ìš° ì…ë ¥ì—ì„œ ì²« ë²ˆì§¸ ë° ë‘ ë²ˆì§¸ ë¬¸ì¥ì´ ìˆëŠ” ìœ„ì¹˜ì™€ ì˜¬ë°”ë¥´ê²Œ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

<Tip>

âœï¸ **ì—¬ëŸ¬ë¶„ ì°¨ë¡€ì…ë‹ˆë‹¤!** í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì˜ ë‘ ë²ˆì§¸ ì›ì†Œê°€ ì •ìƒì ì¸ì§€ í™•ì¸í•´ë³´ì„¸ìš”.

</Tip>

ì—¬ê¸°ì—ì„  í•™ìŠµ ì„¸íŠ¸ì— ëŒ€í•´ì„œë§Œ í™•ì¸í•˜ì§€ë§Œ,  ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ë° í‰ê°€ ì„¸íŠ¸ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

ë°ì´í„° ì„¸íŠ¸ê°€ ë¬¸ì œ ì—†ìœ¼ë¯€ë¡œ ì´ì œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ í™•ì¸í•  ì°¨ë¡€ì…ë‹ˆë‹¤.

### ë°ì´í„°ì„¸íŠ¸ì—ì„œ ë°ì´í„°ë¡œë”ê¹Œì§€

í•™ìŠµ íŒŒì´í”„ë¼ì¸ì—ì„œ ë‹¤ìŒìœ¼ë¡œ ì˜ëª»ë  ìˆ˜ ìˆëŠ” ì‚¬í•­ì€ `Trainer`ê°€ í•™ìŠµ ë˜ëŠ” ê²€ì¦ ì„¸íŠ¸ì—ì„œ ë°°ì¹˜ë¥¼ í˜•ì„±í•˜ë ¤ê³  í•  ë•Œì…ë‹ˆë‹¤. `Trainer`ì˜ ë°ì´í„° ì„¸íŠ¸ê°€ ì •í™•í•˜ë‹¤ê³  í™•ì‹ í•˜ë©´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì—¬ ì§ì ‘ ë°°ì¹˜ë¥¼ í˜•ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê²€ì¦ ë°ì´í„° ë¡œë”ì˜ ê²½ìš° `train`ì„ `eval`ë¡œ ëŒ€ì²´).:

```py
for batch in trainer.get_train_dataloader():
    break
```

ì´ ì½”ë“œëŠ” í•™ìŠµ ë°ì´í„° ë¡œë”ë¥¼ ìƒì„±í•œ ë‹¤ìŒ ë°˜ë³µí•˜ë©° ì²« ë²ˆì§¸ ë°˜ë³µì—ì„œ ì¤‘ì§€í•©ë‹ˆë‹¤. ì½”ë“œê°€ ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ë˜ë©´ ê²€ì‚¬í•  ìˆ˜ ìˆëŠ” ì²« ë²ˆì§¸ í•™ìŠµ ë°°ì¹˜ë¥¼ ì–»ê²Œ ë˜ë©°, ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë¬¸ì œê°€ ë°ì´í„° ë¡œë”ì— ìˆìŒì„ í™•ì‹¤íˆ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.:

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

Tracebackì˜ ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ì¡°ì‚¬í•˜ë©´ ë‹¨ì„œë¥¼ ì œê³µí•˜ê¸° ì¶©ë¶„í• í…Œì§€ë§Œ ì¡°ê¸ˆ ë” íŒŒí—¤ì³ ë³´ê² ìŠµë‹ˆë‹¤. ë°°ì¹˜ ìƒì„± ì¤‘ ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œëŠ” ì˜ˆì œë¥¼ ë‹¨ì¼ ë°°ì¹˜ë¡œ ì¡°í•©í•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ë¯€ë¡œ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš° ê°€ì¥ ë¨¼ì € í™•ì¸í•´ì•¼ í•  ê²ƒì€ `DataLoader`ê°€ ì‚¬ìš©í•˜ëŠ” `collate_fn`ì…ë‹ˆë‹¤.:

```py
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python out
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

ìœ„ ì½”ë“œëŠ” `default_data_collator`ì´ì§€ë§Œ, ì´ ê²½ìš°ì—ëŠ” ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. 'DataCollatorWithPadding' collatorì— ì˜í•´ ìˆ˜í–‰ë˜ëŠ” ë°°ì¹˜ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ìœ¼ë¡œ íŒ¨ë”©ì„ í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ë°ì´í„° ì½œë ˆì´í„°ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `Trainer`ì— ì˜í•´ ì‚¬ìš©ëœë‹¤ê³  í•˜ëŠ”ë° ì—¬ê¸°ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?

ê·¸ ì´ìœ ëŠ” ìš°ë¦¬ê°€ `Trainer`ì— `tokenizer`ë¥¼ ì „ë‹¬í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì›í•˜ëŠ” `DataCollatorWithPadding`ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ ì´ëŸ¬í•œ ì¢…ë¥˜ì˜ ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ë ¤ëŠ” data collatorë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ì£¼ì €í•´ì„  ì•ˆ ë©ë‹ˆë‹¤. ì½”ë“œë¥¼ ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ë´…ì‹œë‹¤.:

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```
ì¢‹ì€ ë‰´ìŠ¤ì¼ê¹Œìš”? ì´ì „ê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í™•ì‹¤íˆ ì§„í–‰ ì¤‘ì´ë€ ëœ»ì´ì§€ìš”. ë‚˜ìœ ì†Œì‹ì€? ëŒ€ì‹  ì•…ëª… ë†’ì€ CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.:

```python out
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```
CUDA ì˜¤ë¥˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë””ë²„ê·¸í•˜ê¸°ê°€ ë§¤ìš° ì–´ë µê¸° ë•Œë¬¸ì— ì¢‹ì§€ ì•Šì€ ìƒí™©ì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì ì‹œ í›„ì— ì‚´í´ë³´ê² ì§€ë§Œ ë¨¼ì € ë°°ì¹˜ ìƒì„±ì— ëŒ€í•œ ë¶„ì„ì„ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤.

data collatorê°€ ì •ìƒì´ë¼ê³  í™•ì‹ í•˜ëŠ” ê²½ìš° ë°ì´í„° ì„¸íŠ¸ì˜ ëª‡ ê°€ì§€ ìƒ˜í”Œì— ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.:

```py
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```
`train_dataset`ì— `Trainer`ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë¬¸ìì—´ ì—´ì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ ì½”ë“œëŠ” ì‹¤íŒ¨í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ ìˆ˜ë™ìœ¼ë¡œ ì œê±°í•˜ê±°ë‚˜ `Trainer`ê°€ ë¬´ëŒ€ ë’¤ì—ì„œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì„ ì •í™•íˆ ë³µì œí•˜ë ¤ë©´ í•´ë‹¹ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë¹„ê³µê°œ `Trainer._remove_unused_columns()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.:

```py
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```
ê·¸ëŸ° ë‹¤ìŒ ì˜¤ë¥˜ê°€ ì§€ì†ë˜ëŠ” ê²½ìš° data collator â€‹â€‹ë‚´ë¶€ì—ì„œ ë°œìƒí•˜ëŠ” ì¼ì„ ì§ì ‘ ë””ë²„ê·¸í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

ë°°ì¹˜ ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ë””ë²„ê¹…í–ˆìœ¼ë¯€ë¡œ ì´ì œ ëª¨ë¸ì„ í†µí•´ ì „ë‹¬í•  ì°¨ë¡€ì…ë‹ˆë‹¤!

### ëª¨ë¸ ì‚´í´ë³´ê¸°

ì•„ë˜ ëª…ë ì–´ë¥¼ ì‹¤í–‰í•¨ìœ¼ë¡œì¨ ë°°ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.:

```py
for batch in trainer.get_train_dataloader():
    break
```

ë…¸íŠ¸ë¶ì—ì„œ ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ì´ì „ì— ë³¸ ê²ƒê³¼ ìœ ì‚¬í•œ CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë…¸íŠ¸ë¶ì„ ë‹¤ì‹œ ì‹œì‘í•˜ê³  `trainer.train()` í–‰ ì—†ì´ ë§ˆì§€ë§‰ ìŠ¤ë‹ˆí«ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. CUDA ì˜¤ë¥˜ì— ëŒ€í•´ ë‘ ë²ˆì§¸ë¡œ ì§œì¦ë‚˜ëŠ” ì ì€ ì»¤ë„ì„ ë³µêµ¬í•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ ë§ê°€ëœ¨ë¦¬ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°€ì¥ ì§œì¦ë‚˜ëŠ” ì ì€ ë””ë²„ê¹…í•˜ê¸° ì–´ë µë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.

ì™œ ê·¸ëŸ´ê¹Œìš”? ì´ê±´ GPU ì‘ë™ ë°©ì‹ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ë§ì€ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ë° ë§¤ìš° íš¨ìœ¨ì ì´ì§€ë§Œ, ì´ëŸ¬í•œ ëª…ë ¹ ì¤‘ í•˜ë‚˜ì˜ ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œ ì´ë¥¼ ì¦‰ì‹œ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì´ GPUì—ì„œ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ì˜ ë™ê¸°í™”ë¥¼ í˜¸ì¶œí•  ë•Œë§Œ ë¬¸ì œê°€ ë°œìƒí–ˆìŒì„ ê¹¨ë‹«ê²Œ ë˜ë¯€ë¡œ ì‹¤ì œë¡œ ì—ëŸ¬ë¥¼ ë§Œë“  ê³³ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ìœ„ì¹˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ì „ Tracebackì„ ë³´ë©´ ì—­ë°©í–¥ íŒ¨ìŠ¤ ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” ìˆœë°©í–¥ ì¤‘ ì–´ë”˜ê°€ì—ì„œ ë¹„ë¡¯ëœ ì—ëŸ¬ì„ì„ ê³§ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ ì´ëŸ¬í•œ ì—ëŸ¬ë¥¼ ì–´ë–»ê²Œ ë””ë²„ê¹…í• ê¹Œìš”? ë‹µì€ ê°„ë‹¨í•©ë‹ˆë‹¤. í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. CUDA ì˜¤ë¥˜ê°€ ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬(GPUì— ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸)ê°€ ì•„ë‹Œ í•œ í•­ìƒ CPUë¡œ ëŒì•„ê°€ì„œ ë””ë²„ê¹…í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ê²½ìš° ëª¨ë¸ì„ CPUì— ë‹¤ì‹œ ë†“ê³  ë°°ì¹˜ì—ì„œ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤. `DataLoader`ê°€ ë°˜í™˜í•œ ë°°ì¹˜ëŠ” ì•„ì§ GPUë¡œ ì´ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.:

```python
outputs = trainer.model.cpu()(**batch)
```

```python out
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```
ì´ì œ ê·¸ë¦¼ì´ ì„ ëª…í•´ì§€ë„¤ìš”. CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ëŒ€ì‹  ì´ì œ ë¡œìŠ¤ ê³„ì‚°ì— 'IndexError'ê°€ ìˆìŠµë‹ˆë‹¤(ì•ì„œ ë§í–ˆë“¯ì´ ì—­ë°©í–¥ íŒ¨ìŠ¤ì™€ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤). ë” ì •í™•í•˜ê²ŒëŠ” ì˜¤ë¥˜ë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ìƒ 2 ì„ì„ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë¸ì˜ ë ˆì´ë¸” ìˆ˜ë¥¼ í™•ì¸í•˜ê¸°ì— ì•„ì£¼ ì¢‹ì€ ìˆœê°„ì…ë‹ˆë‹¤.:

```python
trainer.model.config.num_labels
```

```python out
2
```
ë‘ ê°œì˜ ë ˆì´ë¸”ì„ ì‚¬ìš©í•˜ë©´ 0ê³¼ 1ë§Œ ì •ë‹µìœ¼ë¡œ í—ˆìš©ë˜ì§€ë§Œ ì—ëŸ¬ ë©”ì‹œì§€ì— ë”°ë¥´ë©´ 2ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 2ë¥¼ ì–»ëŠ” ê²ƒì€ ì‹¤ì œë¡œ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ì „ì— ì¶”ì¶œí•œ ë ˆì´ë¸” ì´ë¦„ì„ ê¸°ì–µí•´ë³´ë©´ 3ê°œê°€ ìˆì—ˆìœ¼ë¯€ë¡œ ì¸ë±ìŠ¤ëŠ” 0, 1, 2ê°€ ë°ì´í„°ì„¸íŠ¸ì— ìˆìŠµë‹ˆë‹¤. ë¬¸ì œëŠ” ì„¸ ê°œì˜ ë ˆì´ë¸”ë¡œ ìƒì„±ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ì„ ëª¨ë¸ì— ì•Œë ¤ì£¼ì§€ ì•Šì•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì œ ìˆ˜ì •í•´ ë´…ì‹œë‹¤!

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```
ëª¨ë“  ê²ƒì´ ê´œì°®ì€ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì•„ì§ `trainer.train()` ë¼ì¸ì„ í¬í•¨í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°°ì¹˜ë¥¼ ìš”ì²­í•˜ê³  ëª¨ë¸ì— ì „ë‹¬í•˜ë©´ ì´ì œ ì—ëŸ¬ ì—†ì´ ì‘ë™í•©ë‹ˆë‹¤!

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```
ë‹¤ìŒ ë‹¨ê³„ëŠ” GPUë¡œ ëŒì•„ê°€ ëª¨ë“  ê²ƒì´ ì—¬ì „íˆ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```
ì—¬ì „íˆ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë…¸íŠ¸ë¶ì„ ë‹¤ì‹œ ì‹œì‘í•˜ê³  ìŠ¤í¬ë¦½íŠ¸ì˜ ë§ˆì§€ë§‰ ë²„ì „ë§Œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

### í•œë²ˆì˜ ìµœì í™” ë‹¨ê³„ ìˆ˜í–‰

ì´ì œ ì‹¤ì œë¡œ ëª¨ë¸ì„ í†µê³¼í•˜ëŠ” ë°°ì¹˜ë¥¼ ë¹Œë“œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ìœ¼ë¯€ë¡œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ ë‹¤ìŒ ë‹¨ê³„ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ìµœì í™” ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.:

ì²« ë²ˆì§¸ ë¶€ë¶„ì€ ë¡œìŠ¤ì— ëŒ€í•´ `backward()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.:

```py
loss = outputs.loss
loss.backward()
```
ì´ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²ƒì€ ë§¤ìš° ë“œë¬¼ì§€ë§Œ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ CPUë¡œ ëŒì•„ê°€ ìœ ìš©í•œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°›ìœ¼ì„¸ìš”.

ìµœì í™” ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ `optimizer`ë¥¼ ë§Œë“¤ê³  `step()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.:

```py
trainer.create_optimizer()
trainer.optimizer.step()
```
ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, `Trainer`ì—ì„œ ê¸°ë³¸ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•˜ì§€ë§Œ ì‚¬ìš©ì ì§€ì • ì˜µí‹°ë§ˆì´ì €ê°€ ìˆëŠ” ê²½ìš° ì—¬ê¸°ì—ì„œ ë””ë²„ê¹…ì— ëª‡ ê°€ì§€ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ ì´ìƒí•œ CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ CPUë¡œ ëŒì•„ê°€ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”. CUDA ì˜¤ë¥˜ì— ëŒ€í•´ ë§í•˜ìë©´, ì•ì„œ ìš°ë¦¬ëŠ” íŠ¹ë³„í•œ ê²½ìš°ë¥¼ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. ê·¸ ë¶€ë¶„ì„ ì§€ê¸ˆë¶€í„° ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### CUDA ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ ë‹¤ë£¨ê¸°

`RuntimeError: CUDA out of memory`ë¡œ ì‹œì‘í•˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ëŠ” GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ ì½”ë“œì— ì§ì ‘ ì—°ê²°ë˜ì§€ ì•Šìœ¼ë©° ì™„ë²½í•˜ê²Œ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì—ëŸ¬ëŠ” GPUì˜ ë‚´ë¶€ ë©”ëª¨ë¦¬ì— ë„ˆë¬´ ë§ì€ ê²ƒì„ ë„£ìœ¼ë ¤ê³  í•´ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ë¥¸ CUDA ì˜¤ë¥˜ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµì„ ë‹¤ì‹œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìœ„ì¹˜ì—ì„œ ì»¤ë„ì„ ë‹¤ì‹œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ GPU ê³µê°„ì„ ì ê²Œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. ë¨¼ì € GPUì— ë™ì‹œì— ë‘ ê°œì˜ ëª¨ë¸ì´ ìˆì§€ ì•Šì€ì§€ í™•ì¸í•©ë‹ˆë‹¤(ë¬¼ë¡  ë¬¸ì œ í•´ê²°ì— í•„ìš”í•œ ê²½ìš° ì œì™¸). ê·¸ëŸ° ë‹¤ìŒ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ëª¨ë“  ì¤‘ê°„ ê²°ê³¼ê°’ í¬ê¸°ì™€ ê¸°ìš¸ê¸°ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë” ì‘ì€ ëª¨ë¸ ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

<Tip>

ì½”ìŠ¤ì˜ ë‹¤ìŒ ë¶€ë¶„ì—ì„œëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ê°€ì¥ í° ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ ê¸°ìˆ ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

</Tip>

### ëª¨ë¸ í‰ê°€í•˜ê¸°

ì´ì œ ì½”ë“œì˜ ëª¨ë“  ë¬¸ì œë¥¼ í•´ê²°í–ˆìœ¼ë¯€ë¡œ ëª¨ë“  ê²ƒì´ ì™„ë²½í•˜ê³  í•™ìŠµì´ ì›í™œí•˜ê²Œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë ‡ì£ ? ê·¸ë ‡ê²Œ ë¹ ë¥´ì§„ ì•ŠìŠµë‹ˆë‹¤! `trainer.train()` ëª…ë ¹ì„ ì‹¤í–‰í•˜ë©´ ì²˜ìŒì—ëŠ” ëª¨ë“  ê²ƒì´ ì¢‹ì•„ ë³´ì´ì§€ë§Œ ì ì‹œ í›„ ë‹¤ìŒì˜ ì¶œë ¥ì„ ë³´ê²Œ ë©ë‹ˆë‹¤.:
Now that we've solved all the issues with our code, everything is perfect and the training should run smoothly, right? Not so fast! If you run the `trainer.train()` command, everything will look good at first, but after a while you will get the following:

```py
# This will take a long time and error out, so you shouldn't run this cell
trainer.train()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```
í‰ê°€ ë‹¨ê³„ì—ì„œ ì´ ì˜¤ë¥˜ê°€ ë‚˜íƒ€ë‚¨ì„ ì•Œê²Œ ë˜ì—ˆì„ ê±´ë°ìš”, ì´ê±´ ê³§ ë§ˆì§€ë§‰ìœ¼ë¡œ ë””ë²„ê¹…í•´ì•¼ í•  ì‚¬í•­ì„ì„ ëœ»í•©ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ í›ˆë ¨ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ `Trainer`ì˜ í‰ê°€ ë£¨í”„ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.:

```py
trainer.evaluate()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

<Tip>

ğŸ’¡ ì—ëŸ¬ê°€ ë°œìƒí•˜ê¸° ì „ì— ë§ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ë‚­ë¹„í•˜ì§€ ì•Šë„ë¡ í•­ìƒ `trainer.train()`ì„ ì‹¤í–‰í•˜ê¸° ì „ì— `trainer.evaluate()`ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

</Tip>

í‰ê°€ ë£¨í”„ì—ì„œ ë¬¸ì œë¥¼ ë””ë²„ê¹…í•˜ê¸° ì „ì— ë¨¼ì € ë°ì´í„°ë¥¼ ì‚´í´ë³´ì•˜ëŠ”ì§€, ë°°ì¹˜ë¥¼ ì ì ˆí•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€, ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë“  ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìœ¼ë¯€ë¡œ ë‹¤ìŒì˜ ì½”ë“œë¥¼ ì—ëŸ¬ ì—†ì´ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.:

```py
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

ì—ëŸ¬ëŠ” ë‚˜ì¤‘ì— í‰ê°€ ë‹¨ê³„ê°€ ëë‚  ë•Œ ë°œìƒí•˜ë©° Tracebackì„ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œë©ë‹ˆë‹¤.:

```python trace
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

ì´ê±´ ì—ëŸ¬ê°€ `datasets/metric.py` ëª¨ë“ˆì—ì„œ ë°œìƒí–ˆìŒì„ ì•Œë ¤ì¤ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ê²ƒì€ `compute_metrics()` í•¨ìˆ˜ì˜ ë¬¸ì œì…ë‹ˆë‹¤. ë¡œì§“ê³¼ ë ˆì´ë¸”ì˜ íŠœí”Œì„ NumPy ë°°ì—´ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì…ë ¥í•´ ë³´ê² ìŠµë‹ˆë‹¤.:

```py
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```
ë™ì¼í•œ ì—ëŸ¬ê°€ ë°œìƒí•˜ë¯€ë¡œ ë¬¸ì œëŠ” ë¶„ëª…íˆ í•´ë‹¹ ê¸°ëŠ¥ì— ìˆìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ë‹¤ì‹œ ë³´ë©´ `predictions`ì™€ `labels`ë¥¼ `metric.compute()`ë¡œ ì „ë‹¬í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ê·¸ ë°©ë²•ì— ë¬¸ì œê°€ ìˆëŠ” ê±¸ê¹Œìš”? ì„¤ë§ˆ... í˜•íƒœì„ ê°„ë‹¨íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.:

```py
predictions.shape, labels.shape
```

```python out
((8, 3), (8,))
```
ìš°ë¦¬ì˜ ì˜ˆì¸¡ì€ ì‹¤ì œ ì˜ˆì¸¡ê°’ì´ ì•„ë‹ˆë¼ ì—¬ì „íˆ ë¡œì§“ì…ë‹ˆë‹¤. ì´ê²ƒì´ ë©”íŠ¸ë¦­ì´ ì´ (ë‹¤ì†Œ ëª¨í˜¸í•œ) ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤. ìˆ˜ì •ì€ ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤. `compute_metrics()` í•¨ìˆ˜ì— argmaxë¥¼ ì¶”ê°€í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.:

```py
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))
```

```python out
{'accuracy': 0.625}
```
ì´ì œ ì—ëŸ¬ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤! ì´ê²Œ ë§ˆì§€ë§‰ì´ì—ˆìœ¼ë¯€ë¡œ ì´ì œ ìŠ¤í¬ë¦½íŠ¸ê°€ ëª¨ë¸ì„ ì œëŒ€ë¡œ í•™ìŠµì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤.

ì°¸ê³ ë¡œ ì•„ë˜ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì™„ì „íˆ ìˆ˜ì •ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.:

```py
import numpy as np
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```
ì´ ê²½ìš° ë” ì´ìƒ ë¬¸ì œê°€ ì—†ìœ¼ë©° ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª¨ë¸ì„ íŒŒì¸íŠœë‹ í•  ê²ƒì´ê³  í•©ë¦¬ì ì¸ ê²°ê³¼ë¥¼ ì œê³µí•´ì¤„ ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•™ìŠµì´ ì—ëŸ¬ ì—†ì´ ì§„í–‰ë˜ì—ˆê³  í•™ìŠµëœ ëª¨ë¸ì´ ì „í˜€ ì˜ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ ìš°ë¦¬ëŠ” ë¬´ì—‡ì„ í•  ìˆ˜ ìˆì„ê¹Œìš”? ì´ê²ƒì´ ê¸°ê³„ í•™ìŠµì˜ ê°€ì¥ ì–´ë ¤ìš´ ë¶€ë¶„ì´ë©° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ê¸°ìˆ ì„ ë³´ì—¬ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

<Tip>

ğŸ’¡ ìˆ˜ë™ í•™ìŠµ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ë””ë²„ê·¸í•˜ê¸° ìœ„í•´ ë™ì¼í•œ ë‹¨ê³„ê°€ ì ìš©ë˜ì§€ë§Œ ë” ì‰½ê²Œ ë¶„ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì˜ `model.eval()` ë˜ëŠ” `model.train()` ë˜ëŠ” ê° ë‹¨ê³„ì˜ `zero_grad()`ë¥¼ ìŠì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!

</Tip>

## í•™ìŠµ ì¤‘ ì¡°ìš©í•œ ì—ëŸ¬ ë””ë²„ê¹…

ì—ëŸ¬ ì—†ì´ ì™„ë£Œë˜ì§€ë§Œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•˜ëŠ” í•™ìŠµì„ ë””ë²„ê·¸í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? ì—¬ê¸°ì—ì„œ ëª‡ ê°€ì§€ ì§€ì¹¨ì„ ì œê³µí•˜ê² ì§€ë§Œ ì´ëŸ¬í•œ ì¢…ë¥˜ì˜ ë””ë²„ê¹…ì€ ê¸°ê³„ í•™ìŠµì—ì„œ ê°€ì¥ ì–´ë ¤ìš´ ë¶€ë¶„ì´ë©° ë§ˆë²• ê°™ì€ ë‹µì€ ì—†ë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”.

### ë°ì´í„° í™•ì¸(ë‹¤ì‹œ!)

ëª¨ë¸ì€ ë°ì´í„°ì—ì„œ ì‹¤ì œë¡œ ë­”ê°€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê²½ìš°ì—ë§Œ í•™ìŠµí•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì†ìƒì‹œí‚¤ëŠ” ë²„ê·¸ê°€ ìˆê±°ë‚˜ ë ˆì´ë¸”ì´ ë¬´ì‘ìœ„ë¡œ ì§€ì •ëœ ê²½ìš° ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ í•™ìŠµì„ ì œëŒ€ë¡œ ì§„í–‰í•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í•­ìƒ ë””ì½”ë”©ëœ ì…ë ¥ê³¼ ë ˆì´ë¸”ì„ ë‹¤ì‹œ í™•ì¸í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê³  ë‹¤ìŒì˜ ì§ˆë¬¸ì„ ìŠ¤ìŠ¤ë¡œì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”.:

- ë””ì½”ë”©ëœ ë°ì´í„°ê°€ ì´í•´í• ë§Œí•œì§€?
- ë ˆì´ë¸”ì— ë‚©ë“í•  ìˆ˜ ìˆëŠ”ì§€?
- ë‹¤ë¥¸ ë ˆì´ë¸”ë³´ë‹¤ ì •ë‹µì— ê°€ê¹Œìš´ ë ˆì´ë¸”ì´ ìˆëŠ”ì§€?
- ëª¨ë¸ì´ ë¬´ì‘ìœ„ ë‹µ, ì–¸ì œë‚˜ ê°™ì€ ë‹µì„ ì˜ˆì¸¡í•  ê²½ìš° ì–´ë–¤ ì†ì‹¤ í•¨ìˆ˜ì™€ ë§¤íŠ¸ë¦­ì„ ì •í•´ì•¼í• ì§€?

<Tip warning={true}>

ë¶„ì‚° í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš° ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œì„ ì¶œë ¥í•˜ê³  ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆëŠ”ì§€ ì„¸ ë²ˆ í™•ì¸í•˜ì„¸ìš”. í•œ ê°€ì§€ ì¼ë°˜ì ì¸ ë²„ê·¸ëŠ” ë°ì´í„° ìƒì„± ì‹œ ê° í”„ë¡œì„¸ìŠ¤ê°€ ì„œë¡œ ë‹¤ë¥¸ ë²„ì „ì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ê°–ë„ë¡ ë§Œë“œëŠ” ì„ì˜ì„±ì˜ ì›ì¸ì´ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

</Tip>

ë°ì´í„°ë¥¼ ì‚´í´ë³¸ í›„ ëª¨ë¸ì˜ ëª‡ ê°€ì§€ ì˜ˆì¸¡ì„ ì‚´í´ë³´ê³  ë””ì½”ë”©í•©ë‹ˆë‹¤. ëª¨ë¸ì´ í•­ìƒ ë™ì¼í•œ ê²ƒì„ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš° ë°ì´í„° ì„¸íŠ¸ê°€ í•˜ë‚˜ì˜ ë²”ì£¼(ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš°)ë¡œ í¸í–¥ë˜ì–´ ìˆê¸° ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ê·€ í´ë˜ìŠ¤ë¥¼ ì˜¤ë²„ìƒ˜í”Œë§í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ê¸°ìˆ ì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ˆê¸° ëª¨ë¸ì—ì„œ ì–»ì€ ì†ì‹¤ê°’/ë©”íŠ¸ë¦­ê°’ì´ ë¬´ì‘ìœ„ ì˜ˆì¸¡ì— ëŒ€í•´ ì˜ˆìƒí•œ ì†ì‹¤ê°’/ë©”íŠ¸ë¦­ê°’ê³¼ ë§¤ìš° ë‹¤ë¥¸ ê²½ìš° ë²„ê·¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì†ì‹¤ê°’ ë˜ëŠ” ë©”íŠ¸ë¦­ê°’ì´ ê³„ì‚°ë˜ëŠ” ë°©ì‹ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”. ë§ˆì§€ë§‰ì— ì¶”ê°€ì ì¸ ì—¬ëŸ¬ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë™ì¼í•œ í¬ê¸°ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.

ë°ì´í„°ê°€ ì™„ë²½í•˜ë‹¤ê³  í™•ì‹ í•˜ëŠ” ê²½ìš°, ëª¨ë¸ì´ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆëŠ”ì§€ í•œë²ˆ ê°„ë‹¨í•˜ê²Œ í…ŒìŠ¤íŠ¸ í•´ë³´ì„¸ìš”. 

### í•œë²ˆì˜ ë°°ì¹˜ì— ëª¨ë¸ ê³¼ì í•© í•´ë³´ê¸°


Overfitting is usually something we try to avoid when training, as it means the model is not learning to recognize the general features we want it to but is instead just memorizing the training samples. However, trying to train your model on one batch over and over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high.

Doing this once you have defined your `Trainer` is really easy; just grab a batch of training data, then run a small manual training loop only using that batch for something like 20 steps:

```py
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

<Tip>

ğŸ’¡ If your training data is unbalanced, make sure to build a batch of training data containing all the labels.

</Tip>

The resulting model should have close-to-perfect results on the same `batch`. Let's compute the metric on the resulting predictions:

```py
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python out
{'accuracy': 1.0}
```

100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on any other sentence, it will very likely give you a wrong answer)!

If you don't manage to have your model obtain perfect results like this, it means there is something wrong with the way you framed the problem or your data, so you should fix that. Only when you manage to pass the overfitting test can you be sure that your model can actually learn something.

<Tip warning={true}>

âš ï¸ You will have to recreate your model and your `Trainer` after this test, as the model obtained probably won't be able to recover and learn something useful on your full dataset.

</Tip>

### Don't tune anything until you have a first baseline

Hyperparameter tuning is always emphasized as being the hardest part of machine learning, but it's just the last step to help you gain a little bit on the metric. Most of the time, the default hyperparameters of the `Trainer` will work just fine to give you good results, so don't launch into a time-consuming and costly hyperparameter search until you have something that beats the baseline you have on your dataset.

Once you have a good enough model, you can start tweaking a bit. Don't try launching a thousand runs with different hyperparameters, but compare a couple of runs with different values for one hyperparameter to get an idea of which has the greatest impact.

If you are tweaking the model itself, keep it simple and don't try anything you can't reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn't had any unintended consequences.

### Ask for help

Hopefully you will have found some advice in this section that helped you solve your issue, but if that's not the case, remember you can always ask the community on the [forums](https://discuss.huggingface.co/). 

Here are some additional resources that may prove helpful:

- ["Reproducibility as a vehicle for engineering best practices"](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) by Joel Grus
- ["Checklist for debugging neural networks"](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) by Cecelia Shao
- ["How to unit test machine learning code"](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) by Chase Roberts
- ["A Recipe for Training Neural Networks"](http://karpathy.github.io/2019/04/25/recipe/) by Andrej Karpathy

Of course, not every problem you encounter when training neural nets is your own fault! If you encounter something in the ğŸ¤— Transformers or ğŸ¤— Datasets library that does not seem right, you may have encountered a bug. You should definitely tell us all about it, and in the next section we'll explain exactly how to do that.
