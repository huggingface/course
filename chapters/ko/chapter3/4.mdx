# ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ëŠ” í›ˆë ¨ ë£¨í”„[[a-full-training-loop]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

ì´ì œ `Trainer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìµœì‹  PyTorch ëª¨ë²” ì‚¬ë¡€ë¡œ ì²˜ìŒë¶€í„° í›ˆë ¨ ë£¨í”„ë¥¼ êµ¬í˜„í•˜ì—¬ ì§€ë‚œ ì„¹ì…˜ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, ì„¹ì…˜ 2ì—ì„œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ë‹¤ë£¨ëŠ” ê°„ë‹¨í•œ ìš”ì•½ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

<Tip>

ğŸ—ï¸ **ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ê¸°**: ì´ ì„¹ì…˜ì€ ì´ì „ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. PyTorch í›ˆë ¨ ë£¨í”„ì™€ ëª¨ë²” ì‚¬ë¡€ì— ëŒ€í•œ í¬ê´„ì ì¸ ê°€ì´ë“œëŠ” [ğŸ¤— Transformers í›ˆë ¨ ë¬¸ì„œ](https://huggingface.co/docs/transformers/main/en/training#train-in-native-pytorch)ì™€ [ì»¤ìŠ¤í…€ í›ˆë ¨ ì¿¡ë¶](https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu#model)ì„ í™•ì¸í•˜ì„¸ìš”.

</Tip>

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### í›ˆë ¨ ì¤€ë¹„[[prepare-for-training]]

ì‹¤ì œë¡œ í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ê°ì²´ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ë°°ì¹˜ë¥¼ ë°˜ë³µí•˜ëŠ” ë° ì‚¬ìš©í•  ë°ì´í„°ë¡œë”ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°ì´í„°ë¡œë”ë¥¼ ì •ì˜í•˜ê¸° ì „ì— `tokenized_datasets`ì— ì•½ê°„ì˜ í›„ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì—¬ `Trainer`ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•œ ëª‡ ê°€ì§€ ì‘ì—…ì„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒì´ í•„ìš”í•©ë‹ˆë‹¤:

- ëª¨ë¸ì´ ì˜ˆìƒí•˜ì§€ ì•ŠëŠ” ê°’ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ ì œê±°(`sentence1` ë° `sentence2` ì»¬ëŸ¼ê³¼ ê°™ì€).
- `label` ì»¬ëŸ¼ì„ `labels`ë¡œ ì´ë¦„ ë³€ê²½(ëª¨ë¸ì´ ì¸ìˆ˜ ì´ë¦„ì„ `labels`ë¡œ ì˜ˆìƒí•˜ê¸° ë•Œë¬¸).
- ë°ì´í„°ì…‹ í˜•ì‹ì„ ì„¤ì •í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  PyTorch í…ì„œë¥¼ ë°˜í™˜í•˜ë„ë¡.

`tokenized_datasets`ì—ëŠ” ì´ëŸ¬í•œ ê° ë‹¨ê³„ì— ëŒ€í•œ ë©”ì„œë“œê°€ ìˆìŠµë‹ˆë‹¤:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ê·¸ëŸ° ë‹¤ìŒ ê²°ê³¼ì— ëª¨ë¸ì´ í—ˆìš©í•  ì»¬ëŸ¼ë§Œ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

ì´ì œ ì´ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ë°ì´í„°ë¡œë”ë¥¼ ì‰½ê²Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ë°ì´í„° ì²˜ë¦¬ì— ì‹¤ìˆ˜ê°€ ì—†ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ë°°ì¹˜ë¥¼ ê²€ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

í›ˆë ¨ ë°ì´í„°ë¡œë”ì— ëŒ€í•´ `shuffle=True`ë¥¼ ì„¤ì •í•˜ê³  ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©í•˜ë¯€ë¡œ ì‹¤ì œ ëª¨ì–‘ì€ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì™„ì „íˆ ë§ˆì³¤ìœ¼ë¯€ë¡œ(ëª¨ë“  ML ì‹¤ë¬´ìì—ê²Œ ë§Œì¡±ìŠ¤ëŸ½ì§€ë§Œ ì°¾ê¸° ì–´ë ¤ìš´ ëª©í‘œ), ëª¨ë¸ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. ì´ì „ ì„¹ì…˜ì—ì„œì™€ ì •í™•íˆ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

í›ˆë ¨ ì¤‘ì— ëª¨ë“  ê²ƒì´ ì›í™œí•˜ê²Œ ì§„í–‰ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ ë°°ì¹˜ë¥¼ ì´ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ëª¨ë“  ğŸ¤— Transformers ëª¨ë¸ì€ `labels`ê°€ ì œê³µë  ë•Œ ì†ì‹¤ì„ ë°˜í™˜í•˜ë©°, ë¡œì§“ë„ ì–»ìŠµë‹ˆë‹¤(ë°°ì¹˜ì˜ ê° ì…ë ¥ì— ëŒ€í•´ 2ê°œì”©, ë”°ë¼ì„œ 8 x 2 í¬ê¸°ì˜ í…ì„œ).

í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•  ì¤€ë¹„ê°€ ê±°ì˜ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ë‘ ê°€ì§€ë§Œ ë¹ ì¡ŒìŠµë‹ˆë‹¤. `Trainer`ê°€ ìˆ˜í–‰í•œ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•˜ë ¤ê³  í•˜ë¯€ë¡œ ë™ì¼í•œ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. `Trainer`ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜µí‹°ë§ˆì´ì €ëŠ” `AdamW`ë¡œ, ê°€ì¤‘ì¹˜ ê°ì†Œ ì •ê·œí™”ì— ëŒ€í•œ ë³€í˜•ì´ ìˆëŠ” Adamê³¼ ë™ì¼í•©ë‹ˆë‹¤(Ilya Loshchilovì™€ Frank Hutterì˜ ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) ì°¸ì¡°):

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

<Tip>

ğŸ’¡ **ìµœì‹  ìµœì í™” íŒ**: ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ë‹¤ìŒì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **ê°€ì¤‘ì¹˜ ê°ì†Œê°€ ìˆëŠ” AdamW**: `AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)`
- **8ë¹„íŠ¸ Adam**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìµœì í™”ë¥¼ ìœ„í•´ `bitsandbytes` ì‚¬ìš©
- **ë‹¤ë¥¸ í•™ìŠµë¥ **: ëŒ€í˜• ëª¨ë¸ì—ì„œëŠ” ë” ë‚®ì€ í•™ìŠµë¥ (1e-5 ~ 3e-5)ì´ ì¢…ì¢… ë” ì˜ ì‘ë™

ğŸš€ **ìµœì í™” ë¦¬ì†ŒìŠ¤**: ì˜µí‹°ë§ˆì´ì €ì™€ í›ˆë ¨ ì „ëµì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ğŸ¤— Transformers ìµœì í™” ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/en/performance#optimizer)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

ë§ˆì§€ë§‰ìœ¼ë¡œ, ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ìµœëŒ€ê°’(5e-5)ì—ì„œ 0ê¹Œì§€ì˜ ì„ í˜• ê°ì†Œì…ë‹ˆë‹¤. ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì •ì˜í•˜ë ¤ë©´ ìˆ˜í–‰í•  í›ˆë ¨ ë‹¨ê³„ ìˆ˜ë¥¼ ì•Œì•„ì•¼ í•˜ëŠ”ë°, ì´ëŠ” ì‹¤í–‰í•˜ë ¤ëŠ” ì—í¬í¬ ìˆ˜ì— í›ˆë ¨ ë°°ì¹˜ ìˆ˜(í›ˆë ¨ ë°ì´í„°ë¡œë”ì˜ ê¸¸ì´)ë¥¼ ê³±í•œ ê²ƒì…ë‹ˆë‹¤. `Trainer`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 3ê°œì˜ ì—í¬í¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ë¥¼ ë”°ë¥´ê² ìŠµë‹ˆë‹¤:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### í›ˆë ¨ ë£¨í”„[[the-training-loop]]

ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€: GPUì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë‹¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤(CPUì—ì„œëŠ” í›ˆë ¨ì´ ëª‡ ë¶„ ëŒ€ì‹  ëª‡ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤). ì´ë¥¼ ìœ„í•´ ëª¨ë¸ê³¼ ë°°ì¹˜ë¥¼ ì˜¬ë¦´ `device`ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

ì´ì œ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ì´ ì–¸ì œ ëë‚ ì§€ ê°ì„ ì¡ê¸° ìœ„í•´ `tqdm` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë‹¨ê³„ ìˆ˜ì— ëŒ€í•œ ì§„í–‰ë¥  í‘œì‹œì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

<Tip>

ğŸ’¡ **ìµœì‹  í›ˆë ¨ ìµœì í™”**: í›ˆë ¨ ë£¨í”„ë¥¼ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ë ¤ë©´ ë‹¤ìŒì„ ê³ ë ¤í•˜ì„¸ìš”:

- **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**: `optimizer.step()` ì „ì— `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)` ì¶”ê°€
- **í˜¼í•© ì •ë°€ë„**: ë” ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ `torch.cuda.amp.autocast()`ì™€ `GradScaler` ì‚¬ìš©
- **ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì **: ì—¬ëŸ¬ ë°°ì¹˜ì— ê±¸ì³ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ì—¬ ë” í° ë°°ì¹˜ í¬ê¸° ì‹œë®¬ë ˆì´ì…˜
- **ì²´í¬í¬ì¸íŒ…**: í›ˆë ¨ì´ ì¤‘ë‹¨ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥

ğŸ”§ **êµ¬í˜„ ê°€ì´ë“œ**: ì´ëŸ¬í•œ ìµœì í™”ì˜ ìì„¸í•œ ì˜ˆì œëŠ” [ğŸ¤— Transformers íš¨ìœ¨ì ì¸ í›ˆë ¨ ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one)ì™€ [ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €](https://huggingface.co/docs/transformers/main/en/optimizers)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

í›ˆë ¨ ë£¨í”„ì˜ í•µì‹¬ì´ ì†Œê°œ ë¶€ë¶„ì˜ ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë³´ê³ ë„ ìš”ì²­í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì´ í›ˆë ¨ ë£¨í”„ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì— ëŒ€í•´ ì•„ë¬´ê²ƒë„ ì•Œë ¤ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ í‰ê°€ ë£¨í”„ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.


### í‰ê°€ ë£¨í”„[[the-evaluation-loop]]

As we did earlier, we will use a metric provided by the ğŸ¤— Evaluate library. We've already seen the `metric.compute()` method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method `add_batch()`. Once we have accumulated all the batches, we can get the final result with `metric.compute()`. Here's how to implement all of this in an evaluation loop:

<Tip>

ğŸ“Š **Evaluation Best Practices**: For more sophisticated evaluation strategies and metrics, explore the [ğŸ¤— Evaluate documentation](https://huggingface.co/docs/evaluate/) and the [comprehensive evaluation cookbook](https://github.com/huggingface/evaluation-guidebook).

</Tip>

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Again, your results will be slightly different because of the randomness in the model head initialization and the data shuffling, but they should be in the same ballpark.

<Tip>

âœï¸ **Try it out!** Modify the previous training loop to fine-tune your model on the SST-2 dataset.

</Tip>

### Supercharge your training loop with ğŸ¤— Accelerate[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

The training loop we defined earlier works fine on a single CPU or GPU. But using the [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate) library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. ğŸ¤— Accelerate handles the complexity of distributed training, mixed precision, and device placement automatically. Starting from the creation of the training and validation dataloaders, here is what our manual training loop looks like:

<Tip>

âš¡ **Accelerate Deep Dive**: Learn everything about distributed training, mixed precision, and hardware optimization in the [ğŸ¤— Accelerate documentation](https://huggingface.co/docs/accelerate/) and explore practical examples in the [transformers documentation](https://huggingface.co/docs/transformers/main/en/accelerate).

</Tip>

```py
from accelerate import Accelerator
from torch.optim import AdamW
from transformers import AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

The first line to add is the import line. The second line instantiates an `Accelerator` object that will look at the environment and initialize the proper distributed setup. ğŸ¤— Accelerate handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use `accelerator.device` instead of `device`).

Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimizer to `accelerator.prepare()`. This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the `device` (again, if you want to keep this you can just change it to use `accelerator.device`) and replacing `loss.backward()` with `accelerator.backward(loss)`.

<Tip>
âš ï¸ In order to benefit from the speed-up offered by Cloud TPUs, we recommend padding your samples to a fixed length with the `padding="max_length"` and `max_length` arguments of the tokenizer.
</Tip>

If you'd like to copy and paste it to play around, here's what the complete training loop looks like with ğŸ¤— Accelerate:

```py
from accelerate import Accelerator
from torch.optim import AdamW
from transformers import AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Putting this in a `train.py` script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command:

```bash
accelerate config
```

which will prompt you to answer a few questions and dump your answers in a configuration file used by this command:

```
accelerate launch train.py
```

which will launch the distributed training.

If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a `training_function()` and run a last cell with:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

You can find more examples in the [ğŸ¤— Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples).

<Tip>

ğŸŒ **Distributed Training**: For comprehensive coverage of multi-GPU and multi-node training, check out the [ğŸ¤— Transformers distributed training guide](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many) and the [scaling training cookbook](https://huggingface.co/docs/transformers/main/en/accelerate).

</Tip>

### Next Steps and Best Practices[[next-steps-and-best-practices]]

Now that you've learned how to implement training from scratch, here are some additional considerations for production use:

**Model Evaluation**: Always evaluate your model on multiple metrics, not just accuracy. Use the ğŸ¤— Evaluate library for comprehensive evaluation.

**Hyperparameter Tuning**: Consider using libraries like Optuna or Ray Tune for systematic hyperparameter optimization.

**Model Monitoring**: Track training metrics, learning curves, and validation performance throughout training.

**Model Sharing**: Once trained, share your model on the Hugging Face Hub to make it available to the community.

**Efficiency**: For large models, consider techniques like gradient checkpointing, parameter-efficient fine-tuning (LoRA, AdaLoRA), or quantization methods.

This concludes our deep dive into fine-tuning with custom training loops. The skills you've learned here will serve you well when you need full control over the training process or want to implement custom training logic that goes beyond what the `Trainer` API offers.

## Section Quiz[[section-quiz]]

Test your understanding of custom training loops and advanced training techniques:

### 1. What is the main difference between Adam and AdamW optimizers?

<Question
	choices={[
		{
			text: "AdamW uses a different learning rate schedule.",
			explain: "Learning rate scheduling is separate from the optimizer choice."
		},
		{
			text: "AdamW includes decoupled weight decay regularization.",
			explain: "Correct! AdamW separates weight decay from the gradient-based parameter updates, leading to better regularization.",
            correct: true
		},
		{
			text: "AdamW only works with transformer models.",
			explain: "AdamW can be used with any model architecture, not just transformers."
		},
        {
			text: "AdamW requires less memory than Adam.",
			explain: "Both optimizers have similar memory requirements."
		}
	]}
/>

### 2. In a training loop, what is the correct order of operations?

<Question
	choices={[
		{
			text: "Forward pass â†’ Backward pass â†’ Optimizer step â†’ Zero gradients",
			explain: "Close, but you should zero gradients before the next forward pass to avoid accumulating old gradients."
		},
		{
			text: "Forward pass â†’ Backward pass â†’ Optimizer step â†’ Scheduler step â†’ Zero gradients",
			explain: "Correct! This is the proper order: compute loss, compute gradients, update parameters, update learning rate, then clear gradients.",
            correct: true
		},
		{
			text: "Zero gradients â†’ Forward pass â†’ Optimizer step â†’ Backward pass",
			explain: "The backward pass must come after the forward pass to compute gradients from the loss."
		},
        {
			text: "Forward pass â†’ Zero gradients â†’ Backward pass â†’ Optimizer step",
			explain: "Zeroing gradients before backward pass would eliminate the gradients you just computed."
		}
	]}
/>

### 3. What does the ğŸ¤— Accelerate library primarily help with?

<Question
	choices={[
		{
			text: "Making your models train faster by optimizing the forward pass.",
			explain: "Accelerate doesn't optimize the model architecture itself."
		},
		{
			text: "Automatically selecting the best hyperparameters.",
			explain: "Accelerate doesn't do hyperparameter optimization."
		},
		{
			text: "Enabling distributed training across multiple GPUs/TPUs with minimal code changes.",
			explain: "Correct! Accelerate handles distributed training complexity, allowing your code to run on single or multiple devices seamlessly.",
            correct: true
		},
        {
			text: "Converting models to different frameworks like TensorFlow.",
			explain: "Accelerate works within PyTorch and doesn't convert between frameworks."
		}
	]}
/>

### 4. Why do we move batches to the device in a training loop?

<Question
	choices={[
		{
			text: "To make the training faster.",
			explain: "While it can affect speed, the main reason is compatibility."
		},
		{
			text: "Because the model and data must be on the same device (CPU/GPU) for computation.",
			explain: "Correct! PyTorch requires tensors to be on the same device for operations to work.",
            correct: true
		},
		{
			text: "To save memory.",
			explain: "Moving to device doesn't inherently save memory."
		},
        {
			text: "It's required by the DataLoader.",
			explain: "DataLoader doesn't require specific device placement."
		}
	]}
/>

### 5. What does `model.eval()` do before evaluation?

<Question
	choices={[
		{
			text: "It freezes the model parameters so they can't be updated.",
			explain: "model.eval() doesn't freeze parameters - that would be done by setting requires_grad=False."
		},
		{
			text: "It changes the behavior of layers like dropout and batch normalization for inference.",
			explain: "Correct! eval() mode disables dropout and uses running statistics for batch norm instead of computing them from the current batch.",
            correct: true
		},
		{
			text: "It enables gradient computation for evaluation metrics.",
			explain: "Actually, we typically use torch.no_grad() during evaluation to disable gradient computation."
		},
        {
			text: "It automatically calculates evaluation metrics.",
			explain: "model.eval() only changes layer behavior - you still need to implement metric calculation separately."
		}
	]}
/>

### 6. What is the purpose of `torch.no_grad()` during evaluation?

<Question
	choices={[
		{
			text: "To prevent the model from making predictions.",
			explain: "torch.no_grad() doesn't prevent predictions, just gradient computation."
		},
		{
			text: "To save memory and speed up computation by disabling gradient tracking.",
			explain: "Correct! Since we don't need gradients for evaluation, disabling them saves memory and computation.",
            correct: true
		},
		{
			text: "To enable evaluation mode for the model.",
			explain: "Evaluation mode is enabled with model.eval(), not torch.no_grad()."
		},
        {
			text: "To ensure consistent results across runs.",
			explain: "Reproducibility is handled by setting random seeds, not torch.no_grad()."
		}
	]}
/>

### 7. What changes when you use ğŸ¤— Accelerate in your training loop?

<Question
	choices={[
		{
			text: "You must rewrite your entire training loop from scratch.",
			explain: "Accelerate requires minimal changes to existing PyTorch code."
		},
		{
			text: "You wrap key objects with accelerator.prepare() and use accelerator.backward() instead of loss.backward().",
			explain: "Correct! These are the main changes - prepare your objects and use accelerator.backward() for proper distributed training.",
            correct: true
		},
		{
			text: "You need to specify the number of GPUs in your code.",
			explain: "Accelerate automatically detects available hardware."
		},
        {
			text: "You must use a different optimizer and scheduler.",
			explain: "You can use the same optimizers and schedulers with Accelerate."
		}
	]}
/>

<Tip>

ğŸ’¡ **Key Takeaways:**
- Manual training loops give you complete control but require understanding of the proper sequence: forward â†’ backward â†’ optimizer step â†’ scheduler step â†’ zero gradients
- AdamW with weight decay is the recommended optimizer for transformer models
- Always use `model.eval()` and `torch.no_grad()` during evaluation for correct behavior and efficiency
- ğŸ¤— Accelerate makes distributed training accessible with minimal code changes
- Device management (moving tensors to GPU/CPU) is crucial for PyTorch operations
- Modern techniques like mixed precision, gradient accumulation, and gradient clipping can significantly improve training efficiency

</Tip>
