# ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ëŠ” í›ˆë ¨ ë£¨í”„[[a-full-training-loop]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

ì´ì œ `Trainer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìµœì‹  PyTorch ëª¨ë²” ì‚¬ë¡€ë¥¼ í™œìš©í•´ ì²˜ìŒë¶€í„° í›ˆë ¨ ë£¨í”„ë¥¼ êµ¬í˜„í•˜ì—¬ ì´ì „ ì„¹ì…˜ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì„¹ì…˜ 2ì—ì„œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆë‹¤ê³  ê°€ì •í•˜ê³  ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. í•„ìš”í•œ ëª¨ë“  ë‚´ìš©ì„ ë‹´ì€ ê°„ë‹¨í•œ ìš”ì•½ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<Tip>

ğŸ—ï¸ **ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ê¸°**: ì´ ì„¹ì…˜ì€ ì´ì „ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. PyTorch í›ˆë ¨ ë£¨í”„ì™€ ëª¨ë²” ì‚¬ë¡€ì— ëŒ€í•œ í¬ê´„ì ì¸ ê°€ì´ë“œëŠ” [ğŸ¤— Transformers í›ˆë ¨ ë¬¸ì„œ](https://huggingface.co/docs/transformers/main/en/training#train-in-native-pytorch)ì™€ [ì»¤ìŠ¤í…€ í›ˆë ¨ ì¿¡ë¶](https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu#model)ì„ í™•ì¸í•˜ì„¸ìš”.

</Tip>

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### í›ˆë ¨ ì¤€ë¹„[[prepare-for-training]]

ì‹¤ì œë¡œ í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ê°ì²´ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ë°°ì¹˜ë¥¼ ë°˜ë³µí•˜ëŠ” ë° ì‚¬ìš©í•  ë°ì´í„°ë¡œë”ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°ì´í„°ë¡œë”ë¥¼ ì •ì˜í•˜ê¸° ì „ì— `tokenized_datasets`ì— ì•½ê°„ì˜ í›„ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” `Trainer`ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í–ˆë˜ ì‘ì—…ë“¤ì„ ì§ì ‘ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.

- ëª¨ë¸ì´ ì˜ˆìƒí•˜ì§€ ì•ŠëŠ” ê°’ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì„ ì œê±°í•©ë‹ˆë‹¤(`sentence1` ë° `sentence2` ì»¬ëŸ¼ ë“±).
- `label` ì»¬ëŸ¼ì„ `labels`ë¡œ ì´ë¦„ì„ ë³€ê²½í•©ë‹ˆë‹¤(ëª¨ë¸ì´ ì¸ìˆ˜ ì´ë¦„ì„ `labels`ë¡œ ì˜ˆìƒí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤).
- ë°ì´í„°ì…‹ í˜•ì‹ì„ ì„¤ì •í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  PyTorch í…ì„œë¥¼ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.

`tokenized_datasets`ì—ëŠ” ì´ëŸ¬í•œ ê° ë‹¨ê³„ì— ëŒ€í•œ ë©”ì„œë“œê°€ ìˆìŠµë‹ˆë‹¤.

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ê·¸ëŸ° ë‹¤ìŒ ê²°ê³¼ì— ëª¨ë¸ì´ í—ˆìš©í•  ì»¬ëŸ¼ë§Œ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

ì´ì œ ì´ ì‘ì—…ë“¤ì´ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ ë°ì´í„°ë¡œë”ë¥¼ ì •ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤.

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ë°ì´í„° ì²˜ë¦¬ê°€ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë°°ì¹˜ë¥¼ í•œë²ˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

í›ˆë ¨ ë°ì´í„°ë¡œë”ì— ëŒ€í•´ `shuffle=True`ë¥¼ ì„¤ì •í•˜ê³  ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©í•˜ë¯€ë¡œ ì‹¤ì œ ëª¨ì–‘ì€ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì™„ì „íˆ ë§ˆì³¤ìŠµë‹ˆë‹¤(ëª¨ë“  ML ì‹¤ë¬´ìë“¤ì—ê²ŒëŠ” ë§Œì¡±ìŠ¤ëŸ½ì§€ë§Œ ë‹¬ì„±í•˜ê¸° ì–´ë ¤ìš´ ëª©í‘œì£ !). ë‹¤ìŒìœ¼ë¡œ ëª¨ë¸ì„ ì¤€ë¹„í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ì „ ì„¹ì…˜ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

í›ˆë ¨ ì¤‘ì— ëª¨ë“  ê²ƒì´ ì›í™œí•˜ê²Œ ì§„í–‰ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ ë°°ì¹˜ë¥¼ ì´ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ëª¨ë“  ğŸ¤— Transformers ëª¨ë¸ì€ `labels`ê°€ ì œê³µë  ë•Œ ì†ì‹¤ì„ ë°˜í™˜í•˜ë©°, ë¡œì§“ë„ ì–»ìŠµë‹ˆë‹¤(ë°°ì¹˜ì˜ ê° ì…ë ¥ì— ëŒ€í•´ 2ê°œì”©, ë”°ë¼ì„œ 8 x 2 í¬ê¸°ì˜ í…ì„œ).

í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•  ì¤€ë¹„ê°€ ê±°ì˜ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ë‘ ê°€ì§€ë§Œ ë¹ ì¡ŒìŠµë‹ˆë‹¤. `Trainer`ê°€ ìˆ˜í–‰í•œ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ë³µì œí•˜ë ¤ê³  í•˜ë¯€ë¡œ ë™ì¼í•œ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. `Trainer`ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜µí‹°ë§ˆì´ì €ëŠ” `AdamW`ë¡œ, ê°€ì¤‘ì¹˜ ê°ì†Œ ì •ê·œí™”ì— ëŒ€í•œ ë³€í˜•ì´ ìˆëŠ” Adamê³¼ ë™ì¼í•©ë‹ˆë‹¤(Ilya Loshchilovì™€ Frank Hutterì˜ ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) ì°¸ì¡°)

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

<Tip>

ğŸ’¡ **ìµœì‹  ìµœì í™” íŒ**: ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ë‹¤ìŒì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ê°€ì¤‘ì¹˜ ê°ì†Œê°€ ìˆëŠ” AdamW**: `AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)`
- **8ë¹„íŠ¸ Adam**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìµœì í™”ë¥¼ ìœ„í•´ `bitsandbytes` ì‚¬ìš©
- **ë‹¤ë¥¸ í•™ìŠµë¥ **: ëŒ€í˜• ëª¨ë¸ì—ì„œëŠ” ë” ë‚®ì€ í•™ìŠµë¥ (1e-5 ~ 3e-5)ì´ ì¢…ì¢… ë” ì˜ ì‘ë™

ğŸš€ **ìµœì í™” ë¦¬ì†ŒìŠ¤**: ì˜µí‹°ë§ˆì´ì €ì™€ í›ˆë ¨ ì „ëµì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ğŸ¤— Transformers ìµœì í™” ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/en/performance#optimizer)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

ë§ˆì§€ë§‰ìœ¼ë¡œ, ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ìµœëŒ€ê°’(5e-5)ì—ì„œ 0ê¹Œì§€ì˜ ì„ í˜• ê°ì†Œì…ë‹ˆë‹¤. ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì •ì˜í•˜ë ¤ë©´ ìˆ˜í–‰í•  í›ˆë ¨ ë‹¨ê³„ ìˆ˜ë¥¼ ì•Œì•„ì•¼ í•˜ëŠ”ë°, ì´ëŠ” ì‹¤í–‰í•˜ë ¤ëŠ” ì—í¬í¬ ìˆ˜ì— í›ˆë ¨ ë°°ì¹˜ ìˆ˜(í›ˆë ¨ ë°ì´í„°ë¡œë”ì˜ ê¸¸ì´)ë¥¼ ê³±í•œ ê²ƒì…ë‹ˆë‹¤. `Trainer`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 3ê°œì˜ ì—í¬í¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ë¥¼ ë”°ë¥´ê² ìŠµë‹ˆë‹¤.

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### í›ˆë ¨ ë£¨í”„[[the-training-loop]]

ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ ë” ê³ ë ¤í•  ì ì´ ìˆìŠµë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ë©´ GPUë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤(CPUì—ì„œëŠ” í›ˆë ¨ì´ ëª‡ ë¶„ ëŒ€ì‹  ëª‡ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆê±°ë“ ìš”). ì´ë¥¼ ìœ„í•´ ëª¨ë¸ê³¼ ë°°ì¹˜ë¥¼ ë°°ì¹˜í•  `device`ë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

ì´ì œ í›ˆë ¨í•  ì¤€ë¹„ê°€ ëª¨ë‘ ëë‚¬ìŠµë‹ˆë‹¤! í›ˆë ¨ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ `tqdm` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ ì§„í–‰ë¥  í‘œì‹œì¤„ì„ ì¶”ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤.

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

<Tip>

ğŸ’¡ **ìµœì‹  í›ˆë ¨ ìµœì í™”**: í›ˆë ¨ ë£¨í”„ë¥¼ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ë ¤ë©´ ë‹¤ìŒì„ ê³ ë ¤í•˜ì„¸ìš”.

- **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**: `optimizer.step()` ì „ì— `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)` ì¶”ê°€
- **í˜¼í•© ì •ë°€ë„**: ë” ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ `torch.cuda.amp.autocast()`ì™€ `GradScaler` ì‚¬ìš©
- **ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì **: ì—¬ëŸ¬ ë°°ì¹˜ì— ê±¸ì³ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ì—¬ ë” í° ë°°ì¹˜ í¬ê¸° ì‹œë®¬ë ˆì´ì…˜
- **ì²´í¬í¬ì¸íŒ…**: í›ˆë ¨ì´ ì¤‘ë‹¨ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥

ğŸ”§ **êµ¬í˜„ ê°€ì´ë“œ**: ì´ëŸ¬í•œ ìµœì í™”ì˜ ìì„¸í•œ ì˜ˆì œëŠ” [ğŸ¤— Transformers íš¨ìœ¨ì ì¸ í›ˆë ¨ ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one)ì™€ [ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €](https://huggingface.co/docs/transformers/main/en/optimizers)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

ë³´ì‹œë‹¤ì‹œí”¼ í›ˆë ¨ ë£¨í”„ì˜ í•µì‹¬ êµ¬ì¡°ëŠ” ì†Œê°œ ë¶€ë¶„ì—ì„œ ë´¤ë˜ ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì¬ ìƒíƒœë¡œëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì— ëŒ€í•œ ì •ë³´ë¥¼ ì „í˜€ ì–»ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ê¸°ëŠ¥ì„ ìš”ì²­í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì´ì£ . ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í‰ê°€ ë£¨í”„ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.


### í‰ê°€ ë£¨í”„[[the-evaluation-loop]]

ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ğŸ¤— Evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. `metric.compute()` ë©”ì„œë“œëŠ” ì´ë¯¸ ì•Œê³  ìˆì§€ë§Œ ë©”íŠ¸ë¦­ì—ëŠ” ë˜ ë‹¤ë¥¸ ìœ ìš©í•œ ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤. `add_batch()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ì„œ ì˜ˆì¸¡ ë£¨í”„ë¥¼ ì§„í–‰í•˜ë©´ì„œ ë°°ì¹˜ë³„ë¡œ ê²°ê³¼ë¥¼ ëˆ„ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•œ í›„ì— `metric.compute()`ë¥¼ í˜¸ì¶œí•˜ë©´ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‰ê°€ ë£¨í”„ì—ì„œ ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

<Tip>

ğŸ“Š **í‰ê°€ ëª¨ë²” ì‚¬ë¡€**: ë” ì •êµí•œ í‰ê°€ ì „ëµê³¼ ë©”íŠ¸ë¦­ì— ëŒ€í•´ì„œëŠ” [ğŸ¤— Evaluate ë¬¸ì„œ](https://huggingface.co/docs/evaluate/)ì™€ [í‰ê°€ ì‹¤ë¬´ ê°€ì´ë“œ](https://github.com/huggingface/evaluation-guidebook)ë¥¼ ì‚´í´ë³´ì„¸ìš”.

</Tip>

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, ëª¨ë¸ í—¤ë“œ ì´ˆê¸°í™”ì˜ ë¬´ì‘ìœ„ì„±ê³¼ ë°ì´í„° ì…”í”Œë§ìœ¼ë¡œ ì¸í•´ ê²°ê³¼ê°€ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ê°™ì€ ìˆ˜ì¤€ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ì´ì „ í›ˆë ¨ ë£¨í”„ë¥¼ ìˆ˜ì •í•˜ì—¬ SST-2 ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•´ë³´ì„¸ìš”.

</Tip>

### ğŸ¤— Accelerateë¡œ í›ˆë ¨ ë£¨í”„ ê°•í™”í•˜ê¸°[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

ì•ì„œ ì •ì˜í•œ í›ˆë ¨ ë£¨í”„ëŠ” ë‹¨ì¼ CPU ë˜ëŠ” GPUì—ì„œ ì˜ ì‘ë™í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëª‡ ê°€ì§€ ì¡°ì •ë§Œìœ¼ë¡œ ì—¬ëŸ¬ GPU ë˜ëŠ” TPUì—ì„œ ë¶„ì‚° í›ˆë ¨ì„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ¤— AccelerateëŠ” ë¶„ì‚° í›ˆë ¨, í˜¼í•© ì •ë°€ë„, ì¥ì¹˜ ë°°ì¹˜ì˜ ë³µì¡ì„±ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ë¡œë” ìƒì„±ë¶€í„° ì‹œì‘í•˜ì—¬ ìˆ˜ë™ í›ˆë ¨ ë£¨í”„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<Tip>

âš¡ **Accelerate ì‹¬í™”**: ë¶„ì‚° í›ˆë ¨, í˜¼í•© ì •ë°€ë„, í•˜ë“œì›¨ì–´ ìµœì í™”ì— ëŒ€í•œ ëª¨ë“  ê²ƒì„ [ğŸ¤— Accelerate ë¬¸ì„œ](https://huggingface.co/docs/accelerate/)ì—ì„œ ì•Œì•„ë³´ê³  [transformers ë¬¸ì„œ](https://huggingface.co/docs/transformers/main/en/accelerate)ì—ì„œ ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì‚´í´ë³´ì„¸ìš”.

</Tip>

```py
from accelerate import Accelerator
from torch.optim import AdamW
from transformers import AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ì¶”ê°€í•  ì²« ë²ˆì§¸ ì¤„ì€ import ì¤„ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì¤„ì€ í™˜ê²½ì„ ì‚´í´ë³´ê³  ì ì ˆí•œ ë¶„ì‚° ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ëŠ” `Accelerator` ê°ì²´ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ğŸ¤— AccelerateëŠ” ì¥ì¹˜ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ëª¨ë¸ì„ ì¥ì¹˜ì— ì˜¬ë¦¬ëŠ” ì¤„ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë˜ëŠ” ì›í•œë‹¤ë©´ `device` ëŒ€ì‹  `accelerator.device`ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤).

ê·¸ëŸ° ë‹¤ìŒ ì£¼ìš” ì‘ì—…ì€ ë°ì´í„°ë¡œë”, ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €ë¥¼ `accelerator.prepare()`ì— ë³´ë‚´ëŠ” ì¤„ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ëŠ” ë¶„ì‚° í›ˆë ¨ì´ ì˜ë„í•œ ëŒ€ë¡œ ì‘ë™í•˜ë„ë¡ ì ì ˆí•œ ì»¨í…Œì´ë„ˆì—ì„œ í•´ë‹¹ ê°ì²´ë“¤ì„ ë˜í•‘í•©ë‹ˆë‹¤. ìˆ˜í–‰í•´ì•¼ í•  ë‚˜ë¨¸ì§€ ë³€ê²½ ì‚¬í•­ì€ ë°°ì¹˜ë¥¼ `device`ì— ì˜¬ë¦¬ëŠ” ì¤„ì„ ì œê±°í•˜ê³ (í˜¹ì€ ë°°ì¹˜ë¥¼ deviceì— ì˜¬ë¦¬ëŠ” ì½”ë“œë¥¼ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ accelerator.deviceë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.) `loss.backward()`ë¥¼ `accelerator.backward(loss)`ë¡œ ë°”ê¾¸ëŠ” ê²ƒì…ë‹ˆë‹¤.

<Tip>
âš ï¸ Cloud TPUê°€ ì œê³µí•˜ëŠ” ì†ë„ í–¥ìƒì˜ ì´ì ì„ ì–»ìœ¼ë ¤ë©´ í† í¬ë‚˜ì´ì €ì˜ `padding="max_length"` ë° `max_length` ì¸ìˆ˜ë¡œ ìƒ˜í”Œì„ ê³ ì • ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
</Tip>

ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì‹¶ë‹¤ë©´, ğŸ¤— Accelerateë¥¼ ì‚¬ìš©í•œ ì „ì²´ í›ˆë ¨ ë£¨í”„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```py
from accelerate import Accelerator
from torch.optim import AdamW
from transformers import AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ì´ê²ƒì„ `train.py` ìŠ¤í¬ë¦½íŠ¸ì— ë„£ìœ¼ë©´ ëª¨ë“  ì¢…ë¥˜ì˜ ë¶„ì‚° ì„¤ì •ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ ë©ë‹ˆë‹¤. ë¶„ì‚° ì„¤ì •ì—ì„œ ì‹œë„í•´ ë³´ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.

```bash
accelerate config
```

ì´ ëª…ë ¹ì€ ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•˜ë¼ëŠ” ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê³  ë‹¤ìŒ ëª…ë ¹ì—ì„œ ì‚¬ìš©ë˜ëŠ” êµ¬ì„± íŒŒì¼ì— ë‹µë³€ì„ ë¤í”„í•©ë‹ˆë‹¤.

```
accelerate launch train.py
```

ì´ ëª…ë ¹ì–´ë¥¼ í†µí•´ ë¶„ì‚° í›ˆë ¨ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë…¸íŠ¸ë¶ì—ì„œ ì´ë¥¼ ì‹œë„í•˜ê³  ì‹¶ë‹¤ë©´(ì˜ˆ: Colabì—ì„œ TPUë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´) ì½”ë“œë¥¼ `training_function()`ì— ë¶™ì—¬ë„£ê³  ë‹¤ìŒê³¼ í•¨ê»˜ ë§ˆì§€ë§‰ ì…€ì„ ì‹¤í–‰í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

[ğŸ¤— Accelerate ë¦¬í¬ì§€í† ë¦¬](https://github.com/huggingface/accelerate/tree/main/examples)ì—ì„œ ë” ë§ì€ ì˜ˆì œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Tip>

ğŸŒ **ë¶„ì‚° í›ˆë ¨**: ë‹¤ì¤‘ GPU ë° ë‹¤ì¤‘ ë…¸ë“œ í›ˆë ¨ì— ëŒ€í•œ í¬ê´„ì ì¸ ë‚´ìš©ì€ [ğŸ¤— Transformers ë¶„ì‚° í›ˆë ¨ ê°€ì´ë“œ](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many)ì™€ [í›ˆë ¨ í™•ì¥ ì¿¡ë¶](https://huggingface.co/docs/transformers/main/en/accelerate)ì„ í™•ì¸í•˜ì„¸ìš”.

</Tip>

### ë‹¤ìŒ ë‹¨ê³„ì™€ ëª¨ë²” ì‚¬ë¡€[[next-steps-and-best-practices]]

ì´ì œ ì²˜ìŒë¶€í„° í›ˆë ¨ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìœ¼ë¯€ë¡œ í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì¶”ê°€ì‚¬í•­ì„ ê³ ë ¤í•©ë‹ˆë‹¤.

**ëª¨ë¸ í‰ê°€**: ëª¨ë¸ì€ ì •í™•ë„ë¿ë§Œ ì•„ë‹ˆë¼ í•­ìƒ ì—¬ëŸ¬ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. í¬ê´„ì ì¸ í‰ê°€ë¥¼ ìœ„í•´ ğŸ¤— Evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

**í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ì²´ê³„ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•´ Optuna ë˜ëŠ” Ray Tuneê³¼ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.

**ëª¨ë¸ ëª¨ë‹ˆí„°ë§**: í›ˆë ¨ ì „ë°˜ì— ê±¸ì³ í›ˆë ¨ ë©”íŠ¸ë¦­, í•™ìŠµ ê³¡ì„ , ê²€ì¦ ì„±ëŠ¥ì„ ì¶”ì í•˜ì„¸ìš”.

**ëª¨ë¸ ê³µìœ **: í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ê³µìœ í•˜ì„¸ìš”.

**íš¨ìœ¨ì„±**: ëŒ€í˜• ëª¨ë¸ì˜ ê²½ìš° ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…, íš¨ìœ¨ì ì¸ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •(LoRA, AdaLoRA), ì–‘ìí™” ë°©ë²•ê³¼ ê°™ì€ ê¸°ìˆ ì„ ê³ ë ¤í•˜ì„¸ìš”.

ì´ê²ƒìœ¼ë¡œ ì»¤ìŠ¤í…€ í›ˆë ¨ ë£¨í”„ë¥¼ ì‚¬ìš©í•œ ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ ì‹¬í™” ê³¼ì •ì„ ë§ˆì¹©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë°°ìš´ ê¸°ìˆ ë“¤ì€ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì™„ì „í•œ ì œì–´ê°€ í•„ìš”í•˜ê±°ë‚˜ `Trainer` APIê°€ ì œê³µí•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œëŠ” ì»¤ìŠ¤í…€ í›ˆë ¨ ë¡œì§ì„ êµ¬í˜„í•˜ê³ ì í•  ë•Œ ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.

## ì„¹ì…˜ í€´ì¦ˆ[[section-quiz]]

ì»¤ìŠ¤í…€ í›ˆë ¨ ë£¨í”„ì™€ ê³ ê¸‰ í›ˆë ¨ ê¸°ë²•ì— ëŒ€í•œ ì´í•´ë¥¼ í…ŒìŠ¤íŠ¸í•´ ë³´ì„¸ìš”.

### 1. Adamê³¼ AdamW ì˜µí‹°ë§ˆì´ì €ì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

<Question
	choices={[
		{
			text: "AdamWëŠ” ë‹¤ë¥¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
			explain: "í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ì€ ì˜µí‹°ë§ˆì´ì € ì„ íƒê³¼ ë³„ê°œì…ë‹ˆë‹¤."
		},
		{
			text: "AdamWëŠ” ë¶„ë¦¬ëœ ê°€ì¤‘ì¹˜ ê°ì†Œ ì •ê·œí™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! AdamWëŠ” ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ì™€ ë¶„ë¦¬í•˜ì—¬ ë” ë‚˜ì€ ì •ê·œí™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            correct: true
		},
		{
			text: "AdamWëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œë§Œ ì‘ë™í•©ë‹ˆë‹¤.",
			explain: "AdamWëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
		},
        {
			text: "AdamWëŠ” Adamë³´ë‹¤ ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
			explain: "ë‘ ì˜µí‹°ë§ˆì´ì € ëª¨ë‘ ë¹„ìŠ·í•œ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ì„ ê°€ì§‘ë‹ˆë‹¤."
		}
	]}
/>

### 2. í›ˆë ¨ ë£¨í”„ì—ì„œ ì˜¬ë°”ë¥¸ ì—°ì‚° ìˆœì„œëŠ” ë¬´ì—‡ì¸ê°€ìš”?

<Question
	choices={[
		{
			text: "ìˆœì „íŒŒ â†’ ì—­ì „íŒŒ â†’ ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„ â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”",
			explain: "ë¹„ìŠ·í•˜ì§€ë§Œ, ì´ì „ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ëˆ„ì ë˜ì§€ ì•Šë„ë¡ ë‹¤ìŒ ìˆœì „íŒŒ ì „ì— ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•´ì•¼ í•©ë‹ˆë‹¤."
		},
		{
			text: "ìˆœì „íŒŒ â†’ ì—­ì „íŒŒ â†’ ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„ â†’ ìŠ¤ì¼€ì¤„ëŸ¬ ë‹¨ê³„ â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! ì´ê²ƒì´ ì˜¬ë°”ë¥¸ ìˆœì„œì…ë‹ˆë‹¤: ì†ì‹¤ ê³„ì‚°, ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°, ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸, í•™ìŠµë¥  ì—…ë°ì´íŠ¸, ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”.",
            correct: true
		},
		{
			text: "ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™” â†’ ìˆœì „íŒŒ â†’ ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„ â†’ ì—­ì „íŒŒ",
			explain: "ì†ì‹¤ë¡œë¶€í„° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ ì—­ì „íŒŒê°€ ìˆœì „íŒŒ ë‹¤ìŒì— ì™€ì•¼ í•©ë‹ˆë‹¤."
		},
        {
			text: "ìˆœì „íŒŒ â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™” â†’ ì—­ì „íŒŒ â†’ ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„",
			explain: "ì—­ì „íŒŒ ì „ì— ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ë©´ ë°©ê¸ˆ ê³„ì‚°í•œ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì œê±°ë©ë‹ˆë‹¤."
		}
	]}
/>

### 3. ğŸ¤— Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì£¼ë¡œ ë¬´ì—‡ì„ ë„ì™€ì£¼ë‚˜ìš”?

<Question
	choices={[
		{
			text: "ìˆœì „íŒŒë¥¼ ìµœì í™”í•˜ì—¬ ëª¨ë¸ í›ˆë ¨ì„ ë” ë¹ ë¥´ê²Œ ë§Œë“­ë‹ˆë‹¤.",
			explain: "AccelerateëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ ìì²´ë¥¼ ìµœì í™”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
		},
		{
			text: "ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.",
			explain: "AccelerateëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
		},
		{
			text: "ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½ìœ¼ë¡œ ì—¬ëŸ¬ GPU/TPUì—ì„œ ë¶„ì‚° í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! AccelerateëŠ” ë¶„ì‚° í›ˆë ¨ ë³µì¡ì„±ì„ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì¼ ë˜ëŠ” ë‹¤ì¤‘ ì¥ì¹˜ì—ì„œ ì›í™œí•˜ê²Œ ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.",
            correct: true
		},
        {
			text: "ëª¨ë¸ì„ TensorFlowì™€ ê°™ì€ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.",
			explain: "AccelerateëŠ” PyTorch ë‚´ì—ì„œ ì‘ë™í•˜ë©° í”„ë ˆì„ì›Œí¬ ê°„ ë³€í™˜ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
		}
	]}
/>

### 4. í›ˆë ¨ ë£¨í”„ì—ì„œ ë°°ì¹˜ë¥¼ ì¥ì¹˜ë¡œ ì´ë™í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

<Question
	choices={[
		{
			text: "í›ˆë ¨ì„ ë” ë¹ ë¥´ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤.",
			explain: "ì†ë„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆì§€ë§Œ, ì£¼ëœ ì´ìœ ëŠ” í˜¸í™˜ì„±ì…ë‹ˆë‹¤."
		},
		{
			text: "ëª¨ë¸ê³¼ ë°ì´í„°ê°€ ì—°ì‚°ì„ ìœ„í•´ ë™ì¼í•œ ì¥ì¹˜(CPU/GPU)ì— ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! PyTorchëŠ” ì—°ì‚°ì´ ì‘ë™í•˜ë ¤ë©´ í…ì„œê°€ ë™ì¼í•œ ì¥ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.",
            correct: true
		},
		{
			text: "ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤.",
			explain: "ì¥ì¹˜ë¡œ ì´ë™í•˜ëŠ” ê²ƒ ìì²´ê°€ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤."
		},
        {
			text: "DataLoaderì—ì„œ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.",
			explain: "DataLoaderëŠ” íŠ¹ì • ì¥ì¹˜ ë°°ì¹˜ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
		}
	]}
/>

### 5. í‰ê°€ ì „ì— `model.eval()`ì€ ë¬´ì—‡ì„ í•˜ë‚˜ìš”?

<Question
	choices={[
		{
			text: "ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ì •í•˜ì—¬ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.",
			explain: "model.eval()ì€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ - ê·¸ê²ƒì€ requires_grad=Falseë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
		},
		{
			text: "ë“œë¡­ì•„ì›ƒ ë° ë°°ì¹˜ ì •ê·œí™”ì™€ ê°™ì€ ë ˆì´ì–´ì˜ ë™ì‘ì„ ì¶”ë¡ ìš©ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! eval() ëª¨ë“œëŠ” ë“œë¡­ì•„ì›ƒì„ ë¹„í™œì„±í™”í•˜ê³  í˜„ì¬ ë°°ì¹˜ì—ì„œ ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  ë°°ì¹˜ ì •ê·œí™”ì— ì‹¤í–‰ í†µê³„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            correct: true
		},
		{
			text: "í‰ê°€ ë©”íŠ¸ë¦­ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ í™œì„±í™”í•©ë‹ˆë‹¤.",
			explain: "ì‹¤ì œë¡œëŠ” í‰ê°€ ì¤‘ì— ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ torch.no_grad()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
		},
        {
			text: "í‰ê°€ ë©”íŠ¸ë¦­ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.",
			explain: "model.eval()ì€ ë ˆì´ì–´ ë™ì‘ë§Œ ë³€ê²½í•©ë‹ˆë‹¤ - ë©”íŠ¸ë¦­ ê³„ì‚°ì€ ë³„ë„ë¡œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤."
		}
	]}
/>

### 6. í‰ê°€ ì¤‘ `torch.no_grad()`ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

<Question
	choices={[
		{
			text: "ëª¨ë¸ì´ ì˜ˆì¸¡ì„ í•˜ì§€ ëª»í•˜ë„ë¡ ë°©ì§€í•©ë‹ˆë‹¤.",
			explain: "torch.no_grad()ëŠ” ì˜ˆì¸¡ì„ ë°©ì§€í•˜ì§€ ì•Šê³  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ë§Œ ë°©ì§€í•©ë‹ˆë‹¤."
		},
		{
			text: "ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì ì„ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  ê³„ì‚°ì„ ê°€ì†í™”í•©ë‹ˆë‹¤.",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! í‰ê°€ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ê°€ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¹„í™œì„±í™”í•˜ë©´ ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ëŸ‰ì„ ì ˆì•½í•©ë‹ˆë‹¤.",
            correct: true
		},
		{
			text: "ëª¨ë¸ì˜ í‰ê°€ ëª¨ë“œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.",
			explain: "í‰ê°€ ëª¨ë“œëŠ” torch.no_grad()ê°€ ì•„ë‹Œ model.eval()ë¡œ í™œì„±í™”ë©ë‹ˆë‹¤."
		},
        {
			text: "ì‹¤í–‰ ê°„ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.",
			explain: "ê²°ê³¼ë¥¼ ë™ì¼í•˜ê²Œ ì¬í˜„í•˜ë ¤ë©´ torch.no_grad()ê°€ ì•„ë‹ˆë¼ ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤."
		}
	]}
/>

### 7. í›ˆë ¨ ë£¨í”„ì—ì„œ ğŸ¤— Accelerateë¥¼ ì‚¬ìš©í•  ë•Œ ë¬´ì—‡ì´ ë³€ê²½ë˜ë‚˜ìš”?

<Question
	choices={[
		{
			text: "ì „ì²´ í›ˆë ¨ ë£¨í”„ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.",
			explain: "AccelerateëŠ” ê¸°ì¡´ PyTorch ì½”ë“œì— ìµœì†Œí•œì˜ ë³€ê²½ë§Œ í•„ìš”í•©ë‹ˆë‹¤."
		},
		{
			text: "accelerator.prepare()ë¡œ ì£¼ìš” ê°ì²´ë¥¼ ë˜í•‘í•˜ê³  loss.backward() ëŒ€ì‹  accelerator.backward()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
			explain: "ì •ë‹µì…ë‹ˆë‹¤! ì´ê²ƒë“¤ì´ ì£¼ìš” ë³€ê²½ì‚¬í•­ì…ë‹ˆë‹¤ - ê°ì²´ë¥¼ ì¤€ë¹„í•˜ê³  ì ì ˆí•œ ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•´ accelerator.backward()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            correct: true
		},
		{
			text: "ì½”ë“œì—ì„œ GPU ìˆ˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.",
			explain: "AccelerateëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í•˜ë“œì›¨ì–´ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤."
		},
        {
			text: "ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.",
			explain: "Accelerateì—ì„œ ë™ì¼í•œ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
		}
	]}
/>

<Tip>

ğŸ’¡ **í•µì‹¬ ìš”ì :**
- ìˆ˜ë™ í›ˆë ¨ ë£¨í”„ëŠ” ì™„ì „í•œ ì œì–´ë¥¼ ì œê³µí•˜ì§€ë§Œ ì ì ˆí•œ ìˆœì„œì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤: ìˆœì „íŒŒ â†’ ì—­ì „íŒŒ â†’ ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„ â†’ ìŠ¤ì¼€ì¤„ëŸ¬ ë‹¨ê³„ â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
- ê°€ì¤‘ì¹˜ ê°ì†Œê°€ ìˆëŠ” AdamWëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ê¶Œì¥ë˜ëŠ” ì˜µí‹°ë§ˆì´ì €ì…ë‹ˆë‹¤.
- ì˜¬ë°”ë¥¸ ë™ì‘ê³¼ íš¨ìœ¨ì„±ì„ ìœ„í•´ í‰ê°€ ì¤‘ì—ëŠ” í•­ìƒ `model.eval()`ê³¼ `torch.no_grad()`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ğŸ¤— AccelerateëŠ” ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½ë§Œìœ¼ë¡œ ë¶„ì‚° í›ˆë ¨ì„ ì´ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
- ì¥ì¹˜ ê´€ë¦¬(GPU/CPUë¡œ í…ì„œ ì´ë™)ëŠ” PyTorch ì—°ì‚°ì— ì¤‘ìš”í•©ë‹ˆë‹¤.
- í˜¼í•© ì •ë°€ë„, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì , ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ê³¼ ê°™ì€ ìµœì‹  ê¸°ë²•ì€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>
