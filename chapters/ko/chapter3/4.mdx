# ì „ì²´ í•™ìŠµ[[a-full-training]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

ì´ì œ `Trainer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì´ì „ ì„¹ì…˜ì—ì„œ í–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, ì„¹ì…˜ 2ì—ì„œ ì´ë¯¸ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì´ë²ˆ ì„¹ì…˜ì„ ê³µë¶€í•  ë•Œ í•„ìš”í•œ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„[[prepare-for-training]]

ì‹¤ì œë¡œ í•™ìŠµ ë£¨í”„(training loop)ë¥¼ ì‘ì„±í•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ê°ì²´ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ë°°ì¹˜(batch)ë¥¼ ë°˜ë³µí•˜ëŠ” ë° ì‚¬ìš©í•  dataloadersì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ dataloadersë¥¼ ì •ì˜í•˜ê¸° ì „ì— `Trainer`ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•œ ëª‡ ê°€ì§€ ì‘ì—…ì„ ì§ì ‘ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ `tokenized_datasets`ì— ì•½ê°„ì˜ í›„ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:

- ëª¨ë¸ì´ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” ê°’ì´ ì €ì¥ëœ ì—´(columns)ì„ ì œê±°í•©ë‹ˆë‹¤. (`sentence1`, `sentence2` ë“±)
- ì—´ ë ˆì´ë¸”(column label)ì˜ ì´ë¦„ì„ `labels`ë¡œ ë°”ê¿‰ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ `labels`ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
- íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  PyTorch í…ì„œ(tensors)ë¥¼ ë°˜í™˜í•˜ë„ë¡ datasetsì˜ í˜•ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤.

`tokenized_datasets`ì—ëŠ” ì´ëŸ¬í•œ ì‘ì—…ì„ ìœ„í•œ ë³„ë„ì˜ ë©”ì„œë“œë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ìœ„ì—ì„œ ë³´ë“¯ì´ ê²°ê³¼ì ìœ¼ë¡œ `tokenized_datasets`ì—ëŠ” ëª¨ë¸ì´ í—ˆìš©í•˜ëŠ” `columns`ë§Œ ì¡´ì¬í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

ì´ì œ ì´ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ dataloaderë¥¼ ì‰½ê²Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ë°ì´í„° ì²˜ë¦¬ì— ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ë°°ì¹˜(batch)ë¥¼ ê²€ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

ì‹¤ì œ ë°ì´í„° í˜•íƒœ(shapes)ê°€ ì‚´ì§ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ”ë° ì´ëŠ” í•™ìŠµ dataloaderì— ëŒ€í•´ `shuffle=True`ë¥¼ ì„¤ì •í•˜ê³  ë°°ì¹˜(batch) ë‚´ì—ì„œì˜ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©(padding)í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì´ì œ ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì™„ì „íˆ ëë‚¬ìŠµë‹ˆë‹¤. ê¸°ê³„í•™ìŠµ ì‹¤ë¬´ì(ML practitioners)ë“¤ì—ê²ŒëŠ” ë§Œì¡±ìŠ¤ëŸ½ê¸°ë„ í•˜ê² ì§€ë§Œ ì¼ë¶€ ëª…í™•í•˜ì§€ ì•Šì€ ë¶€ë¶„ë„ ìˆì„ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ë¡œ ëŒì•„ê°€ ë´…ì‹œë‹¤. ì´ì „ ì„¹ì…˜ì—ì„œ ìˆ˜í–‰í•œ ê²ƒê³¼ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”(instantiate)í•©ë‹ˆë‹¤:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë“  ê²ƒë“¤ì´ ì›í™œí•˜ê²Œ ì§„í–‰ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë°°ì¹˜(batch)ë¥¼ ì´ ëª¨ë¸ì— í•œë²ˆ ì „ë‹¬í•´ ë´…ì‹œë‹¤:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ëª¨ë“  ğŸ¤—Transformers ëª¨ë¸ì€ ë§¤ê°œë³€ìˆ˜ì— `labels`ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì†ì‹¤(loss)ê³¼ í•¨ê»˜ logitê°’(batchë‚´ ê° ì…ë ¥ì— ëŒ€í•´ logitê°’ì´ 2ê°œì´ë¯€ë¡œ í¬ê¸°ê°€ 8 x 2ì¸ í…ì„œ)ë„ ë°˜í™˜í•©ë‹ˆë‹¤.

í•™ìŠµ ë£¨í”„(training loop)ë¥¼ ì‘ì„±í•  ì¤€ë¹„ê°€ ê±°ì˜ ë˜ì—ˆìŠµë‹ˆë‹¤! ê·¸ëŸ°ë° ì•„ì§ ìµœì í™” í•¨ìˆ˜(optimizer) ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler) ì§€ì • ì‘ì—…ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì•ì—ì„œ ë°°ìš´ `Trainer`ì˜ ê¸°ë³¸ ì„¤ì •ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. `Trainer`ê°€ ì‚¬ìš©í•˜ëŠ” ìµœì í™” í•¨ìˆ˜ëŠ” `AdamW`ì´ë©° ì´ëŠ” `Adam`ê³¼ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ weight decay regularization (Ilya Loshchilov, Frank Hutter ì €. ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) ì°¸ì¡°)ì„ ì ìš©í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì— ì°¨ì´ê°€ ë‚©ë‹ˆë‹¤:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, Trainerì—ì„œ ë””í´íŠ¸ë¡œ ì‚¬ìš©ë˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)ëŠ” ìµœëŒ€ê°’(5e-5)ì—ì„œ 0ê¹Œì§€ ì„ í˜• ê°ì‡ (linear decay)í•©ë‹ˆë‹¤. ì´ë¥¼ ì ì ˆí•˜ê²Œ ì •ì˜í•˜ë ¤ë©´ ìš°ë¦¬ê°€ ìˆ˜í–‰í•  í•™ìŠµ ë‹¨ê³„ì˜ íšŸìˆ˜ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì‹¤í–‰í•˜ë ¤ëŠ” ì—í¬í¬(epochs) ìˆ˜ì— í•™ìŠµ ë°°ì¹˜(batch)ì˜ ê°œìˆ˜ë¥¼ ê³±í•œ ê²ƒì…ë‹ˆë‹¤. í•™ìŠµ ë°°ì¹˜ì˜ ê°œìˆ˜ëŠ” í•™ìŠµ dataloaderì˜ ê¸¸ì´ì™€ ê°™ìŠµë‹ˆë‹¤. `Trainer`ëŠ” ë””í´íŠ¸ë¡œ 3ê°œì˜ ì—í¬í¬(epochs)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ìŒì„ ë”°ë¦…ë‹ˆë‹¤:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### í•™ìŠµ ë£¨í”„[[the-training-loop]]

ê°€ìš© GPUê°€ ìˆëŠ” ê²½ìš° GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµì€ ëª‡ ë¶„ ì •ë„ê°€ ì•„ë‹ˆë¼ ëª‡ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ëª¨ë¸ê³¼ ë°°ì¹˜(batch)ë¥¼ ì ì¬í•  ì¥ì¹˜(device)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

ì´ì œ í•™ìŠµí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í•™ìŠµì´ ì–¸ì œ ëë‚ ì§€ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ `tqdm` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë‹¨ê³„(training steps)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ í‘œì‹œì¤„(progress bar)ì„ ì¶œë ¥í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

í•™ìŠµ ë£¨í”„(training loop)ì˜ ì£¼ìš” ë¶€ë¶„ì´ ë³¸ ê°•ì¢Œì˜ ì†Œê°œ(Introduction) ì¥ì—ì„œ ì†Œê°œí•œ ë‚´ìš©ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ ì½”ë“œì—ì„œëŠ” ì–´ë– í•œ ì¤‘ê°„ ì¶œë ¥ë„ ì—†ìœ¼ë¯€ë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì´ë‚˜ ì†ì‹¤ ë“±ì— ëŒ€í•´ ì•„ë¬´ ê²ƒë„ ì•Œë ¤ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ í‰ê°€ ë£¨í”„(evaluation loop)ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

### í‰ê°€ ë£¨í”„[[the-evaluation-loop]]

ì´ì „ì— ìˆ˜í–‰í–ˆë˜ ê²ƒì²˜ëŸ¼, ğŸ¤—Evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í‰ê°€ ë©”íŠ¸ë¦­(metrics)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ `metric.compute()` ë©”ì„œë“œë¥¼ ì‚´í´ë³´ì•˜ì§€ë§Œ `metric.add_batch()` ë©”ì„œë“œë¡œ í‰ê°€ ë£¨í”„(evaluation loop)ë¥¼ ì‹¤í–‰í•˜ë©´ì„œ ë°°ì¹˜(batch)ë³„ í‰ê°€ ë©”íŠ¸ë¦­(metrics) ê³„ì‚° ê²°ê³¼ë¥¼ ëˆ„ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë°°ì¹˜(batch)ë¥¼ ëˆ„ì í•˜ê³  ë‚˜ë©´ `metric.compute()`ë¡œ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‰ê°€ ë£¨í”„ì—ì„œ ì´ ëª¨ë“  ê²ƒì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, ëª¨ë¸ í—¤ë“œ ì´ˆê¸°í™”(model head initialization) ë° ë°ì´í„° ì…”í”Œë§ì˜ ë¬´ì‘ìœ„ì„±(randomness) ì¸í•´ ê²°ê³¼ê°€ ì•½ê°„ ì°¨ì´ê°€ ë‚˜ê¸´ í•˜ì§€ë§Œ ê·¸ ì°¨ì´ê°€ í¬ë©´ ì•ˆë©ë‹ˆë‹¤.

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ê¸°** í›ˆë ¨ ë£¨í”„ë¥¼ ìˆ˜ì •í•˜ì—¬ SST-2 ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•´ë³´ì„¸ìš”.

</Tip>

### ğŸ¤—Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í•™ìŠµ ë£¨í”„ ê°€ì†í™”[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

ì•ì—ì„œ ì •ì˜í•œ í•™ìŠµ ë£¨í”„(training loop)ëŠ” ë‹¨ì¼ CPU ë˜ëŠ” ë‹¨ì¼ GPUì—ì„œ ì œëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ [ğŸ¤—Accelerate](https://github.com/huggingface/accelerate) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª‡ ê°€ì§€ ì„¤ì •ë§Œ í•˜ë©´ ì—¬ëŸ¬ GPU ë˜ëŠ” TPUì—ì„œ ë¶„ì‚° í•™ìŠµ(distributed training)ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ë° ê²€ì¦ dataloaderë¥¼ ìƒì„± í•œ í›„, í•™ìŠµ ë£¨í”„(training loop)ì˜ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ìœ„ í•™ìŠµ ë£¨í”„ë¥¼ ìˆ˜ì •í•œ ë³€ê²½ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

ì¶”ê°€í•  ì²« ë²ˆì§¸ ë¼ì¸ì€ `import` ë¼ì¸ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì¶”ê°€ ë¼ì¸ì—ì„œ ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì •ì„ íŒŒì•…í•˜ê³  ì ì ˆí•œ ë¶„ì‚° ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ëŠ” `Accelerator` ê°œì²´ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ğŸ¤—Accelerateê°€ ì¥ì¹˜ ë°°ì¹˜(device placement)ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¥ì¹˜ì— ëª¨ë¸ì„ ë°°ì¹˜í•˜ëŠ” ë¼ì¸(`model.to(device)`)ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë‹ˆë©´ ì›í•  ê²½ìš°, `device` ëŒ€ì‹  `accelerator.device`ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ dataloaders, ëª¨ë¸(model) ë° ìµœì í™” í•¨ìˆ˜(optimizer)ë¥¼ `accelerator.prepare()`ë¡œ ì…ë ¥í•˜ëŠ” ë¶€ë¶„ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì…ë ¥í•œ ê°ì²´ë“¤ì„ ì ì ˆí•œ ì»¨í…Œì´ë„ˆë¡œ ê°ì‹¸ì„œ(wrapping) ë¶„ì‚° í•™ìŠµ(distributed training)ì´ ì˜ë„ëŒ€ë¡œ ì‘ë™ë˜ë„ë¡ í•©ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ ë³€ê²½ ì‚¬í•­ì€ `device`ì— ë°°ì¹˜(batch)ë¥¼ ë³µì‚¬í•˜ëŠ” ë¼ì¸ì„ ì œê±°(í•´ë‹¹ ë¼ì¸ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ `device` ëŒ€ì‹ ì— `accelerate.device`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.)í•˜ê³  `loss.backward()`ë¥¼ `accelerator.backward()`ë¡œ ëŒ€ì¹˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.

<Tip>

âš ï¸ Cloud TPUê°€ ì œê³µí•˜ëŠ” ì†ë„ í–¥ìƒì˜ ì´ì ì„ ì–»ìœ¼ë ¤ë©´ í† í¬ë‚˜ì´ì €ì˜ `padding="max_length"` ë° `max_length` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œì„ ê³ ì • ê¸¸ì´ë¡œ ì±„ìš°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

</Tip>

ì „ì²´ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ğŸ¤—Accelerateë¥¼ ì‚¬ìš©í•œ ë‹¤ìŒì˜ ì™„ì „í•œ í•™ìŠµ ë£¨í”„(training loop)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ìœ„ ì½”ë“œë¥¼ `train.py` ìŠ¤í¬ë¦½íŠ¸ì— ë¶™ì—¬ë„£ê¸°í•˜ì—¬ ëª¨ë“  ì¢…ë¥˜ì˜ ë¶„ì‚° í™˜ê²½(distributed setup)ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„ì‚° í™˜ê²½ì—ì„œ ì‚¬ìš©í•´ ë³´ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤ (ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ë©´ ì•ˆë©ë‹ˆë‹¤. ë°˜ë“œì‹œ ì„œë²„ì— ë¡œê·¸ì¸ì„ í•´ì„œ ì‹¤í–‰í•´ì•¼ ê·¸ ê²°ê³¼ê°€ ë³´ì—¬ì§‘ë‹ˆë‹¤):

```bash
accelerate config
```

ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•˜ë¼ëŠ” ë©”ì‹œì§€ê°€ ëœ¨ê³  ì´ ëª…ë ¹ì´ ì‚¬ìš©í•˜ëŠ” êµ¬ì„± íŒŒì¼ì— ì…ë ¥ëœ ë‹µì„ ë¤í”„í•˜ë¼ëŠ” ë©”ì‹œì§€ê°€ í‘œì‹œë©ë‹ˆë‹¤:

```
accelerate launch train.py
```

ìœ„ ëª…ë ¹ì–´ë¡œ ë¶„ì‚° í•™ìŠµ(distributed training)ì„ ì‹œì‘í•©ë‹ˆë‹¤.

Notebookì—ì„œ ì´ê²ƒì„ ì‹œë„í•˜ë ¤ë©´(ì˜ˆë¥¼ ë“¤ì–´, Colabì˜ TPUë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´) `training_function()`ì— ì½”ë“œë¥¼ ë¶™ì—¬ë„£ê³  ë‹¤ìŒì„ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

ë” ë§ì€ ì˜ˆì œëŠ” [ğŸ¤—Accelerate ë¦¬í¬ì§€í„°ë¦¬](https://github.com/huggingface/accelerate/tree/main/examples)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.