

<FrameworkSwitchCourse {fw} />

# ูพุฑุฏุงุฒุด ุฏุงุฏู 
# Processing the data

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}

ุฏุฑ ุงู ุจุฎุด ุฏุฑ ุงุฏุงูู ูุซุงู [ูุตู ูุจู](/course/chapter2)ุ ูุญูู ุขููุฒุด ฺฉ ูุฏู ุชุฑุชุจ ุฏุฑ ฺฉ ุจูุชฺ ุชูุณุท pytorch ุฑุง ุดุฑุญ ูโุฏูู:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}

ุฏุฑ ุงู ุจุฎุด ุฏุฑ ุงุฏุงูู ูุซุงู [ูุตู ูุจู](/course/chapter2)ุ ูุญูู ุขููุฒุด ฺฉ ูุฏู ุชุฑุชุจ ุฏุฑ ฺฉ ุจูุชฺ ุชูุณุท pytorch ุฑุง ุดุฑุญ ูโุฏูู:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

{/if}

ุงูุจุชู ุขููุฒุด ุจุง ุงุณุชูุงุฏู ุงุฒ ุฏู ุฌููู ุจู ูุชุงุฌ ฺุดูฺฏุฑ ููุชู ูุฎูุงูุฏ ุดุฏ. ุจุฑุง ุจุฏุณุช ุขูุฑุฏู ูุชุงุฌ ุจูุชุฑ ูุงุฒ ุจู ุขูุงุฏู ุณุงุฒ ูุฌููุนูโุฏุงุฏู ุจุฒุฑฺฏุชุฑ ุฎูุงูุฏ ุฏุงุดุช.

ุฏุฑ ุงู ุจุฎุด ูุง ุงุฒ ูุฌููุนูโุฏุงุฏู MRPC (Microsoft Research Paraphrase Corpus) ฺฉู ุฏุฑ ฺฉ [ููุงูู](https://www.aclweb.org/anthology/I05-5002.pdf)ุ ููุดุชูโ William B. Dolan ู Chris Brockett.ุ ูุนุฑู ุดุฏู ุจู ุนููุงู ฺฉ ูุซุงู ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ. ุงู ุฏุงุฏู ุดุงูู ตุธฐฑ ุฌูุช ุฌููู ู ฺฉ ุจุฑฺุณุจ ูโุจุงุดุฏ ฺฉู ุจุฑฺุณุจ ูุดุงูุฏููุฏู ูุชูุงุธุฑ ุจูุฏู ุฌููุงุช (ุจู ุนููุงู ูุซุงูุ ุขุง ุฏู ุฌููู ูุนู ฺฉุณุงู ุฏุงุฑูุฏ ุง ุฎุฑ) ูโุจุงุดุฏ. ุนูุช ุงูุชุฎุงุจ ุงู ูุฌููุนูโุฏุงุฏู ุงู ุงุณุช ฺฉู ูุฌููุนูโุฏุงุฏู ฺฉูฺฺฉุณุช ู ุชุฌุฑุจู ุขููุฒุด ุฏุงุฏู ุฑู ุขู ุขุณุงู ุงุณุช.

### ุจุงุฑฺฏุฐุงุฑ ฺฉ ุฏุงุฏู ุงุฒ ูุงุจ

### Loading a dataset from the Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

ูุงุจ ุชููุง ุดุงูู ูุฏููุง ููโุจุงุดุฏุ ุจูฺฉู ุดุงูู ูุฌููุนูโุฏุงุฏูโูุง ูุชุนุฏุฏ ุฏุฑ ุจุณุงุฑ ุฒุจุงูโูุง ูุฎุชูู ูโุจุงุดุฏ. ุดูุง ูโุชูุงูุฏ ูุฌููุนูโุฏุงุฏูโูุง ุฑุง ุฏุฑ ุงู [ููฺฉ](https://huggingface.co/datasets) ุฌุณุชุฌู ฺฉูุฏ ู ูพุดููุงุฏ ูโฺฉูู ูพุณ ุงุฒ ุงุชูุงู ุงู ุจุฎุด ฺฉ ูุฌููุนูโุฏุงุฏู ุฌุฏุฏ ุฑุง ุฏุฑุงูุช ู ูพุฑุฏุงุฒุด ฺฉูุฏ (ุจุฎุด ูุณุชูุฏุงุช ุนููู ุฑุง ุฏุฑ [ุงูุฌุง](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub) ูุดุงูุฏู ฺฉูุฏ). ุงูุง ุงุฌุงุฒู ุจุฏูุฏ ุงฺฉููู ุฑู ุฏุงุฏู MRPC ุชูุฑฺฉุฒ ฺฉูู! ุงู ุฏุงุฏู ฺฉ ุงุฒ ฑฐ ุฏุงุฏู [GLUE benchmark](https://gluebenchmark.com/) ุงุณุช ฺฉู ฺฉ ูุญฺฉ ุชูู ุดุฏู ุฏุฑ ูุญุท ุฏุงูุดฺฏุงู ุฌูุช ุงูุฏุงุฒู ฺฏุฑ ฺฉุงุฑฺฉุฑุฏ ูุฏููุง ุงุฏฺฏุฑ ูุงุดู ุฏุฑ ฑฐ ูุณุฆูู ุฏุณุชูโุจูุฏ ูุชู ูุฎุชูู ูโุจุงุดุฏ.

ฺฉุชุงุจุฎุงูู ูุฌููุนูโุฏุงุฏู ๐ค ฺฉ ุฏุณุชูุฑ ุจุณุงุฑ ุณุงุฏู ุฌูุช ุฏุงูููุฏ ู ุฐุฎุฑู ุณุงุฒ ฺฉ ูุฌููุนูโุฏุงุฏู ุฏุฑ ูุงุจ ุงุฑุงุฆู ูโฺฉูุฏ. ูุง ูโุชูุงูู ูุฌููุนูโุฏุงุฏู MRPC ุฑุง ุจู ุฑูุด ุฒุฑ ุฏุงูููุฏ ฺฉูู:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

ููุงูุทูุฑ ฺฉู ู ุจูุฏ ฺฉ ุดุก "DatasetDict" ุจุฏุณุช ูโุขูุฑู ฺฉู ุดุงูู ูุฌููุนู trainingุ ูุฌููุนู validationุ ู ูุฌููุนู test ูโุจุงุดุฏ. ูุฑฺฉ ุงุฒ ุงููุง ุดุงูู ฺูุฏู ุณุทูู (`label`ุ `sentence2`ุ `sentence1`ุ ู `idx`) ู ุชุนุฏุงุฏ ูุชุบุฑ ุฑุฏู ุฏุงุฏู ฺฉู ุนูุงุตุฑ ูุฑ ูุฌููุนู ุฑุง ุชุดฺฉู ูโุฏููุฏ ูโุจุงุดุฏ. (ุจูุงุจุฑุงูุ ณุถถธ ุฌูุช ุฌููู ุฏุฑ ูุฌููุนู training ูุฌูุฏ ุฏุงุฑุฏุ ดฐธ ุชุง ุฏุฑ ูุฌููุนู validationุ ู ฑุทฒต ุชุง ุฏุฑ ูุฌููุนู test).

 ุงู ุฏุณุชูุฑ ูุฌููุนูโุฏุงุฏู ุฑุง ุฏุงูููุฏ ู ุจู ุตูุฑุช ูพุด ูุฑุถ ุฏุฑ ุฒุฑุดุงุฎูโ *~/.cache/huggingface/dataset* ุฐุฎุฑู ูฺฉูุฏ. ุงุฒ ูุตู ฒ ุจู ุงุฏ ุฏุงุดุชู ุจุงุดุฏ ฺฉู ูโุชูุงูุฏ ุฒุฑุดุงุฎูโ ุฐุฎุฑูโุณุงุฒุชุงู ุฑุง ุจุง ุชูุธู ูุชุบุฑ ูุญุท `HF_HOME` ุจู ุฏูุฎูุงู ุชุบุฑ ุฏูุฏ.

ูุง ูโุชูุงูู ุจู ูุฑ ุฌูุช ุงุฒ ุฌููุงุช ุฏุฑ ุดุฆ `raw_datasets` ุจุง ุงุณุชูุงุฏู ุงุฒ ุงูุฏุณ ฺฏุฐุงุฑ, ูุงููุฏ ฺฉ dictionary ุฏุณุชุฑุณ ูพุฏุง ฺฉูู: 

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

ูโุจูู ฺฉู ุจุฑฺุณุจโูุง ุงุฒ ูพุด ุงุนุฏุงุฏ ุตุญุญ ูุณุชูุฏุ ุจูุงุจุฑุงู ูุฌุจูุฑ ูุณุชู ูฺ ูพุดโูพุฑุฏุงุฒุด ุฑู ุขููุง ุงูุฌุงู ุฏูู. ุจุฑุง ุงูฺฉู ุจุฏุงูู ฺฉุฏุงู ููุฏุงุฑู ุนุฏุฏู ุตุญุญ ุจู ฺฉุฏุงู ุจุฑฺุณุจ ูุฑุจูุท ูโุดูุฏุ ูโุชูุงูู ูฺฺฏโูุง โ`raw_train_dataset`โูุงู ุฑุง ุจุฑุฑุณ ฺฉูู. ุงู ฺฉุงุฑ ููุน ูุฑ ุณุทูู ุฑุง ุจู ูุง ุฎูุงูุฏ ฺฏูุช.

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

ุฏุฑ ูพุดุช ุตุญููุ `ุจุฑฺุณุจ` ุงุฒ ููุน `ุจุฑฺุณุจู ฺฉูุงุณ` ูโุจุงุดุฏุ ู ูฺฏุงุดุช ุงุนุฏุงุฏ ุตุญุญ ุจู ูุงู ุจุฑฺุณุจ ุฏุฑ ูพูุดูโู *names* ุฐุฎุฑู ุดุฏู ุงุณุช. `0` ูุฑุจูุท ุจู `not_equivalent`ุ ู `1` ูุฑุจูุท ุจู `equivalent` ูโุจุงุดุฏุ.

<Tip>
โ๏ธ **ุงูุชุญุงู ฺฉูุฏ!** ุนูุตุฑ ุดูุงุฑู ฑต ุงุฒ ุฏุงุฏู training ู ุนูุตุฑ ุดูุงุฑู ธท ุงุฒ ุฏุงุฏู validation ุฑุง ูุดุงูุฏู ฺฉูุฏ. ุจุฑฺุณุจูุง ุขููุง ฺุณุชุ
</Tip>

### ูพุดโูพุฑุฏุงุฒุดู ฺฉ ุฏุชุงุณูุช

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ุจู ููุธูุฑ ูพุดโูพุฑุฏุงุฒุด ุฏุชุงุณูุชุ ูุงุฒู ุงุณุช ูุชู ุฑุง ุจู ุงุนุฏุงุฏ ูุงุจู ูพุฑุฏุงุฒุด ุจุฑุง ูุฏู ุชุจุฏู ฺฉูู. ููุงูุทูุฑ ฺฉู ุฏุฑ[ูุตู ูุจู](/course/chapter2) ูุดุงูุฏู ฺฉุฑุฏุฏุ ุงู ฺฉุงุฑ ุจุง ุงุณุชูุงุฏู ุงุฒ ฺฉ ุชูฺฉูุงุฒุฑ ุงูุฌุงู ูโุดูุฏ. ูุง ูโุชูุงูู ฺฉ ุง ฺูุฏ ุฌููู ุฑุง ุจู ุชูฺฉูุงุฒุฑ ุจุฏููุ ุฏุฑ ูุชุฌู ูโุชูุงูู ูุณุชููุง ุชูุงู ุฌููุงุช ุงูู ู ุฏูู ูุฑ ุฌูุช ุฌููู ุฑุง ุจู ุตูุฑุช ุฒุฑ ุชูฺฉูุงุฒ ฺฉูู:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ุจุง ุงู ุญุงูุ ููโุชูุงูู ุฏู ุฌููู ุฑุง ุจู ูุฏู ุงุฑุณุงู ฺฉูู ุชุง ูพุดโุจู ฺฉูุฏ ูุชูุงุธุฑ ูุณุชูุฏ ุง ุฎุฑ. ูุง ูุงุฒ ุฏุงุฑู ุจุง ุฏู ุฑุดุชู ุจู ุตูุฑุช ฺฉ ุฌูุช ุจุฑุฎูุฑุฏ ฺฉููุ ู ูพุดโูพุฑุฏุงุฒุด ููุงุณุจ ุฑุง ุจู ุขู ุงุนูุงู ฺฉูู. ุฎูุดุจุฎุชุงููุ ุชูฺฉูุงุฒุฑ ูโุชูุงูุฏ ฺฉ ุฌูุช ุฑุดุชู ุฑุง ุฏุฑุงูุช ฺฉูุฏ ู ุขูุฑุง ุจู ฺฏูููโุง ฺฉู ูุฏู BERT ูุง ุงูุชุธุงุฑ ุฏุงุฑุฏ ุขูุงุฏู ุณุงุฒ ฺฉูุฏ:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

ุฏุฑ [ูุตู ฒ](/course/chapter2) ุฏุฑ ููุฑุฏ ฺฉูุฏูุง `input_ids` ู `attention_mask` ุจุญุซ ฺฉุฑุฏูุ ุงูุง ุงุฒ ฺฏูุชฺฏู ุฏุฑ ููุฑุฏ `token_type_ids` ุงุฌุชูุงุจ ฺฉุฑุฏู. ุฏุฑ ุงู ูุซุงู ุงู ููุงู ฺุฒ ุงุณุช ฺฉู ุจู ูุฏู ูโฺฏูุฏ ฺฉุฏุงู ุจุฎุด ุงุฒ ูุฑูุฏุ ุฌููู ุงูู ู ฺฉุฏุงู ุจุฎุด ุฌููู ุฏูู ุงุณุช.

<Tip>

โ๏ธ **ุงูุชุญุงู ฺฉูุฏ!** ุนูุตุฑ ุดูุงุฑู ฑต ุฏุงุฏู ุฑุง ุจุฑุฏุงุฑุฏ ู ุฏู ุฌููู ุฑุง ุจู ุตูุฑุช ุฌุฏุงฺฏุงูู ู ุฌูุช ุชูฺฉูุงุฒ ฺฉูุฏ. ุชูุงูุช ุฏู ูุชุฌู ฺุณุชุ

</Tip>

ุงฺฏุฑ ุขุฏโูุง ุฏุงุฎู `input_ids` ุฑุง ุจู ฺฉููุงุช ุฑูุฒฺฏุดุง ฺฉูู:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ุฎูุงูู ุฏุงุดุช:

we will get:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

ุจูุงุจุฑุงู ูโุจูู ฺฉู ูุฏู ุงูุชุธุงุฑ ุฏุงุฑุฏ ููุช ฺฉู ุฏู ุฌููู ุฏุงุฑู ูุฑูุฏโูุง ุจู ุตูุฑุช `[CLS] sentence1 [SEP] sentence2 [SEP]` ุจุงุดูุฏ.

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ููุงูุทูุฑ ฺฉู ูโุจูุฏุ ุจุฎุดูุง ุงุฒ ูุฑูุฏ ฺฉู ูุฑุจูุท ุจู `[CLS] sentence1 [SEP]` ูุณุชูุฏ ุขุฏ ูุดุงูุฏููุฏู ููุน ุชูฺฉูู ุขููุง `0` ู ุจุฎุดูุง ฺฉู ูุฑุจูุท ุจู `sentence2 [SEP]` ูุณุชูุฏ ุขุฏ ูุดุงูุฏููุฏู ููุน ุชูฺฉููโุดุงู `1` ูโุจุงุดุฏ.

ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุงฺฏุฑ ฺฺฉูพููุช ูุชูุงูุช ุฑุง ุงูุชุฎุงุจ ฺฉูุฏุ ุฏุฑ ูุฑูุฏโูุง ูุฒููุง `token_type_ids` ูุฎูุงูุฏ ุฏุงุดุช (ุจู ุนููุงู ูุซุงูุ ุงฺฏุฑ ุงุฒ ฺฉ DistilBERT ุงุณุชูุงุฏู ฺฉูุฏ ุขููุง ุจุงุฒฺฏุฑุฏุงูุฏู ูุฎูุงููุฏ ุดุฏ). ุขููุง ููุท ุฒูุงู ุจุงุฒฺฏุฑุฏุงูุฏู ูโุดููุฏ ฺฉู ูุฏู ูโุฏุงูุฏ ุจุง ุขููุง ฺฺฉุงุฑ ฺฉูุฏุ ุจู ุงู ุฎุงุทุฑ ฺฉู ุขููุง ุฑุง ุฏุฑ ุฒูุงู ูพุดโุชุนูู ุฏุฏู ุงุณุช.

ุฏุฑ ุงูุฌุงุ ูุฏู BERT ุจุง ุขุฏโูุง ฺฉู ูุดุงูุฏููุฏู ููุนู ุชูฺฉู ูุณุชูุฏ ูพุดโุชุนูู ุดุฏูุ ู ุนูุงูู ุจุฑ ูุฏู ูุฏู ุณุงุฒ ุฒุจุงูู ูุงุณฺฉ ฺฉู ุฏุฑ [ูุตู ฑ](/course/chapter1) ุฏุฑ ููุฑุฏ ุขู ุตุญุจุช ฺฉุฑุฏู ูุธููโ ุฏฺฏุฑ ุชุญุช ุนููุงู _ูพุดโุจู ุฌูููโ ุจุนุฏ_ ุจุฑุนูุฏู ุฏุงุฑุฏ. ูุฏู ุงุฒ ุงู ูุธูู ูุฏู ฺฉุฑุฏู ุฑุงุจุทู ุจู ุฌูุช ุฌูููโูุง ูโุจุงุดุฏ.

 ุฏุฑ ุฑูุด ูพุดโุจู ุฌููู ุจุนุฏุ ุฌูุช ุฌููุงุช (ุจุง ฺฉููุงุช ฺฉู ุจู ุทูุฑ ุชุตุงุฏู ูพููุงู ุดุฏูโุงูุฏ) ุจู ูุฏู ุฏุงุฏู ูโุดููุฏ ู ุงุฒ ูุฏู ุฎูุงุณุชู ูโุดูุฏ ูพุดโุจู ฺฉูุฏ ฺฉู ุขุง ุฌููู ุฏูู ุฏุฑ ุงุฏุงููโ ุฌูููโ ุงูู ูุฑุงุฑ ุฏุงุฑุฏ ุง ุฎุฑ. ุจุฑุง ุฎุงุฑุฌ ฺฉุฑุฏู ูุณุฆูู ุงุฒ ุญุงูุช ุจุฏูุ ุฏุฑ ูู ุงุฒ ุญุงูุชูุง ุฏูุฌููู ุฏุฑ ูุชู ุงุตู ุจู ุฏูุจุงู ูู ุขูุฏูโุ ู ุฏุฑ ูู ุฏฺฏุฑ ุงุฒ ุฏู ูุชู ูุชูุงูุช ูโุขูุฏ.

ุฏุฑ ูุฌููุนุ ูุงุฒ ูุณุช ูฺฏุฑุงู ูุฌูุฏ ุง ุนุฏู ูุฌูุฏ `token_type_ids` ุฏุฑ ูุฑูุฏโูุง ุชูฺฉูุงุฒ ุดุฏู ุฎูุฏ ุจุงุดุฏ: ูุงุฏุงู ฺฉู ุงุฒ ฺฺฉูพููุช ฺฉุณุงู ุจุฑุง ุชูฺฉูุงุฒุฑ ู ูุฏู ุงุณุชูุงุฏู ฺฉูุฏุ ููู ฺุฒ ุฎูุจ ูพุด ุฎูุงูุฏ ุฑูุช ฺุฑุง ฺฉู ุชูฺฉูุงุฒุฑ ูโุฏุงูุฏ ฺู ฺุฒ ุจุฑุง ูุฏู ูุฑุงูู ฺฉูุฏ.

ุงฺฉููู ฺฉู ูุดุงูุฏู ฺฉุฑุฏู ฺฺฏููู ุชูฺฉูุงุฒุฑ ูุง ูโุชูุงูุฏ ุจุง ฺฉ ุฌูุช ุฌููู ุจุฑุฎูุฑุฏ ฺฉูุฏุ ูโุชูุงูู ุขูุฑุง ุจุฑุง ุชูฺฉูุงุฒ ฺฉุฑุฏู ฺฉู ุฏุชุงุณูุชูุงู ุจู ฺฉุงุฑ ุจุจุฑู: ูุงููุฏ [ูุตู ูุจู](/course/chapter2)ุ ูุง ูโุชูุงูู ููฺฉูุงุฒุฑ ุฑุง ุจุง ูุณุช ุงุฒ ุฌูุช ุฌูููโูุงุ ุจุง ุฏุงุฏู ูุณุช ุฌููุงุช ุงูู ู ุณูพุณ ูุณุช ุฌููุงุช ุฏููุ ุชุบุฐู ฺฉูู. ุงู ุฑูุด ููฺูู ุจุง ฺฏุฒููโูุง padding ู truncation ฺฉู ุฏุฑ [ูุตู ฒ](/course/chapter2) ูุดุงูุฏู ฺฉุฑุฏู ุณุงุฒฺฏุงุฑ ุฏุงุฑุฏ. ุจูุงุจุฑุงูุ ฺฉ ุฑูุด ุจุฑุง ูพุดโูพุฑุฏุงุฒุดู ุฏุชุงุณูุช training ุงูฺฏููู ูโุจุงุดุฏ:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```
ุงู ุฑูุด ุจู ุฎูุจ ฺฉุงุฑ ูโฺฉูุฏุ ุงูุง ูุดฺฉูุด ุงู ุงุณุช ฺฉู ฺฉ dictionary ุจุฑูฺฏุฑุฏุงูุฏ (ุงุฒ ฺฉูุฏูุง ูุง ุดุงููุ `input_ids`, `attention_mask`, ู `token_type_ids`, ู ููุงุฏุฑ ุขููุง ฺฉู ูุฌููุนูโุง ุงุฒ ูุฌููุนูโูุง ูุณุชูุฏ). ููฺูู ุงู ุฑูุด ููุท ุฒูุงู ฺฉุงุฑ ูโฺฉูุฏ ฺฉู ุญุงูุธู ูููุช ฺฉุงู ุฌูุช ุฐุฎุฑูโุณุงุฒ ฺฉู ุฏุชุงุณูุช ุฏุฑ ุญู ุชูฺฉูุงุฒ ฺฉุฑุฏู ุฏุงุดุชู ุจุงุดุฏ (ุฏุฑ ุญุงู ฺฉู ุฏุชุงุณูุชโูุง ููุฌูุฏ ุฏุฑ ูพุงฺฏุงู ุฏุชุงุณูุช ๐ค ูุงูโูุง ุงุฒ ููุน [Apache Arrow](https://arrow.apache.org/) ูุณุชูุฏ ฺฉู ุฑู ุฏุณฺฉ ุฐุฎุฑู ุดุฏูโุงูุฏุ ุจูุงุจุฑุงู ุดูุง ููุท ูููููโูุง ุฑุง ฺฉู ุฌูุช ุฐุฎุฑู ุฏุฑ ุญุงูุธู ุฏุฑุฎูุงุณุช ฺฉุฑุฏูโุงุฏ ูฺฏู ูโุฏุงุฑุฏ).

ุจู ููุธูุฑ ูฺฏู ุฏุงุดุชู ุฏุงุฏู ุจู ุตูุฑุช ฺฉ ุฏุชุงุณูุชุ ุงุฒ ุชุงุจุน [`()Dataset.map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) ุงุณุชูุงุฏู ูฺฉูู. ฺูุงูฺู ุจู ูพุดโูพุฑุฏุงุฒุดโูุง ุจุดุชุฑ ุนูุงููโุจุฑ ุชูฺฉูุงุฒ ฺฉุฑุฏู ูุงุฒ ุฏุงุดุชู ุจุงุดู ุงู ุฑูุด ุงูุนุทุงูโูพุฐุฑ ูุงุฒู ุฑุง ุจู ูุง ูโุฏูุฏ. ุชุงุจุน `()map` ุจุง ุงุนูุงู ฺฉุฑุฏู ฺฉ ุนููุงุช ุฑู ูุฑ ุนูุตุฑ ุฏุชุงุณูุช ุนูู ูโฺฉูุฏุ ุจูุงุจุฑุงู ุงุฌุงุฒู ุฏูุฏ ุชุงุจุน ุชุนุฑู ฺฉูู ฺฉู ูุฑูุฏโูุง ุฑุง ุชูฺฉูุงุฒ ฺฉูุฏ:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

ุงู ุชุงุจุน ฺฉ ุฏฺฉุดูุฑ (ูุซู ุงููุงู ุฏุงุฎู ุฏุชุงุณูุช) ุฏุฑุงูุช ูฺฉูุฏ ู ุฏฺฉุดูุฑ ุฏฺฏุฑ ุจุง ฺฉูุฏูุง `input_ids`ุ `attention_mask`ุ ู `token_type_ids` ุจุงุฒูโฺฏุฑุฏุงูุฏ. ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ุงุฒ ุขูุฌุง ฺฉู ุชูฺฉูุงุฒุฑ ุฑู ูุณุชูุง ุงุฒ ุฌูุช ุฌูููโูุง ฺฉุงุฑ ูโฺฉูุฏุ ููุงูุทูุฑ ฺฉู ูุจูุง ูุดุงูุฏู ฺฉุฑุฏูุ ุงู ุชุงุจุน ูุฒ ุฏุฑุตูุฑุช ฺฉู ุฏฺฉุดูุฑ `example` ุดุงูู ฺูุฏู ููููู (ูุฑ ฺฉูุฏ ุจู ุนููุงู ูุฌููุนูโุง ุงุฒ ุฌููุงุช) ุจุงุดุฏ ฺฉุงุฑ ูโฺฉูุฏ. ุงู ุจู ูุง ุงู ุงูฺฉุงู ุฑุง ุฎูุงูุฏ ุฏุงุฏ ฺฉู ุงุฒ ฺฏุฒูู `batched=True` ุฏุฑ ูุฑุงุฎูุงู ุชุงุจุน `()map` ุงุณุชูุงุฏู ฺฉูู ฺฉู ุชูฺฉูุงุฒุฑ ุฑุง ุจู ูุฒุงู ุฒุงุฏ ุณุฑุนุชุฑ ุฎูุงูุฏ ฺฉุฑุฏ. ุงู ุชูฺฉูุงุฒุฑ ุจุง ุชูฺฉูุงุฒุฑ ุฏุฑ ฺฉุชุงุจุฎุงูู [๐ค Tokenizers](https://github.com/huggingface/tokenizers) ฺฉู ุจู ุฒุจุงู ุจุฑูุงููโููุณ Rust ููุดุชู ุดุฏู ูพุดุชุจุงู ูโุดูุฏ. ุงู ุชูฺฉูุงุฒุฑ ูโุชูุงูุฏ ุจุณุงุฑ ุณุฑุน ุจุงุดุฏุ ุงูุง ููุท ุจู ุดุฑุท ฺฉู ูุฑูุฏโูุง ุฒุงุฏ ุฑุง ุจู ุตูุฑุช ฺฉุฌุง ุจู ุขู ุจุฏูู.

ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ูุง ูุจุญุซ `padding` ุฑุง ุฏุฑ ุญุงู ุญุงุถุฑ ฺฉูุงุฑ ฺฏุฐุงุดุชูโุงู. ุงู ุจู ุงู ุฎุงุทุฑ ุงุณุช ฺฉู ุงูุฌุงู `padding` ุฑู ููู ูููููโูุง ุจุฑุง ุจุดุชุฑู ุทูู ุจู ุตุฑูู ูุณุช: ุจูุชุฑ ุงุณุช ฺฉู `padding` ุฒูุงู ฺฉู ุฏุฑ ุญุงู ุณุงุฎุชู ุจูุชฺ ูุณุชู ุฑู ูููููโูุง ุงูุฌุงู ุฏููุ ฺุฑุง ฺฉู ุขูููุช ููุท ูุงุฒ ุฏุงุฑู ฺฉู `padding` ุฑุง ุฑู ููุงู ุจูุชฺ ุงูุฌุงู ุฏููุ ู ูู ุจุดุชุฑู ุทูู ุฏุฑ ุณุฑุชุงุณุฑ ุฏุชุงุณูุช. ุงู ุฑูุด ุฒูุงู ฺฉู ูุฑูุฏโูุง ุฏุงุฑุง ุทููโูุง ุจุณุงุฑ ูุชุบุฑ ูุณุชูุฏ ููุช ู ุงูุฑฺ ุฒุงุฏ ุฑุง ุตุฑูู ุฌู ุฎูุงูุฏ ฺฉุฑุฏ. 

ุฏุฑ ุงูุฌุง ูุดุงู ูโุฏูู ฺฺฏููู ุชุงุจุน ุชูฺฉูุงุฒุดู ุฑุง ุฑู ฺฉู ุฏุชุงุณูุช ุจู ฺฉุจุงุฑู ุงุนูุงู ูโฺฉูู. ูุง ุงุฒ `batched=True` ุฏุฑ ูุฑุงุฎูุงู ุชุงุจุน `map` ุงุณุชูุงุฏู ูโฺฉูู ุจูุงุจุฑุงู ุชุงุจุน ุฑู ฺูุฏู ุนูุตุฑ ุงุฒ ุฏุชุงุณูุช ูุง ุจู ฺฉุจุงุฑู ุนูู ูโฺฉูุฏ ูู ุฑู ูุฑ ุนูุตุฑ ุจู ุตูุฑุช ุฌุฏุงฺฏุงูู. ุงู ุงุฌุงุฒู ูโุฏูุฏ ฺฉู ูพุดโูพุฑุฏุงุฒุด ุณุฑุนุชุฑ ุงูุฌุงู ฺฏุฑุฏ:

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```
ุฑูุด ฺฉู ฺฉุชุงุจุฎุงูู ุฏุชุงุณูุช ๐ค ุงู ูพุดโูพุฑุฏุงุฒุด ุฑุง ุงุนูุงู ูโฺฉูุฏุ ุจุง ุงูุฒูุฏู -ููุฏูุง- ุฌุฏุฏ ุจู ุฏุชุงุณูุชโูุงุ ฺฉ ุจู ุงูุฒุง ูุฑ ฺฉูุฏ ุฏุฑ -ุฏฺฉุดูุฑ- ฺฉู ุชูุณุท ุชุงุจุน ูพุดโูพุฑุฏุงุฒุด ุจุงุฒฺฏุฑุฏุงูุฏู ูโุดููุฏุ ูโุจุงุดุฏ:ุ

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

ุดูุง ุญุช ูโุชูุงูุฏ ุฒูุงู ฺฉู ุชุงุจุน ูพุดโูพุฑุฏุงุฒุด ุฎูุฏ ุฑุง ุงุนูุงู ูโฺฉูุฏุ ุจุง ุงุฑุณุงู ุขุฑฺฏููุงู `num_proc` ุฏุฑ ุชุงุจุน `()map` ุงุฒ ูพุฑุฏุงุฒุด ููุงุฒ ุงุณุชูุงุฏู ฺฉูุฏ. ูุง ุขู ุฑุง ุฏุฑ ุงูุฌุง ุงุณุชูุงุฏู ูฺฉุฑุฏู ุจู ุงู ุฎุงุทุฑ ฺฉู ฺฉุชุงุจุฎุงูู ุชูฺฉูุงุฒุฑ ๐ค ุงุฒ ูพุด ุงุฒ ฺูุฏู ุฑุดุชู ูพุฑุฏุงุฒูุฏู ุจุฑุง ุชูฺฉูุงุฒ ฺฉุฑุฏู ุณุฑุนุชุฑ ูููููโูุง ูุง ุงุณุชูุงุฏู ูโฺฉูุฏุ ุงูุง ุฏุฑ ุตูุฑุช ฺฉู ุงุฒ ฺฉ ุชูฺฉูุงุฒุฑ ุณุฑุน ฺฉู ุจุง ุงู ฺฉุชุงุจุฎุงูู ูพุดุชุจุงู ูุดูุฏ ุงุณุชูุงุฏู ููโฺฉูุฏุ ุงู ุฑูุด ูุชูุงูุฏ ูพุดโูพุฑุฏุงุฒุด ุดูุง ุฑุง ุณุฑุนุชุฑ ฺฉูุฏ.


ุชุงุจุน `tokenize_function` ูุง ฺฉ ุฏฺฉุดูุฑ ุดุงูู ฺฉูุฏูุง `input_ids`ุ `attention_mask`ุ ู `token_type_ids`ุ ุจุฑูฺฏุฑุฏุงูุฏ ุจู ฺฏูููโุง ฺฉู ุงู ุณู ููุฏ ุจู ููู ุจุฎุดโูุง ุฏุชุงุณูุช ุงูุฒูุฏู ฺฏุฑุฏูุฏ. ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุงฺฏุฑ ุชุงุจุน ูพุดโูพุฑุฏุงุฒุด ูุง ุจุฑุง ฺฉ ฺฉูุฏ ููุฌูุฏ ุฏุฑ ุฏุชุงุณูุช ฺฉู ุชุงุจุน `()map` ุจู ุขู ุงุนูุงู ุดุฏู ฺฉ ููุฏุงุฑ ุฌุฏุฏ ุจุงุฒูโฺฏุฑุฏุงูุฏ ููฺูู ูโุชูุงูุณุชู ููุฏูุง ููุฌูุฏ ุฑุง ุชุบุฑ ุฏูู.

ุขุฎุฑู ฺุฒ ฺฉู ูุงุฒ ุฏุงุฑู ุงูุฌุงู ุฏูู ุงู ุงุณุช ฺฉู ููฺฏุงู ฺฉู ุนูุงุตุฑ ุฑุง ุจุงูู ุจูุชฺ ูโฺฉููุ ููู ูููููโูุง ุฑุง ุจู ุงูุฏุงุฒู ุทูู ุจููุฏุชุฑู ุนูุตุฑ ููโุทูู ฺฉูู - ุชฺฉูฺฉ ฺฉู ูุง ุจู ุขู *ููโุทููโุณุงุฒ ูพูุง* ูโฺฏูู.


### ููโุทููโุณุงุฒ ูพูุง

### Dynamic padding

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}

ุชุงุจุน ฺฉู ูุณุฆูู ฺฉูุงุฑูู ฺฏุฐุงุดุชู ูููููโูุง ุฏุฑ ฺฉ ุจูุชฺ ูโุจุงุดุฏ *collate function* ุฎูุงูุฏู ูโุดูุฏ. ุงู ุชุงุจุน ุขุฑฺฏููุงู ุงุณุช ฺฉู ุดูุง ูโุชูุงูุฏ ููฺฏุงู ุณุงุฎุชู ฺฉ `DataLoader` ุจู ุฏุงุฎู ุขู ุงุฑุณุงู ฺฉูุฏุ ฺฉู ุฏุฑ ุญุงูุช ูพุด ูุฑุถ ุชุงุจุน ุงุณุช ฺฉู ูููููโูุง ุดูุง ุฑุง ุจู ูุงุชุฑุณ PyTorch ุชุจุฏู ฺฉุฑุฏู ู ุขููุง ุฑุง ุจูู ุงูุญุงู ูโฺฉูุฏ (ุงฺฏุฑ ุนูุงุตุฑ ุดูุง ูุณุชุ ุชุงูพูุ ุง ุฏฺฉุดูุฑ ุจุงุดูุฏ ุงู ฺฉุงุฑ ุจู ุตูุฑุช ุจุงุฒฺฏุดุช ุงูุฌุงู ูโฺฏุฑุฏ). ุงุฒ ุขูุฌุง ฺฉู ูุฑูุฏโูุง ูุง ููู ุจู ฺฉ ุงูุฏุงุฒู ูุณุชูุฏ ุงู ฺฉุงุฑ ุจุฑุง ูุง ุงูฺฉุงู ูพุฐุฑ ูุณุช. ูุง ูพุฑูุณู ููโุทููโุณุงุฒ ุฑุง ุนูุฏุง ุจู ุชุนูู ุงูุฏุงุฎุชูุ ุชุง ููุท ุฏุฑ ุฒูุงู ูุงุฒ ุขูุฑุง ุฑู ูุฑ ุจูุชฺ ุงุฌุฑุง ฺฉูู ู ุงุฒ ุฏุงุดุชู ูุฑูุฏโูุง ุจุด ุงุฒ ุงูุฏุงุฒู ุทููุงู ุจุง ุชุนุฏุงุฏ ุฒุงุฏ ููโุทููโุณุงุฒ ูพุดฺฏุฑ ฺฉูู. ุงู ุฑูุด ูพุฑูุณู ุขููุฒุด ุฑุง ุชุง ุงูุฏุงุฒูโุง ุณุฑุนุช ูโุจุฎุดุฏุ ุงูุง ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุงฺฏุฑ ุดูุง ุฏุฑ ุญุงู ุขููุฒุด ุฑู TPU ูุณุชุฏ ูโุชูุงูุฏ ูุดฺฉู ุณุงุฒ ุจุงุดุฏ - TPU ูุง ุงุดฺฉุงู ูุนู ุฑุง ุชุฑุฌุญ ูโุฏููุฏุ ุญุช ุงฺฏุฑ ูุงุฒ ุจู ููโุทููโุณุงุฒ ุงุถุงูู ุฏุงุดุชู ุจุงุดุฏ.

{:else}

ุชุงุจุน ฺฉู ูุณุฆูู ฺฉูุงุฑูู ฺฏุฐุงุดุชู ูููููโูุง ุฏุฑ ฺฉ ุจูุชฺ ูโุจุงุดุฏ *collate function* ุฎูุงูุฏู ูโุดูุฏ. collator ูพุด ูุฑุถ ุชุงุจุน ุงุณุช ฺฉู ููุท ูููููโูุง ุดูุง ุฑุง ุจู tf.Tensor ุชุจุฏู ฺฉุฑุฏู ู ุขููุง ุฑุง ุจูู ุงูุญุงู ูโฺฉูุฏ (ุงฺฏุฑ ุนูุงุตุฑ ุดูุง ูุณุชุ ุชุงูพูุ ุง ุฏฺฉุดูุฑ ุจุงุดูุฏ ุงู ฺฉุงุฑ ุจู ุตูุฑุช ุจุงุฒฺฏุดุช ุงูุฌุงู ูโฺฏุฑุฏ). ุงุฒ ุขูุฌุง ฺฉู ูุฑูุฏโูุง ูุง ููู ุจู ฺฉ ุงูุฏุงุฒู ูุณุชูุฏ ุงู ฺฉุงุฑ ุจุฑุง ูุง ุงูฺฉุงู ูพุฐุฑ ูุณุช. ูุง ูพุฑูุณู ููโุทููโุณุงุฒ ุฑุง ุนูุฏุง ุจู ุชุนูู ุงูุฏุงุฎุชูุ ุชุง ููุท ุฏุฑ ุฒูุงู ูุงุฒ ุขูุฑุง ุฑู ูุฑ ุจูุชฺ ุงุฌุฑุง ฺฉูู ู ุงุฒ ุฏุงุดุชู ูุฑูุฏโูุง ุจุด ุงุฒ ุงูุฏุงุฒู ุทููุงู ุจุง ุชุนุฏุงุฏ ุฒุงุฏ ููโุทููโุณุงุฒ ูพุดฺฏุฑ ฺฉูู. ุงู ุฑูุด ูพุฑูุณู ุขููุฒุด ุฑุง ุชุง ุงูุฏุงุฒูโุง ุณุฑุนุช ูโุจุฎุดุฏุ ุงูุง ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุงฺฏุฑ ุดูุง ุฏุฑ ุญุงู ุขููุฒุด ุฑู TPU ูุณุชุฏ ูโุชูุงูุฏ ูุดฺฉู ุณุงุฒ ุจุงุดุฏ - TPU ุงุดฺฉุงู ูุนู ุฑุง ุชุฑุฌุญ ูโุฏููุฏุ ุญุช ุงฺฏุฑ ูุงุฒ ุจู ููโุทููโุณุงุฒ ุงุถุงูู ุฏุงุดุชู ุจุงุดุฏ.

{/if}

ุจุฑุง ุงูุฌุงู ุงูฺฉุงุฑ ุฏุฑ ุนููุ ูุง ุจุงุฏ ฺฉ ุชุงุจุน collate ุชุนุฑู ฺฉูู ฺฉู ูุฒุงู ุฏุฑุณุช ุงุฒ ููโุทููโุณุงุฒ ุฑุง ุจู ุขุชูโูุง ุฏุชุงุณูุชโูุง ฺฉู ูุง ูโุฎูุงูู ุจุงูู ุจูุชฺ ฺฉูู ุงุนูุงู ฺฉูุฏ. ุฎูุดุจุฎุชุงููุ ฺฉุชุงุจุฎุงูู ุชุฑูุณููุฑูุฑูุง ๐ค ฺูู ุชุงุจุน ุฑุง ุชุญุช ุนููุงู `DataCollatorWithPadding` ุฏุฑ ุงุฎุชุงุฑ ูุง ูุฑุงุฑ ูุฏูุฏ. ุงู ุชุงุจุน ุจู ูุญุถ ุงูฺฉู ุขูุฑุง ุชุนุฑู (ุนู ุชุนู ฺฉูู ฺู ุชูฺฉู ุจุฑุง ููโุทููโุณุงุฒ ุงุณุชูุงุฏู ฺฉูุฏุ ู ุงูฺฉู ูุฏู ุงูุชุธุงุฑ ููโุทููโุณุงุฒ ุงุฒ ุณูุช ฺูพ ูุฑูุฏโูุง ุฑุง ุฏุงุดุชู ุจุงุดุฏ ุง ุงุฒ ุณูุช ุฑุงุณุช ุขููุง) ฺฉ ุชูฺฉูุงุฒุฑ ุฑุง ุจุฑุฏุงุดุชู ู ูุฑ ฺฉุงุฑ ุฑุง ฺฉู ูุงุฒู ุฏุงุฑุฏ ุงูุฌุงู ูโุฏูุฏ:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

ุงุฌุงุฒู ุฏูุฏ ฺูุฏ ููููู ุงุฒ ูุฌููุนู training ูุงูุ ฺฉู ูโุฎูุงูู ุจุงูู ุจูุชฺ ฺฉููุ ุจุฑุฏุงุฑู ุชุง ุงู ุงุจุฒุงุฑ ุฌุฏุฏ ุฑุง ุงูุชุญุงู ฺฉูู. ุฏุฑ ุงูุฌุง ุณุทููโูุง `idx`ุ `sentence1`ุ ู `sentence2` ุฑุง ุญุฐู ูโฺฉูู ฺุฑุง ฺฉู ุงุญุชุงุฌ ูุฎูุงููุฏ ุดุฏ ู ุดุงูู ุฑุดุชูโูุง ูุชู ูโุดููุฏ (ฺฉู ูุง ููโุชูุงูู ุชูุณูุฑูุง ุงุฒ ุฑุดุชูโูุง ูุชู ุงุฌุงุฏ ฺฉูู) ู ุณูพุณ ูฺฏุงู ูโุงูุฏุงุฒู ุจู ุทูู ูุฑ ูุฑูุฏ ุฏุฑ ูุฑ ุจูุชฺ:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

ุชุนุฌุจ ูุฏุงุฑุฏ ฺฉู ูููููโูุง ุจุง ุทููโูุง ูุชุบุฑุ ุงุฒ ณฒ ุชุง ถท ุจุฏุณุช ูโุขูุฑู. ููโุทููโุณุงุฒ ูพูุง ุจู ุงู ูุนู ุงุณุช ฺฉู ูููููโูุง ููุฌูุฏ ุฏุฑ ุงู ุจูุชฺ ุจุงุฏ ููฺฏ ุจุง ุทูู ถทุ ฺฉู ุจุฒุฑฺฏุชุฑู ุทูู ุฏุงุฎู ุจูุชฺ ูโุจุงุดุฏุ ููโุทูู ุดุฏู ุจุงุดูุฏ. ุจุฏูู ููโุทููโุณุงุฒ ูพูุงุ ููู ูููููโูุง ุฏุฑ ฺฉู ุฏุชุงุณูุช ุจุงุฏ ุจู ุงูุฏุงุฒู ุจุฒุฑฺฏุชุฑู ุทูู ุง ุจุฒุฑฺฏุชุฑู ุทููู ูุงุจู ูพุฐุฑุด ุจุฑุง ูุฏูุ ููโุทูู ุดููุฏ. ุงุฌุงุฒู ุฏูุฏ ุจุฑุฑุณ ฺฉูู ุขุง `data_collator` ูุง ุจูุชฺ ุฑุง ุจู ุฏุฑุณุช ููโุทูู ูโฺฉูุฏ:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

ุจู ูุธุฑ ุฎูุจ ูโุขุฏ! ุงฺฉููู ฺฉู ุงุฒ ูุชู ุฎุงูุต ุจู ุจูุชฺโูุง ุฑุณุฏูโุงู ฺฉู ูุฏู ูุงู ูโุชูุงูุฏ ุจุง ุขููุง ฺฉุงุฑ ฺฉูุฏุ ุขูุงุฏู ูุณุชู ุจุฑุง ุงูุฌุงู ฺฉูฺฉโ ฺฉุฑุฏู ูุฏู:

{/if}

<Tip>

โ๏ธ **ุงูุชุญุงู ฺฉูุฏ!** ูพุฑูุณู ูพุดโูพุฑุฏุงุฒุด ุฑุง ุฑู ุฏุชุงุณูุช GLUE SST-2 ุจุงุฒ ุชฺฉุฑุงุฑ ฺฉูุฏ. ุงุฒ ุขูุฌุง ฺฉู ุงู ูุฌููุนู ุจู ุฌุง ุฌูุช ุดุงูู ุชฺฉ ุฌูููโูุง ูโุจุงุดุฏ ุงู ฺฉุงุฑ ฺฉ ููุฏุงุฑ ูุชูุงูุช ุงุณุชุ ุงูุง ุจูู ฺฉุงุฑูุง ฺฉู ุงูุฌุงู ุฏุงุฏูโุงู ุจุงุฏ ฺฉุณุงู ุจู ูุธุฑ ุจุฑุณูุฏ. ุจุฑุง ฺฉ ฺุงูุด ูุดฺฉูโุชุฑุ ุณุน ฺฉูุฏ ุชุงุจุน ูพุดโูพุฑุฏุงุฒุด ุจููุณุฏ ฺฉู ุจุฑุง ููู ูุณุฆููโูุง GLUE ฺฉุงุฑ ฺฉูุฏ.

</Tip>

{#if fw === 'tf'}

ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ูุง ูุฌููุนูโโุฏุงุฏู ูุงู ู ฺฉ collator ุฏุงุฏู ุฏุฑ ุงุฎุชุงุฑ ุฏุงุฑูุ ุญุงู ูุงุฒ ุฏุงุฑู ฺฉู ุขููุง ุฑุง ฺฉูุงุฑ ูู ูุฑุงุฑ ุฏูู. ูุง ูโุชูุงูุณุชู ุจูุชฺโูุง ุฑุง ุฏุณุช ููุฏ ฺฉุฑุฏู ู ุขููุง ุฑุง collate ฺฉููุ ุงูุง ุงู ุฑูุด ฺฉุงุฑ ุฒุงุฏ ูโุจุฑุฏ ู ุงุญุชูุงูุง ุฎู ูู ุจููู ูุฎูุงูุฏ ุจูุฏ. ุฏุฑ ุนูุถุ ฺฉ ุฑูุด ุณุงุฏู ูุฌูุฏ ุฏุงุฑุฏ ฺฉู ุฑุงู ุญู ุจูููโุง ุจุฑุง ุงู ูุณุฆูู ุงุฑุงุฆู ูฺฉูุฏ: `()to_tf_dataset`. ุงู ุฑูุด ฺฉ `tf.data.Dataset` ุฑุง ุฏูุฑ ุฏุชุงุณูุชโุชุงู ูโูพฺุฏุ ุจุง ฺฉ ุชุงุจุน collation ุฏูุฎูุงู. `tf.data.Dataset` ฺฉ ูุฑูุช ุจูู ุชูุณูุฑููู ุงุณุช ฺฉู ฺฉูุฑุงุณ ูโุชูุงูุฏ ุจุฑุง `()model.fit` ุงุณุชูุงุฏู ฺฉูุฏุ ุฏุฑ ุชูุฌู ููู ฺฉ ุชุงุจุน ูโุชูุงูุฏ ฺฉ ุฏุชุงุณูุช ๐ค ุฑุง  ุจู ุณุฑุนุช ุจู ูุฑูุช ุขูุงุฏู ุจุฑุง ุขููุฒุด ุชุจุฏู ฺฉูุฏ. ุงุฌุงุฒู ุฏูุฏ ุขูุฑุง ุฏุฑ ุนูู ุจุง ุฏุชุงุณูุชโูุงู ูุดุงูุฏู ฺฉูู!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

ุงู ูู ุงุฒ ุงู! ุญุงูุง ูโุชูุงูู ุงู ุฏุชุงุณูุชโูุง ุฑุง ุจู ุฏุฑุณ ุจุนุฏ ุจุจุฑูุ ุฌุง ฺฉู ุขููุฒุด ูพุณ ุงุฒ ููู ุณุฎุชโูุง  ูพุดโูพุฑุฏุงุฒุด ุจู ุทุฑุฒ ุฎูุดุงูุฏ ุณุฑุฑุงุณุช ุฎูุงูุฏ ุจูุฏ.

{/if}
