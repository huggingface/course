<FrameworkSwitchCourse {fw} />

# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ 
# Processing the data

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø«Ø§Ù„ [ÙØµÙ„ Ù‚Ø¨Ù„](/course/chapter2)ØŒ Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Ù…Ø¯Ù„ ØªØ±ØªÛŒØ¨ÛŒ Ø¯Ø± ÛŒÚ© Ø¨ÙØªÚ† ØªÙˆØ³Ø· pytorch Ø±Ø§ Ø´Ø±Ø­ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø«Ø§Ù„ [ÙØµÙ„ Ù‚Ø¨Ù„](/course/chapter2)ØŒ Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Ù…Ø¯Ù„ ØªØ±ØªÛŒØ¨ÛŒ Ø¯Ø± ÛŒÚ© Ø¨ØªÚ† ØªÙˆØ³Ø· pytorch Ø±Ø§ Ø´Ø±Ø­ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

{/if}

Ø§Ù„Ø¨ØªÙ‡ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ú†Ø´Ù…Ú¯ÛŒØ±ÛŒ Ù…Ù†ØªÙ‡ÛŒ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯. Ø¨Ø±Ø§ÛŒ Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ØªØ± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø²Ø±Ú¯ØªØ± Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø¯Ø§Ø´Øª.

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…Ø§ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ MRPC (Microsoft Research Paraphrase Corpus) Ú©Ù‡ Ø¯Ø± ÛŒÚ© [Ù…Ù‚Ø§Ù„Ù‡](https://www.aclweb.org/anthology/I05-5002.pdf)ØŒ Ù†ÙˆØ´ØªÙ‡â€ŒÛŒ William B. Dolan Ùˆ Chris Brockett.ØŒ Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ Ø´Ø§Ù…Ù„ ÛµØŒÛ¸Û°Û± Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ùˆ ÛŒÚ© Ø¨Ø±Ú†Ø³Ø¨ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¨Ø±Ú†Ø³Ø¨ Ù†Ø´Ø§Ù†Ø¯Ù‡Ù†Ø¯Ù‡ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨ÙˆØ¯Ù† Ø¬Ù…Ù„Ø§Øª (Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„ØŒ Ø¢ÛŒØ§ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ù…Ø¹Ù†ÛŒ ÛŒÚ©Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø±Ù†Ø¯ ÛŒØ§ Ø®ÛŒØ±) Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø¹Ù„Øª Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ú©ÙˆÚ†Ú©ÛŒØ³Øª Ùˆ ØªØ¬Ø±Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù† Ø±ÙˆÛŒ Ø¢Ù† Ø¢Ø³Ø§Ù† Ø§Ø³Øª.

Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.

In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a [paper](https://www.aclweb.org/anthology/I05-5002.pdf) by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We've selected it for this chapter because it's a small dataset, so it's easy to experiment with training on it.


### Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒÚ© Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ù‡Ø§Ø¨

### Loading a dataset from the Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Ù‡Ø§Ø¨ ØªÙ†Ù‡Ø§ Ø´Ø§Ù…Ù„ Ù…Ø¯Ù„Ù‡Ø§ Ù†Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯Ø› Ø¨Ù„Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯ Ø¯Ø± Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø²Ø¨Ø§Ù†Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ø§ÛŒÙ† [Ù„ÛŒÙ†Ú©](https://huggingface.co/datasets) Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒØ¯ Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù¾Ø³ Ø§Ø² Ø§ØªÙ…Ø§Ù… Ø§ÛŒÙ† Ø¨Ø®Ø´ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†ÛŒØ¯ (Ø¨Ø®Ø´ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø±Ø§ Ø¯Ø± [Ø§ÛŒÙ†Ø¬Ø§](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub) Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒØ¯). Ø§Ù…Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯Ù‡ÛŒØ¯ Ø§Ú©Ù†ÙˆÙ† Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ MRPC ØªÙ…Ø±Ú©Ø² Ú©Ù†ÛŒÙ…! Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Û±Û° Ø¯Ø§Ø¯Ù‡ [GLUE benchmark](https://gluebenchmark.com/) Ø§Ø³Øª Ú©Ù‡ ÛŒÚ© Ù…Ø­Ú© ØªÙ‡ÛŒÙ‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù…Ø­ÛŒØ· Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ÛŒ Ø¬Ù‡Øª Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú¯ÛŒØ±ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯ Ù…Ø¯Ù„Ù‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†ÛŒ Ø¯Ø± Û±Û° Ù…Ø³Ø¦Ù„Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ† Ù…Ø®ØªÙ„Ù Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.

The Hub doesn't just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets [here](https://huggingface.co/datasets), and we recommend you try to load and process a new dataset once you have gone through this section (see the general documentation [here](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). But for now, let's focus on the MRPC dataset! This is one of the 10 datasets composing the [GLUE benchmark](https://gluebenchmark.com/), which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.

Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ğŸ¤— ÛŒÚ© Ø¯Ø³ØªÙˆØ± Ø¨Ø³ÛŒØ§Ø± Ø³Ø§Ø¯Ù‡ Ø¬Ù‡Øª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø²ÛŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù‡Ø§Ø¨ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ MRPC Ø±Ø§ Ø¨Ù‡ Ø±ÙˆØ´ Ø²ÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒÙ…:

The ğŸ¤— Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ù…ÛŒ Ø¨ÛŒÙ†ÛŒØ¯ ÛŒÚ© Ø´ÛŒØ¡ "DatasetDict" Ø¨Ø¯Ø³Øª Ù…ÛŒâ€ŒØ¢ÙˆØ±ÛŒÙ… Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ trainingØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ validationØŒ Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡ test Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ù‡Ø±ÛŒÚ© Ø§Ø² Ø§ÛŒÙ†Ù‡Ø§ Ø´Ø§Ù…Ù„ Ú†Ù†Ø¯ÛŒÙ† Ø³Ø·ÙˆÙ† (`label`ØŒ `sentence2`ØŒ `sentence1`ØŒ Ùˆ `idx`) Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ù…ØªØºÛŒØ±ÛŒ Ø±Ø¯ÛŒÙ Ø¯Ø§Ø¯Ù‡ Ú©Ù‡ Ø¹Ù†Ø§ØµØ± Ù‡Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø±Ø§ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. (Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Û³ØŒÛ¶Û¶Û¸ Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ training ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Û´Û°Û¸ ØªØ§ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ validationØŒ Ùˆ Û±ØŒÛ·Û²Ûµ ØªØ§ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ test).

As you can see, we get a `DatasetDict` object which contains the training set, the validation set, and the test set. Each of those contains several columns (`sentence1`, `sentence2`, `label`, and `idx`) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set).

 Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´ ÙØ±Ø¶ Ø¯Ø± Ø²ÛŒØ±Ø´Ø§Ø®Ù‡â€ŒÛŒ *~/.cache/huggingface/dataset* Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒÚ©Ù†Ø¯. Ø§Ø² ÙØµÙ„ Û² Ø¨Ù‡ ÛŒØ§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø²ÛŒØ±Ø´Ø§Ø®Ù‡â€ŒÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒØªØ§Ù† Ø±Ø§ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ `HF_HOME` Ø¨Ù‡ Ø¯Ù„Ø®ÙˆØ§Ù‡ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯.

This command downloads and caches the dataset, by default in *~/.cache/huggingface/dataset*. Recall from Chapter 2 that you can customize your cache folder by setting the `HF_HOME` environment variable.

Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ù‡ Ù‡Ø± Ø¬ÙØª Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ø¯Ø± Ø´Ø¦ `raw_datasets` Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ, Ù…Ø§Ù†Ù†Ø¯ ÛŒÚ© dictionary Ø¯Ø³ØªØ±Ø³ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒÙ…: 

We can access each pair of sentences in our `raw_datasets` object by indexing, like with a dictionary:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù… Ú©Ù‡ Ø¨Ø±Ú†Ø³Ø¨Ù‡Ø§ Ø§Ø² Ù¾ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ± ØµØ­ÛŒØ­ Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ù…Ø¬Ø¨ÙˆØ± Ù†ÛŒØ³ØªÛŒÙ… Ù‡ÛŒÚ† Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ÛŒ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø¯Ø§Ù†ÛŒÙ… Ú©Ø¯Ø§Ù… Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ ØµØ­ÛŒØ­ Ø¨Ù‡ Ú©Ø¯Ø§Ù… Ø¨Ø±Ú†Ø³Ø¨ Ù…Ø±Ø¨ÙˆØ· Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ØªØ§Ø¨Ø¹ `features` Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø§Ù† `raw_train_dataset`â€Œ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†ÙˆØ¹ Ù‡Ø± Ø³Ø·ÙˆÙ† Ø±Ø§ Ø¨Ù‡ Ù…Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ú¯ÙØª.

We can see the labels are already integers, so we won't have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the `features` of our `raw_train_dataset`. This will tell us the type of each column:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

Ø¯Ø± Ù¾Ø´Øª ØµØ­Ù†Ù‡ØŒ `Ø¨Ø±Ú†Ø³Ø¨` Ø§Ø² Ù†ÙˆØ¹ `Ø¨Ø±Ú†Ø³Ø¨ Ú©Ù„Ø§Ø³` Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ùˆ Ù†Ú¯Ø§Ø´Øª Ø§Ø¹Ø¯Ø§Ø¯ ØµØ­ÛŒØ­ Ø¨Ù‡ Ù†Ø§Ù… Ø¨Ø±Ú†Ø³Ø¨ Ø¯Ø± Ù¾ÙˆØ´Ù‡â€ŒÙŠ *names* Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. `0` Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ `not_equivalent`ØŒ Ùˆ `1` Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ `equivalent` Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ.

Behind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name is stored in the *names* folder. `0` corresponds to `not_equivalent`, and `1` corresponds to `equivalent`.

<Tip>

âœï¸ **Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯!** Ø¹Ù†ØµØ± Ø´Ù…Ø§Ø±Ù‡ Û±Ûµ Ø§Ø² Ø¯Ø§Ø¯Ù‡ training Ùˆ Ø¹Ù†ØµØ± Ø´Ù…Ø§Ø±Ù‡ Û¸Û· Ø§Ø² Ø¯Ø§Ø¯Ù‡ validation Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ú†Ø³Ø¨Ù‡Ø§ÛŒ Ø¢Ù†Ù‡Ø§ Ú†ÛŒØ³ØªØŸ

</Tip>

<Tip>

âœï¸ **Try it out!** Look at element 15 of the training set and element 87 of the validation set. What are their labels?

</Tip>

### Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´Ù ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡

### Preprocessing a dataset

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ù¾ÛŒØ´ Ø¨Ø±Ø¯Ø§Ø²Ø´ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ÛŒ Ù‚Ø§Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ…. Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯Ø±[ÙØµÙ„ Ù‚Ø¨Ù„](/course/chapter2) Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÛŒÚ© ÛŒØ§ Ú†Ù†Ø¯ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø¯Ù‡ÛŒÙ…ØŒ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù…Ø³ØªÙ‚ÛŒÙ…Ø§ ØªÙ…Ø§Ù… Ø¬Ù…Ù„Ø§Øª Ø§ÙˆÙ„ Ùˆ Ø¯ÙˆÙ… Ù‡Ø± Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†ÛŒÙ…:

To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the [previous chapter](/course/chapter2), this is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ù…Ø¯Ù„ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒÙ… ØªØ§ Ù¾ÛŒØ´Ø¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯ Ù…ØªÙ†Ø§Ø¸Ø± Ù‡Ø³ØªÙ†Ø¯ ÛŒØ§ Ø®ÛŒØ±. Ù…Ø§ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ Ø¯Ùˆ Ø±Ø´ØªÙ‡ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ø¬ÙØª Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ù†ÛŒÙ…ØŒ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ø¨Ù‡ Ø¢Ù† Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†ÛŒÙ…. Ø®ÙˆØ´Ø¨Ø®ØªØ§Ù†Ù‡ØŒ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÛŒÚ© Ø¬ÙØª Ø±Ø´ØªÙ‡ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯ Ùˆ Ø¢Ù†Ø±Ø§ Ø¨Ù‡ Ú¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ BERT Ù…Ø§ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†Ø¯:

However, we can't just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects: 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Ø¯Ø± [ÙØµÙ„ Û²](/course/chapter2) Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ `input_ids` Ùˆ `attention_mask` Ø¨Ø­Ø« Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø§Ù…Ø§ Ø§Ø² Ú¯ÙØªÚ¯Ùˆ Ø¯Ø± Ù…ÙˆØ±Ø¯ `token_type_ids` Ø§Ø¬ØªÙ†Ø§Ø¨ Ú©Ø±Ø¯ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† Ù…Ø«Ø§Ù„ Ø§ÛŒÙ† Ù‡Ù…Ø§Ù† Ú†ÛŒØ²ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ùˆ Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ Ø¬Ù…Ù„Ù‡ Ø¯ÙˆÙ… Ø§Ø³Øª.

We discussed the `input_ids` and `attention_mask` keys in [Chapter 2](/course/chapter2), but we put off talking about `token_type_ids`. In this example, this is what tells the model which part of the input is the first sentence and which is the second sentence.

<Tip>

âœï¸ **Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯!** Ø¹Ù†ØµØ± Ø´Ù…Ø§Ø±Ù‡ Û±Ûµ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯ Ùˆ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ùˆ Ø¬ÙØª ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†ÛŒØ¯. ØªÙØ§ÙˆØª Ø¯Ùˆ Ù†ØªÛŒØ¬Ù‡ Ú†ÛŒØ³ØªØŸ

âœï¸ **Try it out!** Take element 15 of the training set and tokenize the two sentences separately and as a pair. What's the difference between the two results?

</Tip>

Ø§Ú¯Ø± Ø¢ÛŒØ¯ÛŒ Ù‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ `input_ids` Ø±Ø§ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ú©Ù†ÛŒÙ…:

If we decode the IDs inside `input_ids` back to words:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª:

we will get:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒÙ… Ú©Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±Ø¯ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø¯Ø§Ø±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª `[CLS] sentence1 [SEP] sentence2 [SEP]` Ø¨Ø§Ø´Ù†Ø¯.

So we see the model expects the inputs to be of the form `[CLS] sentence1 [SEP] sentence2 [SEP]` when there are two sentences. Aligning this with the `token_type_ids` gives us:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒØ¯ØŒ Ø¨Ø®Ø´Ù‡Ø§ÛŒÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒ Ú©Ù‡ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ `[CLS] sentence1 [SEP]` Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ Ù‡Ù…Ú¯ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø¢ÛŒØ¯ÛŒ Ù†ÙˆØ¹ ØªÙˆÚ©Ù† `0` Ùˆ Ø¨Ø®Ø´Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ `sentence2 [SEP]` Ù‡Ø³ØªÙ†Ø¯ Ù‡Ù…Ú¯ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø¢ÛŒØ¯ÛŒ Ù†ÙˆØ¹ ØªÙˆÚ©Ù† `1` Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯.

As you can see, the parts of the input corresponding to `[CLS] sentence1 [SEP]` all have a token type ID of `0`, while the other parts, corresponding to `sentence2 [SEP]`, all have a token type ID of `1`.

ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ø§Ú¯Ø± Ú†Ú©Ù¾ÙˆÛŒÙ†Øª Ø¯ÛŒÚ¯Ø±ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ØŒ Ø¯Ø± ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ù„Ø²ÙˆÙ…Ø§ `token_type_ids` Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø¯Ø§Ø´Øª (Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„ØŒ Ø§Ú¯Ø± Ø§Ø² ÛŒÚ© DistilBERT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ø¢Ù†Ù‡Ø§ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯). Ø¢Ù†Ù‡Ø§ ÙÙ‚Ø· Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ú©Ù‡ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø¯ Ø¨Ø§ Ø¢Ù†Ù‡Ø§ Ú†Ú©Ø§Ø± Ú©Ù†Ø¯ØŒ Ø¨Ù‡ Ø§ÛŒÙ† Ø®Ø§Ø·Ø± Ú©Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¯Ø± Ø²Ù…Ø§Ù† Ù¾ÛŒØ´â€ŒØªØ¹Ù„ÛŒÙ… Ø¯ÛŒØ¯Ù‡ Ø§Ø³Øª.

Note that if you select a different checkpoint, you won't necessarily have the `token_type_ids` in your tokenized inputs (for instance, they're not returned if you use a DistilBERT model). They are only returned when the model will know what to do with them, because it has seen them during its pretraining. 

Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ØŒ BERT Ø¨Ø§ Ø¢ÛŒØ¯ÛŒ Ù‡Ø§ÛŒ Ù†ÙˆØ¹ ØªÙˆÚ©Ù† Ù¾ÛŒØ´â€ŒØªØ¹Ù„ÛŒÙ… Ø´Ø¯Ù‡ØŒ Ùˆ Ø¯Ø± Ø¨Ø§Ù„Ø§ÛŒ Ù‡Ø¯Ù Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ù…Ø§Ø³Ú©ÛŒ Ú©Ù‡ Ø¯Ø± ÙØµÙ„ --- Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¢Ù† ØµØ­Ø¨Øª Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ ÛŒÚ© ÙˆØ¸ÛŒÙÙ‡ Ø¯ÛŒÚ¯Ø± ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† _next sentence prediction_ Ø¯Ø§Ø±Ø¯. Ù‡Ø¯Ù Ø§Ø² Ø§ÛŒÙ† ÙˆØ¸ÛŒÙÙ‡ Ù…Ø¯Ù„ Ú©Ø±Ø¯Ù† Ø±Ø§Ø¨Ø·Ù‡ Ø¨ÛŒÙ† Ø¬ÙØª Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.

Here, BERT is pretrained with token type IDs, and on top of the masked language modeling objective we talked about in [Chapter 1](/course/chapter1), it has an additional objective called _next sentence prediction_. The goal with this task is to model the relationship between pairs of sentences.

 Ø¯Ø± Ø±ÙˆØ´ Ù¾ÛŒØ´â€Œ Ø¨ÛŒÙ†ÛŒ Ø¬Ù…Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒØŒ Ø¬ÙØª Ø¬Ù…Ù„Ø§ØªÛŒ (Ø¨Ø§ Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¨Ù‡ Ø·ÙˆØ± ØªØµØ§Ø¯ÙÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯) Ø¨Ù‡ Ù…Ø¯Ù„ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ø² Ø¢Ù† Ø®ÙˆØ§Ø³ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯ Ø¢ÛŒØ§ Ø¬Ù…Ù„Ù‡ Ø¯ÙˆÙ… Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ø®ÛŒØ±.Ø¨Ø±Ø§ÛŒ Ø®Ø§Ø±Ø¬ Ú©Ø±Ø¯Ù† ÙˆØ¸ÛŒÙÙ‡ Ø§Ø² Ø­Ø§Ù„Øª Ø¨Ø¯ÛŒÙ‡ÛŒØŒ Ø¯Ø± Ù†ÛŒÙ…ÛŒ Ø§Ø² Ø­Ø§Ù„ØªÙ‡Ø§ Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù‡Ù… Ø¢Ù…Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ùˆ Ø¯Ø± Ù†ÛŒÙ…ÛŒ Ø¯ÛŒÚ¯Ø± Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ø§Ø² Ø¯Ùˆ Ù…ØªÙ† Ù…ØªÙØ§ÙˆØª Ù…ÛŒâ€ŒØ¢ÛŒÙ†Ø¯.

With next sentence prediction, the model is provided pairs of sentences (with randomly masked tokens) and asked to predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents. 

Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ØŒ Ù†ÛŒØ§Ø²ÛŒ Ù†ÛŒØ³Øª Ù†Ú¯Ø±Ø§Ù† ÙˆØ¬ÙˆØ¯ ÛŒØ§ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ `token_type_ids` Ø¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ø§ÛŒ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ø´Ø¯Ù‡ Ø®ÙˆØ¯ Ø¨Ø§Ø´ÛŒØ¯: Ù…Ø§Ø¯Ø§Ù…ÛŒ Ú©Ù‡ Ø§Ø² Ú†Ú©Ù¾ÙˆÛŒÙ†Øª ÛŒÚ©Ø³Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ØŒ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø®ÙˆØ¨ Ù¾ÛŒØ´ Ø®ÙˆØ§Ù‡Ø¯ Ø±ÙØª Ú†Ø±Ø§Ú©Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…ÛŒØ¯Ø§Ù†Ø¯ Ú†Ù‡ Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ØªÙ‡ÛŒÙ‡ Ú©Ù†Ø¯.

In general, you don't need to worry about whether or not there are `token_type_ids` in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.

Ø§Ú©Ù†ÙˆÙ† Ú©Ù‡ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ… Ú†Ú¯ÙˆÙ†Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ ÛŒÚ© Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ù†Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¢Ù†Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø§Ù† Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø¨Ø±ÛŒÙ…: Ù…Ø§Ù†Ù†Ø¯ [ÙØµÙ„ Ù‚Ø¨Ù„](/course/chapter2)ØŒ Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù†ÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ø§ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¬ÙØª Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ØŒ Ø¨Ø§ Ø¯Ø§Ø¯Ù† Ù„ÛŒØ³Øª Ø¬Ù…Ù„Ø§Øª Ø§ÙˆÙ„ Ùˆ Ø³Ù¾Ø³ Ù„ÛŒØ³Øª Ø¬Ù…Ù„Ø§Øª Ø¯ÙˆÙ…ØŒ ØªØºØ°ÛŒÙ‡ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø§ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ padding Ùˆ truncation Ú©Ù‡ Ø¯Ø± [ÙØµÙ„ Û²](/course/chapter2) Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ… Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¯Ø§Ø±Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ ÛŒÚ© Ø±Ø§Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´Ù Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ training Ø§ÛŒÙ†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯:

Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the [previous chapter](/course/chapter2), we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in [Chapter 2](/course/chapter2). So, one way to preprocess the training dataset is:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```
Ø§ÛŒÙ† Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ø¯ØŒ Ø§Ù…Ø§ Ù…Ø´Ú©Ù„Ø´ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ÛŒÚ© dictionary Ø¨Ø±Ù…ÛŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ (Ø´Ø§Ù…Ù„ Ú©Ù„ÛŒØ¯Ù‡Ø§ØŒ `input_ids`, `attention_mask`, Ùˆ `token_type_ids`, Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ±ÛŒ Ú©Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù‡Ø§ Ù‡Ø³ØªÙ†Ø¯). Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§ÛŒÙ† Ø±ÙˆØ´ ÙÙ‚Ø· Ø²Ù…Ø§Ù†ÛŒ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ø¯ Ú©Ù‡ Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆÙ‚Øª Ú©Ø§ÙÛŒ Ø¬Ù‡Øª Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ø­ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ (Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€Œâ€ŒÛŒ ğŸ¤— ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ù†ÙˆØ¹ [Apache Arrow](https://arrow.apache.org/) Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø±ÙˆÛŒ Ø¯ÛŒØ³Ú© Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ù†Ø¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø´Ù…Ø§ ÙÙ‚Ø· Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ú©Ù‡ Ø¬Ù‡Øª Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø²ÛŒ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø±Ø¯ÛŒØ¯ Ù†Ú¯Ù‡ Ù…ÛŒØ¯Ø§Ø±ÛŒØ¯. )

This works well, but it has the disadvantage of returning a dictionary (with our keys, `input_ids`, `attention_mask`, and `token_type_ids`, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the ğŸ¤— Datasets library are [Apache Arrow](https://arrow.apache.org/) files stored on the disk, so you only keep the samples you ask for loaded in memory).

Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ù‡ Ù…Ø§ Ø§Ù†Ø¹Ø·Ø§ÙÙ¾Ø°ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú†Ù†Ø§Ù†Ú†Ù‡ Ø¨Ù‡ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…. ØªØ§Ø¨Ø¹ `map()` Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† ÛŒÚ© Ø¹Ù…Ù„ÛŒØ§Øª Ø±ÙˆÛŒ Ù‡Ø± Ø¹Ù†ØµØ± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ù‡ÛŒØ¯ ØªØ§Ø¨Ø¹ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ… Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ø§ Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†Ø¯:

To keep the data as a dataset, we will use the [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The `map()` method works by applying a function on each element of the dataset, so let's define a function that tokenizes our inputs:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ (Ù…Ø«Ù„ Ø§Ù‚Ù„Ø§Ù… Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡) Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒÚ©Ù†Ø¯ Ùˆ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø§Ø²Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ `input_ids`ØŒ `attention_mask`ØŒ Ùˆ `token_type_ids`. ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ø§Ø² Ø§Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø± Ø±ÙˆÛŒ Ù„ÛŒØ³ØªÙ‡Ø§ÛŒÛŒ Ø§Ø² Ø¬ÙØª Ø¬Ù…Ù„Ù‡ Ù‡Ø§ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ø¯ØŒ Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ù‚Ø¨Ù„Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†ÛŒØ² Ø¯Ø±ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ `example` Ø´Ø§Ù…Ù„ Ú†Ù†Ø¯ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ (Ù‡Ø± Ú©Ù„ÛŒØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¬Ù…Ù„Ø§Øª) Ø¨Ø§Ø´Ø¯ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ø¨Ù‡ Ù…Ø§ Ø§ÛŒÙ† Ø§Ù…Ú©Ø§Ù† Ø±Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø¯ Ú©Ù‡ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡ `batched=True` Ø¯Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ `map()` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… Ú©Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ù‡ Ù…ÛŒØ²Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø³Ø±ÛŒØ¹ØªØ± Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯.

This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`. Note that it also works if the `example` dictionary contains several samples (each key as a list of sentences) since the `tokenizer` works on lists of pairs of sentences, as seen before. This will allow us to use the option `batched=True` in our call to `map()`, which will greatly speed up the tokenization. The `tokenizer` is backed by a tokenizer written in Rust from the [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers) library. This tokenizer can be very fast, but only if we give it lots of inputs at once.

ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ù…Ø§ Ù…Ø¨Ø­Ø« `padding` Ø±Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ú©Ù†Ø§Ø± Ú¯Ø°Ø§Ø´ØªÙ‡ Ø§ÛŒÙ…. Ø§ÛŒÙ† Ø¨Ù‡ Ø§ÛŒÙ† Ø®Ø§Ø·Ø± Ø§Ø³Øª Ú©Ù‡ Ø§Ù†Ø¬Ø§Ù… `padding` Ø±ÙˆÛŒ Ù‡Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø·ÙˆÙ„ Ø¨Ù‡ ØµØ±ÙÙ‡ Ù†ÛŒØ³Øª: Ø¨Ù‡ØªØ± Ø§Ø³Øª Ú©Ù‡ `padding` Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®ØªÙ† Ø¨ØªÚ† Ù‡Ø³ØªÛŒÙ… Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ…ØŒ Ú†Ø±Ø§ Ú©Ù‡ Ø¢Ù†ÙˆÙ‚Øª ÙÙ‚Ø· Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ… Ú©Ù‡ `padding` Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ù…Ø§Ù† Ø¨ØªÚ† Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ…ØŒ Ùˆ Ù†Ù‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø·ÙˆÙ„ Ø¯Ø± Ø³Ø±ØªØ§Ø³Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡. Ø§ÛŒÙ† Ø±ÙˆØ´ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ø·ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…ØªÙ‚ÛŒØ±ÛŒ Ù‡Ø³ØªÙ†Ø¯ ÙˆÙ‚Øª Ùˆ Ø§Ù†Ø±Ú˜ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø±Ø§ ØµØ±ÙÙ‡ Ø¬ÙˆÛŒÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯. 

Note that we've left the `padding` argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it's better to pad the samples when we're building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths! 

Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… Ú†Ú¯ÙˆÙ†Ù‡ ØªØ§Ø¨Ø¹ ØªÙˆÚ©Ù†Ø§ÛŒØ²ÛŒØ´Ù† Ø±Ø§ Ø±ÙˆÛŒ Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ ÛŒÚ©Ø¨Ø§Ø±Ù‡ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ù…Ø§ Ø§Ø² `batched=True` Ø¯Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ `map` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¨Ù†Ø§Ø¨Ø± Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø± Ø±ÙˆÛŒ Ú†Ù†Ø¯ÛŒÙ† Ø¹Ù†ØµØ± Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…Ø§ Ø¨Ù‡ ÛŒÚ©Ø¨Ø§Ø±Ù‡ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù†Ù‡ Ø±ÙˆÛŒ Ù‡Ø± Ø¹Ù†ØµØ± Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡. Ø§ÛŒÙ† Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹ØªØ± Ø§Ù†Ø¬Ø§Ù… Ú¯ÛŒØ±Ø¯:

Here is how we apply the tokenization function on all our datasets at once. We're using `batched=True` in our call to `map` so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```
Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ğŸ¤— Ø§ÛŒÙ† Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø¨Ø§ Ø§ÙØ²ÙˆØ¯Ù† -ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ- Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ ÛŒÚ©ÛŒ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ú©Ù„ÛŒØ¯ Ø¯Ø± -Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ- Ú©Ù‡ ØªÙˆØ³Ø· ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§Ø² Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ

The way the ğŸ¤— Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Ø´Ù…Ø§ Ø­ØªÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ø¨Ø§ Ø§Ø±Ø³Ø§Ù„ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† `num_proc` Ø¯Ø± ØªØ§Ø¨Ø¹ `map()` Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ù…Ø§ Ø¢Ù† Ø±Ø§ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯ÛŒÙ… Ø¨Ù‡ Ø§ÛŒÙ† Ø®Ø§Ø·Ø± Ú©Ù‡ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ğŸ¤— Ø§Ø² Ù¾ÛŒØ´ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† Ø±Ø´ØªÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ù†Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø³Ø±ÛŒØ¹ØªØ± Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø§Ù…Ø§ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø§Ø² ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø³Ø±ÛŒØ¹ Ú©Ù‡ Ø¨Ø§ Ø§ÛŒÙ† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒØ´ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ø§ÛŒÙ† Ø±ÙˆØ´ Ù…ÛŒØªÙˆØ§Ù†Ø¯ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ù…Ø§ Ø±Ø§ Ø³Ø±ÛŒØ¹ØªØ± Ú©Ù†Ø¯.

You can even use multiprocessing when applying your preprocessing function with `map()` by passing along a `num_proc` argument. We didn't do this here because the ğŸ¤— Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.

ØªØ§Ø¨Ø¹ `tokenize_function` Ù…Ø§ ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ `input_ids`ØŒ `attention_mask`ØŒ Ùˆ `token_type_ids`ØŒ Ø¨Ø±Ù…ÛŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ø¨Ù‡ Ú¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ø³Ù‡ ÙÛŒÙ„Ø¯ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø¨Ø®Ø´Ù‡Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§ÙØ²ÙˆØ¯Ù‡ Ú¯Ø±Ø¯Ù†Ø¯. ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ø§Ú¯Ø± ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ù„ÛŒØ¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ú©Ù‡ ØªØ§Ø¨Ø¹ `map()` Ø¨Ù‡ Ø¢Ù† Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ø¨Ø§Ø²Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÛŒÙ… ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒÙ….

Our `tokenize_function` returns a dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`, so those three fields are added to all splits of our dataset. Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied `map()`.

Ø¢Ø®Ø±ÛŒÙ† Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ… Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ Ø¹Ù†Ø§ØµØ± Ø±Ø§ Ø¨Ø§Ù‡Ù… Ø¨ÙØªÚ† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ù‡Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø·ÙˆÙ„ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ø¹Ù†ØµØ± Ù¾ÙØ¯ Ú©Ù†ÛŒÙ… - ØªÚ©Ù†ÛŒÚ©ÛŒ Ú©Ù‡ Ù…Ø§ Ø¨Ù‡ Ø¢Ù† *dynamic padding* (Ù‡Ù…Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ù¾ÙˆÛŒØ§) Ù…ÛŒâ€ŒÚ¯ÙˆÛŒÛŒÙ….

The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as *dynamic padding*.

### Ù‡Ù… Ø·ÙˆÙ„ Ú©Ø±Ø¯Ù† Ù¾ÙˆÛŒØ§

### Dynamic padding

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}

ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ù…Ø³Ø¦ÙˆÙ„ Ú©Ù†Ø§Ø±Ù‡Ù… Ú¯Ø°Ø§Ø´ØªÙ† Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ø¨ÙØªÚ† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ *collate function* Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù‡Ù†Ú¯Ø§Ù… Ø³Ø§Ø®ØªÙ† ÛŒÚ© `DataLoader` Ø¨Ù‡ Ø¯Ø§Ø®Ù„ Ø¢Ù† Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯ØŒ Ú©Ù‡ Ø¯Ø± Ø­Ø§Ù„Øª Ù¾ÛŒØ´ ÙØ±Ø¶ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ PyTorch ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡Ù… Ø§Ù„Ø­Ø§Ù‚ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø§Ú¯Ø± Ø¹Ù†Ø§ØµØ± Ø´Ù…Ø§ Ù„ÛŒØ³ØªØŒ ØªØ§Ù¾Ù„ØŒ ÛŒØ§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§Ø´Ù†Ø¯ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯). Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ù…Ø§ Ù‡Ù…Ù‡ Ø¨Ù‡ ÛŒÚ© Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù†ÛŒØ³ØªÙ†Ø¯ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ø§Ù…Ú©Ø§Ù† Ù¾Ø°ÛŒØ± Ù†ÛŒØ³Øª. Ù…Ø§ Ù¾Ø±ÙˆØ³Ù‡ Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø±Ø§ Ø¹Ù…Ø¯Ø§ Ø¨Ù‡ ØªØ¹ÙˆÛŒÙ‚ Ø§Ù†Ø¯Ø§Ø®ØªÛŒÙ…ØŒ ØªØ§ ÙÙ‚Ø· Ø¯Ø± Ø²Ù…Ø§Ù† Ù†ÛŒØ§Ø² Ø¢Ù†Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø± Ø¨ÙØªÚ† Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒÙ… Ùˆ Ø§Ø² Ø¯Ø§Ø´ØªÙ† ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ÛŒ Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ú¯ÛŒØ±ÛŒ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ø±ÙˆØ´ Ù¾Ø±ÙˆØ³Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ ØªØ§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒØ§ÛŒ Ø³Ø±Ø¹Øª Ù…ÛŒâ€ŒØ¨Ø®Ø´Ø¯ØŒ Ø§Ù…Ø§ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ø§Ú¯Ø± Ø´Ù…Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ TPU Ù‡Ø³ØªÛŒØ¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ø´Ú©Ù„ Ø³Ø§Ø² Ø¨Ø§Ø´Ø¯ - TPU Ø§Ø´Ú©Ø§Ù„ Ù…Ø¹ÛŒÙ† Ø±Ø§ ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ØŒ Ø­ØªÛŒ Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

The function that is responsible for putting together samples inside a batch is called a *collate function*. It's an argument you can pass when you build a `DataLoader`, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won't be possible in our case since the inputs we have won't all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding.

{:else}

ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ù…Ø³Ø¦ÙˆÙ„ Ú©Ù†Ø§Ø±Ù‡Ù… Ú¯Ø°Ø§Ø´ØªÙ† Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ø¨ÙØªÚ† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ *collate function* Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. collator Ù¾ÛŒØ´ ÙØ±Ø¶ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø³Øª Ú©Ù‡ ÙÙ‚Ø· Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¨Ù‡ tf.Tensor ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡Ù… Ø§Ù„Ø­Ø§Ù‚ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø§Ú¯Ø± Ø¹Ù†Ø§ØµØ± Ø´Ù…Ø§ Ù„ÛŒØ³ØªØŒ ØªØ§Ù¾Ù„ØŒ ÛŒØ§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§Ø´Ù†Ø¯ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯). Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ù…Ø§ Ù‡Ù…Ù‡ Ø¨Ù‡ ÛŒÚ© Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù†ÛŒØ³ØªÙ†Ø¯ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ø§Ù…Ú©Ø§Ù† Ù¾Ø°ÛŒØ± Ù†ÛŒØ³Øª. Ù…Ø§ Ù¾Ø±ÙˆØ³Ù‡ Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø±Ø§ Ø¹Ù…Ø¯Ø§ Ø¨Ù‡ ØªØ¹ÙˆÛŒÙ‚ Ø§Ù†Ø¯Ø§Ø®ØªÛŒÙ…ØŒ ØªØ§ ÙÙ‚Ø· Ø¯Ø± Ø²Ù…Ø§Ù† Ù†ÛŒØ§Ø² Ø¢Ù†Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø± Ø¨ÙØªÚ† Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒÙ… Ùˆ Ø§Ø² Ø¯Ø§Ø´ØªÙ† ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ÛŒ Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ú¯ÛŒØ±ÛŒ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ø±ÙˆØ´ Ù¾Ø±ÙˆØ³Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ ØªØ§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒØ§ÛŒ Ø³Ø±Ø¹Øª Ù…ÛŒâ€ŒØ¨Ø®Ø´Ø¯ØŒ Ø§Ù…Ø§ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ø§Ú¯Ø± Ø´Ù…Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ TPU Ù‡Ø³ØªÛŒØ¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ø´Ú©Ù„ Ø³Ø§Ø² Ø¨Ø§Ø´Ø¯ - TPU Ø§Ø´Ú©Ø§Ù„ Ù…Ø¹ÛŒÙ† Ø±Ø§ ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ØŒ Ø­ØªÛŒ Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

The function that is responsible for putting together samples inside a batch is called a *collate function*. The default collator is a function that will just convert your samples to tf.Tensor and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won't be possible in our case since the inputs we have won't all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding.

{/if}

Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø§ÛŒÙ†Ú©Ø§Ø± Ø¯Ø± Ø¹Ù…Ù„ØŒ Ù…Ø§ Ø¨Ø§ÛŒØ¯ ÛŒÚ© ØªØ§Ø¨Ø¹ collate ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ… Ú©Ù‡ Ù…ÛŒØ²Ø§Ù† Ø¯Ø±Ø³ØªÛŒ Ø§Ø² Ù‡Ù… Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¢ÛŒØªÙ…Ù‡Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø§Ù‡Ù… Ø¨ÙØªÚ† Ú©Ù†ÛŒÙ… Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†Ø¯. Ø®ÙˆØ´Ø¨Ø®ØªØ§Ù†Ù‡ØŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø±Ù‡Ø§ÛŒ ğŸ¤— Ú†Ù†ÛŒÙ† ØªØ§Ø¨Ø¹ÛŒ Ø±Ø§ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù† `DataCollatorWithPadding` Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ù…Ø§ Ù‚Ø±Ø§Ø± Ù…ÛŒØ¯Ù‡Ø¯. Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ù…Ø­Ø¶ Ø§ÛŒÙ†Ú©Ù‡ Ø¢Ù†Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯ (Ú©Ù‡ Ú†Ù‡ ØªÙˆÚ©Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯ØŒ Ùˆ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ù‡Ù…Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ø§Ø² Ø³Ù…Øª Ú†Ù¾ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ø§Ø² Ø³Ù…Øª Ø±Ø§Ø³Øª Ø¢Ù†Ù‡Ø§) ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø´ØªÙ‡ Ùˆ Ù‡Ø± Ú©Ø§Ø±ÛŒ Ø±Ø§ Ú©Ù‡ Ù„Ø§Ø²Ù… Ø¯Ø§Ø±ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ğŸ¤— Transformers library provides us with such a function via `DataCollatorWithPadding`. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ù‡ÛŒØ¯ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ training Ù…Ø§Ù†ØŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø§Ù‡Ù… Ø¨ÙØªÚ† Ú©Ù†ÛŒÙ…ØŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒÙ… ØªØ§ Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø³Ø·ÙˆÙ†â€ŒÙ‡Ø§ÛŒ -ØŒ --ØŒ Ùˆ --- Ø±Ø§ Ø®Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú†Ø±Ø§Ú©Ù‡ Ø§Ø­ØªÛŒØ§Ø¬ Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯ Ùˆ Ø´Ø§Ù…Ù„ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ (Ùˆ Ù…Ø§ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ØªÙ†Ø³ÙˆØ±Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø±Ø´ØªÙ‡ Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒÙ…) Ùˆ Ø³Ù¾Ø³ Ù†Ú¯Ø§Ù‡ÛŒ Ù…ÛŒâ€ŒØ§Ù†Ø¯Ø§Ø²ÛŒÙ… Ø¨Ù‡ Ø·ÙˆÙ„ Ù‡Ø± ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± Ù‡Ø± Ø¨ÙØªÚ†:

To test this new toy, let's grab a few samples from our training set that we would like to batch together. Here, we remove the columns `idx`, `sentence1`, and `sentence2` as they won't be needed and contain strings (and we can't create tensors with strings) and have a look at the lengths of each entry in the batch:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

ØªØ¹Ø¬Ø¨ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø·ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªØºÛŒÛŒØ±ØŒ Ø§Ø² Û³Û² ØªØ§ Û¶Û· Ø¨Ø¯Ø³Øª Ù…ÛŒâ€ŒØ¢ÙˆØ±ÛŒÙ…. Ù‡Ù…Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ù¾ÙˆÛŒØ§ Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ø¹Ù†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø§ÛŒÙ† Ø¨ÙØªÚ† Ø¨Ø§ÛŒØ¯ Ù‡Ù…Ú¯ÛŒ Ø¨Ø§ Ø·ÙˆÙ„ Û¶Û·ØŒ Ú©Ù‡ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø·ÙˆÙ„ Ø¯Ø§Ø®Ù„ Ø¨ÙØªÚ† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ù‡Ù…Ø·ÙˆÙ„ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¨Ø¯ÙˆÙ† Ù‡Ù…Ø·ÙˆÙ„ Ø³Ø§Ø²ÛŒ Ù¾ÙˆÛŒØ§ØŒ Ù‡Ù…Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø·ÙˆÙ„ ÛŒØ§ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø·ÙˆÙ„Ù Ù‚Ø§Ø¨Ù„ Ù¾Ø°ÛŒØ±Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ØŒ Ù‡Ù…Ø·ÙˆÙ„ Ø´ÙˆÙ†Ø¯. Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ù‡ÛŒØ¯ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ… Ø¢ÛŒØ§ `data_collator` Ù…Ø§ Ø¨ÙØªÚ† Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ù‡Ù…Ø·ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Let's double-check that our `data_collator` is dynamically padding the batch properly:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Ø¨Ù‡ Ù†Ø¸Ø± Ø®ÙˆØ¨ Ù…ÛŒâ€ŒØ¢ÛŒØ¯! Ø§Ú©Ù†ÙˆÙ† Ú©Ù‡ Ø§Ø² Ù…ØªÙ† Ø®Ø§Ù„Øµ Ø¨Ù‡ Ø¨ÙØªÚ†â€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø³ÛŒØ¯Ù‡â€ŒØ§ÛŒÙ… Ú©Ù‡ Ù…Ø¯Ù„ Ù…Ø§Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ø¢Ù†Ù‡Ø§ Ú©Ø§Ø± Ú©Ù†Ø¯ØŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø§Ø²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„:

Looking good! Now that we've gone from raw text to batches our model can deal with, we're ready to fine-tune it!

{/if}

<Tip>

âœï¸ **Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯!** Ù¾Ø±ÙˆØ³Ù‡ Ù¾ÛŒØ´ Ø¨Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ GLUE SST-2 Ø¨Ø§Ø² ØªÚ©Ø±Ø§Ø± Ú©Ù†ÛŒØ¯. Ø§ÛŒÙ† ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ù…ØªÙØ§ÙˆØª Ø§Ø³Øª Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¬ÙØª Ø´Ø§Ù…Ù„ ØªÚ© Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø§ Ø¨Ù‚ÛŒÙ‡ Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ÛŒÙ… Ø¨Ø§ÛŒØ¯ ÛŒÚ©Ø³Ø§Ù† Ø¨Ù‡ Ù†Ø¸Ø± Ø¨Ø±Ø³Ù†Ø¯. Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú†Ø§Ù„Ø´ Ù…Ø´Ú©Ù„â€ŒØªØ±ØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø³Ø¦Ù„Ù‡â€ŒÙ‡Ø§ÛŒ GLUE Ú©Ø§Ø± Ú©Ù†Ø¯.

âœï¸ **Try it out!** Replicate the preprocessing on the GLUE SST-2 dataset. It's a little bit different since it's composed of single sentences instead of pairs, but the rest of what we did should look the same. For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks.

</Tip>

{#if fw === 'tf'}

ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ù…Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€Œ Ø¯Ø§Ø¯Ù‡ Ù…Ø§Ù† Ùˆ ÛŒÚ© collator Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø­Ø§Ù„ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ… Ú©Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡Ù… ÙˆØµÙ„ Ú©Ù†ÛŒÙ…. Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÛŒÙ… Ø¨ÙØªÚ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø³ØªÛŒ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ collate Ú©Ù†ÛŒÙ…ØŒ Ø§Ù…Ø§ Ø§ÛŒÙ† Ú©Ø§Ø± Ø²ÛŒØ§Ø¯ÛŒ Ù…ÛŒâ€ŒØ¨Ø±Ø¯ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ù‡Ù… Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯. Ø¯Ø± Ø¹ÙˆØ¶ØŒ ÛŒÚ© Ø±ÙˆØ´ Ø³Ø§Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø±Ø§Ù‡ Ø­Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø³Ø¦Ù„Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒÚ©Ù†Ø¯: `to_tf_dataset()`. Ø§ÛŒÙ† Ø±ÙˆØ´ ÛŒÚ© `tf.data.Dataset` Ø±Ø§ Ø¯ÙˆØ± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒØªØ§Ù† Ù…ÛŒâ€ŒÙ¾ÛŒÚ†Ø¯ØŒ Ø¨Ø§ ÛŒÚ© ØªØ§Ø¨Ø¹ collation Ø¯Ù„Ø®ÙˆØ§Ù‡. `tf.data.Dataset` ÛŒÚ© ÙØ±Ù…Øª Ø¨ÙˆÙ…ÛŒ ØªÙ†Ø³ÙˆØ±ÙÙ„Ùˆ Ø§Ø³Øª Ú©Ù‡ Ú©ÙØ±Ø§Ø³ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø±Ø§ÛŒ `tf.data.Dataset` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯ØŒ Ø¯Ø± ØªÙ†ÛŒØ¬Ù‡ Ø§ÛŒÙ† ÛŒÚ© ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ø³Ø±Ø¹Øª ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ğŸ¤— Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ù‡ÛŒØ¯ Ø¢Ù†Ø±Ø§ Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ…Ø§Ù† Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒÙ…!

Now that we have our dataset and a data collator, we need to put them together. We could manually load batches and collate them, but that's a lot of work, and probably not very performant either. Instead, there's a simple method that offers a performant solution to this problem: `to_tf_dataset()`. This will wrap a `tf.data.Dataset` around your dataset, with an optional collation function. `tf.data.Dataset` is a native TensorFlow format that Keras can use for `model.fit()`, so this one method immediately converts a ğŸ¤— Dataset to a format that's ready for training. Let's see it in action with our dataset!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

Ø§ÛŒÙ† Ù‡Ù… Ø§Ø² Ø§ÛŒÙ†! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¢Ù† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø¨Ø±ÛŒÙ…ØŒ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ù¾Ø³ Ø§Ø² Ù‡Ù…Ù‡ Ø³Ø®ØªÛŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆØ³Ù‡ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ù‡ Ø·Ø±Ø² Ø®ÙˆØ´Ø§ÛŒÙ†Ø¯ÛŒ Ø³Ø±Ø±Ø§Ø³Øª Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.

And that's it! We can take those datasets forward into the next lecture, where training will be pleasantly straightforward after all the hard work of data preprocessing.

{/if}
