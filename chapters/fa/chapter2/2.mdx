<FrameworkSwitchCourse {fw} />

<div dir="rtl">
# پشت صحنه خط‌تولید

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
این اولین بخشی است که محتوای آن بسته به اینکه از پایتورچ یا تِنسورفِلو استفاده می کنید کمی متفاوت است. از سویچ بالای صفحه برای انتخاب پلتفرمی که ترجیح می دهید استفاده کنید!
</Tip>


{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

بگذارید با یک مثال کامل شروع کنیم که در آن نگاهی می‌اندازیم به آنچه که در پشت صحنه اتفاق می افتد هنگامی که این کد را در [Chapter 1](/course/chapter1) اجرا کردیم:

<div dir="ltr">

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

</div>

این خروجی را دریافت کردیم:

<div dir="ltr">

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

</div>


همان طور که در در فصل اول دیدیم، این خط تولید از سه مرحله تشکیل شده است: پیش‌پردازش، پردازش ورودی‌ها در مدل و پس پردازش.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
</div>

به صورت خلاصه هرکدام از این مراحل را بررسی می‌کنیم.

## پیش‌پردازش با توکِنایزر

مثل شبکه‌های عصبی دیگر، مدل‌های ترنسفورمر هم نمی‌توانند نوشته خام را پردازش کنند. پس اولین قدم در خط تولید ما تبدیل نوشته خام ورودی به اعدادی است که مدل می‌فهمد. برای این کار از یک *توکِنایزر* استفاده می‌کنیم، که مسئولیت‌های زیر را بر عهده دارد:

- شکستن نوشته به کلمات، قطعات کوچکتر کلمه و علامت‌ها(مانند علائم نگارشی) که به آنها ‌*توکِن* می‌گوییم.
- انتخاب یک عدد صحیح معادل برای هر توکِن.
- اضافه‌کردن ورودی‌های دیگری که ممکن است برای مدل مفید باشند.

همه مراحل این پیش‌پردازش باید دقیقا همان‌طور انجام شوند که قبلا هنگام تعلیم مدل انجام شده است. این اطلاعات در [Model Hub](https://huggingface.co/models) موجود است و توسط تابع <span dir="ltr">`from_pretrained()`</span> از کلاس `AutoTokenizer` دانلود می‌شود. با استفاده از نام کامل مدل که شامل نقطه تعلیم است، این تابع  به صورت خودکار داده‌های توکِنایزر مدل را دریافت نموده و در سیستم شما ذخیره می‌کند. به این ترتیب این داده‌ها فقط بار اولی که کد زیر را اجرا می‌کنید دانلود می‌شوند.

نقطه تعلیم پیش‌فرض برای خط‌تولید `تحلیل احساسات` `distilbert-base-uncased-finetuned-sst-2-english` نام دارد. صفحه توضیحات این مدل را می توانید در [اینجا مشاهده کنید](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). با اجرای کد زیر آن را دانلود می کنیم:




<div dir="ltr">

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

</div>

پس از دریافت توکِنایزر، می توانیم جملات خود را مستقیماً وارد آن کنیم و دیکشنری خروجی را دریافت کنیم که آماده است تا به عنوان ورودی مدل مورد استفاده قرار گیرد! تنها کار باقی مانده تبدیل لیست ID‌های ورودی به تِنسور است. شما می‌توانید از ترنسفورمرهای هاگینگ‌فِیس بدون اطلاع از اینکه کدام فریمورک یادگیری ماشین در پشت صحنه درگیر می‌شود استفاده کنید. ممکن است از پایتورچ، تِنسورفِلو یا حتی فلَکس برای بعضی مدل‌ها استفاده شده باشد. با این وجود مدل‌های ترسفورمر فقط *تِنسور*‌ها را به عنوان ورودی قبول می‌کنند. اگر این بار اولی است که با کلمه تِنسور برخورد دارید، می‌توانید آن‌ها را به عنوان آرایه‌های NumPy تصور کنید. این آرایه‌ها می توانند عددی(تک بعدی)، برداری(یک بعدی)، ماتریس(دو بعدی) یا با ابعاد بیشتر باشند. آن‌ها در واقع تِنسور هستند و تِنسورها در فریمورک‌های یادگیری ماشین رفتاری شبیه به آرایه‌های NumPy دارند و به همان سادگی هم ساخته می‌شوند.

برای مشخص کردن نوع تِنسوری که می‌خواهیم به عنوان خروجی دریافت کنیم(پایتورچ، تِنسورفِلو یا NumPy ساده)، از آرگومان `return_tensors` استفاده می‌کنیم:

<div dir="ltr">

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

</div>


هنوز لازم نیست نگران آرگومان‌های `padding` و `truncation` باشید؛ زیرا بعدتر به آنها خواهیم پرداخت. مسئله اصلی که باید به به خاطر بسپارید، امکان دادن یک جمله یا آرایه‌ای از جمله‌ها به عنوان ورودی و مشخص کردن نوع تِنسورهای خروجی است. اگر نوع خروجی را مشخص نکنید، لیستی از لیست‌ها را دریافت خواهید کرد.

{#if fw === 'pt'}

خروجی تِنسورهای پایتورچ به این شکل است:

<div dir="ltr">

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]),
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

</div>

{:else}

خروجی تِنسورهای تِنسورفِلو به این شکل است:

<div dir="ltr">

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>,
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```

</div>

{/if}


خروجی خود یک دیکشنری با دو کلید `input_ids` و `attention_mask` است. `input_ids` دو ردیف عدد صحیح(یک ردیف برای هر جمله) که شناسه‌های منحصر به فرد توکِن‌های هر جمله هستند. `attention_mask` را بعدتر در همین فصل توضیح خواهیم داد.

## گذر از مدل

{#if fw === 'pt'}

می توانیم مدل از پیش تعلیم دیده را، همانند آن چه در مورد توکِنایزر انجام شد، دانلود کنیم. ترنسوفورمرهای هاگینگ‌فِیس کلاس `AutoModel` را ارا‌ئه می‌دهد که تابعی به نام <span dir="ltr">`from_pretrained()`</span> هم دارد:

<div dir="ltr">

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

</div>

{:else}

می توانیم مدل از پیش تعلیم دیده را، همانند آن چه در مورد توکِنایزر انجام شد، دانلود کنیم. ترنسوفورمرهای هاگینگ‌فِیس کلاس `TFAutoModel` را ارا‌ئه می‌دهد که تابعی به نام <span dir="ltr">`from_pretrained()`</span> هم دارد:

<div dir="ltr">

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```

</div>

{/if}

در این قطعه کد، همان نقطه تعلیمی که در خط تولیدمان قبلا استفاده کردیم را دانلود کرده(که البته باید الان دانلود شده باشد و در سیستم شما موجود است پس نیازی به دانلود مجدد ندارد) و مدلی جدید با آن می‌سازیم.

این معماری تنها شامل ماژول پایهٔ `Transformer` است: با دریافت ورودی،‌ *وضعیت پنهان* را در خروجی تحویل می‌دهد. به این وضعیت‌های پنهان *فیچر* هم می‌گوییم. برای هر ورودی مدل، برداری چندین ‌بعدی که معادل *درک کلی مدل ترنسفورمر از آن ورودی* است را دریافت می‌کنیم.

نگران نباشید اگر درک این مفاهیم سخت است. همه آنها را بعدتر توضیح خواهیم داد.

اگرچه وضعیت‌های پنهان به خودی خود می توانند مفید باشند، آن‌ها معمولا ورودی بخش دیگری از مدل که به آن *سر* مدل می گوییم هستند. در [فصل اول](/course/chapter1)، همه مسائل مختلف را می‌توانستیم توسط یک معماری حل کنیم، و سپس خروجی را به سر متفاوتی در ادامه مدل پاس بدهیم.

### بردار‌های چند بعدی؟

خروجی ماژول `Transformer` تِنسوری است که معمولا سه بعد دارد:

- **اندازه بتج**: تعداد رشته‌های مورد پردازش در یک دسته، که در مثال ما دو عدد است.
- **طول رشته**: طول بردار عددی معادل رشته‌ها، که در مثال ما ۱۶ است.
- **اندازه پنهان**: ابعاد بردار برای هر ورودی مدل.

 به خاطر همین مقدار آخر به این تِنسور «چندین بعدی» می گوییم. اندازه پنهان می‌تواند بسیار بزرگ باشد(معمولا ۷۶۸ برای مدل‌های کوچکتر، و در مدل‌های بزرگتر این عدد به ۳۰۷۲ یا بیشتر هم می رسد)

این را با پاس دادن ورودی‌های پیش‌پردازش شده به مدل خود می توانیم ببینیم:

<div dir="ltr">


{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}


</div>


توجه کنید که خروجی‌های ترنسفورمرهای هاگینگ‌فِیس، مانند `namedtuple`‌ها یا دیکشنری‌ها هستند. شما می‌توانید به هر عضو، با استفاده از نامش(مانند آنچه ما انجام دادیم) یا با کلیدش(`outputs["last_hidden_state"]`) یا حتی اگر دقیقاً از مکان آن اطلاع دارید با شماره‌اش(`outputs[0]`) دسترسی پیدا کنید.

### سر مدل: درک اعداد درون مدل

قسمت سر، بردارهای چندین بعدی وضعیت پنهان را به عنوان ورودی می‌پذیرد و آن‌ها را به ابعادی در فضایی دیگر برای بخش بعدی پروجکت می‌کند. سرها معمولا از یک یا چند لایه خطی تشکیل شده‌اند.


<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

خروجی مدل ترنسفورمر، مستقیماً به سر بخش بعدی پاس داده می‌شود. در این نمودار، مدل ترنسفورمر به لایه embeddings و لایه‌های بعدی آن تقسیم شده است. لایه embeddings هر شناسه ورودی در ورودی توکِنیزه شده را به یک بردار که نماینده آن توکِن است تبدیل می‌کند. لایه‌های بعدی با دستکاری در این بردار‌ها توسط فرآیند اتِنشِن، شکل پایانی بردار نماینده جملات را تولید می‌کنند.

تعداد زیادی از معماری‌‌های مختلف در ترنفورمر‌های هاگینگ‌فِیس وجود دارد که هرکدام برای حل یک مسئله خاص طراحی شده‌اند. در این‌جا فهرست کوتاهی از‌ آنها را آورده‌ایم:

- `*Model` (برای دسترسی به وضعیت‌های پنهان)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- و نمونه‌های دیگر در ‌هاگینگ‌فِیس

{#if fw === 'pt'}
برای این مثال، نیازمند مدلی با سر مخصوص دسته‌بندی رشته‌ها(برای تشخیص منفی یا مثبت بودن جملات) هستیم. پس به جای کلاس `AutoModel` از کلاس `AutoModelForSequenceClassification` استفاده می‌کنیم:

<div dir="ltr">

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

</div>

{:else}
برای این مثال، نیازمند مدلی با سر مخصوص دسته‌بندی رشته‌ها(برای تشخیص منفی یا مثبت بودن جملات) هستیم. پس به جای کلاس `TFAutoModel` از کلاس `TFAutoModelForSequenceClassification` استفاده می‌کنیم:

<div dir="ltr">

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```

</div>

{/if}


حالا اگر نگاهی به ویژگی shape ورودی‌ها بیاندازیم، خوهیم دید که تعداد ابعاد بردارهای نماینده بسیار کمتر است: قسمت سر مدل بردارهای چندین بعدی که قبلا دیدیم را به عنوان ورودی دریافت کرده، و به عنوان خروجی بردارهایی با دو عضو(یکی برای هر دسته در عملیات دستبندی) تولید کرده است.

<div dir="ltr">


```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```

{/if}

</div>

از آنجا که ما تنها دو جمله و دو دسته ممکن داشتیم، خروجی مدل ما شکل ۲ در ۲ دارد.

## پس پردازش خروجی

مقادیری که به عنوان خروجی از مدل‌مان دریافت می‌کنیم به خودی خود قابل درک نیستند. بگذارید نگاهی به آن‌ها بیندازیم:

<div dir="ltr">


```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

</div>

پیش‌بینی مدل ما برای جمله اول `[-1.5607, 1.6123]` و برای جمله دوم `[ 4.1692, -3.3464]` است. این‌ خروجی‌ها مقادیر آماری نیستند، بلکه به آنها *لوجیت* می‌گوییم. آن‌ها مقادیر خام و نرمال‌نشده خروجی از آخرین لایه مدل هستند. برای تبدیل به مقادیر آماری باید آن‌ها را از یک لایه‌ [سافت‌مکس](https://en.wikipedia.org/wiki/Softmax_function) بگذرانیم(تمام ترنسفورمرهای هاگینگ‌فِیس در خروجی لوجیت تولید می‌کنند زیرا معمولا تابع لاس مورد استفاده در تعلیم مدل، آخرین تابع فعال‌ساز مانند سافت‌مکس‌ را با خود تابع لاس مانند کِراس‌آنتروپی ترکیب می‌کند).


<div dir="ltr">

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

</div>


حالا می‌توانیم ببینیم که پیش‌بینی مدل برای جمله اول `[0.9995,  0.0005]` و برای جمله دوم `[0.9995,  0.0005]` است. این‌ها مقادیر آشنای آماری هستند.

برای تبدیل این مقادیر به عنوان دسته تشخیص داده شده می‌توانیم از ویژگی `id2label` تنظیمات مدل استفاده کنیم(در بخش بعدی بیشتر در این مورد صحبت خواهیم کرد):


<div dir="ltr">

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

</div>

اکنون مشخص است که پیش‌بینی‌های مدل از این قرار هستند:

- جمله اول: NEGATIVE: 0.0402, POSITIVE: 0.9598
- جمله دوم: NEGATIVE: 0.9995, POSITIVE: 0.0005


ما با موفقیت سه مرحله خط‌تولید را در اینجا نشان دادیم: پیش‌پردازش توسط توکِنایزرها، گذر ورودی‌ها از مدل و پس‌پردازش! حالا کمی زمان می‌گذاریم تا به صورت عمیق‌تر وارد هر‌کدام از این مراحل شویم.

<Tip>

✏️ **خودتان امتحان کنید!** دو(یا بیشتر) نوشته از خودتان را از خط‌تولید `sentiment-analysis` بگذرانید. سپس مراحلی که در اینجا دیدیم را تکرار کنید و مطمئن شوید همان نتایج را دریافت می کنید!

</Tip>

</div>
