<FrameworkSwitchCourse {fw} />

# Menggabungkan Semuanya[[putting-it-all-together]]

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

Dalam beberapa bagian sebelumnya, kita telah mencoba melakukan sebagian besar proses secara manual. Kita telah mempelajari cara kerja tokenizer dan mengeksplorasi tokenisasi, konversi ke input ID, padding, pemotongan (truncation), dan attention mask.

Namun seperti yang telah kita lihat di bagian 2, API ðŸ¤— Transformers bisa menangani semua ini secara otomatis melalui fungsi tingkat tinggi yang akan kita bahas di sini. Saat Anda memanggil `tokenizer` langsung pada kalimat, Anda akan mendapatkan input yang siap digunakan oleh model:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Di sini, variabel `model_inputs` sudah berisi semua hal yang diperlukan agar model dapat berfungsi dengan baik. Untuk DistilBERT, ini mencakup `input_ids` dan `attention_mask`. Model lain yang memerlukan input tambahan juga akan diberikan oleh objek `tokenizer`.

Seperti yang akan kita lihat dalam contoh-contoh berikut, metode ini sangat kuat. Pertama, ia bisa menangani satu urutan:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Ia juga bisa menangani beberapa urutan sekaligus, tanpa mengubah API:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

Ia juga bisa melakukan padding sesuai dengan berbagai tujuan:

```py
# Padding hingga panjang urutan terpanjang dalam batch
model_inputs = tokenizer(sequences, padding="longest")

# Padding hingga panjang maksimum model
# (512 untuk BERT/DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Padding hingga panjang maksimum yang ditentukan
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

Ia juga bisa memotong (truncate) urutan:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Potong urutan yang melebihi panjang maksimum model
# (512 untuk BERT/DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Potong urutan yang melebihi panjang maksimum yang ditentukan
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

Objek `tokenizer` dapat menangani konversi ke tensor dari framework tertentu, yang kemudian dapat langsung dikirim ke model. Sebagai contoh, pada cuplikan kode berikut kita meminta tokenizer untuk mengembalikan tensor dari berbagai framework â€” `"pt"` akan mengembalikan tensor PyTorch dan `"np"` akan mengembalikan array NumPy:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Mengembalikan tensor PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Mengembalikan array NumPy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Token Khusus[[special-tokens]]

Jika kita melihat `input_ids` yang dikembalikan oleh tokenizer, kita akan melihat perbedaan kecil dibandingkan sebelumnya:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

Satu token ID ditambahkan di awal dan satu di akhir. Mari kita dekode keduanya untuk melihat apa bedanya:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

Tokenizer menambahkan kata khusus `[CLS]` di awal dan `[SEP]` di akhir. Ini karena model dilatih sebelumnya dengan token tersebut, jadi untuk hasil inferensi yang konsisten, token ini juga perlu ditambahkan. Perlu dicatat bahwa tidak semua model menambahkan token khusus, dan ada yang menambahkan token yang berbeda. Beberapa hanya di awal, beberapa hanya di akhir â€” tokenizer tahu apa yang dibutuhkan oleh masing-masing model dan akan mengaturnya untuk Anda.

## Penutup: Dari Tokenizer ke Model[[wrapping-up-from-tokenizer-to-model]]

Sekarang setelah kita melihat semua langkah individual yang digunakan oleh objek `tokenizer` saat diterapkan ke teks, mari kita lihat bagaimana ia menangani beberapa urutan (padding!), urutan sangat panjang (truncation!), dan berbagai jenis tensor langsung melalui API utamanya:

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
