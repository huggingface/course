# Bagaimana jika dataset saya tidak ada di Hub?[[what-if-my-dataset-isnt-on-the-hub]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

Kamu sudah tahu cara menggunakan [Hugging Face Hub](https://huggingface.co/datasets) untuk mengunduh dataset, tetapi sering kali kamu akan bekerja dengan data yang disimpan di laptop kamu atau di server jarak jauh. Dalam bagian ini, kami akan menunjukkan bagaimana 🤗 Datasets dapat digunakan untuk memuat dataset yang tidak tersedia di Hugging Face Hub.

<Youtube id="HyQgpJTkRdE"/>

## Bekerja dengan dataset lokal dan jarak jauh[[working-with-local-and-remote-datasets]]

🤗 Datasets menyediakan skrip pemuatan untuk menangani pemuatan dataset lokal dan jarak jauh. Ini mendukung beberapa format data umum, seperti:

|    Format Data     | Skrip Pemuatan |                        Contoh                          |
| :----------------: | :------------: | :----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     File Teks      |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

Seperti yang ditunjukkan dalam tabel, untuk setiap format data kita hanya perlu menentukan jenis skrip pemuatan di fungsi `load_dataset()`, bersama dengan argumen `data_files` yang menentukan jalur ke satu atau lebih file. Mari kita mulai dengan memuat dataset dari file lokal; nanti kita akan melihat cara melakukan hal yang sama dengan file jarak jauh.

## Memuat dataset lokal[[loading-a-local-dataset]]

Untuk contoh ini, kita akan menggunakan [dataset SQuAD-it](https://github.com/crux82/squad-it/), yang merupakan dataset skala besar untuk tugas tanya-jawab dalam bahasa Italia.

Bagian pelatihan dan pengujian di-host di GitHub, jadi kita bisa mengunduhnya dengan perintah `wget` sederhana:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Ini akan mengunduh dua file terkompresi bernama *SQuAD_it-train.json.gz* dan *SQuAD_it-test.json.gz*, yang bisa kita dekompresi dengan perintah `gzip` di Linux:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- diganti dengan SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- diganti dengan SQuAD_it-train.json
```

Kita dapat melihat bahwa file terkompresi telah diganti dengan _SQuAD_it-train.json_ dan _SQuAD_it-test.json_, dan datanya disimpan dalam format JSON.

<Tip>

✎ Jika kamu bertanya-tanya kenapa ada karakter `!` di perintah shell di atas, itu karena kita menjalankannya di dalam Jupyter notebook. Cukup hapus awalan tersebut jika kamu ingin mengunduh dan mengekstrak dataset dari terminal biasa.

</Tip>

Untuk memuat file JSON dengan fungsi `load_dataset()`, kita hanya perlu tahu apakah kita berurusan dengan JSON biasa (mirip dengan kamus bertingkat) atau JSON Lines (baris per baris JSON). Seperti banyak dataset tanya-jawab, SQuAD-it menggunakan format bertingkat, dengan semua teks disimpan di field `data`. Ini berarti kita dapat memuat dataset dengan menentukan argumen `field` seperti berikut:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Secara default, memuat file lokal akan menghasilkan objek `DatasetDict` dengan split `train`. Kita dapat melihat ini dengan memeriksa objek `squad_it_dataset`:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Ini menunjukkan jumlah baris dan nama kolom yang terkait dengan set pelatihan. Kita bisa melihat salah satu contoh dengan mengakses indeks dari split `train` seperti ini:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si è verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Mantap, kita telah berhasil memuat dataset lokal pertama kita! Tapi walaupun ini bekerja untuk set pelatihan, yang sebenarnya kita inginkan adalah memasukkan split `train` dan `test` dalam satu objek `DatasetDict` sehingga kita bisa menerapkan fungsi `Dataset.map()` ke keduanya sekaligus. Untuk melakukan ini, kita bisa memberikan dictionary ke argumen `data_files` yang memetakan nama setiap split ke file yang sesuai:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Ini adalah hasil yang kita inginkan. Sekarang, kita bisa menerapkan berbagai teknik praproses untuk membersihkan data, melakukan tokenisasi, dan sebagainya.

<Tip>

Argumen `data_files` dari fungsi `load_dataset()` cukup fleksibel dan bisa berupa path file tunggal, daftar path file, atau dictionary yang memetakan nama split ke path file. Kamu juga bisa menggunakan glob untuk mencocokkan pola tertentu sesuai aturan shell Unix (misalnya, kamu bisa memuat semua file JSON dalam direktori sebagai satu split dengan `data_files="*.json"`). Lihat dokumentasi 🤗 Datasets [di sini](https://huggingface.co/docs/datasets/loading#local-and-remote-files) untuk info lebih lanjut.

</Tip>

Skrip pemuatan di 🤗 Datasets sebenarnya mendukung dekompresi otomatis dari file input, jadi kita bisa saja melewati langkah penggunaan `gzip` dan langsung mengarah ke file terkompresi di argumen `data_files`:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Ini sangat berguna jika kamu tidak ingin mengekstrak file GZIP secara manual. Dekompressi otomatis ini juga berlaku untuk format umum lainnya seperti ZIP dan TAR, jadi kamu cukup arahkan `data_files` ke file terkompresi dan langsung bisa digunakan!

Sekarang setelah kamu tahu cara memuat file lokal di laptop atau desktop, mari kita lihat cara memuat file dari server jarak jauh.

## Memuat dataset jarak jauh[[loading-a-remote-dataset]]

Jika kamu bekerja sebagai data scientist atau programmer di perusahaan, ada kemungkinan besar bahwa dataset yang ingin kamu analisis disimpan di server jarak jauh. Untungnya, memuat file jarak jauh semudah memuat file lokal! Alih-alih memberikan path ke file lokal, kita arahkan argumen `data_files` dari `load_dataset()` ke satu atau lebih URL tempat file disimpan. Sebagai contoh, untuk dataset SQuAD-it yang di-host di GitHub, kita cukup arahkan `data_files` ke URL _SQuAD_it-*.json.gz_ seperti ini:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Ini akan menghasilkan objek `DatasetDict` yang sama seperti sebelumnya, tetapi menghemat langkah pengunduhan dan dekompresi file _SQuAD_it-*.json.gz_ secara manual. Ini mengakhiri pembahasan kita tentang berbagai cara memuat dataset yang tidak di-host di Hugging Face Hub. Sekarang setelah kita punya dataset untuk dimainkan, mari kita mulai mengolah data dengan berbagai teknik data wrangling!

<Tip>

✏️ **Coba sendiri!** Pilih dataset lain yang di-host di GitHub atau di [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) dan coba muat baik secara lokal maupun jarak jauh menggunakan teknik yang telah dijelaskan di atas. Untuk tantangan tambahan, coba muat dataset yang disimpan dalam format CSV atau teks (lihat [dokumentasi](https://huggingface.co/docs/datasets/loading#local-and-remote-files) untuk info lebih lanjut tentang format-format ini).

</Tip>
