# Pendahuluan[[introduction]]

<CourseFloatingBanner
    chapter={5}
    classNames="absolute z-10 right-0 top-0"
/>

Di [Bab 3](/course/chapter3) Anda telah mencicipi untuk pertama kalinya pustaka ğŸ¤— Datasets dan melihat bahwa ada tiga langkah utama dalam menyempurnakan model:

1. Memuat dataset dari Hugging Face Hub.
2. Melakukan praproses data dengan `Dataset.map()`.
3. Memuat dan menghitung metrik.

Namun itu baru permukaan dari apa yang bisa dilakukan oleh ğŸ¤— Datasets! Dalam bab ini, kita akan menyelami pustaka tersebut lebih dalam. Sepanjang perjalanan, kita akan menemukan jawaban atas pertanyaan-pertanyaan berikut:

* Apa yang harus dilakukan jika dataset Anda tidak ada di Hub?
* Bagaimana cara memotong dan membagi dataset? (Dan bagaimana jika Anda _benar-benar_ harus menggunakan Pandas?)
* Apa yang harus dilakukan jika dataset Anda sangat besar dan bisa membuat RAM laptop Anda kepanasan?
* Apa itu sebenarnya "memory mapping" dan Apache Arrow?
* Bagaimana cara membuat dataset Anda sendiri dan mengunggahnya ke Hub?

Teknik-teknik yang Anda pelajari di sini akan mempersiapkan Anda untuk tugas tokenisasi lanjutan dan penyempurnaan model di [Bab 6](/course/chapter6) dan [Bab 7](/course/chapter7) â€” jadi ambil kopi Anda dan mari kita mulai!
