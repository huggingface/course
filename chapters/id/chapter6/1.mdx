# Pendahuluan[[introduction]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

Di [Bab 3](/course/chapter3), kita telah melihat cara menyetel ulang (fine-tune) sebuah model untuk suatu tugas tertentu. Saat kita melakukan itu, kita menggunakan tokenizer yang sama dengan yang digunakan saat model tersebut dilatih sebelumnya â€” tetapi apa yang harus kita lakukan ketika ingin melatih model dari awal? Dalam kasus seperti ini, menggunakan tokenizer yang telah dilatih sebelumnya pada korpus dari domain atau bahasa lain biasanya tidak optimal. Sebagai contoh, tokenizer yang dilatih pada korpus berbahasa Inggris akan berkinerja buruk pada korpus teks berbahasa Jepang karena penggunaan spasi dan tanda baca sangat berbeda di antara kedua bahasa tersebut.

Dalam bab ini, Anda akan belajar cara melatih tokenizer baru dari nol menggunakan korpus teks, sehingga tokenizer tersebut dapat digunakan untuk melakukan pretraining pada model bahasa. Semua ini akan dilakukan dengan bantuan pustaka [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers), yang menyediakan tokenizer "cepat" dalam pustaka [ğŸ¤— Transformers](https://github.com/huggingface/transformers). Kita akan melihat lebih dekat fitur-fitur yang disediakan pustaka ini, dan menjelajahi bagaimana tokenizer cepat berbeda dari versi "lambat".

Topik-topik yang akan kita bahas meliputi:

* Cara melatih tokenizer baru yang mirip dengan yang digunakan oleh checkpoint tertentu pada korpus teks baru
* Fitur-fitur khusus dari tokenizer cepat
* Perbedaan antara tiga algoritma tokenisasi sub-kata utama yang digunakan dalam NLP saat ini
* Cara membangun tokenizer dari nol menggunakan pustaka ğŸ¤— Tokenizers dan melatihnya pada beberapa data

Teknik-teknik yang diperkenalkan dalam bab ini akan mempersiapkan Anda untuk bagian di [Bab 7](/course/chapter7/6) di mana kita akan melihat cara membuat model bahasa untuk kode sumber Python. Mari kita mulai dengan melihat apa arti sebenarnya dari "melatih" sebuah tokenizer.
