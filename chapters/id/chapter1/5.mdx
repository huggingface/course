# Bagaimana ðŸ¤— Transformers Menyelesaikan Tugas

<Youtube id="zsfR7eY9Uho" />

Pada bagian [Transformers, apa yang bisa mereka lakukan?](/course/chapter1/3), Anda telah mempelajari tentang pemrosesan bahasa alami (NLP), audio dan suara, tugas visi komputer, serta beberapa aplikasi penting dari teknologi tersebut. Halaman ini akan membahas lebih dalam bagaimana model menyelesaikan tugas-tugas tersebut dan menjelaskan apa yang terjadi di balik layar. Ada banyak cara untuk menyelesaikan sebuah tugas; beberapa model mungkin menggunakan teknik tertentu atau bahkan pendekatan baru. Namun, untuk model Transformer, ide umumnya sama. Karena arsitekturnya fleksibel, sebagian besar model merupakan variasi dari struktur encoder, decoder, atau encoder-decoder.

<Tip>

Sebelum mempelajari varian arsitektur secara spesifik, penting untuk memahami bahwa sebagian besar tugas mengikuti pola yang serupa: data input diproses oleh model, lalu output diinterpretasikan untuk tugas tertentu. Perbedaannya terletak pada bagaimana data dipersiapkan, varian arsitektur model yang digunakan, dan bagaimana output diproses.

</Tip>

Untuk menjelaskan bagaimana tugas diselesaikan, kita akan menelusuri apa yang terjadi di dalam model hingga menghasilkan prediksi yang berguna. Kita akan membahas model berikut beserta tugasnya:

- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) untuk klasifikasi audio dan pengenalan ucapan otomatis (ASR)
- [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) dan [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) untuk klasifikasi gambar
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr) untuk deteksi objek
- [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) untuk segmentasi gambar
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) untuk estimasi kedalaman
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert) untuk tugas NLP seperti klasifikasi teks, klasifikasi token, dan menjawab pertanyaan (menggunakan encoder)
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) untuk tugas NLP seperti generasi teks (menggunakan decoder)
- [BART](https://huggingface.co/docs/transformers/model_doc/bart) untuk tugas NLP seperti ringkasan dan terjemahan (menggunakan encoder-decoder)

<Tip>

Sebelum melangkah lebih jauh, akan sangat membantu jika Anda memahami dasar arsitektur Transformer. Mengetahui cara kerja encoder, decoder, dan attention akan membantu Anda memahami bagaimana berbagai model Transformer bekerja. Pastikan untuk membaca [bagian sebelumnya](https://huggingface.co/course/chapter1/4?fw=pt) untuk informasi lebih lanjut!

</Tip>

## Model Transformer untuk Bahasa

Model bahasa berada di pusat perkembangan NLP modern. Model ini dirancang untuk memahami dan menghasilkan bahasa manusia dengan mempelajari pola dan hubungan statistik antara kata atau token dalam teks.

Transformer awalnya dirancang untuk terjemahan mesin, namun sejak itu telah menjadi arsitektur standar untuk hampir semua tugas AI. Beberapa tugas lebih cocok diselesaikan dengan struktur encoder, lainnya dengan decoder, dan beberapa lagi dengan kombinasi encoder-decoder.

### Bagaimana Model Bahasa Bekerja

Model bahasa bekerja dengan dilatih untuk memprediksi kemungkinan suatu kata berdasarkan konteks kata-kata di sekitarnya. Hal ini memberikan pemahaman dasar tentang bahasa yang dapat digeneralisasi ke berbagai tugas lain.

Ada dua pendekatan utama untuk melatih model Transformer:

1. **Masked language modeling (MLM)**: Digunakan oleh model encoder seperti BERT. Dalam pendekatan ini, beberapa token dalam input disamarkan secara acak, dan model dilatih untuk memprediksi token asli berdasarkan konteks sekitarnya. Ini memungkinkan model mempelajari konteks dua arah (melihat ke depan dan ke belakang).

2. **Causal language modeling (CLM)**: Digunakan oleh model decoder seperti GPT. Pendekatan ini memprediksi token berikutnya berdasarkan semua token sebelumnya. Model hanya boleh menggunakan konteks dari kiri untuk memprediksi token berikutnya.

### Jenis Model Bahasa

Dalam pustaka Transformers, model bahasa umumnya dikategorikan dalam tiga jenis arsitektur:

1. **Model encoder-saja** (misalnya BERT): Menggunakan pendekatan dua arah untuk memahami konteks dari dua arah. Cocok untuk tugas yang membutuhkan pemahaman mendalam seperti klasifikasi, pengenalan entitas bernama, dan menjawab pertanyaan.

2. **Model decoder-saja** (misalnya GPT, Llama): Memproses teks dari kiri ke kanan dan sangat baik untuk tugas generasi teks. Dapat menyelesaikan kalimat, menulis esai, atau bahkan menghasilkan kode dari prompt.

3. **Model encoder-decoder** (misalnya T5, BART): Menggabungkan kedua pendekatan â€” encoder untuk memahami input dan decoder untuk menghasilkan output. Cocok untuk tugas sequence-to-sequence seperti terjemahan, ringkasan, dan tanya jawab.

![transformer-models-for-language](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_architecture.png)

Seperti yang dijelaskan di bagian sebelumnya, model bahasa umumnya dilatih sebelumnya pada jumlah besar data teks secara self-supervised (tanpa anotasi manusia), kemudian disempurnakan (fine-tuned) untuk tugas spesifik. Pendekatan ini, yang disebut pembelajaran transfer (transfer learning), memungkinkan model beradaptasi dengan berbagai tugas NLP dengan data tambahan yang relatif sedikit.

Di bagian selanjutnya, kita akan mengeksplorasi arsitektur model secara spesifik dan bagaimana mereka digunakan untuk tugas-tugas di bidang suara, visi, dan teks.

<Tip>

Memahami bagian mana dari arsitektur Transformer (encoder, decoder, atau keduanya) yang paling cocok untuk suatu tugas NLP sangat penting untuk memilih model yang tepat. Umumnya, tugas yang memerlukan konteks dua arah menggunakan encoder, tugas yang menghasilkan teks menggunakan decoder, dan tugas sequence-to-sequence menggunakan encoder-decoder.

</Tip>

### Generasi Teks

Generasi teks melibatkan pembuatan teks yang koheren dan relevan dengan konteks berdasarkan sebuah prompt atau input.

[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2) adalah model decoder-saja yang telah dilatih sebelumnya (pretrained) pada sejumlah besar data teks. GPT-2 dapat menghasilkan teks yang meyakinkan (meskipun tidak selalu benar!) dari sebuah prompt dan bahkan mampu menyelesaikan tugas NLP lain seperti menjawab pertanyaan, meskipun tidak dilatih secara eksplisit untuk itu.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2 menggunakan [byte pair encoding (BPE)](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe) untuk melakukan tokenisasi kata dan menghasilkan embedding token. Encoding posisi ditambahkan ke token embeddings untuk menunjukkan posisi setiap token dalam urutan. Embedding ini diteruskan melalui beberapa blok decoder untuk menghasilkan *hidden state* akhir. Dalam setiap blok decoder, GPT-2 menggunakan *masked self-attention* yang berarti GPT-2 tidak dapat memperhatikan token di masa depan â€” hanya token sebelumnya yang diperbolehkan.

2. Output dari decoder diteruskan ke kepala model bahasa, yang melakukan transformasi linier untuk mengubah hidden states menjadi logits. Label target adalah token berikutnya dalam urutan, yang dibuat dengan menggeser logits satu posisi ke kanan. Kemudian, *cross-entropy loss* dihitung antara logits dan label untuk memprediksi token berikutnya yang paling mungkin.

Tujuan pelatihan GPT-2 sepenuhnya berdasarkan [causal language modeling](https://huggingface.co/docs/transformers/glossary#causal-language-modeling), yaitu memprediksi kata berikutnya dalam urutan. Ini membuat GPT-2 sangat baik untuk tugas-tugas yang melibatkan generasi teks.

Siap mencoba sendiri generasi teks? Lihat panduan lengkap [causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling) untuk belajar cara fine-tune DistilGPT-2 dan menggunakannya untuk inferensi!

<Tip>

Untuk informasi lebih lanjut tentang generasi teks, lihat panduan [strategi generasi teks](generation_strategies)!

</Tip>

### Klasifikasi Teks

Klasifikasi teks melibatkan pemberian kategori pada dokumen teks, seperti analisis sentimen, klasifikasi topik, atau deteksi spam.

[BERT](https://huggingface.co/docs/transformers/model_doc/bert) adalah model encoder-saja dan merupakan model pertama yang secara efektif mengimplementasikan bidirectional learning untuk memahami representasi teks secara mendalam dengan memperhatikan kata-kata di kedua sisi.

1. BERT menggunakan tokenisasi [WordPiece](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) untuk menghasilkan embedding dari teks. Untuk membedakan antara satu kalimat dan pasangan kalimat, token khusus `[SEP]` ditambahkan. Token `[CLS]` ditambahkan di awal setiap urutan teks. Output akhir dengan token `[CLS]` digunakan sebagai input ke *classification head*. BERT juga menambahkan segment embedding untuk menunjukkan apakah token milik kalimat pertama atau kedua.

2. BERT dilatih dengan dua tujuan utama: masked language modeling dan next-sentence prediction. Dalam masked language modeling, sebagian token disamarkan secara acak, dan model harus memprediksi token yang disamarkan. Dalam next-sentence prediction, model harus memprediksi apakah kalimat B benar-benar mengikuti kalimat A.

3. Embedding input diteruskan melalui beberapa lapisan encoder untuk menghasilkan *hidden state* akhir.

Untuk menggunakan model pretrained BERT dalam klasifikasi teks, tambahkan *classification head* di atas model dasar BERT. Head ini adalah lapisan linier yang menerima hidden state terakhir dan mengubahnya menjadi logits. Cross-entropy loss dihitung antara logits dan label target untuk memprediksi label paling mungkin.

Siap mencoba klasifikasi teks? Lihat panduan lengkap [text classification](https://huggingface.co/docs/transformers/tasks/sequence_classification) untuk belajar cara fine-tune DistilBERT dan menggunakannya untuk inferensi!

### Klasifikasi Token

Klasifikasi token melibatkan pemberian label pada setiap token dalam sebuah urutan, seperti pada pengenalan entitas bernama (NER) atau penandaan part-of-speech.

Untuk menggunakan BERT dalam tugas seperti NER, tambahkan *token classification head* di atas model dasar BERT. Head ini adalah lapisan linier yang menerima *hidden state* terakhir dan mengubahnya menjadi logits. Cross-entropy loss dihitung antara logits dan label setiap token.

Siap mencoba klasifikasi token? Lihat panduan lengkap [token classification](https://huggingface.co/docs/transformers/tasks/token_classification) untuk belajar cara fine-tune DistilBERT dan menggunakannya untuk inferensi!

### Menjawab Pertanyaan

Tugas menjawab pertanyaan melibatkan menemukan jawaban dari suatu pertanyaan berdasarkan konteks atau teks yang disediakan.

Untuk menggunakan BERT dalam tugas tanya jawab, tambahkan *span classification head* di atas model BERT. Lapisan linier ini menghitung logits awal dan akhir dari bagian teks yang mengandung jawaban. Cross-entropy loss dihitung antara logits dan posisi label untuk menemukan rentang teks paling mungkin sebagai jawaban.

Siap mencoba menjawab pertanyaan? Lihat panduan lengkap [question answering](https://huggingface.co/docs/transformers/tasks/question_answering) untuk belajar cara fine-tune DistilBERT dan menggunakannya untuk inferensi!

<Tip>

ðŸ’¡ Perhatikan betapa mudahnya menggunakan BERT untuk berbagai tugas setelah model dilatih sebelumnya. Anda hanya perlu menambahkan *head* tertentu untuk memanipulasi *hidden states* menjadi output yang diinginkan!

</Tip>

### Ringkasan Teks (Summarization)

Ringkasan melibatkan merangkum teks panjang menjadi versi yang lebih pendek sambil tetap mempertahankan informasi dan makna penting.

Model encoder-decoder seperti [BART](https://huggingface.co/docs/transformers/model_doc/bart) dan [T5](model_doc/t5) dirancang untuk pola *sequence-to-sequence* seperti tugas ringkasan.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. Arsitektur encoder BART mirip dengan BERT dan menerima token serta positional embedding dari teks. BART dilatih dengan merusak input dan kemudian membangunnya kembali dengan decoder. Strategi korupsi teks terbaik yang digunakan adalah *text infilling*, di mana sejumlah span teks diganti dengan satu token `[mask]`. Ini mengajarkan model untuk memprediksi jumlah dan isi token yang hilang.

2. Output encoder diteruskan ke decoder yang harus memprediksi token yang disamarkan dan token utuh lainnya. Output dari decoder dikirim ke *language modeling head* untuk mengubah *hidden states* menjadi logits. Cross-entropy loss dihitung antara logits dan label (token yang digeser ke kanan).

Siap mencoba ringkasan teks? Lihat panduan lengkap [summarization](https://huggingface.co/docs/transformers/tasks/summarization) untuk belajar cara fine-tune T5 dan menggunakannya untuk inferensi!

<Tip>

Untuk informasi lebih lanjut tentang generasi teks, lihat panduan [strategi generasi teks](https://huggingface.co/docs/transformers/generation_strategies)!

</Tip>

### Terjemahan

Terjemahan melibatkan mengubah teks dari satu bahasa ke bahasa lain sambil tetap mempertahankan maknanya. Ini juga merupakan tugas sequence-to-sequence, sehingga Anda bisa menggunakan model encoder-decoder seperti [BART](https://huggingface.co/docs/transformers/model_doc/bart) atau [T5](model_doc/t5).

BART dapat disesuaikan untuk terjemahan dengan menambahkan encoder baru yang diinisialisasi secara acak untuk memetakan bahasa sumber ke input yang bisa diterjemahkan ke bahasa target. Encoder ini dilatih dengan cross-entropy loss dari output model. Setelah itu, semua parameter model dilatih bersama. BART kemudian dikembangkan menjadi versi multibahasa, yaitu mBART.

Siap mencoba terjemahan? Lihat panduan lengkap [translation](https://huggingface.co/docs/transformers/tasks/translation) untuk belajar cara fine-tune T5 dan menggunakannya untuk inferensi!

<Tip>

Seperti yang Anda lihat, banyak model mengikuti pola yang serupa meskipun digunakan untuk tugas berbeda. Memahami pola umum ini akan membantu Anda lebih cepat memahami model baru dan menyesuaikannya dengan kebutuhan Anda.

</Tip>

## Modalitas Selain Teks

Transformer tidak hanya terbatas pada teks. Mereka juga dapat diterapkan pada modalitas lain seperti suara dan audio, gambar, bahkan video. Dalam kursus ini kita akan fokus pada teks, namun kita juga akan memperkenalkan secara singkat modalitas lainnya.

### Suara dan Audio

Mari kita mulai dengan bagaimana model Transformer menangani data suara dan audio, yang menghadirkan tantangan unik dibandingkan teks atau gambar.

[Whisper](https://huggingface.co/docs/transformers/main/en/model_doc/whisper) adalah model encoder-decoder (sequence-to-sequence) yang dilatih sebelumnya pada 680.000 jam data audio berlabel. Skala pretraining yang besar ini memungkinkan Whisper memiliki kemampuan zero-shot pada tugas audio dalam bahasa Inggris dan banyak bahasa lain. Decoder memungkinkan Whisper mengubah representasi suara yang dipelajari menjadi output teks tanpa fine-tuning tambahan. Whisper dapat langsung digunakan begitu saja.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/whisper_architecture.png"/>
</div>

Diagram diambil dari [makalah Whisper](https://huggingface.co/papers/2212.04356).

Model ini memiliki dua komponen utama:

1. **Encoder** memproses input audio. Audio mentah diubah terlebih dahulu menjadi log-Mel spectrogram. Spectrogram ini kemudian diteruskan ke jaringan Transformer encoder.

2. **Decoder** mengambil representasi audio yang telah dienkode dan memprediksi token teks secara autoregressive. Decoder adalah Transformer standar yang dilatih untuk memprediksi token teks berikutnya berdasarkan token sebelumnya dan output encoder. Token khusus digunakan di awal input decoder untuk mengarahkan model ke tugas tertentu seperti transkripsi, terjemahan, atau identifikasi bahasa.

Whisper dilatih sebelumnya pada dataset audio beranotasi yang sangat besar dan beragam dari internet. Pretraining berskala besar ini memungkinkan Whisper memiliki performa zero-shot yang kuat pada banyak bahasa dan tugas.

Setelah pretraining, Anda dapat langsung menggunakan Whisper untuk inferensi zero-shot, atau melakukan fine-tuning pada data Anda untuk hasil yang lebih baik pada tugas tertentu seperti ASR (automatic speech recognition) atau terjemahan suara.

<Tip>

Inovasi utama dari Whisper adalah pelatihannya yang menggunakan skala data audio yang sangat besar dan beragam dari internet secara weakly-supervised. Ini memungkinkan model melakukan generalisasi luar biasa pada berbagai bahasa, aksen, dan tugas â€” bahkan tanpa fine-tuning spesifik.

</Tip>

### Pengenalan Ucapan Otomatis (ASR)

Untuk menggunakan model Whisper yang telah dilatih sebelumnya dalam tugas ASR, kita memanfaatkan struktur encoder-decoder secara penuh. Encoder memproses input audio, dan decoder menghasilkan transkrip secara token demi token. Saat fine-tuning, model biasanya dilatih menggunakan loss sequence-to-sequence standar (seperti cross-entropy) untuk memprediksi token teks yang benar dari input audio.

Cara termudah menggunakan model yang sudah di-finetune untuk inferensi adalah melalui `pipeline`.

```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-base.en"
)
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
# Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

Siap mencoba pengenalan suara otomatis? Lihat panduan lengkap [ASR](https://huggingface.co/docs/transformers/tasks/asr) untuk belajar bagaimana fine-tune Whisper dan menggunakannya untuk inferensi!

### Visi Komputer

Selanjutnya, mari kita bahas tugas-tugas visi komputer, yang berhubungan dengan memahami dan menginterpretasikan informasi visual dari gambar atau video.

Ada dua pendekatan utama dalam visi komputer menggunakan Transformer:

1. Membagi gambar menjadi urutan patch, kemudian memprosesnya secara paralel dengan Transformer.
2. Menggunakan CNN modern seperti [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), yang tetap menggunakan layer konvolusi namun dengan desain jaringan modern.

<Tip>

Pendekatan ketiga menggabungkan Transformer dengan konvolusi (misalnya [Convolutional Vision Transformer](https://huggingface.co/docs/transformers/model_doc/cvt) atau [LeViT](https://huggingface.co/docs/transformers/model_doc/levit)). Namun, kita tidak membahasnya di sini karena pada dasarnya mereka hanya menggabungkan dua pendekatan sebelumnya.

</Tip>

ViT dan ConvNeXT umumnya digunakan untuk klasifikasi gambar, sedangkan untuk tugas visi lain seperti deteksi objek, segmentasi, dan estimasi kedalaman, kita akan menggunakan model seperti DETR, Mask2Former, dan GLPN.

### Klasifikasi Gambar

Klasifikasi gambar adalah salah satu tugas dasar dalam visi komputer. Mari kita lihat bagaimana berbagai arsitektur model menyelesaikan masalah ini.

ViT dan ConvNeXT keduanya bisa digunakan untuk klasifikasi gambar, dengan perbedaan utama bahwa ViT menggunakan attention mechanism sedangkan ConvNeXT menggunakan konvolusi.

[ViT](https://huggingface.co/docs/transformers/model_doc/vit) menggantikan seluruh konvolusi dengan arsitektur Transformer murni. Jika Anda sudah memahami Transformer asli, maka Anda hampir memahami ViT.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

Perubahan utama pada ViT terletak pada cara gambar diubah menjadi input untuk Transformer:

1. Gambar dibagi menjadi patch persegi yang tidak tumpang tindih, masing-masing diubah menjadi vektor atau *patch embedding*. Patch embedding dihasilkan dari layer konvolusi 2D yang membuat dimensi input sesuai (misalnya 768 nilai per patch). Gambar 224x224 dapat dibagi menjadi 196 patch 16x16. Seperti teks yang di-tokenisasi, gambar diubah menjadi urutan patch.

2. *Embedding yang dapat dilatih* â€” token khusus `[CLS]` â€” ditambahkan di awal urutan patch, mirip seperti BERT. Hidden state akhir dari token `[CLS]` digunakan sebagai input ke kepala klasifikasi.

3. *Position embedding* ditambahkan karena model tidak tahu urutan spasial patch gambar. Embedding posisi ini juga dapat dilatih dan memiliki ukuran yang sama dengan patch embedding. Semua embedding kemudian diteruskan ke encoder Transformer.

4. Output, khususnya hidden state dari `[CLS]`, diteruskan ke multilayer perceptron (MLP) head. Tujuan pretraining ViT adalah klasifikasi. MLP mengubah output menjadi logits untuk label kelas, lalu menghitung cross-entropy loss untuk menentukan kelas yang paling mungkin.

Siap mencoba klasifikasi gambar? Lihat panduan lengkap [image classification](https://huggingface.co/docs/transformers/tasks/image_classification) untuk belajar bagaimana fine-tune ViT dan menggunakannya untuk inferensi!

<Tip>

Ingat: meskipun tugas-tugas yang berbeda memerlukan pendekatan khusus, sebagian besar model Transformer mengikuti pola arsitektur dan pelatihan yang serupa. Dengan memahami pola umum tersebut, Anda akan lebih mudah belajar dan menerapkan model baru.

</Tip>