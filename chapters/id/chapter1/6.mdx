<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

# Arsitektur Transformer[[transformer-architectures]]

Di bagian sebelumnya, kita telah memperkenalkan arsitektur umum Transformer dan menjelajahi bagaimana model-model ini menyelesaikan berbagai tugas. Sekarang, mari kita lihat lebih dekat tiga varian arsitektur utama dari model Transformer dan pahami kapan sebaiknya menggunakan masing-masing arsitektur.

<Tip>
Ingat bahwa sebagian besar model Transformer menggunakan salah satu dari tiga arsitektur: encoder-only, decoder-only, atau encoder-decoder (sequence-to-sequence). Memahami perbedaan ini akan membantu kamu memilih model yang tepat untuk tugas tertentu.
</Tip>

## Model Encoder[[encoder-models]]

<Youtube id="MUqNwgPjJvQ" />

Model encoder hanya menggunakan bagian encoder dari arsitektur Transformer. Pada setiap tahap, lapisan atensi dapat mengakses seluruh kata dalam kalimat awal. Model ini sering disebut memiliki atensi "bi-directional", dan dikenal sebagai *auto-encoding models*.

Pretraining biasanya dilakukan dengan merusak kalimat (misalnya dengan memask kata secara acak), lalu meminta model untuk memulihkan kalimat aslinya.

Model encoder sangat cocok untuk tugas-tugas yang membutuhkan pemahaman penuh terhadap kalimat, seperti klasifikasi kalimat, named entity recognition, dan question answering ekstraktif.

<Tip>
Seperti yang kita lihat di [Bagaimana ðŸ¤— Transformers menyelesaikan tugas](/chapter1/5), model encoder seperti BERT sangat baik dalam memahami teks karena dapat melihat konteks dari dua arah. Ini membuatnya ideal untuk tugas-tugas yang membutuhkan pemahaman menyeluruh terhadap input.
</Tip>

Contoh model:

- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ModernBERT](https://huggingface.co/docs/transformers/en/model_doc/modernbert)

## Model Decoder[[decoder-models]]

<Youtube id="d_ixlCubqQw" />

Model decoder hanya menggunakan bagian decoder dari Transformer. Setiap kata hanya dapat melihat kata-kata sebelumnya dalam kalimat. Model ini dikenal sebagai *auto-regressive models*.

Pretraining biasanya melibatkan prediksi kata berikutnya dalam kalimat.

Model ini sangat cocok untuk tugas-tugas yang melibatkan generasi teks.

<Tip>
Model decoder seperti GPT dirancang untuk menghasilkan teks dengan memprediksi satu token pada satu waktu. Seperti yang telah kita bahas di [Bagaimana ðŸ¤— Transformers menyelesaikan tugas](/chapter1/5), model ini hanya dapat melihat token-token sebelumnya, yang membuatnya sangat baik untuk generasi teks kreatif, namun kurang ideal untuk tugas-tugas yang membutuhkan pemahaman dua arah.
</Tip>

Contoh model:

- [SmolLM Series](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)
- [LLaMA](https://huggingface.co/docs/transformers/en/model_doc/llama4)
- [Gemma](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3)
- [DeepSeek V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)

### Large Language Models (LLMs) Modern

Sebagian besar LLM saat ini menggunakan arsitektur decoder-only. Model ini mengalami peningkatan pesat baik dari segi ukuran maupun kemampuan.

#### Dua fase pelatihan:

1. **Pretraining** â€“ belajar memprediksi token berikutnya
2. **Instruction tuning** â€“ dilatih untuk mengikuti instruksi manusia

#### Kemampuan umum LLM saat ini:

| Kemampuan | Deskripsi | Contoh |
|-----------|-----------|--------|
| Generasi teks | Menulis teks relevan & koheren | Menulis cerita, esai |
| Ringkasan | Meringkas dokumen panjang | Ringkasan eksekutif |
| Terjemahan | Bahasa-ke-bahasa | Inggris ke Spanyol |
| QA | Jawaban fakta | "Apa ibu kota Prancis?" |
| Generasi kode | Membuat fungsi dari deskripsi | Kode Python |
| Penalaran | Menyelesaikan langkah demi langkah | Masalah logika |
| Few-shot learning | Belajar dari 2â€“3 contoh | Klasifikasi teks |

Anda dapat bereksperimen dengan LLM berbasis decoder langsung di peramban Anda melalui halaman repositori model di Hub. Berikut contoh dengan [GPT-2](https://huggingface.co/openai-community/gpt2), model open source klasik dari OpenAI:

<iframe
	src="https://huggingface.co/openai-community/gpt2"
	frameborder="0"
	width="100%"
	height="450"
></iframe>

## Model Sequence-to-Sequence[[sequence-to-sequence-models]]

<Youtube id="0_4KEb08xrE" />

Model encoder-decoder (atau *sequence-to-sequence*) menggunakan kedua bagian arsitektur Transformer. Encoder melihat seluruh input, sementara decoder hanya melihat token sebelumnya.

Contoh pretraining: T5 mengganti span teks dengan token [MASK], dan model diminta memulihkan teks tersebut.

Model ini sangat cocok untuk tugas yang melibatkan transformasi teks ke teks, seperti ringkasan, terjemahan, atau QA generatif.

<Tip>
Model seperti BART dan T5 menggabungkan kekuatan dua arsitektur: pemahaman input (encoder) dan generasi output (decoder). Ini membuat mereka unggul dalam tugas seperti ringkasan dan terjemahan.
</Tip>

### Aplikasi praktis:

| Aplikasi | Deskripsi | Contoh model |
|----------|-----------|--------------|
| Terjemahan | Bahasa-ke-bahasa | Marian, T5 |
| Ringkasan | Meringkas teks panjang | BART, T5 |
| Data-to-text | Data terstruktur â†’ teks | T5 |
| Koreksi tata bahasa | Memperbaiki kalimat | T5 |
| QA generatif | Jawaban berdasarkan konteks | BART, T5 |

<iframe
	src="https://course-demos-speech-to-speech-translation.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Contoh model:

- [BART](https://huggingface.co/docs/transformers/model_doc/bart)
- [mBART](https://huggingface.co/docs/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/docs/transformers/model_doc/marian)
- [T5](https://huggingface.co/docs/transformers/model_doc/t5)

## Memilih Arsitektur yang Tepat[[choosing-the-right-architecture]]

| Tugas | Arsitektur | Contoh Model |
|-------|-------------|--------------|
| Klasifikasi teks | Encoder | BERT, RoBERTa |
| Generasi teks | Decoder | GPT, LLaMA |
| Terjemahan | Encoder-Decoder | T5, BART |
| Ringkasan | Encoder-Decoder | BART, T5 |
| Named Entity Recognition | Encoder | BERT, RoBERTa |
| QA ekstraktif | Encoder | BERT |
| QA generatif | Decoder atau Seq2Seq | GPT, T5 |
| Chatbot | Decoder | GPT, LLaMA |

<Tip>
Saat memilih model:
1. Apakah tugasmu membutuhkan pemahaman dua arah?
2. Apakah kamu menganalisis teks atau membuat teks baru?
3. Apakah kamu mentransformasi satu bentuk teks ke bentuk lain?

Jawaban-jawaban ini akan membantumu memilih arsitektur yang tepat.
</Tip>

## Mekanisme Attention [[attention-mechanisms]]

Sebagian besar model Transformer menggunakan full attention, dalam arti bahwa matriks attention berbentuk persegi. Ini bisa menjadi hambatan komputasi besar saat menangani teks panjang. Longformer dan Reformer adalah model yang mencoba menjadi lebih efisien dengan menggunakan versi sparsity (jarang) dari matriks attention untuk mempercepat pelatihan.

<Tip>

Mekanisme attention standar memiliki kompleksitas komputasi sebesar O(nÂ²), di mana n adalah panjang urutan. Ini menjadi masalah untuk urutan yang sangat panjang. Mekanisme attention khusus di bawah ini membantu mengatasi keterbatasan tersebut.

</Tip>

### Attention LSH

[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer) menggunakan attention LSH (Locality-Sensitive Hashing). Dalam perhitungan softmax(QKáµ—), hanya elemen terbesar (pada dimensi softmax) dari matriks QKáµ— yang akan memberikan kontribusi berarti. Jadi, untuk setiap query `q` dalam Q, kita hanya mempertimbangkan key `k` dalam K yang dekat dengan `q`. Fungsi hash digunakan untuk menentukan apakah `q` dan `k` cukup dekat. Attention mask dimodifikasi untuk memblokir token saat ini (kecuali pada posisi pertama), karena query dan key akan sama (dan jadi sangat mirip). Karena hasil hash bisa bersifat acak, beberapa fungsi hash digunakan dalam praktik (ditentukan oleh parameter `n_rounds`) dan hasilnya dirata-rata.

### Attention Lokal

[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) menggunakan attention lokal: sering kali, konteks lokal (misalnya, dua token di kiri dan kanan) sudah cukup untuk menentukan aksi dari sebuah token. Selain itu, dengan menumpuk lapisan attention yang memiliki jendela kecil (small window), lapisan terakhir akan memiliki bidang pengamatan (receptive field) yang lebih luas dari hanya token dalam jendela, sehingga memungkinkan model membangun representasi dari seluruh kalimat.

Beberapa token input yang telah dipilih juga diberi global attention: untuk token-token ini, matriks attention bisa mengakses semua token, dan proses ini bersifat simetris â€” semua token lain juga dapat mengakses token khusus tersebut (di samping token dalam jendela lokal mereka). Ini digambarkan dalam Gambar 2d pada makalah aslinya, dan contoh attention mask-nya dapat dilihat di bawah:

<div class="flex justify-center">
    <img scale="50 %" align="center" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png"/>
</div>

Dengan menggunakan matriks attention yang memiliki parameter lebih sedikit, model dapat menerima input dengan panjang urutan yang lebih besar.

### Positional Encoding Aksial

[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer) menggunakan *axial positional encodings*: dalam model Transformer tradisional, positional encoding `E` adalah matriks berukuran \\(l\\) Ã— \\(d\\), dengan \\(l\\) sebagai panjang urutan dan \\(d\\) sebagai dimensi *hidden state*. Untuk teks yang sangat panjang, matriks ini bisa menjadi sangat besar dan memakan banyak ruang di GPU.

Untuk mengatasinya, axial positional encodings memfaktorkan matriks besar `E` menjadi dua matriks yang lebih kecil, `E1` dan `E2`, dengan ukuran masing-masing \\(l_{1} \times d_{1}\\) dan \\(l_{2} \times d_{2}\\), sedemikian hingga \\(l_{1} \times l_{2} = l\\) dan \\(d_{1} + d_{2} = d\\). (Dengan hasil perkalian panjang urutan, ini jauh lebih kecil ukurannya.) Embedding untuk langkah waktu \\(j\\) dalam `E` diperoleh dengan menggabungkan (concatenate) embedding dari \\(j \% l1\\) di `E1` dan \\(j // l1\\) di `E2`.

## Kesimpulan [[conclusion]]

Di bagian ini, kita telah mempelajari tiga arsitektur utama Transformer serta beberapa mekanisme attention khusus. Memahami perbedaan arsitektur ini sangat penting untuk memilih model yang tepat untuk tugas NLP tertentu.

Saat Anda melanjutkan kursus ini, Anda akan mendapatkan pengalaman langsung dengan berbagai arsitektur tersebut dan mempelajari cara *fine-tuning* untuk kebutuhan spesifik Anda. Di bagian selanjutnya, kita akan melihat beberapa keterbatasan dan bias yang ada dalam model-model ini â€” hal-hal penting yang perlu Anda pahami saat akan menggunakannya dalam dunia nyata.
