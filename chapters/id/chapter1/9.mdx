# Bias dan Keterbatasan[[bias-and-limitations]]

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
]} />

Jika Anda berniat menggunakan model pralatih atau versi yang telah disesuaikan (fine-tuned) dalam produksi, harap diingat bahwa meskipun model-model ini merupakan alat yang sangat kuat, mereka memiliki keterbatasan. Yang paling utama adalah bahwa, untuk memungkinkan pelatihan awal (pretraining) pada data dalam jumlah besar, para peneliti sering mengambil semua konten yang bisa mereka temukan — baik yang terbaik maupun yang terburuk — dari internet.

Sebagai ilustrasi singkat, mari kita kembali ke contoh pipeline `fill-mask` dengan model BERT:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

Ketika diminta untuk mengisi kata yang hilang dalam dua kalimat ini, model hanya memberikan satu jawaban yang netral terhadap gender (waiter/waitress). Jawaban lainnya merupakan jenis pekerjaan yang biasanya diasosiasikan dengan gender tertentu — dan ya, "prostitute" (pelacur) termasuk dalam 5 kemungkinan teratas yang diasosiasikan model dengan "perempuan" dan "bekerja". Ini terjadi meskipun BERT adalah salah satu dari sedikit model Transformer yang tidak dibangun dengan mengambil data dari seluruh penjuru internet, melainkan menggunakan data yang tampaknya netral (dilatih pada dataset [Wikipedia Bahasa Inggris](https://huggingface.co/datasets/wikipedia) dan [BookCorpus](https://huggingface.co/datasets/bookcorpus)).

Saat Anda menggunakan alat ini, Anda perlu selalu mengingat bahwa model asli yang Anda gunakan sangat mungkin menghasilkan konten yang seksis, rasis, atau homofobik. Melatih ulang model dengan data Anda tidak akan sepenuhnya menghilangkan bias yang melekat ini.
