# Menyelami Lebih Dalam Inferensi Teks dengan LLM[[inference-with-llms]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="Xp2w1_LKZN4" />

Sejauh ini, kita telah mempelajari arsitektur Transformer dalam berbagai tugas spesifik seperti klasifikasi teks dan ringkasan. Namun, penggunaan utama Large Language Models (LLMs) adalah untuk *text generation* (generasi teks), dan itulah yang akan kita bahas dalam bab ini.

Pada halaman ini, kita akan membahas konsep inti dari proses inferensi LLM—memahami cara model menghasilkan teks dan komponen kunci dalam proses inferensinya.

## Memahami Dasar-dasarnya

Mari mulai dengan hal mendasar. *Inferensi* adalah proses menggunakan LLM yang sudah dilatih untuk menghasilkan teks seperti manusia dari masukan (*prompt*) tertentu. Model bahasa menggunakan pengetahuan dari pelatihan untuk membentuk respons satu kata (token) pada satu waktu. Model memanfaatkan probabilitas yang telah dipelajari dari miliaran parameter untuk memprediksi dan menghasilkan token berikutnya dalam urutan. Proses sekuensial inilah yang memungkinkan LLM menghasilkan teks yang koheren dan relevan secara kontekstual.

## Peran Mekanisme Attention

Mekanisme *attention* memberikan kemampuan pada LLM untuk memahami konteks dan menghasilkan respons yang koheren. Saat memprediksi kata berikutnya, tidak semua kata memiliki bobot yang sama—misalnya, dalam kalimat _"Ibu kota Prancis adalah..."_, kata "Prancis" dan "ibu kota" sangat penting untuk menentukan bahwa kata berikutnya seharusnya "Paris". Kemampuan untuk fokus pada informasi yang relevan ini disebut *attention*.

![Attention](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif)

Proses mengidentifikasi kata-kata yang paling relevan untuk memprediksi token berikutnya ini sangat efektif. Meskipun prinsip dasar pelatihan LLM (memprediksi token selanjutnya) relatif tetap sejak BERT dan GPT-2, ada kemajuan besar dalam skala jaringan saraf dan efisiensi attention untuk konteks panjang dengan biaya komputasi yang lebih rendah.

<Tip>

Singkatnya, mekanisme *attention* adalah kunci bagi LLM untuk menghasilkan teks yang koheren dan sadar konteks. Mekanisme ini membedakan LLM modern dari generasi sebelumnya.

</Tip>

### Panjang Konteks dan Rentang Attention

Sekarang kita memahami *attention*, mari bahas seberapa banyak konteks yang bisa diproses oleh LLM—yaitu panjang konteks atau 'daya ingat' model.

**Panjang konteks** mengacu pada jumlah maksimum token (kata atau bagian kata) yang dapat diproses oleh LLM sekaligus. Anggap saja ini sebagai memori kerja model.

Kemampuan ini dibatasi oleh beberapa faktor praktis:
- Arsitektur dan ukuran model
- Sumber daya komputasi yang tersedia
- Kompleksitas input dan output yang diharapkan

Idealnya, kita ingin memberi model konteks sebanyak mungkin, tetapi keterbatasan perangkat keras dan biaya komputasi membuat hal ini tidak praktis. Oleh karena itu, model dirancang dengan panjang konteks yang berbeda-beda untuk menyeimbangkan kapabilitas dan efisiensi.

<Tip>

**Panjang konteks** adalah jumlah maksimum token yang dapat dipertimbangkan model saat menghasilkan respons.

</Tip>

### Seni Membuat Prompt

Saat kita memberi informasi ke LLM, kita menyusunnya agar dapat mengarahkan hasil output ke tujuan yang kita inginkan. Ini disebut _prompting_.

Memahami bagaimana LLM memproses input membantu kita membuat *prompt* yang lebih efektif. Karena tugas utama model adalah memprediksi token berikutnya dengan menganalisis pentingnya setiap token dalam input, maka cara kita menyusun kalimat sangat memengaruhi hasil.

<Tip>

Desain prompt yang cermat akan mempermudah **mengendalikan arah output LLM sesuai yang diharapkan**.

</Tip>

## Proses Inferensi Dua Tahap

Setelah memahami komponen dasarnya, mari kita lihat bagaimana LLM sebenarnya menghasilkan teks. Prosesnya dibagi menjadi dua fase utama: **prefill** dan **decode**. Keduanya bekerja seperti jalur perakitan, masing-masing memainkan peran penting dalam menghasilkan teks yang koheren.

### Fase Prefill

Fase prefill adalah tahap persiapan—semua "bahan" awal diproses di sini. Terdiri dari tiga langkah utama:

1. **Tokenisasi**: Mengubah teks menjadi token (unit dasar yang bisa dipahami model)
2. **Konversi Embedding**: Mengubah token menjadi representasi numerik yang bermakna
3. **Pemrosesan Awal**: Menjalankan embedding melalui jaringan saraf untuk memahami konteks

Fase ini memerlukan banyak komputasi karena semua token input diproses sekaligus. Ibaratnya seperti membaca seluruh paragraf sebelum mulai menulis jawaban.

Coba berbagai tokenizer secara interaktif di playground berikut:

<iframe src="https://agents-course-the-tokenizer-playground.static.hf.space" frameborder="0" width="850" height="450"></iframe>

### Fase Decode

Setelah prefill selesai, kita masuk ke fase *decode*—di sinilah generasi teks sebenarnya terjadi. Model menghasilkan satu token setiap kali dalam proses yang disebut *autoregressive* (token baru tergantung pada semua token sebelumnya).

Langkah-langkah utama untuk setiap token baru:
1. **Perhitungan Attention**: Melihat semua token sebelumnya untuk memahami konteks
2. **Perhitungan Probabilitas**: Menentukan kemungkinan setiap token berikutnya
3. **Pemilihan Token**: Memilih token berikutnya berdasarkan probabilitas
4. **Pemeriksaan Lanjutan**: Menentukan apakah harus lanjut atau berhenti

Fase ini memerlukan banyak memori karena semua token yang dihasilkan sebelumnya harus dilacak.

## Strategi Sampling

Setelah memahami cara model menghasilkan teks, mari lihat bagaimana kita bisa mengendalikan proses tersebut. Seperti penulis yang bisa memilih antara gaya kreatif atau lugas, kita bisa menyetel cara model memilih token.

Coba lihat visualisasi decoding menggunakan SmolLM2 (akan berhenti saat menemukan token EOS yaitu **<|im_end|>**):

<iframe src="https://agents-course-decoding-visualizer.hf.space" frameborder="0" width="850" height="450"></iframe>

### Memahami Pemilihan Token

Ketika model harus memilih token berikutnya, ia mulai dari probabilitas kasar (logit) untuk seluruh kosa kata. Prosesnya:

![Pemilihan Token](https://huggingface.co/reasoning-course/images/resolve/main/inference/1.png)

1. **Logit Mentah**: Perkiraan awal model untuk setiap token
2. **Kontrol Temperatur**: Menyesuaikan kreativitas (semakin tinggi = lebih kreatif, semakin rendah = lebih pasti)
3. **Top-p Sampling**: Memilih hanya token dengan probabilitas kumulatif hingga ambang tertentu (misal 90%)
4. **Top-k Filtering**: Hanya mempertimbangkan k token teratas

### Mengelola Pengulangan: Menjaga Variasi Output

LLM cenderung mengulang, jadi kita gunakan penalti:

1. **Presence Penalty**: Penalti tetap untuk token yang sudah muncul sebelumnya
2. **Frequency Penalty**: Penalti berdasarkan seberapa sering token muncul

![Repetition](https://huggingface.co/reasoning-course/images/resolve/main/inference/2.png)

Penalti ini diterapkan sebelum sampling, agar model terdorong menggunakan kosa kata yang lebih beragam.

### Mengatur Panjang Output

Seperti cerita yang butuh panjang yang pas, kita bisa atur panjang output:

1. **Batas Token**: Minimal dan maksimal token
2. **Stop Sequence**: Pola tertentu untuk menghentikan generasi
3. **End-of-Sequence**: Deteksi otomatis akhir teks

Contoh: untuk satu paragraf, batasi hingga 100 token dan gunakan `\n\n` sebagai stop sequence.

![Panjang Output](https://huggingface.co/reasoning-course/images/resolve/main/inference/3.png)

### Beam Search: Melihat Beberapa Jalur

Berbeda dari sampling biasa, *beam search* menjelajahi banyak kemungkinan jalur sekaligus seperti pemain catur yang mempertimbangkan beberapa langkah ke depan.

![Beam Search](https://huggingface.co/reasoning-course/images/resolve/main/inference/4.png)

Langkah-langkahnya:
1. Jaga beberapa urutan kandidat
2. Hitung probabilitas token berikutnya untuk tiap urutan
3. Simpan kombinasi terbaik
4. Ulangi hingga selesai
5. Pilih urutan dengan probabilitas tertinggi

Visualisasinya bisa Anda coba di sini:

<iframe src="https://agents-course-beam-search-visualizer.hf.space" frameborder="0" width="850" height="450"></iframe>

## Tantangan Praktis dan Optimasi

Saat mulai mengaplikasikan LLM, ada tantangan praktis yang harus dikelola.

### Metrik Kinerja Utama

Empat metrik penting saat bekerja dengan LLM:

1. **Time to First Token (TTFT)**: Seberapa cepat respons pertama muncul (dipengaruhi oleh prefill)
2. **Time Per Output Token (TPOT)**: Kecepatan menghasilkan token selanjutnya
3. **Throughput**: Berapa banyak permintaan bisa dilayani sekaligus
4. **Penggunaan VRAM**: Seberapa banyak memori GPU yang dibutuhkan

### Tantangan Panjang Konteks

Panjang konteks adalah tantangan besar dalam inferensi:

- **Penggunaan Memori**: Meningkat secara kuadrat dengan panjang konteks
- **Kecepatan Proses**: Menurun secara linier dengan panjang konteks
- **Alokasi Sumber Daya**: Perlu keseimbangan VRAM yang cermat

Contohnya, model seperti [Qwen2.5-1M](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M) mendukung hingga 1 juta token, tapi dengan waktu inferensi yang jauh lebih lambat.

<div style="max-width: 800px; margin: 20px auto; padding: 20px; 
font-family: system-ui;">
    <div style="border: 2px solid #ddd; border-radius: 8px; 
    padding: 20px; margin-bottom: 20px;">
        <div style="display: flex; align-items: center; 
        margin-bottom: 15px;">
            <div style="flex: 1; text-align: center; padding: 
            10px; background: #f0f0f0; border-radius: 4px;">
                Input Text (Raw)
            </div>
            <div style="margin: 0 10px;">→</div>
            <div style="flex: 1; text-align: center; padding: 
            10px; background: #e1f5fe; border-radius: 4px;">
                Tokenized Input
            </div>
        </div>
        <div style="display: flex; margin-bottom: 15px;">
            <div style="flex: 1; border: 1px solid #ccc; 
            padding: 10px; margin: 5px; background: #e8f5e9; 
            border-radius: 4px; text-align: center;">
                Context Window<br/>(e.g., 4K tokens)
                <div style="display: flex; margin-top: 10px;">
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                </div>
            </div>
        </div>
        <div style="display: flex; justify-content: 
        space-between; text-align: center; font-size: 0.9em; 
        color: #666;">
            <div style="flex: 1;">
                <div style="border: 1px solid #ffcc80; padding: 
                8px; margin: 5px; background: #fff3e0; 
                border-radius: 4px;">
                    Memory Usage<br/>∝ Length²
                </div>
            </div>
            <div style="flex: 1;">
                <div style="border: 1px solid #90caf9; padding: 
                8px; margin: 5px; background: #e3f2fd; 
                border-radius: 4px;">
                    Processing Time<br/>∝ Length
                </div>
            </div>
        </div>
    </div>
</div>

### Optimasi KV Cache

Optimasi penting yang digunakan adalah **KV Caching**. Ini menyimpan hasil perhitungan sebelumnya, sehingga:

- Tidak perlu menghitung ulang
- Kecepatan meningkat
- Bisa memproses konteks panjang secara praktis

Biayanya adalah penggunaan memori tambahan, tapi keuntungannya sangat besar.

## Kesimpulan

Memahami inferensi LLM penting agar bisa menggunakannya secara efektif. Kita telah membahas:

- Peran penting attention dan konteks
- Proses inferensi dua tahap
- Beragam strategi sampling
- Tantangan praktis dan cara optimasinya

Dengan menguasai konsep-konsep ini, Anda akan lebih siap membangun aplikasi berbasis LLM yang efisien dan bermanfaat.

Dunia LLM terus berkembang pesat. Tetap ingin tahu dan terus bereksperimen untuk menemukan pendekatan terbaik bagi kasus penggunaan Anda.
