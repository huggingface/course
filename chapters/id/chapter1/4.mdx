# Bagaimana Transformer Bekerja?[[how-do-transformers-work]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

Di bagian ini, kita akan mempelajari arsitektur model Transformer dan memahami lebih dalam konsep seperti *attention*, arsitektur encoder-decoder, dan lainnya.

<Tip warning={true}>

ðŸš€ Kita mulai naik tingkat di sini. Bagian ini bersifat teknis dan detail, jadi jangan khawatir jika Anda tidak langsung memahaminya sepenuhnya. Kita akan mengulas konsep-konsep ini kembali di bab-bab berikutnya.

</Tip>

## Sekilas Sejarah Transformer[[a-bit-of-transformer-history]]

Berikut beberapa titik penting dalam (sejarah singkat) model Transformer:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="Kronologi singkat model Transformer.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="Kronologi singkat model Transformer.">
</div>

Arsitektur [Transformer](https://arxiv.org/abs/1706.03762) diperkenalkan pada Juni 2017, dengan fokus awal pada tugas penerjemahan. Setelah itu, muncul berbagai model penting lainnya, seperti:

- **Juni 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), model Transformer pra-latih pertama, digunakan untuk *fine-tuning* berbagai tugas NLP dan mencapai hasil terbaik saat itu
- **Oktober 2018**: [BERT](https://arxiv.org/abs/1810.04805), model pra-latih besar lainnya yang dirancang untuk menghasilkan representasi kalimat yang lebih baik
- **Februari 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), versi lebih besar dan lebih baik dari GPT, yang sempat tidak dirilis sepenuhnya karena pertimbangan etis
- **Oktober 2019**: [T5](https://huggingface.co/papers/1910.10683), implementasi Transformer sequence-to-sequence untuk multi-tasking
- **Mei 2020**: [GPT-3](https://huggingface.co/papers/2005.14165), versi lebih besar dari GPT-2 yang mampu menyelesaikan berbagai tugas tanpa *fine-tuning* (_zero-shot learning_)
- **Januari 2022**: [InstructGPT](https://huggingface.co/papers/2203.02155), versi GPT-3 yang dilatih agar lebih mampu mengikuti instruksi
- **Januari 2023**: [Llama](https://huggingface.co/papers/2302.13971), model bahasa besar multibahasa
- **Maret 2023**: [Mistral](https://huggingface.co/papers/2310.06825), model 7 miliar parameter yang melampaui Llama 2 13B di semua tolok ukur
- **Mei 2024**: [Gemma 2](https://huggingface.co/papers/2408.00118), keluarga model ringan berkinerja tinggi dengan teknik attention baru
- **November 2024**: [SmolLM2](https://huggingface.co/papers/2502.02737), model kecil dengan performa tinggi untuk perangkat edge/mobile
- Model tipe GPT (disebut juga sebagai Transformer model _auto-regressive_)
- Model tipe BERT (disebut juga sebagai Transformer model _auto-encoding_)
- Model tipe T5 (disebut juga sebagai Transformer model _sequence-to-sequence_)

Kita akan membahas tiap keluarga ini lebih dalam nanti.

## Transformer Adalah Model Bahasa[[transformers-are-language-models]]

Semua model Transformer yang disebutkan sebelumnya (GPT, BERT, T5, dll.) dilatih sebagai *model bahasa*. Artinya, mereka dilatih menggunakan sejumlah besar teks mentah dengan pendekatan *self-supervised*.

*Self-supervised learning* adalah metode pelatihan di mana tujuan pelatihan diturunkan langsung dari data masukan. Artinya, manusia tidak perlu memberikan label secara manual!

Model ini mengembangkan pemahaman statistik terhadap bahasa yang dilatihkan, tetapi kurang bermanfaat untuk tugas praktis tertentu. Oleh karena itu, model pra-latih umum kemudian melalui proses yang disebut *transfer learning* atau *fine-tuning*, yaitu pelatihan tambahan secara *supervised* menggunakan label yang diberikan manusia untuk tugas tertentu.

Contoh tugasnya adalah memprediksi kata berikutnya dalam kalimat setelah membaca kata-kata sebelumnya. Ini disebut *causal language modeling*, karena output tergantung pada input masa lalu dan saat ini â€” tetapi tidak pada masa depan.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Contoh causal language modeling.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Contoh causal language modeling.">
</div>

Contoh lain adalah *masked language modeling*, di mana model diminta memprediksi kata yang di-*masking* dalam sebuah kalimat.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Contoh masked language modeling.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Contoh masked language modeling.">
</div>

## Transformer Adalah Model Besar[[transformers-are-big-models]]

Kecuali beberapa pengecualian (seperti DistilBERT), strategi umum untuk meningkatkan performa model adalah dengan **memperbesar ukuran model** dan **jumlah data pelatihan**.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Jumlah parameter model Transformer modern" width="90%">
</div>

Sayangnya, melatih model besar memerlukan banyak data, waktu, dan sumber daya komputasi. Ini juga berdampak pada lingkungan, seperti ditunjukkan grafik berikut:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="Jejak karbon model bahasa besar.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="Jejak karbon model bahasa besar.">
</div>

<Youtube id="ftWlj4FBHTg"/>

Bahkan grafik di atas berasal dari proyek yang dilakukan dengan niat sadar untuk **mengurangi dampak lingkungan**. Bayangkan jika setiap tim peneliti, mahasiswa, atau perusahaan melatih model dari nol â€” biayanya akan sangat besar!

Inilah mengapa **berbagi model sangat penting**: dengan berbagi *trained weights*, kita mengurangi beban komputasi dan jejak karbon secara global.

Anda bisa menghitung jejak karbon model Anda menggunakan alat seperti [ML CO2 Impact](https://mlco2.github.io/impact/) atau [Code Carbon](https://codecarbon.io/), yang sudah terintegrasi dalam ðŸ¤— Transformers. Baca [blog ini](https://huggingface.co/blog/carbon-emissions-on-the-hub) dan [dokumentasi resmi](https://huggingface.co/docs/hub/model-cards-co2) untuk mempelajari lebih lanjut.

## Transfer Learning [[transfer-learning]]

<Youtube id="BqqfQnyjmgg" />

*Pretraining* adalah proses melatih model dari nol â€” bobot awalnya diinisialisasi secara acak dan tidak memiliki pengetahuan awal.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="Pretraining membutuhkan waktu dan biaya besar.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="Pretraining membutuhkan waktu dan biaya besar.">
</div>

Sebaliknya, *fine-tuning* dilakukan setelah model sudah dilatih. Kita mengambil model pra-latih dan melatih ulang dengan dataset spesifik untuk tugas kita. Kenapa tidak langsung latih model dari awal untuk tugas tersebut?

- Model pra-latih sudah memiliki *pengetahuan umum* dari data besar sebelumnya.
- *Fine-tuning* hanya membutuhkan data dalam jumlah kecil.
- Waktu dan sumber daya yang dibutuhkan jauh lebih sedikit.
- Hasil yang diperoleh umumnya lebih baik dibanding melatih dari nol.

Contohnya, Anda bisa menggunakan model bahasa Inggris umum lalu *fine-tune* dengan korpus artikel arXiv untuk membuat model bahasa ilmiah.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="Fine-tuning jauh lebih murah daripada pretraining.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="Fine-tuning jauh lebih murah daripada pretraining.">
</div>

## Arsitektur Umum Transformer[[general-transformer-architecture]]

Di bagian ini, kita membahas arsitektur umum model Transformer. Jangan khawatir jika belum paham sepenuhnya â€” kita akan bahas komponen-komponennya secara terpisah nanti.

<Youtube id="H39Z_720T5s" />

Model ini terdiri dari dua bagian utama:

- **Encoder (kiri)**: Menerima masukan dan menghasilkan representasi fitur
- **Decoder (kanan)**: Menggunakan representasi dari encoder untuk menghasilkan output

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Arsitektur umum Transformer.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Arsitektur umum Transformer.">
</div>

Penggunaan tergantung pada tugasnya:

- **Model encoder-only**: Cocok untuk klasifikasi kalimat, NER
- **Model decoder-only**: Cocok untuk generasi teks
- **Model encoder-decoder** (sequence-to-sequence): Cocok untuk penerjemahan, rangkuman

## Lapisan Attention[[attention-layers]]

Fitur utama dari Transformer adalah **lapisan attention (attention layers)**. Bahkan, judul makalah aslinya adalah ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)!

Singkatnya, lapisan ini memungkinkan model fokus hanya pada kata-kata yang relevan saat membentuk representasi kata.

Contoh: saat menerjemahkan "You like this course" ke dalam bahasa Prancis, model harus memperhatikan subjek "You" saat menerjemahkan "like", karena bentuk kata kerja tergantung subjek. Saat menerjemahkan "this", model juga harus memperhatikan kata benda "course", karena jenis kelamin kata dalam bahasa Prancis memengaruhi terjemahan.

Intinya: makna kata tergantung konteks â€” dan attention memungkinkan model memanfaatkan konteks tersebut.

## Arsitektur Asli[[the-original-architecture]]

Arsitektur awal Transformer dirancang untuk penerjemahan.

- **Encoder** menerima kalimat sumber (misalnya, bahasa Inggris)
- **Decoder** menghasilkan kalimat target (misalnya, bahasa Prancis)

Selama pelatihan, decoder diberi seluruh kalimat target, tapi *dilarang melihat kata-kata masa depan*. Contohnya: saat memprediksi kata ke-4, decoder hanya melihat kata ke-1 hingga ke-3.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Arsitektur lengkap Transformer.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Arsitektur lengkap Transformer.">
</div>

Lapisan attention pertama di decoder melihat semua input sebelumnya, sedangkan attention kedua menggunakan output dari encoder.

Masking juga digunakan untuk menghindari perhatian ke token spesial seperti padding.

## Arsitektur vs. Checkpoint[[architecture-vs-checkpoints]]

Selama mempelajari Transformer, Anda akan menemukan istilah:

- **Architecture**: Rangka desain model â€” struktur layer dan operasi
- **Checkpoint**: Bobot (weights) hasil pelatihan untuk arsitektur tertentu
- **Model**: Istilah umum yang bisa merujuk ke architecture atau checkpoint

Contoh:
- `BERT` â†’ adalah arsitektur
- `bert-base-cased` â†’ adalah checkpoint hasil pelatihan oleh Google
- "Model BERT" â†’ bisa merujuk ke keduanya tergantung konteks
