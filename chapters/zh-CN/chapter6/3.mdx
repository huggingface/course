<FrameworkSwitchCourse {fw} />

# å¿«é€Ÿæ ‡è®°å™¨çš„ç‰¹æ®Šèƒ½åŠ›

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},
]} />

{/if}

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»”ç»†ç ”ç©¶ ğŸ¤— Transformers ä¸­æ ‡è®°å™¨çš„åŠŸèƒ½ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªä½¿ç”¨å®ƒä»¬æ¥æ ‡è®°è¾“å…¥æˆ–å°† ID è§£ç å›æ–‡æœ¬ï¼Œä½†æ˜¯æ ‡è®°å™¨â€”â€”å°¤å…¶æ˜¯é‚£äº›ç”± ğŸ¤— Tokenizers åº“æ”¯æŒçš„â€”â€”å¯ä»¥åšæ›´å¤šçš„äº‹æƒ…ã€‚ä¸ºäº†è¯´æ˜è¿™äº›é™„åŠ åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦‚ä½•é‡ç°ç»“æœ **token-classification** ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º **ner** ï¼‰ å’Œ **question-answering** æˆ‘ä»¬ç¬¬ä¸€æ¬¡åœ¨[Chapter 1](/course/chapter1)ä¸­é‡åˆ°çš„ç®¡é“.

<Youtube id="g8quOxoqhHQ"/>

åœ¨æ¥ä¸‹æ¥çš„è®¨è®ºä¸­ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åŒºåˆ†â€œæ…¢â€å’Œâ€œå¿«â€åˆ†è¯å™¨ã€‚æ…¢é€Ÿåˆ†è¯å™¨æ˜¯åœ¨ ğŸ¤— Transformers åº“ä¸­ç”¨ Python ç¼–å†™çš„ï¼Œè€Œå¿«é€Ÿç‰ˆæœ¬æ˜¯ç”± ğŸ¤— åˆ†è¯å™¨æä¾›çš„ï¼Œå®ƒä»¬æ˜¯ç”¨ Rust ç¼–å†™çš„ã€‚å¦‚æœä½ è¿˜è®°å¾—åœ¨[Chapter 5](/course/chapter5/3)ä¸­æŠ¥å‘Šäº†å¿«é€Ÿå’Œæ…¢é€Ÿåˆ†è¯å™¨å¯¹è¯ç‰©å®¡æŸ¥æ•°æ®é›†è¿›è¡Œåˆ†è¯æ‰€éœ€çš„æ—¶é—´çš„è¿™å¼ è¡¨ï¼Œæ‚¨åº”è¯¥çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬ç§°å®ƒä»¬ä¸ºâ€œå¿«â€å’Œâ€œæ…¢â€ï¼š

                | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

âš ï¸ å¯¹å•ä¸ªå¥å­è¿›è¡Œåˆ†è¯æ—¶ï¼Œæ‚¨ä¸ä¼šæ€»æ˜¯çœ‹åˆ°ç›¸åŒåˆ†è¯å™¨çš„æ…¢é€Ÿå’Œå¿«é€Ÿç‰ˆæœ¬ä¹‹é—´çš„é€Ÿåº¦å·®å¼‚ã€‚äº‹å®ä¸Šï¼Œå¿«é€Ÿç‰ˆæœ¬å®é™…ä¸Šå¯èƒ½æ›´æ…¢ï¼åªæœ‰åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œæ ‡è®°æ—¶ï¼Œæ‚¨æ‰èƒ½æ¸…æ¥šåœ°çœ‹åˆ°å·®å¼‚ã€‚

</Tip>

## æ‰¹é‡ç¼–ç 

<Youtube id="3umI3tm27Vw"/>

åˆ†è¯å™¨çš„è¾“å‡ºä¸æ˜¯ç®€å•çš„ Python å­—å…¸ï¼›æˆ‘ä»¬å¾—åˆ°çš„å®é™…ä¸Šæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ **BatchEncoding** ç›®çš„ã€‚å®ƒæ˜¯å­—å…¸çš„å­ç±»ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¹‹å‰èƒ½å¤Ÿæ¯«æ— é—®é¢˜åœ°ç´¢å¼•åˆ°è¯¥ç»“æœä¸­çš„åŸå› ï¼‰ï¼Œä½†å…·æœ‰ä¸»è¦ç”±å¿«é€Ÿæ ‡è®°å™¨ä½¿ç”¨çš„é™„åŠ æ–¹æ³•ã€‚

é™¤äº†å®ƒä»¬çš„å¹¶è¡ŒåŒ–èƒ½åŠ›ä¹‹å¤–ï¼Œå¿«é€Ÿæ ‡è®°å™¨çš„å…³é”®åŠŸèƒ½æ˜¯å®ƒä»¬å§‹ç»ˆè·Ÿè¸ªæœ€ç»ˆæ ‡è®°æ¥è‡ªçš„åŸå§‹æ–‡æœ¬èŒƒå›´â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºåç§»æ˜ å°„.è¿™åè¿‡æ¥åˆè§£é”äº†è¯¸å¦‚å°†æ¯ä¸ªå•è¯æ˜ å°„åˆ°å®ƒç”Ÿæˆçš„æ ‡è®°æˆ–å°†åŸå§‹æ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦æ˜ å°„åˆ°å®ƒå†…éƒ¨çš„æ ‡è®°ç­‰åŠŸèƒ½ï¼Œåä¹‹äº¦ç„¶ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª **BatchEncoding** æ ‡è®°å™¨è¾“å‡ºä¸­çš„å¯¹è±¡ï¼š

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

ç”±äº **AutoTokenizer** ç±»é»˜è®¤é€‰æ‹©å¿«é€Ÿæ ‡è®°å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é™„åŠ æ–¹æ³• this **BatchEncoding** å¯¹è±¡æä¾›ã€‚æˆ‘ä»¬æœ‰ä¸¤ç§æ–¹æ³•æ¥æ£€æŸ¥æˆ‘ä»¬çš„åˆ†è¯å™¨æ˜¯å¿«çš„è¿˜æ˜¯æ…¢çš„ã€‚æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ **is_fast** çš„å±æ€§ **tokenizer** ï¼š

```python
tokenizer.is_fast
```

```python out
True
```

æˆ–æ£€æŸ¥æˆ‘ä»¬çš„ç›¸åŒå±æ€§ **encoding** ï¼š

```python
encoding.is_fast
```

```python out
True
```

è®©æˆ‘ä»¬çœ‹çœ‹å¿«é€Ÿæ ‡è®°å™¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿåšä»€ä¹ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®ä»¤ç‰Œè€Œæ— éœ€å°† ID è½¬æ¢å›ä»¤ç‰Œï¼š

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç´¢å¼• 5 å¤„çš„ä»¤ç‰Œæ˜¯ **##yl** ï¼Œå®ƒæ˜¯åŸå§‹å¥å­ä¸­â€œSylvainâ€ä¸€è¯çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ **word_ids()** è·å–æ¯ä¸ªæ ‡è®°æ¥è‡ªçš„å•è¯ç´¢å¼•çš„æ–¹æ³•ï¼š

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åˆ†è¯å™¨çš„ç‰¹æ®Šæ ‡è®° **[CLS]** å’Œ **[SEP]** è¢«æ˜ å°„åˆ° **None** ï¼Œç„¶åæ¯ä¸ªæ ‡è®°éƒ½æ˜ å°„åˆ°å®ƒèµ·æºçš„å•è¯ã€‚è¿™å¯¹äºç¡®å®šä¸€ä¸ªæ ‡è®°æ˜¯å¦åœ¨å•è¯çš„å¼€å¤´æˆ–ä¸¤ä¸ªæ ‡è®°æ˜¯å¦åœ¨åŒä¸€ä¸ªå•è¯ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚æˆ‘ä»¬å¯ä»¥ä¾é  **##** å‰ç¼€ï¼Œä½†å®ƒä»…é€‚ç”¨äºç±»ä¼¼ BERT çš„åˆ†è¯å™¨ï¼›è¿™ç§æ–¹æ³•é€‚ç”¨äºä»»ä½•ç±»å‹çš„æ ‡è®°å™¨ï¼Œåªè¦å®ƒæ˜¯å¿«é€Ÿçš„ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨æ­¤åŠŸèƒ½å°†æ¯ä¸ªå•è¯çš„æ ‡ç­¾æ­£ç¡®åº”ç”¨äºå‘½åå®ä½“è¯†åˆ« (NER) å’Œè¯æ€§ (POS) æ ‡è®°ç­‰ä»»åŠ¡ä¸­çš„æ ‡è®°ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å®ƒæ¥å±è”½æ¥è‡ªå±è”½è¯­è¨€å»ºæ¨¡ä¸­æ¥è‡ªåŒä¸€å•è¯çš„æ‰€æœ‰æ ‡è®°ï¼ˆä¸€ç§ç§°ä¸ºå…¨è¯æ©ç ï¼‰ã€‚

<Tip>

ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆçš„æ¦‚å¿µå¾ˆå¤æ‚ã€‚ä¾‹å¦‚ï¼Œâ€œI'llâ€ï¼ˆâ€œI willâ€çš„ç¼©å†™ï¼‰ç®—ä¸€ä¸¤ä¸ªè¯å—ï¼Ÿå®ƒå®é™…ä¸Šå–å†³äºåˆ†è¯å™¨å’Œå®ƒåº”ç”¨çš„é¢„åˆ†è¯æ“ä½œã€‚ä¸€äº›æ ‡è®°å™¨åªæ˜¯åœ¨ç©ºæ ¼ä¸Šæ‹†åˆ†ï¼Œå› æ­¤ä»–ä»¬ä¼šå°†å…¶è§†ä¸ºä¸€ä¸ªè¯ã€‚å…¶ä»–äººåœ¨ç©ºæ ¼é¡¶éƒ¨ä½¿ç”¨æ ‡ç‚¹ç¬¦å·ï¼Œå› æ­¤å°†å…¶è§†ä¸ºä¸¤ä¸ªè¯ã€‚

âœï¸ è¯•è¯•çœ‹ï¼ä»bert base casedå’Œroberta baseæ£€æŸ¥ç‚¹åˆ›å»ºä¸€ä¸ªæ ‡è®°å™¨ï¼Œå¹¶ç”¨å®ƒä»¬æ ‡è®°â€œ81sâ€ã€‚ä½ è§‚å¯Ÿåˆ°äº†ä»€ä¹ˆï¼ŸIDè¿™ä¸ªè¯æ˜¯ä»€ä¹ˆï¼Ÿ

</Tip>

åŒæ ·ï¼Œæœ‰ä¸€ä¸ª **sentence_ids()** æˆ‘ä»¬å¯ä»¥ç”¨æ¥å°†æ ‡è®°æ˜ å°„åˆ°å®ƒæ¥è‡ªçš„å¥å­çš„æ–¹æ³•ï¼ˆå°½ç®¡åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ **token_type_ids** åˆ†è¯å™¨è¿”å›çš„ä¿¡æ¯å¯ä»¥ä¸ºæˆ‘ä»¬æä¾›ç›¸åŒçš„ä¿¡æ¯ï¼‰ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†ä»»ä½•å•è¯æˆ–æ ‡è®°æ˜ å°„åˆ°åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ï¼Œåä¹‹äº¦ç„¶ï¼Œé€šè¿‡ **word_to_chars()** æˆ–è€… **token_to_chars()** å’Œ **char_to_word()** æˆ–è€… **char_to_token()** æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œ **word_ids()** æ–¹æ³•å‘Šè¯‰æˆ‘ä»¬ **##yl** æ˜¯ç´¢å¼• 3 å¤„å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œä½†å®ƒæ˜¯å¥å­ä¸­çš„å“ªä¸ªå•è¯ï¼Ÿæˆ‘ä»¬å¯ä»¥è¿™æ ·å‘ç°ï¼š

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯ç”±å¿«é€Ÿæ ‡è®°å™¨è·Ÿè¸ªæ¯ä¸ªæ ‡è®°æ¥è‡ªåˆ—è¡¨ä¸­çš„æ–‡æœ¬è·¨åº¦è¿™ä¸€äº‹å®æä¾›æ”¯æŒçš„æŠµæ¶ˆ.ä¸ºäº†è¯´æ˜å®ƒä»¬çš„ç”¨é€”ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å¤åˆ¶ç»“æœ **token-classification** æ‰‹åŠ¨ç®¡é“ã€‚

<Tip>

âœï¸ è¯•è¯•çœ‹ï¼åˆ›å»ºæ‚¨è‡ªå·±çš„ç¤ºä¾‹æ–‡æœ¬ï¼Œçœ‹çœ‹æ‚¨æ˜¯å¦èƒ½ç†è§£å“ªäº›æ ‡è®°ä¸å•è¯ ID ç›¸å…³è”ï¼Œä»¥åŠå¦‚ä½•æå–å•ä¸ªå•è¯çš„å­—ç¬¦è·¨åº¦ã€‚å¯¹äºå¥–åŠ±ç§¯åˆ†ï¼Œè¯·å°è¯•ä½¿ç”¨ä¸¤ä¸ªå¥å­ä½œä¸ºè¾“å…¥ï¼Œçœ‹çœ‹å¥å­ ID æ˜¯å¦å¯¹æ‚¨æœ‰æ„ä¹‰ã€‚

</Tip>

## åœ¨ä»¤ç‰Œåˆ†ç±»ç®¡é“å†…

åœ¨[Chapter 1](/course/chapter1)æˆ‘ä»¬ç¬¬ä¸€æ¬¡å°è¯•ä½¿ç”¨ NERâ€”â€”ä»»åŠ¡æ˜¯è¯†åˆ«æ–‡æœ¬çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸ªäººã€åœ°ç‚¹æˆ–ç»„ç»‡ç­‰å®ä½“â€”â€”ä½¿ç”¨ ğŸ¤— Transformers **pipeline()** åŠŸèƒ½ã€‚ç„¶åï¼Œåœ¨[Chapter 2](/course/chapter2)ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ç®¡é“å¦‚ä½•å°†ä»åŸå§‹æ–‡æœ¬ä¸­è·å–é¢„æµ‹æ‰€éœ€çš„ä¸‰ä¸ªé˜¶æ®µç»„åˆåœ¨ä¸€èµ·ï¼šæ ‡è®°åŒ–ã€é€šè¿‡æ¨¡å‹ä¼ é€’è¾“å…¥å’Œåå¤„ç†ã€‚å‰ä¸¤æ­¥ **token-classification** ç®¡é“ä¸ä»»ä½•å…¶ä»–ç®¡é“ç›¸åŒï¼Œä½†åå¤„ç†ç¨å¾®å¤æ‚ä¸€äº› - è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ï¼

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### é€šè¿‡ç®¡é“è·å¾—åŸºæœ¬ç»“æœ

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è·å–ä¸€ä¸ªæ ‡è®°åˆ†ç±»ç®¡é“ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æ¯”è¾ƒä¸€äº›ç»“æœã€‚é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹æ˜¯[dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english);å®ƒå¯¹å¥å­æ‰§è¡Œ NERï¼š

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

è¯¥æ¨¡å‹æ­£ç¡®åœ°å°†â€œSylvainâ€ç”Ÿæˆçš„æ¯ä¸ªæ ‡è®°è¯†åˆ«ä¸ºä¸€ä¸ªäººï¼Œå°†â€œHugging Faceâ€ç”Ÿæˆçš„æ¯ä¸ªæ ‡è®°è¯†åˆ«ä¸ºä¸€ä¸ªç»„ç»‡ï¼Œå°†â€œBrooklynâ€ç”Ÿæˆçš„æ ‡è®°è¯†åˆ«ä¸ºä¸€ä¸ªä½ç½®ã€‚æˆ‘ä»¬è¿˜å¯ä»¥è¦æ±‚ç®¡é“å°†å¯¹åº”äºåŒä¸€å®ä½“çš„ä»¤ç‰Œç»„åˆåœ¨ä¸€èµ·ï¼š

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

**aggregation_strategy** é€‰æ‹©å°†æ›´æ”¹ä¸ºæ¯ä¸ªåˆ†ç»„å®ä½“è®¡ç®—çš„åˆ†æ•°ã€‚å’Œ **simple** åˆ†æ•°åªæ˜¯ç»™å®šå®ä½“ä¸­æ¯ä¸ªæ ‡è®°çš„åˆ†æ•°çš„å¹³å‡å€¼ï¼šä¾‹å¦‚ï¼Œâ€œSylvainâ€çš„åˆ†æ•°æ˜¯æˆ‘ä»¬åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­çœ‹åˆ°çš„æ ‡è®°åˆ†æ•°çš„å¹³å‡å€¼ **S** , **##yl** , **##va** ï¼Œ å’Œ **##in** .å…¶ä»–å¯ç”¨çš„ç­–ç•¥æ˜¯ï¼š

- `"first"`, å…¶ä¸­æ¯ä¸ªå®ä½“çš„åˆ†æ•°æ˜¯è¯¥å®ä½“çš„ç¬¬ä¸€ä¸ªæ ‡è®°çš„åˆ†æ•°ï¼ˆå› æ­¤å¯¹äºâ€œSylvainâ€ï¼Œå®ƒå°†æ˜¯ 0.993828ï¼Œæ ‡è®°çš„åˆ†æ•°)

- `"max"`,å…¶ä¸­æ¯ä¸ªå®ä½“çš„åˆ†æ•°æ˜¯è¯¥å®ä½“ä¸­æ ‡è®°çš„æœ€å¤§åˆ†æ•°ï¼ˆå› æ­¤å¯¹äºâ€œHugging Faceâ€ï¼Œå®ƒå°†æ˜¯ 0.98879766ï¼Œå³â€œFaceâ€çš„åˆ†æ•°ï¼‰

- `"average"`, å…¶ä¸­æ¯ä¸ªå®ä½“çš„åˆ†æ•°æ˜¯ç»„æˆè¯¥å®ä½“çš„å•è¯åˆ†æ•°çš„å¹³å‡å€¼ï¼ˆå› æ­¤å¯¹äºâ€œSylvainâ€ï¼Œä¸â€œsimpleâ€ç­–ç•¥ï¼Œä½†â€œHugging Faceâ€çš„å¾—åˆ†ä¸º 0.9819ï¼Œâ€œHuggingâ€å¾—åˆ†çš„å¹³å‡å€¼ä¸º 0.975ï¼Œâ€œFaceâ€å¾—åˆ†ä¸º 0.98879ï¼‰

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ä¸ä½¿ç”¨pipelineï¼ˆï¼‰å‡½æ•°çš„æƒ…å†µä¸‹è·å¾—è¿™äº›ç»“æœï¼

### ä»è¾“å…¥åˆ°é¢„æµ‹

{#if fw === 'pt'}

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ ‡è®°æˆ‘ä»¬çš„è¾“å…¥å¹¶å°†å…¶ä¼ é€’ç»™æ¨¡å‹ã€‚è¿™æ˜¯å®Œå…¨æŒ‰ç…§[Chapter 2](/course/chapter2);æˆ‘ä»¬ä½¿ç”¨ **AutoXxx** ç±»ï¼Œç„¶ååœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä½¿ç”¨å®ƒä»¬ï¼š

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

ç”±äºæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ **AutoModelForTokenClassification** åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ºè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°è·å¾—ä¸€ç»„ logitsï¼š

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ ‡è®°æˆ‘ä»¬çš„è¾“å…¥å¹¶å°†å…¶ä¼ é€’ç»™æ¨¡å‹ã€‚è¿™æ˜¯å®Œå…¨æŒ‰ç…§[Chapter 2](/course/chapter2);æˆ‘ä»¬ä½¿ç”¨ **AutoXxx** ç±»ï¼Œç„¶ååœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä½¿ç”¨å®ƒä»¬ï¼š

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

äºæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ **AutoModelForTokenClassification** åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ºè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°è·å¾—ä¸€ç»„ logitsï¼š

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« 19 ä¸ªæ ‡è®°çš„ 1 ä¸ªåºåˆ—çš„æ‰¹æ¬¡ï¼Œæ¨¡å‹æœ‰ 9 ä¸ªä¸åŒçš„æ ‡ç­¾ï¼Œå› æ­¤æ¨¡å‹çš„è¾“å‡ºå…·æœ‰ 1 x 19 x 9 çš„å½¢çŠ¶ã€‚ä¸æ–‡æœ¬åˆ†ç±»ç®¡é“ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ softmax å‡½æ•°æ¥è½¬æ¢è¿™äº› logitsåˆ°æ¦‚ç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨ argmax æ¥è·å¾—é¢„æµ‹ï¼ˆè¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ logits ä¸Šé‡‡ç”¨ argmaxï¼Œå› ä¸º softmax ä¸ä¼šæ”¹å˜é¡ºåºï¼‰ï¼š

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

 **model.config.id2label** å±æ€§åŒ…å«ç´¢å¼•åˆ°æ ‡ç­¾çš„æ˜ å°„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥ç†è§£é¢„æµ‹ï¼š

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œæœ‰ 9 ä¸ªæ ‡ç­¾ï¼š **O** æ˜¯ä¸åœ¨ä»»ä½•å‘½åå®ä½“ä¸­çš„æ ‡è®°çš„æ ‡ç­¾ï¼ˆå®ƒä»£è¡¨â€œå¤–éƒ¨â€ï¼‰ï¼Œç„¶åæˆ‘ä»¬ä¸ºæ¯ç§ç±»å‹çš„å®ä½“ï¼ˆæ‚é¡¹ã€äººå‘˜ã€ç»„ç»‡å’Œä½ç½®ï¼‰æä¾›ä¸¤ä¸ªæ ‡ç­¾ã€‚æ ‡ç­¾ **B-XXX** è¡¨ç¤ºä»¤ç‰Œåœ¨å®ä½“çš„å¼€å¤´ **XXX** å’Œæ ‡ç­¾ **I-XXX** è¡¨ç¤ºä»¤ç‰Œåœ¨å®ä½“å†… **XXX** .ä¾‹å¦‚ï¼Œåœ¨å½“å‰ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹å¯¹ä»¤ç‰Œè¿›è¡Œåˆ†ç±» **S** ä½œä¸º **B-PER** ï¼ˆä¸€ä¸ªäººå®ä½“çš„å¼€å§‹ï¼‰å’Œä»¤ç‰Œ **##yl** , **##va** å’Œ **##in** ä½œä¸º **I-PER** ï¼ˆåœ¨ä¸ªäººå®ä½“å†…ï¼‰

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½è®¤ä¸ºæ¨¡å‹æ˜¯é”™è¯¯çš„ï¼Œå› ä¸ºå®ƒç»™å‡ºäº†æ ‡ç­¾ **I-PER** å¯¹æ‰€æœ‰è¿™å››ä¸ªä»¤ç‰Œï¼Œä½†è¿™å¹¶ä¸å®Œå…¨æ­£ç¡®ã€‚å®é™…ä¸Šæœ‰ä¸¤ç§æ ¼å¼ **B-** å’Œ **I-** æ ‡ç­¾ï¼šIOB1å’ŒIOB2. IOB2 æ ¼å¼ï¼ˆä¸‹é¢ç²‰çº¢è‰²ï¼‰æ˜¯æˆ‘ä»¬ä»‹ç»çš„æ ¼å¼ï¼Œè€Œåœ¨ IOB1 æ ¼å¼ï¼ˆè“è‰²ï¼‰ä¸­ï¼Œæ ‡ç­¾ä»¥ **B-** ä»…ç”¨äºåˆ†éš”ç›¸åŒç±»å‹çš„ä¸¤ä¸ªç›¸é‚»å®ä½“ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹åœ¨ä½¿ç”¨è¯¥æ ¼å¼çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¿™å°±æ˜¯å®ƒåˆ†é…æ ‡ç­¾çš„åŸå›  **I-PER** åˆ° **S** ä»¤ç‰Œã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

äº†è¿™å¼ åœ°å›¾ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½ï¼ˆå‡ ä¹å®Œå…¨ï¼‰é‡ç°ç¬¬ä¸€ä¸ªç®¡é“çš„ç»“æœâ€”â€”æˆ‘ä»¬å¯ä»¥è·å–æ¯ä¸ªæœªè¢«å½’ç±»ä¸ºçš„æ ‡è®°çš„åˆ†æ•°å’Œæ ‡ç­¾ **O** ï¼š

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

è¿™ä¸æˆ‘ä»¬ä¹‹å‰çš„æƒ…å†µéå¸¸ç›¸ä¼¼ï¼Œåªæœ‰ä¸€ä¸ªä¾‹å¤–ï¼šç®¡é“è¿˜ä¸ºæˆ‘ä»¬æä¾›äº†æœ‰å…³ **start** å’Œ **end** åŸå§‹å¥å­ä¸­çš„æ¯ä¸ªå®ä½“ã€‚è¿™æ˜¯æˆ‘ä»¬çš„åç§»æ˜ å°„å°†å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚è¦è·å¾—åç§»é‡ï¼Œæˆ‘ä»¬åªéœ€è¦è®¾ç½® **return_offsets_mapping=True** å½“æˆ‘ä»¬å°†åˆ†è¯å™¨åº”ç”¨äºæˆ‘ä»¬çš„è¾“å…¥æ—¶ï¼š

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

æ¯ä¸ªå…ƒç»„æ˜¯å¯¹åº”äºæ¯ä¸ªæ ‡è®°çš„æ–‡æœ¬è·¨åº¦ï¼Œå…¶ä¸­ **(0, 0)** ä¿ç•™ç”¨äºç‰¹æ®Šä»¤ç‰Œã€‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°ç´¢å¼• 5 å¤„çš„ä»¤ç‰Œæ˜¯ **##yl** ï¼Œ å…¶ä¸­æœ‰ **(12, 14)** ä½œä¸ºè¿™é‡Œçš„æŠµæ¶ˆã€‚å¦‚æœæˆ‘ä»¬åœ¨ç¤ºä¾‹ä¸­æŠ“å–ç›¸åº”çš„åˆ‡ç‰‡ï¼š


```py
example[12:14]
```

æˆ‘ä»¬å¾—åˆ°äº†æ­£ç¡®çš„æ–‡æœ¬è·¨åº¦ï¼Œè€Œæ²¡æœ‰ **##** ï¼š

```python out
yl
```

ä½¿ç”¨è¿™ä¸ªï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å®Œæˆä¹‹å‰çš„ç»“æœï¼š

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

è¿™å’Œæˆ‘ä»¬ä»ç¬¬ä¸€ä¸ªç®¡é“ä¸­å¾—åˆ°çš„ä¸€æ ·ï¼

### åˆ†ç»„å®ä½“

ä½¿ç”¨åç§»é‡æ¥ç¡®å®šæ¯ä¸ªå®ä½“çš„å¼€å§‹å’Œç»“æŸé”®å¾ˆæ–¹ä¾¿ï¼Œä½†è¯¥ä¿¡æ¯å¹¶ä¸æ˜¯ç»å¯¹å¿…è¦çš„ã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬æƒ³è¦å°†å®ä½“ç»„åˆåœ¨ä¸€èµ·æ—¶ï¼Œåç§»é‡å°†ä¸ºæˆ‘ä»¬èŠ‚çœå¤§é‡æ··ä¹±çš„ä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°†ä»¤ç‰Œç»„åˆåœ¨ä¸€èµ· **Hu** , **##gging** ï¼Œ å’Œ **Face** ï¼Œæˆ‘ä»¬å¯ä»¥åˆ¶å®šç‰¹æ®Šçš„è§„åˆ™ï¼Œè¯´å‰ä¸¤ä¸ªåº”è¯¥é™„åŠ ï¼ŒåŒæ—¶åˆ é™¤ **##** ï¼Œä»¥åŠ **Face** åº”è¯¥æ·»åŠ ä¸€ä¸ªç©ºæ ¼ï¼Œå› ä¸ºå®ƒä¸ä»¥ **##** â€” ä½†è¿™ä»…é€‚ç”¨äºè¿™ç§ç‰¹å®šç±»å‹çš„æ ‡è®°å™¨ã€‚æˆ‘ä»¬å¿…é¡»ä¸º SentencePiece æˆ– Byte-Pair-Encoding åˆ†è¯å™¨ï¼ˆæœ¬ç« ç¨åè®¨è®ºï¼‰ã€‚

ç¼–å†™å¦ä¸€ç»„è§„åˆ™ã€‚ä½¿ç”¨åç§»é‡ï¼Œæ‰€æœ‰è‡ªå®šä¹‰ä»£ç éƒ½æ¶ˆå¤±äº†ï¼šæˆ‘ä»¬å¯ä»¥åœ¨åŸå§‹æ–‡æœ¬ä¸­è·å–ä»ç¬¬ä¸€ä¸ªæ ‡è®°å¼€å§‹åˆ°æœ€åä¸€ä¸ªæ ‡è®°ç»“æŸçš„è·¨åº¦ã€‚æ‰€ä»¥ï¼Œåœ¨ä»¤ç‰Œçš„æƒ…å†µä¸‹ **Hu** , **##gging** ï¼Œ å’Œ **Face** ï¼Œæˆ‘ä»¬åº”è¯¥ä»å­—ç¬¦ 33ï¼ˆå¼€å§‹ **Hu** ) å¹¶åœ¨å­—ç¬¦ 45 ä¹‹å‰ç»“æŸï¼ˆç»“æŸ **Face** )ï¼š

```py
example[33:45]
```

```python out
Hugging Face
```

ä¸ºäº†ç¼–å†™åœ¨å¯¹å®ä½“è¿›è¡Œåˆ†ç»„çš„åŒæ—¶å¯¹é¢„æµ‹è¿›è¡Œåå¤„ç†çš„ä»£ç ï¼Œæˆ‘ä»¬å°†è¿ç»­å¹¶æ ‡è®°ä¸ºçš„å®ä½“åˆ†ç»„åœ¨ä¸€èµ· **I-XXX** ï¼Œé™¤äº†ç¬¬ä¸€ä¸ªï¼Œå¯ä»¥æ ‡è®°ä¸º **B-XXX** æˆ–è€… **I-XXX** ï¼ˆå› æ­¤ï¼Œå½“æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå®ä½“æ—¶ï¼Œæˆ‘ä»¬åœæ­¢å¯¹å®ä½“è¿›è¡Œåˆ†ç»„ **O** ï¼Œä¸€ç§æ–°å‹å®ä½“ï¼Œæˆ– **B-XXX** è¿™å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªç›¸åŒç±»å‹çš„å®ä½“æ­£åœ¨å¯åŠ¨ï¼‰ï¼š

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

æˆ‘ä»¬å¾—åˆ°äº†ä¸ç¬¬äºŒæ¡ç®¡é“ç›¸åŒçš„ç»“æœï¼

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

è¿™äº›åç§»é‡éå¸¸æœ‰ç”¨çš„å¦ä¸€ä¸ªä»»åŠ¡ç¤ºä¾‹æ˜¯é—®ç­”ã€‚æ·±å…¥ç ”ç©¶è¿™ä¸ªç®¡é“ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è¿›è¡Œï¼Œä¹Ÿå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿäº†è§£ ğŸ¤— Transformers åº“ä¸­æ ‡è®°å™¨çš„æœ€åä¸€ä¸ªåŠŸèƒ½ï¼šå½“æˆ‘ä»¬å°†è¾“å…¥æˆªæ–­ä¸ºç»™å®šé•¿åº¦æ—¶å¤„ç†æº¢å‡ºçš„æ ‡è®°ã€‚
