<FrameworkSwitchCourse {fw} />

# å¿«é€Ÿ tokenizer çš„ç‰¹æ®Šèƒ½åŠ› [[å¿«é€Ÿ tokenizer çš„ç‰¹æ®Šèƒ½åŠ›]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_tf.ipynb"},
]} />

{/if}

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»”ç»†ç ”ç©¶ ğŸ¤— Transformers ä¸­ tokenizer çš„åŠŸèƒ½ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªä½¿ç”¨å®ƒä»¬æ¥å¯¹æ–‡æœ¬è¿›è¡Œ tokenize æˆ–å°†token ID è§£ç å›æ–‡æœ¬ï¼Œä½†æ˜¯ tokenizer â€”â€” ç‰¹åˆ«æ˜¯ç”±ğŸ¤— Tokenizers åº“æ”¯æŒçš„ tokenizer â€”â€” èƒ½å¤Ÿåšçš„äº‹æƒ…è¿˜æœ‰å¾ˆå¤šã€‚ä¸ºäº†è¯´æ˜è¿™äº›é™„åŠ åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•å¤ç°åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­é¦–æ¬¡é‡åˆ°çš„ `token-classification` ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º `ner` ï¼‰ å’Œ `question-answering` ç®¡é“çš„ç»“æœã€‚

<Youtube id="g8quOxoqhHQ"/>

åœ¨æ¥ä¸‹æ¥çš„è®¨è®ºä¸­ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åŒºåˆ†â€œæ…¢é€Ÿâ€å’Œâ€œå¿«é€Ÿâ€ tokenizer ã€‚æ…¢é€Ÿ tokenizer æ˜¯åœ¨ ğŸ¤— Transformers åº“ä¸­ç”¨ Python ç¼–å†™çš„ï¼Œè€Œå¿«é€Ÿç‰ˆæœ¬æ˜¯ç”± ğŸ¤— Tokenizers æä¾›çš„ï¼Œå®ƒä»¬æ˜¯ç”¨ Rust ç¼–å†™çš„ã€‚å¦‚æœä½ è¿˜è®°å¾—åœ¨ [ç¬¬äº”ç« ](/course/chapter5/3) ä¸­å¯¹æ¯”äº†å¿«é€Ÿå’Œæ…¢é€Ÿ tokenizer å¯¹è¯ç‰©å®¡æŸ¥æ•°æ®é›†è¿›è¡Œ tokenize æ‰€éœ€çš„æ—¶é—´çš„è¿™å¼ è¡¨ï¼Œä½ åº”è¯¥ç†è§£ä¸ºä»€ä¹ˆæˆ‘ä»¬ç§°å®ƒä»¬ä¸ºâ€œå¿«é€Ÿâ€å’Œâ€œæ…¢é€Ÿâ€ï¼š

|               | å¿«é€Ÿ tokenizer      | æ…¢é€Ÿ tokenizer 
:--------------:|:--------------:|:-------------: `batched=True` | 10.8s          | 4min41s `batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

âš ï¸ å¯¹å•ä¸ªå¥å­è¿›è¡Œ tokenize æ—¶ï¼Œä½ ä¸æ€»æ˜¯èƒ½çœ‹åˆ°åŒä¸€ä¸ª tokenizer çš„æ…¢é€Ÿå’Œå¿«é€Ÿç‰ˆæœ¬ä¹‹é—´çš„é€Ÿåº¦å·®å¼‚ã€‚äº‹å®ä¸Šï¼Œå¿«é€Ÿç‰ˆæœ¬å¯èƒ½æ›´æ…¢ï¼åªæœ‰åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œ tokenize æ—¶ï¼Œä½ æ‰èƒ½æ¸…æ¥šåœ°çœ‹åˆ°å·®å¼‚ã€‚

</Tip>

## æ‰¹é‡ç¼–ç  [[æ‰¹é‡ç¼–ç ]]

<Youtube id="3umI3tm27Vw"/>

tokenizer çš„è¾“å‡ºä¸æ˜¯ç®€å•çš„ Python å­—å…¸ï¼›æˆ‘ä»¬å¾—åˆ°çš„å®é™…ä¸Šæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ `BatchEncoding` å¯¹è±¡ã€‚å®ƒæ˜¯å­—å…¸çš„å­ç±»ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¹‹å‰èƒ½å¤Ÿç›´æ¥ä½¿ç”¨ç´¢å¼•è·å–ç»“æœçš„åŸå› ï¼‰ï¼Œä½†æ˜¯å®ƒè¿˜æä¾›äº†ä¸€äº›ä¸»è¦ç”±å¿«é€Ÿ tokenizer æä¾›çš„é™„åŠ æ–¹æ³•ã€‚

é™¤äº†å®ƒä»¬çš„å¹¶è¡ŒåŒ–èƒ½åŠ›ä¹‹å¤–ï¼Œå¿«é€Ÿ tokenizer çš„å…³é”®åŠŸèƒ½æ˜¯å®ƒä»¬å§‹ç»ˆè·Ÿè¸ªæœ€ç»ˆ token ç›¸å¯¹äºçš„åŸå§‹æ–‡æœ¬çš„æ˜ å°„â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸º `åç§»æ˜ å°„ï¼ˆoffset mappingï¼‰` ã€‚è¿™åè¿‡æ¥åˆè§£é”äº†å¦‚å°†æ¯ä¸ªè¯æ˜ å°„åˆ°å®ƒç”Ÿæˆçš„ tokenï¼Œæˆ–è€…å°†åŸå§‹æ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦æ˜ å°„åˆ°å®ƒæ‰€åœ¨çš„ token ç­‰åŠŸèƒ½ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä» tokenizer å¾—åˆ°äº†ä¸€ä¸ª `BatchEncoding` å¯¹è±¡ï¼š

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

ç”±äº `AutoTokenizer` ç±»é»˜è®¤é€‰æ‹©å¿«é€Ÿ tokenizer å› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `BatchEncoding` å¯¹è±¡æä¾›çš„é™„åŠ æ–¹æ³•ã€‚æˆ‘ä»¬æœ‰ä¸¤ç§æ–¹æ³•æ¥æ£€æŸ¥æˆ‘ä»¬çš„ tokenizer æ˜¯å¿«é€Ÿçš„è¿˜æ˜¯æ…¢é€Ÿçš„ã€‚æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ `tokenizer` çš„ `is_fast` å±æ€§ï¼š

```python
tokenizer.is_fast
```

```python out
True
```

æˆ–æ£€æŸ¥æˆ‘ä»¬ `encoding` çš„ `is_fast` å±æ€§ï¼š

```python
encoding.is_fast
```

```python out
True
```

è®©æˆ‘ä»¬çœ‹çœ‹å¿«é€Ÿ tokenizer èƒ½è®©ä¸ºæˆ‘ä»¬æä¾›ä»€ä¹ˆåŠŸèƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¾—åˆ°Ttokenization ä¹‹å‰çš„å•è¯è€Œæ— éœ€å°† ID è½¬æ¢å›å•è¯ï¼š

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç´¢å¼• 5 å¤„çš„ token æ˜¯ `##yl` ï¼Œå®ƒæ˜¯åŸå§‹å¥å­ä¸­â€œSylvainâ€ä¸€è¯çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ `word_ids()` æ–¹æ³•æ¥è·å–æ¯ä¸ª token åŸå§‹å•è¯çš„ç´¢å¼•ï¼š

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° tokenizer çš„ç‰¹æ®Š token `[CLS]` å’Œ `[SEP]` è¢«æ˜ å°„åˆ° `None` ï¼Œç„¶åæ¯ä¸ª token éƒ½æ˜ å°„åˆ°å®ƒæ¥æºçš„å•è¯ã€‚è¿™å¯¹äºç¡®å®šä¸€ä¸ª token æ˜¯å¦åœ¨å•è¯çš„å¼€å¤´æˆ–ä¸¤ä¸ª token æ˜¯å¦åœ¨åŒä¸€ä¸ªå•è¯ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚å¯¹äº BERT ç±»å‹ï¼ˆBERT-likeï¼‰çš„çš„ tokenizer æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¾é  `##` å‰ç¼€æ¥å®ç°è¿™ä¸ªåŠŸèƒ½ï¼›ä¸è¿‡åªè¦æ˜¯å¿«é€Ÿ tokenizer å®ƒæ‰€æä¾›çš„ `word_ids()` æ–¹æ³•é€‚ç”¨äºä»»ä½•ç±»å‹çš„ tokenizer ã€‚åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åˆ©ç”¨è¿™ç§èƒ½åŠ›ï¼Œå°†æˆ‘ä»¬ä¸ºæ¯ä¸ªè¯æ­£ç¡®åœ°å¯¹åº”åˆ°è¯æ±‡ä»»åŠ¡ä¸­çš„æ ‡ç­¾ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å®ƒåœ¨æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆmasked language modelingï¼‰ä¸­æ¥é®ç›–æ¥è‡ªåŒä¸€è¯çš„æ‰€æœ‰ tokenï¼ˆä¸€ç§ç§°ä¸º `å…¨è¯æ©ç ï¼ˆwhole word maskingï¼‰` çš„æŠ€æœ¯ï¼‰ã€‚

<Tip>

è¯çš„æ¦‚å¿µæ˜¯å¤æ‚çš„ã€‚ä¾‹å¦‚ï¼Œâ€œI'llâ€ï¼ˆâ€œI willâ€çš„ç¼©å†™ï¼‰ç®—ä½œä¸€ä¸ªè¯è¿˜æ˜¯ä¸¤ä¸ªè¯ï¼Ÿè¿™å®é™…ä¸Šå–å†³äº tokenizer å’Œå®ƒé‡‡ç”¨çš„é¢„åˆ†è¯æ“ä½œã€‚æœ‰äº› tokenizer åªåœ¨ç©ºæ ¼å¤„åˆ†å‰²ï¼Œæ‰€ä»¥å®ƒä»¬ä¼šæŠŠè¿™ä¸ªçœ‹ä½œæ˜¯ä¸€ä¸ªè¯ã€‚æœ‰äº›å…¶ä»– tokenizer åœ¨ç©ºæ ¼çš„åŸºç¡€ä¹‹ä¸Šè¿˜ä½¿ç”¨æ ‡ç‚¹ï¼Œæ‰€ä»¥ä¼šè®¤ä¸ºå®ƒæ˜¯ä¸¤ä¸ªè¯ã€‚

âœï¸ **è¯•è¯•çœ‹ï¼**ä» `bert base cased` å’Œ `roberta base` checkpoint åˆ›å»ºä¸€ä¸ª tokenizer å¹¶ç”¨å®ƒä»¬å¯¹â€œ81sâ€è¿›è¡Œåˆ†è¯ã€‚ä½ è§‚å¯Ÿåˆ°äº†ä»€ä¹ˆï¼Ÿè¿™äº›è¯çš„ ID æ˜¯ä»€ä¹ˆï¼Ÿ

</Tip>

åŒæ ·ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ª `sentence_ids()` æ–¹æ³•ï¼Œå¯ä»¥ç”¨å®ƒæŠŠä¸€ä¸ª token æ˜ å°„åˆ°å®ƒåŸå§‹çš„å¥å­ï¼ˆå°½ç®¡åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œtokenizer è¿”å›çš„ `token_type_ids`ä¹Ÿå¯ä»¥ä¸ºæˆ‘ä»¬æä¾›ç›¸åŒçš„ä¿¡æ¯ï¼‰ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ `word_to_chars()` æˆ– `token_to_chars()` å’Œ `char_to_word()` æˆ– `char_to_token()` æ–¹æ³•ï¼Œå°†ä»»ä½•è¯æˆ– token æ˜ å°„åˆ°åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ï¼Œåä¹‹äº¦ç„¶ã€‚ä¾‹å¦‚ï¼Œ `word_ids()` æ–¹æ³•å‘Šè¯‰æˆ‘ä»¬ `##yl` æ˜¯ç´¢å¼• 3 å¤„å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œä½†å®ƒæ˜¯å¥å­ä¸­çš„å“ªä¸ªå•è¯ï¼Ÿæˆ‘ä»¬å¯ä»¥è¿™æ ·æ‰¾å‡ºæ¥ï¼š

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

å¦‚å‰æ‰€è¿°ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯ç”±äºå¿«é€Ÿåˆ†è¯å™¨è·Ÿè¸ªæ¯ä¸ª token æ¥è‡ªçš„æ–‡æœ¬èŒƒå›´çš„ä¸€ç»„*åç§»*ã€‚ä¸ºäº†é˜æ˜å®ƒä»¬çš„ä½œç”¨ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•æ‰‹åŠ¨å¤ç° `token-classification` ç®¡é“çš„ç»“æœã€‚


<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** ä½¿ç”¨è‡ªå·±çš„æ–‡æœ¬ï¼Œçœ‹çœ‹ä½ æ˜¯å¦èƒ½ç†è§£å“ªäº› token ä¸å•è¯ ID ç›¸å…³è”ï¼Œä»¥åŠå¦‚ä½•æå–å•ä¸ªå•è¯çš„å­—ç¬¦è·¨åº¦ã€‚é™„åŠ é¢˜ï¼šè¯·å°è¯•ä½¿ç”¨ä¸¤ä¸ªå¥å­ä½œä¸ºè¾“å…¥ï¼Œçœ‹çœ‹å¥å­ ID æ˜¯å¦å¯¹ä½ æœ‰æ„ä¹‰ã€‚

</Tip>

## `token-classification` ç®¡é“å†…éƒ¨æµç¨‹ [[`token-classification`ç®¡é“å†…éƒ¨æµç¨‹]]

åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) æˆ‘ä»¬åˆæ¬¡å°è¯•å®ç°å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰â€”â€”è¯¥ä»»åŠ¡æ˜¯ç¡®å®šæ–‡æœ¬çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºäººåã€åœ°åæˆ–ç»„ç»‡åç­‰å®ä½“â€”â€”å½“æ—¶æ˜¯ä½¿ç”¨ğŸ¤— Transformers çš„ `pipeline()` å‡½æ•°å®ç°çš„ã€‚ç„¶åï¼Œåœ¨ [ç¬¬äºŒç« ](/course/chapter2) ï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªç®¡é“å¦‚ä½•å°†è·å–åŸå§‹æ–‡æœ¬åˆ°é¢„æµ‹ç»“æœçš„ä¸‰ä¸ªé˜¶æ®µæ•´åˆåœ¨ä¸€èµ·ï¼štokenizeã€é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥å’Œåå¤„ç†ã€‚ `token-classification` ç®¡é“ä¸­çš„å‰ä¸¤æ­¥ä¸å…¶ä»–ä»»ä½•ç®¡é“ä¸­çš„æ­¥éª¤ç›¸åŒï¼Œä½†åå¤„ç†ç¨æœ‰å¤æ‚â€”â€”è®©æˆ‘ä»¬çœ‹çœ‹å…·ä½“æƒ…å†µï¼

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### ä½¿ç”¨ç®¡é“è·å¾—åŸºæœ¬ç»“æœ [[ä½¿ç”¨ç®¡é“è·å¾—åŸºæœ¬ç»“æœ]]

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è·å–ä¸€ä¸ª token åˆ†ç±»ç®¡é“ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æ¯”è¾ƒä¸€äº›ç»“æœã€‚è¿™æ¬¡æˆ‘ä»¬é€‰ç”¨çš„æ¨¡å‹æ˜¯ [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english) ï¼›æˆ‘ä»¬ä½¿ç”¨å®ƒå¯¹å¥å­è¿›è¡Œ NERï¼š

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

æ¨¡å‹æ­£ç¡®åœ°è¯†åˆ«å‡ºï¼šâ€œSylvainâ€æ˜¯ä¸€ä¸ªäººï¼Œâ€œHugging Faceâ€æ˜¯ä¸€ä¸ªç»„ç»‡ï¼Œä»¥åŠâ€œBrooklynâ€æ˜¯ä¸€ä¸ªåœ°ç‚¹ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥è®©ç®¡é“å°†åŒä¸€å®ä½“çš„ token ç»„åˆåœ¨ä¸€èµ·ï¼š

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

é€‰æ‹©ä¸åŒçš„ `aggregation_strategy` å¯ä»¥æ›´æ”¹æ¯ä¸ªåˆ†ç»„å®ä½“è®¡ç®—çš„ç­–ç•¥ã€‚å¯¹äº `simple` ç­–ç•¥ï¼Œæœ€ç»ˆçš„åˆ†æ•°å°±æ˜¯ç»™å®šå®ä½“ä¸­æ¯ä¸ª token çš„åˆ†æ•°çš„å¹³å‡å€¼ï¼šä¾‹å¦‚ï¼Œâ€œSylvainâ€çš„åˆ†æ•°æ˜¯æˆ‘ä»¬åœ¨å‰ä¸€ä¸ªä¾‹å­ä¸­çœ‹åˆ°çš„ token `S` ï¼Œ `##yl` ï¼Œ `##va` ï¼Œå’Œ `##in` çš„åˆ†æ•°çš„å¹³å‡å€¼ã€‚å…¶ä»–å¯ç”¨çš„ç­–ç•¥åŒ…æ‹¬ï¼š

- â€œfirstâ€ï¼Œå…¶ä¸­æ¯ä¸ªå®ä½“çš„åˆ†æ•°æ˜¯è¯¥å®ä½“çš„ç¬¬ä¸€ä¸ª token çš„åˆ†æ•°ï¼ˆå› æ­¤å¯¹äºâ€œSylvainâ€ï¼Œåˆ†æ•°å°†æ˜¯ 0.993828ï¼Œè¿™æ˜¯â€œSâ€çš„åˆ†æ•°ï¼‰
- â€œmaxâ€ï¼Œå…¶ä¸­æ¯ä¸ªå®ä½“çš„åˆ†æ•°æ˜¯è¯¥å®ä½“ä¸­ token çš„æœ€å¤§åˆ†æ•°ï¼ˆå› æ­¤å¯¹äºâ€œHugging Faceâ€ï¼Œåˆ†æ•°å°†æ˜¯ 0.98879766ï¼Œå³â€œFaceâ€çš„åˆ†æ•°ï¼‰
- â€œaverageâ€ï¼Œå…¶ä¸­æ¯ä¸ªå®ä½“çš„åˆ†æ•°æ˜¯ç»„æˆè¯¥å®ä½“çš„å•è¯åˆ†æ•°çš„å¹³å‡å€¼ï¼ˆå› æ­¤å¯¹äºâ€œSylvainâ€ï¼Œä¸â€œsimpleâ€ç­–ç•¥ç›¸åŒï¼Œä½†â€œHugging Faceâ€çš„å¾—åˆ†å°†æ˜¯ 0.9819ï¼Œè¿™æ˜¯â€œHuggingâ€çš„åˆ†æ•° 0.975 å’Œâ€œFaceâ€çš„åˆ†æ•° 0.98879 çš„å¹³å‡å€¼ï¼‰

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ä¸ä½¿ç”¨ `pipeline()` å‡½æ•°çš„æƒ…å†µä¸‹è·å¾—è¿™äº›ç»“æœï¼

### ä»è¾“å…¥åˆ°é¢„æµ‹ [[ä»è¾“å…¥åˆ°é¢„æµ‹]]

{#if fw === 'pt'}

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„è¾“å…¥è¿›è¡Œ tokenization å¹¶å°†å…¶ä¼ é€’ç»™æ¨¡å‹ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ [ç¬¬äºŒç« ](/course/chapter2) ä¸­çš„æ–¹æ³•å®Œå…¨ç›¸åŒï¼›æˆ‘ä»¬ä½¿ç”¨ `AutoXxx` ç±»å®ä¾‹åŒ– tokenizer å’Œæ¨¡å‹ï¼Œç„¶åå°†æˆ‘ä»¬çš„ç¤ºä¾‹ä¼ é€’ç»™å®ƒä»¬ï¼š

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

ç”±äºæˆ‘ä»¬åœ¨æ­¤ä½¿ç”¨äº† `AutoModelForTokenClassification` ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾—åˆ°äº†è¾“å…¥åºåˆ—ä¸­æ¯ä¸ª token çš„ä¸€ç»„ logitsï¼š

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æˆ‘ä»¬çš„è¾“å…¥ tokenization å¹¶å°†å…¶ä¼ é€’ç»™æ¨¡å‹ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ [ç¬¬äºŒç« ](/course/chapter2) ä¸­çš„æ–¹æ³•å®Œå…¨ç›¸åŒï¼›æˆ‘ä»¬ä½¿ç”¨ `TFAutoXxx` ç±»å®ä¾‹åŒ– tokenizer å’Œæ¨¡å‹ï¼Œç„¶åå°†æˆ‘ä»¬çš„ç¤ºä¾‹ä¼ é€’ç»™å®ƒä»¬ï¼š

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

ç”±äºæˆ‘ä»¬åœ¨æ­¤ä½¿ç”¨äº† `TFAutoModelForTokenClassification` ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾—åˆ°äº†è¾“å…¥åºåˆ—ä¸­æ¯ä¸ª token çš„ä¸€ç»„ logitsï¼š

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« 19 ä¸ª token åºåˆ—çš„ batch å’Œæœ‰ 9 ä¸ªä¸åŒçš„æ ‡ç­¾ç±»å‹ï¼Œæ‰€ä»¥æ¨¡å‹çš„è¾“å‡ºå½¢çŠ¶ä¸º 1 x 19 x 9ã€‚åƒæ–‡æœ¬åˆ†ç±»ç®¡é“ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ softmax å‡½æ•°å°†è¿™äº› logits è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œå¹¶å– argmax æ¥å¾—åˆ°é¢„æµ‹ï¼ˆè¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ logits ä¸Šç›´æ¥ç®—å– argmaxï¼Œå› ä¸º softmax ä¸ä¼šæ”¹å˜é¡ºåºï¼‰ï¼š

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

`model.config.id2label` å±æ€§åŒ…å«ç´¢å¼•åˆ°æ ‡ç­¾çš„æ˜ å°„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å°†é¢„æµ‹è½¬åŒ–ä¸ºæ ‡ç­¾ï¼š

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

å¦‚å‰æ‰€è¿°ï¼Œè¿™é‡Œæœ‰ 9 ä¸ªæ ‡ç­¾ï¼š `O` æ˜¯ä¸åœ¨ä»»ä½•å®ä½“ä¸­çš„ token çš„æ ‡ç­¾ï¼ˆå®ƒä»£è¡¨â€œoutsideâ€ï¼‰ï¼Œç„¶åæˆ‘ä»¬ä¸ºæ¯ç§ç±»å‹çš„å®ä½“ï¼ˆæ‚é¡¹ã€äººå‘˜ã€ç»„ç»‡å’Œä½ç½®ï¼‰æä¾›ä¸¤ä¸ªæ ‡ç­¾ï¼šæ ‡ç­¾ `B-XXX` è¡¨ç¤º token åœ¨å®ä½“ `XXX` çš„å¼€å¤´ï¼Œæ ‡ç­¾ `I-XXX` è¡¨ç¤º token åœ¨å®ä½“ `XXX` å†…éƒ¨ã€‚ä¾‹å¦‚ï¼Œåœ¨å½“å‰çš„ä¾‹å­ï¼Œæˆ‘ä»¬æœŸæœ›æˆ‘ä»¬çš„æ¨¡å‹å°† token `S` åˆ†ç±»ä¸º `B-PER` ï¼ˆäººç‰©å®ä½“çš„å¼€å§‹ï¼‰ï¼Œå¹¶ä¸”å°† token `##yl` ï¼Œ `##va` å’Œ `##in` åˆ†ç±»ä¸º `I-PER` ï¼ˆäººç‰©å®ä½“çš„å†…éƒ¨ï¼‰

ä½ å¯èƒ½ä¼šè§‰å¾—ä¸Šé¢æ¨¡å‹çš„è¾“å‡ºæ˜¯é”™è¯¯çš„ï¼Œå› ä¸ºå®ƒç»™æ‰€æœ‰è¿™å››ä¸ª token éƒ½æ ‡ä¸Šäº† `I-PER` æ ‡ç­¾ï¼Œä½†è¿™æ ·ç†è§£å¹¶ä¸å®Œå…¨æ­£ç¡®ã€‚å¯¹äº `B-` å’Œ `I-` æ ‡ç­¾ï¼Œå®é™…ä¸Šæœ‰ä¸¤ç§æ ¼å¼ï¼šIOB1 å’Œ IOB2ã€‚æˆ‘ä»¬ä»‹ç»çš„æ˜¯ IOB2 æ ¼å¼ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºçš„ç²‰è‰²ï¼‰ï¼Œè€Œåœ¨ IOB1 æ ¼å¼ï¼ˆè“è‰²ï¼‰ä¸­ï¼Œä»¥ `B-` å¼€å¤´çš„æ ‡ç­¾åªç”¨äºåˆ†éš”åŒä¸€ç±»å‹çš„ä¸¤ä¸ªç›¸é‚»å®ä½“ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹åœ¨ä½¿ç”¨è¯¥æ ¼å¼çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå°† `S` token æ ‡ä¸Šäº† `I-PER` æ ‡ç­¾çš„åŸå› ã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

æœ‰äº†è¿™ä¸ªæ˜ å°„å­—å…¸ï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡ ä¹å®Œå…¨å¤ç°ç®¡é“çš„ç»“æœ â€”â€” æˆ‘ä»¬åªéœ€è¦è·å–æ¯ä¸ªæ²¡æœ‰è¢«åˆ†ç±»ä¸º O çš„ token çš„å¾—åˆ†å’Œæ ‡ç­¾ï¼š

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

è¿™ä¸æˆ‘ä»¬ä¹‹å‰çš„ç»“æœéå¸¸ç›¸ä¼¼ï¼Œä½†æœ‰ä¸€ç‚¹ä¸åŒï¼špipeline è¿˜ç»™æˆ‘ä»¬æä¾›äº†æ¯ä¸ªå®ä½“åœ¨åŸå§‹å¥å­ä¸­çš„ `start` å’Œ `end` çš„ä¿¡æ¯ã€‚å¦‚æœè¦å¤ç°è¿™ä¸ªç‰¹æ€§ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„åç§»æ˜ å°„è¦å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚è¦è·å¾—åç§»é‡ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨ä½¿ç”¨ tokenizer å™¨æ—¶è®¾ç½® `return_offsets_mapping=True` ï¼š

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

æ¯ä¸ªå…ƒç»„éƒ½æ˜¯æ¯ä¸ª token å¯¹åº”çš„æ–‡æœ¬èŒƒå›´ï¼Œå…¶ä¸­ `(0, 0)` æ˜¯ä¸ºç‰¹æ®Š token ä¿ç•™çš„ã€‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°ç´¢å¼• 5 çš„ token æ˜¯ `##yl` ï¼Œå®ƒæ‰€å¯¹åº”çš„åç§»é‡æ˜¯ `(12, 14)` ã€‚å¦‚æœæˆ‘ä»¬æˆªå–æˆ‘ä»¬ä¾‹å­ä¸­çš„å¯¹åº”ç‰‡æ®µï¼š


```py
example[12:14]
```

æˆ‘ä»¬ä¼šå¾—åˆ°æ²¡æœ‰ `##` çš„æ–‡æœ¬ï¼š

```python out
yl
```

ä½¿ç”¨è¿™ä¸ªï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å®Œæˆä¹‹å‰çš„æƒ³æ³•ï¼š

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

æˆ‘ä»¬å¾—åˆ°äº†ä¸ç¬¬ä¸€æ¡ pipeline ç›¸åŒçš„ç»“æœï¼

### å®ä½“åˆ†ç»„ [[å®ä½“åˆ†ç»„]]

ä½¿ç”¨åç§»æ¥ç¡®å®šæ¯ä¸ªå®ä½“çš„å¼€å§‹å’Œç»“æŸçš„ç´¢å¼•å¾ˆæ–¹ä¾¿ï¼Œä½†è¿™å¹¶ä¸æ˜¯å®ƒå”¯ä¸€çš„ç”¨æ³•ã€‚å½“æˆ‘ä»¬å¸Œæœ›å°†å®ä½“åˆ†ç»„åœ¨ä¸€èµ·æ—¶ï¼Œåç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬çœå»å¾ˆå¤šå¤æ‚çš„ä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°† `Hu` ã€ `##gging` å’Œ `Face` token åˆ†ç»„åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¯ä»¥åˆ¶å®šç‰¹æ®Šè§„åˆ™ï¼Œæ¯”å¦‚è¯´å‰ä¸¤ä¸ªåº”è¯¥åœ¨å»é™¤ `##` çš„åŒæ—¶è¿åœ¨ä¸€èµ·ï¼Œ `Face` åº”è¯¥åœ¨ä¸ä»¥ `##` å¼€å¤´çš„æƒ…å†µä¸‹å¢åŠ ç©ºæ ¼ â€”â€” ä½†è¿™äº›è§„åˆ™åªé€‚ç”¨äºè¿™ç§ç‰¹å®šç±»å‹çš„åˆ†è¯å™¨ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ SentencePiece æˆ– Byte-Pair-Encoding åˆ†è¯å™¨ï¼ˆåœ¨æœ¬ç« åé¢è®¨è®ºï¼‰æ—¶å°±è¦é‡æ–°å†™å¦å¤–ä¸€å¥—è§„åˆ™ã€‚

æœ‰äº†åç§»é‡ï¼Œå°±å¯ä»¥å…å»ä¸ºç‰¹å®šåˆ†è¯å™¨å®šåˆ¶ç‰¹æ®Šçš„åˆ†ç»„è§„åˆ™ï¼šæˆ‘ä»¬åªéœ€è¦å–åŸå§‹æ–‡æœ¬ä¸­ä»¥ç¬¬ä¸€ä¸ª token å¼€å§‹å’Œæœ€åä¸€ä¸ª token ç»“æŸçš„èŒƒå›´ã€‚å› æ­¤ï¼Œå‡å¦‚è¯´æˆ‘ä»¬æœ‰ `Hu` ã€ `##gging` å’Œ `Face` tokenï¼Œæˆ‘ä»¬åªéœ€è¦ä»å­—ç¬¦ 33ï¼ˆ `Hu` çš„å¼€å§‹ï¼‰æˆªå–åˆ°å­—ç¬¦ 45ï¼ˆ `Face` çš„ç»“æŸï¼‰ï¼š


```py
example[33:45]
```

```python out
Hugging Face
```

ä¸ºäº†ç¼–å†™å¤„ç†é¢„æµ‹ç»“æœå¹¶åˆ†ç»„å®ä½“çš„ä»£ç ï¼Œæˆ‘ä»¬å°†å¯¹è¿ç»­æ ‡è®°ä¸º `I-XXX` çš„å®ä½“è¿›è¡Œåˆ†ç»„ï¼Œå› ä¸ºåªæœ‰å®ä½“çš„ç¬¬ä¸€ä¸ª token å¯ä»¥è¢«æ ‡è®°ä¸º `B-XXX` æˆ– `I-XXX` ï¼Œå› æ­¤ï¼Œå½“æˆ‘ä»¬é‡åˆ°å®ä½“ `O` ã€æ–°ç±»å‹çš„å®ä½“æˆ– `B-XXX` æ—¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœæ­¢èšåˆåŒä¸€ç±»å‹å®ä½“ã€‚

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # åˆ é™¤ B- æˆ–è€… I-
        label = label[2:]
        start, _ = offsets[idx]

        # è·å–æ‰€æœ‰æ ‡æœ‰ I æ ‡ç­¾çš„token
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # åˆ†æ•°æ˜¯è¯¥åˆ†ç»„å®ä½“ä¸­æ‰€æœ‰tokenåˆ†æ•°çš„å¹³å‡å€¼
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

æˆ‘ä»¬å¾—åˆ°äº†ä¸ç¬¬äºŒæ¡ pipeline ç›¸åŒçš„ç»“æœï¼

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

å¦ä¸€ä¸ªéå¸¸éœ€è¦åç§»é‡çš„ä»»åŠ¡é¢†åŸŸæ˜¯é—®ç­”ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†æ·±å…¥æ¢ç´¢è¿™ä¸ª pipelineã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°ğŸ¤— Transformers åº“ä¸­ tokenizer çš„æœ€åä¸€ä¸ªç‰¹æ€§ï¼šåœ¨æˆ‘ä»¬å°†è¾“å…¥è¶…è¿‡ç»™å®šé•¿åº¦æ—¶ï¼Œå¤„ç†æº¢å‡ºçš„ tokensã€‚