# WordPiece tokenization ç®—æ³•   [[WordPiece tokenizationç®—æ³•]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section6.ipynb"},
]} />

WordPiece æ˜¯ Google å¼€å‘çš„ç”¨äº BERT é¢„è®­ç»ƒçš„åˆ†è¯ç®—æ³•ã€‚è‡ªæ­¤ä¹‹åï¼Œå¾ˆå¤šåŸºäº BERT çš„ Transformer æ¨¡å‹éƒ½å¤ç”¨äº†è¿™ç§æ–¹æ³•ï¼Œæ¯”å¦‚ DistilBERTï¼ŒMobileBERTï¼ŒFunnel Transformers å’Œ MPNETã€‚å®ƒåœ¨è®­ç»ƒæ–¹é¢ä¸ BPE éå¸¸ç±»ä¼¼ï¼Œä½†å®é™…çš„åˆ†è¯æ–¹æ³•æœ‰æ‰€ä¸åŒã€‚

<Youtube id="qpv6ms_t_1A"/>

> [!TIP]
> ğŸ’¡ æœ¬èŠ‚è¯¦ç»†è®²è¿°äº† WordPieceï¼Œç”šè‡³å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„å®ç°ã€‚å¦‚æœä½ åªæƒ³å¯¹è¿™ä¸ªåˆ†è¯ç®—æ³•æœ‰ä¸ªå¤§æ¦‚çš„ç†è§£ï¼Œå¯ä»¥ç›´æ¥è·³åˆ°æœ€åã€‚

## WordPiece è®­ç»ƒ [[WordPiece è®­ç»ƒ]]

> [!WARNING]
> âš ï¸ Google ä»æœªå¼€æº WordPiece è®­ç»ƒç®—æ³•çš„å®ç°ï¼Œå› æ­¤ä»¥ä¸‹æ˜¯æˆ‘ä»¬åŸºäºå·²å‘è¡¨æ–‡çŒ®çš„æœ€ä½³çŒœæµ‹ã€‚å®ƒå¯èƒ½å¹¶é 100ï¼… å‡†ç¡®çš„ã€‚

ä¸BPE ä¸€æ ·ï¼ŒWordPiece ä¹Ÿæ˜¯ä»åŒ…å«æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokens å’Œåˆå§‹å­—æ¯è¡¨çš„å°è¯æ±‡è¡¨å¼€å§‹çš„ã€‚ç”±äºå®ƒæ˜¯é€šè¿‡æ·»åŠ å‰ç¼€ï¼ˆå¦‚ BERT ä¸­çš„ `##` ï¼‰æ¥è¯†åˆ«å­è¯çš„ï¼Œæ¯ä¸ªè¯æœ€åˆéƒ½ä¼šé€šè¿‡åœ¨è¯å†…éƒ¨æ‰€æœ‰å­—ç¬¦å‰æ·»åŠ è¯¥å‰ç¼€è¿›è¡Œåˆ†å‰²ã€‚å› æ­¤ï¼Œä¾‹å¦‚ `"word"` å°†è¢«è¿™æ ·åˆ†å‰²ï¼š

```
w ##o ##r ##d
```

å› æ­¤ï¼Œåˆå§‹å­—æ¯è¡¨åŒ…å«æ‰€æœ‰å‡ºç°åœ¨å•è¯ç¬¬ä¸€ä¸ªä½ç½®çš„å­—ç¬¦ï¼Œä»¥åŠå‡ºç°åœ¨å•è¯å†…éƒ¨å¹¶å¸¦æœ‰ WordPiece å‰ç¼€çš„å­—ç¬¦ã€‚

ç„¶åï¼ŒåŒæ ·åƒ BPE ä¸€æ ·ï¼ŒWordPiece ä¼šå­¦ä¹ åˆå¹¶è§„åˆ™ã€‚ä¸»è¦çš„ä¸åŒä¹‹å¤„åœ¨äºåˆå¹¶å¯¹çš„é€‰æ‹©æ–¹å¼ã€‚WordPiece ä¸æ˜¯é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å¯¹ï¼Œè€Œæ˜¯å¯¹æ¯å¯¹è®¡ç®—ä¸€ä¸ªå¾—åˆ†ï¼Œä½¿ç”¨ä»¥ä¸‹å…¬å¼ï¼š


$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times\mathrm{freq\_of\_second\_element})$$

é€šè¿‡å°†ä¸¤éƒ¨åˆ†åˆåœ¨ä¸€èµ·çš„é¢‘ç‡é™¤ä»¥å…¶ä¸­å„éƒ¨åˆ†çš„é¢‘ç‡çš„ä¹˜ç§¯ï¼Œè¯¥ç®—æ³•ä¼˜å…ˆåˆå¹¶é‚£äº›åœ¨è¯æ±‡è¡¨ä¸­å•ç‹¬å‡ºç°å‡ºç°çš„å¯¹ã€‚ä¾‹å¦‚ï¼Œå³ä½¿ `("un", "##able")` è¿™å¯¹åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°çš„é¢‘ç‡å¾ˆé«˜ï¼Œå®ƒä¹Ÿä¸ä¸€å®šä¼šè¢«åˆå¹¶ï¼Œå› ä¸º `"un"` å’Œ `"##able"` è¿™ä¸¤å¯¹å¯èƒ½ä¼šåœ¨å¾ˆå¤šå…¶ä»–è¯ä¸­å‡ºç°ï¼Œé¢‘ç‡å¾ˆé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåƒ `("hu", "##gging")` è¿™æ ·çš„å¯¹å¯èƒ½ä¼šæ›´å¿«åœ°è¢«åˆå¹¶ï¼ˆå‡è®¾å•è¯â€œhuggingâ€åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°çš„é¢‘ç‡å¾ˆé«˜ï¼‰ï¼Œå› ä¸º `"hu"` å’Œ `"##gging"` å¯èƒ½åˆ†åˆ«å‡ºç°çš„é¢‘ç‡è¾ƒä½ã€‚

æˆ‘ä»¬ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ç›¸åŒçš„è¯æ±‡è¡¨ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ç»è¿‡åˆ†å‰²ä¹‹åå°†ä¼šæ˜¯ï¼š

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

æ‰€ä»¥æœ€åˆçš„è¯æ±‡è¡¨å°†ä¼šæ˜¯ `["b", "h", "p", "##g", "##n", "##s", "##u"]` ï¼ˆå¦‚æœæˆ‘ä»¬æš‚æ—¶å¿½ç•¥ç‰¹æ®Š tokens ï¼‰ã€‚å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸€å¯¹æ˜¯ `("##u", "##g")` ï¼ˆç›®å‰ 20 æ¬¡ï¼‰ï¼Œä½† `"##u"` å’Œå…¶ä»–å•è¯ä¸€èµ·å‡ºç°çš„é¢‘ç‡éå¸¸é«˜ï¼Œæ‰€ä»¥å®ƒçš„åˆ†æ•°ä¸æ˜¯æœ€é«˜çš„ï¼ˆåˆ†æ•°æ˜¯ 1 / 36ï¼‰ã€‚æ‰€æœ‰å¸¦æœ‰ `"##u"` çš„å¯¹å®é™…ä¸Šéƒ½æœ‰ç›¸åŒçš„åˆ†æ•°ï¼ˆ1 / 36ï¼‰ï¼Œæ‰€ä»¥åˆ†æ•°æœ€é«˜çš„å¯¹æ˜¯ `("##g", "##s")` â€”â€” å”¯ä¸€æ²¡æœ‰ `"##u"` çš„å¯¹â€”â€”åˆ†æ•°æ˜¯ 1 / 20ï¼Œæ‰€ä»¥å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶æ˜¯ `("##g", "##s") -> ("##gs")` ã€‚

è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬åˆå¹¶æ—¶ï¼Œæˆ‘ä»¬ä¼šåˆ é™¤ä¸¤ä¸ª tokens ä¹‹é—´çš„ `##` ï¼Œæ‰€ä»¥æˆ‘ä»¬å°† `"##gs"` æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œå¹¶å°†è¯­æ–™åº“çš„å•è¯æŒ‰ç…§æ”¹è§„åˆ™è¿›è¡Œåˆå¹¶ï¼š

```
è¯æ±‡è¡¨: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
è¯­æ–™åº“: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

æ­¤æ—¶ï¼Œ `"##u"` å‡ºç°åœ¨æ‰€æœ‰å¯èƒ½çš„å¯¹ä¸­ï¼Œå› æ­¤å®ƒä»¬æœ€ç»ˆéƒ½å…·æœ‰ç›¸åŒçš„åˆ†æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªå¯¹ä¼šè¢«åˆå¹¶ï¼Œäºæ˜¯æˆ‘ä»¬å¾—åˆ°äº† `("h", "##u") -> "hu"` è§„åˆ™ï¼š

```
è¯æ±‡è¡¨: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
è¯­æ–™åº“: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ç„¶åï¼Œä¸‹ä¸€ä¸ªæœ€ä½³å¾—åˆ†çš„å¯¹æ˜¯ `("hu", "##g")` å’Œ `("hu", "##gs")` ï¼ˆå¾—åˆ†ä¸º 1/15ï¼Œè€Œæ‰€æœ‰å…¶ä»–é…å¯¹çš„å¾—åˆ†ä¸º 1/21ï¼‰ï¼Œå› æ­¤å¾—åˆ†æœ€é«˜çš„ç¬¬ä¸€å¯¹åˆå¹¶ï¼š

```
è¯æ±‡è¡¨: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
è¯­æ–™åº“: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ç„¶åæˆ‘ä»¬å°±æŒ‰æ­¤æ–¹å¼ç»§ç»­ï¼Œç›´åˆ°æˆ‘ä»¬è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡è¡¨å¤§å°ã€‚

> [!TIP]
> âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

## tokenization ç®—æ³• [[tokenization ç®—æ³•]]

WordPiece å’Œ BPE çš„åˆ†è¯æ–¹å¼æœ‰æ‰€ä¸åŒï¼ŒWordPiece åªä¿å­˜æœ€ç»ˆè¯æ±‡è¡¨ï¼Œè€Œä¸ä¿å­˜å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™ã€‚WordPiece ä»å¾…åˆ†è¯çš„è¯å¼€å§‹ï¼Œæ‰¾åˆ°è¯æ±‡è¡¨ä¸­æœ€é•¿çš„å­è¯ï¼Œç„¶ååœ¨å…¶å¤„åˆ†å‰²ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°ç¤ºä¾‹ä¸­å­¦ä¹ åˆ°çš„è¯æ±‡è¡¨ï¼Œå¯¹äºè¯ `"hugs"` ï¼Œä»å¼€å§‹å¤„çš„æœ€é•¿å­è¯åœ¨è¯æ±‡è¡¨ä¸­æ˜¯ `"hug"` ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œåˆ†å‰²ï¼Œå¾—åˆ° `["hug", "##s"]` ã€‚ç„¶åæˆ‘ä»¬ç»§ç»­å¤„ç† `"##s"` ï¼Œå®ƒåœ¨è¯æ±‡è¡¨ä¸­ï¼Œæ‰€ä»¥ `"hugs"` çš„åˆ†è¯ç»“æœæ˜¯ `["hug", "##s"]` ã€‚

å¦‚æœä½¿ç”¨ BPEï¼Œæˆ‘ä»¬ä¼šæŒ‰ç…§å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™è¿›è¡Œåˆå¹¶ï¼Œå¹¶å°†å…¶åˆ†è¯ä¸º `["hu", "##gs"]` ï¼Œä¸åŒçš„å­—è¯åˆ†è¯ç®—æ³•æ‰€ä»¥æœ€ç»ˆå¾—åˆ°çš„ç¼–ç æ˜¯ä¸åŒçš„ã€‚

å†ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ `"bugs"` å°†å¦‚ä½•åˆ†è¯çš„ã€‚ `"b"` æ˜¯ä»è¯æ±‡è¡¨ä¸­å•è¯å¼€å¤´å¼€å§‹çš„æœ€é•¿å­è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œåˆ†å‰²å¹¶å¾—åˆ° `["b", "##ugs"]` ã€‚ç„¶å `"##u"` æ˜¯è¯æ±‡è¡¨ä¸­ä» `"##ugs"` å¼€å§‹çš„æœ€é•¿çš„å­è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œæ‹†åˆ†å¹¶å¾—åˆ° `["b", "##u, "##gs"]` ã€‚æœ€åï¼Œ `"##gs"` åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå› æ­¤ `"bugs"` çš„åˆ†è¯ç»“æœæ˜¯ï¼š `["b", "##u, "##gs"]` ã€‚

å½“åˆ†è¯è¿‡ç¨‹ä¸­æ— æ³•åœ¨è¯æ±‡åº“ä¸­æ‰¾åˆ°è¯¥å­è¯æ—¶ï¼Œæ•´ä¸ªè¯ä¼šè¢«æ ‡è®°ä¸º unknownï¼ˆæœªçŸ¥ï¼‰â€”â€” ä¾‹å¦‚ï¼Œ `"mug"` å°†è¢«æ ‡è®°ä¸º `["[UNK]"]` ï¼Œ `"bum"` ä¹Ÿæ˜¯å¦‚æ­¤ï¼ˆå³ä½¿æˆ‘ä»¬çš„è¯æ±‡è¡¨ä¸­åŒ…å« `"b"` å’Œ `"##u"` å¼€å§‹ï¼Œä½†æ˜¯ `"##m"` ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå› æ­¤æœ€ç»ˆçš„åˆ†è¯ç»“æœåªä¼šæ˜¯ `["[UNK]"]` ï¼Œè€Œä¸æ˜¯ `["b", "##u", "[UNK]"]` ï¼‰ã€‚è¿™æ˜¯ä¸ BPE çš„å¦ä¸€ä¸ªåŒºåˆ«ï¼ŒBPE åªä¼šå°†ä¸åœ¨è¯æ±‡åº“ä¸­çš„å•ä¸ªå­—ç¬¦æ ‡è®°ä¸º unknownã€‚

> [!TIP]
> âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** `"pugs"` å°†è¢«å¦‚ä½•åˆ†è¯ï¼Ÿ

## å®ç° WordPiece [[å®ç° WordPiece]]

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ WordPiece ç®—æ³•çš„å®ç°ã€‚ä¸ BPE ä¸€æ ·ï¼Œè¿™åªæ˜¯æ•™å­¦ç¤ºä¾‹ï¼Œä½ ä¸èƒ½åœ¨å¤§å‹è¯­æ–™åº“ä¸Šä½¿ç”¨ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ä¸­ç›¸åŒçš„è¯­æ–™åº“ï¼š

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†è¯­æ–™åº“é¢„åˆ†è¯ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤åˆ» WordPiece  tokenizer  ï¼ˆå¦‚ BERTï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `bert-base-cased` tokenizer è¿›è¡Œé¢„åˆ†è¯ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ç„¶åæˆ‘ä»¬åœ¨è¿›è¡Œé¢„åˆ†è¯çš„åŒæ—¶ï¼Œè®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼š

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œå­—æ¯è¡¨æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„é›†åˆï¼Œç”±æ‰€æœ‰å•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯ä»¥åŠæ‰€æœ‰ä»¥ `##` ä¸ºå‰ç¼€å’Œåœ¨å•è¯ä¸­çš„å…¶ä»–å­—æ¯ç»„æˆï¼š

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokensï¼Œåœ¨ä½¿ç”¨ BERT çš„æƒ…å†µä¸‹ï¼Œç‰¹æ®Š tokens æ˜¯ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]` ï¼š

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªå•è¯è¿›è¡Œåˆ†å‰²ï¼Œé™¤äº†ç¬¬ä¸€ä¸ªå­—æ¯å¤–ï¼Œå…¶ä»–å­—æ¯éƒ½éœ€è¦ä»¥ `##` ä¸ºå‰ç¼€ï¼š

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è®­ç»ƒäº†ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹çš„åˆ†æ•°ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒï¼š

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹åœ¨åˆå§‹åˆ†å‰²åçš„éƒ¨åˆ†å­—å…¸ï¼š

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

ç°åœ¨ï¼Œåªéœ€è¦ä¸€ä¸ªå¿«é€Ÿå¾ªç¯å°±å¯ä»¥æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„å¯¹ï¼š

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶æ˜¯ `('a', '##b') -> 'ab'` ï¼Œå¹¶ä¸”æˆ‘ä»¬æ·»åŠ  `'ab'` åˆ°è¯æ±‡è¡¨ä¸­ï¼š

```python
vocab.append("ab")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ `splits` å­—å…¸è¿›è¡Œè¿™ç§åˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤å†™å¦ä¸€ä¸ªå‡½æ•°ï¼š

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœï¼š

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†åˆå¹¶å¾ªç¯çš„æ‰€æœ‰ä»£ç ã€‚è®©æˆ‘ä»¬è®¾å®šè¯æ±‡è¡¨çš„å¤§å°ä¸º 70ï¼š

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

ç„¶åæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„è¯æ±‡è¡¨ï¼š

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab','##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç›¸è¾ƒäº BPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰ï¼Œæ­¤åˆ†è¯å™¨åœ¨å­¦ä¹ å•è¯éƒ¨åˆ†ä½œä¸º tokens æ—¶ç¨å¿«ä¸€äº›ã€‚

> [!TIP]
> ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸º ğŸ¤— Tokenizers åº“æ²¡æœ‰ä¸ºè®­ç»ƒå®ç° WordPieceï¼ˆå› ä¸ºæˆ‘ä»¬ä¸å®Œå…¨ç¡®å®šå®ƒçš„çœŸå®å®ç°æ–¹å¼ï¼‰ï¼Œè€Œæ˜¯ä½¿ç”¨äº† BPEã€‚

è¦å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬å…ˆé¢„åˆ†è¯ï¼Œå†è¿›è¡Œåˆ†å‰²ï¼Œç„¶ååœ¨æ¯ä¸ªè¯ä¸Šä½¿ç”¨åˆ†è¯ç®—æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯»æ‰¾ä»ç¬¬ä¸€ä¸ªè¯å¼€å§‹çš„æœ€å¤§å­è¯å¹¶å°†å…¶åˆ†å‰²ï¼Œç„¶åæˆ‘ä»¬å¯¹ç¬¬äºŒéƒ¨åˆ†é‡å¤æ­¤è¿‡ç¨‹ï¼Œä»¥æ­¤ç±»æ¨ï¼Œå¯¹è¯¥è¯ä»¥åŠæ–‡æœ¬ä¸­çš„åç»­è¯è¿›è¡Œåˆ†å‰²ï¼š

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

è®©æˆ‘ä»¬ä½¿ç”¨è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªè¯å’Œä¸€ä¸ªä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ä¸Šæµ‹è¯•ä¸€ä¸‹ï¼š

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå¯¹æ–‡æœ¬åˆ†è¯çš„å‡½æ•°ï¼š

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ–‡æœ¬ä¸Šå°è¯•ï¼š

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

è¿™å°±æ˜¯ WordPiece ç®—æ³•çš„å…¨éƒ¨å†…å®¹ï¼ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ Unigramã€‚