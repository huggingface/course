# 本章简介 [[本章简介]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

在 [第三章](/course/chapter3) 中，我们研究了如何在特定任务上微调模型。当我们需要微调模型时，我们需要使用与模型预训练相同的 tokenizer —— 但是当我们想从头开始训练模型时应该选用哪个 tokenizer ？使用在来自其他领域或语言的语料库上预训练的 tokenizer 通常不是最理想的。例如，在英语语料库上训练的 tokenizer 在日语文本语料库上效果会大打折扣，因为两种语言在空格和标点的使用上有着显著的差异。

在本章中，你将学习如何在一份文本语料库上训练一个全新的 tokenizer，然后将使用它来预训练语言模型。这一切都将在 [🤗 Tokenizers](https://github.com/huggingface/tokenizers) 库的帮助下完成，该库提供了 [🤗 Transformers](https://github.com/huggingface/transformers) 库中的“快速” tokenizer 。 我们将深入探讨这个库所提供的功能，并研究“快速” tokenizer 与“慢速”版本的区别。

本章将涵盖以下主题：

* 如何在新的文本语料库上训练一个类似于给定 checkpoint 所使用的新 tokenizer
* 快速 tokenizer 的特殊功能
* 目前 NLP 中使用的三种主要子词 tokenization 算法之间的差异
* 如何使用🤗 Tokenizers 库从头开始构建 tokenizer 并在一些数据上进行训练

本章介绍的技术将使你为 [第七章](/course/chapter7/6) 中的部分做好准备，在那部分中，我们将尝试为 Python 源代码创建语言模型。让我们首先看一下什么是“训练” tokenizer 