# é€å—åœ°æ„å»ºæ ‡è®°å™¨

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section8.ipynb"},
]} />

æ­£å¦‚æˆ‘ä»¬åœ¨å‰å‡ èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œæ ‡è®°åŒ–åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼š

- è§„èŒƒåŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰ 
- é¢„æ ‡è®°åŒ–ï¼ˆå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ï¼‰ 
- é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ‹†åˆ†çš„è¯æ¥ç”Ÿæˆä¸€ç³»åˆ—æ ‡è®°ï¼‰ 
- åå¤„ç†ï¼ˆæ·»åŠ æ ‡è®°å™¨çš„ç‰¹æ®Šæ ‡è®°ï¼Œç”Ÿæˆæ³¨æ„åŠ›æ©ç å’Œæ ‡è®°ç±»å‹ IDï¼‰ 

æé†’ä¸€ä¸‹ï¼Œè¿™é‡Œå†çœ‹ä¸€ä¸‹æ•´ä¸ªè¿‡ç¨‹

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

ğŸ¤— Tokenizers åº“æ—¨åœ¨ä¸ºæ¯ä¸ªæ­¥éª¤æä¾›å¤šä¸ªé€‰é¡¹ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬æ··åˆå’ŒåŒ¹é…åœ¨ä¸€èµ·ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä»å¤´å¼€å§‹æ„å»ºæ ‡è®°å™¨ï¼Œè€Œä¸æ˜¯åƒæˆ‘ä»¬[ç¬¬äºŒèŠ‚ 2](/course/chapter6/2)é‚£æ ·ä»æ—§çš„æ ‡è®°å™¨è®­ç»ƒæ–°çš„æ ‡è®°å™¨.ç„¶åï¼Œæ‚¨å°†èƒ½å¤Ÿæ„å»ºæ‚¨èƒ½æƒ³åˆ°çš„ä»»ä½•ç±»å‹çš„æ ‡è®°å™¨ï¼

<Youtube id="MR8tZm5ViWU"/>

æ›´å‡†ç¡®åœ°è¯´ï¼Œè¯¥åº“æ˜¯å›´ç»•ä¸€ä¸ªä¸­å¤®â€œTokenizerâ€ç±»æ„å»ºçš„ï¼Œæ„å»ºè¿™ä¸ªç±»çš„æ¯ä¸€éƒ¨åˆ†å¯ä»¥åœ¨å­æ¨¡å—çš„åˆ—è¡¨ä¸­é‡æ–°ç»„åˆï¼š

- `normalizers` åŒ…å«ä½ å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¯èƒ½çš„Normalizerç±»å‹ï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers)ï¼‰ã€‚ 
- `pre_tokenizesr` åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¯èƒ½çš„PreTokenizerç±»å‹ï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers)ï¼‰ã€‚ 
- `models` åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„å„ç§ç±»å‹çš„Modelï¼Œå¦‚BPEã€WordPieceå’ŒUnigramï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models)ï¼‰ã€‚  
- `trainers` åŒ…å«æ‰€æœ‰ä¸åŒç±»å‹çš„ trainerï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªè¯­æ–™åº“è®­ç»ƒä½ çš„æ¨¡å‹ï¼ˆæ¯ç§æ¨¡å‹ä¸€ä¸ªï¼›å®Œæ•´åˆ—è¡¨[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers)ï¼‰ã€‚ 
- `post_processors` åŒ…å«ä½ å¯ä»¥ä½¿ç”¨çš„å„ç§ç±»å‹çš„PostProcessorï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors)ï¼‰ã€‚ 
- `decoders` åŒ…å«å„ç§ç±»å‹çš„Decoderï¼Œå¯ä»¥ç”¨æ¥è§£ç æ ‡è®°åŒ–çš„è¾“å‡ºï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders)ï¼‰ã€‚ 

æ‚¨å¯ä»¥[åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/components.html)æ‰¾åˆ°å®Œæ•´çš„æ¨¡å—åˆ—è¡¨ã€‚

## è·å–è¯­â€‹â€‹æ–™åº“

ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ–°æ ‡è®°å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå°çš„æ–‡æœ¬è¯­æ–™åº“ï¼ˆå› æ­¤ç¤ºä¾‹è¿è¡Œå¾—å¾ˆå¿«ï¼‰ã€‚è·å–è¯­â€‹â€‹æ–™åº“çš„æ­¥éª¤ä¸æˆ‘ä»¬åœ¨[åœ¨è¿™ç« çš„å¼€å§‹]((/course/chapter6/2)é‚£ä¸€å°èŠ‚ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬å°†ä½¿ç”¨[WikiText-2](https://huggingface.co/datasets/wikitext)æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

**get_training_corpus()** å‡½æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è°ƒç”¨çš„æ—¶å€™å°†äº§ç”Ÿ 1,000 ä¸ªæ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥è®­ç»ƒæ ‡è®°å™¨ã€‚

ğŸ¤— Tokenizers ä¹Ÿå¯ä»¥ç›´æ¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•ç”Ÿæˆä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°ä½¿ç”¨çš„æ¥è‡ª WikiText-2 çš„æ‰€æœ‰æ–‡æœ¬/è¾“å…¥ï¼š

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•é€å—æ„å»ºæ‚¨è‡ªå·±çš„ BERTã€GPT-2 å’Œ XLNet æ ‡è®°å™¨ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›ä¸‰ä¸ªä¸»è¦æ ‡è®°åŒ–ç®—æ³•çš„ç¤ºä¾‹ï¼šWordPieceã€BPE å’Œ Unigramã€‚è®©æˆ‘ä»¬ä» BERT å¼€å§‹å§ï¼

## ä»å¤´å¼€å§‹æ„å»º WordPiece æ ‡è®°å™¨

è¦ä½¿ç”¨ ğŸ¤— Tokenizers åº“æ„å»ºæ ‡è®°å™¨ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨**model**å®ä¾‹åŒ–ä¸€ä¸ª **Tokenizer** å¯¹è±¡ä¸  ï¼Œç„¶åå°† **normalizer** , **pre_tokenizer** , **post_processor** ï¼Œ å’Œ **decoder** å±æ€§è®¾ç½®æˆæˆ‘ä»¬æƒ³è¦çš„å€¼ã€‚

å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª **Tokenizer** ä½¿ç”¨ WordPiece æ¨¡å‹ï¼š

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

æˆ‘ä»¬å¿…é¡»æŒ‡å®š **unk_token** è¿™æ ·æ¨¡å‹æ‰çŸ¥é“å½“å®ƒé‡åˆ°ä»¥å‰æ²¡æœ‰è§è¿‡çš„å­—ç¬¦æ—¶è¦è¿”å›ä»€ä¹ˆã€‚æˆ‘ä»¬å¯ä»¥åœ¨æ­¤å¤„è®¾ç½®çš„å…¶ä»–å‚æ•°åŒ…æ‹¬æˆ‘ä»¬æ¨¡å‹çš„**vocabï¼ˆå­—å…¸ï¼‰**ï¼ˆæˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è®¾ç½®å®ƒï¼‰å’Œ **max_input_chars_per_word** å³æ¯ä¸ªå•è¯çš„æœ€å¤§é•¿åº¦ï¼ˆæ¯”ä¼ é€’çš„å€¼é•¿çš„å•è¯å°†è¢«æ‹†åˆ†ï¼‰

æ ‡è®°åŒ–çš„ç¬¬ä¸€æ­¥æ˜¯è§„èŒƒåŒ–ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä»å®ƒå¼€å§‹ã€‚ ç”±äº BERT è¢«å¹¿æ³›ä½¿ç”¨ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªå¯ä»¥ä½¿ç”¨çš„ `BertNormalizer`ï¼Œæˆ‘ä»¬å¯ä»¥ä¸º BERT è®¾ç½®ç»å…¸çš„é€‰é¡¹ï¼š`lowercaseï¼ˆå°å†™ï¼‰` å’Œ `strip_accentsï¼ˆå»é™¤éŸ³è°ƒï¼‰`ï¼Œä¸è¨€è‡ªæ˜ï¼› `clean_text` åˆ é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦å¹¶å°†é‡å¤çš„ç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªï¼› å’Œ `handle_chinese_chars`ï¼Œåœ¨æ±‰å­—å‘¨å›´æ”¾ç½®ç©ºæ ¼ã€‚ è¦å®ç° `bert-base-uncased` ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·è®¾ç½®è¿™ä¸ªè§„èŒƒå™¨ï¼š

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ç„¶è€Œï¼Œä¸€èˆ¬æ¥è¯´ï¼Œåœ¨æ„å»ºæ–°çš„æ ‡è®°å™¨æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å·²ç»åœ¨ ğŸ¤— Tokenizersåº“ä¸­å®ç°çš„éå¸¸æ–¹ä¾¿çš„normalizerâ€”â€”æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ‰‹åŠ¨åˆ›å»º BERT normalizerã€‚ è¯¥åº“æä¾›äº†ä¸€ä¸ªâ€œLowercaseï¼ˆå°å†™ï¼‰â€çš„normalizerå’Œä¸€ä¸ªâ€œStripAccentsâ€çš„normalizerï¼Œæ‚¨å¯ä»¥ä½¿ç”¨â€œåºåˆ—â€ç»„åˆå¤šä¸ªnormalizerï¼š

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

æˆ‘ä»¬ä¹Ÿåœ¨ä½¿ç”¨ **NFD** Unicode normalizerï¼Œå¦åˆ™ **StripAccents** normalizer æ— æ³•æ­£ç¡®è¯†åˆ«å¸¦é‡éŸ³çš„å­—ç¬¦ï¼Œå› æ­¤æ²¡åŠæ³•åˆ é™¤å®ƒä»¬ã€‚

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **normalize** çš„ **normalize_str()** æ–¹æ³•æŸ¥çœ‹å®ƒå¯¹ç»™å®šæ–‡æœ¬çš„å½±å“ï¼š

```python
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
hello how are u?
```

<Tip>

**æ›´è¿›ä¸€æ­¥**å¦‚æœæ‚¨åœ¨åŒ…å« unicode å­—ç¬¦çš„å­—ç¬¦ä¸²ä¸Šæµ‹è¯•å…ˆå‰normalizersçš„ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ‚¨è‚¯å®šä¼šæ³¨æ„åˆ°è¿™ä¸¤ä¸ªnormalizerså¹¶ä¸å®Œå…¨ç­‰æ•ˆã€‚
ä¸ºäº†ä¸è¿‡åº¦ä½¿ç”¨ `normalizers.Sequence` ä½¿ç‰ˆæœ¬è¿‡äºå¤æ‚ï¼Œæˆ‘ä»¬æ²¡æœ‰åŒ…å«å½“ `clean_text` å‚æ•°è®¾ç½®ä¸º `True` æ—¶ `BertNormalizer` éœ€è¦çš„æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ - è¿™æ˜¯é»˜è®¤è¡Œä¸ºã€‚ ä½†ä¸è¦æ‹…å¿ƒï¼šé€šè¿‡åœ¨normalizeråºåˆ—ä¸­æ·»åŠ ä¸¤ä¸ª `normalizers.Replace` å¯ä»¥åœ¨ä¸ä½¿ç”¨æ–¹ä¾¿çš„ `BertNormalizer` çš„æƒ…å†µä¸‹è·å¾—å®Œå…¨ç›¸åŒçš„è§„èŒƒåŒ–ã€‚

</Tip>

æ¥ä¸‹æ¥æ˜¯é¢„æ ‡è®°æ­¥éª¤ã€‚ åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªé¢„æ„å»ºçš„â€œBertPreTokenizerâ€ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

æˆ–è€…æˆ‘ä»¬å¯ä»¥ä»å¤´å¼€å§‹æ„å»ºå®ƒï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

è¯·æ³¨æ„ï¼Œ`Whitespace` é¢„æ ‡è®°å™¨ä¼šåœ¨ç©ºæ ¼å’Œæ‰€æœ‰éå­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿å­—ç¬¦çš„å­—ç¬¦ä¸Šè¿›è¡Œæ‹†åˆ†ï¼Œå› æ­¤åœ¨æœ¬æ¬¡çš„ä¾‹å­ä¸­ä¸Šä¼šæ ¹æ®ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œæ‹†åˆ†ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

å¦‚æœæ‚¨åªæƒ³åœ¨ç©ºç™½å¤„è¿›è¡Œæ‹†åˆ†ï¼Œåˆ™åº”ä½¿ç”¨ **WhitespaceSplit** ä»£æ›¿é¢„æ ‡è®°å™¨ï¼š

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

åƒnormalizersä¸€æ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ **Sequence** ç»„æˆå‡ ä¸ªé¢„æ ‡è®°å™¨ï¼š

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

æ ‡è®°åŒ–ç®¡é“çš„ä¸‹ä¸€æ­¥æ˜¯è¾“å…¥ç»™æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»åœ¨åˆå§‹åŒ–ä¸­æŒ‡å®šäº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä»ç„¶éœ€è¦è®­ç»ƒå®ƒï¼Œè¿™å°†éœ€è¦ä¸€ä¸ª **WordPieceTrainer** .åœ¨ ğŸ¤— Tokenizers ä¸­å®ä¾‹åŒ–è®­ç»ƒå™¨æ—¶è¦è®°ä½çš„ä¸»è¦äº‹æƒ…æ˜¯ï¼Œæ‚¨éœ€è¦å°†æ‚¨æ‰“ç®—ä½¿ç”¨çš„æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ä¼ é€’ç»™å®ƒ - å¦åˆ™å®ƒä¸ä¼šå°†å®ƒä»¬æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œå› ä¸ºå®ƒä»¬ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­ï¼š

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

ä»¥åŠæŒ‡å®š **vocab_sizeï¼ˆè¯å…¸å¤§å°ï¼‰** å’Œ **special_tokensï¼ˆç‰¹æ®Šçš„æ ‡è®°ï¼‰** ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½® **min_frequency** ï¼ˆè®°å·å¿…é¡»å‡ºç°åœ¨è¯æ±‡è¡¨ä¸­çš„æ¬¡æ•°ï¼‰æˆ–æ›´æ”¹ **continuing_subword_prefix** ï¼ˆå¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨ä¸ **##**æŒ‡ä»£å­˜åœ¨ä¸å­—è¯ç›¸åŒçš„å‰ç¼€ ï¼‰ã€‚

è¦ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è¿­ä»£å™¨è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶æ¥è®­ç»ƒæˆ‘ä»¬çš„æ ‡è®°å™¨ï¼Œå®ƒçœ‹èµ·æ¥åƒè¿™æ ·ï¼ˆæˆ‘ä»¬éœ€è¦å…ˆåˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ **WordPiece** ï¼‰ï¼š

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥é€šè¿‡è°ƒç”¨æ–‡æœ¬æ¥æµ‹è¯•æ ‡è®°å™¨ **encode()** æ–¹æ³•ï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

è¿™ä¸ª **encoding** è·å¾—çš„æ˜¯ä¸€ä¸ª **Encoding**å¯¹è±¡ ï¼Œå®ƒçš„å±æ€§ä¸­åŒ…å«æ ‡è®°å™¨çš„æ‰€æœ‰å¿…è¦è¾“å‡ºï¼š **ids** , **type_ids** , **tokens** , **offsets** , **attention_mask** , **special_tokens_mask** ï¼Œ å’Œ **overflowing** .

æ ‡è®°åŒ–ç®¡é“çš„æœ€åä¸€æ­¥æ˜¯åå¤„ç†ã€‚æˆ‘ä»¬éœ€è¦æ·»åŠ  **[CLS]** å¼€å¤´çš„æ ‡è®°å’Œ **[SEP]** æ ‡è®°åœ¨æœ«å°¾ï¼ˆæˆ–åœ¨æ¯ä¸ªå¥å­ä¹‹åï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€å¯¹å¥å­ï¼‰ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª **TemplateProcessor** ä¸ºæ­¤ï¼Œä½†é¦–å…ˆæˆ‘ä»¬éœ€è¦çŸ¥é“ **[CLS]** å’Œ **[SEP]** åœ¨è¯æ±‡è¡¨ä¸­çš„IDï¼š

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

ä¸ºäº†ç»™ **TemplateProcessor** ç¼–å†™æ¨¡æ¿ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šå¦‚ä½•å¤„ç†å•ä¸ªå¥å­å’Œä¸€å¯¹å¥å­ã€‚å¯¹äºä¸¤è€…ï¼Œæˆ‘ä»¬éƒ½ç¼–å†™äº†æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ï¼›ç¬¬ä¸€ä¸ªï¼ˆæˆ–å•ä¸ªï¼‰å¥å­è¡¨ç¤ºä¸º **$A** ï¼Œè€Œç¬¬äºŒä¸ªå¥å­ï¼ˆå¦‚æœå¯¹ä¸€å¯¹è¿›è¡Œç¼–ç ï¼‰è¡¨ç¤ºä¸º **$B** .å¯¹äºè¿™äº›ç‰¹æ®Šæ ‡è®°å’Œå¥å­ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä½¿ç”¨åœ¨å†’å·åæŒ‡å®šç›¸åº”çš„æ ‡è®°ç±»å‹ IDã€‚

å› æ­¤ç»å…¸çš„ BERT æ¨¡æ¿å®šä¹‰å¦‚ä¸‹ï¼š

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ç‰¹æ®Šæ ‡è®°çš„ IDï¼Œä»¥ä¾¿æ ‡è®°å™¨å¯ä»¥æ­£ç¡®åœ°å°†ç‰¹æ®Šæ ‡è®°è½¬æ¢ä¸ºå®ƒä»¬çš„ IDã€‚

æ·»åŠ åï¼Œæˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹å°†è¾“å‡ºå‡ºï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

åœ¨ä¸€å¯¹å¥å­ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ­£ç¡®çš„ç»“æœï¼š
```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å‡ ä¹ä»å¤´å¼€å§‹æ„å»ºäº†è¿™ä¸ªæ ‡è®°å™¨â€”â€”ä½†æ˜¯è¿˜æœ‰æœ€åä¸€æ­¥æ˜¯æŒ‡å®šä¸€ä¸ªè§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬ä¹‹å‰çš„ **encoding** ï¼š

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

å¾ˆå¥½ï¼æˆ‘ä»¬å¯ä»¥å°†æ ‡è®°å™¨ä¿å­˜åœ¨ä¸€ä¸ª JSON æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.save("tokenizer.json")
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**from_file()** æ–¹æ³•ä»è¯¥æ–‡ä»¶é‡Œé‡æ–°åŠ è½½ **Tokenizer** å¯¹è±¡ï¼š

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

è¦åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨è¿™ä¸ªæ ‡è®°å™¨ï¼Œæˆ‘ä»¬å¿…é¡»å°†å®ƒåŒ…è£¹åœ¨ä¸€ä¸ª **PreTrainedTokenizerFast** ç±»ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ³›å‹ç±»ï¼Œæˆ–è€…ï¼Œå¦‚æœæˆ‘ä»¬çš„æ ‡è®°å™¨å¯¹åº”äºç°æœ‰æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨è¯¥ç±»ï¼ˆä¾‹å¦‚è¿™é‡Œçš„ **BertTokenizerFast** ï¼‰ã€‚å¦‚æœæ‚¨åº”ç”¨æœ¬è¯¾æ¥æ„å»ºå…¨æ–°çš„æ ‡è®°å™¨ï¼Œåˆ™å¿…é¡»ä½¿ç”¨ç¬¬ä¸€ä¸ªé€‰é¡¹ã€‚

è¦å°†æ ‡è®°å™¨åŒ…è£…åœ¨ `PreTrainedTokenizerFast` ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬æ„å»ºçš„æ ‡è®°å™¨ä½œä¸º`tokenizer_object` ä¼ é€’ï¼Œæˆ–è€…å°†æˆ‘ä»¬ä¿å­˜ä¸º`tokenizer_file` çš„æ ‡è®°å™¨æ–‡ä»¶ä¼ é€’ã€‚ è¦è®°ä½çš„å…³é”®æ˜¯æˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨è®¾ç½®æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œå› ä¸ºè¯¥ç±»æ— æ³•ä» `tokenizer` å¯¹è±¡æ¨æ–­å‡ºå“ªä¸ªæ ‡è®°æ˜¯æ©ç æ ‡è®°ã€`[CLS]` æ ‡è®°ç­‰ï¼š

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

å¦‚æœæ‚¨ä½¿ç”¨ç‰¹å®šçš„æ ‡è®°å™¨ç±»ï¼ˆä¾‹å¦‚ **BertTokenizerFast** )ï¼Œæ‚¨åªéœ€è¦æŒ‡å®šä¸é»˜è®¤æ ‡è®°ä¸åŒçš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ­¤å¤„æ²¡æœ‰ï¼‰ï¼š

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

ç„¶åï¼Œæ‚¨å¯ä»¥åƒä½¿ç”¨ä»»ä½•å…¶ä»– ğŸ¤— Transformers æ ‡è®°å™¨ä¸€æ ·ä½¿ç”¨æ­¤æ ‡è®°å™¨ã€‚ä½ å¯ä»¥ç”¨ **save_pretrained()** æ–¹æ³•ï¼Œæˆ–ä½¿ç”¨ **push_to_hub()** æ–¹æ³•ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†å¦‚ä½•æ„å»º WordPiece æ ‡è®°å™¨ï¼Œè®©æˆ‘ä»¬å¯¹ BPE æ ‡è®°å™¨è¿›è¡ŒåŒæ ·çš„æ“ä½œã€‚å› ä¸ºæ‚¨å·²ç»çŸ¥é“äº†æ‰€æœ‰æ­¥éª¤ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šè¿›è¡Œåœ°æ›´å¿«ä¸€ç‚¹ï¼Œå¹¶ä¸”åªçªå‡ºå±•ç¤ºä¸¤è€…ä¸ä¸€æ ·çš„åœ°æ–¹ã€‚

## ä»å¤´å¼€å§‹æ„å»º BPE æ ‡è®°å™¨

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª GPT-2 æ ‡è®°å™¨ã€‚ä¸ BERT æ ‡è®°å™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ **Tokenizer** åˆå§‹åŒ–ä¸€ä¸ªBPE æ¨¡å‹ï¼š

```python
tokenizer = Tokenizer(models.BPE())
```

å’Œ BERT ä¸€æ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªè¯æ±‡è¡¨æ¥åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ `vocab` å’Œ `merges`ï¼‰ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è¿™æ ·å»åšã€‚ æˆ‘ä»¬ä¹Ÿä¸éœ€è¦æŒ‡å®šâ€œunk_tokenâ€ï¼Œå› ä¸º GPT-2 ä½¿ç”¨çš„å­—èŠ‚çº§ BPEï¼Œä¸éœ€è¦â€œunk_tokenâ€ã€‚

GPT-2 ä¸ä½¿ç”¨å½’ä¸€åŒ–å™¨ï¼Œå› æ­¤æˆ‘ä»¬è·³è¿‡è¯¥æ­¥éª¤å¹¶ç›´æ¥è¿›å…¥é¢„æ ‡è®°åŒ–ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

æˆ‘ä»¬åœ¨æ­¤å¤„æ·»åŠ åˆ° `ByteLevel` çš„é€‰é¡¹æ˜¯ä¸åœ¨å¥å­å¼€å¤´æ·»åŠ ç©ºæ ¼ï¼ˆé»˜è®¤ä¸ºtureï¼‰ã€‚ æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ä½¿ç”¨è¿™ä¸ªæ ‡è®°å™¨å¯¹ä¹‹å‰ç¤ºä¾‹æ–‡æœ¬çš„é¢„æ ‡è®°ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

æ¥ä¸‹æ¥æ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚å¯¹äº GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Šæ ‡è®°æ˜¯æ–‡æœ¬ç»“æŸæ ‡è®°ï¼š

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ä¸ `WordPieceTrainer` ä»¥åŠ `vocab_size` å’Œ `special_tokens` ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®š `min_frequency` å¦‚æœæˆ‘ä»¬æ„¿æ„ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯å°¾åç¼€ï¼ˆå¦‚ `</w>` )ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `end_of_word_suffix` è®¾ç½®å®ƒã€‚

è¿™ä¸ªæ ‡è®°å™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè®­ç»ƒï¼š

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ ‡è®°åŒ–åçš„ç»“æœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

æˆ‘ä»¬å¯¹ GPT-2 æ ‡è®°å™¨æ·»åŠ å­—èŠ‚çº§åå¤„ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

`trim_offsets = False` é€‰é¡¹æŒ‡ç¤ºæˆ‘ä»¬åº”è¯¥ä¿ç•™ä»¥ 'Ä ' å¼€å¤´çš„æ ‡è®°çš„åç§»é‡ï¼šè¿™æ ·åç§»é‡çš„å¼€å¤´å°†æŒ‡å‘å•è¯ä¹‹å‰çš„ç©ºæ ¼ï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ªå•è¯çš„å­—ç¬¦ï¼ˆå› ä¸ºç©ºæ ¼åœ¨æŠ€æœ¯ä¸Šæ˜¯æ ‡è®°çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚ è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åˆšåˆšç¼–ç çš„æ–‡æœ¬çš„ç»“æœï¼Œå…¶ä¸­ `'Ä test'` æ˜¯ç´¢å¼•ç¬¬ 4 å¤„çš„æ ‡è®°ï¼š

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªå­—èŠ‚çº§è§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.ByteLevel()
```

æˆ‘ä»¬å¯ä»¥ä»”ç»†æ£€æŸ¥å®ƒæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

å¾ˆå¥½ï¼ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ï¼Œæˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜æ ‡è®°å™¨ï¼Œå¹¶å°†å®ƒåŒ…è£…åœ¨ä¸€ä¸ª **PreTrainedTokenizerFast** æˆ–è€… **GPT2TokenizerFast** å¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformersä¸­ä½¿ç”¨å®ƒï¼š

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

æˆ–è€…ï¼š

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

ä½œä¸ºæœ€åä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä»å¤´å¼€å§‹æ„å»º Unigram æ ‡è®°å™¨ã€‚

## ä»å¤´å¼€å§‹æ„å»º Unigram æ ‡è®°å™¨

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª XLNet æ ‡è®°å™¨ã€‚ä¸ä¹‹å‰çš„æ ‡è®°å™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ Unigram æ¨¡å‹åˆå§‹åŒ–ä¸€ä¸ª **Tokenizer** ï¼š

```python
tokenizer = Tokenizer(models.Unigram())
```

åŒæ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¯æ±‡è¡¨åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ã€‚

å¯¹äºæ ‡å‡†åŒ–ï¼ŒXLNet ä½¿ç”¨äº†ä¸€äº›æ›¿æ¢çš„æ–¹æ³•ï¼ˆæ¥è‡ª SentencePieceï¼‰ï¼š

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

è¿™ä¼šå–ä»£ **â€œ** å’Œ **â€** å’Œ **â€** ä»¥åŠä»»ä½•ä¸¤ä¸ªæˆ–å¤šä¸ªç©ºæ ¼ä¸å•ä¸ªç©ºæ ¼çš„åºåˆ—ï¼Œä»¥åŠåˆ é™¤æ–‡æœ¬ä¸­çš„é‡éŸ³ä»¥è¿›è¡Œæ ‡è®°ã€‚

ç”¨äºä»»ä½• SentencePiece æ ‡è®°å™¨çš„é¢„æ ‡è®°å™¨æ˜¯ `Metaspace`ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·æŸ¥çœ‹ç¤ºä¾‹æ–‡æœ¬çš„é¢„æ ‡è®°åŒ–ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("â–Let's", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', (14, 29))]
```

æ¥ä¸‹æ¥æ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚ XLNet æœ‰ä¸å°‘ç‰¹æ®Šçš„æ ‡è®°ï¼š

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ä¸è¦å¿˜è®°`UnigramTrainer` çš„ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°æ˜¯`unk_token`ã€‚ æˆ‘ä»¬è¿˜å¯ä»¥ä¼ é€’ç‰¹å®šäº Unigram ç®—æ³•çš„å…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚åˆ é™¤æ ‡è®°çš„æ¯ä¸ªæ­¥éª¤çš„â€œshrinking_factorï¼ˆæ”¶ç¼©å› å­ï¼‰â€ï¼ˆé»˜è®¤ä¸º 0.75ï¼‰æˆ–æŒ‡å®šç»™å®šæ ‡è®°çš„æœ€å¤§é•¿åº¦çš„â€œmax_piece_lengthâ€ï¼ˆé»˜è®¤ä¸º 16ï¼‰ .

è¿™ä¸ªæ ‡è®°å™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè®­ç»ƒï¼š

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ ‡è®°åŒ–åçš„ç»“æœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']
```

A peculiarity of XLNet is that it puts the `<cls>` token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens). It's padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the `<cls>` and `<sep>` tokens:
XLNet çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯å®ƒå°†`<cls>` æ ‡è®°æ”¾åœ¨å¥å­çš„æœ«å°¾ï¼Œç±»å‹ID ä¸º2ï¼ˆä»¥å°†å…¶ä¸å…¶ä»–æ ‡è®°åŒºåˆ†å¼€æ¥ï¼‰ã€‚å®ƒä¼šå°†ç»“æœå¡«å……åœ¨å·¦ä¾§ã€‚ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡æ¿å¤„ç†æ‰€æœ‰ç‰¹æ®Šæ ‡è®°å’Œæ ‡è®°ç±»å‹ IDï¼Œä¾‹å¦‚ BERTï¼Œä½†é¦–å…ˆæˆ‘ä»¬å¿…é¡»è·å– `<cls>` å’Œ `<sep>` æ ‡è®°çš„ IDï¼š

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

æ¨¡æ¿å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¼–ç ä¸€å¯¹å¥å­æ¥æµ‹è¯•å®ƒçš„å·¥ä½œåŸç†ï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ª **Metaspace** è§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.Metaspace()
```

æˆ‘ä»¬å®Œæˆäº†è¿™ä¸ªæ ‡è®°å™¨ï¼ æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜æ ‡è®°å™¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨å®ƒï¼Œå¯ä»¥å°†å®ƒåŒ…è£…åœ¨ `PreTrainedTokenizerFast` æˆ– `XLNetTokenizerFast` ä¸­ã€‚ ä½¿ç”¨ `PreTrainedTokenizerFast` æ—¶è¦æ³¨æ„çš„ä¸€ä»¶äº‹æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰ğŸ¤— Transformers åº“åº”è¯¥åœ¨å·¦ä¾§å¡«å……ç‰¹æ®Šæ ‡è®°ï¼š
```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

æˆ–è€…ï¼š

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

ç°åœ¨æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨å„ç§æ„å»ºå—æ¥æ„å»ºç°æœ‰çš„æ ‡è®°å™¨ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨ ğŸ¤— tokenizeråº“ç¼–å†™æ‚¨æƒ³è¦çš„ä»»ä½•æ ‡è®°å™¨ï¼Œå¹¶èƒ½å¤Ÿåœ¨ ğŸ¤— Transformersä¸­ä½¿ç”¨å®ƒã€‚