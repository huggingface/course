# Unigram tokenization ç®—æ³• [[Unigram tokenizationç®—æ³•]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section7.ipynb"},
]} />

Unigram ç®—æ³•å¸¸ç”¨äº SentencePiece ä¸­ï¼Œè¯¥åˆ‡åˆ†ç®—æ³•è¢« AlBERTï¼ŒT5ï¼ŒmBARTï¼ŒBig Bird å’Œ XLNet ç­‰æ¨¡å‹å¹¿æ³›é‡‡ç”¨ã€‚

<Youtube id="TGZfZVuF9Yc"/>

> [!TIP]
> ğŸ’¡ æœ¬èŠ‚å°†æ·±å…¥æ¢è®¨ Unigramï¼Œç”šè‡³å±•ç¤ºå®Œæ•´çš„å®ç°è¿‡ç¨‹ã€‚å¦‚æœä½ åªæƒ³å¤§è‡´äº†è§£ tokenization ç®—æ³•ï¼Œå¯ä»¥ç›´æ¥è·³åˆ°ç« èŠ‚æœ«å°¾ã€‚

## Unigram è®­ç»ƒ [[Unigram è®­ç»ƒ]]

ä¸BPE å’Œ WordPiece ç›¸æ¯”ï¼ŒUnigram çš„å·¥ä½œæ–¹å¼æ­£å¥½ç›¸åï¼šå®ƒä»ä¸€ä¸ªå¤§è¯æ±‡åº“å¼€å§‹ï¼Œç„¶åé€æ­¥åˆ é™¤è¯æ±‡ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡åº“å¤§å°ã€‚æ„å»ºåŸºç¡€è¯æ±‡åº“æœ‰å¤šç§æ–¹æ³•ï¼šä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€‰å–é¢„åˆ‡åˆ†è¯æ±‡ä¸­æœ€å¸¸è§çš„å­ä¸²ï¼Œæˆ–è€…åœ¨å…·æœ‰å¤§è¯æ±‡é‡çš„åˆå§‹è¯­æ–™åº“ä¸Šè¿›è¡Œ BPE å¾—åˆ°ä¸€ä¸ªåˆå§‹è¯åº“ã€‚

åœ¨è®­ç»ƒçš„æ¯ä¸€æ­¥ï¼ŒUnigram ç®—æ³•éƒ½ä¼šåœ¨ç»™å®šå½“å‰è¯æ±‡çš„æƒ…å†µä¸‹è®¡ç®—è¯­æ–™åº“çš„æŸå¤±ã€‚ç„¶åï¼Œå¯¹äºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªç¬¦å·ï¼Œç®—æ³•è®¡ç®—å¦‚æœåˆ é™¤è¯¥ç¬¦å·ï¼Œæ•´ä½“æŸå¤±ä¼šå¢åŠ å¤šå°‘ï¼Œå¹¶å¯»æ‰¾åˆ é™¤åæŸå¤±å¢åŠ æœ€å°‘çš„ç¬¦å·ã€‚è¿™äº›ç¬¦å·å¯¹è¯­æ–™åº“çš„æ•´ä½“æŸå¤±å½±å“è¾ƒå°ï¼Œå› æ­¤ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒä»¬â€œç›¸å¯¹ä¸å¿…è¦â€å¹¶ä¸”æ˜¯ç§»é™¤çš„æœ€ä½³å€™é€‰è€…ã€‚

è¿™ä¸ªè¿‡ç¨‹éå¸¸æ¶ˆè€—è®¡ç®—èµ„æºï¼Œå› æ­¤æˆ‘ä»¬ä¸åªæ˜¯åˆ é™¤ä¸æœ€ä½æŸå¤±å¢é•¿ç›¸å…³çš„å•ä¸ªç¬¦å·ï¼Œè€Œæ˜¯åˆ é™¤ä¸æœ€ä½æŸå¤±å¢é•¿ç›¸å…³çš„ç™¾åˆ†ä¹‹/p ï¼ˆp æ˜¯ä¸€ä¸ªå¯ä»¥æ§åˆ¶çš„è¶…å‚æ•°ï¼Œé€šå¸¸æ˜¯ 10 æˆ– 20ï¼‰çš„ç¬¦å·ã€‚ç„¶åé‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¯æ±‡åº“è¾¾åˆ°æ‰€éœ€å¤§å°ã€‚

æ³¨æ„ï¼Œæˆ‘ä»¬æ°¸è¿œä¸ä¼šåˆ é™¤åŸºç¡€çš„å•ä¸ªå­—ç¬¦ï¼Œä»¥ç¡®ä¿ä»»ä½•è¯éƒ½èƒ½è¢«åˆ‡åˆ†ã€‚

ç„¶è€Œï¼Œè¿™ä»ç„¶æœ‰äº›æ¨¡ç³Šï¼šç®—æ³•çš„ä¸»è¦éƒ¨åˆ†æ˜¯åœ¨è¯æ±‡åº“ä¸­è®¡ç®—è¯­æ–™åº“çš„æŸå¤±å¹¶è§‚å¯Ÿå½“æˆ‘ä»¬ä»è¯æ±‡åº“ä¸­ç§»é™¤ä¸€äº›ç¬¦å·æ—¶æŸå¤±å¦‚ä½•å˜åŒ–ï¼Œä½†æˆ‘ä»¬å°šæœªè§£é‡Šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚è¿™ä¸€æ­¥ä¾èµ–äº Unigram æ¨¡å‹çš„åˆ‡åˆ†ç®—æ³•ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ã€‚

æˆ‘ä»¬å°†å¤ç”¨å‰é¢ä¾‹å­ä¸­çš„è¯­æ–™åº“ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

è€Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å–è¿™ä¸ªè¯­æ–™åº“ä¸­æ‰€æœ‰çš„å­ä¸²ä½œä¸ºåˆå§‹è¯æ±‡åº“ï¼š

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## tokenization ç®—æ³• [[tokenizationç®—æ³•]]

Unigram æ¨¡å‹æ˜¯ä¸€ç§è¯­è¨€æ¨¡å‹ï¼Œå®ƒè®¤ä¸ºæ¯ä¸ªç¬¦å·éƒ½ä¸å…¶ä¹‹å‰çš„ç¬¦å·ç‹¬ç«‹ã€‚è¿™æ˜¯æœ€ç®€å•çš„è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤ç»™å®šä¹‹å‰çš„ä¸Šä¸‹æ–‡æƒ…å†µä¸‹ï¼Œç¬¦å· X çš„æ¦‚ç‡å°±æ˜¯ç¬¦å· X çš„æ¦‚ç‡ã€‚æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ Unigram è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬ï¼Œæˆ‘ä»¬çš„é¢„æµ‹æ€»ä¼šè¾“å‡ºæœ€å¸¸è§çš„ç¬¦å·ã€‚

ç»™å®šç¬¦å·çš„æ¦‚ç‡æ˜¯å…¶åœ¨åŸå§‹è¯­æ–™åº“ä¸­çš„é¢‘ç‡ï¼ˆæˆ‘ä»¬è®¡ç®—å®ƒå‡ºç°çš„æ¬¡æ•°ï¼‰ï¼Œé™¤ä»¥è¯æ±‡åº“ä¸­æ‰€æœ‰ç¬¦å·çš„é¢‘ç‡æ€»å’Œï¼ˆä»¥ç¡®ä¿æ¦‚ç‡æ€»å’Œä¸º 1ï¼‰ã€‚ä¾‹å¦‚ï¼Œ `"ug"` å‡ºç°åœ¨ `"hug"` ï¼Œ `"pug"` å’Œ `"hugs"` ä¸­ï¼Œæ‰€ä»¥å®ƒåœ¨æˆ‘ä»¬çš„è¯­æ–™åº“ä¸­çš„é¢‘ç‡æ˜¯ 20ã€‚

ä»¥ä¸‹æ˜¯è¯æ±‡åº“ä¸­æ‰€æœ‰å¯èƒ½å‡ºç°å­è¯çš„é¢‘ç‡ï¼š

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

æ‰€ä»¥ï¼Œæ‰€æœ‰é¢‘ç‡ä¹‹å’Œä¸º 210ï¼Œå­è¯ `"ug"` å‡ºç°çš„æ¦‚ç‡æ˜¯ 20/210ã€‚

> [!TIP]
> âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ç¼–å†™ä»£ç è®¡ç®—ä¸Šè¿°é¢‘ç‡ï¼Œç„¶åéªŒè¯ç»“æœçš„å‡†ç¡®æ€§ï¼Œä»¥åŠæ¦‚ç‡çš„æ€»å’Œæ˜¯å¦æ­£ç¡®ã€‚

ç°åœ¨ï¼Œä¸ºäº†å¯¹ä¸€ä¸ªç»™å®šçš„å•è¯è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬ä¼šæŸ¥çœ‹æ‰€æœ‰å¯èƒ½çš„åˆ†è¯ç»„åˆï¼Œå¹¶æ ¹æ® Unigram æ¨¡å‹è®¡ç®—å‡ºæ¯ç§å¯èƒ½çš„æ¦‚ç‡ã€‚ç”±äºæ‰€æœ‰çš„åˆ†è¯éƒ½è¢«è§†ä¸ºç‹¬ç«‹çš„ï¼Œå› æ­¤è¿™ä¸ªå•è¯åˆ†è¯çš„æ¦‚ç‡å°±æ˜¯æ¯ä¸ªå­è¯æ¦‚ç‡çš„ä¹˜ç§¯ã€‚ä¾‹å¦‚ï¼Œå°† `"pug"` åˆ†è¯ä¸º `["p", "u", "g"]` çš„æ¦‚ç‡ä¸ºï¼š


$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°† â€œpugâ€ åˆ†è¯ä¸º `["pu", "g"]` çš„æ¦‚ç‡ä¸ºï¼š

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

å› æ­¤ï¼Œåè€…çš„å¯èƒ½æ€§æ›´å¤§ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåˆ†è¯æ•°æœ€å°‘çš„åˆ†è¯æ–¹å¼å°†å…·æœ‰æœ€é«˜çš„æ¦‚ç‡ï¼ˆå› ä¸ºæ¯ä¸ªåˆ†è¯éƒ½è¦é™¤ä»¥ 210ï¼‰ï¼Œè¿™æ­£ç¬¦åˆæˆ‘ä»¬çš„ç›´è§‰ï¼šå°†ä¸€ä¸ªè¯åˆ†å‰²ä¸ºå°½å¯èƒ½å°‘çš„å­è¯ã€‚

åˆ©ç”¨ Unigram æ¨¡å‹å¯¹ä¸€ä¸ªè¯è¿›è¡Œåˆ†è¯ï¼Œå°±æ˜¯æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„åˆ†è¯æ–¹å¼ã€‚ä»¥ `"pug"` ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å„ç§å¯èƒ½åˆ†è¯æ–¹å¼çš„æ¦‚ç‡å¦‚ä¸‹ï¼š

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

å› æ­¤ï¼Œ `"pug"` å°†è¢«åˆ†è¯ä¸º `["p", "ug"]` æˆ– `["pu", "g"]` ï¼Œå–å†³äºå“ªç§åˆ†è¯æ–¹å¼æ’åœ¨å‰é¢ï¼ˆæ³¨æ„ï¼Œåœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸­ï¼Œåƒè¿™æ ·çš„ç›¸ç­‰æƒ…å†µå°†å¾ˆå°‘è§ï¼‰ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„åˆ†è¯æ–¹å¼å¹¶è®¡ç®—å…¶æ¦‚ç‡æ˜¯å®¹æ˜“çš„ï¼Œä½†åœ¨è¯­æ–™åº“æ¯”è¾ƒå¤§çš„æƒ…å†µä¸‹æœ‰äº›å›°éš¾ã€‚æœ‰ä¸€ä¸ªç»å…¸çš„ç®—æ³•å¯ä»¥ç”¨æ¥è®¡ç®—è¿™ä¸ªæ¦‚ç‡ï¼Œå«åš `Viterbi ç®—æ³•` ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªå›¾æ¥è¡¨ç¤ºä¸€ä¸ªç»™å®šå•è¯çš„æ‰€æœ‰å¯èƒ½åˆ†è¯ï¼Œå¦‚æœä»å­—ç¬¦ `a` åˆ°å­—ç¬¦ `b` çš„å­è¯åœ¨è¯æ±‡è¡¨ä¸­ï¼Œé‚£ä¹ˆå°±å­˜åœ¨ä¸€ä¸ªä» `a` åˆ° `b` çš„åˆ†æ”¯ï¼Œåˆ†æ”¯çš„è¾¹å°±æ˜¯è¿›è¡Œè¿™ä¸ªåˆ‡åˆ†çš„æ¦‚ç‡ã€‚

ä¸ºäº†åœ¨å›¾ä¸­æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„è·¯å¾„ï¼ŒViterbi ç®—æ³•ä¼šç¡®å®šå‡ºæ¯ä¸ªä½ç½®ä¸Šç»“æŸçš„æœ€ä½³å¾—åˆ†åˆ†å‰²ã€‚æˆ‘ä»¬ä»å¤´åˆ°å°¾è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥é€šè¿‡éå†æ‰€æœ‰åœ¨å½“å‰ä½ç½®ç»“æŸçš„å­è¯ï¼Œç„¶åä½¿ç”¨è¿™ä¸ªå­è¯å¼€å§‹ä½ç½®çš„æœ€ä½³å¾—åˆ†ï¼Œæ‰¾åˆ°æœ€é«˜å¾—åˆ†ã€‚ç„¶åï¼Œæˆ‘ä»¬åªéœ€è¦å›æº¯èµ°è¿‡çš„è·¯å¾„ï¼Œå°±èƒ½æ‰¾åˆ°æœ€ç»ˆçš„æœ€ä¼˜è·¯å¾„ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨æˆ‘ä»¬çš„è¯æ±‡è¡¨å’Œå•è¯ `"unhug"` çš„ä¾‹å­ã€‚å¯¹äºæ¯ä¸ªä½ç½®ï¼Œæœ€ä½³åˆ‡åˆ†å­è¯çš„åˆ†æ•°å¦‚ä¸‹ï¼š

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

å› æ­¤ â€œunhugâ€ å°†è¢«åˆ†è¯ä¸º `["un", "hug"]` ã€‚


> [!TIP]
> âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼**  ç¡®å®šå•è¯ â€œhuggunâ€ çš„åˆ†è¯æ–¹å¼ä»¥åŠå…¶å¾—åˆ†ã€‚

## å›åˆ°è®­ç»ƒ [[å›åˆ°è®­ç»ƒ]]

æˆ‘ä»¬å·²ç»äº†è§£äº†å¦‚ä½•è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥æ›´è¯¦ç»†åœ°äº†è§£ä¸€ä¸‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•è®¡ç®—æŸå¤±å€¼ã€‚åœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éƒ½ä¼šå°†è¯­æ–™åº“ä¸­çš„æ¯ä¸ªè¯è¿›è¡Œåˆ†è¯ï¼Œåˆ†è¯æ‰€ä½¿ç”¨çš„è¯è¡¨å’Œ Unigram æ¨¡å‹æ˜¯åŸºäºç›®å‰çš„æƒ…å†µï¼ˆå³æ ¹æ®æ¯ä¸ªè¯åœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡ï¼‰æ¥ç¡®å®šçš„ã€‚ç„¶åï¼ŒåŸºäºè¿™ç§åˆ†è¯ç»“æœï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—å‡ºæŸå¤±å€¼ï¼ˆlossï¼‰ã€‚

è¯­æ–™åº“ä¸­çš„æ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ªåˆ†æ•°ï¼ŒæŸå¤±ï¼ˆlossï¼‰å€¼æ˜¯è¿™äº›åˆ†æ•°çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ â€”â€” å³æ‰€æœ‰è¯çš„è¯­æ–™åº“ä¸­æ‰€æœ‰è¯çš„ `-log(P(word))` æ€»å’Œ 

è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„ä¾‹å­ï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„è¯­æ–™åº“ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ¯ä¸ªå•è¯çš„åˆ†è¯åŠå…¶ç›¸åº”çš„å¾—åˆ†å¦‚ä¸‹ï¼š

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

å› æ­¤ï¼ŒæŸå¤±å€¼ï¼ˆlossï¼‰æ˜¯ï¼š

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ç§»é™¤æ¯ä¸ª token å¯¹æŸå¤±å€¼çš„å½±å“ã€‚è¿™ä¸ªè¿‡ç¨‹é¢‡ä¸ºç¹çï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œä»…å¯¹ä¸¤ä¸ªå•è¯è¿›è¡Œæ¼”ç¤ºï¼Œåœ¨æˆ‘ä»¬ç¼–å†™ä»£ç æ¥ååŠ©å¤„ç†è¿™ä¸ªè¿‡ç¨‹æ—¶ï¼Œå†å¯¹å…¨éƒ¨çš„è¯è¿›è¡Œ tokenization çš„å¤„ç†ã€‚åœ¨è¿™ä¸ªï¼ˆéå¸¸ï¼‰ç‰¹æ®Šçš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¹å•è¯çš„ä¸¤ç§ç­‰æ•ˆçš„åˆ†è¯æ–¹å¼ï¼šä¾‹å¦‚ï¼Œâ€œpugâ€å¯ä»¥è¢«åˆ†è¯ä¸º `["pu", "g"]` ï¼Œä¹Ÿå¯ä»¥è¢«åˆ†è¯ä¸º `["p", "ug"]` ï¼Œè·å¾—çš„åˆ†æ•°æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œå»é™¤è¯æ±‡è¡¨ä¸­çš„ `"pu"` æŸå¤±å€¼è¿˜ä¼šæ˜¯ä¸€æ ·çš„ã€‚

ä½†æ˜¯ï¼Œå»é™¤ `"hug"` ä¹‹åï¼ŒæŸå¤±ä¼šå˜å¾—æ›´ç³Ÿï¼Œå› ä¸º `"hug"` å’Œ `"hugs"` çš„ tokenization ä¼šå˜æˆï¼š

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

è¿™äº›å˜åŒ–å°†å¯¼è‡´æŸå¤±å¢åŠ ï¼š

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

å› æ­¤ï¼Œ `"pu"` tokens å¯èƒ½ä¼šä»è¯æ±‡è¡¨ä¸­ç§»é™¤ï¼Œä½† `"hug"` åˆ™ä¸ä¼šã€‚

## å®ç° Unigram [[å®ç° Unigram]]

ç°åœ¨è®©æˆ‘ä»¬åœ¨ä»£ç ä¸­å®ç°ä¸Šé¢çœ‹åˆ°çš„æ‰€å†…å®¹ã€‚ä¸ BPE å’Œ WordPiece ä¸€æ ·ï¼Œè¿™ä¸æ˜¯ Unigram ç®—æ³•çš„é«˜æ•ˆå®ç°ï¼ˆæ°æ°ç›¸åï¼Œè¿™å¥—ä»£ç çš„æ•ˆç‡éå¸¸ä½ï¼‰ï¼Œä½†å®ƒåº”è¯¥å¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£å®ƒã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„è¯­æ–™åº“ä½œä¸ºç¤ºä¾‹ï¼š

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `xlnet-base-cased` ä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

ä¸ BPE å’Œ WordPiece ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ï¼š

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„è¯æ±‡è¡¨åˆå§‹åŒ–ä¸ºå¤§äºæˆ‘ä»¬æœ€ç»ˆæƒ³è¦çš„è¯æ±‡é‡ã€‚æˆ‘ä»¬å¿…é¡»åŒ…å«æ‰€æœ‰åŸºæœ¬çš„å•ä¸ªå­—ç¬¦ï¼ˆå¦åˆ™æˆ‘ä»¬å°†æ— æ³•å¯¹æ¯ä¸ªå•è¯èµ‹äºˆä¸€ä¸ª token ï¼‰ï¼Œä½†å¯¹äºè¾ƒå¤§çš„å­å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å°†åªä¿ç•™æœ€å¸¸è§çš„å­—ç¬¦ï¼Œå› æ­¤æˆ‘ä»¬æŒ‰å‡ºç°é¢‘ç‡å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼š

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # å¾ªç¯éå†é•¿åº¦è‡³å°‘ä¸º2çš„å­å­—
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# æŒ‰é¢‘ç‡å¯¹å­è¯æ’åº
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

æˆ‘ä»¬ç”¨æœ€ä¼˜çš„å­è¯å¯¹å­—ç¬¦è¿›è¡Œåˆ†ç»„ï¼Œä»¥è·å¾—å¤§å°ä¸º 300 çš„åˆå§‹è¯æ±‡è¡¨ï¼š

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

> [!TIP]
> ğŸ’¡ SentencePiece ä½¿ç”¨ä¸€ç§åä¸ºå¢å¼ºåç¼€æ•°ç»„ï¼ˆESAï¼‰çš„æ›´é«˜æ•ˆçš„ç®—æ³•æ¥åˆ›å»ºåˆå§‹è¯æ±‡è¡¨ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ‰€æœ‰é¢‘ç‡çš„æ€»å’Œï¼Œå°†é¢‘ç‡è½¬åŒ–ä¸ºæ¦‚ç‡ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å°†å­˜å‚¨æ¦‚ç‡çš„å¯¹æ•°ï¼Œå› ä¸ºç›¸è¾ƒäºå°æ•°ç›¸ä¹˜ï¼Œå¯¹æ•°ç›¸åŠ åœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šï¼Œè€Œä¸”è¿™å°†ç®€åŒ–æ¨¡å‹æŸå¤±çš„è®¡ç®—ï¼š

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ç°åœ¨ï¼Œä¸»å‡½æ•°æ˜¯ä½¿ç”¨ Viterbi ç®—æ³•å¯¹å•è¯è¿›è¡Œåˆ†è¯ã€‚åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é‚£æ ·ï¼Œè¿™ä¸ªç®—æ³•ä¼šè®¡ç®—å‡ºæ¯ä¸ªè¯çš„æœ€å¥½çš„åˆ†å‰²æ–¹å¼ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªç»“æœä¿å­˜åœ¨ä¸€ä¸ªå«åš `best_segmentations` çš„å˜é‡é‡Œã€‚æˆ‘ä»¬ä¼šä¸ºè¯çš„æ¯ä¸€ä¸ªä½ç½®ï¼ˆä» 0 å¼€å§‹ï¼Œä¸€ç›´åˆ°è¯çš„æ€»é•¿åº¦ï¼‰éƒ½ä¿å­˜ä¸€ä¸ªå­—å…¸ï¼Œå­—å…¸é‡Œæœ‰ä¸¤ä¸ªé”®ï¼šæœ€å¥½çš„åˆ†å‰²ä¸­æœ€åä¸€ä¸ªè¯çš„èµ·å§‹ä½ç½®ï¼Œä»¥åŠæœ€å¥½çš„åˆ†å‰²çš„å¾—åˆ†ã€‚æœ‰äº†æœ€åä¸€ä¸ªè¯çš„èµ·å§‹ä½ç½®ï¼Œå½“æˆ‘ä»¬æŠŠæ•´ä¸ªåˆ—è¡¨éƒ½å¡«æ»¡åï¼Œæˆ‘ä»¬å°±èƒ½æ‰¾åˆ°å®Œæ•´çš„åˆ†å‰²æ–¹å¼ã€‚

æˆ‘ä»¬åªéœ€è¦ä¸¤ä¸ªå¾ªç¯å°±å¯ä»¥å¡«å……è¿™ä¸ªåˆ—è¡¨ï¼šä¸€ä¸ªä¸»å¾ªç¯ç”¨æ¥éå†æ¯ä¸ªå¯èƒ½çš„å¼€å§‹ä½ç½®ï¼Œç¬¬äºŒä¸ªå¾ªç¯åˆ™è¯•ç€æ‰¾å‡ºæ‰€æœ‰ä»¥è¿™ä¸ªå¼€å§‹ä½ç½®å¼€å§‹çš„å­ä¸²ã€‚å¦‚æœè¿™ä¸ªå­ä¸²åœ¨æˆ‘ä»¬çš„è¯è¡¨é‡Œï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ‰¾åˆ°äº†ä¸€ä¸ªæ–°çš„åˆ†è¯æ–¹å¼ï¼Œè¿™ä¸ªåˆ†è¯æ–¹å¼ä¼šåœ¨å½“å‰ä½ç½®ç»“æŸã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šæŠŠè¿™ä¸ªæ–°çš„åˆ†è¯æ–¹å¼å’Œ `best_segmentations` é‡Œçš„å†…å®¹è¿›è¡Œæ¯”è¾ƒã€‚

å½“ä¸»å¾ªç¯ç»“æŸåï¼Œæˆ‘ä»¬å°±ä»è¯çš„æœ€åä¸€ä¸ªä½ç½®å¼€å§‹ï¼Œç„¶åä¸€æ­¥æ­¥å¾€å‰è·³ï¼Œè·³è¿‡çš„æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬éƒ½ä¼šè®°å½•ä¸‹æ¥ï¼Œç›´åˆ°æˆ‘ä»¬å›åˆ°è¯çš„å¼€å¤´ã€‚

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # best_score_at_startåº”è¯¥ç”±å¾ªç¯çš„å‰é¢çš„æ­¥éª¤è®¡ç®—å’Œå¡«å……
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # å¦‚æœæˆ‘ä»¬å‘ç°ä»¥ end_idx ç»“å°¾çš„æ›´å¥½åˆ†æ®µ,æˆ‘ä»¬ä¼šæ›´æ–°
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°å•è¯çš„ tokens  -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

æˆ‘ä»¬å·²ç»å¯ä»¥åœ¨ä¸€äº›è¯ä¸Šå°è¯•æˆ‘ä»¬çš„åˆå§‹æ¨¡å‹ï¼š

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

ç°åœ¨ï¼Œè®¡ç®—è¯­æ–™åº“ä¸Šçš„åˆ†è¯æŸå¤±å°±å¾ˆç®€å•äº†ï¼

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦æœ‰æ•ˆï¼š

```python
compute_loss(model)
```

```python out
413.10377642940875
```

è®¡ç®—æ¯ä¸ªè¯çš„åˆ†æ•°ä¹Ÿå¹¶ééš¾äº‹ï¼›æˆ‘ä»¬åªéœ€è¦è®¡ç®—é€šè¿‡åˆ é™¤æ¯ä¸ªè¯å¾—åˆ°çš„æ¨¡å‹çš„æŸå¤±ï¼š

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # æˆ‘ä»¬å°†ä¿ç•™é•¿åº¦ä¸º 1 çš„ tokens
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

æˆ‘ä»¬å¯ä»¥è¯•è¯•çœ‹å¯¹äºç»™å®šçš„è¯æ˜¯å¦æœ‰æ•ˆï¼š

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

å› ä¸º `"ll"` è¿™ä¸ªå­è¯åœ¨ `"Hopefully"` è¿™ä¸ªè¯çš„åˆ†è¯ä¸­è¢«ä½¿ç”¨äº†ï¼Œå¦‚æœæˆ‘ä»¬æŠŠå®ƒåˆ æ‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šéœ€è¦ç”¨ä¸¤ä¸ª `"l"` æ¥ä»£æ›¿ï¼Œæ‰€ä»¥æˆ‘ä»¬é¢„è®¡å®ƒä¼šå¯¼è‡´æŸå¤±å€¼å¢åŠ ã€‚è€Œ `"his"` è¿™ä¸ªè¯åªåœ¨ `"This"` è¿™ä¸ªè¯é‡Œé¢è¢«ä½¿ç”¨ï¼Œè€Œä¸” `"This"` æ˜¯ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„è¯è¢«åˆ†å‰²çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬é¢„è®¡åˆ é™¤å®ƒçš„æŸå¤±å€¼å˜åŒ–ä¼šæ˜¯é›¶ã€‚ä¸‹é¢å°±æ˜¯å®éªŒç»“æœï¼š

```python out
6.376412403623874
0.0
```

> [!TIP]
> ğŸ’¡ è¿™ç§æ–¹å¼æ•ˆç‡éå¸¸ä½ï¼Œæ‰€ä»¥ SentencePiece ä½¿ç”¨äº†ä¸€ç§ä¼°ç®—æ–¹æ³•æ¥è®¡ç®—å¦‚æœæ²¡æœ‰ X tokenï¼Œæ¨¡å‹çš„æŸå¤±ä¼šæ˜¯å¤šå°‘ï¼šå®ƒä¸æ˜¯é‡æ–°å¼€å§‹ï¼Œè€Œæ˜¯åªæ˜¯ç”¨å‰©ä¸‹çš„è¯è¡¨é‡Œ X token çš„åˆ†è¯æ–¹å¼æ¥æ›¿ä»£å®ƒã€‚è¿™æ ·ï¼Œæ‰€æœ‰çš„å¾—åˆ†éƒ½èƒ½åœ¨å’Œæ¨¡å‹æŸå¤±ä¸€èµ·çš„åŒæ—¶è®¡ç®—å‡ºæ¥ã€‚

è‡³æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åšçš„æœ€åä¸€ä»¶äº‹å°±æ˜¯å°†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokens æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œç„¶åå¾ªç¯ç›´åˆ°æˆ‘ä»¬ä»è¯æ±‡è¡¨ä¸­å‰ªé™¤è¶³å¤Ÿå¤šçš„ tokens ä»¥è¾¾åˆ°æˆ‘ä»¬æœŸæœ›çš„è§„æ¨¡ï¼š

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # åˆ é™¤åˆ†æ•°æœ€ä½çš„percent_to_remov tokens ã€‚
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ç„¶åï¼Œè¦å¯¹æŸäº›æ–‡æœ¬è¿›è¡Œ tokenizationï¼Œæˆ‘ä»¬åªéœ€è¿›è¡Œé¢„åˆ†è¯ç„¶åä½¿ç”¨æˆ‘ä»¬çš„ `encode_word()` å‡½æ•°ï¼š

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

è‡³æ­¤ Unigram çš„ä»‹ç»å®Œæ¯•ï¼æœŸæœ›æ­¤åˆ»ä½ å·²æ„Ÿè§‰è‡ªèº«å¦‚åŒé¢†åŸŸçš„ä¸“å®¶ä¸€èˆ¬ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ğŸ¤—Tokenizers åº“çš„åŸºæœ¬æ„é€ æ¨¡å—ï¼Œå¹¶å±•ç¤ºå¦‚ä½•ä½¿ç”¨å®ƒä»¬æ„å»ºè‡ªå·±çš„ tokenizer 