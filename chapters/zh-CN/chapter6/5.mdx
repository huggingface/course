# BPE tokenization ç®—æ³• [[BPE tokenizationç®—æ³•]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb"},
]} />

å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æœ€åˆè¢«å¼€å‘ä¸ºä¸€ç§å‹ç¼©æ–‡æœ¬çš„ç®—æ³•ï¼Œç„¶ååœ¨é¢„è®­ç»ƒ GPT æ¨¡å‹æ—¶è¢« OpenAI ç”¨äº tokenizationã€‚è®¸å¤š Transformer æ¨¡å‹éƒ½ä½¿ç”¨å®ƒï¼ŒåŒ…æ‹¬ GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTaã€‚

<Youtube id="HEikzVL-lZU"/>

> [!TIP]
> ğŸ’¡ æœ¬èŠ‚æ·±å…¥ä»‹ç»äº† BPEï¼Œç”šè‡³å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„å®ç°ã€‚å¦‚æœä½ åªæƒ³å¤§è‡´äº†è§£ tokenization ç®—æ³•ï¼Œå¯ä»¥è·³åˆ°æœ€åã€‚

## BPE è®­ç»ƒ [[BPE è®­ç»ƒ]]

BPE è®­ç»ƒé¦–å…ˆè®¡ç®—è¯­æ–™åº“ä¸­ä½¿ç”¨çš„å”¯ä¸€å•è¯é›†åˆï¼ˆåœ¨å®Œæˆæ ‡å‡†åŒ–å’Œé¢„åˆ†è¯æ­¥éª¤ä¹‹åï¼‰ï¼Œç„¶åå–å‡ºç”¨æ¥ç¼–å†™è¿™äº›è¯çš„æ‰€æœ‰ç¬¦å·æ¥æ„å»ºè¯æ±‡è¡¨ã€‚ä¸¾ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­ï¼Œå‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“ä½¿ç”¨äº†è¿™äº”ä¸ªè¯ï¼š

```
"hug", "pug", "pun", "bun", "hugs"
```

åŸºç¡€å•è¯é›†åˆå°†æ˜¯ `["b", "g", "h", "n", "p", "s", "u"]` ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåŸºæœ¬è¯æ±‡è¡¨å°†è‡³å°‘åŒ…å«æ‰€æœ‰ ASCII å­—ç¬¦ï¼Œå¯èƒ½è¿˜åŒ…å«ä¸€äº› Unicode å­—ç¬¦ã€‚å¦‚æœä½ æ­£åœ¨ tokenization ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­çš„å­—ç¬¦ï¼Œåˆ™è¯¥å­—ç¬¦å°†è½¬æ¢ä¸ºæœªçŸ¥ tokensï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®¸å¤š NLP æ¨¡å‹åœ¨åˆ†æå¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„å†…å®¹çš„ç»“æœéå¸¸ç³Ÿç³•çš„åŸå› ä¹‹ä¸€ã€‚

> [!TIP]
> GPT-2 å’Œ RoBERTa ï¼ˆè¿™ä¸¤è€…éå¸¸ç›¸ä¼¼ï¼‰çš„ tokenizer æœ‰ä¸€ä¸ªå·§å¦™çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼šä»–ä»¬ä¸æŠŠå•è¯çœ‹æˆæ˜¯ç”¨ Unicode å­—ç¬¦ç¼–å†™çš„ï¼Œè€Œæ˜¯ç”¨å­—èŠ‚ç¼–å†™çš„ã€‚è¿™æ ·ï¼ŒåŸºæœ¬è¯æ±‡è¡¨çš„å¤§å°å¾ˆå°ï¼ˆ256ï¼‰ï¼Œä½†æ˜¯èƒ½åŒ…å«å‡ ä¹æ‰€æœ‰ä½ èƒ½æƒ³è±¡çš„å­—ç¬¦ï¼Œè€Œä¸ä¼šæœ€ç»ˆè½¬æ¢ä¸ºæœªçŸ¥ tokens è¿™ä¸ªæŠ€å·§è¢«ç§°ä¸º `å­—èŠ‚çº§ï¼ˆbyte-levelï¼‰ BPE` ã€‚

è·å¾—è¿™ä¸ªåŸºç¡€å•è¯é›†åˆåï¼Œæˆ‘ä»¬é€šè¿‡å­¦ä¹  `åˆå¹¶ï¼ˆmergesï¼‰` æ¥æ·»åŠ æ–°çš„ tokens ç›´åˆ°è¾¾åˆ°æœŸæœ›çš„è¯æ±‡è¡¨å¤§å°ã€‚åˆå¹¶æ˜¯å°†ç°æœ‰è¯æ±‡è¡¨ä¸­çš„ä¸¤ä¸ªå…ƒç´ åˆå¹¶ä¸ºä¸€ä¸ªæ–°å…ƒç´ çš„è§„åˆ™ã€‚æ‰€ä»¥ï¼Œä¸€å¼€å§‹ä¼šåˆ›å»ºå‡ºå«æœ‰ä¸¤ä¸ªå­—ç¬¦çš„ tokens ç„¶åï¼Œéšç€è®­ç»ƒçš„è¿›å±•ï¼Œä¼šäº§ç”Ÿæ›´é•¿çš„å­è¯ã€‚

åœ¨åˆ†è¯å™¨è®­ç»ƒæœŸé—´çš„ä»»ä½•ä¸€æ­¥ï¼ŒBPE ç®—æ³•éƒ½ä¼šæœç´¢æœ€å¸¸è§çš„ç°æœ‰ tokens å¯¹ ï¼ˆåœ¨è¿™é‡Œï¼Œâ€œå¯¹â€æ˜¯æŒ‡ä¸€ä¸ªè¯ä¸­çš„ä¸¤ä¸ªè¿ç»­ tokens ï¼‰ã€‚æœ€å¸¸è§çš„è¿™ä¸€å¯¹ä¼šè¢«åˆå¹¶ï¼Œç„¶åæˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚

å›åˆ°æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ï¼Œè®©æˆ‘ä»¬å‡è®¾å•è¯å…·æœ‰ä»¥ä¸‹é¢‘ç‡ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ„æ€æ˜¯ `"hug"` åœ¨è¯­æ–™åº“ä¸­å‡ºç°äº† 10 æ¬¡ï¼Œ `"pug"` å‡ºç°äº† 5 æ¬¡ï¼Œ `"pun"` å‡ºç°äº† 12 æ¬¡ï¼Œ `"bun"` å‡ºç°äº† 4 æ¬¡ï¼Œ `"hugs"` å‡ºç°äº† 5 æ¬¡ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå­—ç¬¦ï¼ˆå½¢æˆæˆ‘ä»¬åˆå§‹è¯æ±‡è¡¨çš„å­—ç¬¦ï¼‰æ¥å¼€å§‹è®­ç»ƒï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†æ¯ä¸ªå•è¯è§†ä¸ºä¸€ä¸ª tokens åˆ—è¡¨ï¼š

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ç„¶åæˆ‘ä»¬çœ‹çœ‹ç›¸é‚»çš„å­—ç¬¦å¯¹ã€‚ `("h", "u")` åœ¨è¯ `"hug"` å’Œ `"hugs"` ä¸­å‡ºç°ï¼Œæ‰€ä»¥åœ¨è¯­æ–™åº“ä¸­æ€»å…±å‡ºç°äº† 15 æ¬¡ã€‚ç„¶è€Œï¼Œæœ€å¸¸è§çš„å¯¹å±äº `("u", "g")` ï¼Œå®ƒåœ¨ `"hug"` ã€ `"pug"` å’Œ `"hugs"` ä¸­å‡ºç°ï¼Œæ€»å…±åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°äº† 20 æ¬¡ã€‚

å› æ­¤ï¼Œtokenizer å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "g") -> "ug"` ï¼Œæ„æ€å°±æ˜¯ `"ug"` å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œä¸”åº”åœ¨è¯­æ–™åº“çš„æ‰€æœ‰è¯ä¸­åˆå¹¶è¿™ä¸€å¯¹ã€‚åœ¨è¿™ä¸ªé˜¶æ®µç»“æŸæ—¶ï¼Œè¯æ±‡è¡¨å’Œè¯­æ–™åº“çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```
è¯æ±‡è¡¨: ["b", "g", "h", "n", "p", "s", "u", "ug"]
è¯­æ–™åº“: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€äº›å¯¹ï¼Œç»§ç»­åˆå¹¶çš„è¯ä¼šäº§ç”Ÿä¸€ä¸ªæ¯”ä¸¤ä¸ªå­—ç¬¦é•¿çš„ tokens ä¾‹å¦‚ `("h", "ug")` ï¼Œåœ¨è¯­æ–™åº“ä¸­å‡ºç° 15 æ¬¡ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé˜¶æ®µå‡ºç°é¢‘ç‡æœ€é«˜çš„å¯¹æ˜¯ `("u", "n")` ï¼Œåœ¨è¯­æ–™åº“ä¸­å‡ºç° 16 æ¬¡ï¼Œæ‰€ä»¥å­¦åˆ°çš„ç¬¬äºŒä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "n") -> "un"` ã€‚å°†å…¶æ·»åŠ åˆ°è¯æ±‡è¡¨å¹¶åˆå¹¶æ‰€æœ‰ç°æœ‰çš„è¿™ä¸ªå¯¹ï¼Œå°†å‡ºç°ï¼š

```
è¯æ±‡è¡¨: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
è¯­æ–™åº“: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æœ€é¢‘ç¹çš„ä¸€å¯¹æ˜¯ `("h", "ug")` ï¼Œæ‰€ä»¥æˆ‘ä»¬å­¦ä¹ äº†åˆå¹¶è§„åˆ™ `("h", "ug") -> "hug"` ï¼Œè¿™å½¢æˆäº†æˆ‘ä»¬ç¬¬ä¸€ä¸ªä¸‰ä¸ªå­—æ¯çš„ tokens åˆå¹¶åï¼Œè¯­æ–™åº“å¦‚ä¸‹æ‰€ç¤ºï¼š

```
è¯æ±‡è¡¨: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
è¯­æ–™åº“: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

æˆ‘ä»¬ç»§ç»­è¿™æ ·åˆå¹¶ï¼Œç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æ‰€éœ€çš„è¯æ±‡é‡ã€‚

> [!TIP]
> âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ä½ è®¤ä¸ºä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

## tokenization [[tokenization]]

å®Œæˆè®­ç»ƒä¹‹åå°±å¯ä»¥å¯¹æ–°çš„è¾“å…¥ tokenization äº†ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œæ–°çš„è¾“å…¥ä¼šä¾ç…§ä»¥ä¸‹æ­¥éª¤å¯¹æ–°è¾“å…¥è¿›è¡Œ tokenizationï¼š

1. æ ‡å‡†åŒ–
2. é¢„åˆ†è¯
3. å°†å•è¯æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦
4. æ ¹æ®å­¦ä¹ çš„åˆå¹¶è§„åˆ™ï¼ŒæŒ‰é¡ºåºåˆå¹¶æ‹†åˆ†çš„å­—ç¬¦

è®©æˆ‘ä»¬ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç¤ºä¾‹ä¸ºä¾‹ï¼ŒTokenizer å­¦ä¹ åˆ°äº†ä¸‰ä¸ªåˆå¹¶è§„åˆ™ï¼š

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå•è¯ `"bug"` å°†è¢«è½¬åŒ–ä¸º `["b", "ug"]` ã€‚ç„¶è€Œ `"mug"` ï¼Œå°†è¢«è½¬æ¢ä¸º `["[UNK]", "ug"]` ï¼Œå› ä¸ºå­—æ¯ `"m"` ä¸å†åŸºæœ¬è¯æ±‡è¡¨ä¸­ã€‚åŒæ ·ï¼Œå•è¯ `"thug"` ä¼šè¢«è½¬æ¢ä¸º `["[UNK]", "hug"]` ï¼šå­—æ¯ `"t"` ä¸åœ¨åŸºæœ¬è¯æ±‡è¡¨ä¸­ï¼Œä½¿ç”¨åˆå¹¶è§„åˆ™é¦–å…ˆä¼šå°† `"u"` å’Œ `"g"` åˆå¹¶ï¼Œç„¶åå°† `"h"` å’Œ `"ug"` åˆå¹¶ã€‚

> [!TIP]
> âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ä½ è®¤ä¸ºè¿™ä¸ªè¯ `"unhug"` å°†å¦‚ä½•è¢« tokenizationï¼Ÿ

## å®ç° BPE ç®—æ³•[[å®ç° BPE ç®—æ³•]]

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ BPE ç®—æ³•çš„å®ç°ã€‚è¿™å¹¶ä¸æ˜¯åœ¨å¤§å‹è¯­æ–™åº“ä¸Šå®é™…ä½¿ç”¨çš„ç»è¿‡ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼›æˆ‘ä»¬åªæ˜¯æƒ³å‘ä½ å±•ç¤ºä»£ç ï¼Œä»¥ä¾¿ä½ å¯ä»¥æ›´å¥½åœ°ç†è§£ç®—æ³•

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯­æ–™åº“ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå«æœ‰å‡ å¥è¯çš„ç®€å•è¯­æ–™åº“ï¼š

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†è¯¥è¯­æ–™åº“é¢„åˆ†è¯ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤ç°ä¸€ä¸ª BPE  tokenizer ï¼ˆä¾‹å¦‚ GPT-2ï¼‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `gpt2` åˆ†è¯å™¨è¿›è¡Œé¢„åˆ†è¯ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ç„¶åï¼Œæˆ‘ä»¬åœ¨è¿›è¡Œé¢„åˆ†è¯çš„åŒæ—¶è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼š

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

ä¸‹ä¸€æ­¥æ˜¯è®¡ç®—åŸºç¡€è¯æ±‡è¡¨ï¼Œè¿™ç”±è¯­æ–™åº“ä¸­ä½¿ç”¨çš„æ‰€æœ‰å­—ç¬¦ç»„æˆï¼š

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokens å¯¹äº GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Š tokens æ˜¯ `"<|endoftext|>"` ï¼š

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

æˆ‘ä»¬ç°åœ¨éœ€è¦å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå•ç‹¬çš„å­—ç¬¦ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¼€å§‹è®­ç»ƒï¼š

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ç°åœ¨æˆ‘ä»¬å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹å­—ç¬¦çš„é¢‘ç‡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒï¼š

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªå­—å…¸åœ¨åˆå§‹åˆå¹¶åçš„ä¸€äº›ç»“æœï¼š

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

ç°åœ¨ï¼Œåªéœ€è¦ä¸€ä¸ªç®€å•çš„å¾ªç¯å°±å¯ä»¥æ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„å¯¹ï¼š


```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ä ', 't') 7
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶è§„åˆ™æ˜¯ `('Ä ', 't') -> 'Ä t'` ï¼Œæˆ‘ä»¬å°† `'Ä t'` æ·»åŠ åˆ°è¯æ±‡è¡¨ï¼š

```python
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„ `splits` å­—å…¸ä¸­è¿›è¡Œè¿™ä¸ªåˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™å¦ä¸€ä¸ªå‡½æ•°ï¼š

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿä¸€ä¸‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœï¼š

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```python out
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰ä»£ç ï¼Œå¯ä»¥å¾ªç¯ç›´åˆ°æˆ‘ä»¬å­¦ä¹ åˆ°æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰åˆå¹¶ã€‚è®©æˆ‘ä»¬æŠŠç›®æ ‡è¯æ±‡è¡¨çš„å¤§å°è®¾å®šä¸º 50ï¼š

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

æœ€ç»ˆï¼Œæˆ‘ä»¬å­¦ä¹ äº† 19 æ¡åˆå¹¶è§„åˆ™ï¼ˆåˆå§‹è¯æ±‡é‡ä¸º 31 â€”â€” å­—æ¯è¡¨ä¸­çš„ 30 ä¸ªå­—ç¬¦ï¼ŒåŠ ä¸Šç‰¹æ®Š token ï¼‰ï¼š

```py
print(merges)
```

```python out
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

è¯æ±‡è¡¨ç”±ç‰¹æ®Š token åˆå§‹å­—æ¯å’Œæ‰€æœ‰åˆå¹¶ç»“æœç»„æˆï¼š

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

> [!TIP]
> ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` å¯èƒ½ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸ºå½“æœ‰å¤šä¸ªå‡ºç°é¢‘ç‡æœ€é«˜çš„å¯¹æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹©é‡åˆ°çš„ç¬¬ä¸€ä¸ªï¼Œè€Œ ğŸ¤— Tokenizers åº“æ ¹æ®å†…éƒ¨ ID é€‰æ‹©ç¬¬ä¸€ä¸ªã€‚

ä¸ºäº†å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œé¢„åˆ†è¯ã€æ‹†åˆ†ï¼Œç„¶åä½¿ç”¨å­¦åˆ°çš„æ‰€æœ‰åˆå¹¶è§„åˆ™ï¼š

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

æˆ‘ä»¬å¯ä»¥å°è¯•åœ¨ä»»ä½•ç”±å­—æ¯è¡¨ä¸­çš„å­—ç¬¦ç»„æˆçš„æ–‡æœ¬ä¸Šè¿›è¡Œæ­¤æ“ä½œï¼š


```py
tokenize("This is not a token.")
```

```python out
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

> [!WARNING]
> âš ï¸ å¦‚æœå­˜åœ¨æœªçŸ¥å­—ç¬¦ï¼Œæˆ‘ä»¬çš„å®ç°å°†æŠ›å‡ºé”™è¯¯ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰åšä»»ä½•å¤„ç†å®ƒä»¬ã€‚GPT-2 å®é™…ä¸Šæ²¡æœ‰æœªçŸ¥ tokens ï¼ˆä½¿ç”¨å­—èŠ‚çº§ BPE æ—¶ä¸å¯èƒ½å¾—åˆ°æœªçŸ¥å­—ç¬¦ï¼‰ï¼Œä½†è¿™é‡Œçš„ä»£ç å¯èƒ½ä¼šå‡ºç°è¿™ä¸ªé”™è¯¯ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶æœªåœ¨åˆå§‹è¯æ±‡ä¸­åŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ã€‚BPE çš„è¿™ä¸€éƒ¨åˆ†å·²è¶…å‡ºäº†æœ¬èŠ‚çš„èŒƒå›´ï¼Œå› æ­¤æˆ‘ä»¬çœç•¥äº†ä¸€äº›ç»†èŠ‚ã€‚

è‡³æ­¤ï¼ŒBPE ç®—æ³•çš„ä»‹ç»å°±åˆ°æ­¤ç»“æŸï¼æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ WordPiece ç®—æ³•ã€‚