<FrameworkSwitchCourse {fw} />

# åœ¨ QA ç®¡é“ä¸­ä½¿ç”¨å¿«é€Ÿ tokenizer  [[åœ¨ QA ç®¡é“ä¸­ä½¿ç”¨å¿«é€Ÿ tokenizer ]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3b_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3b_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3b_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3b_tf.ipynb"},
]} />

{/if}

æˆ‘ä»¬ç°åœ¨å°†æ·±å…¥ç ”ç©¶ `question-answering` ç®¡é“ï¼Œçœ‹çœ‹å¦‚ä½•åˆ©ç”¨åç§»é‡ä»ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ä¸­è·å–å½“å‰é—®é¢˜çš„ç­”æ¡ˆï¼Œè¿™ä¸æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­å¤„ç†åˆ†ç»„å®ä½“çš„æ–¹å¼æœ‰äº›ç›¸ä¼¼ã€‚æˆ‘ä»¬ä¼šçœ‹åˆ°å¦‚ä½•å¤„ç†é‚£äº›å› ä¸ºè¿‡é•¿è€Œæœ€ç»ˆè¢«æˆªæ–­çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ã€‚å¦‚æœä½ å¯¹é—®ç­”ä»»åŠ¡ä¸æ„Ÿå…´è¶£ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€èŠ‚ã€‚

{#if fw === 'pt'}

<Youtube id="_wxyB3j3mk4"/>

{:else}

<Youtube id="b3u8RzBCX9Y"/>

{/if}

## ä½¿ç”¨ `question-answering` ç®¡é“ [[ä½¿ç”¨ `question-answering` ç®¡é“]]

æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `question-answering` åƒè¿™æ ·çš„ç®¡é“ä»¥è·å¾—é—®é¢˜çš„ç­”æ¡ˆï¼š

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

ä¸å…¶ä»–ä¸èƒ½å¤„ç†è¶…è¿‡æ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦çš„æ–‡æœ¬çš„ç®¡é“ä¸åŒï¼Œè¿™ä¸ªç®¡é“å¯ä»¥å¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œå¹¶ä¸”å³ä½¿ç­”æ¡ˆåœ¨æœ«å°¾ä¹Ÿèƒ½è¿”å›é—®é¢˜çš„ç­”æ¡ˆï¼š


```py
long_context = """
ğŸ¤— Transformers: State of the Art NLP

ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„ï¼

## ä½¿ç”¨æ¨¡å‹è¿›è¡Œé—®ç­” [[ä½¿ç”¨æ¨¡å‹è¿›è¡Œé—®ç­”]]

ä¸ä»»ä½•å…¶ä»–ç®¡é“ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œ tokenizeï¼Œç„¶åå°†å…¶ä¼ å…¥æ¨¡å‹ã€‚ `question-answering` ç®¡é“é»˜è®¤æƒ…å†µä¸‹ç”¨äºçš„ checkpoint æ˜¯ [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad) ï¼ˆåå­—ä¸­çš„"squad"æºè‡ªæ¨¡å‹å¾®è°ƒæ‰€ç”¨çš„æ•°æ®é›†ï¼›æˆ‘ä»¬å°†åœ¨ [ç¬¬ä¸ƒç« ](/course/chapter7/7) è¯¦ç»†è®¨è®º SQuAD æ•°æ®é›†ï¼‰:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)
```

{/if}

è¯·æ³¨æ„åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é—®é¢˜æ”¾åœ¨å‰é¢å’Œä¸Šä¸‹æ–‡æ”¾åé¢ï¼Œä¸€èµ·ä½œä¸ºä¸€å¯¹è¿›è¡Œtokenizationã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

é—®ç­”æ¨¡å‹çš„å·¥ä½œæ–¹å¼ä¸æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢çœ‹åˆ°çš„æ¨¡å‹ç•¥æœ‰ä¸åŒã€‚ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œæ¨¡å‹è®­ç»ƒçš„ç›®æ ‡æ˜¯æ¥é¢„æµ‹ç­”æ¡ˆå¼€å§‹çš„ token çš„ç´¢å¼•ï¼ˆè¿™é‡Œæ˜¯ 21ï¼‰å’Œç­”æ¡ˆç»“æŸçš„ token çš„ç´¢å¼•ï¼ˆè¿™é‡Œæ˜¯ 24ï¼‰ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™äº›æ¨¡å‹ä¸è¿”å›ä¸€ä¸ª logits çš„å¼ é‡ï¼Œè€Œæ˜¯è¿”å›ä¸¤ä¸ªï¼šä¸€ä¸ªå¯¹åº”äºç­”æ¡ˆçš„å¼€å§‹ token çš„ logitsï¼Œå¦ä¸€ä¸ªå¯¹åº”äºç­”æ¡ˆçš„ç»“æŸ token çš„ logitsã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„è¾“å…¥åŒ…å«äº† 66 ä¸ª token ï¼Œå› æ­¤æˆ‘ä»¬å¾—åˆ°ï¼š

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

{:else}

```python out
(1, 66) (1, 66)
```

{/if}

ä¸ºäº†å°†è¿™äº› logits è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª softmax å‡½æ•°â€”â€”ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬å±è”½äº†ä¸å±äºä¸Šä¸‹æ–‡çš„ç´¢å¼•ã€‚æˆ‘ä»¬çš„è¾“å…¥æ ¼å¼æ˜¯ `[CLS] question [SEP] context [SEP]` ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å±è”½ question çš„ tokens ä»¥åŠ `[SEP]`  token ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬å°†ä¿ç•™ `[CLS]` ï¼Œå› ä¸ºæŸäº›æ¨¡å‹ä½¿ç”¨å®ƒæ¥è¡¨ç¤ºç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚

ç”±äºæˆ‘ä»¬å°†åœ¨ä¹‹åä½¿ç”¨ softmaxï¼Œæˆ‘ä»¬åªéœ€è¦å°†æˆ‘ä»¬æƒ³è¦å±è”½çš„ logits æ›¿æ¢ä¸ºä¸€ä¸ªå¤§çš„è´Ÿæ•°å°±å¯ä»¥åœ¨è®¡ç®— softmax çš„æ—¶å€™å±è”½ä»–ä»¬ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ `-10000` ï¼š

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
# å±è”½é™¤ context ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹
mask = [i != 1 for i in sequence_ids]
# ä¸å±è”½ [CLS] token
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# å±è”½é™¤ context ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹
mask = [i != 1 for i in sequence_ids]
# ä¸å±è”½ [CLS] token
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

ç°åœ¨æˆ‘ä»¬å·²ç»å±è”½äº†ä¸æˆ‘ä»¬ä¸æƒ³é¢„æµ‹çš„ä½ç½®ç›¸å¯¹åº”çš„ logitsï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ softmaxï¼š

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å¯ä»¥å–å¼€å§‹å’Œç»“æŸæ¦‚ç‡çš„ argmax â€”â€” ä½†æ˜¯æˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªæ¯”ç»“æŸç´¢å¼•å¤§çš„å¼€å§‹ç´¢å¼•ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦é‡‡å–ä¸€äº›æ›´å¤šçš„æªæ–½æ¥å¤„ç†è¿™äº›ç‰¹æ®Šæƒ…å†µã€‚æˆ‘ä»¬å°†åœ¨æ»¡è¶³ `start_index <= end_index` çš„å‰æä¸‹è®¡ç®—æ¯ä¸ªå¯èƒ½çš„ `start_index` å’Œ `end_index` çš„æ¦‚ç‡ï¼Œç„¶åå–æ¦‚ç‡æœ€é«˜çš„ `(start_index, end_index)` å…ƒç»„ã€‚

å‡è®¾äº‹ä»¶"ç­”æ¡ˆå¼€å§‹äº `start_index` "å’Œ"ç­”æ¡ˆç»“æŸäº `end_index` "æ˜¯ç‹¬ç«‹çš„ï¼Œç­”æ¡ˆåœ¨ `start_index` å¼€å§‹å¹¶åœ¨ `end_index` ç»“æŸçš„æ¦‚ç‡æ˜¯ï¼š

$$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times\mathrm{end\_probabilities}[\mathrm{end\_index}]$$ 

æ‰€ä»¥ï¼Œè¦è®¡ç®—æ‰€æœ‰çš„åˆ†æ•°ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—æ‰€æœ‰çš„ `start_index <= end_index` çš„ \($$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times\mathrm{end\_probabilities}[\mathrm{end\_index}]$$\)  çš„ä¹˜ç§¯ã€‚


é¦–å…ˆè®©æˆ‘ä»¬è®¡ç®—æ‰€æœ‰å¯èƒ½çš„ä¹˜ç§¯ï¼š

```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

{#if fw === 'pt'}

ç„¶åæˆ‘ä»¬å°† `start_index > end_index` çš„å€¼è®¾ç½®ä¸º `0` æ¥å±è”½ä»–ä»¬ï¼ˆå…¶ä»–æ¦‚ç‡éƒ½æ˜¯æ­£æ•°ï¼‰ã€‚ `torch.triu()` å‡½æ•°è¿”å›ä¼ å…¥çš„ 2D å¼ é‡çš„ä¸Šä¸‰è§’éƒ¨åˆ†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥å®Œæˆå±è”½ï¼š

```py
import numpy as np

scores = torch.triu(scores)
```

{:else}

ç„¶åæˆ‘ä»¬å°† `start_index > end_index` çš„å€¼è®¾ç½®ä¸º `0` æ¥å±è”½ä»–ä»¬ï¼ˆå…¶ä»–æ¦‚ç‡éƒ½æ˜¯æ­£æ•°ï¼‰ã€‚ `np.triu()` å‡½æ•°è¿”å›ä¼ å…¥çš„ 2D å¼ é‡çš„ä¸Šä¸‰è§’éƒ¨åˆ†ï¼Œæ‰€ä»¥æˆ‘ä»¬åšå¯ä»¥ä½¿ç”¨å®ƒæ¥å®Œæˆå±è”½ï¼š
```py
scores = np.triu(scores)
```

{/if}

ç°åœ¨æˆ‘ä»¬åªéœ€è¦å¾—åˆ°æœ€å¤§å€¼çš„ç´¢å¼•ã€‚ç”±äº PyTorch å°†è¿”å›å±•å¹³ï¼ˆflattenedï¼‰åå¼ é‡ä¸­çš„ç´¢å¼•ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨å‘ä¸‹å–æ•´çš„é™¤æ³• `//` å’Œå–æ¨¡ `%` æ“ä½œæ¥è·å¾— `start_index` å’Œ `end_index` ï¼š

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

æˆ‘ä»¬è¿˜æ²¡æœ‰å®Œå…¨å®Œæˆï¼Œä½†è‡³å°‘æˆ‘ä»¬å·²ç»æœ‰äº†æ­£ç¡®çš„ç­”æ¡ˆåˆ†æ•°ï¼ˆä½ å¯ä»¥é€šè¿‡å°†å…¶ä¸ä¸Šä¸€èŠ‚ä¸­çš„ç¬¬ä¸€ä¸ªç»“æœè¿›è¡Œæ¯”è¾ƒæ¥æ£€æŸ¥è¿™ä¸€ç‚¹ï¼‰ï¼š

```python out
0.97773
```

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** è®¡ç®—äº”ä¸ªæœ€å¯èƒ½çš„ç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸç´¢å¼•ã€‚

</Tip>

æˆ‘ä»¬æœ‰äº†ç­”æ¡ˆçš„ `start_index` å’Œ `end_index` ï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬åªéœ€è¦å°†ä»–ä»¬è½¬æ¢ä¸ºä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ç´¢å¼•ã€‚è¿™å°±æ˜¯åç§»é‡å°†ä¼šéå¸¸æœ‰ç”¨çš„åœ°æ–¹ã€‚æˆ‘ä»¬å¯ä»¥åƒæˆ‘ä»¬åœ¨ token åˆ†ç±»ä»»åŠ¡ä¸­é‚£æ ·è·å–åç§»é‡å¹¶ä½¿ç”¨å®ƒä»¬ï¼š

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

ç°åœ¨æˆ‘ä»¬åªéœ€è¦æ ¼å¼åŒ–æ‰€æœ‰å†…å®¹ï¼Œè·å–æˆ‘ä»¬çš„ç»“æœï¼š


```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

å¤ªæ£’äº†ï¼è¿™å’Œæˆ‘ä»¬ä¸Šé¢è·å–çš„ç»“æœä¸€æ ·ï¼

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** ä½¿ç”¨ä½ ä¹‹å‰è®¡ç®—çš„æœ€ä½³åˆ†æ•°æ¥æ˜¾ç¤ºäº”ä¸ªæœ€å¯èƒ½çš„ç­”æ¡ˆã€‚ä½ å¯ä»¥å›åˆ°ä¹‹å‰çš„ QA pipelineï¼Œå¹¶åœ¨è°ƒç”¨æ—¶ä¼ å…¥ `top_k=5` æ¥å¯¹æ¯”æ£€æŸ¥ä½ çš„ç»“æœã€‚

</Tip>

## å¤„ç†é•¿æ–‡æœ¬ [[å¤„ç†é•¿æ–‡æœ¬]]

å¦‚æœæˆ‘ä»¬å°è¯•å°†æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„é•¿é—®é¢˜å’Œé•¿ä¸Šä¸‹æ–‡è¿›è¡Œ tokenizeï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªæ¯” `question-answering` pipeline ä¸­ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰æ›´å¤§çš„ tokens æ•°é‡ï¼š

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„è¾“å…¥æˆªæ–­åˆ°æ¨¡å‹å…è®¸è¾“å…¥çš„æœ€å¤§é•¿åº¦ã€‚æˆ‘ä»¬å¯ä»¥ç”¨å‡ ç§æ–¹å¼åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†æˆ‘ä»¬ä¸æƒ³æˆªæ–­é—®é¢˜éƒ¨åˆ†ï¼Œåªæƒ³æˆªæ–­ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼Œå¹¶ä¸”ç”±äºä¸Šä¸‹æ–‡éƒ¨åˆ†æ˜¯ç¬¬äºŒé¡¹ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `"only_second"` æˆªæ–­ç­–ç•¥ã€‚ç„¶ååˆå‡ºç°äº†æ–°çš„é—®é¢˜ï¼šé—®é¢˜çš„ç­”æ¡ˆå¯èƒ½åœ¨æˆªæ–­åè¢«ä¸¢å¼ƒäº†ï¼Œå¹¶æ²¡æœ‰åœ¨æˆªæ–­åä¿ç•™ä¸‹æ¥çš„ä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬é€‰äº†ä¸€ä¸ªé—®é¢˜ï¼Œå…¶ä¸­çš„ç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡çš„æœ«å°¾ï¼Œå½“æˆ‘ä»¬æˆªæ–­å®ƒæ—¶ï¼Œç­”æ¡ˆå°±ä¸åœ¨é‡Œé¢äº†ï¼š

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""
```

è¿™æ„å‘³ç€æ¨¡å‹å°†å¾ˆéš¾æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ `question-answering` ç®¡é“å…è®¸æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡åˆ†æˆæ›´å°çš„å—ï¼ŒæŒ‡å®šæœ€å¤§é•¿åº¦ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬ä¸åœ¨åˆšå¥½å¯èƒ½æ‰¾åˆ°ç­”æ¡ˆçš„åœ°æ–¹å°†ä¸Šä¸‹æ–‡åˆ†å‰²ï¼Œå®ƒè¿˜åœ¨å„å—ä¹‹é—´åŒ…å«äº†ä¸€äº›é‡å ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ  `return_overflowing_tokens=True` å‚æ•°ï¼Œå¹¶å¯ä»¥ç”¨ `stride` å‚æ•°æŒ‡å®šæˆ‘ä»¬æƒ³è¦çš„é‡å é•¿åº¦æ¥è®© tokenizer ï¼ˆå¿«é€Ÿæˆ–æ…¢é€Ÿï¼‰ä¸ºæˆ‘ä»¬åšè¿™ä¸ªå·¥ä½œã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨è¾ƒçŸ­çš„å¥å­çš„ä¾‹å­ï¼š

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå¥å­å·²è¢«åˆ†æˆå¤šä¸ªå—ï¼Œä½¿å¾—æ¯ä¸ªæ¡ç›® `inputs["input_ids"]` æœ€å¤šæœ‰ 6 ä¸ª token ï¼ˆæˆ‘ä»¬éœ€è¦æ·»åŠ å¡«å……ä»¥ä½¿åˆ†å‰²åçš„æœ€åä¸€ä¸ªæ¡ç›®ä¸å…¶ä»–æ¡ç›®çš„å¤§å°ç›¸åŒï¼‰å¹¶ä¸”æ¯ä¸ªæ¡ç›®ä¹‹é—´æœ‰ 2 ä¸ª token çš„é‡å ã€‚

è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹tokenizationçš„ç»“æœï¼š

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```


æ­£å¦‚æˆ‘ä»¬æ‰€é¢„æœŸçš„ï¼Œæˆ‘ä»¬å¾—åˆ°äº†inputs ID å’Œæ³¨æ„åŠ›æ©ç ã€‚æœ€åä¸€ä¸ªé”®ï¼Œoverflow_to_sample_mappingï¼Œæ˜¯ä¸€ä¸ªæ˜ å°„ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªç»“æœå¯¹åº”å“ªä¸ªå¥å­â€”â€”åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ 7 ä¸ªç»“æœï¼Œå®ƒä»¬éƒ½æ¥è‡ªæˆ‘ä»¬ä¼ é€’ç»™ tokenizer çš„ï¼ˆå”¯ä¸€çš„ï¼‰å¥å­ï¼š

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

å½“æˆ‘ä»¬ä¸€èµ·å¯¹å¤šä¸ªå¥å­ tokenize æ—¶ï¼Œè¿™ä¼šæ›´æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ªï¼š

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

è¾“å‡ºçš„ç»“æœæ˜¯ï¼š

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

è¿™æ„å‘³ç€ç¬¬ä¸€å¥è¯è¢«åˆ†å‰²æˆ 7 ä¸ªå—ï¼Œå°±åƒä¸Šé¢çš„ä¾‹å­ä¸€æ ·ï¼Œåé¢çš„ 4 ä¸ªå—æ¥è‡ªç¬¬äºŒå¥è¯ã€‚

ç°åœ¨è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„é•¿ä¸Šä¸‹æ–‡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ `question-answering pipeline` ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ 384 ä½œä¸ºæœ€å¤§é•¿åº¦ï¼Œæ­¥é•¿ä¸º 128ï¼Œè¿™ä¸æ¨¡å‹çš„å¾®è°ƒæ–¹å¼ç›¸å¯¹åº”ï¼ˆä½ å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨ pipeline æ—¶ä¼ é€’ `max_seq_len` å’Œ `stride` å‚æ•°æ¥è°ƒæ•´è¿™äº›å‚æ•°ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ tokenization æ—¶å°†ä½¿ç”¨å¾®è°ƒæ—¶ä½¿ç”¨çš„è¿™äº›å‚æ•°ã€‚æˆ‘ä»¬è¿˜ä¼šæ·»åŠ å¡«å……ï¼ˆä½¿æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ„å»ºæ‹¼æ¥æˆä¸€ä¸ªçŸ©å½¢çš„å¼ é‡ï¼‰ï¼Œå¹¶è·å–åç§»é‡ï¼š


```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

è¿™äº› `inputs` å°†åŒ…å«æ¨¡å‹æœŸæœ›çš„inputs ID å’Œæ³¨æ„åŠ›æ©ç ï¼Œä»¥åŠæˆ‘ä»¬åˆšåˆšè°ˆåˆ°çš„åç§»é‡å’Œ `overflow_to_sample_mapping` ã€‚ç”±äºæ¨¡å‹ä¸éœ€è¦è¿™ä¸¤ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬å°†å®ƒä»¬ä» `inputs` ä¸­åˆ é™¤ï¼ˆæˆ‘ä»¬ä¸ä¼šå­˜å‚¨æ˜ å°„çš„å­—å…¸ï¼Œå› ä¸ºè¿™åœ¨è¿™é‡Œæ²¡æœ‰ç”¨ï¼‰ç„¶åå°† `inputs` è½¬æ¢ä¸ºå¼ é‡ï¼š


{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python out
(2, 384)
```

{/if}

æˆ‘ä»¬çš„é•¿ä¸Šä¸‹æ–‡è¢«åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œè¿™æ„å‘³ç€åœ¨ç»è¿‡æˆ‘ä»¬çš„æ¨¡å‹åï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸¤ç»„å¼€å§‹å’Œç»“æŸçš„ logitsï¼š

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python out
(2, 384) (2, 384)
```

{/if}

å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬åœ¨è®¡ç®— softmax ä¹‹å‰å±è”½ä¸å±äºä¸Šä¸‹æ–‡çš„ token ã€‚æˆ‘ä»¬è¿˜å±è”½äº†æ‰€æœ‰å¡«å…… token ï¼ˆå¦‚æ³¨æ„åŠ›æ©ç ï¼‰ï¼š

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
# å±è”½é™¤ context tokens ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹
mask = [i != 1 for i in sequence_ids]
# å–æ¶ˆå¯¹ [CLS] token çš„å±è”½
mask[0] = False
# å±è”½æ‰€æœ‰çš„ [PAD] tokens
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
# å±è”½é™¤ context tokens ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹
mask = [i != 1 for i in sequence_ids]
# å–æ¶ˆå¯¹ [CLS] tokençš„å±è”½
mask[0] = False
# å±è”½æ‰€æœ‰çš„ [PAD] tokens
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ softmax å°†æˆ‘ä»¬çš„ logits è½¬æ¢ä¸ºæ¦‚ç‡ï¼š

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()
```

{/if}

ä¸‹ä¸€æ­¥ä¸æˆ‘ä»¬å¯¹çŸ­çš„ä¸Šä¸‹æ–‡æ‰€åšçš„ç±»ä¼¼ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬å°†å¯¹ä¸¤ä¸ªå—åˆ†åˆ«è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬ä¸ºæ‰€æœ‰å¯èƒ½çš„å›ç­”èŒƒå›´èµ‹äºˆä¸€ä¸ªå¾—åˆ†ï¼Œç„¶åé€‰æ‹©å¾—åˆ†æœ€é«˜çš„èŒƒå›´ï¼š

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

è¿™ä¸¤ä¸ªå€™é€‰èŒƒå›´å¯¹åº”çš„æ˜¯æ¨¡å‹åœ¨æ¯ä¸ªå—ä¸­èƒ½å¤Ÿæ‰¾åˆ°çš„æœ€å¥½çš„ç­”æ¡ˆã€‚æ¨¡å‹å¯¹äºæ­£ç¡®çš„ç­”æ¡ˆåœ¨ç¬¬äºŒéƒ¨åˆ†æ›´æœ‰ä¿¡å¿ƒï¼ˆè¿™æ˜¯ä¸ªå¥½å…†å¤´ï¼ï¼‰ã€‚ç°åœ¨æˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸¤ä¸ª token èŒƒå›´æ˜ å°„åˆ°ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦èŒƒå›´ï¼ˆæˆ‘ä»¬åªéœ€è¦æ˜ å°„ç¬¬äºŒä¸ªå°±èƒ½å¾—åˆ°æˆ‘ä»¬çš„ç­”æ¡ˆï¼Œä½†æ˜¯çœ‹çœ‹æ¨¡å‹åœ¨ç¬¬ä¸€å—ä¸­é€‰å–äº†ä»€ä¹ˆä½œä¸ºç­”æ¡ˆè¿˜æ˜¯å¾ˆæœ‰æ„æ€çš„ï¼‰ã€‚


<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼**  è°ƒæ•´ä¸Šé¢çš„ä»£ç ï¼Œä»¥è¿”å›äº”ä¸ªæœ€å¯èƒ½çš„ç­”æ¡ˆçš„å¾—åˆ†å’ŒèŒƒå›´ï¼ˆå¯¹äºæ•´ä¸ªä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å•ä¸ªå—ï¼‰ã€‚

</Tip>

æˆ‘ä»¬ä¹‹å‰æŠ“å– `offsets` çš„å®é™…ä¸Šæ˜¯ä¸€ä¸ªåç§»é‡åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬å—éƒ½æœ‰ä¸€ä¸ªåˆ—è¡¨ï¼š

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python out
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

å¦‚æœæˆ‘ä»¬é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ç¬¬äºŒä¸ªç»“æœï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸ QA ç®¡é“ç›¸åŒç»“æœâ€”â€”è€¶ï¼

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** ä½¿ç”¨ä½ ä¹‹å‰è®¡ç®—çš„æœ€ä½³åˆ†æ•°æ¥æ˜¾ç¤ºäº”ä¸ªæœ€å¯èƒ½çš„ç­”æ¡ˆï¼ˆå¯¹äºæ•´ä¸ªä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å•ä¸ªå—ï¼‰ã€‚å¦‚æœæƒ³è¦ä¸ pipeline å¯¹æ¯”æ£€æŸ¥ä½ çš„ç»“æœçš„è¯ï¼Œè¿”å›ä¹‹å‰çš„ QA ç®¡é“ï¼Œå¹¶åœ¨è°ƒç”¨æ—¶ä¼ å…¥ `top_k=5` çš„å‚æ•°ã€‚

</Tip>

æˆ‘ä»¬å·²ç»ç»“æŸäº†æˆ‘ä»¬å¯¹ tokenizer èƒ½åŠ›çš„æ·±å…¥æ¢ç©¶ã€‚åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨ä¸€ç³»åˆ—å¸¸è§çš„ NLP ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å°†å¯¹è¿™äº›å†…å®¹å†æ¬¡ä»˜è¯¸å®è·µã€‚