<FrameworkSwitchCourse {fw} />

# ä»å¤´å¼€å§‹è®­ç»ƒå› æœè¯­è¨€æ¨¡å‹

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
]} />

{/if}

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡é‡ç”¨é¢„è®­ç»ƒçš„æƒé‡æ¥é’ˆå¯¹æ–°ç”¨ä¾‹å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒã€‚æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬ä¸€ç« ](/course/chapter1), è¿™é€šå¸¸ç§°ä¸ºè¿ç§»å­¦ä¹ ï¼Œè¿™æ˜¯å°† Transformer æ¨¡å‹åº”ç”¨äºå¤§å¤šæ•°æ ‡è®°æ•°æ®ç¨€ç–çš„ç°å®ä¸–ç•Œç”¨ä¾‹çš„éå¸¸æˆåŠŸçš„ç­–ç•¥ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸åŒçš„æ–¹æ³•å¹¶ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨æœ‰å¤§é‡æ•°æ®å¹¶ä¸”å®ƒä¸ç”¨äºå¯ç”¨æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®æœ‰å¾ˆå¤§ä¸åŒï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºæ¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè€Œä¸ä»…ä»…æ˜¯å¾®è°ƒç°æœ‰çš„æ¨¡å‹ã€‚è®­ç»ƒæ–°æ¨¡å‹æœ‰æ„ä¹‰çš„ç¤ºä¾‹åŒ…æ‹¬ç”±éŸ³ç¬¦ã€åˆ†å­åºåˆ—ï¼ˆå¦‚ DNAï¼‰æˆ–ç¼–ç¨‹è¯­è¨€ç»„æˆçš„æ•°æ®é›†ã€‚åè€…æœ€è¿‘å—åˆ°å…³æ³¨ï¼Œè¿™è¦å½’åŠŸäº TabNine å’Œ GitHub çš„ Copilot ç­‰å·¥å…·ï¼Œå®ƒä»¬ç”± OpenAI çš„ Codex æ¨¡å‹æä¾›æ”¯æŒï¼Œå¯ä»¥ç”Ÿæˆé•¿ä»£ç åºåˆ—ã€‚è¿™ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æœ€å¥½ä½¿ç”¨è‡ªå›å½’æˆ–å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰æ¥è§£å†³ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä»£ç ç”Ÿæˆæ¨¡å‹çš„ç¼©å°ç‰ˆæœ¬ï¼šæˆ‘ä»¬å°†ä½¿ç”¨ Python ä»£ç çš„å­é›†ä¸“æ³¨äºå•è¡Œå®Œæˆè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ã€‚åœ¨ Python ä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šç»å¸¸æ¥è§¦ Python æ•°æ®ç§‘å­¦å †æ ˆï¼ŒåŒ…æ‹¬ `matplotlib` , `seaborn` , `pandas` ï¼Œ å’Œ `scikit-learn` åº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œé€šå¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°±å¤ªå¥½äº†ã€‚

<Youtube id="Vpjb1lu0MDk"/>

åœ¨[ç¬¬å…­ç« ](/course/chapter6) æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„åˆ†è¯å™¨æ¥å¤„ç† Python æºä»£ç ï¼Œä½†æˆ‘ä»¬ä»ç„¶éœ€è¦ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†æ¥é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„åˆ†è¯å™¨åº”ç”¨åˆ°æºè‡ª GitHub å­˜å‚¨åº“çš„ Python ä»£ç è¯­æ–™åº“ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ `Trainer` API å’Œ ğŸ¤— Accelerate æ¥è®­ç»ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-codeparrot-ds-darkmode.hf.space" frameBorder="0" height="300" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

è¿™å®é™…ä¸Šå±•ç¤ºäº†ä½¿ç”¨æœ¬èŠ‚ä¸­è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28)æ‰¾åˆ°ã€‚è¯·æ³¨æ„ï¼Œç”±äºåœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿäº†ä¸€äº›éšæœºåŒ–ï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°ç•¥æœ‰ä¸åŒçš„ç»“æœã€‚
## æ”¶é›†æ•°æ®

Python ä»£ç å¯ä»¥ä» GitHub ç­‰ä»£ç å­˜å‚¨åº“ä¸­è·å¾—ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŠ“å–æ¯ä¸ª Python å­˜å‚¨åº“æ¥ä½¿ç”¨å®ƒä»¬æ¥åˆ›å»ºæ•°æ®é›†ã€‚è¿™æ˜¯åœ¨[Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/)é¢„è®­ç»ƒå¤§å‹çš„GPT-2 æ¨¡å‹ã€‚ä½¿ç”¨å¤§çº¦ 180 GB çš„ GitHub è½¬å‚¨ï¼Œå…¶ä¸­åŒ…å«å¤§çº¦ 2000 ä¸‡ä¸ª Python æ–‡ä»¶ï¼Œç§°ä¸º `codeparrot` ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶ååœ¨[Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot)ä¸Šåˆ†äº«å‡ºæ¥äº†.

ç„¶è€Œï¼Œå¯¹å®Œæ•´è¯­æ–™åº“çš„è®­ç»ƒæ—¢è€—æ—¶åˆè´¹åŠ›ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸ Python æ•°æ®ç§‘å­¦å †æ ˆç›¸å…³çš„æ•°æ®é›†å­é›†ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬å¼€å§‹è¿‡æ»¤ `codeparrot` åŒ…å«æ­¤å †æ ˆä¸­ä»»ä½•åº“çš„æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®é›†ã€‚ç”±äºæ•°æ®é›†çš„å¤ªå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›é¿å…ä¸‹è½½å®ƒï¼›å› æ­¤åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æµåŠŸèƒ½æ¥åŠ¨æ€è¿‡æ»¤å®ƒã€‚ä¸ºäº†ä½¿ç”¨å‰é¢æåˆ°çš„åº“è¿‡æ»¤ä»£ç ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

è®©æˆ‘ä»¬ç”¨ä¸¤ä¸ªä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æµå¼ä¼ è¾“æ•°æ®é›†å¹¶è¿‡æ»¤æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ ï¼š

```py
def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ­¤å‡½æ•°åº”ç”¨äºæµæ•°æ®é›†ï¼š

```py
# This cell will take a very long time to execute, so you should skip it and go to
# the next one!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

è¿™ç»™æˆ‘ä»¬ç•™ä¸‹äº†å¤§çº¦ 3% çš„åŸå§‹æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†ä»ç„¶ç›¸å½“å¯è§‚â€”â€”ç»“æœæ•°æ®é›†æœ‰ 6 GBï¼ŒåŒ…å« 600,000 ä¸ª Python è„šæœ¬ï¼è¿‡æ»¤å®Œæ•´æ•°æ®é›†å¯èƒ½éœ€è¦ 2-3 å°æ—¶ï¼Œå…·ä½“å–å†³äºæ‚¨çš„æœºå™¨å’Œå¸¦å®½ã€‚å¦‚æœæ‚¨ä¸æƒ³è‡ªå·±ç»å†è¿™ä¸ªæ¼«é•¿çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨ Hub ä¸Šæä¾›è¿‡æ»¤åçš„æ•°æ®é›†ä¾›æ‚¨ä¸‹è½½ï¼š

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="train")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éœ€è¦ä¸€æ®µæ—¶é—´ã€‚æˆ‘ä»¬å»ºè®®æ‚¨é¦–å…ˆé€šè¿‡å–æ¶ˆæ³¨é‡Šä»¥ä¸Šä¸¤è¡Œçš„æ³¨é‡Šå¯¹æ•°æ®æ ·æœ¬è¿è¡Œè®­ç»ƒå¾ªç¯ï¼Œå¹¶ç¡®ä¿è®­ç»ƒæˆåŠŸå®Œæˆå¹¶å­˜å‚¨æ¨¡å‹ã€‚æ²¡æœ‰ä»€ä¹ˆæ¯”æœ€åä¸€æ­¥çš„è®­ç»ƒå¤±è´¥æ›´ä»¤äººæ²®ä¸§çš„äº†ï¼Œå› ä¸ºä½ å¿˜è®°åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æˆ–è€…å› ä¸ºä¿å­˜è·¯å¾„åœ¨è®­ç»ƒå¾ªç¯ç»“æŸæ—¶æœ‰ä¸€ä¸ªé”™å­—ï¼

</Tip>

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ¥è‡ªæ•°æ®é›†çš„ä¾‹å­ã€‚æˆ‘ä»¬å°†åªæ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰ 200 ä¸ªå­—ç¬¦ï¼š

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `content` å­—æ®µåŒ…å«æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒçš„ä»£ç ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦é¢„å¤„ç†æ–‡æœ¬ï¼Œä½¿å…¶é‡‡ç”¨é€‚åˆé¢„è®­ç»ƒçš„æ ¼å¼ã€‚

## å‡†å¤‡æ•°æ®é›†

<Youtube id="ma1TrR7gE7I"/>

ç¬¬ä¸€æ­¥æ˜¯å¯¹æ•°æ®è¿›è¡Œæ ‡è®°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†å…¶ç”¨äºè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„ç›®æ ‡ä¸»è¦æ˜¯è‡ªåŠ¨å®ŒæˆçŸ­å‡½æ•°è°ƒç”¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä¿æŒä¸Šä¸‹æ–‡å¤§å°ç›¸å¯¹è¾ƒå°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹å¹¶ä¸”å®ƒéœ€è¦çš„å†…å­˜æ˜¾ç€å‡å°‘ã€‚å¦‚æœæ‚¨çš„åº”ç”¨ç¨‹åºæ‹¥æœ‰æ›´å¤šä¸Šä¸‹æ–‡å¾ˆé‡è¦ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å¸Œæœ›æ¨¡å‹åŸºäºå…·æœ‰å‡½æ•°å®šä¹‰çš„æ–‡ä»¶ç¼–å†™å•å…ƒæµ‹è¯•ï¼‰ï¼Œè¯·ç¡®ä¿å¢åŠ è¯¥æ•°é‡ï¼Œä½†è¯·è®°ä½ï¼Œè¿™éœ€è¦æ›´å¤§çš„ GPU å†…å­˜å ç”¨ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å¤§å°å›ºå®šä¸º 128 ä¸ªæ ‡è®°ï¼Œè€Œä¸æ˜¯ GPT-2 æˆ– GPT-3 ä¸­åˆ†åˆ«ä½¿ç”¨çš„ 1,024 æˆ– 2,048 ä¸ªæ ‡è®°ã€‚


å¤§å¤šæ•°æ–‡æ¡£åŒ…å«è¶…è¿‡ 128 ä¸ªæ ‡è®°ï¼Œå› æ­¤ç®€å•åœ°å°†è¾“å…¥æˆªæ–­åˆ°æœ€å¤§é•¿åº¦å°†æ¶ˆé™¤æˆ‘ä»¬æ•°æ®é›†çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `return_overflowing_tokens` æ ‡è®°æ•´ä¸ªè¾“å…¥å¹¶å°†å…¶åˆ†æˆå‡ ä¸ªå—çš„é€‰é¡¹ï¼Œå°±åƒæˆ‘ä»¬åœ¨[ç¬¬å…­ç« ](/course/chapter6/4). æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ `return_length` é€‰é¡¹è‡ªåŠ¨è¿”å›æ¯ä¸ªåˆ›å»ºçš„å—çš„é•¿åº¦ã€‚é€šå¸¸æœ€åä¸€ä¸ªå—ä¼šå°äºä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘ä»¬ä¼šå»æ‰è¿™äº›å—ä»¥é¿å…å¡«å……é—®é¢˜ï¼›å› ä¸ºæ— è®ºå¦‚ä½•æˆ‘ä»¬éƒ½æœ‰å¤§é‡æ•°æ®ã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å‰ä¸¤ä¸ªç¤ºä¾‹æ¥ç¡®åˆ‡äº†è§£è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å¯ä»¥çœ‹ åˆ°ï¼Œä»è¿™ä¸¤ä¸ªç¤ºä¾‹ä¸­æˆ‘ä»¬æ€»å…±å¾—åˆ°äº† 34 ä¸ªç‰‡æ®µã€‚æŸ¥çœ‹å—é•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ–‡æ¡£æœ«å°¾çš„å—éƒ½å°‘äº 128 ä¸ªæ ‡è®°ï¼ˆåˆ†åˆ«ä¸º 117 å’Œ 41ï¼‰ã€‚è¿™äº›ä»…ä»£è¡¨æˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°å°†å®ƒä»¬æ‰”æ‰ã€‚é€šè¿‡ `overflow_to_sample_mapping` å­—æ®µï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é‡å»ºå“ªäº›å—å±äºå“ªäº›è¾“å…¥æ ·æœ¬ã€‚

é€šè¿‡è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªæ–¹ä¾¿çš„ğŸ¤— Datasets ä¸­çš„` Dataset.map()` å‡½æ•°ï¼Œå°±æ˜¯ä¸éœ€è¦ä¸€å¯¹ä¸€çš„æ˜ å°„ï¼›æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬ä¸‰èŠ‚](/course/chapter7/3),æˆ‘ä»¬å¯ä»¥åˆ›å»ºå…·æœ‰æ¯”è¾“å…¥æ‰¹æ¬¡æ›´å¤šæˆ–æ›´å°‘å…ƒç´ çš„æ‰¹æ¬¡ã€‚è¿™åœ¨æ‰§è¡Œæ›´æ”¹å…ƒç´ æ•°é‡çš„æ•°æ®å¢å¼ºæˆ–æ•°æ®è¿‡æ»¤ç­‰æ“ä½œæ—¶éå¸¸æœ‰ç”¨ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå½“å°†æ¯ä¸ªå…ƒç´ æ ‡è®°ä¸ºæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å—æ—¶ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ–‡æ¡£ä¸­åˆ›å»ºäº†è®¸å¤šæ ·æœ¬ã€‚æˆ‘ä»¬åªéœ€è¦ç¡®ä¿åˆ é™¤ç°æœ‰çš„åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å¤§å°å­˜åœ¨å†²çªã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿ç•™å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥é€‚å½“åœ°é‡å¤å®ƒä»¬ï¼Œå¹¶åœ¨`Dataset.map()` è°ƒç”¨ä¸­è¿”å›å®ƒä»¬:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

æˆ‘ä»¬ç°åœ¨æœ‰ 1670 ä¸‡ä¸ªç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹æœ‰ 128 ä¸ªtokens ï¼Œæ€»å…±ç›¸å½“äºå¤§çº¦ 21 äº¿ä¸ªtokens ã€‚ä½œä¸ºå‚è€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ«åœ¨ 300 å’Œ 1000 äº¿ä¸ªtokens ä¸Šè®­ç»ƒï¼Œå…¶ä¸­ Codex æ¨¡å‹ä» GPT-3 æ£€æŸ¥ç‚¹åˆå§‹åŒ–ã€‚æˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­çš„ç›®æ ‡ä¸æ˜¯ä¸è¿™äº›æ¨¡å‹ç«äº‰ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ç”Ÿæˆé•¿è€Œè¿è´¯çš„æ–‡æœ¬ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªç¼©å°ç‰ˆæœ¬ï¼Œä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿè‡ªåŠ¨å®ŒæˆåŠŸèƒ½ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®é›†ï¼Œè®©æˆ‘ä»¬è®¾ç½®æ¨¡å‹ï¼


<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** æ‘†è„±æ‰€æœ‰å°äºä¸Šä¸‹æ–‡å¤§å°çš„å—åœ¨è¿™é‡Œå¹¶ä¸æ˜¯ä»€ä¹ˆå¤§é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å°ä¸Šä¸‹æ–‡çª—å£ã€‚éšç€ä¸Šä¸‹æ–‡å¤§å°çš„å¢åŠ ï¼ˆæˆ–è€…å¦‚æœæ‚¨æœ‰ä¸€ä¸ªçŸ­æ–‡æ¡£è¯­æ–™åº“ï¼‰ï¼Œè¢«ä¸¢å¼ƒçš„å—çš„æ¯”ä¾‹ä¹Ÿä¼šå¢åŠ ã€‚å‡†å¤‡æ•°æ®çš„æ›´æœ‰æ•ˆæ–¹æ³•æ˜¯å°†æ‰€æœ‰æ ‡è®°åŒ–çš„æ ·æœ¬åŠ å…¥ä¸€ä¸ªæ‰¹æ¬¡ä¸­ï¼Œæ¯ä¸ªè¯­æ–™ä¹‹é—´æœ‰ä¸€ä¸ª`eos_token_id` æ ‡è®°, ç„¶åå¯¹è¿æ¥çš„åºåˆ—æ‰§è¡Œåˆ†å—ã€‚ä½œä¸ºç»ƒä¹ ï¼Œä¿®æ”¹ `tokenize()`å‡½æ•°ä»¥ä½¿ç”¨è¯¥æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦è®¾ç½®`truncation=False` å’Œåˆ é™¤æ ‡è®°ç”Ÿæˆå™¨ä¸­çš„å…¶ä»–å‚æ•°ä»¥è·å–å®Œæ•´çš„æ ‡è®° ID åºåˆ—ã€‚

</Tip>


## åˆå§‹åŒ–æ–°æ¨¡å‹

æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯æ–°åˆå§‹åŒ–ä¸€ä¸ª GPT-2 æ¨¡å‹ã€‚æˆ‘ä»¬å°†å¯¹æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ä¸å°å‹ GPT-2 æ¨¡å‹ç›¸åŒçš„é…ç½®ï¼Œå› æ­¤æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒé…ç½®ï¼Œç¡®ä¿åˆ†è¯å™¨å¤§å°ä¸æ¨¡å‹è¯æ±‡é‡å¤§å°åŒ¹é…å¹¶è®¾ç½® `bos` å’Œ `eos` ï¼ˆåºåˆ—çš„å¼€å§‹å’Œç»“æŸï¼‰ä»¤ç‰Œ IDï¼š

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ä½¿ç”¨è¯¥é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è‡ªå·±åˆå§‹åŒ–æ¨¡å‹

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

é€šè¿‡è¯¥é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½æ–°æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬åˆšå¼€å§‹ä¸ä½¿ç”¨`from_pretrained()`å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è‡ªå·±åˆå§‹åŒ–æ¨¡å‹ï¼š

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 1.24 äº¿ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹å…¶è¿›è¡Œè°ƒæ•´ã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªè´Ÿè´£åˆ›å»ºæ‰¹æ¬¡çš„æ•°æ®æ•´ç†å™¨ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `DataCollatorForLanguageModeling` ï¼Œå®ƒæ˜¯ä¸“ä¸ºè¯­è¨€å»ºæ¨¡è€Œè®¾è®¡ï¼ˆé¡¾åæ€ä¹‰ï¼‰ã€‚é™¤äº†å †å å’Œå¡«å……æ‰¹æ¬¡ï¼Œå®ƒè¿˜è´Ÿè´£åˆ›å»ºè¯­è¨€æ¨¡å‹æ ‡ç­¾â€”â€”åœ¨å› æœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œè¾“å…¥ä¹Ÿç”¨ä½œæ ‡ç­¾ï¼ˆåªæ˜¯ç§»åŠ¨äº†ä¸€ä¸ªå…ƒç´ ï¼‰ï¼Œå¹¶ä¸”è¿™ä¸ªæ•°æ®æ•´ç†å™¨åœ¨è®­ç»ƒæœŸé—´å³æ—¶åˆ›å»ºå®ƒä»¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å¤åˆ¶ `input_ids`ã€‚ 

æ³¨æ„ `DataCollatorForLanguageModeling` æ”¯æŒæ©ç è¯­è¨€å»ºæ¨¡ (MLM) å’Œå› æœè¯­è¨€å»ºæ¨¡ (CLM)ã€‚é»˜è®¤æƒ…å†µä¸‹å®ƒä¸º MLM å‡†å¤‡æ•°æ®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®`mlm=False`å‚æ•°åˆ‡æ¢åˆ° CLM ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¤ºä¾‹å·²ç»å †å åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æ‰€æœ‰å¼ é‡éƒ½å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚

{#if fw === 'tf'}

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`to_tf_dataset()`æ–¹æ³•ï¼Œä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„æ•°æ®æ•´ç†å™¨å°†æ•°æ®é›†è½¬æ¢ä¸ºTensorFlowæ•°æ®é›†ï¼š

```python
tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

âš ï¸  ç§»åŠ¨è¾“å…¥å’Œæ ‡ç­¾ä»¥å¯¹é½å®ƒä»¬å‘ç”Ÿåœ¨æ¨¡å‹å†…éƒ¨ï¼Œå› æ­¤æ•°æ®æ•´ç†å™¨åªéœ€å¤åˆ¶è¾“å…¥ä»¥åˆ›å»ºæ ‡ç­¾ã€‚

</Tip>


ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å®é™…è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹çš„ä¸€åˆ‡äº†â€”â€”æ¯•ç«Ÿè¿™ä¸æ˜¯é‚£ä¹ˆå¤šå·¥ä½œï¼åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥ç™»å½• Hugging Faceã€‚å¦‚æœæ‚¨åœ¨ç¬”è®°æœ¬ä¸Šå·¥ä½œï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å®ç”¨ç¨‹åºåŠŸèƒ½ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æ‚¨çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'pt'}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨ `Trainer` .æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡ï¼Œå¹¶è¿›è¡Œä¸€äº›Warmupå’Œæœ‰æ•ˆæ‰¹é‡å¤§å°ä¸º 256 ( `per_device_train_batch_size` * `gradient_accumulation_steps`ï¼‰ã€‚å½“å•ä¸ªæ‰¹æ¬¡ä¸é€‚åˆå†…å­˜æ—¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¹¶é€šè¿‡å¤šæ¬¡å‘å‰/å‘åä¼ é€’é€æ­¥å»ºç«‹æ¢¯åº¦ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹ `Trainer`å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®æ‚¨æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒé›†è¿˜æ˜¯åœ¨è®­ç»ƒé›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿è¡Œå®ƒï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 æˆ– 2 ä¸ªå°æ—¶ï¼Œå› æ­¤è¯·å–æ¯å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼

```py
trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œæ ‡è®°å™¨æ¨é€åˆ° Hubï¼š

```py
trainer.push_to_hub()
```

{:else}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒè¶…å‚æ•°å¹¶è°ƒç”¨ `compile()` å’Œ `fit()`ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¦æœ‰ä¸€äº›é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼š

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨`model.fit()ï¼Œ`å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚ä½ æ˜¯åœ¨å®Œæ•´çš„è®­ç»ƒé›†è¿˜æ˜¯ä»–çš„å­é›†ä¸Šè¿è¡Œï¼Œè¿™å°†åˆ†åˆ«éœ€è¦20å’Œ2ä¸ªå°æ—¶ï¼Œæ‰€ä»¥æ‹¿ä¸€äº›å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œåˆ†è¯å™¨æ¨é€åˆ°ä¸­å¿ƒï¼š

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

âœï¸ **è¯•è¯•çœ‹!** é™¤äº†`TrainingArguments` ä¹‹å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦å¤§çº¦30è¡Œä»£ç å°±å¯ä»¥ä»åŸå§‹æ–‡æœ¬åˆ°è®­ç»ƒGPT-2ã€‚ ç”¨ä½ è‡ªå·±çš„æ•°æ®é›†è¯•è¯•çœ‹ï¼Œçœ‹çœ‹ä½ èƒ½ä¸èƒ½å¾—åˆ°å¥½çš„ç»“æœï¼

</Tip>

<Tip>

{#if fw === 'pt'}

ğŸ’¡ å¦‚æœæ‚¨å¯ä»¥è®¿é—®å…·æœ‰å¤šä¸ª GPU çš„æœºå™¨ï¼Œè¯·å°è¯•åœ¨é‚£é‡Œè¿è¡Œä»£ç ã€‚ `Trainer`è‡ªåŠ¨ç®¡ç†å¤šå°æœºå™¨ï¼Œè¿™å¯ä»¥æå¤§åœ°åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

{:else}

ğŸ’¡ å¦‚æœæ‚¨æœ‰æƒè®¿é—®å…·æœ‰å¤šä¸ª GPU çš„è®¡ç®—æœºï¼Œåˆ™å¯ä»¥å°è¯•ä½¿ç”¨ `MirroredStrategy` ä¸Šä¸‹æ–‡æ¥å¤§å¹…åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª`tf.distribute.MirroredStrategy`å¯¹è±¡ï¼Œå¹¶ç¡®ä¿ `to_tf_dataset` å‘½ä»¤ä»¥åŠæ¨¡å‹åˆ›å»ºå’Œå¯¹ `fit()`çš„è°ƒç”¨éƒ½åœ¨å…¶ `scope()` context. ä¸Šä¸‹æ–‡ä¸­è¿è¡Œã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æœ‰å…³æ­¤å†…å®¹çš„æ–‡æ¡£[here](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## ä½¿ç”¨ç®¡é“ç”Ÿæˆä»£ç 

ç°åœ¨æ˜¯å…³é”®çš„éƒ¨åˆ†ï¼šè®©æˆ‘ä»¬çœ‹çœ‹ç»è¿‡è®­ç»ƒçš„æ¨¡å‹çš„å®é™…æ•ˆæœå¦‚ä½•ï¼æˆ‘ä»¬å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°æŸå¤±ç¨³æ­¥ä¸‹é™ï¼Œä½†ä¸ºäº†è®©æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æŸäº›æµ‹è¯•ä¸Šçš„è¡¨ç°å¦‚ä½•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åŒ…è£…åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„`pipeline` ï¼Œå¦‚æœæœ‰å¯ç”¨çš„ï¼Œæˆ‘ä»¬ä¼šå°†å®ƒæ”¾åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿç”Ÿæˆï¼š

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

è®©æˆ‘ä»¬ä»åˆ›å»ºæ•£ç‚¹å›¾çš„ç®€å•ä»»åŠ¡å¼€å§‹ï¼š

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

ç»“æœçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚å®ƒä¹Ÿé€‚ç”¨äº `pandas` ç±»å‹ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦ä½¿ç”¨ä¸¤ä¸ªæ•°ç»„å¯ä»¥åˆ›å»ºä¸€ä¸ª `DataFrame` ï¼š

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

å¾ˆå¥½ï¼Œè¿™æ˜¯æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”å°½ç®¡å®ƒéšåå†æ¬¡æ’å…¥äº†åˆ— `x` ã€‚ç”±äºç”Ÿæˆçš„tokenæ•°é‡æœ‰é™ï¼Œä»¥ä¸‹ `for` å¾ªç¯è¢«åˆ‡æ–­ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥åšä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…å¹¶è®©æ¨¡å‹å¸®åŠ©æˆ‘ä»¬åˆ†ç»„æ“ä½œï¼š

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

ä¸é”™;è¿™æ˜¯æ­£ç¡®çš„åšæ³•ã€‚æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦ä¹Ÿå¯ä»¥å°†å…¶ç”¨äº `scikit-learn` å¹¶å»ºç«‹ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

çœ‹çœ‹è¿™å‡ ä¸ªä¾‹å­ï¼Œä¼¼ä¹æ¨¡å‹å·²ç»å­¦ä¹ äº†Pythonæ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ã€‚å½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å½»åº•åœ°è¯„ä¼°æ¨¡å‹ï¼Œä½†è¿™ä»ç„¶æ˜¯ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„åŸå‹ã€‚

{:else}

ä»è¿™å‡ ä¸ªä¾‹å­æ¥çœ‹ï¼Œæ¨¡å‹ä¼¼ä¹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ï¼ˆå½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼‰ã€‚ç„¶è€Œï¼Œæœ‰æ—¶éœ€è¦å¯¹æ¨¡å‹è®­ç»ƒè¿›è¡Œæ›´å¤šå®šåˆ¶æ‰èƒ½å®ç°ç»™å®šç”¨ä¾‹çš„å¿…è¦æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åŠ¨æ€æ›´æ–°æ‰¹é‡å¤§å°æˆ–æœ‰ä¸€ä¸ªæ¡ä»¶è®­ç»ƒå¾ªç¯æ¥å³æ—¶è·³è¿‡åç¤ºä¾‹æ€ä¹ˆåŠï¼Ÿä¸€ç§é€‰æ‹©æ˜¯å°† `Trainer` å¹¶æ·»åŠ å¿…è¦çš„æ›´æ”¹ï¼Œä½†æœ‰æ—¶ä»å¤´å¼€å§‹ç¼–å†™è®­ç»ƒå¾ªç¯ä¼šæ›´ç®€å•ã€‚è¿™å°±æ˜¯ğŸ¤— Accelerate çš„ç”¨æ­¦ä¹‹åœ°ã€‚   

{/if}

{#if fw === 'pt'}

## ç”¨ ğŸ¤— Accelerate è®­ç»ƒ

æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ `Trainer` ï¼Œè¿™å¯ä»¥å…è®¸ä¸€äº›è‡ªå®šä¹‰ã€‚ç„¶è€Œï¼Œæœ‰æ—¶æˆ‘ä»¬æƒ³è¦å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯ï¼Œæˆ–è€…æˆ‘ä»¬æƒ³è¦è¿›è¡Œä¸€äº›å¥‡ç‰¹çš„æ›´æ”¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥ä»‹ç»ä½¿ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹çš„æ­¥éª¤ã€‚ä¸ºäº†è®©äº‹æƒ…å˜å¾—æ›´æœ‰è¶£ï¼Œæˆ‘ä»¬è¿˜å°†åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ä¸€äº›ä¿®æ”¹ã€‚

<Youtube id="Hm8_PgVTFuc"/>

ç”±äºæˆ‘ä»¬ä¸»è¦å¯¹æ•°æ®ç§‘å­¦åº“çš„åˆç†è‡ªåŠ¨å¡«å……æ„Ÿå…´è¶£ï¼Œå› æ­¤å¯¹æ›´å¤šä½¿ç”¨è¿™äº›åº“çš„è®­ç»ƒæ ·æœ¬ç»™äºˆæ›´å¤šæƒé‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å…³é”®å­—è½»æ¾è¯†åˆ«è¿™äº›ç¤ºä¾‹ï¼Œä¾‹å¦‚ `plt`ã€`pd`ã€`sk`ã€`fit`å’Œ`predict`ç­‰å…³é”®å­—ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è¯†åˆ«è¿™äº›ç¤ºä¾‹ï¼Œè¿™äº›å…³é”®å­—æ˜¯matplotlibæœ€å¸¸ç”¨çš„å¯¼å…¥åç§°ã€‚`Pyplot`, `pandas`å’Œ`sklearn`ä»¥åŠåè€…çš„æ‹Ÿåˆ/é¢„æµ‹æ¨¡å¼ã€‚å¦‚æœè¿™äº›éƒ½è¡¨ç¤ºä¸ºå•ä¸ªæ ‡è®°ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾æ£€æŸ¥å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨è¾“å…¥åºåˆ—ä¸­ã€‚æ ‡è®°å¯èƒ½æœ‰ä¸€ä¸ªç©ºæ ¼å‰ç¼€ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å°†åœ¨æ ‡è®°å™¨è¯æ±‡è¡¨ä¸­æ£€æŸ¥è¿™äº›ç‰ˆæœ¬ã€‚ä¸ºäº†éªŒè¯å®ƒæ˜¯å¦æœ‰æ•ˆï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªæµ‹è¯•token ï¼Œè¯¥token åº”æ‹†åˆ†ä¸ºå¤šä¸ªtokensï¼š

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

å¤ªå¥½äº†ï¼Œè¿™ä¼¼ä¹å¾ˆå¥½ç”¨ï¼æˆ‘ä»¬ç°åœ¨å¯ä»¥ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œå®ƒå°†è¾“å…¥åºåˆ—ã€logits å’Œæˆ‘ä»¬åˆšåˆšé€‰æ‹©çš„å…³â€‹â€‹é”®æ ‡è®°ä½œä¸ºè¾“å…¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¹é½ logits å’Œè¾“å…¥ï¼šå‘å³ç§»åŠ¨ä¸€ä¸ªçš„è¾“å…¥åºåˆ—å½¢æˆæ ‡ç­¾ï¼Œå› ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°æ˜¯å½“å‰æ ‡è®°çš„æ ‡ç­¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»è¾“å…¥åºåˆ—çš„ç¬¬äºŒä¸ªæ ‡è®°å¼€å§‹æ ‡è®°æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæ¨¡å‹æ— è®ºå¦‚ä½•éƒ½ä¸ä¼šå¯¹ç¬¬ä¸€ä¸ªæ ‡è®°è¿›è¡Œé¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬åˆ‡æ–­æœ€åä¸€ä¸ª logitï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å®Œæ•´è¾“å…¥åºåˆ—ä¹‹åçš„æ ‡è®°çš„æ ‡ç­¾ã€‚æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸­æ‰€æœ‰å…³é”®å­—çš„å‡ºç°æ¬¡æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ºç°æ¬¡æ•°ä½œä¸ºæƒé‡è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŠ æƒå¹³å‡å€¼ã€‚ç”±äºæˆ‘ä»¬ä¸æƒ³æ‰”æ‰æ‰€æœ‰æ²¡æœ‰å…³é”®å­—çš„æ ·æœ¬ï¼Œæˆ‘ä»¬å°†æƒé‡åŠ 1ï¼š

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

åœ¨æˆ‘ä»¬å¼€å§‹ä½¿ç”¨è¿™ä¸ªå¾ˆæ£’çš„æ–°æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€äº›ä¸œè¥¿ï¼š

- æˆ‘ä»¬éœ€è¦æ•°æ®åŠ è½½å™¨æ¥æ‰¹é‡åŠ è½½æ•°æ®ã€‚
- æˆ‘ä»¬éœ€è¦è®¾ç½®æƒé‡è¡°å‡å‚æ•°ã€‚
- æœ‰æ—¶æˆ‘ä»¬æƒ³è¦æ±‚å€¼ï¼Œå› æ­¤å°†æ±‚å€¼ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­æ˜¯æœ‰æ„ä¹‰çš„ã€‚

è®©æˆ‘ä»¬ä»æ•°æ®åŠ è½½å™¨å¼€å§‹ã€‚æˆ‘ä»¬åªéœ€è¦å°†æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸º `"torch"`ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å°†å®ƒä¼ é€’ç»™ PyTorch `DataLoader` ,åŒæ—¶è®¾ç½®é€‚å½“çš„æ‰¹é‡å¤§å°ï¼š

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹å‚æ•°è¿›è¡Œåˆ†ç»„ï¼Œä»¥ä¾¿ä¼˜åŒ–å™¨çŸ¥é“å“ªäº›å°†è·å¾—é¢å¤–çš„æƒé‡è¡°å‡ã€‚é€šå¸¸ï¼Œæ‰€æœ‰åå·®å’Œ LayerNorm æƒé‡é¡¹éƒ½ä¸å—æ­¤é™åˆ¶ï¼›ä»¥ä¸‹æˆ‘ä»¬å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼š

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

ç”±äºæˆ‘ä»¬å¸Œæœ›åœ¨è®­ç»ƒæœŸé—´å®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿä¸ºæ­¤ç¼–å†™ä¸€ä¸ªå‡½æ•°ã€‚å®ƒåªæ˜¯è¿è¡Œè¯„ä¼°æ•°æ®åŠ è½½å™¨å¹¶æ”¶é›†è·¨è¿›ç¨‹çš„æ‰€æœ‰æŸå¤±ï¼š

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

é€šè¿‡ `evaluate()` å‡½æ•°æˆ‘ä»¬å®šæœŸå¯ä»¥è·å–æŸå¤±å€¼å’Œ[perplexity](/course/chapter7/3)ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡æ–°å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç¡®ä¿æˆ‘ä»¬å†æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒï¼š

```py
model = GPT2LMHeadModel(config)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¹‹å‰çš„å‡½æ•°æ¥åˆ†å‰²æƒé‡è¡°å‡çš„å‚æ•°ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ç°åœ¨è®©æˆ‘ä»¬å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒï¼š

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœæ‚¨åœ¨ TPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ™éœ€è¦å°†ä»ä¸Šé¢çš„å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸“ç”¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [ç¬¬ 3 ç« ](/course/chapter3) for more details.

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å‘é€äº†æˆ‘ä»¬çš„ `train_dataloader`åˆ° `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆåœ¨å‡†å¤‡å¥½dataloaderåæ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºè¯¥æ–¹æ³•ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œè¦å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `Repository` å·¥ä½œæ–‡ä»¶å¤¹ä¸­çš„å¯¹è±¡ã€‚å¦‚æœæ‚¨å°šæœªç™»å½•ï¼Œè¯·å…ˆç™»å½• Hugging Faceã€‚æˆ‘ä»¬å°†ä»æˆ‘ä»¬æƒ³è¦ä¸ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¡®å®šå­˜å‚¨åº“åç§°ï¼ˆæ‚¨å¯ä»¥è‡ªç”±åœ°ç”¨è‡ªå·±çš„é€‰æ‹©æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«æ‚¨çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨`get_full_repo_name()`å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¹ä¸­å…‹éš†è¯¥å­˜å‚¨åº“ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„å­˜å‚¨åº“çš„å…‹éš†ï¼š

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸Šä¼ æˆ‘ä»¬ä¿å­˜çš„ä»»ä½•å†…å®¹ `output_dir` é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬è®­ç»ƒä¹‹å‰ï¼Œè®©æˆ‘ä»¬è¿è¡Œä¸€ä¸ªå¿«é€Ÿæµ‹è¯•ï¼Œçœ‹çœ‹è¯„ä¼°å‡½æ•°æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

è¿™äº›æŸå¤±å’Œå›°æƒ‘åº¦çš„å€¼éå¸¸é«˜ï¼Œä½†è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒè¿‡æ¨¡å‹ã€‚æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½ç¼–å†™è®­ç»ƒè„šæœ¬çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šè®­ç»ƒå¾ªç¯ã€‚åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬éå†æ•°æ®åŠ è½½å™¨å¹¶å°†æ‰¹æ¬¡ä¼ é€’ç»™æ¨¡å‹ã€‚æœ‰äº† logitsï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯„ä¼°æˆ‘ä»¬çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„æ•°é‡æ¥ç¼©æ”¾æŸå¤±ï¼Œä»¥ä¾¿åœ¨èšåˆæ›´å¤šæ­¥éª¤æ—¶ä¸ä¼šäº§ç”Ÿæ›´å¤§çš„æŸå¤±ã€‚åœ¨æˆ‘ä»¬ä¼˜åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜å‰ªè¾‘äº†æ¢¯åº¦ä»¥è·å¾—æ›´å¥½çš„æ”¶æ•›æ€§ã€‚æœ€åï¼Œæ¯éš”å‡ æ­¥ï¼Œæˆ‘ä»¬å°±ä¼šä½¿ç”¨æ–°çš„ `evaluate()` å‡½æ•°è¯„ä¼°æ¨¡å‹ï¼š

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=len(train_dataloader)
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

å°±æ˜¯è¿™æ · - æ‚¨ç°åœ¨æ‹¥æœ‰è‡ªå·±çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰çš„è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦è¿›ä¸€æ­¥è‡ªå®šä¹‰ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹!** åˆ›å»ºé€‚åˆæ‚¨çš„ç”¨ä¾‹çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å¦ä¸€ä¸ªè‡ªå®šä¹‰æ­¥éª¤ã€‚

</Tip>

<Tip>

âœï¸ **è¯•è¯•çœ‹!** åœ¨è¿è¡Œé•¿æ—¶é—´çš„è®­ç»ƒå®éªŒæ—¶ï¼Œæœ€å¥½ä½¿ç”¨ TensorBoard æˆ– Weights Biases ç­‰å·¥å…·è®°å½•é‡è¦æŒ‡æ ‡ã€‚å‘è®­ç»ƒå¾ªç¯æ·»åŠ é€‚å½“çš„æ—¥å¿—è®°å½•ï¼Œä»¥ä¾¿æ‚¨å§‹ç»ˆå¯ä»¥æ£€æŸ¥è®­ç»ƒçš„è¿›è¡Œæƒ…å†µã€‚going.

</Tip>

{/if}