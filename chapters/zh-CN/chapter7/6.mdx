<FrameworkSwitchCourse {fw} />

# ä»å¤´å¼€å§‹è®­ç»ƒå› æœè¯­è¨€æ¨¡å‹ [[ä»å¤´å¼€å§‹è®­ç»ƒå› æœè¯­è¨€æ¨¡å‹]]

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
]} />

{/if}

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤ç”¨é¢„è®­ç»ƒçš„æƒé‡ï¼Œç„¶åä½¿ç”¨æ–°çš„æ•°æ®å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°çš„åº”ç”¨åœºæ™¯ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­çœ‹åˆ°çš„ï¼Œè¿™é€šå¸¸ç§°ä¸º `è¿ç§»å­¦ä¹ ï¼ˆtransfer learningï¼‰` ï¼Œå¯¹äºå¤§å¤šæ•°æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„åº”ç”¨åœºæ™¯ï¼Œå®ƒæ˜¯ä¸€ç§å°† Transformer æ¨¡å‹åº”ç”¨åˆ°å¤§éƒ¨åˆ†çœŸå®çš„åº”ç”¨åœºæ™¯ä¸­çš„ä¸€ä¸ªéå¸¸æˆåŠŸçš„ç­–ç•¥ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸åŒçš„æ–¹æ³•å¹¶ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ã€‚å¦‚æœä½ æœ‰å¤§é‡æ•°æ®è€Œä¸”è¿™äº›æ•°æ®ä¸å¯ç”¨æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®å·®å¼‚å¾ˆå¤§ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç›¸æ¯”ä»…å¾®è°ƒç°æœ‰æ¨¡å‹ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ç¤ºä¾‹åŒ…æ‹¬ç”±éŸ³ä¹ç¬¦å·ã€DNA ç­‰åˆ†å­åºåˆ—æˆ–ç¼–ç¨‹è¯­è¨€ç»„æˆçš„æ•°æ®é›†ã€‚ç¼–ç¨‹è¯­è¨€ç»„æˆçš„æ•°æ®é›†æœ€è¿‘å¹¿æ³›åœ°å—åˆ°å…³æ³¨ï¼Œè¿™è¦å½’åŠŸäº TabNine å’Œ GitHub çš„ Copilot ç­‰å·¥å…·çš„æµè¡Œï¼Œå®ƒä»¬ç”± OpenAI çš„ Codex æ¨¡å‹æä¾›æ”¯æŒï¼Œå¯ä»¥ç”Ÿæˆé•¿ä»£ç åºåˆ—ã€‚è¿™ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æœ€é€‚åˆä½¿ç”¨è‡ªå›å½’æˆ–å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰ã€‚

åœ¨è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç²¾ç®€ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šä½¿ç”¨ Python ä»£ç çš„ä¸€ä¸ªæ•°æ®é›†ï¼Œæ¥å®ç°ä¸€è¡Œä»£ç çš„è¡¥å…¨ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”Ÿæˆå®Œæ•´çš„å‡½æ•°æˆ–ç±»ã€‚å½“ä½ ä½¿ç”¨ Python å¤„ç†æ•°æ®æ—¶ï¼Œä½ ç»å¸¸ä¼šæ¥è§¦åˆ° Python æ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬ `matplotlib` ï¼Œ `seaborn` ï¼Œ `pandas` ï¼Œå’Œ `scikit-learn` è¿™äº›åº“ã€‚å½“ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿç”¨æ¨¡å‹æ¥è‡ªåŠ¨ç»™å‡ºæ°å½“çš„æ¨èå‘½ä»¤å°±å¤ªå¥½äº†ï¼

<Youtube id="Vpjb1lu0MDk"/>

åœ¨ [ç¬¬å…­ç« ](/course/chapter6) ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ tokenizer æ¥å¤„ç† Python æºä»£ç ï¼Œä½†æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†æ¥é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ tokenizer å¤„ç†ä¸€ä¸ªæ¥è‡ª GitHub ä»“åº“çš„ Python ä»£ç è¯­æ–™åº“ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Trainer API å’Œ ğŸ¤— Accelerate æ¥è®­ç»ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

è¿™é‡Œå±•ç¤ºçš„æ˜¯ä¸€ä¸ªå·²ç»è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ï¼Œå®ƒå°±æ˜¯ä½¿ç”¨æœ¬èŠ‚ä¸­çš„ä»£ç è®­ç»ƒçš„ã€‚ä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28) æ‰¾åˆ°å®ƒã€‚æ³¨æ„ï¼Œç”±äºæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰ä¸€äº›éšæœºæ€§ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°ç¨å¾®ä¸åŒçš„ç»“æœã€‚

## æ”¶é›†æ•°æ® [[æ”¶é›†æ•°æ®]]

æˆ‘ä»¬å¯ä»¥ä»è¯¸å¦‚ GitHub è¿™æ ·çš„ä»£ç ä»“åº“ä¸­è·å–ä¸°å¯Œçš„ Python ä»£ç ï¼Œé€šè¿‡å¯¹æ¯ä¸ª Python ä»“åº“è¿›è¡ŒæŠ“å–ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚è¿™å°±æ˜¯åœ¨ [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) ä¸­é¢„è®­ç»ƒä¸€ä¸ªå¤§å‹ GPT-2 æ¨¡å‹çš„æ–¹æ³•ã€‚å¼€å‘è€…æ•´ç†äº†åä¸º `codeparrot` çš„ä¸€ä¸ªå¤§çº¦ä¸º 180GB çš„ GitHub æ•°æ®é›†ï¼Œ å…¶ä¸­åŒ…å«å¤§çº¦ 2,000 ä¸‡ä¸ªçš„Python æ–‡ä»¶ã€‚ å¼€å‘è€…ç”¨è¿™äº›æ–‡ä»¶æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶åœ¨ [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot) ä¸Šåˆ†äº«äº†è¿™ä¸ªæ•°æ®é›†ã€‚

ç„¶è€Œï¼Œä½¿ç”¨å®Œæ•´è¯­æ–™åº“çš„è®­ç»ƒæ—¢è€—æ—¶åˆè´¹åŠ›ï¼Œæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ° Python æ•°æ®ç§‘å­¦æ ˆç›¸å…³çš„æ•°æ®é›†å­é›†ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬ä» `codeparrot` æ•°æ®é›†ä¸­ç­›é€‰å‡ºåŒ…å«è¿™ä¸ªæ ˆä¸­æ‰€æœ‰ç›¸å…³åº“çš„æ‰€æœ‰æ–‡ä»¶ã€‚ç”±äºæ•°æ®é›†çš„å¤ªå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›é¿å…ç›´æ¥æŠŠå…¨éƒ¨çš„æ•°æ®é›†ä¸‹è½½ä¸‹æ¥ï¼›å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æµå¼ä¼ è¾“çš„æ–¹æ³•æ¥åŠ¨æ€è¿‡æ»¤å®ƒã€‚ä¸ºäº†ä½¿ç”¨ä¸Šè¿°çš„åº“æ¥ç­›é€‰ä»£ç æ ·æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

è®©æˆ‘ä»¬ç”¨ä¸¤ä¸ªä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æµå¼ä¼ è¾“æ•°æ®é›†å¹¶è¿‡æ»¤æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ ï¼š

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™é‡Œå‡½æ•°æµå¼å¤„ç†æ•°æ®é›†ï¼š

```py
# æ‰§è¡Œè¿™ä¸ªä»£ç å—éœ€è¦éå¸¸é•¿çš„æ—¶é—´,å› æ­¤ä½ å¯ä»¥è·³è¿‡å®ƒ,ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ª!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

å®Œæˆè¿™ä¸ªæ“ä½œåï¼Œæˆ‘ä»¬è¿‡æ»¤åçš„æ•°æ®é›†åªæœ‰åŸå§‹æ•°æ®é›†çš„å¤§çº¦ 3ï¼…ï¼Œä½†è¿™ä»ç„¶æ˜¯ç›¸å½“å¯è§‚çš„å¤§å°â€”â€”æœ€ç»ˆçš„æ•°æ®é›†æ˜¯ 6GBï¼Œç”± 600,000 ä¸ª Python è„šæœ¬ç»„æˆï¼

è¿‡æ»¤å®Œæ•´çš„æ•°æ®é›†å¯èƒ½éœ€è¦ 2-3 å°æ—¶ï¼Œè¿™å–å†³äºä½ çš„æœºå™¨æ€§èƒ½å’Œå¸¦å®½ã€‚å¦‚æœä½ ä¸æƒ³äº²è‡ªç»å†è¿™ä¸ªæ¼«é•¿çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨ Hub ä¸Šæä¾›äº†è¿‡æ»¤åçš„æ•°æ®é›†ä¾›ä½ ä¸‹è½½ï¼š

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ¥è‡ªæ•°æ®é›†çš„ä¾‹å­ã€‚æˆ‘ä»¬å°†åªæ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰ 200 ä¸ªå­—ç¬¦ï¼š

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ `content` å­—æ®µåŒ…å«äº†æˆ‘ä»¬å¸Œæœ›æ¨¡å‹è®­ç»ƒçš„ä»£ç ã€‚æœ‰äº†è¿™ä¸ªæ•°æ®é›†ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œä¸€äº›å¤„ç†ï¼Œä»¥ä¾¿å®ƒä»¬é€‚åˆäºé¢„è®­ç»ƒã€‚

## å‡†å¤‡æ•°æ®é›† [[å‡†å¤‡æ•°æ®é›†]]

<Youtube id="ma1TrR7gE7I"/>

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œè¿™æ ·æ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è‡ªåŠ¨è¡¥å…¨çŸ­çš„å‡½æ•°è°ƒç”¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†ä¸Šä¸‹æ–‡å¤§å°è®¾ç½®å¾—ç›¸å¯¹è¾ƒå°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸”éœ€è¦çš„å†…å­˜ä¹Ÿå¤§å¤§å‡å°‘ã€‚å¦‚æœä½ çš„åº”ç”¨éœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ï¼Œä½ å¸Œæœ›æ¨¡å‹æ ¹æ®åŒ…å«å‡½æ•°å®šä¹‰çš„æ–‡ä»¶ç¼–å†™å•å…ƒæµ‹è¯•ï¼‰ï¼Œé‚£ä¹ˆåº”è¯¥å¢å¤§è¯¥æ•°å­—ï¼Œä½†æ˜¯ä¹Ÿè¦è®°ä½è¿™ä¼šå¢åŠ  GPU æ˜¾å­˜çš„å ç”¨ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å¤§å°å›ºå®šä¸º 128 ä¸ª tokens è€Œä¸æ˜¯åœ¨ GPT-2 æˆ– GPT-3 ä¸­ä½¿ç”¨çš„ 1,024 æˆ– 2,048 ä¸ª tokens 

å¤§å¤šæ•°æ–‡æ¡£éƒ½åŒ…å«è¶…è¿‡ 128 ä¸ª tokens å› æ­¤ç®€å•åœ°å°†è¾“å…¥æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ä¼šåˆ é™¤æˆ‘ä»¬æ•°æ®é›†çš„å¾ˆä¸€å¤§éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `return_overflowing_tokens` é€‰é¡¹å°†æ•´ä¸ªè¾“å…¥è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸ºå‡ ä¸ªå—ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬å…­ç« ](/course/chapter6/4) ä¸­æ‰€åšçš„é‚£æ ·ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ `return_length` é€‰é¡¹è‡ªåŠ¨è¿”å›åˆ›å»ºçš„æ¯ä¸ªå—çš„é•¿åº¦ã€‚é€šå¸¸ï¼Œæœ€åä¸€ä¸ªå—çš„å¤§å°ä¼šå°äºä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘ä»¬å°†å»æ‰æœ€åä¸€å—ä»¥é¿å…å¡«å……é—®é¢˜ï¼›å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ‰€ä»¥ä¸éœ€è¦å®ƒä»¬ã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å‰ä¸¤ä¸ªç¤ºä¾‹æ¥å…·ä½“äº†è§£ç»“æœæ€ä¹ˆæ ·ï¼š

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸¤ä¸ªä¾‹å­æ€»å…±å¾—åˆ°äº† 34 ä¸ªå—ã€‚æŸ¥çœ‹å—é•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ–‡æ¡£æœ«ç«¯çš„å—å°‘äº 128 ä¸ª tokens ï¼ˆåˆ†åˆ«ä¸º 117 å’Œ 41ï¼‰ã€‚ä¸è¿‡è¿™äº›åªå æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„æ€»å—æ•°çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ”¾å¿ƒåœ°ä¸¢æ‰å®ƒä»¬ã€‚é€šè¿‡ `overflow_to_sample_mapping` å­—æ®µï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ†è¾¨å‡ºå“ªäº›å—å±äºå“ªä¸ªæ ·æœ¬ã€‚

åœ¨è¿™ä¸ªæ“ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ğŸ¤— Datasets ä¸­çš„ `Dataset.map()` å‡½æ•°çš„ä¸€ä¸ªä¾¿æ·çš„ç‰¹æ€§ï¼Œå³å®ƒå¹¶ä¸éœ€è¦ä¸€å¯¹ä¸€åœ°è®¾ç½®åˆ†å—åå’Œåˆ†å—å‰çš„æ˜ å°„å…³ç³»ï¼›æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰èŠ‚](/course/chapter7/3) ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªç”±åœ°å°†ä¸€ä¸ªæ ·æœ¬æ‹†åˆ†æˆ–è€…åˆ é™¤éƒ¨åˆ†æ ·æœ¬æ¥åˆ›å»ºæ¯”è¾“å…¥çš„ `batch_size` æ›´å¤šæˆ–æ›´å°‘å…ƒç´ çš„ batchã€‚ `Dataset.map()` å‡½æ•°ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬å…³è”æ˜ å°„å…³ç³»ï¼Œå½“è¿›è¡Œåƒæ•°æ®å¢å¼ºæˆ–æ•°æ®è¿‡æ»¤è¿™æ ·æ”¹å˜å…ƒç´ æ•°é‡çš„æ“ä½œæ—¶éå¸¸æœ‰ç”¨ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œå½“å°†æ¯ä¸ªæ ·æœ¬åˆ†è¯å¹¶åˆ†å‰²æˆæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å—æ—¶ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ ·æœ¬ä¸­åˆ›å»ºäº†è®¸å¤šæ ·æœ¬ã€‚æˆ‘ä»¬éœ€è¦åˆ é™¤åŸæœ¬çš„åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å¤§å°å’Œæˆ‘ä»¬åˆ†å‰²åçš„å¤§å°ä¸ä¸€æ ·ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿ç•™å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å¤åˆ¶å®ƒä»¬æ¥å¡«å……ï¼Œå¹¶åœ¨ `Dataset.map()` è°ƒç”¨ä¸­è¿”å›å®ƒä»¬ã€‚

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

æˆ‘ä»¬ç°åœ¨æœ‰ 1670 ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 128 ä¸ª tokens æ€»å…±ç›¸å½“äºå¤§çº¦ 21 äº¿ä¸ª tokensã€‚ä½œä¸ºå‚è€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ«åœ¨ 300 å’Œ 1000 äº¿ä¸ª tokens ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå…¶ä¸­ Codex æ¨¡å‹ä» GPT-3 checkpoint åˆå§‹åŒ–ã€‚æœ¬èŠ‚çš„ç›®æ ‡ä¸æ˜¯ä¸è¿™äº›èƒ½ç”Ÿæˆé•¿ä¸”è¿è´¯æ–‡æœ¬çš„æ¨¡å‹ç«äº‰ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªèƒ½ä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿè‡ªåŠ¨ä»£ç è¡¥å…¨åŠŸèƒ½çš„ç²¾ç®€ç‰ˆæœ¬ã€‚

æ—¢ç„¶æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®é›†ï¼Œé‚£å°±æ¥è®¾ç½®æ¨¡å‹å§ï¼

<Tip>

âœï¸ **è¯•ä¸€è¯•ï¼**è¿™é‡Œæˆ‘ä»¬åˆ é™¤äº†æ‰€æœ‰å°äºè®¾å®šçš„ä¸Šä¸‹æ–‡å¤§å°çš„å—ï¼Œå¹¶ä¸ä¼šé€ æˆå¤§é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ¯”è¾ƒå°çš„ä¸Šä¸‹æ–‡çª—å£ã€‚éšç€å¢å¤§ä¸Šä¸‹æ–‡å¤§å°ï¼ˆæˆ–è€…è¯­æ–™åº“ä¸­çš„æ–‡æ¡£é•¿åº¦éƒ½å¾ˆçŸ­ï¼‰ï¼Œè¢«æŠ›å¼ƒçš„å—çš„æ¯”ä¾‹ä¹Ÿä¼šå¢åŠ ã€‚æ›´æœ‰æ•ˆæ–¹æ³•æ˜¯å°†æ‰€æœ‰ tokenize åçš„æ ·æœ¬æ‹¼æ¥èµ·æ¥åŠ å…¥ä¸€ä¸ª batch ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬ä¹‹é—´æœ‰ä¸€ä¸ª `eos_token_id` token ä½œä¸ºåˆ†éš”ï¼Œç„¶åå¯¹è¿æ¥åçš„åºåˆ—è¿›è¡Œåˆ‡å—å¤„ç†ã€‚ä½œä¸ºç»ƒä¹ ï¼Œä¿®æ”¹ `tokenize()` å‡½æ•°ä»¥åˆ©ç”¨è¿™ç§æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†è·å–å®Œæ•´çš„ token  ID åºåˆ—ä½ éœ€è¦è®¾ç½® `truncation=False` ï¼Œå¹¶åˆ é™¤ tokenizer ä¸­çš„å…¶ä»–å‚æ•°ã€‚

</Tip>

## åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ [[åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹]]

æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯åˆå§‹åŒ–ä¸€ä¸ªå…¨æ–°åœ° GPT-2 æ¨¡å‹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ è½½é¢„è®­ç»ƒé…ç½®æ¥åˆå§‹åŒ–ä¸€ä¸ªä¸ GPT-2 small ç›¸åŒçš„é…ç½®çš„æ¨¡å‹ï¼Œå¹¶ç¡®ä¿ tokenizer å¤§å°ä¸æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°åŒ¹é…ï¼Œä»¥åŠè®¾ç½® `bos` å’Œ `eos` ï¼ˆåºåˆ—çš„å¼€å§‹å’Œç»“æŸï¼‰ token IDsï¼š

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

æœ‰äº†è¿™ä¸ªé…ç½®å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥åŠ è½½ä¸€ä¸ªå…¨æ–°çš„ GPT-2 æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹è€Œä¸æ˜¯ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼š

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

æœ‰äº†è¿™ä¸ªé…ç½®ï¼Œï¼Œæˆ‘ä»¬å°±å¯ä»¥åŠ è½½ä¸€ä¸ªå…¨æ–°çš„ GPT-2 æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹è€Œä¸æ˜¯ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼š

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # æ„å»ºæ¨¡å‹
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple)                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

æˆ‘ä»¬çš„æ–°æ¨¡å‹æœ‰ 124M ä¸ªå‚æ•°éœ€è¦è®­ç»ƒã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰ï¼Œå®ƒå°†è´Ÿè´£åˆ›å»º Batchã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `DataCollatorForLanguageModeling` ï¼Œé¡¾åæ€ä¹‰ï¼Œå®ƒä¸“é—¨ç”¨äºè¯­è¨€å»ºæ¨¡ã€‚é™¤äº†å †å å’Œå¡«å……åˆ›å»º Batch ä¹‹å¤–ï¼Œå®ƒè¿˜è´Ÿè´£åˆ›å»ºè¯­è¨€æ¨¡å‹çš„å¾…é¢„æµ‹çš„æ ‡ç­¾ â€”â€” åœ¨å› æœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œè¾“å…¥å°±æ˜¯å¾…é¢„æµ‹çš„æ ‡ç­¾ï¼ˆåªæ˜¯åç§»ä¸€ä¸ªå…ƒç´ ï¼‰ï¼Œè€Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶å°†è¾“å…¥åç§»ä¸€ä¸ªå…ƒç´ æ¥åˆ›å»ºå®ƒä»¬ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦å¤åˆ¶ `input_ids` ã€‚

æ³¨æ„ï¼Œ `DataCollatorForLanguageModeling` åŒæ—¶æ”¯æŒæ©ç è¯­è¨€å»ºæ¨¡ ï¼ˆMLMï¼‰ å’Œå› æœè¯­è¨€å»ºæ¨¡ ï¼ˆCLMï¼‰ã€‚é»˜è®¤æƒ…å†µä¸‹å®ƒå®‰è£… MLM éœ€è¦çš„æ ¼å¼å‡†å¤‡æ•°æ®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® `mlm=False` å‚æ•°åˆ‡æ¢åˆ° CLMã€‚

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¤ºä¾‹çš„æ•°æ®å·²ç»å¤„ç†å¥½äº†ï¼Œå¹¶ä¸”æ‰€æœ‰ tensor éƒ½å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚

{#if fw === 'tf'}

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `prepare_tf_dataset()` æ–¹æ³•ï¼Œå°†ä¸Šé¢åˆ›å»ºçš„æ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰å°†æ•°æ®é›†è½¬æ¢ä¸º TensorFlow æ•°æ®é›†ï¼š

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

âš ï¸ è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—å¯¹é½å°†åœ¨æ¨¡å‹å†…éƒ¨è‡ªåŠ¨è¿›è¡Œï¼Œæ‰€ä»¥æ•°æ®æ•´ç†å™¨åªéœ€å¤åˆ¶è¾“å…¥åºåˆ—æ¥åˆ›å»ºç›®æ ‡åºåˆ—ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ‰€æœ‰ä¸œè¥¿ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†â€”â€”å¥½åƒä¹Ÿä¸æ˜¯é‚£ä¹ˆå›°éš¾ï¼åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥ç™»å½•åˆ° Hugging Faceã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ Notebook è¿è¡Œä»£ç ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å®ç”¨å‡½æ•°è¿›è¡Œç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ Notebook ä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'pt'}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨ `Trainer` ã€‚æœ¬æ¬¡çš„è®­ç»ƒä¸­æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ï¼Œå¹¶è¿›è¡Œä¸€äº› Warmupã€‚è®­ç»ƒçš„ batch size æ˜¯ 256 ï¼ˆ `per_device_train_batch_size` * `gradient_accumulation_steps` ï¼‰ã€‚å½“å•ä¸ª batch æ— æ³•æ”¾å…¥å†…å­˜æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¹¶é€šè¿‡å¤šæ¬¡å‘å‰/å‘åä¼ é€’é€æ­¥ç´¯ç§¯æ¢¯åº¦ã€‚å½“æˆ‘ä»¬åœ¨æœ¬èŠ‚æœ€åä½¿ç”¨ ğŸ¤— Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ç°åœ¨æˆ‘ä»¬åªéœ€å¯åŠ¨ `Trainer` å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®ä½ æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒé›†è¿˜æ˜¯åœ¨è®­ç»ƒé›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿è¡Œå®ƒï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 æˆ– 2 ä¸ªå°æ—¶ï¼Œå› æ­¤è¯·å–æ¯å’–å•¡æˆ–è€…æ‰¾ä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼

```py
trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œ tokenizer æ¨é€åˆ° Hubï¼š

```py
trainer.push_to_hub()
```

{:else}

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒè¶…å‚æ•°å¹¶è°ƒç”¨ `compile()` å’Œ `fit()` æ–¹æ³•ã€‚ç„¶åä½¿ç”¨å¸¦æœ‰ä¸€äº›é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼š

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ç°åœ¨æˆ‘ä»¬åªéœ€è°ƒç”¨ `model.fit()` ï¼Œå¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®ä½ æ˜¯å¦åœ¨å®Œæ•´çš„è®­ç»ƒé›†æˆ–è€…è®­ç»ƒé›†çš„å­é›†ä¸Šè¿è¡Œï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 å°æ—¶æˆ–è€… 2 å°æ—¶ï¼Œæ‰€ä»¥æ‹¿ä¸€äº›å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œ tokenizer æ¨é€åˆ° Hubï¼š

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** é™¤äº† `TrainingArguments` ä¹‹å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦å¤§çº¦ 30 è¡Œä»£ç å°±å¯ä»¥ä»åŸå§‹æ–‡æœ¬åˆ°è®­ç»ƒ GPT-2ã€‚ç”¨ä½ è‡ªå·±çš„æ•°æ®é›†è¯•è¯•çœ‹ï¼Œçœ‹çœ‹ä½ èƒ½ä¸èƒ½å¾—åˆ°å¥½çš„ç»“æœï¼

</Tip>

<Tip>

{#if fw === 'pt'}

ğŸ’¡ å¦‚æœä½ èƒ½ä½¿ç”¨å¤š GPU çš„æœºå™¨ï¼Œå°è¯•åœ¨é‚£é‡Œè¿è¡Œä»£ç ã€‚ `Trainer` è‡ªåŠ¨ç®¡ç†å¤šå°æœºå™¨ï¼Œè¿™èƒ½æå¤§åœ°åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

{:else}

ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨å…·æœ‰å¤šä¸ª GPU çš„è®¡ç®—æœºï¼Œåˆ™å¯ä»¥å°è¯•ä½¿ç”¨ `MirroredStrategy` ä¸Šä¸‹æ–‡æ¥å¤§å¹…åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä½ éœ€è¦åˆ›å»ºä¸€ä¸ª `tf.distribute.MirroredStrategy` å¯¹è±¡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰çš„ `to_tf_dataset` æˆ– `prepare_tf_dataset()` æ–¹æ³•ä»¥åŠæ¨¡å‹åˆ›å»ºå’Œå¯¹ `fit()` çš„è°ƒç”¨éƒ½åœ¨å…¶ `scope()` ä¸Šä¸‹æ–‡ä¸­è¿è¡Œã€‚ä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit) æŸ¥çœ‹æœ‰å…³æ­¤å†…å®¹çš„æ–‡æ¡£ã€‚

{/if}

</Tip>

## ä½¿ç”¨ pipeline è¿›è¡Œä»£ç ç”Ÿæˆ [[ä½¿ç”¨ç®¡é“ç”Ÿæˆä»£ç ]]

ç°åœ¨æ˜¯è§è¯å¥‡è¿¹çš„æ—¶åˆ»ï¼šæˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°åº•è¡¨ç°å¦‚ä½•ï¼æˆ‘ä»¬å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°æŸå¤±æŒç»­ä¸‹é™ï¼Œä½†è¦æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬å°±çœ‹çœ‹å®ƒå¯¹ä¸€äº›æç¤ºçš„ååº”å¦‚ä½•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„ `pipeline` ä¸­ï¼Œå¹¶å¦‚æœæœ‰ GPU å¯ç”¨ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒæ”¾åœ¨ GPU ä¸ŠåŠ å¿«ç”Ÿæˆé€Ÿåº¦ï¼š

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

è®©æˆ‘ä»¬ä»ç®€å•çš„åˆ›å»ºæ•£ç‚¹å›¾ä»»åŠ¡å¼€å§‹ï¼š

```py
txt = """\
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä½¿ç”¨ x,y åˆ›å»ºæ•£ç‚¹å›¾
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä½¿ç”¨ x,y åˆ›å»ºæ•£ç‚¹å›¾
plt.scatter(x, y)

# åˆ›å»ºæ•£ç‚¹
```

ç»“æœçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚é‚£ä¹ˆå¯¹äº `pandas` æ“ä½œä¹Ÿå¯ä»¥å—ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦èƒ½ä»ä¸¤ä¸ªæ•°ç»„åˆ›å»ºä¸€ä¸ª `DataFrame` ï¼š

```py
txt = """\
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä» x å’Œ y åˆ›å»º dataframe
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä» x å’Œ y åˆ›å»º dataframe
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

å¾ˆå¥½ï¼Œè¿™æ˜¯æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”å°½ç®¡å®ƒåˆæŠŠ `x` é‡å¤æ’å…¥äº†ä¸€æ¬¡ã€‚è€Œä¸”ç”±äºç”Ÿæˆçš„ token æ•°é‡æœ‰é™ï¼Œæ‰€ä»¥ä¸‹é¢çš„ `for` å¾ªç¯è¢«åˆ‡æ–­äº†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½åšäº›æ›´å¤æ‚çš„äº‹æƒ…ï¼Œè®©æ¨¡å‹å¸®åŠ©æˆ‘ä»¬ä½¿ç”¨ `groupby` æ“ä½œï¼š

```py
txt = """\
# æœ‰èŒä¸š,æ”¶å…¥å’Œåå­—çš„ dataframe
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# è®¡ç®—æ¯ä¸ªèŒä¸šçš„å¹³å‡æ”¶å…¥
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# æœ‰èŒä¸š,æ”¶å…¥å’Œåå­—çš„ dataframe
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# è®¡ç®—æ¯ä¸ªèŒä¸šçš„å¹³å‡æ”¶å…¥
profession = df.groupby(['profession']).mean()

# è®¡ç®—
```

ä¸é”™ï¼›æ˜¯æ­£ç¡®çš„ã€‚æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦èƒ½å¼•å¯¼æ¨¡å‹ä½¿ç”¨ `scikit-learn` å¹¶å»ºç«‹ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```py
txt = """
# ä» scikit-learn å¯¼å…¥éšæœºæ£®æ—å›å½’å™¨
from sklearn.ensemble import RandomForestRegressor

# ç”¨ X, y æ‹Ÿåˆå¸¦æœ‰ 300 ä¸ªä¼°ç®—å™¨çš„éšæœºæ£®æ—æ¨¡å‹:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# ä» scikit-learn å¯¼å…¥éšæœºæ£®æ—å›å½’å™¨
from sklearn.ensemble import RandomForestRegressor

# ç”¨ X, y æ‹Ÿåˆå¸¦æœ‰ 300 ä¸ªä¼°ç®—å™¨çš„éšæœºæ£®æ—æ¨¡å‹:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

ä»ä¸Šè¿°å‡ ä¸ªç¤ºä¾‹æ¥çœ‹ï¼Œä¼¼ä¹æ¨¡å‹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦æ ˆçš„ä¸€äº›è¯­æ³•ã€‚å½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å½»åº•åœ°è¯„ä¼°æ¨¡å‹ï¼Œä½†è¿™ä»ç„¶æ˜¯ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„åŸå‹ã€‚

{:else}

ä»è¿™å‡ ä¸ªä¾‹å­æ¥çœ‹ï¼Œæ¨¡å‹ä¼¼ä¹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ï¼ˆå½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼‰ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å€™å®ƒéœ€è¦æ›´å¤šçš„æ¨¡å‹è®­ç»ƒå®šåˆ¶æ¥è¾¾åˆ°ç‰¹å®šæƒ…å¢ƒæ‰€éœ€çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åŠ¨æ€æ›´æ–° `batch_size` æˆ–æ·»åŠ ä¸€ä¸ªæ¡ä»¶è®­ç»ƒå¾ªç¯æ¥è·³è¿‡åç¤ºä¾‹æ€ä¹ˆåŠï¼Ÿä¸€ç§é€‰æ‹©æ˜¯ä¿®æ”¹ `Trainer` æ·»åŠ æ–°çš„åŠŸèƒ½ï¼Œä½†æœ‰æ—¶ä»å¤´å¼€å§‹ç¼–å†™è®­ç»ƒå¾ªç¯ä¼šæ›´ç®€å•ã€‚è¿™å°±æ˜¯ğŸ¤— Accelerate çš„ç”¨æ­¦ä¹‹åœ°ã€‚

{/if}

{#if fw === 'pt'}

## ä½¿ç”¨ğŸ¤— Accelerate è¿›è¡Œè®­ç»ƒ [[ä½¿ç”¨ğŸ¤— Accelerate è¿›è¡Œè®­ç»ƒ]]

æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ `Trainer` è®­ç»ƒæ¨¡å‹ï¼Œåœ¨ `Trainer` ä¸­å¯ä»¥å¯¹è®­ç»ƒè¿‡ç¨‹å¯ä»¥é€šè¿‡ä¿®æ”¹ä¸€äº›å‚æ•°è¿›è¡Œä¸€äº›å®šåˆ¶ã€‚ç„¶è€Œï¼Œæœ‰æ—¶æˆ‘ä»¬æƒ³è¦å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯ï¼Œæˆ–è€…æˆ‘ä»¬æƒ³è¦è¿›è¡Œä¸€äº›æ›´è‡ªç”±çš„çš„æ›´æ”¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œæœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†è®©äº‹æƒ…å˜å¾—æ›´æœ‰è¶£ï¼Œç›¸æ¯”äºä¸Šé¢çš„ `Trainer` æˆ‘ä»¬è¿˜å°†åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ä¸€äº›ä¿®æ”¹ã€‚

<Youtube id="Hm8_PgVTFuc"/>

ç”±äºæˆ‘ä»¬ä¸»è¦å…³æ³¨çš„æ˜¯ä¸ºæ•°æ®ç§‘å­¦åº“æä¾›åˆç†çš„ä»£ç è‡ªåŠ¨è¡¥å……åŠŸèƒ½ï¼Œå› æ­¤å¯¹äºæ›´å¤šä½¿ç”¨è¿™äº›åº“çš„è®­ç»ƒæ ·æœ¬èµ‹äºˆæ›´é«˜çš„æƒé‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `plt` ã€ `pd` ã€ `sk` ã€ `fit` å’Œ `predict` ç­‰å…³é”®è¯æ¥è½»æ¾åœ°è¯†åˆ«å‡ºè¿™äº›ä¾‹å­ï¼Œè¿™äº›å…³é”®è¯æ˜¯ `matplotlib.pyplot` ã€ `pandas` å’Œ `sklearn` å¯¼å…¥åæœ€å¸¸ç”¨é‡å‘½åçš„åç§°ï¼Œä»¥åŠ `sklearn` çš„ `fit/predict` æ–¹æ³•ã€‚å¦‚æœè¿™äº›åœ¨æ¨¡å‹çš„å†…éƒ¨æ˜¯ç”¨å•ä¸€çš„ä¸€ä¸ª `token` è¡¨ç¤ºçš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ token çš„ id è½»æ¾åœ°æ£€æŸ¥å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨è¾“å…¥åºåˆ—ä¸­ã€‚ç„¶è€Œï¼ŒTokens æœ‰å¯èƒ½æœ‰ç©ºæ ¼å‰ç¼€ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿéœ€è¦åœ¨ tokenizer è¯æ±‡è¡¨ä¸­æ£€æŸ¥è¿™äº›å…³é”®è¯ã€‚ä¸ºäº†éªŒè¯è¿™ä¸ªç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä¼šåœ¨æµ‹è¯•æ ·æœ¬ä¸­æ·»åŠ ä¸€ä¸ªåº”è¯¥è¢«åˆ†å‰²ä¸ºå¤šä¸ª tokens çš„æµ‹è¯• tokenï¼š

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

å¤ªå¥½äº†ï¼Œè¿™ä¸ªæ–¹æ³•ä¼¼ä¹å¾ˆæœ‰æ•ˆï¼æˆ‘ä»¬ç°åœ¨å¯ä»¥ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°ï¼Œå®ƒçš„è¾“å…¥æœ‰è¾“å…¥åºåˆ—ã€logits å’Œæˆ‘ä»¬åˆšåˆšé€‰æ‹©çš„å…³é”®å­—ã€‚é¦–å…ˆéœ€è¦å¯¹é½ `logits` å’Œ `inputs` : å¹¶å°†è¾“å…¥åºåˆ—å³ç§»ä¸€ä¸ªå•ä½å½¢æˆç›®æ ‡åºåˆ—ï¼Œå› ä¸ºä¸‹ä¸€ä¸ª `token` å°±æ˜¯å½“å‰ `token` çš„é¢„æµ‹çš„ç›®æ ‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»è¾“å…¥åºåˆ—çš„ç¬¬äºŒä¸ª `token` å¼€å§‹è®¾ç½®æ ‡ç­¾ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šé¢„æµ‹ç¬¬ä¸€ä¸ª `token`ã€‚ç„¶åæˆ‘ä»¬æˆªæ–­æœ€åä¸€ä¸ª `logit`ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å®Œæ•´è¾“å…¥åºåˆ—åé¢çš„æ ‡ç­¾ã€‚æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸­æ‰€æœ‰å…³é”®è¯çš„å‡ºç°æ¬¡æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ºç°æ¬¡æ•°ä½œä¸ºæƒé‡ï¼Œè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŠ æƒå¹³å‡å€¼ã€‚ç”±äºæˆ‘ä»¬ä¸æƒ³æŠ›å¼ƒæ‰€æœ‰æ²¡æœ‰å…³é”®è¯çš„æ ·æœ¬ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„æƒé‡éƒ½åŠ  1ï¼š

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # å·¦ç§» tokens < n é¢„æµ‹ n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # è®¡ç®—æ¯ä¸€ä¸ªtokençš„loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # å¯¹äºæ¯ä¸ªæ ·æœ¬é‡æ–°è°ƒæ•´å¤§å°å¹¶å¹³å‡
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # è®¡ç®—å¹¶ç¼©æ”¾æƒé‡
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # è®¡ç®—è¯„ä»·æƒé‡
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

åœ¨æˆ‘ä»¬å¼€å§‹ä½¿ç”¨è¿™ä¸ªç²¾å¦™çš„æ–°æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€äº›äº‹æƒ…ï¼š

- æˆ‘ä»¬éœ€è¦æ•°æ®åŠ è½½å™¨æ¥æ‰¹é‡åŠ è½½æ•°æ®ã€‚
- æˆ‘ä»¬éœ€è¦è®¾ç½®æƒé‡è¡°å‡å‚æ•°ã€‚
- æœ‰æ—¶æˆ‘ä»¬åœ¨è°ƒè¯•æ¨¡å‹çš„æ—¶å€™å¯èƒ½éœ€è¦ä¸´æ—¶è¯„ä¼°ï¼Œæ‰€ä»¥å°†è¯„ä¼°ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ã€‚

è®©æˆ‘ä»¬ä»æ•°æ®åŠ è½½å™¨å¼€å§‹ã€‚æˆ‘ä»¬åªéœ€è¦å°†æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸º `"torch"` ï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä¼ é€’ç»™ä¸€ä¸ªå…·æœ‰é€‚å½“ `batch size` çš„ PyTorch çš„ `DataLoader` ï¼š

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‚æ•°åˆ†ç»„ï¼Œä»¥ä¾¿ä¼˜åŒ–å™¨çŸ¥é“å“ªäº›å‚æ•°éœ€è¦è¿›è¡Œé¢å¤–çš„æƒé‡è¡°å‡ã€‚é€šå¸¸ï¼Œæ‰€æœ‰çš„åç½®å’Œ LayerNorm æƒé‡é¡¹éƒ½ä¸éœ€è¦è¿›è¡Œæƒé‡è¡°å‡ï¼›å› æ­¤æˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

æˆ‘ä»¬å¸Œæœ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™ä¸€ä¸ªå‡½æ•°ã€‚å®ƒåªéœ€éå†è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼Œå¹¶æ”¶é›†æ‰€æœ‰è¿›ç¨‹ä¸­çš„æŸå¤±å€¼ï¼š

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

é€šè¿‡ `evaluate()` å‡½æ•°æˆ‘ä»¬å®šæœŸå¯ä»¥è·å–æŸå¤±å€¼å’Œ [å›°æƒ‘åº¦ï¼ˆperplexityï¼‰](/course/chapter7/3) ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡æ–°åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç¡®ä¿æˆ‘ä»¬å†æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè€Œä¸æ˜¯ä»ä¸Šé¢çš„ `Trainer` ç»§ç»­å¾®è°ƒï¼š

```py
model = GPT2LMHeadModel(config)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¹‹å‰çš„å‡½æ•°æ¥åˆ†å‰²æƒé‡è¡°å‡çš„å‚æ•°ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ç°åœ¨è®©æˆ‘ä»¬å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒï¼š

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œä½ éœ€è¦å°†ä¸Šè¿°å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åˆ°ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜… [ç¬¬ä¸‰ç« ](/course/chapter3) ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` ä¼ é€’ç»™äº† `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `len()` æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨å‡†å¤‡å¥½ `dataloader` åå†ä½¿ç”¨ `len()` ï¼Œå› ä¸ºæ”¹åŠ¨ `dataloader` ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š


```py
num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç™»å½•çš„è¯ï¼Œé¦–å…ˆéœ€è¦ç™»å½•åˆ° Hugging Faceï¼Œæˆ‘ä»¬å°†æ ¹æ®æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„åå­—æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨ `get_full_repo_name()` å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„ `repo_name`ï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¯¥ä»“åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœæœ¬åœ°å·²ç»å­˜åœ¨ä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„ä»“åº“çš„å…‹éš†åœ¨æœ¬åœ°çš„ç‰ˆæœ¬ï¼š
```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

ç›®å‰çš„æŸå¤±å’Œå›°æƒ‘åº¦éƒ½æ˜¯éå¸¸é«˜çš„å€¼ï¼Œä½†è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ã€‚åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä¸ºç¼–å†™è®­ç»ƒè„šæœ¬çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šè®­ç»ƒå¾ªç¯å·²ç»åšå¥½äº†å‡†å¤‡ã€‚åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬è¿­ä»£éå†æ•°æ®åŠ è½½å™¨å¹¶å°†æˆæ‰¹é‡çš„æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚æœ‰äº†æ¨¡å‹è¾“å‡ºçš„ logitsï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°è®¡ç®—æŸä¼¤ã€‚æˆ‘ä»¬é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„æ•°é‡æ¥ç¼©æ”¾æŸå¤±ï¼Œä»¥é¿å…åœ¨èšåˆæ›´å¤šæ­¥éª¤æ—¶äº§ç”Ÿæ›´å¤§çš„æŸå¤±ã€‚åœ¨æˆ‘ä»¬ä¼˜åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå‰ªè£æ¢¯åº¦æ¥æ›´å¥½çš„æ”¶æ•›ã€‚æœ€åï¼Œæ¯éš”ä¸€æ®µæ­¥æ•°ï¼Œæˆ‘ä»¬ç”¨æ–°çš„ `evaluate()` å‡½æ•°åœ¨è¯„ä¼°é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼š

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

å°±æ˜¯è¿™æ · - ä½ ç°åœ¨æ‹¥æœ‰è‡ªå·±çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰çš„è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦è¿›ä¸€æ­¥å®šåˆ¶ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** åˆ›å»ºé€‚åˆä½ çš„ç”¨ä¾‹çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å¦ä¸€ä¸ªè‡ªå®šä¹‰æ­¥éª¤ã€‚

</Tip>

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼**  å½“è¿è¡Œé•¿æ—¶é—´çš„è®­ç»ƒå®éªŒæ—¶ï¼Œä½¿ç”¨ TensorBoard æˆ– Weights & Biases ç­‰å·¥å…·è®°å½•é‡è¦æŒ‡æ ‡æ˜¯ä¸ªå¥½ä¸»æ„ã€‚å‘è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ é€‚å½“çš„æ—¥å¿—è®°å½•ï¼Œè¿™æ ·ä½ å¯ä»¥éšæ—¶æ£€æŸ¥è®­ç»ƒè¿›åº¦ã€‚

</Tip>

{/if}