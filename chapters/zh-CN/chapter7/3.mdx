<FrameworkSwitchCourse {fw} />

# å¾®è°ƒæ©ç è¯­è¨€æ¨¡å‹ï¼ˆmasked language modelï¼‰ [[å¾®è°ƒæ©ç è¯­è¨€æ¨¡å‹ï¼ˆmasked language modelï¼‰]]

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section3_tf.ipynb"},
]} />

{/if}

å¯¹äºè®¸å¤šæ¶‰åŠ Transformer æ¨¡å‹çš„ NLP ä»»åŠ¡ï¼Œä½ å¯ä»¥ç®€å•åœ°ä» Hugging Face Hub ä¸­è·å–ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œç„¶åç›´æ¥åœ¨ä½ çš„æ•°æ®ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥å®Œæˆæ‰‹å¤´çš„ä»»åŠ¡ã€‚åªè¦ç”¨äºé¢„è®­ç»ƒçš„è¯­æ–™åº“ä¸ç”¨äºå¾®è°ƒçš„è¯­æ–™åº“æ²¡æœ‰å¤ªå¤§åŒºåˆ«ï¼Œè¿ç§»å­¦ä¹ é€šå¸¸ä¼šäº§ç”Ÿå¾ˆå¥½çš„ç»“æœã€‚

ä½†æ˜¯ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦å…ˆåœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œç„¶åå†è®­ç»ƒç‰¹å®šäºä»»åŠ¡çš„ headã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„æ•°æ®é›†åŒ…å«æ³•å¾‹åˆåŒæˆ–ç§‘å­¦æ–‡ç« ï¼Œåƒ BERT è¿™æ ·çš„æ™®é€š Transformer æ¨¡å‹é€šå¸¸ä¼šå°†ä½ è¯­æ–™åº“ä¸­çš„ç‰¹å®šé¢†åŸŸè¯è§†ä¸ºç¨€æœ‰ tokens ï¼Œå¯¼è‡´æ€§èƒ½å¯èƒ½ä¸å°½å¦‚äººæ„ã€‚é€šè¿‡åœ¨ç‰¹å®šé¢†åŸŸå†…æ•°æ®ä¸Šå¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä½ å¯ä»¥æé«˜è®¸å¤šä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿™æ„å‘³ç€ä½ é€šå¸¸åªéœ€æ‰§è¡Œä¸€æ¬¡æ­¤æ­¥éª¤ï¼

è¿™ç§åœ¨ç‰¹å®šé¢†åŸŸå†…æ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¿‡ç¨‹é€šå¸¸ç§°ä¸º `é¢†åŸŸé€‚åº”ï¼ˆdomain adaptationï¼‰` ã€‚å®ƒäº 2018 å¹´ç”± [ULMFiT](https://arxiv.org/abs/1801.06146) æ¨å¹¿ï¼Œ NLP çš„é¦–æ‰¹ç¥ç»æ¶æ„ä¹‹ä¸€ ï¼ˆåŸºäº LSTMï¼‰ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†ä½¿ç”¨ ULMFiTè¿™æ˜¯ä½¿è¿ç§»å­¦ä¹ çœŸæ­£é€‚ç”¨äº è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”çš„ç¤ºä¾‹ï¼›åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åšç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨ Transformer è€Œä¸æ˜¯ LSTMï¼

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg" alt="ULMFiT."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg" alt="ULMFiT."/>
</div>

åœ¨æœ¬èŠ‚ç»“æŸæ—¶ï¼Œä½ å°†åœ¨ Hub ä¸Šæ‹¥æœ‰ä¸€ä¸ª [æ©ç è¯­è¨€æ¨¡å‹(masked language model)](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) ï¼Œè¯¥æ¨¡å‹å¯ä»¥è‡ªåŠ¨è¡¥å…¨å¥å­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

<iframe src="https://course-demos-distilbert-base-uncased-finetuned-imdb.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

<Youtube id="mqElG5QJWUg"/>

<Tip>

ğŸ™‹ å¦‚æœä½ å¯¹â€œæ©ç è¯­è¨€å»ºæ¨¡â€å’Œâ€œé¢„è®­ç»ƒæ¨¡å‹â€è¿™ä¸¤ä¸ªæœ¯è¯­æ„Ÿåˆ°é™Œç”Ÿï¼Œè¯·å›é¡¾ [ç¬¬ä¸€ç« ](/course/chapter1) ï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­è§£é‡Šäº†æ‰€æœ‰è¿™äº›æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶é™„æœ‰è§†é¢‘ï¼

</Tip>

## é€‰æ‹©ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ [[é€‰æ‹©ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹]]

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä¸ºæ©ç è¯­è¨€å»ºæ¨¡é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚ä»¥ä¸‹å±å¹•æˆªå›¾æ‰€ç¤ºï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads) ä¸Šé€‰æ‹©â€œFill-Maskâ€è¿‡æ»¤å™¨ï¼š

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png" alt="Hub models." width="80%"/>
</div>

å°½ç®¡ BERT å’Œ RoBERTa ç³»åˆ—æ¨¡å‹çš„ä¸‹è½½é‡æœ€å¤§ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨åä¸º [DistilBERT](https://huggingface.co/distilbert-base-uncased) çš„æ¨¡å‹ã€‚å®ƒå¯ä»¥æ›´å¿«åœ°è®­ç»ƒï¼Œè€Œä¸”å¯¹ä¸‹æ¸¸æ€§èƒ½å‡ ä¹æ²¡æœ‰æŸå¤±ã€‚å®ƒä½¿ç”¨äº†ä¸€ç§ç§°ä¸º [`çŸ¥è¯†è’¸é¦ï¼ˆknowledge distillationï¼‰`](https://en.wikipedia.org/wiki/Knowledge_distillation) çš„ç‰¹æ®ŠæŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­ä½¿ç”¨åƒ BERT è¿™æ ·çš„å¤§å‹â€œæ•™å¸ˆæ¨¡å‹â€æ¥æŒ‡å¯¼å‚æ•°å°‘å¾—å¤šçš„â€œå­¦ç”Ÿæ¨¡å‹â€çš„è®­ç»ƒã€‚åœ¨æœ¬èŠ‚ä¸­å¯¹çŸ¥è¯†è’¸é¦è¿›è¡Œè¯¦ç»†è§£é‡Šä¼šä½¿æˆ‘ä»¬åç¦»æœ¬èŠ‚ä¸»é¢˜å¤ªè¿œï¼Œä½†å¦‚æœä½ æœ‰å…´è¶£ï¼Œå¯ä»¥é˜…è¯» [`ä½¿ç”¨ Transformers è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processing with Transformersï¼‰`](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) ï¼ˆä¿—ç§° Transformers æ•™ç§‘ä¹¦ï¼‰ä¸­çŸ¥è¯†è’¸é¦çš„ç›¸å…³å†…å®¹ã€‚

{#if fw === 'pt'}

è®©æˆ‘ä»¬ç»§ç»­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `AutoModelForMaskedLM` ç±»ä¸‹è½½ DistilBERTï¼š

```python
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `num_parameters()` æ–¹æ³•æŸ¥çœ‹æ¨¡å‹æœ‰å¤šå°‘å‚æ•°ï¼š

```python
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")
```

```python out
'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'
```

{:else}

è®©æˆ‘ä»¬ç»§ç»­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `TFAutoModelForMaskedLM` ç±»ä¸‹è½½ DistilBERTï¼š

```python
from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `summary()` æ–¹æ³•æŸ¥çœ‹æ¨¡å‹æœ‰å¤šå°‘å‚æ•°ï¼š

```python
model(model.dummy_inputs)  # æ„å»ºæ¨¡å‹
model.summary()
```

```python out
Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 distilbert (TFDistilBertMai  multiple                 66362880  
 nLayer)                                                         
                                                                 
 vocab_transform (Dense)     multiple                  590592    
                                                                 
 vocab_layer_norm (LayerNorm  multiple                 1536      
 alization)                                                      
                                                                 
 vocab_projector (TFDistilBe  multiple                 23866170  
 rtLMHead)                                                       
                                                                 
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________
```

{/if}

DistilBERT å¤§çº¦æœ‰ 6700 ä¸‡ä¸ªå‚æ•°ï¼Œå¤§çº¦åªæœ‰ BERT base æ¨¡å‹çš„äºŒåˆ†ä¹‹ä¸€ï¼Œè¿™å¤§è‡´æ„å‘³ç€è®­ç»ƒçš„é€Ÿåº¦å¯ä»¥æé«˜ä¸¤å€ â€”â€” éå¸¸æ£’ï¼ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¯¹äºä¸‹é¢çš„ä¸€å°éƒ¨åˆ†æ–‡æœ¬ï¼Œè¿™ä¸ªæ¨¡å‹æœ€æœ‰å¯èƒ½é¢„æµ‹ä»€ä¹ˆï¼š

```python
text = "This is a great [MASK]."
```

ä½œä¸ºäººç±»ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ `[MASK]` token æœ‰å¾ˆå¤šå¯èƒ½æ€§ï¼Œä¾‹å¦‚ â€œdayâ€ã€â€œrideâ€ æˆ–è€… â€œpaintingâ€ã€‚å¯¹äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œé¢„æµ‹å–å†³äºæ¨¡å‹æ‰€è®­ç»ƒçš„è¯­æ–™åº“ï¼Œå› ä¸ºå®ƒä¼šå­¦ä¹ è·å–æ•°æ®ä¸­å­˜åœ¨çš„è¯­æ–™ç»Ÿè®¡åˆ†å¸ƒã€‚ä¸ BERT ä¸€æ ·ï¼ŒDistilBERT åœ¨ [English Wikipedia](https://huggingface.co/datasets/wikipedia) å’Œ [BookCorpus](https://huggingface.co/datasets/bookcorpus) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬çŒœæƒ³æ¨¡å‹å¯¹ `[MASK]` çš„é¢„æµ‹èƒ½å¤Ÿåæ˜ è¿™äº›é¢†åŸŸã€‚ä¸ºäº†é¢„æµ‹ `[MASK]` ï¼Œæˆ‘ä»¬éœ€è¦ DistilBERT çš„ tokenizer æ¥å¤„ç†è¾“å…¥ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¹Ÿä» Hub ä¸‹è½½å®ƒï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

æœ‰äº† tokenizer å’Œæ¨¡å‹ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å°†æˆ‘ä»¬çš„ç¤ºä¾‹æ–‡æœ¬ä¼ é€’ç»™æ¨¡å‹ï¼Œæå– logitsï¼Œå¹¶æ‰“å°å‡ºå‰ 5 ä¸ªå€™é€‰è¯ï¼š

{#if fw === 'pt'}

```python
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# æ‰¾åˆ° [MASK] çš„ä½ç½®å¹¶æå–å…¶ logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# é€‰æ‹©å…·æœ‰æœ€é«˜ logits çš„ [MASK] å€™é€‰è¯
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")
```

{:else}

```python
import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# æ‰¾åˆ° [MASK] çš„ä½ç½®å¹¶æå–å…¶ logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# é€‰æ‹©å…·æœ‰æœ€é«˜ logits çš„ [MASK] å€™é€‰è¯
# é€šè¿‡åœ¨ argsort å‰å¯¹æ•°ç»„å–è´Ÿ,æ¥å¾—åˆ°æœ€å¤§çš„ logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")
```

{/if}

```python out
'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'
```

æˆ‘ä»¬å¯ä»¥ä»è¾“å‡ºä¸­çœ‹åˆ°ï¼Œæ¨¡å‹çš„é¢„æµ‹çš„æ˜¯æ—¥å¸¸æœ¯è¯­ï¼Œè€ƒè™‘åˆ°æ¨¡å‹è®­ç»ƒçš„è¯­æ–™æ•°æ®ä¸»è¦æ¥æºäºç»´åŸºç™¾ç§‘ï¼Œè¿™å¹¶ä¸å¥‡æ€ªã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†è¿™ä¸ªé¢†åŸŸæ”¹å˜æˆç¨å¾®æ›´åŠ ç‹¬ç‰¹â€”â€”é«˜åº¦ä¸¤æåˆ†åŒ–çš„ç”µå½±è¯„è®ºï¼

## æ•°æ®é›† [[æ•°æ®é›†]]

ä¸ºäº†å±•ç¤ºé¢†åŸŸé€‚åº”æ€§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªIMDBçš„ [å¤§å‹ç”µå½±è¯„è®ºæ•°æ®é›†(Large Movie Review Dataset)](https://huggingface.co/datasets/imdb)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”µå½±è¯„è®ºè¯­æ–™åº“ï¼Œé€šå¸¸ç”¨äºå¯¹æƒ…æ„Ÿåˆ†ææ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡åœ¨è¿™ä¸ªè¯­æ–™åº“ä¸Šå¯¹ DistilBERT è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬æœŸæœ›è¯­è¨€æ¨¡å‹ä¼šä»å…¶é¢„è®­ç»ƒçš„ç»´åŸºç™¾ç§‘çš„äº‹å®æ€§æ•°æ®ï¼Œé€‚åº”åˆ°æ›´ä¸»è§‚çš„ç”µå½±è¯„è®ºçš„é¢†åŸŸã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Datasets ä¸­çš„ `load_dataset()` å‡½æ•°ä» Hugging Face ä¸­è·å–æ•°æ®ï¼š

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `train` å’Œ `test` åˆ†åˆ«åŒ…å«äº† 25,000 æ¡è¯„è®ºï¼Œè¿˜æœ‰ä¸€ä¸ªæ²¡æœ‰çš„æ ‡ç­¾çš„ `unsupervisedï¼ˆæ— ç›‘ç£ï¼‰` éƒ¨åˆ†åŒ…å« 50,000 æ¡è¯„è®ºã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬ä»é‡Œé¢å–ä¸€äº›æ ·æœ¬ï¼Œæ¥äº†è§£ä¸€ä¸‹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„æ–‡æœ¬çš„ç‰¹ç‚¹ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹çš„å‰å‡ ç« ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†æŠŠ `Dataset.shuffle()` å‡½æ•°é“¾æ¥åˆ° `Dataset.select()` å‡½æ•°åˆ›å»ºéšæœºæ ·æœ¬ï¼š

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")
```

```python out

'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clichÃ©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'
```

æ˜¯çš„ï¼Œè¿™äº›è‚¯å®šæ˜¯ç”µå½±è¯„è®ºï¼Œå¦‚æœä½ å¹´é¾„è¶³å¤Ÿå¤§ï¼Œä½ ç”šè‡³å¯èƒ½ä¼šç†è§£ä¸Šè¿°è¯„è®ºä¸­å…³äºæ‹¥æœ‰ VHS ï¼ˆä¸€ç§å¤è€çš„ç›’å¼æ‘„åƒæœºæ ¼å¼ï¼‰ç‰ˆæœ¬çš„è¯„è®ºğŸ˜œï¼è™½ç„¶è¯­è¨€æ¨¡å‹ä¸éœ€è¦é¢„å…ˆæ ‡æ³¨å¥½çš„æ ‡ç­¾ï¼Œä½†æˆ‘ä»¬å·²ç»å¯ä»¥çœ‹åˆ°æ•°æ®é›†å…¶å®åŒ…å«äº†æ ‡ç­¾ï¼Œ `0` ä»£è¡¨è´Ÿé¢è¯„è®ºï¼Œ `1` ä»£è¡¨æ­£é¢è¯„è®ºã€‚

<Tip>

âœï¸ **è¯•ä¸€è¯•ï¼** åˆ›å»ºä¸€ä¸ª `unsupervised` éƒ¨åˆ†çš„éšæœºæ ·æœ¬ï¼Œå¹¶éªŒè¯å…¶æ ‡ç­¾æ—¢ä¸æ˜¯ `0` ä¹Ÿä¸æ˜¯ `1` ã€‚æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥æ£€æŸ¥ `train` å’Œ `test` éƒ¨åˆ†çš„æ ‡ç­¾ç¡®å®æ˜¯ `0` æˆ– `1` â€”â€” æ¯ä¸ª NLP å®è·µè€…åœ¨å¼€å§‹æ–°é¡¹ç›®æ—¶éƒ½åº”è¯¥å¯¹æ•°æ®æ ‡æ³¨è¿›è¡Œçš„æœ‰ç”¨çš„ã€åˆç†çš„æ£€æŸ¥ï¼

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å¿«é€Ÿæµè§ˆäº†ä¸€ä¸‹æ•°æ®ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦æ·±å…¥å‡†å¤‡è¿™äº›æ•°æ®ä»¥ä¾›è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡ã€‚å¦‚æˆ‘ä»¬æ‰€è§ï¼Œä¸æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](https://chat.openai.com/course/chapter3) çœ‹åˆ°çš„åºåˆ—åˆ†ç±»ä»»åŠ¡ç›¸æ¯”ï¼Œè¿™é‡Œéœ€è¦é‡‡å–ä¸€äº›é¢å¤–çš„æ­¥éª¤ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

## é¢„å¤„ç†æ•°æ® [[é¢„å¤„ç†æ•°æ®]]

<Youtube id="8PmhEIXhBvI"/>

å¯¹äºè‡ªå›å½’å’Œæ©ç è¯­è¨€å»ºæ¨¡ï¼Œå¸¸è§çš„é¢„å¤„ç†æ­¥éª¤æ˜¯å°†æ‰€æœ‰çš„æ–‡æœ¬æ‹¼æ¥èµ·æ¥ï¼Œç„¶åå†å°†æ•´ä¸ªè¯­æ–™åº“åˆ‡å‰²ä¸ºç›¸åŒå¤§å°çš„å—ã€‚è¿™ä¸æˆ‘ä»¬ä¹‹å‰çš„åšæ³•æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œæˆ‘ä»¬ä¹‹å‰åªæ˜¯å¯¹å•ä¸ªçš„ç¤ºä¾‹è¿›è¡Œ tokenizeã€‚ä¸ºä»€ä¹ˆè¦å°†æ‰€æœ‰çš„ç¤ºä¾‹è¿æ¥åœ¨ä¸€èµ·å‘¢ï¼ŸåŸå› æ˜¯å¦‚æœå•ä¸ªç¤ºä¾‹å¤ªé•¿ï¼Œå¯èƒ½ä¼šè¢«æˆªæ–­ï¼Œè¿™ä¼šå¯¼è‡´æˆ‘ä»¬å¤±å»å¯èƒ½å¯¹è¯­è¨€å»ºæ¨¡ä»»åŠ¡æœ‰ç”¨çš„ä¿¡æ¯ï¼

å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆä¼šåƒå¾€å¸¸ä¸€æ ·å¯¹è¯­æ–™åº“è¿›è¡Œ tokenize å¤„ç†ï¼Œä½†æ˜¯ä¸åœ¨ tokenizer ä¸­è®¾ç½® `truncation=True` é€‰é¡¹ã€‚å¦‚æœæˆ‘ä»¬æœ‰å¯ä»¥ä½¿ç”¨å¿«é€Ÿ tokenizerï¼ˆå¦‚ [ç¬¬å…­ç« ](/course/chapter6/3) ä¸­æ‰€è¿°ï¼‰ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–å•è¯çš„ `ID`ï¼Œå› ä¸ºåé¢æˆ‘ä»¬éœ€è¦ç”¨åˆ°å®ƒä»¬æ¥è¿›è¡Œå…¨è¯æ©ç ã€‚æœ€åæˆ‘ä»¬å°†æŠŠè¿™ä¸ªè¿‡ç¨‹å°è£…åœ¨ä¸€ä¸ªç®€å•çš„å‡½æ•°ä¸­ï¼Œå¹¶åˆ é™¤ `text` å’Œ `label` åˆ—ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦å®ƒä»¬ã€‚

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# ä½¿ç”¨ batched=True æ¥æ¿€æ´»å¿«é€Ÿå¤šçº¿ç¨‹!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})
```

ç”±äº DistilBERT æ˜¯ä¸€ä¸ªç±»ä¼¼ BERT çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¼–ç åçš„æ–‡æœ¬åŒ…å«äº†æˆ‘ä»¬åœ¨ä¹‹å‰ç« èŠ‚ä¸­çœ‹åˆ°çš„ `input_ids` å’Œ `attention_mask` ï¼Œä»¥åŠæˆ‘ä»¬æ·»åŠ çš„ `word_ids` ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹ç”µå½±è¯„è®ºè¿›è¡Œäº† `tokenize`ï¼Œä¸‹ä¸€æ­¥æ˜¯å°†å®ƒä»¬å…¨éƒ¨ç»„åˆåœ¨ä¸€èµ·å¹¶å°†ç»“æœåˆ†å‰²æˆå—ã€‚ä½†æ˜¯ï¼Œè¿™äº›å—åº”è¯¥æœ‰å¤šå¤§å‘¢ï¼Ÿè¿™æœ€ç»ˆå°†å–å†³äºä½ å¯ä»¥ä½¿ç”¨çš„æ˜¾å­˜å¤§å°ï¼Œä½†ä¸€ä¸ªå¥½çš„èµ·ç‚¹æ˜¯æŸ¥çœ‹æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚è¿™å¯ä»¥åœ¨ tokenizer çš„ `model_max_length` å±æ€§ä¸­æ‰¾åˆ°ï¼š

```python
tokenizer.model_max_length
```

```python out
512
```

è¯¥å€¼æ¥è‡ªäºä¸ checkpoint ç›¸å…³è”çš„ `tokenizer_config.json` æ–‡ä»¶ï¼›åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šä¸‹æ–‡å¤§å°æ˜¯ 512 ä¸ª `tokens` ä¸ BERT æ¨¡å‹ä¸€æ ·ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** ä¸€äº› Transformer æ¨¡å‹ï¼Œä¾‹å¦‚ [BigBird](https://huggingface.co/google/bigbird-roberta-base) å’Œ [Longformer](hf.co/allenai/longformer-base-4096) å…·æœ‰æ¯” BERT å’Œå…¶ä»–æ—©æœŸ Transformer æ¨¡å‹æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€‰æ‹©ä¸€ä¸ª `checkpoint` æ¥å®ä¾‹åŒ– `tokenizer` å¹¶éªŒè¯ `model_max_length` æ˜¯å¦ä¸æ¨¡å‹å¡ä¸Šæ ‡æ³¨çš„å¤§å°ä¸€è‡´ã€‚

</Tip>

å› æ­¤ï¼Œä¸ºäº†å¯ä»¥åœ¨åƒ Google Colab é‚£æ ·çš„ GPU ä¸Šè¿è¡Œæˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©ä¸€ä¸ªç¨å°ä¸€ç‚¹ã€å¯ä»¥æ”¾å…¥å†…å­˜ä¸­çš„åˆ†å—å¤§å°ï¼š

```python
chunk_size = 128
```

<Tip warning={true}>

æ³¨æ„ï¼Œåœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œä½¿ç”¨å°çš„å—å¯èƒ½ä¼šæœ‰ä¸¢å¤±é•¿å¥å­ä¹‹é—´çš„è¯­ä¹‰ä¿¡æ¯ä»è€Œå¯¹æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿä¸åˆ©çš„å½±å“ï¼Œæ‰€ä»¥å¦‚æœæ˜¾å­˜æ¡ä»¶å…è®¸çš„è¯ï¼Œä½ åº”è¯¥é€‰æ‹©ä¸€ä¸ªä¸ä½ å°†è¦ä½¿ç”¨æ¨¡å‹çš„ç›¸åŒ¹é…çš„å¤§å°ã€‚

</Tip>

ç°åœ¨æ¥åˆ°äº†æœ€æœ‰è¶£çš„éƒ¨åˆ†ã€‚ä¸ºäº†å±•ç¤ºå¦‚ä½•æŠŠè¿™äº›ç¤ºä¾‹è¿æ¥åœ¨ä¸€ï¼Œæˆ‘ä»¬ä»åˆ†è¯åçš„è®­ç»ƒé›†ä¸­å–å‡ºå‡ ä¸ªè¯„è®ºï¼Œå¹¶æ‰“å°å‡ºæ¯ä¸ªè¯„è®ºçš„ token æ•°é‡ï¼š

```python
# åˆ‡ç‰‡ä¼šä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python out
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„å­—å…¸æ¨å¯¼å¼å°†æ‰€æœ‰è¿™äº›ç¤ºä¾‹è¿æ¥åœ¨ä¸€èµ·ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")
```

```python out
'>>> Concatenated reviews length: 951'
```

å¾ˆæ£’ï¼Œæ€»é•¿åº¦è®¡ç®—å‡ºæ¥äº† â€”â€” ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è¿æ¥çš„è¯„è®ºæ‹†åˆ†ä¸ºå¤§å°ä¸º `chunk_size` çš„å—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿­ä»£äº† `concatenated_examples` ä¸­çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ä¸ºæ¯ä¸ªç‰¹å¾åˆ†å—ã€‚ç»“æœæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯ç‰¹å¾çš„åç§°ï¼Œå€¼æ˜¯å¯¹åº”å€¼ç»è¿‡åˆ†å—çš„åˆ—è¡¨ï¼š

```python
chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")
```

```python out
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

æ­£å¦‚ä½ åœ¨è¿™ä¸ªä¾‹å­ä¸­çœ‹åˆ°çš„ï¼Œæœ€åä¸€ä¸ªå—é€šå¸¸ä¼šå°äºæ‰€è®¾ç½®çš„åˆ†å—çš„å¤§å°ã€‚æœ‰ä¸¤ç§å¸¸è§çš„ç­–ç•¥æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼š

* å¦‚æœæœ€åä¸€ä¸ªå—å°äº `chunk_size` ï¼Œå°±ä¸¢å¼ƒã€‚
* å¡«å……æœ€åä¸€ä¸ªå—ï¼Œç›´åˆ°å…¶é•¿åº¦ç­‰äº `chunk_size` ã€‚

æˆ‘ä»¬å°†åœ¨è¿™é‡Œé‡‡ç”¨ç¬¬ä¸€ç§æ–¹æ³•ï¼Œæœ€åè®©æˆ‘ä»¬å°†ä¸Šè¿°æ‰€æœ‰é€»è¾‘åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„å·²åˆ†è¯æ•°æ®é›†ä¸Šï¼š

```python
def group_texts(examples):
    # æ‹¼æ¥æ‰€æœ‰çš„æ–‡æœ¬
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # è®¡ç®—æ‹¼æ¥æ–‡æœ¬çš„é•¿åº¦
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # å¦‚æœæœ€åä¸€ä¸ªå—å°äº chunk_size,æˆ‘ä»¬å°†å…¶ä¸¢å¼ƒ
    total_length = (total_length // chunk_size) * chunk_size
    # æŒ‰æœ€å¤§é•¿åº¦åˆ†å—
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ labels åˆ—
    result["labels"] = result["input_ids"].copy()
    return result
```

æ³¨æ„ï¼Œåœ¨ `group_texts()` çš„æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ `labels` åˆ—ï¼Œå®ƒæ˜¯é€šè¿‡å¤åˆ¶ `input_ids` åˆ—å½¢æˆçš„ã€‚è¿™æ˜¯å› ä¸ºåœ¨æ©ç è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹è¾“å…¥ä¸­éšæœºé®ä½(Masked)çš„ tokenï¼Œæˆ‘ä»¬ä¿å­˜äº†è®©æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹ä»ä¸­å­¦ä¹  `[Mask]` çš„ç­”æ¡ˆã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬å¼ºå¤§çš„ `Dataset.map()` å‡½æ•°å°† `group_texts()` åº”ç”¨åˆ°æˆ‘ä»¬çš„å·²åˆ†è¯æ•°æ®é›†ä¸Šï¼š

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})
```

é€šè¿‡å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ¯”åŸæ¥çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ 25000 ä¸ªä¾‹å­å¤šå¾—å¤šçš„è¯„è®ºæ•°æ®ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ç°åœ¨æœ‰äº†æ¶‰åŠè·¨è¶ŠåŸå§‹è¯­æ–™åº“ä¸­å¤šä¸ªä¾‹å­çš„è¿ç»­æ ‡è®°çš„ä¾‹å­ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨å…¶ä¸­ä¸€ä¸ªå—ä¸­æŸ¥æ‰¾ç‰¹æ®Šçš„ `[SEP]` å’Œ `[CLS]` tokens æ¥æ¸…æ™°åœ°çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªé‡å çš„ç”µå½±è¯„è®ºï¼Œä¸€ä¸ªå…³äºé«˜ä¸­ç”µå½±ï¼Œå¦ä¸€ä¸ªå…³äºæ— å®¶å¯å½’çš„é—®é¢˜ã€‚è®©æˆ‘ä»¬ä¹Ÿæ£€æŸ¥ä¸€ä¸‹æ©ç è¯­è¨€æ¨¡å‹å¾…é¢„æµ‹çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```python out
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

æ­£å¦‚æˆ‘ä»¬ä¸Šé¢çš„ `group_texts()` å‡½æ•°æ‰€é¢„æœŸçš„é‚£æ ·ï¼Œè¿™çœ‹èµ·æ¥ä¸è§£ç çš„ `input_ids` å®Œå…¨ç›¸åŒ â€”â€” ä½†æ˜¯è¦æ€ä¹ˆæ ·æ‰èƒ½è®©æˆ‘ä»¬çš„çš„æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸€äº›ä¸œè¥¿å‘¢ï¼Ÿæˆ‘ä»¬ç¼ºå°‘ä¸€ä¸ªå…³é”®çš„æ­¥éª¤ï¼šåœ¨è¾“å…¥ä¸­éšæœºæ’å…¥ `[MASK]` tokenï¼è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å¾®è°ƒæœŸé—´ä½¿ç”¨ç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨æ¥å®æ—¶åœ°å®Œæˆè¿™ä¸ªæ­¥éª¤ã€‚

## ä½¿ç”¨ `Trainer` API å¾®è°ƒ DistilBERT [[ä½¿ç”¨ `Trainer` API å¾®è°ƒ DistilBERT]]

å¾®è°ƒæ©ç è¯­è¨€æ¨¡å‹å‡ ä¹ä¸å¾®è°ƒåºåˆ—åˆ†ç±»æ¨¡å‹ç›¸åŒï¼Œå°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) æ‰€åšçš„é‚£æ ·ã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ï¼Œå®ƒå¯ä»¥éšæœºå±è”½æ¯æ‰¹æ–‡æœ¬ä¸­çš„ä¸€äº› tokensã€‚å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers ä¸ºè¿™é¡¹ä»»åŠ¡å‡†å¤‡äº†ä¸“ç”¨çš„ `DataCollatorForLanguageModeling` ã€‚æˆ‘ä»¬åªéœ€è¦å°† tokenizer å’Œä¸€ä¸ª `mlm_probability` å‚æ•°ï¼ˆæ©ç›– tokens çš„æ¯”ä¾‹ï¼‰ä¼ é€’ç»™å®ƒã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å°† `mlm_probability` å‚æ•°è®¾ç½®ä¸º 15%ï¼Œè¿™æ˜¯ `BERT` é»˜è®¤çš„æ•°é‡ï¼Œä¹Ÿæ˜¯æ–‡çŒ®ä¸­æœ€å¸¸è§çš„é€‰æ‹©ã€‚

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

ä¸ºäº†äº†è§£éšæœºæ©ç æ•°æ®æ•´ç†å™¨çš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬æŠŠä¸€äº›ä¾‹å­è¾“å…¥åˆ°æ•°æ®æ•´ç†å™¨ã€‚ç”±äºæ•°æ®æ•´ç†å™¨æœŸæœ›æ¥æ”¶ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸ä¸­å­˜å‚¨ä¸€æ®µè¿ç»­æ–‡æœ¬çš„å—ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆéå†æ•°æ®é›†å–å‡ºæ¥ä¸€äº›æ ·æœ¬æ•°æ®ï¼Œç„¶åå°†æ ·æœ¬æ•°æ®è¾“å…¥åˆ°æ•´ç†å™¨ã€‚åœ¨è¾“å…¥åˆ°æ•°æ®æ•´ç†å™¨ä¹‹é—´ï¼Œæˆ‘ä»¬åˆ é™¤äº† `word_ids` è¿™ä¸ªé”®ï¼Œå› ä¸ºå®ƒä¸éœ€è¦è¿™ä¸ªé”®ã€‚

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george å®‡in stated )å…¬ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

å¾ˆæ£’ï¼ŒæˆåŠŸäº†ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ `[MASK]` tokens å·²éšæœºæ’å…¥æˆ‘ä»¬æ–‡æœ¬ä¸­çš„ä¸åŒä½ç½®ã€‚è¿™äº›å°†æ˜¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å¿…é¡»é¢„æµ‹çš„ tokens  â€”â€” æ•°æ®æ•´ç†å™¨çš„ç¾å¦™ä¹‹å¤„åœ¨äºï¼Œå®ƒä¼šåœ¨æ¯ä¸ª batch ä¸­éšæœºæ’å…¥ `[MASK]` ï¼

<Tip>

âœï¸ **è¯•ä¸€è¯•ï¼** å¤šè¿è¡Œä¸Šé¢çš„ä»£ç ç‰‡æ®µå‡ æ¬¡ï¼Œäº²çœ¼çœ‹çœ‹éšæœºé®è”½çš„æ•ˆæœï¼ä¹Ÿå¯ä»¥ç”¨ `tokenizer.convert_ids_to_tokens()` æ›¿æ¢ `tokenizer.decode()` æ–¹æ³•ï¼Œçœ‹çœ‹åªæŠŠä¸€ä¸ªç»™å®šå•è¯çš„å•ä¸ª token é®è”½ï¼Œè€Œä¿æŒè¿™ä¸ªå•è¯å…¶ä»– tokens ä¸å˜çš„æ•ˆæœã€‚

</Tip>

{#if fw === 'pt'}

éšæœºæ©ç çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œå½“ä½¿ç”¨ `Trainer` æ—¶ï¼Œæ¯æ¬¡è®¡ç®—å‡ºæ¥çš„è¯„ä¼°ç»“æœä¼šæœ‰äº›è®¸ä¸åŒï¼Œå³ä½¿æˆ‘ä»¬ä¼šå¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ•´ç†å™¨ã€‚ç¨åï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ ä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡Œå¾®è°ƒæ—¶ï¼Œ å°±ä¼šçœ‹åˆ°å¦‚ä½•åˆ©ç”¨çµæ´»çš„è‡ªå®šä¹‰è¯„ä¼°å¾ªç¯çš„æ¥å†»ç»“éšæœºæ€§ã€‚
{/if}

åœ¨ä¸ºæ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä¸ä»…ä»…å¯ä»¥é®è”½å•ä¸ª `token`ï¼Œè¿˜å¯ä»¥ä¸€æ¬¡é®è”½æ•´ä¸ªå•è¯çš„æ‰€æœ‰ `token`ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºå…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰ï¼Œæˆ‘ä»¬å°±éœ€è¦è‡ªå·±æ„å»ºä¸€ä¸ªæ•°æ®æ•´ç†å™¨ã€‚æ•°æ®æ•´ç†å™¨çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ ·æœ¬åˆ—è¡¨å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºä¸€ä¸ª `batch`ï¼Œæ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬è¿™æ ·åšå§ï¼æˆ‘ä»¬å°†ä½¿ç”¨å…ˆå‰è®¡ç®—çš„`word ID`ï¼Œæ„å»ºä¸€ä¸ªå•è¯ç´¢å¼•å’Œç›¸åº” `token` ä¹‹é—´çš„æ˜ å°„ï¼Œç„¶åéšæœºå†³å®šé®è”½å“ªäº›å•è¯ï¼Œå¹¶ä½¿ç”¨è¿™ç§æ–¹æ³•å¯¹è¾“å…¥è¿›è¡Œé®è”½ã€‚è¯·æ³¨æ„ï¼Œé™¤äº†ä¸æ©ç å¯¹åº”çš„æ ‡ç­¾å¤–ï¼Œæ‰€æœ‰å…¶ä»–çš„æ ‡ç­¾å‡åº”è¯¥è®¾ç½®ä¸º `-100` ã€‚

{#if fw === 'pt'}

```py
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # åˆ›å»ºä¸€ä¸ªå•è¯ä¸å¯¹åº” token ç´¢å¼•ä¹‹é—´çš„æ˜ å°„
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # éšæœºé®è”½å•è¯
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return default_data_collator(features)
```

{:else}

```py
import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # åˆ›å»ºä¸€ä¸ªå•è¯ä¸å¯¹åº” token ç´¢å¼•ä¹‹é—´çš„æ˜ å°„
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # éšæœºé®è”½å•è¯
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return tf_default_data_collator(features)
```

{/if}

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¹‹å‰çš„æ ·æœ¬ä¸Šè¯•è¯•å®ƒï¼š

```py
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python out
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** å¤šæ¬¡è¿è¡Œä¸Šé¢çš„ä»£ç ç‰‡æ®µï¼Œäº²çœ¼çœ‹çœ‹éšæœºé®è”½çš„æ•ˆæœï¼ä¹Ÿå¯ä»¥å°† `tokenizer.decode()` æ–¹æ³•æ›¿æ¢ä¸º `tokenizer.convert_ids_to_tokens()` ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ç»™å®šå•è¯çš„æ‰€æœ‰ tokens æ€»æ˜¯è¢«ä¸€èµ·é®è”½ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸¤ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œå‰©ä¸‹çš„å¾®è°ƒæ­¥éª¤ä¸å…¶ä»–ä»»åŠ¡ç±»ä¼¼éƒ½æ˜¯æ ‡å‡†çš„ã€‚å¦‚æœä½ åœ¨ Google Colab ä¸Šè¿è¡Œå¹¶ä¸”æ²¡æœ‰å¹¸è¿åœ°åˆ†é…åˆ°ç¥ç§˜çš„ P100 GPUğŸ˜­ï¼Œé‚£ä¹ˆè®­ç»ƒå¯èƒ½ä¼šéœ€è¦ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆå°†è®­ç»ƒé›†çš„å¤§å°å‡å°åˆ°å‡ åƒä¸ªä¾‹å­ã€‚ä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥å¾—åˆ°ä¸€ä¸ªç›¸å½“ä¸é”™çš„è¯­è¨€æ¨¡å‹ï¼åœ¨ ğŸ¤— Datasets ä¸­å¿«é€Ÿç­›é€‰æ•°æ®é›†çš„æ–¹æ³•æ˜¯ä½¿ç”¨æˆ‘ä»¬åœ¨ [ç¬¬äº”ç« ](/course/chapter5) ä¸­çœ‹åˆ°çš„ `Dataset.train_test_split()` å‡½æ•°ï¼š

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})
```

è¿è¡Œä¸Šè¿°ä»£ç ä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„ `train` å’Œ `test` æ•°æ®é›†ï¼Œè®­ç»ƒé›†å¤§å°ä¸º 10,000 ä¸ªç¤ºä¾‹ï¼ŒéªŒè¯çš„å¤§å°æ˜¯è®­ç»ƒé›†çš„ 10ï¼… â€”â€” å¦‚æœä½ æœ‰ä¸€ä¸ªå¼ºå¤§çš„ GPUï¼Œå¯ä»¥è‡ªè¡Œå¢åŠ è¿™ä¸ªæ¯”ä¾‹ï¼æˆ‘ä»¬æ¥ä¸‹æ¥è¦åšçš„äº‹æƒ…æ˜¯ç™»å½• Hugging Face Hubã€‚å¦‚æœä½ åœ¨ Notebook ä¸­è¿è¡Œè¿™æ®µä»£ç ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹çš„å·¥å…·å‡½æ•°è¿›è¡Œç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

å®ƒå°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œåœ¨å…¶ä¸­ä½ å¯ä»¥è¾“å…¥ä½ çš„è´¦å·å’Œå¯†ç è¿›è¡Œç™»é™†ã€‚æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ä½ æœ€å–œæ¬¢çš„ç»ˆç«¯ä¸­è¾“å…¥æŒ‡ä»¤ï¼š

```
huggingface-cli login
```

ç„¶ååœ¨é‚£é‡Œç™»å½•ã€‚

{#if fw === 'tf'}

ç™»å½•åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„ `tf.data` æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `prepare_tf_dataset()` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šæ ¹æ®æ‰€ä½¿ç”¨çš„æ¨¡å‹è‡ªåŠ¨æ¨æ–­å“ªäº›åˆ—åº”è¯¥å­˜å…¥æ•°æ®é›†ã€‚å¦‚æœä½ æƒ³å‡†ç¡®æ§åˆ¶è¦ä½¿ç”¨çš„åˆ—ï¼Œå¯ä»¥æ”¹ç”¨ `Dataset.to_tf_dataset()` æ–¹æ³•ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåªä½¿ç”¨æ ‡å‡†æ•°æ®æ•´ç†å™¨ï¼Œä½†ä½ ä¹Ÿå¯ä»¥å°è¯•å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰æ•°æ®æ•´ç†å™¨ï¼Œå¹¶ä½œä¸ºä¸€ä¸ªç»ƒä¹ å¯¹æ¯”ä¸€ä¸‹æ•ˆæœï¼š

```python
tf_train_dataset = model.prepare_tf_dataset(
    downsampled_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = model.prepare_tf_dataset(
    downsampled_dataset["test"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```
æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨ğŸ¤— Transformers åº“çš„ `create_optimizer()` å‡½æ•°æ¥è®¾ç½®è¶…å‚æ•°å¹¶ç¼–è¯‘æ¨¡å‹ï¼Œè¯¥å‡½æ•°æä¾›äº†ä¸€ä¸ªå¸¦æœ‰çº¿æ€§å­¦ä¹ ç‡è¡°å‡çš„ `AdamW` ä¼˜åŒ–å™¨ã€‚åœ¨ `compile()` çš„å‚æ•°ä¸­æˆ‘ä»¬æ²¡æœ‰æŒ‡å®šæŸå¤±å‡½æ•°ï¼Œè¿™ä»£è¡¨ä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤æŸå¤±å‡½æ•°ï¼Œç„¶åæˆ‘ä»¬å°†è®­ç»ƒç²¾åº¦è®¾ç½®ä¸º`mixed_ï¬‚oat16` ã€‚æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Colab GPU æˆ–è€…å…¶ä»–ä¸æ”¯æŒ float16 åŠ é€Ÿçš„ GPUï¼Œä½ å¯èƒ½åº”è¯¥æ³¨é‡Šæ‰è¿™ä¸€è¡Œã€‚

å¦å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾ç½®äº†ä¸€ä¸ª `PushToHubCallback` ï¼Œå®ƒå°†åœ¨æ¯ä¸ª epoch åå°†æ¨¡å‹ä¿å­˜åˆ° Hubã€‚ä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€åˆ°çš„ä»“åº“çš„åç§°ï¼ˆå¦‚æœä½ æƒ³æŠŠå®ƒæ¨é€åˆ°ä¸€ä¸ªç»„ç»‡ï¼Œä½ å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œè¦å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) ï¼Œå°±éœ€è¦æ·»åŠ  `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ã€‚åœ¨é»˜è®¤çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œè¯¥ä»“åº“çš„åç§°æ˜¯ `lewtun/distilbert-finetuned-imdb` ã€‚

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ä½¿ç”¨ float16 ç²¾åº¦è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")

model_name = model_checkpoint.split("/")[-1]
callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)
```

æˆ‘ä»¬ç°åœ¨å·²ç»å‡†å¤‡å¥½è¿è¡Œ `model.fit()` äº† â€”â€” ä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç®€å•äº†è§£ä¸€ä¸‹ `å›°æƒ‘åº¦ï¼ˆperplexityï¼‰` ï¼Œå®ƒæ˜¯ä¸€ç§å¸¸ç”¨çš„è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚

{:else}

ç™»é™†åï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®š `Trainer` çš„å‚æ•°ï¼š

```python
from transformers import TrainingArguments

batch_size = 64
# åœ¨æ¯ä¸ª epoch è¾“å‡ºè®­ç»ƒçš„ loss
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è°ƒæ•´äº†ä¸€äº›é»˜è®¤é€‰é¡¹ï¼ŒåŒ…æ‹¬ `logging_steps` ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥è·Ÿè¸ªæ¯ä¸ª epoch çš„è®­ç»ƒæŸå¤±ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº† `fp16=True` æ¥å®ç°æ··åˆç²¾åº¦è®­ç»ƒï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜è®­ç»ƒé€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ `Trainer` å°†åˆ é™¤æ¨¡å‹çš„ `forward()` æ–¹æ³•ä¸­æœªä½¿ç”¨çš„åˆ—ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä½ ä½¿ç”¨å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰æ•°æ®æ•´ç†å™¨ï¼Œä½ è¿˜éœ€è¦è®¾ç½® `remove_unused_columns=False` ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šåœ¨è®­ç»ƒæœŸé—´ä¸¢å¤± `word_ids` åˆ—ã€‚

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€åˆ°çš„ä»“åº“çš„åç§°ï¼ˆå¦‚æœä½ æƒ³æŠŠå®ƒæ¨é€åˆ°ä¸€ä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œå°±åœ¨ `TrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­å¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œå®ƒå°†æ˜¯ `"lewtun/distilbert-finetuned-imdb"` ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æ‹¥æœ‰äº†åˆå§‹åŒ– `Trainer` æ‰€éœ€çš„æ‰€æœ‰è¦ç´ ã€‚è¿™é‡Œæˆ‘ä»¬åªä½¿ç”¨äº†æ ‡å‡†çš„ `data_collator` ï¼Œä½†ä½ å¯ä»¥å°è¯•ä½¿ç”¨å…¨è¯å±è”½ä½œä¸ºæ•°æ®æ•´ç†å™¨çš„ä¸€ä¸ªç»ƒä¹ ï¼Œå¹¶å¯¹æ¯”ä¸€ä¸‹ä¸åŒå±è”½æ–¹å¼çš„ç»“æœæœ‰ä»€ä¹ˆä¸åŒï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿è¡Œ `trainer.train()` â€”â€” ä½†åœ¨æ­¤ä¹‹å‰è®©æˆ‘ä»¬ç®€è¦åœ°çœ‹ä¸€ä¸‹ `å›°æƒ‘åº¦ï¼ˆperplexityï¼‰` ï¼Œè¿™æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½å¸¸ç”¨çš„æŒ‡æ ‡ã€‚

{/if}

### è¯­è¨€æ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ [[è¯­è¨€æ¨¡å‹çš„å›°æƒ‘åº¦(perplexity)]]

<Youtube id="NURcDHhYe98"/>

è¯­è¨€å»ºæ¨¡ä¸æ–‡æœ¬åˆ†ç±»æˆ–é—®ç­”ç­‰å…¶ä»–ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªå¸¦æ ‡ç­¾çš„è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œè€Œè¯­è¨€å»ºæ¨¡åˆ™æ²¡æœ‰ä»»ä½•æ˜ç¡®çš„æ ‡ç­¾ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•ç¡®å®šä»€ä¹ˆæ˜¯å¥½çš„è¯­è¨€æ¨¡å‹å‘¢ï¼Ÿå°±åƒæ‰‹æœºä¸­çš„è‡ªåŠ¨æ›´æ­£åŠŸèƒ½ä¸€æ ·ï¼Œä¸€ä¸ªå¥½çš„è¯­è¨€æ¨¡å‹ä¼šè¾ƒé«˜æ¦‚ç‡è¾“å‡ºä¸€ä¸ªè¯­æ³•æ­£ç¡®çš„å¥å­ï¼Œè¾ƒä½æ¦‚ç‡è¾“å‡ºæ— æ„ä¹‰çš„å¥å­ã€‚ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ›´ç›´è§‚æ„Ÿå—ï¼Œä½ å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ä¸€æ•´å¥—â€œè‡ªåŠ¨æ›´æ­£å¤±è´¥â€çš„ä¾‹å­ã€‚å…¶ä¸­ï¼Œäººä»¬æ‰‹æœºä¸­çš„æ¨¡å‹äº§ç”Ÿäº†ä¸€äº›ç›¸å½“æœ‰è¶£ï¼ˆå¹¶ä¸”å¸¸å¸¸ä¸å¦¥å½“ï¼‰çš„è‡ªåŠ¨ç”Ÿæˆçš„ç»“æœï¼

{#if fw === 'pt'}

å¦‚æœæµ‹è¯•é›†ä¸»è¦ç”±è¯­æ³•æ­£ç¡®çš„å¥å­ç»„æˆï¼Œé‚£ä¹ˆè¡¡é‡è¯­è¨€æ¨¡å‹è´¨é‡çš„ä¸€ç§æ–¹å¼å°±æ˜¯è®¡ç®—å®ƒç»™æµ‹è¯•é›†ä¸­æ‰€æœ‰å¥å­çš„ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ã€‚é«˜æ¦‚ç‡è¡¨ç¤ºæ¨¡å‹å¯¹æœªè§è¿‡çš„ä¾‹å­ä¸æ„Ÿåˆ°â€œæƒŠè®¶â€æˆ–â€œå›°æƒ‘â€ï¼Œè¿™è¡¨æ˜å®ƒå·²ç»å­¦ä¹ äº†è¯­è¨€çš„åŸºæœ¬è¯­æ³•æ¨¡å¼ã€‚å›°æƒ‘åº¦æœ‰å¾ˆå¤šç§æ•°å­¦å®šä¹‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çš„å®šä¹‰æ˜¯äº¤å‰ç†µæŸå¤±çš„æŒ‡æ•°ã€‚å…·ä½“æ–¹æ³•æ˜¯ä½¿ç”¨ `Trainer.evaluate()`æ–¹æ³•è®¡ç®—æµ‹è¯•é›†ä¸Šçš„äº¤å‰ç†µæŸå¤±ï¼Œå–ç»“æœçš„æŒ‡æ•°æ¥è®¡ç®—é¢„è®­ç»ƒæ¨¡å‹çš„å›°æƒ‘åº¦ã€‚

```python
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

å¦‚æœæµ‹è¯•é›†ä¸»è¦ç”±è¯­æ³•æ­£ç¡®çš„å¥å­ç»„æˆï¼Œé‚£ä¹ˆè¡¡é‡è¯­è¨€æ¨¡å‹è´¨é‡çš„ä¸€ç§æ–¹å¼å°±æ˜¯è®¡ç®—å®ƒç»™æµ‹è¯•é›†ä¸­æ‰€æœ‰å¥å­çš„ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ã€‚é«˜æ¦‚ç‡è¡¨ç¤ºæ¨¡å‹å¯¹æœªè§è¿‡çš„ä¾‹å­ä¸æ„Ÿåˆ°â€œæƒŠè®¶â€æˆ–â€œå›°æƒ‘â€ï¼Œè¿™è¡¨æ˜å®ƒå·²ç»å­¦ä¹ äº†è¯­è¨€çš„åŸºæœ¬è¯­æ³•æ¨¡å¼ã€‚å›°æƒ‘åº¦æœ‰å¾ˆå¤šç§æ•°å­¦å®šä¹‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çš„å®šä¹‰æ˜¯äº¤å‰ç†µæŸå¤±çš„æŒ‡æ•°ã€‚å…·ä½“æ–¹æ³•æ˜¯ä½¿ç”¨ `model.evaluate()` æ–¹æ³•è®¡ç®—æµ‹è¯•é›†ä¸Šçš„äº¤å‰ç†µæŸå¤±ï¼Œå–ç»“æœçš„æŒ‡æ•°æ¥è®¡ç®—é¢„è®­ç»ƒæ¨¡å‹çš„å›°æƒ‘åº¦ã€‚

```python
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 21.75
```

è¾ƒä½çš„å›°æƒ‘åº¦åˆ†æ•°æ„å‘³ç€æ›´å¥½çš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„åˆå§‹æ¨¡å‹çš„å›°æƒ‘åº¦ç›¸å½“åœ°é«˜ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥é€šè¿‡å¾®è°ƒæ¥é™ä½å®ƒï¼ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè¿è¡Œè®­ç»ƒå¾ªç¯ï¼š

{#if fw === 'pt'}

```python
trainer.train()
```

{:else}

```python
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

ç„¶ååƒä¹‹å‰é‚£æ ·è®¡ç®—æµ‹è¯•é›†ä¸Šçš„ç»“æœå›°æƒ‘åº¦ï¼š

{#if fw === 'pt'}

```python
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

```python
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 11.32
```

å¤ªæ£’äº†â€”â€”å›°æƒ‘åº¦æ˜¾è‘—é™ä½ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬æ¨¡å‹å·²ç»å­¦ä¹ åˆ°äº†ç”µå½±è¯„è®ºé¢†åŸŸçš„ä¸€äº›çŸ¥è¯†ï¼

{#if fw === 'pt'}

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å°†å¸¦æœ‰è®­ç»ƒä¿¡æ¯çš„æ¨¡å‹å¡ç‰‡æ¨é€åˆ° Hubï¼ˆcheckpoint åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°±å·²ç»ä¿å­˜äº†ï¼‰ï¼š

```python
trainer.push_to_hub()
```

{/if}

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** å°†æ•°æ®æ•´ç†å™¨æ”¹ä¸ºå…¨è¯å±è”½çš„æ•°æ®æ•´ç†å™¨åè¿è¡Œä¸Šé¢çš„è®­ç»ƒã€‚ä½ èƒ½å¾—åˆ°æ›´å¥½çš„ç»“æœå—ï¼Ÿ

</Tip>

{#if fw === 'pt'} 

åœ¨æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¯¹è®­ç»ƒå¾ªç¯åšä»»ä½•ç‰¹æ®Šçš„å¤„ç†ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦å®ç°ä¸€äº›è‡ªå®šä¹‰é€»è¾‘ã€‚å¯¹äºè¿™äº›åº”ç”¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ ğŸ¤— Accelerate â€”â€” è®©æˆ‘ä»¬çœ‹ä¸€çœ‹ï¼

## ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒ DistilBERT [[ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒ DistilBERT]]

ä»ä¸Šé¢çš„ `Trainer` è®­ç»ƒæµç¨‹ä¸­æˆ‘ä»¬å°±èƒ½å‘ç°ï¼Œå¾®è°ƒä¸€ä¸ªæ©ç çš„è¯­è¨€æ¨¡å‹ä¸ [ç¬¬ä¸‰ç« ](https://chat.openai.com/course/chapter3) ä¸­çš„æ–‡æœ¬åˆ†ç±»éå¸¸ç›¸ä¼¼ã€‚äº‹å®ä¸Šï¼Œå”¯ä¸€çš„ä¸åŒä¹‹å¤„æ˜¯ä½¿ç”¨äº†ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ï¼Œæˆ‘ä»¬å·²ç»åœ¨æœ¬èŠ‚çš„å‰é¢è®¨è®ºè¿‡è¿™ä¸ªé—®é¢˜äº†ï¼
ç„¶è€Œï¼Œæˆ‘ä»¬æ³¨æ„åˆ° `DataCollatorForLanguageModeling` åœ¨æ¯æ¬¡è¯„ä¼°æ—¶ä¹Ÿä¼šè¿›è¡Œéšæœºé®ç½©ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ¯æ¬¡è®­ç»ƒè¿è¡Œä¸­éƒ½ä¼šçœ‹åˆ°å›°æƒ‘åº¦å¾—åˆ†æœ‰äº›æ³¢åŠ¨ã€‚æ¶ˆé™¤è¿™ç§éšæœºæ€§çš„ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Š `ä»…è¿›è¡Œä¸€æ¬¡` é®ç½©ï¼Œç„¶ååœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä½¿ç”¨ğŸ¤— Transformers ä¸­çš„é»˜è®¤æ•°æ®æ•´ç†å™¨æ¥æ”¶é›† batchã€‚ä¸ºå®ç°è¿™ä¸ªè¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œç±»ä¼¼äºæˆ‘ä»¬ç¬¬ä¸€æ¬¡ä½¿ç”¨ `DataCollatorForLanguageModeling` æ—¶è¿›è¡Œé®ç½©çš„æ–¹å¼ï¼š

```python
def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸€åˆ—åˆ›å»ºä¸€ä¸ªæ–°çš„"masked"åˆ—
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸Šè¿°å‡½æ•°åº”ç”¨åˆ°æµ‹è¯•é›†ï¼Œå¹¶åˆ é™¤æœªè¿›è¡Œé®ç½©çš„åˆ—ï¼Œè¿™æ ·å°±å®ç°äº†ä½¿ç”¨é®ç½©è¿‡çš„æ•°æ®æ›¿æ¢åŸå§‹è¾“å…¥çš„æ•°æ®ã€‚ä½ å¯ä»¥é€šè¿‡å°†ä¸Šè¿° `data_collator` æ›¿æ¢ä¸ºæ”¯æŒå…¨è¯é®ç½©çš„æ•°æ®æ•´ç†å™¨å¹¶ä¸”åˆ é™¤ä¸‹é¢çš„ç¬¬ä¸€è¡Œï¼ˆå…¨è¯é®ç½©çš„æ•°æ®æ•´ç†å™¨éœ€è¦ `word_ids` åˆ—æ¥å®šä½åŒä¸€ä¸ªå•è¯ä¸­çš„ä¸åŒ `token`ï¼‰æ¥å®ç°å…¨è¯é®ç½©

```py
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)
```

ç„¶åæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®¾ç½® DataLoaderï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨ğŸ¤— Transformers ä¸­çš„ `default_data_collator` æ¥è®¾ç½® DataLoaderï¼š

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)
```

ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å°†éµå¾ªğŸ¤— Accelerate çš„æ ‡å‡†æ­¥éª¤ã€‚ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯é‡æ–°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š

```
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ç„¶åæˆ‘ä»¬éœ€è¦æŒ‡å®šä¼˜åŒ–å™¨ï¼›æˆ‘ä»¬å°†ä½¿ç”¨æ ‡å‡†çš„ `AdamW` ï¼š

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

æœ‰äº†è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç”¨ `Accelerator` å¯¹è±¡åŒ…è£…æ‰€æœ‰çš„ç»„ä»¶ï¼Œä»¥è¿›è¡Œè®­ç»ƒï¼š

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œ DataLoader éƒ½é…ç½®å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š

```python
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åšçš„æœ€åä¸€ä»¶äº‹å°±æ˜¯åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Hub åº“çš„ `get_full_repo_name`ï¼Œç”Ÿæˆæˆ‘ä»¬ä»“åº“çš„å…¨åï¼š

```python
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Hub çš„ `Repository` ç±»åˆ›å»ºå¹¶å…‹éš†ä»“åº“ï¼š

```python
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)
```

å®Œæˆåï¼Œåªéœ€å†™å‡ºå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯å³å¯ï¼š

```python
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # ä¿å­˜å¹¶ä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409
```

å¾ˆæ£’ï¼Œæˆ‘ä»¬å·²ç»èƒ½å¤Ÿè¯„ä¼°æ¯ä¸ª epoch çš„å›°æƒ‘åº¦ï¼Œå¹¶ç¡®ä¿è¿è¡Œçš„ç»“æœå¯ä»¥å¤ç°ï¼

{/if}

## ä½¿ç”¨æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ [[ä½¿ç”¨æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹]]

ä½ å¯ä»¥ä½¿ç”¨ Hub ä¸Šçš„æ¨¡å‹éƒ¨ä»¶æˆ–è€…åœ¨æœ¬åœ°ä½¿ç”¨ğŸ¤— Transformers çš„ `pipeline` åŠ è½½å¾®è°ƒæ¨¡å‹é¢„æµ‹æ–‡æœ¬ã€‚è®©æˆ‘ä»¬ä½¿ç”¨åè€…é€šè¿‡ `fill-mask` pipeline ä¸‹è½½æˆ‘ä»¬çš„æ¨¡å‹ï¼š

```python
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†æ–‡æœ¬â€œThis is a great [MASK]â€æä¾›ç»™ pipelineï¼Œçœ‹çœ‹å‰ 5 ä¸ªé¢„æµ‹æ˜¯ä»€ä¹ˆï¼š

```python
preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")
```

```python out
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

Niceï¼â€”â€” æˆ‘ä»¬çš„æ¨¡å‹æ˜¾ç„¶å·²ç»è°ƒæ•´äº†å®ƒçš„æƒé‡æ¥é¢„æµ‹ä¸ç”µå½±æ›´å¯†åˆ‡ç›¸å…³çš„è¯ï¼

<Youtube id="0Oxphw4Q9fo"/>

è¿™æ ‡å¿—ç€æˆ‘ä»¬ç¬¬ä¸€æ¬¡è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å®éªŒåˆ°ç°åœ¨å°±ç»“æŸäº†ã€‚åœ¨ [ç¬¬ 6 èŠ‚](https://chat.openai.com/course/en/chapter7/6) ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè‡ªåŠ¨å›å½’æ¨¡å‹ï¼Œæ¯”å¦‚ GPT-2ï¼›å¦‚æœä½ æƒ³çœ‹çœ‹å¦‚ä½•é¢„è®­ç»ƒä½ è‡ªå·±çš„ Transformer æ¨¡å‹ï¼Œå°±èµ¶å¿«å»é‚£é‡Œçœ‹çœ‹å§ï¼

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** ä¸ºäº†é‡åŒ–é¢†åŸŸé€‚åº”çš„å¥½å¤„ï¼Œåˆ†åˆ«ä½¿ç”¨é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ DistilBERT checkpoint ä»¥åŠæ•°æ®é›†è‡ªå¸¦çš„ IMDb æ ‡ç­¾æ¥å¾®è°ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå¹¶å¯¹æ¯”ä¸€ä¸‹è¿™ä¸ªä¸¤ä¸ª checkpoint çš„å·®å¼‚ã€‚å¦‚æœä½ éœ€è¦å¤ä¹ æ–‡æœ¬åˆ†ç±»çš„çŸ¥è¯†ï¼Œè¯·æŸ¥çœ‹ [ç¬¬ä¸‰ç« ](/course/chapter3) ã€‚
</Tip>
