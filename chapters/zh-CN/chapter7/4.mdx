<FrameworkSwitchCourse {fw} />

# ç¿»è¯‘

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
]} />

{/if}

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ç¿»è¯‘ã€‚è¿™æ˜¯å¦ä¸€ä¸ª[sequence-to-sequence ä»»åŠ¡](/course/chapter1/7)ï¼Œè¿™æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ªå¯ä»¥è¡¨è¿°ä¸ºä»ä¸€ä¸ªåºåˆ—åˆ°å¦ä¸€ä¸ªåºåˆ—çš„é—®é¢˜ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œè¿™ä¸ªé—®é¢˜éå¸¸ç±»ä¼¼[æ–‡æœ¬æ‘˜è¦](/course/chapter7/6)ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥å°†æˆ‘ä»¬å°†åœ¨æ­¤å¤„å­¦ä¹ åˆ°çš„ä¸€äº›å†…å®¹è¿ç§»åˆ°å…¶ä»–çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **é£æ ¼è¿ç§»** : åˆ›å»ºä¸€ä¸ªæ¨¡å‹å°†æŸç§é£æ ¼è¿ç§»åˆ°ä¸€æ®µæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ­£å¼çš„é£æ ¼è¿ç§»åˆ°ä¼‘é—²çš„é£æ ¼æˆ–èå£«æ¯”äºšè‹±è¯­åˆ°ç°ä»£è‹±è¯­ï¼‰
- **ç”Ÿæˆé—®é¢˜çš„å›ç­”** ï¼šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆ

<Youtube id="1JvfrvZgi6c"/>

å¦‚æœæ‚¨æœ‰è¶³å¤Ÿå¤§çš„ä¸¤ç§ï¼ˆæˆ–æ›´å¤šï¼‰è¯­è¨€çš„æ–‡æœ¬è¯­æ–™åº“ï¼Œæ‚¨å¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°çš„ç¿»è¯‘æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨[å› æœè¯­è¨€å»ºæ¨¡](/course/chapter7/6)éƒ¨åˆ†ä¸­æ‰€åšçš„é‚£æ ·ã€‚ç„¶è€Œï¼Œå¾®è°ƒç°æœ‰çš„ç¿»è¯‘æ¨¡å‹ä¼šæ›´å¿«ï¼Œæ— è®ºæ˜¯ä»åƒ mT5 æˆ– mBART è¿™æ ·çš„å¤šè¯­è¨€æ¨¡å‹å¾®è°ƒåˆ°ç‰¹å®šçš„è¯­è¨€å¯¹ï¼Œæˆ–è€…æ˜¯ä¸“é—¨ç”¨äºä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ¨¡å‹ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹[KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4)ä¸Šçš„Marianæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹ç»è¿‡äº†ä»è‹±è¯­åˆ°æ³•è¯­çš„ç¿»è¯‘é¢„è®­ç»ƒ(å› ä¸ºå¾ˆå¤šâ€œ Hugging Faceâ€çš„å‘˜å·¥ä¼šè¯´è¿™ä¸¤ç§è¯­è¨€)ï¼Œå®ƒæ˜¯[KDEåº”ç”¨ç¨‹åº](https://apps.kde.org/)æœ¬åœ°åŒ–æ–‡ä»¶çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ¨¡å‹å·²ç»åœ¨ä»[Opus æ•°æ®é›†](https://opus.nlpl.eu/)(å®é™…ä¸ŠåŒ…å«KDE4æ•°æ®é›†)ä¸­æå–çš„æ³•è¯­å’Œè‹±è¯­æ–‡æœ¬çš„å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„å…ˆè®­ç»ƒã€‚ä½†æ˜¯ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å…¶é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨äº†è¿™éƒ¨åˆ†æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°ï¼Œç»è¿‡å¾®è°ƒåï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç‰ˆæœ¬ã€‚

å®Œæˆåï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè¿™æ ·çš„ç¿»è¯‘ï¼š

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr-darkmode.hf.space" frameBorder="0" height="350" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ä¸å‰é¢çš„éƒ¨åˆ†ä¸€æ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„å®é™…æ¨¡å‹ï¼Œå¹¶[åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.)æŸ¥çœ‹æ¨¡å‹è¾“å‡ºçš„ç»“æœ

## å‡†å¤‡æ•°æ®

ä¸ºäº†ä»å¤´å¼€å§‹å¾®è°ƒæˆ–è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ•°æ®é›†ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4)åœ¨æœ¬èŠ‚ä¸­ï¼Œä½†æ‚¨å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´ä»£ç ä»¥ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®ï¼Œåªè¦æ‚¨æœ‰è¦äº’è¯‘çš„ä¸¤ç§è¯­è¨€çš„å¥å­å¯¹ã€‚å¦‚æœæ‚¨éœ€è¦å¤ä¹ å¦‚ä½•å°†è‡ªå®šä¹‰æ•°æ®åŠ è½½åˆ° **Dataset** å¯ä»¥å¤ä¹ ä¸€ä¸‹[ç¬¬äº”ç« ](/course/chapter5).

### KDE4 æ•°æ®é›†

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ **load_dataset()** å‡½æ•°ï¼š

```py
from datasets import load_dataset, load_metric

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒçš„è¯­è¨€å¯¹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒä»¬çš„ä»£ç æ¥æŒ‡å®šå®ƒä»¬ã€‚è¯¥æ•°æ®é›†å…±æœ‰ 92 ç§è¯­è¨€å¯ç”¨ï¼›æ‚¨å¯ä»¥é€šè¿‡[æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/kde4)å±•å¼€å…¶ä¸Šçš„è¯­è¨€æ ‡ç­¾æ¥æŸ¥çœ‹å®ƒä»¬.

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

æˆ‘ä»¬æ¥çœ‹çœ‹æ•°æ®é›†ï¼š

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

æˆ‘ä»¬æœ‰ 210,173 å¯¹å¥å­ï¼Œä½†åœ¨ä¸€æ¬¡è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºè‡ªå·±çš„éªŒè¯é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬äº”ç« ](/course/chapter5)å­¦çš„çš„é‚£æ ·, **Dataset** æœ‰ä¸€ä¸ª **train_test_split()** æ–¹æ³•,å¯ä»¥å¸®æˆ‘ä»¬æ‹†åˆ†æ•°æ®é›†ã€‚æˆ‘ä»¬å°†è®¾å®šå›ºå®šçš„éšæœºæ•°ç§å­ï¼š

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

æˆ‘ä»¬å¯ä»¥å°† **test** çš„é”®é‡å‘½åä¸º **validation** åƒè¿™æ ·ï¼š

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼š

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬è¯·æ±‚çš„ä¸¤ç§è¯­è¨€çš„ä¸¤ä¸ªå¥å­ã€‚è¿™ä¸ªå……æ»¡æŠ€æœ¯è®¡ç®—æœºç§‘å­¦æœ¯è¯­çš„æ•°æ®é›†çš„ä¸€ä¸ªç‰¹æ®Šä¹‹å¤„åœ¨äºå®ƒä»¬éƒ½å®Œå…¨ç”¨æ³•è¯­ç¿»è¯‘ã€‚ç„¶è€Œï¼Œæ³•å›½å·¥ç¨‹å¸ˆé€šå¸¸å¾ˆæ‡’æƒ°ï¼Œåœ¨äº¤è°ˆæ—¶ï¼Œå¤§å¤šæ•°è®¡ç®—æœºç§‘å­¦ä¸“ç”¨è¯æ±‡éƒ½ç”¨è‹±è¯­è¡¨è¿°ã€‚ä¾‹å¦‚ï¼Œâ€œthreadsâ€è¿™ä¸ªè¯å¾ˆå¯èƒ½å‡ºç°åœ¨æ³•è¯­å¥å­ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æŠ€æœ¯å¯¹è¯ä¸­ï¼›ä½†åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œå®ƒè¢«ç¿»è¯‘æˆæ›´æ­£ç¡®çš„â€œfils de Discussionâ€ã€‚æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ä¸€ä¸ªæ›´å¤§çš„æ³•è¯­å’Œè‹±è¯­å¥å­è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œé‡‡ç”¨äº†æ›´ç®€å•çš„é€‰æ‹©ï¼Œå³ä¿ç•™å•è¯çš„åŸæ ·:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

è¿™ç§æƒ…å†µçš„å¦ä¸€ä¸ªä¾‹å­æ˜¯â€œpluginâ€è¿™ä¸ªè¯ï¼Œå®ƒä¸æ˜¯æ­£å¼çš„æ³•è¯­è¯ï¼Œä½†å¤§å¤šæ•°æ¯è¯­äººå£«éƒ½èƒ½ç†è§£ï¼Œä¹Ÿä¸ä¼šè´¹å¿ƒå»ç¿»è¯‘ã€‚
åœ¨ KDE4 æ•°æ®é›†ä¸­ï¼Œè¿™ä¸ªè¯åœ¨æ³•è¯­ä¸­è¢«ç¿»è¯‘æˆæ›´æ­£å¼çš„â€œmodule dâ€™extensionâ€ï¼š
```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ç„¶è€Œï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹åšæŒä½¿ç”¨ç®€ç»ƒè€Œç†Ÿæ‚‰çš„è‹±æ–‡å•è¯ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

çœ‹çœ‹æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ˜¯å¦èƒ½è¯†åˆ«æ•°æ®é›†çš„è¿™äº›ç‰¹æ®Šæ€§ã€‚ï¼ˆå‰§é€è­¦å‘Šï¼šå®ƒä¼šï¼‰ã€‚

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** å¦ä¸€ä¸ªåœ¨æ³•è¯­ä¸­ç»å¸¸ä½¿ç”¨çš„è‹±è¯­å•è¯æ˜¯â€œemailâ€ã€‚åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰¾åˆ°ä½¿ç”¨è¿™ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ã€‚å®ƒæ˜¯å¦‚ä½•ç¿»è¯‘çš„ï¼Ÿé¢„è®­ç»ƒæ¨¡å‹å¦‚ä½•ç¿»è¯‘åŒä¸€ä¸ªè‹±æ–‡å¥å­ï¼Ÿ

</Tip>

### å¤„ç†æ•°æ®

<Youtube id="XAR8jnZZuUs"/>

æ‚¨ç°åœ¨åº”è¯¥çŸ¥é“æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥è¯¥åšäº›ä»€ä¹ˆäº†ï¼šæ‰€æœ‰æ–‡æœ¬éƒ½éœ€è¦è½¬æ¢ä¸ºtoken IDï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿç†è§£å®ƒä»¬ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶æ ‡è®°è¾“å…¥å’Œç›®æ ‡ã€‚æˆ‘ä»¬çš„é¦–è¦ä»»åŠ¡æ˜¯åˆ›å»ºæˆ‘ä»¬çš„ **tokenizer** å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Marian è‹±è¯­åˆ°æ³•è¯­çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœæ‚¨ä½¿ç”¨å¦ä¸€å¯¹è¯­è¨€å°è¯•æ­¤ä»£ç ï¼Œè¯·ç¡®ä¿è°ƒæ•´æ¨¡å‹Checkpointã€‚[Helsinki-NLP](https://huggingface.co/Helsinki-NLP)ç»„ç»‡æä¾›äº†å¤šç§è¯­è¨€çš„ä¸€åƒå¤šç§æ¨¡å‹ã€‚

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="tf")
```

æ‚¨å¯ä»¥å°† **model_checkpoint** æ›´æ¢ä¸º[Hub](https://huggingface.co/models)ä¸Šä½ å–œæ¬¢çš„ä»»ä½•å…¶ä»–å‹å·ï¼Œæˆ–æœ¬åœ°ä¿å­˜çš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ ‡è®°å™¨ã€‚

<Tip>

ğŸ’¡ å¦‚æœæ­£åœ¨ä½¿ç”¨martã€mBART-50æˆ–M2 M100ç­‰å¤šè¯­è¨€æ ‡è®°å™¨ï¼Œåˆ™éœ€è¦åœ¨tokenizerä¸­è®¾ç½®tokenizer.src_langå’Œtokenizer.tgt_langä¸ºæ­£ç¡®çš„è¾“å…¥å’Œç›®æ ‡çš„è¯­è¨€ä»£ç ã€‚

</Tip>

æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡éå¸¸ç®€å•ã€‚ åªéœ€è¦è®°ä½ä¸€ä»¶äº‹ï¼šæ‚¨ç…§å¸¸å¤„ç†è¾“å…¥ï¼Œä½†å¯¹äºè¿™æ¬¡çš„è¾“å‡ºç›®æ ‡ï¼Œæ‚¨éœ€è¦å°†æ ‡è®°å™¨åŒ…è£…åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨â€œas_target_tokenizer()â€ä¸­ã€‚

Python ä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¼•å…¥äº† **with** è¯­å¥ï¼Œå½“æ‚¨æœ‰ä¸¤ä¸ªç›¸å…³çš„æ“ä½œè¦æˆå¯¹æ‰§è¡Œæ—¶å¾ˆæœ‰ç”¨ã€‚æœ€å¸¸è§çš„ä¾‹å­æ˜¯å½“æ‚¨å†™å…¥æˆ–è¯»å–æ–‡ä»¶æ—¶ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```
with open(file_path) as f:
    content = f.read()
```

è¿™é‡Œæˆå¯¹æ‰§è¡Œçš„ä¸¤ä¸ªç›¸å…³æ“ä½œæ˜¯æ‰“å¼€å’Œå…³é—­æ–‡ä»¶çš„æ“ä½œã€‚æ‰“å¼€çš„æ–‡ä»¶få¯¹åº”çš„å¯¹è±¡åªåœ¨withä¸‹çš„ç¼©è¿›å—å†…æœ‰æ•ˆï¼›åœ¨è¯¥å—ä¹‹å‰æ‰“å¼€ï¼Œåœ¨è¯¥å—çš„æœ«å°¾å…³é—­ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ as_target_tokenizer() å°†åœ¨æ‰§è¡Œç¼©è¿›å—ä¹‹å‰å°†æ ‡è®°å™¨è®¾ç½®ä¸ºè¾“å‡ºè¯­è¨€ï¼ˆæ­¤å¤„ä¸ºæ³•è¯­ï¼‰ï¼Œç„¶åå°†å…¶è®¾ç½®å›è¾“å…¥è¯­è¨€ï¼ˆæ­¤å¤„ä¸ºè‹±è¯­ï¼‰ã€‚

å› æ­¤ï¼Œé¢„å¤„ç†ä¸€ä¸ªæ ·æœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence)
with tokenizer.as_target_tokenizer():
    targets = tokenizer(fr_sentence)
```

å¦‚æœæˆ‘ä»¬å¿˜è®°åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­æ ‡è®°ç›®æ ‡ï¼Œå®ƒä»¬å°†è¢«è¾“å…¥æ ‡è®°å™¨æ ‡è®°ï¼Œåœ¨Marianæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¼šå¯¼è‡´è¾“å‡ºçš„å¼‚å¸¸ï¼š

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(targets["input_ids"]))
```

```python out
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œä½¿ç”¨è‹±è¯­æ ‡è®°å™¨æ¥é¢„å¤„ç†æ³•è¯­å¥å­ä¼šäº§ç”Ÿæ›´å¤šçš„æ ‡è®°ï¼Œå› ä¸ºæ ‡è®°å™¨ä¸çŸ¥é“ä»»ä½•æ³•è¯­å•è¯(é™¤äº†é‚£äº›ä¹Ÿå‡ºç°åœ¨è‹±è¯­è¯­è¨€ä¸­çš„å•è¯ï¼Œæ¯”å¦‚â€œdiscussionâ€)ã€‚

`inputs` å’Œ `targets` éƒ½æ˜¯å¸¦æœ‰æˆ‘ä»¬å¸¸ç”¨é”®ï¼ˆè¾“å…¥ IDã€æ³¨æ„æ©ç ç­‰ï¼‰çš„å­—å…¸ï¼Œæ‰€ä»¥æœ€åä¸€æ­¥æ˜¯åœ¨è¾“å…¥ä¸­è®¾ç½®ä¸€ä¸ª `"labels"` é”®ã€‚ æˆ‘ä»¬åœ¨æ•°æ®é›†çš„é¢„å¤„ç†å‡½æ•°ä¸­æ‰§è¡Œæ­¤æ“ä½œï¼š

```python
max_input_length = 128
max_target_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Set up the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ºè¾“å…¥å’Œè¾“å‡ºè®¾ç½®äº†ç›¸åŒçš„æœ€å¤§é•¿åº¦ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ–‡æœ¬çœ‹èµ·æ¥å¾ˆçŸ­ï¼Œæˆ‘ä»¬ä½¿ç”¨ 128ã€‚

<Tip>

ğŸ’¡å¦‚æœä½ ä½¿ç”¨çš„æ˜¯T5æ¨¡å‹(æ›´å…·ä½“åœ°è¯´ï¼Œæ˜¯T5 -xxxæ£€æŸ¥ç‚¹ä¹‹ä¸€)ï¼Œæ¨¡å‹å°†éœ€è¦æ–‡æœ¬è¾“å…¥æœ‰ä¸€ä¸ªå‰ç¼€æ¥è¡¨ç¤ºæ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä»è‹±è¯­åˆ°æ³•è¯­çš„ç¿»è¯‘

</Tip>

<Tip warning={true}>

âš ï¸ æˆ‘ä»¬ä¸å…³æ³¨ç›®æ ‡çš„æ³¨æ„åŠ›æ©ç ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šéœ€è¦å®ƒã€‚ç›¸åï¼Œå¯¹åº”äºå¡«å……æ ‡è®°çš„æ ‡ç­¾åº”è®¾ç½®ä¸º-100ï¼Œä»¥ä¾¿åœ¨lossè®¡ç®—ä¸­å¿½ç•¥å®ƒä»¬ã€‚è¿™å°†åœ¨ç¨åç”±æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å®Œæˆï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åº”ç”¨åŠ¨æ€å¡«å……ï¼Œä½†æ˜¯å¦‚æœæ‚¨åœ¨æ­¤å¤„ä½¿ç”¨å¡«å……ï¼Œæ‚¨åº”è¯¥è°ƒæ•´é¢„å¤„ç†å‡½æ•°ä»¥å°†ä¸å¡«å……æ ‡è®°å¯¹åº”çš„æ‰€æœ‰æ ‡ç­¾è®¾ç½®ä¸º -100ã€‚

</Tip>

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§åº”ç”¨è¯¥é¢„å¤„ç†ï¼š

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ç°åœ¨æ•°æ®å·²ç»è¿‡é¢„å¤„ç†ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼

{#if fw === 'pt'}

## ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹

ä½¿ç”¨ `Trainer` çš„å®é™…ä»£ç å°†ä¸ä»¥å‰ç›¸åŒï¼Œåªæ˜¯ç¨ä½œæ”¹åŠ¨ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer)ï¼Œ å®ƒæ˜¯ `Trainer` çš„å­ç±»ï¼Œå®ƒå¯ä»¥æ­£ç¡®å¤„ç†è¿™ç§åºåˆ—åˆ°åºåˆ—çš„è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨ `generate()` æ–¹æ³•æ¥é¢„æµ‹è¾“å…¥çš„è¾“å‡ºã€‚ å½“æˆ‘ä»¬è®¨è®ºè¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸€ç‚¹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®é™…çš„æ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚ æˆ‘ä»¬å°†ä½¿ç”¨é€šå¸¸çš„ `AutoModel` APIï¼š

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®é™…çš„æ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚ æˆ‘ä»¬å°†ä½¿ç”¨é€šå¸¸çš„ `AutoModel` APIï¼š

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

ğŸ’¡ `Helsinki-NLP/opus-mt-en-fr` checkpointåªæœ‰ PyTorch çš„æƒé‡ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨`from_pretrained()`æ–¹æ³•åŠ è½½æ¨¡å‹æ—¶ä¸ä¼šä½¿ç”¨ `from_pt=True` å‚æ•°ã€‚ å½“æ‚¨æŒ‡å®š`from_pt=True`ï¼Œåº“ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶è½¬æ¢PyTorch ä¸ºæ‚¨æä¾›æƒé‡ã€‚ å¦‚æ‚¨æ‰€è§ï¼Œä½¿ç”¨ğŸ¤—transormer åœ¨ä¸¤è€…ä¹‹é—´åˆ‡æ¢éå¸¸ç®€å•

</Tip>

{/if}

è¯·æ³¨æ„ï¼Œè¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè®­ç»ƒè¿‡çš„æ¨¡å‹ï¼Œå¹¶ä¸”å®é™…ä¸Šå·²ç»å¯ä»¥ä½¿ç”¨ï¼Œå› æ­¤æ²¡æœ‰å…³äºä¸¢å¤±æƒé‡æˆ–æ–°åˆå§‹åŒ–æƒé‡çš„è­¦å‘Šã€‚

### æ•°æ®æ•´ç†

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ•´ç†å™¨æ¥å¤„ç†åŠ¨æ€æ‰¹å¤„ç†çš„å¡«å……ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸èƒ½åƒ[ç¬¬3ç« ](/course/chapter3)é‚£æ ·ä½¿ç”¨å¸¦å¡«å……çš„**DataCollatorWithPadding**ï¼Œå› ä¸ºå®ƒåªå¡«å……è¾“å…¥ï¼ˆè¾“å…¥IDã€æ³¨æ„æ©ç å’Œä»¤ç‰Œç±»å‹IDï¼‰ã€‚æˆ‘ä»¬çš„æ ‡ç­¾ä¹Ÿåº”è¯¥å¡«å……åˆ°æ ‡ç­¾ä¸­é‡åˆ°çš„æœ€å¤§é•¿åº¦ã€‚è€Œä¸”ï¼Œå¦‚å‰æ‰€è¿°ï¼Œç”¨äºå¡«å……æ ‡ç­¾çš„å¡«å……å€¼åº”ä¸º-100ï¼Œè€Œä¸æ˜¯æ ‡è®°å™¨çš„å¡«å……æ ‡è®°ï¼Œä»¥ç¡®ä¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥è¿™äº›å¡«å……å€¼ã€‚

è¿™ä¸€åˆ‡éƒ½å¯ä»¥ç”± [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) å®Œæˆã€‚ ä¸ `DataCollatorWithPadding` ä¸€æ ·ï¼Œå®ƒé‡‡ç”¨ç”¨äºé¢„å¤„ç†è¾“å…¥çš„`tokenizer`ï¼Œä½†å®ƒä¹Ÿé‡‡ç”¨`model`ã€‚ è¿™æ˜¯å› ä¸ºæ•°æ®æ•´ç†å™¨è¿˜å°†è´Ÿè´£å‡†å¤‡è§£ç å™¨è¾“å…¥ IDï¼Œå®ƒä»¬æ˜¯æ ‡ç­¾åç§»ä¹‹åå½¢æˆçš„ï¼Œå¼€å¤´å¸¦æœ‰ç‰¹æ®Šæ ‡è®°ã€‚ ç”±äºä¸åŒæ¶æ„çš„è¿™ç§è½¬å˜ç•¥æœ‰ä¸åŒï¼Œå› æ­¤â€œDataCollatorForSeq2Seqâ€éœ€è¦çŸ¥é“â€œæ¨¡å‹â€å¯¹è±¡ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬åªéœ€åœ¨æˆ‘ä»¬æ ‡è®°åŒ–è®­ç»ƒé›†ä¸­çš„éƒ¨åˆ†æ•°æ®ä¸Šè°ƒç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„æ ‡ç­¾æ˜¯å¦å·²ä½¿ç”¨ **-100** å¡«å……åˆ°æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦ï¼š

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è§£ç å™¨è¾“å…¥ IDï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯æ ‡ç­¾çš„åç§»å½¢æˆçš„ç‰ˆæœ¬ï¼š

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾ï¼š

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

æˆ‘ä»¬å°†æŠŠè¿™ä¸ª `data_collator` ä¼ é€’ç»™ `Seq2SeqTrainer`ã€‚ æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯„ä¼°æŒ‡æ ‡ã€‚

{:else}

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ `data_collator` å°†æˆ‘ä»¬çš„æ¯ä¸ªæ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset`ï¼Œå‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼š

```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### è¯„ä¼°æŒ‡æ ‡

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

`Seq2SeqTrainer` æ·»åŠ åˆ°å…¶è¶…ç±» `Trainer` çš„åŠŸèƒ½æ˜¯åœ¨è¯„ä¼°æˆ–é¢„æµ‹æœŸé—´ä½¿ç”¨ `generate()` æ–¹æ³•çš„èƒ½åŠ›ã€‚ åœ¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹å°†ä½¿ç”¨å¸¦æœ‰æ³¨æ„æ©ç çš„â€œdecoder_input_idsâ€ï¼Œä»¥ç¡®ä¿å®ƒä¸ä½¿ç”¨é¢„æµ‹çš„æ ‡è®°ä¹‹åçš„æ ‡è®°ï¼Œä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ— æ³•ä½¿ç”¨é¢„æµ‹çš„æ ‡è®°ä¹‹åçš„æ ‡è®°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ ‡ç­¾ï¼Œå› æ­¤ä½¿ç”¨ç›¸åŒçš„è®¾ç½®ä½¿ç”¨å¸¦æœ‰æ³¨æ„æ©ç çš„â€œdecoder_input_idsâ€ï¼Œè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸ªå¥½ä¸»æ„ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬ä¸€ç« ](/course/chapter1/6)çœ‹åˆ°çš„ï¼Œè§£ç å™¨é€šè¿‡ä¸€ä¸ªä¸€ä¸ªåœ°é¢„æµ‹æ ‡è®°æ¥æ‰§è¡Œæ¨ç†â€”â€”è¿™æ˜¯ğŸ¤— Transformers åœ¨å¹•åé€šè¿‡ **generate()** æ–¹æ³•å®ç°çš„ã€‚å¦‚æœæˆ‘ä»¬è®¾ç½® predict_with_generate=Trueï¼ŒSeq2 Seq Trainer å°†å…è®¸æˆ‘ä»¬ä½¿ç”¨è¯¥æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚


{/if}

ç”¨äºç¿»è¯‘çš„ä¼ ç»ŸæŒ‡æ ‡æ˜¯[BLEU åˆ†æ•°](https://en.wikipedia.org/wiki/BLEU), ç”±Kishore Papineniç­‰äººåœ¨[2002å¹´çš„ä¸€ç¯‡æ–‡ç« ](https://aclanthology.org/P02-1040.pdf)ä¸­å¼•å…¥ã€‚BLEU åˆ†æ•°è¯„ä¼°ç¿»è¯‘ä¸å…¶æ ‡ç­¾çš„æ¥è¿‘ç¨‹åº¦ã€‚å®ƒä¸è¡¡é‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„å¯æ‡‚åº¦æˆ–è¯­æ³•æ­£ç¡®æ€§ï¼Œè€Œæ˜¯ä½¿ç”¨ç»Ÿè®¡è§„åˆ™æ¥ç¡®ä¿ç”Ÿæˆè¾“å‡ºä¸­çš„æ‰€æœ‰å•è¯ä¹Ÿå‡ºç°åœ¨ç›®æ ‡ä¸­ã€‚æ­¤å¤–ï¼Œå¦‚æœç›¸åŒå•è¯åœ¨ç›®æ ‡ä¸­æ²¡æœ‰é‡å¤ï¼Œåˆ™æœ‰è§„åˆ™æƒ©ç½šç›¸åŒå•è¯çš„é‡å¤ï¼ˆä»¥é¿å…æ¨¡å‹è¾“å‡ºç±»ä¼¼ **the the the the the**çš„å¥å­ ) å¹¶è¾“å‡ºæ¯”ç›®æ ‡ä¸­çŸ­çš„å¥å­ï¼ˆä»¥é¿å…æ¨¡å‹è¾“å‡ºåƒ **the** è¿™æ ·çš„å¥å­ï¼‰ã€‚

BLEU çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ–‡æœ¬å·²ç»è¢«åˆ†è¯ï¼Œè¿™ä½¿å¾—æ¯”è¾ƒä½¿ç”¨ä¸åŒæ ‡è®°å™¨çš„æ¨¡å‹ä¹‹é—´çš„åˆ†æ•°å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œå½“ä»Šç”¨äºåŸºå‡†ç¿»è¯‘æ¨¡å‹çš„æœ€å¸¸ç”¨æŒ‡æ ‡æ˜¯[SacreBLEU](https://github.com/mjpost/sacrebleu)ï¼Œå®ƒé€šè¿‡æ ‡å‡†åŒ–æ ‡è®°åŒ–æ­¥éª¤è§£å†³äº†è¿™ä¸ªç¼ºç‚¹ï¼ˆå’Œå…¶ä»–çš„ä¸€äº›ç¼ºç‚¹ï¼‰ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… SacreBLEU åº“ï¼š

```py
!pip install sacrebleu
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°±åƒæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)é‚£æ ·é€šè¿‡ **load_metric()** åŠ è½½å®ƒ ï¼š

```py
from datasets import load_metric

metric = load_metric("sacrebleu")
```

è¯¥æŒ‡æ ‡å°†æ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œç›®æ ‡ç»“æœã€‚å®ƒæ—¨åœ¨æ¥å—å¤šä¸ªå¯æ¥å—çš„ç›®æ ‡ï¼Œå› ä¸ºåŒä¸€ä¸ªå¥å­é€šå¸¸æœ‰å¤šä¸ªå¯æ¥å—çš„ç¿»è¯‘â€”â€”æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†åªæä¾›ä¸€ä¸ªï¼Œä½†åœ¨ NLP ä¸­æ‰¾åˆ°å°†å¤šä¸ªå¥å­ä½œä¸ºæ ‡ç­¾çš„æ•°æ®é›†ä¸æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚å› æ­¤ï¼Œé¢„æµ‹ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨ï¼Œè€Œå‚è€ƒåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨çš„åˆ—è¡¨ã€‚

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªä¾‹å­ï¼š

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

è¿™å¾—åˆ°äº† 46.75 çš„ BLEU åˆ†æ•°ï¼Œè¿™æ˜¯ç›¸å½“ä¸é”™çš„â€”â€”ä½œä¸ºå‚è€ƒï¼ŒåŸå§‹ Transformer æ¨¡å‹åœ¨[â€œAttention Is All You Needâ€ è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf)ç±»ä¼¼çš„è‹±è¯­å’Œæ³•è¯­ç¿»è¯‘ä»»åŠ¡ä¸­è·å¾—äº† 41.8 çš„ BLEU åˆ†æ•°ï¼ ï¼ˆæœ‰å…³å„ä¸ªæŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯ï¼Œä¾‹å¦‚ **counts** å’Œ **bp** ï¼Œè§[SacreBLEU ä»“åº“](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ä½¿ç”¨ç¿»è¯‘æ¨¡å‹ä¸­ç»å¸¸å‡ºç°çš„ä¸¤ç§ç³Ÿç³•çš„é¢„æµ‹ç±»å‹ï¼ˆå¤§é‡é‡å¤æˆ–å¤ªçŸ­ï¼‰ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ç›¸å½“ç³Ÿç³•çš„ BLEU åˆ†æ•°ï¼š

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

åˆ†æ•°å¯ä»¥ä» 0 åˆ° 100ï¼Œè¶Šé«˜è¶Šå¥½ã€‚

{#if fw === 'tf'}

ä¸ºäº†ä»æ¨¡å‹è¾“å‡ºå¯ä»¥è¢«è¯„ä¼°åŸºå‡†å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **tokenizer.batch_decode()** æ–¹æ³•ã€‚æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ ‡ç­¾ä¸­æ‰€æœ‰ **-100** ï¼ˆæ ‡è®°å™¨ä¼šè‡ªåŠ¨ä¸ºå¡«å……æ ‡è®°åšåŒæ ·çš„äº‹æƒ…ï¼‰ï¼š

```py
import numpy as np


def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch in tf_generate_dataset:
        predictions = model.generate(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = batch["labels"].numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

ä¸ºäº†ä»æ¨¡å‹è¾“å‡ºåˆ°åº¦é‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚ æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ ‡ç­¾ä¸­çš„æ‰€æœ‰ `-100`ï¼ˆæ ‡è®°å™¨å°†è‡ªåŠ¨å¯¹å¡«å……æ ‡è®°æ‰§è¡Œç›¸åŒæ“ä½œï¼‰ï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # In case the model returns more than the prediction logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100s in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

ç°åœ¨è¿™å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼

### å¾®è°ƒæ¨¡å‹

ç¬¬ä¸€æ­¥æ˜¯ç™»å½• Hugging Faceï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥å°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©æ‚¨åœ¨notebookä¸­å®Œæˆæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æ‚¨çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šè¿è¡Œä»£ç ï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'tf'}

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­å¾—åˆ°äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼š

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚ æ³¨æ„å½“ä½¿ç”¨ `tf.keras.mixed_precision.set_global_policy("mixed_float16")`æ—¶â€”â€”è¿™å°†å‘Šè¯‰ Keras ä½¿ç”¨ float16 è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯ä»¥æ˜¾ç€æé«˜æ”¯æŒå®ƒçš„ GPUï¼ˆNvidia 20xx/V100 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰çš„é€Ÿåº¦ã€‚

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `PushToHubCallback` ä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ 2 èŠ‚]((/course/chapter7/2)) ä¸­çœ‹åˆ°çš„ï¼Œç„¶åæˆ‘ä»¬åªéœ€æ‹Ÿåˆæ¨¡å‹æ—¶æ·»åŠ è¯¥å›è°ƒå‡½æ•°ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“æ‚¨æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œæ‚¨ä¹Ÿå¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments`ã€‚ é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„å­˜å‚¨åº“å°†åœ¨æ‚¨çš„å‘½åç©ºé—´ä¸­ï¼Œå¹¶ä»¥æ‚¨è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤è¿™é‡Œå°†æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"`ã€‚

<Tip>

ğŸ’¡å¦‚æœæ‚¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™å®ƒéœ€è¦æ˜¯æ‚¨è¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°†åœ¨å®šä¹‰æ‚¨çš„åç§°æ—¶ä¼šé‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</Tip>

æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒç»“æŸåæˆ‘ä»¬çš„æŒ‡æ ‡æ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•æ‚¨çš„æ¨¡å‹å¹¶ä¸æ‚¨çš„æœ‹å‹åˆ†äº«ã€‚ æ‚¨å·²ç»æˆåŠŸåœ°å¾®è°ƒäº†ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ¨¡å‹â€”â€”æ­å–œï¼

{:else}

ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `Seq2SeqTrainingArguments`ã€‚ ä¸ `Trainer` ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `TrainingArguments` çš„å­ç±»ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šå¯ä»¥è®¾ç½®çš„å­—æ®µï¼š

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

é™¤äº†é€šå¸¸çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€æ‰¹æ¬¡å¤§å°å’Œä¸€äº›æƒé‡è¡°å‡ï¼‰ä¹‹å¤–ï¼Œä¸æˆ‘ä»¬åœ¨å‰å‡ èŠ‚ä¸­çœ‹åˆ°çš„ç›¸æ¯”ï¼Œè¿™é‡Œæœ‰ä¸€äº›å˜åŒ–ï¼š

- æˆ‘ä»¬æ²¡æœ‰è®¾ç½®ä»»ä½•å®šæœŸè¯„ä¼°ï¼Œå› ä¸ºè¯„ä¼°éœ€è¦è€—è´¹ä¸€å®šçš„æ—¶é—´ï¼›æˆ‘ä»¬åªä¼šåœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰å’Œç»“æŸä¹‹åè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ä¸€æ¬¡ã€‚
- æˆ‘ä»¬è®¾ç½®fp16=Trueï¼Œè¿™å¯ä»¥åŠ å¿«æ”¯æŒfp16çš„ GPU ä¸Šçš„è®­ç»ƒé€Ÿåº¦ã€‚
- å’Œä¸Šé¢æˆ‘ä»¬è®¨è®ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬è®¾ç½®predict_with_generate=True
- æˆ‘ä»¬ç”¨push_to_hub=Trueåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubã€‚

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“æ‚¨æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œæ‚¨ä¹Ÿå¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments`ã€‚ é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„å­˜å‚¨åº“å°†åœ¨æ‚¨çš„å‘½åç©ºé—´ä¸­ï¼Œå¹¶ä»¥æ‚¨è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤è¿™é‡Œå°†æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"`ã€‚

<Tip>

ğŸ’¡å¦‚æœæ‚¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™å®ƒéœ€è¦æ˜¯æ‚¨è¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°†åœ¨å®šä¹‰æ‚¨çš„åç§°æ—¶ä¼šé‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</Tip>


æœ€åï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ **Seq2SeqTrainer** ï¼š

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å°†é¦–å…ˆæŸ¥çœ‹æˆ‘ä»¬çš„æ¨¡å‹è·å¾—çš„åˆ†æ•°ï¼Œä»¥ä»”ç»†æ£€æŸ¥æˆ‘ä»¬çš„å¾®è°ƒæ²¡æœ‰è®©äº‹æƒ…å˜å¾—æ›´ç³Ÿã€‚æ­¤å‘½ä»¤éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨æ‰§è¡Œæ—¶å–æ¯å’–å•¡ï¼š

```python
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

BLEUçš„åˆ†æ•°è¿˜ä¸é”™ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬çš„æ¨¡å‹å·²ç»æ“…é•¿å°†è‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­å¥å­ã€‚

æ¥ä¸‹æ¥æ˜¯è®­ç»ƒï¼Œè¿™ä¹Ÿéœ€è¦ä¸€äº›æ—¶é—´ï¼š

```python
trainer.train()
```

è¯·æ³¨æ„ï¼Œå½“è®­ç»ƒå‘ç”Ÿæ—¶ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ªæ—¶æœŸï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œæ‚¨å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­æ‚¨çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å†æ¬¡è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å¸Œæœ›æˆ‘ä»¬ä¼šçœ‹åˆ° BLEU åˆ†æ•°æœ‰æ‰€æ”¹å–„ï¼

```py
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

è¿™æ˜¯è¿‘ 14 ç‚¹çš„æ”¹è¿›ï¼Œè¿™å¾ˆæ£’ã€‚

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ **push_to_hub()** æ–¹æ³•æ¥ç¡®ä¿æˆ‘ä»¬ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ã€‚è¿™ **Trainer** è¿˜åˆ›å»ºäº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡å¹¶ä¸Šä¼ ã€‚æ­¤æ¨¡å‹å¡åŒ…å«å¯å¸®åŠ©æ¨¡å‹ä¸­å¿ƒä¸ºæ¨ç†æ¼”ç¤ºé€‰æ‹©å°éƒ¨ä»¶çš„å…ƒæ•°æ®ã€‚é€šå¸¸ä¸éœ€è¦åšé¢å¤–çš„æ›´æ”¹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»æ¨¡å‹ç±»ä¸­æ¨æ–­å‡ºæ­£ç¡®çš„å°éƒ¨ä»¶ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›¸åŒçš„æ¨¡å‹ç±»å¯ä»¥ç”¨äºæ‰€æœ‰ç±»å‹çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬æŒ‡å®šå®ƒæ˜¯ä¸€ä¸ªç¿»è¯‘æ¨¡å‹ï¼š

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

å¦‚æœæ‚¨æƒ³æ£€æŸ¥å‘½ä»¤æ‰§è¡Œçš„ç»“æœï¼Œæ­¤å‘½ä»¤å°†è¿”å›å®ƒåˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼Œå¯ä»¥æ‰“å¼€urlè¿›è¡Œæ£€æŸ¥ï¼š

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

åœ¨æ­¤é˜¶æ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•æ‚¨çš„æ¨¡å‹å¹¶ä¸æ‚¨çš„æœ‹å‹åˆ†äº«ã€‚æ‚¨å·²æˆåŠŸå¾®è°ƒç¿»è¯‘ä»»åŠ¡çš„æ¨¡å‹ - æ­å–œï¼

å¦‚æœæ‚¨æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

{/if}

{#if fw === 'pt'}

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è½»æ¾è‡ªå®šä¹‰æ‰€éœ€çš„éƒ¨åˆ†ã€‚å®ƒçœ‹èµ·æ¥å¾ˆåƒæˆ‘ä»¬åœ¨[æœ¬ç« ç¬¬äºŒèŠ‚](/course/chapter7/2)å’Œ[ç¬¬ä¸‰ç« ç¬¬å››å°èŠ‚](/course/chapter3/4)æ‰€åšçš„ã€‚

### å‡†å¤‡è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡

æ‚¨å·²ç»å¤šæ¬¡çœ‹åˆ°æ‰€æœ‰è¿™äº›ï¼Œå› æ­¤è¿™ä¸€å—ä¼šç®€ç•¥è¿›è¡Œã€‚é¦–å…ˆæˆ‘ä»¬å°†æ„å»ºæˆ‘ä»¬çš„æ•°æ®é›†çš„**DataLoader** ï¼Œåœ¨å°†æ•°æ®é›†è®¾ç½®ä¸º **torch** æ ¼å¼ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº† PyTorch å¼ é‡ï¼š

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šç»§ç»­ä¸Šä¸€èŠ‚çš„å¾®è°ƒï¼Œè€Œæ˜¯å†æ¬¡ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é‡æ–°è®­ç»ƒï¼š

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ï¼š

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰æ‰€æœ‰è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ã€‚ è¯·è®°ä½ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Colab ç¬”è®°æœ¬è®­ç»ƒä¸­ä½¿ç”¨TPUï¼Œåˆ™éœ€è¦å°†æ‰€æœ‰è¿™äº›ä»£ç ç§»åŠ¨åˆ°è®­ç»ƒå‡½æ•°ä¸­ï¼Œå¹¶ä¸”ä¸åº”æ‰§è¡Œä»»ä½•å®ä¾‹åŒ–â€œåŠ é€Ÿå™¨â€çš„å¯¹è±¡ã€‚

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‘é€äº†æˆ‘ä»¬çš„ **train_dataloader** åˆ° **accelerator.prepare()** ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆåœ¨å‡†å¤‡å¥½æ•°æ®åŠ è½½å™¨åæ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºè¯¥æ–¹æ³•ä¼šæ›´æ”¹ **DataLoader** .æˆ‘ä»¬ä½¿ç”¨ä»å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œè¦å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª **Repository** å·¥ä½œæ–‡ä»¶å¤¹ä¸­çš„å¯¹è±¡ã€‚å¦‚æœæ‚¨å°šæœªç™»å½•ï¼Œè¯·å…ˆç™»å½• Hugging Faceã€‚æˆ‘ä»¬å°†ä»æˆ‘ä»¬æƒ³è¦ä¸ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¡®å®šå­˜å‚¨åº“åç§°ï¼ˆæ‚¨å¯ä»¥è‡ªç”±åœ°ç”¨è‡ªå·±çš„é€‰æ‹©æ›¿æ¢ **repo_name** ï¼›å®ƒéœ€è¦åŒ…å«æ‚¨çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨**get_full_repo_name()**å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¹ä¸­å…‹éš†è¯¥å­˜å‚¨åº“ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„å­˜å‚¨åº“çš„å…‹éš†ï¼š

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ **repo.push_to_hub()** æ–¹æ³•ä¸Šä¼ æˆ‘ä»¬ä¿å­˜çš„ä»»ä½•å†…å®¹ **output_dir** ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ä¸Šä¼ è¿‡ç¨‹ä¸­çš„æ¨¡å‹ã€‚

### è®­ç»ƒå¾ªç¯

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å®ƒçš„è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ª **postprocess()** å‡½æ•°æ¥æ”¶é¢„æµ‹ç»“æœå’Œæ­£ç¡®æ ‡ç­¾å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæˆ‘ä»¬ **metric** å¯¹è±¡æ‰€éœ€è¦çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼š

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

è®­ç»ƒå¾ªç¯çœ‹èµ·æ¥å’Œ[æœ¬ç« ç¬¬äºŒèŠ‚](/course/chapter7/2)ä¸[ç¬¬ä¸‰ç« ](/course/chapter3)å¾ˆåƒï¼Œåœ¨è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›ä¸åŒ - æ‰€ä»¥è®©æˆ‘ä»¬ä¸“æ³¨äºè¿™ä¸€ç‚¹ï¼

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯æˆ‘ä»¬ä½¿ç”¨ `generate()` æ–¹æ³•æ¥è®¡ç®—é¢„æµ‹ï¼Œä½†è¿™æ˜¯æˆ‘ä»¬åŸºç¡€æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œè€Œä¸æ˜¯åŒ…è£…æ¨¡å‹ğŸ¤— Accelerate åœ¨ `prepare()` æ–¹æ³•ä¸­åˆ›å»ºã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å…ˆè§£åŒ…æ¨¡å‹ï¼Œç„¶åè°ƒç”¨è¿™ä¸ªæ–¹æ³•ã€‚

ç¬¬äºŒä»¶äº‹æ˜¯ï¼Œå°±åƒ[token åˆ†ç±»](/course/chapter7/2)ï¼Œä¸¤ä¸ªè¿›ç¨‹å¯èƒ½å°†è¾“å…¥å’Œæ ‡ç­¾å¡«å……ä¸ºä¸åŒçš„å½¢çŠ¶ï¼Œå› æ­¤æˆ‘ä»¬åœ¨è°ƒç”¨ **gather()** æ–¹æ³•ä¹‹å‰ä½¿ç”¨ **accelerator.pad_across_processes()** ä½¿é¢„æµ‹å’Œæ ‡ç­¾å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™æ ·åšï¼Œè¯„ä¼°è¦ä¹ˆå‡ºé”™ï¼Œè¦ä¹ˆæ°¸è¿œåœ¨é˜»å¡ã€‚

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

å®Œæˆæ­¤æ“ä½œåï¼Œæ‚¨åº”è¯¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå…¶ç»“æœä¸ä½¿ç”¨ `Seq2SeqTrainer` è®­ç»ƒçš„æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚ æ‚¨å¯ä»¥åœ¨ [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerateï¼‰æŸ¥çœ‹è®­ç»ƒå®Œçš„ç»“æœã€‚ å¦‚æœæ‚¨æƒ³æµ‹è¯•å¯¹è®­ç»ƒå¾ªç¯çš„ä»»ä½•è°ƒæ•´ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç ç›´æ¥å®ç°å®ƒä»¬ï¼

{/if}

## ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

æˆ‘ä»¬å·²ç»å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•å°†æˆ‘ä»¬åœ¨æ¨¡å‹ä¸­å¿ƒå¾®è°ƒçš„æ¨¡å‹ä¸æ¨ç†å°éƒ¨ä»¶ä¸€èµ·ä½¿ç”¨ã€‚ è¦åœ¨â€œç®¡é“â€ä¸­æœ¬åœ°ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼š

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹å°†å…¶çŸ¥è¯†é€‚åº”äº†æˆ‘ä»¬å¯¹å…¶è¿›è¡Œå¾®è°ƒçš„è¯­æ–™åº“ï¼Œè€Œä¸æ˜¯å•ç‹¬ç•™ä¸‹è‹±æ–‡å•è¯â€œthreadsâ€ï¼Œè€Œæ˜¯å°†å…¶ç¿»è¯‘æˆæ³•è¯­å®˜æ–¹ç‰ˆæœ¬ã€‚ â€œâ€çš„ç¿»è¯‘ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

é£æ ¼é€‚åº”çš„å¦ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** â€œç”µå­é‚®ä»¶â€è¿™ä¸ªè¯åœ¨æ¨¡å‹è¿”å›äº†ä»€ä¹ˆï¼Ÿ

</Tip>
