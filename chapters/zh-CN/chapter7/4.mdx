<FrameworkSwitchCourse {fw} />

# ç¿»è¯‘ [[ç¿»è¯‘]]

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
]} />

{/if}

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ç¿»è¯‘ã€‚è¿™æ˜¯å¦ä¸€ä¸ª [sequence-to-sequence ä»»åŠ¡](/course/chapter1/7) ï¼Œç€è¿™æ˜¯ä¸€ä¸ªå¯ä»¥è¡¨è¿°ä¸ºè¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—è¾“å‡ºå¦ä¸€ä¸ªåºåˆ—çš„é—®é¢˜ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œè¿™ä¸ªé—®é¢˜éå¸¸ç±»ä¼¼ [æ–‡æœ¬æ‘˜è¦](/course/chapter7/6) ï¼Œå¹¶ä¸”ä½ å¯ä»¥å°†æˆ‘ä»¬å°†åœ¨æ­¤å¤„å­¦ä¹ åˆ°çš„ä¸€äº›æŠ€å·§è¿ç§»åˆ°å…¶ä»–çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **é£æ ¼è¿ç§»** åˆ›å»ºä¸€ä¸ªæ¨¡å‹å°†æŸç§é£æ ¼è¿ç§»åˆ°ä¸€æ®µæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ­£å¼çš„é£æ ¼è¿ç§»åˆ°ä¼‘é—²çš„é£æ ¼ï¼Œæˆ–ä»èå£«æ¯”äºšè‹±è¯­è¿ç§»åˆ°ç°ä»£è‹±è¯­ï¼‰ã€‚
- **ç”Ÿæˆé—®é¢˜çš„å›ç­”** åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆã€‚

<Youtube id="1JvfrvZgi6c"/>

å¦‚æœä½ æœ‰è¶³å¤Ÿå¤§çš„ä¸¤ç§ï¼ˆæˆ–æ›´å¤šï¼‰è¯­è¨€çš„æ–‡æœ¬è¯­æ–™åº“ï¼Œä½ å¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°çš„ç¿»è¯‘æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨ [å› æœè¯­è¨€å»ºæ¨¡](/course/chapter7/6) éƒ¨åˆ†ä¸­æ‰€åšçš„é‚£æ ·ã€‚ç„¶è€Œï¼Œå¾®è°ƒç°æœ‰çš„ç¿»è¯‘æ¨¡å‹ä¼šæ›´å¿«ï¼Œæ— è®ºæ˜¯ä»åƒ mT5 æˆ– mBART è¿™æ ·çš„å¤šè¯­è¨€æ¨¡å‹å¾®è°ƒåˆ°ç‰¹å®šçš„è¯­è¨€å¯¹ï¼Œè¿˜æ˜¯ä»ç‰¹å®šè¯­æ–™åº“çš„ä¸€ç§è¯­è¨€åˆ°å¦ä¸€ç§è¯­è¨€çš„ä¸“ç”¨ç¿»è¯‘æ¨¡å‹ã€‚

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ [KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4) ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„ Marian æ¨¡å‹ï¼Œç”¨æ¥æŠŠè‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„ï¼ˆå› ä¸ºå¾ˆå¤š Hugging Face çš„å‘˜å·¥éƒ½ä¼šè¯´è¿™ä¸¤ç§è¯­è¨€ï¼‰ã€‚KDE4 æ•°æ®é›†æ˜¯ä¸€ä¸ª [KDE åº”ç”¨](https://apps.kde.org/) æœ¬åœ°åŒ–çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ¨¡å‹å·²ç»åœ¨ä» [Opus æ•°æ®é›†](https://opus.nlpl.eu/) ï¼ˆå®é™…ä¸ŠåŒ…å« KDE4 æ•°æ®é›†ï¼‰ä¸­æå–çš„æ³•è¯­å’Œè‹±è¯­æ–‡æœ¬çš„å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„å…ˆè®­ç»ƒã€‚ä¸è¿‡ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å…¶é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨äº†è¿™éƒ¨åˆ†æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°ï¼Œç»è¿‡å¾®è°ƒåï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç‰ˆæœ¬ã€‚

å®Œæˆåï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè¿™æ ·çš„ç¿»è¯‘ï¼š

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ä¸å‰é¢å‡ èŠ‚ä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„å®é™…æ¨¡å‹ï¼Œå¹¶ [åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.) æŸ¥çœ‹æ¨¡å‹è¾“å‡ºçš„ç»“æœã€‚

## å‡†å¤‡æ•°æ® [[å‡†å¤‡æ•°æ®]]

ä¸ºäº†ä»å¤´å¼€å§‹å¾®è°ƒæˆ–è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ•°æ®é›†ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4) ã€‚åªè¦æ•°æ®é›†ä¸­æœ‰äº’è¯‘çš„ä¸¤ç§è¯­è¨€çš„å¥å­å¯¹ï¼Œå°±å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´æœ¬èŠ‚çš„ä»£ç ä»¥ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚å¦‚æœä½ éœ€è¦å¤ä¹ å¦‚ä½•å°†è‡ªå®šä¹‰æ•°æ®åŠ è½½åˆ° `Dataset` ï¼Œå¯ä»¥å¤ä¹ ä¸€ä¸‹ [ç¬¬äº”ç« ](/course/chapter5) ã€‚

### KDE4 æ•°æ®é›† [[KDE4 æ•°æ®é›†]]

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `load_dataset()` å‡½æ•°ä¸‹è½½æ•°æ®é›†ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–çš„è¯­è¨€å¯¹ï¼Œä½ å¯ä»¥ä½¿ç”¨è¯­è¨€ä»£ç æ¥è®¾ç½®ä½ æƒ³ä½¿ç”¨çš„è¯­è¨€å¯¹ã€‚è¯¥æ•°æ®é›†å…±æœ‰ 92 ç§è¯­è¨€å¯ç”¨ï¼›ä½ å¯ä»¥é€šè¿‡å±•å¼€ [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/kde4) ä¸Šçš„è¯­è¨€æ ‡ç­¾æ¥æŸ¥çœ‹æ•°æ®é›†æ”¯æŒçš„è¯­è¨€æ ‡ç­¾ã€‚

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

æˆ‘ä»¬æ¥çœ‹çœ‹æ•°æ®é›†ï¼š

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

æˆ‘ä»¬ä¸‹è½½çš„æ•°æ®é›†æœ‰ 210,173 å¯¹å¥å­ï¼Œåœ¨ä¸€æ¬¡è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé™¤äº†è®­ç»ƒé›†ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦åˆ›å»ºè‡ªå·±çš„éªŒè¯é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬äº”ç« ](/course/chapter5) å­¦çš„çš„é‚£æ ·ï¼Œ `Dataset` æœ‰ä¸€ä¸ª `train_test_split()` æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬ã€‚æˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªå›ºå®šçš„éšæœºæ•°ç§å­ä»¥ä¿è¯ç»“æœå¯ä»¥å¤ç°ï¼š

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

æˆ‘ä»¬å¯ä»¥åƒä¸‹é¢è¿™æ ·å°† `test` é”®é‡å‘½åä¸º `validation`ï¼š

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼š

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåŒ…å«æˆ‘ä»¬é€‰æ‹©çš„ä¸¤ç§è¯­è¨€çš„ä¸¤ä¸ªå¥å­çš„å­—å…¸ã€‚è¿™ä¸ªå……æ»¡æŠ€æœ¯è®¡ç®—æœºç§‘å­¦æœ¯è¯­çš„æ•°æ®é›†çš„ä¸€ä¸ªç‰¹æ®Šä¹‹å¤„åœ¨äºå®ƒä»¬éƒ½å®Œå…¨ç”¨æ³•è¯­ç¿»è¯‘ã€‚ç„¶è€Œç°å®ä¸­ï¼Œæ³•å›½å·¥ç¨‹å¸ˆåœ¨äº¤è°ˆæ—¶ï¼Œå¤§å¤šæ•°è®¡ç®—æœºç§‘å­¦ä¸“ç”¨è¯æ±‡éƒ½ç”¨è‹±è¯­è¡¨è¿°ã€‚ä¾‹å¦‚ï¼Œâ€œthreadsâ€è¿™ä¸ªè¯å¾ˆå¯èƒ½å‡ºç°åœ¨æ³•è¯­å¥å­ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æŠ€æœ¯å¯¹è¯ä¸­ï¼›ä½†åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œå®ƒè¢«ç¿»è¯‘æˆæ›´å‡†ç¡®çš„â€œfils de Discussionâ€ã€‚æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ä¸€ä¸ªæ›´å¤§çš„æ³•è¯­å’Œè‹±è¯­å¥å­è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ‰€ä»¥è¾“å‡ºçš„æ˜¯åŸå§‹çš„è‹±è¯­è¡¨è¾¾ï¼š

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

è¿™ç§æƒ…å†µçš„å¦ä¸€ä¸ªä¾‹å­å¯ä»¥åœ¨â€œpluginâ€è¿™ä¸ªè¯ä¸Šçœ‹åˆ°ï¼Œå®ƒå¹¶éæ­£å¼çš„æ³•è¯­è¯æ±‡ï¼Œä½†å¤§å¤šæ•°æ¯è¯­æ˜¯æ³•è¯­çš„äººéƒ½èƒ½å¤Ÿçœ‹æ‡‚å¹¶ä¸”ä¸ä¼šå»ç¿»è¯‘å®ƒã€‚ä¸è¿‡ï¼Œåœ¨ KDE4 æ•°æ®é›†ä¸­ï¼Œè¿™ä¸ªè¯è¢«ç¿»è¯‘æˆäº†æ›´æ­£å¼çš„æ³•è¯­è¯æ±‡â€œmodule d'extensionâ€ï¼š

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ç„¶è€Œï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹åšæŒä½¿ç”¨ç®€ç»ƒè€Œç†Ÿæ‚‰çš„è‹±æ–‡å•è¯ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

çœ‹çœ‹æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ˜¯å¦èƒ½å­¦ä¹ åˆ°æ•°æ®é›†çš„è¿™äº›ç‰¹æ®Šç‰¹æ€§ã€‚ï¼ˆå‰§é€è­¦å‘Šï¼šå®ƒèƒ½ï¼‰ã€‚

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** å¦ä¸€ä¸ªåœ¨æ³•è¯­ä¸­ç»å¸¸ä½¿ç”¨çš„è‹±è¯­å•è¯æ˜¯â€œemailâ€ã€‚åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰¾åˆ°ä½¿ç”¨è¿™ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ã€‚åœ¨æ•°æ®é›†ä¸­å®ƒæ˜¯å¦‚ä½•ç¿»è¯‘çš„ï¼Ÿé¢„è®­ç»ƒæ¨¡å‹å¦‚ä½•ç¿»è¯‘åŒä¸€ä¸ªè‹±æ–‡å¥å­ï¼Ÿ

</Tip>

### å¤„ç†æ•°æ® [[å¤„ç†æ•°æ®]]

<Youtube id="XAR8jnZZuUs"/>

ä½ ç°åœ¨åº”è¯¥å¯ä»¥é¢„æµ‹æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥è¯¥åšäº›ä»€ä¹ˆäº†ï¼šå°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸º token IDs çš„é›†åˆï¼Œè¿™æ ·æ¨¡å‹æ‰å¯ä»¥ç†è§£å®ƒä»¬ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶å¯¹åŸå§‹æ–‡æœ¬å’Œç¿»è¯‘åçš„æ–‡æœ¬åŒæ—¶è¿›è¡Œ tokenizeã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»º `tokenizer` å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Marian è‹±è¯­åˆ°æ³•è¯­çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœä½ ä½¿ç”¨ä¸‹é¢çš„ä»£ç å¾®è°ƒå¦ä¸€å¯¹è¯­è¨€ï¼Œè¯·è®°å¾—æ›´æ”¹ä¸‹é¢ä»£ç ä¸­çš„ checkpointã€‚ [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) ç»„ç»‡æä¾›äº†è¶…è¿‡ä¸€åƒä¸ªå¤šè¯­è¨€æ¨¡å‹ã€‚

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

ä½ ä¹Ÿå¯ä»¥å°† `model_checkpoint` æ›¿æ¢ä¸ºä½ ä» [Hub](https://huggingface.co/models) ä¸­é€‰æ‹©çš„å…¶ä»–æ¨¡å‹ï¼Œæˆ–è€…ä¸€ä¸ªä¿å­˜äº†é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizer çš„æœ¬åœ°æ–‡ä»¶å¤¹ã€‚

<Tip>

ğŸ’¡ å¦‚æœä½ åœ¨ä½¿ç”¨ä¸€ä¸ªå¤šè¯­è¨€çš„ tokenizerï¼Œæ¯”å¦‚ mBARTï¼ŒmBART-50ï¼Œæˆ–è€… M2M100ï¼Œä½ éœ€è¦é€šè¿‡è®¾ç½® `tokenizer.src_lang` å’Œ `tokenizer.tgt_lang` æ¥åœ¨ tokenizer ä¸­æŒ‡å®šè¾“å…¥å’Œç›®æ ‡çš„è¯­è¨€ä»£ç ã€‚

</Tip>

æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡ç›¸å½“ç®€å•ã€‚åªæœ‰ä¸€ç‚¹è¦è®°ä½ï¼›ä½ éœ€è¦ç¡®ä¿ tokenizer å¤„ç†çš„ç›®æ ‡æ˜¯è¾“å‡ºè¯­è¨€ï¼ˆåœ¨è¿™é‡Œæ˜¯æ³•è¯­ï¼‰ã€‚ä½ å¯ä»¥é€šè¿‡å°†ç›®æ ‡è¯­è¨€ä¼ é€’ç»™ tokenizer çš„ `__call__` æ–¹æ³•çš„ `text_targets` å‚æ•°æ¥å®Œæˆæ­¤æ“ä½œã€‚

ä¸ºäº†æ¼”ç¤ºè®¾ç½®çš„æ–¹æ³•ï¼Œè®©æˆ‘ä»¬å¤„ç†è®­ç»ƒé›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬ï¼š

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¾“å‡ºåŒ…å«äº†ä¸è‹±è¯­å¥å­çš„ `inputs IDs`ï¼Œè€Œä¸æ³•è¯­å¥å­çš„ IDs å­˜å‚¨åœ¨ `labels` å­—æ®µä¸­ã€‚å¦‚æœä½ å¿˜è®°è®¾ç½® labels çš„ `tokenizer`ï¼Œé»˜è®¤æƒ…å†µä¸‹ `labels` å°†ç”±è¾“å…¥çš„ `tokenizer`ï¼ˆè¯­è¨€ç±»å‹ä¸ä¸€æ ·ï¼‰ è¿›è¡Œ `tokenize`ï¼Œå¯¹äº Marian æ¨¡å‹æ¥è¯´ï¼Œæ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

å¦‚ä½ æ‰€è§ï¼Œå¦‚æœç”¨è‹±è¯­çš„ tokenizer æ¥é¢„å¤„ç†æ³•è¯­å¥å­ï¼Œä¼šäº§ç”Ÿæ›´å¤šçš„ tokensï¼Œå› ä¸ºè¿™ä¸ª tokenizer ä¸è®¤è¯†ä»»ä½•æ³•è¯­å•è¯ï¼ˆé™¤äº†é‚£äº›åœ¨è‹±è¯­é‡Œä¹Ÿå‡ºç°çš„ï¼Œæ¯”å¦‚â€œdiscussionâ€ï¼‰ã€‚

æœ€åä¸€æ­¥æ˜¯å®šä¹‰æˆ‘ä»¬æ•°æ®é›†çš„é¢„å¤„ç†å‡½æ•°ï¼š

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

è¯·æ³¨æ„ï¼Œä¸Šè¿°ä»£ç ä¹Ÿä¸ºè¾“å…¥å’Œè¾“å‡ºè®¾ç½®äº†ç›¸åŒçš„æœ€å¤§é•¿åº¦ã€‚ç”±äºè¦å¤„ç†çš„æ–‡æœ¬çœ‹èµ·æ¥å¾ˆçŸ­ï¼Œå› æ­¤åœ¨è¿™é‡Œå°†æœ€å¤§é•¿åº¦è®¾ç½®ä¸º 128ã€‚

<Tip>

ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ T5 æ¨¡å‹ï¼ˆæ›´å…·ä½“åœ°è¯´ï¼Œä¸€ä¸ª `t5-xxx` checkpoint ï¼‰ï¼Œæ¨¡å‹ä¼šæœŸæœ›æ–‡æœ¬è¾“å…¥æœ‰ä¸€ä¸ªå‰ç¼€æŒ‡ç¤ºç›®å‰çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ `translate: English to French:` ã€‚

</Tip>

<Tip warning={true}>

âš ï¸ æˆ‘ä»¬ä¸éœ€è¦å¯¹å¾…é‡æµ‹çš„ç›®æ ‡è®¾ç½®æ³¨æ„åŠ›æ©ç ï¼Œå› ä¸ºæ¨¡å‹åºåˆ—åˆ°åºåˆ—çš„ä¸ä¼šéœ€è¦å®ƒã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬åº”è¯¥å°†å¡«å……ï¼ˆpaddingï¼‰ token å¯¹åº”çš„æ ‡ç­¾è®¾ç½®ä¸º `-100` ï¼Œä»¥ä¾¿åœ¨ loss è®¡ç®—ä¸­å¿½ç•¥å®ƒä»¬ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨åŠ¨æ€å¡«å……ï¼Œè¿™å°†åœ¨ç¨åç”±æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å®Œæˆï¼Œä½†æ˜¯å¦‚æœä½ åœ¨æ­¤å¤„å°±æ‰“ç®—è¿›è¡Œå¡«å……ï¼Œä½ åº”è¯¥è°ƒæ•´é¢„å¤„ç†å‡½æ•°ï¼Œå°†æ‰€æœ‰å¡«å……ï¼ˆpaddingï¼‰ token å¯¹åº”çš„æ ‡ç­¾è®¾ç½®ä¸º `-100` ã€‚

</Tip>

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸€æ¬¡æ€§ä½¿ç”¨ä¸Šè¿°é¢„å¤„ç†å¤„ç†æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®ã€‚

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ç°åœ¨æ•°æ®å·²ç»è¿‡é¢„å¤„ç†ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹äº†ï¼

{#if fw === 'pt'}

## ä½¿ç”¨ `Trainer` API å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨ `Trainer` API å¾®è°ƒæ¨¡å‹]]

ä½¿ç”¨ `Trainer` çš„ä»£ç å°†ä¸ä»¥å‰ç›¸åŒï¼Œåªæ˜¯ç¨ä½œæ”¹åŠ¨ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œå°†ä½¿ç”¨ [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) ï¼Œå®ƒæ˜¯ `Trainer` çš„å­ç±»ï¼Œå®ƒä½¿ç”¨ `generate()` æ–¹æ³•æ¥é¢„æµ‹è¾“å…¥çš„è¾“å‡ºç»“æœï¼Œå¹¶ä¸”å¯ä»¥æ­£ç¡®å¤„ç†è¿™ç§åºåˆ—åˆ°åºåˆ—çš„è¯„ä¼°ã€‚å½“æˆ‘ä»¬è®¨è®ºè¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸€ç‚¹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¸ç”¨çš„ `AutoModel` APIï¼š

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹]]

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¸ç”¨çš„ `AutoModel` APIï¼š

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

ğŸ’¡ `Helsinki-NLP/opus-mt-en-fr` checkpoint åªæœ‰ PyTorch çš„æƒé‡ï¼Œæ‰€ä»¥å¦‚æœä½ å°è¯•ä½¿ç”¨ `from_pretrained()` æ–¹æ³•åŠ è½½æ¨¡å‹å¹¶ä¸”å¿˜è®°è®¾ç½®`from_pt=True` å‚æ•°çš„æ—¶å€™ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªé”™è¯¯ã€‚å½“ä½ è®¾å®š `from_pt=True` æ—¶ï¼ŒğŸ¤—transormer ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ä¸ºä½ è½¬æ¢ PyTorch æƒé‡ã€‚å¦‚ä½ æ‰€è§ï¼Œä½¿ç”¨ğŸ¤—transormer åœ¨ä¸¤ç§æ¡†æ¶ä¹‹é—´åˆ‡æ¢éå¸¸ç®€å•ã€‚

</Tip>

{/if}

æ³¨æ„ï¼Œè¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå·²ç»åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œè¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œå®é™…ä¸Šå·²ç»å¯ä»¥ç›´æ¥ä½¿ç”¨äº†ï¼Œæ‰€ä»¥æ²¡æœ‰æ”¶åˆ°å…³äºç¼ºå°‘æƒé‡æˆ–é‡æ–°åˆå§‹åŒ–çš„æƒé‡çš„è­¦å‘Šã€‚

### æ•°æ®æ•´ç† [[æ•°æ®æ•´ç†]]

åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ•´ç†å™¨æ¥åŠ¨æ€æ‰¹å¤„ç†å¡«å……ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½åƒ [ç¬¬ä¸‰ç« ](/course/chapter3) é‚£æ ·ç›´æ¥ä½¿ç”¨ `DataCollatorWithPadding` ï¼Œå› ä¸ºå®ƒåªå¡«å……è¾“å…¥çš„éƒ¨åˆ†ï¼ˆinputs IDã€æ³¨æ„æ©ç å’Œ token ç±»å‹ IDï¼‰ã€‚æˆ‘ä»¬çš„æ ‡ç­¾ä¹Ÿåº”è¯¥è¢«å¡«å……åˆ°æ‰€æœ‰æ ‡ç­¾ä¸­æœ€å¤§çš„é•¿åº¦ã€‚è€Œä¸”ï¼Œå¦‚å‰æ‰€è¿°ï¼Œç”¨äºå¡«å……æ ‡ç­¾çš„å¡«å……å€¼åº”ä¸º `-100` ï¼Œè€Œä¸æ˜¯ tokenizer é»˜è®¤çš„çš„å¡«å…… tokenï¼Œè¿™æ ·æ‰å¯ä»¥åœ¨ç¡®ä¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥è¿™äº›å¡«å……å€¼ã€‚

ä¸Šè¿°çš„è¿™äº›éœ€æ±‚éƒ½å¯ä»¥ç”± [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) å®Œæˆã€‚å®ƒä¸ `DataCollatorWithPadding` ä¸€æ ·ï¼Œå®ƒæ¥æ”¶ç”¨äºé¢„å¤„ç†è¾“å…¥çš„ `tokenizer` ï¼ŒåŒæ—¶å®ƒä¹Ÿæ¥æ”¶ä¸€ä¸ª `model` å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºæ•°æ®æ•´ç†å™¨è¿˜å°†è´Ÿè´£å‡†å¤‡è§£ç å™¨ `inputs ID`ï¼Œå®ƒä»¬æ˜¯æ ‡ç­¾åç§»ä¹‹åå½¢æˆçš„ï¼Œå¼€å¤´å¸¦æœ‰ç‰¹æ®Š `token` ã€‚ç”±äºå¯¹äºä¸åŒçš„æ¨¡å‹æ¶æ„æœ‰ç¨å¾®ä¸åŒçš„åç§»æ–¹å¼ï¼Œæ‰€ä»¥ `DataCollatorForSeq2Seq` è¿˜éœ€è¦æ¥æ”¶ `model` å¯¹è±¡ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬åœ¨å·²ç»å®Œæˆ tokenize çš„è®­ç»ƒé›†ä¸­çš„éƒ¨åˆ†æ•°æ®ä¸Šè°ƒç”¨å®ƒï¼Œæµ‹è¯•ä¸€ä¸‹å…¶åŠŸèƒ½ï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„æ ‡ç­¾æ˜¯å¦å·²ç»ç”¨ `-100` å¡«å……åˆ° batch çš„æœ€å¤§é•¿åº¦ï¼š

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è§£ç å™¨çš„ inputs IDï¼Œå¯ä»¥çœ‹åˆ°å®ƒä»¬æ˜¯æ ‡ç­¾ç»è¿‡åç§»åçš„ç»“æœï¼š

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾ï¼š

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

æŠŠ `data_collator` ä¼ é€’ç»™ `Seq2SeqTrainer` åå°±å®Œæˆäº†æ•°æ®æ•´ç†ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯„ä¼°æŒ‡æ ‡ã€‚
{:else}

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ `data_collator` å°†æˆ‘ä»¬çš„æ¯ä¸ªæ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` ï¼Œè¿™æ ·å°±å®Œæˆäº†æ•°æ®æ•´ç†ï¼š

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### è¯„ä¼°æŒ‡æ ‡ [[è¯„ä¼°æŒ‡æ ‡]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

`Seq2SeqTrainer` æ˜¯ `Trainer` ç±»çš„ä¸€ä¸ªå­ç±»ï¼Œå®ƒçš„ä¸»è¦å¢å¼ºç‰¹æ€§æ˜¯åœ¨è¯„ä¼°æˆ–é¢„æµ‹æ—¶ä½¿ç”¨ `generate()` æ–¹æ³•ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šåˆ©ç”¨ `decoder_input_ids` å’Œä¸€ä¸ªç‰¹æ®Šçš„æ³¨æ„åŠ›æ©ç æ¥åŠ é€Ÿè®­ç»ƒã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ—¶çœ‹åˆ°éƒ¨åˆ†ç›®æ ‡åºåˆ—ï¼Œä½†ç¡®ä¿å®ƒä¸ä¼šä½¿ç”¨é¢„æµ‹tokenä¹‹åçš„ä¿¡æ¯ã€‚è¿™ç§ä¼˜åŒ–ç­–ç•¥æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„æ ‡ç­¾å€¼ï¼Œå› æ­¤æ— æ³•ç”Ÿæˆ `decoder_input_ids` å’Œç›¸åº”çš„æ³¨æ„åŠ›æ©ç ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æ— æ³•åœ¨æ¨ç†æ—¶ä½¿ç”¨è¿™ç§è®­ç»ƒæ—¶çš„ä¼˜åŒ–æ–¹æ³•ã€‚

ä¸ºäº†ç¡®ä¿è¯„ä¼°ç»“æœèƒ½å¤Ÿå‡†ç¡®åæ˜ æ¨¡å‹åœ¨å®é™…ä½¿ç”¨ä¸­çš„è¡¨ç°ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨è¯„ä¼°é˜¶æ®µæ¨¡æ‹ŸçœŸå®æ¨ç†çš„æ¡ä»¶ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ä½¿ç”¨åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1/6) ä¸­ä»‹ç»çš„ ğŸ¤— Transformers åº“ä¸­çš„ `generate()` æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé€ä¸ªç”Ÿæˆtokenï¼ŒçœŸå®åœ°æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä¾èµ–äºè®­ç»ƒæ—¶çš„ä¼˜åŒ–æŠ€å·§ã€‚è¦å¯ç”¨è¿™ä¸ªåŠŸèƒ½ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒæ—¶æ·»åŠ  `predict_with_generate=True` å‚æ•°ã€‚è¿™æ ·åšå¯ä»¥ç¡®ä¿æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ›´åŠ æ¥è¿‘æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚


{/if}

ç”¨äºç¿»è¯‘çš„ä¼ ç»ŸæŒ‡æ ‡æ˜¯ [BLEU åˆ†æ•°](https://en.wikipedia.org/wiki/BLEU) ï¼Œå®ƒæœ€åˆç”± Kishore Papineni ç­‰äººåœ¨ 2002 å¹´çš„ [ä¸€ç¯‡æ–‡ç« ](https://aclanthology.org/P02-1040.pdf) ä¸­å¼•å…¥ã€‚BLEU åˆ†æ•°è¯„ä¼°ç¿»è¯‘ä¸å‚è€ƒç¿»è¯‘çš„æ¥è¿‘ç¨‹åº¦ã€‚å®ƒä¸è¡¡é‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„å¯ç†è§£æ€§æˆ–è¯­æ³•æ­£ç¡®æ€§ï¼Œè€Œæ˜¯ä½¿ç”¨ç»Ÿè®¡è§„åˆ™æ¥ç¡®ä¿ç”Ÿæˆè¾“å‡ºä¸­çš„æ‰€æœ‰å•è¯ä¹Ÿå‡ºç°åœ¨å‚è€ƒçš„è¾“å‡ºä¸­ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›è§„åˆ™å¯¹é‡å¤çš„è¯è¿›è¡Œæƒ©ç½šï¼Œå¦‚æœè¿™äº›è¯åœ¨è¾“å‡ºä¸­é‡å¤å‡ºç°ï¼ˆæ¨¡å‹è¾“å‡ºåƒâ€œthe the the the theâ€è¿™æ ·çš„å¥å­ï¼‰ï¼›æˆ–è€…è¾“å‡ºçš„å¥å­é•¿åº¦æ¯”ç›®æ ‡ä¸­çš„çŸ­ï¼ˆæ¨¡å‹è¾“å‡ºåƒâ€œtheâ€è¿™æ ·çš„å¥å­ï¼‰éƒ½ä¼šè¢«æƒ©ç½šã€‚

BLEU çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯çš„è¾“å…¥æ˜¯å·²åˆ†è¯çš„æ–‡æœ¬ï¼Œè¿™ä½¿å¾—æ¯”è¾ƒä½¿ç”¨ä¸åŒåˆ†è¯å™¨çš„æ¨¡å‹ä¹‹é—´çš„åˆ†æ•°å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œå½“ä»Šç”¨äºè¯„ä¼°ç¿»è¯‘æ¨¡å‹çš„æœ€å¸¸ç”¨æŒ‡æ ‡æ˜¯ [SacreBLEU](https://github.com/mjpost/sacrebleu) ï¼Œå®ƒé€šè¿‡æ ‡å‡†åŒ–çš„åˆ†è¯æ­¥éª¤è§£å†³äº†è¿™ä¸ªç¼ºç‚¹ï¼ˆå’Œå…¶ä»–çš„ä¸€äº›ç¼ºç‚¹ï¼‰ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… SacreBLEU åº“ï¼š

```py
!pip install sacrebleu
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°±åƒåœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) é‚£æ ·é€šè¿‡ `evaluate.load()` åŠ è½½å®ƒ 

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

SacreBLEU æŒ‡æ ‡ä¸­å¾…è¯„ä¼°çš„é¢„æµ‹å’Œå‚è€ƒçš„ç›®æ ‡è¯‘æ–‡è¾“å…¥çš„æ ¼å¼éƒ½æ˜¯æ–‡æœ¬ã€‚å®ƒçš„è®¾è®¡æ˜¯ä¸ºäº†æ”¯æŒå¤šä¸ªå‚è€ƒç¿»è¯‘ï¼Œå› ä¸ºåŒä¸€å¥è¯é€šå¸¸æœ‰å¤šç§å¯æ¥å—çš„ç¿»è¯‘â€”â€”è™½ç„¶æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†åªæä¾›ä¸€ä¸ªï¼Œä½†åœ¨ NLP ä¸­æ‰¾åˆ°å°†å¤šä¸ªå¥å­ä½œä¸ºæ ‡ç­¾çš„æ•°æ®é›†æ˜¯å¾ˆå¸¸è§çš„ã€‚å› æ­¤ï¼Œé¢„æµ‹ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨ï¼Œè€Œå‚è€ƒåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨çš„åˆ—è¡¨ã€‚

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªä¾‹å­ï¼š

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

è¾¾åˆ°äº† 46.75 çš„ BLEU åˆ†æ•°ï¼Œè¿™æ˜¯ç›¸å½“ä¸é”™çš„â€”â€”ä½œä¸ºå‚è€ƒï¼ŒåŸå§‹ Transformer æ¨¡å‹åœ¨ [â€œAttention Is All You Needâ€ è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf) ç±»ä¼¼çš„è‹±è¯­å’Œæ³•è¯­ç¿»è¯‘ä»»åŠ¡ä¸­è·å¾—äº† 41.8 çš„ BLEU åˆ†æ•°ï¼ï¼ˆå…³äºå…¶ä»–æŒ‡æ ‡çš„å«ä¹‰ï¼Œä¾‹å¦‚ `counts` å’Œ `bp` ï¼Œå¯ä»¥å‚è§ [SacreBLEUä»“åº“](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74) ï¼‰å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•å°†ç¿»è¯‘æ¨¡å‹ä¸­ç»å¸¸å‡ºç°çš„ä¸¤ç§ç³Ÿç³•çš„é¢„æµ‹ç±»å‹ï¼ˆå¤§é‡é‡å¤æˆ–å¤ªçŸ­ï¼‰è¾“å…¥ç»™æŒ‡æ ‡è®¡ç®—çš„å‡½æ•°ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ç›¸å½“ç³Ÿç³•çš„ BLEU åˆ†æ•°ï¼š

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

åˆ†æ•°å¯ä»¥ä» 0 åˆ° 100ï¼Œè¶Šé«˜è¶Šå¥½ã€‚

{#if fw === 'tf'}

ä¸ºäº†å°†æ¨¡å‹çš„è¾“å‡ºè½¬åŒ–ä¸ºè¯„ä¼°æŒ‡æ ‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚å› ä¸º tokenizer ä¼šè‡ªåŠ¨å¤„ç†å¡«å…… `tokens`ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ‰€æœ‰æ ‡ç­¾ä¸­çš„ å¡«å……çš„`-100` ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šæ¥æ”¶ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šè®¡ç®— BLEU æŒ‡æ ‡ã€‚

æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ä¸ªæ˜¾è‘—æå‡æ€§èƒ½çš„æŠ€å·§ - ä½¿ç”¨ [XLA](https://www.tensorflow.org/xla) ï¼ŒTensorFlow çš„çº¿æ€§ä»£æ•°åŠ é€Ÿç¼–è¯‘å™¨ï¼Œç¼–è¯‘æˆ‘ä»¬çš„ç”Ÿæˆä»£ç ã€‚XLA å¯¹æ¨¡å‹çš„è®¡ç®—å›¾è¿›è¡Œäº†å„ç§ä¼˜åŒ–ï¼Œä»è€Œæ˜¾è‘—æå‡äº†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚å¦‚ Hugging Face çš„ [åšå®¢](https://huggingface.co/blog/tf-xla-generate) æ‰€è¿°ï¼Œå½“æˆ‘ä»¬çš„è¾“å…¥å½¢çŠ¶æ¯”è¾ƒæ•´é½æ—¶ï¼ŒXLA èƒ½å¤Ÿå¾ˆå¥½åœ°æé«˜è®¡ç®—æ•ˆç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç”¨å¡«å……æ•´ç†å™¨åˆ¶ä½œä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒæŠŠè¾“å…¥è¡¥é½åˆ° 128 çš„å€æ•°ï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨ `@tf.function(jit_compile=True)` è£…é¥°å™¨è£…é¥°æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°ï¼Œè¿™å°†æŠŠæ•´ä¸ªå‡½æ•°æ ‡è®°ä¸ºä½¿ç”¨ XLA ç¼–è¯‘ã€‚

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

ä¸ºäº†å°†æ¨¡å‹çš„è¾“å‡ºè½¬åŒ–ä¸ºè¯„ä¼°æŒ‡æ ‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚å› ä¸º tokenizer ä¼šè‡ªåŠ¨å¤„ç†å¡«å……çš„ tokensï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ ‡ç­¾ä¸­çš„æ‰€æœ‰ `-100` tokenï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # å¦‚æœæ¨¡å‹è¿”å›çš„å†…å®¹è¶…è¿‡äº†é¢„æµ‹çš„logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # ç”±äºæˆ‘ä»¬æ— æ³•è§£ç  -100,å› æ­¤å°†æ ‡ç­¾ä¸­çš„ -100 æ›¿æ¢æ‰
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ä¸€äº›ç®€å•çš„åå¤„ç†
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

ç°åœ¨è¿™å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼

### å¾®è°ƒæ¨¡å‹ [[å¾®è°ƒæ¨¡å‹]]

ç¬¬ä¸€æ­¥æ˜¯ç™»å½• Hugging Faceï¼Œè¿™æ ·ä½ å°±å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ç»“æœä¸Šä¼ åˆ° Hubä¸­ã€‚æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©ä½ åœ¨ notebook ä¸­å®Œæˆç™»é™†ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ notebook ä¸Šè¿è¡Œä»£ç ï¼Œå¯ä»¥åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
huggingface-cli login
```

{#if fw === 'tf'}

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ•ˆæœæ€ä¹ˆæ ·ã€‚
```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

ç°åœ¨å¯ä»¥å‡†å¤‡ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹äº†ã€‚è¿è¡Œ `tf.keras.mixed_precision.set_global_policy("mixed_ï¬‚oat16")` åï¼Œ Keras å°†ä½¿ç”¨ ï¬‚oat16 ç²¾åº¦è¿›è¡Œè®­ç»ƒï¼Œè¿™æ ·å¯ä»¥æ˜¾è‘—æé«˜æ”¯æŒåŠç²¾åº¦ GPUï¼ˆNvidia 20xx/V100 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰çš„è®­ç»ƒé€Ÿåº¦

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚

num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

æ¥ä¸‹æ¥å°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ 2 èŠ‚](/course/chapter7/2) ä¸­å­¦åˆ°çš„ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª `PushToHubCallback` å›è°ƒå‡½æ•°ï¼Œå¹¶æ‹Ÿåˆæ¨¡å‹çš„æ—¶å€™æ·»åŠ è¯¥å›è°ƒå‡½æ•°ï¼Œè¿™æ ·å°±å¯ä»¥è®­ç»ƒæœŸé—´å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸Šä¼ åˆ° Hubã€‚

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„æ¨¡å‹ä»“åº“çš„åç§°ï¼ˆå½“ä½ æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œå°±å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `Seq2SeqTrainingArguments`æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·é‡Œï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­å®ƒæ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"` ã€‚

<Tip>

ğŸ’¡å¦‚æœæ­£åœ¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œåˆ™å®ƒåº”è¯¥æ˜¯ç›®æ ‡æ¨é€ä»“åº“çš„åœ¨æœ¬åœ°å…‹éš†åœ¨æœ¬åœ°çš„ç‰ˆæœ¬ã€‚å¦‚æœä¸æ˜¯ï¼Œå½“è°ƒç”¨ `model.fit()` æ—¶ä¼šæ”¶åˆ°é”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„è·¯å¾„ã€‚

</Tip>

æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒç»“æŸåæˆ‘ä»¬çš„æ¨¡å‹çš„ BLEU çš„åˆ†æ•°ï¼š

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚æ­å–œä½ ï¼Œä½ å·²ç»æˆåŠŸåœ°åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼

{:else}

å®Œæˆè¿™äº›æ­¥éª¤ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `Seq2SeqTrainingArguments` äº†ã€‚ä¸ `Trainer` ä¸€æ ·ï¼Œå®ƒæ˜¯ `TrainingArguments` çš„å­ç±»ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šå¯ä»¥è®¾ç½®çš„å­—æ®µï¼š

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

é™¤äº†é€šå¸¸çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€æ‰¹æ¬¡å¤§å°å’Œä¸€äº›æƒé‡è¡°å‡ï¼‰ä¹‹å¤–ï¼Œè¿™é‡Œçš„éƒ¨åˆ†å‚æ•°ä¸æˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚çœ‹åˆ°çš„æœ‰ä¸€äº›ä¸åŒï¼š

- æˆ‘ä»¬æ²¡æœ‰è®¾ç½®å®šæœŸè¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºè¯„ä¼°éœ€è¦è€—è´¹ä¸€å®šçš„æ—¶é—´ï¼›æˆ‘ä»¬å°†åªåœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰å’Œç»“æŸä¹‹åè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ä¸€æ¬¡ã€‚
- æˆ‘ä»¬è®¾ç½® `fp16=True` ï¼Œè¿™å¯ä»¥åŠ å¿«åœ¨æ”¯æŒ fp16 çš„ GPU ä¸Šçš„è®­ç»ƒé€Ÿåº¦ã€‚
- å’Œä¹‹å‰æˆ‘ä»¬è®¨è®ºçš„ä¸€æ ·ï¼Œæˆ‘ä»¬è®¾ç½® `predict_with_generate=True` ã€‚
- æˆ‘ä»¬è®¾ç½®äº† `push_to_hub=True` ï¼Œåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubã€‚

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“ä½ æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œå°±å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `Seq2SeqTrainingArguments` æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­å®ƒæ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"` ã€‚

<Tip>

ğŸ’¡å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œåˆ™å®ƒåº”è¯¥æ˜¯æ¨é€çš„ä»“åº“å…‹éš†åœ¨æœ¬åœ°çš„ç‰ˆæœ¬ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å°†åœ¨å®šä¹‰ä½ çš„ `Seq2SeqTrainer` åç§°æ—¶ä¼šé‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</Tip>

æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Seq2SeqTrainer` ï¼š

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæŸ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹ç›®å‰çš„ BLEU åˆ†æ•°ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„å¾®è°ƒå¹¶æœªä½¿æƒ…å†µå˜å¾—æ›´ç³Ÿã€‚è¿™ä¸ªå‘½ä»¤éœ€è¦ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨æ‰§è¡ŒæœŸé—´å»å–æ¯å’–å•¡ï¼š

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

BLEU å¾—åˆ†ä¸º 39 å¹¶ä¸ç®—å¤ªå·®ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬çš„æ¨¡å‹å·²ç»æ“…é•¿å°†è‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­å¥å­ã€‚

æ¥ä¸‹æ¥æ˜¯è®­ç»ƒï¼Œè¿™ä¹Ÿéœ€è¦ä¸€äº›æ—¶é—´ï¼š

```python
trainer.train()
```

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯å½“ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å†æ¬¡è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å¸Œæœ›æˆ‘ä»¬ä¼šçœ‹åˆ° BLEU åˆ†æ•°æœ‰æ‰€æé«˜ï¼

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

å¯ä»¥çœ‹åˆ°è¿‘ 14 ç‚¹çš„æ”¹è¿›ï¼Œè¿™å¾ˆæ£’ï¼

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` æ–¹æ³•æ¥ç¡®ä¿æˆ‘ä»¬ä¸Šä¼ äº†æ¨¡å‹æœ€æ–°çš„ç‰ˆæœ¬ã€‚ `Trainer` è¿˜åˆ›å»ºäº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡ç‰‡å¹¶ä¸Šä¼ åˆ° Hub ã€‚è¿™ä¸ªæ¨¡å‹å¡ç‰‡åŒ…å«äº†å¯ä»¥å¸®åŠ© Hub ä¸ºæ¨ç†æ¼”ç¤ºé€‰æ‹©å°éƒ¨ä»¶çš„å…ƒæ•°æ®ï¼Œé€šå¸¸æƒ…å†µä¸‹æˆ‘ä»¬ä¸éœ€è¦åšé¢å¤–çš„æ›´æ”¹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»æ¨¡å‹ç±»ä¸­æ¨æ–­å‡ºæ­£ç¡®çš„å°éƒ¨ä»¶ï¼Œä½†åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå®ƒåªèƒ½é€šè¿‡æ¨¡å‹ç±»æ¨æ–­è¿™æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬è¡¥å……ä¸€ä¸‹å…·ä½“çš„æ¨¡å‹ç±»åˆ«ã€‚

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

å¦‚æœä½ æƒ³æ£€æŸ¥å‘½ä»¤æ‰§è¡Œçš„ç»“æœï¼Œæ­¤å‘½ä»¤å°†è¿”å›å®ƒåˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼Œä½ å¯ä»¥æ‰“å¼€ url è¿›è¡Œæ£€æŸ¥ï¼š

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

åœ¨æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨ Model Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹ï¼Œå¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œäº†æ¨¡å‹çš„å¾®è°ƒï¼Œæ­å–œä½ ï¼

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

{/if}

{#if fw === 'pt'}

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ [[è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯]]

æˆ‘ä»¬ç°åœ¨æ¥çœ‹ä¸€ä¸‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾å®šåˆ¶ä½ éœ€è¦çš„éƒ¨åˆ†ã€‚å®ƒå°†ä¸æˆ‘ä»¬åœ¨ [ç¬¬ 2 èŠ‚](https://chat.openai.com/course/chapter7/2) å’Œ [ç¬¬ 3 èŠ‚](https://chat.openai.com/course/chapter3/4) ä¸­åšçš„éå¸¸ç›¸ä¼¼ã€‚

### å‡†å¤‡è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡ [[å‡†å¤‡è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡]]

ç”±äºè¿™é‡Œçš„æ­¥éª¤åœ¨ä¹‹å‰çš„ç« èŠ‚å·²ç»å‡ºç°è¿‡å¾ˆå¤šæ¬¡ï¼Œå› æ­¤è¿™é‡Œåªåšç®€ç•¥è¯´æ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®é›†è®¾ç½®ä¸º `torch` æ ¼å¼ï¼Œè¿™æ ·å¯ä»¥å°†æ•°æ®é›†çš„æ ¼å¼è½¬æ¢ä¸º `PyTorch` å¼ é‡ï¼Œç„¶åæˆ‘ä»¬ç”¨æ•°æ®é›†æ„å»º `DataLoader` ï¼š

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šç»§ç»­ä¸Šä¸€èŠ‚çš„å¾®è°ƒï¼Œè€Œæ˜¯å†æ¬¡ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é‡æ–°è®­ç»ƒï¼š

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

å‡†å¤‡å¥½è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ä¸­ã€‚è¯·è®°ä½ï¼Œå¦‚æœä½ æƒ³åœ¨ Colab Notebook ä¸Šä½¿ç”¨ TPU è¿›è¡Œè®­ç»ƒï¼Œä½ éœ€è¦å°†æ‰€æœ‰è¿™äº›ä»£ç ç§»åŠ¨åˆ°ä¸€ä¸ªè®­ç»ƒå‡½æ•°ä¸­ï¼Œå¹¶ä¸”è¿™ä¸ªè®­ç»ƒå‡½æ•°ä¸åº”è¯¥åŒ…å«å®ä¾‹åŒ– `Accelerator` `çš„ä»£ç ã€‚æ¢å¥è¯è¯´ï¼ŒAccelerator` çš„å®ä¾‹åŒ–åº”è¯¥åœ¨è¿™ä¸ªå‡½æ•°ä¹‹å¤–è¿›è¡Œã€‚è¿™ä¹ˆåšçš„åŸå› æ˜¯ï¼ŒTPU åœ¨ Colab ä¸­çš„å·¥ä½œæ–¹å¼æœ‰äº›ç‰¹æ®Šã€‚TPU è¿è¡Œæ—¶ä¼šé‡æ–°æ‰§è¡Œæ•´ä¸ªå•å…ƒæ ¼çš„ä»£ç ï¼Œå› æ­¤å¦‚æœ `Accelerator` çš„å®ä¾‹åŒ–åœ¨è®­ç»ƒå‡½æ•°å†…éƒ¨ï¼Œå®ƒå¯èƒ½ä¼šè¢«å¤šæ¬¡å®ä¾‹åŒ–ï¼Œå¯¼è‡´é”™è¯¯ã€‚

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ä¸­äº†ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆåœ¨å‡†å¤‡å¥½æ•°æ®åŠ è½½å™¨åå†æ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºæ›´æ”¹æ•°æ®åŠ è½½å™¨ä¼šæ”¹å˜ `DataLoader` çš„é•¿åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hugging Face Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ å°šæœªç™»å½• Hugging Faceï¼Œè¯·å…ˆè¿›è¡Œç™»å½•ã€‚æˆ‘ä»¬å°†æ ¹æ®æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ã€‚ä½ å¯ä»¥ä½¿ç”¨è‡ªå·±é€‰æ‹©çš„åç§°æ›¿æ¢ `repo_name`ï¼Œä½†è¯·ç¡®ä¿åŒ…å«ä½ çš„ç”¨æˆ·åã€‚å¦‚æœä½ ä¸ç¡®å®šå½“å‰çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨` get_full_repo_name()` å‡½æ•°æ¥æŸ¥çœ‹ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¹ä¸­å…‹éš†è¯¥å­˜å‚¨åº“ã€‚å¦‚æœå·²ç»å­˜åœ¨ä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„å­˜å‚¨åº“å…‹éš†åˆ°æœ¬åœ°çš„ç‰ˆæœ¬ï¼š

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ æˆ‘ä»¬åœ¨ `output_dir` ä¸­ä¿å­˜çš„æ‰€æœ‰æ–‡ä»¶ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

### è®­ç»ƒå¾ªç¯ [[è®­ç»ƒå¾ªç¯]]

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ª `postprocess()` å‡½æ•°ç”¨äºæ¥æ”¶é¢„æµ‹å€¼å’Œå‚è€ƒç¿»è¯‘å¯¹äºçš„æ ‡ç­¾å€¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º `metric` å¯¹è±¡æ‰€éœ€è¦çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚ï¼š

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # æ›¿æ¢æ ‡ç­¾ä¸­çš„ -100,å› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬ã€‚
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ä¸€äº›ç®€å•çš„åå¤„ç†
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

è®­ç»ƒå¾ªç¯çœ‹èµ·æ¥å’Œæœ¬ç«  [ç¬¬ 2 èŠ‚](/course/chapter7/2) ä¸ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­ä»£ç å¾ˆç›¸ä¼¼ï¼Œåªæ˜¯åœ¨è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›ä¸åŒ â€”â€” æ‰€ä»¥è®©æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸€ä¸‹è¿™ä¸€ç‚¹ï¼

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ `generate()` æ–¹æ³•æ¥è®¡ç®—é¢„æµ‹ï¼Œä½†è¿™æ˜¯æˆ‘ä»¬åŸºç¡€æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œè€Œä¸æ˜¯ğŸ¤— Accelerate åœ¨ `prepare()` æ–¹æ³•ä¸­åˆ›å»ºçš„å°è£…æ¨¡å‹ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é¦–å…ˆ `unwrap_model` ï¼Œç„¶åè°ƒç”¨æ­¤æ–¹æ³•ã€‚

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ç”¨æ¥è®¡ç®—é¢„æµ‹çš„ `generate()` å‡½æ•°æ˜¯åŸºç¡€æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œè€Œä¸æ˜¯ğŸ¤— Accelerate åœ¨ `prepare()` å‡½æ•°ä¸­åˆ›å»ºçš„å°è£…æ¨¡å‹ã€‚è¿™å°±æ˜¯åœ¨è°ƒç”¨æ­¤å‡½æ•°ä¹‹å‰å…ˆè°ƒç”¨`unwrap_model`ï¼Œçš„åŸå› ã€‚

ç¬¬äºŒä¸ªè¦æ³¨æ„çš„æ˜¯ï¼Œå°±åƒ [token åˆ†ç±»](https://chat.openai.com/course/chapter7/2) ä¸€æ ·ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°è¿™ä¸¤ä¸ªè¿‡ç¨‹å¯èƒ½ä»¥ä¸åŒçš„å½¢çŠ¶å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œäº†å¡«å……ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è°ƒç”¨ `gather()` å‡½æ•°ä¹‹å‰ä½¿ç”¨ `accelerator.pad_across_processes()` æ–¹æ³•ï¼Œä½¿é¢„æµ‹å’Œæ ‡ç­¾å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™ä¹ˆåšï¼Œé‚£ä¹ˆè¯„ä¼°çš„è¿‡ç¨‹å°†ä¼šå‡ºé”™æˆ–è¢«æ°¸è¿œæŒ‚èµ·ã€‚

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # éœ€è¦å¡«å……é¢„æµ‹å’Œæ ‡ç­¾æ‰èƒ½è°ƒç”¨gather()
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # ä¿å­˜å’Œä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

è®­ç»ƒå®Œæˆä¹‹åï¼Œä½ å°±æœ‰äº†ä¸€ä¸ªæ¨¡å‹ï¼Œæœ€ç»ˆçš„ BLEU åˆ†æ•°åº”è¯¥ä¸ `Seq2SeqTrainer` è®­ç»ƒçš„æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚ä½ å¯ä»¥åœ¨ [huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate) ä¸ŠæŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨æ­¤ä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³æµ‹è¯•å¯¹è®­ç»ƒå¾ªç¯çš„ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢çš„ä»£ç æ¥å®ç°ï¼

{/if}

## ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ [[ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹]]

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ã€‚è¦åœ¨æœ¬åœ°çš„ `pipeline` ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼š

```py
from transformers import pipeline

# å°†å…¶æ›¿æ¢æˆä½ è‡ªå·±çš„ checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

å’Œé¢„æƒ³çš„ä¸€æ ·ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹é€‚åº”äº†æˆ‘ä»¬å¾®è°ƒå®ƒçš„è¯­æ–™åº“ï¼Œæ²¡æœ‰ä¿ç•™è‹±è¯­å•è¯â€œthreadsâ€ï¼Œè€Œæ˜¯å°†å®ƒç¿»è¯‘æˆå®˜æ–¹çš„æ³•è¯­ç‰ˆæœ¬ã€‚å¯¹äºâ€œpluginâ€ä¹Ÿæ˜¯å¦‚æ­¤ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

è¿™æ˜¯å¦ä¸€ä¸ªé¢†åŸŸé€‚åº”çš„å¥½ä¾‹å­ï¼

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** æŠŠä¹‹å‰æ‰¾åˆ°çš„åŒ…å«å•è¯â€œemailâ€æ ·æœ¬è¾“å…¥æ¨¡å‹ï¼Œä¼šè¿”å›ä»€ä¹ˆç»“æœï¼Ÿ

</Tip>
