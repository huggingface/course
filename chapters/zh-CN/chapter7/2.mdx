<FrameworkSwitchCourse {fw} />

# Token åˆ†ç±»

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
]} />

{/if}

æˆ‘ä»¬å°†æ¢ç´¢çš„ç¬¬ä¸€ä¸ªåº”ç”¨æ˜¯Tokenåˆ†ç±»ã€‚è¿™ä¸ªé€šç”¨ä»»åŠ¡åŒ…æ‹¬ä»»ä½•å¯ä»¥è¡¨è¿°ä¸ºâ€œä¸ºå¥å­ä¸­çš„è¯æˆ–å­—åˆ†é…æ ‡ç­¾â€çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **å®ä½“å‘½åè¯†åˆ« (NER)**: æ‰¾å‡ºå¥å­ä¸­çš„å®ä½“ï¼ˆå¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªå®ä½“æˆ–â€œæ— å®ä½“â€æŒ‡å®šä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾ã€‚
- **è¯æ€§æ ‡æ³¨ (POS)**: å°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸ºå¯¹åº”äºç‰¹å®šçš„è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚
- **åˆ†å—ï¼ˆchunkingï¼‰**: æ‰¾åˆ°å±äºåŒä¸€å®ä½“çš„Tokenã€‚è¿™ä¸ªä»»åŠ¡(å¯ç»“åˆPOSæˆ–NER)å¯ä»¥ä»»ä½•å°†ä¸€å—Tokenä½œä¸ºåˆ¶å®šä¸€ä¸ªæ ‡ç­¾(é€šå¸¸æ˜¯B -),å¦ä¸€ä¸ªæ ‡ç­¾(é€šå¸¸I -)è¡¨ç¤ºTokenæ˜¯å¦æ˜¯åŒä¸€å—,å’Œç¬¬ä¸‰ä¸ªæ ‡ç­¾(é€šå¸¸æ˜¯O)è¡¨ç¤ºTokenä¸å±äºä»»ä½•å—ã€‚ä¹Ÿå°±æ˜¯æ ‡å‡ºå¥å­ä¸­çš„çŸ­è¯­å—ï¼Œä¾‹å¦‚åè¯çŸ­è¯­ï¼ˆNPï¼‰ï¼ŒåŠ¨è¯çŸ­è¯­ï¼ˆVPï¼‰ç­‰ã€‚

<Youtube id="wVHdVlPScxA"/>

å½“ç„¶ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–ç±»å‹çš„tokenåˆ†ç±»é—®é¢˜ï¼›è¿™äº›åªæ˜¯å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ NER ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ (BERT)ï¼Œç„¶åè¯¥æ¨¡å‹å°†èƒ½å¤Ÿè®¡ç®—å¦‚ä¸‹é¢„æµ‹ï¼š

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-bert-finetuned-ner-darkmode.hf.space" frameBorder="0" height="350" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

æ‚¨å¯ä»¥[åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hubçš„æ¨¡å‹ï¼Œå¯ä»¥å°è¯•è¾“å…¥ä¸€äº›å¥å­çœ‹çœ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

## å‡†å¤‡æ•°æ®

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆæ ‡è®°åˆ†ç±»çš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[CoNLL-2003 æ•°æ®é›†](https://huggingface.co/datasets/conll2003), å…¶ä¸­åŒ…å«æ¥è‡ªè·¯é€ç¤¾çš„æ–°é—»æŠ¥é“ã€‚

<Tip>

ğŸ’¡ åªè¦æ‚¨çš„æ•°æ®é›†ç”±å¸¦æœ‰ç›¸åº”æ ‡ç­¾çš„åˆ†å‰²æˆå•è¯å¹¶çš„æ–‡æœ¬ç»„æˆï¼Œæ‚¨å°±èƒ½å¤Ÿå°†è¿™é‡Œæè¿°çš„æ•°æ®å¤„ç†è¿‡ç¨‹åº”ç”¨åˆ°æ‚¨è‡ªå·±çš„æ•°æ®é›†ã€‚å¦‚æœéœ€è¦å¤ä¹ å¦‚ä½•åœ¨.Datasetä¸­åŠ è½½è‡ªå®šä¹‰æ•°æ®ï¼Œè¯·å‚é˜…[Chapter 5](/course/chapter5)ã€‚

</Tip>

### CoNLL-2003 æ•°æ®é›†

è¦åŠ è½½ CoNLL-2003 æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ æ¥è‡ª ğŸ¤— Datasets åº“çš„**load_dataset()** æ–¹æ³•ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

è¿™å°†ä¸‹è½½å¹¶ç¼“å­˜æ•°æ®é›†ï¼Œå°±åƒå’Œæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3) åŠ è½½GLUE MRPC æ•°æ®é›†ä¸€æ ·ã€‚æ£€æŸ¥è¿™ä¸ªå¯¹è±¡å¯ä»¥è®©æˆ‘ä»¬çœ‹åˆ°å­˜åœ¨å“ªäº›åˆ—ï¼Œä»¥åŠè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´æ˜¯å¦‚ä½•åˆ†å‰²çš„:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®é›†åŒ…å«æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ä¸‰ä¸ªä»»åŠ¡çš„æ ‡ç­¾ï¼šNERã€POS å’Œchunkingã€‚ä¸å…¶ä»–æ•°æ®é›†çš„ä¸€ä¸ªå¾ˆå¤§åŒºåˆ«æ˜¯è¾“å…¥æ–‡æœ¬ä¸æ˜¯ä½œä¸ºå¥å­æˆ–æ–‡æ¡£å‘ˆç°çš„ï¼Œè€Œæ˜¯å•è¯åˆ—è¡¨ï¼ˆæœ€åä¸€åˆ—ç§°ä¸º **tokens** ï¼Œä½†å®ƒåŒ…å«çš„æ˜¯è¿™äº›è¯æ˜¯é¢„å…ˆæ ‡è®°åŒ–çš„è¾“å…¥ï¼Œä»ç„¶éœ€è¦é€šè¿‡æ ‡è®°å™¨è¿›è¡Œå­è¯æ ‡è®°ï¼‰ã€‚

æˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

ç”±äºæˆ‘ä»¬è¦æ‰§è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ NER æ ‡ç­¾ï¼š

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

è¿™ä¸€åˆ—æ˜¯ç±»æ ‡ç­¾çš„åºåˆ—ã€‚å…ƒç´ çš„ç±»å‹åœ¨ner_featureçš„featureå±æ€§ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥ç‰¹æ€§çš„nameså±æ€§æ¥è®¿é—®åç§°åˆ—è¡¨:   

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

å› æ­¤ï¼Œè¿™ä¸€åˆ—åŒ…å«çš„å…ƒç´ æ˜¯ClassLabelsçš„åºåˆ—ã€‚åºåˆ—å…ƒç´ çš„ç±»å‹åœ¨`ner_feature`çš„`feature`ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥`feature`çš„`names`å±æ€§æ¥è®¿é—®åç§°åˆ—è¡¨:

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

æˆ‘ä»¬åœ¨[ç¬¬å…­ç« ](/course/chapter6/3), æ·±å…¥ç ”ç©¶**token-classification** ç®¡é“æ—¶å·²ç»çœ‹åˆ°äº†è¿™äº›æ ‡ç­¾ ï¼Œä½†ä¸ºäº†å¿«é€Ÿå¤ä¹ ï¼š

- `O` è¡¨ç¤ºè¿™ä¸ªè¯ä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚
- `B-PER`/`I-PER`æ„å‘³ç€è¿™ä¸ªè¯å¯¹åº”äºäººåå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-ORG`/`I-ORG` çš„æ„æ€æ˜¯è¿™ä¸ªè¯å¯¹åº”äºç»„ç»‡åç§°å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-LOC`/`I-LOC` æŒ‡çš„æ˜¯æ˜¯è¿™ä¸ªè¯å¯¹åº”äºåœ°åå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-MISC`/`I-MISC` è¡¨ç¤ºè¯¥è¯å¯¹åº”äºä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚

ç°åœ¨è§£ç æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ ‡ç­¾ï¼š

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

ä¾‹å¦‚æ··åˆ **B-** å’Œ **I-** æ ‡ç­¾ï¼Œè¿™æ˜¯ç›¸åŒçš„ä»£ç åœ¨ç´¢å¼• 4 çš„è®­ç»ƒé›†å…ƒç´ ä¸Šçš„é¢„æµ‹ç»“æœï¼š

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè·¨è¶Šä¸¤ä¸ªå•è¯çš„å®ä½“ï¼Œå¦‚â€œEuropean Unionâ€å’Œâ€œWerner Zwingmannâ€ï¼Œæ¨¡å‹ä¸ºç¬¬ä¸€ä¸ªå•è¯æ ‡æ³¨äº†ä¸€ä¸ªB-æ ‡ç­¾ï¼Œä¸ºç¬¬äºŒä¸ªå•è¯æ ‡æ³¨äº†ä¸€ä¸ªI-æ ‡ç­¾ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** ä½¿ç”¨ POS æˆ–chunkingæ ‡ç­¾è¯†åˆ«åŒä¸€ä¸ªå¥å­ã€‚

</Tip>

### å¤„ç†æ•°æ®

<Youtube id="iY2AZYdZAr0"/>

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬éœ€è¦è½¬æ¢ä¸ºToken IDï¼Œç„¶åæ¨¡å‹æ‰èƒ½ç†è§£å®ƒä»¬ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬å…­ç« ](/course/chapter6/)æ‰€å­¦çš„é‚£æ ·ã€‚ä¸è¿‡åœ¨æ ‡è®°ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªå¾ˆå¤§çš„åŒºåˆ«æ˜¯æˆ‘ä»¬æœ‰pre-tokenizedçš„è¾“å…¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œtokenizer APIå¯ä»¥å¾ˆå®¹æ˜“åœ°å¤„ç†è¿™ä¸ªé—®é¢˜;æˆ‘ä»¬åªéœ€è¦ç”¨ä¸€ä¸ªç‰¹æ®Šçš„tokenizerã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»º`tokenizer`å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»ä¸‹è½½å¹¶ç¼“å­˜å…³è”çš„åˆ†è¯å™¨å¼€å§‹ï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ä½ å¯ä»¥æ›´æ¢æŠŠ `model_checkpoint` æ›´æ¢ä¸º [Hub](https://huggingface.co/models),ä¸Šæ‚¨å–œæ¬¢çš„ä»»ä½•å…¶ä»–å‹å·ï¼Œæˆ–ä½¿ç”¨æ‚¨æœ¬åœ°ä¿å­˜çš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯åˆ†è¯å™¨éœ€è¦ç”± ğŸ¤— Tokenizers åº“æ”¯æŒï¼Œæœ‰ä¸€ä¸ªâ€œå¿«é€Ÿâ€ç‰ˆæœ¬å¯ç”¨ã€‚ä½ å¯ä»¥åœ¨[è¿™å¼ å¤§è¡¨](https://huggingface.co/transformers/#supported-frameworks), ä¸Šçœ‹åˆ°æ‰€æœ‰å¸¦æœ‰å¿«é€Ÿç‰ˆæœ¬çš„æ¶æ„ï¼Œæˆ–è€…æ£€æŸ¥  æ‚¨å¯ä»¥é€šè¿‡æŸ¥çœ‹å®ƒ`is_fast` å±æ€§æ¥æ£€æµ‹æ­£åœ¨ä½¿ç”¨çš„`tokenizer`å¯¹è±¡æ˜¯å¦ç”± ğŸ¤— Tokenizers æ”¯æŒï¼š

```py
tokenizer.is_fast
```

```python out
True
```

è¦å¯¹é¢„å…ˆæ ‡è®°çš„è¾“å…¥è¿›è¡Œæ ‡è®°ï¼Œæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨æˆ‘ä»¬çš„`tokenizer` åªéœ€æ·»åŠ  `is_split_into_words=True`:

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåˆ†è¯å™¨æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®ŠToken(`[CLS]` åœ¨å¼€å§‹å’Œ`[SEP]` æœ€å) è€Œå¤§å¤šæ•°å•è¯æœªè¢«ä¿®æ”¹ã€‚ç„¶è€Œï¼Œå•è¯ `lamb`,è¢«åˆ†ä¸ºä¸¤ä¸ªå­å•è¯ `la` and `##mb`ã€‚è¿™å¯¼è‡´äº†è¾“å…¥å’Œæ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…:æ ‡ç­¾åˆ—è¡¨åªæœ‰9ä¸ªå…ƒç´ ï¼Œè€Œæˆ‘ä»¬çš„è¾“å…¥ç°åœ¨æœ‰12ä¸ªtoken ã€‚è®¡ç®—ç‰¹æ®ŠTokenå¾ˆå®¹æ˜“(æˆ‘ä»¬çŸ¥é“å®ƒä»¬åœ¨å¼€å¤´å’Œç»“å°¾)ï¼Œä½†æˆ‘ä»¬è¿˜éœ€è¦ç¡®ä¿æ‰€æœ‰æ ‡ç­¾ä¸é€‚å½“çš„å•è¯å¯¹é½ã€‚
å¹¸è¿çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å¿«é€Ÿåˆ†è¯å™¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è®¿é—®ğŸ¤— Tokenizersè¶…èƒ½åŠ›ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†æ¯ä¸ªä»¤ç‰Œæ˜ å°„åˆ°å…¶ç›¸åº”çš„å•è¯ï¼ˆå¦‚[Chapter 6](/course/chapter6/3)):

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

é€šè¿‡ä¸€ç‚¹ç‚¹å·¥ä½œï¼Œæˆ‘ä»¬å¯ä»¥æ‰©å±•æˆ‘ä»¬çš„æ ‡ç­¾åˆ—è¡¨ä»¥åŒ¹é…token ã€‚æˆ‘ä»¬å°†åº”ç”¨çš„ç¬¬ä¸€æ¡è§„åˆ™æ˜¯ï¼Œç‰¹æ®Štoken çš„æ ‡ç­¾ä¸º `-100` ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ `-100` æ˜¯ä¸€ä¸ªåœ¨æˆ‘ä»¬å°†ä½¿ç”¨çš„æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚ç„¶åï¼Œæ¯ä¸ªtoken éƒ½ä¼šè·å¾—ä¸å…¶æ‰€åœ¨å•è¯çš„token ç›¸åŒçš„æ ‡ç­¾ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒä¸€å®ä½“çš„ä¸€éƒ¨åˆ†ã€‚å¯¹äºå•è¯å†…éƒ¨ä½†ä¸åœ¨å¼€å¤´çš„Tokenï¼Œæˆ‘ä»¬å°†`B-` æ›¿æ¢ä¸º `I-` (å› ä¸ºtoken ä¸ä»¥å®ä½“å¼€å¤´):

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€å¥è¯ä¸Šè¯•ä¸€è¯•ï¼š

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„å‡½æ•°ä¸ºå¼€å¤´å’Œç»“å°¾çš„ä¸¤ä¸ªç‰¹æ®Šæ ‡è®°æ·»åŠ äº†  `-100` ï¼Œå¹¶ä¸ºåˆ†æˆä¸¤ä¸ªæ ‡è®°çš„å•è¯æ·»åŠ äº†ä¸€ä¸ªæ–°çš„`0` ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** ä¸€äº›ç ”ç©¶äººå‘˜æ›´å–œæ¬¢æ¯ä¸ªè¯åªå½’å±ä¸€ä¸ªæ ‡ç­¾, å¹¶åˆ†é… `-100` ç»™å®šè¯ä¸­çš„å…¶ä»–å­æ ‡è®°ã€‚è¿™æ˜¯ä¸ºäº†é¿å…åˆ†è§£æˆå¤§é‡å­æ ‡è®°çš„é•¿è¯å¯¹æŸå¤±é€ æˆä¸¥é‡å½±å“ã€‚æŒ‰ç…§æ­¤è§„åˆ™æ›´æ”¹å‰ä¸€ä¸ªå‡½æ•°ä½¿æ ‡ç­¾ä¸è¾“å…¥idå¯¹é½ã€‚

</Tip>

ä¸ºäº†é¢„å¤„ç†æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦æ ‡è®°æ‰€æœ‰è¾“å…¥å¹¶åœ¨æ‰€æœ‰æ ‡ç­¾ä¸Šåº”ç”¨ `align_labels_with_tokens()` ã€‚ä¸ºäº†åˆ©ç”¨æˆ‘ä»¬çš„å¿«é€Ÿåˆ†è¯å™¨çš„é€Ÿåº¦ä¼˜åŠ¿ï¼Œæœ€å¥½åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå› æ­¤æˆ‘ä»¬å°†ç¼–å†™ä¸€ä¸ªå¤„ç†ç¤ºä¾‹åˆ—è¡¨çš„å‡½æ•°å¹¶ä½¿ç”¨å¸¦ `batched=True` æœ‰é€‰é¡¹çš„ `Dataset.map()`æ–¹æ³• .ä¸æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹å”¯ä¸€ä¸åŒçš„æ˜¯å½“åˆ†è¯å™¨çš„è¾“å…¥æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–è€…åƒä¾‹å­ä¸­çš„å•è¯åˆ—è¡¨ï¼‰æ—¶  `word_ids()` å‡½æ•°éœ€è¦è·å–æˆ‘ä»¬æƒ³è¦å•è¯çš„ç´¢å¼•çš„IDï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿæ·»åŠ å®ƒï¼š

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¡«å……æˆ‘ä»¬çš„è¾“å…¥ï¼›æˆ‘ä»¬ç¨åä¼šåœ¨ä½¿ç”¨æ•°æ®æ•´ç†å™¨åˆ›å»ºbatchæ—¶è¿™æ ·åšã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰é¢„å¤„ç†åº”ç”¨äºæ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†ï¼š

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

æˆ‘ä»¬å·²ç»å®Œæˆäº†æœ€éš¾çš„éƒ¨åˆ†ï¼ç°åœ¨æ•°æ®å·²ç»è¢«é¢„å¤„ç†äº†ï¼Œå®é™…çš„è®­ç»ƒçœ‹èµ·æ¥å¾ˆåƒæˆ‘ä»¬[ç¬¬ä¸‰ç« ](/course/chapter3)åšçš„.

{#if fw === 'pt'}

## ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹

ä½¿ç”¨ `Trainer` çš„å®é™…ä»£ç ä¼šå’Œä»¥å‰ä¸€æ ·ï¼›å”¯ä¸€çš„å˜åŒ–æ˜¯æ•°æ®æ•´ç†æˆæ—¶æ‰¹å¤„ç†çš„æ–¹å¼å’Œåº¦é‡è®¡ç®—å‡½æ•°ã€‚

{:else}

## ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹

ä½¿ç”¨Kerasçš„å®é™…ä»£ç å°†ä¸ä¹‹å‰éå¸¸ç›¸ä¼¼;å”¯ä¸€çš„å˜åŒ–æ˜¯å°†æ•°æ®æ•´ç†æˆæ‰¹å¤„ç†çš„æ–¹å¼å’ŒæŒ‡æ ‡è®¡ç®—å‡½æ•°ã€‚

{/if}


### æ•°æ®æ’åº

æˆ‘ä»¬ä¸èƒ½åƒ[ç¬¬ä¸‰ç« ](/course/chapter3)é‚£æ ·åªä½¿ç”¨ä¸€ä¸ª `DataCollatorWithPadding `å› ä¸ºè¿™åªä¼šå¡«å……è¾“å…¥ï¼ˆè¾“å…¥ IDã€æ³¨æ„æ©ç å’Œæ ‡è®°ç±»å‹ IDï¼‰ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çš„æ ‡ç­¾åº”è¯¥ä»¥ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„æ–¹å¼å¡«å……ï¼Œä»¥ä¾¿å®ƒä»¬ä¿æŒé•¿åº¦ç›¸åŒï¼Œä½¿ç”¨  `-100 ` ï¼Œè¿™æ ·åœ¨æŸå¤±è®¡ç®—ä¸­å°±å¯ä»¥å¿½ç•¥ç›¸åº”çš„é¢„æµ‹ã€‚

è¿™ä¸€åˆ‡éƒ½æ˜¯ç”±ä¸€ä¸ª [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification)å®Œæˆ.å®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰å¡«å……çš„æ•°æ®æ•´ç†å™¨å®ƒéœ€è¦  `tokenizer ` ç”¨äºé¢„å¤„ç†è¾“å…¥ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šæµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒé›†ä¸­çš„ç¤ºä¾‹åˆ—è¡¨ä¸Šè°ƒç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

è®©æˆ‘ä»¬å°†å…¶ä¸æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼š

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œç¬¬äºŒç»„æ ‡ç­¾çš„é•¿åº¦å·²ç»ä½¿ç”¨ `-100` å¡«å……åˆ°ä¸ç¬¬ä¸€ç»„æ ‡ç­¾ç›¸åŒã€‚

{:else}

æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å·²å‡†å¤‡å°±ç»ªï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨å®ƒæ¥åˆ¶ä½œä¸€ä¸ªå¸¦æœ‰`to_tf_dataset()`æ–¹æ³•çš„`tf.data.Dataset`ã€‚

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```


 Next stop: the model itself.

{/if}

{#if fw === 'tf'}

### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶Tokenåˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForTokenClassification` ç±»ã€‚å®šä¹‰è¿™ä¸ªæ¨¡å‹æ—¶è¦è®°ä½çš„ä¸»è¦äº‹æƒ…æ˜¯ä¼ é€’ä¸€äº›å…³äºæˆ‘ä»¬çš„æ ‡ç­¾æ•°é‡çš„ä¿¡æ¯ã€‚æ‰§è¡Œæ­¤æ“ä½œçš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå¾ˆå¥½çš„æ¨ç†å°éƒ¨ä»¶ï¼Œå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·ï¼Œæœ€å¥½è®¾ç½®æ­£ç¡®çš„æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚

å®ƒä»¬åº”è¯¥ç”±ä¸¤ä¸ªå­—å…¸è®¾ç½®ï¼Œ `id2label` å’Œ `label2id` ï¼Œå…¶ä¸­åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ï¼Œåä¹‹äº¦ç„¶ï¼š

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä¼ é€’ç»™ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°†åœ¨æ¨¡å‹çš„é…ç½®ä¸­è®¾ç½®ï¼Œç„¶åä¿å­˜å¹¶ä¸Šä¼ åˆ°Hubï¼š

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3),å®šä¹‰æˆ‘ä»¬çš„ `AutoModelForSequenceClassification` ï¼Œåˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–°Tokenåˆ†ç±»å¤´çš„æƒé‡ï¼‰ï¼Œæˆ‘ä»¬å°†è¦è®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬å°†åœ¨ä¸€åˆ†é’Ÿå†…å®Œæˆï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ä»”ç»†æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®æ•°é‡çš„æ ‡ç­¾ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœæ‚¨çš„æ¨¡å‹æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œåˆ™åœ¨ç¨åè°ƒç”¨ `model.fit()` æ—¶å°†æ”¶åˆ°ä¸€ä¸ªæ¨¡ç³Šçš„é”™è¯¯ã€‚è°ƒè¯•èµ·æ¥å¯èƒ½å¾ˆçƒ¦äººï¼Œå› æ­¤è¯·ç¡®ä¿æ‰§è¡Œæ­¤æ£€æŸ¥ä»¥ç¡®è®¤æ‚¨å…·æœ‰é¢„æœŸçš„æ ‡ç­¾æ•°ã€‚

</Tip>

### å¾®è°ƒæ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä¸è¿‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšä¸¤ä»¶äº‹ï¼šåº”è¯¥ç™»å½•åˆ°Hugging Faceå¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°ã€‚å¦‚æœä½ åœ¨notebookä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªä¾¿åˆ©åŠŸèƒ½å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æ‚¨çš„ Hugging Face è´¦å·å’Œå¯†ç ã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

ç™»å½•åï¼Œæˆ‘ä»¬å¯ä»¥å‡†å¤‡ç¼–è¯‘æ¨¡å‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚ğŸ¤— Transformersæä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„`create_optimizer()` å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†ä¸ºæ‚¨æä¾›ä¸€ä¸ª`AdamW`ä¼˜åŒ–å™¨ï¼Œå…¶ä¸­åŒ…å«é€‚å½“çš„æƒé‡è¡°å‡å’Œå­¦ä¹ é€Ÿç‡è¡°å‡è®¾ç½®ï¼Œä¸å†…ç½®çš„`Adam`ä¼˜åŒ–å™¨ç›¸ä¼¼ï¼Œè¿™ä¸¤è€…éƒ½å°†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼š

```python
from transformers import create_optimizer
import tensorflow as tf

# Train in mixed-precision float16
# Comment this line out if you're using a GPU that will not benefit from this
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

è¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ä¸º`compile()`æä¾›`loss`å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹å®é™…ä¸Šå¯ä»¥åœ¨å†…éƒ¨è®¡ç®—æŸå¤± - å¦‚æœæ‚¨ç¼–è¯‘æ—¶æ²¡æœ‰æŸå¤±å¹¶åœ¨è¾“å…¥å­—å…¸ä¸­æä¾›æ ‡ç­¾ï¼ˆå°±åƒæˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­æ‰€åšçš„é‚£æ ·ï¼‰ï¼Œé‚£ä¹ˆæ¨¡å‹å°†ä½¿ç”¨è¯¥å†…éƒ¨æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œè¿™å°†é€‚ç”¨äºæ‚¨é€‰æ‹©çš„ä»»åŠ¡å’Œæ¨¡å‹ç±»å‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`PushToHubCallback`ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œå¹¶ä½¿ç”¨è¯¥å›è°ƒæ¥æ‹Ÿåˆæ¨¡å‹ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

æ‚¨ä¹‹å‰å·²ç»çœ‹è¿‡å…¶ä¸­çš„å¤§éƒ¨åˆ†å†…å®¹ï¼šæˆ‘ä»¬è®¾ç½®äº†ä¸€äº›è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€è¦è®­ç»ƒçš„ epoch æ•°å’Œæƒé‡è¡°å‡ï¼‰ï¼Œç„¶åæˆ‘ä»¬æŒ‡å®š `push_to_hub=True` è¡¨æ˜æˆ‘ä»¬æƒ³è¦ä¿å­˜æ¨¡å‹å¹¶åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³è¦å°†æˆ‘ä»¬çš„ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚è¯·æ³¨æ„ï¼Œå¯ä»¥ä½¿ç”¨hub_model_idå‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°(ç‰¹åˆ«æ˜¯ï¼Œå¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°æ¥æ¨é€åˆ°ä¸€ä¸ªç»„ç»‡)ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ°[`huggingface-course` organization](https://huggingface.co/huggingface-course), æˆ‘ä»¬æ·»åŠ äº† `hub_model_id=huggingface-course/bert-finetuned-ner` åˆ° `TrainingArguments` .é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„å­˜å‚¨åº“å°†åœ¨æ‚¨çš„å‘½åç©ºé—´ä¸­å¹¶ä»¥æ‚¨è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­å®ƒå°†æ˜¯ `sgugger/bert-finetuned-ner` .

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆè¾“å‡ºç›®å½•å¿…é¡»æ˜¯ä»åŒä¸€ä¸ªå­˜å‚¨åº“cloneä¸‹æ¥çš„ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°†åœ¨å£°æ˜ `model.fit()` æ—¶é‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</Tip>

è¯·æ³¨æ„ï¼Œå½“è®­ç»ƒå‘ç”Ÿæ—¶ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ªepoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œæ‚¨å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­æ‚¨çš„è®­ç»ƒã€‚

åœ¨æ­¤é˜¶æ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°ç»„ä»¶æ¥æµ‹è¯•æ¨¡å‹å¹¶ä¸æœ‹å‹å…±äº«ã€‚æ‚¨å·²ç»æˆåŠŸå¾®è°ƒäº†ä»¤ç‰Œåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ - æ­å–œï¼ä½†æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åˆ°åº•æœ‰å¤šå¥½å‘¢ï¼Ÿæˆ‘ä»¬åº”è¯¥è¯„ä¼°ä¸€äº›æŒ‡æ ‡æ¥æ‰¾å‡ºç­”æ¡ˆã€‚

{/if}


### è¯„ä¼°æŒ‡æ ‡

{#if fw === 'pt'}

ä¸ºäº†è®© `Trainer` åœ¨æ¯ä¸ªepochè®¡ç®—ä¸€ä¸ªåº¦é‡ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ª `compute_metrics()` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—é¢„æµ‹å’Œæ ‡ç­¾æ•°ç»„ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«åº¦é‡åç§°å’Œå€¼çš„å­—å…¸

ç”¨äºè¯„ä¼°Tokenåˆ†ç±»é¢„æµ‹çš„ä¼ ç»Ÿæ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval). è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£…seqevalåº“ï¼š

```py
!pip install seqeval
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ è½½å®ƒ `load_metric()` å‡½æ•°å°±åƒæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)åšçš„é‚£æ ·ï¼š

{:else}

ç”¨äºè¯„ä¼°Tokenåˆ†ç±»é¢„æµ‹çš„ä¼ ç»Ÿæ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval). è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£…seqevalåº“ï¼š

```py
!pip install seqeval
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ è½½å®ƒ `load_metric()` å‡½æ•°å°±åƒæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)åšçš„é‚£æ ·ï¼š

{/if}

```py
from datasets import load_metric

metric = load_metric("seqeval")
```

è¿™ä¸ªè¯„ä¼°æ–¹å¼ä¸æ ‡å‡†ç²¾åº¦ä¸åŒ:å®ƒå®é™…ä¸Šå°†æ ‡ç­¾åˆ—è¡¨ä½œä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•´æ•°ï¼Œå› æ­¤åœ¨å°†é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™å®ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®Œå…¨è§£ç å®ƒä»¬ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è·å¾—ç¬¬ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹çš„æ ‡ç­¾:

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ”¹ç´¢å¼• 2 å¤„çš„å€¼æ¥ä¸ºé‚£äº›åˆ›å»ºå‡çš„é¢„æµ‹ï¼š

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

è¯·æ³¨æ„ï¼Œè¯¥æŒ‡æ ‡çš„è¾“å…¥æ˜¯é¢„æµ‹åˆ—è¡¨ï¼ˆä¸ä»…ä»…æ˜¯ä¸€ä¸ªï¼‰å’Œæ ‡ç­¾åˆ—è¡¨ã€‚è¿™æ˜¯è¾“å‡ºï¼š

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

å®ƒè¿”å›å¾ˆå¤šä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚å¯¹äºæˆ‘ä»¬çš„åº¦é‡è®¡ç®—ï¼Œæˆ‘ä»¬å°†åªä¿ç•™æ€»åˆ†ï¼Œä½†å¯ä»¥éšæ„è°ƒæ•´ `compute_metrics()` å‡½æ•°è¿”å›æ‚¨æƒ³è¦æŸ¥çœ‹çš„æ‰€æœ‰æŒ‡æ ‡ã€‚

è¿™`compute_metrics()` å‡½æ•°é¦–å…ˆé‡‡ç”¨ logits çš„ argmax å°†å®ƒä»¬è½¬æ¢ä¸ºé¢„æµ‹ï¼ˆåƒå¾€å¸¸ä¸€æ ·ï¼Œlogits å’Œæ¦‚ç‡çš„é¡ºåºç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦åº”ç”¨ softmaxï¼‰ã€‚ç„¶åæˆ‘ä»¬å¿…é¡»å°†æ ‡ç­¾å’Œé¢„æµ‹ä»æ•´æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚æˆ‘ä»¬åˆ é™¤æ ‡ç­¾ä¸º `-100` æ‰€æœ‰å€¼ ï¼Œç„¶åå°†ç»“æœä¼ é€’ç»™ `metric.compute()` æ–¹æ³•ï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

ç°åœ¨å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½å®šä¹‰æˆ‘ä»¬çš„ `Trainer` .æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ª `model` å¾®è°ƒï¼

{:else}

å®ƒè¿”å›å¾ˆå¤šä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚å¯¹äºæˆ‘ä»¬çš„åº¦é‡è®¡ç®—ï¼Œæˆ‘ä»¬å°†åªä¿ç•™æ€»åˆ†ï¼Œä½†å¯ä»¥éšæ„è°ƒæ•´ `compute_metrics()` å‡½æ•°è¿”å›æ‚¨æƒ³è¦æŸ¥çœ‹çš„æ‰€æœ‰æŒ‡æ ‡ã€‚

è¿™`compute_metrics()` å‡½æ•°é¦–å…ˆé‡‡ç”¨ logits çš„ argmax å°†å®ƒä»¬è½¬æ¢ä¸ºé¢„æµ‹ï¼ˆåƒå¾€å¸¸ä¸€æ ·ï¼Œlogits å’Œæ¦‚ç‡çš„é¡ºåºç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦åº”ç”¨ softmaxï¼‰ã€‚ç„¶åæˆ‘ä»¬å¿…é¡»å°†æ ‡ç­¾å’Œé¢„æµ‹ä»æ•´æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚æˆ‘ä»¬åˆ é™¤æ ‡ç­¾ä¸º `-100` æ‰€æœ‰å€¼ ï¼Œç„¶åå°†ç»“æœä¼ é€’ç»™ `metric.compute()` æ–¹æ³•ï¼š

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

ä¸æˆ‘ä»¬çš„æ¨¡å‹ç›¸æ¯”ï¼Œæ‚¨çš„æ¨¡å‹åšå¾—å¦‚ä½•ï¼Ÿå¦‚æœæ‚¨è·å¾—ç±»ä¼¼çš„æ•°å­—ï¼Œé‚£ä¹ˆæ‚¨çš„è®­ç»ƒå°±æ˜¯æˆåŠŸçš„ï¼

{/if}

{#if fw === 'pt'}

### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶Tokenåˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForTokenClassification` ç±»ã€‚å®šä¹‰è¿™ä¸ªæ¨¡å‹æ—¶è¦è®°ä½çš„ä¸»è¦äº‹æƒ…æ˜¯ä¼ é€’ä¸€äº›å…³äºæˆ‘ä»¬çš„æ ‡ç­¾æ•°é‡çš„ä¿¡æ¯ã€‚æ‰§è¡Œæ­¤æ“ä½œçš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå¾ˆå¥½çš„æ¨ç†å°éƒ¨ä»¶ï¼Œå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·ï¼Œæœ€å¥½è®¾ç½®æ­£ç¡®çš„æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚

å®ƒä»¬åº”è¯¥ç”±ä¸¤ä¸ªå­—å…¸è®¾ç½®ï¼Œ `id2label` å’Œ `label2id` ï¼Œå…¶ä¸­åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ï¼Œåä¹‹äº¦ç„¶ï¼š

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä¼ é€’ç»™ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°†åœ¨æ¨¡å‹çš„é…ç½®ä¸­è®¾ç½®ï¼Œç„¶åä¿å­˜å¹¶ä¸Šä¼ åˆ°Hubï¼š

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3),å®šä¹‰æˆ‘ä»¬çš„ `AutoModelForSequenceClassification` ï¼Œåˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–°Tokenåˆ†ç±»å¤´çš„æƒé‡ï¼‰ï¼Œæˆ‘ä»¬å°†è¦è®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬å°†åœ¨ä¸€åˆ†é’Ÿå†…å®Œæˆï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ä»”ç»†æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®æ•°é‡çš„æ ‡ç­¾ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœæ¨¡å‹çš„æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œç¨åè°ƒç”¨Trainer.train()æ–¹æ³•æ—¶ä¼šå‡ºç°ä¸€ä¸ªæ¨¡ç³Šçš„é”™è¯¯ï¼ˆç±»ä¼¼äºâ€œCUDA error: device-side assert triggeredâ€ï¼‰ã€‚è¿™æ˜¯ç”¨æˆ·æŠ¥å‘Šæ­¤ç±»é”™è¯¯çš„ç¬¬ä¸€ä¸ªåŸå› ï¼Œå› æ­¤è¯·ç¡®ä¿è¿›è¡Œè¿™æ ·çš„æ£€æŸ¥ä»¥ç¡®è®¤æ‚¨æ‹¥æœ‰é¢„æœŸæ•°é‡çš„æ ‡ç­¾ã€‚

</Tip>

### å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼åœ¨å®šä¹‰æˆ‘ä»¬çš„ `Trainer`ä¹‹å‰ï¼Œæˆ‘ä»¬åªéœ€è¦åšæœ€åä¸¤ä»¶äº‹ï¼šç™»å½• Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ã€‚å¦‚æœæ‚¨åœ¨notebookä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©æ‚¨ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```
è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æ‚¨çš„ Hugging Face è´¦å·å’Œå¯†ç ã€‚å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

Once this is done, we can define our `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

æ‚¨ä¹‹å‰å·²ç»çœ‹è¿‡å…¶ä¸­çš„å¤§éƒ¨åˆ†å†…å®¹ï¼šæˆ‘ä»¬è®¾ç½®äº†ä¸€äº›è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€è¦è®­ç»ƒçš„ epoch æ•°å’Œæƒé‡è¡°å‡ï¼‰ï¼Œç„¶åæˆ‘ä»¬æŒ‡å®š `push_to_hub=True` è¡¨æ˜æˆ‘ä»¬æƒ³è¦ä¿å­˜æ¨¡å‹å¹¶åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³è¦å°†æˆ‘ä»¬çš„ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚è¯·æ³¨æ„ï¼Œå¯ä»¥ä½¿ç”¨hub_model_idå‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°(ç‰¹åˆ«æ˜¯ï¼Œå¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°æ¥æ¨é€åˆ°ä¸€ä¸ªç»„ç»‡)ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ°[`huggingface-course` organization](https://huggingface.co/huggingface-course), æˆ‘ä»¬æ·»åŠ äº† `hub_model_id=huggingface-course/bert-finetuned-ner` åˆ° `TrainingArguments` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„å­˜å‚¨åº“å°†åœ¨æ‚¨çš„å‘½åç©ºé—´ä¸­å¹¶ä»¥æ‚¨è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­å®ƒå°†æ˜¯ `sgugger/bert-finetuned-ner`ã€‚

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆè¾“å‡ºç›®å½•å¿…é¡»æ˜¯ä»åŒä¸€ä¸ªå­˜å‚¨åº“cloneä¸‹æ¥çš„ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°†åœ¨å£°æ˜ `Trainer` æ—¶é‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</Tip>

æœ€åï¼Œæˆ‘ä»¬åªæ˜¯å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Trainer` å¹¶å¯åŠ¨è®­ç»ƒï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

è¯·æ³¨æ„ï¼Œå½“è®­ç»ƒå‘ç”Ÿæ—¶ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ªepoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œæ‚¨å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­æ‚¨çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` ç¡®ä¿æˆ‘ä»¬ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬

```py
trainer.push_to_hub(commit_message="Training complete")
```

This command returns the URL of the commit it just did, if you want to inspect it:

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

è¿™ `Trainer` è¿˜åˆ›å»ºäº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡å¹¶ä¸Šä¼ ã€‚åœ¨æ­¤é˜¶æ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•æ‚¨çš„æ¨¡å‹å¹¶ä¸æ‚¨çš„æœ‹å‹åˆ†äº«ã€‚æ‚¨å·²æˆåŠŸåœ¨Tokenåˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ - æ­å–œï¼

å¦‚æœæ‚¨æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·æ‚¨å¯ä»¥è½»æ¾å®šä¹‰æ‰€éœ€çš„éƒ¨åˆ†ã€‚å®ƒçœ‹èµ·æ¥å¾ˆåƒæˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3/4), æ‰€åšçš„ï¼Œå¯¹è¯„ä¼°è¿›è¡Œäº†ä¸€äº›æ›´æ”¹ã€‚

### åšå¥½è®­ç»ƒå‰çš„å‡†å¤‡
é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†æ„å»º `DataLoader` ã€‚æˆ‘ä»¬å°†é‡ç”¨æˆ‘ä»¬çš„ `data_collator` ä½œä¸ºä¸€ä¸ª `collate_fn` å¹¶æ‰“ä¹±è®­ç»ƒé›†ï¼Œä½†ä¸æ‰“ä¹±éªŒè¯é›†ï¼š

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šä»ä¹‹å‰çš„è®­ç»ƒç»§ç»­è®­ç»ƒï¼Œè€Œæ˜¯å†æ¬¡ä» BERT é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹ï¼š

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

ç„¶åæˆ‘ä»¬å°†éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç»å…¸ `AdamW` ï¼Œè¿™å°±åƒ `Adam` ï¼Œä½†åœ¨åº”ç”¨æƒé‡è¡°å‡çš„æ–¹å¼ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼š
```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Once we have all those objects, we can send them to the `accelerator.prepare()` method:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœæ‚¨åœ¨ TPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ™éœ€è¦å°†ä»¥ä¸Šå•å…ƒæ ¼ä¸­çš„æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸“ç”¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [ç¬¬3ç« ](/course/chapter3)ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å‘é€äº†æˆ‘ä»¬çš„ `train_dataloader` åˆ° `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆåœ¨å‡†å¤‡å¥½dataloaderåæ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºè¯¥æ–¹æ³•ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œè¦å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `Repository` å·¥ä½œæ–‡ä»¶å¤¹ä¸­çš„å¯¹è±¡ã€‚å¦‚æœæ‚¨å°šæœªç™»å½•ï¼Œè¯·å…ˆç™»å½• Hugging Faceã€‚æˆ‘ä»¬å°†ä»æˆ‘ä»¬æƒ³è¦ä¸ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¡®å®šå­˜å‚¨åº“åç§°ï¼ˆæ‚¨å¯ä»¥è‡ªç”±åœ°ç”¨è‡ªå·±çš„é€‰æ‹©æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«æ‚¨çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨`get_full_repo_name()`å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.

### Training loop

### è®­ç»ƒå¾ªç¯
æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å®ƒçš„è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ª `postprocess()` æ¥å—é¢„æµ‹å’Œæ ‡ç­¾å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨çš„å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯ `metric`å¯¹è±¡éœ€è¦çš„è¾“å…¥æ ¼å¼ï¼š

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç¼–å†™è®­ç»ƒå¾ªç¯ã€‚åœ¨å®šä¹‰ä¸€ä¸ªè¿›åº¦æ¡æ¥è·Ÿè¸ªè®­ç»ƒçš„è¿›è¡Œåï¼Œå¾ªç¯åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

- è®­ç»ƒæœ¬èº«ï¼Œè¿™æ˜¯å¯¹`train_dataloader`çš„ç»å…¸è¿­ä»£ï¼Œå‘å‰ä¼ é€’æ¨¡å‹ï¼Œç„¶ååå‘ä¼ é€’å’Œä¼˜åŒ–å‚æ•°
- è¯„ä¼°,åœ¨è·å¾—æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºå:å› ä¸ºä¸¤ä¸ªè¿›ç¨‹å¯èƒ½å°†è¾“å…¥å’Œæ ‡ç­¾å¡«å……æˆä¸åŒçš„å½¢çŠ¶,åœ¨è°ƒç”¨`gather()`æ–¹æ³•å‰æˆ‘ä»¬éœ€è¦ä½¿ç”¨`accelerator.pad_across_processes()`æ¥è®©é¢„æµ‹å’Œæ ‡ç­¾å½¢çŠ¶ç›¸åŒã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™æ ·åšï¼Œè¯„ä¼°è¦ä¹ˆå‡ºé”™ï¼Œè¦ä¹ˆæ°¸è¿œä¸ä¼šå¾—åˆ°ç»“æœã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç»“æœå‘é€ç»™`metric.add_batch()`ï¼Œå¹¶åœ¨è®¡ç®—å¾ªç¯ç»“æŸåè°ƒç”¨`metric.compute()`ã€‚
- ä¿å­˜å’Œä¸Šä¼ ï¼Œé¦–å…ˆä¿å­˜æ¨¡å‹å’Œæ ‡è®°å™¨ï¼Œç„¶åè°ƒç”¨`repo.push_to_hub()`ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚æ•°`blocking=False`å‘Šè¯‰ğŸ¤— hub åº“ç”¨åœ¨å¼‚æ­¥è¿›ç¨‹ä¸­æ¨é€ã€‚è¿™æ ·ï¼Œè®­ç»ƒå°†æ­£å¸¸ç»§ç»­ï¼Œå¹¶ä¸”è¯¥ï¼ˆé•¿ï¼‰æŒ‡ä»¤å°†åœ¨åå°æ‰§è¡Œã€‚

è¿™æ˜¯è®­ç»ƒå¾ªç¯çš„å®Œæ•´ä»£ç ï¼š

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

æœè¿™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡çœ‹åˆ°ç”¨ ğŸ¤— Accelerate ä¿å­˜çš„æ¨¡å‹ï¼Œè®©æˆ‘ä»¬èŠ±ç‚¹æ—¶é—´æ£€æŸ¥ä¸€ä¸‹å®ƒé™„å¸¦çš„ä¸‰è¡Œä»£ç ï¼š

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

ç¬¬ä¸€è¡Œæ˜¯ä¸è¨€è‡ªæ˜çš„ï¼šå®ƒå‘Šè¯‰æ‰€æœ‰è¿›ç¨‹ç­‰åˆ°éƒ½å¤„äºé‚£ä¸ªé˜¶æ®µå†ç»§ç»­(é˜»å¡)ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªè¿‡ç¨‹ä¸­éƒ½æœ‰ç›¸åŒçš„æ¨¡å‹ã€‚ç„¶åè·å–`unwrapped_model`ï¼Œå®ƒæ˜¯æˆ‘ä»¬å®šä¹‰çš„åŸºæœ¬æ¨¡å‹ã€‚
`accelerator.prepare()`æ–¹æ³•å°†æ¨¡å‹æ›´æ”¹ä¸ºåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å·¥ä½œï¼Œæ‰€ä»¥å®ƒä¸å†æœ‰`save_pretraining()`æ–¹æ³•;`accelerator.unwrap_model()`æ–¹æ³•å°†æ’¤é”€è¯¥æ­¥éª¤ã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒç”¨`save_pretraining()`ï¼Œä½†å‘Šè¯‰è¯¥æ–¹æ³•ä½¿ç”¨`accelerator.save()`è€Œä¸æ˜¯`torch.save()`ã€‚ 

å½“å®Œæˆä¹‹åï¼Œä½ åº”è¯¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå®ƒäº§ç”Ÿçš„ç»“æœä¸`Trainer`çš„ç»“æœéå¸¸ç›¸ä¼¼ã€‚ä½ å¯ä»¥åœ¨[hugs face-course/bert-fine - tuning -ner-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate)ä¸­æŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³æµ‹è¯•è®­ç»ƒå¾ªç¯çš„ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°å®ƒä»¬!

{/if}

## ä½¿ç”¨å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬å·²ç»å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬åœ¨æ¨¡å‹ä¸­å¿ƒå¾®è°ƒçš„æ¨¡å‹å’Œæ¨ç†å°éƒ¨ä»¶ã€‚åœ¨æœ¬åœ°ä½¿ç”¨å®ƒ `pipeline` ï¼Œæ‚¨åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼š

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

å¤ªæ£’äº†ï¼æˆ‘ä»¬çš„æ¨¡å‹ä¸æ­¤ç®¡é“çš„é»˜è®¤æ¨¡å‹ä¸€æ ·æœ‰æ•ˆï¼
