<FrameworkSwitchCourse {fw} />

# Token åˆ†ç±» [[Token åˆ†ç±»]]

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
]} />

{/if}

æˆ‘ä»¬å°†é¦–å…ˆæ¢è®¨çš„åº”ç”¨æ˜¯ Token åˆ†ç±»ã€‚è¿™ä¸ªé€šç”¨ä»»åŠ¡æ¶µç›–äº†æ‰€æœ‰å¯ä»¥è¡¨è¿°ä¸ºâ€œç»™å¥å­ä¸­çš„è¯æˆ–å­—è´´ä¸Šæ ‡ç­¾â€çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **å®ä½“å‘½åè¯†åˆ« ï¼ˆNERï¼‰**ï¼šæ‰¾å‡ºå¥å­ä¸­çš„å®ä½“ï¼ˆå¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªå®ä½“æŒ‡å®šä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾ï¼Œå¦‚æœæ²¡æœ‰å®ä½“åˆ™ä¼šè¾“å‡ºæ— å®ä½“çš„æ ‡ç­¾ã€‚
- **è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰**ï¼šå°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸ºå¯¹åº”äºç‰¹å®šçš„è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚
- **åˆ†å—ï¼ˆchunkingï¼‰**ï¼šæ‰¾å‡ºå±äºåŒä¸€å®ä½“çš„ tokens è¿™ä¸ªä»»åŠ¡ï¼ˆå¯ä»¥ä¸è¯æ€§æ ‡æ³¨æˆ–å‘½åå®ä½“è¯†åˆ«ç»“åˆï¼‰å¯ä»¥è¢«æè¿°ä¸ºå°†ä½äºå—å¼€å¤´çš„ token èµ‹äºˆä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ `B-` â€ ï¼ˆBeginï¼‰ï¼Œä»£è¡¨è¯¥tokenä½äºå®ä½“çš„å¼€å¤´ï¼‰ï¼Œå°†ä½äºå—å†…çš„ tokens èµ‹äºˆå¦ä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ `I-` â€ï¼ˆinnerï¼‰ä»£è¡¨è¯¥tokenä½äºå®ä½“çš„å†…éƒ¨ï¼‰ï¼Œå°†ä¸å±äºä»»ä½•å—çš„ tokens èµ‹äºˆç¬¬ä¸‰ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ `O` â€ ï¼ˆouterï¼‰ä»£è¡¨è¯¥tokenä¸å±äºä»»ä½•å®ä½“ï¼‰ã€‚
<Youtube id="wVHdVlPScxA"/>

å½“ç„¶ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–ç±»å‹çš„ token åˆ†ç±»é—®é¢˜ï¼›è¿™äº›åªæ˜¯å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ NER ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ ï¼ˆBERTï¼‰ï¼Œç„¶åè¯¥æ¨¡å‹å°†èƒ½å¤Ÿç”Ÿæˆå¦‚ä¸‹é¢„æµ‹ï¼š

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ä½ å¯ä»¥ [åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn) ã€‚æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ï¼Œå¯ä»¥å°è¯•è¾“å…¥ä¸€äº›å¥å­æµ‹è¯•ä¸€ä¸‹æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

## å‡†å¤‡æ•°æ® [[å‡†å¤‡æ•°æ®]]

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆ token åˆ†ç±»çš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [CoNLL-2003 æ•°æ®é›†](https://huggingface.co/datasets/conll2003) ï¼Œè¯¥æ•°æ®é›†æ¥æºäºè·¯é€ç¤¾çš„æ–°é—»æŠ¥é“ã€‚

<Tip>

ğŸ’¡ åªè¦ä½ çš„æ•°æ®é›†ç”±åˆ†è¯æ–‡æœ¬å’Œå¯¹åº”çš„æ ‡ç­¾ç»„æˆï¼Œå°±èƒ½å¤Ÿå°†è¿™é‡Œæè¿°çš„æ•°æ®å¤„ç†è¿‡ç¨‹åº”ç”¨åˆ°è‡ªå·±çš„æ•°æ®é›†ä¸­ã€‚å¦‚æœéœ€è¦å¤ä¹ å¦‚ä½•åœ¨ `Dataset` ä¸­åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·å¤ä¹  [ç¬¬äº”ç« ](/course/chapter5) ã€‚

</Tip>

### CoNLL-2003 æ•°æ®é›† [[CoNLL-2003 æ•°æ®é›†]]

è¦åŠ è½½ CoNLL-2003 æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Datasets åº“çš„ `load_dataset()` æ–¹æ³•ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

è¿™å°†ä¸‹è½½å¹¶ç¼“å­˜æ•°æ®é›†ï¼Œå°±åƒå’Œæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) åŠ è½½ GLUE MRPC æ•°æ®é›†ä¸€æ ·ã€‚æŸ¥çœ‹ `raw_datasets` å¯¹è±¡å¯ä»¥è®©æˆ‘ä»¬çœ‹åˆ°æ•°æ®é›†ä¸­å­˜åœ¨å“ªäº›åˆ—ï¼Œä»¥åŠè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´æ˜¯å¦‚ä½•åˆ’åˆ†çš„ï¼š

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

å¯ä»¥çœ‹åˆ°æ•°æ®é›†åŒ…å«äº†æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ä¸‰é¡¹ä»»åŠ¡çš„æ ‡ç­¾ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ä»¥åŠåˆ†å—ï¼ˆchunkingï¼‰ã€‚è¿™ä¸ªæ•°æ®é›†ä¸å…¶ä»–æ•°æ®é›†çš„ä¸€ä¸ªæ˜¾è‘—åŒºåˆ«åœ¨äºï¼Œè¾“å…¥æ–‡æœ¬çš„é‚£ä¸€åˆ—å¹¶éä»¥å¥å­æˆ–æ•´ç‰‡çš„æ–‡æœ¬çš„å½¢å¼å­˜å‚¨ï¼Œè€Œæ˜¯ä»¥å•è¯åˆ—è¡¨çš„å½¢å¼ï¼ˆæœ€åä¸€åˆ—è¢«ç§°ä¸º `tokens` ï¼Œä¸è¿‡ `tokens` åˆ—ä¿å­˜çš„è¿˜æ˜¯å•è¯ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›é¢„å…ˆåˆ†è¯çš„è¾“å…¥ä»éœ€è¦ç»è¿‡ tokenizer è¿›è¡Œå­è¯åˆ†è¯å¤„ç†ï¼‰ã€‚

æˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

ç”±äºæˆ‘ä»¬è¦è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹ NER æ ‡ç­¾ï¼š

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

è¿™äº›æ˜¯å¯ä»¥ç›´æ¥è®­ç»ƒçš„æ•´æ•°ç±»åˆ«æ ‡ç­¾ï¼Œå½“æˆ‘ä»¬æƒ³è¦æ£€æŸ¥æ•°æ®æ—¶ï¼Œç›´æ¥çœ‹æ•´æ•°çš„æ ‡ç­¾ä¸æ˜¯å¾ˆç›´è§‚ã€‚å°±åƒåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹æ•°æ®é›†çš„ `features` å±æ€§æ¥æ‰¾åˆ°è¿™äº›æ•´æ•°å’Œæ ‡ç­¾åç§°ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼š

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº† `ClassLabels` ç±»å‹çš„åºåˆ—ã€‚åºåˆ—ä¸­å…ƒç´ çš„ç±»å‹å­˜å‚¨åœ¨ `ner_feature` çš„ `feature` ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥ `feature` çš„ `names` å±æ€§æ¥è®¿é—®æ ‡ç­¾åç§°çš„åˆ—è¡¨ï¼š

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

æˆ‘ä»¬åœ¨ [ç¬¬å…­ç« ](/course/chapter6/3) æ·±å…¥ç ”ç©¶ `token-classification` ç®¡é“æ—¶å·²ç»çœ‹åˆ°è¿‡è¿™äº›æ ‡ç­¾ æˆ‘ä»¬åœ¨è¿™é‡Œè¿›è¡Œä¸€ä¸ªå¿«é€Ÿçš„å›é¡¾ï¼š

- `O` è¡¨ç¤ºè¿™ä¸ªè¯ä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚
- `B-PER` / `I-PER` æ„å‘³ç€è¿™ä¸ªè¯å¯¹åº”äºäººåå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-ORG` / `I-ORG` çš„æ„æ€æ˜¯è¿™ä¸ªè¯å¯¹åº”äºç»„ç»‡åç§°å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-LOC` / `I-LOC` æŒ‡çš„æ˜¯æ˜¯è¿™ä¸ªè¯å¯¹åº”äºåœ°åå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-MISC` / `I-MISC` è¡¨ç¤ºè¿™ä¸ªè¯å¯¹åº”ä¸€ä¸ªå…¶ä»–å®ä½“ï¼ˆä¸å±äºç‰¹å®šç±»åˆ«æˆ–ç±»åˆ«ä¹‹å¤–ï¼‰çš„å¼€å¤´ / å†…éƒ¨ã€‚

æ¥ä¸‹æ¥ä½¿ç”¨å¯¹è¿™äº›æ ‡ç­¾åå¯¹æ•°æ®é›†çš„ `ner_tags` è¿›è¡Œè§£ç ï¼Œå°†å¾—åˆ°ä»¥ä¸‹è¾“å‡ºç»“æœ:

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è®­ç»ƒé›†ä¸­ç´¢å¼•ä¸º 4 çš„æ•°æ®ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒæ—¶åŒ…å« `B-` å’Œ `I-` æ ‡ç­¾çš„ä¾‹å­ï¼š

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çš„è¾“å‡ºä¸­æ‰€çœ‹åˆ°çš„ï¼Œè·¨è¶Šä¸¤ä¸ªå•è¯çš„å®ä½“ï¼Œå¦‚â€œEuropean Unionâ€å’Œâ€œWerner Zwingmannâ€ï¼Œæ•°æ®é›†æŠŠç¬¬ä¸€ä¸ªå•è¯æ ‡æ³¨ä¸ºäº† `B-` æ ‡ç­¾ï¼Œå°†ç¬¬äºŒä¸ªå•è¯æ ‡è®°ä¸ºäº† `I-` æ ‡ç­¾ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** æ£€æŸ¥åŒä¸€ä¸ªå¥å­çš„è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰æˆ–åˆ†å—ï¼ˆchunkingï¼‰åˆ—ï¼ŒæŸ¥çœ‹è¾“å‡ºçš„ç»“æœã€‚

</Tip>

### å¤„ç†æ•°æ® [[å¤„ç†æ•°æ®]]

<Youtube id="iY2AZYdZAr0"/>

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬éœ€è¦è½¬æ¢ä¸º Token IDï¼Œç„¶åæ¨¡å‹æ‰èƒ½ç†è§£å®ƒä»¬ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬å…­ç« ](/course/chapter6/) æ‰€å­¦çš„é‚£æ ·ã€‚ä¸è¿‡ä¸ tokens åˆ†ç±»ä»»åŠ¡ä¸åŒçš„æ˜¯æ•°æ®é›†å·²ç»å®Œæˆäº†é¢„åˆ†è¯ï¼Œæˆ‘ä»¬åœ¨å¤„ç†çš„è¿‡ç¨‹ä¸­ä¸éœ€è¦å†æ¬¡åˆ†è¯ã€‚å¹¸è¿çš„æ˜¯ï¼Œtokenizer API å¯ä»¥å¾ˆå®¹æ˜“åœ°å¤„ç†è¿™ç§å·®å¼‚ï¼›æˆ‘ä»¬åªéœ€è¦é€šè¿‡ä¸€ä¸ªç‰¹æ®Šçš„æ ‡å¿—å‘Šè¯‰ tokenizerå³å¯ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»º `tokenizer` å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»ä¸‹è½½å¹¶ç¼“å­˜å…³è”çš„ `tokenizer` å¼€å§‹ï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ä½ å¯ä»¥æ›´æ¢æŠŠ `model_checkpoint` æ›´æ¢ä¸º [Hub](https://huggingface.co/models) ä¸Šä½ å–œæ¬¢çš„ä»»ä½•å…¶ä»–æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨æœ¬åœ°ä¿å­˜çš„é¢„è®­ç»ƒæ¨¡å‹å’Œ `tokenizer`ã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯ tokenizer éœ€è¦ç”± ğŸ¤— Tokenizers åº“æ”¯æŒï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªâ€œå¿«é€Ÿâ€ç‰ˆæœ¬å¯ç”¨ã€‚ä½ å¯ä»¥åœ¨ [Huggingface æ¨¡å‹åç«¯æ”¯æŒè¡¨](https://huggingface.co/transformers/#supported-frameworks) ä¸Šçœ‹åˆ°æ‰€æœ‰å¸¦æœ‰å¿«é€Ÿç‰ˆæœ¬çš„ `tokenizer` æ¶æ„ï¼Œæˆ–è€…ä½ ä¹Ÿå¯ä»¥é€šè¿‡æŸ¥çœ‹å®ƒ `is_fast` å±æ€§æ¥æ£€æµ‹æ­£åœ¨ä½¿ç”¨çš„ `tokenizer` å¯¹è±¡æ˜¯å¦ç”± ğŸ¤— Tokenizers æ”¯æŒï¼š

```py
tokenizer.is_fast
```

```python out
True
```

æˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨æˆ‘ä»¬çš„ `tokenizer` å¯¹é¢„å…ˆåˆ†è¯çš„è¾“å…¥è¿›è¡Œ `tokenize` ï¼Œåªéœ€é¢å¤–æ·»åŠ  `is_split_into_words=True` å‚æ•°ï¼š

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œ`tokenizer` åœ¨ç»“æœä¸­æ·»åŠ äº†æ¨¡å‹éœ€è¦ä½¿ç”¨çš„ç‰¹æ®Š tokensï¼ˆåœ¨å¼€å¤´çš„ `[CLS]` ï¼Œåœ¨ç»“å°¾çš„ `[SEP]` ï¼‰ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†å•è¯ä¿æŒä¸å˜ã€‚ä¸è¿‡ï¼Œå•è¯ `lamb` è¢«åˆ†è¯ä¸ºä¸¤ä¸ªå­è¯ï¼Œ `la` å’Œ `##mb` ã€‚è¿™å¯¼è‡´äº†è¾“å…¥å’Œæ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…ï¼šæ ‡ç­¾åˆ—è¡¨åªæœ‰ 9 ä¸ªå…ƒç´ ï¼Œè€Œæˆ‘ä»¬çš„è¾“å…¥ç°åœ¨æœ‰ 12 ä¸ª tokensã€‚è§£å†³ç‰¹æ®Š tokens çš„é—®é¢˜å¾ˆå®¹æ˜“ï¼ˆæˆ‘ä»¬å·²ç»çŸ¥é“äº†æ¯ä¸ª `token` å¼€å§‹å’Œç»“æŸçš„ä½ç½®ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿çš„æ˜¯æˆ‘ä»¬å°†æ‰€æœ‰çš„æ ‡ç­¾ä¸æ­£ç¡®çš„è¯å¯¹é½ã€‚


å¹¸è¿çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å¿«é€Ÿ tokens å› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Tokenizers è¶…èƒ½åŠ›ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†æ¯ä¸ª token æ˜ å°„åˆ°å…¶ç›¸åº”çš„å•è¯ï¼ˆå¦‚ [ç¬¬å…­ç« ](/course/chapter6/3) ä¸­æ‰€å­¦ï¼‰ï¼š

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

åªéœ€è¦ä¸€ç‚¹ç‚¹å·¥ä½œï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‰©å±•æˆ‘ä»¬çš„æ ‡ç­¾åˆ—è¡¨ã€‚æˆ‘ä»¬å°†æ·»åŠ çš„ä½¿å…¶ä¸ `token` åˆ—è¡¨ç›¸åŒ¹é…ã€‚ æ·»åŠ çš„ç¬¬ä¸€æ¡è§„åˆ™æ˜¯ï¼Œç‰¹æ®Š tokens çš„æ ‡ç­¾è®¾ç½®ä¸º `-100` ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼Œ `-100` ä¼šè¢«æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰å¿½ç•¥ã€‚ç„¶åå°†æ¯ä¸ª `token` çš„æ ‡ç­¾è®¾ç½®ä¸ºè¿™ä¸ª `token` æ‰€åœ¨å•è¯å¼€å¤´çš„ `token` çš„æ ‡ç­¾ï¼Œå› ä¸ºåŒä¸€ä¸ªå•è¯ä¸€å®šæ˜¯åŒä¸€ä¸ªå®ä½“çš„ä¸€éƒ¨åˆ†ã€‚æœ€åå°†å•è¯çš„å†…éƒ¨ `tokens` æ ‡ç­¾ä¸­çš„ `B-` æ›¿æ¢ä¸º `I-`ï¼Œå› ä¸ºè¯¥ `token` ä¸åœ¨å®ä½“çš„å¼€å¤´ï¼Œ`B-` åœ¨æ¯ä¸ªå®ä½“ä¸­åªåº”è¯¥å‡ºç°ä¸€æ¬¡ï¼š

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # æ–°å•è¯çš„å¼€å§‹!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # ç‰¹æ®Šçš„token
            new_labels.append(-100)
        else:
            # ä¸å‰ä¸€ä¸ª tokens ç±»å‹ç›¸åŒçš„å•è¯
            label = labels[word_id]
            # å¦‚æœæ ‡ç­¾æ˜¯ B-XXX æˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸º I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

è®©æˆ‘ä»¬åœ¨æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¸Šè¯•ä¸€è¯•ï¼š

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„å‡½æ•°ä¸ºå¼€å¤´å’Œç»“å°¾æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Š tokens ï¼š`-100` ï¼Œå¹¶ä¸ºåˆ‡åˆ†æˆä¸¤ä¸ª tokens çš„å•è¯æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ `0` æ ‡ç­¾ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** æœ‰äº›ç ”ç©¶äººå‘˜æ›´å–œæ¬¢åªä¸ºæ¯ä¸ªå•åªåˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼Œå¯¹è¯¥å•è¯å…¶ä»–éƒ¨åˆ†çš„ token åˆ†é… `-100` æ ‡ç­¾ã€‚è¿™æ˜¯ä¸ºäº†é¿å…é‚£äº›åˆ†è§£æˆè®¸å¤šå­è¯çš„é•¿å•è¯å¯¹æŸå¤±ä½œå‡ºè¿‡å¤šçš„è´¡çŒ®ã€‚è¯·æŒ‰ç…§è¿™ä¸ªæ€è·¯ï¼Œæ”¹å˜ä¹‹å‰çš„å‡½æ•°ï¼Œä½¿æ ‡ç­¾ä¸ inputs ID å¯¹é½ã€‚

</Tip>

ä¸ºäº†é¢„å¤„ç†æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œ `tokenize`ï¼Œå¹¶ä½¿ç”¨ `align_labels_with_tokens()` å‡½æ•°å¤„ç†æ‰€æœ‰æ ‡ç­¾ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å¿«é€Ÿ tokenizer çš„ä¼˜åŠ¿ï¼Œæœ€å¥½æ˜¯åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬ä¸€èµ·è¿›è¡Œ `tokenize`ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç¼–å†™ä¸€ä¸ªå¤„ç†ä¸€ç»„ç¤ºä¾‹çš„å‡½æ•°ï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰ `batched=True` å‚æ•°çš„ `Dataset.map()` æ–¹æ³•ã€‚ä¸æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹å”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œå½“ `tokenizer` çš„è¾“å…¥æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–ç¤ºä¾‹ä¸­å•è¯çš„åˆ—è¡¨çš„åˆ—è¡¨ï¼‰æ—¶ï¼Œ `word_ids()` å‡½æ•°éœ€è¦æ ¹æ®åˆ—è¡¨çš„ç´¢å¼•è·å– `token` çš„ IDï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿåœ¨ä¸‹é¢çš„å‡½æ•°ä¸­æ·»åŠ äº†è¿™ä¸ªåŠŸèƒ½ï¼š

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¡«å……æˆ‘ä»¬çš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†åœ¨ç¨åä½¿ç”¨æ•°æ®æ•´ç†å™¨åˆ›å»º `batch` æ—¶è¿›è¡Œã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸€æ¬¡æ€§ä½¿ç”¨é¢„å¤„ç†å‡½æ•°å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼š

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

æˆ‘ä»¬å·²ç»å®Œæˆäº†æœ€å›°éš¾çš„éƒ¨åˆ†ï¼ç°åœ¨æ•°æ®å·²ç»ç»è¿‡äº†é¢„å¤„ç†ï¼Œå®é™…çš„è®­ç»ƒè¿‡ç¨‹å°†ä¼šä¸æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) æ‰€åšçš„å¾ˆç›¸ä¼¼ã€‚

{#if fw === 'pt'}

## ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹]]

ä½¿ç”¨ `Trainer` çš„ä»£ç ä¼šå’Œä»¥å‰ä¸€æ ·ï¼Œå”¯ä¸€çš„å˜åŒ–æ˜¯æ•°æ®å¦‚ä½•æ•´ç†æˆ `batch` ä»¥åŠè®¡ç®—è¯„ä¼°çš„åˆ†æ•°ã€‚

{:else}

## ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹]]

ä½¿ç”¨ `Keras` çš„ä»£ç å°†ä¸ä¹‹å‰éå¸¸ç›¸ä¼¼ï¼›å”¯ä¸€çš„å˜åŒ–æ˜¯æ•°æ®å¦‚ä½•æ•´ç†æˆ `batch` ä»¥åŠè®¡ç®—è¯„ä¼°çš„åˆ†æ•°ã€‚

{/if}

### æ•´ç†æ•°æ® [[æ•´ç†æ•°æ®]]

æˆ‘ä»¬ä¸èƒ½åƒ [ç¬¬ä¸‰ç« ](/course/chapter3) é‚£æ ·ç›´æ¥ä½¿ç”¨ `DataCollatorWithPadding` ï¼Œå› ä¸ºé‚£æ ·åªä¼šå¡«å……è¾“å…¥ï¼ˆinputs IDã€æ³¨æ„æ©ç å’Œ tokens ç±»å‹ IDï¼‰ã€‚é™¤äº†è¾“å…¥éƒ¨åˆ†ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ ‡ç­¾ä¹Ÿä½¿ç”¨ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„æ–¹å¼å¡«å……ï¼Œä»¥ä¿è¯å®ƒä¸è¾“å…¥çš„å¤§å°ç›¸åŒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `-100` è¿›è¡Œå¡«å……ï¼Œä»¥ä¾¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥å¡«å……çš„å†…å®¹ã€‚

ä»¥ä¸Šçš„è¿™äº›è¿‡ç¨‹å¯ä»¥ç”± [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification) å®ç°ã€‚å®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰å¡«å……åŠŸèƒ½çš„æ•°æ®æ•´ç†å™¨ï¼Œä½¿ç”¨æ—¶åªéœ€è¦ä¼ å…¥ç”¨äºé¢„å¤„ç†è¾“å…¥çš„ `tokenizer` ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šæµ‹è¯•è¿™ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆåœ¨è®­ç»ƒé›†ä¸­çš„å‡ ä¸ªç¤ºä¾‹ä¸Šè°ƒç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

å°†æ•°æ®æ•´ç†å™¨çš„ç»“æœä¸æ•°æ®é›†ä¸­æœªç»å¤„ç†çš„ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå·²ç»ä½¿ç”¨ `-100` å°†ç¬¬äºŒç»„æ ‡ç­¾å¡«å……åˆ°äº†ä¸ç¬¬ä¸€ç»„æ ‡ç­¾ç›¸åŒçš„é•¿åº¦ã€‚

{:else}

æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å·²å‡†å¤‡å°±ç»ªï¼ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ `to_tf_dataset()` æ–¹æ³•åˆ›å»ºä¸€ä¸ª `tf.data.Dataset` ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ `model.prepare_tf_dataset()` æ¥ä½¿ç”¨æ›´å°‘çš„æ¨¡æ¿ä»£ç æ¥åˆ›å»º `Dataset`â€”â€”ä½ å°†åœ¨æœ¬ç« çš„å…¶ä»–å°èŠ‚ä¸­çœ‹åˆ°è¿™æ ·çš„æ“ä½œã€‚
```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

ä¸‹ä¸€ç«™ï¼šæ¨¡å‹æœ¬èº«ã€‚

{/if}

{#if fw === 'tf'}

### åˆå§‹åŒ–æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶ Token åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `TFAutoModelForTokenClassification` ç±»ã€‚å®ä¾‹åŒ–æ­¤æ¨¡å‹æ—¶è¦è®°å¾—ä¼ é€’æˆ‘ä»¬æ ‡ç­¾çš„æ•°é‡ï¼Œæœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦åˆ›å»ºä¸€ä¸ªå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·çš„æ¨ç†å°éƒ¨ä»¶ï¼Œé‚£ä¹ˆå°±éœ€è¦ç”¨æœ€æ ‡å‡†çš„æ–¹æ³•æ­£ç¡®è®¾ç½®æ ‡ç­¾çš„å¯¹åº”å…³ç³»ã€‚

æœ€æ ‡å‡†çš„æ–¹æ³•æ˜¯ç”¨ä¸¤ä¸ªå­—å…¸ `id2label` å’Œ `label2id` æ¥è®¾ç½®å®ƒä»¬ï¼Œè¿™ä¸¤ä¸ªå­—å…¸åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠåå‘çš„æ˜ å°„ï¼š

```py
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç°åœ¨æˆ‘ä»¬åªéœ€å°†è¿™ä¸¤ä¸ªå­—å…¸ä¼ é€’ç»™ `TFAutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„é…ç½®ä¸­ï¼Œç„¶åè¢«æ­£ç¡®åœ°ä¿å­˜å’Œä¸Šä¼ åˆ° Hubï¼š

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­å®ä¾‹åŒ– `TFAutoModelForTokenClassification` ç±»ä¸€æ · åˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹å¤´éƒ¨çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–° Token åˆ†ç±»å¤´éƒ¨çš„æƒé‡ï¼‰ï¼Œåˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬é©¬ä¸Šå°±è¦è®­ç»ƒè¿™äº›æƒé‡ã€‚é¦–å…ˆè®©æˆ‘ä»¬ç¡®è®¤ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ‡ç­¾æ•°é‡ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœä½ çš„æ¨¡å‹çš„æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œé‚£ä¹ˆåœ¨åé¢è°ƒç”¨ `model.fit()` æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ™¦æ¶©çš„é”™è¯¯ã€‚è¿™å¯èƒ½ä¼šä»¤äººçƒ¦æ¼ï¼Œæ‰€ä»¥ç¡®ä¿ä½ åšäº†è¿™ä¸ªæ£€æŸ¥ï¼Œç¡®è®¤ä½ çš„æ ‡ç­¾æ•°é‡æ˜¯æ­£ç¡®çš„ã€‚

</Tip>

### å¾®è°ƒæ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä¸è¿‡ï¼Œæˆ‘ä»¬åœ¨è¿™ä¹‹å‰è¿˜è¦åšä¸¤ä»¶äº‹ï¼šç™»å½•åˆ° Hugging Face å¹¶è®¾ç½®æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªä¾¿æ·å‡½æ•°å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face è´¦å·å’Œå¯†ç ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ notebook ä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹ä»£ç ï¼š

```bash
huggingface-cli login
```

ç™»å½•åï¼Œæˆ‘ä»¬å¯ä»¥ç¼–è¯‘æˆ‘ä»¬æ¨¡å‹æ‰€éœ€è¦çš„æ‰€æœ‰é…ç½®ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªä¾¿æ·çš„ `create_optimizer()` å‡½æ•°ï¼Œä¸ `TensorFlow` å†…ç½®çš„ `Adam` ä¼˜åŒ–å™¨ç›¸æ¯”ï¼Œå®ƒå¯ä»¥æä¾›ä¸€ä¸ªå¸¦æœ‰æ°å½“çš„æƒé‡è¡°å‡å’Œå­¦ä¹ ç‡è¡°å‡æœºåˆ¶çš„ `AdamW` ä¼˜åŒ–å™¨ï¼Œä¸è¿‡è¿™ä¸¤ä¸ªæœºåˆ¶éƒ½å°†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼š

```python
from transformers import create_optimizer
import tensorflow as tf

# åœ¨æ··åˆç²¾åº¦ float16 ä¸­è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ²¡æœ‰ç»™ `compile()` æä¾› `loss` å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹å®é™…ä¸Šå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ–¹å¼è‡ªåŠ¨è®¡ç®—æŸå¤± â€”â€” å¦‚æœä½ åœ¨ç¼–è¯‘æ—¶æ²¡æœ‰æä¾›æŸå¤±çš„è®¡ç®—æ–¹æ³•æˆ–åœ¨è¾“å…¥ä¸­æä¾›çœŸå®çš„æ ‡ç­¾å€¼ï¼ˆå°±åƒæˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­æ‰€åšçš„é‚£æ ·ï¼‰ï¼Œé‚£ä¹ˆæ¨¡å‹å°†ä½¿ç”¨å†…éƒ¨é»˜è®¤çš„ `loss` è®¡ç®—æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå…·ä½“çš„è®¡ç®—æ–¹å¼å–å†³äºä½ é€‰æ‹©çš„ä»»åŠ¡å’Œæ¨¡å‹ç±»å‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `PushToHubCallback` å›è°ƒå‡½æ•°ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œå¹¶åœ¨æ‹Ÿåˆæ¨¡å‹æ—¶æ·»åŠ è¯¥å›è°ƒå‡½æ•°ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

ä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³è¦æ¨é€çš„ä»“åº“çš„å…¨åï¼ˆç‰¹åˆ«éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä½ éœ€è¦æ¨é€ç»™æŸä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/bert-finetuned-ner"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™ä¸ªä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¹‹å†…ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œä¾‹å¦‚ `"cool_huggingface_user/bert-finetuned-ner"` ã€‚

<Tip>

ğŸ’¡ å¦‚æœè®¾ç½®äº†æ¨¡å‹ä¿å­˜çš„è·¯å¾„ï¼Œå¹¶ä¸”åœ¨é‚£é‡Œå·²ç»å­˜åœ¨äº†ä¸€ä¸ªéç©ºçš„åŒåæ–‡ä»¶å¤¹ï¼Œé‚£ä¹ˆè¯¥ç›®å½•åº”è¯¥æ˜¯ Hub ä»“åº“å…‹éš†åœ¨æœ¬åœ°çš„ç‰ˆæœ¬ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™ä¼šåœ¨è°ƒç”¨ model.ï¬t() æ–¹æ³•æ—¶æ”¶åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„è·¯å¾„ã€‚

</Tip>

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰éœ€è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

åˆ°æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ä¸€ä¸ª tokens åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œä½ ï¼ä½†æˆ‘ä»¬çš„æ¨¡å‹çœŸçš„å¥½ç”¨å—ï¼Ÿæˆ‘ä»¬åº”è¯¥æ‰¾å‡ºä¸€äº›æŒ‡æ ‡æ¥å¯¹æ¨¡å‹è¿›è¯„ä¼°ã€‚

{/if}

### è¯„ä¼°æŒ‡æ ‡ [[è¯„ä¼°æŒ‡æ ‡]]

{#if fw === 'pt'}

è¦è®© `Trainer` åœ¨æ¯ä¸ªå‘¨æœŸè®¡ç®—ä¸€ä¸ªæŒ‡æ ‡ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ª `compute_metrics()` å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„è¾“å…¥æ˜¯é¢„æµ‹å€¼å’Œæ ‡ç­¾çš„æ•°ç»„ï¼Œå¹¶è¿”å›å¸¦æœ‰æŒ‡æ ‡åç§°å’Œè¯„ä¼°ç»“æœçš„å­—å…¸ã€‚

ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ç»å…¸æ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval) ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval åº“ï¼š

```py
!pip install seqeval
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ `evaluate.load()` å‡½æ•°åŠ è½½å®ƒï¼Œå°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­æ‰€åšçš„é‚£æ ·ï¼š

{:else}

ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ç»å…¸æ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval) ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval åº“ï¼š

```py
!pip install seqeval
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ `evaluate.load()` å‡½æ•°åŠ è½½å®ƒï¼Œå°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­æ‰€åšçš„é‚£æ ·ï¼š

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

è¯¥æŒ‡æ ‡å’Œå¸¸è§„çš„è¯„æµ‹æŒ‡æ ‡æœ‰äº›åŒºåˆ«ï¼šå®ƒéœ€è¦å­—ç¬¦ä¸²å½¢å¼çš„æ ‡ç­¾åˆ—è¡¨è€Œä¸æ˜¯æ•´æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨å°†å®ƒä»¬ä¼ é€’ç»™æŒ‡æ ‡ä¹‹å‰å°†é¢„æµ‹å€¼å’Œæ ‡ç­¾ç”±æ•°å­—è§£ç ä¸ºå­—ç¬¦ä¸²ã€‚è®©æˆ‘ä»¬å…ˆç”¨ä¸€äº›æµ‹è¯•æ•°æ®çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

é¦–å…ˆï¼Œè·å–ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾ã€‚
```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ”¹ç´¢å¼• 2 å¤„çš„å€¼æ¥ä¸ºè¿™äº›æ ‡ç­¾åˆ›å»ºå‡çš„é¢„æµ‹å€¼æ¥æµ‹è¯•è¯¥æŒ‡æ ‡ï¼š

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

è¯·æ³¨æ„ï¼Œè¯¥æŒ‡æ ‡çš„è¾“å…¥æ˜¯é¢„æµ‹åˆ—è¡¨ï¼ˆä¸æ˜¯ä¸€ä¸ªï¼‰å’Œæ ‡ç­¾åˆ—è¡¨ï¼Œè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

ç»“æœè¿”å›å¾ˆå¤šä¿¡æ¯ï¼åŒ…æ‹¬æ¯ä¸ªå•ç‹¬å®ä½“åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å°†åªä¿ç•™æ€»åˆ†ï¼Œä½†æ˜¯ä½ å¯ä»¥è‡ªç”±åœ°è°ƒæ•´ `compute_metrics()` å‡½æ•°è¿”å›çš„æ‰€éœ€è¦æŒ‡æ ‡ã€‚è¿™ä¸ªå‡½æ•°ä¸­æˆ‘ä»¬é¦–å…ˆä¼šå…ˆå–é¢„æµ‹ `logits` çš„ `argmax`ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé¢„æµ‹å€¼ã€‚ `compute_metrics()` å‡½æ•°é¦–å…ˆå– `logits` çš„ `argmax`ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºé¢„æµ‹å€¼ï¼ˆé€šå¸¸æƒ…å†µä¸‹ï¼Œlogits å’Œæ¦‚ç‡çš„é¡ºåºæ˜¯ç›¸åŒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨ softmaxï¼‰ã€‚ç„¶åæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾å’Œé¢„æµ‹å€¼éƒ½ä»æ•´æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚æˆ‘ä»¬åˆ é™¤æ‰€æœ‰æ ‡ç­¾ä¸º `-100` çš„å€¼ï¼Œæœ€åå°†ç»“æœä¼ é€’ç»™ `metric.compute()` æ–¹æ³•ï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

ç°åœ¨å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬ä¸‹é¢å°±å¯ä»¥å¼€å§‹å®šä¹‰æˆ‘ä»¬çš„ `Trainer` äº†ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ª `model` å¹¶å¯¹å…¶å¾®è°ƒï¼

{:else}

å®ƒè¿”å›äº†å¤§é‡çš„ä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬å°è¯•ä½¿ç”¨çœŸå®çš„æ¨¡å‹é¢„æµ‹æ¥è®¡ç®—åˆ†æ•°ã€‚

åœ¨ä½¿ç”¨åŸºäºTensorFlow çš„æ¡†æ¶æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¸ä¼šæŠŠé¢„æµ‹æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´åºåˆ—é•¿åº¦ä¸ç»Ÿä¸€ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ç›´æ¥ä½¿ç”¨ `model.predict()` æ–¹æ³• â€”â€” ä½†è¿™å¹¶ä¸èƒ½é˜»æ­¢æˆ‘ä»¬ã€‚æˆ‘ä»¬å¯ä»¥é€æ‰¹è·å–ä¸€äº›é¢„æµ‹å¹¶åœ¨åœ¨æ­¤çš„è¿‡ç¨‹ä¸­å°†å®ƒä»¬æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„åˆ—è¡¨ï¼Œåˆ é™¤è¡¨ç¤º `masking/padding` çš„ `-100` tokens ç„¶ååœ¨åˆå¹¶åçš„å¤§åˆ—è¡¨ä¸Šè®¡ç®—åº¦é‡å€¼ï¼š

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict_on_batch(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

ä¸æˆ‘ä»¬çš„æ¨¡å‹ç›¸æ¯”ï¼Œä½ çš„æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ï¼Ÿå¦‚æœä½ çœ‹åˆ°ç±»ä¼¼çš„æ•°å­—ï¼Œé‚£ä¹ˆä½ çš„è®­ç»ƒå°±æˆåŠŸäº†ï¼

{/if}

{#if fw === 'pt'}

### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶ Token åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForTokenClassification` ç±»ã€‚å®šä¹‰æ­¤æ¨¡å‹æ—¶è¦è®°å¾—ä¼ é€’æˆ‘ä»¬æ ‡ç­¾çš„æ•°é‡ï¼Œæœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·çš„æ¨ç†å°éƒ¨ä»¶ï¼Œå°±éœ€è¦ç”¨æœ€æ ‡å‡†çš„æ–¹æ³•æ­£ç¡®è®¾ç½®æ ‡ç­¾çš„å¯¹åº”å…³ç³»ã€‚

æœ€æ ‡å‡†çš„æ–¹å¼æ˜¯ç”¨ä¸¤ä¸ªå­—å…¸ `id2label` å’Œ `label2id` æ¥è®¾ç½®æ ‡ç­¾ï¼Œè¿™ä¸¤ä¸ªå­—å…¸åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠåå‘çš„æ˜ å°„ï¼š

```py
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç°åœ¨æˆ‘ä»¬åªéœ€å°†å®ƒä»¬ä¼ é€’ç»™ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„é…ç½®ä¸­ï¼Œç„¶åè¢«æ­£ç¡®åœ°ä¿å­˜å’Œä¸Šä¼ åˆ° Hubï¼š

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­å®šä¹‰ `AutoModelForSequenceClassification` ç±»ä¸€æ · åˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´éƒ¨çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–° Token åˆ†ç±»å¤´éƒ¨çš„æƒé‡ï¼‰ï¼Œä¸è¿‡ä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬é©¬ä¸Šå°±è¦è®­ç»ƒè¿™äº›æƒé‡ã€‚åœ¨è¿™ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç¡®è®¤ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ‡ç­¾æ•°é‡ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœä½ çš„æ¨¡å‹çš„æ ‡ç­¾æ•°é‡æœ‰é”™è¯¯ï¼Œé‚£ä¹ˆåœ¨åé¢è°ƒç”¨ `Trainer.train()` æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ™¦æ¶©çš„é”™è¯¯ï¼ˆç±»ä¼¼äºâ€œCUDA errorï¼šdevice-side assert triggeredâ€ï¼‰ã€‚è¿™å¯èƒ½ä¼šä»¤äººçƒ¦æ¼ï¼Œæ‰€ä»¥ç¡®ä¿ä½ åšäº†è¿™ä¸ªæ£€æŸ¥ï¼Œç¡®è®¤ä½ çš„æ ‡ç­¾æ•°é‡æ˜¯æ­£ç¡®ã€‚

</Tip>

### å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼åœ¨å®šä¹‰ `Trainer` ä¹‹å‰ï¼Œæˆ‘ä»¬åªéœ€è¦åšæœ€åä¸¤ä»¶äº‹ï¼šç™»å½• Hugging Face å¹¶è®¾ç½®æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªæ–¹ä¾¿çš„å°å·¥å…·å¯ä»¥å¸®åŠ©ä½ ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

ç™»å½•åï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¾ç½®æˆ‘ä»¬çš„ `TrainingArguments` ï¼š

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

ä½ å·²ç»å¯¹å¤§å¤šæ•°å†…å®¹æœ‰æ‰€äº†è§£äº†ï¼šæˆ‘ä»¬è®¾ç½®äº†ä¸€äº›è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒçš„è½®æ•°å’Œæƒé‡è¡°å‡ï¼‰ï¼Œå¹¶è®¾å®š `push_to_hub=True` ï¼Œè¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›åœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ç»“æŸæ—¶ä¿å­˜å¹¶è¯„ä¼°æ¨¡å‹ï¼Œç„¶åå°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æ³¨æ„ï¼Œä½ å¯ä»¥é€šè¿‡ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€çš„ä»“åº“çš„åç§°ï¼ˆç‰¹åˆ«éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä½ éœ€è¦æ¨é€ç»™æŸä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `TrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/bert-finetuned-ner"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¹‹å†…ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä»“åº“çš„åœ°å€æ˜¯ `"sgugger/bert-finetuned-ner"` ã€‚

<Tip>

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºè·¯å¾„å·²ç»å­˜åœ¨ä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œé‚£ä¹ˆå®ƒéœ€è¦æ˜¯ä½ æƒ³æ¨é€åˆ° hub çš„ä»“åº“çš„å…‹éš†åœ¨æœ¬åœ°çš„ç‰ˆæœ¬ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å°†åœ¨å£°æ˜ `Trainer` æ—¶é‡åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„è·¯å¾„ã€‚

</Tip>

æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Trainer` å¹¶å¯åŠ¨è®­ç»ƒï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬

```py
trainer.push_to_hub(commit_message="Training complete")
```

å¦‚æœä½ æƒ³æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œè¿™ä¸ªå‘½ä»¤ä¼šè¿”å›åˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼š

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

åŒæ—¶ `Trainer` è¿˜åˆ›å»ºå¹¶ä¸Šä¼ äº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡ã€‚åˆ°æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ä¸€ä¸ª tokens åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œä½ ï¼

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ [[è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯]]

ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾åœ°å®šåˆ¶ä½ éœ€è¦çš„éƒ¨åˆ†ã€‚å®ƒä¸æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3/4) ä¸­æ‰€åšçš„å†…å®¹å¾ˆç›¸ä¼¼ï¼Œä½†å¯¹è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›æ”¹åŠ¨ã€‚

### åšå¥½è®­ç»ƒå‰çš„å‡†å¤‡ [[åšå¥½è®­ç»ƒå‰çš„å‡†å¤‡]]

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†æ„å»º `DataLoader` ã€‚æˆ‘ä»¬å°† `data_collator` ä¼ é€’ç»™ `collate_fn` å‚æ•°å¹¶éšæœºæ‰“ä¹±è®­ç»ƒé›†ï¼Œä½†ä¸æ‰“ä¹±éªŒè¯é›†ï¼š

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸æ˜¯ç»§ç»­ä¹‹å‰çš„å¾®è°ƒï¼Œè€Œæ˜¯é‡æ–°å¼€å§‹ä» BERT å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç»å…¸ `AdamW` ï¼Œå®ƒç±»ä¼¼äº `Adam` ï¼Œä½†åœ¨æƒé‡è¡°å‡çš„æ–¹å¼ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

å½“æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰è¿™äº›å¯¹è±¡ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬å‘ä¼ é€’ç»™ `accelerator.prepare()` æ–¹æ³•ï¼š

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœä½ æ­£åœ¨ TPU ä¸Šè®­ç»ƒï¼Œä½ éœ€è¦å°†ä¸Šé¢å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æ›´å¤šè¯¦æƒ…è¯·å›é¡¾ [ç¬¬ä¸‰ç« ](/course/chapter3) ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` ä¼ é€’ç»™äº† `accelerator.prepare()` æ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ `len()` æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨å‡†å¤‡å¥½ `dataloader` åå†ä½¿ç”¨ `len()` ï¼Œå› ä¸ºæ”¹åŠ¨ `dataloader` ä¼šæ”¹å˜è®­ç»ƒé•¿åº¦çš„æ•°é‡ã€‚è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªä»å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç™»å½•çš„è¯ï¼Œé¦–å…ˆéœ€è¦ç™»å½•åˆ° Hugging Faceï¼Œç„¶åæ ¹æ®æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥å°† `repo_name` æ›¿æ¢ä¸ºä½ å–œæ¬¢çš„åå­—ï¼›åªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åå³å¯ï¼Œä½ å¯ä»¥ä½¿ç”¨ `get_full_repo_name()` å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„ `repo_name` ï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¯¥ä»“åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœæœ¬åœ°å·²ç»å­˜åœ¨åŒåçš„æ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹å¿…é¡»æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„ä»“åº“å…‹éš†åœ¨æœ¬åœ°çš„ç‰ˆæœ¬ï¼š

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚å®ƒå¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

### è®­ç»ƒå¾ªç¯ [[è®­ç»ƒå¾ªç¯]]

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `postprocess()` å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¼šæ¥æ”¶æ¨¡å‹çš„é¢„æµ‹å’ŒçœŸå®çš„æ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ `metric` ï¼ˆè¯„ä¼°å‡½æ•°ï¼‰å¯¹è±¡éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç¼–å†™è®­ç»ƒå¾ªç¯ã€‚åœ¨å®šä¹‰ä¸€ä¸ªè¿›åº¦æ¡æ¥è·Ÿè¸ªè®­ç»ƒçš„è¿›åº¦åï¼Œå¾ªç¯åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

- è®­ç»ƒæœ¬èº«ï¼Œè¿™æ˜¯ç»å…¸çš„è¿­ä»£è¿‡ç¨‹ï¼Œå³åœ¨ `train_dataloader` ä¸Šè¿›è¡Œè¿­ä»£ï¼Œåœ¨æ¨¡å‹ä¸Šå‰å‘ä¼ æ’­è®­ç»ƒæ•°æ®ï¼Œç„¶ååå‘ä¼ é€’ `loss` å’Œä¼˜åŒ–å‚æ•°
- è¯„ä¼°ï¼Œåœ¨è·å–æ¨¡å‹åœ¨ä¸€ä¸ª `batch` ä¸Šçš„è¾“å‡ºä¹‹åï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼šç”±äºä¸¤ä¸ªè¿›ç¨‹å¯èƒ½å·²å°†è¾“å…¥å’Œæ ‡ç­¾å¡«å……åˆ°ä¸åŒçš„å½¢çŠ¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `accelerator.pad_across_processes()` ä½¿é¢„æµ‹å’ŒçœŸå®çš„æ ‡ç­¾åœ¨è°ƒç”¨ `gather()` æ–¹æ³•ä¹‹å‰å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™æ ·åšï¼Œè¯„ä¼°å¾ªç¯å°†ä¼šå‡ºé”™æˆ–æ— é™æœŸå¡ä½ã€‚æœ€åæˆ‘ä»¬å°†ç»“æœå‘é€åˆ° `metric.add_batch()` æ–¹æ³•ä¸­ï¼Œå¹¶åœ¨è¯„ä¼°å¾ªç¯ç»“æŸæ—¶è°ƒç”¨ `metric.compute()` æ–¹æ³•ã€‚
- ä¿å­˜å’Œä¸Šä¼ ï¼Œé¦–å…ˆä¿å­˜æ¨¡å‹å’Œ `tokenizer` ç„¶åè°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚æ•° `blocking=False` æ¥å‘Šè¯‰ ğŸ¤— `Hub` åº“åœ¨ä¸€ä¸ªå¼‚æ­¥è¿›ç¨‹ä¸­æ¨é€ã€‚è¿™æ ·ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œè¿™ä¸ªæŒ‡ä»¤åœ¨åå°å°†æ¨¡å‹å’Œ `tokenizer` æ¨é€åˆ° `hub`ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„è®­ç»ƒå¾ªç¯ä»£ç ï¼š

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # å¡«å……æ¨¡å‹çš„é¢„æµ‹å’Œæ ‡ç­¾åæ‰èƒ½è°ƒç”¨ gathere()
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # ä¿å­˜å¹¶ä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡çœ‹åˆ°ä½¿ç”¨ ğŸ¤— Accelerate ä¿å­˜æ¨¡å‹ï¼Œè®©æˆ‘ä»¬èŠ±ç‚¹æ—¶é—´æ¥äº†è§£ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ä¸­çš„ä¸‰è¡Œä»£ç ï¼š

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

ç¬¬ä¸€è¡Œæ˜¯ä¸è¨€è‡ªæ˜çš„ï¼šå®ƒå‘Šè¯‰æ‰€æœ‰çš„è¿›ç¨‹å…ˆç­‰å¾…ï¼Œç›´åˆ°æ‰€æœ‰çš„è¿›ç¨‹éƒ½å¤„äºè¿™ä¸ªé˜¶æ®µå†ç»§ç»­ï¼ˆé˜»å¡ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½æœ‰ç›¸åŒçš„æ¨¡å‹ã€‚
ç¬¬äºŒè¡Œä»£ç ç”¨äºè·å– `unwrapped_model` ï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬å®šä¹‰çš„åŸºæœ¬æ¨¡å‹ã€‚ `accelerator.prepare()` æ–¹æ³•ä¼šä¸ºäº†åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å·¥ä½œè€Œå¯¹æ¨¡å‹è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼Œæ‰€ä»¥å®ƒä¸å†æœ‰ `save_pretraining()` æ–¹æ³•ï¼›ä½¿ç”¨ `accelerator.unwrap_model()` æ–¹æ³•å¯ä»¥æ’¤é”€å¯¹æ¨¡å‹çš„æ›´æ”¹ã€‚
åœ¨ç¬¬ä¸‰è¡Œä»£ç ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨ `save_pretraining()` ï¼Œå¹¶æŒ‡å®š `accelerator.save()` ä½œä¸º `save_function` è€Œä¸æ˜¯é»˜è®¤çš„ `torch.save()` ã€‚

å®Œæˆè¿™äº›æ“ä½œåï¼Œä½ åº”è¯¥æ‹¥æœ‰ä¸€ä¸ªä¸ `Trainer` è®­ç»ƒå‡ºçš„æ¨¡å‹ç»“æœç›¸å½“ç±»ä¼¼çš„æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨ [huggingface-course/bert-finetuned-ner-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate) æŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨è¿™äº›ä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³åœ¨è®­ç»ƒå¾ªç¯ä¸­æµ‹è¯•ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°å®ƒä»¬ï¼

{/if}

## ä½¿ç”¨å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨å¾®è°ƒæ¨¡å‹]]

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨åœ¨æ¨¡å‹ä¸­å¿ƒå¾®è°ƒçš„æ¨¡å‹å’Œæ¨ç†å°éƒ¨ä»¶ã€‚åœ¨æœ¬åœ°ä½¿ç”¨ `pipeline` æ¥ä½¿ç”¨å®ƒéå¸¸å®¹æ˜“ï¼Œä½ åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡ç­¾ï¼š

```py
from transformers import pipeline

# å°†æ­¤æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

å¤ªæ£’äº†ï¼æˆ‘ä»¬çš„æ¨¡å‹ä¸æ­¤ç®¡é“çš„é»˜è®¤æ¨¡å‹ä¸€æ ·æœ‰æ•ˆï¼
