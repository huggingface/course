<FrameworkSwitchCourse {fw} />

# æå–æ–‡æœ¬æ‘˜è¦ [[æå–æ–‡æœ¬æ‘˜è¦]]

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},
]} />

{/if}


åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Transformer æ¨¡å‹å°†é•¿ç¯‡æ–‡æ¡£å‹ç¼©ä¸ºæ‘˜è¦ï¼Œè¿™é¡¹ä»»åŠ¡ç§°ä¸ºæ–‡æœ¬æ‘˜è¦ã€‚è¿™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ç³»åˆ—èƒ½åŠ›ï¼Œä¾‹å¦‚ç†è§£é•¿ç¯‡æ–‡ç« å¹¶ä¸”ç”Ÿæˆèƒ½å¤Ÿæ•æ‰æ–‡æ¡£ä¸­ä¸»è¦ä¸»é¢˜çš„è¿è´¯æ–‡æœ¬ã€‚ä½†æ˜¯ï¼Œå¦‚æœåšå¾—å¥½ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å‡è½»å„ä¸ªé¢†åŸŸçš„äººè¯¦ç»†é˜…è¯»é•¿æ–‡æ¡£çš„è´Ÿæ‹…ï¼Œä»è€ŒåŠ å¿«ä¸šåŠ¡æµç¨‹ã€‚

<Youtube id="yHnr5Dk2zCI"/>

å°½ç®¡åœ¨ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization=downloads) ä¸Šå·²ç»å­˜åœ¨å„ç§æå–æ–‡æœ¬æ‘˜è¦çš„å¾®è°ƒæ¨¡å‹ï¼Œä½†æ˜¯å‡ ä¹æ‰€æœ‰çš„è¿™äº›æ¨¡å‹éƒ½åªé€‚ç”¨äºè‹±æ–‡æ–‡æ¡£ã€‚å› æ­¤ï¼Œä¸ºäº†åœ¨æœ¬èŠ‚ä¸­æ·»åŠ ä¸€äº›ä¸ä¸€æ ·çš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬å°†ä¸ºè‹±è¯­å’Œè¥¿ç­ç‰™è¯­è®­ç»ƒä¸€ä¸ªåŒè¯­æ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ç»“æŸæ—¶ï¼Œä½ å°†æœ‰ä¸€ä¸ªå¯ä»¥æ€»ç»“å®¢æˆ·è¯„è®ºçš„ [æ¨¡å‹](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) ã€‚

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

å¦‚æœä½ è¯•ä¸€è¯•çš„è¯ï¼Œå°±å‘ç°æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆéå¸¸ç®€æ´çš„æ‘˜è¦ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä»å®¢æˆ·åœ¨äº§å“è¯„è®ºä¸­æä¾›çš„æ ‡é¢˜ä¸­å­¦åˆ°çš„ã€‚è®©æˆ‘ä»¬é¦–å…ˆä¸ºè¿™é¡¹ä»»åŠ¡å‡†å¤‡ä¸€ä¸ªåˆé€‚çš„åŒè¯­è¯­æ–™åº“ã€‚

## å‡†å¤‡å¤šè¯­è¨€è¯­æ–™åº“ [[å‡†å¤‡å¤šè¯­è¨€è¯­æ–™åº“]]

æˆ‘ä»¬å°†ä½¿ç”¨ [å¤šè¯­è¨€äºšé©¬é€Šè¯„è®ºè¯­æ–™åº“](https://huggingface.co/datasets/amazon_reviews_multi) åˆ›å»ºæˆ‘ä»¬çš„åŒè¯­æ‘˜è¦å™¨ã€‚è¯¥è¯­æ–™åº“ç”±å…­ç§è¯­è¨€çš„äºšé©¬é€Šäº§å“è¯„è®ºç»„æˆï¼Œé€šå¸¸ç”¨äºå¤šè¯­è¨€åˆ†ç±»å™¨çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œç”±äºæ¯æ¡è¯„è®ºéƒ½é™„æœ‰ä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæˆ‘ä»¬æ¨¡å‹å­¦ä¹ çš„å‚è€ƒæ‘˜è¦ï¼é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä» Hugging Face Hub ä¸‹è½½è‹±è¯­å’Œè¥¿ç­ç‰™è¯­å­é›†ï¼š

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

å¦‚ä½ æ‰€è§ï¼Œåœ¨è‹±è¯­æ•°æ®é›†çš„ `train` éƒ¨åˆ†æœ‰ 200,000 æ¡è¯„è®ºï¼Œ `validation` å’Œ `test` éƒ¨åˆ†æœ‰ 5,000 æ¡è¯„è®ºã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„è¯„è®ºæ­£æ–‡å’Œæ ‡é¢˜ä¿å­˜åœ¨ `review_body` å’Œ `review_title` åˆ—ä¸­ã€‚è®©æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥ä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å–ä¸€äº›æ ·æœ¬ï¼Œè¯¥å‡½æ•°ä½¿ç”¨æˆ‘ä»¬åœ¨ [ç¬¬äº”ç« ](/course/chapter5) å­¦åˆ°è¿‡ï¼š

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itâ€™s job and itâ€™s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** æ›´æ”¹ `Dataset.shuffle()` å‘½ä»¤ä¸­çš„éšæœºç§å­ä»¥æ¢ç´¢è¯­æ–™åº“ä¸­çš„å…¶ä»–è¯„è®ºã€‚å¦‚æœä½ æ˜¯è¯´è¥¿ç­ç‰™è¯­çš„äººï¼Œè¯·æŸ¥çœ‹ `spanish_dataset` ä¸­çš„ä¸€äº›è¯„è®ºï¼Œçœ‹çœ‹æ ‡é¢˜æ˜¯å¦åƒæ˜¯åˆç†çš„æ‘˜è¦ã€‚

</Tip>

è¿™ä¸ªç¤ºä¾‹æ˜¾ç¤ºäº†äººä»¬é€šå¸¸åœ¨ç½‘ä¸Šè¯„è®ºçš„å¤šæ ·æ€§ï¼Œä»ç§¯æçš„åˆ°æ¶ˆæçš„ï¼ˆä»¥åŠä»‹äºä¸¤è€…ä¹‹é—´çš„è¯„è®ºï¼ï¼‰ã€‚å°½ç®¡å¸¦æœ‰â€œmehâ€æ ‡é¢˜çš„ç¤ºä¾‹çš„ä¿¡æ¯é‡ä¸å¤§ï¼Œä½†å…¶ä»–æ ‡é¢˜çœ‹èµ·æ¥åƒæ˜¯å¯¹è¯„è®ºæœ¬èº«çš„ä¸é”™çš„æ€»ç»“ã€‚åœ¨å•ä¸ª GPU ä¸Šè®­ç»ƒæ‰€æœ‰ 400,000 æ¡è¯„è®ºçš„æ‘˜è¦æ¨¡å‹å°†èŠ±è´¹å¤ªé•¿æ—¶é—´ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸“æ³¨äºä¸ºå•ä¸ªäº§å“é¢†åŸŸç”Ÿæˆæ‘˜è¦ã€‚ä¸ºäº†äº†è§£æˆ‘ä»¬å¯ä»¥é€‰æ‹©å“ªäº›é¢†åŸŸï¼Œè®©æˆ‘ä»¬å°† `english_dataset` è½¬æ¢ä¸º `pandas.DataFrame` ï¼Œå¹¶è®¡ç®—æ¯ä¸ªäº§å“ç±»åˆ«çš„è¯„è®ºæ•°é‡ï¼š

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# æ˜¾ç¤ºå‰ 20 ä¸ªäº§å“çš„æ•°é‡
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

åœ¨è‹±è¯­æ•°æ®é›†ä¸­ï¼Œæœ€å—æ¬¢è¿çš„äº§å“æ˜¯å®¶å±…ç”¨å“ã€æœè£…å’Œæ— çº¿ç”µå­äº§å“ã€‚ä¸è¿‡ï¼Œä¸ºäº†å¸¦æœ‰äºšé©¬é€Šçš„ç‰¹è‰²ï¼Œè®©æˆ‘ä»¬ä¸“æ³¨äºæ€»ç»“ä¹¦ç±çš„è¯„è®ºâ€”â€”æ¯•ç«Ÿï¼Œè¿™æ˜¯äºšé©¬é€Šè¿™å®¶å…¬å¸æˆç«‹çš„åŸºç¡€ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªç¬¦åˆè¦æ±‚çš„äº§å“ç±»åˆ«ï¼ˆ `book` å’Œ `digital_ebook_purchase` ï¼‰ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç”¨è¿™ä¸¤ä¸ªäº§å“ç±»åˆ«è¿‡æ»¤ä¸¤ç§è¯­è¨€çš„æ•°æ®é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬äº”ç« ](/course/chapter5) å­¦åˆ°çš„ï¼Œ `Dataset.filter()` å‡½æ•°å¯ä»¥è®©æˆ‘ä»¬éå¸¸æœ‰æ•ˆåœ°å¯¹æ•°æ®é›†è¿›è¡Œåˆ‡ç‰‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥è¿›è¡Œæ­¤æ“ä½œï¼š

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

å½“æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‡½æ•°å¯¹ `english_dataset` å’Œ `spanish_dataset` è¿‡æ»¤åï¼Œç»“æœå°†åªåŒ…å«æ¶‰åŠä¹¦ç±ç±»åˆ«çš„é‚£äº›è¡Œã€‚åœ¨ä½¿ç”¨è¿‡æ»¤å™¨ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°† `english_dataset` çš„æ ¼å¼ä» `"pandas"` åˆ‡æ¢å› `"arrow"` ï¼š

```python
english_dataset.reset_format()
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿‡æ»¤å™¨åŠŸèƒ½ï¼Œä½œä¸ºä¸€ä¸ªåŸºæœ¬çš„æ£€æŸ¥ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€äº›è¯„è®ºçš„æ ·æœ¬ï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯å¦ç¡®å®ä¸ä¹¦ç±æœ‰å…³ï¼š

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

å¥½å§ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯„è®ºå¹¶ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„ä¹¦ç±ï¼Œä¹Ÿå¯èƒ½æ˜¯æŒ‡æ—¥å†å’Œ OneNote ç­‰ç”µå­åº”ç”¨ç¨‹åºç­‰å†…å®¹ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¯¥é¢†åŸŸä¼¼ä¹ä¹Ÿé€‚åˆè®­ç»ƒæ‘˜è¦æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬æŸ¥ç­›é€‰é€‚åˆæ­¤ä»»åŠ¡çš„å„ç§æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜æœ‰æœ€åä¸€ç‚¹æ•°æ®å‡†å¤‡è¦åšï¼šå°†è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡è¯„è®ºä½œä¸ºå•ä¸ª `DatasetDict` å¯¹è±¡ç»„åˆèµ·æ¥ã€‚ğŸ¤— Datasets æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ `concatenate_datasets()` å‡½æ•°ï¼Œå®ƒï¼ˆåå¦‚å…¶å®ï¼‰å°†æŠŠä¸¤ä¸ª `Dataset` å¯¹è±¡å †å åœ¨ä¸€èµ·ã€‚å› æ­¤ï¼Œä¸ºäº†åˆ›å»ºæˆ‘ä»¬çš„åŒè¯­æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†éå†æ•°æ®é›†çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œå¹¶æ‰“ä¹±ç»“æœä»¥ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹ä¸ä¼šè¿‡åº¦æ‹Ÿåˆå•ä¸€è¯­è¨€ï¼š

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# æŒ‘é€‰ä¸€äº›æ ·ä¾‹
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÃ‘ADO'
'>> Review: Me llegÃ³ el dÃ­a que tocaba, junto a otros libros que pedÃ­, pero la caja llegÃ³ en mal estado lo cual daÃ±Ã³ las esquinas de los libros porque venÃ­an sin protecciÃ³n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

è¿™çš„ç¡®çœ‹èµ·æ¥åƒæ˜¯æ··åˆäº†è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„è¯„è®ºï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªè®­ç»ƒè¯­æ–™åº“ï¼Œæœ€åè¦æ£€æŸ¥çš„ä¸€ä»¶äº‹æ˜¯è¯„è®ºåŠå…¶æ ‡é¢˜ä¸­å•è¯çš„åˆ†å¸ƒã€‚è¿™å¯¹äºæ‘˜è¦ä»»åŠ¡å°¤å…¶é‡è¦ï¼Œå…¶ä¸­æ•°æ®ä¸­å¦‚æœå‡ºç°å¤§é‡å‚è€ƒæ‘˜è¦è¿‡äºç®€çŸ­ä¼šä½¿æ¨¡å‹åå‘äºç”Ÿæˆçš„æ‘˜è¦ä¸­ä»…æœ‰ä¸€ä¸¤ä¸ªå•è¯ã€‚ä¸‹é¢çš„å›¾ä¸­æ˜¾ç¤ºäº†å•è¯åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰äº›æ ‡é¢˜ä¸¥é‡åå‘äº 1-2 ä¸ªå•è¯ï¼š

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†è¿‡æ»¤æ‰æ ‡é¢˜éå¸¸çŸ­çš„ç¤ºä¾‹ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´æœ‰æ•ˆçš„æ‘˜è¦ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡æ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç²—ç•¥çš„å¯å‘å¼æ–¹æ³•åœ¨ç©ºç™½å¤„æ‹†åˆ†æ ‡é¢˜çš„å•è¯ï¼Œç„¶åç”¨æˆ‘ä»¬å¼ºå¤§çš„ `Dataset.filter()` æ–¹æ³•å¦‚ä¸‹ï¼š

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æˆ‘ä»¬çš„è¯­æ–™åº“ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›å¯ä»¥å¯¹å…¶è¿›è¡Œå¾®è°ƒçš„å¯é€‰çš„ Transformer æ¨¡å‹ï¼

## æ–‡æœ¬æ‘˜è¦æ¨¡å‹ [[æ–‡æœ¬æ‘˜è¦æ¨¡å‹]]

å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç§ç±»ä¼¼äºæœºå™¨ç¿»è¯‘çš„ä»»åŠ¡ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªåƒè¯„è®ºè¿™æ ·çš„æ–‡æœ¬æ­£æ–‡ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å…¶â€œç¿»è¯‘â€æˆä¸€ä¸ªè¾ƒçŸ­çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶æ•æ‰åˆ°è¾“å…¥æ–‡æœ¬çš„ä¸»è¦ç‰¹å¾ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç”¨äºæ–‡æœ¬æ‘˜è¦çš„ Transformer æ¨¡å‹é‡‡ç”¨äº†æˆ‘ä»¬åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) é‡åˆ°çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚å°½ç®¡æœ‰ä¸€äº›ä¾‹å¤–ï¼Œä¾‹å¦‚ GPT ç³»åˆ—æ¨¡å‹ï¼Œå®ƒä»¬åœ¨ few-shotï¼ˆå°‘é‡å¾®è°ƒï¼‰ä¹‹åä¹Ÿå¯ä»¥æå–æ‘˜è¦ã€‚ä¸‹è¡¨åˆ—å‡ºäº†ä¸€äº›å¯ä»¥è¿›è¡Œæ‘˜è¦å¾®è°ƒçš„æµè¡Œé¢„è®­ç»ƒæ¨¡å‹ã€‚

| Transformer æ¨¡å‹ | æè¿°                                                                                                                                                                                                    | å¤šç§è¨€ï¼Ÿ|
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
| [GPT-2](https://huggingface.co/gpt2-xl) | è™½ç„¶è®­ç»ƒä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œä½†ä½ å¯ä»¥é€šè¿‡åœ¨è¾“å…¥æ–‡æœ¬æœ«å°¾é™„åŠ â€œTL;DRâ€æ¥ä½¿ GPT-2 ç”Ÿæˆæ‘˜è¦ã€‚|      âŒ       |
| [PEGASUS](https://huggingface.co/google/pegasus-large) | åœ¨é¢„è®­ç»ƒæ—¶çš„ç›®æ ‡æ˜¯æ¥é¢„æµ‹å¤šå¥å­æ–‡æœ¬ä¸­çš„å±è”½å¥å­ã€‚è¿™ä¸ªé¢„è®­ç»ƒç›®æ ‡æ¯”æ™®é€šè¯­è¨€å»ºæ¨¡æ›´æ¥è¿‘æ–‡æœ¬æ‘˜è¦ï¼Œå¹¶ä¸”åœ¨æµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ã€‚|      âŒ       |
| [T5](https://huggingface.co/t5-base) | é€šç”¨çš„ Transformer æ¶æ„ï¼Œæ‰€æœ‰ä»»åŠ¡éƒ½ä»¥æ–‡æœ¬åˆ°æ–‡æœ¬çš„æ¡†æ¶è¿›è¡Œæè¿°ï¼›ä¾‹å¦‚ï¼Œæ¨¡å‹æ–‡æœ¬æ‘˜è¦çš„è¾“å…¥æ ¼å¼æ˜¯ `summarize: ARTICLE` ã€‚|      âŒ       |
| [mT5](https://huggingface.co/google/mt5-base) | T5 çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œåœ¨å¤šè¯­è¨€ Common Crawl è¯­æ–™åº“ ï¼ˆmC4ï¼‰ ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–äº† 101 ç§è¯­è¨€ã€‚|      âœ…       |
| [BART](https://huggingface.co/facebook/bart-base) | ä¸€ç§æ–°é¢–çš„ Transformer æ¶æ„ï¼Œå…¶ä¸­åŒ…å«ç»è¿‡è®­ç»ƒçš„ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆï¼Œä»¥é‡å»ºè¢«ç ´åçš„è¾“å…¥ï¼Œç»“åˆäº† BERT å’Œ GPT-2 çš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚|      âŒ       |
| [mBART-50](https://huggingface.co/facebook/mbart-large-50) | BART çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œé¢„è®­ç»ƒäº† 50 ç§è¯­è¨€ã€‚|      âœ…       |

ä»æ­¤è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¤§å¤šæ•°ç”¨äºæ‘˜è¦çš„ Transformer æ¨¡å‹ï¼ˆä»¥åŠå¤§å¤šæ•° NLP ä»»åŠ¡ï¼‰éƒ½æ˜¯å•ä¸€è¯­è¨€çš„ã€‚å¦‚æœä½ çš„ä»»åŠ¡æ‰€ä½¿ç”¨çš„è¯­è¨€æ˜¯â€œæœ‰å¤§é‡è¯­æ–™åº“â€ï¼ˆå¦‚è‹±è¯­æˆ–å¾·è¯­ï¼‰çš„è¯­è¨€ï¼Œè¿™å¾ˆå¥½ã€‚ä½†å¯¹äºä¸–ç•Œå„åœ°æ­£åœ¨ä½¿ç”¨çš„æ•°åƒç§å…¶ä»–è¯­è¨€ï¼Œåˆ™ä¸ç„¶ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ç±»å¤šè¯­è¨€ Transformer æ¨¡å‹ï¼Œå¦‚ mT5 å’Œ mBARTï¼Œå¯ä»¥è§£å†³é—®é¢˜ã€‚è¿™äº›æ¨¡å‹ä¹Ÿæ˜¯ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œä½†æœ‰ä¸€ç‚¹ä¸åŒï¼šå®ƒä»¬ä¸æ˜¯åœ¨ä¸€ç§è¯­è¨€çš„è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œè€Œæ˜¯åŒæ—¶åœ¨ 50 å¤šç§è¯­è¨€çš„æ–‡æœ¬ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼

æˆ‘ä»¬å°†ä½¿ç”¨ mT5ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº T5 çš„æœ‰è¶£æ¶æ„ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ä¸­è¿›è¡Œäº†é¢„è®­ç»ƒã€‚åœ¨ T5 ä¸­ï¼Œæ¯ä¸ª NLP ä»»åŠ¡éƒ½æ˜¯ä»¥ä»»åŠ¡å‰ç¼€ï¼ˆå¦‚ `summarize:` ï¼‰çš„å½¢å¼å®šä¹‰çš„ï¼Œæ¨¡å‹æ ¹æ®ä¸åŒçš„ä»»åŠ¡ç”Ÿæˆä¸åŒçš„æ–‡æœ¬ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™è®© T5 å˜å¾—éå¸¸é€šç”¨ï¼Œå› ä¸ºä½ å¯ä»¥ç”¨ä¸€ä¸ªæ¨¡å‹è§£å†³å¾ˆå¤šä»»åŠ¡ï¼


<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

mT5 ä¸ä½¿ç”¨å‰ç¼€ï¼Œä½†å…·æœ‰ T5 çš„å¤§éƒ¨åˆ†åŠŸèƒ½ï¼Œå¹¶ä¸”å…·æœ‰å¤šè¯­è¨€çš„ä¼˜åŠ¿ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»é€‰æ‹©äº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å‡†å¤‡æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** å®Œæˆæœ¬èŠ‚åï¼Œå¯ä»¥å°è¯•æ¯”è¾ƒä¸€ä¸‹ mT5 å’Œç”¨ç›¸åŒæŠ€æœ¯å¾®è°ƒè¿‡çš„ mBART çš„æ€§èƒ½ã€‚é™„åŠ çš„æŒ‘æˆ˜ï¼šåªåœ¨è‹±æ–‡è¯„è®ºä¸Šå¾®è°ƒ T5ã€‚å› ä¸º T5 æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å‰ç¼€æç¤ºï¼Œä½ éœ€è¦åœ¨ä¸‹é¢çš„é¢„å¤„ç†æ­¥éª¤ä¸­å°† `summarize:` æ·»åŠ åˆ°è¾“å…¥ä¾‹å­å‰ã€‚

</Tip>

## é¢„å¤„ç†æ•°æ® [[é¢„å¤„ç†æ•°æ®]]

<Youtube id="1m7BerpSq8A"/>

æˆ‘ä»¬æ¥ä¸‹æ¥çš„ä»»åŠ¡æ˜¯å¯¹æˆ‘ä»¬çš„è¯„è®ºåŠå…¶æ ‡é¢˜è¿›è¡Œ tokenize å’Œ encode ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆåŠ è½½ä¸é¢„è®­ç»ƒæ¨¡å‹ checkpoint ç›¸å…³çš„ tokenizerï¼Œè¿™æ¬¡æˆ‘ä»¬å°†ä½¿ç”¨è¾ƒå°çš„ `mt5-small` ä½œä¸ºæˆ‘ä»¬çš„ checkpoint è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨åˆç†çš„æ—¶é—´æ¶ˆè€—å†…å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

ğŸ’¡åœ¨ NLP é¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œä¸€ä¸ªå¥½çš„åšæ³•æ˜¯åœ¨å°æ ·æœ¬æ•°æ®ä¸Šè®­ç»ƒä¸€ç±»â€œå°â€æ¨¡å‹ã€‚è¿™ä½¿ä½ å¯ä»¥æ›´å¿«åœ°è°ƒè¯•å’Œè¿­ä»£ç«¯åˆ°ç«¯å·¥ä½œæµã€‚å½“ä½ å¯¹ç»“æœæœ‰ä¿¡å¿ƒä¹‹åï¼Œä½ åªéœ€è¦é€šè¿‡ç®€å•åœ°æ›´æ”¹æ¨¡å‹ checkpoint å°±å¯ä»¥åœ¨è¾ƒå¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼

</Tip>

è®©æˆ‘ä»¬åœ¨ä¸€ä¸ªå°æ ·æœ¬ä¸Šæµ‹è¯• mT5  tokenizer 

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç†Ÿæ‚‰çš„ `input_ids` å’Œ `attention_mask` ï¼Œæˆ‘ä»¬åœ¨ [ç¬¬3ç« ](https://chat.openai.com/course/chapter3) çš„ç¬¬ä¸€æ¬¡å¾®è°ƒå®éªŒä¸­é‡åˆ°è¿‡ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ tokenizer çš„ `convert_ids_to_tokens()` å‡½æ•°è§£ç è¿™äº›è¾“å…¥ IDï¼Œçœ‹çœ‹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„æ˜¯ä»€ä¹ˆç±»å‹çš„ tokenizerï¼š

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']
```

ä»ç‰¹æ®Šçš„ Unicode å­—ç¬¦ `â–` å’Œè¡¨ç¤ºåºåˆ—ç»“æŸ `</s>` token å¯ä»¥çœ‹å‡ºæ¥ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨åŸºäº [ç¬¬6ç« ](https://chat.openai.com/course/chapter6) ä¸­è®¨è®ºçš„ Unigram å­è¯åˆ†è¯ç®—æ³•çš„ SentencePiece tokenizer ã€‚ Unigram å¯¹äºå¤šè¯­è¨€è¯­æ–™åº“ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒè®© SentencePiece ä¸å¿…å—å£éŸ³ã€æ ‡ç‚¹ç¬¦å·ä»¥åŠå¾ˆå¤šè¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰æ²¡æœ‰ç©ºç™½å­—ç¬¦çš„å½±å“ï¼Œåªä¸“æ³¨äºæ‰¾å‡ºæœ€ä¼˜çš„åˆ†è¯æ–¹å¼ã€‚

ä¸ºäº†å¯¹æˆ‘ä»¬çš„è¯­æ–™åº“ tokenize ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†ä¸æ‘˜è¦ä»»åŠ¡ä¼šé‡åˆ°çš„ä¸€ä¸ªç»†å¾®é—®é¢˜ï¼šå› ä¸ºæˆ‘ä»¬çš„è¾“å‡ºç›®æ ‡ä¹Ÿæ˜¯æ–‡æœ¬ï¼Œæ‰€ä»¥è¾“å…¥å’Œè¾“å‡ºåŠ èµ·æ¥å¯èƒ½è¶…è¿‡æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦å¯¹è¯„è®ºåŠå…¶æ ‡é¢˜è¿›è¡Œæˆªæ–­ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šå°†è¿‡é•¿çš„è¾“å…¥ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚ğŸ¤— Transformers ä¸­çš„ tokenizer æä¾›äº†ä¸€ä¸ªç»å¦™çš„ `text_target` å‚æ•°ï¼Œå…è®¸ä½ å°†ç›®æ ‡æ–‡æœ¬ä¸è¾“å…¥å¹¶è¡Œ tokenizeã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä¸º mT5 å¤„ç†è¾“å…¥å’Œç›®æ ‡æ–‡æœ¬çš„ç¤ºä¾‹ï¼š

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

è®©æˆ‘ä»¬é€æ­¥è§£æè¿™æ®µä»£ç ï¼Œç†è§£å‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº† `max_input_length` å’Œ `max_target_length` çš„å€¼ï¼Œè¿™äº›å€¼è®¾å®šäº†æˆ‘ä»¬çš„è¯„è®ºå’Œæ ‡é¢˜çš„æœ€å¤§é•¿åº¦ã€‚ç”±äºè¯„è®ºä¸»ä½“é€šå¸¸æ¯”æ ‡é¢˜å¤§å¾—å¤šï¼Œæˆ‘ä»¬ç›¸åº”åœ°è°ƒæ•´äº†è¿™äº›å€¼ã€‚

é€šè¿‡ `preprocess_function()` å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨è¿™é—¨è¯¾ç¨‹ä¸­å¹¿æ³›ä½¿ç”¨çš„æ–¹ä¾¿çš„ `Dataset.map()` å‡½æ•°ï¼Œè½»æ¾åœ°å¯¹æ•´ä¸ªè¯­æ–™åº“ tokenize ã€‚

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

æ—¢ç„¶è¯­æ–™åº“å·²ç»é¢„å¤„ç†å®Œæ¯•ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›å¸¸ç”¨çš„æ‘˜è¦æŒ‡æ ‡ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹é¢å³å°†çœ‹åˆ°çš„ï¼Œåœ¨è¡¡é‡æœºå™¨ç”Ÿæˆçš„æ–‡æœ¬çš„è´¨é‡æ–¹é¢æ²¡æœ‰çµä¸¹å¦™è¯ã€‚

<Tip>

ğŸ’¡ ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°æˆ‘ä»¬åœ¨ä¸Šé¢çš„ `Dataset.map()` å‡½æ•°ä¸­ä½¿ç”¨äº† `batched=True` ã€‚è¿™å°†ä»¥ 1000ï¼ˆé»˜è®¤å€¼ï¼‰çš„ batch size å¯¹ç¤ºä¾‹ç»§ç»­ç¼–ç ï¼Œå¹¶è®©ä½ å¯ä»¥åˆ©ç”¨ ğŸ¤— Transformers ä¸­å¿«é€Ÿ tokenizer çš„å¤šçº¿ç¨‹åŠŸèƒ½ã€‚åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå°è¯•ä½¿ç”¨ `batched=True` æ¥åŠ é€Ÿä½ çš„é¢„å¤„ç†ï¼

</Tip>

## æ–‡æœ¬æ‘˜è¦çš„è¯„ä¼°æŒ‡æ ‡ [[æ–‡æœ¬æ‘˜è¦çš„è¯„ä¼°æŒ‡æ ‡]]

<Youtube id="TMshhnrEXlg"/>

ä¸æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­æ¶µç›–çš„å¤§å¤šæ•°å…¶ä»–ä»»åŠ¡ç›¸æ¯”ï¼Œè¡¡é‡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦æˆ–ç¿»è¯‘ï¼‰çš„å¥½åå¹¶ä¸é‚£ä¹ˆç®€å•ã€‚ä¾‹å¦‚ï¼Œå¯¹äºâ€œæˆ‘å–œæ¬¢é˜…è¯»é¥¥é¥¿æ¸¸æˆâ€è¿™æ ·çš„è¯„è®ºï¼Œå¯èƒ½æœ‰å¤šä¸ªæœ‰æ•ˆæ‘˜è¦ï¼Œä¾‹å¦‚â€œæˆ‘å–œæ¬¢é¥¥é¥¿æ¸¸æˆâ€æˆ–â€œé¥¥é¥¿æ¸¸æˆæ˜¯ä¸€æœ¬å¥½ä¹¦â€ã€‚æ˜¾ç„¶ï¼Œåœ¨ç”Ÿæˆçš„æ‘˜è¦å’Œæ ‡ç­¾ä¹‹é—´è¿›è¡ŒæŸç§ç²¾ç¡®åŒ¹é…å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆâ€”â€”å³ä½¿æ˜¯äººç±»åœ¨è¿™æ ·çš„è¯„ä¼°æŒ‡æ ‡ä¸‹ä¹Ÿä¼šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæ¯ä¸ªäººéƒ½æœ‰è‡ªå·±çš„å†™ä½œé£æ ¼ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ€å¸¸ç”¨çš„æŒ‡æ ‡ä¹‹ä¸€æ˜¯[ROUGE åˆ†æ•°](https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼ˆRecall-Oriented Understudy for Gisting Evaluation çš„ç¼©å†™ï¼‰ã€‚è¯¥æŒ‡æ ‡èƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯å°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„é€šå¸¸ç”±äººç±»åˆ›å»ºçš„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå‡è®¾æˆ‘ä»¬è¦æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ‘˜è¦ï¼š

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```
æ¯”è¾ƒå®ƒä»¬çš„ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—é‡å å•è¯çš„æ•°é‡ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ä¸º 6ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰äº›ç²—ç³™ï¼Œå› æ­¤ ROUGE æ˜¯åŸºäºè®¡ç®—è®¡ç®—é‡å éƒ¨åˆ†çš„ `ç²¾ç¡®åº¦(Precision)` å’Œ `å¬å›ç‡(Recall)` åˆ†æ•°æ¥è®¡ç®—çš„ã€‚

<Tip>

ğŸ™‹ å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡å¬è¯´ç²¾ç¡®åº¦ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰ï¼Œè¯·ä¸è¦æ‹…å¿ƒâ€”â€”æˆ‘ä»¬å°†ä¸€èµ·é€šè¿‡ä¸€äº›æ¸…æ™°çš„ç¤ºä¾‹æ¥ç†è§£å®ƒä»¬ã€‚è¿™äº›æŒ‡æ ‡é€šå¸¸åœ¨åˆ†ç±»ä»»åŠ¡ä¸­é‡åˆ°ï¼Œæ‰€ä»¥å¦‚æœä½ æƒ³äº†è§£åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ç²¾ç¡®åº¦ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰æ˜¯å¦‚ä½•å®šä¹‰çš„ï¼Œæˆ‘ä»¬å»ºè®®ä½ æŸ¥çœ‹ `scikit-learn` çš„ [æŒ‡å—](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) ã€‚

</Tip>

å¯¹äº ROUGEï¼Œå¬å›ç‡è¡¡é‡çš„æ˜¯å‚è€ƒæ‘˜è¦ä¸­è¢«ç”Ÿæˆæ‘˜è¦æ•è·çš„å†…å®¹é‡ã€‚å¦‚æœæˆ‘ä»¬åªæ˜¯æ¯”è¾ƒå•è¯ï¼Œå¬å›ç‡å¯ä»¥æŒ‰ç…§ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

$$ \mathrm{å¬å›ç‡} = \frac{\mathrm{é‡å è¯çš„æ•°é‡}}{\mathrm{å‚è€ƒæ‘˜è¦ä¸­çš„æ€»è¯æ•°}} $$

å¯¹äºä¸Šé¢çš„é‚£ä¸ªä¾‹å­ï¼Œè¿™ä¸ªå…¬å¼ç»™å‡ºäº† 6/6 = 1 çš„å®Œç¾å¬å›ç‡ï¼›å³ï¼Œå‚è€ƒæ‘˜è¦ä¸­çš„æ‰€æœ‰å•è¯æ¨¡å‹éƒ½ç”Ÿæˆå‡ºæ¥äº†ã€‚è¿™å¬èµ·æ¥å¯èƒ½å¾ˆæ£’ï¼Œä½†æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ç”Ÿæˆçš„æ‘˜è¦æ˜¯â€œæˆ‘çœŸçš„å¾ˆå–œæ¬¢æ•´æ™šé˜…è¯»é¥¥é¥¿æ¸¸æˆâ€ã€‚è¿™ä¹Ÿä¼šæœ‰å®Œç¾çš„ recallï¼Œä½†å¯ä»¥è¯´è¿™æ˜¯ä¸€ä¸ªæ›´ç³Ÿç³•çš„æ€»ç»“ï¼Œå› ä¸ºå®ƒå¾ˆå†—é•¿ã€‚ä¸ºäº†é€‚åº”äºè¿™äº›åœºæ™¯ï¼Œæˆ‘ä»¬è¿˜è®¡ç®—äº†ç²¾ç¡®åº¦ï¼Œå®ƒåœ¨ ROUGE ä¸Šä¸‹æ–‡ä¸­è¡¡é‡äº†ç”Ÿæˆçš„æ‘˜è¦ä¸­æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„ï¼š

$$ \mathrm{ç²¾ç¡®åº¦} = \frac{\mathrm{é‡å è¯çš„æ•°é‡}}{\mathrm{ç”Ÿæˆæ‘˜è¦ä¸­çš„æ€»è¯æ•°}} $$

è¯¦ç»†æ‘˜è¦ä½¿ç”¨è¿™ç§è®¡ç®—æ–¹æ³•ä¼šå¾—åˆ° 6/10 = 0.6 çš„ç²¾ç¡®åº¦ï¼Œè¿™æ¯”è¾ƒçŸ­çš„æ‘˜è¦è·å¾—çš„ 6/7 = 0.86 çš„ç²¾ç¡®åº¦è¦å·®å¾—å¤šã€‚åœ¨å®è·µä¸­ï¼Œé€šå¸¸ä¼šå…ˆè®¡ç®—è®¡ç®—ç²¾åº¦å’Œå¬å›ç‡ï¼Œç„¶åå¾—åˆ° F1 åˆ†æ•°ï¼ˆç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨ğŸ¤— Datasets ä¸­é€šè¿‡å®‰è£… `rouge_score` åŒ…æ¥å®ç°è¿™äº›è®¡ç®—ï¼š

```py
!pip install rouge_score
```

ç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼åŠ è½½ ROUGE æŒ‡æ ‡ï¼š

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

æ¥ç€æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `rouge_score.compute()` å‡½æ•°æ¥ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰çš„æŒ‡æ ‡ï¼š

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

å“‡ï¼Œè¿™ä¸ªè¾“å‡ºä¸­åŒ…å«äº†å¾ˆå¤šä¿¡æ¯â€”â€”å®ƒä»¬éƒ½ä»£è¡¨ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿé¦–å…ˆï¼ŒğŸ¤— Datasets è®¡ç®—äº†ç²¾åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°çš„ç½®ä¿¡åŒºé—´ï¼›ä¹Ÿäº›å°±æ˜¯ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ `low` ã€ `mid` å’Œ `high` å±æ€§ã€‚æ­¤å¤–ï¼ŒğŸ¤— Datasets è¿˜è®¡ç®—äº†åŸºäºåœ¨æ¯”è¾ƒç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦æ—¶çš„é‡‡ç”¨ä¸åŒæ–‡æœ¬ç²’åº¦çš„å„ç§ ROUGE å¾—åˆ†ã€‚ `rouge1` æµ‹é‡çš„æ˜¯ç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦ä¸­å•ä¸ªå•è¯çš„é‡å ç¨‹åº¦ã€‚
ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æå–å‡ºæˆ‘ä»¬å¾—åˆ†çš„ `mid` å€¼ï¼š

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```
å¤ªå¥½äº†ï¼Œç²¾ç¡®åº¦å’Œå¬å›ç‡çš„æ•°å­—éƒ½å¯¹ä¸Šäº†ï¼é‚£ä¹ˆå…¶ä»–çš„ ROUGE å¾—åˆ†è¡¨ç¤ºä»€ä¹ˆå«ä¹‰å‘¢ï¼Ÿ `rouge2` åº¦é‡äº†äºŒå…ƒè¯ç»„ï¼ˆè€ƒè™‘å•è¯å¯¹çš„é‡å ï¼‰ä¹‹é—´çš„é‡å ï¼Œè€Œ `rougeL` å’Œ `rougeLsum` é€šè¿‡å¯»æ‰¾ç”Ÿæˆçš„æ‘˜è¦å’Œå‚è€ƒæ‘˜è¦ä¸­æœ€é•¿çš„å…¬å…±å­ä¸²æ¥åº¦é‡å•è¯çš„æœ€é•¿åŒ¹é…åºåˆ—ã€‚ `rougeLsum` ä¸­çš„â€œsumâ€æŒ‡çš„æ˜¯è¯¥æŒ‡æ ‡æ˜¯åœ¨æ•´ä¸ªæ‘˜è¦ä¸Šè®¡ç®—çš„ï¼Œè€Œ `rougeL` æ˜¯æŒ‡åœ¨å„ä¸ªå¥å­ä¸Šè®¡ç®—çš„å¹³å‡å€¼ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** è‡ªå·±æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦ï¼Œçœ‹çœ‹ä½¿ç”¨ evaluate å¾—å‡ºçš„ ROUGE åˆ†æ•°æ˜¯å¦ä¸åŸºäºç²¾ç¡®åº¦å’Œå¬å›ç‡å…¬å¼çš„æ‰‹åŠ¨è®¡ç®—ä¸€è‡´ã€‚é™„åŠ çš„æŒ‘æˆ˜ï¼šå°†æ–‡æœ¬åˆ‡åˆ†ä¸ºé•¿åº¦ä¸º2çš„è¯ç»„ï¼Œå¹¶æ‰‹åŠ¨è®¡ç®—ç²¾åº¦å’Œå¬å›ç‡ä¸ `rouge2` æŒ‡æ ‡çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡è¿›è¡Œå¯¹æ¯”ã€‚

</Tip>

æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº› ROUGE åˆ†æ•°æ¥è·Ÿè¸ªæˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åšæ¯ä¸ªä¼˜ç§€çš„ NLP ä»ä¸šè€…éƒ½åº”è¯¥åšçš„äº‹æƒ…ï¼šåˆ›å»ºä¸€ä¸ªå¼ºå¤§è€Œç®€å•çš„ baselineï¼

### åˆ›å»ºå¼ºå¤§çš„ baseline [[åˆ›å»ºå¼ºå¤§çš„ baseline]]

å¯¹äºæ–‡æœ¬æ‘˜è¦ï¼Œä¸€ä¸ªå¸¸è§çš„å‚è€ƒ baseline æ˜¯ç®€å•åœ°å–æ–‡ç« çš„å‰ä¸‰å¥è¯ä½œä¸ºæ‘˜è¦ï¼Œé€šå¸¸ç§°ä¸º `lead-3` baselineã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¥å·ï¼ˆè‹±æ–‡ä½¿ç”¨ï¼ï¼‰æ¥è·Ÿè¸ªå¥å­è¾¹ç•Œï¼Œä½†è¿™åœ¨â€œU.S.â€ or â€œU.N.â€ä¹‹ç±»çš„é¦–å­—æ¯ç¼©ç•¥è¯ä¸Šä¼šè®¡ç®—é”™è¯¯ã€‚æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ `nltk` åº“ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ›´å¥½çš„ç®—æ³•æ¥å¤„ç†è¿™äº›æƒ…å†µã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å®‰è£…è¯¥åŒ…ï¼š

```python
!pip install nltk
```

ç„¶åä¸‹è½½æ ‡ç‚¹è§„åˆ™ï¼š

```python
import nltk

nltk.download("punkt")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä» `nltk` å¯¼å…¥å¥å­çš„ tokenizer å¹¶åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°ç”¨æ¥æå–è¯„è®ºä¸­çš„å‰ä¸‰ä¸ªå¥å­ã€‚æ–‡æœ¬æ‘˜è¦çš„é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”æ¯ä¸ªæ‘˜è¦ï¼Œå› æ­¤æˆ‘ä»¬ä¹ŸæŒ‰ç…§è¿™æ ·çš„è§„åˆ™å¤„ç†ï¼Œå¹¶åœ¨è®­ç»ƒé›†çš„ç¤ºä¾‹ä¸Šå¯¹å…¶è¿›è¡Œæµ‹è¯•ï¼š

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

è¿™ä¼¼ä¹æœ‰æ•ˆï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬ç°åœ¨å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œä»æ•°æ®é›†ä¸­æå–è¿™äº›â€œæ‘˜è¦â€å¹¶è®¡ç®— baseline çš„ ROUGE åˆ†æ•°ï¼š

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥è®¡ç®—éªŒè¯é›†ä¸Šçš„ ROUGE åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨ Pandas å¯¹è¾“å‡ºçš„ç»“æœè¿›è¡Œä¸€äº›ç¾åŒ–ï¼š

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `rouge2` çš„åˆ†æ•°æ˜æ˜¾ä½äºå…¶ä»–çš„rougeï¼›è¿™å¯èƒ½åæ˜ äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³è¯„è®ºæ ‡é¢˜é€šå¸¸å¾ˆç®€æ´ï¼Œå› æ­¤ `lead-3` baseline è¿‡äºå†—é•¿å¯¼è‡´å¾—åˆ†ä¸é«˜ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¾ˆå¥½çš„å‚è€ƒåŸºå‡†ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘å¾®è°ƒ mT5ï¼

{#if fw === 'pt'}

## ä½¿ç”¨ `Trainer` API å¾®è°ƒ mT5  [[ä½¿ç”¨ `Trainer` API å¾®è°ƒ mT5]]

å¾®è°ƒæ¨¡å‹æ¥æå–æ‘˜è¦ä¸æˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä»‹ç»çš„å…¶ä»–ä»»åŠ¡éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä» `mt5-small` checkpoint ä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºæ‘˜è¦æå–æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ AutoModelForSeq2SeqLM ç±»åŠ è½½æ¨¡å‹ï¼Œè¯¥ç±»ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹æƒé‡ï¼š

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ä½¿ç”¨ `Keras` API å¾®è°ƒ mT5  [[ä½¿ç”¨ `Keras` API å¾®è°ƒ mT5]]

å¾®è°ƒæ¨¡å‹æ¥æå–æ‘˜è¦ä¸æˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä»‹ç»çš„å…¶ä»–ä»»åŠ¡éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä» `mt5-small` checkpoint ä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºæ‘˜è¦æå–æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `TFAutoModelForSeq2SeqLM` ç±»åŠ è½½æ¨¡å‹ï¼Œè¯¥ç±»ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹æƒé‡ï¼š

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

ğŸ’¡ å¦‚æœä½ æƒ³çŸ¥é“ä¸ºä»€ä¹ˆåœ¨å®ä¾‹åŒ–çš„è¿‡ç¨‹ä¸­æ²¡æœ‰çœ‹åˆ°ä»»ä½•å…³äºå¾®è°ƒæ¨¡å‹çš„è­¦å‘Šï¼Œé‚£æ˜¯å› ä¸ºå¯¹äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¿ç•™äº†ç½‘ç»œçš„æ‰€æœ‰æƒé‡ã€‚ä¸æ­¤ç›¸æ¯”ï¼Œåœ¨ [ç¬¬ä¸‰ç« ](https://chat.openai.com/course/chapter3) ä¸­çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ç½‘ç»œæ›¿æ¢äº†é¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ã€‚

</Tip>

æˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ç™»å½• Hugging Face Hubã€‚å¦‚æœä½ åœ¨ notebook ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å®ç”¨ç¨‹åºå‡½æ•°è¿›è¡Œæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°å·¥å…·ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„å‡­æ®ã€‚æˆ–è€…ï¼Œä½ å¯ä»¥åœ¨ä½ çš„ç»ˆç«¯è¿è¡Œè¿™æ¡å‘½ä»¤æ¥ç™»é™†ï¼š

```
huggingface-cli login
```

{#if fw === 'pt'}

ä¸ºäº†åœ¨è®­ç»ƒæœŸé—´è®¡ç®— `ROUGE` åˆ†æ•°ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒæœŸé—´ç”Ÿæˆæ–‡æœ¬å½¢å¼çš„æ‘˜è¦ã€‚å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸“ç”¨çš„ `Seq2SeqTrainingArguments` å’Œ `Seq2SeqTrainer` ç±»ï¼Œå¯ä»¥è‡ªåŠ¨ä¸ºæˆ‘ä»¬å®Œæˆè¿™é¡¹å·¥ä½œï¼ä¸ºäº†äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä¸ºæˆ‘ä»¬çš„å®éªŒå®šä¹‰è¶…å‚æ•°å’Œå…¶ä»–å‚æ•°,åœ¨åé¢çš„çš„è®­ç»ƒè¿‡ç¨‹ä¼šè®²åˆ°å¦‚ä½•å®ç°çš„ã€‚

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# æ¯ä¸ªè®­ç»ƒå‘¨æœŸéƒ½è¾“å‡ºè®­ç»ƒæŸå¤±
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬æŠŠ `predict_with_generate` å‚æ•°è®¾ç½®ä¸º `True` ï¼Œè¿™æ ·å¯ä»¥åœ¨è¯„ä¼°æœŸé—´ç”Ÿæˆæ‘˜è¦æ¥è®¡ç®—æ¯ä¸ª `epoch` çš„ ROUGE åˆ†æ•°ã€‚æ­£å¦‚åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­å­¦åˆ°çš„ï¼Œæ¨¡å‹çš„ `generate()` å‡½æ•°å®ç°äº†ä½¿ç”¨è§£ç å™¨é€ä¸ªé¢„æµ‹å•è¯æ¥æ¨ç†ç”Ÿæˆçš„æ–‡æœ¬ã€‚è®¾ç½® `predict_with_generate=True` åï¼Œ`Seq2SeqTrainer` ä¼šåœ¨è¯„ä¼°æ—¶ä½¿ç”¨ `generate()` å‡½æ•°ã€‚é™¤æ­¤ä¹‹å¤–æˆ‘ä»¬è¿˜è°ƒæ•´é»˜è®¤çš„è¶…å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€`epochs` æ•°å’Œæƒé‡è¡°å‡ï¼Œå¹¶ä¸”è®¾ç½® `save_total_limit` é€‰é¡¹ï¼Œ ä½¿è®­ç»ƒæœŸé—´æœ€å¤šåªèƒ½ä¿å­˜ 3 ä¸ª `checkpoint` çš„é€‰é¡¹ã€‚â€”â€”è¿™æ˜¯å› ä¸ºå³ä½¿æ˜¯ mT5 çš„â€œsmallâ€ç‰ˆæœ¬ä¹Ÿä½¿ç”¨å¤§çº¦ 1 GB çš„ç¡¬ç›˜ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é™åˆ¶ä¿å­˜çš„å‰¯æœ¬æ•°é‡æ¥èŠ‚çœä¸€ç‚¹ç©ºé—´ã€‚

è®¾ç½® `push_to_hub=True`  é€‰é¡¹å `Trainer` ä¼šåœ¨è®­ç»ƒåè‡ªåŠ¨å°†æ¨¡å‹æ¨é€åˆ° Hub ä¸­ï¼›ä½ å¯ä»¥åœ¨ `output_dir` æŒ‡å®šçš„ä½ç½®ä¸‹çš„ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„ä»“åº“ã€‚è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„ä»“åº“çš„åç§°ï¼ˆç‰¹åˆ«æ˜¯å½“ä½ æƒ³è¦æ¨é€åˆ°ç»„ç»‡æ—¶ï¼Œå°±å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `Seq2SeqTrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` ã€‚

ä¸ºäº†åœ¨è®­ç»ƒæœŸé—´è¯„ä¼°æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸º `Trainer` æä¾›ä¸€ä¸ª `compute_metrics()` å‡½æ•°ã€‚å¯¹äºæ‘˜è¦æ¨¡å‹æ¥è¯´ï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨ `rouge_score.compute()` è¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å°†è¾“å‡ºå’Œå‚è€ƒæ‘˜è¦è§£ç ä¸ºæ–‡æœ¬ï¼Œç„¶åæ‰èƒ½è®¡ç®— ROUGE åˆ†æ•°ã€‚ä¸‹é¢çš„å‡½æ•°å°±å®Œæˆäº†è§£ç å’Œè®¡ç®—åˆ†æ•°ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜ä½¿ç”¨äº† `nltk` ä¸­çš„ `sent_tokenize()` å‡½æ•°å°†æ‘˜è¦å¥å­ç”¨æ¢è¡Œç¬¦åˆ†éš”å¼€ï¼š

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # å°†ç”Ÿæˆçš„æ‘˜è¦è§£ç ä¸ºæ–‡æœ¬
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # æ›¿æ¢æ ‡ç­¾ä¸­çš„-100,å› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # å°†å‚è€ƒæ‘˜è¦è§£ç ä¸ºæ–‡æœ¬
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGEæœŸæœ›æ¯ä¸ªå¥å­åéƒ½æœ‰ä¸€ä¸ªæ¢è¡Œç¬¦
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # è®¡ç®—ROUGEåˆ†æ•°
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # è®¡ç®—ROUGEåˆ†æ•°
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡å®šä¹‰ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆdata collatorï¼‰ã€‚ç”±äº mT5 æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨çš„ Transformer æ¨¡å‹ï¼Œå› æ­¤åœ¨å°†æ•°æ®æ•´ç†æˆ batch æ—¶æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œé‚£å°±æ˜¯åœ¨è§£ç æœŸé—´ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾å‘å³ç§»åŠ¨ä¸€ä¸ªå•ä½ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿è§£ç å™¨åªçœ‹åˆ°ä¹‹å‰çš„å‚è€ƒåºåˆ—ï¼Œè€Œä¸æ˜¯å½“å‰è¦é¢„æµ‹çš„ `token` æˆ–ä¹‹åçš„å‚è€ƒåºåˆ—ï¼Œè¿™æ ·æ¨¡å‹å°±èƒ½é¿å…å®¹æ˜“è®°ä½æ ‡ç­¾ã€‚è¿™ç±»ä¼¼ä¸åœ¨ [å› æœè¯­è¨€æ¨¡å‹](https://chat.openai.com/course/chapter7/6) è¿™æ ·çš„ä»»åŠ¡ä¸­ä½¿ç”¨æ©ç è‡ªæ³¨æ„åŠ›çš„æœºåˆ¶ç±»ä¼¼ã€‚

å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ª `DataCollatorForSeq2Seq` æ•´ç†å™¨ï¼Œå®ƒä¼šåŠ¨æ€åœ°å¡«å……æˆ‘ä»¬çš„è¾“å…¥å’Œæ ‡ç­¾ã€‚æˆ‘ä»¬åªéœ€è¦æä¾› `tokenizer` å’Œ `model` æ—¢å¯å®ä¾‹åŒ–è¿™ä¸ªæ•´ç†å™¨ï¼š

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

è®©æˆ‘ä»¬çœ‹çœ‹å½“ç»™è¿™ä¸ªæ•´ç†å™¨æä¾›ä¸€ä¸ªå°æ‰¹æ¬¡çš„æ ·æœ¬æ—¶ï¼Œå®ƒçš„å¤„ç†è¿‡ç¨‹æ˜¯æ€ä¹ˆæ ·çš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ é™¤å¸¦æœ‰å­—ç¬¦ä¸²çš„åˆ—ï¼Œå› ä¸ºæ•´ç†å™¨ä¸çŸ¥é“å¦‚ä½•å¯¹è¿™äº›å…ƒç´ è¿›è¡Œå¡«å……ï¼ˆpaddingï¼‰ï¼š

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

ç”±äº collator éœ€è¦ä¸€ä¸ª `dict` çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ª `dict` ä»£è¡¨æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿéœ€è¦åœ¨å°†æ•°æ®ä¼ ç»™æ•°æ®æ•´ç†å™¨ä¹‹å‰ï¼Œå°†æ•°æ®æ•´ç†æˆé¢„æœŸçš„æ ¼å¼ï¼š

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œç¬¬äºŒä¸ªä¾‹å­æ¯”ç¬¬ä¸€ä¸ªä¾‹å­è¦é•¿ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªä¾‹å­çš„ `input_ids` å’Œ `attention_mask` åœ¨å³è¾¹ç”¨ `[PAD]` token ï¼ˆID ä¸º `0` ï¼‰è¿›è¡Œäº†å¡«å……ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ ‡ç­¾ä¹Ÿä½¿ç”¨ `-100` è¿›è¡Œäº†å¡«å……ï¼Œä»¥ç¡®ä¿å¡«å……çš„ `tokens` è¢«æŸå¤±å‡½æ•°å¿½ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤šäº†ä¸€ä¸ªæ–°çš„ `decoder_input_ids` å­—æ®µï¼Œå®ƒæ˜¯é€šè¿‡åœ¨ç¬¬ä¸€ä¸ªæ¡ç›®ä¸­æ’å…¥ `[PAD]` `tokens` æ¥å°†æ ‡ç­¾å‘å³ç§»åŠ¨ä¸€ä¸ª token å½¢æˆçš„ã€‚

{#if fw === 'pt'}

æˆ‘ä»¬ç»ˆäºæ‹¥æœ‰äº†è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰çš„å‰æœŸå‡†å¤‡ï¼æˆ‘ä»¬ç°åœ¨åªéœ€è¦ä½¿ç”¨æ ‡å‡†å‚æ•°å®ä¾‹åŒ– Trainer 

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ç„¶åå¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒï¼š

```python
trainer.train()
```

åœ¨è®­ç»ƒæœŸé—´ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°è®­ç»ƒæŸå¤±é€æ¸å‡å°ï¼Œå¹¶ä¸” ROUGE åˆ†æ•°éšç€ epoch çš„å¢åŠ è€Œå¢åŠ ã€‚è®­ç»ƒå®Œæˆä¹‹åï¼Œä½ å¯ä»¥é€šè¿‡è¿è¡Œ `Trainer.evaluate()` æ¥æŸ¥çœ‹æœ€åçš„ ROUGE åˆ†æ•°ï¼š

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

ä»åˆ†æ•°ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è½»æ¾è¶…è¿‡äº†æˆ‘ä»¬çš„ `lead-3` baselineâ€”â€”å¾ˆå¥½ï¼æœ€åè¦åšçš„æ˜¯å°†æ¨¡å‹æƒé‡æ¨é€åˆ° Hubï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```


ä¸Šé¢çš„ä»£ç ä¼šæŠŠ checkpoint å’Œé…ç½®æ–‡ä»¶ä¿å­˜åˆ° `output_dir` ï¼Œç„¶åå°†æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ åˆ° Hubã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ `tags` å‚æ•°æŒ‡å®šæ¨¡å‹çš„ç±»å‹ï¼Œè¿™æ ·å°±å¯ä»¥ç¡®ä¿åœ¨ Hub ä¸Šçš„å°å·¥å…·ä¼šæ˜¯ä¸€ä¸ªæ‘˜è¦ç”Ÿæˆçš„å°å·¥å…·ï¼Œè€Œä¸æ˜¯ä¸ mT5 æ¶æ„çš„é»˜è®¤æ–‡æœ¬ç”Ÿæˆå°å·¥å…·ï¼ˆå…³äºæ¨¡å‹æ ‡ç­¾çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ [ğŸ¤—Hubæ–‡æ¡£](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined) ï¼‰ã€‚ `trainer.push_to_hub()` çš„è¾“å‡ºæ˜¯å¸¦æœ‰ Git æäº¤å“ˆå¸Œçš„ URLï¼Œæ‰€ä»¥ä½ å¯ä»¥æ‰“å¼€ URL è½»æ¾æŸ¥çœ‹æ¨¡å‹åº“çš„ä¿®æ”¹è®°å½•ï¼


åœ¨ç»“æŸæœ¬èŠ‚ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate æä¾›çš„åº•å±‚ API å¯¹ mT5 è¿›è¡Œå¾®è°ƒã€‚

{:else}

æˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ä¸œè¥¿äº†ï¼æˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„æ•°æ®æ•´ç†å™¨å°†æˆ‘ä»¬çš„æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` ï¼Œç„¶å `compile()` å’Œ `fit()` æ¨¡å‹ã€‚é¦–å…ˆï¼Œè½¬æ¢æ•°æ®é›†ï¼š
```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰è®­ç»ƒè¶…å‚æ•°å¹¶ç¼–è¯‘ï¼š

```python
from transformers import create_optimizer
import tensorflow as tf

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚

num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

æœ€åï¼Œè®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬æ·»åŠ äº† `PushToHubCallback` ï¼Œå®ƒä¼šåœ¨æ¯ä¸ª epoch åå°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨åé¢ç”¨ä¸Šä¼ å¥½çš„æ¨¡å‹æ¥è¿›è¡Œæ¨ç†ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°äº†ä¸€äº›æŸå¤±å€¼ï¼Œä½†æ²¡æœ‰çœ‹åˆ°ä¹‹å‰è®¡ç®—çš„ `ROUGE` åˆ†æ•°ã€‚è¦è·å¾—è¿™äº›æŒ‡æ ‡ï¼Œéœ€è¦å…ˆè·å–æ¨¡å‹çš„è¾“å‡ºç»“æœå¹¶å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚ç„¶åæ„å»ºå‚è€ƒæ‘˜è¦å’Œé¢„æµ‹çš„åˆ—è¡¨ï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥è®¡ç®— `ROUGE` åˆ†æ•°äº†ï¼ˆè¯·æ³¨æ„ï¼Œå¦‚æœåœ¨è¿™ä¸€éƒ¨åˆ†é‡åˆ°ç¼ºå°‘ `tqdm` åº“ï¼Œåˆ™å¯èƒ½éœ€è¦æ‰§è¡Œ `!pip install tqdm` ï¼‰ã€‚

æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ç§å¯ä»¥æ˜¾ç€æé«˜æ€§èƒ½çš„æŠ€å·§ â€”â€” ä½¿ç”¨ TensorFlow çš„çº¿æ€§ä»£æ•°åŠ é€Ÿç¼–è¯‘å™¨ [XLA](https://www.tensorflow.org/xla) ç¼–è¯‘æˆ‘ä»¬çš„ä»£ç ã€‚XLA å¯¹æ¨¡å‹çš„è®¡ç®—å›¾è¿›è¡Œäº†å„ç§ä¼˜åŒ–ï¼Œå¹¶æ˜¾ç€æé«˜äº†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚æ­£å¦‚ Hugging Face [åšå®¢](https://huggingface.co/blog/tf-xla-generate) ä¸­æ‰€è¿°ï¼Œå½“æˆ‘ä»¬çš„è¾“å…¥å½¢çŠ¶å˜åŒ–ä¸å¤§æ—¶ï¼ŒXLA æ•ˆæœæœ€ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†è¾“å…¥å¡«å……åˆ° 128 çš„å€æ•°ï¼Œå¹¶ä½¿ç”¨å¡«å……æ•´ç†å™¨åˆ›å»ºä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨ `@tf.function(jit_compile=True)` è£…é¥°å™¨è£…é¥°æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°ï¼Œè¿™å°†æŠŠæ•´ä¸ªå‡½æ•°æ ‡è®°ä¸ºç”¨ XLA ç¼–è¯‘ã€‚

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

æˆ‘ä»¬æœ‰äº†å‚è€ƒæ‘˜è¦å’Œæ¨¡å‹è¾“å‡ºé¢„æµ‹çš„å­—ç¬¦ä¸²çš„åˆ—è¡¨ä¹‹åï¼Œè®¡ç®— ROUGE åˆ†æ•°å°±å¾ˆå®¹æ˜“äº†ï¼š

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒ mT5 [[ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒ mT5]]

ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ä¸æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­é‡åˆ°çš„æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹éå¸¸ç›¸ä¼¼ã€‚ä¸æ–‡æœ¬åˆ†ç±»çš„ä¸»è¦åŒºåˆ«åœ¨äºæ‘˜è¦æ¨¡å‹éœ€è¦åœ¨è®­ç»ƒæœŸé—´æ˜¾å¼ç”Ÿæˆæ‘˜è¦å¹¶å®ç° ROUGE åˆ†æ•°çš„è®¡ç®—ï¼ˆè¯·è®°ä½ï¼Œ `Seq2SeqTrainer` å·²ç»ä¸ºæˆ‘ä»¬å®ç°äº†ç”Ÿæˆæ‘˜è¦çš„éƒ¨åˆ†ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•åœ¨ ğŸ¤— Accelerate ä¸­å®ç°è¿™ä¸¤ä¸ªè¦æ±‚ï¼

### ä¸ºè®­ç»ƒåšå¥½å‡†å¤‡ [[ä¸ºè®­ç»ƒåšå¥½å‡†å¤‡]]

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªæ•°æ®åˆ†ç»„åˆ›å»ºä¸€ä¸ª `DataLoader` ã€‚ç”±äº PyTorch çš„ dataloaders çš„è¾“å…¥æ˜¯ç”±å¼ é‡ç»„æˆçš„ batchï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†æ•°æ®é›†çš„æ ¼å¼è®¾å®šä¸º `"torch"` ï¼š

```python
tokenized_datasets.set_format("torch")
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–æ•°æ®æ•´ç†å™¨ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥å®šä¹‰æˆ‘ä»¬çš„ DataLoaderï¼š

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æˆ‘ä»¬è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€‚ä¸æˆ‘ä»¬çš„å…¶ä»–ä¾‹å­ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `AdamW` ï¼Œè¿™ä¸ªä¼˜åŒ–å™¨å¤§å¤šæ•°åœºæ™¯ä¸‹éƒ½å¾ˆæœ‰æ•ˆï¼š

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ä¸ºäº†é‡æ–°å¼€å§‹å¾®è°ƒï¼Œè€Œä¸æ˜¯ä»ä¸Šé¢å¾®è°ƒè¿‡çš„æ¨¡å‹ç»§ç»­å¾®è°ƒï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°å®ä¾‹åŒ– modelã€‚

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œ dataloaders è¾“å…¥åˆ° `accelerator.prepare()` æ–¹æ³•ä¸­ï¼š

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨å¦‚æœä½ åœ¨ TPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ™éœ€è¦å°†ä¸Šè¿°æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æœ‰å…³ TPU çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å›é¡¾ [ç¬¬ä¸‰ç« ](/course/chapter3) ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æˆ‘ä»¬çš„å¯¹è±¡ï¼Œè¿˜æœ‰ä¸‰ä¸ªäº‹æƒ…éœ€è¦åš

* å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦è®¡åˆ’ã€‚
* å®ç°ä¸€ä¸ªåŠŸèƒ½æ¥å¯¹æ¨¡å‹è¾“å‡ºçš„æ‘˜è¦è¿›è¡Œåç»­å¤„ç†ä»¥è¿›è¡Œè¯„ä¼°ã€‚
* åœ¨ Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹æ¨é€åˆ°è¯¥ä»“åº“ã€‚

å¯¹äºå­¦ä¹ ç‡è°ƒåº¦ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‰å‡ èŠ‚ä¸­çš„æ ‡å‡†çº¿æ€§è¡°å‡ï¼š

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

å¯¹äºåç»­å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°ï¼Œå°†ç”Ÿæˆçš„æ‘˜è¦æ‹†åˆ†ä¸ºç”±æ¢è¡Œç¬¦åˆ†éš”çš„å¥å­ã€‚è¿™æ˜¯ ROUGE æŒ‡æ ‡éœ€è¦çš„è¾“å…¥æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¥å®ç°ï¼š

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE éœ€è¦æ¯ä¸ªå¥å­åæœ‰ä¸€ä¸ªæ¢è¡Œç¬¦
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬æ˜¯å¦‚ä½•å®šä¹‰ `Seq2SeqTrainer` çš„ `compute_metrics()` å‡½æ•°ï¼Œé‚£ä¹ˆä½ åº”è¯¥å¯¹ä¸Šè¿°çš„ä»£ç æ¥æ„Ÿåˆ°å¾ˆç†Ÿæ‚‰ã€‚

æœ€åï¼Œæˆ‘ä»¬éœ€è¦åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åä¸ºğŸ¤— Hub çš„ python åº“ã€‚æˆ‘ä»¬åªéœ€è¦ä¸ºæˆ‘ä»¬çš„ä»“åº“å–ä¸€ä¸ª IDï¼ŒHub åº“ä¸­æœ‰ä¸€ä¸ªå®ç”¨çš„å‡½æ•°å¯ä»¥å°†ä»“åº“ ID ä¸ç”¨æˆ· ID ç»„åˆèµ·æ¥ï¼š

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªä»“åº“å…‹éš†åˆ°æ¨¡å‹ä¿å­˜çš„è·¯å¾„ä¸­ï¼Œè¯¥ç›®å½•å°†å­˜å‚¨è®­ç»ƒç”Ÿæˆçš„æ–‡ä»¶ï¼š

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨è®­ç»ƒæœŸé—´é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•å°†æ¨¡å‹æ¨é€åˆ° Hubï¼ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡å†™å‡ºå®Œæ•´çš„è®­ç»ƒå¾ªç¯æ¥ç»“æŸæˆ‘ä»¬çš„åˆ†æã€‚

### è®­ç»ƒå¾ªç¯ [[è®­ç»ƒå¾ªç¯]]

æ–‡æœ¬æ‘˜è¦çš„è®­ç»ƒå¾ªç¯ä¸æˆ‘ä»¬é‡åˆ°çš„å…¶ä»– ğŸ¤— Accelerate ç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼Œå¤§è‡´åˆ†ä¸ºå››ä¸ªä¸»è¦æ­¥éª¤ï¼š

1. é€šè¿‡åœ¨æ¯ä¸ª epoch è¿­ä»£ `train_dataloader` ä¸­çš„æ‰€æœ‰ç¤ºä¾‹æ¥è®­ç»ƒæ¨¡å‹ã€‚
2. åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ç”Ÿæˆæ‘˜è¦ï¼Œé¦–å…ˆç”Ÿæˆ tokens ç„¶åå°†å®ƒä»¬ï¼ˆå’Œå‚è€ƒæ‘˜è¦ï¼‰è§£ç ä¸ºæ–‡æœ¬ã€‚
3. ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çš„æ–¹æ³•è®¡ç®— ROUGE åˆ†æ•°ã€‚
4. ä¿å­˜ checkpoint å¹¶å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ° Hubã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¾èµ– `Repository` å¯¹è±¡çš„å·§å¦™çš„ `blocking=False` å‚æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ª epoch å¼‚æ­¥åœ°ä¸Šä¼  checkpointï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç»§ç»­è®­ç»ƒï¼Œè€Œä¸å¿…ç­‰å¾…ä¸ GB å¤§å°çš„æ¨¡å‹æ…¢å‘¼å‘¼çš„ä¸Šä¼ ï¼

è¿™äº›æ­¥éª¤å¯ä»¥åœ¨ä»¥ä¸‹ä»£ç å—ä¸­çœ‹åˆ°ï¼š

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # å¦‚æœæˆ‘ä»¬æ²¡æœ‰å¡«å……åˆ°æœ€å¤§é•¿åº¦,æˆ‘ä»¬éœ€è¦å¡«å……æ ‡ç­¾
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # æ›¿æ¢æ ‡ç­¾ä¸­çš„ -100,å› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # è®¡ç®—è¯„ä¼°çš„ loss
    result = rouge_score.compute()
    # æå–ä¸­ä½ ROUGE åˆ†æ•°
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # ä¿å­˜å’Œä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

å°±æ˜¯è¿™æ ·ï¼è¿è¡Œæ­¤ç¨‹åºåï¼Œä½ å°†è·å¾—ä¸æˆ‘ä»¬ä½¿ç”¨â€œTrainerâ€è·å¾—çš„æ¨¡å‹å’Œç»“æœéå¸¸ç›¸ä¼¼çš„æ¨¡å‹å’Œç»“æœã€‚

{/if}

## ä½¿ç”¨ä½ å¾®è°ƒçš„æ¨¡å‹ [[ä½¿ç”¨ä½ å¾®è°ƒçš„æ¨¡å‹]]

å°†æ¨¡å‹æ¨é€åˆ° Hub åï¼Œä½ å¯ä»¥é€šè¿‡æ¨ç†å°éƒ¨ä»¶æˆ– `pipeline` å¯¹è±¡æ¥ä½¿ç”¨å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

æˆ‘ä»¬å¯ä»¥å°†æµ‹è¯•é›†ï¼ˆæ¨¡å‹è¿˜æ²¡æœ‰è§è¿‡çš„ä¸€äº›æ•°æ®ï¼‰ä¸­å–ä¸€äº›æ ·æœ¬æä¾›ç»™æˆ‘ä»¬çš„ç®¡é“ï¼Œæ¥æ„Ÿå—ä¸€ä¸‹ç”Ÿæˆçš„æ‘˜è¦çš„è´¨é‡ã€‚é¦–å…ˆè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼ŒåŒæ—¶æ˜¾ç¤ºè¯„è®ºã€æ ‡é¢˜å’Œç”Ÿæˆçš„æ‘˜è¦ï¼š

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å…¶ä¸­ä¸€ä¸ªè‹±æ–‡æ‘˜è¦çš„ä¾‹å­ï¼š

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnâ€™t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itâ€™s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

è¿™è¿˜ä¸é”™ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®é™…ä¸Šå·²ç»èƒ½å¤Ÿé€šè¿‡å¢åŠ éƒ¨åˆ†æ–°è¯æ¥ç”Ÿæˆæ€»ç»“çš„æ‘˜è¦äº†ã€‚æˆ‘ä»¬æ¨¡å‹æœ€é…·çš„æ–¹é¢æ˜¯å®ƒæ˜¯åŒè¯­çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜å¯ä»¥ç”Ÿæˆè¥¿ç­ç‰™è¯­è¯„è®ºçš„æ‘˜è¦ï¼š

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ç”Ÿæˆçš„æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡çš„æ„æ€æ˜¯â€œéå¸¸å®¹æ˜“é˜…è¯»â€ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ˜¯ç›´æ¥ä»è¯„è®ºä¸­æå–çš„ã€‚åŒæ—¶ï¼Œè¿™ä¸ªä¾‹å­è¿˜å±•ç°äº†mT5 æ¨¡å‹çš„å¤šç§åŠŸèƒ½ç‰¹æ€§ï¼Œå¹¶æ”¯æŒå¤„ç†å¤šè¯­è¨€çš„è¯­æ–™åº“ï¼

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å°è¯•ä¸€ä¸ªç¨å¾®å¤æ‚ä¸€ç‚¹çš„ä»»åŠ¡ï¼šä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚