<FrameworkSwitchCourse {fw} />

# 章节简介 [[章节简介]]

在[第三章](/course/chapter3)，您了解了如何微调文本分类的模型。在本章中，我们将处理以下常见的 NLP 任务：

- 词元（token）分类
- 掩码语言建模（如 BERT）
- 文本摘要
- 翻译
- 因果语言建模预训练（如 GPT-2）
- 问答

{#if fw === 'pt'}

为此，您需要利用[第三章](/course/chapter3)中学到的 `Trainer` API 和 🤗 Accelerate 库、[第五章](/course/chapter5)中的 🤗 Datasets 库以及[第六章](/course/chapter6)中的 🤗 Tokenizers 库的所有知识。我们同样会将结果上传到模型中心，就像我们在[第四章](/course/chapter4)中所做的那样，所以这确实是融会贯通的一章！

每个部分都可以独立阅读，并将向您展示如何使用 `Trainer` API 或按照您自己的训练循环训练模型，并采用 🤗 Accelerate 加速。你可以随意跳过任何一部分，专注于您最感兴趣的部分：`Trainer` API 非常适用于微调（fine-tuning）或训练您的模型，且无需担心幕后发生的事情；而采用 `Accelerate` 的训练循环可以让您更轻松地自定义所需的任何结构。

{:else}

为此，您需要利用[第三章](/course/chapter3)中学到的有关 Keras API、[第五章](/course/chapter5)中的 🤗 Datasets 库以及[第六章](/course/chapter6)中的 🤗 Tokenizers 库的所有知识。我们同样会将结果上传到模型中心，就像我们在[第四章](/course/chapter4)中所做的那样，所以这确实是融会贯通的一章！

每个部分都可以独立阅读。

{/if}


<Tip>

如果您按顺序阅读这些部分，您会注意到它们有很多共同的代码和陈述。 重复是有意为之的，让您可以深入（或稍后返回）任何您感兴趣的任务并找到一个完整的工作示例。

</Tip>
