# å‡ºç°é”™è¯¯æ—¶è¯¥æ€ä¹ˆåŠ [[å‡ºç°é”™è¯¯æ—¶è¯¥æ€ä¹ˆåŠ]]

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section2.ipynb"},
]} />

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å½“ä½ å°è¯•ä»æ–°è°ƒæ•´çš„ Transformer æ¨¡å‹ç”Ÿæˆé¢„æµ‹æ—¶å¯èƒ½å‘ç”Ÿçš„ä¸€äº›å¸¸è§é”™è¯¯ã€‚æœ¬èŠ‚ä¸ºå°† [ç¬¬å››èŠ‚](/course/chapter8/section4) åšå‡†å¤‡ï¼Œåœ¨é‚£ä¸€èŠ‚ä¸­æ¢ç´¢å¦‚ä½•è°ƒè¯•è®­ç»ƒé˜¶æ®µæœ¬èº«ã€‚

<Youtube id="DQ-CpJn6Rc4"/>

æˆ‘ä»¬ä¸ºè¿™ä¸€èŠ‚å‡†å¤‡äº†ä¸€ä¸ª [æ¨¡æ¿ä»“åº“](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) ï¼Œå¦‚æœä½ æƒ³è¿è¡Œæœ¬ç« ä¸­çš„ä»£ç ï¼Œé¦–å…ˆéœ€è¦å°†æ¨¡å‹å¤åˆ¶åˆ°è‡ªå·±çš„ [Hugging Face Hub](https://huggingface.co) è´¦å·ã€‚è¿™éœ€è¦ä½ åœ¨ Jupyter Notebook ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥ç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

æˆ–åœ¨ä½ æœ€å–œæ¬¢çš„ç»ˆç«¯ä¸­æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```bash
huggingface-cli login
```

è¿™é‡Œå°†ä¼šæç¤ºä½ è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ï¼Œç™»é™†åä¼šè‡ªåŠ¨åœ¨ `~/.cache/huggingface/` ä¿å­˜ä¸€ä¸ªä»¤ç‰Œ å®Œæˆç™»å½•åï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹åŠŸèƒ½å…‹éš†æ¨¡æ¿ä»“åº“ï¼š

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # å…‹éš†ä»“åº“å¹¶æå–æœ¬åœ°è·¯å¾„
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # åœ¨ Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ–°ä»“åº“
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # å…‹éš†ç©ºä»“åº“
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # å¤åˆ¶æ–‡ä»¶
    copy_tree(template_repo_dir, new_repo_dir)
    # ä¸Šä¼ åˆ° Hub ä¸Š
    repo.push_to_hub()
```

ç°åœ¨å½“ä½ è°ƒç”¨ `copy_repository_template()` æ—¶ï¼Œå®ƒå°†åœ¨ä½ çš„å¸æˆ·ä¸‹åˆ›å»ºæ¨¡æ¿ä»“åº“çš„å‰¯æœ¬ã€‚

## è°ƒè¯• ğŸ¤— Transformers çš„ `pipeline` [[è°ƒè¯• ğŸ¤— Transformers çš„ `pipeline`]]

æ¥ä¸‹æ¥è¦å¼€å§‹æˆ‘ä»¬è°ƒè¯• Transformer æ¨¡å‹çš„å¥‡å¦™ä¸–ç•Œä¹‹æ—…ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹æƒ…æ™¯ï¼šä½ æ­£åœ¨ä¸ä¸€ä½åŒäº‹åˆä½œå¼€å‘ä¸€ä¸ªé—®ç­”çš„é¡¹ç›®ï¼Œè¿™ä¸ªé¡¹ç›®å¯ä»¥å¸®åŠ©ç”µå­å•†åŠ¡ç½‘ç«™çš„å®¢æˆ·æ‰¾åˆ°æœ‰å…³æ¶ˆè´¹å“ä¸€äº›é—®é¢˜çš„å›ç­”ã€‚å‡å¦‚ä½ çš„åŒäº‹ç»™ä½ å‘äº†è¿™æ ·ä¸€æ¡æ¶ˆæ¯ï¼š

> å—¨ï¼æˆ‘åˆšåˆšä½¿ç”¨äº† Hugging Face è¯¾ç¨‹çš„ [ç¬¬ä¸ƒç« ](/course/chapter7/7) ä¸­çš„æŠ€æœ¯è¿›è¡Œäº†ä¸€ä¸ªå®éªŒï¼Œå¹¶åœ¨ SQuAD ä¸Šè·å¾—äº†ä¸€äº›å¾ˆæ£’çš„ç»“æœï¼æˆ‘è§‰å¾—æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å‹ä½œä¸ºé¡¹ç›®çš„èµ·ç‚¹ã€‚Hub ä¸Šçš„æ¨¡å‹ ID æ˜¯ `lewtun/distillbert-base-uncased-finetuned-squad-d5716d28`ã€‚ä½ æ¥æµ‹è¯•ä¸€ä¸‹ ï¼‰

ä½ é¦–å…ˆæƒ³åˆ°çš„æ˜¯ä½¿ç”¨ ğŸ¤— Transformers ä¸­çš„ `pipeline` ï¼š

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

å•Šå“¦ï¼Œå¥½åƒå‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼å¦‚æœä½ æ˜¯ç¼–ç¨‹æ–°æ‰‹ï¼Œè¿™ç±»é”™è¯¯ä¸€å¼€å§‹çœ‹èµ·æ¥æœ‰ç‚¹ç¥ç§˜ ï¼ˆ `OSError` åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ã€‚å…¶å®è¿™é‡Œæ˜¾ç¤ºçš„é”™è¯¯åªæ˜¯å…¨éƒ¨é”™è¯¯æŠ¥å‘Šä¸­æœ€åä¸€éƒ¨åˆ†ï¼Œç§°ä¸º `Python traceback` ï¼ˆåˆåå †æ ˆè·Ÿè¸ªï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨ Google Colab ä¸Šè¿è¡Œæ­¤ä»£ç ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼äºä»¥ä¸‹å±å¹•æˆªå›¾çš„å†…å®¹ï¼š

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png" alt="A Python traceback." width="100%"/>
</div>

è¿™äº›æŠ¥å‘Šä¸­åŒ…å«å¾ˆå¤šä¿¡æ¯ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å…³é”®éƒ¨åˆ†ã€‚é˜…è¯»è¿™æ ·çš„æŠ¥å‘Šæ—¶çš„é˜…è¯»é¡ºåºæ¯”è¾ƒç‰¹æ®Šï¼Œåº”è¯¥æŒ‰ç…§ä»åº•éƒ¨åˆ°é¡¶éƒ¨çš„é¡ºåºé˜…è¯»ï¼Œå¦‚æœä½ ä¹ æƒ¯äºä»ä¸Šåˆ°ä¸‹é˜…è¯»æ–‡æœ¬ï¼Œè¿™å¯èƒ½å¬èµ·æ¥å¾ˆå¥‡æ€ªï¼Œä½†å®ƒåæ˜ äº†ä¸€ä¸ªäº‹å®ï¼štraceback æ˜¾ç¤ºäº†åœ¨ä¸‹è½½æ¨¡å‹å’Œ tokenizer æ—¶ `pipeline` å‡½æ•°è°ƒç”¨çš„é¡ºåºã€‚ï¼ˆæŸ¥çœ‹ [ç¬¬äºŒç« ](/course/chapter2) äº†è§£æœ‰å…³ `pipeline` å†…éƒ¨åŸç†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚ï¼‰

<Tip>

ğŸš¨ çœ‹åˆ° Google Colab ä¸­ traceback ä¸­é—´ â€œ6 framesâ€ çš„æ¤­åœ†å½¢è“è‰²æ¡†äº†å—ï¼Ÿè¿™æ˜¯ Colab çš„ä¸€ä¸ªç‰¹æ®ŠåŠŸèƒ½ï¼Œå®ƒä¼šè‡ªåŠ¨å°† traceback çš„ä¸­é—´éƒ¨åˆ†å‹ç¼©ä¸ºâ€œframesâ€ã€‚å¦‚æœä½ æ— æ³•æ‰¾åˆ°é”™è¯¯çš„æ¥æºï¼Œå¯ä»¥é€šè¿‡å•å‡»è¿™ä¸¤ä¸ªå°ç®­å¤´æ¥å±•å¼€å®Œæ•´çš„ tracebackã€‚

</Tip>

è¿™æ„å‘³ç€ traceback çš„æœ€åä¸€è¡Œæ˜¾ç¤ºçš„æ˜¯æœ€åä¸€æ¡é”™è¯¯æ¶ˆæ¯å’Œå¼•å‘çš„å¼‚å¸¸åç§°ã€‚åœ¨è¿™é‡Œï¼Œå¼‚å¸¸ç±»å‹æ˜¯ `OSError` ï¼Œè¡¨ç¤ºè¿™ä¸ªé”™è¯¯ä¸ç³»ç»Ÿç›¸å…³ã€‚å¦‚æœæˆ‘ä»¬é˜…è¯»éšä¹‹é™„ç€çš„é”™è¯¯æ¶ˆæ¯ï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„ `config.json` æ–‡ä»¶ä¼¼ä¹æœ‰é—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºäº†ä¸¤ä¸ªä¿®å¤çš„å»ºè®®ï¼š

```python out
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

<Tip>

ğŸ’¡ å¦‚æœä½ é‡åˆ°éš¾ä»¥ç†è§£çš„é”™è¯¯æ¶ˆæ¯ï¼Œåªéœ€å°†è¯¥æ¶ˆæ¯å¤åˆ¶å¹¶ç²˜è´´åˆ° Google æˆ– [Stack Overflow](https://stackoverflow.com) æœç´¢æ ä¸­ã€‚ä½ å¾ˆæœ‰å¯èƒ½ä¸æ˜¯ç¬¬ä¸€ä¸ªé‡åˆ°é”™è¯¯çš„äººï¼Œå› æ­¤å¾ˆæœ‰å¯èƒ½åœ¨ç¤¾åŒºä¸­æ‰¾åˆ°å…¶ä»–äººå‘å¸ƒçš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨ Stack Overflow ä¸Šæœç´¢ `OSError: Can't load config for` ç»™å‡ºäº†å‡ ä¸ª [ç»“æœ](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+) ï¼Œå¯ä»¥ä½œä¸ºä½ è§£å†³é—®é¢˜çš„èµ·ç‚¹ã€‚

</Tip>

ç¬¬ä¸€ä¸ªå»ºè®®æ˜¯æ£€æŸ¥æ¨¡å‹ ID æ˜¯å¦çœŸçš„æ­£ç¡®ï¼Œæ‰€ä»¥é¦–å…ˆè¦åšçš„å°±æ˜¯å¤åˆ¶æ ‡ç­¾å¹¶å°†å…¶ç²˜è´´åˆ° Hub çš„æœç´¢æ ä¸­ï¼š

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png" alt="The wrong model name." width="100%"/>
</div>

å—¯ï¼Œçœ‹èµ·æ¥ä½ åŒäº‹çš„æ¨¡å‹ç¡®å®ä¸åœ¨ Hub ä¸Šã€‚ä½†æ˜¯ä»”ç»†çœ‹æ¨¡å‹åç§°å°±ä¼šå‘ç°ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªé”™åˆ«å­—ï¼æ­£ç¡®çš„ DistilBERT çš„åç§°ä¸­åªæœ‰ä¸€ä¸ª â€œlâ€ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¿®æ­£åå¯»æ‰¾ `lewtun/distilbert-base-uncased-finetuned-squad-d5716d28`ï¼š

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png" alt="The right model name." width="100%"/>
</div>

å¥½çš„ï¼Œè¿™æ¬¡æœ‰ç»“æœäº†ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ ID å†æ¬¡å°è¯•ä¸‹è½½æ¨¡å‹ï¼š

```python
model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

å•Šï¼Œå†æ¬¡å¤±è´¥ã€‚ä¸è¦æ°”é¦ï¼Œæ¬¢è¿æ¥åˆ°æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆçš„æ—¥å¸¸ç”Ÿæ´»ï¼å‰é¢æˆ‘ä»¬å·²ç»ä¿®æ­£äº†æ¨¡å‹ IDï¼Œæ‰€ä»¥é—®é¢˜ä¸€å®šå‡ºåœ¨ä»“åº“æœ¬èº«ã€‚è¿™é‡Œæä¾›ä¸€ç§å¿«é€Ÿè®¿é—® ğŸ¤— Hub ä¸Šä»“åº“å†…å®¹çš„æ–¹æ³•â€”â€”é€šè¿‡ `huggingface_hub` åº“çš„ `list_repo_files()` å‡½æ•°ï¼š

```python
from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)
```

```python out
['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']
```

 å¥‡æ€ªçš„æ˜¯â€”â€”ä»“åº“ä¸­ä¼¼ä¹æ²¡æœ‰é…ç½® `config.json` æ–‡ä»¶ï¼éš¾æ€ªæˆ‘ä»¬çš„ `pipeline` æ— æ³•åŠ è½½æ¨¡å‹ï¼›ä½ çš„åŒäº‹ä¸€å®šæ˜¯åœ¨å¾®è°ƒåå¿˜è®°å°†è¿™ä¸ªæ–‡ä»¶ä¸Šä¼ åˆ° Hubã€‚åœ¨è¿™ç§æƒ…å†µä¸‹é—®é¢˜ä¼¼ä¹å¾ˆå®¹æ˜“è§£å†³ï¼šè¦æ±‚ä»–æ·»åŠ æ–‡ä»¶ï¼Œæˆ–è€…æˆ‘ä»¬ä»æ¨¡å‹ ID ä¸­å¯ä»¥çœ‹å‡ºä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯ [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased) ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥ä¸‹è½½æ­¤æ¨¡å‹çš„é…ç½®æ–‡ä»¶å¹¶å°†å…¶ä¸Šä¼ åˆ°ä½ ä»¬çš„ä»“åº“åæŸ¥çœ‹è¿™æ ·æ˜¯å¦å¯ä»¥è§£å†³é—®é¢˜ã€‚åœ¨è¿™é‡Œæ¶‰åŠåˆ° [ç¬¬äºŒç« ](/course/chapter2) ä¸­å­¦ä¹ çš„æŠ€å·§ï¼Œä½¿ç”¨ `AutoConfig` ç±»ä¸‹è½½æ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼š

```python
from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)
```

<Tip warning={true}>

ğŸš¨ åœ¨è¿™é‡Œé‡‡ç”¨çš„æ–¹æ³•å¹¶ä¸æ˜¯ç™¾åˆ†ä¹‹ç™¾å¯é çš„ï¼Œå› ä¸ºä½ çš„åŒäº‹å¯èƒ½åœ¨å¾®è°ƒæ¨¡å‹ä¹‹å‰å·²ç»è°ƒæ•´äº† `distilbert-base-uncased` é…ç½®ã€‚åœ¨ç°å®æƒ…å†µä¸­ï¼Œæˆ‘ä»¬åº”è¯¥å…ˆä¸ä»–ä»¬æ ¸å®ï¼Œä½†åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å…ˆå‡è®¾ä»–ä»¬ä½¿ç”¨çš„æ˜¯é»˜è®¤é…ç½®ã€‚

</Tip>

ä¸Šä¸€æ­¥æˆåŠŸåï¼Œå¯ä»¥ä½¿ç”¨ `config` çš„ `push_to_hub()` æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ°æ¨¡å‹ä»“åº“ï¼š

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

ç°åœ¨å¯ä»¥é€šè¿‡ä»æœ€æ–°æäº¤çš„ `main` åˆ†æ”¯ä¸­åŠ è½½æ¨¡å‹æ¥æµ‹è¯•æ˜¯å¦æœ‰æ•ˆï¼š

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

ğŸ¤— Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)
```

```python out
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}
```

æˆåŠŸäº†ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä½ åˆšåˆšå­¦åˆ°çš„ä¸œè¥¿ï¼š

- Python ä¸­çš„é”™è¯¯æ¶ˆæ¯ç§°ä¸º `tracebacks` ï¼Œæ³¨æ„éœ€è¦ä»ä¸‹åˆ°ä¸Šé˜…è¯»ã€‚é”™è¯¯æ¶ˆæ¯çš„æœ€åä¸€è¡Œé€šå¸¸åŒ…å«å®šä½é—®é¢˜æ ¹æºæ‰€éœ€çš„ä¿¡æ¯ã€‚
- å¦‚æœæœ€åä¸€è¡Œæ²¡æœ‰åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œéœ€è¦è¿›è¡Œ `tracebacks` ï¼ˆå‘ä¸Šé€çº§æŸ¥çœ‹ï¼‰ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥ç¡®å®šæºä»£ç ä¸­å‘ç”Ÿé”™è¯¯çš„ä½ç½®ã€‚
- å¦‚æœæ²¡æœ‰ä»»ä½•é”™è¯¯æ¶ˆæ¯å¯ä»¥å¸®åŠ©ä½ è°ƒè¯•é—®é¢˜ï¼Œè¯·å°è¯•åœ¨çº¿æœç´¢ç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚
- `huggingface_hub` åº“æä¾›äº†ä¸€å¥—å·¥å…·ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™äº›å·¥å…·ä¸ Hub ä¸Šçš„ä»“åº“è¿›è¡Œäº¤äº’å’Œè°ƒè¯•ã€‚

ç°åœ¨ä½ çŸ¥é“å¦‚ä½•è°ƒè¯• `pipeline` ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªæ›´æ£˜æ‰‹çš„ä¾‹å­ï¼Œå³è°ƒè¯•æ¨¡å‹æœ¬èº«çš„å‰å‘ä¼ æ’­çš„è¿‡ç¨‹ã€‚

## è°ƒè¯•æ¨¡å‹çš„å‰å‘ä¼ æ’­ [[è°ƒè¯•æ¨¡å‹çš„å‰å‘ä¼ æ’­]]

å°½ç®¡ `pipeline` å¯¹äºå¤§å¤šæ•°éœ€è¦å¿«é€Ÿç”Ÿæˆé¢„æµ‹ç»“æœçš„åº”ç”¨åœºæ™¯æ¥è¯´éå¸¸æœ‰ç”¨ï¼Œä½†æ˜¯æœ‰æ—¶ä½ éœ€è¦è®¿é—®æ¨¡å‹çš„ logits ï¼ˆæ¯”å¦‚ä½ æƒ³è¦å®ç°ä¸€äº›çš„è‡ªå®šä¹‰åç»­å¤„ç†ï¼‰ã€‚ä¸ºäº†çœ‹çœ‹åœ¨è¿™ç§æƒ…å†µä¸‹å¯èƒ½ä¼šå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä» `pipeline` ä¸­è·å–æ¨¡å‹å’Œ `tokenizers` 

```python
tokenizer = reader.tokenizer
model = reader.model
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œçœ‹çœ‹åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹æ˜¯å¦å—æ”¯æŒæˆ‘ä»¬æœ€å–œæ¬¢çš„æ¡†æ¶ï¼š

```python
question = "Which frameworks can I use?"
```

æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸ƒç« ](/course/chapter7) ä¸­å­¦ä¹ çš„ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥éœ€è¦å¯¹è¾“å…¥è¿›è¡Œ tokenizeï¼Œæå–èµ·å§‹å’Œç»“æŸ token çš„ logitsï¼Œç„¶åè§£ç ç­”æ¡ˆæ‰€åœ¨çš„æ–‡æœ¬èŒƒå›´ï¼š

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆå¼€å¤´
answer_start = torch.argmax(answer_start_scores)
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆç»“å°¾
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""
```

çœ‹èµ·æ¥æˆ‘ä»¬çš„ä»£ç ä¸­æœ‰ä¸€ä¸ªé”™è¯¯ï¼ä¸ç”¨ç´§å¼ ï¼Œä½ å¯ä»¥åœ¨ Notebook ä¸­ä½¿ç”¨ Python è°ƒè¯•å™¨ï¼š

<Youtube id="rSPyvPw0p9k"/>

æˆ–è€…è·Ÿéšæˆ‘ä»¬ä¸€èµ·ä»ç»ˆç«¯ä¸­é€æ­¥è¯•éªŒæ‰¾åˆ°é”™è¯¯ï¼š

<Youtube id="5PkZ4rbHL6c"/>

åœ¨è¿™é‡Œï¼Œé”™è¯¯æ¶ˆæ¯å‘Šè¯‰æˆ‘ä»¬ `'list' object has no attribute 'size'` ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ª `-->` ç®­å¤´æŒ‡å‘ `model(**inputs)` ä¸­å¼•å‘é—®é¢˜çš„è¡Œã€‚ä½ å¯ä»¥ä½¿ç”¨ Python è°ƒè¯•å™¨ç”¨äº¤äº’æ–¹å¼æ¥è°ƒè¯•å®ƒï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬åªéœ€æ‰“å°å‡ºä¸€éƒ¨åˆ† `inputs` ï¼Œæ£€æŸ¥ä¸€ä¸‹å®ƒçš„å€¼ï¼š

```python
inputs["input_ids"][:5]
```

```python out
[101, 2029, 7705, 2015, 2064]
```

è¿™å½“ç„¶çœ‹èµ·æ¥åƒä¸€ä¸ªæ™®é€šçš„ Python `list` ï¼Œä½†è®©æˆ‘ä»¬ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ç±»å‹ï¼š

```python
type(inputs["input_ids"])
```

```python out
list
```

æ˜¯çš„ï¼Œçš„ç¡®æ˜¯ä¸€ä¸ª Python `list` ã€‚é‚£ä¹ˆå‡ºäº†ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿå›æƒ³ä¸€ä¸‹æˆ‘ä»¬åœ¨ [ç¬¬äºŒç« ](/course/chapter2) ğŸ¤— Transformers ä¸­çš„ `AutoModelForXxx` ç±»ä¸­çš„ `tensors` ï¼ˆPyTorch æˆ–è€… TensorFlowï¼‰è¿›è¡Œçš„æ“ä½œï¼Œä¾‹å¦‚åœ¨ PyTorch ä¸­ï¼Œä¸€ä¸ªå¸¸è§çš„æ“ä½œæ˜¯ä½¿ç”¨ `Tensor.size()` æ–¹æ³•æå–å¼ é‡çš„ç»´åº¦ã€‚è®©æˆ‘ä»¬å†å›åˆ° traceback ä¸­ï¼Œçœ‹çœ‹å“ªä¸€è¡Œè§¦å‘äº†å¼‚å¸¸ï¼š

```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

çœ‹èµ·æ¥æˆ‘ä»¬çš„ä»£ç è¯•å›¾è°ƒç”¨ `input_ids.size()` ï¼Œä½† `.size()` æ–¹æ³•æ˜¾ç„¶ä¸é€‚ç”¨äº Python çš„ `list` ï¼Œ `list` åªæ˜¯ä¸€ä¸ªå®¹å™¨ï¼Œä¸æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚æˆ‘ä»¬å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿåœ¨ Stack Overflow ä¸Šæœç´¢é”™è¯¯æ¶ˆæ¯ï¼Œå¯ä»¥æ‰¾åˆ°å¾ˆå¤šç›¸å…³çš„ [ç»“æœ](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f) ã€‚å•å‡»ç¬¬ä¸€ä¸ªæœç´¢ç»“æœï¼Œå°±ä¼šæ‰¾åˆ°ä¸æˆ‘ä»¬ç±»ä¼¼çš„é—®é¢˜çš„ç­”æ¡ˆï¼Œç­”æ¡ˆå¦‚ä¸‹é¢çš„å±å¹•æˆªå›¾æ‰€ç¤ºï¼š

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png" alt="An answer from Stack Overflow." width="100%"/>
</div>

è¿™é‡Œå»ºè®®æˆ‘ä»¬æ·»åŠ  `return_tensors='pt'` åˆ° Tokenizerï¼Œè®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æ˜¯å¦å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³é—®é¢˜ï¼š

```python out
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆå¼€å¤´
answer_start = torch.argmax(answer_start_scores)
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆç»“å°¾
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""
```

æˆåŠŸäº†ï¼è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œå±•ç¤ºäº† Stack Overflow ç¤¾åŒºçš„å®ç”¨æ€§ï¼Œæœç´¢ç±»ä¼¼çš„é—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»ç¤¾åŒºä¸­å…¶ä»–äººçš„ç»éªŒä¸­å—ç›Šã€‚ç„¶è€Œï¼Œåƒæˆ‘ä»¬è¿™æ ·çš„æœç´¢ä¸æ˜¯æ€»èƒ½æ‰¾åˆ°ç›¸å…³çš„ç­”æ¡ˆï¼Œé‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä½ è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå¹¸è¿çš„æ˜¯ï¼Œåœ¨ [Hugging Face è®ºå›](https://discuss.huggingface.co/) ä¸Šæœ‰ä¸€ä¸ªå‹å¥½çš„å¼€å‘è€…ç¤¾åŒºå¯ä»¥å¸®åŠ©ä½ ï¼åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹åœ¨è¯¥å¹³å°ä¸­æœ‰æ•ˆåœ°å¾—åˆ°é—®é¢˜çš„å›ç­”ã€‚
