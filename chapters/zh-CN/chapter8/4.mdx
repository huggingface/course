<FrameworkSwitchCourse {fw} />

# è°ƒè¯•è®­ç»ƒç®¡é“

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4.ipynb"},
]} />

ä½ å·²ç»ç¼–å†™äº†ä¸€ä¸ªæ¼‚äº®çš„è„šæœ¬æ¥è®­ç»ƒæˆ–å¾®è°ƒç»™å®šä»»åŠ¡çš„æ¨¡å‹ï¼Œå°½èŒå°½è´£åœ°éµå¾ª [Chapter 7](/course/chapter7) ä¸­çš„å»ºè®®ã€‚ ä½†æ˜¯å½“ä½ å¯åŠ¨å‘½ä»¤ `trainer.train()` æ—¶ï¼Œå¯æ€•çš„äº‹æƒ…å‘ç”Ÿäº†ï¼šä½ å¾—åˆ°ä¸€ä¸ªé”™è¯¯ğŸ˜±ï¼ æˆ–è€…æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¸€åˆ‡ä¼¼ä¹éƒ½å¾ˆå¥½ï¼Œè®­ç»ƒè¿è¡Œæ²¡æœ‰é”™è¯¯ï¼Œä½†ç”Ÿæˆçš„æ¨¡å‹å¾ˆç³Ÿç³•ã€‚ åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•è°ƒè¯•æ­¤ç±»é—®é¢˜ã€‚

## è°ƒè¯•è®­ç»ƒç®¡é“

<Youtube id="L-WSwUWde1U"/>

å½“æ‚¨åœ¨ `trainer.train()` ä¸­é‡åˆ°é”™è¯¯æ—¶ï¼Œå®ƒå¯èƒ½æ¥è‡ªå¤šä¸ªæ¥æºï¼Œå› ä¸º `Trainer` é€šå¸¸ä¼šå°†å¾ˆå¤šä¸œè¥¿æ”¾åœ¨ä¸€èµ·ç»„åˆè¿è¡Œã€‚ å®ƒå°†datasetsè½¬æ¢ä¸ºdataloadersï¼Œå› æ­¤é—®é¢˜å¯èƒ½å‡ºåœ¨datasetsä¸­ï¼Œæˆ–è€…åœ¨å°è¯•å°†datasetsçš„å…ƒç´ ä¸€èµ·æ‰¹å¤„ç†æ—¶å‡ºç°é—®é¢˜ã€‚ ç„¶åå®ƒéœ€è¦å‡†å¤‡ä¸€æ‰¹æ•°æ®å¹¶å°†å…¶æä¾›ç»™æ¨¡å‹ï¼Œå› æ­¤é—®é¢˜å¯èƒ½å‡ºåœ¨æ¨¡å‹ä»£ç ä¸­ã€‚ ä¹‹åï¼Œå®ƒä¼šè®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œä¼˜åŒ–å™¨ï¼Œå› æ­¤é—®é¢˜ä¹Ÿå¯èƒ½å‡ºåœ¨æ‚¨çš„ä¼˜åŒ–å™¨ä¸­ã€‚ å³ä½¿è®­ç»ƒä¸€åˆ‡é¡ºåˆ©ï¼Œå¦‚æœæ‚¨çš„è¯„ä¼°æŒ‡æ ‡æœ‰é—®é¢˜ï¼Œè¯„ä¼°æœŸé—´ä»ç„¶å¯èƒ½å‡ºç°é—®é¢˜ã€‚

è°ƒè¯• `trainer.train()` ä¸­å‡ºç°çš„é”™è¯¯çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰‹åŠ¨æ£€æŸ¥æ•´ä¸ªç®¡é“ï¼Œçœ‹çœ‹å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚ é”™è¯¯é€šå¸¸å¾ˆå®¹æ˜“è§£å†³ã€‚

ä¸ºäº†è¯æ˜è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹è„šæœ¬ï¼ˆå°è¯•ï¼‰åœ¨ [MNLI æ•°æ®é›†](https://huggingface.co/datasets/glue)ä¸Šå¾®è°ƒ DistilBERT æ¨¡å‹ï¼š

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

å¦‚æœä½ å°è¯•æ‰§è¡Œå®ƒï¼Œä½ ä¼šé‡åˆ°ä¸€ä¸ªç›¸å½“ç¥ç§˜çš„é”™è¯¯ï¼š

```python out
'ValueError: You have to specify either input_ids or inputs_embeds'
```

### æ£€æŸ¥æ•°æ®

è¿™æ˜¯ä¸è¨€è€Œå–»çš„ï¼Œå¦‚æœä½ çš„æ•°æ®è¢«ç ´åï¼Œâ€œTrainerâ€å°†æ— æ³•å½¢æˆæ‰¹æ¬¡ï¼Œæ›´ä¸ç”¨è¯´è®­ç»ƒä½ çš„æ¨¡å‹äº†ã€‚ æ‰€ä»¥é¦–å…ˆï¼Œä½ éœ€è¦çœ‹çœ‹ä½ çš„è®­ç»ƒé›†ä¸­æœ‰ä»€ä¹ˆã€‚

ä¸ºäº†é¿å…èŠ±è´¹æ— æ•°å°æ—¶è¯•å›¾æ£€æŸ¥å’Œä¿®å¤ä¸æ˜¯é”™è¯¯æ¥æºçš„ä¸œè¥¿ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨ `trainer.train_dataset` è¿›è¡Œæ£€æŸ¥ã€‚ æ‰€ä»¥è®©æˆ‘ä»¬åœ¨è¿™é‡Œè¿™æ ·åšï¼š

```py
trainer.train_dataset[0]
```

```python out
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```

ä½ æ³¨æ„åˆ°æœ‰ä»€ä¹ˆä¸å¯¹å—ï¼Ÿ ä¸ç¼ºå°‘æœ‰å…³ `input_ids` çš„é”™è¯¯æ¶ˆæ¯ç›¸ç»“åˆï¼Œåº”è¯¥è®©æ‚¨æ„è¯†åˆ°æ•°æ®é›†é‡Œæ˜¯æ–‡æœ¬ï¼Œè€Œä¸æ˜¯æ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚ åœ¨è¿™ä¸ªä¾‹å­ï¼Œè¾“å‡ºçš„åŸå§‹é”™è¯¯ä¿¡æ¯éå¸¸å…·æœ‰è¯¯å¯¼æ€§ï¼Œå› ä¸º `Trainer` ä¼šè‡ªåŠ¨åˆ é™¤ä¸æ¨¡å‹ç­¾åä¸åŒ¹é…çš„åˆ—ï¼ˆå³æ¨¡å‹é¢„æœŸçš„å‚æ•°ï¼‰ã€‚ è¿™æ„å‘³ç€åœ¨è¿™é‡Œï¼Œé™¤äº†æ ‡ç­¾ä¹‹å¤–çš„æ‰€æœ‰ä¸œè¥¿éƒ½è¢«ä¸¢å¼ƒäº†ã€‚ å› æ­¤ï¼Œåˆ›å»ºæ‰¹æ¬¡ç„¶åå°†å®ƒä»¬å‘é€åˆ°æ¨¡å‹æ²¡æœ‰é—®é¢˜ï¼Œè€Œæ¨¡å‹åˆæŠ±æ€¨å®ƒæ²¡æœ‰æ”¶åˆ°æ­£ç¡®çš„è¾“å…¥ã€‚

ä¸ºä»€ä¹ˆæ²¡æœ‰å¤„ç†æ•°æ®ç”Ÿæˆæ ‡è®°å‘¢ï¼Ÿ æˆ‘ä»¬ç¡®å®åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨äº†â€œDataset.map()â€æ–¹æ³•æ¥å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨æ ‡è®°å™¨ã€‚ ä½†æ˜¯å¦‚æœä½ ä»”ç»†çœ‹ä»£ç ï¼Œä½ ä¼šå‘ç°æˆ‘ä»¬åœ¨å°†è®­ç»ƒå’Œè¯„ä¼°é›†ä¼ é€’ç»™`Trainer`æ—¶çŠ¯äº†ä¸€ä¸ªé”™è¯¯ã€‚ æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰ä½¿ç”¨ `tokenized_datasets`ï¼Œè€Œæ˜¯ä½¿ç”¨äº† `raw_datasets` ğŸ¤¦ã€‚ æ‰€ä»¥è®©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

è¿™ä¸ªæ–°ä»£ç ç°åœ¨ä¼šç»™å‡ºä¸€ä¸ªä¸åŒçš„é”™è¯¯ï¼š

```python out
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```

æŸ¥çœ‹tracebackï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é”™è¯¯å‘ç”Ÿåœ¨æ•°æ®æ•´ç†æ­¥éª¤ä¸­ï¼š

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

æ‰€ä»¥ï¼Œæˆ‘ä»¬åº”è¯¥å»ç ”ç©¶ä¸€ä¸‹é‚£ä¸ªã€‚ ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬è¿™æ ·åšä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®Œæˆæ£€æŸ¥æˆ‘ä»¬çš„æ•°æ®ï¼Œ å…ˆç¡®å®šå®ƒ100%æ˜¯æ­£ç¡®çš„ã€‚

åœ¨è°ƒè¯•è¯¾ç¨‹çš„å†…å®¹æ—¶ï¼Œæ‚¨åº”è¯¥å§‹ç»ˆåšçš„ä¸€ä»¶äº‹æ˜¯æŸ¥çœ‹æ¨¡å‹çš„è§£ç è¾“å…¥ã€‚ æˆ‘ä»¬æ— æ³•ç†è§£ç›´æ¥æä¾›ç»™å®ƒçš„æ•°å­—ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥çœ‹çœ‹è¿™äº›æ•°å­—ä»£è¡¨ä»€ä¹ˆã€‚ ä¾‹å¦‚ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œè¿™æ„å‘³ç€æŸ¥çœ‹æ‚¨ä¼ é€’çš„å›¾ç‰‡åƒç´ çš„è§£ç ï¼Œåœ¨è¯­éŸ³ä¸­æ„å‘³ç€è§£ç åçš„éŸ³é¢‘æ ·æœ¬ï¼Œå¯¹äºæˆ‘ä»¬çš„ NLP ç¤ºä¾‹ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨æˆ‘ä»¬çš„æ ‡è®°å™¨è§£ç çš„è¾“å…¥ï¼š

```py
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python out
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

æ‰€ä»¥è¿™ä¼¼ä¹æ˜¯æ­£ç¡®çš„ã€‚ æ‚¨åº”è¯¥å¯¹è¾“å…¥ä¸­çš„æ‰€æœ‰é”®æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
trainer.train_dataset[0].keys()
```

```python out
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

è¯·æ³¨æ„ï¼Œä¸æ¨¡å‹æ¥å—çš„è¾“å…¥ä¸å¯¹åº”çš„é”®å°†è¢«è‡ªåŠ¨ä¸¢å¼ƒï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬å°†ä»…ä¿ç•™ `input_ids`ã€`attention_mask` å’Œ `label`ï¼ˆå°†é‡å‘½åä¸º `labels`ï¼‰ã€‚ è¦ä»”ç»†æ£€æŸ¥æ¨¡å‹è¾“å…¥çš„åˆ—ï¼Œæ‚¨å¯ä»¥æ‰“å°æ¨¡å‹çš„ç±»ï¼Œç„¶åæŸ¥çœ‹å…¶æ–‡æ¡£ï¼š

```py
type(trainer.model)
```

```python out
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

æ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åœ¨[åœ¨è¿™ä¸ªé¡µé¢](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification)å¯ä»¥æ£€æŸ¥ä¸Šæ¥å—çš„å‚æ•°ã€‚ `Trainer` ä¹Ÿä¼šè®°å½•å®ƒä¸¢å¼ƒçš„åˆ—ã€‚

æˆ‘ä»¬é€šè¿‡è§£ç æ£€æŸ¥äº†è¾“å…¥ ID æ˜¯å¦æ­£ç¡®ã€‚ æ¥ä¸‹æ¥æ˜¯æ£€æŸ¥ `attention_mask`ï¼š

```py
tokenizer.decode(trainer.train_dataset[0]["attention_mask"])
```

```python out
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ç”±äºæˆ‘ä»¬æ²¡æœ‰åœ¨é¢„å¤„ç†ä¸­åº”ç”¨å¡«å……ï¼Œè¿™çœ‹èµ·æ¥éå¸¸è‡ªç„¶ã€‚ ä¸ºç¡®ä¿è¯¥æ³¨æ„æ©ç æ²¡æœ‰é—®é¢˜ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥å®ƒä¸è¾“å…¥ ID çš„é•¿åº¦æ˜¯å¦ç›¸åŒï¼š

```py
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python out
True
```

é‚£æŒºå¥½çš„ï¼ æœ€åï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„æ ‡ç­¾ï¼š

```py
trainer.train_dataset[0]["label"]
```

```python out
1
```

ä¸è¾“å…¥ ID ä¸€æ ·ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ¬èº«å¹¶æ²¡æœ‰çœŸæ­£æ„ä¹‰çš„æ•°å­—ã€‚ æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œæ•´æ•°å’Œæ ‡ç­¾åç§°ä¹‹é—´çš„æ˜ å°„å­˜å‚¨åœ¨æ•°æ®é›†ç›¸åº” *feature* çš„ `names` å±æ€§ä¸­ï¼š

```py
trainer.train_dataset.features["label"].names
```

```python out
['entailment', 'neutral', 'contradiction']
```

æ‰€ä»¥`1`è¡¨ç¤º`neutral`ï¼Œè¡¨ç¤ºæˆ‘ä»¬ä¸Šé¢çœ‹åˆ°çš„ä¸¤å¥è¯å¹¶ä¸çŸ›ç›¾ï¼Œä¹Ÿæ²¡æœ‰åŒ…å«å…³ç³»ã€‚ è¿™ä¼¼ä¹æ˜¯æ­£ç¡®çš„ï¼

æˆ‘ä»¬è¿™é‡Œæ²¡æœ‰ä»¤ç‰Œç±»å‹ IDï¼Œå› ä¸º DistilBERT ä¸éœ€è¦å®ƒä»¬ï¼› å¦‚æœæ‚¨çš„æ¨¡å‹ä¸­æœ‰ä¸€äº›ï¼Œæ‚¨è¿˜åº”è¯¥ç¡®ä¿å®ƒä»¬æ­£ç¡®åŒ¹é…è¾“å…¥ä¸­ç¬¬ä¸€å¥å’Œç¬¬äºŒå¥çš„ä½ç½®ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** æ£€æŸ¥è®­ç»ƒæ•°æ®é›†çš„ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å¦æ­£ç¡®ã€‚

</Tip>

æˆ‘ä»¬åœ¨è¿™é‡Œåªå¯¹è®­ç»ƒé›†è¿›è¡Œæ£€æŸ¥ï¼Œä½†æ‚¨å½“ç„¶åº”è¯¥ä»¥åŒæ ·çš„æ–¹å¼ä»”ç»†æ£€æŸ¥éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

ç°åœ¨æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„æ•°æ®é›†çœ‹èµ·æ¥ä¸é”™ï¼Œæ˜¯æ—¶å€™æ£€æŸ¥è®­ç»ƒç®¡é“çš„ä¸‹ä¸€æ­¥äº†ã€‚

### ä» datasets åˆ° dataloaders

è®­ç»ƒç®¡é“ä¸­å¯èƒ½å‡ºé”™çš„ä¸‹ä¸€ä»¶äº‹æ˜¯å½“â€œTrainerâ€å°è¯•ä»è®­ç»ƒæˆ–éªŒè¯é›†å½¢æˆæ‰¹æ¬¡æ—¶ã€‚ ä¸€æ—¦ä½ ç¡®å®š `Trainer` çš„æ•°æ®é›†æ˜¯æ­£ç¡®çš„ï¼Œä½ å¯ä»¥å°è¯•é€šè¿‡æ‰§è¡Œä»¥ä¸‹æ“ä½œæ‰‹åŠ¨å½¢æˆä¸€ä¸ªæ‰¹æ¬¡ï¼ˆå¯ä»¥å°† `train` æ›¿æ¢ä¸º `eval` ç”¨äºéªŒè¯æ•°æ®åŠ è½½å™¨ï¼‰ï¼š

```py
for batch in trainer.get_train_dataloader():
    break
```

æ­¤ä»£ç åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè¿­ä»£ï¼Œåœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶åœæ­¢ã€‚ å¦‚æœä»£ç æ‰§è¡Œæ²¡æœ‰é”™è¯¯ï¼Œé‚£ä¹ˆæ‚¨å°±æœ‰äº†å¯ä»¥æ£€æŸ¥çš„ç¬¬ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ï¼Œå¦‚æœä»£ç å‡ºé”™ï¼Œæ‚¨å¯ä»¥ç¡®å®šé—®é¢˜å‡ºåœ¨æ•°æ®åŠ è½½å™¨ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

æ£€æŸ¥trackbackçš„æœ€åä¸€ä¸ªå †æ ˆçš„è¾“å‡ºåº”è¯¥è¶³ä»¥ç»™ä½ ä¸€ä¸ªçº¿ç´¢ï¼Œä½†è®©æˆ‘ä»¬åšæ›´å¤šçš„æŒ–æ˜ã€‚ æ‰¹å¤„ç†åˆ›å»ºè¿‡ç¨‹ä¸­çš„å¤§å¤šæ•°é—®é¢˜æ˜¯ç”±äºå°†ç¤ºä¾‹æ•´ç†åˆ°å•ä¸ªæ‰¹å¤„ç†ä¸­è€Œå‡ºç°çš„ï¼Œå› æ­¤åœ¨æœ‰ç–‘é—®æ—¶é¦–å…ˆè¦æ£€æŸ¥çš„æ˜¯æ‚¨çš„ DataLoader æ­£åœ¨ä½¿ç”¨ä»€ä¹ˆ collate_fnï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python out
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

æ‰€ä»¥ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯ `default_data_collator`ï¼Œä½†è¿™ä¸æ˜¯æˆ‘ä»¬åœ¨è¿™ç§æƒ…å†µä¸‹æƒ³è¦çš„ã€‚ æˆ‘ä»¬å¸Œæœ›å°†ç¤ºä¾‹å¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€é•¿çš„å¥å­ï¼Œè¿™æ˜¯ç”± `DataCollatorWithPadding` æ•´ç†å™¨å®Œæˆçš„ã€‚ è€Œè¿™ä¸ªæ•°æ®æ”¶é›†å™¨åº”è¯¥æ˜¯é»˜è®¤è¢« `Trainer` ä½¿ç”¨çš„ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œæ²¡æœ‰ä½¿ç”¨å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰å°† `tokenizer` ä¼ é€’ç»™ `Trainer`ï¼Œæ‰€ä»¥å®ƒæ— æ³•åˆ›å»ºæˆ‘ä»¬æƒ³è¦çš„ `DataCollatorWithPadding`ã€‚ åœ¨å®è·µä¸­ï¼Œæ‚¨åº”è¯¥æ˜ç¡®åœ°ä¼ é€’æ‚¨æƒ³è¦ä½¿ç”¨çš„æ•°æ®æ•´ç†å™¨ï¼Œä»¥ç¡®ä¿é¿å…è¿™äº›ç±»å‹çš„é”™è¯¯ã€‚ è®©æˆ‘ä»¬è°ƒæ•´æˆ‘ä»¬çš„ä»£ç æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼š

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

å¥½æ¶ˆæ¯ï¼Ÿ æˆ‘ä»¬æ²¡æœ‰å¾—åˆ°ä¸ä»¥å‰ç›¸åŒçš„é”™è¯¯ï¼Œè¿™ç»å¯¹æ˜¯è¿›æ­¥ã€‚ åæ¶ˆæ¯ï¼Ÿ æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªè‡­åæ˜­è‘—çš„ CUDA é”™è¯¯ï¼š

```python out
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```

è¿™å¾ˆç³Ÿç³•ï¼Œå› ä¸º CUDA é”™è¯¯é€šå¸¸å¾ˆéš¾è°ƒè¯•ã€‚ æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬å®Œæˆå¯¹æ‰¹å¤„ç†åˆ›å»ºçš„åˆ†æã€‚

å¦‚æœæ‚¨ç¡®å®šæ‚¨çš„æ•°æ®æ•´ç†å™¨æ˜¯æ­£ç¡®çš„ï¼Œåˆ™åº”å°è¯•å°†å…¶åº”ç”¨äºæ•°æ®é›†çš„å‡ ä¸ªæ ·æœ¬ï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```

æ­¤ä»£ç å°†å¤±è´¥ï¼Œå› ä¸º `train_dataset` åŒ…å«å­—ç¬¦ä¸²åˆ—ï¼Œ`Trainer` é€šå¸¸ä¼šåˆ é™¤è¿™äº›åˆ—ã€‚ æ‚¨å¯ä»¥æ‰‹åŠ¨åˆ é™¤å®ƒä»¬ï¼Œæˆ–è€…å¦‚æœæ‚¨æƒ³å‡†ç¡®åœ°ä¿®æ”¹ `Trainer` åœ¨å¹•åæ‰€åšçš„äº‹æƒ…ï¼Œæ‚¨å¯ä»¥è°ƒç”¨ç§æœ‰çš„ `Trainer._remove_unused_columns()` æ–¹æ³•æ¥æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```

å¦‚æœé”™è¯¯ä»ç„¶å­˜åœ¨ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿæ‰‹åŠ¨è°ƒè¯•æ•°æ®æ•´ç†å™¨å†…éƒ¨ä»¥ç¡®å®šå…·ä½“çš„é—®é¢˜ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»è°ƒè¯•äº†æ‰¹å¤„ç†åˆ›å»ºè¿‡ç¨‹ï¼Œæ˜¯æ—¶å€™å°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹äº†ï¼

### æ£€æŸ¥æ¨¡å‹

æ‚¨åº”è¯¥èƒ½å¤Ÿé€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥è·å¾—ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼š

```py
for batch in trainer.get_train_dataloader():
    break
```

å¦‚æœæ‚¨åœ¨notebookä¸­è¿è¡Œæ­¤ä»£ç ï¼Œæ‚¨å¯èƒ½ä¼šæ”¶åˆ°ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ç±»ä¼¼çš„ CUDA é”™è¯¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦é‡æ–°å¯åŠ¨notebookå¹¶é‡æ–°æ‰§è¡Œæœ€åä¸€ä¸ªç‰‡æ®µï¼Œè€Œä¸è¿è¡Œ `trainer.train()` è¡Œ.è¿™æ˜¯å…³äº CUDA é”™è¯¯çš„ç¬¬äºŒä¸ªæœ€çƒ¦äººçš„äº‹æƒ…ï¼šå®ƒä»¬ä¼šç ´åæ‚¨çš„Cudaå†…æ ¸ï¼Œè€Œä¸”æ— æ³•æ¢å¤ã€‚å®ƒä»¬æœ€çƒ¦äººçš„äº‹æƒ…æ˜¯å®ƒä»¬å¾ˆéš¾è°ƒè¯•ã€‚

è¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿå®ƒä¸ GPU çš„å·¥ä½œæ–¹å¼æœ‰å…³ã€‚å®ƒä»¬åœ¨å¹¶è¡Œæ‰§è¡Œå¤§é‡æ“ä½œæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†ç¼ºç‚¹æ˜¯å½“å…¶ä¸­ä¸€æ¡æŒ‡ä»¤å¯¼è‡´é”™è¯¯æ—¶ï¼Œæ‚¨ä¸ä¼šç«‹å³çŸ¥é“ã€‚åªæœ‰å½“ç¨‹åºåœ¨ GPU ä¸Šè°ƒç”¨å¤šä¸ªè¿›ç¨‹çš„åŒæ­¥å¤„ç†æ—¶ï¼Œå®ƒæ‰ä¼šæ„è¯†åˆ°å‡ºç°é—®é¢˜ï¼Œå› æ­¤é”™è¯¯å®é™…ä¸Šæ˜¯åœ¨ä¸åˆ›å»ºå®ƒçš„åŸå› æ— å…³çš„åœ°æ–¹å¼•å‘çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä¹‹å‰çš„trackbackï¼Œé”™è¯¯æ˜¯åœ¨å‘åä¼ é€’æœŸé—´å¼•å‘çš„ï¼Œä½†æˆ‘ä»¬ä¼šåœ¨ä¸€åˆ†é’Ÿå†…çœ‹åˆ°å®ƒå®é™…ä¸Šæºäºå‘å‰ä¼ é€’ä¸­çš„æŸäº›ä¸œè¥¿ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è°ƒè¯•è¿™äº›é”™è¯¯å‘¢ï¼Ÿç­”æ¡ˆå¾ˆç®€å•ï¼šæˆ‘ä»¬æ²¡æœ‰ã€‚é™¤éæ‚¨çš„ CUDA é”™è¯¯æ˜¯å†…å­˜ä¸è¶³é”™è¯¯ï¼ˆè¿™æ„å‘³ç€æ‚¨çš„ GPU ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼‰ï¼Œå¦åˆ™æ‚¨åº”è¯¥å§‹ç»ˆè¿”å› CPU è¿›è¡Œè°ƒè¯•ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€å°†æ¨¡å‹æ”¾å› CPU ä¸Šå¹¶åœ¨æˆ‘ä»¬çš„ä¸€æ‰¹æ•°æ®ä¸­è°ƒç”¨å®ƒâ€”â€”â€œDataLoaderâ€è¿”å›çš„é‚£æ‰¹æ•°æ®å°šæœªç§»åŠ¨åˆ° GPUï¼š

```python
outputs = trainer.model.cpu()(**batch)
```

```python out
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```

æ‰€ä»¥ï¼Œæ€è·¯è¶Šæ¥è¶Šæ¸…æ™°äº†ã€‚ æˆ‘ä»¬ç°åœ¨åœ¨æŸå¤±è®¡ç®—ä¸­æ²¡æœ‰å‡ºç° CUDA é”™è¯¯ï¼Œè€Œæ˜¯æœ‰ä¸€ä¸ªâ€œIndexErrorâ€ï¼ˆå› æ­¤ä¸æˆ‘ä»¬ä¹‹å‰æ‰€è¯´çš„åå‘ä¼ æ’­æ— å…³ï¼‰ã€‚ æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ˜¯Target 2 é€ æˆäº†é”™è¯¯ï¼Œæ‰€ä»¥è¿™æ˜¯æ£€æŸ¥æ¨¡å‹æ ‡ç­¾æ•°é‡çš„å¥½æ—¶æœºï¼š

```python
trainer.model.config.num_labels
```

```python out
2
```

æœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼Œåªå…è®¸ 0 å’Œ 1 ä½œä¸ºç›®æ ‡ï¼Œä½†æ˜¯æ ¹æ®é”™è¯¯ä¿¡æ¯æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª 2ã€‚å¾—åˆ°ä¸€ä¸ª 2 å®é™…ä¸Šæ˜¯æ­£å¸¸çš„ï¼šå¦‚æœæˆ‘ä»¬è®°å¾—æˆ‘ä»¬ä¹‹å‰æå–çš„æ ‡ç­¾åç§°ï¼Œæœ‰ä¸‰ä¸ªï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰ç´¢å¼• 0 , 1 å’Œ 2 åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ã€‚ é—®é¢˜æ˜¯æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå®ƒåº”è¯¥åˆ›å»ºä¸‰ä¸ªæ ‡ç­¾ã€‚ æ‰€ä»¥è®©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

æˆ‘ä»¬è¿˜æ²¡æœ‰åŒ…å« `trainer.train()` è¡Œï¼Œä»¥ä¾¿èŠ±æ—¶é—´æ£€æŸ¥ä¸€åˆ‡æ˜¯å¦æ­£å¸¸ã€‚ å¦‚æœæˆ‘ä»¬è¯·æ±‚ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®å¹¶å°†å…¶ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå®ƒç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

ä¸‹ä¸€æ­¥æ˜¯å›åˆ° GPU å¹¶æ£€æŸ¥ä¸€åˆ‡æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼š

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

å¦‚æœä»ç„¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¡®ä¿é‡æ–°å¯åŠ¨notebookå¹¶ä»…æ‰§è¡Œè„šæœ¬çš„æœ€åä¸€ä¸ªç‰ˆæœ¬ã€‚

### æ‰§è¡Œä¸€ä¸ªä¼˜åŒ–å™¨æ­¥éª¤

ç°åœ¨æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬å¯ä»¥æ„å»ºå®é™…é€šè¿‡æ¨¡å‹æ£€æŸ¥çš„æˆæ‰¹æ¬¡çš„æ•°æ®ï¼Œæˆ‘ä»¬å·²ç»ä¸ºè®­ç»ƒç®¡é“çš„ä¸‹ä¸€æ­¥åšå¥½å‡†å¤‡ï¼šè®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ã€‚

ç¬¬ä¸€éƒ¨åˆ†åªæ˜¯åœ¨ loss ä¸Šè°ƒç”¨ `backward()` æ–¹æ³•ï¼š

```py
loss = outputs.loss
loss.backward()
```

åœ¨è¿™ä¸ªé˜¶æ®µå¾ˆå°‘å‡ºç°é”™è¯¯ï¼Œä½†å¦‚æœç¡®å®å‡ºç°é”™è¯¯ï¼Œè¯·è¿”å› CPU ä»¥è·å–æœ‰ç”¨çš„é”™è¯¯æ¶ˆæ¯ã€‚

è¦æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œæˆ‘ä»¬åªéœ€è¦åˆ›å»º `optimizer` å¹¶è°ƒç”¨å®ƒçš„ `step()` æ–¹æ³•ï¼š

```py
trainer.create_optimizer()
trainer.optimizer.step()
```

åŒæ ·ï¼Œå¦‚æœæ‚¨åœ¨ `Trainer` ä¸­ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨ï¼Œåˆ™åœ¨æ­¤é˜¶æ®µæ‚¨ä¸åº”è¯¥æ”¶åˆ°é”™è¯¯ï¼Œä½†å¦‚æœæ‚¨æœ‰è‡ªå®šä¹‰ä¼˜åŒ–å™¨ï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°ä¸€äº›é—®é¢˜éœ€è¦åœ¨è¿™é‡Œè°ƒè¯•ã€‚ å¦‚æœæ‚¨åœ¨æ­¤é˜¶æ®µé‡åˆ°å¥‡æ€ªçš„ CUDA é”™è¯¯ï¼Œè¯·ä¸è¦å¿˜è®°è¿”å› CPUã€‚ è¯´åˆ° CUDA é”™è¯¯ï¼Œå‰é¢æˆ‘ä»¬æåˆ°äº†ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚ ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚

### å¤„ç† CUDA out-of-memoryé”™è¯¯

æ¯å½“æ‚¨æ”¶åˆ°ä»¥`RuntimeError: CUDA out of memory`å¼€å¤´çš„é”™è¯¯æ¶ˆæ¯æ—¶ï¼Œè¿™è¡¨æ˜æ‚¨çš„ GPU å†…å­˜ä¸è¶³ã€‚ è¿™ä¸æ‚¨çš„ä»£ç æ²¡æœ‰ç›´æ¥å…³è”ï¼Œå¹¶ä¸”å®ƒå¯èƒ½å‘ç”Ÿåœ¨è¿è¡Œè‰¯å¥½çš„ä»£ç ä¸­ã€‚ æ­¤é”™è¯¯æ„å‘³ç€æ‚¨è¯•å›¾åœ¨ GPU çš„å†…éƒ¨å­˜å‚¨å™¨ä¸­æ”¾å…¥å¤ªå¤šä¸œè¥¿ï¼Œè¿™å¯¼è‡´äº†é”™è¯¯ã€‚ ä¸å…¶ä»– CUDA é”™è¯¯ä¸€æ ·ï¼Œæ‚¨éœ€è¦é‡æ–°å¯åŠ¨å†…æ ¸æ‰èƒ½å†æ¬¡è¿è¡Œè®­ç»ƒã€‚

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨åªéœ€è¦ä½¿ç”¨æ›´å°‘çš„ GPU ç©ºé—´â€”â€”è¿™å¾€å¾€è¯´èµ·æ¥å®¹æ˜“åšèµ·æ¥éš¾ã€‚ é¦–å…ˆï¼Œç¡®ä¿æ‚¨æ²¡æœ‰åŒæ—¶åœ¨ GPU ä¸Šè¿è¡Œä¸¤ä¸ªæ¨¡å‹ï¼ˆå½“ç„¶ï¼Œé™¤éæ‚¨çš„é—®é¢˜éœ€è¦è¿™æ ·åšï¼‰ã€‚ ç„¶åï¼Œæ‚¨å¯èƒ½åº”è¯¥å‡å°‘batchçš„å¤§å°ï¼Œå› ä¸ºå®ƒç›´æ¥å½±å“æ¨¡å‹çš„æ‰€æœ‰ä¸­é—´è¾“å‡ºçš„å¤§å°åŠå…¶æ¢¯åº¦ã€‚ å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·è€ƒè™‘ä½¿ç”¨è¾ƒå°ç‰ˆæœ¬çš„æ¨¡å‹ã€‚

<Tip>

åœ¨è¯¾ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æ›´å…ˆè¿›çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å¸®åŠ©æ‚¨å‡å°‘å†…å­˜å ç”¨å¹¶è®©æ‚¨å¾®è°ƒæœ€å¤§çš„æ¨¡å‹ã€‚

</Tip>

### è¯„ä¼°æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å·²ç»è§£å†³äº†ä»£ç çš„æ‰€æœ‰é—®é¢˜ï¼Œä¸€åˆ‡éƒ½å¾ˆå®Œç¾ï¼Œè®­ç»ƒåº”è¯¥å¯ä»¥é¡ºåˆ©è¿›è¡Œï¼Œå¯¹å§ï¼Ÿ æ²¡é‚£ä¹ˆå¿«ï¼ å¦‚æœä½ è¿è¡Œ `trainer.train()` å‘½ä»¤ï¼Œä¸€å¼€å§‹ä¸€åˆ‡çœ‹èµ·æ¥éƒ½ä¸é”™ï¼Œä½†è¿‡ä¸€ä¼šå„¿ä½ ä¼šå¾—åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

```py
# This will take a long time and error out, so you shouldn't run this cell
trainer.train()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

æ‚¨å°†æ„è¯†åˆ°æ­¤é”™è¯¯å‡ºç°åœ¨è¯„ä¼°é˜¶æ®µï¼Œå› æ­¤è¿™æ˜¯æˆ‘ä»¬éœ€è¦è°ƒè¯•çš„æœ€åä¸€ä»¶äº‹ã€‚

æ‚¨å¯ä»¥åƒè¿™æ ·åœ¨è®­ç»ƒä¸­ç‹¬ç«‹è¿è¡Œ`Trainer`çš„è¯„ä¼°å¾ªç¯ï¼š

```py
trainer.evaluate()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

<Tip>

ğŸ’¡ æ‚¨åº”è¯¥å§‹ç»ˆç¡®ä¿åœ¨å¯åŠ¨ `trainer.train()` ä¹‹å‰ `trainer.evaluate()`æ˜¯å¯ä»¥è¿è¡Œçš„ï¼Œä»¥é¿å…åœ¨é‡åˆ°é”™è¯¯ä¹‹å‰æµªè´¹å¤§é‡è®¡ç®—èµ„æºã€‚

</Tip>

åœ¨å°è¯•è°ƒè¯•è¯„ä¼°å¾ªç¯ä¸­çš„é—®é¢˜ä¹‹å‰ï¼Œæ‚¨åº”è¯¥é¦–å…ˆç¡®ä¿æ‚¨å·²ç»æŸ¥çœ‹äº†æ•°æ®ï¼Œèƒ½å¤Ÿæ­£ç¡®åœ°å½¢æˆæ‰¹å¤„ç†ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å…¶ä¸Šè¿è¡Œæ‚¨çš„æ¨¡å‹ã€‚ æˆ‘ä»¬å·²ç»å®Œæˆäº†æ‰€æœ‰è¿™äº›æ­¥éª¤ï¼Œå› æ­¤å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç è€Œä¸ä¼šå‡ºé”™ï¼š

```py
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

ç¨ç­‰ä¸€ä¼šå„¿ï¼Œé”™è¯¯å‡ºç°ï¼Œåœ¨è¯„ä¼°é˜¶æ®µç»“æŸæ—¶ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹trackbackï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼š

```python trace
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

è¿™å‘Šè¯‰æˆ‘ä»¬é”™è¯¯æºè‡ª `datasets/metric.py` æ¨¡å—â€”â€”æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çš„ `compute_metrics()` å‡½æ•°çš„é—®é¢˜ã€‚ å®ƒéœ€è¦ä¸€ä¸ªå¸¦æœ‰ logits å’Œæ ‡ç­¾çš„å…ƒç»„ä½œä¸º NumPy æ•°ç»„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯•è¾“å…¥å®ƒï¼š

```py
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

æˆ‘ä»¬å¾—åˆ°åŒæ ·çš„é”™è¯¯ï¼Œæ‰€ä»¥é—®é¢˜è‚¯å®šå‡ºåœ¨é‚£ä¸ªå‡½æ•°ä¸Šã€‚ å¦‚æœæˆ‘ä»¬å›é¡¾å®ƒçš„ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒåªæ˜¯å°†â€œé¢„æµ‹â€å’Œâ€œçœŸå®çš„æ ‡ç­¾â€è½¬å‘åˆ°â€œmetric.compute()â€ã€‚ é‚£ä¹ˆè¿™ç§æ–¹æ³•æœ‰é—®é¢˜å—ï¼Ÿ å¹¶ä¸çœŸåœ°ã€‚ è®©æˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸€ä¸‹å½¢çŠ¶ï¼š

```py
predictions.shape, labels.shape
```

```python out
((8, 3), (8,))
```

æˆ‘ä»¬çš„é¢„æµ‹ä»ç„¶æ˜¯ logitsï¼Œè€Œä¸æ˜¯å®é™…çš„é¢„æµ‹ï¼Œè¿™å°±æ˜¯metricsè¿”å›è¿™ä¸ªï¼ˆæœ‰ç‚¹æ¨¡ç³Šï¼‰é”™è¯¯çš„åŸå› ã€‚ ä¿®å¤å¾ˆç®€å•ï¼› æˆ‘ä»¬åªéœ€è¦åœ¨ `compute_metrics()` å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ª argmaxï¼š

```py
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))
```

```python out
{'accuracy': 0.625}
```

ç°åœ¨æˆ‘ä»¬çš„é”™è¯¯å·²ä¿®å¤ï¼ è¿™æ˜¯æœ€åä¸€ä¸ªï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è„šæœ¬ç°åœ¨å°†æ­£ç¡®è®­ç»ƒæ¨¡å‹ã€‚

ä½œä¸ºå‚è€ƒï¼Œè¿™é‡Œæ˜¯å®Œå…¨ä¿®æ­£å¥½çš„è„šæœ¬ï¼š

```py
import numpy as np
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æ›´å¤šé”™è¯¯ï¼Œæˆ‘ä»¬çš„è„šæœ¬å°†å¾®è°ƒä¸€ä¸ªåº”è¯¥ç»™å‡ºåˆç†ç»“æœçš„æ¨¡å‹ã€‚ ä½†æ˜¯ï¼Œå¦‚æœè®­ç»ƒæ²¡æœ‰ä»»ä½•é”™è¯¯ï¼Œè€Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ ¹æœ¬è¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿ è¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºä¸€äº›å¯ä»¥æä¾›å¸®åŠ©çš„æŠ€æœ¯ã€‚

<Tip>

ğŸ’¡ å¦‚æœæ‚¨ä½¿ç”¨æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œåˆ™ç›¸åŒçš„æ­¥éª¤ä¹Ÿé€‚ç”¨äºè°ƒè¯•è®­ç»ƒç®¡é“ï¼Œè€Œä¸”æ›´å®¹æ˜“å°†å®ƒä»¬åˆ†å¼€ã€‚ ä½†æ˜¯ï¼Œè¯·ç¡®ä¿æ‚¨æ²¡æœ‰å¿˜è®°æ­£ç¡®ä½ç½®çš„ `model.eval()` æˆ– `model.train()`ï¼Œæˆ–è€…æ¯ä¸ªæ­¥éª¤ä¸­çš„ `zero_grad()`ï¼

</Tip>

## åœ¨è®­ç»ƒæœŸé—´è°ƒè¯•é™é»˜ï¼ˆæ²¡æœ‰ä»»ä½•é”™è¯¯æç¤ºï¼‰é”™è¯¯

æˆ‘ä»¬å¯ä»¥åšäº›ä»€ä¹ˆæ¥è°ƒè¯•ä¸€ä¸ªæ²¡æœ‰é”™è¯¯åœ°å®Œæˆä½†æ²¡æœ‰å¾—åˆ°å¥½çš„ç»“æœçš„è®­ç»ƒï¼Ÿ æˆ‘ä»¬ä¼šåœ¨è¿™é‡Œç»™ä½ ä¸€äº›æç¤ºï¼Œä½†è¯·æ³¨æ„ï¼Œè¿™ç§è°ƒè¯•æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œå¹¶ä¸”æ²¡æœ‰ç¥å¥‡çš„ç­”æ¡ˆã€‚

### æ£€æŸ¥æ‚¨çš„æ•°æ®ï¼ˆå†æ¬¡ï¼ï¼‰

åªæœ‰åœ¨ç†è®ºä¸Šå¯ä»¥ä»æ‚¨çš„æ•°æ®ä¸­å­¦åˆ°ä»»ä½•ä¸œè¥¿æ—¶ï¼Œæ‚¨çš„æ¨¡å‹æ‰ä¼šå­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚ å¦‚æœå­˜åœ¨æŸåæ•°æ®çš„é”™è¯¯æˆ–æ ‡ç­¾æ˜¯éšæœºå±æ€§çš„ï¼Œé‚£ä¹ˆæ‚¨å¾ˆå¯èƒ½ä¸ä¼šåœ¨æ•°æ®é›†ä¸Šè·å¾—ä»»ä½•çŸ¥è¯†ã€‚ å› æ­¤ï¼Œå§‹ç»ˆé¦–å…ˆä»”ç»†æ£€æŸ¥æ‚¨çš„è§£ç è¾“å…¥å’Œæ ‡ç­¾ï¼Œç„¶åé—®è‡ªå·±ä»¥ä¸‹é—®é¢˜ï¼š

- è§£ç åçš„æ•°æ®æ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿ
- ä½ è®¤åŒè¿™äº›æ ‡ç­¾å—ï¼Ÿ
- æœ‰æ²¡æœ‰ä¸€ä¸ªæ ‡ç­¾æ¯”å…¶ä»–æ ‡ç­¾æ›´å¸¸è§ï¼Ÿ
- å¦‚æœæ¨¡å‹é¢„æµ‹éšæœºçš„ç­”æ¡ˆ/æ€»æ˜¯ç›¸åŒçš„ç­”æ¡ˆï¼Œé‚£ä¹ˆloss/è¯„ä¼°æŒ‡æ ‡åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ

<Tip warning={true}>

âš ï¸ å¦‚æœæ‚¨æ­£åœ¨è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·åœ¨æ¯ä¸ªè¿‡ç¨‹ä¸­æ‰“å°æ•°æ®é›†çš„æ ·æœ¬ï¼Œå¹¶ä¸‰æ¬¡æ£€æŸ¥æ‚¨æ˜¯å¦å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚ ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯åœ¨æ•°æ®åˆ›å»ºä¸­æœ‰ä¸€äº›éšæœºæ€§æ¥æºï¼Œè¿™ä½¿å¾—æ¯ä¸ªè¿›ç¨‹éƒ½æœ‰ä¸åŒç‰ˆæœ¬çš„æ•°æ®é›†ã€‚

</Tip>

æŸ¥çœ‹æ‚¨çš„æ•°æ®åï¼ŒæŸ¥çœ‹æ¨¡å‹çš„ä¸€äº›é¢„æµ‹å¹¶å¯¹å…¶è¿›è¡Œè§£ç ã€‚ å¦‚æœæ¨¡å‹æ€»æ˜¯é¢„æµ‹åŒæ ·çš„äº‹æƒ…ï¼Œé‚£å¯èƒ½æ˜¯å› ä¸ºä½ çš„æ•°æ®é›†åå‘ä¸€ä¸ªç±»åˆ«ï¼ˆé’ˆå¯¹åˆ†ç±»é—®é¢˜ï¼‰ï¼› è¿‡é‡‡æ ·ç¨€æœ‰ç±»ç­‰æŠ€æœ¯å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚

å¦‚æœæ‚¨åœ¨åˆå§‹æ¨¡å‹ä¸Šè·å¾—çš„loss/è¯„ä¼°æŒ‡æ ‡ä¸æ‚¨æœŸæœ›çš„éšæœºé¢„æµ‹çš„loss/è¯„ä¼°æŒ‡æ ‡éå¸¸ä¸åŒï¼Œè¯·ä»”ç»†æ£€æŸ¥æ‚¨çš„lossæˆ–è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹å¼ï¼Œå› ä¸ºé‚£é‡Œå¯èƒ½å­˜åœ¨é”™è¯¯ã€‚ å¦‚æœæ‚¨ä½¿ç”¨æœ€åæ·»åŠ çš„å¤šä¸ªlossï¼Œè¯·ç¡®ä¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„è§„æ¨¡ã€‚

å½“æ‚¨ç¡®å®šæ‚¨çš„æ•°æ®æ˜¯å®Œç¾çš„æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¥æŸ¥çœ‹æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

### åœ¨ä¸€æ‰¹ä¸Šè¿‡åº¦æ‹Ÿåˆä½ çš„æ¨¡å‹

è¿‡åº¦æ‹Ÿåˆé€šå¸¸æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒæ—¶å°½é‡é¿å…çš„äº‹æƒ…ï¼Œå› ä¸ºè¿™æ„å‘³ç€æ¨¡å‹æ²¡æœ‰å­¦ä¹ è¯†åˆ«æˆ‘ä»¬æƒ³è¦çš„ä¸€èˆ¬ç‰¹å¾ï¼Œè€Œåªæ˜¯è®°ä½äº†è®­ç»ƒæ ·æœ¬ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€éåˆä¸€éåœ°å°è¯•åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸Šè®­ç»ƒæ‚¨çš„æ¨¡å‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æµ‹è¯•ï¼Œå¯ä»¥æ£€æŸ¥æ‚¨çš„é—®é¢˜æ˜¯å¦å¯ä»¥é€šè¿‡æ‚¨å°è¯•è®­ç»ƒçš„æ¨¡å‹æ¥è§£å†³ã€‚ å®ƒè¿˜å°†å¸®åŠ©æ‚¨æŸ¥çœ‹æ‚¨çš„åˆå§‹å­¦ä¹ ç‡æ˜¯å¦å¤ªé«˜ã€‚

ä¸€æ—¦ä½ å®šä¹‰äº†ä½ çš„ `Trainer` ä¹‹åï¼Œè¿™æ ·åšçœŸçš„å¾ˆå®¹æ˜“ï¼› åªéœ€è·å–ä¸€æ‰¹è®­ç»ƒæ•°æ®ï¼Œç„¶åä»…ä½¿ç”¨è¯¥æ‰¹æ¬¡è¿è¡Œä¸€ä¸ªå°å‹æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œå¤§çº¦ 20 æ­¥ï¼š

```py
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

<Tip>

ğŸ’¡ å¦‚æœæ‚¨çš„è®­ç»ƒæ•°æ®ä¸å¹³è¡¡ï¼Œè¯·ç¡®ä¿æ„å»ºä¸€æ‰¹åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ã€‚

</Tip>

ç”Ÿæˆçš„æ¨¡å‹åœ¨ä¸€ä¸ªâ€œæ‰¹æ¬¡â€ä¸Šåº”è¯¥æœ‰æ¥è¿‘å®Œç¾çš„ç»“æœã€‚ è®©æˆ‘ä»¬è®¡ç®—ç»“æœé¢„æµ‹çš„æŒ‡æ ‡ï¼š

```py
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python out
{'accuracy': 1.0}
```

100% å‡†ç¡®ç‡ï¼Œç°åœ¨è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¿‡æ‹Ÿåˆç¤ºä¾‹ï¼ˆè¿™æ„å‘³ç€å¦‚æœä½ åœ¨ä»»ä½•å…¶ä»–å¥å­ä¸Šå°è¯•ä½ çš„æ¨¡å‹ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šç»™ä½ ä¸€ä¸ªé”™è¯¯çš„ç­”æ¡ˆï¼‰ï¼

å¦‚æœä½ æ²¡æœ‰è®¾æ³•è®©ä½ çš„æ¨¡å‹è·å¾—è¿™æ ·çš„å®Œç¾ç»“æœï¼Œè¿™æ„å‘³ç€ä½ æ„å»ºé—®é¢˜æˆ–æ•°æ®çš„æ–¹å¼æœ‰é—®é¢˜ï¼Œæ‰€ä»¥ä½ åº”è¯¥ä¿®å¤å®ƒã€‚ åªæœ‰å½“ä½ å¯ä»¥é€šè¿‡è¿‡æ‹Ÿåˆæµ‹è¯•æ—¶ï¼Œä½ æ‰èƒ½ç¡®å®šä½ çš„æ¨¡å‹å®é™…ä¸Šå¯ä»¥å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚

<Tip warning={true}>

âš ï¸ åœ¨æ­¤æµ‹è¯•ä¹‹åï¼Œæ‚¨å°†ä¸å¾—ä¸é‡æ–°åˆ›å»ºæ‚¨çš„æ¨¡å‹å’Œâ€œTrainerâ€ï¼Œå› ä¸ºè·å¾—çš„æ¨¡å‹å¯èƒ½æ— æ³•åœ¨æ‚¨çš„å®Œæ•´æ•°æ®é›†ä¸Šæ¢å¤å’Œå­¦ä¹ æœ‰ç”¨çš„ä¸œè¥¿ã€‚

</Tip>

### åœ¨ä½ æœ‰ç¬¬ä¸€ä¸ªåŸºçº¿ä¹‹å‰ä¸è¦è°ƒæ•´ä»»ä½•ä¸œè¥¿

è¶…å‚æ•°è°ƒä¼˜æ€»æ˜¯è¢«å¼ºè°ƒä¸ºæœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œä½†è¿™åªæ˜¯å¸®åŠ©æ‚¨åœ¨æŒ‡æ ‡ä¸Šæœ‰æ‰€æ”¶è·çš„æœ€åä¸€æ­¥ã€‚ å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ`Trainer` çš„é»˜è®¤è¶…å‚æ•°å¯ä»¥å¾ˆå¥½åœ°ä¸ºæ‚¨æä¾›è‰¯å¥½çš„ç»“æœï¼Œå› æ­¤åœ¨æ‚¨è·å¾—è¶…å‡ºæ•°æ®é›†åŸºçº¿çš„ä¸œè¥¿ä¹‹å‰ï¼Œä¸è¦å¼€å§‹è¿›è¡Œè€—æ—¶ä¸”æ˜‚è´µçš„è¶…å‚æ•°æœç´¢ .

ä¸€æ—¦ä½ æœ‰ä¸€ä¸ªè¶³å¤Ÿå¥½çš„æ¨¡å‹ï¼Œä½ å°±å¯ä»¥å¼€å§‹ç¨å¾®è°ƒæ•´ä¸€ä¸‹ã€‚ ä¸è¦å°è¯•ä½¿ç”¨ä¸åŒçš„è¶…å‚æ•°å¯åŠ¨ä¸€åƒæ¬¡è¿è¡Œï¼Œè€Œæ˜¯æ¯”è¾ƒä¸€ä¸ªè¶…å‚æ•°çš„ä¸åŒå€¼çš„å‡ æ¬¡è¿è¡Œï¼Œä»¥äº†è§£å“ªä¸ªå½±å“æœ€å¤§ã€‚

å¦‚æœæ‚¨æ­£åœ¨è°ƒæ•´æ¨¡å‹æœ¬èº«ï¼Œä¸è¦å°è¯•ä»»ä½•æ‚¨æ— æ³•åˆç†è¯æ˜çš„äº‹æƒ…ã€‚ å§‹ç»ˆç¡®ä¿æ‚¨è¿”å›è¿‡æ‹Ÿåˆæµ‹è¯•ä»¥éªŒè¯æ‚¨çš„æ›´æ”¹æ²¡æœ‰äº§ç”Ÿä»»ä½•æ„å¤–åæœã€‚

### è¯·æ±‚å¸®å¿™

å¸Œæœ›æ‚¨ä¼šåœ¨æœ¬èŠ‚ä¸­æ‰¾åˆ°ä¸€äº›å¯ä»¥å¸®åŠ©æ‚¨è§£å†³é—®é¢˜çš„å»ºè®®ï¼Œä½†å¦‚æœä¸æ˜¯è¿™æ ·ï¼Œè¯·è®°ä½æ‚¨å¯ä»¥éšæ—¶åœ¨ [è®ºå›](https://discuss.huggingface.co/) ä¸Šå‘ç¤¾åŒºæé—®ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¢å¤–èµ„æºï¼š

- [â€œä½œä¸ºå·¥ç¨‹æœ€ä½³å®è·µå·¥å…·çš„å†ç°æ€§â€](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p)ï¼Œä½œè€…ï¼šJoel Grus
- [â€œç¥ç»ç½‘ç»œè°ƒè¯•æ¸…å•â€](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) ä½œè€…ï¼šCecelia Shao
- [â€œå¦‚ä½•å¯¹æœºå™¨å­¦ä¹ ä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•â€](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) by Chase Roberts
- [â€œè®­ç»ƒç¥ç»ç½‘ç»œçš„ç§˜è¯€â€](http://karpathy.github.io/2019/04/25/recipe/)ä½œè€…ï¼šAndrej Karpathy

å½“ç„¶ï¼Œå¹¶ä¸æ˜¯ä½ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶é‡åˆ°çš„æ¯ä¸€ä¸ªé—®é¢˜éƒ½æ˜¯ä½ è‡ªå·±çš„é”™ï¼ å¦‚æœæ‚¨åœ¨ ğŸ¤— Transformers æˆ– ğŸ¤— Datasets åº“ä¸­é‡åˆ°çœ‹èµ·æ¥ä¸æ­£ç¡®çš„å†…å®¹ï¼Œæ‚¨å¯èƒ½é‡åˆ°äº†é”™è¯¯ã€‚ ä½ åº”è¯¥å‘Šè¯‰æˆ‘ä»¬è¿™ä¸€åˆ‡ï¼Œåœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‡†ç¡®è§£é‡Šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
