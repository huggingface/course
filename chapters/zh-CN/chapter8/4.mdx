<FrameworkSwitchCourse {fw} />

# è°ƒè¯•è®­ç»ƒç®¡é“ [[è°ƒè¯•è®­ç»ƒç®¡é“]]

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4.ipynb"},
]} />

å‡å¦‚ä½ å·²ç»å°½å¯èƒ½åœ°éµå¾ª [ç¬¬ä¸ƒç« ](/course/chapter7) ä¸­çš„å»ºè®®ï¼Œç¼–å†™äº†ä¸€æ®µæ¼‚äº®çš„ä»£ç æ¥è®­ç»ƒæˆ–å¾®è°ƒç»™å®šä»»åŠ¡çš„æ¨¡å‹ã€‚ä½†æ˜¯å½“ä½ å¯åŠ¨å‘½ä»¤ `trainer.train()` æ—¶ï¼Œå¯æ€•çš„äº‹æƒ…å‘ç”Ÿäº†ï¼šä½ å¾—åˆ°ä¸€ä¸ªé”™è¯¯ğŸ˜±ï¼æˆ–è€…æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè™½ç„¶çœ‹èµ·æ¥ä¸€åˆ‡ä¼¼ä¹éƒ½æ­£å¸¸ï¼Œè®­ç»ƒè¿è¡Œæ²¡æœ‰é”™è¯¯ï¼Œä½†ç”Ÿæˆçš„æ¨¡å‹å¾ˆç³Ÿç³•ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•è°ƒè¯•æ­¤ç±»é—®é¢˜ã€‚

## è°ƒè¯•è®­ç»ƒç®¡é“ [[è°ƒè¯•è®­ç»ƒç®¡é“]]

<Youtube id="L-WSwUWde1U"/>

å½“ä½ åœ¨ `trainer.train()` ä¸­é‡åˆ°é”™è¯¯æ—¶ï¼Œå®ƒæœ‰å¯èƒ½æ¥è‡ªå¤šä¸ªä¸åŒçš„æ¥æºï¼Œå› ä¸º `Trainer` ä¼šå°†å¾ˆå¤šæ¨¡å—æ”¾åœ¨ä¸€èµ·ç»„åˆè¿è¡Œã€‚é¦–å…ˆå®ƒä¼šå°† datasets è½¬æ¢ä¸º dataloaders å› æ­¤é—®é¢˜å¯èƒ½å‡ºåœ¨ datasets ä¸­ï¼Œæˆ–è€…åœ¨å°è¯•å°† datasets çš„å…ƒç´ ä¸€èµ·æ‰¹å¤„ç†æ—¶å‡ºç°é—®é¢˜ã€‚æ¥ç€å®ƒä¼šå‡†å¤‡ä¸€æ‰¹æ•°æ®å¹¶å°†å…¶æä¾›ç»™æ¨¡å‹ï¼Œå› æ­¤é—®é¢˜å¯èƒ½å‡ºåœ¨æ¨¡å‹ä»£ç ä¸­ã€‚ä¹‹åï¼Œå®ƒä¼šè®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œä¼˜åŒ–å™¨ï¼Œå› æ­¤é—®é¢˜ä¹Ÿå¯èƒ½å‡ºåœ¨ä½ çš„ä¼˜åŒ–å™¨ä¸­ã€‚å³ä½¿è®­ç»ƒä¸€åˆ‡é¡ºåˆ©ï¼Œå¦‚æœä½ çš„è¯„ä¼°æŒ‡æ ‡æœ‰é—®é¢˜ï¼Œè¯„ä¼°æœŸé—´ä»ç„¶å¯èƒ½å‡ºç°é—®é¢˜ã€‚

è°ƒè¯• `trainer.train()` ä¸­å‡ºç°çš„é”™è¯¯çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰‹åŠ¨æ£€æŸ¥æ•´ä¸ªç®¡é“ï¼Œçœ‹çœ‹å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œé”™è¯¯å¾ˆå®¹æ˜“è§£å†³ã€‚

ä¸ºäº†è®²è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨ä»¥ä¸‹è„šæœ¬åœ¨ [MNLI æ•°æ®é›†](https://huggingface.co/datasets/glue) ä¸Šå¾®è°ƒ DistilBERT æ¨¡å‹ï¼š

```py
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

å¦‚æœä½ å°è¯•è¿è¡Œå®ƒï¼Œä½ ä¼šé‡åˆ°ä¸€ä¸ªç›¸å½“æ™¦æ¶©çš„é”™è¯¯ï¼š

```python out
'ValueError: You have to specify either input_ids or inputs_embeds'
```

### æ£€æŸ¥æ•°æ® [[æ£€æŸ¥æ•°æ®]]

æ˜¾è€Œæ˜“è§ï¼Œå¦‚æœä½ çš„æ•°æ®æŸåäº†ï¼Œ `Trainer` å°†æ— æ³•å°†æ•°æ®æ•´ç†æˆ batchï¼Œæ›´ä¸ç”¨è¯´è®­ç»ƒä½ çš„æ¨¡å‹äº†ã€‚å› æ­¤ï¼Œé¦–å…ˆéœ€è¦æ£€æŸ¥ä¸€ä¸‹ä½ çš„è®­ç»ƒé›†é‡Œé¢çš„å†…å®¹ã€‚

ä¸ºäº†é¿å…èŠ±è´¹æ— æ•°å°æ—¶è¯•å›¾ä¿®å¤ä¸æ˜¯é”™è¯¯æ¥æºçš„é—®é¢˜ï¼Œé¿å…å¤šæ¬¡å°è£…çš„å¹²æ‰°ï¼Œæˆ‘ä»¬å»ºè®®ä½ åªä½¿ç”¨ `trainer.train_dataset` è¿›è¡Œæ£€æŸ¥ã€‚æ‰€ä»¥è®©æˆ‘ä»¬åœ¨è¿™é‡Œè¿™æ ·å°è¯•ä¸€ä¸‹ï¼š

```py
trainer.train_dataset[0]
```

```python out
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```

ä½ æ³¨æ„åˆ°æœ‰ä»€ä¹ˆä¸å¯¹å—ï¼Ÿä¸ç¼ºå°‘ `input_ids` çš„é”™è¯¯æ¶ˆæ¯ç›¸ç»“åˆï¼Œä½ åº”è¯¥å¯ä»¥æ„è¯†åˆ°æ•°æ®é›†é‡Œæ˜¯æ–‡æœ¬ï¼Œè€Œä¸æ˜¯æ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè¾“å‡ºçš„åŸå§‹é”™è¯¯ä¿¡æ¯éå¸¸å…·æœ‰è¯¯å¯¼æ€§ï¼Œå› ä¸º `Trainer` ä¼šè‡ªåŠ¨åˆ é™¤ä¸æ¨¡å‹é…ç½®ä¸­ä¸éœ€è¦çš„åˆ— ï¼ˆå³æ¨¡å‹é¢„æœŸçš„è¾“å…¥å‚æ•°ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è¿™é‡Œï¼Œé™¤äº†æ ‡ç­¾ä¹‹å¤–çš„æ‰€æœ‰ä¸œè¥¿éƒ½è¢«ä¸¢å¼ƒäº†ã€‚å› æ­¤ï¼Œåˆ›å»º batch ç„¶åå°†å®ƒä»¬å‘é€åˆ°æ¨¡å‹æ—¶æ²¡æœ‰é—®é¢˜ï¼Œä½†æ˜¯æ¨¡å‹ä¼šæç¤ºæ²¡æœ‰æ”¶åˆ°æ­£ç¡®çš„è¾“å…¥ã€‚

ä¸ºä»€ä¹ˆæ¨¡å‹æ²¡æœ‰å¾—åˆ° input_ids è¿™ä¸€åˆ—å‘¢ï¼Ÿæˆ‘ä»¬ç¡®å®åœ¨æ•°æ®é›†ä¸Šè°ƒç”¨äº† `Dataset.map()` æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ `tokenizer` å¤„ç†äº†æ¯ä¸ªæ ·æœ¬ã€‚ä½†æ˜¯å¦‚æœä½ ä»”ç»†çœ‹ä»£ç ï¼Œä½ ä¼šå‘ç°æˆ‘ä»¬åœ¨å°†è®­ç»ƒå’Œè¯„ä¼°é›†ä¼ é€’ç»™ `Trainer` æ—¶çŠ¯äº†ä¸€ä¸ªé”™è¯¯ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰ä½¿ç”¨ `tokenized_datasets` ï¼Œè€Œæ˜¯ä½¿ç”¨äº† `raw_datasets` ğŸ¤¦ã€‚æ‰€ä»¥è®©æˆ‘ä»¬æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼

```py
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

è¿è¡Œè¿™ä¸ªæ–°ä»£ç ä¼šè¾“å‡ºä¸€ä¸ªæ–°çš„é”™è¯¯ğŸ˜¥ï¼š

```python out
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```

æŸ¥çœ‹ tracebackï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é”™è¯¯å‘ç”Ÿåœ¨æ•°æ®æ•´ç†æ­¥éª¤ä¸­ï¼š

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

æ‰€ä»¥ï¼Œæˆ‘ä»¬åº”è¯¥å»ç ”ç©¶ä¸€ä¸‹é‚£ä¸ªã€‚ç„¶è€Œï¼Œåœ¨æ£€æŸ¥æ•°æ®æ•´ç†æ­¥éª¤ä¹‹å‰ï¼Œè¯·å…ˆå®Œæˆæ‰€æœ‰çš„æ•°æ®æ£€æŸ¥ï¼Œç¡®å®šæ•°æ®æ˜¯100% æ­£ç¡®çš„ã€‚

åœ¨è°ƒè¯•è®­ç»ƒæ•°æ®æ—¶ï¼Œä½ åº”è¯¥å§‹ç»ˆè¦è®°å¾—æ£€æŸ¥è§£ç åçš„åŸå§‹æ–‡æœ¬è¾“å…¥ï¼Œè€Œä¸æ˜¯ä¸€å¤§ä¸²ç¼–ç åçš„æ•°å­—ã€‚å› ä¸ºæˆ‘ä»¬æ— æ³•ç†è§£ç›´æ¥æä¾›ç»™å®ƒçš„æ•°å­—ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥çœ‹çœ‹è¿™äº›æ•°å­—ä»£è¡¨ä»€ä¹ˆã€‚ä¾‹å¦‚ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œè¿™æ„å‘³ç€æŸ¥çœ‹ä½ ä¼ é€’è§£ç åçš„åƒç´ å›¾ç‰‡ï¼Œåœ¨è¯­éŸ³ä¸­æ„å‘³ç€è§£ç åçš„éŸ³é¢‘æ ·æœ¬ï¼Œå¯¹äºæˆ‘ä»¬çš„ NLP ç¤ºä¾‹ï¼Œè¿™æ„å‘³ç€åº”è¯¥æ£€æŸ¥ `tokenizer` è§£ç åçš„åŸå§‹è¾“å…¥æ–‡æœ¬ï¼š

```py
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python out
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

å®ƒçœ‹èµ·æ¥æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æ“ä½œæ£€æŸ¥å…¶ä»–çš„é”®ã€‚

```py
trainer.train_dataset[0].keys()
```

```python out
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

è¯·æ³¨æ„ï¼Œæ¨¡å‹ä¸èƒ½æ¥æ”¶çš„è¾“å…¥å¯¹åº”çš„é”®å°†è¢«è‡ªåŠ¨ä¸¢å¼ƒï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬å°†ä»…éœ€è¦æ£€æŸ¥ `input_ids` ã€ `attention_mask` å’Œ `label` ï¼ˆå®ƒå°†è¢«é‡å‘½åä¸º `labels` ï¼‰ã€‚ä¸ºäº†é˜²æ­¢è®°é”™ï¼Œå¯ä»¥å…ˆæ‰“å°æ¨¡å‹çš„ç±»ï¼Œç„¶ååœ¨å…¶æ–‡æ¡£ä¸­çœ‹çœ‹å®ƒéœ€è¦è¾“å…¥å“ªäº›åˆ—ã€‚

```py
type(trainer.model)
```

```python out
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

æ‰€ä»¥åœ¨æˆ‘ä»¬çš„è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ [æ¨¡å‹æ–‡æ¡£](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification) ä¸­æŸ¥çœ‹è¿™ä¸ªæ¨¡å‹æ¥å—çš„å‚æ•°ã€‚åŒæ ·çš„ `Trainer` ä¹Ÿä¼šè®°å½•å®ƒä¸¢å¼ƒçš„åˆ—ã€‚

æˆ‘ä»¬é€šè¿‡è§£ç æ£€æŸ¥äº† `inputs ID` æ˜¯å¦æ­£ç¡®ã€‚æ¥ä¸‹æ¥åº”è¯¥æ£€æŸ¥ `attention_mask` ï¼š

```py
trainer.train_dataset[0]["attention_mask"]
```

```python out
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ç”±äºæˆ‘ä»¬æ²¡æœ‰åœ¨é¢„å¤„ç†ä¸­ä½¿ç”¨å¡«å……ï¼Œè¿™çœ‹èµ·æ¥æ²¡ä»€ä¹ˆé—®é¢˜ã€‚ä¸ºç¡®ä¿è¯¥æ³¨æ„æ©ç æ²¡æœ‰é—®é¢˜ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥å®ƒä¸ `inputs ID` çš„é•¿åº¦æ˜¯å¦ç›¸åŒï¼š

```py
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python out
True
```

é‚£æŒºå¥½çš„ï¼æœ€åï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„æ ‡ç­¾ï¼š

```py
trainer.train_dataset[0]["label"]
```

```python out
1
```

ä¸inputs ID ä¸€æ ·ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ¬èº«å¹¶æ²¡æœ‰çœŸæ­£æ„ä¹‰çš„æ•°å­—ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥çš„æ˜¯å®ƒæ‰€å¯¹åº”çš„çœŸå®çš„æ ‡ç­¾åç§°æ˜¯å¦æ˜¯æ­£ç¡®çš„ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰å­¦åˆ°çš„ï¼Œæ ‡ç­¾ ID å’Œæ ‡ç­¾åä¹‹é—´çš„æ˜ å°„å­˜å‚¨åœ¨æ•°æ®é›† `features` é‡Œçš„ `names` å±æ€§ä¸­ï¼š

```py
trainer.train_dataset.features["label"].names
```

```python out
['entailment', 'neutral', 'contradiction']
```

æ‰€ä»¥ `1` è¡¨ç¤º `neutral` ï¼Œè¡¨ç¤ºæˆ‘ä»¬ä¸Šé¢çœ‹åˆ°çš„ä¸¤å¥è¯å¹¶ä¸çŸ›ç›¾ï¼Œä¹Ÿæ²¡æœ‰åŒ…å«å…³ç³»ã€‚è¿™ä¹Ÿçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ï¼

æˆ‘ä»¬è¿™é‡Œæ²¡æœ‰ token ç±»å‹ IDï¼Œå› ä¸º DistilBERT ä¸éœ€è¦å®ƒä»¬ï¼›å¦‚æœä½ çš„æ¨¡å‹ä¸­æœ‰ï¼Œä½ è¿˜åº”è¯¥ç¡®ä¿ token ç±»å‹ ID å¯ä»¥æ­£ç¡®åŒ¹é…è¾“å…¥ä¸­ç¬¬ä¸€å¥å’Œç¬¬äºŒå¥çš„ä½ç½®ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** æ£€æŸ¥è®­ç»ƒæ•°æ®é›†çš„ç¬¬äºŒä¸ªæ¡æ•°æ®æ˜¯å¦æ­£ç¡®ã€‚

</Tip>

æˆ‘ä»¬åœ¨è¿™é‡Œåªå¯¹è®­ç»ƒé›†è¿›è¡Œæ£€æŸ¥ï¼Œä½†ä½ å½“ç„¶åº”è¯¥ä»¥åŒæ ·çš„æ–¹å¼ä»”ç»†æ£€æŸ¥éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

ç°åœ¨æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„æ•°æ®é›†çœ‹èµ·æ¥ä¸é”™ï¼Œä¸‹ä¸€æ­¥æ˜¯æ—¶å€™æ£€æŸ¥è®­ç»ƒç®¡é“äº†ã€‚

### ä» datasets åˆ° dataloaders [[ä» datasets åˆ° dataloaders]]

è®­ç»ƒç®¡é“ä¸­å¯èƒ½å‡ºé”™çš„ä¸‹ä¸€ä¸ªæ“ä½œå‘ç”Ÿåœ¨ `Trainer` å°è¯•ä»è®­ç»ƒæˆ–éªŒè¯é›†ä¸­ç”Ÿæˆ batch æ—¶ã€‚å½“ä½ ç¡®å®š `Trainer` çš„æ•°æ®é›†æ˜¯æ­£ç¡®çš„åï¼Œä½ å¯ä»¥å°è¯•é€šè¿‡æ‰§è¡Œä»¥ä¸‹ä»£ç æ‰‹åŠ¨å½¢æˆä¸€ä¸ª batchï¼ˆå½“è¦æµ‹è¯•éªŒè¯é›†çš„ dataloaders æ—¶ï¼Œå¯ä»¥å°† `train` æ›¿æ¢ä¸º `eval` ï¼‰ï¼š

```py
for batch in trainer.get_train_dataloader():
    break
```

ä¸Šè¿°ä»£ç ä¼šåˆ›å»ºè®­ç»ƒé›†çš„æ•°æ®åŠ è½½å™¨ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè¿­ä»£ä¸€æ¬¡ã€‚å¦‚æœä»£ç æ‰§è¡Œæ²¡æœ‰é”™è¯¯ï¼Œé‚£ä¹ˆä½ å°±æœ‰äº†å¯ä»¥æ£€æŸ¥çš„ç¬¬ä¸€ä¸ª batchï¼Œå¦‚æœä»£ç å‡ºé”™ï¼Œä½ å¯ä»¥ç¡®å®šé—®é¢˜å‡ºåœ¨æ•°æ®åŠ è½½å™¨ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

Trackback çš„æœ€åä¸€ä¸ªå †æ ˆçš„è¾“å‡ºåº”è¯¥è¶³å¤Ÿç»™ä½ ä¸€äº›çº¿ç´¢ï¼Œä½†è®©æˆ‘ä»¬å†æ·±å…¥æŒ–æ˜ä¸€ä¸‹åˆ›å»º batch çš„è¿‡ç¨‹ã€‚åˆ›å»º batch è¿‡ç¨‹ä¸­çš„å¤§å¤šæ•°é—®é¢˜æ˜¯åœ¨å°†æ•°æ®æ•´ç†åˆ°å•ä¸ª batch ä¸­æ—¶å‡ºç°çš„ï¼Œ å› æ­¤åœ¨å‡ºç°é—®é¢˜æ—¶é¦–å…ˆè¦æ£€æŸ¥çš„æ˜¯ DataLoader æ­£åœ¨ä½¿ç”¨çš„ `collate_fn`:
```py
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python out
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

æ‰€ä»¥ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯ `default_data_collator` ï¼Œä½†è¿™ä¸æ˜¯æˆ‘ä»¬åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­æƒ³è¦ä½¿ç”¨çš„ `collate_fn` ã€‚æˆ‘ä»¬å¸Œæœ›å°† `batch` ä¸­çš„æ¯ä¸ªå¥å­å¡«å……åˆ° `batch` ä¸­æœ€é•¿çš„å¥å­ï¼Œè¿™é¡¹åŠŸèƒ½æ˜¯ç”± `DataCollatorWithPadding` æ•´ç†å™¨å®ç°çš„ã€‚è€Œ `Trainer` é»˜è®¤ä½¿ç”¨çš„æ•°æ®æ•´ç†å™¨å°±æ˜¯å®ƒï¼Œä¸ºä»€ä¹ˆè¿™é‡Œæ²¡æœ‰ä½¿ç”¨å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯å› ä¸ºæˆ‘ä»¬åœ¨ä¸Šé¢çš„ä»£ç ä¸­å¹¶æ²¡æœ‰æŠŠ `tokenizer` ä¼ é€’ç»™ `Trainer` ï¼Œæ‰€ä»¥å®ƒæ— æ³•åˆ›å»ºæˆ‘ä»¬æƒ³è¦çš„ `DataCollatorWithPadding` ã€‚åœ¨å®è·µä¸­ï¼Œä½ åº”è¯¥æ˜ç¡®åœ°ä¼ é€’ä½ æƒ³è¦ä½¿ç”¨çš„æ•°æ®æ•´ç†å™¨ï¼Œä»¥ç¡®ä¿é¿å…è¿™äº›ç±»å‹çš„é”™è¯¯ã€‚è®©æˆ‘ä»¬ä¿®æ”¹ä»£ç ä»¥å®ç°è¿™ä¸€ç‚¹ï¼š

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å¾—åˆ°ä¸ä»¥å‰ç›¸åŒçš„é”™è¯¯ï¼Œè¿™ç»å¯¹æ˜¯è¿›æ­¥ã€‚åæ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªè‡­åæ˜­è‘—çš„ CUDA é”™è¯¯ï¼š

```python out
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```

è¿™å¾ˆç³Ÿç³•ï¼Œå› ä¸º CUDA é”™è¯¯é€šå¸¸å¾ˆéš¾è°ƒè¯•ã€‚æˆ‘ä»¬å°†åœ¨ç¨åå†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç°åœ¨è®©æˆ‘ä»¬å…ˆå®Œæˆå¯¹åˆ›å»ºçš„ `batch` çš„åˆ†æã€‚

å¦‚æœä½ ç¡®å®šä½ çš„æ•°æ®æ•´ç†å™¨æ˜¯æ­£ç¡®çš„ï¼Œé‚£ä¹ˆåº”è¯¥å°è¯•ç”¨å®ƒæ¥å¤„ç†æ•°æ®é›†çš„å‡ ä¸ªæ ·æœ¬ï¼Œæ£€æŸ¥ä¸€ä¸‹åœ¨åˆ›å»ºæ ·æœ¬çš„æ—¶å€™æ˜¯å¦ä¼šå‡ºç°é”™è¯¯ï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```

ä¸Šè¿°ä»£ç è¿è¡Œå¤±è´¥ï¼Œå› ä¸º `train_dataset` åŒ…å«å­—ç¬¦ä¸²åˆ—ï¼Œ `Trainer` é€šå¸¸ä¼šåˆ é™¤è¿™äº›åˆ—ï¼Œåœ¨è¿™æ ·çš„å•æ­¥è°ƒè¯•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ç§æœ‰çš„ `trainer._remove_unused_ columns()` æ–¹æ³•æ‰‹åŠ¨åˆ é™¤è¿™äº›åˆ—ã€‚ï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```

å¦‚æœåˆå‡ºç°äº†æ–°çš„é”™è¯¯ï¼Œåˆ™åº”è¯¥æ‰‹åŠ¨è°ƒè¯•æ•°æ®æ•´ç†å™¨ä»¥ç¡®å®šå…·ä½“çš„é—®é¢˜ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»è°ƒè¯•äº†åˆ›å»º `batch` è¿‡ç¨‹ï¼Œæ˜¯æ—¶å€™å°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹äº†ï¼

### æ£€æŸ¥æ¨¡å‹ [[æ£€æŸ¥æ¨¡å‹]]

ä½ å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥è·å¾—ä¸€ä¸ª `batch` çš„æ•°æ®ï¼š

```py
for batch in trainer.get_train_dataloader():
    break
```

å¦‚æœä½ åœ¨ notebook ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œä½ å¯èƒ½ä¼šæ”¶åˆ°ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ç±»ä¼¼çš„ CUDA é”™è¯¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦é‡æ–°å¯åŠ¨ notebook å¹¶é‡æ–°æ‰§è¡Œæœ€åä¸€æ®µä»£ç ï¼Œä½†æ˜¯æš‚æ—¶ä¸è¦è¿è¡Œ `trainer.train()` è¿™ä¸€è¡Œå‘½ä»¤ã€‚è¿™æ˜¯å…³äº CUDA é”™è¯¯çš„ç¬¬äºŒä¸ªæœ€çƒ¦äººçš„äº‹æƒ…ï¼šå®ƒä»¬ä¼šç ´åä½ çš„ Cuda å†…æ ¸ï¼Œè€Œä¸”æ— æ³•æ¢å¤ã€‚å®ƒä»¬æœ€çƒ¦äººçš„äº‹æƒ…æ˜¯å®ƒä»¬å¾ˆéš¾è°ƒè¯•ã€‚

è¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿå®ƒä¸ GPU çš„å·¥ä½œæ–¹å¼æœ‰å…³ã€‚å®ƒä»¬åœ¨å¹¶è¡Œæ‰§è¡Œå¤§é‡æ“ä½œæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†ç¼ºç‚¹æ˜¯å½“å…¶ä¸­ä¸€æ¡æŒ‡ä»¤å¯¼è‡´é”™è¯¯æ—¶ï¼Œä½ ä¸ä¼šç«‹å³çŸ¥é“ã€‚åªæœ‰å½“ç¨‹åºåœ¨ GPU ä¸Šè°ƒç”¨å¤šä¸ªè¿›ç¨‹çš„åŒæ­¥å¤„ç†æ—¶ï¼Œæ‰ä¼šè¾“å‡ºä¸€äº›é”™è¯¯ä¿¡æ¯ï¼Œä½†äº‹å®ä¸ŠçœŸå®è§¦å‘é”™è¯¯çš„åœ°æ–¹å¾€å¾€ä¸è¾“å‡ºé”™è¯¯ä¿¡æ¯çš„åœ°æ–¹å¹¶ä¸ä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä¹‹å‰çš„ Trackbackï¼Œé”™è¯¯æ˜¯åœ¨åå‘ä¼ æ’­æœŸé—´å¼•å‘çš„ï¼Œä½†æˆ‘ä»¬ä¼šåœ¨ä¸€åˆ†é’Ÿåçœ‹åˆ°é”™è¯¯å®é™…ä¸Šæºäºå‰å‘ä¼ æ’­çš„æŸäº›ä¸œè¥¿ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è°ƒè¯•è¿™äº›é”™è¯¯å‘¢ï¼Ÿç­”æ¡ˆå¾ˆç®€å•ï¼šä¸è°ƒè¯•ã€‚é™¤éä½ çš„ CUDA é”™è¯¯æ˜¯å†…å­˜ä¸è¶³é”™è¯¯ï¼ˆè¿™æ„å‘³ç€ä½ çš„ GPU ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼‰ï¼Œé™¤æ­¤ä¹‹å¤–ä½ åº”è¯¥å§‹ç»ˆè¿”å›åˆ° CPU è¿›è¡Œè°ƒè¯•ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€å°†æ¨¡å‹æ”¾å› CPU ç„¶åå°†ä¸€ä¸ª `batch` çš„æ•°æ®é€å…¥æ¨¡å‹ã€‚ç°åœ¨ `DataLoader` è¿”å›çš„é‚£æ‰¹æ•°æ®è¿˜å°šæœªç§»åŠ¨åˆ° GPUï¼Œå› æ­¤å¯ä»¥ç›´æ¥é€å…¥è¿™ä¸ª `batch`ï¼š

```python
outputs = trainer.model.cpu()(**batch)
```

```python out
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```

ç°åœ¨ï¼Œæƒ…å†µè¶Šæ¥è¶Šæ˜æœ—äº†ã€‚æŸå¤±è®¡ç®—ä¸­æ²¡æœ‰å‡ºç°CUDA é”™è¯¯ï¼Œè€Œå‡ºç°äº†ä¸€ä¸ª `IndexError`
ï¼ˆå› æ­¤ä¸åå‘ä¼ æ’­æ— å…³ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ˜¯ Target 2 é€ æˆäº†é”™è¯¯ï¼Œè¿™ä¸ªæ—¶å€™é€šå¸¸åº”è¯¥æ£€æŸ¥ä¸€ä¸‹æ¨¡å‹æ ‡ç­¾çš„æ•°é‡ã€‚

```python
trainer.model.config.num_labels
```

```python out
2
```

å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹æœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼Œåªæœ‰ 0 å’Œ 1 ä½œä¸ºç›®æ ‡ï¼Œä½†æ˜¯æ ¹æ®é”™è¯¯ä¿¡æ¯æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª 2ã€‚å¾—åˆ°ä¸€ä¸ª 2 å®é™…ä¸Šæ˜¯æ­£å¸¸çš„ï¼šå¦‚æœä½ è¿˜æœ‰å°è±¡ï¼Œæˆ‘ä»¬ä¹‹å‰æå–äº†ä¸‰ä¸ªæ ‡ç­¾åç§°ï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æˆ‘ä»¬æœ‰ 0ã€1 å’Œ 2 ä¸‰ä¸ªæ ‡ç­¾ç´¢å¼•ï¼Œè€Œé—®é¢˜æ˜¯æˆ‘ä»¬å¹¶æ²¡æœ‰å‘Šè¯‰æ¨¡å‹ï¼Œå®ƒåº”è¯¥åˆ›å»ºä¸‰ä¸ªæ ‡ç­¾ã€‚è®©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

ä¸ºäº†æ–¹ä¾¿æ£€æŸ¥ä¸€åˆ‡æ˜¯å¦æ­£å¸¸ï¼Œç°åœ¨å…ˆä¸è¦è¿è¡Œ `trainer.train()` å‘½ä»¤ã€‚å…ˆè¯·æ±‚ä¸€ä¸ª batch çš„æ•°æ®å¹¶å°†å…¶ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¦‚æœå®ƒç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

é‚£ä¹ˆä¸‹ä¸€æ­¥å°±å¯ä»¥å›åˆ° GPU å¹¶æ£€æŸ¥æˆ‘ä»¬çš„ä¿®æ”¹æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼š

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

å¦‚æœä»ç„¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¡®ä¿é‡æ–°å¯åŠ¨ notebook å¹¶ä»…æ‰§è¡Œæœ€åä¸€ç‰ˆçš„ä»£ç ï¼ˆå¦‚æœä¹‹å‰å‡ºç°äº†CUDA é”™è¯¯ï¼Œé‚£ä¹ˆCUDA å†…æ ¸å°±ä¼šè¢«ç ´åï¼Œä¹‹åå“ªæ€•æ‰§è¡Œæ­£ç¡®çš„ä»£ç ä¹Ÿä¼šå‡ºç°é”™è¯¯ï¼‰ã€‚

### æ‰§è¡Œä¸€ä¸ªä¼˜åŒ–å™¨æ­¥éª¤ [[æ‰§è¡Œä¸€ä¸ªä¼˜åŒ–å™¨æ­¥éª¤]]

ç°åœ¨æˆ‘ä»¬å·²ç»å¯ä»¥æ„å»ºé€šè¿‡æ¨¡å‹æ£€æŸ¥çš„æˆæ‰¹æ¬¡çš„æ•°æ®ï¼Œæˆ‘ä»¬å·²ç»ä¸ºè®­ç»ƒç®¡é“çš„ä¸‹ä¸€æ­¥åšå¥½å‡†å¤‡ï¼šæ¥ä¸‹æ¥æ˜¯è®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œä¼˜åŒ–å™¨è¿­ä»£ã€‚

ç¬¬ä¸€éƒ¨åˆ†æ˜¯åœ¨ loss ä¸Šè°ƒç”¨ `backward()` æ–¹æ³•ï¼š

```py
loss = outputs.loss
loss.backward()
```

åœ¨è¿™ä¸ªé˜¶æ®µå¾ˆå°‘å‡ºç°é”™è¯¯ï¼Œä½†å¦‚æœç¡®å®å‡ºç°é”™è¯¯ï¼Œé‚£ä¹ˆéœ€è¦è¿”å› CPU æ¥è·å–æ›´æœ‰ç”¨çš„é”™è¯¯æ¶ˆæ¯ã€‚

è¦æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œæˆ‘ä»¬åªéœ€è¦åˆ›å»º `optimizer` å¹¶è°ƒç”¨å®ƒçš„ `step()` æ–¹æ³•ï¼š

```py
trainer.create_optimizer()
trainer.optimizer.step()
```

åŒæ ·ï¼Œå¦‚æœä½ åœ¨ `Trainer` ä¸­ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆåœ¨æ­¤é˜¶æ®µä½ ä¸åº”è¯¥æ”¶åˆ°é”™è¯¯ï¼Œä½†å¦‚æœä½ æœ‰è‡ªå®šä¹‰ä¼˜åŒ–å™¨ï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°ä¸€äº›é—®é¢˜ï¼Œéœ€è¦åœ¨è¿™é‡Œè°ƒè¯•ã€‚å¦‚æœä½ åœ¨æ­¤é˜¶æ®µé‡åˆ°å¥‡æ€ªçš„ CUDA é”™è¯¯ï¼Œè¯·ä¸è¦å¿˜è®°è¿”å› CPUã€‚è¯´åˆ° CUDA é”™è¯¯ï¼Œå‰é¢æˆ‘ä»¬æåˆ°äº†ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ç§æƒ…å†µã€‚

### å¤„ç† CUDA out-of-memory é”™è¯¯ [[å¤„ç† CUDA out-of-memoryé”™è¯¯]]

æ¯å½“ä½ æ”¶åˆ°ä»¥ `RuntimeError: CUDA out of memory` å¼€å¤´çš„é”™è¯¯æ¶ˆæ¯æ—¶ï¼Œè¿™è¡¨æ˜ä½ çš„æ˜¾å­˜ä¸è¶³ã€‚è¿™ä¸ä½ çš„ä»£ç æ²¡æœ‰ç›´æ¥å…³ç³»ï¼Œå¹¶ä¸”å®ƒä¹Ÿå¯èƒ½å‘ç”Ÿåœ¨è¿è¡Œè‰¯å¥½çš„ä»£ç ä¸­ã€‚æ­¤é”™è¯¯æ„å‘³ç€ä½ è¯•å›¾åœ¨ GPU çš„æ˜¾å­˜ä¸­æ”¾å…¥å¤ªå¤šä¸œè¥¿ï¼Œè¿™å¯¼è‡´äº†é”™è¯¯ã€‚ä¸å…¶ä»– CUDA é”™è¯¯ä¸€æ ·ï¼Œä½ éœ€è¦é‡æ–°å¯åŠ¨å†…æ ¸æ‰èƒ½å†æ¬¡è¿è¡Œè®­ç»ƒã€‚

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ åªéœ€è¦ä½¿ç”¨æ›´å°‘çš„æ˜¾å­˜â€”è¿™å¾€å¾€è¯´èµ·æ¥å®¹æ˜“åšèµ·æ¥éš¾ã€‚é¦–å…ˆï¼Œç¡®ä¿ä½ æ²¡æœ‰åŒæ—¶åœ¨ GPU ä¸Šè¿è¡Œä¸¤ä¸ªæ¨¡å‹ï¼ˆå½“ç„¶ï¼Œé™¤éåœ¨è§£å†³é—®é¢˜æ—¶å¿…é¡»è¦è¿™æ ·åšï¼‰ã€‚ç„¶åï¼Œä½ å¯èƒ½åº”è¯¥å‡å°‘ batch çš„å¤§å°ï¼Œå› ä¸ºå®ƒç›´æ¥å½±å“æ¨¡å‹çš„æ‰€æœ‰ä¸­é—´è¾“å‡ºçš„å¤§å°åŠå…¶æ¢¯åº¦ã€‚å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·è€ƒè™‘ä½¿ç”¨è¾ƒå°ç‰ˆæœ¬çš„æ¨¡å‹ï¼Œæˆ–è€…æ›´æ¢æœ‰æ›´å¤§æ˜¾å­˜çš„è®¾å¤‡ã€‚

<Tip>

åœ¨è¯¾ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æ›´å…ˆè¿›çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å¸®åŠ©ä½ å‡å°‘å†…å­˜å ç”¨å¹¶è®©ä½ å¾®è°ƒè¶…å¤§çš„æ¨¡å‹ã€‚

</Tip>

### è¯„ä¼°æ¨¡å‹ [[è¯„ä¼°æ¨¡å‹]]

ç°åœ¨æˆ‘ä»¬å·²ç»è§£å†³äº†æ‰€æœ‰çš„ä»£ç é—®é¢˜ï¼Œä¸€åˆ‡éƒ½å¾ˆå®Œç¾ï¼Œè®­ç»ƒåº”è¯¥å¯ä»¥é¡ºåˆ©è¿›è¡Œï¼Œå¯¹å§ï¼Ÿæ²¡é‚£ä¹ˆå¿«ï¼å¦‚æœä½ è¿è¡Œ `trainer.train()` å‘½ä»¤ï¼Œä¸€å¼€å§‹ä¸€åˆ‡çœ‹èµ·æ¥éƒ½ä¸é”™ï¼Œä½†è¿‡ä¸€ä¼šå„¿ä½ ä¼šå¾—åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

```py
# è¿™å°†èŠ±è´¹å¾ˆé•¿æ—¶é—´å¹¶ä¸”ä¼šå‡ºé”™,æ‰€ä»¥ä¸è¦ç›´æ¥è¿è¡Œè¿™ä¸ªå•å…ƒ
trainer.train()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

ä½ å°†æ„è¯†åˆ°æ­¤é”™è¯¯å‡ºç°åœ¨è¯„ä¼°é˜¶æ®µï¼Œå› æ­¤è¿™æ˜¯æˆ‘ä»¬éœ€è¦è°ƒè¯•çš„æœ€åä¸€ä»¶äº‹ã€‚

ä½ å¯ä»¥ç‹¬ç«‹äºè®­ç»ƒï¼Œå•ç‹¬è¿è¡Œ Trainer çš„è¯„ä¼°å¾ªç¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
trainer.evaluate()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

<Tip>

ğŸ’¡ ä½ åº”è¯¥å§‹ç»ˆç¡®ä¿åœ¨å¯åŠ¨ `trainer.train()` ä¹‹å‰ `trainer.evaluate()` æ˜¯å¯ä»¥è¿è¡Œçš„ï¼Œä»¥é¿å…åœ¨é‡åˆ°é”™è¯¯ä¹‹å‰æµªè´¹å¤§é‡è®¡ç®—èµ„æºã€‚

</Tip>

åœ¨å°è¯•è°ƒè¯•è¯„ä¼°å¾ªç¯ä¸­çš„é—®é¢˜ä¹‹å‰ï¼Œä½ åº”è¯¥é¦–å…ˆç¡®ä¿ä½ å·²ç»æ£€æŸ¥äº†æ•°æ®ï¼Œèƒ½å¤Ÿæ­£ç¡®åœ°å½¢æˆäº† batch å¹¶ä¸”å¯ä»¥åœ¨å…¶ä¸Šè¿è¡Œä½ çš„æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»å®Œæˆäº†æ‰€æœ‰è¿™äº›æ­¥éª¤ï¼Œå› æ­¤å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç è€Œä¸ä¼šå‡ºé”™ï¼š

```py
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

ç¨ç­‰ä¸€ä¼šå„¿ï¼Œé”™è¯¯å°±ä¼šå‡ºç°ï¼Œåœ¨è¯„ä¼°é˜¶æ®µç»“æŸæ—¶è¾“å‡ºäº†ä¸€ä¸ªé”™è¯¯ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ Trackbackï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼š

```python trace
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

è¿™å‘Šè¯‰æˆ‘ä»¬é”™è¯¯æºè‡ª `datasets/metric.py` æ¨¡å—æ‰€ä»¥å¾ˆæœ‰å¯èƒ½æ˜¯è®¡ç®— `compute_metrics()` å‡½æ•°æ—¶å‡ºç°çš„é—®é¢˜ã€‚å®ƒéœ€è¦è¾“å…¥ NumPy æ•°ç»„æ ¼å¼çš„ logits å€¼å’Œæ ‡ç­¾çš„å…ƒç»„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯•å°†å…¶æä¾›ç»™å®ƒï¼š

```py
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

æˆ‘ä»¬å¾—åˆ°äº†åŒæ ·çš„é”™è¯¯ï¼Œæ‰€ä»¥é—®é¢˜è‚¯å®šå‡ºåœ¨é‚£ä¸ªå‡½æ•°ä¸Šã€‚å¦‚æœæˆ‘ä»¬å›é¡¾å®ƒçš„ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒåªæ˜¯å°† `predictions` å’Œ `labels` è½¬å‘åˆ° `metric.compute()` ã€‚é‚£ä¹ˆè¿™ç§æ–¹æ³•æœ‰é—®é¢˜å—ï¼Ÿä¸ä¸€å®šã€‚è®©æˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸€ä¸‹è¾“å…¥çš„å½¢çŠ¶ï¼š

```py
predictions.shape, labels.shape
```

```python out
((8, 3), (8,))
```

æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºæ˜¯ä¸‰ä¸ªæ ‡ç­¾çš„ logits å€¼ï¼Œè€Œä¸æ˜¯æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾idï¼Œè¿™å°±æ˜¯ metrics è¿”å›è¿™ä¸ªï¼ˆæœ‰ç‚¹æ¨¡ç³Šï¼‰é”™è¯¯çš„åŸå› ã€‚ä¿®å¤å¾ˆç®€å•ï¼›æˆ‘ä»¬åªéœ€è¦åœ¨ `compute_metrics()` å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ª argmaxï¼š

```py
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))
```

```python out
{'accuracy': 0.625}
```

ç°åœ¨æˆ‘ä»¬çš„é”™è¯¯å·²ä¿®å¤ï¼è¿™æ˜¯æœ€åä¸€ä¸ªé”™è¯¯ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è„šæœ¬ç°åœ¨å°†å¯ä»¥æ­£ç¡®åœ°è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

ä½œä¸ºå‚è€ƒï¼Œè¿™é‡Œæ˜¯å®Œå…¨ä¿®å¤çš„ä»£ç ï¼š

```py
import numpy as np
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æ›´å¤šé”™è¯¯ï¼Œæˆ‘ä»¬çš„è„šæœ¬å°†å¾®è°ƒä¸€ä¸ªåº”è¯¥ç»™å‡ºåˆç†ç»“æœçš„æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œå¦‚æœè®­ç»ƒæ²¡æœ‰ä»»ä½•é”™è¯¯ï¼Œè€Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ ¹æœ¬è¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿè¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºä¸€äº›å¯ä»¥å¸®åŠ©è§£å†³è¿™ç±»é—®é¢˜çš„æŠ€å·§ã€‚
<Tip>

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œè°ƒè¯•è®­ç»ƒæµç¨‹æ—¶ä¹Ÿéœ€è¦éµå¾ªç›¸åŒçš„æ­¥éª¤ï¼Œè€Œä¸”æ›´å®¹æ˜“å°†è®­ç»ƒä¸­çš„å„ä¸ªæ­¥éª¤åˆ†å¼€è°ƒè¯•ã€‚ä½†æ˜¯ï¼Œè¯·ç¡®ä¿ä½ æ²¡æœ‰å¿˜è®°åœ¨åˆé€‚çš„ä½ç½®è°ƒç”¨ `model.eval()` æˆ– `model.train()` ï¼Œä¹Ÿä¸è¦å¿˜è®°åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨ `zero_grad()` ï¼

</Tip>

## åœ¨è®­ç»ƒæœŸé—´è°ƒè¯•é™é»˜ï¼ˆæ²¡æœ‰ä»»ä½•é”™è¯¯æç¤ºï¼‰é”™è¯¯ [[åœ¨è®­ç»ƒæœŸé—´è°ƒè¯•é™é»˜ï¼ˆæ²¡æœ‰ä»»ä½•é”™è¯¯æç¤ºï¼‰é”™è¯¯]]

å¦‚ä½•è°ƒè¯•ä¸€ä¸ªæ²¡æœ‰é”™è¯¯ä½†ä¹Ÿæ²¡æœ‰å¾—åˆ°å¥½çš„ç»“æœçš„è®­ç»ƒï¼Ÿæ¥ä¸‹æ¥ä¼šç»™å‡ºä¸€äº›å¯ä»¥å‚è€ƒçš„åšæ³•ï¼Œä½†è¯·æ³¨æ„ï¼Œè¿™ç§è°ƒè¯•æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œå¹¶ä¸”æ²¡æœ‰ä¸‡èƒ½çš„çµä¸¹å¦™è¯ã€‚

### å†æ¬¡æ£€æŸ¥ä½ çš„æ•°æ®  [[å†æ¬¡æ£€æŸ¥ä½ çš„æ•°æ®]]

ç†è®ºä¸Šï¼Œåªæœ‰æ•°æ®ä¸­å­˜åœ¨å¯ä»¥å­¦ä¹ çš„çŸ¥è¯†ï¼Œæ¨¡å‹æ‰ä¼šå­¦åˆ°ä¸€äº›çŸ¥è¯†ã€‚å¦‚æœæ•°æ®å·²ç»è¢«æŸåäº†æˆ–æ ‡ç­¾æ˜¯éšæœºçš„ï¼Œé‚£ä¹ˆæ¨¡å‹å¾ˆå¯èƒ½æ— æ³•ä»æ•°æ®é›†ä¸­è·å¾—ä»»ä½•çŸ¥è¯†ã€‚å› æ­¤ï¼Œå§‹ç»ˆé¦–å…ˆä»”ç»†æ£€æŸ¥ä½ çš„è§£ç åçš„è¾“å…¥å’ŒçœŸå®çš„æ ‡ç­¾ï¼Œç„¶åé—®è‡ªå·±ä»¥ä¸‹é—®é¢˜ï¼š

- è§£ç åçš„æ–‡æœ¬æ•°æ®ä½ æ˜¯å¦å¯ä»¥æ­£å¸¸é˜…è¯»å’Œç†è§£ï¼Ÿ
- ä½ è®¤åŒè¿™äº›æ ‡ç­¾å¯¹äºæ–‡æœ¬çš„æè¿°å—ï¼Ÿ
- æœ‰æ²¡æœ‰ä¸€ä¸ªæ ‡ç­¾æ¯”å…¶ä»–æ ‡ç­¾æ›´å¸¸è§ï¼Ÿ
- å¦‚æœæ¨¡å‹é¢„æµ‹çš„ç­”æ¡ˆæ˜¯éšæœºçš„æˆ–æ€»æ˜¯ç›¸åŒçš„ï¼Œé‚£ä¹ˆ loss/ è¯„ä¼°æŒ‡æ ‡åº”è¯¥æ˜¯å¤šå°‘ï¼Œæ˜¯å¦æ¨¡å‹æ ¹æœ¬æ²¡èƒ½å­¦åˆ°ä»»ä½•çŸ¥è¯†ï¼Ÿ

<Tip warning={true}>

âš ï¸ å¦‚æœä½ æ­£åœ¨è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­æ‰“å°æ•°æ®é›†çš„æ ·æœ¬å¹¶ä»”ç»†æ ¸å¯¹ï¼Œç¡®ä¿ä½ å¾—åˆ°çš„æ˜¯ç›¸åŒçš„å†…å®¹ã€‚ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯åœ¨æ•°æ®åˆ›å»ºè¿‡ç¨‹ä¸­æœ‰ä¸€äº›éšæœºæ€§ï¼Œå¯¼è‡´æ¯ä¸ªè¿›ç¨‹å…·æœ‰ä¸åŒç‰ˆæœ¬çš„æ•°æ®é›†ã€‚

</Tip>

åœ¨æ£€æŸ¥æ•°æ®åï¼Œå¯ä»¥æ£€æŸ¥æ¨¡å‹çš„ä¸€äº›é¢„æµ‹å¹¶å¯¹å…¶è¿›è¡Œè§£ç ã€‚ å¦‚æœæ¨¡å‹æ€»æ˜¯é¢„æµ‹åŒæ ·çš„ç±»åˆ«ï¼Œé‚£ä¹ˆå¯èƒ½æ˜¯å› ä¸ºè¿™ä¸ªç±»åˆ«åœ¨æ•°æ®é›†ä¸­çš„æ¯”ä¾‹æ¯”è¾ƒé«˜ï¼ˆé’ˆå¯¹åˆ†ç±»é—®é¢˜ï¼‰ï¼› è¿‡é‡‡æ ·ç¨€æœ‰ç±»ç­‰æŠ€æœ¯å¯èƒ½ä¼šå¯¹è§£å†³è¿™ç§é—®é¢˜æœ‰å¸®åŠ©ã€‚æˆ–è€…ï¼Œè¿™ä¹Ÿå¯èƒ½æ˜¯ç”±è®­ç»ƒçš„è®¾ç½®ï¼ˆå¦‚é”™è¯¯çš„è¶…å‚æ•°è®¾ç½®ï¼‰å¼•èµ·çš„ã€‚

å¦‚æœåœ¨åˆå§‹æ¨¡å‹ä¸Šè·å¾—çš„ loss/ è¯„ä¼°æŒ‡æ ‡ä¸é¢„ä¼°çš„éšæœºæ—¶é¢„æµ‹çš„ loss/ è¯„ä¼°æŒ‡æ ‡éå¸¸ä¸åŒï¼Œåˆ™åº”è¯¥ä»”ç»†æ£€æŸ¥ loss/ è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹å¼ï¼Œå› ä¸ºå…¶ä¸­å¯èƒ½å­˜åœ¨é”™è¯¯ã€‚å¦‚æœä½¿ç”¨å¤šä¸ª lossï¼Œå¹¶å°†å…¶ç›¸åŠ è®¡ç®—æœ€åçš„lossï¼Œåˆ™åº”è¯¥ç¡®ä¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„æ¯”ä¾‹å¤§å°ã€‚

å½“ä½ ç¡®å®šä½ çš„æ•°æ®æ˜¯å®Œç¾çš„ä¹‹åï¼Œåˆ™å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„è¿‡æ‹Ÿåˆæµ‹è¯•æ¥æŸ¥çœ‹æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç”¨å…¶è¿›è¡Œè®­ç»ƒã€‚

### åœ¨ä¸€ä¸ª batch ä¸Šè¿‡æ‹Ÿåˆæ¨¡å‹ [[åœ¨ä¸€ä¸ª batch ä¸Šè¿‡æ‹Ÿåˆæ¨¡å‹]]

è¿‡æ‹Ÿåˆé€šå¸¸æ˜¯åœ¨è®­ç»ƒæ—¶å°½é‡é¿å…çš„äº‹æƒ…ï¼Œå› ä¸ºè¿™æ„å‘³ç€æ¨¡å‹æ²¡æœ‰è¯†åˆ«å¹¶å­¦ä¹ æˆ‘ä»¬æƒ³è¦çš„ä¸€èˆ¬ç‰¹å¾ï¼Œè€Œåªæ˜¯è®°ä½äº†è®­ç»ƒæ ·æœ¬ã€‚ ä½†ä¸€éåˆä¸€éåœ°å°è¯•åœ¨ä¸€ä¸ª batch ä¸Šè®­ç»ƒæ¨¡å‹å¯ä»¥æ£€æŸ¥æ•°æ®é›†æ‰€æè¿°çš„é—®é¢˜æ˜¯å¦å¯ä»¥é€šè¿‡è®­ç»ƒçš„æ¨¡å‹æ¥è§£å†³ï¼Œ å®ƒè¿˜å°†å¸®åŠ©æŸ¥çœ‹ä½ çš„åˆå§‹å­¦ä¹ ç‡æ˜¯å¦å¤ªé«˜äº†ã€‚

åœ¨å®šä¹‰å¥½ `Trainer` ä¹‹åï¼Œè¿™æ ·åšçœŸçš„å¾ˆå®¹æ˜“ï¼›åªéœ€è·å–ä¸€æ‰¹è®­ç»ƒæ•°æ®ï¼Œç„¶åä»…ä½¿ç”¨è¿™ä¸ª `batch` è¿è¡Œä¸€ä¸ªå°å‹æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œå¤§çº¦ 20 æ­¥ï¼š

```py
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

<Tip>

ğŸ’¡ å¦‚æœä½ çš„è®­ç»ƒæ•°æ®ä¸å¹³è¡¡ï¼Œè¯·ç¡®ä¿æ„å»ºä¸€æ‰¹åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ã€‚

</Tip>

ç”Ÿæˆçš„æ¨¡å‹åœ¨ä¸€ä¸ª `batch` ä¸Šåº”è¯¥æœ‰æ¥è¿‘å®Œç¾çš„ç»“æœã€‚è®©æˆ‘ä»¬è®¡ç®—ç»“æœé¢„æµ‹çš„è¯„ä¼°æŒ‡æ ‡ï¼š

```py
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python out
{'accuracy': 1.0}
```

100ï¼… å‡†ç¡®ç‡ï¼Œç°åœ¨è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¿‡æ‹Ÿåˆç¤ºä¾‹ï¼ˆè¿™æ„å‘³ç€å¦‚æœä½ åœ¨ä»»ä½•å…¶ä»–å¥å­ä¸Šå°è¯•ä½ çš„æ¨¡å‹ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šç»™ä½ ä¸€ä¸ªé”™è¯¯çš„ç­”æ¡ˆï¼‰ï¼

å¦‚æœä½ æ²¡æœ‰è®¾æ³•è®©ä½ çš„æ¨¡å‹è·å¾—è¿™æ ·çš„å®Œç¾ç»“æœï¼Œè¿™æ„å‘³ç€æ„å»ºé—®é¢˜çš„æ–¹å¼æˆ–æ•°æ®æœ‰é—®é¢˜ã€‚åªæœ‰å½“ä½ é€šè¿‡äº†è¿‡æ‹Ÿåˆæµ‹è¯•ï¼Œæ‰èƒ½ç¡®å®šä½ çš„æ¨¡å‹ç†è®ºä¸Šç¡®å®å¯ä»¥å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚

<Tip warning={true}>

âš ï¸ åœ¨æ­¤æµ‹è¯•ä¹‹åï¼Œä½ éœ€è¦åˆ›å»ºæ¨¡å‹å’Œ `Trainer` ï¼Œå› ä¸ºè·å¾—çš„æ¨¡å‹å¯èƒ½æ— æ³•åœ¨ä½ çš„å®Œæ•´æ•°æ®é›†ä¸Šæ¢å¤å’Œå­¦ä¹ æœ‰ç”¨çš„ä¸œè¥¿ã€‚

</Tip>

### åœ¨ä½ æœ‰ç¬¬ä¸€ä¸ª baseline æ¨¡å‹ä¹‹å‰ä¸è¦è°ƒæ•´ä»»ä½•ä¸œè¥¿ [[åœ¨ä½ æœ‰ç¬¬ä¸€ä¸ªbaseline æ¨¡å‹ä¹‹å‰ä¸è¦è°ƒæ•´ä»»ä½•ä¸œè¥¿]]

è¶…å‚æ•°è°ƒä¼˜æ€»æ˜¯è¢«å¼ºè°ƒä¸ºæœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œä½†è¿™åªæ˜¯å¸®åŠ©ä½ åœ¨æŒ‡æ ‡ä¸Šæœ‰æ‰€æ”¶è·çš„æœ€åä¸€æ­¥ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ `Trainer` çš„é»˜è®¤è¶…å‚æ•°å¯ä»¥å¾ˆå¥½åœ°ä¸ºä½ æä¾›è‰¯å¥½çš„ç»“æœï¼Œå› æ­¤åœ¨ä½ æ‹¥æœ‰æ•°æ®é›†ä¸Šçš„ baseline æ¨¡å‹ä¹‹å‰ï¼Œä¸è¦æ€¥äºè¿›è¡Œè€—æ—¶å’Œæ˜‚è´µçš„è¶…å‚æ•°æœç´¢ã€‚

åœ¨æœ‰ä¸€ä¸ªè¶³å¤Ÿå¥½çš„æ¨¡å‹åï¼Œå°±å¯ä»¥å¼€å§‹å¾®è°ƒäº†ã€‚å°½é‡é¿å…ä½¿ç”¨ä¸åŒçš„è¶…å‚æ•°è¿›è¡Œä¸€åƒæ¬¡è¿è¡Œï¼Œè€Œè¦æ¯”è¾ƒä¸€ä¸ªè¶…å‚æ•°å–ä¸åŒæ•°å€¼çš„å‡ æ¬¡è¿è¡Œï¼Œä»è€Œäº†è§£å“ªä¸ªè¶…å‚æ•°çš„å½±å“æœ€å¤§ï¼Œä»è€Œç†è§£è¶…å‚æ•°å€¼çš„æ”¹å˜ä¸äºæ¨¡å‹è®­ç»ƒä¹‹é—´çš„å…³ç³»ã€‚

å¦‚æœæ­£åœ¨è°ƒæ•´æ¨¡å‹æœ¬èº«ï¼Œè¯·ä¿æŒç®€å•ï¼Œä¸è¦ç›´æ¥å¯¹æ¨¡å‹è¿›è¡Œéå¸¸å¤æ‚çš„æ— æ³•ç†è§£æˆ–è€…è¯æ˜çš„ä¿®æ”¹ï¼Œè¦ä¸€æ­¥æ­¥ä¿®æ”¹ï¼ŒåŒæ—¶å°è¯•ç†è§£å’Œè¯æ˜è¿™æ¬¡ä¿®æ”¹å¯¹æ¨¡å‹äº§ç”Ÿçš„å½±å“ï¼Œå¹¶ä¸” ç¡®ä¿é€šè¿‡è¿‡æ‹Ÿåˆæµ‹è¯•æ¥éªŒè¯ä¿®æ”¹æ²¡æœ‰å¼•å‘å…¶ä»–çš„é—®é¢˜ã€‚

### è¯·æ±‚å¸®å¿™ [[è¯·æ±‚å¸®å¿™]]

å¸Œæœ›ä½ ä¼šåœ¨æœ¬è¯¾ç¨‹ä¸­æ‰¾åˆ°ä¸€äº›å¯ä»¥å¸®åŠ©ä½ è§£å†³é—®é¢˜çš„å»ºè®®ï¼Œé™¤æ­¤ä¹‹å¤–å¯ä»¥éšæ—¶åœ¨ [è®ºå›](https://discuss.huggingface.co/) ä¸Šå‘ç¤¾åŒºæé—®ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¢å¤–èµ„æºï¼š

-Joel Grus çš„ [â€œä½œä¸ºå·¥ç¨‹æœ€ä½³å®è·µå·¥å…·çš„å†ç°æ€§â€](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) 
- Cecelia Shao çš„ [â€œç¥ç»ç½‘ç»œè°ƒè¯•æ¸…å•â€](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) 
- Chase Roberts çš„ [â€œå¦‚ä½•å¯¹æœºå™¨å­¦ä¹ ä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•â€](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) 
- Andrej Karpathy çš„ [â€œè®­ç»ƒç¥ç»ç½‘ç»œçš„ç§˜è¯€â€](http://karpathy.github.io/2019/04/25/recipe) 

å½“ç„¶ï¼Œå¹¶ä¸æ˜¯ä½ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶é‡åˆ°çš„æ¯ä¸ªé—®é¢˜éƒ½æ˜¯ä½ è‡ªå·±çš„é”™ï¼å¦‚æœä½ åœ¨ğŸ¤— Transformers æˆ–ğŸ¤— Datasets åº“ä¸­é‡åˆ°äº†ä¼¼ä¹ä¸æ­£ç¡®çš„ä¸œè¥¿ï¼Œä½ å¯èƒ½é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ã€‚ä½ åº”è¯¥å‘Šè¯‰æˆ‘ä»¬æ‰€æœ‰è¿™äº›é—®é¢˜ï¼Œåœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è§£é‡Šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚