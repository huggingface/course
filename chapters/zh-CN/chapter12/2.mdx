# 强化学习简介及其在大语言模型中的作用

欢迎来到第一节！

我们即将要开启我们的学习之旅，进入振奋人心的强化学习（RL）的世界，一起探索它是如何为语言模型的训练开创新范式 —— 这些模型或许正是你每天都在使用的呢

> [!TIP]
> 在本章节中，我们将会聚焦于用于语言模型的强化学习。当然，强化学习是一个涵盖广泛的领域，其应用远不止语言模型。如果你对学习更多强化学习相关的知识感兴趣，你可以查看[深度强化学习课程](https://huggingface.co/courses/deep-rl-course/en/unit1/introduction)。

本节旨在为你提供一份友好清晰的强化学习入门指南，即使你此前从未接触过也无妨。我们会逐一讲解其核心概念，并探讨为什么强化学习在大语言模型（LLMs）领域变得愈发重要。

## 什么是强化学习（RL）?

想象一下你正在训练一只小狗，想教会它“坐下”这个指令。你会说“坐下”，如果小狗照做了，你就给它一点零食并给予表扬。如果它没有坐下，你可能会温柔地引导它，或者再试一次。久而久之，小狗逐渐学会将“坐下”这个动作与积极的奖励（零食和表扬）关联起来，并且在你下次发出指令时，他更可能听从。在强化学习中，我们称这种反馈为**奖励**（Reward）。

简而言之，这就是强化学习背后的基本思想！不同之处在于，在实际的RL任务中，小狗被替换成**语言模型**（在强化学习中，我们会称之为**智能体**），而训练者的角色责备提供反馈的**环境**所取代。

![强化学习术语流程](https://huggingface.co/reasoning-course/images/resolve/main/grpo/3.jpg)

让我们来拆解一下强化学习的关键关键组成部分：

### 智能体

智能体是我们的学习主体。在小狗的例子中，小狗就是智能体；在LLM的语境下，LLM本身就变成了我们想要训练的智能体。智能体负责做出决策，并从环境和和给予的奖励中学习。

### 环境

环境是智能体所处并与之互动的世界。对于小狗来说，它的环境就是你的房子和身为训练者的你。对于LLM来说，环境则更为抽象 —— 它可能是与之交互的用户，也可能是我们专门为其设置的模拟场景。环境会向智能体提供反馈。

### 动作

动作（或行动）是智能体在环境中可以执行的选择。以小狗为例，其动作包括“坐下”、“站立”、“吠叫”等。对于LLM来说，动作则可能是：生成句子中的下一个词、从多个选项中选择一个答案来回答问题、或是决定如何在一段对话中回复。

### 奖励

奖励是智能体执行动作之后，环境给予的反馈。奖励通常以数字形式表示。

**正向奖励**就像零食和夸奖 —— 它们告诉智能体：“真棒，你做得对！”

**负向奖励**（或者惩罚）更像是温和的否定 —— 它们提示智能体：“这个不太对，试试别的行动吧。” 对于小狗来说，零食就是奖励。

对于大语言模型，奖励的设计旨在反映模型在特定任务上的表现——比如回答是否有帮助（helpful）、是否真实（truthful），以及是否无害（harmless）。

### 策略

策略是智能体选择行动时所依据的决策准则。就像小狗需要理解当你喊“坐下”时，它应该怎么做一样。在强化学习中，策略正是我们真正试图学习和改进的核心。它可以是一系列规则，或者一个函数，用于指导智能体在不同的情境下应该采取什么行动。起初，策略可能是随机的，但随着智能体不断学习，策略会变得更擅长选择能带来更高奖励的行动。

## 强化学习的过程：试错法

![强化学习的过程](https://huggingface.co/reasoning-course/images/resolve/main/grpo/1.jpg)

强化学习通过试错过程得以实现：

| 步骤 | 具体过程 | 简介 |
|------|---------|-------------|
| 1. 观察 | 智能观察环境 | 智能体获取其当前状态和周围环境的信息 |
| 2. 行动 | 智能体基于当前策略采取行动 | 智能体运用已学到的策略决定下一步行动 |
| 3. 反馈 | 环境给予智能体奖励 | 智能体收到其行动好坏的反馈 |
| 4. 学习 | 智能体根据奖励更新策略 | 智能体调整其策略 —— 强化可以导致高奖励的行动，避免采取导致低奖励的行动 |
| 5. 迭代 | 重复此过程 | 这个学习的循环为持续下去，帮助智能体持续不断地提升决策能力 |

回想一下学习骑自行车的过程。一开始你可能会摇摇晃晃甚至摔跤（负向奖励！）。但当你成功保持平衡并平稳地踩动踏板时，你会感觉非常愉快（正向奖励！）。你会根据这些反馈调整你的动作 —— 比如微微倾斜身体、加快蹬踏速度等等 —— 直到你完全学会骑车。强化学习的原理也是如此 —— 它正式通过交互和反馈来进行学习。

## 强化学习在大语言模型（LLMs）中的作用

那么，为什么强化学习对大语言模型如此重要？

当然，训练出真正优秀的大语言模型绝非易事。我们可以使用互联网上的海量文本来训练它们，使它们变得非常擅长预测句子中的下一个词。正如我们在[第二章](/course/chapters/zh-CN/chapter2/1)所学，它们正是通过这种方式学会生成流畅且语法正确的文字。

然而，仅仅是表达流利是不够的。我们希望大语言模型不仅能擅长组合词句，我们还希望它们可以具备以下特质：

* **有用**：可以提供实用且相关的信息。
* **无害**：避免生成有毒、有偏见或者伤害性的内容。
* **与人类的偏好对齐**：以一种人类认为自然、有帮助和吸引人的方式回复。

预训练大语言模型的方法主要依赖于让模型学会根据文本数据预测下一个词，常常容易在上述几个方面存在不足。

尽管有监督训练非常擅长生成结构化输出，但是在生成有用、无害和对齐的回复时还是会有些力不从心。我们在[第十一章](/course/chapters/zh-CN/chapter11/1)深入探讨了有监督训练。

经过微调过的模型或许可以生成流利且结构化的文字，但它的回答仍可能有事实错误、带有偏见、或者没有真正有效地回答用户的问题。

**这时就需要强化学习了！** 强化学习为我们提供了一种微调这些预训练大模型的方法，从而更好实现我们所期望的模型应具备的特性。它就像给我们的“大语言模型狗”进行额外的训练，让它变成一只举止得体、乐于助人的伙伴，而不是只知道流利地汪汪叫的狗！

## 基于人类反馈的强化学习（RLHF）

**基于人类反馈的强化学习（RLHF）**是一种非常流行的对齐语言模型的技术。在RLHF中，我们以人类的反馈作为强化学习中的“奖励”信号。以下是它的工作原理：

1. **收集人类偏好：**我们会请人类比较大语言模型针对同一输入提示词生成的不同回复，并给出他们的偏好。比如，我们会评估者展示关于“法国的首都是哪里？”这一问题的两个不同的答案，并询问“哪个答案更好？”。
2. **训练奖励模型**：我们会使用这些人类偏好数据去训练一个独立的模型，称之为“**奖励模型**”。这个奖励模型会学习预测人类更喜欢哪一种回复，并学会基于实用性、无害性和与人类偏好的对齐程度对回复进行评分。
3. **使用强化学习微调大语言模型**：现在，我们可以使用这个奖励模型作为大语言模型智能体的环境。大语言模型生成回答（行动），然后奖励模型对这些回复打分（提供奖励）。本质上，奖励模型学习了人类的偏好，然后我们训练大语言模型去生成奖励模型认为好的文本，也就是人类偏好的文本。

![强化学习基础概念](https://huggingface.co/reasoning-course/images/resolve/main/grpo/2.jpg)  

从更宏观的视角来看，大语言模型使用强化学习有以下好处：

| 好处 | 具体描述 |
|---------|-------------|
| 增强可控性 | 强化学习让我们能够更好地控制大语言模型生成的内容。我们可以引导模型生成更符合特定目标的文本，比如更有帮助、更有创意或者更简洁。 |
| 加强与人类价值观的对齐 | RLHF技术尤其有助于大语言模型对齐一些复杂且通常主观的人类偏好。我们很难写下明确的规则来定义“什么是一个好的答案”，但人类却可以轻松判断和对比回复。RLHF正是让模型得以从这些人类的判断中学习。 |
| 减少不良行为 | 强化学习可以用于减少大语言模型的负面行为，比如生成有害言论、传播错误信息或者展示偏见。通过设计惩罚这些行为的消极奖励，我们可以矫正模型避免这些行为。 |

基于人类反馈的强化学习已经被用于训练许多当今最受欢迎的大语言模型中，比如OpenAI的GPT-4、谷歌的Gemini、DeepSeek的R1模型。实现RLHF的技术范围很广，其复杂性和精妙程度又更有千秋。在本章中，我们会重点介绍**群体相对策略优化（GRPO）**算法，这是一种经证明能有效训练大语言模型、使其变得实用、无害并与人类偏好对齐的RLHF技术。

## 为什么我们关注GRPO（群体相对策略优化）？

实现RLHF的技术有很多，但本课程重点介绍GRPO，因为它代表了语言模型的强化学习领域的一项重大进展。

让我们先简要了解一下其他两种流行的RLHF技术：

- 近端策略优化（PPO）
- 直接偏好优化（DPO）

近端策略优化（PPO）是最早的高效RLHF技术之一。它采用策略梯度方法，依据独立的奖励模型所提供的反馈来更新策略。

直接偏好优化（DPO）是后来发展出的、更为简化的技术，它不需要独立的奖励模型，而是直接使用偏好数据来训练。本质上，DPO将问题转化为一个分类任务：判断回复应该被采纳还是被拒绝。

> [!TIP]
> DPO 和 PPO 本身都是基于不同设计目的的复杂的强化学习算法，我们不会在本课程中深入讲解。如果你有兴趣进一步了解，可以查阅以下资料:
>
> - [近端策略优化](https://huggingface.co/docs/trl/main/en/ppo_trainer)
> - [直接偏好优化](https://huggingface.co/docs/trl/main/en/dpo_trainer)

区别于DPO和PPO，GRPO将相似的样本归为一组，并以组为单位进行比较。相比其他方法，这种基于组合的算法能够提供更稳定的梯度和更好的收敛特性。

GRPO不像DPO那样直接使用偏好数据，而是通过从模型或者函数得来的奖励信号来比较相似样本组。

GRPO在如何获取奖励信号方面非常灵活 - 它可以像PPO那样使用奖励模型，但并非绝对需要。这是因为GRPO可以从任何可以评估回复质量的函数或者模型中获取奖励信号。

例如，我们可以使用一个计算长度的函数来奖励更简短的回复，用一个数学求解器来验证解答的正确性，或者用一个判断事实准确性的函数奖励事实准确性更高的回复。这些灵活性使得GRPO在处理各类不同的对齐任务时，展现了极强的通用性。

---

恭喜你完成了模块1！你现在已经对强化学习以及其在塑造未来大语言模型的关键作用有了扎实的认知。你掌握了强化学习的基本概念，理解了它为什么被用于大语言模型，并初步认识了该领域的一个关键算法GRPO。

在下一个模块中，我们将动手实践，深入研读DeepSeek R1的论文，看看这些概念是如何在实际中应用的。

## 章末小测试

### 1. 强化学习的核心组成部分有哪些？

<Question
    choices={[
        {
            text: "智能体，环境，行动，奖励和策略",
            explain: "正确！这些都是强化学习系统的基本组成部分。",
            correct: true
        },
        {
            text: "模型，数据，损失函数和优化器",
            explain: "这些与有监督学习的关联更紧密。"
        },
        {
            text: "输入，输出和隐藏层",
            explain: "这些是神经网络结构的组成部分，并非强化学习独有。"
        }
    ]}
/>

### 2. 什么是使用RLHF训练语言模型的主要优势?

<Question
    choices={[
        {
            text: "它能让模型更好地与人类偏好和价值观对齐。",
            explain: "正确！RLHF使用人类反馈来引导模型表现出更有用、无害且符合期望的行为。",
            correct: true
        },
        {
            text: "它让模型更快地生成文本",
            explain: "RLHF的主要目的不是提升生成速度。"
        },
        {
            text: "它减少模型的内存占用。",
            explain: "RLHF并不侧重模型的效率或内存的优化。"
        }
    ]}
/>

### 3. 在大语言模型的语境下，强化学习中的“行动“是指什么？

<Question
    choices={[
        {
            text: "生成单词或者在对话中选择回复",
            explain: "正确！对于大语言模型来说，“行动”具体指文本生成的决策。",
            correct: true
        },
        {
            text: "更新模型权重",
            explain: "这是训练过程的一部分，不是在强化学习概念下的“行动”"
        },
        {
            text: "处理输入词元",
            explain: "这是模型的内部运算过程，不是在强化学习概念中的“行动”"
        }
    ]}
/>

### 4. 在使用强化学习训练语言模型时，奖励的作用是什么？

<Question
    choices={[
        {
            text: "根据模型生成的回复在多大程度上符合期望行为提供反馈信号",
            explain: "正确！奖励会引导模型生成更有用、真实和恰当的回复。",
            correct: true
        },
        {
            text: "衡量模型的词库大小",
            explain: "奖励并非用于评估词库知识"
        },
        {
            text: "决定模型训练速度",
            explain: "奖励提供的是回复质量的反馈，不是训练效率。"
        }
    ]}
/>

### 5. 在大语言模型的语境下，强化学习中的“奖励”是指什么？

<Question
    choices={[
        {
            text: "一个衡量回答质量的数值分数",
            explain: "正确！奖励提供的是回答质量的反馈，能够指导模型学习期望的行为模式",
            correct: true
        },
        {
            text: "一个生成回复的函数",
            explain: "奖励是对回复质量的反馈，不是生成过程本身。"
        },
        {
            text: "一个评估回复质量的模型",
            explain: "奖励是对回复质量的反馈，并非一个评估模型。"
        }
    ]}
/>
