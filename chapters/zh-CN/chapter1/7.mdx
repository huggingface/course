# 编码器-解码器模型 [[编码器-解码器模型]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

编码器-解码器模型（也称为序列到序列模型）同时使用 Transformer 架构的编码器和解码器两个部分。在每个阶段，编码器的注意力层可以访问输入句子中的所有单词，而解码器的注意力层只能访问位于输入中将要预测单词前面的单词。

这些模型的预训练可以使用训练编码器或解码器模型的方式来完成，但通常会更加复杂。例如， [T5](https://huggingface.co/t5-base) 通过用单个掩码特殊词替换随机文本范围（可能包含多个词）进行预训练，然后目标是预测被遮盖单词原始的文本。

序列到序列模型最适合于围绕根据给定输入生成新句子的任务，如摘要、翻译或生成性问答。

该系列模型的典型代表有：

- [BART](https://huggingface.co/transformers/model_doc/bart) 
- [mBART](https://huggingface.co/transformers/model_doc/mbart) 
- [Marian](https://huggingface.co/transformers/model_doc/marian) 
- [T5](https://huggingface.co/transformers/model_doc/t5) 
