<FrameworkSwitchCourse {fw} />

# ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

ğŸ¤— Transformersæä¾›äº†ä¸€ä¸ª **Trainer** ç±»æ¥å¸®åŠ©æ‚¨åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œæ‚¨åªéœ€è¦æ‰§è¡Œå‡ ä¸ªæ­¥éª¤æ¥åˆ›å»º **Trainer** .æœ€éš¾çš„éƒ¨åˆ†å¯èƒ½æ˜¯ä¸º **Trainer.train()**é…ç½®è¿è¡Œç¯å¢ƒï¼Œå› ä¸ºå®ƒåœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦ä¼šéå¸¸æ…¢ã€‚å¦‚æœæ‚¨æ²¡æœ‰è®¾ç½® GPUï¼Œæ‚¨å¯ä»¥è®¿é—®å…è´¹çš„ GPU æˆ– TPU[Google Colab](https://colab.research.google.com/).

ä¸‹é¢çš„ç¤ºä¾‹å‡è®¾æ‚¨å·²ç»æ‰§è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ç¤ºä¾‹ã€‚ä¸‹é¢è¿™æ®µä»£ç ï¼Œæ¦‚æ‹¬äº†æ‚¨éœ€è¦æå‰è¿è¡Œçš„ä»£ç ï¼š

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Training

åœ¨æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ **Trainer** ä¹‹å‰é¦–å…ˆè¦å®šä¹‰ä¸€ä¸ª **TrainingArguments** ç±»ï¼Œå®ƒå°†åŒ…å« **Trainer**ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ‰€æœ‰è¶…å‚æ•°ã€‚æ‚¨å”¯ä¸€å¿…é¡»æä¾›çš„å‚æ•°æ˜¯ä¿å­˜è®­ç»ƒæ¨¡å‹çš„ç›®å½•ï¼Œä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ã€‚å¯¹äºå…¶ä½™çš„å‚æ•°ï¼Œæ‚¨å¯ä»¥ä¿ç•™é»˜è®¤å€¼ï¼Œè¿™å¯¹äºåŸºæœ¬å¾®è°ƒåº”è¯¥éå¸¸æœ‰æ•ˆã€‚

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¯·å°†push_to_hub=Trueæ·»åŠ åˆ°TrainingArgumentsä¹‹ä¸­. æˆ‘ä»¬å°†åœ¨[ç¬¬å››ç« ](/course/chapter4/3)ä¸­è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ã€‚

</Tip>

ç¬¬äºŒæ­¥æ˜¯å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ­£å¦‚åœ¨[ä¹‹å‰çš„ç« èŠ‚](/2_Using Transformers/Introduction)ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **AutoModelForSequenceClassification** ç±»ï¼Œå®ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼š

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œå’Œ[ç¬¬äºŒç« ](/course/chapter2)ä¸ä¸€æ ·çš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–æ­¤é¢„è®­ç»ƒæ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åœ¨å¥å­å¯¹åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨å·²ç»è¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯æ·»åŠ äº†ä¸€ä¸ªé€‚åˆå¥å­åºåˆ—åˆ†ç±»çš„æ–°å¤´éƒ¨ã€‚è­¦å‘Šè¡¨æ˜ä¸€äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºä¸¢å¼ƒçš„é¢„è®­ç»ƒå¤´çš„é‚£äº›ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ–°å¤´çš„é‚£äº›ï¼‰ã€‚æœ€åé¼“åŠ±æ‚¨è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ç°åœ¨è¦åšçš„ã€‚

ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰ä¸€ä¸ª **Trainer** é€šè¿‡å°†ä¹‹å‰æ„é€ çš„æ‰€æœ‰å¯¹è±¡ä¼ é€’ç»™å®ƒâ€”â€”æˆ‘ä»¬çš„**model** ã€**training_args** ï¼Œè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œ**data_collator** ï¼Œå’Œ **tokenizer** ï¼š

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

è¯·æ³¨æ„ï¼Œå½“æ‚¨åœ¨è¿™é‡Œå®Œæˆ**tokenizer**åï¼Œé»˜è®¤ **Trainer**ä½¿ç”¨ çš„**data_collator**ä¼šä½¿ç”¨ä¹‹å‰é¢„å®šä¹‰çš„ **DataCollatorWithPadding** ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨è¿™ä¸ªä¾‹å­ä¸­è·³è¿‡ **data_collator=data_collator**ã€‚åœ¨ç¬¬ 2 èŠ‚ä¸­å‘æ‚¨å±•ç¤ºè¿™éƒ¨åˆ†å¤„ç†ä»ç„¶å¾ˆé‡è¦ï¼

ä¸ºäº†è®©é¢„è®­ç»ƒæ¨¡å‹åœ¨åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨**Trainer**çš„**train()** æ–¹æ³• ï¼š

```py
trainer.train()
```

è¿™å°†å¼€å§‹å¾®è°ƒï¼ˆåœ¨GPUä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰ï¼Œå¹¶æ¯500æ­¥æŠ¥å‘Šä¸€æ¬¡è®­ç»ƒæŸå¤±ã€‚ä½†æ˜¯ï¼Œå®ƒä¸ä¼šå‘Šè¯‰æ‚¨æ¨¡å‹çš„æ€§èƒ½å¦‚ä½•ï¼ˆæˆ–è´¨é‡å¦‚ä½•ï¼‰ã€‚è¿™æ˜¯å› ä¸º:

1. æˆ‘ä»¬æ²¡æœ‰é€šè¿‡å°†**evaluation_strategy**è®¾ç½®ä¸ºâ€œ**steps**â€(åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°çš„æ—¶å€™è¯„ä¼°)æˆ–â€œ**epoch**â€(åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¯„ä¼°)æ¥å‘Šè¯‰**Trainer**åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œè¯„ä¼°ã€‚
2. æˆ‘ä»¬æ²¡æœ‰ä¸º**Trainer**æä¾›ä¸€ä¸ª**compute_metrics()**å‡½æ•°æ¥ç›´æ¥è®¡ç®—æ¨¡å‹çš„å¥½å(å¦åˆ™è¯„ä¼°å°†åªè¾“å‡ºlossï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªéå¸¸ç›´è§‚çš„æ•°å­—)ã€‚


### è¯„ä¼°

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ„å»ºä¸€ä¸ªæœ‰ç”¨çš„ **compute_metrics()** å‡½æ•°å¹¶åœ¨æˆ‘ä»¬ä¸‹æ¬¡è®­ç»ƒæ—¶ä½¿ç”¨å®ƒã€‚è¯¥å‡½æ•°å¿…é¡»é‡‡ç”¨ **EvalPrediction** å¯¹è±¡ï¼ˆå¸¦æœ‰ **predictions** å’Œ **label_ids** å­—æ®µçš„å‚æ•°å…ƒç»„ï¼‰å¹¶å°†è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²åˆ°æµ®ç‚¹æ•°çš„å­—å…¸ï¼ˆå­—ç¬¦ä¸²æ˜¯è¿”å›çš„æŒ‡æ ‡çš„åç§°ï¼Œè€Œæµ®ç‚¹æ•°æ˜¯å®ƒä»¬çš„å€¼ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **Trainer.predict()** å‘½ä»¤æ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼š

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

 **predict()** çš„è¾“å‡ºç»“æœæ˜¯å…·æœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç»„ï¼š **predictions** , **label_ids** ï¼Œ å’Œ **metrics** .è¿™ **metrics** å­—æ®µå°†åªåŒ…å«ä¼ é€’çš„æ•°æ®é›†çš„lossï¼Œä»¥åŠä¸€äº›è¿è¡Œæ—¶é—´ï¼ˆé¢„æµ‹æ‰€éœ€çš„æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å®šä¹‰äº†è‡ªå·±çš„ **compute_metrics()** å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™ **Trainer** ï¼Œè¯¥å­—æ®µè¿˜å°†åŒ…å«**compute_metrics()**çš„ç»“æœã€‚

**predict()** æ–¹æ³•æ˜¯å…·æœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç»„ï¼š **predictions** , **label_ids** ï¼Œ å’Œ **metrics** .è¿™ **metrics** å­—æ®µå°†åªåŒ…å«ä¼ é€’çš„æ•°æ®é›†çš„lossï¼Œä»¥åŠä¸€äº›è¿è¡Œæ—¶é—´ï¼ˆé¢„æµ‹æ‰€éœ€çš„æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å®šä¹‰äº†è‡ªå·±çš„ **compute_metrics()** å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™ **Trainer** ï¼Œè¯¥å­—æ®µè¿˜å°†åŒ…å«**compute_metrics()** çš„ç»“æœã€‚å¦‚ä½ çœ‹åˆ°çš„ï¼Œ **predictions** æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º 408 x 2 çš„äºŒç»´æ•°ç»„ï¼ˆ408 æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ä¸­å…ƒç´ çš„æ•°é‡ï¼‰ã€‚è¿™äº›æ˜¯æˆ‘ä»¬ä¼ é€’ç»™**predict()**çš„æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ çš„ç»“æœ(logits)ï¼ˆæ­£å¦‚ä½ åœ¨[ä¹‹å‰çš„ç« èŠ‚](/course/chapter2)çœ‹åˆ°çš„æƒ…å†µï¼‰ã€‚è¦å°†æˆ‘ä»¬çš„é¢„æµ‹çš„å¯ä»¥ä¸çœŸæ­£çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦åœ¨ç¬¬äºŒä¸ªè½´ä¸Šå–æœ€å¤§å€¼çš„ç´¢å¼•ï¼š

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

ç°åœ¨å»ºç«‹æˆ‘ä»¬çš„ **compute_metric()** å‡½æ•°æ¥è¾ƒä¸ºç›´è§‚åœ°è¯„ä¼°æ¨¡å‹çš„å¥½åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— [Evaluate](https://github.com/huggingface/evaluate/) åº“ä¸­çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åŠ è½½ä¸ MRPC æ•°æ®é›†å…³è”çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡ä½¿ç”¨ **evaluate.load()** å‡½æ•°ã€‚è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª **compute()**æ–¹æ³•æˆ‘ä»¬å¯ä»¥ç”¨æ¥è¿›è¡Œåº¦é‡è®¡ç®—çš„æ–¹æ³•ï¼š

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

æ‚¨è·å¾—çš„ç¡®åˆ‡ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¨¡å‹å¤´çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šå½±å“æœ€ç»ˆå»ºç«‹çš„æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 85.78%ï¼ŒF1 åˆ†æ•°ä¸º 89.97ã€‚è¿™æ˜¯ç”¨äºè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ç»“æœçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚è€Œåœ¨[BERT è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf)ä¸­å±•ç¤ºçš„åŸºç¡€æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚é‚£æ˜¯ **uncased** æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰æ­£åœ¨ä½¿ç”¨ **cased** æ¨¡å‹ï¼Œé€šè¿‡æ”¹è¿›å¾—åˆ°äº†æ›´å¥½çš„ç»“æœã€‚

æœ€åå°†æ‰€æœ‰ä¸œè¥¿æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„ **compute_metrics()** å‡½æ•°ï¼š

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ä¸ºäº†æŸ¥çœ‹æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸçš„å¥½åï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨**compute_metrics()**å‡½æ•°å®šä¹‰ä¸€ä¸ªæ–°çš„ **Trainer** ï¼š

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è®¾ç½®äº†äº†ä¸€ä¸ªæ–°çš„ **TrainingArguments** å®ƒçš„**evaluation_strategy** è®¾ç½®ä¸º **epoch** å¹¶åˆ›å»ºäº†ä¸€ä¸ªæ–°æ¨¡å‹ã€‚å¦‚æœä¸åˆ›å»ºæ–°çš„æ¨¡å‹å°±ç›´æ¥è®­ç»ƒï¼Œå°±åªä¼šç»§ç»­è®­ç»ƒä¹‹å‰æˆ‘ä»¬å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚è¦å¯åŠ¨æ–°çš„è®­ç»ƒè¿è¡Œï¼Œæˆ‘ä»¬æ‰§è¡Œï¼š

```
trainer.train()
```

è¿™ä¸€æ¬¡ï¼Œå®ƒå°†åœ¨è®­ç»ƒlossä¹‹å¤–ï¼Œè¿˜ä¼šè¾“å‡ºæ¯ä¸ª epoch ç»“æŸæ—¶çš„éªŒè¯losså’ŒæŒ‡æ ‡ã€‚åŒæ ·ï¼Œç”±äºæ¨¡å‹çš„éšæœºå¤´éƒ¨åˆå§‹åŒ–ï¼Œæ‚¨è¾¾åˆ°çš„å‡†ç¡®ç‡/F1 åˆ†æ•°å¯èƒ½ä¸æˆ‘ä»¬å‘ç°çš„ç•¥æœ‰ä¸åŒï¼Œä½†å®ƒåº”è¯¥åœ¨åŒä¸€èŒƒå›´å†…ã€‚

è¿™ **Trainer** å°†åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šå¼€ç®±å³ç”¨ï¼Œå¹¶æä¾›è®¸å¤šé€‰é¡¹ï¼Œä¾‹å¦‚æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåœ¨è®­ç»ƒçš„å‚æ•°ä¸­ä½¿ç”¨ **fp16 = True** ï¼‰ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬ 10 ç« è®¨è®ºå®ƒæ”¯æŒçš„æ‰€æœ‰å†…å®¹ã€‚

ä½¿ç”¨**Trainer** APIå¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚å¯¹æœ€å¸¸è§çš„ NLP ä»»åŠ¡æ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹å°†åœ¨ç¬¬ 7 ç« ä¸­ç»™å‡ºï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨çº¯ PyTorch ä¸­æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹!** ä½¿ç”¨æ‚¨åœ¨ç¬¬ 2 èŠ‚ä¸­è¿›è¡Œçš„æ•°æ®å¤„ç†ï¼Œåœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚

</Tip>

