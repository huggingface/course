<FrameworkSwitchCourse {fw} />

# å¤„ç†æ•°æ® [[å¤„ç†æ•°æ®]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}

åœ¨è¿™ä¸€å°èŠ‚ä½ å°†å­¦ä¹  [ç¬¬ä¸€å°èŠ‚](/course/chapter2) ä¸­æåˆ°çš„â€œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰åŠ è½½å¤§å‹æ•°æ®é›†â€ï¼Œä¸‹é¢æ˜¯ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•°æ®åœ¨ PyTorch ä¸Šè®­ç»ƒå¥å­åˆ†ç±»å™¨çš„ä¸€ä¸ªä¾‹å­ï¼š

```python
import torch
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# å’Œä¹‹å‰ä¸€æ ·
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# æ–°å¢éƒ¨åˆ†
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

{:else}

åœ¨è¿™ä¸€å°èŠ‚ä½ å°†å­¦ä¹  [ç¬¬ä¸€å°èŠ‚](/course/chapter2) ä¸­æåˆ°çš„â€œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰åŠ è½½å¤§å‹æ•°æ®é›†â€ï¼Œä¸‹é¢æ˜¯ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•°æ®åœ¨ TensorFlow ä¸Šè®­ç»ƒå¥å­åˆ†ç±»å™¨çš„ä¸€ä¸ªä¾‹å­ï¼š

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# å’Œä¹‹å‰ä¸€æ ·
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# æ–°å¢éƒ¨åˆ†
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

{/if}

ç„¶è€Œä»…ç”¨ä¸¤å¥è¯è®­ç»ƒæ¨¡å‹ä¸ä¼šäº§ç”Ÿå¾ˆå¥½çš„æ•ˆæœï¼Œä½ éœ€è¦å‡†å¤‡ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†æ‰èƒ½å¾—åˆ°æ›´å¥½çš„è®­ç»ƒç»“æœã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»¥ MRPCï¼ˆå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“ï¼‰æ•°æ®é›†ä¸ºä¾‹ï¼Œè¯¥æ•°æ®é›†ç”±å¨å»‰Â·å¤šå…°å’Œå…‹é‡Œæ–¯Â·å¸ƒç½—å…‹ç‰¹åœ¨ [è¿™ç¯‡æ–‡ç« ](https://www.aclweb.org/anthology/I05-5002.pdf) å‘å¸ƒï¼Œç”± 5801 å¯¹å¥å­ç»„æˆï¼Œæ¯ä¸ªå¥å­å¯¹å¸¦æœ‰ä¸€ä¸ªæ ‡ç­¾æ¥æŒ‡ç¤ºå®ƒä»¬æ˜¯å¦ä¸ºåŒä¹‰ï¼ˆå³ä¸¤ä¸ªå¥å­çš„æ„æ€ç›¸åŒï¼‰ã€‚åœ¨æœ¬ç« é€‰æ‹©è¯¥æ•°æ®é›†çš„åŸå› æ˜¯å®ƒçš„æ•°æ®ä½“é‡å°ï¼Œå®¹æ˜“å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

## ä»æ¨¡å‹ä¸­å¿ƒï¼ˆHubï¼‰åŠ è½½æ•°æ®é›† [[ä»æ¨¡å‹ä¸­å¿ƒï¼ˆHubï¼‰åŠ è½½æ•°æ®é›†]]

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸ä»…ä»…åŒ…å«æ¨¡å‹ï¼Œè¿˜æœ‰è®¸å¤šåˆ«çš„è¯­è¨€çš„æ•°æ®é›†ã€‚è®¿é—® [Datasets](https://huggingface.co/datasets) çš„é“¾æ¥å³å¯è¿›è¡Œæµè§ˆã€‚æˆ‘ä»¬å»ºè®®ä½ åœ¨å®Œæˆæœ¬èŠ‚çš„å­¦ä¹ åé˜…è¯»ä¸€ä¸‹ [åŠ è½½å’Œå¤„ç†æ–°çš„æ•°æ®é›†](https://huggingface.co/docs/datasets/loading) è¿™ç¯‡æ–‡ç« ï¼Œè¿™ä¼šè®©ä½ å¯¹ huggingface çš„æ•°æ®é›†ç†è§£æ›´åŠ æ¸…æ™°ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ MRPC æ•°æ®é›†ä¸­çš„ [GLUE åŸºå‡†æµ‹è¯•æ•°æ®é›†](https://gluebenchmark.com) ä½œä¸ºæˆ‘ä»¬è®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ï¼Œå®ƒæ˜¯æ„æˆ MRPC æ•°æ®é›†çš„ 10 ä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼Œä½œä¸ºä¸€ä¸ªç”¨äºè¡¡é‡æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ 10 ä¸ªä¸åŒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­æ€§èƒ½çš„å­¦æœ¯åŸºå‡†ã€‚

ğŸ¤— Datasets åº“æä¾›äº†ä¸€æ¡éå¸¸ä¾¿æ·çš„å‘½ä»¤ï¼Œå¯ä»¥åœ¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸Šä¸‹è½½å’Œç¼“å­˜æ•°æ®é›†ã€‚ä½ å¯ä»¥ä»¥ä¸‹ä»£ç ä¸‹è½½ MRPC æ•°æ®é›†ï¼š
<Tip>

âš ï¸ **è­¦å‘Š** ç¡®ä¿ä½ å·²ç»è¿è¡Œ `pip install datasets` å®‰è£…äº† `datasets`ã€‚ç„¶åï¼Œå†ç»§ç»­ä¸‹é¢çš„åŠ è½½ MRPC æ•°æ®é›†å’Œæ‰“å°å‡ºæ¥æŸ¥çœ‹å…¶å†…å®¹ã€‚

</Tip> 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

ç°åœ¨æˆ‘ä»¬è·å¾—äº†ä¸€ä¸ª `DatasetDict` å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æ¯ä¸€ä¸ªé›†åˆéƒ½åŒ…å« 4 ä¸ªåˆ—ï¼ˆ `sentence1` ï¼Œ `sentence2` ï¼Œ `label` å’Œ `idx` ï¼‰ä»¥åŠä¸€ä¸ªä»£è¡¨è¡Œæ•°çš„å˜é‡ï¼ˆæ¯ä¸ªé›†åˆä¸­çš„è¡Œçš„ä¸ªæ•°ï¼‰ã€‚è¿è¡Œç»“æœæ˜¾ç¤ºè¯¥è®­ç»ƒé›†ä¸­æœ‰ 3668 å¯¹å¥å­ï¼ŒéªŒè¯é›†ä¸­æœ‰ 408 å¯¹ï¼Œæµ‹è¯•é›†ä¸­æœ‰ 1725 å¯¹ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥å‘½ä»¤ä¼šä¸‹è½½æ•°æ®é›†å¹¶ç¼“å­˜åˆ° `~/.cache/huggingface/datasets` ã€‚å›æƒ³åœ¨ç¬¬ 2 ç« ä¸­æˆ‘ä»¬å­¦åˆ°è¿‡ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® `HF_HOME` ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜çš„æ–‡ä»¶å¤¹ã€‚

æˆ‘ä»¬å¯ä»¥è®¿é—®è¯¥æ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ª `raw_train_dataset` å¯¹è±¡ï¼Œä¾‹å¦‚ä½¿ç”¨å­—å…¸ï¼š

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python
{
    "idx": 0,
    "label": 1,
    "sentence1": 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
    "sentence2": 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
}
```

ç°åœ¨å¯ä»¥çœ‹åˆ°æ ‡ç­¾å·²ç»æ˜¯æ•´æ•°äº†ï¼Œå› æ­¤ä¸éœ€è¦å¯¹æ ‡ç­¾åšä»»ä½•é¢„å¤„ç†ã€‚å¦‚æœæƒ³è¦çŸ¥é“ä¸åŒæ•°å­—å¯¹åº”æ ‡ç­¾çš„å®é™…å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ `raw_train_dataset` çš„ `features` ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬æ¯åˆ—çš„ç±»å‹ï¼š

```py
raw_train_dataset.features
```

```python
{
    "sentence1": Value(dtype="string", id=None),
    "sentence2": Value(dtype="string", id=None),
    "label": ClassLabel(
        num_classes=2, names=["not_equivalent", "equivalent"], names_file=None, id=None
    ),
    "idx": Value(dtype="int32", id=None),
}
```

ä¸Šé¢çš„ä¾‹å­ä¸­çš„ `Labelï¼ˆæ ‡ç­¾ï¼‰` æ˜¯ä¸€ç§ `ClassLabelï¼ˆåˆ†ç±»æ ‡ç­¾ï¼‰` ï¼Œä¹Ÿå°±æ˜¯ä½¿ç”¨æ•´æ•°å»ºç«‹èµ·ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„å…³ç³»ã€‚ `0` å¯¹åº”äº `not_equivalentï¼ˆéåŒä¹‰ï¼‰` ï¼Œ `1` å¯¹åº”äº `equivalentï¼ˆåŒä¹‰ï¼‰` ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** æŸ¥çœ‹è®­ç»ƒé›†çš„ç¬¬ 15 è¡Œå…ƒç´ å’ŒéªŒè¯é›†çš„ 87 è¡Œå…ƒç´ ã€‚ä»–ä»¬çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆï¼Ÿ

</Tip>

## é¢„å¤„ç†æ•°æ®é›† [[é¢„å¤„ç†æ•°æ®é›†]]

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ä¸ºäº†é¢„å¤„ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚åœ¨ [ç¬¬äºŒç« ](/course/chapter2) æˆ‘ä»¬å·²ç»å­¦ä¹ è¿‡ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ª Tokenizer å®Œæˆçš„ï¼Œæˆ‘ä»¬å¯ä»¥å‘ Tokenizer è¾“å…¥ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªå¥å­åˆ—è¡¨ã€‚ä»¥ä¸‹ä»£ç è¡¨ç¤ºå¯¹æ¯å¯¹å¥å­ä¸­çš„æ‰€æœ‰ç¬¬ä¸€å¥å’Œæ‰€æœ‰ç¬¬äºŒå¥è¿›è¡Œ tokenizeï¼š

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ä¸è¿‡åœ¨å°†ä¸¤å¥è¯ä¼ é€’ç»™æ¨¡å‹ï¼Œé¢„æµ‹è¿™ä¸¤å¥è¯æ˜¯å¦æ˜¯åŒä¹‰ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç»™è¿™ä¸¤å¥è¯ä¾æ¬¡è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†ã€‚Tokenizer ä¸ä»…ä»…å¯ä»¥è¾“å…¥å•ä¸ªå¥å­ï¼Œè¿˜å¯ä»¥è¾“å…¥ä¸€ç»„å¥å­ï¼Œå¹¶æŒ‰ç…§ BERT æ¨¡å‹æ‰€éœ€è¦çš„è¾“å…¥è¿›è¡Œå¤„ç†ï¼š

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python
{
    "input_ids": [
        101,
        2023,
        2003,
        1996,
        2034,
        6251,
        1012,
        102,
        2023,
        2003,
        1996,
        2117,
        2028,
        1012,
        102,
    ],
    "token_type_ids": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
    "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
}
```

æˆ‘ä»¬åœ¨ [ç¬¬äºŒç« ](/course/chapter2) è®¨è®ºäº† `è¾“å…¥è¯id(input_ids)` å’Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` ï¼Œä½†å°šæœªè®¨è®º `tokenç±»å‹ID(token_type_ids)` ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œ `tokenç±»å‹ID(token_type_ids)` çš„ä½œç”¨å°±æ˜¯å‘Šè¯‰æ¨¡å‹è¾“å…¥çš„å“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬ä¸€å¥ï¼Œå“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬äºŒå¥ã€‚

<Tip>

âœï¸ ** è¯•è¯•çœ‹ï¼** é€‰å–è®­ç»ƒé›†ä¸­çš„ç¬¬ 15 ä¸ªå…ƒç´ ï¼Œå°†ä¸¤å¥è¯åˆ†åˆ«è¿›è¡Œtokenizationã€‚ç»“æœå’Œä¸Šæ–¹çš„ä¾‹å­æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

</Tip>

å¦‚æœå°† `input_ids` ä¸­çš„ id è½¬æ¢å›æ–‡å­—ï¼š

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

å°†å¾—åˆ°ï¼š

```python
[
    "[CLS]",
    "this",
    "is",
    "the",
    "first",
    "sentence",
    ".",
    "[SEP]",
    "this",
    "is",
    "the",
    "second",
    "one",
    ".",
    "[SEP]",
]
```

æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°æ¨¡å‹éœ€è¦è¾“å…¥çš„å½¢å¼æ˜¯ `[CLS] sentence1 [SEP] sentence2 [SEP]` ã€‚æ‰€ä»¥å½“æœ‰ä¸¤å¥è¯çš„æ—¶å€™ï¼Œ `tokenç±»å‹ID(token_type_ids)` çš„å€¼æ˜¯ï¼š

```python
[
    "[CLS]",
    "this",
    "is",
    "the",
    "first",
    "sentence",
    ".",
    "[SEP]",
    "this",
    "is",
    "the",
    "second",
    "one",
    ".",
    "[SEP]",
]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

ç°åœ¨è¾“å…¥ä¸­ `[CLS] sentence1 [SEP]` å®ƒä»¬çš„ `token_type_ids` å‡ä¸º `0` ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ä¾‹å¦‚ `sentence2 [SEP]` ï¼Œæ‰€æœ‰çš„ `token_type_ids` å‡ä¸º `1` ã€‚

è¯·æ³¨æ„ï¼Œå¦‚æœé€‰æ‹©å…¶ä»–çš„ checkpointï¼Œä¸ä¸€å®šå…·æœ‰ `token_type_ids` ï¼Œæ¯”å¦‚ï¼ŒDistilBERT æ¨¡å‹å°±ä¸ä¼šè¿”å›ã€‚åªæœ‰å½“ tokenizer åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨è¿‡è¿™ä¸€å±‚ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹åœ¨æ„å»ºæ—¶éœ€è¦å®ƒä»¬æ—¶ï¼Œæ‰ä¼šè¿”å› `token_type_ids` ã€‚

åœ¨è¿™é‡Œï¼ŒBERT ä½¿ç”¨äº†å¸¦æœ‰ `token_type_ids` çš„é¢„è®­ç»ƒ tokenizerï¼Œé™¤äº†æˆ‘ä»¬åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­è®¨è®ºçš„æ©ç è¯­è¨€å»ºæ¨¡ï¼Œè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„åº”ç”¨ç±»å‹ç§°ä¸ºâ€œä¸‹ä¸€å¥é¢„æµ‹â€ã€‚è¿™ä¸ªä»»åŠ¡çš„ç›®æ ‡æ˜¯å¯¹å¥å­å¯¹ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚


åœ¨ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¼šç»™æ¨¡å‹è¾“å…¥æˆå¯¹çš„å¥å­ï¼ˆå¸¦æœ‰éšæœºé®ç½©çš„ tokenï¼‰ï¼Œå¹¶è¦æ±‚é¢„æµ‹ç¬¬äºŒä¸ªå¥å­æ˜¯å¦ç´§è·Ÿç¬¬ä¸€ä¸ªå¥å­ã€‚ä¸ºäº†ä½¿ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ•°æ®é›†ä¸­ä¸€æœ‰ä¸€åŠå¥å­å¯¹ä¸­çš„å¥å­åœ¨åŸå§‹æ–‡æ¡£ä¸­é¡ºåºæ’åˆ—ï¼Œå¦ä¸€åŠå¥å­å¯¹ä¸­çš„ä¸¤ä¸ªå¥å­æ¥è‡ªä¸¤ä¸ªä¸åŒçš„æ–‡æ¡£ã€‚

ä¸€èˆ¬æ¥è¯´æ— éœ€è¦æ‹…å¿ƒåœ¨ä½ çš„è¾“å…¥ä¸­æ˜¯å¦éœ€è¦æœ‰ `token_type_ids` ã€‚åªè¦ä½ ä½¿ç”¨ç›¸åŒçš„ checkpoint çš„ Tokenizer å’Œæ¨¡å‹ï¼ŒTokenizer å°±ä¼šçŸ¥é“å‘æ¨¡å‹æä¾›ä»€ä¹ˆï¼Œä¸€åˆ‡éƒ½ä¼šé¡ºåˆ©è¿›è¡Œã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº† Tokenizer å¦‚ä½•å¤„ç†ä¸€å¯¹å¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼šå°±åƒåœ¨ [ç¬¬äºŒç« ](/course/chapter2) ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç»™ Tokenizer æä¾›ä¸€å¯¹å¥å­ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å®ƒç¬¬ä¸€ä¸ªå¥å­çš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ç¬¬äºŒä¸ªå¥å­çš„åˆ—è¡¨ã€‚è¿™ä¹Ÿä¸æˆ‘ä»¬åœ¨ [ç¬¬äºŒç« ](/course/chapter2) ä¸­çœ‹åˆ°çš„å¡«å……å’Œæˆªæ–­é€‰é¡¹å…¼å®¹ã€‚å› æ­¤é¢„å¤„ç†è®­ç»ƒæ•°æ®é›†çš„ä¸€ç§æ–¹æ³•æ˜¯ï¼š

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

è¿™ç§æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†æœ‰ä¸€ä¸ªç¼ºç‚¹æ˜¯å®ƒè¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼ˆå­—å…¸çš„é”®æ˜¯ `è¾“å…¥è¯id(input_ids)` ï¼Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` å’Œ `tokenç±»å‹ID(token_type_ids)` ï¼Œå­—å…¸çš„å€¼æ˜¯é”®æ‰€å¯¹åº”å€¼çš„åˆ—è¡¨ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è½¬æ¢è¿‡ç¨‹ä¸­è¦æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥å­˜å‚¨æ•´ä¸ªæ•°æ®é›†æ‰ä¸ä¼šå‡ºé”™ã€‚ä¸è¿‡æ¥è‡ªğŸ¤— Datasets åº“ä¸­çš„æ•°æ®é›†æ˜¯ä»¥ [Apache Arrow](https://arrow.apache.org) æ ¼å¼å­˜å‚¨åœ¨ç£ç›˜ä¸Šçš„ï¼Œå› æ­¤ä½ åªéœ€å°†æ¥ä¸‹æ¥è¦ç”¨çš„æ•°æ®åŠ è½½åœ¨å†…å­˜ä¸­ï¼Œè€Œä¸æ˜¯åŠ è½½æ•´ä¸ªæ•°æ®é›†ï¼Œè¿™å¯¹å†…å­˜å®¹é‡çš„éœ€æ±‚æ¯”è¾ƒå‹å¥½ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ [Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) æ–¹æ³•å°†æ•°æ®ä¿å­˜ä¸º dataset æ ¼å¼ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦åšæ›´å¤šçš„é¢„å¤„ç†è€Œä¸ä»…ä»…æ˜¯ tokenization å®ƒè¿˜æ”¯æŒäº†ä¸€äº›é¢å¤–çš„è‡ªå®šä¹‰çš„æ–¹æ³•ã€‚ `map()` æ–¹æ³•çš„å·¥ä½œåŸç†æ˜¯ä½¿ç”¨ä¸€ä¸ªå‡½æ•°å¤„ç†æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå¯¹è¾“å…¥è¿›è¡Œ tokenize çš„å‡½æ•°ï¼š

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

è¯¥å‡½æ•°æ¥æ”¶ä¸€ä¸ªå­—å…¸ï¼ˆä¸ dataset çš„é¡¹ç±»ä¼¼ï¼‰å¹¶è¿”å›ä¸€ä¸ªåŒ…å« `è¾“å…¥è¯id(input_ids)` ï¼Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` å’Œ `token_type_ids` é”®çš„æ–°å­—å…¸ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœ `example` å­—å…¸æ‰€å¯¹åº”çš„å€¼åŒ…å«å¤šä¸ªå¥å­ï¼ˆæ¯ä¸ªé”®ä½œä¸ºä¸€ä¸ªå¥å­åˆ—è¡¨ï¼‰ï¼Œé‚£ä¹ˆå®ƒä¾ç„¶å¯ä»¥è¿è¡Œï¼Œå°±åƒå‰é¢çš„ä¾‹å­ä¸€æ ·ï¼Œ `tokenizer` å¯ä»¥å¤„ç†æˆå¯¹çš„å¥å­åˆ—è¡¨ï¼Œè¿™æ ·çš„è¯æˆ‘ä»¬å¯ä»¥åœ¨è°ƒç”¨ `map()` æ—¶ä½¿ç”¨è¯¥é€‰é¡¹ `batched=True` ï¼Œè¿™å°†æ˜¾è‘—åŠ å¿«å¤„ç†çš„é€Ÿåº¦ã€‚ `tokenizer` æ¥è‡ª [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers) åº“ï¼Œç”± Rust ç¼–å†™è€Œæˆã€‚å½“ä¸€æ¬¡ç»™å®ƒå¾ˆå¤šè¾“å…¥æ—¶ï¼Œè¿™ä¸ª `tokenizer` å¯ä»¥å¤„ç†åœ°éå¸¸å¿«ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æš‚æ—¶åœ¨ `tokenize_function` ä¸­çœç•¥äº† padding å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºå°†æ‰€æœ‰çš„æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦æœ‰äº›æµªè´¹ã€‚ä¸€ä¸ªæ›´å¥½çš„åšæ³•æ˜¯ï¼šåœ¨æ„å»º batch çš„æ—¶å€™ã€‚è¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……åˆ°æ¯ä¸ª batch ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥é•¿åº¦ä¸ç¨³å®šæ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›ï¼

ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ä¸€æ¬¡æ€§ `tokenize_function` å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨è°ƒç”¨ `map` æ—¶ä½¿ç”¨äº† `batch =True` ï¼Œè¿™æ ·å‡½æ•°å°±å¯ä»¥åŒæ—¶å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯åˆ†åˆ«å¤„ç†æ¯ä¸ªå…ƒç´ ï¼Œè¿™æ ·å¯ä»¥æ›´å¿«è¿›è¡Œé¢„å¤„ç†ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ tokenization å‡½æ•°å¤„ç†æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨è°ƒç”¨ map æ—¶ä½¿ç”¨äº† `batched=True` ï¼Œå› æ­¤è¯¥å‡½æ•°ä¼šä¸€æ¬¡æ€§å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯å•ç‹¬å¤„ç†æ¯ä¸ªå…ƒç´ ã€‚è¿™æ ·å¯ä»¥å®ç°æ›´å¿«çš„é¢„å¤„ç†ã€‚

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ğŸ¤—Datasets åº“è¿›è¡Œè¿™ç§å¤„ç†çš„æ–¹å¼æ˜¯å‘æ•°æ®é›†æ·»åŠ æ–°çš„å­—æ®µï¼Œæ¯ä¸ªå­—æ®µå¯¹åº”é¢„å¤„ç†å‡½æ•°è¿”å›çš„å­—å…¸ä¸­çš„æ¯ä¸ªé”®ï¼š

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

åœ¨ä½¿ç”¨é¢„å¤„ç†å‡½æ•° `map()` æ—¶ï¼Œç”šè‡³å¯ä»¥é€šè¿‡ä¼ é€’ `num_proc` å‚æ•°å¹¶è¡Œå¤„ç†ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰è¿™æ ·åšï¼Œå› ä¸ºåœ¨è¿™ä¸ªä¾‹å­ä¸­ğŸ¤— Tokenizers åº“å·²ç»ä½¿ç”¨å¤šçº¿ç¨‹æ¥æ›´å¿«åœ°å¯¹æ ·æœ¬ tokenizeï¼Œä½†æ˜¯å¦‚æœæ²¡æœ‰ä½¿ç”¨è¯¥åº“æ”¯æŒçš„å¿«é€Ÿ tokenizerï¼Œä½¿ç”¨ `num_proc` å¯èƒ½ä¼šåŠ å¿«é¢„å¤„ç†ã€‚

æˆ‘ä»¬çš„ `tokenize_function` è¿”å›åŒ…å« `è¾“å…¥è¯id(input_ids)` ï¼Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` å’Œ `token_type_ids` é”®çš„å­—å…¸ï¼Œè¿™ä¸‰ä¸ªå­—æ®µè¢«æ·»åŠ åˆ°æ•°æ®é›†çš„ä¸‰ä¸ªé›†åˆé‡Œï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼‰ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœé¢„å¤„ç†å‡½æ•° `map()` ä¸ºç°æœ‰é”®è¿”å›ä¸€ä¸ªæ–°å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `map()` å‡½æ•°è¿”å›çš„æ–°å€¼ä¿®æ”¹ç°æœ‰çš„å­—æ®µã€‚

æˆ‘ä»¬æœ€åéœ€è¦åšçš„æ˜¯å°†æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°è¯¥ batch ä¸­æœ€é•¿å…ƒç´ çš„é•¿åº¦ï¼Œè¿™ç§æŠ€æœ¯è¢«ç§°ä¸ºåŠ¨æ€å¡«å……ã€‚

## åŠ¨æ€å¡«å…… [[åŠ¨æ€å¡«å……]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}

è´Ÿè´£åœ¨æ‰¹å¤„ç†ä¸­å°†æ•°æ®æ•´ç†ä¸ºä¸€ä¸ª batch çš„å‡½æ•°ç§°ä¸º `collate å‡½æ•°` ã€‚è¿™æ˜¯ä¸€ä¸ªå¯ä»¥åœ¨æ„å»º `DataLoader` æ—¶ä¼ é€’çš„ä¸€ä¸ªå‚æ•°ï¼Œé»˜è®¤æ˜¯ä¸€ä¸ªå°†ä½ çš„æ•°æ®é›†è½¬æ¢ä¸º PyTorch å¼ é‡å¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥çš„å‡½æ•°ï¼ˆå¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œåˆ™ä¼šä½¿ç”¨é€’å½’è¿›è¡Œæ‹¼æ¥ï¼‰ã€‚è¿™åœ¨æœ¬ä¾‹å­ä¸­ä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¾“å…¥çš„å¤§å°å¯èƒ½æ˜¯ä¸ç›¸åŒçš„ã€‚æˆ‘ä»¬ç‰¹æ„æ¨è¿Ÿäº†å¡«å……çš„æ—¶é—´ï¼Œåªåœ¨æ¯ä¸ª batch ä¸Šè¿›è¡Œå¿…è¦çš„å¡«å……ï¼Œä»¥é¿å…å‡ºç°æœ‰å¤§é‡å¡«å……çš„è¿‡é•¿è¾“å…¥ã€‚è¿™å°†å¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œéœ€è¦æ³¨æ„ä¸€ä¸ªé—®é¢˜â€”â€”TPU å–œæ¬¢å›ºå®šçš„å½¢çŠ¶ï¼Œå³ä½¿è¿™éœ€è¦é¢å¤–å¡«å……å¾ˆå¤šæ— ç”¨çš„ tokenã€‚

{:else}

è´Ÿè´£åœ¨æ‰¹å¤„ç†ä¸­å°†æ•°æ®æ•´ç†ä¸ºä¸€ä¸ª batch çš„å‡½æ•°ç§°ä¸º `collate å‡½æ•°` ã€‚é»˜è®¤çš„æ‹¼åˆå‡½æ•°åªä¼šå°†ä½ çš„æ ·æœ¬è½¬æ¢ä¸º `tf.Tensor` å¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥ï¼ˆå¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œåˆ™ä¼šä½¿ç”¨é€’å½’è¿›è¡Œæ‹¼æ¥ï¼‰ã€‚è¿™åœ¨æœ¬ä¾‹ä¸­æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¾“å…¥ä¸æ˜¯éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚æˆ‘ä»¬ç‰¹æ„æ¨è¿Ÿäº†å¡«å……æ—¶é—´ï¼Œåªåœ¨æ¯ä¸ª batch ä¸Šè¿›è¡Œå¡«å……ï¼Œä»¥é¿å…æœ‰å¤ªå¤šå¡«å……çš„è¿‡é•¿çš„è¾“å…¥ã€‚è¿™å°†å¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œéœ€è¦æ³¨æ„ä¸€ä¸ªé—®é¢˜â€”â€”TPU å–œæ¬¢å›ºå®šçš„å½¢çŠ¶ï¼Œå³ä½¿è¿™éœ€è¦é¢å¤–å¡«å……å¾ˆå¤šæ— ç”¨çš„ tokenã€‚

{/if}

ä¸ºäº†è§£å†³å¥å­é•¿åº¦ä¸ç»Ÿä¸€çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰ä¸€ä¸ª collate å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¼šå°†æ¯ä¸ª batch å¥å­å¡«å……åˆ°æ­£ç¡®çš„é•¿åº¦ã€‚å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤—transformer åº“é€šè¿‡ `DataCollatorWithPadding` ä¸ºæˆ‘ä»¬æä¾›äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°ã€‚å½“ä½ å®ä¾‹åŒ–å®ƒæ—¶ï¼Œå®ƒéœ€è¦ä¸€ä¸ª tokenizer ï¼ˆç”¨æ¥çŸ¥é“ä½¿ç”¨å“ªç§å¡«å…… token ä»¥åŠæ¨¡å‹æœŸæœ›åœ¨è¾“å…¥çš„å·¦è¾¹å¡«å……è¿˜æ˜¯å³è¾¹å¡«å……ï¼‰ï¼Œç„¶åå®ƒä¼šè‡ªåŠ¨å®Œæˆæ‰€æœ‰éœ€è¦çš„æ“ä½œï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

{/if}

ä¸ºäº†æµ‹è¯•è¿™ä¸ªæ–°ä¸œè¥¿ï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­æŠ½å–å‡ ä¸ªæ ·æœ¬ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ é™¤åˆ— `idx` ï¼Œ `sentence1` å’Œ `sentence2` ï¼Œå› ä¸ºä¸éœ€è¦å®ƒä»¬ï¼Œè€Œä¸”åˆ é™¤åŒ…å«å­—ç¬¦ä¸²çš„åˆ—ï¼ˆæˆ‘ä»¬ä¸èƒ½ç”¨å­—ç¬¦ä¸²åˆ›å»ºå¼ é‡ï¼‰ï¼Œç„¶åæŸ¥çœ‹ä¸€ä¸ª batch ä¸­æ¯ä¸ªæ¡ç›®çš„é•¿åº¦ï¼š

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python
[50, 59, 47, 67, 59, 50, 62, 32]
```

ä¸å‡ºæ‰€æ–™ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸åŒé•¿åº¦çš„æ ·æœ¬ï¼Œä» 32 åˆ° 67ã€‚åŠ¨æ€å¡«å……æ„å‘³ç€è¿™ä¸ª batch éƒ½åº”è¯¥å¡«å……åˆ°é•¿åº¦ä¸º 67ï¼Œè¿™æ˜¯è¿™ä¸ª batch ä¸­çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ²¡æœ‰åŠ¨æ€å¡«å……ï¼Œæ‰€æœ‰çš„æ ·æœ¬éƒ½å¿…é¡»å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æ¨¡å‹å¯ä»¥æ¥å—çš„æœ€å¤§é•¿åº¦ã€‚è®©æˆ‘ä»¬å†æ¬¡æ£€æŸ¥ `data_collator` æ˜¯å¦æ­£ç¡®åœ°åŠ¨æ€å¡«å……äº†è¿™æ‰¹æ ·æœ¬ï¼š

```py:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python
{
    "attention_mask": TensorShape([8, 67]),
    "input_ids": TensorShape([8, 67]),
    "token_type_ids": TensorShape([8, 67]),
    "labels": TensorShape([8]),
}
```

{:else}

```python
{
    "attention_mask": torch.Size([8, 67]),
    "input_ids": torch.Size([8, 67]),
    "token_type_ids": torch.Size([8, 67]),
    "labels": torch.Size([8]),
}
```

çœ‹èµ·æ¥ä¸é”™ï¼ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ä»åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºäº†æ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚

{/if}

<Tip>

âœï¸ ** è¯•è¯•çœ‹ï¼** åœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¤åˆ»ä¸Šè¿°é¢„å¤„ç†ã€‚å®ƒæœ‰ç‚¹ä¸åŒï¼Œå› ä¸ºå®ƒæ˜¯ç”±å•å¥è€Œä¸æ˜¯æˆå¯¹çš„å¥å­ç»„æˆçš„ï¼Œä½†æ˜¯æˆ‘ä»¬æ‰€åšçš„å…¶ä»–äº‹æƒ…çœ‹èµ·æ¥åº”è¯¥æ˜¯ä¸€æ ·çš„ã€‚å¦ä¸€ä¸ªè¿›é˜¶çš„æŒ‘æˆ˜æ˜¯å°è¯•ç¼–å†™ä¸€ä¸ªå¯ç”¨äºä»»ä½• GLUE ä»»åŠ¡çš„é¢„å¤„ç†å‡½æ•°ã€‚

</Tip>

{#if fw === 'tf'}

ç°åœ¨æˆ‘ä»¬æœ‰äº† dataset å’Œ data collatorï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ data collator æ‰¹é‡åœ°å¤„ç† datasetã€‚æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨åŠ è½½æ‰¹æ¬¡å¹¶è¿›è¡Œæ•´åˆï¼Œä½†è¿™éœ€è¦å¤§é‡å·¥ä½œï¼Œæ€§èƒ½ä¹Ÿæœ‰å¯èƒ½ä¸å¥½ã€‚ç›¸åï¼Œæœ‰ä¸€ä¸ªç®€å•çš„æ–¹æ³•ä¸ºè¿™ä¸ªé—®é¢˜æä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼š `to_tf_dataset()` ã€‚å®ƒå°†æŠŠä½ çš„æ•°æ®é›†åŒ…è£…ä¸€ä¸ª `tf.data.Dataset` ç±»ä¸­ï¼Œè¿™ä¸ªæ–¹æ³•å¸¦æœ‰ä¸€ä¸ªå¯é€‰çš„ data collator åŠŸèƒ½ã€‚ `tf.data.Dataset` æ˜¯ TensorFlow çš„æœ¬åœ°æ ¼å¼ï¼ŒKeras å¯ä»¥ç›´æ¥ç”¨å®ƒæ¥è¿›è¡Œ `model.fit()` ï¼Œå› æ­¤è¿™ç§æ–¹æ³•ä¼šç«‹å³å°†ğŸ¤— Dataset è½¬æ¢ä¸ºå¯ç”¨äºè®­ç»ƒçš„æ ¼å¼ã€‚è®©æˆ‘ä»¬ç”¨æˆ‘ä»¬çš„æ•°æ®é›†æ¼”ç¤ºä¸€ä¸‹è¿™ä¸ªæ–¹æ³•ï¼

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

å°±æ˜¯è¿™æ ·ï¼æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›æ•°æ®é›†å¸¦å…¥ä¸‹ä¸€èŠ‚ï¼Œåœ¨ç»è¿‡æ‰€æœ‰è‰°è‹¦çš„æ•°æ®é¢„å¤„ç†å·¥ä½œä¹‹åï¼Œè®­ç»ƒå°†å˜å¾—éå¸¸ç®€å•å’Œæ„‰å¿«ã€‚

{/if}
