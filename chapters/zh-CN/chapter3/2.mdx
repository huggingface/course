<FrameworkSwitchCourse {fw} />

# å¤„ç†æ•°æ®

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
è¿™ä¸€å°èŠ‚å­¦ä¹ [ç¬¬ä¸€å°èŠ‚](/course/chapter2)ä¸­æåˆ°çš„â€œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å¤§å‹æ•°æ®é›†â€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•°æ®åœ¨PyTorchä¸Šè®­ç»ƒå¥å­åˆ†ç±»å™¨çš„ä¸€ä¸ªä¾‹å­ï¼š

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
è¿™ä¸€å°èŠ‚å­¦ä¹ [ç¬¬ä¸€å°èŠ‚](/course/chapter2)ä¸­æåˆ°çš„â€œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å¤§å‹æ•°æ®é›†â€ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•°æ®åœ¨TensorFlowä¸Šè®­ç»ƒå¥å­åˆ†ç±»å™¨çš„ä¸€ä¸ªä¾‹å­ï¼š

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

å½“ç„¶ï¼Œä»…ä»…ç”¨ä¸¤å¥è¯è®­ç»ƒæ¨¡å‹ä¸ä¼šäº§ç”Ÿå¾ˆå¥½çš„æ•ˆæœã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œæ‚¨éœ€è¦å‡†å¤‡ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨MRPCï¼ˆå¾®è½¯ç ”ç©¶é‡Šä¹‰è¯­æ–™åº“ï¼‰æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ï¼Œè¯¥æ•°æ®é›†ç”±å¨å»‰Â·å¤šå…°å’Œå…‹é‡Œæ–¯Â·å¸ƒç½—å…‹ç‰¹åœ¨[è¿™ç¯‡æ–‡ç« ](https://www.aclweb.org/anthology/I05-5002.pdf)å‘å¸ƒã€‚è¯¥æ•°æ®é›†ç”±5801å¯¹å¥å­ç»„æˆï¼Œæ¯ä¸ªå¥å­å¯¹å¸¦æœ‰ä¸€ä¸ªæ ‡ç­¾ï¼ŒæŒ‡ç¤ºå®ƒä»¬æ˜¯å¦ä¸ºåŒä¹‰ï¼ˆå³ï¼Œå¦‚æœä¸¤ä¸ªå¥å­çš„æ„æ€ç›¸åŒï¼‰ã€‚æˆ‘ä»¬åœ¨æœ¬ç« ä¸­é€‰æ‹©äº†å®ƒï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå°æ•°æ®é›†ï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“å¯¹å®ƒè¿›è¡Œè®­ç»ƒã€‚

### ä»æ¨¡å‹ä¸­å¿ƒï¼ˆHubï¼‰åŠ è½½æ•°æ®é›†

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸åªæ˜¯åŒ…å«æ¨¡å‹ï¼›å®ƒä¹Ÿæœ‰è®¸å¤šä¸åŒè¯­è¨€çš„å¤šä¸ªæ•°æ®é›†ã€‚ç‚¹å‡»[æ•°æ®é›†](https://huggingface.co/datasets)çš„é“¾æ¥å³å¯è¿›è¡Œæµè§ˆã€‚æˆ‘ä»¬å»ºè®®æ‚¨åœ¨é˜…è¯»æœ¬èŠ‚åé˜…è¯»ä¸€ä¸‹[åŠ è½½å’Œå¤„ç†æ–°çš„æ•°æ®é›†](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)è¿™ç¯‡æ–‡ç« ï¼Œè¿™ä¼šè®©æ‚¨å¯¹huggingfaceçš„darasetsæ›´åŠ æ¸…æ™°ã€‚ä½†ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨MRPCæ•°æ®é›†ä¸­çš„[GLUE åŸºå‡†æµ‹è¯•æ•°æ®é›†](https://gluebenchmark.com/)ï¼Œå®ƒæ˜¯æ„æˆMRPCæ•°æ®é›†çš„10ä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼Œè¿™æ˜¯ä¸€ä¸ªå­¦æœ¯åŸºå‡†ï¼Œç”¨äºè¡¡é‡æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨10ä¸ªä¸åŒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

ğŸ¤— Datasetsåº“æä¾›äº†ä¸€ä¸ªéå¸¸ä¾¿æ·çš„å‘½ä»¤ï¼Œå¯ä»¥åœ¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸Šä¸‹è½½å’Œç¼“å­˜æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹çš„ä»£ç ä¸‹è½½MRPCæ•°æ®é›†ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ª**DatasetDict**å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æ¯ä¸€ä¸ªé›†åˆéƒ½åŒ…å«å‡ ä¸ªåˆ—(**sentence1**, **sentence2**, **label**, and **idx**)ä»¥åŠä¸€ä¸ªä»£è¡¨è¡Œæ•°çš„å˜é‡ï¼Œå³æ¯ä¸ªé›†åˆä¸­çš„è¡Œçš„ä¸ªæ•°ï¼ˆå› æ­¤ï¼Œè®­ç»ƒé›†ä¸­æœ‰3668å¯¹å¥å­ï¼ŒéªŒè¯é›†ä¸­æœ‰408å¯¹ï¼Œæµ‹è¯•é›†ä¸­æœ‰1725å¯¹ï¼‰ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤å‘½ä»¤åœ¨ä¸‹è½½æ•°æ®é›†å¹¶ç¼“å­˜åˆ° **~/.cache/huggingface/dataset**. å›æƒ³ä¸€ä¸‹ç¬¬2ç« ï¼Œæ‚¨å¯ä»¥é€šè¿‡è®¾ç½®**HF_HOME**ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜çš„æ–‡ä»¶å¤¹ã€‚

æˆ‘ä»¬å¯ä»¥è®¿é—®æˆ‘ä»¬æ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ª**raw_train_dataset**å¯¹è±¡ï¼Œå¦‚ä½¿ç”¨å­—å…¸ï¼š

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ ‡ç­¾å·²ç»æ˜¯æ•´æ•°äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å¯¹æ ‡ç­¾åšä»»ä½•é¢„å¤„ç†ã€‚è¦çŸ¥é“å“ªä¸ªæ•°å­—å¯¹åº”äºå“ªä¸ªæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹**raw_train_dataset**çš„**features**. è¿™å°†å‘Šè¯‰æˆ‘ä»¬æ¯åˆ—çš„ç±»å‹ï¼š

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¹‹ä¸­,**Labelï¼ˆæ ‡ç­¾ï¼‰** æ˜¯ä¸€ç§**ClassLabelï¼ˆåˆ†ç±»æ ‡ç­¾ï¼‰**ï¼Œä½¿ç”¨æ•´æ•°å»ºç«‹èµ·åˆ°ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„å…³ç³»ã€‚**0**å¯¹åº”äº**not_equivalent**ï¼Œ**1**å¯¹åº”äº**equivalent**ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** æŸ¥çœ‹è®­ç»ƒé›†çš„ç¬¬15è¡Œå…ƒç´ å’ŒéªŒè¯é›†çš„87è¡Œå…ƒç´ ã€‚ä»–ä»¬çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆï¼Ÿ

</Tip>

### é¢„å¤„ç†æ•°æ®é›†

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ä¸ºäº†é¢„å¤„ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚æ­£å¦‚ä½ åœ¨[ç¬¬äºŒç« ](/course/chapter2)ä¸Šçœ‹åˆ°çš„é‚£æ ·

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ç„¶è€Œï¼Œåœ¨ä¸¤å¥è¯ä¼ é€’ç»™æ¨¡å‹ï¼Œé¢„æµ‹è¿™ä¸¤å¥è¯æ˜¯å¦æ˜¯åŒä¹‰ä¹‹å‰ã€‚æˆ‘ä»¬éœ€è¦è¿™ä¸¤å¥è¯ä¾æ¬¡è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†ã€‚å¹¸è¿çš„æ˜¯ï¼Œæ ‡è®°å™¨ä¸ä»…ä»…å¯ä»¥è¾“å…¥å•ä¸ªå¥å­è¿˜å¯ä»¥è¾“å…¥ä¸€ç»„å¥å­ï¼Œå¹¶æŒ‰ç…§æˆ‘ä»¬çš„BERTæ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥è¿›è¡Œå¤„ç†ï¼š

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

æˆ‘ä»¬åœ¨[ç¬¬äºŒç« ](/course/chapter2) è®¨è®ºäº†**è¾“å…¥è¯id(input_ids)** å’Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** ï¼Œä½†æˆ‘ä»¬åœ¨é‚£ä¸ªæ—¶å€™æ²¡æœ‰è®¨è®º**ç±»å‹æ ‡è®°ID(token_type_ids)**ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ**ç±»å‹æ ‡è®°ID(token_type_ids)**çš„ä½œç”¨å°±æ˜¯å‘Šè¯‰æ¨¡å‹è¾“å…¥çš„å“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬ä¸€å¥ï¼Œå“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬äºŒå¥ã€‚

<Tip>

âœï¸ ** è¯•è¯•çœ‹ï¼** é€‰å–è®­ç»ƒé›†ä¸­çš„ç¬¬15ä¸ªå…ƒç´ ï¼Œå°†ä¸¤å¥è¯åˆ†åˆ«æ ‡è®°ä¸ºä¸€å¯¹ã€‚ç»“æœå’Œä¸Šæ–¹çš„ä¾‹å­æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

</Tip>

å¦‚æœæˆ‘ä»¬å°†**input_ids**ä¸­çš„idè½¬æ¢å›æ–‡å­—:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

æˆ‘ä»¬å°†å¾—åˆ°ï¼š

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°æ¨¡å‹éœ€è¦è¾“å…¥çš„å½¢å¼æ˜¯ **[CLS] sentence1 [SEP] sentence2 [SEP]**ã€‚å› æ­¤ï¼Œå½“æœ‰ä¸¤å¥è¯çš„æ—¶å€™ã€‚**ç±»å‹æ ‡è®°ID(token_type_ids)** çš„å€¼æ˜¯ï¼š

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

å¦‚æ‚¨æ‰€è§ï¼Œè¾“å…¥ä¸­ **[CLS] sentence1 [SEP]** å®ƒä»¬çš„ç±»å‹æ ‡è®°IDå‡ä¸º**0**ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ï¼Œå¯¹åº”äº**sentence2 [SEP]**ï¼Œæ‰€æœ‰çš„ç±»å‹æ ‡è®°IDå‡ä¸º**1**.

è¯·æ³¨æ„ï¼Œå¦‚æœé€‰æ‹©å…¶ä»–çš„æ£€æŸ¥ç‚¹ï¼Œåˆ™ä¸ä¸€å®šå…·æœ‰**ç±»å‹æ ‡è®°ID(token_type_ids)**ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨DistilBERTæ¨¡å‹ï¼Œå°±ä¸ä¼šè¿”å›å®ƒä»¬ï¼‰ã€‚åªæœ‰å½“å®ƒåœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨è¿‡è¿™ä¸€å±‚ï¼Œæ¨¡å‹åœ¨æ„å»ºæ—¶ä¾èµ–å®ƒä»¬ï¼Œæ‰ä¼šè¿”å›å®ƒä»¬ã€‚

ç”¨ç±»å‹æ ‡è®°IDå¯¹BERTè¿›è¡Œé¢„è®­ç»ƒ,å¹¶ä¸”ä½¿ç”¨[ç¬¬ä¸€ç« ](/course/chapter1)çš„é®ç½©è¯­è¨€æ¨¡å‹ï¼Œè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„åº”ç”¨ç±»å‹ï¼Œå«åšä¸‹ä¸€å¥é¢„æµ‹. è¿™é¡¹ä»»åŠ¡çš„ç›®æ ‡æ˜¯å»ºç«‹æˆå¯¹å¥å­ä¹‹é—´å…³ç³»çš„æ¨¡å‹ã€‚

åœ¨ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¼šç»™æ¨¡å‹è¾“å…¥æˆå¯¹çš„å¥å­ï¼ˆå¸¦æœ‰éšæœºé®ç½©çš„æ ‡è®°ï¼‰ï¼Œå¹¶è¢«è¦æ±‚é¢„æµ‹ç¬¬äºŒä¸ªå¥å­æ˜¯å¦ç´§è·Ÿç¬¬ä¸€ä¸ªå¥å­ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ•°æ®é›†ä¸­ä¸€åŠçš„ä¸¤ä¸ªå¥å­åœ¨åŸå§‹æ–‡æ¡£ä¸­æŒ¨åœ¨ä¸€èµ·ï¼Œå¦ä¸€åŠçš„ä¸¤ä¸ªå¥å­æ¥è‡ªä¸¤ä¸ªä¸åŒçš„æ–‡æ¡£ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œä½ ä¸éœ€è¦æ‹…å¿ƒæ˜¯å¦æœ‰**ç±»å‹æ ‡è®°ID(token_type_ids)**ã€‚åœ¨æ‚¨çš„æ ‡è¾“å…¥ä¸­ï¼šåªè¦æ‚¨å¯¹æ ‡è®°å™¨å’Œæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ£€æŸ¥ç‚¹ï¼Œä¸€åˆ‡éƒ½ä¼šå¾ˆå¥½ï¼Œå› ä¸ºæ ‡è®°å™¨çŸ¥é“å‘å…¶æ¨¡å‹æä¾›ä»€ä¹ˆã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†æ ‡è®°å™¨å¦‚ä½•å¤„ç†ä¸€å¯¹å¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå¤„ç†ï¼šå¦‚[ä¹‹å‰çš„ç« èŠ‚](/course/chapter2)ï¼Œæˆ‘ä»¬å¯ä»¥ç»™æ ‡è®°å™¨æä¾›ä¸€ç»„å¥å­ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å®ƒç¬¬ä¸€ä¸ªå¥å­çš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ç¬¬äºŒä¸ªå¥å­çš„åˆ—è¡¨ã€‚è¿™ä¹Ÿä¸æˆ‘ä»¬åœ¨[ç¬¬äºŒç« ](/course/chapter2)ä¸­çœ‹åˆ°çš„å¡«å……å’Œæˆªæ–­é€‰é¡¹å…¼å®¹. å› æ­¤ï¼Œé¢„å¤„ç†è®­ç»ƒæ•°æ®é›†çš„ä¸€ç§æ–¹æ³•æ˜¯ï¼š

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

è¿™å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒçš„ç¼ºç‚¹æ˜¯è¿”å›å­—å…¸ï¼ˆå­—å…¸çš„é”®æ˜¯**è¾“å…¥è¯id(input_ids)** ï¼Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** å’Œ **ç±»å‹æ ‡è®°ID(token_type_ids)**ï¼Œå­—å…¸çš„å€¼æ˜¯é”®æ‰€å¯¹åº”å€¼çš„åˆ—è¡¨ï¼‰ã€‚è€Œä¸”åªæœ‰å½“æ‚¨åœ¨è½¬æ¢è¿‡ç¨‹ä¸­æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥å­˜å‚¨æ•´ä¸ªæ•°æ®é›†æ—¶æ‰ä¸ä¼šå‡ºé”™ï¼ˆè€ŒğŸ¤—æ•°æ®é›†åº“ä¸­çš„æ•°æ®é›†æ˜¯ä»¥[Apache Arrow](https://arrow.apache.org/)æ–‡ä»¶å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼Œå› æ­¤æ‚¨åªéœ€å°†æ¥ä¸‹æ¥è¦ç”¨çš„æ•°æ®åŠ è½½åœ¨å†…å­˜ä¸­ï¼Œå› æ­¤ä¼šå¯¹å†…å­˜å®¹é‡çš„éœ€æ±‚è¦ä½ä¸€äº›ï¼‰ã€‚

ä¸ºäº†å°†æ•°æ®ä¿å­˜ä¸ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map)æ–¹æ³•ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦åšæ›´å¤šçš„é¢„å¤„ç†è€Œä¸ä»…ä»…æ˜¯æ ‡è®°åŒ–ï¼Œé‚£ä¹ˆè¿™ä¹Ÿç»™äº†æˆ‘ä»¬ä¸€äº›é¢å¤–çš„è‡ªå®šä¹‰çš„æ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•çš„å·¥ä½œåŸç†æ˜¯åœ¨æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ä¸Šåº”ç”¨ä¸€ä¸ªå‡½æ•°ï¼Œå› æ­¤è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ ‡è®°è¾“å…¥çš„å‡½æ•°ï¼š

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

æ­¤å‡½æ•°çš„è¾“å…¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼ˆä¸æ•°æ®é›†çš„é¡¹ç±»ä¼¼ï¼‰ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«**è¾“å…¥è¯id(input_ids)** ï¼Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** å’Œ **ç±»å‹æ ‡è®°ID(token_type_ids)** é”®çš„æ–°å­—å…¸ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœåƒä¸Šé¢çš„**ç¤ºä¾‹**ä¸€æ ·ï¼Œå¦‚æœé”®æ‰€å¯¹åº”çš„å€¼åŒ…å«å¤šä¸ªå¥å­ï¼ˆæ¯ä¸ªé”®ä½œä¸ºä¸€ä¸ªå¥å­åˆ—è¡¨ï¼‰ï¼Œé‚£ä¹ˆå®ƒä¾ç„¶å¯ä»¥å·¥ä½œï¼Œå°±åƒå‰é¢çš„ä¾‹å­ä¸€æ ·æ ‡è®°å™¨å¯ä»¥å¤„ç†æˆå¯¹çš„å¥å­åˆ—è¡¨ã€‚è¿™æ ·çš„è¯æˆ‘ä»¬å¯ä»¥åœ¨è°ƒç”¨**map()**ä½¿ç”¨è¯¥é€‰é¡¹ **batched=True** ï¼Œè¿™å°†æ˜¾è‘—åŠ å¿«æ ‡è®°ä¸æ ‡è®°çš„é€Ÿåº¦ã€‚è¿™ä¸ª**æ ‡è®°å™¨**æ¥è‡ª[ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)åº“ç”±Rustç¼–å†™è€Œæˆã€‚å½“æˆ‘ä»¬ä¸€æ¬¡ç»™å®ƒå¤§é‡çš„è¾“å…¥æ—¶ï¼Œè¿™ä¸ªæ ‡è®°å™¨å¯ä»¥éå¸¸å¿«ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨åœ¨æ ‡è®°å‡½æ•°ä¸­çœç•¥äº†**padding**å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºåœ¨æ ‡è®°çš„æ—¶å€™å°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦çš„æ•ˆç‡ä¸é«˜ã€‚ä¸€ä¸ªæ›´å¥½çš„åšæ³•ï¼šåœ¨æ„å»ºæ‰¹å¤„ç†æ—¶å¡«å……æ ·æœ¬æ›´å¥½ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……åˆ°è¯¥æ‰¹å¤„ç†ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥é•¿åº¦å˜åŒ–å¾ˆå¤§æ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›!

ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰€æœ‰æ•°æ®é›†ä¸ŠåŒæ—¶åº”ç”¨æ ‡è®°å‡½æ•°ã€‚æˆ‘ä»¬åœ¨è°ƒç”¨**map**æ—¶ä½¿ç”¨äº†**batch =True**ï¼Œè¿™æ ·å‡½æ•°å°±å¯ä»¥åŒæ—¶åº”ç”¨åˆ°æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ä¸Šï¼Œè€Œä¸æ˜¯åˆ†åˆ«åº”ç”¨åˆ°æ¯ä¸ªå…ƒç´ ä¸Šã€‚è¿™å°†ä½¿æˆ‘ä»¬çš„é¢„å¤„ç†å¿«è®¸å¤š

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ğŸ¤—Datasetsåº“åº”ç”¨è¿™ç§å¤„ç†çš„æ–¹å¼æ˜¯å‘æ•°æ®é›†æ·»åŠ æ–°çš„å­—æ®µï¼Œæ¯ä¸ªå­—æ®µå¯¹åº”é¢„å¤„ç†å‡½æ•°è¿”å›çš„å­—å…¸ä¸­çš„æ¯ä¸ªé”®:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

åœ¨ä½¿ç”¨é¢„å¤„ç†å‡½æ•°**map()**æ—¶ï¼Œç”šè‡³å¯ä»¥é€šè¿‡ä¼ é€’**num_proc**å‚æ•°ä½¿ç”¨å¹¶è¡Œå¤„ç†ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰è¿™æ ·åšï¼Œå› ä¸ºğŸ¤—æ ‡è®°å™¨åº“å·²ç»ä½¿ç”¨å¤šä¸ªçº¿ç¨‹æ¥æ›´å¿«åœ°æ ‡è®°æˆ‘ä»¬çš„æ ·æœ¬ï¼Œä½†æ˜¯å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨è¯¥åº“æ”¯æŒçš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œä½¿ç”¨**num_proc**å¯èƒ½ä¼šåŠ å¿«é¢„å¤„ç†ã€‚

æˆ‘ä»¬çš„**æ ‡è®°å‡½æ•°(tokenize_function)**è¿”å›åŒ…å«**è¾“å…¥è¯id(input_ids)** ï¼Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** å’Œ **ç±»å‹æ ‡è®°ID(token_type_ids)** é”®çš„å­—å…¸,æ‰€ä»¥è¿™ä¸‰ä¸ªå­—æ®µè¢«æ·»åŠ åˆ°æ•°æ®é›†çš„æ ‡è®°çš„ç»“æœä¸­ã€‚æ³¨æ„ï¼Œå¦‚æœé¢„å¤„ç†å‡½æ•°**map()**ä¸ºç°æœ‰é”®è¿”å›ä¸€ä¸ªæ–°å€¼ï¼Œé‚£å°†ä¼šä¿®æ”¹åŸæœ‰é”®çš„å€¼ã€‚

æœ€åä¸€ä»¶æˆ‘ä»¬éœ€è¦åšçš„äº‹æƒ…æ˜¯ï¼Œå½“æˆ‘ä»¬ä¸€èµ·æ‰¹å¤„ç†å…ƒç´ æ—¶ï¼Œå°†æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°æœ€é•¿å…ƒç´ çš„é•¿åº¦â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºåŠ¨æ€å¡«å……ã€‚

### åŠ¨æ€å¡«å……

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
è´Ÿè´£åœ¨æ‰¹å¤„ç†ä¸­å°†æ•°æ®æ•´ç†ä¸ºä¸€ä¸ªbatchçš„å‡½æ•°ç§°ä¸º*collateå‡½æ•°*ã€‚å®ƒæ˜¯ä½ å¯ä»¥åœ¨æ„å»º**DataLoader**æ—¶ä¼ é€’çš„ä¸€ä¸ªå‚æ•°ï¼Œé»˜è®¤æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†æŠŠä½ çš„æ•°æ®é›†è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥(å¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œåˆ™ä¼šä½¿ç”¨é€’å½’)ã€‚è¿™åœ¨æˆ‘ä»¬çš„è¿™ä¸ªä¾‹å­ä¸­ä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¾“å…¥ä¸æ˜¯éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚æˆ‘ä»¬æ•…æ„åœ¨ä¹‹åæ¯ä¸ªbatchä¸Šè¿›è¡Œå¡«å……ï¼Œé¿å…æœ‰å¤ªå¤šå¡«å……çš„è¿‡é•¿çš„è¾“å…¥ã€‚è¿™å°†å¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨TPUä¸Šè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜â€”â€”TPUå–œæ¬¢å›ºå®šçš„å½¢çŠ¶ï¼Œå³ä½¿è¿™éœ€è¦é¢å¤–çš„å¡«å……ã€‚

{:else}

è´Ÿè´£åœ¨æ‰¹å¤„ç†ä¸­å°†æ•°æ®æ•´ç†ä¸ºä¸€ä¸ªbatchçš„å‡½æ•°ç§°ä¸º*collateå‡½æ•°*ã€‚å®ƒåªä¼šå°†æ‚¨çš„æ ·æœ¬è½¬æ¢ä¸º tf.Tensorå¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥(å¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œåˆ™ä¼šä½¿ç”¨é€’å½’)ã€‚è¿™åœ¨æˆ‘ä»¬çš„è¿™ä¸ªä¾‹å­ä¸­ä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¾“å…¥ä¸æ˜¯éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚æˆ‘ä»¬æ•…æ„åœ¨ä¹‹åæ¯ä¸ªbatchä¸Šè¿›è¡Œå¡«å……ï¼Œé¿å…æœ‰å¤ªå¤šå¡«å……çš„è¿‡é•¿çš„è¾“å…¥ã€‚è¿™å°†å¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨TPUä¸Šè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜â€”â€”TPUå–œæ¬¢å›ºå®šçš„å½¢çŠ¶ï¼Œå³ä½¿è¿™éœ€è¦é¢å¤–çš„å¡«å……ã€‚

{/if}

ä¸ºäº†è§£å†³å¥å­é•¿åº¦ç»Ÿä¸€çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰ä¸€ä¸ªcollateå‡½æ•°ï¼Œè¯¥å‡½æ•°ä¼šå°†æ¯ä¸ªbatchå¥å­å¡«å……åˆ°æ­£ç¡®çš„é•¿åº¦ã€‚å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤—transformeråº“é€šè¿‡**DataCollatorWithPadding**ä¸ºæˆ‘ä»¬æä¾›äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°ã€‚å½“ä½ å®ä¾‹åŒ–å®ƒæ—¶ï¼Œéœ€è¦ä¸€ä¸ªæ ‡è®°å™¨(ç”¨æ¥çŸ¥é“ä½¿ç”¨å“ªä¸ªè¯æ¥å¡«å……ï¼Œä»¥åŠæ¨¡å‹æœŸæœ›å¡«å……åœ¨å·¦è¾¹è¿˜æ˜¯å³è¾¹)ï¼Œå¹¶å°†åšä½ éœ€è¦çš„ä¸€åˆ‡:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

ä¸ºäº†æµ‹è¯•è¿™ä¸ªæ–°ç©å…·ï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­æŠ½å–å‡ ä¸ªæ ·æœ¬ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬åˆ é™¤åˆ—**idx**, **sentence1**å’Œ**sentence2**ï¼Œå› ä¸ºä¸éœ€è¦å®ƒä»¬ï¼Œå¹¶æŸ¥çœ‹ä¸€ä¸ªbatchä¸­æ¯ä¸ªæ¡ç›®çš„é•¿åº¦:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

æ¯«æ— ç–‘é—®ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸åŒé•¿åº¦çš„æ ·æœ¬ï¼Œä»32åˆ°67ã€‚åŠ¨æ€å¡«å……æ„å‘³ç€è¯¥æ‰¹ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½åº”è¯¥å¡«å……åˆ°é•¿åº¦ä¸º67ï¼Œè¿™æ˜¯è¯¥æ‰¹ä¸­çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ²¡æœ‰åŠ¨æ€å¡«å……ï¼Œæ‰€æœ‰çš„æ ·æœ¬éƒ½å¿…é¡»å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æ¨¡å‹å¯ä»¥æ¥å—çš„æœ€å¤§é•¿åº¦ã€‚è®©æˆ‘ä»¬å†æ¬¡æ£€æŸ¥**data_collator**æ˜¯å¦æ­£ç¡®åœ°åŠ¨æ€å¡«å……äº†è¿™æ‰¹æ ·æœ¬ï¼š

```py:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

çœ‹èµ·æ¥ä¸é”™ï¼ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å°†åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºäº†æ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼

{/if}

<Tip>

âœï¸ ** è¯•è¯•çœ‹ï¼** åœ¨GLUE SST-2æ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†ã€‚å®ƒæœ‰ç‚¹ä¸åŒï¼Œå› ä¸ºå®ƒæ˜¯ç”±å•ä¸ªå¥å­è€Œä¸æ˜¯æˆå¯¹çš„å¥å­ç»„æˆçš„ï¼Œä½†æ˜¯æˆ‘ä»¬æ‰€åšçš„å…¶ä»–äº‹æƒ…çœ‹èµ·æ¥åº”è¯¥æ˜¯ä¸€æ ·çš„ã€‚å¦ä¸€ä¸ªæ›´éš¾çš„æŒ‘æˆ˜ï¼Œè¯·å°è¯•ç¼–å†™ä¸€ä¸ªå¯ç”¨äºä»»ä½•GLUEä»»åŠ¡çš„é¢„å¤„ç†å‡½æ•°ã€‚

</Tip>

{#if fw === 'tf'}

ç°åœ¨æˆ‘ä»¬æœ‰äº†datasetå’Œdata collatorï¼Œæˆ‘ä»¬éœ€è¦å°†datasetæ‰¹é‡åœ°åº”ç”¨data collatorã€‚ æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨åŠ è½½æ‰¹æ¬¡å¹¶æ•´ç†å®ƒä»¬ï¼Œä½†è¿™éœ€è¦å¤§é‡å·¥ä½œï¼Œè€Œä¸”å¯èƒ½æ€§èƒ½ä¹Ÿä¸æ˜¯å¾ˆå¥½ã€‚ ç›¸åï¼Œæœ‰ä¸€ä¸ªç®€å•çš„æ–¹æ³•å¯ä»¥ä¸ºè¿™ä¸ªé—®é¢˜æä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼š`to_tf_dataset()`ã€‚ è¿™å°†åœ¨æ‚¨çš„æ•°æ®é›†ä¸Šè°ƒç”¨ä¸€ä¸ª `tf.data.Dataset`çš„æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•å¸¦æœ‰ä¸€ä¸ªå¯é€‰çš„data collatoråŠŸèƒ½ã€‚ `tf.data.Dataset` æ˜¯ Keras å¯ç”¨äº `model.fit()` çš„åŸç”Ÿ TensorFlow æ ¼å¼ï¼Œå› æ­¤è¿™ç§æ–¹æ³•ä¼šç«‹å³å°†ğŸ¤— Dataset è½¬æ¢ä¸ºå¯ç”¨äºè®­ç»ƒçš„æ ¼å¼ã€‚ è®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šæ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

å°±æ˜¯è¿™æ ·ï¼ æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ•°æ®é›†å¸¦å…¥ä¸‹ä¸€èŠ‚ï¼Œåœ¨ç»è¿‡æ‰€æœ‰è‰°è‹¦çš„æ•°æ®é¢„å¤„ç†å·¥ä½œä¹‹åï¼Œè®­ç»ƒå°†å˜å¾—éå¸¸ç®€å•ã€‚

{/if}
