# ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

ç°åœ¨ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•åœ¨ä¸ä½¿ç”¨`Trainer`ç±»çš„æƒ…å†µä¸‹è·å¾—ä¸ä¸Šä¸€èŠ‚ç›¸åŒçš„ç»“æœã€‚åŒæ ·ï¼Œæˆ‘ä»¬å‡è®¾æ‚¨å·²ç»å­¦ä¹ äº†ç¬¬ 2 èŠ‚ä¸­çš„æ•°æ®å¤„ç†ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ï¼Œæ¶µç›–äº†æ‚¨éœ€è¦çš„æ‰€æœ‰å†…å®¹:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### è®­ç»ƒå‰çš„å‡†å¤‡

åœ¨å®é™…ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€äº›å¯¹è±¡ã€‚ç¬¬ä¸€ä¸ªæ˜¯æˆ‘ä»¬å°†ç”¨äºè¿­ä»£æ‰¹æ¬¡çš„æ•°æ®åŠ è½½å™¨ã€‚æˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„`tokenized_datasets`åšä¸€äº›å¤„ç†ï¼Œæ¥å¤„ç†`Trainer`è‡ªåŠ¨ä¸ºæˆ‘ä»¬åšçš„ä¸€äº›äº‹æƒ…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦:

- åˆ é™¤ä¸æ¨¡å‹ä¸æœŸæœ›çš„å€¼ç›¸å¯¹åº”çš„åˆ—ï¼ˆå¦‚`sentence1`å’Œ`sentence2`åˆ—ï¼‰ã€‚
- å°†åˆ—å`label`é‡å‘½åä¸º`labels`ï¼ˆå› ä¸ºæ¨¡å‹æœŸæœ›å‚æ•°æ˜¯`labels`ï¼‰ã€‚
- è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ï¼Œä½¿å…¶è¿”å› PyTorch å¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ã€‚

é’ˆå¯¹ä¸Šé¢çš„æ¯ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬çš„ `tokenized_datasets` éƒ½æœ‰ä¸€ä¸ªæ–¹æ³•:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åªæœ‰æ¨¡å‹èƒ½å¤Ÿæ¥å—çš„åˆ—:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾å®šä¹‰æ•°æ®åŠ è½½å™¨:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ä¸ºäº†å¿«é€Ÿæ£€éªŒæ•°æ®å¤„ç†ä¸­æ²¡æœ‰é”™è¯¯ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æ£€éªŒå…¶ä¸­çš„ä¸€ä¸ªæ‰¹æ¬¡:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

è¯·æ³¨æ„ï¼Œå®é™…çš„å½¢çŠ¶å¯èƒ½ä¸æ‚¨ç•¥æœ‰ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºè®­ç»ƒæ•°æ®åŠ è½½å™¨è®¾ç½®äº†`shuffle=True`ï¼Œå¹¶ä¸”æ¨¡å‹ä¼šå°†å¥å­å¡«å……åˆ°`batch`ä¸­çš„æœ€å¤§é•¿åº¦ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œå…¨å®Œæˆäº†æ•°æ®é¢„å¤„ç†ï¼ˆå¯¹äºä»»ä½• ML ä»ä¸šè€…æ¥è¯´éƒ½æ˜¯ä¸€ä¸ªä»¤äººæ»¡æ„ä½†éš¾ä»¥å®ç°çš„ç›®æ ‡ï¼‰ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘æ¨¡å‹ã€‚æˆ‘ä»¬å®Œå…¨åƒåœ¨ä¸Šä¸€èŠ‚ä¸­æ‰€åšçš„é‚£æ ·å®ä¾‹åŒ–å®ƒ:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```
ä¸ºäº†ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬å°†`batch`ä¼ é€’ç»™è¿™ä¸ªæ¨¡å‹:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

å½“æˆ‘ä»¬æä¾› `labels` æ—¶ï¼Œ ğŸ¤— Transformers æ¨¡å‹éƒ½å°†è¿”å›è¿™ä¸ª`batch`çš„`loss`ï¼Œæˆ‘ä»¬è¿˜å¾—åˆ°äº† `logits`(`batch`ä¸­çš„æ¯ä¸ªè¾“å…¥æœ‰ä¸¤ä¸ªï¼Œæ‰€ä»¥å¼ é‡å¤§å°ä¸º 8 x 2)ã€‚

æˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯äº†ï¼æˆ‘ä»¬åªæ˜¯ç¼ºå°‘ä¸¤ä»¶äº‹ï¼šä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ç”±äºæˆ‘ä»¬è¯•å›¾è‡ªè¡Œå®ç° `Trainer`çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚`Trainer` ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ `AdamW` , ä¸ `Adam` ç›¸åŒï¼Œä½†åœ¨æƒé‡è¡°å‡æ­£åˆ™åŒ–æ–¹é¢æœ‰æ‰€ä¸åŒ(å‚è§[â€œDecoupled Weight Decay Regularizationâ€](https://arxiv.org/abs/1711.05101)ä½œè€…:Ilya Loshchilov å’Œ Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

æœ€åï¼Œé»˜è®¤ä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åªæ˜¯ä»æœ€å¤§å€¼ (5e-5) åˆ° 0 çš„çº¿æ€§è¡°å‡ã€‚ ä¸ºäº†å®šä¹‰å®ƒï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘ä»¬è®­ç»ƒçš„æ¬¡æ•°ï¼Œå³æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¬¡æ•°(epochs)ä¹˜ä»¥çš„æ•°æ®é‡ï¼ˆè¿™æ˜¯æˆ‘ä»¬æ‰€æœ‰è®­ç»ƒæ•°æ®çš„æ•°é‡ï¼‰ã€‚`Trainer`é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨ä¸‰ä¸ª`epochs`ï¼Œå› æ­¤æˆ‘ä»¬å®šä¹‰è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### è®­ç»ƒå¾ªç¯

æœ€åä¸€ä»¶äº‹ï¼šå¦‚æœæˆ‘ä»¬å¯ä»¥è®¿é—® GPU,æˆ‘ä»¬å°†å¸Œæœ›ä½¿ç”¨ GPU(åœ¨ CPU ä¸Šï¼Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶è€Œä¸æ˜¯å‡ åˆ†é’Ÿ)ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `device`,å®ƒåœ¨GPUå¯ç”¨çš„æƒ…å†µä¸‹æŒ‡å‘GPU æˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„æ¨¡å‹å’Œ`batche`æ”¾åœ¨`device`ä¸Š:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒäº†ï¼ä¸ºäº†äº†è§£è®­ç»ƒä½•æ—¶ç»“æŸï¼Œæˆ‘ä»¬ä½¿ç”¨ `tqdm` åº“,åœ¨è®­ç»ƒæ­¥éª¤æ•°ä¸Šæ·»åŠ äº†ä¸€ä¸ªè¿›åº¦æ¡:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

æ‚¨å¯ä»¥çœ‹åˆ°è®­ç»ƒå¾ªç¯çš„æ ¸å¿ƒä¸ä»‹ç»ä¸­çš„éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬æ²¡æœ‰è¦æ±‚ä»»ä½•æ£€éªŒï¼Œæ‰€ä»¥è¿™ä¸ªè®­ç»ƒå¾ªç¯ä¸ä¼šå‘Šè¯‰æˆ‘ä»¬ä»»ä½•å…³äºæ¨¡å‹ç›®å‰çš„çŠ¶æ€ã€‚æˆ‘ä»¬éœ€è¦ä¸ºæ­¤æ·»åŠ ä¸€ä¸ªè¯„ä¼°å¾ªç¯ã€‚


### è¯„ä¼°å¾ªç¯

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ğŸ¤— Evaluate åº“æä¾›çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å·²ç»äº†è§£äº† `metric.compute()` æ–¹æ³•ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ `add_batch()`æ–¹æ³•è¿›è¡Œé¢„æµ‹å¾ªç¯æ—¶ï¼Œå®é™…ä¸Šè¯¥æŒ‡æ ‡å¯ä»¥ä¸ºæˆ‘ä»¬ç´¯ç§¯æ‰€æœ‰ `batch` çš„ç»“æœã€‚ä¸€æ—¦æˆ‘ä»¬ç´¯ç§¯äº†æ‰€æœ‰ `batch` ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ `metric.compute()` å¾—åˆ°æœ€ç»ˆç»“æœ .ä»¥ä¸‹æ˜¯åœ¨è¯„ä¼°å¾ªç¯ä¸­å®ç°æ‰€æœ‰è¿™äº›çš„æ–¹æ³•:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

åŒæ ·ï¼Œç”±äºæ¨¡å‹å¤´éƒ¨åˆå§‹åŒ–å’Œæ•°æ®æ”¹ç»„çš„éšæœºæ€§ï¼Œæ‚¨çš„ç»“æœä¼šç•¥æœ‰ä¸åŒï¼Œä½†å®ƒä»¬åº”è¯¥åœ¨åŒä¸€ä¸ªèŒƒå›´å†…ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼** ä¿®æ”¹ä¹‹å‰çš„è®­ç»ƒå¾ªç¯ä»¥åœ¨ SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

</Tip>

### Sä½¿ç”¨ğŸ¤— AccelerateåŠ é€Ÿæ‚¨çš„è®­ç»ƒå¾ªç¯

<Youtube id="s7dy8QRgjJ0" />

æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è®­ç»ƒå¾ªç¯åœ¨å•ä¸ª CPU æˆ– GPU ä¸Šè¿è¡Œè‰¯å¥½ã€‚ä½†æ˜¯ä½¿ç”¨[ğŸ¤— Accelerate](https://github.com/huggingface/accelerate)åº“ï¼Œåªéœ€è¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚ä»åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨å¼€å§‹ï¼Œæˆ‘ä»¬çš„æ‰‹åŠ¨è®­ç»ƒå¾ªç¯å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ä»¥ä¸‹æ˜¯å˜åŒ–:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

è¦æ·»åŠ çš„ç¬¬ä¸€è¡Œæ˜¯å¯¼å…¥`Accelerator`ã€‚ç¬¬äºŒè¡Œå®ä¾‹åŒ–ä¸€ä¸ª `Accelerator`å¯¹è±¡ ï¼Œå®ƒå°†æŸ¥çœ‹ç¯å¢ƒå¹¶åˆå§‹åŒ–é€‚å½“çš„åˆ†å¸ƒå¼è®¾ç½®ã€‚ ğŸ¤— Accelerate ä¸ºæ‚¨å¤„ç†æ•°æ®åœ¨è®¾å¤‡é—´çš„ä¼ é€’ï¼Œå› æ­¤æ‚¨å¯ä»¥åˆ é™¤å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šçš„é‚£è¡Œä»£ç ï¼ˆæˆ–è€…ï¼Œå¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä½¿ç”¨ `accelerator.device` ä»£æ›¿ `device` ï¼‰ã€‚

ç„¶åå¤§éƒ¨åˆ†å·¥ä½œä¼šåœ¨å°†æ•°æ®åŠ è½½å™¨ã€æ¨¡å‹å’Œä¼˜åŒ–å™¨å‘é€åˆ°çš„`accelerator.prepare()`ä¸­å®Œæˆã€‚è¿™å°†ä¼šæŠŠè¿™äº›å¯¹è±¡åŒ…è£…åœ¨é€‚å½“çš„å®¹å™¨ä¸­ï¼Œä»¥ç¡®ä¿æ‚¨çš„åˆ†å¸ƒå¼è®­ç»ƒæŒ‰é¢„æœŸå·¥ä½œã€‚è¦è¿›è¡Œçš„å…¶ä½™æ›´æ”¹æ˜¯åˆ é™¤å°†`batch`æ”¾åœ¨ `device` çš„é‚£è¡Œä»£ç ï¼ˆåŒæ ·ï¼Œå¦‚æœæ‚¨æƒ³ä¿ç•™å®ƒï¼Œæ‚¨å¯ä»¥å°†å…¶æ›´æ”¹ä¸ºä½¿ç”¨ `accelerator.device` ) å¹¶å°† `loss.backward()` æ›¿æ¢ä¸º`accelerator.backward(loss)`ã€‚

<Tip>
âš ï¸ ä¸ºäº†ä½¿äº‘ç«¯ TPU æä¾›çš„åŠ é€Ÿå‘æŒ¥æœ€å¤§çš„æ•ˆç›Šï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æ ‡è®°å™¨(tokenizer)çš„ `padding=max_length` å’Œ `max_length` å‚æ•°å°†æ‚¨çš„æ ·æœ¬å¡«å……åˆ°å›ºå®šé•¿åº¦ã€‚
</Tip>

å¦‚æœæ‚¨æƒ³å¤åˆ¶å¹¶ç²˜è´´æ¥ç›´æ¥è¿è¡Œï¼Œä»¥ä¸‹æ˜¯ ğŸ¤— Accelerate çš„å®Œæ•´è®­ç»ƒå¾ªç¯:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

æŠŠè¿™ä¸ªæ”¾åœ¨ `train.py` æ–‡ä»¶ä¸­ï¼Œå¯ä»¥è®©å®ƒåœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè¿è¡Œã€‚è¦åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è¯•ç”¨å®ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:

```bash
accelerate config
```

è¿™å°†è¯¢é—®æ‚¨å‡ ä¸ªé…ç½®çš„é—®é¢˜å¹¶å°†æ‚¨çš„å›ç­”è½¬å‚¨åˆ°æ­¤å‘½ä»¤ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ä¸­:

```
accelerate launch train.py
```

è¿™å°†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

è¿™å°†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒã€‚å¦‚æœæ‚¨æƒ³åœ¨ Notebook ä¸­å°è¯•æ­¤æ“ä½œï¼ˆä¾‹å¦‚ï¼Œåœ¨ Colab ä¸Šä½¿ç”¨ TPU è¿›è¡Œæµ‹è¯•ï¼‰ï¼Œåªéœ€å°†ä»£ç ç²˜è´´åˆ° `training_function()` å¹¶ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œæœ€åä¸€ä¸ªå•å…ƒæ ¼:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

æ‚¨å¯ä»¥åœ¨[ğŸ¤— Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples)æ‰¾åˆ°æ›´å¤šçš„ç¤ºä¾‹ã€‚
