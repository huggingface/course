<FrameworkSwitchCourse {fw} />

# ä½¿ç”¨ Keras å¾®è°ƒä¸€ä¸ªæ¨¡å‹ [[ä½¿ç”¨ Keras å¾®è°ƒä¸€ä¸ªæ¨¡å‹]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3_tf.ipynb"},
]} />

ä¸€æ—¦å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªå‰©ä¸‹æœ€åçš„å‡ ä¸ªæ­¥éª¤æ¥è®­ç»ƒæ¨¡å‹ã€‚ ä½†æ˜¯è¯·æ³¨æ„ï¼Œ`model.fit()` å‘½ä»¤åœ¨ CPU ä¸Šè¿è¡Œä¼šéå¸¸ç¼“æ…¢ã€‚ å¦‚æœä½ æ²¡æœ‰GPUï¼Œä½ å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com)ï¼ˆå›½å†…ç½‘ç»œæ— æ³•ä½¿ç”¨ï¼‰ ä¸Šä½¿ç”¨å…è´¹çš„ GPU æˆ– TPUã€‚

ä¸‹é¢çš„ä»£ç ç¤ºä¾‹å‡è®¾ä½ å·²ç»è¿è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ä»£ç ç¤ºä¾‹ã€‚ ä¸‹é¢æ˜¯åœ¨å¼€å§‹å­¦ä¹ è¿™ä¸€èŠ‚ä¹‹å‰ä½ éœ€è¦è¿è¡Œçš„ä»£ç ï¼š

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

## è®­ç»ƒæ¨¡å‹ [[è®­ç»ƒæ¨¡å‹]]

ä»ğŸ¤— Transformers å¯¼å…¥çš„ TensorFlow æ¨¡å‹å·²ç»æ˜¯ Keras æ¨¡å‹ã€‚ ä¸‹é¢çš„è§†é¢‘æ˜¯å¯¹ Keras çš„ç®€çŸ­ä»‹ç»ã€‚

<Youtube id="rnTGBy2ax1c"/>

è¿™æ„å‘³ç€ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œåªéœ€è¦å¾ˆå°‘çš„å·¥ä½œå°±å¯ä»¥å¼€å§‹å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

<Youtube id="AUozVp78dhk"/>

å’Œ[ç¬¬äºŒç« ](/course/chapter2)ä½¿ç”¨çš„æ–¹æ³•ä¸€æ ·, æˆ‘ä»¬å°†ä½¿ç”¨äºŒåˆ†ç±»çš„ `TFAutoModelForSequenceClassification`ç±»ï¼Œæˆ‘ä»¬å°†æœ‰ä¸¤ä¸ªæ ‡ç­¾: 

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œä¸ [ç¬¬äºŒç« ](/course/chapter2) ä¸åŒçš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–è¿™ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚ è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰å¯¹å¥å­å¯¹çš„åˆ†ç±»è¿›è¡Œé¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒï¼Œå¹¶ä¸”æ’å…¥äº†ä¸€ä¸ªé€‚åˆåºåˆ—åˆ†ç±»çš„æ–° headã€‚ è­¦å‘Šè¡¨æ˜ï¼Œè¿™äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºä¸¢å¼ƒçš„é¢„è®­ç»ƒ head æƒé‡ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼ˆå¯¹åº”äºæ–° head çš„æƒé‡ï¼‰ã€‚ æœ€åå®ƒå»ºè®®ä½ è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ç°åœ¨è¦åšçš„ã€‚

ä¸ºäº†åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸Šè°ƒç”¨ `compile()` æ–¹æ³•ï¼Œç„¶åå°†æˆ‘ä»¬çš„æ•°æ®ä¼ é€’ç»™ `fit()` æ–¹æ³•ã€‚ è¿™å°†å¯åŠ¨å¾®è°ƒè¿‡ç¨‹ï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰å¹¶è¾“å‡ºè®­ç»ƒæŸå¤±ï¼Œä»¥åŠæ¯ä¸ª epoch ç»“æŸæ—¶çš„éªŒè¯æŸå¤±ã€‚

> [!TIP]
> è¯·æ³¨æ„ğŸ¤— Transformers æ¨¡å‹å…·æœ‰å¤§å¤šæ•° Keras æ¨¡å‹æ‰€æ²¡æœ‰çš„ç‰¹æ®Šèƒ½åŠ›â€”â€”å®ƒä»¬å¯ä»¥è‡ªåŠ¨ä½¿ç”¨å†…éƒ¨è®¡ç®—çš„æŸå¤±ã€‚ å¦‚æœä½ æ²¡æœ‰åœ¨ `compile()` ä¸­è®¾ç½®æŸå¤±å‚æ•°ï¼Œå®ƒä»¬å¯ä»¥è‡ªåŠ¨ä½¿ç”¨é€‚å½“çš„æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨å†…éƒ¨è®¡ç®—ã€‚ è¯·æ³¨æ„ï¼Œè¦ä½¿ç”¨å†…éƒ¨æŸå¤±ï¼Œä½ éœ€è¦å°†æ ‡ç­¾ä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ä¼ å…¥æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä½œä¸ºå•ç‹¬çš„æ ‡ç­¾ï¼ˆè¿™æ˜¯åœ¨ Keras æ¨¡å‹ä¸­ä½¿ç”¨æ ‡ç­¾çš„å¸¸è§„æ–¹å¼ï¼‰ã€‚ ä½ å°†åœ¨è¯¾ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†ä¸­çœ‹åˆ°è¿™æ–¹é¢çš„ç¤ºä¾‹ï¼Œæ­£ç¡®å®šä¹‰æŸå¤±å‡½æ•°å¯èƒ½ä¼šæœ‰äº›æ£˜æ‰‹ã€‚ ç„¶è€Œå¯¹äºåºåˆ—åˆ†ç±»æ¥è¯´ï¼Œæ ‡å‡†çš„ Keras æŸå¤±å‡½æ•°æ•ˆæœå¾ˆå¥½ï¼Œå› æ­¤æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨å®ƒã€‚

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

> [!WARNING]
> è¯·æ³¨æ„è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸å¸¸è§çš„é™·é˜±â€”â€”ä½ å¯ä»¥æŠŠæŸå¤±çš„åç§°ä½œä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ä¼ é€’ç»™ Kerasï¼Œä½†é»˜è®¤æƒ…å†µä¸‹ï¼ŒKeras ä¼šå‡è®¾ä½ å·²ç»å¯¹è¾“å‡ºè¿›è¡Œäº† softmaxã€‚ ç„¶è€Œï¼Œè®¸å¤šæ¨¡å‹åœ¨ç»è¿‡ softmax å‡½æ•°ä¹‹å‰è¾“å‡ºçš„æ˜¯è¢«ç§°ä¸º `logits` çš„å€¼ã€‚ æˆ‘ä»¬éœ€è¦å‘Šè¯‰æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å·²ç»ä½¿ç”¨ softmax å‡½æ•°è¿›è¡Œäº†å¤„ç†ï¼Œå”¯ä¸€çš„æ–¹æ³•æ˜¯ä¼ é€’ä¸€ä¸ªæŸå¤±å‡½æ•°å¹¶ä¸”åœ¨å‚æ•°çš„éƒ¨åˆ†å‘Šè¯‰æ¨¡å‹ï¼Œè€Œä¸æ˜¯åªä¼ é€’ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚


## æ”¹å–„è®­ç»ƒçš„æ•ˆæœ [[æ”¹å–„è®­ç»ƒçš„æ•ˆæœ]]

<Youtube id="cpzq6ESSM5c"/>

å¦‚æœä½ å°è¯•ä¸Šè¿°ä»£ç ï¼Œå®ƒçš„ç¡®å¯ä»¥è¿è¡Œï¼Œä½†ä½ ä¼šå‘ç°æŸå¤±ä¸‹é™å¾—å¾ˆæ…¢æˆ–è€…ä¸è§„å¾‹ã€‚ ä¸»è¦åŸå› æ˜¯`å­¦ä¹ ç‡`ã€‚ ä¸æŸå¤±ä¸€æ ·ï¼Œå½“æˆ‘ä»¬æŠŠä¼˜åŒ–å™¨çš„åç§°ä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ç»™ Keras æ—¶ï¼ŒKeras ä¼šåˆå§‹åŒ–è¯¥ä¼˜åŒ–å™¨å…·æœ‰æ‰€æœ‰å‚æ•°çš„é»˜è®¤å€¼ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€‚ ä¸è¿‡æ ¹æ®é•¿æœŸç»éªŒï¼Œæˆ‘ä»¬çŸ¥é“Transformer æ¨¡å‹çš„é€šå¸¸çš„æœ€ä½³å­¦ä¹ ç‡æ¯” Adam çš„é»˜è®¤å€¼ï¼ˆå³1e-3ï¼Œä¹Ÿå†™æˆä¸º 10 çš„ -3 æ¬¡æ–¹ï¼Œæˆ– 0.001ï¼‰ä½å¾—å¤šã€‚ 5e-5ï¼ˆ0.00005ï¼‰ï¼Œæ˜¯ä¸€ä¸ªæ›´å¥½çš„èµ·å§‹ç‚¹ã€‚

é™¤äº†é™ä½å­¦ä¹ ç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ç¬¬äºŒä¸ªæŠ€å·§ï¼šæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ…¢æ…¢é™ä½å­¦ä¹ ç‡ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œä½ æœ‰æ—¶ä¼šçœ‹åˆ°è¿™è¢«ç§°ä¸º å­¦ä¹ ç‡çš„`è¡°å‡ï¼ˆdecayingï¼‰` æˆ– `é€€ç«ï¼ˆannealingï¼‰`ã€‚ åœ¨ Keras ä¸­ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨ `å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆlearning rate schedulerï¼‰`ã€‚ ä¸€ä¸ªå¥½ç”¨çš„è°ƒåº¦å™¨æ˜¯`PolynomialDecay`â€”â€”å°½ç®¡å®ƒçš„åå­—å«`PolynomialDecayï¼ˆå¤šé¡¹å¼è¡°å‡ï¼‰`ï¼Œä½†åœ¨é»˜è®¤è®¾ç½®ä¸‹ï¼Œå®ƒåªæ˜¯ç®€å•å°†å­¦ä¹ ç‡ä»åˆå§‹å€¼çº¿æ€§è¡°å‡åˆ°æœ€ç»ˆå€¼ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ä¸è¿‡ä¸ºäº†æ­£ç¡®ä½¿ç”¨è°ƒåº¦ç¨‹åºï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰å®ƒè®­ç»ƒçš„æ¬¡æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„`num_train_steps`è®¡ç®—å¾—åˆ°ã€‚

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

> [!TIP]
> ğŸ¤— Transformers åº“è¿˜æœ‰ä¸€ä¸ª `create_optimizer()` å‡½æ•°ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå…·æœ‰å­¦ä¹ ç‡è¡°å‡çš„ `AdamW` ä¼˜åŒ–å™¨ã€‚ è¿™æ˜¯ä¸€ä¸ªå¿«æ·çš„æ–¹å¼ï¼Œä½ å°†åœ¨æœ¬è¯¾ç¨‹çš„åç»­éƒ¨åˆ†ä¸­è¯¦ç»†äº†è§£ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰äº†å…¨æ–°çš„ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨å®ƒè¿›è¡Œè®­ç»ƒã€‚ é¦–å…ˆï¼Œè®©æˆ‘ä»¬é‡æ–°åŠ è½½æ¨¡å‹ï¼Œé‡æ–°è®¾ç½®åˆšåˆšè®­ç»ƒæ—¶çš„æƒé‡ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨å¯¹å…¶è¿›è¡Œç¼–è¯‘ï¼š

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¬¡è¿›è¡Œfitï¼š

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

> [!TIP]
> ğŸ’¡ å¦‚æœä½ æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œä½ å¯ä»¥åœ¨ `model.fit()` æ–¹æ³•ä¸­ä¼ é€’ä¸€ä¸ª `PushToHubCallback`ã€‚ æˆ‘ä»¬å°†åœ¨ [ç¬¬å››ç« ](/course/chapter4/3) ä¸­è¿›ä¸€æ­¥äº†è§£è¿™ä¸ªé—®é¢˜ã€‚

## æ¨¡å‹é¢„æµ‹ [[æ¨¡å‹é¢„æµ‹]]

<Youtube id="nx10eh4CoOs"/>


è®­ç»ƒå’Œè§‚å¯ŸæŸå¤±çš„ä¸‹é™éƒ½æ˜¯éå¸¸å¥½ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³ä»è®­ç»ƒåçš„æ¨¡å‹ä¸­å¾—åˆ°è¾“å‡ºï¼Œæˆ–è€…è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼Œæˆ–è€…åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ¨¡å‹ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`predict()` æ–¹æ³•ã€‚ è¿™å°†è¿”å›æ¨¡å‹çš„è¾“å‡ºå¤´çš„`logits`æ•°å€¼ï¼Œæ¯ä¸ªç±»ä¸€ä¸ªã€‚

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ argmax å°†è¿™äº› logits è½¬æ¢ä¸ºæ¨¡å‹çš„ç±»åˆ«é¢„æµ‹ï¼Œæœ€é«˜çš„ logitsï¼Œå¯¹åº”äºæœ€æœ‰å¯èƒ½çš„ç±»åˆ«

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è¿™äº› `preds` æ¥è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼ æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åœ°åŠ è½½ä¸ MRPC æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡ä½¿ç”¨çš„æ˜¯ `evaluate.load()` å‡½æ•°ã€‚ è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è¿›è¡ŒæŒ‡æ ‡çš„è®¡ç®—ï¼š

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ç”±äºæ¨¡å‹å¤´çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜æ¨¡å‹è¾¾åˆ°çš„æŒ‡æ ‡ï¼Œå› æ­¤ä½ å¾—åˆ°çš„æœ€ç»ˆç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º85.78%ï¼ŒF1åˆ†æ•°ä¸º89.97ã€‚ è¿™å°±æ˜¯ç”¨äºè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ä¸Šçš„ç»“æœä¸¤ä¸ªæŒ‡æ ‡ã€‚ [BERT è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf) ä¸­çš„è¡¨æ ¼æŠ¥å‘Šäº†åŸºæœ¬æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚ é‚£æ˜¯ `uncased` æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰ä½¿ç”¨çš„æ˜¯ `cased` æ¨¡å‹ï¼Œè¿™ä¹Ÿå¯ä»¥è§£é‡Šä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šå¾—åˆ°äº†æ›´å¥½çš„ç»“æœã€‚

å…³äºä½¿ç”¨ Keras API è¿›è¡Œå¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚ åœ¨[ç¬¬ä¸ƒç« ](/course/chapter7)å°†ç»™å‡ºå¯¹å¤§å¤šæ•°å¸¸è§ NLP ä»»åŠ¡å¾®è°ƒæˆ–è®­ç»ƒçš„ç¤ºä¾‹ã€‚å¦‚æœä½ æƒ³åœ¨ Keras API ä¸Šç£¨ç»ƒè‡ªå·±çš„æŠ€èƒ½ï¼Œè¯·å°è¯•ä½¿ç¬¬2èŠ‚æ‰€å­¦çš„çš„æ•°æ®å¤„ç†æŠ€å·§åœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚