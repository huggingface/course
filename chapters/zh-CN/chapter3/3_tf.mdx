<FrameworkSwitchCourse {fw} />

# ä½¿ç”¨ Keras å¾®è°ƒä¸€ä¸ªæ¨¡å‹

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},
]} />

å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œæ‚¨åªå‰©ä¸‹æœ€åçš„å‡ ä¸ªæ­¥éª¤æ¥è®­ç»ƒæ¨¡å‹ã€‚ ä½†æ˜¯è¯·æ³¨æ„ï¼Œ`model.fit()` å‘½ä»¤åœ¨ CPU ä¸Šè¿è¡Œä¼šéå¸¸ç¼“æ…¢ã€‚ å¦‚æœæ‚¨æ²¡æœ‰GPUï¼Œåˆ™å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com/) ä¸Šä½¿ç”¨å…è´¹çš„ GPU æˆ– TPU(éœ€è¦æ¢¯å­)ã€‚

è¿™ä¸€èŠ‚çš„ä»£ç ç¤ºä¾‹å‡è®¾æ‚¨å·²ç»æ‰§è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ä»£ç ç¤ºä¾‹ã€‚ ä¸‹é¢ä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ï¼ŒåŒ…å«äº†åœ¨å¼€å§‹å­¦ä¹ è¿™ä¸€èŠ‚ä¹‹å‰æ‚¨éœ€è¦çš„æ‰§è¡Œçš„ä»£ç ï¼š

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### è®­ç»ƒæ¨¡å‹

ä»ğŸ¤— Transformers å¯¼å…¥çš„ TensorFlow æ¨¡å‹å·²ç»æ˜¯ Keras æ¨¡å‹ã€‚ ä¸‹é¢çš„è§†é¢‘æ˜¯å¯¹ Keras çš„ç®€çŸ­ä»‹ç»ã€‚

<Youtube id="rnTGBy2ax1c"/>

è¿™æ„å‘³ç€ï¼Œä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œå°±éœ€è¦å¾ˆå°‘çš„å·¥ä½œå°±å¯ä»¥å¼€å§‹å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

<Youtube id="AUozVp78dhk"/>

å’Œ[ç¬¬äºŒç« ](/course/chapter2)ä½¿ç”¨çš„æ–¹æ³•ä¸€æ ·, æˆ‘ä»¬å°†ä½¿ç”¨äºŒåˆ†ç±»çš„ `TFAutoModelForSequenceClassification`ç±»: 

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

æ‚¨ä¼šæ³¨æ„åˆ°ï¼Œä¸ [ç¬¬äºŒç« ](/course/chapter2) ä¸åŒçš„æ˜¯ï¼Œæ‚¨åœ¨å®ä¾‹åŒ–æ­¤é¢„è®­ç»ƒæ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚ è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰å¯¹å¥å­å¯¹è¿›è¡Œåˆ†ç±»è¿›è¡Œé¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯æ’å…¥äº†ä¸€ä¸ªé€‚åˆåºåˆ—åˆ†ç±»çš„æ–° headã€‚ è­¦å‘Šè¡¨æ˜ä¸€äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºä¸¢å¼ƒçš„é¢„è®­ç»ƒå¤´ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼ˆæ–°å¤´çš„æƒé‡ï¼‰ã€‚ æœ€åé¼“åŠ±æ‚¨è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ç°åœ¨è¦åšçš„ã€‚

è¦åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸Šè°ƒç”¨ `compile()` æ–¹æ³•ï¼Œç„¶åå°†æˆ‘ä»¬çš„æ•°æ®ä¼ é€’ç»™ `fit()` æ–¹æ³•ã€‚ è¿™å°†å¯åŠ¨å¾®è°ƒè¿‡ç¨‹ï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰å¹¶è¾“å‡ºè®­ç»ƒlossï¼Œä»¥åŠæ¯ä¸ª epoch ç»“æŸæ—¶çš„éªŒè¯lossã€‚

<Tip>

è¯·æ³¨æ„ğŸ¤— Transformers æ¨¡å‹å…·æœ‰å¤§å¤šæ•° Keras æ¨¡å‹æ‰€æ²¡æœ‰çš„ç‰¹æ®Šèƒ½åŠ›â€”â€”å®ƒä»¬å¯ä»¥è‡ªåŠ¨ä½¿ç”¨å†…éƒ¨è®¡ç®—çš„lossã€‚ å¦‚æœæ‚¨æ²¡æœ‰åœ¨ `compile()` ä¸­è®¾ç½®æŸå¤±å‡½æ•°ï¼Œä»–ä»¬å°†é»˜è®¤ä½¿ç”¨å†…éƒ¨è®¡ç®—çš„æŸå¤±ã€‚ è¯·æ³¨æ„ï¼Œè¦ä½¿ç”¨å†…éƒ¨æŸå¤±ï¼Œæ‚¨éœ€è¦å°†æ ‡ç­¾ä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ä¼ é€’ï¼Œè€Œä¸æ˜¯ä½œä¸ºå•ç‹¬çš„æ ‡ç­¾ï¼ˆè¿™æ˜¯åœ¨ Keras æ¨¡å‹ä¸­ä½¿ç”¨æ ‡ç­¾çš„æ­£å¸¸æ–¹å¼ï¼‰ã€‚ æ‚¨å°†åœ¨è¯¾ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†ä¸­çœ‹åˆ°è¿™æ–¹é¢çš„ç¤ºä¾‹ï¼Œå…¶ä¸­å®šä¹‰æ­£ç¡®çš„æŸå¤±å‡½æ•°å¯èƒ½å¾ˆæ£˜æ‰‹ã€‚ ç„¶è€Œï¼Œå¯¹äºåºåˆ—åˆ†ç±»ï¼Œæ ‡å‡†çš„ Keras æŸå¤±å‡½æ•°å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œæ‰€ä»¥æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨å®ƒã€‚

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

è¯·æ³¨æ„è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸å¸¸è§çš„é™·é˜±â€”â€”ä½ åªæ˜¯*å¯ä»¥*å°†æŸå¤±çš„åç§°ä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ç»™ Kerasï¼Œä½†é»˜è®¤æƒ…å†µä¸‹ï¼ŒKeras ä¼šå‡è®¾ä½ å·²ç»å¯¹è¾“å‡ºåº”ç”¨äº† softmaxã€‚ ç„¶è€Œï¼Œè®¸å¤šæ¨¡å‹åœ¨åº”ç”¨ softmax ä¹‹å‰å°±è¾“å‡ºï¼Œä¹Ÿç§°ä¸º *logits*ã€‚ æˆ‘ä»¬éœ€è¦å‘Šè¯‰æŸå¤±å‡½æ•°æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦ç»è¿‡äº†softmaxï¼Œå”¯ä¸€çš„æ–¹æ³•æ˜¯ç›´æ¥è°ƒç”¨å®ƒï¼Œè€Œä¸æ˜¯ç”¨å­—ç¬¦ä¸²çš„åç§°ã€‚

</Tip>


### æå‡è®­ç»ƒçš„æ•ˆæœ

<Youtube id="cpzq6ESSM5c"/>

å¦‚æœæ‚¨å°è¯•ä¸Šé¢çš„ä»£ç ï¼Œå®ƒè‚¯å®šä¼šè¿è¡Œï¼Œä½†æ‚¨ä¼šå‘ç°lossåªæ˜¯ç¼“æ…¢æˆ–é›¶æ˜Ÿåœ°ä¸‹é™ã€‚ ä¸»è¦åŸå› æ˜¯*å­¦ä¹ ç‡*ã€‚ ä¸lossä¸€æ ·ï¼Œå½“æˆ‘ä»¬å°†ä¼˜åŒ–å™¨çš„åç§°ä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ç»™ Keras æ—¶ï¼ŒKeras ä¼šåˆå§‹åŒ–è¯¥ä¼˜åŒ–å™¨å…·æœ‰æ‰€æœ‰å‚æ•°çš„é»˜è®¤å€¼ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€‚ ä½†æ˜¯ï¼Œæ ¹æ®é•¿æœŸç»éªŒï¼Œæˆ‘ä»¬çŸ¥é“Transformer æ¨¡å‹æ›´é€‚åˆä½¿ç”¨æ¯” Adam çš„é»˜è®¤å€¼ï¼ˆ1e-3ï¼‰ä¹Ÿå†™æˆä¸º 10 çš„ -3 æ¬¡æ–¹ï¼Œæˆ– 0.001ï¼Œä½å¾—å¤šçš„å­¦ä¹ ç‡ã€‚ 5e-5 (0.00005) æ¯”é»˜è®¤å€¼å¤§çº¦ä½ 20 å€ï¼Œæ˜¯ä¸€ä¸ªæ›´å¥½çš„èµ·ç‚¹ã€‚

é™¤äº†é™ä½å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬è¿˜æœ‰ç¬¬äºŒä¸ªæŠ€å·§ï¼šæˆ‘ä»¬å¯ä»¥æ…¢æ…¢é™ä½å­¦ä¹ ç‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚ åœ¨æ–‡çŒ®ä¸­ï¼Œæ‚¨æœ‰æ—¶ä¼šçœ‹åˆ°è¿™è¢«ç§°ä¸º *decaying* æˆ– *annealing*å­¦ä¹ ç‡ã€‚ åœ¨ Keras ä¸­ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨ *learning rate scheduler*ã€‚ ä¸€ä¸ªå¥½ç”¨çš„æ˜¯`PolynomialDecay`â€”â€”å°½ç®¡æœ‰è¿™ä¸ªåå­—ï¼Œä½†åœ¨é»˜è®¤è®¾ç½®ä¸‹ï¼Œå®ƒåªæ˜¯ç®€å•åœ°ä»åˆå§‹å€¼çº¿æ€§è¡°å‡å­¦ä¹ ç‡å€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ç»ˆå€¼ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ä½†æ˜¯ï¼Œ ä¸ºäº†æ­£ç¡®ä½¿ç”¨è°ƒåº¦ç¨‹åºï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰å®ƒè®­ç»ƒçš„æ¬¡æ•°ã€‚ æˆ‘ä»¬å°†åœ¨ä¸‹é¢ä¸ºå…¶è®¡ç®—â€œnum_train_stepsâ€ã€‚

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é™¤ä»¥batch sizeå†ä¹˜ä»¥ epochã€‚
# æ³¨æ„è¿™é‡Œçš„tf_train_datasetæ˜¯ä¸€ä¸ªè½¬åŒ–ä¸ºbatchåçš„ tf.data.Datasetï¼Œ
# ä¸æ˜¯åŸæ¥çš„ Hugging Face Datasetï¼Œæ‰€ä»¥å®ƒçš„ len() å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

ğŸ¤— Transformers åº“è¿˜æœ‰ä¸€ä¸ª `create_optimizer()` å‡½æ•°ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå…·æœ‰å­¦ä¹ ç‡è¡°å‡çš„ `AdamW` ä¼˜åŒ–å™¨ã€‚ è¿™æ˜¯ä¸€ä¸ªä¾¿æ·çš„æ–¹å¼ï¼Œæ‚¨å°†åœ¨æœ¬è¯¾ç¨‹çš„åç»­éƒ¨åˆ†ä¸­è¯¦ç»†äº†è§£ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬æœ‰äº†å…¨æ–°çš„ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨å®ƒè¿›è¡Œè®­ç»ƒã€‚ é¦–å…ˆï¼Œè®©æˆ‘ä»¬é‡æ–°åŠ è½½æ¨¡å‹ï¼Œä»¥é‡ç½®æˆ‘ä»¬åˆšåˆšè¿›è¡Œçš„è®­ç»ƒè¿è¡Œå¯¹æƒé‡çš„æ›´æ”¹ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨å¯¹å…¶è¿›è¡Œç¼–è¯‘ï¼š

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¬¡è¿›è¡Œfitï¼š

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œæ‚¨å¯ä»¥åœ¨ `model.fit()` æ–¹æ³•ä¸­ä¼ é€’ `PushToHubCallback`ã€‚ æˆ‘ä»¬å°†åœ¨ [ç¬¬å››ç« ](/course/chapter4/3) ä¸­è¿›è¡Œä»‹ç»

</Tip>

### æ¨¡å‹é¢„æµ‹

<Youtube id="nx10eh4CoOs"/>


è®­ç»ƒå’Œè§‚å¯Ÿçš„lossä¸‹é™éƒ½éå¸¸å¥½ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³ä»è®­ç»ƒåçš„æ¨¡å‹ä¸­è·å¾—è¾“å‡ºï¼Œæˆ–è€…è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼Œæˆ–è€…åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ¨¡å‹å‘¢ï¼Ÿ ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`predict()` æ–¹æ³•ã€‚ è¿™å°†è¿”å›æ¨¡å‹çš„è¾“å‡ºå¤´çš„*logits*æ•°å€¼ï¼Œæ¯ä¸ªç±»ä¸€ä¸ªã€‚

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

æˆ‘ä»¬å¯ä»¥å°†è¿™äº› logit è½¬æ¢ä¸ºæ¨¡å‹çš„ç±»åˆ«é¢„æµ‹ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ argmax æ‰¾åˆ°æœ€é«˜çš„ logitï¼Œå®ƒå¯¹åº”äºæœ€æœ‰å¯èƒ½çš„ç±»åˆ«ï¼š

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è¿™äº› `preds` æ¥è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼ æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åœ°åŠ è½½ä¸ MRPC æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡ä½¿ç”¨çš„æ˜¯ `evaluate.load()` å‡½æ•°ã€‚ è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è¿›è¡Œåº¦é‡è®¡ç®—ï¼š

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

æ‚¨è·å¾—çš„ç¡®åˆ‡ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¨¡å‹å¤´çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜å®ƒè·å¾—çš„æŒ‡æ ‡ã€‚ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 85.78%ï¼ŒF1 å¾—åˆ†ä¸º 89.97ã€‚ è¿™äº›æ˜¯ç”¨äºè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ç»“æœçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚ [BERT è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf) ä¸­çš„è¡¨æ ¼æŠ¥å‘Šäº†åŸºæœ¬æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚ é‚£æ˜¯ `uncased` æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰ä½¿ç”¨çš„æ˜¯ `cased` æ¨¡å‹ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šè·å¾—æ›´å¥½çš„ç»“æœã€‚

ä½¿ç”¨ Keras API è¿›è¡Œå¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚ ç¬¬ 7 ç« å°†ç»™å‡ºå¯¹å¤§å¤šæ•°å¸¸è§ NLP ä»»åŠ¡æ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ã€‚å¦‚æœæ‚¨æƒ³åœ¨ Keras API ä¸Šç£¨ç»ƒè‡ªå·±çš„æŠ€èƒ½ï¼Œè¯·å°è¯•ä½¿ç¬¬äºŒèŠ‚æ‰€ä½¿ç”¨çš„çš„æ•°æ®å¤„ç†åœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚