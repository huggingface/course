<FrameworkSwitchCourse {fw} />

# 模型 [[模型]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
在本节中，我们将更详细地了解如何创建和使用模型。我们将使用
AutoModel类，它极大程度的方便您从*checkpoint*实例化任何模型。

AutoModel 类与其相关类本质上是对库中各种模型的简化包装。其智能之处在于能够自动识别检查点所对应的模型架构，并据此实例化相应的模型。

{:else}
在本节中，我们将更详细地了解如何创建和使用模型。我们将使用
AutoModel类，它极大程度的方便您从*checkpoint*实例化任何模型。

AutoModel 类与其相关类本质上是对库中各种模型的简化包装。其智能之处在于能够自动识别检查点所对应的模型架构，并据此实例化相应的模型。

{/if}

如果您知道要使用的模型类型，则可以使用直接定义其体系结构的类。让我们看看这是如何与BERT模型一起工作的。

## 创建转换器 [[创建转换器]]

初始化BERT模型需要做的第一件事是加载配置对象：

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```
{/if}

配置包含许多用于构建模型的属性：

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

虽然您还没有看到所有这些属性都做了什么，但您应该认识到其中的一些属性：`hidden_size`属性定义了`hidden_states`向量的大小，`num_hidden_layers`定义了Transformer模型的层数。

### 不同的加载方式 [[不同的加载方式]]

从默认配置创建模型会使用随机值对其进行初始化：


{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}


该模型可以在这种状态下使用，但会输出胡言乱语；首先需要对其进行训练，我们可以根据手头的任务从头开始训练模型，但正如您在
[Chapter 1](/course/chapter1)
所见，这将需要很长的时间和大量的数据，并将产生不可忽视的环境影响。为了避免不必要的重复工作，必须能够共享和重用已经训练过的模型。


加载已经训练过的Transformers模型很简单-我们可以使用`from_pretrained()`
方法：

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

正如您之前看到的，我们可以用等效的`AutoModel`类替换`BertModel`类。从现在开始，我们将这样做，因为这会产生检查点不可知的代码；如果您的代码适用于一个*checkpoint*，那么它应该与另一个*checkpoint*无缝地工作。即使体系结构不同，这也适用，只要检查点是针对类似任务（例如，情绪分析任务）训练的。

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

正如您之前看到的，我们可以用等效的`AutoModel`替换`BertModel`类。从现在开始，我们将这样做，因为这会产生检查点不可知的代码；如果您的代码适用于一个*checkpoint*，那么它应该与另一个*checkpoint*无缝地工作。即使体系结构不同，这也适用，只要检查点是针对类似任务（例如，情绪分析任务）训练的。

{/if}

在上面的代码示例中，我们没有使用BertConfig，而是通过`bert-base-cased`标识符加载了一个预训练模型。这是一个模型检查点，由`BERT`的作者自己训练；您可以在
[model card](https://huggingface.co/bert-base-cased)中找到更多细节.



该模型现在使用*checkpoint*的所有权重进行初始化。它可以直接用于对训练过的任务进行推理，也可以对新任务进行微调。通过预先训练重量而不是从头开始的训练，我们可以很快取得好的效果。



权重已下载并缓存在缓存文件夹中（因此将来对from_pretrained()方法的调用将不会重新下载它们）默认为
~/.cache/huggingface/transformers
. 您可以通过设置
HF_HOME
环境变量来自定义缓存文件夹。



用于加载模型的标识符可以是模型中心Hub上任何模型的标识符，只要它与`BERT`体系结构兼容。可以找到可用的`BERT`*checkpoint*的完整列表
[here](https://huggingface.co/models?filter=bert)
.
### 保存模型 [[保存模型]]

保存模型和加载模型一样简单--我们使用
save_pretrained()
方法，类似于
from_pretrained()
方法：

```py
model.save_pretrained("directory_on_my_computer")
```

这会将两个文件保存到磁盘：

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

如果你看一下
config.json
文件，您将识别构建模型体系结构所需的属性。该文件还包含一些元数据，例如*checkpoint*的来源以及上次保存检查点时使用的🤗 Transformers版本。

{#if fw === 'pt'}
这个 *pytorch_model.bin* 文件就是众所周知的*state dictionary*; 它包含模型的所有权重。这两个文件密切相关；这个配置是了解您的模型架构所必需的，而模型权重则是您模型的参数。

{:else}
这个 *tf_model.h5* 文件就是众所周知的*state dictionary*; 它包含模型的所有权重。这两个文件密切相关；这个配置是了解您的模型架构所必需的，而模型权重则是您模型的参数。

{/if}

### 使用Transformers模型进行推理 [[使用Transformers模型进行推理]]

既然您知道了如何加载和保存模型，那么让我们尝试使用它进行一些预测。Transformer模型只能处理数字——分词器生成的数字。但在我们讨论*Tokenizer*之前，让我们先探讨模型接受哪些输入。

*Tokenizer*可以将输入转换为适当的框架张量，但为了帮助您了解发生了什么，我们将快速了解在将输入发送到模型之前必须做什么。

假设我们有几个序列：

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

*Tokenizer*将这些转换为词汇表索引，通常称为
*input IDs*
. 每个序列现在都是一个数字列表！结果是：

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

这是一个编码序列的列表：一个嵌套列表（列表的列表）。张量只接受矩形形状（可以想象成矩阵）。这个“数组”已经是矩形形状，因此将其转换为张量很容易：

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### 使用张量作为模型的输入 [[使用张量作为模型的输入]]



在模型中使用张量非常简单-我们只需将输入称为模型：


```python
output = model(model_inputs)
```




虽然模型接受许多不同的参数，但只有输入 ID 是必需的。 我们将在后面解释其他参数的作用以及何时需要它们，但首先我们需要仔细研究构建 Transformer 模型可以理解的输入的*tokenizer*。