<FrameworkSwitchCourse {fw} />

# å¤„ç†å¤šä¸ªåºåˆ—

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æœ€ç®€å•çš„ç”¨ä¾‹ï¼šå¯¹ä¸€ä¸ªå°é•¿åº¦çš„åºåˆ—è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œä¸€äº›é—®é¢˜å·²ç»å‡ºç°ï¼š

* æˆ‘ä»¬å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ï¼Ÿ


* æˆ‘ä»¬å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ä¸åŒé•¿åº¦?


* è¯æ±‡ç´¢å¼•æ˜¯è®©æ¨¡å‹æ­£å¸¸å·¥ä½œçš„å”¯ä¸€è¾“å…¥å—ï¼Ÿ


* æ˜¯å¦å­˜åœ¨åºåˆ—å¤ªé•¿çš„é—®é¢˜ï¼Ÿ

è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›é—®é¢˜ä¼šå¸¦æ¥ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ğŸ¤— Transformers APIè§£å†³å®ƒä»¬

## æ¨¡å‹éœ€è¦ä¸€æ‰¹è¾“å…¥

åœ¨ä¸Šä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œæ‚¨çœ‹åˆ°äº†åºåˆ—å¦‚ä½•è½¬æ¢ä¸ºæ•°å­—åˆ—è¡¨ã€‚è®©æˆ‘ä»¬å°†æ­¤æ•°å­—åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶å°†å…¶å‘é€åˆ°æ¨¡å‹ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

å“¦ï¼Œä¸ï¼ä¸ºä»€ä¹ˆå¤±è´¥äº†ï¼Ÿâ€œæˆ‘ä»¬éµå¾ªäº†ç¬¬2èŠ‚ä¸­ç®¡é“çš„æ­¥éª¤ã€‚

é—®é¢˜æ˜¯æˆ‘ä»¬å‘æ¨¡å‹å‘é€äº†ä¸€ä¸ªåºåˆ—ï¼Œè€ŒğŸ¤— Transformersæ¨¡å‹é»˜è®¤æƒ…å†µä¸‹éœ€è¦å¤šä¸ªå¥å­ã€‚åœ¨è¿™é‡Œï¼Œå½“æˆ‘ä»¬å°†åˆ†è¯å™¨åº”ç”¨äºä¸€ä¸ªåº”ç”¨ç¨‹åºæ—¶ï¼Œæˆ‘ä»¬å°è¯•åœ¨å¹•åå®Œæˆåˆ†è¯å™¨æ‰€åšçš„ä¸€åˆ‡ï¼Œä½†å¦‚æœä»”ç»†è§‚å¯Ÿï¼Œæ‚¨ä¼šå‘ç°å®ƒä¸ä»…å°†è¾“å…¥IDåˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œè¿˜åœ¨å…¶é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªç»´åº¦ï¼š

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

è®©æˆ‘ä»¬é‡è¯•å¹¶æ·»åŠ ä¸€ä¸ªæ–°ç»´åº¦ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

æˆ‘ä»¬æ‰“å°è¾“å…¥IDä»¥åŠç”Ÿæˆçš„logits-ä»¥ä¸‹æ˜¯è¾“å‡ºï¼š

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

*Batching* æ˜¯ä¸€æ¬¡é€šè¿‡æ¨¡å‹å‘é€å¤šä¸ªå¥å­çš„è¡Œä¸ºã€‚å¦‚æœä½ åªæœ‰ä¸€å¥è¯ï¼Œä½ å¯ä»¥ç”¨ä¸€ä¸ªåºåˆ—æ„å»ºä¸€ä¸ªæ‰¹æ¬¡ï¼š


```
batched_ids = [ids, ids]
```

è¿™æ˜¯ä¸€æ‰¹ä¸¤ä¸ªç›¸åŒçš„åºåˆ—ï¼

<Tip>

âœï¸ **Try it out!** è¯•è¯•çœ‹ï¼å°†æ­¤åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡å¹¶é€šè¿‡æ¨¡å‹ä¼ é€’ã€‚æ£€æŸ¥æ‚¨æ˜¯å¦è·å¾—ä¸ä¹‹å‰ç›¸åŒçš„ç™»å½•ï¼ˆä½†æ˜¯åªæœ‰ä¸¤æ¬¡ï¼‰
</Tip>

æ‰¹å¤„ç†å…è®¸æ¨¡å‹åœ¨è¾“å…¥å¤šä¸ªå¥å­æ—¶å·¥ä½œã€‚ä½¿ç”¨å¤šä¸ªåºåˆ—å°±åƒä½¿ç”¨å•ä¸ªåºåˆ—æ„å»ºæ‰¹ä¸€æ ·ç®€å•ã€‚ä¸è¿‡ï¼Œè¿˜æœ‰ç¬¬äºŒä¸ªé—®é¢˜ã€‚å½“ä½ è¯•å›¾å°†ä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰å¥å­ç»„åˆåœ¨ä¸€èµ·æ—¶ï¼Œå®ƒä»¬çš„é•¿åº¦å¯èƒ½ä¸åŒã€‚å¦‚æœæ‚¨ä»¥å‰ä½¿ç”¨è¿‡å¼ é‡ï¼Œé‚£ä¹ˆæ‚¨çŸ¥é“å®ƒä»¬å¿…é¡»æ˜¯çŸ©å½¢ï¼Œå› æ­¤æ— æ³•å°†è¾“å…¥IDåˆ—è¡¨ç›´æ¥è½¬æ¢ä¸ºå¼ é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸å¡«å……è¾“å…¥ã€‚

## å¡«å……è¾“å…¥

ä»¥ä¸‹åˆ—è¡¨ä¸èƒ½è½¬æ¢ä¸ºå¼ é‡ï¼š

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¡«å……ä½¿å¼ é‡å…·æœ‰çŸ©å½¢ã€‚Paddingé€šè¿‡åœ¨å€¼è¾ƒå°‘çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªåä¸ºPadding tokençš„ç‰¹æ®Šå•è¯æ¥ç¡®ä¿æˆ‘ä»¬æ‰€æœ‰çš„å¥å­é•¿åº¦ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰10ä¸ªåŒ…å«10ä¸ªå•è¯çš„å¥å­å’Œ1ä¸ªåŒ…å«20ä¸ªå•è¯çš„å¥å­ï¼Œå¡«å……å°†ç¡®ä¿æ‰€æœ‰å¥å­éƒ½åŒ…å«20ä¸ªå•è¯ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œç”Ÿæˆçš„å¼ é‡å¦‚ä¸‹æ‰€ç¤ºï¼š

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

å¯ä»¥åœ¨tokenizer.pad_token_idä¸­æ‰¾åˆ°å¡«å……ä»¤ç‰ŒID. è®©æˆ‘ä»¬ä½¿ç”¨å®ƒï¼Œå°†æˆ‘ä»¬çš„ä¸¤å¥è¯åˆ†åˆ«å‘é€åˆ°æ¨¡å‹ä¸­ï¼Œå¹¶åˆ†æ‰¹å‘é€åˆ°ä¸€èµ·ï¼š


{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

æˆ‘ä»¬æ‰¹å¤„ç†é¢„æµ‹ä¸­çš„logitsæœ‰ç‚¹é—®é¢˜ï¼šç¬¬äºŒè¡Œåº”è¯¥ä¸ç¬¬äºŒå¥çš„logitsç›¸åŒï¼Œä½†æˆ‘ä»¬å¾—åˆ°äº†å®Œå…¨ä¸åŒçš„å€¼ï¼


è¿™æ˜¯å› ä¸ºTransformeræ¨¡å‹çš„å…³é”®ç‰¹æ€§æ˜¯å…³æ³¨å±‚ï¼Œå®ƒå°†æ¯ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡åŒ–ã€‚è¿™äº›å°†è€ƒè™‘å¡«å……æ ‡è®°ï¼Œå› ä¸ºå®ƒä»¬æ¶‰åŠåºåˆ—ä¸­çš„æ‰€æœ‰æ ‡è®°ã€‚ä¸ºäº†åœ¨é€šè¿‡æ¨¡å‹ä¼ é€’ä¸åŒé•¿åº¦çš„å•ä¸ªå¥å­æ—¶ï¼Œæˆ–è€…åœ¨ä¼ é€’ä¸€æ‰¹åº”ç”¨äº†ç›¸åŒå¥å­å’Œå¡«å……çš„å¥å­æ—¶è·å¾—ç›¸åŒçš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„å±‚å¿½ç•¥å¡«å……æ ‡è®°ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨ attention maskæ¥å®ç°çš„ã€‚

## æ³¨æ„åŠ›é¢å…·

*Attention masks*æ˜¯ä¸è¾“å…¥IDå¼ é‡å½¢çŠ¶å®Œå…¨ç›¸åŒçš„å¼ é‡ï¼Œç”¨0å’Œ1å¡«å……ï¼š1sè¡¨ç¤ºåº”æ³¨æ„ç›¸åº”çš„æ ‡è®°ï¼Œ0sè¡¨ç¤ºä¸åº”æ³¨æ„ç›¸åº”çš„æ ‡è®°ï¼ˆå³ï¼Œæ¨¡å‹çš„æ³¨æ„åŠ›å±‚åº”å¿½ç•¥å®ƒä»¬ï¼‰ã€‚

è®©æˆ‘ä»¬ç”¨attention maskå®Œæˆä¸Šä¸€ä¸ªç¤ºä¾‹ï¼š

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†è¯¥æ‰¹ä¸­ç¬¬äºŒä¸ªå¥å­çš„ç›¸åŒç™»å½•ã€‚

è¯·æ³¨æ„ï¼Œç¬¬äºŒä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªå€¼æ˜¯ä¸€ä¸ªå¡«å……IDï¼Œå®ƒåœ¨attention maskä¸­æ˜¯ä¸€ä¸ª0å€¼ã€‚

<Tip>

âœï¸ è¯•è¯•çœ‹ï¼åœ¨ç¬¬2èŠ‚ä¸­ä½¿ç”¨çš„ä¸¤ä¸ªå¥å­ä¸Šæ‰‹åŠ¨åº”ç”¨æ ‡è®°åŒ–ï¼ˆâ€œæˆ‘ä¸€ç”Ÿéƒ½åœ¨ç­‰å¾…æ‹¥æŠ±è¯¾ç¨‹ã€‚â€å’Œâ€œæˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼â€ï¼‰ã€‚é€šè¿‡æ¨¡å‹ä¼ é€’å®ƒä»¬ï¼Œå¹¶æ£€æŸ¥æ‚¨æ˜¯å¦è·å¾—ä¸ç¬¬2èŠ‚ä¸­ç›¸åŒçš„ç™»å½•ã€‚ç°åœ¨ä½¿ç”¨å¡«å……æ ‡è®°å°†å®ƒä»¬æ‰¹å¤„ç†åœ¨ä¸€èµ·ï¼Œç„¶ååˆ›å»ºé€‚å½“çš„æ³¨æ„æ©ç ã€‚æ£€æŸ¥é€šè¿‡æ¨¡å‹æ—¶æ˜¯å¦è·å¾—ç›¸åŒçš„ç»“æœï¼

</Tip>

## é•¿åºåˆ—

å¯¹äºTransformersæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹çš„åºåˆ—é•¿åº¦æ˜¯æœ‰é™çš„ã€‚å¤§å¤šæ•°æ¨¡å‹å¤„ç†å¤šè¾¾512æˆ–1024ä¸ªä»¤ç‰Œçš„åºåˆ—ï¼Œå½“è¦æ±‚å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œä¼šå´©æºƒã€‚æ­¤é—®é¢˜æœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š



* ä½¿ç”¨æ”¯æŒçš„åºåˆ—é•¿åº¦è¾ƒé•¿çš„æ¨¡å‹ã€‚


* æˆªæ–­åºåˆ—ã€‚


æ¨¡å‹æœ‰ä¸åŒçš„æ”¯æŒåºåˆ—é•¿åº¦ï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨å¤„ç†å¾ˆé•¿çš„åºåˆ—ã€‚
[Longformer](https://huggingface.co/transformers/model_doc/longformer.html)
è¿™æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œå¦ä¸€ä¸ªæ˜¯
[LED](https://huggingface.co/transformers/model_doc/led.html)
. å¦‚æœæ‚¨æ­£åœ¨å¤„ç†ä¸€é¡¹éœ€è¦å¾ˆé•¿åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨æŸ¥çœ‹è¿™äº›æ¨¡å‹ã€‚

å¦åˆ™ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨é€šè¿‡æŒ‡å®šmax_sequence_lengthå‚æ•°ï¼š

```py
sequence = sequence[:max_sequence_length]
```
