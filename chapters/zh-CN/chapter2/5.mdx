<FrameworkSwitchCourse {fw} />

# å¤„ç†å¤šä¸ªåºåˆ— [[å¤„ç†å¤šä¸ªåºåˆ—]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æœ€ç®€å•çš„æ¡ˆä¾‹ï¼šå¯¹ä¸€ä¸ªè¾ƒçŸ­çš„å¥å­è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œä¸€äº›é—®é¢˜å·²ç»å‡ºç°ï¼š

* æˆ‘ä»¬å¦‚ä½•å¤„ç†å¤šä¸ªå¥å­ï¼Ÿ

* æˆ‘ä»¬å¦‚ä½•å¤„ç†ä¸åŒé•¿åº¦çš„å¤šä¸ªå¥å­ï¼Ÿ

* è¯æ±‡ç´¢å¼•æ˜¯å”¯ä¸€å¯ä»¥è®©æ¨¡å‹è¿è¡Œçš„è¾“å…¥å—ï¼Ÿ

* æ˜¯å¦å­˜åœ¨å¥å­å¤ªé•¿çš„é—®é¢˜ï¼Ÿ

è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›é—®é¢˜ä¼šå¸¦æ¥ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ğŸ¤— Transformers API è§£å†³å®ƒä»¬

## æ¨¡å‹éœ€è¦ä¸€æ‰¹è¾“å…¥ [[æ¨¡å‹éœ€è¦ä¸€æ‰¹è¾“å…¥]]

åœ¨ä¸Šä¸€ä¸ªç»ƒä¹ ä¸­ï¼Œä½ çœ‹åˆ°äº†å¥å­å¦‚ä½•è½¬æ¢ä¸ºæ•°å­—åˆ—è¡¨ã€‚è®©æˆ‘ä»¬å°†æ­¤æ•°å­—åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶å°†å…¶å‘é€åˆ°æ¨¡å‹ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# è¿™ä¸€è¡Œä¼šè¿è¡Œå¤±è´¥
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# è¿™ä¸€è¡Œä¼šè¿è¡Œå¤±è´¥
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

å“¦ï¼Œä¸ï¼ä¸ºä»€ä¹ˆå¤±è´¥äº†ï¼Ÿâ€œæˆ‘ä»¬çš„ç¡®æ˜¯æŒ‰ç…§ç¬¬ 2 èŠ‚ä¸­ç®¡é“çš„æ­¥éª¤ä¸€æ­¥æ­¥æ¥åšçš„ã€‚

é—®é¢˜æ˜¯æˆ‘ä»¬å‘æ¨¡å‹å‘é€äº†ä¸€ä¸ªå•ç‹¬çš„å¥å­ï¼Œè€ŒğŸ¤— Transformers æ¨¡å‹é»˜è®¤æƒ…å†µä¸‹éœ€è¦ä¸€ä¸ªå¥å­åˆ—è¡¨ã€‚åœ¨è¿™é‡Œï¼Œå½“æˆ‘ä»¬è¯•å›¾é‡ç° tokenizer åœ¨è¾“å…¥ `sequence` ååœ¨å…¶å†…éƒ¨è¿›è¡Œçš„æ‰€æœ‰æ“ä½œã€‚ä½†å¦‚æœä½ ä»”ç»†è§‚å¯Ÿï¼Œä½ ä¼šå‘ç° tokenizer ä¸ä»…ä»…æ˜¯å°† inputs ID çš„åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œå®ƒè¿˜åœ¨å…¶ä¸Šæ·»åŠ äº†ä¸€ä¸ªç»´åº¦ï¼š

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡å¹¶æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

è®©æˆ‘ä»¬æ‰“å° inputs ID ä»¥åŠç”Ÿæˆçš„ logits å€¼ï¼Œä»¥ä¸‹æ˜¯è¾“å‡ºï¼š

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

æ‰¹å¤„ç†ï¼ˆBatchingï¼‰æ˜¯ä¸€æ¬¡æ€§é€šè¿‡æ¨¡å‹å‘é€å¤šä¸ªå¥å­çš„è¡Œä¸ºã€‚å¦‚æœä½ åªæœ‰ä¸€å¥è¯ï¼Œä½ å¯ä»¥æ„å»ºä¸€ä¸ªåªæœ‰ä¸€ä¸ªå¥å­çš„ batchï¼š


```
batched_ids = [ids, ids]
```

è¿™å°±æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªç›¸åŒå¥å­çš„ batch 

<Tip>

âœï¸   **è¯•è¯•çœ‹ï¼**  å°†è¿™ä¸ª `batched_ids` åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶é€šè¿‡ä½ çš„æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚æ£€æŸ¥ä½ æ˜¯å¦å¾—åˆ°äº†ä¸ä¹‹å‰ç›¸åŒçš„ logits å€¼ï¼ˆä½†æ˜¯é‡å¤äº†ä¸¤æ¬¡ï¼‰ï¼

</Tip>

æ‰¹å¤„ç†æ”¯æŒæ¨¡å‹åœ¨è¾“å…¥å¤šä¸ªå¥å­æ—¶å·¥ä½œã€‚ä½¿ç”¨å¤šä¸ªå¥å­å°±åƒä½¿ç”¨å•ä¸ªå¥å­æ„å»ºæ‰¹ä¸€æ ·ç®€å•ã€‚ä¸è¿‡ï¼Œè¿˜æœ‰ç¬¬äºŒä¸ªé—®é¢˜ã€‚å½“ä½ è¯•å›¾å°†ä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰å¥å­ç»„åˆåœ¨ä¸€èµ·æ—¶ï¼Œå®ƒä»¬çš„é•¿åº¦å¯èƒ½ä¸åŒã€‚å¦‚æœä½ ä»¥å‰ä½¿ç”¨è¿‡å¼ é‡ï¼Œé‚£ä¹ˆä½ çŸ¥é“å®ƒä»¬å¿…é¡»æ˜¯çŸ©å½¢ï¼Œå› æ­¤æ— æ³•å°† inputs ID åˆ—è¡¨ç›´æ¥è½¬æ¢ä¸ºå¼ é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸å¡«å……è¾“å…¥ï¼ˆPaddingï¼‰ã€‚

## å¡«å……è¾“å…¥ï¼ˆPaddingï¼‰ [[å¡«å……è¾“å…¥ï¼ˆPaddingï¼‰]]

ä»¥ä¸‹åˆ—è¡¨ä¸èƒ½è½¬æ¢ä¸ºå¼ é‡ï¼š

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¡«å……ä½¿å¼ é‡æˆä¸ºæ ‡å‡†çš„çŸ©å½¢ã€‚Padding é€šè¿‡åœ¨å€¼è¾ƒå°‘çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªåä¸º `padding_id` çš„ç‰¹æ®Šå•è¯æ¥ç¡®ä¿æˆ‘ä»¬æ‰€æœ‰çš„å¥å­é•¿åº¦ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ 10 ä¸ªåŒ…å« 10 ä¸ªå•è¯çš„å¥å­å’Œ 1 ä¸ªåŒ…å« 20 ä¸ªå•è¯çš„å¥å­ï¼Œå¡«å……èƒ½ç¡®ä¿æ‰€æœ‰å¥å­éƒ½åŒ…å« 20 ä¸ªå•è¯ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œå¡«å……åçš„å¼ é‡å¦‚ä¸‹æ‰€ç¤ºï¼š

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

æˆ‘ä»¬å¯ä»¥åœ¨ `tokenizer.pad_token_id` ä¸­æ‰¾åˆ°å¡«å…… token çš„ IDã€‚è®©æˆ‘ä»¬ä½¿ç”¨å®ƒï¼Œåˆ†åˆ«ç”¨æ¨¡å‹å¤„ç†è¿™ä¸¤ä¸ªå¥å­ï¼Œç„¶åå†å°è¯•å°†è¿™ä¸¤ä¸ªå¥å­æ”¾åœ¨ä¸€èµ·ç”¨æ¨¡å‹æ‰¹å¤„ç†ï¼š

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

å’¦ï¼Œæˆ‘ä»¬æ‰¹å¤„ç†é¢„æµ‹ä¸­çš„ logits å€¼æœ‰ç‚¹é—®é¢˜ï¼šç¬¬äºŒè¡Œåº”è¯¥ä¸ç¬¬äºŒå¥çš„ logits ç›¸åŒï¼Œä½†æˆ‘ä»¬å¾—åˆ°äº†å®Œå…¨ä¸åŒçš„å€¼ï¼

è¿™æ˜¯å› ä¸º Transformer æ¨¡å‹çš„å…³é”®ç‰¹æ€§ï¼šæ³¨æ„åŠ›å±‚ï¼Œå®ƒè€ƒè™‘äº†æ¯ä¸ª token çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ª token çš„å«ä¹‰å¹¶éå•ç‹¬å­˜åœ¨çš„ï¼Œå®ƒçš„å«ä¹‰è¿˜å–å†³äºå®ƒåœ¨å¥å­ä¸­çš„ä½ç½®ä»¥åŠå‘¨å›´çš„å…¶ä»– tokensã€‚å½“æˆ‘ä»¬ä½¿ç”¨å¡«å……ï¼ˆpaddingï¼‰æ¥å¤„ç†é•¿åº¦ä¸åŒçš„å¥å­æ—¶ï¼Œæˆ‘ä»¬ä¼šæ·»åŠ ç‰¹æ®Šçš„â€œå¡«å…… tokenâ€æ¥ä½¿æ‰€æœ‰å¥å­è¾¾åˆ°ç›¸åŒçš„é•¿åº¦ã€‚ä½†æ˜¯ï¼Œæ³¨æ„åŠ›å±‚ä¼šå°†è¿™äº›å¡«å…… token ä¹Ÿçº³å…¥è€ƒè™‘ï¼Œå› ä¸ºå®ƒä»¬ä¼šå…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ tokensã€‚è¿™å°±å¯¼è‡´äº†ä¸€ä¸ªé—®é¢˜ï¼šå°½ç®¡å¡«å…… token æœ¬èº«å¹¶æ²¡æœ‰å®é™…çš„å«ä¹‰ï¼Œä½†å®ƒä»¬çš„å­˜åœ¨ä¼šå½±å“æ¨¡å‹å¯¹å¥å­çš„ç†è§£ã€‚æˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„å±‚å¿½ç•¥å¡«å…… tokenã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å±‚æ¥å®ç°çš„ã€‚

## æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å±‚ [[æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å±‚]]

æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰æ˜¯ä¸ inputs ID å¼ é‡å½¢çŠ¶å®Œå…¨ç›¸åŒçš„å¼ é‡ï¼Œç”¨ 0 å’Œ 1 å¡«å……ï¼š1 è¡¨ç¤ºåº”å…³æ³¨ç›¸åº”çš„ tokensï¼Œ0 è¡¨ç¤ºåº”å¿½ç•¥ç›¸åº”çš„ tokensï¼ˆå³ï¼Œå®ƒä»¬åº”è¢«æ¨¡å‹çš„æ³¨æ„åŠ›å±‚å¿½è§†ï¼‰ã€‚

è®©æˆ‘ä»¬ç”¨ attention mask ä¿®æ”¹ä¸Šä¸€ä¸ªç¤ºä¾‹ï¼š

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†æ‰¹å¤„ç†ä¸­ç¬¬äºŒå¥è¯ç›¸åŒçš„ logits å€¼ã€‚

æ³¨æ„ç¬¬äºŒåºåˆ—çš„æœ€åä¸€ä¸ªå€¼æ˜¯å¡«å…… IDï¼Œå…¶åœ¨æ³¨æ„åŠ›æ©ç ä¸­çš„å€¼ä¸º 0ã€‚

<Tip>

âœï¸ **è¯•è¯•çœ‹ï¼**åœ¨ç¬¬äºŒèŠ‚ä½¿ç”¨çš„ä¸¤ä¸ªå¥å­ï¼ˆâ€œI've been waiting for a HuggingFace course my whole life.â€ å’Œ â€œI hate this so much!â€ï¼‰ä¸Šæ‰‹åŠ¨è¿›è¡Œ tokenizeã€‚å°†å®ƒä»¬è¾“å…¥æ¨¡å‹å¹¶æ£€æŸ¥ä½ æ˜¯å¦å¾—åˆ°äº†ä¸ç¬¬äºŒèŠ‚ç›¸åŒçš„ logits å€¼ã€‚ç„¶åä½¿ç”¨å¡«å…… token å°†å®ƒä»¬ä¸€èµ·è¿›è¡Œæ‰¹å¤„ç†ï¼Œç„¶ååˆ›å»ºåˆé€‚çš„æ³¨æ„åŠ›æ©ç ã€‚æ£€æŸ¥æ¨¡å‹è®¡ç®—åæ˜¯å¦å¾—åˆ°äº†ç›¸åŒçš„ç»“æœï¼

</Tip>

## æ›´é•¿çš„å¥å­ [[æ›´é•¿çš„å¥å­]]

å¯¹äº Transformers æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹çš„åºåˆ—é•¿åº¦æ˜¯æœ‰é™çš„ã€‚å¤§å¤šæ•°æ¨¡å‹å¤„ç†å¤šè¾¾ 512 æˆ– 1024 ä¸ª çš„ tokens åºåˆ—ï¼Œå½“ä½¿ç”¨æ¨¡å‹å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œä¼šå´©æºƒã€‚æ­¤é—®é¢˜æœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š

* ä½¿ç”¨æ”¯æŒæ›´é•¿åºåˆ—é•¿åº¦çš„æ¨¡å‹ã€‚
* æˆªæ–­ä½ çš„åºåˆ—ã€‚

ä¸åŒçš„æ¨¡å‹æ”¯æŒçš„åºåˆ—é•¿åº¦å„ä¸ç›¸åŒï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†éå¸¸é•¿çš„åºåˆ—ã€‚ä¾‹å¦‚ [Longformer](https://huggingface.co/transformers/model_doc/longformer) å’Œ [LED](https://huggingface.co/transformers/model_doc/led) ã€‚å¦‚æœä½ æ­£åœ¨å¤„ç†éœ€è¦éå¸¸é•¿åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å»ºè®®ä½ è€ƒè™‘ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚

å¦å¤–ï¼Œæˆ‘ä»¬å»ºè®®ä½ é€šè¿‡è®¾å®š `max_sequence_length` å‚æ•°æ¥æˆªæ–­åºåˆ—ï¼š

```py
sequence = sequence[:max_sequence_length]
```
