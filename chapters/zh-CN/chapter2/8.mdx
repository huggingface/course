<FrameworkSwitchCourse {fw} />

<!-- DISABLE-FRONTMATTER-SECTIONS -->

# 章末小测试 [[章末小测验]]

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

### 1. 语言建模 Pipeline 的顺序是什么？
<Question
	choices={[
		{
			text: "首先是模型它处理文本并返回原始预测。然后，分词器会对这些预测进行解释，并在需要时将它们转换回文本",
			explain: "模型不能理解文本！必须首先使用标记器(Tokenizer)将文本转换为id，之后才可以输入给模型。"
		},
		{
			text: "首先是 标记器(Tokenizer)，它处理文本并返回id。模型根据这些 id 并输出预测，可以是一些文本。",
			explain: "该模型的预测结果是id而不是文本。为了将预测转换回文本，必须使用标记器(Tokenizer)！"
		},
		{
			text: "标记器(Tokenizer) 处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用 标记器(Tokenizer) 将这些预测转换回某些文本。",
			explain: "正确! 标记器(Tokenizer)可以用于id与文本的相互转换。",
            correct: true
		}
	]}
/>

### 2. Transformer模型的输出有多少个维度，每个维度分别是什么？
<Question
	choices={[
		{
			text: "2个维度，分别是: 序列长度(Sequence Length)和批次大小(Batch Size)",
			explain: "错! 该模型的张量输出还具有第三个维度: 隐藏层大小(Hidden Size)。"
		},
		{
			text: "2个维度，分别是: 序列长度(Sequence Length)和隐藏层大小(Hidden Size)",
			explain: "错! 所有Transformer模型都批量进行计算，即使是单个序列; 那么批次大小(Batch Size)为1！"
		},
		{
			text: "3个维度，分别是: 序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)",
			explain: "正确！",
            correct: true
		}
	]}
/>

### 3.下列哪一个是Subword标记(Tokenization)的例子（从分词的颗粒度来划分）？
<Question
	choices={[
		{
			text: "WordPiece",
			explain: "是的，这是一个Subword 标记化(Tokenization)的例子！",
            correct: true
		},
		{
			text: "基于单个字符的标记(Tokenization)",
			explain: "基于字符的标记(Tokenization)不是一种Subword 标记化(Tokenization)。"
		},
		{
			text: "基于空格和标点符号的分割",
			explain: "这是一种基于单词标记化(Tokenization)！"
		},
		{
			text: "BPE(Byte Pair Encoding)",
			explain: "是的，这是一个Subword 标记化(Tokenization)的例子！",
            correct: true
        },
		{
			text: "Unigram",
			explain: "是的，这是一个Subword 标记化(Tokenization)的例子！",
            correct: true
        },
		{
			text: "以上都不是",
			explain: "不对！"
        }
	]}
/>

### 4.什么是模型的Head层？
<Question
	choices={[
		{
			text: "原始Transformer网络的一种组件，将张量(Tensors)重定向到它们的正确层",
			explain: "不对! 没有这样的组件。"
		},
		{
			text: "也称为自注意力(self-attention)机制，它根据序列中的其他标记(Token)来调整一个标记(Token)",
			explain: "不对！ 自注意力层确实包含注意力“Head”层，但这些不是模型的head层。"
		},
		{
			text: "一个附加组件，通常由一个或几个层组成，用于将Transformer的预测转换为特定于任务的输出",
			explain: "没错。它的全名是Adaptation Heads，也被简单地称为模型的头部，在不同的任务上有不同的形式: 语言模型Heads，问题回答Heads，序列分类Heads..。",
			correct: true
		} 
	]}
/>

{#if fw === 'pt'}
### 5.什么是AutoModel？
<Question
	choices={[
		{
			text: "根据您的数据自动进行训练的模型",
			explain: "错误。您可能把AutoModel与我们的<a href='https://huggingface.co/autotrain'>AutoTrain</a> 产品相混淆了？"
		},
		{
			text: "一个根据Checkpoint(检查点)返回模型体系结构的对象",
			explain: "确切地说: <code>AutoModel</code>只需要知道初始化的Checkpoint(检查点)就可以返回正确的体系结构。",
			correct: true
		},
		{
			text: "一种可以自动检测输入语言来加载正确权重的模型",
			explain: "不正确; 虽然有些Checkpoint(检查点)和模型能够处理多种语言，但是没有内置的工具可以根据语言自动选择Checkpoint(检查点)。您应该前往 <a href='https://huggingface.co/models'>Model Hub</a> 寻找完成所需任务的最佳Checkpoint(检查点)！"
		} 
	]}
/>

{:else}
### 5.什么是 TFAutoModel？
<Question
	choices={[
		{
			text: "根据您的数据自动进行训练的模型",
			explain: "错误。您可能把TFAutoModel与我们的<a href='https://huggingface.co/autotrain'>AutoTrain</a> 产品相混淆了？"
		},
		{
			text: "一个根据Checkpoint(检查点)返回模型体系结构的对象",
			explain: "确切地说: <code>TFAutoModel</code>只需要知道初始化的Checkpoint(检查点)就可以返回正确的体系结构。",
			correct: true
		},
		{
			text: "一种可以自动检测输入语言来加载正确权重的模型",
			explain: "不正确; 虽然有些Checkpoint(检查点)和模型能够处理多种语言，但是没有内置的工具可以根据语言自动选择Checkpoint(检查点)。您应该前往 <a href='https://huggingface.co/models'>Model Hub</a> 寻找完成所需任务的最佳Checkpoint(检查点)！"
		} 
	]}
/>

{/if}

### 6.当将不同长度的序列批处理在一起时，需要进行哪些处理？
<Question
	choices={[
		{
			text: "截短",
			explain: "是的，截断是一个正确的方式，可以将他们转化为一个固定长度的矩形序列。这是唯一的一个答案吗？",
			correct: true
		},
		{
			text: "返回Tensors",
			explain: "虽然其他技术允许返回矩形张量，但在批处理序列时不可以直接返回长度不一致的Tensors。(序列必须是固定长度)"
		},
		{
			text: "填充",
			explain: "是的，填充是一个正确的方式，可以将他们转化为一个固定长度的矩形序列。这是唯一的一个答案吗？",
			correct: true
		}, 
		{
			text: "注意力遮蔽(Attention masking)",
			explain: "当然！当处理不同长度的序列时，注意力遮蔽是最重要的。然而，这并不是唯一需要进行的处理。",
			correct: true
		} 
	]}
/>

### 7.将 SoftMax激活函数应用于序列分类(Sequence Classification)模型的 logits 输出有什么意义？
<Question
	choices={[
		{
			text: "它软化了logits输出，使结果更可靠。",
			explain: "不， SoftMax激活函数不会影响结果的可靠性。"
		},
		{
			text: "它限定了上下界，使模型的输出结果可以被解释。",
			explain: "正确！结果值被压缩到了0和1之间。不过，这并不是我们使用SoftMax激活函数的唯一原因。",
            correct: true
		},
		{
			text: "输出的和是1，从而产生一个可能的概率解释。",
			explain: "没错，但这并不是我们使用SoftMax激活函数的唯一原因。",
            correct: true
		}
	]}
/>

### 8.大多数标记器(Tokenizer)的API以什么方法为核心？
<Question
	choices={[
		{
			text: "<code>编码</code> ，因为它可以将文本编码为id，将预测的id解码为文本",
			explain: "错! 虽然 <code>编码</code> 方法确实存在于标记器中，但是它不存在于模型中。"
		},
		{
			text: "直接调用标记器(Tokenizer)对象。",
			explain: "完全正确！标记化器(Tokenizer) 的 <code>__call__</code>方法是一个非常强大的方法，可以处理几乎任何事情。它也是从模型中获取预测的方法。",
			correct: true
		},
		{
			text: "<code>pad</code>(填充)",
			explain: "错! <code>pad</code>(填充)非常有用，但它只是标记器(Tokenizer) API的一部分。"
		},
		{
			text: "<code>tokenize</code>(标记)",
			explain: "可以说，<code>tokenize</code>(标记)方法是最有用的方法之一，但它不是标记器(Tokenizer) API的核心方法。"
		}
	]}
/>

### 9.这个代码示例中的`result`变量包含什么？
```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```

<Question
	choices={[
		{
			text: "字符串列表，每个字符串都是一个标记(Token)",
			explain: "正确! 把这个转换成id后，就可以传输给模型了！",
            correct: true
		},
		{
			text: "一个ID的列表",
			explain: "不正确; 这就是 <code>__call__</code> 或 <code>convert_tokens_to_ids</code>方法的作用！"
		},
		{
			text: "包含所有标记(Token)的字符串",
			explain: "这将是次优的，因为Tokenizer会将字符串拆分为多个标记的列表。"
		}
	]}
/>

{#if fw === 'pt'}
### 10.下面的代码有什么错误吗？
```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "不，看起来是对的。",
			explain: "不幸的是，将一个模型与一个用不同Checkpoint(检查点)训练的Tokenizer(标记器)耦合在并不是一个好主意。模型没有在这个这个Tokenizer(标记器)上训练来理解Tokenizer(标记器)的输出，因此模型输出(如果它可以运行的话)不会有任何意义。"
		},
		{
			text: "Tokenizer(标记器)和模型应该来自相同的Checkpoint(检查点)。",
			explain: "对！",
            correct: true
		},
		{
			text: "由于每个输入都是一个Batch，因此可以使用标记器(Tokenizer)对其进行平移和截断来改善这段代码。",
			explain: "的确，每个模型都需要Batch类型的输入。然而，截断或填充这个序列并不一定有意义，这里只有一句话，而这些技术是用来批处理一个句子列表的。"
		}
	]}
/>

{:else}
### 10.下面的代码有什么错误吗？
```py
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "不，看起来是对的。",
			explain: "不幸的是，将一个模型与一个用不同Checkpoint(检查点)训练的Tokenizer(标记器)耦合在并不是一个好主意。模型没有在这个这个Tokenizer(标记器)上训练来理解Tokenizer(标记器)的输出，因此模型输出(如果它可以运行的话)不会有任何意义。"
		},
		{
			text: "Tokenizer(标记器)和模型应该来自相同的Checkpoint(检查点)。",
			explain: "对！",
            correct: true
		},
		{
			text: "由于每个输入都是一个Batch，因此可以使用标记器(Tokenizer)对其进行平移和截断来改善这段代码。",
			explain: "的确，每个模型都需要Batch类型的输入。然而，截断或填充这个序列并不一定有意义，这里只有一句话，而这些技术是用来批处理一个句子列表的。"
		}
	]}
/>

{/if}
