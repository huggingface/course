<FrameworkSwitchCourse {fw} />

# æŠŠå®ƒä»¬æ”¾åœ¨ä¸€èµ·

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"},
]} />

{/if}

åœ¨æœ€åå‡ èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨å°½æœ€å¤§åŠªåŠ›æ‰‹å·¥å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œã€‚æˆ‘ä»¬æ¢è®¨äº†æ ‡è®°åŒ–å™¨çš„å·¥ä½œåŸç†ï¼Œå¹¶ç ”ç©¶äº†æ ‡è®°åŒ–ã€åˆ°è¾“å…¥IDçš„è½¬æ¢ã€å¡«å……ã€æˆªæ–­å’Œæ³¨æ„æ©ç ã€‚

ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬2èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼ŒğŸ¤— Transformers APIå¯ä»¥é€šè¿‡ä¸€ä¸ªé«˜çº§å‡½æ•°ä¸ºæˆ‘ä»¬å¤„ç†æ‰€æœ‰è¿™äº›ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œæ·±å…¥è®¨è®ºã€‚å½“ä½ ç›´æ¥åœ¨å¥å­ä¸Šè°ƒç”¨æ ‡è®°å™¨æ—¶ï¼Œä½ ä¼šå¾—åˆ°å‡†å¤‡é€šè¿‡æ¨¡å‹ä¼ é€’çš„è¾“å…¥

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

è¿™é‡Œï¼Œ`model_inputs`
å˜é‡åŒ…å«æ¨¡å‹è‰¯å¥½è¿è¡Œæ‰€éœ€çš„ä¸€åˆ‡ã€‚å¯¹äºDistilBERTï¼Œå®ƒåŒ…æ‹¬è¾“å…¥ IDå’Œæ³¨æ„åŠ›æ©ç (attention mask)ã€‚å…¶ä»–æ¥å—é¢å¤–è¾“å…¥çš„æ¨¡å‹ä¹Ÿä¼šæœ‰æ ‡è®°å™¨å¯¹è±¡çš„è¾“å‡ºã€‚

æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ä¸€äº›ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ï¼Œè¿™ç§æ–¹æ³•éå¸¸å¼ºå¤§ã€‚é¦–å…ˆï¼Œå®ƒå¯ä»¥æ ‡è®°å•ä¸ªåºåˆ—ï¼š

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

å®ƒè¿˜ä¸€æ¬¡å¤„ç†å¤šä¸ªåºåˆ—ï¼Œå¹¶ä¸”APIæ²¡æœ‰ä»»ä½•å˜åŒ–ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

å®ƒå¯ä»¥æ ¹æ®å‡ ä¸ªç›®æ ‡è¿›è¡Œå¡«å……ï¼š

```py
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å®ƒè¿˜å¯ä»¥æˆªæ–­åºåˆ—:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

æ ‡è®°å™¨å¯¹è±¡å¯ä»¥å¤„ç†åˆ°ç‰¹å®šæ¡†æ¶å¼ é‡çš„è½¬æ¢ï¼Œç„¶åå¯ä»¥ç›´æ¥å‘é€åˆ°æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æç¤ºæ ‡è®°å™¨ä»ä¸åŒçš„æ¡†æ¶è¿”å›å¼ é‡â€”â€”`"pt"`è¿”å›Py Torchå¼ é‡ï¼Œ`"tf"`è¿”å›TensorFlowå¼ é‡ï¼Œ`"np"`è¿”å›NumPyæ•°ç»„ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## ç‰¹æ®Šè¯ç¬¦(token)

å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹æ ‡è®°å™¨è¿”å›çš„è¾“å…¥ IDï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒä»¬ä¸ä¹‹å‰çš„ç•¥æœ‰ä¸åŒï¼š

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

ä¸€ä¸ªåœ¨å¼€å§‹æ—¶æ·»åŠ äº†ä¸€ä¸ªæ ‡è®°(token) IDï¼Œä¸€ä¸ªåœ¨ç»“æŸæ—¶æ·»åŠ äº†ä¸€ä¸ªæ ‡è®°(token) IDã€‚è®©æˆ‘ä»¬è§£ç ä¸Šé¢çš„ä¸¤ä¸ªIDåºåˆ—ï¼Œçœ‹çœ‹è¿™æ˜¯æ€ä¹ˆå›äº‹ï¼š

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

æ ‡è®°å™¨åœ¨å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šå•è¯`[CLS]`ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šå•è¯`[SEP]`ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ç”¨è¿™äº›æ•°æ®é¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥ä¸ºäº†å¾—åˆ°ç›¸åŒçš„æ¨ç†ç»“æœï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ·»åŠ å®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œæœ‰äº›æ¨¡å‹ä¸æ·»åŠ ç‰¹æ®Šå•è¯ï¼Œæˆ–è€…æ·»åŠ ä¸åŒçš„å•è¯ï¼›æ¨¡å‹ä¹Ÿå¯èƒ½åªåœ¨å¼€å¤´æˆ–ç»“å°¾æ·»åŠ è¿™äº›ç‰¹æ®Šå•è¯ã€‚åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œæ ‡è®°å™¨éƒ½çŸ¥é“éœ€è¦å“ªäº›è¯ç¬¦ï¼Œå¹¶å°†ä¸ºæ‚¨å¤„ç†è¿™äº›è¯ç¬¦ã€‚

## ç»“æŸï¼šä»æ ‡è®°å™¨åˆ°æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†æ ‡è®°å™¨å¯¹è±¡åœ¨åº”ç”¨äºæ–‡æœ¬æ—¶ä½¿ç”¨çš„æ‰€æœ‰å•ç‹¬æ­¥éª¤ï¼Œè®©æˆ‘ä»¬æœ€åä¸€æ¬¡çœ‹çœ‹å®ƒå¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ï¼ˆå¡«å……ï¼ï¼‰ï¼Œéå¸¸é•¿çš„åºåˆ—ï¼ˆæˆªæ–­ï¼ï¼‰ï¼Œä»¥åŠå¤šç§ç±»å‹çš„å¼ é‡åŠå…¶ä¸»è¦APIï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
