<FrameworkSwitchCourse {fw} />

# ç»¼åˆåº”ç”¨ [[ç»¼åˆåº”ç”¨]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_tf.ipynb"},
]} />

{/if}

åœ¨è¿‡å»çš„å‡ ä¸ªç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»å°è¯•å°½å¯èƒ½æ‰‹åŠ¨å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œã€‚æˆ‘ä»¬æ¢ç´¢äº† tokenizer çš„è¿è¡Œæœºåˆ¶ï¼Œå¹¶ä¸”äº†è§£äº†åˆ†è¯ã€è½¬æ¢ä¸º inputs IDã€å¡«å……ã€æˆªæ–­ä»¥åŠæ³¨æ„åŠ›æ©ç çš„å¤„ç†æ–¹å¼ã€‚

ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äºŒèŠ‚ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼ŒğŸ¤— Transformers API èƒ½å¤Ÿé€šè¿‡ä¸€ä¸ªé«˜çº§å‡½æ•°ä¸ºæˆ‘ä»¬å¤„ç†æ‰€æœ‰è¿™äº›å·¥ä½œï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±è¦æ·±å…¥ç ”ç©¶è¿™ä¸ªå‡½æ•°ã€‚å½“ä½ ç›´æ¥åœ¨å¥å­ä¸Šè°ƒç”¨ä½ çš„ `tokenizer` æ—¶ï¼Œå°±å¯ä»¥å¾—åˆ°è½¬æ¢åçš„å¯ä»¥ç›´æ¥æ”¾å…¥æ¨¡å‹çš„æ•°æ®äº†ï¼š

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

è¿™é‡Œï¼Œ `model_inputs` å˜é‡åŒ…å«æ¨¡å‹è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰æ•°æ®ã€‚åœ¨ DistilBERT ä¸­ï¼Œå®ƒåŒ…æ‹¬ inputs ID å’Œæ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰ã€‚å…¶ä»–æ¥å—é¢å¤–è¾“å…¥çš„æ¨¡å‹ä¹Ÿä¼šæœ‰å¯¹åº”çš„ tokenizer å¯ä»¥å°†è¾“å…¥è½¬åŒ–ä¸ºæ¨¡å‹æ‰€éœ€è¦çš„è¾“å…¥ã€‚

æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ä¸€äº›ä¾‹å­ä¸­çœ‹åˆ°çš„ï¼Œè¿™ä¸ªå‡½æ•°éå¸¸å¼ºå¤§ã€‚é¦–å…ˆï¼Œå®ƒå¯ä»¥å¯¹å•ä¸ªå¥å­è¿›è¡Œå¤„ç†ï¼š

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

å®ƒè¿˜ä¸€æ¬¡å¤„ç†å¤šä¸ªå¤šä¸ªï¼Œå¹¶ä¸” API çš„ç”¨æ³•å®Œå…¨ä¸€è‡´ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

å®ƒå¯ä»¥ä½¿ç”¨å¤šç§ä¸åŒçš„æ–¹æ³•å¯¹ç›®æ ‡è¿›è¡Œå¡«å……ï¼š

```py
# å°†å¥å­åºåˆ—å¡«å……åˆ°æœ€é•¿å¥å­çš„é•¿åº¦
model_inputs = tokenizer(sequences, padding="longest")

# å°†å¥å­åºåˆ—å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# å°†å¥å­åºåˆ—å¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å®ƒè¿˜å¯ä»¥å¯¹åºåˆ—è¿›è¡Œæˆªæ–­ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# å°†æˆªæ–­æ¯”æ¨¡å‹æœ€å¤§é•¿åº¦é•¿çš„å¥å­åºåˆ—
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# å°†æˆªæ–­é•¿äºæŒ‡å®šæœ€å¤§é•¿åº¦çš„å¥å­åºåˆ—
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` å¯¹è±¡å¯ä»¥å¤„ç†æŒ‡å®šæ¡†æ¶å¼ é‡çš„è½¬æ¢ï¼Œç„¶åå¯ä»¥ç›´æ¥å‘é€åˆ°æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å‘Šè¯‰ tokenizer è¿”å›ä¸åŒæ¡†æ¶çš„å¼ é‡ â€”â€” `"pt"` è¿”å› PyTorch å¼ é‡ï¼Œ `"tf"` è¿”å› TensorFlow å¼ é‡ï¼Œè€Œ `"np"` åˆ™è¿”å› NumPy æ•°ç»„ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# è¿”å› PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# è¿”å› TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# è¿”å› NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## ç‰¹æ®Šçš„ tokens [[ç‰¹æ®Šçš„ tokens ]]

å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹ tokenizer è¿”å›çš„ inputs IDï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒä»¬ä¸ä¹‹å‰çš„ç•¥æœ‰ä¸åŒï¼š

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

å¥å­çš„å¼€å§‹å’Œç»“æŸåˆ†åˆ«å¢åŠ äº†ä¸€ä¸ª inputs IDã€‚æˆ‘ä»¬æ¥è§£ç ä¸Šè¿°çš„ä¸¤ä¸ª ID åºåˆ—ï¼Œçœ‹çœ‹æ˜¯æ€ä¹ˆå›äº‹ï¼š

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

 tokenizer åœ¨å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šå•è¯ `[CLS]` ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šå•è¯ `[SEP]` ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨äº†è¿™äº›å­—è¯ï¼Œæ‰€ä»¥ä¸ºäº†å¾—åˆ°ç›¸åŒçš„æ¨æ–­ç»“æœï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ å®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œæœ‰äº›æ¨¡å‹ä¸æ·»åŠ ç‰¹æ®Šå•è¯ï¼Œæˆ–è€…æ·»åŠ ä¸åŒçš„ç‰¹æ®Šå•è¯ï¼›æ¨¡å‹ä¹Ÿå¯èƒ½åªåœ¨å¼€å¤´æˆ–ç»“å°¾æ·»åŠ è¿™äº›ç‰¹æ®Šå•è¯ã€‚æ— è®ºå¦‚ä½•ï¼Œtokenizer çŸ¥é“å“ªäº›æ˜¯å¿…éœ€çš„ï¼Œå¹¶ä¼šä¸ºä½ å¤„ç†è¿™äº›é—®é¢˜ã€‚

## å°ç»“ï¼šä» tokenizer åˆ°æ¨¡å‹ [[å°ç»“ï¼šä» tokenizer åˆ°æ¨¡å‹]]

ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ° `tokenizer` å¯¹è±¡åœ¨å¤„ç†æ–‡æœ¬æ—¶çš„æ‰€æœ‰æ­¥éª¤ï¼Œè®©æˆ‘ä»¬æœ€åå†çœ‹ä¸€æ¬¡å®ƒå¦‚ä½•é€šè¿‡ tokenizer  API å¤„ç†å¤šä¸ªåºåˆ—ï¼ˆå¡«å……ï¼ï¼‰ï¼Œéå¸¸é•¿çš„åºåˆ—ï¼ˆæˆªæ–­ï¼ï¼‰ä»¥åŠå¤šç§ç±»å‹çš„å¼ é‡ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
