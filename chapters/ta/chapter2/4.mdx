<FrameworkSwitchCourse {fw} />

# டோக்கனைசர்கள்[[tokenizers]]

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ta/chapter2/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ta/chapter2/section4_pt.ipynb"},
]} />

<Youtube id="VFp38yj8h3A"/>

டோக்கனைசர்கள் NLP பைப்லைனின் முக்கிய கூறுகளில் ஒன்றாகும். அவை ஒரு நோக்கத்திற்காக சேவை செய்கின்றன: உரையை மாதிரியால் செயலாக்கக்கூடிய தரவுகளாக மொழிபெயர்ப்பது. மாதிரிகள் எண்களை மட்டுமே செயலாக்க முடியும், எனவே டோக்கனைசர்கள் நமது உரை உள்ளீடுகளை எண் தரவுகளாக மாற்ற வேண்டும். இந்த பிரிவில், டோக்கனைசேஷன் பைப்லைனில் சரியாக என்ன நடக்கிறது என்பதை ஆராய்வோம்.

NLP பணிகளில், பொதுவாக செயலாக்கப்படும் தரவு மூல உரை. அத்தகைய உரைக்கு ஒரு எடுத்துக்காட்டு இங்கே:

```
Jim Henson was a puppeteer
```

இருப்பினும், மாதிரிகள் எண்களை மட்டுமே செயலாக்க முடியும், எனவே மூல உரையை எண்களாக மாற்றுவதற்கான வழியைக் கண்டுபிடிக்க வேண்டும். அதைத்தான் டோக்கனைசர்கள் செய்கின்றன, இதைச் செய்ய பல வழிகள் உள்ளன. மிகவும் அர்த்தமுள்ள பிரதிநிதித்துவத்தைக் கண்டுபிடிப்பதே குறிக்கோள் - அதாவது, மாதிரிக்கு மிகவும் அர்த்தமுள்ள ஒன்று - மற்றும், முடிந்தால், மிகச்சிறிய பிரதிநிதித்துவம்.

டோக்கனைசேஷன் அல்காரிதம்களின் சில எடுத்துக்காட்டுகளைப் பார்ப்போம், மேலும் டோக்கனைசேஷன் பற்றி உங்களுக்கு ஏற்படக்கூடிய சில கேள்விகளுக்கு பதிலளிக்க முயற்சிப்போம்.

## வார்த்தை அடிப்படையிலான[[word-based]]

<Youtube id="nhJxYji1aho"/>

நினைவுக்கு வரும் முதல் வகை டோக்கனைசர் _வார்த்தை அடிப்படையிலான_. இது பொதுவாக சில விதிகளுடன் அமைக்க மற்றும் பயன்படுத்த மிகவும் எளிதானது, மேலும் இது பெரும்பாலும் ஒழுக்கமான முடிவுகளை அளிக்கிறது. எடுத்துக்காட்டாக, கீழே உள்ள படத்தில், மூல உரையை வார்த்தைகளாகப் பிரித்து, அவை ஒவ்வொன்றிற்கும் ஒரு எண் பிரதிநிதித்துவத்தைக் கண்டுபிடிப்பதே குறிக்கோள்:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" alt="ஒரு வார்த்தை அடிப்படையிலான டோக்கனைசேஷன் எடுத்துக்காட்டு."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg" alt="ஒரு வார்த்தை அடிப்படையிலான டோக்கனைசேஷன் எடுத்துக்காட்டு."/>
</div>

உரையைப் பிரிக்க வெவ்வேறு வழிகள் உள்ளன. எடுத்துக்காட்டாக, பைத்தானின் `split()` செயல்பாட்டைப் பயன்படுத்துவதன் மூலம் உரையை வார்த்தைகளாக டோக்கனைஸ் செய்ய நாம் வெற்றிடத்தைப் பயன்படுத்தலாம்:

```py
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

நிறுத்தற்குறிகளுக்கான கூடுதல் விதிகளைக் கொண்ட வார்த்தை டோக்கனைசர்களின் மாறுபாடுகளும் உள்ளன. இந்த வகையான டோக்கனைசர் மூலம், நாம் சில பெரிய "சொல்லஞ்சரங்களுடன்" முடிவடையும், அங்கு ஒரு சொல்லஞ்சரம் என்பது நமது கார்பஸில் உள்ள மொத்த சுயாதீன டோக்கன்களின் எண்ணிக்கையால் வரையறுக்கப்படுகிறது.

ஒவ்வொரு வார்த்தைக்கும் 0 முதல் சொல்லஞ்சரத்தின் அளவு வரை ஒரு ஐடி ஒதுக்கப்படுகிறது. மாதிரி இந்த ஐடிகளைப் பயன்படுத்தி ஒவ்வொரு வார்த்தையையும் அடையாளம் காட்டுகிறது.

ஒரு மொழியை வார்த்தை அடிப்படையிலான டோக்கனைசர் மூலம் முழுமையாக மறைக்க விரும்பினால், மொழியில் உள்ள ஒவ்வொரு வார்த்தைக்கும் ஒரு அடையாளங்காட்டி இருக்க வேண்டும், இது ஒரு பெரிய அளவிலான டோக்கன்களை உருவாக்கும். எடுத்துக்காட்டாக, ஆங்கில மொழியில் 500,000 க்கும் மேற்பட்ட வார்த்தைகள் உள்ளன, எனவே ஒவ்வொரு வார்த்தையிலிருந்தும் ஒரு உள்ளீட்டு ஐடிக்கு ஒரு வரைபடத்தை உருவாக்க, நாம் அந்த பல ஐடிகளைக் கண்காணிக்க வேண்டும். மேலும், "dog" போன்ற வார்த்தைகள் "dogs" போன்ற வார்த்தைகளிலிருந்து வித்தியாசமாக குறிப்பிடப்படுகின்றன, மேலும் "dog" மற்றும் "dogs" ஒத்தவை என்பதை மாதிரிக்கு ஆரம்பத்தில் அறிய வழி இருக்காது: இது இரண்டு வார்த்தைகளையும் தொடர்பில்லாதவை என அடையாளம் காணும். "run" மற்றும் "running" போன்ற பிற ஒத்த வார்த்தைகளுக்கும் இது பொருந்தும், மாதிரி ஆரம்பத்தில் ஒத்ததாகக் காணாது.

இறுதியாக, நமது சொல்லஞ்சரத்தில் இல்லாத வார்த்தைகளைக் குறிக்க ஒரு தனிப்பயன் டோக்கன் தேவை. இது "தெரியாத" டோக்கன் என்று அழைக்கப்படுகிறது, இது பெரும்பாலும் "[UNK]" அல்லது "<unk>" எனக் குறிக்கப்படுகிறது. டோக்கனைசர் இந்த டோக்கன்களில் பலவற்றை உருவாக்குவதை நீங்கள் கண்டால் இது பொதுவாக ஒரு மோசமான அறிகுறியாகும், ஏனெனில் அது ஒரு வார்த்தையின் விவேகமான பிரதிநிதித்துவத்தை மீட்டெடுக்க முடியவில்லை, மேலும் நீங்கள் வழியில் தகவல்களை இழக்கிறீர்கள். சொல்லஞ்சரத்தை உருவாக்கும்போது, டோக்கனைசர் முடிந்தவரை சில வார்த்தைகளை அறியப்படாத டோக்கனாக டோக்கனைஸ் செய்யும் வகையில் அதைச் செய்வதே குறிக்கோள்.

அறியப்படாத டோக்கன்களின் அளவைக் குறைப்பதற்கான ஒரு வழி, ஒரு நிலை ஆழமாகச் செல்வது, _எழுத்து அடிப்படையிலான_ டோக்கனைசரைப் பயன்படுத்துவது.

## எழுத்து அடிப்படையிலான[[character-based]]

<Youtube id="ssLq_EK2jLE"/>

எழுத்து அடிப்படையிலான டோக்கனைசர்கள் உரையை வார்த்தைகளாக இல்லாமல் எழுத்துக்களாகப் பிரிக்கின்றன. இதற்கு இரண்டு முதன்மை நன்மைகள் உள்ளன:

- சொல்லஞ்சரம் மிகவும் சிறியது.
- சொல்லஞ்சரத்திற்கு வெளியே (தெரியாத) டோக்கன்கள் மிகக் குறைவு, ஏனெனில் ஒவ்வொரு வார்த்தையும் எழுத்துக்களிலிருந்து உருவாக்கப்படலாம்.

ஆனால் இங்கேயும் இடைவெளிகள் மற்றும் நிறுத்தற்குறிகள் குறித்து சில கேள்விகள் எழுகின்றன:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" alt="ஒரு எழுத்து அடிப்படையிலான டோக்கனைசேஷன் எடுத்துக்காட்டு."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg" alt="ஒரு எழுத்து அடிப்படையிலான டோக்கனைசேஷன் எடுத்துக்காட்டு."/>
</div>

இந்த அணுகுமுறையும் சரியானதல்ல. பிரதிநிதித்துவம் இப்போது வார்த்தைகளை விட எழுத்துக்களை அடிப்படையாகக் கொண்டிருப்பதால், உள்ளுணர்வாக, அது குறைவான அர்த்தமுள்ளதாக இருக்கிறது என்று ஒருவர் வாதிடலாம்: ஒவ்வொரு எழுத்தும் தனியாக அதிகம் அர்த்தப்படுத்துவதில்லை, அதேசமயம் வார்த்தைகளின் நிலை அப்படி. இருப்பினும், இது மீண்டும் மொழிக்கு ஏற்ப வேறுபடுகிறது; சீன மொழியில், எடுத்துக்காட்டாக, ஒவ்வொரு எழுத்தும் லத்தீன் மொழியில் உள்ள ஒரு எழுத்தை விட அதிக தகவல்களைக் கொண்டுள்ளது.

கருத்தில் கொள்ள வேண்டிய மற்றொரு விஷயம் என்னவென்றால், நமது மாதிரியால் செயலாக்கப்பட வேண்டிய மிக அதிக எண்ணிக்கையிலான டோக்கன்களுடன் நாம் முடிவடையும்: ஒரு வார்த்தை வார்த்தை அடிப்படையிலான டோக்கனைசருடன் ஒரே ஒரு டோக்கனாக மட்டுமே இருக்கும், அது எழுத்துக்களாக மாற்றப்படும்போது எளிதாக 10 அல்லது அதற்கு மேற்பட்ட டோக்கன்களாக மாறும்.

இரு உலகங்களிலும் சிறந்ததைப் பெற, இரண்டு அணுகுமுறைகளையும் இணைக்கும் மூன்றாவது நுட்பத்தைப் பயன்படுத்தலாம்: *துணை வார்த்தை டோக்கனைசேஷன்*.

## துணை வார்த்தை டோக்கனைசேஷன்[[subword-tokenization]]

<Youtube id="zHvTiHr506c"/>

துணை வார்த்தை டோக்கனைசேஷன் அல்காரிதம்கள் அடிக்கடி பயன்படுத்தப்படும் வார்த்தைகளை சிறிய துணை வார்த்தைகளாகப் பிரிக்கக்கூடாது, ஆனால் அரிதான வார்த்தைகளை அர்த்தமுள்ள துணை வார்த்தைகளாக சிதைக்க வேண்டும் என்ற கொள்கையை நம்பியுள்ளன.

உதாரணமாக, "annoyingly" என்பது ஒரு அரிதான வார்த்தையாகக் கருதப்படலாம், மேலும் அதை "annoying" மற்றும் "ly" என சிதைக்கலாம். இவை இரண்டும் தனித்தனி துணை வார்த்தைகளாக அடிக்கடி தோன்றும், அதே நேரத்தில் "annoyingly" என்பதன் பொருள் "annoying" மற்றும் "ly" ஆகியவற்றின் கூட்டுப் பொருளால் வைக்கப்படுகிறது.

ஒரு துணை வார்த்தை டோக்கனைசேஷன் அல்காரிதம் "Let's do tokenization!" என்ற வரிசையை எவ்வாறு டோக்கனைஸ் செய்யும் என்பதைக் காட்டும் ஒரு எடுத்துக்காட்டு இங்கே:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" alt="ஒரு துணை வார்த்தை டோக்கனைசேஷன் அல்காரிதம்."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg" alt="ஒரு துணை வார்த்தை டோக்கனைசேஷன் அல்காரிதம்."/>
</div>

இந்த துணை வார்த்தைகள் நிறைய சொற்பொருள் அர்த்தங்களை வழங்குகின்றன: எடுத்துக்காட்டாக, மேலே உள்ள எடுத்துக்காட்டில் "tokenization" என்பது "token" மற்றும் "ization" எனப் பிரிக்கப்பட்டது, இரண்டு டோக்கன்கள் ஒரு சொற்பொருள் அர்த்தத்தைக் கொண்டுள்ளன, அதே நேரத்தில் விண்வெளி-திறமையானவை (ஒரு நீண்ட வார்த்தையைக் குறிக்க இரண்டு டோக்கன்கள் மட்டுமே தேவை). இது சிறிய சொல்லஞ்சரங்களுடன் ஒப்பீட்டளவில் நல்ல கவரேஜையும், அறியப்படாத டோக்கன்கள் இல்லாததையும் பெற அனுமதிக்கிறது.

இந்த அணுகுமுறை துருக்கியம் போன்ற ஒட்டுமொத்த மொழிகளில் குறிப்பாக பயனுள்ளதாக இருக்கும், அங்கு நீங்கள் துணை வார்த்தைகளை ஒன்றாக இணைப்பதன் மூலம் (கிட்டத்தட்ட) தன்னிச்சையாக நீண்ட சிக்கலான வார்த்தைகளை உருவாக்கலாம்.

### மேலும் பல![[and-more]]

ஆச்சரியப்படத்தக்க வகையில், இன்னும் பல நுட்பங்கள் உள்ளன. சிலவற்றைக் குறிப்பிட:

- பைட்-நிலை BPE, GPT-2 இல் பயன்படுத்தப்பட்டது
- WordPiece, BERT இல் பயன்படுத்தப்பட்டது
- SentencePiece அல்லது Unigram, பல பன்மொழி மாதிரிகளில் பயன்படுத்தப்பட்டது

டோக்கனைசர்கள் எவ்வாறு செயல்படுகின்றன என்பதைப் பற்றிய போதுமான அறிவு இப்போது உங்களுக்கு இருக்க வேண்டும்.

## ஏற்றுதல் மற்றும் சேமித்தல்[[loading-and-saving]]

டோக்கனைசர்களை ஏற்றுவதும் சேமிப்பதும் மாடல்களைப் போலவே எளிமையானது. உண்மையில், இது அதே இரண்டு முறைகளை அடிப்படையாகக் கொண்டது: `from_pretrained()` மற்றும் `save_pretrained()`. இந்த முறைகள் டோக்கனைசரால் பயன்படுத்தப்படும் அல்காரிதம் (மாதிரியின் *கட்டிடக்கலை* போன்றது) மற்றும் அதன் சொல்லஞ்சரம் (மாதிரியின் *எடைகள்* போன்றது) ஆகியவற்றை ஏற்றும் அல்லது சேமிக்கும்.

BERT உடன் அதே சோதனைச் சாவடியுடன் பயிற்சி பெற்ற BERT டோக்கனைசரை ஏற்றுவது மாடலை ஏற்றுவது போலவே செய்யப்படுகிறது, தவிர நாம் `BertTokenizer` வகுப்பைப் பயன்படுத்துகிறோம்:

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

`AutoModel` ஐப் போலவே, `AutoTokenizer` வகுப்பும் சோதனைச் சாவடியின் பெயரின் அடிப்படையில் நூலகத்தில் உள்ள சரியான டோக்கனைசர் வகுப்பைப் பிடிக்கும், மேலும் எந்தவொரு சோதனைச் சாவடியுடனும் நேரடியாகப் பயன்படுத்தலாம்:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

முந்தைய பிரிவில் காட்டப்பட்டுள்ளபடி நாம் இப்போது டோக்கனைசரைப் பயன்படுத்தலாம்:

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ஒரு டோக்கனைசரைச் சேமிப்பது ஒரு மாடலைச் சேமிப்பதைப் போன்றது:

```py
tokenizer.save_pretrained("directory_on_my_computer")
```

[பாடம் 3](/course/chapter3) இல் `token_type_ids` பற்றி மேலும் பேசுவோம், மேலும் `attention_mask` விசையை சிறிது நேரம் கழித்து விளக்குவோம். முதலில், `input_ids` எவ்வாறு உருவாக்கப்படுகின்றன என்பதைப் பார்ப்போம். இதைச் செய்ய, டோக்கனைசரின் இடைநிலை முறைகளைப் பார்க்க வேண்டும்.

## குறியாக்கம்[[encoding]]

<Youtube id="Yffk5aydLzg"/>

உரையை எண்களாக மொழிபெயர்ப்பது _குறியாக்கம்_ எனப்படும். குறியாக்கம் இரண்டு-படி செயல்முறையில் செய்யப்படுகிறது: டோக்கனைசேஷன், அதைத் தொடர்ந்து உள்ளீட்டு ஐடிகளாக மாற்றுதல்.

நாம் பார்த்தபடி, முதல் படி உரையை வார்த்தைகளாக (அல்லது வார்த்தைகளின் பகுதிகள், நிறுத்தற்குறிகள் போன்றவை) பிரிப்பது, பொதுவாக *டோக்கன்கள்* என்று அழைக்கப்படுகிறது. அந்த செயல்முறையை நிர்வகிக்கக்கூடிய பல விதிகள் உள்ளன, அதனால்தான் மாதிரியின் பெயரைப் பயன்படுத்தி டோக்கனைசரை நாம் நிறுவ வேண்டும், மாதிரி முன்பயிற்சி செய்யப்பட்டபோது பயன்படுத்தப்பட்ட அதே விதிகளைப் பயன்படுத்துவதை உறுதிசெய்ய.

இரண்டாவது படி அந்த டோக்கன்களை எண்களாக மாற்றுவது, எனவே நாம் அவற்றிலிருந்து ஒரு டென்சரை உருவாக்கி அவற்றை மாதிரிக்கு ஊட்டலாம். இதைச் செய்ய, டோக்கனைசருக்கு ஒரு *சொல்லஞ்சரம்* உள்ளது, இது `from_pretrained()` முறையுடன் அதை நாம் நிறுவும்போது பதிவிறக்கும் பகுதியாகும். மீண்டும், மாதிரி முன்பயிற்சி செய்யப்பட்டபோது பயன்படுத்தப்பட்ட அதே சொல்லஞ்சரத்தைப் பயன்படுத்த வேண்டும்.

இரண்டு படிகளைப் பற்றிய சிறந்த புரிதலைப் பெற, அவற்றை தனித்தனியாக ஆராய்வோம். அந்த படிகளின் இடைநிலை முடிவுகளை உங்களுக்குக் காட்ட டோக்கனைசேஷன் பைப்லைனின் பகுதிகளைச் செய்யும் சில முறைகளைப் பயன்படுத்துவோம் என்பதை நினைவில் கொள்க, ஆனால் நடைமுறையில், உங்கள் உள்ளீடுகளில் டோக்கனைசரை நேரடியாக அழைக்க வேண்டும் (பிரிவு 2 இல் காட்டப்பட்டுள்ளபடி).

### டோக்கனைசேஷன்[[tokenization]]

டோக்கனைசேஷன் செயல்முறை டோக்கனைசரின் `tokenize()` முறையால் செய்யப்படுகிறது:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

இந்த முறையின் வெளியீடு சரங்களின் பட்டியல் அல்லது டோக்கன்கள்:

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

இந்த டோக்கனைசர் ஒரு துணை வார்த்தை டோக்கனைசர்: அதன் சொல்லஞ்சரத்தால் குறிப்பிடக்கூடிய டோக்கன்களைப் பெறும் வரை அது வார்த்தைகளைப் பிரிக்கிறது. `transformer` உடன் இங்கே அப்படித்தான், இது இரண்டு டோக்கன்களாகப் பிரிக்கப்பட்டுள்ளது: `transform` மற்றும் `##er`.

### டோக்கன்களிலிருந்து உள்ளீட்டு ஐடிகளுக்கு[[from-tokens-to-input-ids]]

உள்ளீட்டு ஐடிகளாக மாற்றுவது `convert_tokens_to_ids()` டோக்கனைசர் முறையால் கையாளப்படுகிறது:

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

இந்த வெளியீடுகள், பொருத்தமான கட்டமைப்பு டென்சராக மாற்றப்பட்டவுடன், இந்த அத்தியாயத்தில் முன்னர் காணப்பட்டபடி ஒரு மாதிரிக்கு உள்ளீடுகளாகப் பயன்படுத்தப்படலாம்.

<Tip>

✏️ **முயற்சி செய்து பாருங்கள்!** பிரிவு 2 இல் நாம் பயன்படுத்திய உள்ளீட்டு வாக்கியங்களில் கடைசி இரண்டு படிகளை (டோக்கனைசேஷன் மற்றும் உள்ளீட்டு ஐடிகளாக மாற்றுதல்) மீண்டும் செய்யவும் ("I've been waiting for a HuggingFace course my whole life." மற்றும் "I hate this so much!"). நாம் முன்பு பெற்ற அதே உள்ளீட்டு ஐடிகளைப் பெறுகிறீர்களா என்பதைச் சரிபார்க்கவும்!

</Tip>

## டிகோடிங்[[decoding]]

*டிகோடிங்* என்பது வேறு வழியில் செல்கிறது: சொல்லஞ்சரக் குறியீடுகளிலிருந்து, நாம் ஒரு சரத்தைப் பெற விரும்புகிறோம். இதை `decode()` முறையுடன் பின்வருமாறு செய்யலாம்:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

`decode` முறை குறியீடுகளை மீண்டும் டோக்கன்களாக மாற்றுவது மட்டுமல்லாமல், ஒரே வார்த்தைகளின் பகுதியாக இருந்த டோக்கன்களை ஒன்றிணைத்து படிக்கக்கூடிய வாக்கியத்தை உருவாக்குகிறது என்பதைக் கவனியுங்கள். நாம் புதிய உரையை கணிக்கும் மாதிரிகளைப் பயன்படுத்தும்போது இந்த நடத்தை மிகவும் பயனுள்ளதாக இருக்கும் (ஒரு தூண்டுதலிலிருந்து உருவாக்கப்பட்ட உரை, அல்லது மொழிபெயர்ப்பு அல்லது சுருக்கம் போன்ற வரிசைக்கு-வரிசை சிக்கல்களுக்கு).

இப்போது ஒரு டோக்கனைசர் கையாளக்கூடிய அணு செயல்பாடுகளை நீங்கள் புரிந்து கொள்ள வேண்டும்: டோக்கனைசேஷன், ஐடிகளாக மாற்றுதல் மற்றும் ஐடிகளை மீண்டும் ஒரு சரமாக மாற்றுதல். இருப்பினும், நாம் பனிப்பாறையின் நுனியை மட்டுமே சுரண்டியுள்ளோம். பின்வரும் பிரிவில், நமது அணுகுமுறையை அதன் வரம்புகளுக்கு எடுத்துச் சென்று அவற்றை எவ்வாறு சமாளிப்பது என்று பார்ப்போம்.
