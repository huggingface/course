<FrameworkSwitchCourse {fw} />

# роЕройрпИродрпНродрпИропрпБроорпН роТройрпНро▒ро╛роХ роЗрогрпИродрпНродро▓рпН[[putting-it-all-together]]

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ta/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ta/chapter2/section6_pt.ipynb"},
]} />

роХроЯроирпНрод роЪро┐ро▓ рокро┐ро░ро┐ро╡рпБроХро│ро┐ро▓рпН, рокрпЖро░рпБроорпНрокро╛ро▓ро╛рой ро╡рпЗро▓рпИроХро│рпИ роХрпИропро╛ро▓рпН роЪрпЖропрпНроп роОроЩрпНроХро│ро╛ро▓рпН роорпБроЯро┐роирпНродро╡ро░рпИ роорпБропро▒рпНроЪро┐родрпНродрпБ ро╡ро░рпБроХро┐ро▒рпЛроорпН. роЯрпЛроХрпНроХройрпИроЪро░рпНроХро│рпН роОро╡рпНро╡ро╛ро▒рпБ роЪрпЖропро▓рпНрокроЯрпБроХро┐ройрпНро▒рой роОройрпНрокродрпИ роиро╛роЩрпНроХро│рпН роЖро░ро╛ропрпНроирпНродрпЛроорпН рооро▒рпНро▒рпБроорпН роЯрпЛроХрпНроХройрпИроЪрпЗро╖ройрпН, роЙро│рпНро│рпАроЯрпНроЯрпБ роРроЯро┐роХро│ро╛роХ рооро╛ро▒рпНро▒рпБродро▓рпН, рокрпЗроЯро┐роЩрпН, роЪрпБро░рпБроХрпНроХрпБродро▓рпН рооро▒рпНро▒рпБроорпН роХро╡рой роорпБроХроорпВроЯро┐роХро│рпИрокрпН рокро╛ро░рпНродрпНродрпЛроорпН.

роЗро░рпБрокрпНрокро┐ройрпБроорпН, рокро┐ро░ро┐ро╡рпБ 2 роЗро▓рпН роиро╛роорпН рокро╛ро░рпНродрпНродродрпБ рокрпЛро▓рпН, ЁЯдЧ роЯро┐ро░ро╛ройрпНро╕рпНрокро╛ро░рпНрооро░рпНро╕рпН API роЖройродрпБ роЙропро░рпН-роиро┐ро▓рпИ роЪрпЖропро▓рпНрокро╛роЯрпНроЯро┐ройрпН роорпВро▓роорпН роироороХрпНроХро╛роХ роЗродрпИропрпЖро▓рпНро▓ро╛роорпН роХрпИропро╛ро│ роорпБроЯро┐ропрпБроорпН, роЕродрпИ роиро╛роорпН роЗроЩрпНроХрпЗ роЖро░ро╛ропрпНро╡рпЛроорпН. роЙроЩрпНроХро│рпН `tokenizer`-роР роирпЗро░роЯро┐ропро╛роХ ро╡ро╛роХрпНроХро┐ропродрпНродро┐ро▓рпН роЕро┤рпИроХрпНроХрпБроорпНрокрпЛродрпБ, роЙроЩрпНроХро│рпН рооро╛родро┐ро░ро┐ роорпВро▓роорпН роЕройрпБрокрпНрокродрпН родропро╛ро░ро╛роХ роЙро│рпНро│ роЙро│рпНро│рпАроЯрпБроХро│рпИ роирпАроЩрпНроХро│рпН родро┐ро░рпБроорпНрокрокрпН рокрпЖро▒рпБро╡рпАро░рпНроХро│рпН:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

роЗроЩрпНроХрпЗ, `model_inputs` рооро╛ро▒ро┐ роТро░рпБ рооро╛родро┐ро░ро┐ роиройрпНроХрпБ роЪрпЖропро▓рпНрокроЯ родрпЗро╡рпИропро╛рой роЕройрпИродрпНродрпИропрпБроорпН роХрпКрогрпНроЯрпБро│рпНро│родрпБ. роЯро┐ро╕рпНроЯро┐ро▓рпНрокро░рпНроЯрпНроЯрпБроХрпНроХрпБ, роЕродро┐ро▓рпН роЙро│рпНро│рпАроЯрпНроЯрпБ роРроЯро┐роХро│рпН рооро▒рпНро▒рпБроорпН роХро╡рой роорпБроХроорпВроЯро┐ роЖроХро┐ропро╡рпИ роЕроЯроЩрпНроХрпБроорпН. роХрпВроЯрпБродро▓рпН роЙро│рпНро│рпАроЯрпБроХро│рпИ роПро▒рпНроХрпБроорпН рокро┐ро▒ рооро╛родро┐ро░ро┐роХро│рпБроорпН `tokenizer` рокрпКро░рпБро│ро╛ро▓рпН роЕроирпНрод ро╡рпЖро│ро┐ропрпАроЯрпБроХро│рпИроХрпН роХрпКрогрпНроЯро┐ро░рпБроХрпНроХрпБроорпН.

роХрпАро┤рпЗропрпБро│рпНро│ роЪро┐ро▓ роОроЯрпБродрпНродрпБроХрпНроХро╛роЯрпНроЯрпБроХро│ро┐ро▓рпН роиро╛роорпН рокро╛ро░рпНрокрпНрокродрпБ рокрпЛро▓рпН, роЗроирпНрод роорпБро▒рпИ рооро┐роХро╡рпБроорпН роЪроХрпНродро┐ ро╡ро╛ропрпНроирпНродродрпБ. роорпБродро▓ро┐ро▓рпН, роЗродрпБ роТро░рпБ роТро▒рпНро▒рпИ ро╡ро░ро┐роЪрпИропрпИ роЯрпЛроХрпНроХройрпИро╕рпН роЪрпЖропрпНропро▓ро╛роорпН:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

роЗродрпБ API роЗро▓рпН роОроирпНрод рооро╛ро▒рпНро▒роорпБроорпН роЗро▓рпНро▓ро╛рооро▓рпН, роТро░рпЗ роирпЗро░родрпНродро┐ро▓рпН рокро▓ ро╡ро░ро┐роЪрпИроХро│рпИропрпБроорпН роХрпИропро╛ро│рпБроХро┐ро▒родрпБ:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

роЗродрпБ рокро▓ роирпЛроХрпНроХроЩрпНроХро│ро┐ройрпНрокроЯро┐ рокрпЗроЯрпН роЪрпЖропрпНропро▓ро╛роорпН:

```py
# ро╡ро░ро┐роЪрпИроХро│рпИ роЕродро┐роХрокроЯрпНроЪ ро╡ро░ро┐роЪрпИ роирпАро│роорпН ро╡ро░рпИ рокрпЗроЯрпН роЪрпЖропрпНропрпБроорпН
model_inputs = tokenizer(sequences, padding="longest")

# ро╡ро░ро┐роЪрпИроХро│рпИ рооро╛родро┐ро░ро┐ роЕродро┐роХрокроЯрпНроЪ роирпАро│роорпН ро╡ро░рпИ рокрпЗроЯрпН роЪрпЖропрпНропрпБроорпН
# (BERT роЕро▓рпНро▓родрпБ DistilBERT роХрпНроХрпБ 512)
model_inputs = tokenizer(sequences, padding="max_length")

# ро╡ро░ро┐роЪрпИроХро│рпИ роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роЕродро┐роХрокроЯрпНроЪ роирпАро│роорпН ро╡ро░рпИ рокрпЗроЯрпН роЪрпЖропрпНропрпБроорпН
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

роЗродрпБ ро╡ро░ро┐роЪрпИроХро│рпИропрпБроорпН роЪрпБро░рпБроХрпНроХро▓ро╛роорпН:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# рооро╛родро┐ро░ро┐ роЕродро┐роХрокроЯрпНроЪ роирпАро│родрпНродрпИ ро╡ро┐роЯ роирпАро│рооро╛рой ро╡ро░ро┐роЪрпИроХро│рпИ роЪрпБро░рпБроХрпНроХрпБроорпН
# (BERT роЕро▓рпНро▓родрпБ DistilBERT роХрпНроХрпБ 512)
model_inputs = tokenizer(sequences, truncation=True)

# роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роЕродро┐роХрокроЯрпНроЪ роирпАро│родрпНродрпИ ро╡ро┐роЯ роирпАро│рооро╛рой ро╡ро░ро┐роЪрпИроХро│рпИ роЪрпБро░рпБроХрпНроХрпБроорпН
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` рокрпКро░рпБро│рпН роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роХроЯрпНроЯроорпИрокрпНрокрпБ роЯрпЖройрпНроЪро░рпНроХро│ро╛роХ рооро╛ро▒рпНро▒рпБро╡родрпИроХрпН роХрпИропро╛ро│ роорпБроЯро┐ропрпБроорпН, рокро┐ройрпНройро░рпН роЕро╡рпИ роирпЗро░роЯро┐ропро╛роХ рооро╛родро┐ро░ро┐роХрпНроХрпБ роЕройрпБрокрпНрокрокрпНрокроЯро▓ро╛роорпН. роОроЯрпБродрпНродрпБроХрпНроХро╛роЯрпНроЯро╛роХ, рокро┐ройрпНро╡ро░рпБроорпН роХрпБро▒ро┐ропрпАроЯрпБ рооро╛родро┐ро░ро┐ропро┐ро▓рпН, ро╡рпЖро╡рпНро╡рпЗро▒рпБ роХроЯрпНроЯроорпИрокрпНрокрпБроХро│ро┐ро▓ро┐ро░рпБроирпНродрпБ роЯрпЖройрпНроЪро░рпНроХро│рпИродрпН родро┐ро░рпБрокрпНрокро┐родрпН родро░рпБрооро╛ро▒рпБ роЯрпЛроХрпНроХройрпИроЪро░рпИродрпН родрпВрогрпНроЯрпБроХро┐ро▒рпЛроорпН тАФ `"pt"` рокрпИроЯро╛ро░рпНроЪрпН роЯрпЖройрпНроЪро░рпНроХро│рпИропрпБроорпН `"np"` NumPy ро╡ро░ро┐роЪрпИроХро│рпИропрпБроорпН ро╡ро┤роЩрпНроХрпБроХро┐ро▒родрпБ:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch роЯрпЖройрпНроЪро░рпНроХро│рпИ ро╡ро┤роЩрпНроХрпБроХро┐ро▒родрпБ
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# NumPy ро╡ро░ро┐роЪрпИроХро│рпИ ро╡ро┤роЩрпНроХрпБроХро┐ро▒родрпБ
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## роЪро┐ро▒рокрпНрокрпБ роЯрпЛроХрпНроХройрпНроХро│рпН[[special-tokens]]

роЯрпЛроХрпНроХройрпИроЪро░ро╛ро▓рпН родро┐ро░рпБрокрпНрокро┐ропро│ро┐роХрпНроХрокрпНрокроЯрпНроЯ роЙро│рпНро│рпАроЯрпНроЯрпБ роРроЯро┐роХро│рпИрокрпН рокро╛ро░рпНродрпНродро╛ро▓рпН, роЕро╡рпИ роиро╛роорпН роорпБройрпНрокрпБ ро╡рпИродрпНродро┐ро░рпБроирпНродродрпИ ро╡ро┐роЯ роЪро▒рпНро▒рпБ ро╡ро┐родрпНродро┐ропро╛роЪрооро╛роХ роЗро░рпБрокрпНрокродрпИроХрпН роХро╛рогрпНрокрпЛроорпН:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

роТро░рпБ роЯрпЛроХрпНроХройрпН роРроЯро┐ роЖро░роорпНрокродрпНродро┐ро▓рпБроорпН, роТройрпНро▒рпБ роорпБроЯро┐ро╡ро┐ро▓рпБроорпН роЪрпЗро░рпНроХрпНроХрокрпНрокроЯрпНроЯродрпБ. роЗродрпБ роОродрпИрокрпН рокро▒рпНро▒ро┐ропродрпБ роОройрпНрокродрпИрокрпН рокро╛ро░рпНроХрпНроХ, роорпЗро▓рпЗ роЙро│рпНро│ роЗро░рогрпНроЯрпБ роРроЯро┐ ро╡ро░ро┐роЪрпИроХро│рпИропрпБроорпН роЯро┐роХрпЛроЯрпН роЪрпЖропрпНро╡рпЛроорпН:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

роЯрпЛроХрпНроХройрпИроЪро░рпН роЖро░роорпНрокродрпНродро┐ро▓рпН `[CLS]` роОройрпНро▒ роЪро┐ро▒рокрпНрокрпБ ро╡ро╛ро░рпНродрпНродрпИропрпИропрпБроорпН роорпБроЯро┐ро╡ро┐ро▓рпН `[SEP]` роОройрпНро▒ роЪро┐ро▒рокрпНрокрпБ ро╡ро╛ро░рпНродрпНродрпИропрпИропрпБроорпН роЪрпЗро░рпНродрпНродродрпБ. роПройрпЖройрпНро▒ро╛ро▓рпН, рооро╛родро┐ро░ро┐ роЕро╡ро▒рпНро▒рпБроЯройрпН роорпБройрпНрокрпЗ рокропро┐ро▒рпНроЪро┐ рокрпЖро▒рпНро▒родрпБ, роОройро╡рпЗ роЕройрпБрооро╛ройродрпНродро┐ро▒рпНроХрпБ роЕродрпЗ роорпБроЯро┐ро╡рпБроХро│рпИрокрпН рокрпЖро▒, роиро╛роорпБроорпН роЕро╡ро▒рпНро▒рпИроЪрпН роЪрпЗро░рпНроХрпНроХ ро╡рпЗрогрпНроЯрпБроорпН. роЪро┐ро▓ рооро╛родро┐ро░ро┐роХро│рпН роЪро┐ро▒рокрпНрокрпБ ро╡ро╛ро░рпНродрпНродрпИроХро│рпИроЪрпН роЪрпЗро░рпНрокрпНрокродро┐ро▓рпНро▓рпИ, роЕро▓рпНро▓родрпБ ро╡рпЗро▒рпБрокроЯрпНроЯро╡ро▒рпНро▒рпИроЪрпН роЪрпЗро░рпНроХрпНроХро┐ройрпНро▒рой; рооро╛родро┐ро░ро┐роХро│рпН роЗроирпНрод роЪро┐ро▒рокрпНрокрпБ ро╡ро╛ро░рпНродрпНродрпИроХро│рпИ роЖро░роорпНрокродрпНродро┐ро▓рпН роороЯрпНроЯрпБроорпН, роЕро▓рпНро▓родрпБ роорпБроЯро┐ро╡ро┐ро▓рпН роороЯрпНроЯрпБроорпН роЪрпЗро░рпНроХрпНроХро▓ро╛роорпН. роОроирпНродро╡рпКро░рпБ роЪроирпНродро░рпНрокрпНрокродрпНродро┐ро▓рпБроорпН, роЯрпЛроХрпНроХройрпИроЪро░рпН роОро╡рпИ роОродро┐ро░рпНрокро╛ро░рпНроХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой роОройрпНрокродрпИ роЕро▒ро┐роирпНродро┐ро░рпБроХрпНроХро┐ро▒родрпБ рооро▒рпНро▒рпБроорпН роЙроЩрпНроХро│рпБроХрпНроХро╛роХ роЗродрпИроХрпН роХрпИропро╛ро│рпБроорпН.

## роорпБроЯро┐ро╡ро╛роХ: роЯрпЛроХрпНроХройрпИроЪро░ро┐ро▓ро┐ро░рпБроирпНродрпБ рооро╛родро┐ро░ро┐роХрпНроХрпБ[[wrapping-up-from-tokenizer-to-model]]

`tokenizer` рокрпКро░рпБро│рпН роЙро░рпИроХро│ро┐ро▓рпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроорпНрокрпЛродрпБ рокропройрпНрокроЯрпБродрпНродрпБроорпН роЕройрпИродрпНродрпБ родройро┐рокрпНрокроЯрпНроЯ рокроЯро┐роХро│рпИропрпБроорпН роЗрокрпНрокрпЛродрпБ роиро╛роорпН рокро╛ро░рпНродрпНродрпБро│рпНро│рпЛроорпН, роЕродройрпН роорпБроХрпНроХро┐роп API роорпВро▓роорпН рокро▓ ро╡ро░ро┐роЪрпИроХро│рпН (рокрпЗроЯро┐роЩрпН!), рооро┐роХ роирпАрогрпНроЯ ро╡ро░ро┐роЪрпИроХро│рпН (роЪрпБро░рпБроХрпНроХрпБродро▓рпН!), рооро▒рпНро▒рпБроорпН рокро▓ ро╡роХрпИропро╛рой роЯрпЖройрпНроЪро░рпНроХро│рпИ роЗродрпБ роОро╡рпНро╡ро╛ро▒рпБ роХрпИропро╛ро│ роорпБроЯро┐ропрпБроорпН роОройрпНрокродрпИ роТро░рпБ роЗро▒рпБродро┐ роорпБро▒рпИропро╛роХрокрпН рокро╛ро░рпНрокрпНрокрпЛроорпН:

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
