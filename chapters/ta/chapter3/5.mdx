# கற்றல் வளைவுகளைப் புரிந்துகொள்வது[[understanding-learning-curves]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ta/chapter3/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ta/chapter3/section5.ipynb"},
]} />

நீங்கள் `Trainer` API மற்றும் தனிப்பயன் பயிற்சி சுழற்சிகள் இரண்டையும் பயன்படுத்தி உத்தமமாகச் சரிசெய்வதை (fine-tuning) செயல்படுத்தக் கற்றுக்கொண்டதால், முடிவுகளை எவ்வாறு விளக்குவது என்பதைப் புரிந்துகொள்வது மிகவும் முக்கியம். கற்றல் வளைவுகள் (Learning curves) உங்கள் மாதிரியின் செயல்திறனைப் பயிற்சியின் போது மதிப்பீடு செய்வதற்கும், செயல்திறனைக் குறைக்கும் முன் சாத்தியமான சிக்கல்களைக் கண்டறிவதற்கும் மதிப்புமிக்க கருவிகளாகும்.

இந்த பகுதியில், துல்லியம் மற்றும் இழப்பு வளைவுகளை எவ்வாறு படிப்பது மற்றும் விளக்குவது, வெவ்வேறு வளைவு வடிவங்கள் நமது மாதிரியின் நடத்தை பற்றி என்ன சொல்கின்றன என்பதைப் புரிந்துகொள்வது, மற்றும் பொதுவான பயிற்சி சிக்கல்களை எவ்வாறு தீர்ப்பது என்பதைக் கற்றுக்கொள்வோம்.

## கற்றல் வளைவுகள் என்றால் என்ன?[[what-are-learning-curves]]

கற்றல் வளைவுகள் என்பது பயிற்சியின் போது காலப்போக்கில் உங்கள் மாதிரியின் செயல்திறன் அளவீடுகளின் காட்சிப் பிரதிநிதித்துவங்கள் ஆகும். கண்காணிக்க வேண்டிய இரண்டு மிக முக்கியமான வளைவுகள்:

- **இழப்பு வளைவுகள் (Loss curves)**: பயிற்சியின் படிகள் அல்லது காலகட்டங்களில் (epochs) மாதிரியின் பிழை (இழப்பு) எவ்வாறு மாறுகிறது என்பதைக் காட்டுகிறது.
- **துல்லிய வளைவுகள் (Accuracy curves)**: பயிற்சியின் படிகள் அல்லது காலகட்டங்களில் சரியான கணிப்புகளின் சதவீதத்தைக் காட்டுகிறது.

இந்த வளைவுகள் நமது மாதிரி திறம்பட கற்கிறதா என்பதைப் புரிந்துகொள்ள உதவுகின்றன மற்றும் செயல்திறனை மேம்படுத்துவதற்கான மாற்றங்களைச் செய்வதில் நமக்கு வழிகாட்டுகின்றன. Transformers-இல், இந்த அளவீடுகள் ஒவ்வொரு தொகுதிக்கும் (batch) தனித்தனியாகக் கணக்கிடப்பட்டு பின்னர் வட்டில் பதிவு செய்யப்படுகின்றன. பின்னர், இந்த வளைவுகளைக் காட்சிப்படுத்தவும், காலப்போக்கில் நமது மாதிரியின் செயல்திறனைக் கண்காணிக்கவும் [Weights & Biases](https://wandb.ai/) போன்ற நூலகங்களைப் பயன்படுத்தலாம்.

### இழப்பு வளைவுகள்[[loss-curves]]

இழப்பு வளைவு காலப்போக்கில் மாதிரியின் பிழை எவ்வாறு குறைகிறது என்பதைக் காட்டுகிறது. ஒரு வழக்கமான வெற்றிகரமான பயிற்சி ஓட்டத்தில், கீழே உள்ளதைப் போன்ற ஒரு வளைவைக் காண்பீர்கள்:

![Loss Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/1.png)

- **அதிக ஆரம்ப இழப்பு**: மாதிரி மேம்படுத்தல் இல்லாமல் தொடங்குகிறது, எனவே கணிப்புகள் ஆரம்பத்தில் மோசமாக இருக்கும்.
- **குறையும் இழப்பு**: பயிற்சி முன்னேறும்போது, இழப்பு பொதுவாகக் குறைய வேண்டும்.
- **ஒருங்கிணைதல் (Convergence)**: இறுதியில், இழப்பு ஒரு குறைந்த மதிப்பில் நிலைபெறுகிறது, இது மாதிரி தரவுகளில் உள்ள வடிவங்களைக் கற்றுக்கொண்டதைக் குறிக்கிறது.

முந்தைய அத்தியாயங்களில் செய்தது போல, இந்த அளவீடுகளைக் கண்காணிக்கவும், அவற்றை ஒரு டாஷ்போர்டில் காட்சிப்படுத்தவும் `Trainer` API-ஐப் பயன்படுத்தலாம். Weights & Biases உடன் இதை எப்படி செய்வது என்பதற்கான ஒரு எடுத்துக்காட்டு கீழே உள்ளது.

```python
# Example of tracking loss during training with the Trainer
from transformers import Trainer, TrainingArguments
import wandb

# Train and automatically log metrics
trainer.train()
```

### துல்லிய வளைவுகள்[[accuracy-curves]]

துல்லிய வளைவு காலப்போக்கில் சரியான கணிப்புகளின் சதவீதத்தைக் காட்டுகிறது. இழப்பு வளைவுகளைப் போலல்லாமல், துல்லிய வளைவுகள் பொதுவாக மாதிரி கற்கும்போது அதிகரிக்க வேண்டும் மற்றும் பொதுவாக இழப்பு வளைவை விட அதிகமான படிகளைக் கொண்டிருக்கலாம்.

![Accuracy Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/2.png)

- **குறைவாகத் தொடங்குதல்**: ஆரம்பத் துல்லியம் குறைவாக இருக்க வேண்டும், ஏனெனில் மாதிரி இன்னும் தரவுகளில் உள்ள வடிவங்களைக் கற்றுக்கொள்ளவில்லை.
- **அதிகரிக்கும் துல்லியம்**: பயிற்சி முன்னேறும்போது, துல்லியம் பொதுவாக அதிகரிக்க வேண்டும்.
- **பீடபூமிகளைக் காட்டலாம் (May show plateaus)**: துல்லியம் மென்மையாக அதிகரிப்பதற்குப் பதிலாக தனித்தனி தாவல்களில் அதிகரிக்கிறது, ஏனெனில் மாதிரி உண்மையான லேபிள்களுக்கு நெருக்கமான கணிப்புகளைச் செய்கிறது.

<Tip>

இழப்பு மற்றும் துல்லிய வளைவுகள் பொதுவாக வெவ்வேறு வடிவங்களைக் கொண்டுள்ளன. இழப்பு வளைவுகள் மென்மையாகக் குறைகின்றன, அதேசமயம் துல்லிய வளைவுகள் பெரும்பாலும் "படிப்படியான" வடிவத்தைக் காட்டுகின்றன. ஏனென்றால், இழப்பு என்பது ஒரு தொடர்ச்சியான மதிப்பு, ஆனால் துல்லியம் என்பது தனித்தனி கணிப்புகளை அடிப்படையாகக் கொண்டது.

</Tip>

## கற்றல் வளைவுகளை எவ்வாறு விளக்குவது[[how-to-interpret-learning-curves]]

ஒரு நன்கு நடந்த பயிற்சி ஓட்டம் பொதுவாகக் கீழே உள்ளதைப் போன்ற வளைவு வடிவங்களைக் காட்டுகிறது:

![Healthy Loss Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/5.png)

மேலே உள்ள விளக்கப்படத்தைப் பார்ப்போம். இது இழப்பு வளைவு (இடதுபுறம்) மற்றும் அதனுடன் தொடர்புடைய துல்லிய வளைவு (வலதுபுறம்) இரண்டையும் காட்டுகிறது. இந்த வளைவுகள் தனித்துவமான குணாதிசயங்களைக் கொண்டுள்ளன.

#### பயிற்சியின் போது[[during-training]]

பயிற்சியின் போது உங்கள் கற்றல் வளைவுகளைக் கண்காணிப்பது, சிக்கல்களை முன்கூட்டியே கண்டறிந்து தலையிட உங்களை அனுமதிக்கிறது. கவனிக்க வேண்டிய சில முக்கிய விஷயங்கள் இங்கே:

1. **இழப்பு ஒருங்கிணைதல்**: இழப்பு இன்னும் குறைகிறதா அல்லது அது ஒரு பீடபூமியை அடைந்துவிட்டதா?
2. **அதிகப் பொருத்தத்திற்கான அறிகுறிகள் (Overfitting signs)**: பயிற்சி இழப்பு குறையும்போது சரிபார்ப்பு இழப்பு (validation loss) அதிகரிக்கத் தொடங்குகிறதா?
3. **கற்றல் விகிதம் (Learning rate)**: வளைவுகள் மிகவும் ஒழுங்கற்றவையா (LR மிக அதிகம்) அல்லது மிகவும் தட்டையானவையா (LR மிகக் குறைவு)?
4. **நிலைத்தன்மை**: சிக்கல்களைக் குறிக்கும் திடீர் கூர்முனைகள் அல்லது வீழ்ச்சிகள் உள்ளதா?

#### பயிற்சிக்குப் பிறகு[[after-training]]

பயிற்சி முடிந்ததும், இறுதி மாதிரியின் செயல்திறனை மதிப்பிடுவதற்கு வளைவுகள் உதவுகின்றன. முக்கிய கருத்தாய்வுகள்:

1. **இறுதி செயல்திறன்**: இறுதி சரிபார்ப்பு இழப்பு மற்றும் துல்லியம் உங்கள் தேவைகளைப் பூர்த்தி செய்கிறதா?
2. **ஒட்டுமொத்த பொருத்தம்**: பயிற்சி மற்றும் சரிபார்ப்பு வளைவுகளுக்கு இடையில் குறிப்பிடத்தக்க இடைவெளி உள்ளதா, இது அதிகப் பொருத்தத்தைக் குறிக்கிறது?

## பொதுவான சிக்கல்கள் மற்றும் அவற்றின் தீர்வுகள்[[common-issues-and-their-solutions]]

#### 1. அதிகப் பொருத்தம் (Overfitting)[[overfitting]]

அதிகப் பொருத்தம் என்பது உங்கள் மாதிரி பயிற்சித் தரவை மிக நன்றாகக் கற்றுக்கொண்டு, புதிய, காணப்படாத தரவுகளுக்குப் பொதுமைப்படுத்தத் தவறும்போது நிகழ்கிறது.

![Overfitting](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/10.png)

**அறிகுறிகள்:**

- பயிற்சி இழப்பு தொடர்ந்து குறையும்போது சரிபார்ப்பு இழப்பு அதிகரிக்கிறது அல்லது பீடபூமியை அடைகிறது.
- பயிற்சி மற்றும் சரிபார்ப்புத் துல்லியத்திற்கு இடையில் பெரிய இடைவெளி.
- சரிபார்ப்புத் துல்லியத்தை விட பயிற்சித் துல்லியம் மிகவும் அதிகம்.

**அதிகப் பொருத்தத்திற்கான தீர்வுகள்:**
- **ஒழுங்குபடுத்துதல் (Regularization)**: dropout, weight decay, அல்லது பிற ஒழுங்குபடுத்தும் நுட்பங்களைச் சேர்க்கவும்.
- **தரவு பெருக்குதல் (Data augmentation)**: பயிற்சித் தரவின் பன்முகத்தன்மையை அதிகரிக்கவும்.
- **முன்கூட்டியே நிறுத்துதல் (Early stopping)**: சரிபார்ப்பு இழப்பு மேம்படுவதை நிறுத்தும்போது பயிற்சியை நிறுத்தவும்.

கீழே உள்ள எடுத்துக்காட்டில், சரிபார்ப்பு இழப்பு மேம்படுவதை நிறுத்தும்போது பயிற்சியை நிறுத்த `EarlyStoppingCallback`-ஐப் பயன்படுத்துகிறோம்.

```python
from transformers import Trainer, TrainingArguments
from transformers.integrations import EarlyStoppingCallback

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    # The following parameters are defined elsewhere in the full notebook
    # data_collator=data_collator,
    # processing_class=tokenizer,
    # compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

# trainer.train()
```

#### 2. குறைந்தப் பொருத்தம் (Underfitting)[[underfitting]]

குறைந்தப் பொருத்தம் என்பது உங்கள் மாதிரி பயிற்சித் தரவில் உள்ள அடிப்படை வடிவங்களைக் கற்றுக்கொள்ளத் தவறும்போது நிகழ்கிறது.

![Underfitting](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/11.png)

**அறிகுறிகள்:**
- பயிற்சி மற்றும் சரிபார்ப்பு இழப்பு இரண்டும் அதிகமாக இருந்து, பீடபூமியை அடைகின்றன.
- பயிற்சி மற்றும் சரிபார்ப்புத் துல்லியம் இரண்டும் குறைவாக இருக்கும்.

**குறைந்தப் பொருத்தத்திற்கான தீர்வுகள்:**
- **மாதிரி திறனை அதிகரிக்கவும்**: ஒரு பெரிய மாதிரியைப் பயன்படுத்தவும்.
- **அம்சப் பொறியியல் (Feature engineering)**: மிகவும் பொருத்தமான அம்சங்களைச் சேர்க்கவும்.
- **பயிற்சியை நீட்டிக்கவும்**: மாதிரிக்கு கற்க அதிக நேரம் கொடுக்கவும்.
- **தரவு தரத்தைச் சரிபார்க்கவும்**: உங்கள் தரவு சரியாக முன்தயாரிக்கப்பட்டுள்ளதா என்பதை உறுதிப்படுத்தவும்.

கீழே உள்ள மாதிரியில், மாதிரி தரவுகளில் உள்ள வடிவங்களைக் கற்றுக்கொள்ள முடியுமா என்று பார்க்க அதிக காலகட்டங்களுக்குப் பயிற்சி அளிக்கிறோம்.

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
)
```

#### 3. ஒழுங்கற்ற கற்றல் வளைவுகள்[[erratic-learning-curves]]

ஒழுங்கற்ற அல்லது மிகவும் ஏற்ற இறக்கமான வளைவுகள் பயிற்சி செயல்முறையில் நிலையற்றத்தன்மையைக் குறிக்கலாம்.

![Erratic Learning Curves](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/12.png)

**அறிகுறிகள்:**
- இழப்பு மற்றும் துல்லியத்தில் திடீர் கூர்முனைகள் அல்லது வீழ்ச்சிகள்.
- மிகவும் சத்தமான அல்லது ஒழுங்கற்ற வளைவுகள்.

**ஒழுங்கற்ற வளைவுகளுக்கான தீர்வுகள்:**
- **கற்றல் விகிதத்தைக் குறைக்கவும்**: ஒரு குறைந்த கற்றல் விகிதம் பயிற்சியை நிலைப்படுத்த உதவும்.
- **தொகுதி அளவை அதிகரிக்கவும் (Increase batch size)**: பெரிய தொகுதிகள் மிகவும் நிலையான சரிவுகளை (gradients) வழங்குகின்றன.
- **சரிவு கிளிப்பிங் (Gradient clipping)**: வெடிக்கும் சரிவுகளைத் தடுக்கவும்.
- **சிறந்த தரவு முன்தயாரிப்பு**: சீரான தரவு தரத்தை உறுதிப்படுத்தவும்.

கீழே உள்ள மாதிரியில், கற்றல் விகிதத்தைக் குறைத்து, தொகுதி அளவை அதிகரிக்கிறோம்.

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=1e-4,
    per_device_train_batch_size=16,
)
```

## முக்கிய குறிப்புகள்[[key-takeaways]]

- கற்றல் வளைவுகள் உங்கள் மாதிரியின் பயிற்சி நடத்தையைப் புரிந்துகொள்வதற்கான ஒரு சக்திவாய்ந்த கருவியாகும்.
- அதிகப் பொருத்தம் என்பது பயிற்சித் தரவில் நல்ல செயல்திறன் ஆனால் சரிபார்ப்புத் தரவில் மோசமான செயல்திறன் மூலம் வகைப்படுத்தப்படுகிறது.
- குறைந்தப் பொருத்தம் பயிற்சி மற்றும் சரிபார்ப்புத் தரவு இரண்டிலும் மோசமான செயல்திறனைக் காட்டுகிறது.
- Weights & Biases போன்ற கருவிகள் கற்றல் வளைவுகளைக் கண்காணிப்பதையும் பகுப்பாய்வு செய்வதையும் எளிதாக்குகின்றன.
- முன்கூட்டியே நிறுத்துதல் மற்றும் சரியான ஒழுங்குபடுத்துதல் ஆகியவை மிகவும் பொதுவான பயிற்சி சிக்கல்களைத் தீர்க்கும்.

## வினாடி வினா[[quiz]]

<Quiz
	questions={[
		{
			question: "உங்கள் பயிற்சி இழப்பு தொடர்ந்து குறைந்து, ஆனால் உங்கள் சரிபார்ப்பு இழப்பு அதிகரிக்கத் தொடங்கினால் என்ன நடக்கிறது?",
			choices: [
				{
					text: "மாதிரி குறைந்தப் பொருத்தத்தில் உள்ளது.",
					explain: "குறைந்தப் பொருத்தம் என்பது பயிற்சி மற்றும் சரிபார்ப்பு செயல்திறன் இரண்டும் மோசமாக இருக்கும்போது நிகழ்கிறது."
				},
				{
					text: "மாதிரி அதிகப் பொருத்தத்தில் உள்ளது.",
					explain: "சரி! இது அதிகப் பொருத்தத்தின் ஒரு உன்னதமான அறிகுறி - மாதிரி பயிற்சித் தரவில் நன்றாகச் செயல்படுகிறது, ஆனால் காணப்படாத சரிபார்ப்புத் தரவில் மோசமாகச் செயல்படுகிறது.",
					correct: true
				},
				{
					text: "கற்றல் விகிதம் மிகவும் குறைவாக உள்ளது.",
					explain: "ஒரு குறைந்த கற்றல் விகிதம் மெதுவான கற்றலை ஏற்படுத்தும், பயிற்சி மற்றும் சரிபார்ப்பு செயல்திறனுக்கு இடையிலான வேறுபாட்டை ஏற்படுத்தாது."
				},
				{
					text: "தரவுத்தொகுப்பு மிகவும் சிறியது.",
					explain: "சிறிய தரவுத்தொகுப்புகள் அதிகப் பொருத்தத்திற்கு பங்களிக்கக்கூடும் என்றாலும், இந்த குறிப்பிட்ட முறை தரவுத்தொகுப்பின் அளவைப் பொருட்படுத்தாமல் அதிகப் பொருத்தத்தின் வரையறையாகும்."
				}
			]
		},
        {
			question: "துல்லிய வளைவுகள் ஏன் மென்மையான அதிகரிப்புகளுக்குப் பதிலாக \"படிப்படியான\" அல்லது பீடபூமி போன்ற வடிவத்தைக் காட்டுகின்றன?",
			choices: [
				{
					text: "துல்லியக் கணக்கீட்டில் ஒரு பிழை உள்ளது.",
					explain: "படிப்படியான முறை சாதாரணமானது மற்றும் எதிர்பார்க்கப்படுகிறது, ஒரு பிழை அல்ல."
				},
				{
					text: "துல்லியம் என்பது ஒரு தனித்தனி அளவீடு ஆகும், இது கணிப்புகள் முடிவு எல்லைகளைக் கடக்கும்போது மட்டுமே மாறுகிறது.",
					explain: "சரி! இழப்பைப் போலல்லாமல், துல்லியம் தனித்தனி கணிப்பு முடிவுகளைச் சார்ந்துள்ளது, எனவே நம்பிக்கையில் சிறிய மேம்பாடுகள் ஒரு வாசல் கடக்கும் வரை இறுதித் துல்லியத்தை மாற்றாது.",
					correct: true
				},
				{
					text: "மாதிரி திறம்பட கற்கவில்லை.",
					explain: "மாதிரி நன்றாகக் கற்கும்போதும் படிப்படியான துல்லிய வளைவுகள் சாதாரணமானவை."
				},
				{
					text: "தொகுதி அளவு மிகவும் சிறியது.",
					explain: "தொகுதி அளவு பயிற்சி நிலைத்தன்மையை பாதிக்கிறது, ஆனால் துல்லிய அளவீடுகளின் இயல்பான தனித்தனி தன்மையை விளக்காது."
				}
			]
		},
        {
			question: "ஒழுங்கற்ற, மிகவும் ஏற்ற இறக்கமான கற்றல் வளைவுகளை நீங்கள் கவனிக்கும்போது சிறந்த அணுகுமுறை என்ன?",
			choices: [
				{
					text: "ஒருங்கிணைதலை வேகப்படுத்த கற்றல் விகிதத்தை அதிகரிக்கவும்.",
					explain: "கற்றல் விகிதத்தை அதிகரிப்பது ஏற்ற இறக்கங்களை மோசமாக்கும்."
				},
				{
					text: "கற்றல் விகிதத்தைக் குறைத்து, தொகுதி அளவை அதிகரிக்கவும்.",
					explain: "சரி! குறைந்த கற்றல் விகிதங்கள் மற்றும் பெரிய தொகுதி அளவுகள் பொதுவாக மிகவும் நிலையான பயிற்சிக்கு வழிவகுக்கும்.",
					correct: true
				},
				{
					text: "மாதிரி மேம்படாது என்பதால் உடனடியாகப் பயிற்சியை நிறுத்தவும்.",
					explain: "ஒழுங்கற்ற வளைவுகளை பெரும்பாலும் ஹைப்பர்பராமீட்டர் சரிசெய்தல் மூலம் சரிசெய்ய முடியும்."
				},
				{
					text: "முற்றிலும் மாறுபட்ட மாதிரி கட்டமைப்பிற்கு மாறவும்.",
					explain: "இது முன்கூட்டியே எடுக்கப்பட்ட முடிவு - ஒழுங்கற்ற வளைவுகள் பொதுவாக ஹைப்பர்பராமீட்டர் சரிசெய்தல் மூலம் சரிசெய்யக்கூடியவை."
				}
			]
		},
        {
			question: "நீங்கள் எப்போது முன்கூட்டியே நிறுத்துவதைக் கருத்தில் கொள்ள வேண்டும்?",
			choices: [
				{
					text: "எப்போதும், ஏனெனில் இது எந்தவொரு அதிகப் பொருத்தத்தையும் தடுக்கிறது.",
					explain: "முன்கூட்டியே நிறுத்துதல் பயனுள்ளது, ஆனால் எப்போதும் அவசியமில்லை, குறிப்பாக மற்ற ஒழுங்குபடுத்தும் முறைகள் வேலை செய்யும்போது."
				},
				{
					text: "சரிபார்ப்பு செயல்திறன் மேம்படுவதை நிறுத்தும்போது அல்லது மோசமடையத் தொடங்கும்போது.",
					explain: "சரி! மாதிரி இனி சிறப்பாகப் பொதுமைப்படுத்தாதபோது பயிற்சியை நிறுத்துவதன் மூலம் அதிகப் பொருத்தத்தைத் தடுக்க முன்கூட்டியே நிறுத்துதல் உதவுகிறது.",
					correct: true
				},
				{
					text: "பயிற்சி இழப்பு இன்னும் வேகமாக குறையும்போது மட்டுமே.",
					explain: "பயிற்சி இழப்பு வேகமாக குறைந்து, சரிபார்ப்பு செயல்திறன் நன்றாக இருந்தால், நீங்கள் பயிற்சியைத் தொடர விரும்பலாம்."
				},
				{
					text: "ஒருபோதும், ஏனெனில் இது மாதிரி அதன் முழு திறனை அடைவதைத் தடுக்கிறது.",
					explain: "முன்கூட்டியே நிறுத்துதல் என்பது ஒரு மதிப்புமிக்க நுட்பமாகும், இது அதிகப் பொருத்தத்தைத் தடுப்பதன் மூலம் இறுதி மாதிரி செயல்திறனை அடிக்கடி மேம்படுத்துகிறது."
				}
			]
		},
        {
			question: "உங்கள் மாதிரி குறைந்தப் பொருத்தத்தில் இருக்கலாம் என்பதைக் குறிப்பது எது?",
			choices: [
				{
					text: "பயிற்சித் துல்லியம் சரிபார்ப்புத் துல்லியத்தை விட மிகவும் அதிகம்.",
					explain: "இது அதிகப் பொருத்தத்தை விவரிக்கிறது, குறைந்தப் பொருத்தத்தை அல்ல."
				},
				{
					text: "பயிற்சி மற்றும் சரிபார்ப்பு செயல்திறன் இரண்டும் மோசமாக இருந்து, முன்கூட்டியே பீடபூமியை அடைகின்றன.",
					explain: "சரி! மாதிரி வடிவங்களைக் கற்றுக்கொள்ளும் திறன் இல்லாதபோது குறைந்தப் பொருத்தம் ஏற்படுகிறது, இதன் விளைவாக பயிற்சி மற்றும் சரிபார்ப்புத் தரவு இரண்டிலும் மோசமான செயல்திறன் ஏற்படுகிறது.",
					correct: true
				},
				{
					text: "கற்றல் வளைவுகள் எந்த ஏற்ற இறக்கங்களும் இல்லாமல் மிகவும் மென்மையாக உள்ளன.",
					explain: "மென்மையான வளைவுகள் பொதுவாக நல்லவை மற்றும் குறைந்தப் பொருத்தத்தைக் குறிக்காது."
				},
				{
					text: "சரிபார்ப்பு இழப்பு பயிற்சி இழப்பை விட வேகமாக குறைகிறது.",
					explain: "இது உண்மையில் ஒரு நேர்மறையான அறிகுறியாக இருக்கும், ஒரு சிக்கல் அல்ல."
				}
			]
		}
	]}
/>
