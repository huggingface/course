# ЁЯдЧ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпНроХро│рпН роЯро╛ро╕рпНроХрпНроХрпБроХро│рпИ роОрокрпНрокроЯро┐родрпН родрпАро░рпНроХрпНроХро┐ройрпНро▒рой

<Youtube id="zsfR7eY9Uho" />

[роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпНроХро│рпН, роЕро╡рпИроХро│ро╛ро▓рпН роОройрпНрой роЪрпЖропрпНроп роорпБроЯро┐ропрпБроорпН?](/course/chapter1/3) рокроХрпБродро┐ропро┐ро▓рпН, роирпАроЩрпНроХро│рпН роЗропро▒рпНроХрпИ роорпКро┤ро┐ роЪрпЖропро▓ро╛роХрпНроХроорпН (NLP), рокрпЗроЪрпНроЪрпБ рооро▒рпНро▒рпБроорпН роЖроЯро┐ропрпЛ, роХрогро┐ройро┐ рокро╛ро░рпНро╡рпИ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпН рооро▒рпНро▒рпБроорпН роЕро╡ро▒рпНро▒ро┐ройрпН роЪро┐ро▓ роорпБроХрпНроХро┐ропрокрпН рокропройрпНрокро╛роЯрпБроХро│рпИрокрпН рокро▒рпНро▒ро┐ роЕро▒ро┐роирпНродрпБ роХрпКрогрпНроЯрпАро░рпНроХро│рпН. роЗроирпНродрокрпН рокроХрпНроХроорпН, рооро╛роЯро▓рпНроХро│рпН роЗроирпНрод роЯро╛ро╕рпНроХрпНроХрпБроХро│рпИ роОрокрпНрокроЯро┐родрпН родрпАро░рпНроХрпНроХро┐ройрпНро▒рой роОройрпНрокродрпИ роЙройрпНройро┐рокрпНрокро╛роХрокрпН рокро╛ро░рпНродрпНродрпБ, родро┐ро░рпИроХрпНроХрпБрокрпН рокро┐ройрпНройро╛ро▓рпН роОройрпНрой роироЯроХрпНроХро┐ро▒родрпБ роОройрпНрокродрпИ ро╡ро┐ро│роХрпНроХрпБроорпН. роТро░рпБ роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роЯро╛ро╕рпНроХрпНроХрпИродрпН родрпАро░рпНроХрпНроХ рокро▓ ро╡ро┤ро┐роХро│рпН роЙро│рпНро│рой, роЪро┐ро▓ рооро╛роЯро▓рпНроХро│рпН роЪро┐ро▓ роЯрпЖроХрпНройро┐роХрпНроХрпБроХро│рпИроЪрпН роЪрпЖропро▓рпНрокроЯрпБродрпНродро▓ро╛роорпН роЕро▓рпНро▓родрпБ роЯро╛ро╕рпНроХрпНроХрпИ роТро░рпБ рокрпБродро┐роп роХрпЛрогродрпНродро┐ро▓рпН роХрпВроЯ роЕрогрпБроХро▓ро╛роорпН, роЖройро╛ро▓рпН роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН рооро╛роЯро▓рпНроХро│рпБроХрпНроХрпБ, рокрпКродрпБро╡ро╛рой ропрпЛроЪройрпИ роТройрпНро▒рпБродро╛ройрпН. роЕродройрпН роирпЖроХро┐ро┤рпНро╡ро╛рой роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░рпН роХро╛ро░рогрооро╛роХ, рокрпЖро░рпБроорпНрокро╛ро▓ро╛рой рооро╛роЯро▓рпНроХро│рпН роТро░рпБ роОройрпНроХрпЛроЯро░рпН, роТро░рпБ роЯро┐роХрпЛроЯро░рпН роЕро▓рпНро▓родрпБ роТро░рпБ роОройрпНроХрпЛроЯро░рпН-роЯро┐роХрпЛроЯро░рпН роХроЯрпНроЯроорпИрокрпНрокро┐ройрпН роТро░рпБ ро╡рпЗро░ро┐ропройрпНроЯрпНроЯро╛роХ роЙро│рпНро│рой.

<Tip>

роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░ро▓рпН ро╡рпЗро░ро┐ропройрпНроЯрпНроХро│рпБроХрпНроХрпБро│рпН роЪрпЖро▓рпНро╡родро▒рпНроХрпБ роорпБройрпН, рокрпЖро░рпБроорпНрокро╛ро▓ро╛рой роЯро╛ро╕рпНроХрпНроХрпБроХро│рпН роТро░рпЗ рооро╛родро┐ро░ро┐ропро╛рой рокрпЗроЯрпНроЯро░рпНройрпИрокрпН рокро┐ройрпНрокро▒рпНро▒рпБроХро┐ройрпНро▒рой роОройрпНрокродрпИрокрпН рокрпБро░ро┐роирпНродрпБроХрпКро│рпНро╡родрпБ роЙродро╡ро┐ропро╛роХ роЗро░рпБроХрпНроХрпБроорпН: роЙро│рпНро│рпАроЯрпНроЯрпБ роЯрпЗроЯрпНроЯро╛ роТро░рпБ рооро╛роЯро▓рпН роорпВро▓роорпН роЪрпЖропро▓ро╛роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ, роорпЗро▓рпБроорпН ро╡рпЖро│ро┐ропрпАроЯрпБ роТро░рпБ роХрпБро▒ро┐рокрпНрокро┐роЯрпНроЯ роЯро╛ро╕рпНроХрпНроХро┐ро▒рпНроХро╛роХ ро╡ро┐ро│роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ. роЯрпЗроЯрпНроЯро╛ роОро╡рпНро╡ро╛ро▒рпБ родропро╛ро░ро┐роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ, роОроирпНрод рооро╛роЯро▓рпН роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░рпН ро╡рпЗро░ро┐ропройрпНроЯрпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроХро┐ро▒родрпБ, рооро▒рпНро▒рпБроорпН ро╡рпЖро│ро┐ропрпАроЯрпБ роОро╡рпНро╡ро╛ро▒рпБ роЪрпЖропро▓ро╛роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ роОройрпНрокродро┐ро▓рпН ро╡рпЗро▒рпБрокро╛роЯрпБроХро│рпН роЙро│рпНро│рой.

</Tip>

роЯро╛ро╕рпНроХрпНроХрпБроХро│рпН роОро╡рпНро╡ро╛ро▒рпБ родрпАро░рпНроХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой роОройрпНрокродрпИ ро╡ро┐ро│роХрпНроХ, рокропройрпБро│рпНро│ роХрогро┐рокрпНрокрпБроХро│рпИ ро╡рпЖро│ро┐ропро┐роЯ рооро╛роЯро▓рпБроХрпНроХрпБро│рпН роОройрпНрой роироЯроХрпНроХро┐ро▒родрпБ роОройрпНрокродрпИрокрпН рокро▒рпНро▒ро┐рокрпН рокро╛ро░рпНрокрпНрокрпЛроорпН. рокро┐ройрпНро╡ро░рпБроорпН рооро╛роЯро▓рпНроХро│рпН рооро▒рпНро▒рпБроорпН роЕро╡ро▒рпНро▒рпБроЯройрпН родрпКроЯро░рпНрокрпБроЯрпИроп роЯро╛ро╕рпНроХрпНроХрпБроХро│рпИ роиро╛роорпН роХро╡ро░рпН роЪрпЖропрпНро╡рпЛроорпН:

- роЖроЯро┐ропрпЛ роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН рооро▒рпНро▒рпБроорпН роЖроЯрпНроЯрпЛроорпЗроЯрпНроЯро┐роХрпН ро╕рпНрокрпАроЪрпН ро░рпЖроХроХрпНройро┐ро╖ройрпН (ASR) роХрпНроХрпБ [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)
- роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпБроХрпНроХрпБ [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) рооро▒рпНро▒рпБроорпН [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)
- роЖрокрпНроЬрпЖроХрпНроЯрпН роЯро┐роЯрпЖроХрпНро╖ройрпБроХрпНроХрпБ [DETR](https://huggingface.co/docs/transformers/model_doc/detr)
- роЗроорпЗроЬрпН роЪрпЖроХрпНроорпЖройрпНроЯрпЗро╖ройрпБроХрпНроХрпБ [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)
- роЯрпЖрокрпНродрпН роОро╕рпНроЯро┐роорпЗро╖ройрпБроХрпНроХрпБ [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)
- роОройрпНроХрпЛроЯро░рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроорпН роЯрпЖроХрпНро╕рпНроЯрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН, роЯрпЛроХрпНроХройрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН рооро▒рпНро▒рпБроорпН роХрпЗро│рпНро╡ро┐ рокродро┐ро▓рпН рокрпЛройрпНро▒ NLP роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБ [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- роЯро┐роХрпЛроЯро░рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроорпН роЯрпЖроХрпНро╕рпНроЯрпН роЬрпЖройро░рпЗро╖ройрпН рокрпЛройрпНро▒ NLP роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБ [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
- роОройрпНроХрпЛроЯро░рпН-роЯро┐роХрпЛроЯро░рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроорпН роЪрпБро░рпБроХрпНроХроорпН рооро▒рпНро▒рпБроорпН роорпКро┤ро┐рокрпЖропро░рпНрокрпНрокрпБ рокрпЛройрпНро▒ NLP роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБ [BART](https://huggingface.co/docs/transformers/model_doc/bart)

<Tip>

роирпАроЩрпНроХро│рпН роорпЗро▓рпЗ роЪрпЖро▓рпНро╡родро▒рпНроХрпБ роорпБройрпН, роЕроЪро▓рпН роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░рпИрокрпН рокро▒рпНро▒ро┐роп роЪро┐ро▓ роЕроЯро┐рокрпНрокроЯрпИ роЕро▒ро┐ро╡рпБ роЗро░рпБрокрпНрокродрпБ роиро▓рпНро▓родрпБ. роОройрпНроХрпЛроЯро░рпНроХро│рпН, роЯро┐роХрпЛроЯро░рпНроХро│рпН рооро▒рпНро▒рпБроорпН роЕроЯрпНроЯрпЖройрпНро╖ройрпН роОро╡рпНро╡ро╛ро▒рпБ роЪрпЖропро▓рпНрокроЯрпБроХро┐ройрпНро▒рой роОройрпНрокродрпИ роЕро▒ро┐ро╡родрпБ, ро╡рпЖро╡рпНро╡рпЗро▒рпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН рооро╛роЯро▓рпНроХро│рпН роОро╡рпНро╡ро╛ро▒рпБ роЪрпЖропро▓рпНрокроЯрпБроХро┐ройрпНро▒рой роОройрпНрокродрпИрокрпН рокрпБро░ро┐роирпНродрпБроХрпКро│рпНро│ роЙроЩрпНроХро│рпБроХрпНроХрпБ роЙродро╡рпБроорпН. роорпЗро▓рпБроорпН родроХро╡ро▓рпБроХрпНроХрпБ роОроЩрпНроХро│рпН [роорпБроирпНродрпИроп рокроХрпБродро┐ропрпИ](https://huggingface.co/course/chapter1/4?fw=pt) рокро╛ро░рпНроХрпНроХро╡рпБроорпН!

</Tip>

## роорпКро┤ро┐роХрпНроХрпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН рооро╛роЯро▓рпНроХро│рпН

роорпКро┤ро┐ рооро╛роЯро▓рпНроХро│рпН роиро╡рпАрой NLP-роЗройрпН роЗродропродрпНродро┐ро▓рпН роЙро│рпНро│рой. роЕро╡рпИ роЯрпЖроХрпНро╕рпНроЯрпНроЯро┐ро▓рпН роЙро│рпНро│ ро╡ро╛ро░рпНродрпНродрпИроХро│рпН роЕро▓рпНро▓родрпБ роЯрпЛроХрпНроХройрпНроХро│рпБроХрпНроХрпБ роЗроЯрпИропро┐ро▓ро╛рой рокрпБро│рпНро│ро┐ро╡ро┐ро╡ро░ рокрпЗроЯрпНроЯро░рпНройрпНроХро│рпН рооро▒рпНро▒рпБроорпН роЙро▒ро╡рпБроХро│рпИроХрпН роХро▒рпНро▒рпБроХрпНроХрпКро│рпНро╡родройрпН роорпВро▓роорпН рооройро┐род роорпКро┤ро┐ропрпИрокрпН рокрпБро░ро┐роирпНродрпБроХрпКро│рпНро│ро╡рпБроорпН роЙро░рпБро╡ро╛роХрпНроХро╡рпБроорпН ро╡роЯро┐ро╡роорпИроХрпНроХрокрпНрокроЯрпНроЯрпБро│рпНро│рой.

роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роЖро░роорпНрокродрпНродро┐ро▓рпН роорпЖро╖ро┐ройрпН роЯро┐ро░ро╛ройрпНро╕рпНро▓рпЗро╖ройрпБроХрпНроХро╛роХ ро╡роЯро┐ро╡роорпИроХрпНроХрокрпНрокроЯрпНроЯродрпБ, роЕродройрпНрокро┐ро▒роХрпБ, роЗродрпБ роЕройрпИродрпНродрпБ AI роЯро╛ро╕рпНроХрпНроХрпБроХро│рпИропрпБроорпН родрпАро░рпНрокрпНрокродро▒рпНроХро╛рой роЗропро▓рпНрокрпБроиро┐ро▓рпИ роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░ро╛роХ рооро╛ро▒ро┐ропрпБро│рпНро│родрпБ. роЪро┐ро▓ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпН роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░ро┐ройрпН роОройрпНроХрпЛроЯро░рпН роЕроорпИрокрпНрокрпБроХрпНроХрпБ роПро▒рпНро▒ро╡рпИ, рооро▒рпНро▒ро╡рпИ роЯро┐роХрпЛроЯро░рпБроХрпНроХрпБ рооро┐роХро╡рпБроорпН рокрпКро░рпБродрпНродрооро╛ройро╡рпИ. роЗройрпНройрпБроорпН роЪро┐ро▓ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпН роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░ро┐ройрпН роОройрпНроХрпЛроЯро░рпН-роЯро┐роХрпЛроЯро░рпН роЕроорпИрокрпНрокрпБ роЗро░рогрпНроЯрпИропрпБроорпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ройрпНро▒рой.

### роорпКро┤ро┐ рооро╛роЯро▓рпНроХро│рпН роОрокрпНрокроЯро┐ ро╡рпЗро▓рпИ роЪрпЖропрпНроХро┐ройрпНро▒рой

роорпКро┤ро┐ рооро╛роЯро▓рпНроХро│рпН роЪрпБро▒рпНро▒ро┐ропрпБро│рпНро│ ро╡ро╛ро░рпНродрпНродрпИроХро│ро┐ройрпН роЪрпВро┤ро▓рпИроХрпН роХрпКрогрпНроЯрпБ роТро░рпБ ро╡ро╛ро░рпНродрпНродрпИропро┐ройрпН роиро┐роХро┤рпНродроХро╡рпИроХрпН роХрогро┐роХрпНроХрокрпН рокропро┐ро▒рпНро▒рпБро╡ро┐роХрпНроХрокрпНрокроЯрпБро╡родройрпН роорпВро▓роорпН роЪрпЖропро▓рпНрокроЯрпБроХро┐ройрпНро▒рой. роЗродрпБ роЕро╡ро▒рпНро▒рпБроХрпНроХрпБ роорпКро┤ро┐ропро┐ройрпН роТро░рпБ роЕроЯро┐рокрпНрокроЯрпИрокрпН рокрпБро░ро┐родро▓рпИ роЕро│ро┐роХрпНроХро┐ро▒родрпБ, роЗродрпБ рооро▒рпНро▒ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБроорпН рокрпКродрпБроорпИрокрпНрокроЯрпБродрпНродрокрпНрокроЯро▓ро╛роорпН.

роТро░рпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН рооро╛роЯро▓рпИрокрпН рокропро┐ро▒рпНро▒рпБро╡ро┐роХрпНроХ роЗро░рогрпНроЯрпБ роорпБроХрпНроХро┐роп роЕрогрпБроХрпБроорпБро▒рпИроХро│рпН роЙро│рпНро│рой:

1. **рооро╛ро╕рпНроХрпНроЯрпБ ро▓ро╛роЩрпНроХрпБро╡рпЗроЬрпН рооро╛роЯро▓ро┐роЩрпН (MLM)**: BERT рокрпЛройрпНро▒ роОройрпНроХрпЛроЯро░рпН рооро╛роЯро▓рпНроХро│ро╛ро▓рпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроХро┐ро▒родрпБ, роЗроирпНрод роЕрогрпБроХрпБроорпБро▒рпИ роЙро│рпНро│рпАроЯрпНроЯро┐ро▓рпН роЪро┐ро▓ роЯрпЛроХрпНроХройрпНроХро│рпИродрпН родрпЛро░ро╛ропрооро╛роХ рооро▒рпИродрпНродрпБ, роЪрпБро▒рпНро▒ро┐ропрпБро│рпНро│ роЪрпВро┤ро▓ро┐ройрпН роЕроЯро┐рокрпНрокроЯрпИропро┐ро▓рпН роЕроЪро▓рпН роЯрпЛроХрпНроХройрпНроХро│рпИроХрпН роХрогро┐роХрпНроХ рооро╛роЯро▓рпИрокрпН рокропро┐ро▒рпНро▒рпБро╡ро┐роХрпНроХро┐ро▒родрпБ. роЗродрпБ рооро╛роЯро▓рпБроХрпНроХрпБ роЗро░рпБ родро┐роЪрпИ роЪрпВро┤ро▓рпИроХрпН роХро▒рпНро▒рпБроХрпНроХрпКро│рпНро│ роЕройрпБроородро┐роХрпНроХро┐ро▒родрпБ (рооро▒рпИроХрпНроХрокрпНрокроЯрпНроЯ ро╡ро╛ро░рпНродрпНродрпИроХрпНроХрпБ роорпБройрпНройрпБроорпН рокро┐ройрпНройрпБроорпН роЙро│рпНро│ ро╡ро╛ро░рпНродрпНродрпИроХро│рпИрокрпН рокро╛ро░рпНрокрпНрокродрпБ).

2. **роХро╛ро╕ро▓рпН ро▓ро╛роЩрпНроХрпБро╡рпЗроЬрпН рооро╛роЯро▓ро┐роЩрпН (CLM)**: GPT рокрпЛройрпНро▒ роЯро┐роХрпЛроЯро░рпН рооро╛роЯро▓рпНроХро│ро╛ро▓рпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроХро┐ро▒родрпБ, роЗроирпНрод роЕрогрпБроХрпБроорпБро▒рпИ ро╡ро░ро┐роЪрпИропро┐ро▓рпН роЙро│рпНро│ роЕройрпИродрпНродрпБ роорпБроирпНродрпИроп роЯрпЛроХрпНроХройрпНроХро│ро┐ройрпН роЕроЯро┐рокрпНрокроЯрпИропро┐ро▓рпН роЕроЯрпБродрпНрод роЯрпЛроХрпНроХройрпИроХрпН роХрогро┐роХрпНроХро┐ро▒родрпБ. рооро╛роЯро▓рпН роЕроЯрпБродрпНрод роЯрпЛроХрпНроХройрпИроХрпН роХрогро┐роХрпНроХ роЗроЯрооро┐ро░рпБроирпНродрпБ (роорпБроирпНродрпИроп роЯрпЛроХрпНроХройрпНроХро│рпН) роороЯрпНроЯрпБроорпЗ роЪрпВро┤ро▓рпИрокрпН рокропройрпНрокроЯрпБродрпНрод роорпБроЯро┐ропрпБроорпН.

### роорпКро┤ро┐ рооро╛роЯро▓рпНроХро│ро┐ройрпН ро╡роХрпИроХро│рпН

In the Transformers library, language models generally fall into three architectural categories:

1. **Encoder-only models** (like BERT): These models use a bidirectional approach to understand context from both directions. They're best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering.

2. **Decoder-only models** (like GPT, Llama): These models process text from left to right and are particularly good at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt.

3. **Encoder-decoder models** (like T5, BART): These models combine both approaches, using an encoder to understand the input and a decoder to generate output. They excel at sequence-to-sequence tasks like translation, summarization, and question answering.

![transformer-models-for-language](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_architecture.png)

As we covered in the previous section, language models are typically pretrained on large amounts of text data in a self-supervised manner (without human annotations), then fine-tuned on specific tasks. This approach, known as transfer learning, allows these models to adapt to many different NLP tasks with relatively small amounts of task-specific data.

In the following sections, we'll explore specific model architectures and how they're applied to various tasks across speech, vision, and text domains.

<Tip>

Understanding which part of the Transformer architecture (encoder, decoder, or both) is best suited for a particular NLP task is key to choosing the right model. Generally, tasks requiring bidirectional context use encoders, tasks generating text use decoders, and tasks converting one sequence to another use encoder-decoders.

</Tip>

### Text generation

Text generation involves creating coherent and contextually relevant text based on a prompt or input.

[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2) is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2 uses [byte pair encoding (BPE)](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can't attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT's [`mask`] token because, in masked self-attention, an attention mask is used to set the score to `0` for future tokens.

2. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.

GPT-2's pretraining objective is based entirely on [causal language modeling](https://huggingface.co/docs/transformers/glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.

Ready to try your hand at text generation? Check out our complete [causal language modeling guide](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!

<Tip>
For more information about text generation, check out the [text generation strategies](generation_strategies) guide!
</Tip>

### Text classification

роЗроирпНрод роЯро╛ро╕рпНроХрпНроХро┐ро▒рпНроХрпБ, роирпАроЩрпНроХро│рпН BERT рокрпЛройрпНро▒ роТро░рпБ роОройрпНроХрпЛроЯро░рпН-роороЯрпНроЯрпБроорпН рооро╛роЯро▓рпИрокрпН рокропройрпНрокроЯрпБродрпНродро▓ро╛роорпН. роЗро░рпБрокрпНрокро┐ройрпБроорпН, роТро░рпБ рокрпКродрпБро╡ро╛рой роЕрогрпБроХрпБроорпБро▒рпИ [BART](https://huggingface.co/docs/transformers/model_doc/bart) роЕро▓рпНро▓родрпБ [T5](https://huggingface.co/docs/transformers/model_doc/t5) рокрпЛройрпНро▒ роТро░рпБ роОройрпНроХрпЛроЯро░рпН-роЯро┐роХрпЛроЯро░рпН рооро╛роЯро▓рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБро╡родро╛роХрпБроорпН, роЕро╡рпИ роХрпБро▒ро┐рокрпНрокро╛роХ роЪрпАроХрпНроХрпБро╡рпЖройрпНро╕рпН-роЯрпБ-роЪрпАроХрпНроХрпБро╡рпЖройрпНро╕рпН роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХро╛роХ ро╡роЯро┐ро╡роорпИроХрпНроХрокрпНрокроЯрпНроЯрпБро│рпНро│рой. роОройрпНроХрпЛроЯро░рпН роЙро│рпНро│рпАроЯрпНроЯрпБ роЪрпВро┤ро▓рпИроЪрпН роЪрпЖропро▓ро╛роХрпНроХрпБроХро┐ро▒родрпБ, роорпЗро▓рпБроорпН роЯро┐роХрпЛроЯро░рпН роЕроирпНродроЪрпН роЪрпВро┤ро▓ро┐ройрпН роЕроЯро┐рокрпНрокроЯрпИропро┐ро▓рпН рокродро┐ро▓рпИ роЙро░рпБро╡ро╛роХрпНроХрпБроХро┐ро▒родрпБ. роЗродрпБ рооро╛роЯро▓рпБроХрпНроХрпБ рооро┐роХро╡рпБроорпН роЪро┐роХрпНроХро▓ро╛рой роХрпЗро│рпНро╡ро┐роХро│рпИроХрпН роХрпИропро╛ро│ро╡рпБроорпН роорпЗро▓рпБроорпН роЗропро▓рпНрокро╛рой роТро▓ро┐роХрпНроХрпБроорпН рокродро┐ро▓рпНроХро│рпИ роЙро░рпБро╡ро╛роХрпНроХро╡рпБроорпН роЕройрпБроородро┐роХрпНроХро┐ро▒родрпБ.

### роЪрпБро░рпБроХрпНроХроорпН

роЪрпБро░рпБроХрпНроХроорпН роОройрпНрокродрпБ роТро░рпБ роЖро╡рогродрпНродро┐ройрпН роорпБроХрпНроХро┐ропродрпН родроХро╡ро▓рпИрокрпН рокро╛родрпБроХро╛роХрпНроХрпБроорпН роЕродрпЗ ро╡рпЗро│рпИропро┐ро▓рпН роЕродройрпН роТро░рпБ роХрпБро▒рпБроХро┐роп, роЪрпБро░рпБроХрпНроХрокрпНрокроЯрпНроЯ рокродро┐рокрпНрокрпИ роЙро░рпБро╡ро╛роХрпНроХрпБроорпН роЯро╛ро╕рпНроХрпН роЖроХрпБроорпН.

роХрпЗро│рпНро╡ро┐ рокродро┐ро▓рпИрокрпН рокрпЛро▓ро╡рпЗ, роЪрпБро░рпБроХрпНроХроорпБроорпН роТро░рпБ роЪрпАроХрпНроХрпБро╡рпЖройрпНро╕рпН-роЯрпБ-роЪрпАроХрпНроХрпБро╡рпЖройрпНро╕рпН роЯро╛ро╕рпНроХрпН роЖроХрпБроорпН. роОройрпНроХрпЛроЯро░рпН роирпАрогрпНроЯ роЙро│рпНро│рпАроЯрпНроЯрпБ роЯрпЖроХрпНро╕рпНроЯрпНроЯрпИроЪрпН роЪрпЖропро▓ро╛роХрпНроХрпБроХро┐ро▒родрпБ, роорпЗро▓рпБроорпН роЯро┐роХрпЛроЯро░рпН роТро░рпБ роХрпБро▒рпБроХро┐роп роЪрпБро░рпБроХрпНроХродрпНродрпИ роЙро░рпБро╡ро╛роХрпНроХрпБроХро┐ро▒родрпБ. [BART](https://huggingface.co/docs/transformers/model_doc/bart) рооро▒рпНро▒рпБроорпН [T5](https://huggingface.co/docs/transformers/model_doc/t5) рокрпЛройрпНро▒ рооро╛роЯро▓рпНроХро│рпН роЪрпБро░рпБроХрпНроХродрпНродро┐ро▒рпНроХрпБрокрпН рокро┐ро░рокро▓рооро╛рой родрпЗро░рпНро╡рпБроХро│рпН, роПройрпЖройро┐ро▓рпН роЕро╡рпИ роирпАрогрпНроЯ роЙро│рпНро│рпАроЯрпНроЯрпБ ро╡ро░ро┐роЪрпИроХро│рпИроХрпН роХрпИропро╛ро│ро╡рпБроорпН роЙропро░рпНродро░, роТродрпНродро┐роЪрпИро╡ро╛рой роЪрпБро░рпБроХрпНроХроЩрпНроХро│рпИ роЙро░рпБро╡ро╛роХрпНроХро╡рпБроорпН роорпБроЯро┐ропрпБроорпН.

роЪрпБро░рпБроХрпНроХродрпНродро┐ро▓рпН роЙроЩрпНроХро│рпН родро┐ро▒роорпИропрпИ роорпБропро▒рпНроЪро┐роХрпНроХродрпН родропро╛ро░ро╛? T5-роР роОрокрпНрокроЯро┐ роГрокрпИройрпН-роЯро┐ропрпВройрпН роЪрпЖропрпНро╡родрпБ рооро▒рпНро▒рпБроорпН роЕродрпИ роЗройрпНроГрокро░ройрпНро╕рпБроХрпНроХрпБрокрпН рокропройрпНрокроЯрпБродрпНродрпБро╡родрпБ роОройрпНрокродрпИ роЕро▒ро┐роп роОроЩрпНроХро│рпН роорпБро┤рпБроорпИропро╛рой [роЪрпБро░рпБроХрпНроХроорпН ро╡ро┤ро┐роХро╛роЯрпНроЯро┐ропрпИрокрпН](https://huggingface.co/docs/transformers/tasks/summarization) рокро╛ро░рпБроЩрпНроХро│рпН!

<Tip>

BART is pretrained by corrupting text with a text infilling objective, where a number of text spans are replaced with a single `<mask>` token. Unlike other objectives, text infilling teaches the model to predict how many tokens are missing. The decoder is used to autoregressively generate the uncorrupted text.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. BART's pretraining objective starts by corrupting the original text. The original text is shown at the bottom of the above image. Some tokens are randomly replaced with a `<mask>` token. The corrupted text is passed to the encoder.

2. The encoder's output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder's output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.

Ready to try your hand at summarization? Check out our complete [summarization guide](https://huggingface.co/docs/transformers/tasks/summarization) to learn how to finetune T5 and use it for inference!

</Tip>

### Translation

Translation involves converting text from one language to another while preserving its meaning. Translation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](https://huggingface.co/docs/transformers/model_doc/bart) or [T5](model_doc/t5) to do it. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.

BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder's embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step.
BART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages.

Ready to try your hand at translation? Check out our complete [translation guide](https://huggingface.co/docs/transformers/tasks/translation) to learn how to finetune T5 and use it for inference!

<Tip>

As you've seen throughout this guide, many models follow similar patterns despite addressing different tasks. Understanding these common patterns can help you quickly grasp how new models work and how to adapt existing models to your specific needs.

</Tip>

## Modalities beyond text

Transformers are not limited to text. They can also be applied to other modalities like speech and audio, images, and video. Of course, on this course we will focus on text, but we can briefly introduce the other modalities.

### Speech and audio

Let's start by exploring how Transformer models handle speech and audio data, which presents unique challenges compared to text or images.

[Whisper](https://huggingface.co/docs/transformers/main/en/model_doc/whisper) is a encoder-decoder (sequence-to-sequence) transformer pretrained on 680,000 hours of labeled audio data. This amount of pretraining data enables zero-shot performance on audio tasks in English and many other languages. The decoder allows Whisper to map the encoders learned speech representations to useful outputs, such as text, without additional fine-tuning. Whisper just works out of the box.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/whisper_architecture.png"/>
</div>

Diagram is from [Whisper paper](https://huggingface.co/papers/2212.04356).

This model has two main components:

1. An **encoder** processes the input audio. The raw audio is first converted into a log-Mel spectrogram. This spectrogram is then passed through a Transformer encoder network.

2. A **decoder** takes the encoded audio representation and autoregressively predicts the corresponding text tokens. It's a standard Transformer decoder trained to predict the next text token given the previous tokens and the encoder output. Special tokens are used at the beginning of the decoder input to steer the model towards specific tasks like transcription, translation, or language identification.

Whisper was pretrained on a massive and diverse dataset of 680,000 hours of labeled audio data collected from the web. This large-scale, weakly supervised pretraining is the key to its strong zero-shot performance across many languages and tasks.

Now that Whisper is pretrained, you can use it directly for zero-shot inference or finetune it on your data for improved performance on specific tasks like automatic speech recognition or speech translation!

<Tip>

The key innovation in Whisper is its training on an unprecedented scale of diverse, weakly supervised audio data from the internet. This allows it to generalize remarkably well to different languages, accents, and tasks without task-specific finetuning.

</Tip>

### Automatic speech recognition

To use the pretrained model for automatic speech recognition, you leverage its full encoder-decoder structure. The encoder processes the audio input, and the decoder autoregressively generates the transcript token by token. When fine-tuning, the model is typically trained using a standard sequence-to-sequence loss (like cross-entropy) to predict the correct text tokens based on the audio input.

The easiest way to use a fine-tuned model for inference is within a `pipeline`.

```python
from transformers import pipeline

transcriber = pipeline()
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
# Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

родро╛ройро┐ропроЩрпНроХрпБ рокрпЗроЪрпНроЪрпБ роЕроЩрпНроХрпАроХро╛ро░родрпНродро┐ро▓рпН роЙроЩрпНроХро│рпН родро┐ро▒роорпИропрпИ роорпБропро▒рпНроЪро┐роХрпНроХродрпН родропро╛ро░ро╛? Whisper-роР роОрокрпНрокроЯро┐ роГрокрпИройрпН-роЯро┐ропрпВройрпН роЪрпЖропрпНро╡родрпБ рооро▒рпНро▒рпБроорпН роЕродрпИ роЗройрпНроГрокро░ройрпНро╕рпБроХрпНроХрпБрокрпН рокропройрпНрокроЯрпБродрпНродрпБро╡родрпБ роОройрпНрокродрпИ роЕро▒ро┐роп роОроЩрпНроХро│рпН роорпБро┤рпБроорпИропро╛рой [родро╛ройро┐ропроЩрпНроХрпБ рокрпЗроЪрпНроЪрпБ роЕроЩрпНроХрпАроХро╛ро░ ро╡ро┤ро┐роХро╛роЯрпНроЯро┐ропрпИрокрпН](https://huggingface.co/docs/transformers/tasks/asr) рокро╛ро░рпБроЩрпНроХро│рпН!

### роХрогро┐ройро┐ рокро╛ро░рпНро╡рпИ

роЗрокрпНрокрпЛродрпБ роХрогро┐ройро┐ рокро╛ро░рпНро╡рпИ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБроЪрпН роЪрпЖро▓рпНро╡рпЛроорпН, роЕро╡рпИ рокроЯроЩрпНроХро│рпН роЕро▓рпНро▓родрпБ ро╡рпАроЯро┐ропрпЛроХрпНроХро│ро┐ро▓ро┐ро░рпБроирпНродрпБ роХро╛роЯрпНроЪро┐родрпН родроХро╡ро▓рпИрокрпН рокрпБро░ро┐роирпНродрпБроХрпКрогрпНроЯрпБ ро╡ро┐ро│роХрпНроХрпБро╡родрпИроХрпН роХрпИропро╛ро│рпБроХро┐ройрпНро▒рой.

роХрогро┐ройро┐ рокро╛ро░рпНро╡рпИ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпИ роЕрогрпБроХ роЗро░рогрпНроЯрпБ ро╡ро┤ро┐роХро│рпН роЙро│рпНро│рой:

1. роТро░рпБ рокроЯродрпНродрпИ рокрпЗроЯрпНроЪрпНроХро│ро┐ройрпН ро╡ро░ро┐роЪрпИропро╛роХрокрпН рокро┐ро░ро┐родрпНродрпБ, роЕро╡ро▒рпНро▒рпИ роТро░рпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роорпВро▓роорпН роЗрогрпИропро╛роХроЪрпН роЪрпЖропро▓ро╛роХрпНроХрпБро╡родрпБ.
2. [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) рокрпЛройрпНро▒ роТро░рпБ роиро╡рпАрой CNN-роРрокрпН рокропройрпНрокроЯрпБродрпНродрпБро╡родрпБ, роЗродрпБ роХройрпНро╡ро▓рпНропрпВро╖ройро▓рпН ро▓рпЗропро░рпНроХро│рпИ роироорпНрокро┐ропрпБро│рпНро│родрпБ роЖройро╛ро▓рпН роиро╡рпАрой роирпЖроЯрпНро╡рпКро░рпНроХрпН ро╡роЯро┐ро╡роорпИрокрпНрокрпБроХро│рпИ роПро▒рпНро▒рпБроХрпНроХрпКро│рпНроХро┐ро▒родрпБ.

<Tip>

роорпВройрпНро▒ро╛ро╡родрпБ роЕрогрпБроХрпБроорпБро▒рпИ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпНроХро│рпИ роХройрпНро╡ро▓рпНропрпВро╖ройрпНроХро│рпБроЯройрпН роХро▓роХрпНроХро┐ро▒родрпБ (роЙродро╛ро░рогрооро╛роХ, [Convolutional Vision Transformer](https://huggingface.co/docs/transformers/model_doc/cvt) роЕро▓рпНро▓родрпБ [LeViT](https://huggingface.co/docs/transformers/model_doc/levit)). роиро╛роорпН роЕро╡ро▒рпНро▒рпИрокрпН рокро▒рпНро▒ро┐ ро╡ро┐ро╡ро╛родро┐роХрпНроХ рооро╛роЯрпНроЯрпЛроорпН, роПройрпЖройро┐ро▓рпН роЕро╡рпИ роиро╛роорпН роЗроЩрпНроХрпЗ роЖро░ро╛ропрпБроорпН роЗро░рогрпНроЯрпБ роЕрогрпБроХрпБроорпБро▒рпИроХро│рпИропрпБроорпН роЗрогрпИроХрпНроХро┐ройрпНро▒рой.

</Tip>

ViT рооро▒рпНро▒рпБроорпН ConvNeXT рокрпКродрпБро╡ро╛роХ роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпБроХрпНроХрпБрокрпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроХро┐ройрпНро▒рой, роЖройро╛ро▓рпН роЖрокрпНроЬрпЖроХрпНроЯрпН роЯро┐роЯрпЖроХрпНро╖ройрпН, роЪрпЖроХрпНроорпЖройрпНроЯрпЗро╖ройрпН, рооро▒рпНро▒рпБроорпН роЯрпЖрокрпНродрпН роОро╕рпНроЯро┐роорпЗро╖ройрпН рокрпЛройрпНро▒ рокро┐ро▒ рокро╛ро░рпНро╡рпИ роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБ, роиро╛роорпН роорпБро▒рпИропрпЗ DETR, Mask2Former рооро▒рпНро▒рпБроорпН GLPN-роРрокрпН рокро╛ро░рпНрокрпНрокрпЛроорпН; роЗроирпНрод рооро╛роЯро▓рпНроХро│рпН роЕроирпНрод роЯро╛ро╕рпНроХрпНроХрпБроХро│рпБроХрпНроХрпБ рооро┐роХро╡рпБроорпН рокрпКро░рпБродрпНродрооро╛ройро╡рпИ.

### роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН

роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН роОройрпНрокродрпБ роЕроЯро┐рокрпНрокроЯрпИроХрпН роХрогро┐ройро┐ рокро╛ро░рпНро╡рпИ роЯро╛ро╕рпНроХрпНроХрпБроХро│ро┐ро▓рпН роТройрпНро▒ро╛роХрпБроорпН. ро╡рпЖро╡рпНро╡рпЗро▒рпБ рооро╛роЯро▓рпН роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░рпНроХро│рпН роЗроирпНродрокрпН роЪро┐роХрпНроХро▓рпИ роОро╡рпНро╡ро╛ро▒рпБ роЕрогрпБроХрпБроХро┐ройрпНро▒рой роОройрпНрокродрпИрокрпН рокро╛ро░рпНрокрпНрокрпЛроорпН.

ViT рооро▒рпНро▒рпБроорпН ConvNeXT роЖроХро┐роп роЗро░рогрпНроЯрпБроорпН роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпБроХрпНроХрпБрокрпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯро▓ро╛роорпН; роорпБроХрпНроХро┐роп ро╡рпЗро▒рпБрокро╛роЯрпБ роОройрпНройро╡рпЖройрпНро▒ро╛ро▓рпН, ViT роТро░рпБ роЕроЯрпНроЯрпЖройрпНро╖ройрпН роорпЖроХрпНроХро╛ройро┐роЪродрпНродрпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ, роЕродрпЗ роирпЗро░родрпНродро┐ро▓рпН ConvNeXT роХройрпНро╡ро▓рпНропрпВро╖ройрпНроХро│рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ.

[ViT](https://huggingface.co/docs/transformers/model_doc/vit) роХройрпНро╡ро▓рпНропрпВро╖ройрпНроХро│рпИ роорпБро┤рпБро╡родрпБрооро╛роХ роТро░рпБ родрпВроп роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роЖро░рпНроХрпНроХро┐роЯрпЖроХрпНроЪро░рпН роорпВро▓роорпН рооро╛ро▒рпНро▒рпБроХро┐ро▒родрпБ. роирпАроЩрпНроХро│рпН роЕроЪро▓рпН роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпИрокрпН рокро▒рпНро▒ро┐ роЕро▒ро┐роирпНродро┐ро░рпБроирпНродро╛ро▓рпН, роирпАроЩрпНроХро│рпН роПро▒рпНроХройро╡рпЗ ViT-роРрокрпН рокрпБро░ро┐роирпНродрпБроХрпКро│рпНро╡родро▒рпНроХро╛рой рокро╛родрпИропро┐ро▓рпН рокро╛родро┐ родрпВро░роорпН ро╡роирпНродрпБро╡ро┐роЯрпНроЯрпАро░рпНроХро│рпН.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

ViT роЕро▒ро┐роорпБроХрокрпНрокроЯрпБродрпНродро┐роп роорпБроХрпНроХро┐роп рооро╛ро▒рпНро▒роорпН, рокроЯроЩрпНроХро│рпН роТро░рпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпБроХрпНроХрпБ роОро╡рпНро╡ро╛ро▒рпБ роЕро│ро┐роХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой роОройрпНрокродро┐ро▓рпНродро╛ройрпН роЙро│рпНро│родрпБ:

1. роТро░рпБ рокроЯроорпН роЪродрпБро░, роТройрпНро▒рпЛроЯрпКройрпНро▒рпБ роЪрпЗро░ро╛род рокрпЗроЯрпНроЪрпНроХро│ро╛роХрокрпН рокро┐ро░ро┐роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ, роТро╡рпНро╡рпКройрпНро▒рпБроорпН роТро░рпБ ро╡рпЖроХрпНроЯро░рпН роЕро▓рпНро▓родрпБ *рокрпЗроЯрпНроЪрпН роЙроЯрпНрокрпКродро┐ро╡ро╛роХ* рооро╛ро▒рпНро▒рокрпНрокроЯрпБроХро┐ро▒родрпБ. рокрпЗроЯрпНроЪрпН роЙроЯрпНрокрпКродро┐ро╡рпБроХро│рпН роТро░рпБ роХройрпНро╡ро▓рпНропрпВро╖ройро▓рпН 2D ро▓рпЗропро░ро┐ро▓ро┐ро░рпБроирпНродрпБ роЙро░рпБро╡ро╛роХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой, роЗродрпБ роЪро░ро┐ропро╛рой роЙро│рпНро│рпАроЯрпНроЯрпБрокрпН рокро░ро┐рооро╛рогроЩрпНроХро│рпИ роЙро░рпБро╡ро╛роХрпНроХрпБроХро┐ро▒родрпБ (роЗродрпБ роТро░рпБ роЕроЯро┐рокрпНрокроЯрпИ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпБроХрпНроХрпБ роТро╡рпНро╡рпКро░рпБ рокрпЗроЯрпНроЪрпН роЙроЯрпНрокрпКродро┐ро╡ро┐ро▒рпНроХрпБроорпН 768 роородро┐рокрпНрокрпБроХро│рпН). роЙроЩрпНроХро│ро┐роЯроорпН 224x224 рокро┐роХрпНроЪро▓рпН рокроЯроорпН роЗро░рпБроирпНродро╛ро▓рпН, роЕродрпИ 196 16x16 рокроЯрокрпН рокрпЗроЯрпНроЪрпНроХро│ро╛роХрокрпН рокро┐ро░ро┐роХрпНроХро▓ро╛роорпН. роЯрпЖроХрпНро╕рпНроЯрпН роОро╡рпНро╡ро╛ро▒рпБ ро╡ро╛ро░рпНродрпНродрпИроХро│ро╛роХ роЯрпЛроХрпНроХройрпИро╕рпН роЪрпЖропрпНропрокрпНрокроЯрпБроХро┐ро▒родрпЛ, роЕродрпЗрокрпЛро▓рпН роТро░рпБ рокроЯроорпН рокрпЗроЯрпНроЪрпНроХро│ро┐ройрпН ро╡ро░ро┐роЪрпИропро╛роХ "роЯрпЛроХрпНроХройрпИро╕рпН" роЪрпЖропрпНропрокрпНрокроЯрпБроХро┐ро▒родрпБ.

2. роТро░рпБ *роХро▒рпНроХроХрпНроХрпВроЯро┐роп роЙроЯрпНрокрпКродро┐ро╡рпБ* - роТро░рпБ роЪро┐ро▒рокрпНрокрпБ `[CLS]` роЯрпЛроХрпНроХройрпН - BERT-роРрокрпН рокрпЛро▓ро╡рпЗ рокрпЗроЯрпНроЪрпН роЙроЯрпНрокрпКродро┐ро╡рпБроХро│ро┐ройрпН родрпКроЯроХрпНроХродрпНродро┐ро▓рпН роЪрпЗро░рпНроХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ. `[CLS]` роЯрпЛроХрпНроХройро┐ройрпН роЗро▒рпБродро┐ рооро▒рпИроХрпНроХрокрпНрокроЯрпНроЯ роиро┐ро▓рпИ, роЗрогрпИроХрпНроХрокрпНрокроЯрпНроЯ роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН ро╣рпЖроЯрпНроЯро┐ро▒рпНроХро╛рой роЙро│рпНро│рпАроЯро╛роХрокрпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроХро┐ро▒родрпБ; рооро▒рпНро▒ ро╡рпЖро│ро┐ропрпАроЯрпБроХро│рпН рокрпБро▒роХрпНроХрогро┐роХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой. роЗроирпНрод роЯрпЛроХрпНроХройрпН, рокроЯродрпНродро┐ройрпН роТро░рпБ рокро┐ро░родро┐роиро┐родро┐родрпНродрпБро╡родрпНродрпИ роОро╡рпНро╡ро╛ро▒рпБ роХрпБро▒ро┐ропро╛роХрпНроХроорпН роЪрпЖропрпНро╡родрпБ роОройрпНрокродрпИроХрпН роХро▒рпНро▒рпБроХрпНроХрпКро│рпНро│ рооро╛роЯро▓рпБроХрпНроХрпБ роЙродро╡рпБроХро┐ро▒родрпБ.

3. рокрпЗроЯрпНроЪрпН рооро▒рпНро▒рпБроорпН роХро▒рпНроХроХрпНроХрпВроЯро┐роп роЙроЯрпНрокрпКродро┐ро╡рпБроХро│ро┐ро▓рпН роЪрпЗро░рпНроХрпНроХ ро╡рпЗрогрпНроЯро┐роп роХроЯрпИроЪро┐ ро╡ро┐ро╖ропроорпН *роиро┐ро▓рпИ роЙроЯрпНрокрпКродро┐ро╡рпБроХро│рпН* роЖроХрпБроорпН, роПройрпЖройро┐ро▓рпН рокроЯрокрпН рокрпЗроЯрпНроЪрпНроХро│рпН роОро╡рпНро╡ро╛ро▒рпБ ро╡ро░ро┐роЪрпИрокрпНрокроЯрпБродрпНродрокрпНрокроЯрпНроЯрпБро│рпНро│рой роОройрпНрокродрпБ рооро╛роЯро▓рпБроХрпНроХрпБродрпН родрпЖро░ро┐ропро╛родрпБ. роиро┐ро▓рпИ роЙроЯрпНрокрпКродро┐ро╡рпБроХро│рпБроорпН роХро▒рпНроХроХрпНроХрпВроЯро┐ропро╡рпИ рооро▒рпНро▒рпБроорпН рокрпЗроЯрпНроЪрпН роЙроЯрпНрокрпКродро┐ро╡рпБроХро│ро┐ройрпН роЕродрпЗ роЕро│ро╡рпИроХрпН роХрпКрогрпНроЯрпБро│рпНро│рой. роЗро▒рпБродро┐ропро╛роХ, роЕройрпИродрпНродрпБ роЙроЯрпНрокрпКродро┐ро╡рпБроХро│рпБроорпН роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роОройрпНроХрпЛроЯро░рпБроХрпНроХрпБ роЕройрпБрокрпНрокрокрпНрокроЯрпБроХро┐ройрпНро▒рой.

4. ро╡рпЖро│ро┐ропрпАроЯрпБ, роХрпБро▒ро┐рокрпНрокро╛роХ `[CLS]` роЯрпЛроХрпНроХройрпБроЯройрпН роЙро│рпНро│ ро╡рпЖро│ро┐ропрпАроЯрпБ роороЯрпНроЯрпБроорпН, роТро░рпБ рооро▓рпНроЯро┐ро▓рпЗропро░рпН рокрпЖро░рпНроЪрпЖрокрпНроЯрпНро░ро╛ройрпН ро╣рпЖроЯрпНроЯро┐ро▒рпНроХрпБ (MLP) роЕройрпБрокрпНрокрокрпНрокроЯрпБроХро┐ро▒родрпБ. ViT-ропро┐ройрпН роорпБройрпНродропро╛ро░ро┐рокрпНрокрпБ роирпЛроХрпНроХроорпН ро╡рпЖро▒рпБрооройрпЗ роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН роЖроХрпБроорпН. рооро▒рпНро▒ роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН ро╣рпЖроЯрпНроХро│рпИрокрпН рокрпЛро▓ро╡рпЗ, MLP ро╣рпЖроЯрпН ро╡рпЖро│ро┐ропрпАроЯрпНроЯрпИ ро╡роХрпБрокрпНрокрпБ ро▓рпЗрокро┐ро│рпНроХро│ро┐ройрпН роорпАродрпБ ро▓ро╛роЬро┐роЯрпНроХро│ро╛роХ рооро╛ро▒рпНро▒ро┐, рооро┐роХро╡рпБроорпН роЪро╛родрпНродро┐ропрооро╛рой ро╡роХрпБрокрпНрокрпИроХрпН роХрогрпНроЯро▒ро┐роп роХро┐ро░ро╛ро╕рпН-роОройрпНроЯрпНро░рпЛрокро┐ ро▓ро╛ро╕рпИроХрпН роХрогроХрпНроХро┐роЯрпБроХро┐ро▒родрпБ.

роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройро┐ро▓рпН роЙроЩрпНроХро│рпН родро┐ро▒роорпИропрпИ роорпБропро▒рпНроЪро┐роХрпНроХродрпН родропро╛ро░ро╛? ViT-роР роОрокрпНрокроЯро┐ роГрокрпИройрпН-роЯро┐ропрпВройрпН роЪрпЖропрпНро╡родрпБ рооро▒рпНро▒рпБроорпН роЕродрпИ роЗройрпНроГрокро░ройрпНро╕рпБроХрпНроХрпБрокрпН рокропройрпНрокроЯрпБродрпНродрпБро╡родрпБ роОройрпНрокродрпИ роЕро▒ро┐роп роОроЩрпНроХро│рпН роорпБро┤рпБроорпИропро╛рой [роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН ро╡ро┤ро┐роХро╛роЯрпНроЯро┐ропрпИрокрпН](https://huggingface.co/docs/transformers/tasks/image_classification) рокро╛ро░рпБроЩрпНроХро│рпН!

<Tip>

ViT рооро▒рпНро▒рпБроорпН BERT-роХрпНроХрпБ роЗроЯрпИропро┐ро▓ро╛рой роЗрогрпИропрпИ роХро╡ройро┐ропрпБроЩрпНроХро│рпН: роЗро░рогрпНроЯрпБроорпН роТроЯрпНроЯрпБроорпКродрпНрод рокро┐ро░родро┐роиро┐родро┐родрпНродрпБро╡родрпНродрпИрокрпН рокро┐роЯро┐роХрпНроХ роТро░рпБ роЪро┐ро▒рокрпНрокрпБ роЯрпЛроХрпНроХройрпИ (`[CLS]`) рокропройрпНрокроЯрпБродрпНродрпБроХро┐ройрпНро▒рой, роЗро░рогрпНроЯрпБроорпН роЕро╡ро▒рпНро▒ро┐ройрпН роЙроЯрпНрокрпКродро┐ро╡рпБроХро│ро┐ро▓рпН роиро┐ро▓рпИродрпН родроХро╡ро▓рпИроЪрпН роЪрпЗро░рпНроХрпНроХро┐ройрпНро▒рой, роорпЗро▓рпБроорпН роЗро░рогрпНроЯрпБроорпН роЯрпЛроХрпНроХройрпНроХро│рпН/рокрпЗроЯрпНроЪрпНроХро│ро┐ройрпН ро╡ро░ро┐роЪрпИропрпИроЪрпН роЪрпЖропро▓ро╛роХрпНроХ роТро░рпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роОройрпНроХрпЛроЯро░рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ройрпНро▒рой.

</Tip>рпИропро╛рой [роЗроорпЗроЬрпН роХро┐ро│ро╛роЪро┐роГрокро┐роХрпЗро╖ройрпН ро╡ро┤ро┐роХро╛роЯрпНроЯро┐ропрпИрокрпН](https://huggingface.co/docs/transformers/tasks/image_classification) рокро╛ро░рпБроЩрпНроХро│рпН!

<Tip>

ViT рооро▒рпНро▒рпБроорпН BERT-роХрпНроХрпБ роЗроЯрпИропро┐ро▓ро╛рой роЗрогрпИропрпИ роХро╡ройро┐ропрпБроЩрпНроХро│рпН: роЗро░рогрпНроЯрпБроорпН роТроЯрпНроЯрпБроорпКродрпНрод рокро┐ро░родро┐роиро┐родро┐родрпНродрпБро╡родрпНродрпИрокрпН рокро┐роЯро┐роХрпНроХ роТро░рпБ роЪро┐ро▒рокрпНрокрпБ роЯрпЛроХрпНроХройрпИ (`[CLS]`) рокропройрпНрокроЯрпБродрпНродрпБроХро┐ройрпНро▒рой, роЗро░рогрпНроЯрпБроорпН роЕро╡ро▒рпНро▒ро┐ройрпН роЙроЯрпНрокрпКродро┐ро╡рпБроХро│ро┐ро▓рпН роиро┐ро▓рпИродрпН родроХро╡ро▓рпИроЪрпН роЪрпЗро░рпНроХрпНроХро┐ройрпНро▒рой, роорпЗро▓рпБроорпН роЗро░рогрпНроЯрпБроорпН роЯрпЛроХрпНроХройрпНроХро│рпН/рокрпЗроЯрпНроЪрпНроХро│ро┐ройрпН ро╡ро░ро┐роЪрпИропрпИроЪрпН роЪрпЖропро▓ро╛роХрпНроХ роТро░рпБ роЯро┐ро░ро╛ройрпНро╕рпНроГрокро╛ро░рпНрооро░рпН роОройрпНроХрпЛроЯро░рпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ройрпНро▒рой.

</Tip>
