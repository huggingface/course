# LLM-களுடன் உரை உருவாக்கும் அனுமானம் பற்றிய ஒரு ஆழமான பார்வை[[inference-with-llms]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="Xp2w1_LKZN4" />

இதுவரை, நாம் டிரான்ஸ்ஃபார்மர் கட்டமைப்பை உரை வகைப்பாடு அல்லது சுருக்கம் போன்ற பல்வேறு தனித்தனி பணிகளுடன் தொடர்புபடுத்தி ஆராய்ந்தோம். இருப்பினும், பெரிய மொழி மாதிரிகள் (LLMs) பெரும்பாலும் உரை உருவாக்கத்திற்காகவே பயன்படுத்தப்படுகின்றன, இதைத்தான் இந்த அத்தியாயத்தில் நாம் ஆராயப் போகிறோம்.

இந்தப் பக்கத்தில், LLM அனுமானத்தின் பின்னணியில் உள்ள முக்கியக் கருத்துக்களை ஆராய்வோம், இந்த மாதிரிகள் எவ்வாறு உரையை உருவாக்குகின்றன மற்றும் அனுமானச் செயல்பாட்டில் உள்ள முக்கிய கூறுகள் பற்றிய விரிவான புரிதலை வழங்குவோம்.

## அடிப்படைகளைப் புரிந்துகொள்ளுதல்

அடிப்படைகளிலிருந்து தொடங்குவோம். அனுமானம் என்பது, கொடுக்கப்பட்ட ஒரு உள்ளீட்டுத் தூண்டுதலிலிருந்து (input prompt) மனிதனைப் போன்ற உரையை உருவாக்க, பயிற்சி பெற்ற ஒரு LLM-ஐப் பயன்படுத்தும் செயல்முறையாகும். மொழி மாதிரிகள், பயிற்சியிலிருந்து பெற்ற தங்கள் அறிவைப் பயன்படுத்தி, ஒரு நேரத்தில் ஒரு வார்த்தையாக பதில்களை உருவாக்குகின்றன. இந்த மாதிரி, பில்லியன் கணக்கான அளவுருக்களிலிருந்து (parameters) கற்றுக்கொண்ட நிகழ்தகவுகளைப் பயன்படுத்தி, ஒரு தொடரில் அடுத்த டோக்கனைக் கணித்து உருவாக்குகிறது. இந்த தொடர்முறை உருவாக்கம் தான் LLM-களை ஒத்திசைவான மற்றும் சூழலுக்குப் பொருத்தமான உரையை உருவாக்க அனுமதிக்கிறது.

## கவனத்தின் பங்கு

கவன (attention) இயந்திரமுறை தான் LLM-களுக்கு சூழலைப் புரிந்துகொள்ளவும், ஒத்திசைவான பதில்களை உருவாக்கவும் திறனை அளிக்கிறது. அடுத்த வார்த்தையைக் கணிக்கும்போது, ஒரு வாக்கியத்தில் உள்ள ஒவ்வொரு வார்த்தைக்கும் சமமான முக்கியத்துவம் இருப்பதில்லை - உதாரணமாக, *"பிரான்சின் தலைநகரம் ..."* என்ற வாக்கியத்தில், "பிரான்ஸ்" மற்றும் "தலைநகரம்" ஆகிய வார்த்தைகள் அடுத்ததாக "பாரிஸ்" வர வேண்டும் என்பதைத் தீர்மானிக்க முக்கியமானவை. பொருத்தமான தகவல்களில் கவனம் செலுத்தும் இந்தத் திறனையே நாம் கவனம் (attention) என்கிறோம்.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="கவனத்தின் காட்சி Gif" width="60%">

அடுத்த டோக்கனைக் கணிக்க மிகவும் பொருத்தமான வார்த்தைகளைக் கண்டறியும் இந்த செயல்முறை நம்பமுடியாத அளவிற்கு பயனுள்ளதாக நிரூபிக்கப்பட்டுள்ளது. LLM-களுக்குப் பயிற்சி அளிப்பதற்கான அடிப்படைக் கொள்கை—அடுத்த டோக்கனைக் கணிப்பது—BERT மற்றும் GPT-2 முதல் பொதுவாக ஒரே மாதிரியாக இருந்தாலும், நரம்பியல் நெட்வொர்க்குகளை அளவிடுவதிலும், கவன இயந்திரமுறையை நீண்ட மற்றும் நீண்ட தொடர்களுக்கு குறைந்த செலவில் செயல்பட வைப்பதிலும் குறிப்பிடத்தக்க முன்னேற்றங்கள் ஏற்பட்டுள்ளன.

<Tip>

சுருக்கமாக, கவன இயந்திரமுறை தான் LLM-கள் ஒத்திசைவான மற்றும் சூழல்-சார்ந்த உரையை உருவாக்க முக்கிய காரணமாகும். இது நவீன LLM-களை முந்தைய தலைமுறை மொழி மாதிரிகளிலிருந்து வேறுபடுத்துகிறது.

</Tip>

### சூழல் நீளம் மற்றும் கவன வரம்பு

இப்போது நாம் கவனத்தைப் புரிந்துகொண்டோம், ஒரு LLM உண்மையில் எவ்வளவு சூழலைக் கையாள முடியும் என்பதை ஆராய்வோம். இது நம்மை சூழல் நீளம் அல்லது மாதிரியின் 'கவன வரம்பிற்கு' அழைத்துச் செல்கிறது.

சூழல் நீளம் என்பது, LLM ஒரே நேரத்தில் செயலாக்கக்கூடிய டோக்கன்களின் (வார்த்தைகள் அல்லது வார்த்தைகளின் பகுதிகள்) அதிகபட்ச எண்ணிக்கையைக் குறிக்கிறது. இதை மாதிரியின் செயல்படும் நினைவகத்தின் (working memory) அளவாக நினையுங்கள்.

இந்தத் திறன்கள் பல நடைமுறைக் காரணிகளால் கட்டுப்படுத்தப்படுகின்றன:
- மாதிரியின் கட்டமைப்பு மற்றும் அளவு
- கிடைக்கக்கூடிய கணினி வளங்கள்
- உள்ளீடு மற்றும் விரும்பிய வெளியீட்டின் சிக்கலான தன்மை

ஒரு சிறந்த உலகில், நாம் மாதிரிக்கு வரம்பற்ற சூழலை வழங்க முடியும், ஆனால் வன்பொருள் கட்டுப்பாடுகள் மற்றும் கணினிச் செலவுகள் இதை நடைமுறைக்கு ஒவ்வாததாக ஆக்குகின்றன. இதனால்தான் வெவ்வேறு மாதிரிகள் திறனையும் செயல்திறனையும் சமநிலைப்படுத்த வெவ்வேறு சூழல் நீளங்களுடன் வடிவமைக்கப்பட்டுள்ளன.

<Tip>

சூழல் நீளம் என்பது, ஒரு பதிலை உருவாக்கும்போது மாதிரி ஒரே நேரத்தில் கருத்தில் கொள்ளக்கூடிய டோக்கன்களின் அதிகபட்ச எண்ணிக்கையாகும்.

</Tip>

### தூண்டுதலின் கலை

நாம் LLM-களுக்கு தகவல்களை அனுப்பும்போது, நமது உள்ளீட்டை LLM-இன் உருவாக்கத்தை விரும்பிய வெளியீட்டை நோக்கி வழிநடத்தும் வகையில் கட்டமைக்கிறோம். இது _தூண்டுதல்_ (prompting) என்று அழைக்கப்படுகிறது.

LLM-கள் தகவல்களை எவ்வாறு செயலாக்குகின்றன என்பதைப் புரிந்துகொள்வது, சிறந்த தூண்டுதல்களை உருவாக்க நமக்கு உதவுகிறது. மாதிரியின் முதன்மைப் பணி ஒவ்வொரு உள்ளீட்டு டோக்கனின் முக்கியத்துவத்தையும் பகுப்பாய்வு செய்வதன் மூலம் அடுத்த டோக்கனைக் கணிப்பதாகும் என்பதால், உங்கள் உள்ளீட்டுத் தொடரின் வார்த்தைகள் மிக முக்கியமானதாகிறது.

<Tip>

தூண்டுதலின் கவனமான வடிவமைப்பு, **LLM-இன் உருவாக்கத்தை விரும்பிய வெளியீட்டை நோக்கி வழிநடத்துவதை எளிதாக்குகிறது**.

</Tip>

## இரண்டு-கட்ட அனுமானச் செயல்முறை

இப்போது நாம் அடிப்படைக் கூறுகளைப் புரிந்துகொண்டோம், LLM-கள் உண்மையில் எவ்வாறு உரையை உருவாக்குகின்றன என்பதைப் பார்ப்போம். இந்த செயல்முறையை இரண்டு முக்கிய கட்டங்களாகப் பிரிக்கலாம்: முன் நிரப்புதல் (prefill) மற்றும் குறிவிலக்குதல் (decode). இந்த கட்டங்கள் ஒரு அசெம்பிளி லைன் போல ஒன்றிணைந்து செயல்படுகின்றன, ஒவ்வொன்றும் ஒத்திசைவான உரையை உருவாக்குவதில் முக்கிய பங்கு வகிக்கின்றன.

### முன் நிரப்புதல் கட்டம்

முன் நிரப்புதல் கட்டம் சமையலில் தயாரிப்பு நிலை போன்றது - இங்குதான் அனைத்து ஆரம்பப் பொருட்களும் செயலாக்கப்பட்டு தயாராக வைக்கப்படுகின்றன. இந்த கட்டத்தில் மூன்று முக்கிய படிகள் உள்ளன:

1. **டோக்கனைசேஷன் (Tokenization)**: உள்ளீட்டு உரையை டோக்கன்களாக மாற்றுதல் (இவை மாதிரி புரிந்துகொள்ளும் அடிப்படைக் கட்டுமானத் தொகுதிகள் என்று நினையுங்கள்)
2. **உட்பொதித்தல் மாற்றம் (Embedding Conversion)**: இந்த டோக்கன்களை அவற்றின் பொருளைப் பிடிக்கும் எண் பிரதிநிதித்துவங்களாக மாற்றுதல்
3. **ஆரம்பச் செயலாக்கம் (Initial Processing)**: இந்த உட்பொதிப்புகளை மாதிரியின் நரம்பியல் நெட்வொர்க்குகள் மூலம் இயக்கி சூழலைப் பற்றிய ஆழமான புரிதலை உருவாக்குதல்

இந்தக் கட்டம் கணினி அடிப்படையில் தீவிரமானது, ஏனெனில் இது அனைத்து உள்ளீட்டு டோக்கன்களையும் ஒரே நேரத்தில் செயலாக்க வேண்டும். ஒரு பதிலை எழுதத் தொடங்குவதற்கு முன் ஒரு முழுப் பத்தியைப் படித்துப் புரிந்துகொள்வது போல இதை நினையுங்கள்.

கீழேயுள்ள ஊடாடும் விளையாட்டு மைதானத்தில் வெவ்வேறு டோக்கனைசர்களுடன் நீங்கள் பரிசோதனை செய்யலாம்:

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

### குறிவிலக்குதல் கட்டம்

முன் நிரப்புதல் கட்டம் உள்ளீட்டைச் செயலாக்கிய பிறகு, நாம் குறிவிலக்குதல் கட்டத்திற்குச் செல்கிறோம் - இங்குதான் உண்மையான உரை உருவாக்கம் நிகழ்கிறது. மாதிரி ஒரு நேரத்தில் ஒரு டோக்கனை உருவாக்குகிறது, இதை நாம் தன்னியக்க செயல்முறை (autoregressive process) என்கிறோம் (இங்கு ஒவ்வொரு புதிய டோக்கனும் முந்தைய அனைத்து டோக்கன்களையும் சார்ந்துள்ளது).

குறிவிலக்குதல் கட்டம் ஒவ்வொரு புதிய டோக்கனுக்கும் நிகழும் பல முக்கிய படிகளை உள்ளடக்கியது:
1. **கவனக் கணக்கீடு (Attention Computation)**: சூழலைப் புரிந்துகொள்ள முந்தைய அனைத்து டோக்கன்களையும் திரும்பிப் பார்த்தல்
2. **நிகழ்தகவுக் கணக்கீடு (Probability Calculation)**: சாத்தியமான ஒவ்வொரு அடுத்த டோக்கனின் நிகழ்தகவையும் தீர்மானித்தல்
3. **டோக்கன் தேர்வு (Token Selection)**: இந்த நிகழ்தகவுகளின் அடிப்படையில் அடுத்த டோக்கனைத் தேர்ந்தெடுத்தல்
4. **தொடர்தல் சரிபார்ப்பு (Continuation Check)**: உருவாக்கத்தைத் தொடர வேண்டுமா அல்லது நிறுத்த வேண்டுமா என்று தீர்மானித்தல்

இந்தக் கட்டம் நினைவகத்தை அதிகம் பயன்படுத்தும், ஏனெனில் மாதிரி முன்பு உருவாக்கப்பட்ட அனைத்து டோக்கன்களையும் அவற்றின் உறவுகளையும் கண்காணிக்க வேண்டும்.

## மாதிரி உத்திகள்

இப்போது மாதிரி எவ்வாறு உரையை உருவாக்குகிறது என்பதைப் புரிந்துகொண்டோம், இந்த உருவாக்கச் செயல்முறையைக் கட்டுப்படுத்தக்கூடிய பல்வேறு வழிகளை ஆராய்வோம். ஒரு எழுத்தாளர் அதிக படைப்பாற்றலுடன் இருக்க வேண்டுமா அல்லது அதிக துல்லியத்துடன் இருக்க வேண்டுமா என்பதைத் தேர்ந்தெடுப்பதைப் போலவே, மாதிரி அதன் டோக்கன் தேர்வுகளை எவ்வாறு செய்கிறது என்பதை நாம் சரிசெய்யலாம்.

SmolLM2 உடன் இந்த இடத்தில் நீங்கள் அடிப்படைக் குறிவிலக்கு செயல்முறையுடன் தொடர்பு கொள்ளலாம் (நினைவில் கொள்ளுங்கள், இது ஒரு **EOS** டோக்கனை அடையும் வரை குறிவிலக்குகிறது, இந்த மாதிரிக்கு அது **<|im_end|>** ஆகும்):

<iframe
	src="https://agents-course-decoding-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

### டோக்கன் தேர்வைப் புரிந்துகொள்ளுதல்: நிகழ்தகவுகளிலிருந்து டோக்கன் தேர்வுகளுக்கு

மாதிரி அடுத்த டோக்கனைத் தேர்ந்தெடுக்க வேண்டியிருக்கும் போது, அது அதன் சொற்களஞ்சியத்தில் உள்ள ஒவ்வொரு வார்த்தைக்கும் மூல நிகழ்தகவுகளுடன் (logits எனப்படும்) தொடங்குகிறது. ஆனால் இந்த நிகழ்தகவுகளை உண்மையான தேர்வுகளாக மாற்றுவது எப்படி? இந்த செயல்முறையை உடைத்துப் பார்ப்போம்:

![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/1.png)  

1. **மூல லாஜிட்கள் (Raw Logits)**: சாத்தியமான ஒவ்வொரு அடுத்த வார்த்தையைப் பற்றிய மாதிரியின் ஆரம்ப உள்ளுணர்வுகள் என இவைகளை நினையுங்கள்
2. **வெப்பநிலை கட்டுப்பாடு (Temperature Control)**: ஒரு படைப்பாற்றல் டயல் போல - அதிக அமைப்புகள் (>1.0) தேர்வுகளை அதிக சீரற்றதாகவும் படைப்பாற்றல் மிக்கதாகவும் ஆக்குகின்றன, குறைந்த அமைப்புகள் (<1.0) அவற்றை அதிக கவனம் செலுத்தியதாகவும் தீர்மானகரமானதாகவும் ஆக்குகின்றன
3. **டாப்-பி (Top-p) மாதிரி எடுத்தல்**: சாத்தியமான எல்லா வார்த்தைகளையும் கருத்தில் கொள்வதற்குப் பதிலாக, நாம் தேர்ந்தெடுத்த நிகழ்தகவு வரம்பை (எ.கா., முதல் 90%) அடையும் மிகவும் சாத்தியமானவற்றை மட்டுமே பார்க்கிறோம்
4. **டாப்-கே (Top-k) வடிகட்டுதல்**: ஒரு மாற்று அணுகுமுறை, இதில் நாம் k மிகவும் சாத்தியமான அடுத்த வார்த்தைகளை மட்டுமே கருத்தில் கொள்கிறோம்

### மீண்டும் மீண்டும் வருவதை நிர்வகித்தல்: வெளியீட்டைப் తాజాగా வைத்திருத்தல்

LLM-களுடன் ஒரு பொதுவான சவால், அவை தங்களைத் தாங்களே மீண்டும் மீண்டும் சொல்லிக்கொள்ளும் போக்கு - ஒரே விஷயங்களுக்குத் திரும்பத் திரும்ப வரும் ஒரு பேச்சாளரைப் போலவே. இதை நிவர்த்தி செய்ய, நாம் இரண்டு வகையான தண்டனைகளைப் பயன்படுத்துகிறோம்:

1. **இருப்புத் தண்டனை (Presence Penalty)**: ஒரு டோக்கன் முன்பு தோன்றியிருந்தால், அது எத்தனை முறை தோன்றியது என்பதைப் பொருட்படுத்தாமல், அதற்கு ஒரு நிலையான தண்டனை விதிக்கப்படுகிறது. இது மாதிரி ஒரே வார்த்தைகளை மீண்டும் பயன்படுத்துவதைத் தடுக்க உதவுகிறது.
2. **அதிர்வெண் தண்டனை (Frequency Penalty)**: ஒரு டோக்கன் எவ்வளவு அடிக்கடி பயன்படுத்தப்பட்டது என்பதன் அடிப்படையில் அதிகரிக்கும் ஒரு அளவீட்டுத் தண்டனை. ஒரு வார்த்தை எவ்வளவு அதிகமாகத் தோன்றுகிறதோ, அவ்வளவு குறைவாக அது மீண்டும் தேர்ந்தெடுக்கப்படும்.

![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/2.png)  

இந்தத் தண்டனைகள் டோக்கன் தேர்வு செயல்முறையின் ஆரம்பத்திலேயே பயன்படுத்தப்படுகின்றன, மற்ற மாதிரி உத்திகள் பயன்படுத்தப்படுவதற்கு முன்பு மூல நிகழ்தகவுகளைச் சரிசெய்கின்றன. மாதிரியை புதிய சொற்களஞ்சியத்தை ஆராய ஊக்குவிக்கும் மென்மையான தூண்டுதல்களாக இவைகளை நினையுங்கள்.

### உருவாக்க நீளத்தைக் கட்டுப்படுத்துதல்: எல்லைகளை அமைத்தல்

ஒரு நல்ல கதைக்கு சரியான வேகம் மற்றும் நீளம் தேவைப்படுவது போலவே, நமது LLM எவ்வளவு உரையை உருவாக்குகிறது என்பதைக் கட்டுப்படுத்த நமக்கு வழிகள் தேவை. இது நடைமுறைப் பயன்பாடுகளுக்கு முக்கியமானது - நாம் ஒரு ட்வீட்-நீள பதிலை உருவாக்குகிறோமா அல்லது ஒரு முழு வலைப்பதிவு இடுகையை உருவாக்குகிறோமா என்பதைப் பொறுத்து.

உருவாக்க நீளத்தை பல வழிகளில் கட்டுப்படுத்தலாம்:
1. **டோக்கன் வரம்புகள்**: குறைந்தபட்ச மற்றும் அதிகபட்ச டோக்கன் எண்ணிக்கையை அமைத்தல்
2. **நிறுத்தத் தொடர்கள்**: உருவாக்கத்தின் முடிவைக் குறிக்கும் குறிப்பிட்ட வடிவங்களை வரையறுத்தல்
3. **தொடர்-முடிவு கண்டறிதல்**: மாதிரி இயற்கையாகவே அதன் பதிலை முடிக்க அனுமதித்தல்

உதாரணமாக, நாம் ஒரு பத்தியை உருவாக்க விரும்பினால், அதிகபட்சமாக 100 டோக்கன்களை அமைக்கலாம் மற்றும் "\n\n" ஐ நிறுத்தத் தொடராகப் பயன்படுத்தலாம். இது நமது வெளியீடு கவனம் செலுத்தியதாகவும் அதன் நோக்கத்திற்குப் பொருத்தமான அளவிலும் இருப்பதை உறுதி செய்கிறது.

![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/3.png)  

### பீம் தேடல்: சிறந்த ஒத்திசைவுக்காக முன்னோக்கிப் பார்த்தல்

இதுவரை நாம் விவாதித்த உத்திகள் ஒரு நேரத்தில் ஒரு டோக்கன் முடிவுகளை எடுக்கும்போது, பீம் தேடல் ஒரு முழுமையான அணுகுமுறையை எடுக்கிறது. ஒவ்வொரு படியிலும் ஒரே ஒரு தேர்வுக்கு உறுதியளிப்பதற்குப் பதிலாக, அது ஒரே நேரத்தில் பல சாத்தியமான பாதைகளை ஆராய்கிறது - பல நகர்வுகளை முன்கூட்டியே சிந்திக்கும் ஒரு சதுரங்க வீரரைப் போல.

![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/4.png)  

இது எவ்வாறு செயல்படுகிறது என்பது இங்கே:
1. ஒவ்வொரு படியிலும், பல வேட்பாளர் தொடர்களை (பொதுவாக 5-10) பராமரிக்கவும்
2. ஒவ்வொரு வேட்பாளருக்கும், அடுத்த டோக்கனுக்கான நிகழ்தகவுகளைக் கணக்கிடவும்
3. தொடர்கள் மற்றும் அடுத்த டோக்கன்களின் மிகவும் நம்பிக்கைக்குரிய சேர்க்கைகளை மட்டுமே வைத்திருக்கவும்
4. விரும்பிய நீளம் அல்லது நிறுத்த நிலையை அடையும் வரை இந்த செயல்முறையைத் தொடரவும்
5. ஒட்டுமொத்தமாக அதிக நிகழ்தகவு கொண்ட தொடரைத் தேர்ந்தெடுக்கவும்

நீங்கள் பீம் தேடலை இங்கே பார்வைக்கு ஆராயலாம்:

<iframe
	src="https://agents-course-beam-search-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

இந்த அணுகுமுறை பெரும்பாலும் அதிக ஒத்திசைவான மற்றும் இலக்கணப்படி சரியான உரையை உருவாக்குகிறது, இருப்பினும் இது எளிமையான முறைகளை விட அதிக கணினி வளங்கள் தேவைப்படுகிறது.

## நடைமுறைச் சவால்கள் மற்றும் மேம்படுத்தல்

LLM அனுமானம் பற்றிய நமது ஆய்வை முடிக்கும்போது, இந்த மாதிரிகளைப் பயன்படுத்தும்போது நீங்கள் எதிர்கொள்ளும் நடைமுறைச் சவால்களையும், அவற்றின் செயல்திறனை எவ்வாறு அளவிடுவது மற்றும் மேம்படுத்துவது என்பதையும் பார்ப்போம்.

### முக்கிய செயல்திறன் அளவீடுகள்

LLM-களுடன் பணிபுரியும்போது, நான்கு முக்கியமான அளவீடுகள் உங்கள் செயல்படுத்தல் முடிவுகளை வடிவமைக்கும்:

1. **முதல் டோக்கனுக்கான நேரம் (TTFT)**: முதல் பதிலை எவ்வளவு விரைவாகப் பெற முடியும்? இது பயனர் அனுபவத்திற்கு முக்கியமானது மற்றும் முதன்மையாக முன் நிரப்புதல் கட்டத்தால் பாதிக்கப்படுகிறது.
2. **வெளியீட்டு டோக்கனுக்கான நேரம் (TPOT)**: அடுத்தடுத்த டோக்கன்களை எவ்வளவு வேகமாக உருவாக்க முடியும்? இது ஒட்டுமொத்த உருவாக்க வேகத்தைத் தீர்மானிக்கிறது.
3. **செயல்திறன் (Throughput)**: ஒரே நேரத்தில் எத்தனை கோரிக்கைகளைக் கையாள முடியும்? இது அளவிடுதல் மற்றும் செலவுத் திறனைப் பாதிக்கிறது.
4. **VRAM பயன்பாடு**: உங்களுக்கு எவ்வளவு GPU நினைவகம் தேவை? இது பெரும்பாலும் நிஜ-உலகப் பயன்பாடுகளில் முதன்மைக் கட்டுப்பாடாகிறது.

### சூழல் நீளச் சவால்

LLM அனுமானத்தில் மிக முக்கியமான சவால்களில் ஒன்று சூழல் நீளத்தை திறம்பட நிர்வகிப்பதாகும். நீண்ட சூழல்கள் அதிக தகவல்களை வழங்குகின்றன, ஆனால் கணிசமான செலவுகளுடன் வருகின்றன:

- **நினைவகப் பயன்பாடு**: சூழல் நீளத்துடன் இருபடி அளவில் வளர்கிறது
- **செயலாக்க வேகம்**: நீண்ட சூழல்களுடன் நேர்கோட்டில் குறைகிறது
- **வள ஒதுக்கீடு**: VRAM பயன்பாட்டை கவனமாக சமநிலைப்படுத்த வேண்டும்

[Qwen2.5-1M](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M) போன்ற சமீபத்திய மாதிரிகள் ஈர்க்கக்கூடிய 1M டோக்கன் சூழல் சாளரங்களை வழங்குகின்றன, ஆனால் இது கணிசமாக மெதுவான அனுமான நேரங்களின் விலையில் வருகிறது. உங்கள் குறிப்பிட்ட பயன்பாட்டு வழக்கிற்கு சரியான சமநிலையைக் கண்டுபிடிப்பதே முக்கியம்.


<div style="max-width: 800px; margin: 20px auto; padding: 20px; 
font-family: system-ui;">
    <div style="border: 2px solid #ddd; border-radius: 8px; 
    padding: 20px; margin-bottom: 20px;">
        <div style="display: flex; align-items: center; 
        margin-bottom: 15px;">
            <div style="flex: 1; text-align: center; padding: 
            10px; background: #f0f0f0; border-radius: 4px;">
                உள்ளீட்டு உரை (மூல)
            </div>
            <div style="margin: 0 10px;">→</div>
            <div style="flex: 1; text-align: center; padding: 
            10px; background: #e1f5fe; border-radius: 4px;">
                டோக்கனைஸ் செய்யப்பட்ட உள்ளீடு
            </div>
        </div>
        <div style="display: flex; margin-bottom: 15px;">
            <div style="flex: 1; border: 1px solid #ccc; 
            padding: 10px; margin: 5px; background: #e8f5e9; 
            border-radius: 4px; text-align: center;">
                சூழல் சாளரம்<br/>(எ.கா., 4K டோக்கன்கள்)
                <div style="display: flex; margin-top: 10px;">
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                    <div style="flex: 1; background: #81c784; 
                    margin: 2px; height: 20px; border-radius: 
                    2px;"></div>
                </div>
            </div>
        </div>
        <div style="display: flex; justify-content: 
        space-between; text-align: center; font-size: 0.9em; 
        color: #666;">
            <div style="flex: 1;">
                <div style="border: 1px solid #ffcc80; padding: 
                8px; margin: 5px; background: #fff3e0; 
                border-radius: 4px;">
                    நினைவகப் பயன்பாடு<br/>∝ நீளம்²
                </div>
            </div>
            <div style="flex: 1;">
                <div style="border: 1px solid #90caf9; padding: 
                8px; margin: 5px; background: #e3f2fd; 
                border-radius: 4px;">
                    செயலாக்க நேரம்<br/>∝ நீளம்
                </div>
            </div>
        </div>
    </div>
</div>

### KV கேச் மேம்படுத்தல்

இந்தச் சவால்களை எதிர்கொள்ள, மிகவும் சக்திவாய்ந்த மேம்படுத்தல்களில் ஒன்று KV (Key-Value) கேச்சிங் ஆகும். இந்த நுட்பம் இடைப்பட்ட கணக்கீடுகளைச் சேமித்து மீண்டும் பயன்படுத்துவதன் மூலம் அனுமான வேகத்தை கணிசமாக மேம்படுத்துகிறது. இந்த மேம்படுத்தல்:
- மீண்டும் மீண்டும் செய்யப்படும் கணக்கீடுகளைக் குறைக்கிறது
- உருவாக்க வேகத்தை மேம்படுத்துகிறது
- நீண்ட-சூழல் உருவாக்கத்தை நடைமுறைக்கு சாத்தியமாக்குகிறது

இதன் வர்த்தகம் கூடுதல் நினைவகப் பயன்பாடு ஆகும், ஆனால் செயல்திறன் நன்மைகள் பொதுவாக இந்தச் செலவை விட அதிகமாக இருக்கும்.

## முடிவுரை

LLM அனுமானத்தைப் புரிந்துகொள்வது, இந்த சக்திவாய்ந்த மாதிரிகளை திறம்பட பயன்படுத்துவதற்கும் மேம்படுத்துவதற்கும் முக்கியமானது. நாம் முக்கிய கூறுகளைப் பார்த்தோம்:

- கவனம் மற்றும் சூழலின் அடிப்படைக் பங்கு
- இரண்டு-கட்ட அனுமானச் செயல்முறை
- உருவாக்கத்தைக் கட்டுப்படுத்த பல்வேறு மாதிரி உத்திகள்
- நடைமுறைச் சவால்கள் மற்றும் மேம்படுத்தல்கள்

இந்தக் கருத்துகளில் தேர்ச்சி பெறுவதன் மூலம், LLM-களை திறம்படவும் திறமையாகவும் பயன்படுத்தும் பயன்பாடுகளை உருவாக்க நீங்கள் நன்கு தயாராக இருப்பீர்கள்.

LLM அனுமானத் துறை வேகமாக வளர்ந்து வருகிறது என்பதை நினைவில் கொள்ளுங்கள், புதிய நுட்பங்கள் மற்றும் மேம்படுத்தல்கள் தொடர்ந்து வெளிவருகின்றன. ஆர்வத்துடன் இருங்கள் மற்றும் உங்கள் குறிப்பிட்ட பயன்பாட்டு நிகழ்வுகளுக்கு எது சிறந்தது என்பதைக் கண்டறிய வெவ்வேறு அணுகுமுறைகளுடன் தொடர்ந்து பரிசோதனை செய்யுங்கள்.
