# Introduzione

## Benvenuto/a al corso di ðŸ¤—!

<Youtube id="00GKzGyWFEs" />

Questo corso ti insegnerÃ  a eseguire compiti di elaborazione del linguaggio naturale (*Natural Language Processing*, NLP) utilizzando le librerie dell'ecosistema di [Hugging Face](https://huggingface.co/): [Transformer ðŸ¤—](https://github.com/huggingface/transformers), [Dataset ðŸ¤—](https://github.com/huggingface/datasets), [Tokenizer ðŸ¤—](https://github.com/huggingface/tokenizers), e [Accelerate ðŸ¤—](https://github.com/huggingface/accelerate). Ti insegneremo anche ad usare il nostro [Hugging Face Hub](https://huggingface.co/models), che Ã¨ completamente gratuito e senza pubblicitÃ .


## Contenuti

Eccoti un breve riassunto dei contenuti del corso:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- I capitoli da 1 a 4 forniscono un'introduzione ai concetti principali della libreria Transformer ðŸ¤—. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello della [Hugging Face Hub](https://huggingface.co/models), ottimizzarlo in un dataset, e condividere i tuoi risultati nel Hub!
- I capitoli da 5 a 8 insegnano le basi dei Dataset ðŸ¤— e dei Tokeniser ðŸ¤—, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP piÃ¹ comuni in maniera autonoma.
- I capitoli da 9 a 12 vanno oltre il NLP, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere dimostrazioni (*demo*) dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare i Transformer ðŸ¤— per qualsiasi problema di apprendimento automatico, o quasi!

Questo corso:

* Richiede una buona conoscenza di Python
* Andrebbe seguito di preferenza a seguito di un corso introduttivo di *deep learning* (apprendimento profondo), come ad esempio [fast.ai's](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) oppure uno dei programmi sviluppati da [DeepLearning.AI](https://www.deeplearning.ai/)
* Non richiede conoscenze pregresse di [PyTorch](https://pytorch.org/) o [TensorFlow](https://www.tensorflow.org/), nonostante sia gradita una conoscienza anche superficiale dell'uno o dell'altro

Quando avrai completato questo corso, ti raccomandiamo di passare al [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh) di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e Memoria a breve termine a lungo termine (*LSTM*)!

## Chi siamo?

A proposito degli autori:

**Matthew Carrigan** Ã¨ Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato Ã¨ stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l'Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell'immortalitÃ  cibernetica.

**Lysandre Debut** Ã¨ Machine Learning Engineer da Hugging Face e ha lavorato ai Transformer ðŸ¤— fin dalle primissime tappe del loro sviluppo. Il suo obiettivo Ã¨ di rendere l'elaborazione del linguaggio naturale accessibile a tutti sviluppando strumenti con un semplice API.

**Sylvain Gugger** Ã¨ Research Engineer da Hugging Face e uno dei principali manutentori della libreria Transformers ðŸ¤—. In passato, Ã¨ stato Research Scientist da fast.ai, e ha scritto [Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/) con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning (*apprendimento profondo*) piÃ¹ accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate.

**Merve Noyan** Ã¨ developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l'accesso al deep learning.

**Lucile Saulnier** Ã¨ machine learning engineer da Hugging Face, e sviluppa e supporta l'utilizzo di strumenti open source. Ãˆ anche attivamente coinvolta in numerosi progetti di ricerca nell'ambito dell'elaborazione del linguaggio naturale, come ad esempio collaborative training e BigScience.

**Lewis Tunstall** Ã¨ machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunitÃ  piÃ¹ ampia. Ãˆ anche co-autore dell'imminente [Oâ€™Reilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/).

**Leandro von Werra** Ã¨ machine learning engineer nel team open-source di Hugging Face, nonchÃ© co-autore dell'imminente [Oâ€™Reilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/). Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli dello stack di machine learning.

Sei pronto/a a iniziare? In questo capitolo, imparerai:
* Ad utilizzare la funzione `pipeline()` per eseguire compiti di NLP come la generazione e classificazione di testi
* L'architettura dei Transformer
* Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d'uso
