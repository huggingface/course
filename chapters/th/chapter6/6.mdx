# WordPiece tokenization

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/th/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/th/chapter6/section6.ipynb"},
]} />

WordPiece ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tokenization ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ Google ‡πÄ‡∏û‡∏∑‡πà‡∏≠ pretrain ‡πÇ‡∏°‡πÄ‡∏î‡∏• BERT ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Transformer ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö BERT ‡πÄ‡∏ä‡πà‡∏ô DistilBERT, MobileBERT, Funnel Transformers, ‡πÅ‡∏•‡∏∞ MPNET

WordPiece ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö BPE ‡πÉ‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÅ‡∏ï‡πà‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô

<Youtube id="qpv6ms_t_1A"/>

<Tip>

üí° ‡∏ö‡∏ó‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á WordPiece ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ implement ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏î‡πâ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

</Tip>

## Training algorithm

<Tip warning={true}>

‚ö†Ô∏è ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Google ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô WordPiece ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô paper ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 100%

</Tip>

‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö BPE ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° WordPiece ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å vocabulary ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ä‡πâ ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô prefix ‡πÄ‡∏ä‡πà‡∏ô `##` (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô BERT) ‡πÑ‡∏ß‡πâ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏°‡∏µ prefix ‡∏ô‡∏µ‡πâ

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"word"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```
w ##o ##r ##d
```

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏ó‡∏∏‡∏Å‡πÜ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ prefix ‡∏û‡∏¥‡πÄ‡∏®‡∏©

‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö BPE ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô WordPiece ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏é‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ merge ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Ñ‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏π‡πà token ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡∏°‡∏≤ merge ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î WordPiece ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏π‡πà ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏´‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏π‡πà token ‡∏î‡πâ‡∏ß‡∏¢‡∏ú‡∏•‡∏Ñ‡∏π‡∏ì‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÉ‡∏ô‡∏Ñ‡∏π‡πà ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏π‡πà ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏π‡∏á
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ï‡πâ‡∏≠‡∏á merge `("un", "##able")` ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô vocabulary ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á `"un"` ‡πÅ‡∏•‡∏∞ `"##able"` ‡∏ï‡πà‡∏≤‡∏á‡∏û‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏Å‡πá‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏π‡∏á
‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏±‡∏ö ‡∏Ñ‡∏π‡πà‡πÄ‡∏ä‡πà‡∏ô `("hu", "##gging")` ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ (‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà "hugging" ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏π‡∏á‡πÉ‡∏ô vocabulary ) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ ‡∏ó‡∏±‡πâ‡∏á`"hu"` ‡πÅ‡∏•‡∏∞ `"##gging" ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡πá‡∏û‡∏ö‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏ö‡πà‡∏≠‡∏¢

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô BPE :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ :

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡∏Ñ‡∏∑‡∏≠ `["b", "h", "p", "##g", "##n", "##s", "##u"]` (‡πÄ‡∏£‡∏≤‡∏Ç‡∏≠‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ)
‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ `("##u", "##g")` ‡∏ã‡∏∂‡πà‡∏á‡∏û‡∏ö 20 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏ï‡πà‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token `"##u"` ‡∏à‡∏∞‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏π‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ score ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (1 / 36)
‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ `"##u"` ‡∏à‡∏∞‡πÑ‡∏î‡πâ score ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ (1 / 36) ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô score ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà `("##g", "##s")` ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ `"##u"` ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ score ‡πÄ‡∏õ‡πá‡∏ô 1 / 20 ‡πÅ‡∏•‡∏∞‡∏Å‡∏é‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ `("##g", "##s") -> ("##gs")`

‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ merge ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏•‡∏ö ‡∏ï‡∏±‡∏ß `##` ‡∏≠‡∏≠‡∏Å‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á token ‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞ merge ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ `"##gs"` ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° token ‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô vocabulary ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏é‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏≥‡πÉ‡∏ô corpus ‡∏î‡πâ‡∏ß‡∏¢ :

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ `"##u"` ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏π‡πà ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏π‡πà‡∏à‡∏∞‡∏°‡∏µ score ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô

‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏é‡πÉ‡∏î‡∏Å‡∏é‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ merge ‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `("h", "##u") -> "hu"` ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ :

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ score ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ `("hu", "##g")` ‡πÅ‡∏•‡∏∞ `("hu", "##gs")` ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏°‡∏µ score ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 1/15 (‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡∏°‡∏µ score ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 1/21) ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å‡πÉ‡∏ô list ‡∏°‡∏≤‡πÉ‡∏ä‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠ merge :

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ vocabulary ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å‡∏û‡∏≠

<Tip>

‚úèÔ∏è **‡∏ï‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ö‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß!** ‡∏Å‡∏é merge ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

</Tip>

## Tokenization algorithm

‡∏Å‡∏≤‡∏£ tokenization ‡πÉ‡∏ô WordPiece ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å BPE ‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà WordPiece ‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ vocabulary ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‡∏Å‡∏é merge
‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞ tokenize ‡∏Ñ‡∏≥‡πÉ‡∏î‡∏Ñ‡∏≥‡∏´‡∏ô‡∏∂‡πà‡∏á WordPiece ‡∏à‡∏∞‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô vocabulary ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏ô‡∏±‡πâ‡∏ô
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ vocabulary ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ tokenize ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"hugs"` ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ `"hug"` ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏°‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["hug", "##s"]`
‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏î‡∏π‡∏ó‡∏µ‡πà `"##s"` ‡πÄ‡∏£‡∏≤‡∏û‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô token ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÑ‡∏î‡πâ `["hug", "##s"]`
‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ BPE ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏±‡∏ô‡∏à‡∏∞ tokenize ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["hu", "##gs"]`

‡∏°‡∏≤‡∏î‡∏π‡∏≠‡∏µ‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"bugs"`
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÑ‡∏õ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ `"b"` ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô vocabulary ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ `["b", "##ugs"]`
‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏π‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"##ugs"` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏û‡∏ö‡∏ß‡πà‡∏≤ `"##u"` ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô vocabulary ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô  `["b", "##u, "##gs"]`
‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏π‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"##gs"` ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏û‡∏ö‡∏ß‡πà‡∏≤ ‡∏°‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏°‡∏±‡∏ô‡∏≠‡∏µ‡∏Å

‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ unknown
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"mug"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å tokenize ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô `["[UNK]"]` ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"bum"` ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≠ `"b"` ‡πÅ‡∏•‡∏∞ `"##u"` ‡πÉ‡∏ô vocabulary ‡πÅ‡∏ï‡πà‡∏ß‡πà‡∏≤ `"##m"` ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô  vocabulary ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô `["[UNK]"]` ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÅ‡∏¢‡∏Å‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô `["b", "##u", "[UNK]"]`
‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å BPE ‡πÇ‡∏î‡∏¢ BPE ‡∏à‡∏∞‡∏î‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÅ‡∏•‡∏∞‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô vocabulary ‡∏Å‡πá‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏±‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô unknown

<Tip>

‚úèÔ∏è **‡∏ñ‡∏∂‡∏á‡∏ï‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß!** ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"pugs"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å tokenize ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?

</Tip>

## Implementing WordPiece

‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ implement ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° WordPiece ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≠‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á BPE ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö corpus ‡πÉ‡∏´‡∏ç‡πà‡πÜ
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ corpus ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ö‡∏ó BPE :

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á pre-tokenize corpus ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÜ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á WordPiece tokenizer (‡πÄ‡∏ä‡πà‡∏ô BERT) ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ `bert-base-cased` tokenizer ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ pre-tokenize

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô corpus :

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ã‡πá‡∏ï‡∏Ç‡∏≠‡∏á alphabet ‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ã‡πá‡∏ï‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà `##` ‡πÑ‡∏ß‡πâ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ :

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° token ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á list ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BERT ‡∏Ñ‡∏∑‡∏≠ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]` :

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏Å‡∏±‡∏ô ‡πÇ‡∏î‡∏¢‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° `##` ‡πÑ‡∏ß‡πâ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ :

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏π‡πà token‡∏Å‡∏±‡∏ô :

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ :

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ score ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ loop ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠ `('a', '##b') -> 'ab'` ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° `'ab'`  ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô vocabulary :

```python
vocab.append("ab")
```

‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡πà‡∏≠ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å merge ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô dictionary `splits` ‡∏Å‡πà‡∏≠‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡∏µ‡πâ :

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ merge ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å :

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏°‡∏µ‡∏ó‡∏∏‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ tokenizer ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ merge ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î vocabulary ‡πÄ‡∏õ‡πá‡∏ô 70 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ :

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

‡∏°‡∏≤‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á vocabulary ‡∏Å‡∏±‡∏ô :

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

‡∏ñ‡πâ‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö BPE ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ tokenizer ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢

<Tip>

üí° ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ `train_new_from_iterator()` ‡∏Å‡∏±‡∏ö corpus ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ vocabulary ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡∏ô‡∏±‡πà‡∏ô‡∏Å‡πá‡πÄ‡∏û‡∏£‡∏≤‡∏∞ ü§ó Tokenizers library ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ WordPiece ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ BPE


</Tip>

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ tokenize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ pre-tokenize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á tokenize ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÇ‡∏î‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏≥‡πÑ‡∏õ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏≠‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡∏µ‡πâ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡πÜ‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÑ‡∏õ

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

‡∏°‡∏≤‡∏ó‡∏î‡∏•‡∏≠‡∏á tokenize ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô vocabulary ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡∏Å‡∏±‡∏ô :

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠ tokenize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏ô :

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á :

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö WordPiece ‡πÉ‡∏ô‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Unigram ‡∏Å‡∏±‡∏ô
