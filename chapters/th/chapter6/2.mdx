# ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏à‡∏≤‡∏Å tokenizer ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},
]} />

‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÉ‡∏ä‡πâ language model ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏î‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡∏π‡∏Å‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å
‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡∏£‡∏ô langauge model ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡πÅ‡∏•‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ tokenizer ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?

‡πÉ‡∏ô[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2](/course/chapter2) ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ (_subword tokenization algorithm_ )
‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥(subword)‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡∏µ‡πâ tokenizer ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£*‡πÄ‡∏ó‡∏£‡∏ô*

‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á tokenizer ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏±‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° 3 ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏ô‡∏µ‡πâ

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenize ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ stochastic gradient descent ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡πà‡∏≤ loss ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å batch ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° random ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô (‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ seed ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ random ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô)
‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenize ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡∏µ‡πâ‡∏Å‡πá‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (deterministic) ‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
</Tip>

## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Assembling a corpus)

ü§ó Transformers ‡∏°‡∏µ API ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô tokenizer ‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `AutoTokenizer.train_new_from_iterator()`

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-2 ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏£‡∏Å ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏£‡∏±‡∏™‡πÄ‡∏ã‡∏µ‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏ö‡∏ö‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠ Python code


‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ [ü§ó Datasets](https://github.com/huggingface/datasets) library ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `load_dataset()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≤‡∏ß‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ cache ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• [CodeSearchNet](https://huggingface.co/datasets/code_search_net)
‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark)
‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å open source libraries ‡∏à‡∏≤‡∏Å GitHub ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏†‡∏≤‡∏©‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Python


```py
from datasets import load_dataset

# This can take a few minutes to load, so grab a coffee or tea while you wait!
raw_datasets = load_dataset("code_search_net", "python")
```

‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡πá‡∏Ñ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á


```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô docstrings ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô code ‡πÅ‡∏•‡∏∞‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô token ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `whole_func_string` ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤

‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏°‡∏≤‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏à‡∏∞ print ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ :

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô _iterator_ ‡∏Ç‡∏≠‡∏á list ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (_iterator_ of lists of texts) ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô list ‡∏Ç‡∏≠‡∏á list ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (list of list of texts)
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ list ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏õ‡πá‡∏ô batch ‡∏à‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà input ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô iterator ‡∏Å‡πá‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Python ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô memory ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
‡∏ñ‡πâ‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ ü§ó Datasets ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô RAM ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô disk ‡πÅ‡∏ó‡∏ô

‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á list ‡∏Ç‡∏≠‡∏á list ‡∏Ç‡∏≠‡∏á 1,000 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (list of lists of 1,000 texts) ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• input ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô memory:

```py
# Don't uncomment the following line unless your dataset is small!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ Python generator ‡πÅ‡∏ó‡∏ô ‡∏Å‡πá‡∏à‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Python ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô memory ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á generator ‡∏Å‡πá‡∏á‡πà‡∏≤‡∏¢‡πÜ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏° `[` ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏•‡πá‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ `(` ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å `raw_datasets` ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô memory ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á iterator ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Python object ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß

‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ `for` loop   ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô iterator ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô memory ‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô(‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà `for` loop ‡∏ß‡∏ô‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á item ‡∏ô‡∏±‡πâ‡∏ô) ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ loop ‡∏à‡∏∞‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà 1000 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô memory ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ memory ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å

‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á generator ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ
```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ print ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á print ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á list ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏•‡∏¥‡∏ï Python generator ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ó‡∏ô:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á generator ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢ ‡πÉ‡∏ä‡πâ `for` loop ‡πÅ‡∏•‡∏∞ `yield` statement:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á generator ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô ‡πÅ‡∏ï‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô logic ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á list comprehension

## ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenizer

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏°‡∏µ iterator ‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô batch ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î tokenizer ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å(‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∑‡∏≠ GPT-2)

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î tokenizer ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÅ‡∏£‡∏Å
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏∂‡∏ó‡∏∂‡∏°‡∏Ç‡∏≠‡∏á tokenizer ‡∏´‡∏£‡∏∑‡∏≠ token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ï‡πà‡∏≤‡∏á‡πÜ tokenizer ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô GPT-2 ‡∏™‡∏¥‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå(vocabulary) ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ

‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ tokenizer ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤ ‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ :

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'n', 'umbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two',
 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`', '."', '""', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

tokenizer ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏ä‡πà‡∏ô `ƒ†` ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÅ‡∏ó‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (space) ‡πÅ‡∏•‡∏∞ `ƒä` ‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà (newline)
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏à‡∏∞‡∏î‡∏µ‡∏ô‡∏±‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞ token ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÜ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô token ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á 4 ‡∏´‡∏£‡∏∑‡∏≠ 8 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î)

‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô tokenizer ‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå `_` ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô


‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Python method ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ã‡∏±‡∏Å‡∏û‡∏±‡∏Å ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î 1.6 GB ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏£‡πá‡∏ß (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 1 ‡∏ô‡∏≤‡∏ó‡∏µ 16 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡∏ö‡∏ô‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π AMD Ryzen 9 3900X CPU ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ 12 cores)

‡∏™‡∏¥‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ß‡∏£‡∏£‡∏π‡πâ‡∏Ñ‡∏∑‡∏≠ `AutoTokenizer.train_new_from_iterator()` ‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß

‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ß‡πà‡∏≤ ü§ó Transformers library ‡∏°‡∏µ tokenizer ‡∏™‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏•‡πâ‡∏ß‡∏ô ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á(‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß)‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å ü§ó Tokenizers library ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤ [Rust](https://www.rust-lang.org) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô
‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤ Python ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô data science ‡πÅ‡∏•‡∏∞ deep learning ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö parallel (‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÜ‡∏Å‡∏±‡∏ô) ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∑‡πà‡∏ô
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏π‡∏ì‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó neural network ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤ CUDA ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô C library ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö GPU
‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà Huggingface ‡∏™‡∏£‡πâ‡∏≤‡∏á ü§ó Tokenizers library ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤

‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏Å‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏©‡∏≤ Rust ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß‡∏ô‡∏µ‡πâ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏©‡∏≤ CUDA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏∞‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô GPU

ü§ó Tokenizers library ‡∏°‡∏µ Python bindings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö method ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤ Rust
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÅ‡∏ö‡∏ö parallel ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏±‡∏ô tokenizer ‡∏Å‡∏±‡∏ö
input ‡πÅ‡∏ö‡∏ö batch [Chapter 3](/course/chapter3)

‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß (‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Å‡∏£‡∏ì‡∏µ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡πá‡∏Ñ‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà[‡∏ô‡∏µ‡πà](https://huggingface.co/transformers/#supported-frameworks))
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß `AutoTokenizer` API ‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏™‡∏°‡∏≠

‡πÉ‡∏ô section ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö feature ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó token classification ‡∏´‡∏£‡∏∑‡∏≠ question answering

‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á tokenizer ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö tokenizer ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏î‡∏π

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`',
 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`."""', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÄ‡∏´‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© `ƒ†` ‡πÅ‡∏•‡∏∞ `ƒä` ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ tokenizer ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ token ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô token `ƒäƒ†ƒ†ƒ†` ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤(indentation) ‡πÅ‡∏•‡∏∞ `ƒ†"""` ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡∏™‡∏≤‡∏°‡∏ï‡∏±‡∏ß ‡∏ó‡∏µ‡πà‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô docstring
‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏µ‡πâ ‡∏¢‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢ ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏ó‡∏µ‡πà `_`

‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡∏î‡∏£‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡∏´‡∏≤‡∏Å‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö tokenizer ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á tokenizer ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° input ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô tokenizer ‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ tokenizer ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà



```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

‡∏°‡∏≤‡∏î‡∏π‡∏≠‡∏µ‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô :

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ƒ†Linear', 'Layer', '():', 'ƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'init', '__(', 'self', ',', 'ƒ†input', '_', 'size', ',',
 'ƒ†output', '_', 'size', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'weight', 'ƒ†=', 'ƒ†torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'ƒ†output', '_', 'size', ')', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'bias', 'ƒ†=', 'ƒ†torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ƒäƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'call', '__(', 'self', ',', 'ƒ†x', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',
 'ƒ†return', 'ƒ†x', 'ƒ†@', 'ƒ†self', '.', 'weights', 'ƒ†+', 'ƒ†self', '.', 'bias', 'ƒäƒ†ƒ†ƒ†ƒ†']
```

‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô token ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏´‡πá‡∏ô token ‡∏Ç‡∏≠‡∏á double indentation ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ `ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†` ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏ô Python ‡πÄ‡∏ä‡πà‡∏ô `class`, `init`, `call`, `self` ‡πÅ‡∏•‡∏∞ `return` ‡∏Å‡πá‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞ token ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡πà‡∏≤ tokenizer ‡∏à‡∏∞‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå `_` ‡πÅ‡∏•‡∏∞ `.` ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô camel-cased ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÄ‡∏ä‡πà‡∏ô `LinearLayer` ‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["ƒ†Linear", "Layer"]`


## ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å tokenizer

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô tokenizer ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ã‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏±‡∏ô‡πÑ‡∏ß‡πâ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ã‡∏ü‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ method ‡∏ä‡∏∑‡πà‡∏≠ `save_pretrained()`

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ü‡πâ‡∏°‡∏á‡∏≤‡∏ô (folder) ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ *code-search-net-tokenizer* ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏Ç‡∏≠‡∏á tokenizer ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÅ‡∏ä‡∏£‡πå tokenizer ‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏±‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hub ‡∏Ç‡∏≠‡∏á Huggingface ‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á login ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏Å‡πà‡∏≠‡∏ô
‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô notebook (‡πÄ‡∏ä‡πà‡∏ô Jupyter notebook) ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ function ‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å

```python
from huggingface_hub import notebook_login

notebook_login()
```

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô widget ‡πÉ‡∏´‡πâ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ö‡∏±‡∏ç‡∏ä‡∏µ Hugging Face
‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ notebook ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô terminal

```bash
huggingface-cli login
```

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ push tokenizer ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hub ‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á repository ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠ `code-search-net-tokenizer` ‡πÉ‡∏ô namespace ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏ã‡∏∂‡πà‡∏á repository ‡∏ô‡∏µ‡πâ‡∏Å‡πá‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö tokenizer ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏ß‡πâ ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏Å‡πá‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î tokenizer ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `from_pretrained()`
```py
# Replace "huggingface-course" below with your actual namespace to use your own tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏Å‡πá‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô ‡πÅ‡∏•‡∏∞ fine-tune language model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ô[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 7](/course/chapter7) ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ö‡∏ó‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö fast tokenizer ‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô `train_new_from_iterator()` ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡πâ‡∏≤‡∏á