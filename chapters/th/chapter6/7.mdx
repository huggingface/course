# Unigram tokenization

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/th/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/th/chapter6/section7.ipynb"},
]} />

‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° Unigram ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô SentencePiece ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏µ‡∏Å‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ tokenization ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡πà‡∏ô AlBERT, T5, mBART, Big Bird, ‡πÅ‡∏•‡∏∞ XLNet

<Youtube id="TGZfZVuF9Yc"/>

<Tip>

üí° ‡∏ö‡∏ó‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á Unigram ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ implement ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏î‡πâ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

</Tip>

## Training algorithm

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö BPE ‡πÅ‡∏•‡∏∞ WordPiece ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö Unigram ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏•‡∏ö token ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ç‡∏ô‡∏≤‡∏î vocabulary ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ BPE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î vocabulary ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà
‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° Unigram ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡∏Ç‡∏≠‡∏á training corpus ‡∏ã‡∏∂‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö vocabulary ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏±‡πâ‡∏ô
‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏•‡∏ö ‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ

‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ token ‡∏ó‡∏µ‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ loss ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
token ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Ñ‡πà‡∏≤ loss ‡∏°‡∏≤‡∏Å ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ ‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏°‡∏≤‡∏Å ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö‡∏°‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÑ‡∏î‡πâ
‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏π‡∏á ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà‡∏•‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏≠‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏•‡∏ö \\(p\\) ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏Ç‡∏≠‡∏á token ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤loss ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (\\(p\\) ‡∏Ñ‡∏∑‡∏≠ hyperparameter ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏î‡πâ ‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 10 ‡∏´‡∏£‡∏∑‡∏≠ 20)
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ç‡∏ô‡∏≤‡∏î vocabulary ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô (base characters) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ tokenize ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ
‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡∏Ç‡∏≠‡∏á corpus ‡πÅ‡∏•‡∏∞‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ loss ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏•‡∏ö token ‡∏ï‡∏±‡∏ß‡πÉ‡∏î‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡∏±‡∏ô

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tokenization ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Unigram
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ corpus ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÜ :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô :

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Tokenization algorithm

‡πÇ‡∏°‡πÄ‡∏î‡∏• Unigram ‡πÄ‡∏õ‡πá‡∏ô language model ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÇ‡∏î‡∏¢‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö token ‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡πÜ (not dependent)
Unigram ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô language model ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô(probability)‡∏Ç‡∏≠‡∏á token ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ token ‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢
‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ Unigram language model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏à‡∏∞‡∏ú‡∏•‡∏¥‡∏ï‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (language model ‡∏à‡∏∞ predict ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô‡∏ú‡∏•‡∏¥‡∏ï‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)

‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á token ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡πÜ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏ö‡πÉ‡∏ô corpus ‡∏´‡∏≤‡∏£‡∏Å‡∏±‡∏ö ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á token ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏ô vocabulary (‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ 1)

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô `"ug"` ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ (subword) ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô `"hug"`, `"pug"`, ‡πÅ‡∏•‡∏∞ `"hugs"` ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ 20

‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏∏‡∏Å‡πÜ‡∏ï‡∏±‡∏ß‡πÉ‡∏ô vocabulary ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ :

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ 210 ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á `"ug"` ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ 20/210

<Tip>

‚úèÔ∏è **‡∏ï‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ö‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß!** ‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏π‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

</Tip>

‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ tokenize ‡∏Ñ‡∏≥‡πÜ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏∏‡∏Å‡πÜ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (segmentation) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ segmentation ‡∏î‡πâ‡∏ß‡∏¢ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• Unigram
‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö token ‡∏ï‡∏±‡∏ß‡∏≠‡∏∑‡πà‡∏ô ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ segmentation ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢ ‡∏ô‡∏≥‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏¢‡πà‡∏≠‡∏¢‡πÉ‡∏ô segmentation ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Ñ‡∏π‡∏ì‡∏Å‡∏±‡∏ô

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤ tokenize ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"pug"` ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ `["p", "u", "g"]` ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á segmentation ‡∏ô‡∏µ‡πâ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÅ‡∏ö‡πà‡∏á‡∏°‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["pu", "g"]` ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏Å‡πá‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö :

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡πâ‡∏≤ segmentation ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡πâ‡∏≠‡∏¢ ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠ 210)
‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏î‡∏µ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡πÜ ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á `"pug"` ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ segmentation ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ `["p", "ug"]` ‡∏´‡∏£‡∏∑‡∏≠ `["pu", "g"]` ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å segmentation ‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏à‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏î‡∏Å‡πà‡∏≠‡∏ô (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°‡πÉ‡∏ô corpus ‡πÉ‡∏´‡∏ç‡πà‡πÜ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏¢ segmentation ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô)
‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô‡πÜ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì segmentation ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏∞‡∏¢‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ

‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏≤ segmentation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤ ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ *Viterbi algorithm*
‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏´‡∏≤ segmentation ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞ tokenize ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤ _a_ ‡πÅ‡∏•‡∏∞ _b_ ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏Å‡πá‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏à‡∏≤‡∏Å _a_ ‡πÑ‡∏õ‡∏´‡∏≤ _b_ ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡πà‡∏≠‡πÑ‡∏õ
‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á segmentation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ segmentation ‡∏ó‡∏µ‡πà‡∏°‡∏µ score ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score ‡∏à‡∏≤‡∏Å‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏≤‡∏ü ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞ loop ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏∏‡∏Å‡πÜ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ score ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≠‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"unhug"` ‡∏Å‡∏±‡∏ô ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏°‡∏µ score ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î :

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô `"unhug"` ‡∏Å‡πá‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô `["un", "hug"]`

<Tip>

‚úèÔ∏è **‡∏ï‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ö‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß!** ‡∏•‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏á `"huggun"`‡πÅ‡∏•‡∏∞ score ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô

</Tip>

## ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ tokenize ‡∏Ñ‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞ tokenize ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô corpus ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô vocabulary ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• Unigram ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÉ‡∏ô corpus (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô)
‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô corpus ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ score ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡πà‡∏≤ loss ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å negative log likelihood ‡∏Ç‡∏≠‡∏á score ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á `-log(P(word))` ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏≥

‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ corpus ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ segmentation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡πÅ‡∏•‡∏∞ score ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• Ngram ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì :

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏≤ loss ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ :

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```
‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏•‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏î‡∏Ñ‡∏≥‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏≠‡∏≠‡∏Å ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ñ‡πà‡∏≤ loss ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà token ‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏±‡∏á‡∏à‡∏≥‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏™‡∏≠‡∏á segmentation ‡∏ó‡∏µ‡πà‡∏°‡∏µ score ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô `"pug"` ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô `["p", "ug"]` ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢
‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏•‡∏ö `"pu"` ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤ loss ‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏•‡∏ö `"hug"` ‡∏≠‡∏≠‡∏Å ‡∏Ñ‡πà‡∏≤ loss ‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å ‡∏ô‡∏±‡πà‡∏ô‡∏Å‡πá‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á `"hug"` ‡πÅ‡∏•‡∏∞ `"hugs"` ‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡πÉ‡∏´‡πâ score ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤ loss ‡∏£‡∏ß‡∏°‡∏Å‡πá‡∏à‡∏∞‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô :

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏•‡∏ö `"pu"` ‡∏≠‡∏≠‡∏Å ‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö `"hug"` ‡πÑ‡∏ß‡πâ

## Implementing Unigram

‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤ implement ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á BPE ‡πÅ‡∏•‡∏∞ WordPiece ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ corpus ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÜ :

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• `xlnet-base-cased` :

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö BPE ‡πÅ‡∏•‡∏∞ WordPiece ‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô(‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà)‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô corpus :

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ ‡∏Ç‡∏ô‡∏≤‡∏î vocabulary ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

vocabulary ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞ tokenize ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏™‡πà‡∏ß‡∏ô token ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô(subwords) ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÜ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Loop through the subwords of length at least 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sort subwords by frequency
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('‚ñÅt', 7), ('is', 5), ('er', 5), ('‚ñÅa', 5), ('‚ñÅto', 4), ('to', 4), ('en', 4), ('‚ñÅT', 3), ('‚ñÅTh', 3), ('‚ñÅThi', 3)]
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏•‡∏∞ subwords ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏™‡∏π‡∏á‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ ‡∏°‡∏≤‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡∏Ç‡∏ô‡∏≤‡∏î 300 token :

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>

üí° SentencePiece ‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° ‡∏ä‡∏∑‡πà‡∏≠ Enhanced Suffix Array (ESA) ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Ngram ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô

</Tip>

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡πÜ‡∏Ñ‡∏≥ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏•‡∏≠‡∏Å‡∏≤‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏π‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡πÜ‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏ô‡∏¢‡∏±‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ :

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á tokenizer ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ tokenize ‡∏Ñ‡∏≥ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° Viterbi ‡∏ä‡πà‡∏ß‡∏¢
‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° Viterbi ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô corpus ‡∏ã‡∏∂‡πà‡∏á segmentation ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡πÉ‡∏ô list ‡∏ä‡∏∑‡πà‡∏≠ `best_segmentations`
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞ tokenize ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0 ‡∏ñ‡∏∂‡∏á ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥) ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠ index ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á token ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô segmentation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏µ‡∏¢‡πå‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ score ‡∏Ç‡∏≠‡∏á segmentation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö index ‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì segmentation ‡πÄ‡∏ï‡πá‡∏°‡πÜ‡πÑ‡∏î‡πâ ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô list ‡πÅ‡∏•‡πâ‡∏ß

‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞ for loop ‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡πÇ‡∏î‡∏¢ loop ‡πÅ‡∏£‡∏Å‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ ‡∏™‡πà‡∏ß‡∏ô loop ‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏à‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡∏±‡πâ‡∏ô
‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á segmentation ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å segmentation ‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö score ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏Å‡∏±‡∏ö score ‡∏Ç‡∏≠‡∏á segmentation ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô `best_segmentations`
‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å loop ‡∏à‡∏ö ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏´‡∏≤ segmentation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ input ‡πÇ‡∏î‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÑ‡∏õ‡∏´‡∏≤‡∏ï‡πâ‡∏ô‡∏Ñ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á :

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # This should be properly filled by the previous steps of the loop
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # If we have found a better segmentation ending at end_idx, we update
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # We did not find a tokenization of the word -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```
‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß :

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢ :

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì loss ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ô :

```python
compute_loss(model)
```

```python out
413.10377642940875
```

‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏Å‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏•‡∏ö token ‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å :

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # We always keep tokens of length 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡∏±‡∏ö token ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å `"ll"` ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ tokenize ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"Hopefully"` ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏•‡∏ö‡∏°‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ `"l"` ‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏ó‡∏ô ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ ‡∏Ñ‡πà‡∏≤ loss ‡∏à‡∏∞‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
‡∏™‡πà‡∏ß‡∏ô `"his"` ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"This"` ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å tokenize ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏°‡∏±‡∏ô‡πÄ‡∏≠‡∏á(‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á) ‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏•‡∏ö‡∏°‡∏±‡∏ô‡∏≠‡∏≠‡∏Å `"his"` ‡∏Å‡πá‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Ñ‡πà‡∏≤ loss ‡∏°‡∏≤‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏±‡∏ô :

```python out
6.376412403623874
0.0
```

<Tip>

üí° ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ô‡∏±‡∏Å ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô SentencePiece ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ loss ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏•‡∏≠‡∏á‡∏•‡∏ö token ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏≠‡∏Å ‡πÇ‡∏î‡∏¢‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà token ‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ segmentation ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ token ‡πÄ‡∏ï‡πá‡∏°‡πÜ
‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡πÜ‡∏ï‡∏±‡∏ß‡πÑ‡∏î‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏õ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÜ‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ loss ‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢

</Tip>

‡∏™‡∏¥‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡∏û‡∏¥‡πà‡∏° token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ä‡πâ‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ô vocabulary ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô loop ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏•‡∏ö token ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary ‡∏à‡∏ô‡πÑ‡∏î‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏≠‡πÉ‡∏à :

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Remove percent_to_remove tokens with the lowest scores.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞ tokenize ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ pre-tokenization ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `encode_word()` ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ :

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅHugging', '‚ñÅFace', '‚ñÅ', 'c', 'ou', 'r', 's', 'e', '.']
```

‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° Unigram ‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô tokenizer ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏ô‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á ü§ó Tokenizers library ‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á tokenizer ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
