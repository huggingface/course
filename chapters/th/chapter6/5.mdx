# Byte-Pair Encoding tokenization

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"},
]} />

‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡πâ‡∏ß Byte-Pair Encoding (BPE) ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á (compress texts) ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á OpenAI ‡πÑ‡∏î‡πâ‡∏ô‡∏≥‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏£‡∏ô GPT ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Transformer ‡πÄ‡∏ä‡πà‡∏ô GPT, GPT-2, RoBERTa, BART, ‡πÅ‡∏•‡∏∞ DeBERTa

<Youtube id="HEikzVL-lZU"/>

<Tip>

üí° ‡∏ö‡∏ó‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á BPE ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ implement ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏î‡πâ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

</Tip>

## ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

BPE ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ normalization ‡πÅ‡∏•‡∏∞ pre-tokenization ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß) ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå (vocabulary) ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥ ‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÜ‡∏Å‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡πâ‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô :

```
"hug", "pug", "pun", "bun", "hugs"
```

vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ `["b", "g", "h", "n", "p", "s", "u"]`  ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ASCII ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ Unicode ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢

‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ ‡∏°‡∏µ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô training corpus ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô unknown token ‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• NLP ‡∏à‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏ô‡∏±‡∏Å

<Tip>

Tokenizer ‡∏Ç‡∏≠‡∏á GPT-2 ‡πÅ‡∏•‡∏∞ RoBERTa (‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô) ‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô Unicode ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô byte ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡πÉ‡∏´‡πâ vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å (256) ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å‡πÜ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÄ‡∏õ‡πá‡∏ô unknown token ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ *byte-level BPE*

</Tip>

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° token ‡πÉ‡∏´‡∏°‡πà‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ vocabulary ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏û‡∏≠‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô BPE ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ *merges* ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏é‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏™‡∏≠‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÉ‡∏ô vocabulary ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
‡∏ï‡∏≠‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏Å‡∏é merges ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Å‡πá‡∏à‡∏∞‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô BPE ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏Ñ‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á token ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô)
‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏µ‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏£‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡πÑ‡∏õ

‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ `"hug"` ‡∏û‡∏ö 10 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô corpus, `"pug"` ‡∏û‡∏ö 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á, `"pun"` ‡∏û‡∏ö 12 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á, `"bun"` ‡∏û‡∏ö 4 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á, ‡πÅ‡∏•‡∏∞ `"hugs"` ‡∏û‡∏ö 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÇ‡∏î‡∏¢‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß) ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡πÜ token

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏π‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡∏π‡πà ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏π‡πà `("h", "u")` ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"hug"` ‡πÅ‡∏•‡∏∞ `"hugs"` ‡πÅ‡∏•‡∏∞‡∏û‡∏ö 15 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô corpus
‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ `("u", "g")` ‡∏ã‡∏∂‡πà‡∏á‡∏û‡∏ö‡πÉ‡∏ô ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"hug"`, `"pug"`, ‡πÅ‡∏•‡∏∞ `"hugs"` ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠ 20 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Å‡∏é‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ merge ‡∏ó‡∏µ‡πà tokenizer ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏∑‡∏≠ `("u", "g") -> "ug"` ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° `"ug"` ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô vocabulary ‡πÅ‡∏•‡∏∞‡πÉ‡∏ô corpus ‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡∏Å‡πá‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô token ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏î‡πâ‡∏ß‡∏¢

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ vocabulary ‡πÅ‡∏•‡∏∞ corpus ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ token ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏π‡πà `("h", "ug")` ‡∏ã‡∏∂‡πà‡∏á‡∏û‡∏ö 15 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô corpus
‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ `("u", "n")` ‡∏ã‡∏∂‡πà‡∏á‡∏û‡∏ö 16 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏é‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ `("u", "n") -> "un"` ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏π‡πà‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡πÉ‡∏ô vocabulary ‡πÅ‡∏•‡∏∞ merge token ‡πÉ‡∏ô corpus ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ :

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ `("h", "ug")` ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∑‡∏≠ `("h", "ug") -> "hug"` ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ token ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ merge ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ :

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á vocabulary ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

<Tip>

‚úèÔ∏è **‡∏ï‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß!** ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏Å‡∏é merge ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

</Tip>

## Tokenization algorithm

‡∏Å‡∏≤‡∏£ tokenization ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÇ‡∏î‡∏¢ input ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å tokenize ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ

1. Normalization (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)
2. Pre-tokenization (‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ tokenize ‡∏à‡∏£‡∏¥‡∏á)
3. ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
4. ‡πÉ‡∏ä‡πâ‡∏Å‡∏é merge ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤

‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏é‡∏™‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ :

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤`"bug"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["b", "ug"]` ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"mug"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["[UNK]", "ug"]` ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ `"m"` ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
‡πâ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"thug"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô `["[UNK]", "hug"]` ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ `"t"` ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡∏Å‡∏é‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏£‡∏ß‡∏° `"u"` ‡πÅ‡∏•‡∏∞ `"g"` ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô `"hu"` ‡πÅ‡∏•‡∏∞ `"g"` ‡∏Å‡πá‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô

<Tip>

‚úèÔ∏è **‡∏ï‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß!** ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `"unhug"` ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

</Tip>

## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á BPE (Implementing BPE)

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ implement ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° BPE ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà implementation ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ BPE ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ corpus ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á corpus ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ :

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ pre-tokenize corpus ‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÜ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á BPE tokenizer ‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô GPT-2 ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ `gpt2` tokenizer ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ pre-tokenize

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'ƒ†is': 2, 'ƒ†the': 1, 'ƒ†Hugging': 1, 'ƒ†Face': 1, 'ƒ†Course': 1, '.': 4, 'ƒ†chapter': 1,
    'ƒ†about': 1, 'ƒ†tokenization': 1, 'ƒ†section': 1, 'ƒ†shows': 1, 'ƒ†several': 1, 'ƒ†tokenizer': 1, 'ƒ†algorithms': 1,
    'Hopefully': 1, ',': 1, 'ƒ†you': 1, 'ƒ†will': 1, 'ƒ†be': 1, 'ƒ†able': 1, 'ƒ†to': 1, 'ƒ†understand': 1, 'ƒ†how': 1,
    'ƒ†they': 1, 'ƒ†are': 1, 'ƒ†trained': 1, 'ƒ†and': 1, 'ƒ†generate': 1, 'ƒ†tokens': 1})
```

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÉ‡∏ô corpus :

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'ƒ†']
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ list ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢ GPT-2 ‡πÉ‡∏ä‡πâ token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏Ñ‡∏∑‡∏≠ `"<|endoftext|>"` :

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô corpus ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô :

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏π‡πà token :

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

‡∏°‡∏≤‡∏î‡∏π‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô dictionary) ‡∏Å‡∏±‡∏ô :

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('ƒ†', 'i'): 2
('ƒ†', 't'): 7
('t', 'h'): 3
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡πÜ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ :

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('ƒ†', 't') 7
```

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Å‡∏é‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ `('ƒ†', 't') -> 'ƒ†t'` ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° `'ƒ†t'` ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô vocabulary :

```python
merges = {("ƒ†", "t"): "ƒ†t"}
vocab.append("ƒ†t")
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ merge ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô dictionary `splits` ‡∏î‡πâ‡∏ß‡∏¢ ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ :

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

‡πÅ‡∏•‡∏∞‡∏ô‡∏µ‡πà‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ merge ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å :

```py
splits = merge_pair("ƒ†", "t", splits)
print(splits["ƒ†trained"])
```

```python out
['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd']
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏°‡∏µ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á vocabulary ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 50 :

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∑‡∏≠ tokenizer ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô 19 ‡∏Å‡∏é (vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏°‡∏µ 31 token ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞ token ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏≠‡∏µ‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß) :

```py
print(merges)
```

```python out
{('ƒ†', 't'): 'ƒ†t', ('i', 's'): 'is', ('e', 'r'): 'er', ('ƒ†', 'a'): 'ƒ†a', ('ƒ†t', 'o'): 'ƒ†to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ƒ†to', 'k'): 'ƒ†tok',
 ('ƒ†tok', 'en'): 'ƒ†token', ('n', 'd'): 'nd', ('ƒ†', 'is'): 'ƒ†is', ('ƒ†t', 'h'): 'ƒ†th', ('ƒ†th', 'e'): 'ƒ†the',
 ('i', 'n'): 'in', ('ƒ†a', 'b'): 'ƒ†ab', ('ƒ†token', 'i'): 'ƒ†tokeni'}
```

‡∏™‡πà‡∏ß‡∏ô vocabulary ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ token ‡∏û‡∏¥‡πÄ‡∏®‡∏©, ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô, ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ merge ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á :

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†', 'ƒ†t', 'is', 'er', 'ƒ†a', 'ƒ†to', 'en', 'Th', 'This', 'ou', 'se',
 'ƒ†tok', 'ƒ†token', 'nd', 'ƒ†is', 'ƒ†th', 'ƒ†the', 'in', 'ƒ†ab', 'ƒ†tokeni']
```

<Tip>

üí° ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ `train_new_from_iterator()` ‡∏Å‡∏±‡∏ö corpus ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ vocabulary ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏π‡πà token ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏à‡∏≠ ‡∏™‡πà‡∏ß‡∏ô ü§ó Tokenizers library ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° ID

</Tip>

‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ tokenize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏∂‡πà‡∏á ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏∑‡∏≠ pre-tokenize ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ tokenize ‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ apply ‡∏Å‡∏é merge :

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô :

```py
tokenize("This is not a token.")
```

```python out
['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.']
```

<Tip warning={true}>

‚ö†Ô∏è ‡∏Å‡∏≤‡∏£ implementation ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞ return error ‡∏ñ‡πâ‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏à‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô vocabulary ‡∏ô‡∏±‡πà‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ
‡πÉ‡∏ô GPT-2 ‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ unknown token ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ byte-level BPE ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà unknown ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡πÜ byte ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á vocabulary ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô
‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡∏∂‡∏Å ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Ç‡∏≠‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏õ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ

</Tip>

‡∏ô‡∏µ‡πà‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° BPE ‡πÉ‡∏ô‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π WordPiece ‡∏Å‡∏±‡∏ô