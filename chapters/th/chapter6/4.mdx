# Normalization ‡πÅ‡∏•‡∏∞ pre-tokenization

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/th/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/th/chapter6/section4.ipynb"},
]} />

‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° 3 ‡πÅ‡∏ö‡∏ö ‡∏Ç‡∏≠‡∏á subword tokenization ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer (Byte-Pair Encoding [BPE], WordPiece, ‡πÅ‡∏•‡∏∞ Unigram)
‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô preprocessing ‡∏ó‡∏µ‡πà tokenizer ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ tokenize ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô

‡∏ö‡∏ó‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÉ‡∏ô‡πÑ‡∏õ‡∏õ‡πå‡πÑ‡∏•‡∏ô‡πå tokenization:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô subtokens ‡∏ï‡∏±‡∏ß tokenizer ‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏Ñ‡∏∑‡∏≠ _normalization_ ‡πÅ‡∏•‡∏∞ _pre-tokenization_

## Normalization

<Youtube id="4IIC2jI9CaU"/>

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô normalization ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å ‡πÅ‡∏•‡∏∞/‡∏´‡∏£‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏≠‡∏≠‡∏Å (accents) ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ö [Unicode normalization](http://www.unicode.org/reports/tr15/) (‡πÄ‡∏ä‡πà‡∏ô NFC ‡∏´‡∏£‡∏∑‡∏≠ NFKC) ‡∏ô‡∏µ‡πà‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà tokenizer ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô

ü§ó Transformers `tokenizer` ‡∏°‡∏µ attribute ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ `backend_tokenizer` ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á tokenizer ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á ü§ó Tokenizers library:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

attribute ‡∏ä‡∏∑‡πà‡∏≠ `normalizer` ‡∏Ç‡∏≠‡∏á `tokenizer` object ‡∏°‡∏µ method ‡∏ä‡∏∑‡πà‡∏≠ `normalize_str()` ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ normalization ‡πÑ‡∏î‡πâ:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("H√©ll√≤ h√¥w are √º?"))
```

```python out
'hello how are u?'
```

‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ checkpoint `bert-base-uncased` ‡∏Å‡∏≤‡∏£ normalization ‡∏à‡∏∂‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏≠‡∏≠‡∏Å

<Tip>

‚úèÔ∏è **‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏™‡∏¥!** ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡∏à‡∏≤‡∏Å checkpoint `bert-base-cased` ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏ô‡∏Å‡∏±‡∏ö input ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á tokenizer ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô cased ‡πÅ‡∏•‡∏∞ uncased

</Tip>

## Pre-tokenization

<Youtube id="grlLV8AIXug"/>

‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô tokenizer ‡∏à‡∏≤‡∏Å raw text ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ ‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô entity ‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡πÄ‡∏ä‡πà‡∏ô‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ‡∏≥ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ pre-tokenization ‡∏î‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 2](/course/chapter2) tokenizer ‡πÅ‡∏ö‡∏ö word-based ‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏£‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞ ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á subtokens ‡∏ó‡∏µ‡πà tokenizer ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fast tokenizer ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏à‡∏∞‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô pre-tokenization ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ method ‡∏ä‡∏∑‡πà‡∏≠ `pre_tokenize_str()` ‡∏Ç‡∏≠‡∏á attribute ‡∏ä‡∏∑‡πà‡∏≠ `pre_tokenizer` ‡∏à‡∏≤‡∏Å `tokenizer` object:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß tokenizer ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö offsets ‡∏î‡πâ‡∏ß‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á offsets mapping ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° input ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á(‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ are) ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ offsets ‡∏¢‡∏±‡∏á‡∏ô‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏Ñ‡πà‡∏≤ offsets ‡∏ï‡∏£‡∏á `are` ‡πÅ‡∏•‡∏∞ `you`

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ BERT tokenizer ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô pre-tokenization ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏™‡πà‡∏ß‡∏ô tokenizer ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏î‡πâ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ tokenizer ‡∏Ç‡∏≠‡∏á GPT-2:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ `ƒ†` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ó‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ decode token ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),
 ('?', (19, 20))]
```

‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ä‡πà‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô BERT tokenizer

‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π T5 tokenizer ‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠ SentencePiece :

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('‚ñÅHello,', (0, 6)), ('‚ñÅhow', (7, 10)), ('‚ñÅare', (11, 14)), ('‚ñÅyou?', (16, 20))]
```

‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡πÉ‡∏ô GPT-2 tokenizer T5 tokenizer ‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏© (`_`) ‡πÅ‡∏ï‡πà‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏£‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏£‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏£‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏î‡πâ‡∏ß‡∏¢ (‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ `Hello`) ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á `are` ‡πÅ‡∏•‡∏∞ `you`

‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤ tokenizers ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏Å‡∏±‡∏ô ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà SentencePiece ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡∏á ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å‡∏™‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° 3 ‡πÅ‡∏ö‡∏ö ‡∏Ç‡∏≠‡∏á subword tokenization

## SentencePiece

[SentencePiece](https://github.com/google/sentencepiece) ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ preprocessing ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å‡∏™‡∏≤‡∏°‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡∏°‡∏±‡∏ô‡∏°‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞ Unicode ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© `‚ñÅ` ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö Unigram algorithm (‡∏î‡∏π[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 7](/course/chapter7/7)) ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô pre-tokenization ‡πÄ‡∏•‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡πÄ‡∏ä‡πà‡∏ô ‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô

‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏•‡∏±‡∏Å‡∏≠‡∏µ‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á SentencePiece ‡∏Ñ‡∏∑‡∏≠ *reversible tokenization* (‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ): ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ treat ‡∏û‡∏ß‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏ß‡∏•‡∏≤ decode ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡∏∑‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° (concatenate)‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡πÅ‡∏•‡∏∞ ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `_` ‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà ‡∏ñ‡∏π‡∏Å normalized

‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ BERT tokenizer ‡∏à‡∏∞‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ô‡∏£‡∏ß‡∏° token ‡∏Å‡∏•‡∏±‡∏ö ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°

## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°

‡πÉ‡∏ô‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏™‡∏≤‡∏°‡πÅ‡∏ö‡∏ö ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö subword tokenization ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà BPE (‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö GPT-2 ‡πÅ‡∏•‡∏∞ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ), WordPiece (‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö BERT), ‡πÅ‡∏•‡∏∞ Unigram (‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö T5 ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ)
‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏î‡∏π‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

‡πÇ‡∏°‡πÄ‡∏î‡∏• | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô | ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å vocabulary ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏é‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° token ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô |  ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å vocabulary ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏é‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° token ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô | ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å vocabulary ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏é‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö token ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary
‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô | ‡∏£‡∏ß‡∏° token ‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢ | ‡∏£‡∏ß‡∏° token ‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ score ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÇ‡∏î‡∏¢ score ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏π‡πà token ‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏ñ‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ï‡πà‡∏≥ | ‡∏•‡∏ö token ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å vocabulary ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ loss ‡∏•‡∏î‡∏•‡∏á ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤ loss ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å training corpus
‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô | ‡∏Å‡∏é‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° token (merge rules) ‡πÅ‡∏•‡∏∞ vocabulary | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏Ñ‡πà vocabulary | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô vocabulary ‡πÅ‡∏•‡∏∞ score ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token
Encoding | ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô |  ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocabulary ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ï‡πâ‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ‡∏à‡∏ô‡∏´‡∏°‡∏î‡∏Ñ‡∏≥ | ‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ score ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

‡πÉ‡∏ô‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö BPE ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏±‡∏ô!