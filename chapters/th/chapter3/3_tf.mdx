<FrameworkSwitchCourse {fw} />

# ‡∏Å‡∏≤‡∏£ Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ Keras

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},
]} />

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô section ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡πá‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏≤‡∏ô‡∏≠‡∏µ‡∏Å‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `model.fit()` ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ö‡∏ô CPU ‡πÑ‡∏î‡πâ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡πÜ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GPU ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á free GPUs ‡∏´‡∏£‡∏∑‡∏≠ TPUs ‡πÑ‡∏î‡πâ‡∏ö‡∏ô [Google Colab](https://colab.research.google.com/)

‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏±‡∏ô‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ô‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô section ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### ‡∏Å‡∏≤‡∏£ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•

‡πÇ‡∏°‡πÄ‡∏î‡∏• TensorFlow ‡∏ó‡∏µ‡πà import ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡πà ü§ó Transformers ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Keras ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Å‡∏±‡∏ö Keras

<Youtube id="rnTGBy2ax1c"/>

‡∏ô‡∏±‡πà‡∏ô‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡πá‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤

<Youtube id="AUozVp78dhk"/>

‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡πÉ‡∏ô [previous chapter](/course/chapter2) ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ `AutoModelForSequenceClassification` class ‡πÇ‡∏î‡∏¢‡∏°‡∏µ 2 labels:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡πÉ‡∏ô [Chapter 2](/course/chapter2) ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å BERT ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£ pretrained ‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏π‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô head ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÉ‡∏™‡πà head ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö (sequence classification) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÅ‡∏ó‡∏ô ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡∏ß‡πà‡∏≤ weights ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ (weights ‡∏Ç‡∏≠‡∏á head ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ) ‡πÅ‡∏•‡∏∞ weights ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô (‡∏Ç‡∏≠‡∏á head ‡πÉ‡∏´‡∏°‡πà) ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° (randomly initialized) ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏∞ fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ dataset ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ `compile()` ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î `fit()` ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏° fine-tuning (‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏ô‡∏≤‡∏ó‡∏µ‡∏ö‡∏ô GPU) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô training loss ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á validation loss ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch 
To fine-tune the model on our dataset, we just have to `compile()` our model and then pass our data to the `fit()` method. This will start the fine-tuning process (which should take a couple of minutes on a GPU) and report training loss as it goes, plus the validation loss at the end of each epoch.

<Tip>

‡∏Ñ‡∏ß‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• ü§ó Transformers ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏• Keras ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏ô‡∏±‡πà‡∏ô‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å loss ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏á ‡πÇ‡∏î‡∏¢‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ loss ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏™‡πà‡∏≠‡∏≤‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ï‡πå loss ‡πÉ‡∏ô‡πÄ‡∏°‡∏ò‡∏≠‡∏î `compile()` ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤ ‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÉ‡∏ä‡πâ internal loss ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á labels ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á input ‡∏î‡πâ‡∏ß‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡∏™‡πà‡∏á‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö labels ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Keras ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ô Part 2 ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏£‡πå‡∏™ ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î loss function ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏¢‡∏∏‡πà‡∏á‡∏¢‡∏≤‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô sequence classification ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ standard Keras loss function ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ - ‡∏Ñ‡∏∏‡∏ì *‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ* ‡πÅ‡∏Ñ‡πà‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á loss ‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Keras ‡πÅ‡∏ï‡πà‡πÇ‡∏î‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß Keras ‡∏à‡∏∞‡∏™‡∏±‡∏ô‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ softmax ‡∏Å‡∏±‡∏ö outputs ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ softmax ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ logits ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å loss function ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ call ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ

</Tip>


### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

<Youtube id="cpzq6ESSM5c"/>

‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô ‡∏°‡∏±‡∏ô‡∏Å‡πá‡∏à‡∏∞‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏û‡∏ö‡∏ß‡πà‡∏≤ loss ‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á‡πÑ‡∏î‡πâ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å *learning rate*
‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö loss ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏™‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á optimizer ‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ Keras ‡∏à‡∏∞ initialize ‡∏ï‡∏±‡∏ß optimizer ‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å ‡πÜ parameters ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á learning rate ‡∏î‡πâ‡∏ß‡∏¢
‡πÅ‡∏ï‡πà‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô ‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• transformer ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å learning rate ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Adam (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ñ‡∏∑‡∏≠ 1e-3 ‡∏´‡∏£‡∏∑‡∏≠ 10 ‡∏¢‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á -3 ‡∏´‡∏£‡∏∑‡∏≠ 0.001.)
‡∏Ñ‡πà‡∏≤ learning rate ‡∏ó‡∏µ‡πà 5e-5 (0.00005) ‡∏ã‡∏∂‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Adam ‡∏ñ‡∏∂‡∏á 20 ‡πÄ‡∏ó‡πà‡∏≤ ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤

‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î learning rate ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ trick ‡∏≠‡∏µ‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î learning rate ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡πâ‡∏≤ ‡πÜ ‡πÑ‡∏î‡πâ ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 
*decaying* ‡∏´‡∏£‡∏∑‡∏≠ *annealing* the learning rate ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô Keras ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ *learning rate scheduler* ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∑‡∏≠
`PolynomialDecay` ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏•‡∏î learning rate ‡∏•‡∏á‡πÅ‡∏ö‡∏ö linearly ‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô scheduler ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏õ‡πá‡∏ô `num_train_steps` ‡∏î‡∏±‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡πà ü§ó Transformers ‡∏Å‡πá‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡πà‡∏ô `create_optimizer()` ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á `AdamW` optimizer ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ learning rate decay ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏•‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏î‡∏ß‡∏Å ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô section ‡∏ï‡πà‡∏≠ ‡πÜ ‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏£‡πå‡∏™

</Tip>

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏°‡∏µ optimizer ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏≠‡∏µ‡πà‡∏¢‡∏° ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡∏•‡∏≠‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏Ç‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠ reset weights ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞ compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ optimizer ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá fit ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

üí° ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ç‡∏∂‡πâ‡∏ô Hub ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà `push_to_hub=True` ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô `TrainingArguments` ‡∏î‡πâ‡∏ß‡∏¢ ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô [Chapter 4](/course/chapter4/3)

</Tip>

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•

<Youtube id="nx10eh4CoOs"/>


‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏á‡∏î‡∏π loss ‡∏Ñ‡πà‡∏≠‡∏¢ ‡πÜ ‡∏•‡∏î‡∏•‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô production ‡∏•‡πà‡∏∞? ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏∞‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡πÅ‡∏Ñ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î `predict()` ‡∏Å‡πá‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô *logits* ‡∏à‡∏≤‡∏Å output head ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß‡∏ï‡πà‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏•‡∏≤‡∏™)

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á logits ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ class ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ `argmax` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ logit ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö class ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÉ‡∏ä‡πâ `preds` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤! ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î metrics ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö MRPC dataset ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡πà‡∏ô `evaluate.load()` ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏û‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏°‡∏ò‡∏≠‡∏î `compute()` ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metric ‡πÑ‡∏î‡πâ:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏≤‡∏à‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÑ‡∏õ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ weight ‡∏Ç‡∏≠‡∏á model head ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô metrics ‡πÑ‡∏î‡πâ ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ accuracy ‡∏ó‡∏µ‡πà 85.78% ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ validation set ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤ F1 score ‡∏ó‡∏µ‡πà 89.97 ‡∏ã‡∏∂‡πà‡∏á metrics ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô metrics ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏ú‡∏• MRPC dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GLUE benchmark ‡πÇ‡∏î‡∏¢‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) ‡πÑ‡∏î‡πâ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ F1 score ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà 88.9 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö base model ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• `uncased` ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• `cased` ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤

‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ fine-tune ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ `Trainer` API ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£ fine-tune ‡∏Å‡∏±‡∏ö NLP tasks ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Chapter 7 ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡∏ù‡∏∂‡∏Å‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Keras API ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ GLUE SST-2 dataset ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡πÑ‡∏ß‡πâ‡πÉ‡∏ô section 2
