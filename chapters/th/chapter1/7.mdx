# โมเดล sequence-to-sequence

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

โมเดล encoder-decoder (หรือเรียกอีกชื่อหนึ่งว่า *โมเดล sequence-to-sequence*) ใช้ทั้งสองส่วนในสถาปัตยกรรม Transformer ในแต่ละชั้น attention layer ของ encoder จะเข้าถึงคำทั้งหมดในประโยคเริ่มต้นได้ ในขณะที่ attention layer ของ decoder สามารถเข้าถึงได้เพียงคำที่อยู่ตำแหน่งก่อนหน้าคำที่กำหนดใน input เท่านั้น

โมเดล pretrain สามารถเทรนมาในลักษณะเดียวกับโมเดล encoder หรือโมเดล decoder ก็ได้ แต่โดยมากแล้วจะซับซ้อนมากกว่า ตัวอย่างเช่น [T5](https://huggingface.co/t5-base) ถูกเทรนมาโดยการแทนที่กลุ่มคำ(ซึ่งอาจจะมีเพียงคำเดียวหรือหลายคำก็ได้)ด้วยคำพิเศษคำเดียว และเป้าหมายคือให้ทำนายข้อความที่คำพิเศษคำนี้แทนที่มา

โมเดล sequence-to-sequence เหมาะกับงานในการสร้างประโยคขึ้นมาใหม่จาก input ที่กำหนดให้ เช่น การสรุปความ, การแปลภาษา, หรือการสร้างคำตอบจากคำถาม

ตัวแทนโมเดลในกลุ่มนี้ได้แก่:

- [BART](https://huggingface.co/transformers/model_doc/bart.html)
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)
- [T5](https://huggingface.co/transformers/model_doc/t5.html)
