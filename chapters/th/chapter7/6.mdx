<FrameworkSwitchCourse {fw} />

# ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô[[‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥ ‡∏î‡∏±‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô [‡∏ö‡∏ó‡∏ó‡∏µ‡πà 1](/course/th/chapter1) ‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ _transfer Learning_ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ ‡πÉ‡∏ô‡∏ö‡∏ó‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏ô‡πâ‡∏ï‡∏î‡∏ô‡∏ï‡∏£‡∏µ ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏•‡∏Å‡∏∏‡∏• ‡πÄ‡∏ä‡πà‡∏ô DNA ‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏ú‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πá‡∏ß ‡πÜ ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏ä‡πà‡∏ô Copilot ‡∏Ç‡∏≠‡∏á TabNine ‡πÅ‡∏•‡∏∞ GitHub ‡∏ã‡∏∂‡πà‡∏á‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÇ‡∏î‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• Codex ‡∏Ç‡∏≠‡∏á OpenAI ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ ‡∏á‡∏≤‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏ö‡∏ö‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (auto-regressive) ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ ‡πÄ‡∏ä‡πà‡∏ô GPT-2

‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏•‡∏á: ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏° ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Python ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏™‡πÅ‡∏ï‡πá‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Python ‡∏ö‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ `matplotlib`, `seaborn`, `pandas` ‡πÅ‡∏•‡∏∞ `scikit-learn` ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏ü‡∏£‡∏°‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏Å‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏Å‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡∏Ñ‡∏á‡∏à‡∏∞‡∏î‡∏µ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ

<Youtube id="Vpjb1lu0MDk"/>

‡πÉ‡∏ô [‡∏ö‡∏ó‡∏ó‡∏µ‡πà 6](/course/th/chapter6) ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÑ‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ã‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏Ñ‡πâ‡∏î Python ‡πÅ‡∏ï‡πà‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Å‡∏±‡∏ö‡∏Ñ‡∏•‡∏±‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö GitHub ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ `Trainer` API ‡πÅ‡∏•‡∏∞ ü§ó Accelerate ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏°‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏¢!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hub ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡∏î‡∏π‡πÑ‡∏î‡πâ[‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28) ‡πÇ‡∏õ‡∏£‡∏î‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
 
## ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•[[‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•]]

‡πÇ‡∏Ñ‡πâ‡∏î Python ‡∏°‡∏µ‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î ‡πÄ‡∏ä‡πà‡∏ô GitHub ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö Python ‡∏ó‡∏∏‡∏Å‡∏≠‡∏±‡∏ô ‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô [‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Transformers](https://learning.oreilly.com/library/view/natural-Language-processing/9781098136789/) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-2 ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏≤‡∏¢‡πÇ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GitHub ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 180 GB ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Python ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 20 ‡∏•‡πâ‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ `codeparrot` ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏à‡∏∂‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡πÅ‡∏ä‡∏£‡πå‡∏ö‡∏ô [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot)

‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏°‡∏≤‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ä‡∏∏‡∏î‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏™‡πÅ‡∏ï‡πá‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Python ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏°‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• `codeparrot` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡∏™‡πÅ‡∏ï‡πá‡∏Å‡∏ô‡∏µ‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏™‡∏ï‡∏£‡∏µ‡∏°‡∏°‡∏¥‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÅ‡∏ó‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡∏≤‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏ï‡∏£‡∏µ‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ï‡∏£‡∏µ‡∏°‡∏°‡∏¥‡πà‡∏á‡πÑ‡∏î‡πâ:

```py
# This cell will take a very long time to execute, so you should skip it and go to
# the next one!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

‡∏ô‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3% ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡∏ã‡∏∂‡πà‡∏á‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà - ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∑‡∏≠ 6 GB ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå Python 600,000 ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå!

‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 2-3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ô‡∏î‡πå‡∏ß‡∏¥‡∏î‡∏ó‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏¢‡∏≤‡∏ß‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏ô Hub ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà ‡πÄ‡∏£‡∏≤‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÇ‡∏î‡∏¢‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏™‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡πà‡∏≤‡∏´‡∏á‡∏∏‡∏î‡∏´‡∏á‡∏¥‡∏î‡πÑ‡∏õ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πà‡∏á‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏•‡∏∑‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ú‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏°!

</Tip>

‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞ 200 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ü‡∏¥‡∏•‡∏î‡πå `content` ‡∏°‡∏µ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤

## ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•[[‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•]]

<Youtube id="ma1TrR7gE7I"/>

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏î‡πâ ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ï‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÇ‡∏õ‡∏£‡∏î‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ GPU ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà 128 ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 1,024 ‡∏´‡∏£‡∏∑‡∏≠ 2,048 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô GPT-2 ‡∏´‡∏£‡∏∑‡∏≠ GPT-3 ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö

‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏µ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 128 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `return_overflowing_tokens` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏™‡πà‡∏ß‡∏ô ‡∏î‡∏±‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡πÉ‡∏ô [‡∏ö‡∏ó‡∏ó‡∏µ‡πà 6](/course/th/chapter6/4) ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `return_length` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ö‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏∞‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏° ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏à‡∏≤‡∏Å‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏£‡∏Å:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 34 ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≤‡∏Å‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ä‡∏¥‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏°‡∏µ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 128 ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô (117 ‡πÅ‡∏•‡∏∞ 41 ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö) ‡∏™‡∏¥‡πà‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏®‡∏©‡πÄ‡∏™‡∏µ‡πâ‡∏¢‡∏ß‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏¥‡πâ‡∏á‡∏°‡∏±‡∏ô‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏¥‡∏•‡∏î‡πå `overflow_to_sample_mapping` ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ä‡∏¥‡πâ‡∏ô‡πÉ‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÉ‡∏î

‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `Dataset.map()` ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ü§ó ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡πà‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á ‡∏î‡∏±‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô [‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3](/course/th/chapter7/3) ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ó‡∏ä‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏°‡∏≤‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ö‡∏ó‡∏ä‡πå‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÑ‡∏î‡πâ ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏Å‡∏±‡∏ô ‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 16.7 ‡∏•‡πâ‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏°‡∏µ 128 ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2.1 ‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-3 ‡πÅ‡∏•‡∏∞ Codex ‡∏Ç‡∏≠‡∏á OpenAI ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô 300 ‡πÅ‡∏•‡∏∞ 100 ‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏• Codex ‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPT-3 ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏±‡∏ô‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤!

<Tip>

‚úèÔ∏è **‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏™‡∏¥!** ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏±‡πâ‡∏ô) ‡πÄ‡∏®‡∏©‡∏Ç‡∏≠‡∏á‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÇ‡∏¢‡∏ô‡∏ó‡∏¥‡πâ‡∏á‡∏Å‡πá‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô `eos_token_id` ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô ‡πÉ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `tokenize()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏õ‡∏£‡∏î‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `truncation=False` ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏£‡∏´‡∏±‡∏™‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

</Tip>


## ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà[[‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà]]

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-2 ‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∏‡πà‡∏ô GPT-2 ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô `bos` ‡πÅ‡∏•‡∏∞ `eos` (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞ ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö) ‡∏£‡∏´‡∏±‡∏™‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÇ‡∏õ‡∏£‡∏î‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `from_pretrained()` ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÇ‡∏õ‡∏£‡∏î‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `from_pretrained()` ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå 124M ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏π‡πÅ‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ó‡∏ä‡πå‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö `DataCollatorForLanguageModeling` ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤ (‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î) ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏û‡∏ï‡∏ä‡πå‡πÅ‡∏•‡πâ‡∏ß ‡∏°‡∏±‡∏ô‡∏¢‡∏±‡∏á‡∏î‡∏π‡πÅ‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏•‡πÄ‡∏ö‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢ -- ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ ‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ (‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏õ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ã‡πâ‡∏≥ `input_ids`

‡πÇ‡∏õ‡∏£‡∏î‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤ `DataCollatorForLanguageModeling` ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏≤‡∏™‡∏Å‡πå (MLM) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ (CLM) ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏à‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MLM ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ CLM ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ï‡πå `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

‡∏•‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô

{#if fw === 'tf'}

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î `prepare_tf_dataset()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TensorFlow ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡πÅ‡∏ô‡∏ß‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∂‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö

</Tip>


‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏≠‡∏∞‡πÑ‡∏£‡∏°‡∏≤‡∏Å! ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö Hugging Face ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡∏π‡∏ó‡∏¥‡∏•‡∏¥‡∏ï‡∏µ‡πâ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

```python
from huggingface_hub import notebook_login

notebook_login()
```

‡∏ô‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏î‡πÄ‡∏à‡πá‡∏ï‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö Hugging Face ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ

‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡∏°‡∏¥‡∏ô‡∏±‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á `Trainer` ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÇ‡∏Ñ‡πÑ‡∏ã‡∏ô‡πå‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏≠‡∏£‡πå‡∏°‡∏≠‡∏±‡∏û‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏ö‡∏ó‡∏ä‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`) ‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÑ‡∏•‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö (Gradient) ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏ï‡∏ä‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏•‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠/‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ ü§ó Accelerate

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° `Trainer` ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏° ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 20 ‡∏´‡∏£‡∏∑‡∏≠ 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü‡∏™‡∏±‡∏Å‡πÅ‡∏Å‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏î‡∏µ‡πÜ ‡∏™‡∏±‡∏Å‡πÄ‡∏•‡πà‡∏°‡∏°‡∏≤‡∏≠‡πà‡∏≤‡∏ô!

```py
trainer.train()
```

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÑ‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hub ‡πÑ‡∏î‡πâ:

```py
trainer.push_to_hub()
```

{:else}

‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å `compile()` ‡πÅ‡∏•‡∏∞ `fit()` ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏≠‡∏£‡πå‡∏°‡∏≠‡∏±‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏°:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å `model.fit()` ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏° ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 20 ‡∏´‡∏£‡∏∑‡∏≠ 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü‡∏™‡∏±‡∏Å‡πÅ‡∏Å‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏î‡∏µ‡πÜ ‡∏™‡∏±‡∏Å‡πÄ‡∏•‡πà‡∏°‡∏°‡∏≤‡∏≠‡πà‡∏≤‡∏ô! ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ú‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÑ‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hub ‡πÑ‡∏î‡πâ:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

‚úèÔ∏è **‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏™‡∏¥!** ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 30 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å `TrainingArguments` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å GPT-2 ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà!

</Tip>

<Tip>

{#if fw === 'pt'}

üí° ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ GPU ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏±‡πà‡∏ô `Trainer` ‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å

{:else}

üí° ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ GPU ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó `MirroredStrategy` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏≠‡∏ö‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå `tf.distribute.MirroredStrategy` ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏ò‡∏≠‡∏î `to_tf_dataset()` ‡∏´‡∏£‡∏∑‡∏≠ `prepare_tf_dataset()` ‡πÉ‡∏î‡πÜ ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å `fit()` ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó `‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï()` ‡∏î‡∏π‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏î‡πâ[‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit)

{/if}

</Tip>

## ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏õ‡∏õ‡πå‡πÑ‡∏•‡∏ô‡πå[[‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏ß‡∏¢‡πÑ‡∏õ‡∏õ‡πå‡πÑ‡∏•‡∏ô‡πå]]

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏´‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á ‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô! ‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏ß‡πâ‡πÉ‡∏ô `‡πÑ‡∏õ‡∏õ‡πå‡πÑ‡∏•‡∏ô‡πå` ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏°‡∏±‡∏ô‡∏•‡∏á‡∏ö‡∏ô GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß ‡∏´‡∏≤‡∏Å‡∏°‡∏µ:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ (scatter plot):

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏π‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏°‡∏±‡∏ô‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á `pandas` ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤? ‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á `DataFrame` ‡∏à‡∏≤‡∏Å‡∏™‡∏≠‡∏á‡∏≠‡∏≤‡∏£‡πå‡πÄ‡∏£‡∏¢‡πå‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡πÄ‡∏•‡∏¢ ‡∏ô‡∏±‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÅ‡∏ó‡∏£‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `x` ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏µ‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡∏Å‡∏≤‡∏£‡∏ß‡∏ô‡∏ã‡πâ‡∏≥ 'for' ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏à‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å ‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏° ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ `groupby`:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

‡∏Å‡πá‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏ß‡∏ô‡∏∞; ‡∏ô‡∏±‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥ ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏î‡∏π‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö `scikit-learn` ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡πÅ‡∏ï‡πá‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Python ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡πÅ‡∏´‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ï‡πà‡∏ô‡∏µ‡πà‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à

{:else}

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡πÅ‡∏ï‡πá‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Python (‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡πÅ‡∏´‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏´‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏ö‡∏ï‡∏ä‡πå‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡∏ô‡∏≤‡∏°‡∏¥‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏¢‡πà‡∏≠‡∏¢ `Trainer` ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏∞‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡∏°‡∏µ ü§ó Accelerate ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢

{/if}

{#if fw === 'pt'}

## ‡πÄ‡∏ó‡∏£‡∏ô‡∏Å‡∏±‡∏ö ü§ó Accelerate[[‡πÄ‡∏ó‡∏£‡∏ô‡∏Å‡∏±‡∏ö-accelerate]]

‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ `Trainer` ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏Å‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ ü§ó Accelerate ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡πÅ‡∏•‡∏∞‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏∏‡∏î‡∏´‡∏±‡∏Å‡∏°‡∏∏‡∏°‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ß‡∏á‡∏à‡∏£‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ã‡πâ‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢

<Youtube id="Hm8_PgVTFuc"/>

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡∏à‡∏∂‡∏á‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ä‡πà‡∏ô `plt`, `pd`, `sk`, `fit` ‡πÅ‡∏•‡∏∞ `predict` ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö `matplotlib.pyplot`, `pandas ` ‡πÅ‡∏•‡∏∞ `sklearn` ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏û‡∏≠‡∏î‡∏µ/‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á ‡∏´‡∏≤‡∏Å‡∏™‡∏¥‡πà‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏î‡πâ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÑ‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏°‡∏≤‡∏Å ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ! ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‡πÅ‡∏•‡∏∞‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï ‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï: ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡πÅ‡∏£‡∏Å‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ï‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏¥‡πâ‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° 1 ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ö‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô:

- ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î (batches)
- ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (weight decay parameters)
- ‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏ß‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô

‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô `"torch"` ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏¢‡∏±‡∏á PyTorch `DataLoader` ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏ö‡∏ó‡∏ä‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏î‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏≠‡∏Ñ‡∏ï‡∏¥ (bias) ‡πÅ‡∏•‡∏∞‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å LayerNorm ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô ‡∏°‡∏±‡∏ô‡πÅ‡∏Ñ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `evaluate()` ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÅ‡∏•‡∏∞ [‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏á‡∏ô‡∏™‡∏ô‡πÄ‡∏ó‡πà‡∏´‡πå](/course/th/chapter7/3) ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠ ‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏ù‡∏∂‡∏Å‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á:

```py
model = GPT2LMHeadModel(config)
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÑ‡∏î‡πâ:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ö‡∏ô TPU ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡∏î‡∏π [‡∏ö‡∏ó‡∏ó‡∏µ‡πà 3](/course/th/chapter3) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

</Tip>

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á `train_dataloader` ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `accelerator.prepare()` ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÑ‡∏î‡πâ ‡πÇ‡∏õ‡∏£‡∏î‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏™‡∏°‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° dataloader ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏¥‡∏Å (classic linear schedule) ‡∏à‡∏≤‡∏Å‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (learning rate) ‡∏ñ‡∏∂‡∏á 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ô‡∏µ‡πâ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hub ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏≠‡∏ö‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå `Repository` ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡∏Ç‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö Hugging Face Hub ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≤‡∏Å ID ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ (‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏±‡∏á‡πÄ‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `repo_name` ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `get_full_repo_name()` ‡∏ó‡∏≥ ):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏Ñ‡∏•‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏•‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô `output_dir` ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏°‡∏ò‡∏≠‡∏î `repo.push_to_hub()` ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch ‡πÑ‡∏î‡πâ

‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ù‡∏∂‡∏Å ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏á‡∏ô‡∏™‡∏ô‡πÄ‡∏ó‡πà‡∏´‡πå ‡πÅ‡∏ï‡πà‡∏ô‡∏±‡πà‡∏ô‡∏Å‡πá‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÅ‡∏õ‡∏•‡∏Å‡πÉ‡∏à‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏°‡∏µ‡∏ó‡∏∏‡∏Å‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°: ‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏° ‡πÉ‡∏ô‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡∏ï‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÅ‡∏ö‡∏ó‡∏ä‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ ‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏™‡∏°‡πÑ‡∏•‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏ï‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏•‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏à‡∏ö‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ô‡∏µ‡πâ ‡∏ó‡∏∏‡∏Å‡∏™‡∏≠‡∏á‡∏™‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `evaluate()` ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏Å‡πá‡∏°‡∏µ‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ ‡πÄ‡∏ä‡πà‡∏ô GPT-2 ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ

<Tip>

‚úèÔ∏è **‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏™‡∏¥!** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏•‡∏á‡πÉ‡∏ô‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å

</Tip>

<Tip>

‚úèÔ∏è **‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏™‡∏¥!** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô TensorBoard ‡∏´‡∏£‡∏∑‡∏≠ Weights & Biases ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡πÑ‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

</Tip>

{/if}
