<FrameworkSwitchCourse {fw} />

# การสรุปความหมาย[[การสรุปความหมาย]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
]} />

{/if}


ในส่วนนี้ เราจะมาดูกันว่าโมเดล Transformer สามารถใช้เพื่อย่อเอกสารขนาดยาวให้เป็นบทสรุปได้อย่างไร งานที่เรียกว่า _text summarization_ นี่เป็นหนึ่งในงาน NLP ที่ท้าทายที่สุด เนื่องจากต้องใช้ความสามารถที่หลากหลาย เช่น การทำความเข้าใจข้อความที่ยาว และการสร้างข้อความที่สอดคล้องกันซึ่งรวบรวมหัวข้อหลักในเอกสาร อย่างไรก็ตาม เมื่อทำได้ดี การสรุปข้อความเป็นเครื่องมืออันทรงพลังที่สามารถเพิ่มความเร็วให้กับกระบวนการทางธุรกิจต่างๆ โดยแบ่งเบาภาระของผู้เชี่ยวชาญโดเมนในการอ่านเอกสารขนาดยาวอย่างละเอียด

<Youtube id="yHnr5Dk2zCI"/>

แม้ว่าจะมีโมเดลที่ได้รับการปรับแต่งอย่างละเอียดสำหรับการสรุปอยู่แล้วบน [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads) แต่โมเดลเกือบทั้งหมดเหมาะสำหรับเอกสารภาษาอังกฤษเท่านั้น ดังนั้น เพื่อเพิ่มความแปลกใหม่ในส่วนนี้ เราจะฝึกโมเดลสองภาษาสำหรับภาษาอังกฤษและสเปน ในตอนท้ายของส่วนนี้ คุณจะมี [model](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) ที่สามารถสรุปบทวิจารณ์ของลูกค้าได้เหมือนกับที่แสดงไว้ที่นี่ :

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ดังที่เราจะเห็นว่าการสรุปเหล่านี้มีความกระชับเนื่องจากได้เรียนรู้จากชื่อที่ลูกค้าระบุไว้ในการวิจารณ์ผลิตภัณฑ์ของตน เริ่มต้นด้วยการรวบรวมคลังข้อมูลสองภาษาที่เหมาะสมสำหรับงานนี้

## การเตรียมคลังข้อมูลหลายภาษา[[การเตรียมคลังข้อมูลหลายภาษา]]

เราจะใช้ [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) เพื่อสร้างตัวสรุปสองภาษาของเรา คลังข้อมูลนี้ประกอบด้วยบทวิจารณ์ผลิตภัณฑ์ของ Amazon ในหกภาษา และโดยทั่วไปจะใช้เพื่อเปรียบเทียบตัวแยกประเภทหลายภาษา อย่างไรก็ตาม เนื่องจากการรีวิวแต่ละครั้งมีชื่อย่อประกอบ เราจึงสามารถใช้ชื่อดังกล่าวเป็นบทสรุปเป้าหมายเพื่อให้โมเดลของเราเรียนรู้ได้! ในการเริ่มต้น ให้ดาวน์โหลดชุดย่อยภาษาอังกฤษและสเปนจาก Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

อย่างที่คุณเห็น สำหรับแต่ละภาษา มีบทวิจารณ์ 200,000 รายการสำหรับการแยก 'train' และ 5,000 บทวิจารณ์สำหรับการแยก `validation` และ `test` แต่ละรายการ ข้อมูลบทวิจารณ์ที่เราสนใจมีอยู่ในคอลัมน์ `review_body` และ `review_title` ลองมาดูตัวอย่างบางส่วนโดยการสร้างฟังก์ชันง่ายๆ ที่จะสุ่มตัวอย่างจากชุดการฝึกด้วยเทคนิคที่เราเรียนรู้ใน [บทที่ 5](/course/th/chapter5):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

✏️ **ลองดูสิ!** เปลี่ยนการสุ่ม seed ในคำสั่ง `Dataset.shuffle()` เพื่อสำรวจบทวิจารณ์อื่นๆ ในคลังข้อมูล หากคุณเป็นผู้พูดภาษาสเปน ลองดูบทวิจารณ์บางส่วนใน `spanish_dataset` เพื่อดูว่าชื่อต่างๆ ดูเหมือนเป็นบทสรุปที่สมเหตุสมผลหรือไม่

</Tip>

ตัวอย่างนี้แสดงความหลากหลายของบทวิจารณ์ที่มักพบทางออนไลน์ ตั้งแต่เชิงบวกไปจนถึงเชิงลบ (และทุกอย่างในระหว่างนั้น!) แม้ว่าตัวอย่างที่มีชื่อ "meh" จะไม่ได้ให้ข้อมูลมากนัก แต่ชื่ออื่นๆ ก็ดูเหมือนเป็นการสรุปบทวิจารณ์ที่ดีพอสมควร การฝึกอบรมโมเดลการสรุปสำหรับการตรวจสอบทั้งหมด 400,000 รายการอาจใช้เวลานานเกินไปบน GPU ตัวเดียว ดังนั้น เราจะมุ่งเน้นไปที่การสร้างข้อมูลสรุปสำหรับโดเมนเดียวของผลิตภัณฑ์แทน เพื่อให้เข้าใจว่าเราสามารถเลือกโดเมนใดได้ ให้แปลง `english_dataset` เป็น `pandas.DataFrame` และคำนวณจำนวนบทวิจารณ์ต่อหมวดหมู่ผลิตภัณฑ์:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

สินค้ายอดนิยมในชุดข้อมูลภาษาอังกฤษคือสินค้าในครัวเรือน เสื้อผ้า และอุปกรณ์อิเล็กทรอนิกส์ไร้สาย อย่างไรก็ตาม เพื่อให้ยึดถือธีมของ Amazon ต่อไป เราจะมุ่งเน้นไปที่การสรุปบทวิจารณ์หนังสือ เพราะนี่คือสิ่งที่บริษัทก่อตั้งขึ้น! เราเห็นหมวดหมู่ผลิตภัณฑ์สองหมวดหมู่ที่เหมาะกับใบเรียกเก็บเงิน (`หนังสือ` และ `digital_ebook_purchase`) ดังนั้น เรามากรองชุดข้อมูลทั้งสองภาษาสำหรับผลิตภัณฑ์เหล่านี้เท่านั้น ดังที่เราเห็นใน [บทที่ 5](/course/th/chapter5) ฟังก์ชัน `Dataset.filter()` ช่วยให้เราสามารถแบ่งชุดข้อมูลได้อย่างมีประสิทธิภาพมาก ดังนั้นเราจึงกำหนดฟังก์ชันง่ายๆ เพื่อทำสิ่งนี้ได้:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

ตอนนี้เมื่อเราใช้ฟังก์ชันนี้กับ `english_dataset` และ `spanish_dataset` ผลลัพธ์จะมีเฉพาะแถวที่เกี่ยวข้องกับหมวดหมู่หนังสือ ก่อนที่จะใช้ตัวกรอง มาเปลี่ยนรูปแบบของ `english_dataset` จาก `"pandas"` กลับไปเป็น `"arrow"`:

```python
english_dataset.reset_format()
```

จากนั้น เราสามารถใช้ฟังก์ชันตัวกรอง และเพื่อเป็นการตรวจสอบความเหมาะสม เราจะตรวจสอบตัวอย่างบทวิจารณ์เพื่อดูว่าเกี่ยวข้องกับหนังสือจริงหรือไม่:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

โอเค เราจะเห็นว่าบทวิจารณ์ไม่ได้เกี่ยวกับหนังสืออย่างเคร่งครัด และอาจอ้างถึงสิ่งต่างๆ เช่น ปฏิทินและแอปพลิเคชันอิเล็กทรอนิกส์ เช่น OneNote อย่างไรก็ตาม ดูเหมือนว่าโดเมนจะมีสิทธิ์ในการฝึกอบรมโมเดลการสรุป ก่อนที่เราจะดูโมเดลต่างๆ ที่เหมาะกับงานนี้ เรามีการเตรียมข้อมูลขั้นตอนสุดท้ายที่ต้องทำ: รวมบทวิจารณ์ภาษาอังกฤษและสเปนเป็นออบเจ็กต์ `DatasetDict` เดียว 🤗 Dataset มีฟังก์ชัน `concatenate_datasets()` ที่มีประโยชน์ ซึ่ง (ตามชื่อที่แนะนำ) จะซ้อนวัตถุ `Dataset` สองชิ้นไว้ซ้อนกัน ดังนั้น ในการสร้างชุดข้อมูลสองภาษา เราจะวนซ้ำแต่ละการแยก เชื่อมต่อชุดข้อมูลสำหรับการแยกนั้น และสับเปลี่ยนผลลัพธ์เพื่อให้แน่ใจว่าแบบจำลองของเราจะไม่พอดีกับภาษาเดียว:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÑADO'
'>> Review: Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

ดูเหมือนว่ารีวิวนี้เป็นการผสมผสานระหว่างรีวิวภาษาอังกฤษและสเปนอย่างแน่นอน! ตอนนี้เรามีคลังข้อมูลการฝึกอบรมแล้ว สิ่งสุดท้ายที่ต้องตรวจสอบคือการแจกแจงคำในบทวิจารณ์และชื่อเรื่อง นี่เป็นสิ่งสำคัญอย่างยิ่งสำหรับงานการสรุป โดยที่การสรุปอ้างอิงสั้นๆ ในข้อมูลสามารถโน้มน้าวโมเดลให้ส่งออกเพียงหนึ่งหรือสองคำในสรุปที่สร้างขึ้นได้ โครงเรื่องด้านล่างแสดงการแจกแจงคำ และเราจะเห็นว่าชื่อเรื่องมีการบิดเบือนอย่างมากไปเพียง 1-2 คำ:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

เพื่อจัดการกับสิ่งนี้ เราจะกรองตัวอย่างที่มีชื่อสั้นมากออก เพื่อให้แบบจำลองของเราสามารถสร้างบทสรุปที่น่าสนใจยิ่งขึ้น เนื่องจากเรากำลังจัดการกับข้อความภาษาอังกฤษและสเปน เราจึงสามารถใช้การศึกษาพฤติกรรมคร่าวๆ เพื่อแยกชื่อบนช่องว่าง จากนั้นใช้เมธอด `Dataset.filter()` ที่เชื่อถือได้ของเราดังนี้:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

ตอนนี้เราได้เตรียมคลังข้อมูลของเราแล้ว เรามาดูโมเดล Transformer ที่เป็นไปได้สองสามแบบที่อาจปรับแต่งได้!

## โมเดลสำหรับการสรุปข้อความ[[โมเดลสำหรับการสรุปข้อความ]]

หากคุณลองคิดดู การสรุปข้อความเป็นงานประเภทเดียวกับการแปลด้วยคอมพิวเตอร์ เรามีเนื้อหาข้อความเช่นบทวิจารณ์ที่เราต้องการ "translate" เป็นเวอร์ชันที่สั้นกว่าซึ่งรวบรวมคุณลักษณะเด่นของอินพุต ดังนั้น โมเดล Transformer ส่วนใหญ่สำหรับการสรุปจึงใช้สถาปัตยกรรมตัวเข้ารหัส-ตัวถอดรหัสที่เราพบครั้งแรกใน [บทที่ 1](/course/th/chapter1) แม้ว่าจะมีข้อยกเว้นบางประการ เช่น ตระกูลโมเดล GPT ซึ่งสามารถใช้ในการสรุปได้ในไม่กี่- การตั้งค่าช็อต ตารางต่อไปนี้แสดงรายการโมเดลที่ได้รับการฝึกล่วงหน้ายอดนิยมบางรุ่นที่สามารถปรับแต่งอย่างละเอียดเพื่อการสรุปได้

| Transformer model | Description                                                                                                                                                                                                    | Multilingual? |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending "TL;DR" at the end of the input text.                                                                          |      ❌       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks. |      ❌       |
|     [T5](https://huggingface.co/t5-base)      | A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is `summarize: ARTICLE`.                              |      ❌       |
|     [mT5](https://huggingface.co/google/mt5-base)     | A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages.                                                                                                |      ✅       |
|    [BART](https://huggingface.co/facebook/bart-base)     | A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2.                                    |      ❌       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | A multilingual version of BART, pretrained on 50 languages.                                                                                                                                                     |      ✅       |

ดังที่คุณเห็นจากตารางนี้ โมเดล Transformer ส่วนใหญ่สำหรับการสรุป (และงาน NLP ส่วนใหญ่) เป็นแบบภาษาเดียว วิธีนี้จะดีมากหากงานของคุณเป็นภาษาที่มี "ทรัพยากรสูง" เช่น อังกฤษหรือเยอรมัน แต่จะน้อยกว่านั้นสำหรับภาษาอื่นๆ หลายพันภาษาที่ใช้ทั่วโลก โชคดีที่มีรุ่น Transformer หลายภาษา เช่น mT5 และ mBART เข้ามาช่วยเหลือ แบบจำลองเหล่านี้ได้รับการฝึกอบรมล่วงหน้าโดยใช้การสร้างแบบจำลองภาษา แต่มีจุดหักมุม: แทนที่จะฝึกอบรมเกี่ยวกับคลังข้อมูลของภาษาเดียว แบบจำลองเหล่านี้ได้รับการฝึกอบรมร่วมกันเกี่ยวกับข้อความในกว่า 50 ภาษาในคราวเดียว!

เราจะมุ่งเน้นไปที่ mT5 ซึ่งเป็นสถาปัตยกรรมที่น่าสนใจซึ่งใช้ T5 ซึ่งได้รับการฝึกฝนล่วงหน้าในกรอบงานข้อความเป็นข้อความ ใน T5 งาน NLP ทุกงานได้รับการกำหนดไว้ในรูปแบบของคำนำหน้าพร้อมท์ เช่น `summarize:` ซึ่งเป็นเงื่อนไขให้โมเดลปรับข้อความที่สร้างขึ้นให้เข้ากับพร้อมท์ ดังแสดงในรูปด้านล่าง สิ่งนี้ทำให้ T5 มีความหลากหลายอย่างยิ่ง เนื่องจากคุณสามารถแก้ไขงานต่างๆ มากมายด้วยโมเดลเดียว!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

mT5 ไม่ได้ใช้คำนำหน้า แต่มีความอเนกประสงค์เหมือนกับ T5 อยู่มาก และมีข้อดีคือสามารถพูดได้หลายภาษา ตอนนี้เราได้เลือกแบบจำลองแล้ว มาดูการเตรียมข้อมูลสำหรับการฝึกอบรมกันดีกว่า


<Tip>

✏️ **ลองดูสิ!** เมื่อคุณได้ศึกษาส่วนนี้แล้ว มาดูกันว่า mT5 เปรียบเทียบกับ mBART ได้ดีเพียงใดโดยการปรับแต่งส่วนหลังด้วยเทคนิคเดียวกัน สำหรับคะแนนโบนัส คุณสามารถลองปรับแต่ง T5 อย่างละเอียดเฉพาะบทวิจารณ์ภาษาอังกฤษได้ เนื่องจาก T5 มีพรอมต์คำนำหน้าพิเศษ คุณจะต้องเติม `summarize:` ไว้ข้างหน้าตัวอย่างอินพุตในขั้นตอนการประมวลผลล่วงหน้าด้านล่าง

</Tip>

## การประมวลผลข้อมูลล่วงหน้า[[การประมวลผลข้อมูลล่วงหน้า]]

<Youtube id="1m7BerpSq8A"/>

ภารกิจต่อไปของเราคือการสร้างโทเค็นและเข้ารหัสบทวิจารณ์และชื่อเรื่องของเรา ตามปกติ เราจะเริ่มต้นด้วยการโหลดโทเค็นไนเซอร์ที่เกี่ยวข้องกับจุดตรวจสอบโมเดลที่ได้รับการฝึกล่วงหน้า เราจะใช้ `mt5-small` เป็นจุดตรวจสอบของเรา เพื่อให้เราปรับแต่งโมเดลได้อย่างละเอียดในระยะเวลาที่เหมาะสม:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

💡 ในช่วงแรกของโครงการ NLP แนวทางปฏิบัติที่ดีคือการฝึกอบรมคลาสโมเดล "small" บนตัวอย่างข้อมูลขนาดเล็ก ซึ่งจะทำให้คุณสามารถแก้ไขจุดบกพร่องและทำซ้ำได้เร็วขึ้นในเวิร์กโฟลว์ตั้งแต่ต้นทางถึงปลายทาง เมื่อคุณมั่นใจในผลลัพธ์แล้ว คุณสามารถขยายขนาดโมเดลได้ตลอดเวลาโดยเพียงแค่เปลี่ยนจุดตรวจสอบโมเดล!

</Tip>

มาทดสอบโทเค็น mT5 ด้วยตัวอย่างเล็กๆ น้อยๆ:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ที่นี่เราจะเห็น `input_ids` และ `attention_mask` ที่คุ้นเคยซึ่งเราพบในการทดลองปรับแต่งครั้งแรกใน [บทที่ 3](/course/th/chapter3) มาถอดรหัส ID อินพุตเหล่านี้ด้วยฟังก์ชัน `convert_ids_to_tokens()` ของโทเค็นเพื่อดูว่าโทเค็นที่เรากำลังจัดการกับอยู่ประเภทใด:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '</s>']
```

อักขระ Unicode พิเศษ `▁` และโทเค็นจุดสิ้นสุดของลำดับ `</s>` ระบุว่าเรากำลังจัดการกับโทเค็น Sentence Piece ซึ่งขึ้นอยู่กับอัลกอริธึมการแบ่งส่วน Unigram ที่กล่าวถึงใน [บทที่ 6](/course/th/chapter6 ). Unigram มีประโยชน์อย่างยิ่งสำหรับคลังข้อมูลหลายภาษา เนื่องจากทำให้ Sentence Piece ไม่เชื่อเรื่องสำเนียง เครื่องหมายวรรคตอน และข้อเท็จจริงที่ว่าหลายภาษา เช่น ภาษาญี่ปุ่น ไม่มีอักขระช่องว่าง

ในการสร้างโทเค็นคลังข้อมูลของเรา เราต้องจัดการกับรายละเอียดปลีกย่อยที่เกี่ยวข้องกับการสรุป เนื่องจากป้ายกำกับของเราเป็นข้อความด้วย จึงเป็นไปได้ที่ป้ายเหล่านี้มีขนาดเกินขนาดบริบทสูงสุดของโมเดล ซึ่งหมายความว่าเราจำเป็นต้องใช้การตัดทอนทั้งบทวิจารณ์และชื่อเรื่องเพื่อให้แน่ใจว่าเราจะไม่ส่งข้อมูลที่ยาวเกินไปไปยังโมเดลของเรา โทเค็นไนเซอร์ใน 🤗 Transformers มีอาร์กิวเมนต์ `text_target` ที่ดี ซึ่งช่วยให้คุณสร้างโทเค็นป้ายกำกับขนานกับอินพุตได้ นี่คือตัวอย่างวิธีประมวลผลอินพุตและเป้าหมายสำหรับ mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

มาดูโค้ดนี้เพื่อทำความเข้าใจว่าเกิดอะไรขึ้น สิ่งแรกที่เราทำคือกำหนดค่าสำหรับ `max_input_length` และ `max_target_length` ซึ่งกำหนดขีดจำกัดสูงสุดสำหรับระยะเวลาในการวิจารณ์และชื่อของเรา เนื่องจากโดยทั่วไปเนื้อหาบทวิจารณ์จะมีขนาดใหญ่กว่าชื่อเรื่องมาก เราจึงปรับขนาดค่าเหล่านี้ให้สอดคล้องกัน

ด้วย `preprocess_function()` จึงเป็นเรื่องง่ายที่จะสร้างโทเค็นคลังข้อมูลทั้งหมดโดยใช้ฟังก์ชัน `Dataset.map()` ที่มีประโยชน์ซึ่งเราใช้อย่างกว้างขวางตลอดหลักสูตรนี้:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

เมื่อคลังข้อมูลได้รับการประมวลผลล่วงหน้าแล้ว เรามาดูหน่วยวัดบางส่วนที่ใช้กันทั่วไปสำหรับการสรุปกัน ดังที่เราจะเห็นแล้วว่าการวัดคุณภาพของข้อความที่เครื่องสร้างขึ้นนั้นไม่มีประเด็นเสียเลย

<Tip>

💡 คุณอาจสังเกตเห็นว่าเราใช้ `batched=True` ในฟังก์ชัน `Dataset.map()` ด้านบน วิธีนี้จะเข้ารหัสตัวอย่างเป็นกลุ่มละ 1,000 รายการ (ค่าเริ่มต้น) และช่วยให้คุณสามารถใช้ความสามารถแบบมัลติเธรดของโทเค็นไนเซอร์ที่รวดเร็วใน 🤗 Transformers หากเป็นไปได้ ลองใช้ `batched=True` เพื่อให้ได้ประโยชน์สูงสุดจากการประมวลผลล่วงหน้าของคุณ!

</Tip>


## เมตริกสำหรับการสรุปข้อความ[[เมตริกสำหรับการสรุปข้อความ]]

<Youtube id="TMshhnrEXlg"/>

เมื่อเปรียบเทียบกับงานอื่นๆ ส่วนใหญ่ที่เราได้กล่าวถึงในหลักสูตรนี้ การวัดประสิทธิภาพของงานการสร้างข้อความ เช่น การสรุปหรือการแปลนั้นไม่ได้ตรงไปตรงมามากนัก ตัวอย่างเช่น เมื่อได้รับบทวิจารณ์เช่น "ฉันชอบอ่าน Hunger Games" มีบทสรุปที่ถูกต้องหลายรายการ เช่น "ฉันชอบ Hunger Games" หรือ "Hunger Games เป็นหนังสือที่น่าอ่าน" เห็นได้ชัดว่าการใช้การจับคู่แบบตรงทั้งหมดระหว่างบทสรุปที่สร้างขึ้นกับป้ายกำกับไม่ใช่วิธีแก้ปัญหาที่ดี แม้แต่มนุษย์ก็ยังได้รับผลที่ไม่ดีหากใช้เมตริกดังกล่าว เนื่องจากเราทุกคนมีสไตล์การเขียนเป็นของตัวเอง

สำหรับการสรุป ตัวชี้วัดที่ใช้บ่อยที่สุดประการหนึ่งคือ [คะแนน ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (ย่อมาจาก Recall-Oriented Understudy for Gisting Evaling) แนวคิดพื้นฐานเบื้องหลังหน่วยวัดนี้คือการเปรียบเทียบข้อมูลสรุปที่สร้างขึ้นกับชุดข้อมูลสรุปอ้างอิงที่โดยทั่วไปแล้วมนุษย์สร้างขึ้น เพื่อให้แม่นยำยิ่งขึ้น สมมติว่าเราต้องการเปรียบเทียบข้อมูลสรุปสองรายการต่อไปนี้:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

วิธีหนึ่งในการเปรียบเทียบอาจเป็นการนับจำนวนคำที่ทับซ้อนกัน ซึ่งในกรณีนี้คือ 6 อย่างไรก็ตาม วิธีนี้ค่อนข้างหยาบ ดังนั้น ROUGE จึงใช้การคำนวณคะแนน _precision_ และ _recall_ สำหรับการทับซ้อนกันแทน

<Tip>

🙋 ไม่ต้องกังวลหากนี่เป็นครั้งแรกที่คุณได้ยินเกี่ยวกับความแม่นยำ (precision) และการจดจำ (recall) เราจะพิจารณาตัวอย่างที่ชัดเจนร่วมกันเพื่อให้ทุกอย่างชัดเจน เมตริกเหล่านี้มักจะพบในงานจำแนกประเภท ดังนั้นหากคุณต้องการทำความเข้าใจว่าความแม่นยำและการจดจำถูกกำหนดไว้อย่างไรในบริบทนั้น เราขอแนะนำให้ตรวจสอบ [คำแนะนำ](https://scikit-learn.org/stable) `scikit-learn` /auto_examples/model_selection/plot_precision_recall.html)

</Tip>

สำหรับ ROUGE การเรียกคืนจะวัดว่าข้อมูลสรุปที่สร้างขึ้นบันทึกได้มากน้อยเพียงใด หากเราเพียงเปรียบเทียบคำศัพท์ การจำสามารถคำนวณได้ตามสูตรต่อไปนี้:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

สำหรับตัวอย่างง่ายๆ ข้างต้น สูตรนี้ให้ค่าที่จำได้สมบูรณ์แบบคือ 6/6 = 1; กล่าวคือ คำทั้งหมดในข้อมูลสรุปอ้างอิงถูกสร้างขึ้นโดยแบบจำลอง อาจฟังดูดี แต่ลองจินตนาการดูว่าบทสรุปที่เราสร้างขึ้นคือ "ฉันชอบอ่าน Hunger Games ทั้งคืนจริงๆ" สิ่งนี้น่าจะมีการเรียกคืนได้อย่างสมบูรณ์แบบ แต่อาจเป็นบทสรุปที่แย่กว่านั้นเนื่องจากมีรายละเอียดมาก เพื่อจัดการกับสถานการณ์เหล่านี้ เรายังคำนวณความแม่นยำด้วย ซึ่งในบริบท ROUGE จะวัดว่าสรุปที่สร้างขึ้นมีความเกี่ยวข้องมากน้อยเพียงใด:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

การใช้สิ่งนี้กับการสรุปแบบละเอียดของเราจะให้ความแม่นยำ 6/10 = 0.6 ซึ่งแย่กว่าความแม่นยำที่ 6/7 = 0.86 ที่ได้จากอันที่สั้นกว่าของเราอย่างมาก ในทางปฏิบัติ มักจะคำนวณทั้งความแม่นยำและการเรียกคืน จากนั้นจึงรายงานคะแนน F1 (ค่าเฉลี่ยฮาร์มอนิกของความแม่นยำและการเรียกคืน) เราสามารถทำได้ง่ายๆ ใน 🤗 ชุดข้อมูลโดยการติดตั้งแพ็คเกจ `rouge_score` ก่อน:

```py
!pip install rouge_score
```

จากนั้นโหลดเมตริก ROUGE ดังต่อไปนี้:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

จากนั้นเราสามารถใช้ฟังก์ชัน `rouge_score.compute()` เพื่อคำนวณเมตริกทั้งหมดในคราวเดียว:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

โอ้ มีข้อมูลมากมายในผลลัพธ์นั้น ทั้งหมดนี้หมายความว่าอย่างไร อันดับแรก 🤗 Datasets จะคำนวณช่วงความเชื่อมั่นเพื่อความแม่นยำ การเรียกคืน และคะแนน F1 คุณลักษณะเหล่านี้คือคุณลักษณะ `low`, `mid`, และ `high` ที่คุณเห็นได้ที่นี่ นอกจากนี้ 🤗 Datasets ยังคำนวณคะแนน ROUGE ที่หลากหลาย ซึ่งขึ้นอยู่กับความละเอียดของข้อความประเภทต่างๆ เมื่อเปรียบเทียบผลสรุปที่สร้างขึ้นและผลอ้างอิง รูปแบบ `rouge1` คือการทับซ้อนของยูนิแกรม นี่เป็นเพียงวิธีที่หรูหราในการพูดการทับซ้อนกันของคำ และเป็นหน่วยเมตริกที่เราได้กล่าวถึงข้างต้นทุกประการ เพื่อยืนยันสิ่งนี้ เราจะดึงค่า `mid` ของคะแนนของเราออกมา:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

เยี่ยมมาก ความแม่นยำและการจดจำตัวเลขตรงกัน! แล้วคะแนน ROUGE อื่นๆ เหล่านั้นล่ะ? `rouge2` วัดการทับซ้อนระหว่างบิ๊กแกรม (ลองนึกถึงการทับซ้อนกันของคู่คำ) ในขณะที่ `rougeL` และ `rougeLsum` วัดลำดับการจับคู่ที่ยาวที่สุดโดยการค้นหาสตริงย่อยทั่วไปที่ยาวที่สุดในข้อมูลสรุปที่สร้างขึ้นและการอ้างอิง "ผลรวม" ใน `rougeLsum` หมายถึงข้อเท็จจริงที่ว่าเมตริกนี้คำนวณจากผลสรุปทั้งหมด ในขณะที่ `rougeL` คำนวณเป็นค่าเฉลี่ยในแต่ละประโยค

<Tip>

✏️ **ลองดูสิ!** สร้างตัวอย่างของคุณเองของการสรุปที่สร้างขึ้นและอ้างอิง และดูว่าคะแนน ROUGE ที่ได้นั้นสอดคล้องกับการคำนวณด้วยตนเองตามสูตรเพื่อความแม่นยำและการจดจำหรือไม่ สำหรับคะแนนโบนัส ให้แบ่งข้อความออกเป็นบิ๊กแกรมและเปรียบเทียบความแม่นยำและการจดจำสำหรับเมตริก `rouge2`

</Tip>

เราจะใช้คะแนน ROUGE เหล่านี้เพื่อติดตามประสิทธิภาพของแบบจำลองของเรา แต่ก่อนที่จะทำเช่นนั้น เรามาทำสิ่งที่ผู้ปฏิบัติงาน NLP ที่ดีทุกคนควรทำ: สร้างบรรทัดฐาน (baseline) ที่แข็งแกร่งแต่เรียบง่าย!

### การสร้างบรรทัดฐานที่แข็งแกร่ง[[การสร้างบรรทัดฐานที่แข็งแกร่ง]]

บรรทัดฐานทั่วไปสำหรับการสรุปข้อความคือการใช้สามประโยคแรกของบทความ ซึ่งมักเรียกว่าบรรทัดฐาน _lead-3_ เราสามารถใช้จุดหยุดเพื่อติดตามขอบเขตของประโยคได้ แต่จะล้มเหลวเมื่อใช้ตัวย่อเช่น "U.S." หรือ "U.N." -- ดังนั้น เราจะใช้ไลบรารี `nltk` แทน ซึ่งมีอัลกอริธึมที่ดีกว่าในการจัดการกรณีเหล่านี้ คุณสามารถติดตั้งแพ็คเกจโดยใช้ `pip` ดังนี้:

```python
!pip install nltk
```

จากนั้นดาวน์โหลดกฎเครื่องหมายวรรคตอน:

```python
import nltk

nltk.download("punkt")
```

ต่อไป เราจะนำเข้า tokenizer ประโยคจาก `nltk` และสร้างฟังก์ชันง่ายๆ เพื่อแยกสามประโยคแรกในการทบทวน แบบแผนในการสรุปข้อความคือการแยกแต่ละสรุปด้วยการขึ้นบรรทัดใหม่ ดังนั้นเรามารวมสิ่งนี้และทดสอบกับตัวอย่างการฝึกอบรม:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

ดูเหมือนว่าจะได้ผล ดังนั้นตอนนี้เรามาใช้ฟังก์ชันที่แยก "summaries" เหล่านี้จากชุดข้อมูลและคำนวณคะแนน ROUGE สำหรับข้อมูลพื้นฐาน:

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

จากนั้นเราสามารถใช้ฟังก์ชันนี้เพื่อคำนวณคะแนน ROUGE เหนือชุดการตรวจสอบความถูกต้อง และทำให้สวยงามขึ้นเล็กน้อยโดยใช้ Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

เราจะเห็นได้ว่าคะแนน `rouge2` ต่ำกว่าคะแนนที่เหลืออย่างมาก สิ่งนี้น่าจะสะท้อนถึงความจริงที่ว่าชื่อบทวิจารณ์มักจะกระชับ ดังนั้นบรรทัดฐานของ Lead-3 จึงละเอียดเกินไป ตอนนี้เรามีพื้นฐานที่ดีแล้ว เรามามุ่งความสนใจไปที่การปรับแต่ง mT5 กันดีกว่า!

{#if fw === 'pt'}

## ปรับแต่ง mT5 อย่างละเอียดด้วย `Trainer` API[[ปรับแต่ง-mT5-อย่างละเอียดด้วย-trainer-api]]

การปรับแต่งแบบจำลองอย่างละเอียดสำหรับการสรุปจะคล้ายกับงานอื่นๆ ที่เรากล่าวถึงในบทนี้มาก สิ่งแรกที่เราต้องทำคือโหลดโมเดลที่ได้รับการฝึกมาจากจุดตรวจสอบ `mt5-small` เนื่องจากการสรุปเป็นงานตามลำดับ เราจึงสามารถโหลดโมเดลด้วยคลาส `AutoModelForSeq2SeqLM` ซึ่งจะดาวน์โหลดและแคชน้ำหนักโดยอัตโนมัติ:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## การปรับแต่ง mT5 อย่างละเอียดด้วย Keras[[การปรับแต่ง-mt5-อย่างละเอียดด้วย-keras]]

การปรับแต่งแบบจำลองอย่างละเอียดสำหรับการสรุปจะคล้ายกับงานอื่นๆ ที่เรากล่าวถึงในบทนี้มาก สิ่งแรกที่เราต้องทำคือโหลดโมเดลที่ได้รับการฝึกมาจากจุดตรวจสอบ `mt5-small` เนื่องจากการสรุปเป็นงานตามลำดับ เราจึงสามารถโหลดโมเดลด้วยคลาส `TFAutoModelForSeq2SeqLM` ซึ่งจะดาวน์โหลดและแคชน้ำหนักโดยอัตโนมัติ:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

💡 หากคุณสงสัยว่าเหตุใดคุณไม่เห็นคำเตือนใดๆ เกี่ยวกับการปรับแต่งโมเดลในงานดาวน์สตรีม นั่นเป็นเพราะว่าสำหรับงานตามลำดับต่อลำดับ เราจะเก็บน้ำหนักทั้งหมดของเครือข่ายไว้ เปรียบเทียบสิ่งนี้กับโมเดลการจัดหมวดหมู่ข้อความของเราใน [บทที่ 3](/course/th/chapter3) โดยที่ส่วนหัวของโมเดลที่ฝึกไว้ล่วงหน้าถูกแทนที่ด้วยเครือข่ายที่เริ่มต้นแบบสุ่ม

</Tip>

สิ่งต่อไปที่เราต้องทำคือเข้าสู่ระบบ Hugging Face Hub หากคุณใช้โค้ดนี้ในโน้ตบุ๊ก คุณสามารถทำได้โดยใช้ฟังก์ชันยูทิลิตีต่อไปนี้:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ซึ่งจะแสดงวิดเจ็ตที่คุณสามารถป้อนข้อมูลรับรองของคุณได้ หรือคุณสามารถรันคำสั่งนี้ในเทอร์มินัลของคุณและเข้าสู่ระบบที่นั่น:

```
huggingface-cli login
```

{#if fw === 'pt'}

เราจะต้องสร้างข้อมูลสรุปเพื่อคำนวณคะแนน ROUGE ระหว่างการฝึกอบรม โชคดีที่ 🤗 Transformers มีคลาส `Seq2SeqTrainingArguments` และ `Seq2SeqTrainer` โดยเฉพาะที่สามารถทำสิ่งนี้ให้เราได้โดยอัตโนมัติ! หากต้องการดูวิธีการทำงาน ขั้นแรกเรามากำหนดไฮเปอร์พารามิเตอร์และข้อโต้แย้งอื่นๆ สำหรับการทดสอบของเรากันก่อน:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

ในที่นี้ อาร์กิวเมนต์ `predict_with_generate` ได้รับการตั้งค่าเพื่อระบุว่าเราควรสร้างบทสรุประหว่างการประเมิน เพื่อให้เราสามารถคำนวณคะแนน ROUGE สำหรับแต่ละยุคได้ ตามที่กล่าวไว้ใน [บทที่ 1](/course/th/chapter1) ตัวถอดรหัสทำการอนุมานโดยการทำนายโทเค็นทีละรายการ และดำเนินการโดยใช้เมธอด `generate()` ของโมเดล การตั้งค่า `predict_with_generate=True` จะบอก `Seq2SeqTrainer` ให้ใช้วิธีการนั้นในการประเมิน นอกจากนี้เรายังได้ปรับไฮเปอร์พารามิเตอร์เริ่มต้นบางส่วนด้วย เช่น อัตราการเรียนรู้ จำนวนยุค และการลดลงของน้ำหนัก และเราได้ตั้งค่าตัวเลือก `save_total_limit` ให้บันทึกจุดตรวจสอบได้สูงสุด 3 จุดในระหว่างการฝึกเท่านั้น ทั้งนี้เป็นเพราะแม้แต่ mT5 เวอร์ชัน "small" ใช้พื้นที่ฮาร์ดไดรฟ์ประมาณ GB และเราสามารถประหยัดพื้นที่ได้เล็กน้อยด้วยการจำกัดจำนวนสำเนาที่เราบันทึก

อาร์กิวเมนต์ `push_to_hub=True` จะช่วยให้เราสามารถผลักดันโมเดลไปที่ Hub หลังจากการฝึกอบรม คุณจะพบพื้นที่เก็บข้อมูลใต้โปรไฟล์ผู้ใช้ของคุณในตำแหน่งที่กำหนดโดย `output_dir` โปรดทราบว่าคุณสามารถระบุชื่อของพื้นที่เก็บข้อมูลที่คุณต้องการพุชไปได้ด้วยอาร์กิวเมนต์ `hub_model_id` (โดยเฉพาะ คุณจะต้องใช้อาร์กิวเมนต์นี้เพื่อพุชไปยังองค์กร) ตัวอย่างเช่น เมื่อเราผลักโมเดลไปที่ [`huggingface-course` Organization](https://huggingface.co/huggingface-course) เราได้เพิ่ม `hub_model_id="huggingface-course/mt5-finetuned-amazon-en- es"` ถึง `Seq2SeqTrainingArguments`

สิ่งต่อไปที่เราต้องทำคือจัดเตรียมฟังก์ชัน `compute_metrics()` ให้กับเทรนเนอร์ เพื่อให้เราสามารถประเมินโมเดลของเราระหว่างการฝึกได้ สำหรับการสรุป สิ่งนี้มีความเกี่ยวข้องมากกว่าการเรียก `rouge_score.compute()` กับการทำนายของแบบจำลอง เนื่องจากเราจำเป็นต้อง _decode_ ผลลัพธ์และป้ายกำกับเป็นข้อความก่อนจึงจะสามารถคำนวณคะแนน ROUGE ได้ ฟังก์ชันต่อไปนี้ทำหน้าที่ดังกล่าวทุกประการ และยังใช้ฟังก์ชัน `sent_tokenize()` จาก `nltk` เพื่อแยกประโยคสรุปด้วยการขึ้นบรรทัดใหม่:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

ต่อไป เราต้องกำหนดตัวเปรียบเทียบข้อมูลสำหรับงานลำดับต่อลำดับของเรา เนื่องจาก mT5 เป็นโมเดล Transformer ตัวเข้ารหัส-ตัวถอดรหัส (encoder-decoder) ความละเอียดอ่อนอย่างหนึ่งในการเตรียมแบทช์ของเราก็คือในระหว่างการถอดรหัส เราจำเป็นต้องเลื่อนป้ายกำกับไปทางขวาทีละป้าย สิ่งนี้จำเป็นเพื่อให้แน่ใจว่าตัวถอดรหัสจะเห็นเฉพาะป้ายความจริงภาคพื้นดินก่อนหน้าเท่านั้น ไม่ใช่ป้ายปัจจุบันหรืออนาคต ซึ่งจะง่ายต่อการจดจำสำหรับแบบจำลอง สิ่งนี้คล้ายกับวิธีการเอาแต่สนใจตนเองมาใช้กับอินพุตในงาน เช่น [การสร้างแบบจำลองภาษาเชิงสาเหตุ](/course/th/chapter7/6)

โชคดีที่ 🤗 Transformers มีตัวเปรียบเทียบ `DataCollatorForSeq2Seq` ที่จะแพดอินพุตและป้ายกำกับแบบไดนามิกให้เรา หากต้องการสร้างตัวอย่าง collator นี้ เราเพียงแค่ต้องจัดเตรียม `tokenizer` และ `model`:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

มาดูกันว่าตัวเปรียบเทียบนี้ผลิตอะไรได้บ้างเมื่อป้อนตัวอย่างชุดเล็กๆ ขั้นแรก เราต้องลบคอลัมน์ที่มีสตริงออก เนื่องจากตัวเปรียบเทียบไม่ทราบวิธีแพดองค์ประกอบเหล่านี้:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

เนื่องจากผู้เปรียบเทียบคาดหวังรายการ `dict`s โดยที่ `dict` แต่ละรายการแสดงถึงตัวอย่างเดียวในชุดข้อมูล เราจึงต้องโต้แย้งข้อมูลให้อยู่ในรูปแบบที่คาดหวังก่อนที่จะส่งต่อไปยังตัวเปรียบเทียบข้อมูล:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

สิ่งสำคัญที่ต้องสังเกตที่นี่คือตัวอย่างแรกยาวกว่าตัวอย่างที่สอง ดังนั้น `input_ids` และ `attention_mask` ของตัวอย่างที่สองจึงได้รับการเสริมไว้ทางด้านขวาด้วยโทเค็น `[PAD]` (ซึ่งมี ID เป็น ` 0`) ในทำนองเดียวกัน เราจะเห็นว่า `labels` ได้รับการเสริมด้วย `-100`s เพื่อให้แน่ใจว่าโทเค็นการเสริมจะถูกละเว้นโดยฟังก์ชันการสูญเสีย และสุดท้าย เราจะเห็น `decoder_input_ids` ใหม่ ซึ่งได้เลื่อนป้ายกำกับไปทางขวาโดยการใส่โทเค็น `[PAD]` ในรายการแรก

{#if fw === 'pt'}

ในที่สุดเราก็มีส่วนผสมทั้งหมดที่จำเป็นในการฝึกแล้ว! ตอนนี้เราเพียงแค่ต้องยกตัวอย่างผู้ฝึกสอนด้วยอาร์กิวเมนต์มาตรฐาน:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

and launch our training run:

```python
trainer.train()
```

ในระหว่างการฝึก คุณควรเห็นว่าการสูญเสียการฝึกลดลงและคะแนน ROUGE จะเพิ่มขึ้นในแต่ละยุค เมื่อการฝึกอบรมเสร็จสิ้น คุณสามารถดูคะแนน ROUGE สุดท้ายได้โดยการเรียกใช้ `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

จากคะแนนเราจะเห็นว่าโมเดลของเรามีประสิทธิภาพเหนือกว่าพื้นฐาน lead-3 ของเราอย่างคล่องแคล่ว เยี่ยมมาก! สิ่งสุดท้ายที่ต้องทำคือดันตุ้มน้ำหนักโมเดลไปที่ฮับ ดังนี้:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

การดำเนินการนี้จะบันทึกจุดตรวจสอบและไฟล์การกำหนดค่าไปที่ `output_dir` ก่อนที่จะอัปโหลดไฟล์ทั้งหมดไปยัง Hub ด้วยการระบุอาร์กิวเมนต์ `tags` เรายังมั่นใจได้ว่าวิดเจ็ตบน Hub จะเป็นอันหนึ่งสำหรับไปป์ไลน์การสรุป แทนที่จะเป็นการสร้างข้อความเริ่มต้นที่เชื่อมโยงกับสถาปัตยกรรม mT5 (สำหรับข้อมูลเพิ่มเติมเกี่ยวกับแท็กโมเดล โปรดดู [🤗 เอกสารประกอบ Hub ](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)) เอาต์พุตจาก `trainer.push_to_hub()` คือ URL ไปยังคอมมิตแฮชของ Git ดังนั้นคุณจึงสามารถดูการเปลี่ยนแปลงที่เกิดขึ้นกับที่เก็บโมเดลได้อย่างง่ายดาย!

เพื่อสรุปส่วนนี้ เรามาดูกันว่าเราจะปรับแต่ง mT5 อย่างละเอียดได้อย่างไรโดยใช้ฟีเจอร์ระดับต่ำที่ 🤗 Accelerate มอบให้

{:else}

เราเกือบจะพร้อมที่จะฝึกแล้ว! เราเพียงแค่ต้องแปลงชุดข้อมูลของเราเป็น `tf.data.Dataset`s โดยใช้ตัวรวบรวมข้อมูลที่เรากำหนดไว้ด้านบน จากนั้นจึง `compile()` และ `fit()` โมเดล, ขั้นตอนแรก datasets:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

ตอนนี้ เรากำหนดไฮเปอร์พารามิเตอร์การฝึกอบรมและคอมไพล์:

```python
from transformers import create_optimizer
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

และสุดท้าย เราก็พอดีกับโมเดล เราใช้ `PushToHubCallback` เพื่อบันทึกโมเดลลงใน Hub หลังจากแต่ละ epoch ซึ่งจะทำให้เราใช้แบบจำลองนั้นเพื่อการอนุมานในภายหลัง:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

เราได้รับค่าที่สูญเสียไประหว่างการฝึก แต่จริงๆ แล้วเราต้องการเห็นหน่วยเมตริก ROUGE ที่เราคำนวณไว้ก่อนหน้านี้ ในการรับหน่วยเมตริกเหล่านั้น เราจะต้องสร้างเอาต์พุตจากโมเดลและแปลงเป็นสตริง มาสร้างรายการป้ายกำกับและการคาดคะเนสำหรับเมตริก ROUGE เพื่อเปรียบเทียบกัน (โปรดทราบว่าหากคุณได้รับข้อผิดพลาดในการนำเข้าสำหรับส่วนนี้ คุณอาจต้องรันคำสั่ง `!pip install tqdm`) นอกจากนี้เรายังจะใช้เคล็ดลับที่เพิ่มประสิทธิภาพได้อย่างมาก ด้วยการคอมไพล์โค้ดการสร้างของเราด้วย [XLA](https://www.tensorflow.org/xla) ซึ่งเป็นคอมไพเลอร์พีชคณิตเชิงเส้นแบบเร่งของ TensorFlow XLA ใช้การปรับให้เหมาะสมต่างๆ กับกราฟการคำนวณของโมเดล และส่งผลให้มีการปรับปรุงความเร็วและการใช้หน่วยความจำอย่างมีนัยสำคัญ ตามที่อธิบายไว้ใน Hugging Face [บล็อก](https://huggingface.co/blog/tf-xla-generate) XLA ทำงานได้ดีที่สุดเมื่อรูปร่างอินพุตของเราไม่แตกต่างกันมากเกินไป เพื่อจัดการสิ่งนี้ เราจะแพดอินพุตของเราให้เป็นทวีคูณของ 128 และสร้างชุดข้อมูลใหม่ด้วยตัวเปรียบเทียบการแพด จากนั้นเราจะใช้ decorator `@tf.function(jit_compile=True)` กับฟังก์ชันการสร้างของเรา ซึ่ง ทำเครื่องหมายฟังก์ชันทั้งหมดสำหรับการคอมไพล์ด้วย XLA

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

เมื่อเรามีรายการป้ายกำกับและสตริงการทำนายแล้ว การคำนวณคะแนน ROUGE ก็เป็นเรื่องง่าย:

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## ปรับแต่ง mT5 อย่างละเอียดด้วย 🤗 Accelerate[[ปรับแต่ง-mT5-อย่างละเอียดด้วย-accelerate]]

การปรับแต่งโมเดลของเราอย่างละเอียดด้วย 🤗 Accelerate นั้นคล้ายคลึงกับตัวอย่างการจัดหมวดหมู่ข้อความที่เราพบใน [บทที่ 3](/course/th/chapter3) มาก ความแตกต่างที่สำคัญคือความจำเป็นในการสร้างข้อมูลสรุปอย่างชัดเจนระหว่างการฝึกอบรมและกำหนดวิธีที่เราคำนวณคะแนน ROUGE (โปรดจำไว้ว่า `Seq2SeqTrainer` ดูแลรุ่นของเรา) มาดูกันว่าเราจะนำข้อกำหนดทั้งสองนี้ไปปฏิบัติได้อย่างไรภายใน 🤗 เร่งความเร็ว!

### เตรียมทุกอย่างเพื่อการฝึกโมเดล[[เตรียมทุกอย่างเพื่อการฝึกโมเดล]]

สิ่งแรกที่เราต้องทำคือสร้าง `DataLoader` สำหรับการแยกแต่ละส่วนของเรา เนื่องจากตัวโหลดข้อมูล PyTorch คาดว่าจะมีเทนเซอร์เป็นชุด เราจึงต้องตั้งค่ารูปแบบเป็น `"torch"` ในชุดข้อมูลของเรา:

```python
tokenized_datasets.set_format("torch")
```

ตอนนี้เรามีชุดข้อมูลที่ประกอบด้วยเทนเซอร์เท่านั้น สิ่งต่อไปที่ต้องทำคือสร้างอินสแตนซ์ `DataCollatorForSeq2Seq` อีกครั้ง สำหรับสิ่งนี้ เราจำเป็นต้องจัดเตรียมโมเดลเวอร์ชันใหม่ ดังนั้นมาโหลดอีกครั้งจากแคชของเรา:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

จากนั้นเราสามารถจำลองตัวรวบรวมข้อมูลและใช้สิ่งนี้เพื่อกำหนดตัวโหลดข้อมูลของเรา:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

สิ่งต่อไปที่ต้องทำคือกำหนดเครื่องมือเพิ่มประสิทธิภาพที่เราต้องการใช้ เช่นเดียวกับตัวอย่างอื่นๆ เราจะใช้ `AdamW` ซึ่งทำงานได้ดีกับปัญหาส่วนใหญ่:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

สุดท้ายนี้ เราป้อนโมเดล เครื่องมือเพิ่มประสิทธิภาพ และตัวโหลดข้อมูลให้กับเมธอด `accelerator.prepare()`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

🚨 หากคุณกำลังฝึกบน TPU คุณจะต้องย้ายโค้ดทั้งหมดข้างต้นไปยังฟังก์ชันการฝึกโดยเฉพาะ ดู [บทที่ 3](/course/th/chapter3) สำหรับรายละเอียดเพิ่มเติม

</Tip>

ตอนนี้เราได้เตรียมวัตถุของเราแล้ว เหลืออีก 3 อย่างที่ต้องทำ:

* กำหนดตารางอัตราการเรียนรู้ (learning rate schedule)
* สร้้างฟังก์ชันเพื่อวิเคราะห์ผลข้อมูลสรุปหลังการประเมินผล
* สร้างพื้นที่เก็บข้อมูลบน Hub ที่เราสามารถผลักดันโมเดลของเราไปได้

สำหรับตารางอัตราการเรียนรู้ เราจะใช้ตารางเชิงเส้นมาตรฐานจากส่วนก่อนหน้า:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

สำหรับการวิเคราะห์หลังการประมวลผล เราจำเป็นต้องมีฟังก์ชันที่แยกข้อมูลสรุปที่สร้างขึ้นออกเป็นประโยคที่คั่นด้วยการขึ้นบรรทัดใหม่ นี่คือรูปแบบที่หน่วยเมตริก ROUGE คาดหวัง และเราสามารถทำได้โดยใช้ข้อมูลโค้ดต่อไปนี้:

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

สิ่งนี้น่าจะดูคุ้นเคยสำหรับคุณ หากคุณจำได้ว่าเรากำหนดฟังก์ชัน `compute_metrics()` ของ `Seq2SeqTrainer` ได้อย่างไร

สุดท้าย เราจำเป็นต้องสร้างที่เก็บแบบจำลองบน Hugging Face Hub สำหรับสิ่งนี้ เราสามารถใช้ไลบรารี 🤗 Hub ที่มีชื่อเหมาะสมได้ เราเพียงแค่ต้องกำหนดชื่อสำหรับพื้นที่เก็บข้อมูลของเรา และไลบรารีก็มีฟังก์ชันยูทิลิตี้เพื่อรวมรหัสพื้นที่เก็บข้อมูลเข้ากับโปรไฟล์ผู้ใช้:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

ตอนนี้เราสามารถใช้ชื่อพื้นที่เก็บข้อมูลนี้เพื่อโคลนเวอร์ชันในเครื่องไปยังไดเร็กทอรีผลลัพธ์ของเราซึ่งจะจัดเก็บสิ่งประดิษฐ์การฝึกอบรม:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

สิ่งนี้จะช่วยให้เราสามารถผลักดันสิ่งประดิษฐ์กลับไปที่ Hub โดยการเรียกเมธอด `repo.push_to_hub()` ในระหว่างการฝึกอบรม! ตอนนี้เรามาสรุปการวิเคราะห์ของเราโดยเขียนลูปการฝึกอบรมออกไป

### ลูปการฝึกอบรม[[ลูปการฝึกอบรม]]

ลูปการฝึกอบรมสำหรับการสรุปค่อนข้างคล้ายกับตัวอย่างอื่นๆของ 🤗 Accelerate ที่เราเคยพบและแบ่งออกเป็นสี่ขั้นตอนหลักคร่าวๆ:

1. ฝึกโมเดลโดยวนซ้ำตัวอย่างทั้งหมดใน `train_dataloader` สำหรับแต่ละ epoch
2. สร้างสรุปโมเดลในตอนท้ายของแต่ละ epoch โดยสร้างโทเค็นก่อน จากนั้นจึงถอดรหัสโทเค็น (และสรุปข้อมูลอ้างอิง) ให้เป็นข้อความ
3. คำนวณคะแนน ROUGE โดยใช้เทคนิคเดียวกับที่เราเห็นก่อนหน้านี้
4. บันทึกจุดตรวจและผลักดันทุกอย่างเข้าสู่ศูนย์กลาง ที่นี่เราอาศัยอาร์กิวเมนต์ `blocking=False` ที่ดีของอ็อบเจ็กต์ `Repository` เพื่อให้เราสามารถผลักดันจุดตรวจสอบต่อยุค _asynchronously_ ได้ สิ่งนี้ช่วยให้เราสามารถฝึกอบรมต่อไปได้โดยไม่ต้องรอการอัปโหลดที่ค่อนข้างช้าที่เกี่ยวข้องกับโมเดลขนาด GB!

ขั้นตอนเหล่านี้สามารถดูได้ในบล็อคโค้ดต่อไปนี้:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

แค่นั้นแหละ! เมื่อคุณรันสิ่งนี้ คุณจะมีโมเดลและผลลัพธ์ที่ค่อนข้างคล้ายกับที่เราได้รับจาก `Trainer`.

{/if}

## การใช้โมเดลที่ปรับแต่งของคุณ[[การใช้โมเดลที่ปรับแต่งของคุณ]]

เมื่อคุณผลักโมเดลไปที่ฮับแล้ว คุณสามารถเล่นกับมันผ่านวิดเจ็ตการอนุมานหรือด้วยอ็อบเจ็กต์ `pipeline` ดังต่อไปนี้:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

เราสามารถป้อนตัวอย่างจากชุดทดสอบ (ซึ่งโมเดลไม่เคยเห็น) ไปยังไปป์ไลน์ของเราเพื่อให้เข้าใจถึงคุณภาพของสรุป ขั้นแรก ลองใช้ฟังก์ชันง่ายๆ เพื่อแสดงบทวิจารณ์ ชื่อ และข้อมูลสรุปที่สร้างขึ้นด้วยกัน:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

ลองมาดูตัวอย่างภาษาอังกฤษตัวอย่างหนึ่งที่เราได้รับ:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn’t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It’s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

นี่ก็ไม่ได้แย่เกินไป! เราจะเห็นได้ว่าแบบจำลองของเราสามารถทำการสรุป _abstractive_ ได้โดยการเพิ่มส่วนของบทวิจารณ์ด้วยคำศัพท์ใหม่ และบางทีแง่มุมที่ยอดเยี่ยมที่สุดของโมเดลของเราก็คือเป็นแบบสองภาษา ดังนั้นเราจึงสามารถสร้างบทสรุปบทวิจารณ์ภาษาสเปนได้:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

สรุปแปลเป็นภาษาอังกฤษว่า "Very easy to read" ซึ่งเราเห็นในกรณีนี้ก็ดึงมาจากรีวิวโดยตรง อย่างไรก็ตาม สิ่งนี้แสดงให้เห็นถึงความอเนกประสงค์ของรุ่น mT5 และทำให้คุณได้สัมผัสประสบการณ์ในการจัดการกับคลังข้อมูลที่พูดได้หลายภาษา!

ต่อไป เราจะหันความสนใจไปที่งานที่ซับซ้อนมากขึ้นเล็กน้อย: การฝึกโมเดลภาษาตั้งแต่เริ่มต้น
