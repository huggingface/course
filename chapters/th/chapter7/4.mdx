<FrameworkSwitchCourse {fw} />

# การแปลความหมาย[[การแปลความหมาย]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
]} />

{/if}

ตอนนี้มาดำดิ่งสู่การแปล นี่เป็นอีกหนึ่ง [งานลำดับต่อลำดับ](/course/chapter1/7) ซึ่งหมายความว่าเป็นปัญหาที่สามารถกำหนดได้ว่าเป็นการไปจากลำดับหนึ่งไปอีกลำดับหนึ่ง ในแง่นั้น ปัญหาค่อนข้างใกล้เคียงกับ [การสรุป](/course/th/chapter7/6) และคุณสามารถปรับสิ่งที่เราจะเห็นที่นี่กับปัญหาตามลำดับอื่นๆ เช่น:

- **การถ่ายโอนรูปแบบ**: การสร้างโมเดลที่ *แปล* ข้อความที่เขียนด้วยสไตล์หนึ่งไปเป็นอีกสไตล์หนึ่ง (เช่น จากเป็นทางการไปเป็นไม่เป็นทางการ หรือจากเช็คสเปียร์จากภาษาอังกฤษเป็นภาษาอังกฤษสมัยใหม่)
- **การตอบคำถามเชิงสร้างสรรค์**: การสร้างแบบจำลองที่สร้างคำตอบสำหรับคำถามตามบริบท

<Youtube id="1JvfrvZgi6c"/>

หากคุณมีคลังข้อความที่ใหญ่เพียงพอในสองภาษา (หรือมากกว่า) คุณสามารถฝึกโมเดลการแปลใหม่ตั้งแต่ต้นได้เหมือนกับที่เราฝึกในหัวข้อ [การสร้างแบบจำลองภาษาเชิงสาเหตุ](/course/th/chapter7/6) อย่างไรก็ตาม การปรับแต่งโมเดลการแปลที่มีอยู่อย่างละเอียดจะเร็วขึ้น ไม่ว่าจะเป็นโมเดลหลายภาษา เช่น mT5 หรือ mBART ที่คุณต้องการปรับแต่งให้เข้ากับคู่ภาษาเฉพาะ หรือแม้แต่โมเดลเฉพาะสำหรับการแปลจากภาษาหนึ่งเป็นอีกภาษาหนึ่ง ที่คุณต้องการปรับแต่งให้เข้ากับคลังข้อมูลเฉพาะของคุณ

ในส่วนนี้ เราจะปรับแต่งโมเดล Marian ที่ได้รับการฝึกล่วงหน้าเพื่อแปลจากภาษาอังกฤษเป็นภาษาฝรั่งเศส (เนื่องจากพนักงาน Hugging Face จำนวนมากพูดทั้งสองภาษาเหล่านั้น) บน [ชุดข้อมูล KDE4](https://huggingface.co/datasets/kde4 ) ซึ่งเป็นชุดข้อมูลของไฟล์ที่แปลแล้วสำหรับ [แอป KDE](https://apps.kde.org/) แบบจำลองที่เราจะใช้ได้รับการฝึกอบรมล่วงหน้าในคลังข้อความภาษาฝรั่งเศสและอังกฤษขนาดใหญ่ที่นำมาจาก [ชุดข้อมูล Opus](https://opus.nlpl.eu/) ซึ่งมีชุดข้อมูล KDE4 จริงๆ แต่ถึงแม้ว่าโมเดลที่ได้รับการฝึกล่วงหน้าที่เราใช้จะได้เห็นข้อมูลนั้นในระหว่างการฝึกล่วงหน้าแล้ว เราก็จะเห็นว่าเราสามารถรับเวอร์ชันที่ดีขึ้นได้หลังจากการปรับแต่งอย่างละเอียด

เมื่อเสร็จแล้วเราจะได้โมเดลที่สามารถทำนายได้ดังนี้:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

เช่นเดียวกับในส่วนก่อนหน้า คุณจะพบโมเดลจริงที่เราจะฝึกและอัปโหลดไปยัง Hub โดยใช้โค้ดด้านล่าง และตรวจสอบการคาดการณ์อีกครั้ง [ที่นี่](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## การเตรียมข้อมูล[[การเตรียมข้อมูล]]

หากต้องการปรับแต่งหรือฝึกโมเดลการแปลตั้งแต่เริ่มต้น เราจำเป็นต้องมีชุดข้อมูลที่เหมาะกับงานนี้ ตามที่กล่าวไว้ก่อนหน้านี้ เราจะใช้ [ชุดข้อมูล KDE4](https://huggingface.co/datasets/kde4) ในส่วนนี้ แต่คุณสามารถปรับโค้ดเพื่อใช้ข้อมูลของคุณเองได้อย่างง่ายดาย ตราบใดที่คุณมีคู่กัน ของประโยคในสองภาษาที่คุณต้องการแปลจากและเป็นไปในนั้น ย้อนกลับไปที่ [บทที่ 5](/course/th/chapter5) หากคุณต้องการคำเตือนเกี่ยวกับวิธีการโหลดข้อมูลที่กำหนดเองใน `Dataset`.

### ชุดข้อมูล KDE4[[ชุดข้อมูล-kde4]]

ตามปกติ เราจะดาวน์โหลดชุดข้อมูลของเราโดยใช้ฟังก์ชัน `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

หากคุณต้องการทำงานกับคู่ภาษาอื่น คุณสามารถระบุภาษาเหล่านั้นด้วยรหัสของพวกเขาได้ ชุดข้อมูลนี้มีภาษาให้เลือกทั้งหมด 92 ภาษา คุณสามารถดูทั้งหมดได้โดยขยายแท็กภาษาใน [การ์ดชุดข้อมูล](https://huggingface.co/datasets/kde4)

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

มาดูชุดข้อมูลกันดีกว่า:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

เรามีประโยค 210,173 คู่ แต่ในการแยกประโยคเดียว ดังนั้น เราจะต้องสร้างชุดการตรวจสอบของเราเอง ดังที่เราเห็นใน [บทที่ 5](/course/th/chapter5) `Dataset` มีเมธอด `train_test_split()` ที่สามารถช่วยเราได้ เราจะจัดเตรียมเมล็ดพันธุ์สำหรับการทำซ้ำ:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

เราสามารถเปลี่ยนชื่อคีย์ `"test"` เป็น `"validation"` ได้ดังนี้:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ตอนนี้เรามาดูองค์ประกอบหนึ่งของชุดข้อมูลกัน:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par défaut, développer les fils de discussion'}
```

เราได้รับพจนานุกรมที่มีสองประโยคในคู่ภาษาที่เราขอ ความพิเศษประการหนึ่งของชุดข้อมูลนี้ซึ่งเต็มไปด้วยคำศัพท์ทางเทคนิคด้านวิทยาการคอมพิวเตอร์ก็คือคำศัพท์ทั้งหมดได้รับการแปลเป็นภาษาฝรั่งเศสทั้งหมด อย่างไรก็ตาม วิศวกรชาวฝรั่งเศสจะทิ้งคำเฉพาะด้านวิทยาการคอมพิวเตอร์ส่วนใหญ่เป็นภาษาอังกฤษเมื่อพวกเขาพูด ตัวอย่างเช่น คำว่า "threads" อาจปรากฏในประโยคภาษาฝรั่งเศส โดยเฉพาะอย่างยิ่งในการสนทนาทางเทคนิค แต่ในชุดข้อมูลนี้ มันถูกแปลเป็น "fils de discussion" ที่ถูกต้องมากขึ้น โมเดลที่ฝึกไว้ล่วงหน้าที่เราใช้ ซึ่งได้รับการฝึกฝนกับคลังประโยคภาษาฝรั่งเศสและอังกฤษที่มีขนาดใหญ่กว่า เลือกใช้ตัวเลือกที่ง่ายกว่าในการคงคำไว้ดังนี้:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par défaut pour les threads élargis'}]
```

อีกตัวอย่างหนึ่งของพฤติกรรมนี้สามารถเห็นได้จากคำว่า "plugin" ซึ่งไม่ใช่คำภาษาฝรั่งเศสอย่างเป็นทางการ แต่เจ้าของภาษาส่วนใหญ่จะเข้าใจและไม่สนใจที่จะแปล
ในชุดข้อมูล KDE4 คำนี้ได้รับการแปลเป็นภาษาฝรั่งเศสเป็นภาษา "module d'extension" ที่เป็นทางการมากขึ้น:

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

อย่างไรก็ตาม โมเดลที่ได้รับการฝึกล่วงหน้าของเรายังคงยึดตามคำภาษาอังกฤษที่กระชับและคุ้นเคย:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

น่าสนใจที่จะดูว่าโมเดลที่ได้รับการปรับแต่งของเรานั้นคำนึงถึงความเฉพาะเจาะจงของชุดข้อมูลหรือไม่ (เตือนไว้ก่อน: มันจะเป็นเช่นนั้น)

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

✏️ **ตาคุณ!** อีกคำภาษาอังกฤษที่มักใช้ในภาษาฝรั่งเศสคือ "email" ค้นหาตัวอย่างแรกในชุดข้อมูลการฝึกอบรมที่ใช้คำนี้ มันแปลยังไงบ้าง? โมเดลที่ผ่านการฝึกอบรมจะแปลประโยคภาษาอังกฤษเดียวกันได้อย่างไร

</Tip>

### การประมวลผลข้อมูล[[การประมวลผลข้อมูล]]

<Youtube id="XAR8jnZZuUs"/>

ตอนนี้คุณควรรู้การเจาะลึกแล้ว: ข้อความทั้งหมดจำเป็นต้องแปลงเป็นชุดรหัสโทเค็นเพื่อให้แบบจำลองเข้าใจได้ สำหรับงานนี้ เราจะต้องสร้างโทเค็นทั้งอินพุตและเป้าหมาย ภารกิจแรกของเราคือสร้างวัตถุ `tokenizer` ของเรา ตามที่ระบุไว้ก่อนหน้านี้ เราจะใช้โมเดลฝึกภาษาอังกฤษแบบ Marian เป็นภาษาฝรั่งเศส หากคุณกำลังลองใช้โค้ดนี้กับคู่ภาษาอื่น ตรวจสอบให้แน่ใจว่าได้ปรับจุดตรวจสอบโมเดลแล้ว องค์กร [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) มีโมเดลมากกว่าพันรายการในหลายภาษา

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

คุณยังสามารถแทนที่ `model_checkpoint` ด้วยโมเดลอื่นๆ ที่คุณต้องการจาก [Hub](https://huggingface.co/models) หรือโฟลเดอร์ในเครื่องที่คุณได้บันทึกโมเดลที่ฝึกไว้ล่วงหน้าและ tokenizer

<Tip>

💡 หากคุณใช้โทเค็นไนเซอร์หลายภาษา เช่น mBART, mBART-50 หรือ M2M100 คุณจะต้องตั้งค่ารหัสภาษาของอินพุตและเป้าหมายของคุณในโทเค็นโดยการตั้งค่า `tokenizer.src_lang` และ `tokenizer.tgt_lang` ทางด้านขวา ค่านิยม

</Tip>

การจัดเตรียมข้อมูลของเราค่อนข้างตรงไปตรงมา มีเพียงสิ่งเดียวที่ต้องจำ คุณต้องแน่ใจว่า tokenizer ประมวลผลเป้าหมายในภาษาเอาต์พุต (ในที่นี้คือภาษาฝรั่งเศส) คุณสามารถทำได้โดยส่งเป้าหมายไปยังอาร์กิวเมนต์ `text_targets` ของเมธอด `__call__` ของ tokenizer

หากต้องการดูวิธีการทำงาน เราจะประมวลผลตัวอย่างของแต่ละภาษาในชุดการฝึกอบรมหนึ่งตัวอย่าง:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

ดังที่เราเห็น ผลลัพธ์มี ID อินพุตที่เกี่ยวข้องกับประโยคภาษาอังกฤษ ในขณะที่ ID ที่เกี่ยวข้องกับภาษาฝรั่งเศสจะถูกเก็บไว้ในฟิลด์ `labels` หากคุณลืมระบุว่าคุณกำลังโทเค็นป้ายกำกับ ป้ายเหล่านั้นจะถูกโทเค็นโดยอินพุตโทเค็นไนเซอร์ ซึ่งในกรณีของโมเดล Marian จะทำงานได้ไม่ดีเลย:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']
['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']
```

ดังที่เราเห็น การใช้ tokenizer ภาษาอังกฤษเพื่อประมวลผลประโยคภาษาฝรั่งเศสล่วงหน้าส่งผลให้มี token มากขึ้น เนื่องจาก tokenizer ไม่รู้จักคำภาษาฝรั่งเศสใดๆ (ยกเว้นคำที่ปรากฏในภาษาอังกฤษ เช่น "discussion")

เนื่องจาก `inputs` เป็นพจนานุกรมที่มีคีย์ปกติของเรา (ID อินพุต มาสก์ความสนใจ ฯลฯ) ขั้นตอนสุดท้ายคือการกำหนดฟังก์ชันการประมวลผลล่วงหน้าที่เราจะนำไปใช้กับชุดข้อมูล:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

โปรดทราบว่าเราตั้งค่าความยาวสูงสุดเท่ากันสำหรับอินพุตและเอาต์พุตของเรา เนื่องจากข้อความที่เรากำลังพูดถึงดูค่อนข้างสั้น เราจึงใช้ 128

<Tip>

💡 หากคุณกำลังใช้โมเดล T5 (โดยเฉพาะอย่างยิ่ง หนึ่งในจุดตรวจสอบ `t5-xxx`) โมเดลจะคาดหวังว่าอินพุตข้อความจะมีคำนำหน้าระบุงานที่ทำอยู่ เช่น `translate: English to French:`

</Tip>

<Tip warning={true}>

⚠️ เราไม่ใส่ใจกับหน้ากากความสนใจของเป้าหมาย เนื่องจากโมเดลไม่ได้คาดหวังไว้ แต่ควรตั้งค่าป้ายกำกับที่สอดคล้องกับโทเค็นการเติมเป็น `-100` แทน เพื่อที่ป้ายเหล่านั้นจะถูกละเว้นในการคำนวณการสูญเสีย ผู้รวบรวมข้อมูลของเราจะดำเนินการนี้ในภายหลัง เนื่องจากเราใช้การเติมแบบไดนามิก แต่หากคุณใช้การเติมภายในที่นี่ คุณควรปรับฟังก์ชันการประมวลผลล่วงหน้าเพื่อตั้งค่าป้ายกำกับทั้งหมดที่สอดคล้องกับโทเค็นการเติมเป็น `-100`

</Tip>

ตอนนี้เราสามารถใช้การประมวลผลล่วงหน้านั้นกับการแยกชุดข้อมูลทั้งหมดของเราได้ในคราวเดียว:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

เมื่อข้อมูลได้รับการประมวลผลล่วงหน้าแล้ว เราก็พร้อมที่จะปรับแต่งโมเดลที่ฝึกไว้ล่วงหน้าของเราแล้ว!

{#if fw === 'pt'}

## การปรับแต่งโมเดลอย่างละเอียดด้วย `Trainer` API[[การปรับแต่งโมเดลอย่างละเอียดด้วย-trainer-api]]

โค้ดจริงที่ใช้ `Trainer` จะเหมือนเดิม โดยมีการเปลี่ยนแปลงเพียงเล็กน้อย: เราใช้ [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) ที่นี่ ซึ่งเป็นคลาสย่อยของ `Trainer` ที่จะช่วยให้เราจัดการกับการประเมินได้อย่างเหมาะสม โดยใช้เมธอด `generate()` เพื่อทำนายเอาต์พุตจากอินพุต เราจะเจาะลึกรายละเอียดมากขึ้นเมื่อเราพูดถึงการคำนวณหน่วยเมตริก

ก่อนอื่น เราต้องการโมเดลจริงเพื่อปรับแต่งอย่างละเอียด เราจะใช้ `AutoModel` API ตามปกติ:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## การปรับแต่งโมเดลอย่างละเอียดด้วย Keras[[การปรับแต่งโมเดลอย่างละเอียดด้วย-keras]]

ก่อนอื่น เราต้องการโมเดลจริงเพื่อปรับแต่งอย่างละเอียด เราจะใช้ `AutoModel` API ตามปกติ:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

💡 จุด checkpoint `Helsinki-NLP/opus-mt-en-fr` มีเพียงน้ำหนักของ PyTorch ดังนั้น
คุณจะได้รับข้อผิดพลาดหากคุณพยายามโหลดโมเดลโดยไม่ใช้
อาร์กิวเมนต์ `from_pt=True` ในเมธอด `from_pretrained()` เมื่อคุณระบุ
`from_pt=True` ไลบรารีจะดาวน์โหลดและแปลงน้ำหนัก PyTorch สำหรับคุณโดยอัตโนมัติ อย่างที่คุณเห็น มันง่ายมากที่จะสลับไปมาระหว่าง
เฟรมเวิร์กใน 🤗 Transformers!

</Tip>

{/if}

ปรดทราบว่าครั้งนี้ เรากำลังใช้แบบจำลองที่ได้รับการฝึกในงานแปลและสามารถใช้งานได้จริงแล้ว ดังนั้นจึงไม่มีคำเตือนเกี่ยวกับน้ำหนักที่หายไปหรือน้ำหนักที่เริ่มต้นใหม่

### การรวบรวมข้อมูล[[การรวบรวมข้อมูล]]

เราจะต้องมีตัวเปรียบเทียบข้อมูลเพื่อจัดการกับช่องว่างภายในสำหรับการจัดชุดแบบไดนามิก เราไม่สามารถใช้ `DataCollatorWithPadding` เหมือนใน [บทที่ 3](/course/th/chapter3) ในกรณีนี้ไม่ได้ เนื่องจากนั่นเป็นเพียงการเสริมอินพุตเท่านั้น (ID อินพุต, มาสก์ความสนใจ และ ID ประเภทโทเค็น) ฉลากของเราควรได้รับการบุให้มีความยาวสูงสุดที่พบในฉลาก และตามที่กล่าวไว้ก่อนหน้านี้ ค่าการเสริมที่ใช้ในการเสริมป้ายกำกับควรเป็น `-100` และไม่ใช่โทเค็นการเสริมของโทเค็น เพื่อให้แน่ใจว่าค่าที่เสริมไว้เหล่านั้นจะถูกละเว้นในการคำนวณการสูญเสีย

ทั้งหมดนี้ทำโดย [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) เช่นเดียวกับ `DataCollatorWithPadding` มันต้องใช้ `tokenizer` ที่ใช้ในการประมวลผลอินพุตล่วงหน้า แต่ก็ใช้ `model` ด้วย เนื่องจากตัวรวบรวมข้อมูลนี้ยังต้องรับผิดชอบในการเตรียม ID อินพุตตัวถอดรหัส ซึ่งเป็นเวอร์ชันที่ถูกเลื่อนของป้ายกำกับด้วยโทเค็นพิเศษที่จุดเริ่มต้น เนื่องจากการเปลี่ยนแปลงนี้ทำแตกต่างกันเล็กน้อยสำหรับสถาปัตยกรรมที่แตกต่างกัน `DataCollatorForSeq2Seq` จำเป็นต้องทราบอ็อบเจ็กต์ `model`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

เพื่อทดสอบสิ่งนี้กับตัวอย่างบางส่วน เราเพียงเรียกมันในรายการตัวอย่างจากชุดการฝึกโทเค็นของเรา:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

เราสามารถตรวจสอบได้ว่าฉลากของเราได้รับการเสริมความยาวสูงสุดของชุดโดยใช้ `-100`:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

และเรายังสามารถดู ID อินพุตของตัวถอดรหัสเพื่อดูว่าเป็นเวอร์ชันที่เลื่อนของป้ายกำกับ:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ต่อไปนี้เป็นป้ายกำกับสำหรับองค์ประกอบที่หนึ่งและที่สองในชุดข้อมูลของเรา:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

เราจะส่ง `data_collator` นี้ไปยัง `Seq2SeqTrainer` ต่อไปเรามาดูตัวชี้วัดกัน

{:else}

ตอนนี้เราสามารถใช้ `data_collator` นี้เพื่อแปลงชุดข้อมูลแต่ละชุดของเราเป็น `tf.data.Dataset` พร้อมสำหรับการฝึกอบรม:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### เมตริก[[เมตริก]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

คุณลักษณะที่ `Seq2SeqTrainer` เพิ่มให้กับ `Trainer` ระดับซูเปอร์คลาสคือความสามารถในการใช้เมธอด `generate()` ในระหว่างการประเมินหรือการทำนาย ในระหว่างการฝึก โมเดลจะใช้ `decoder_input_ids` พร้อมกับมาส์กความสนใจเพื่อให้แน่ใจว่าจะไม่ใช้โทเค็นหลังจากโทเค็นที่พยายามคาดการณ์ เพื่อเร่งความเร็วในการฝึก ในระหว่างการอนุมาน เราจะไม่สามารถใช้สิ่งเหล่านั้นได้เนื่องจากเราไม่มีป้ายกำกับ ดังนั้นจึงเป็นความคิดที่ดีที่จะประเมินแบบจำลองของเราด้วยการตั้งค่าเดียวกัน

ดังที่เราเห็นใน [บทที่ 1](/course/th/chapter1/6) ตัวถอดรหัสทำการอนุมานโดยการทำนายโทเค็นทีละรายการ ซึ่งเป็นสิ่งที่นำไปใช้เบื้องหลังใน 🤗 Transformers ด้วยเมธอด `generate()` `Seq2SeqTrainer` จะให้เราใช้วิธีการนั้นในการประเมินหากเราตั้งค่า `predict_with_generate=True`

{/if}

ตัวชี้วัดแบบดั้งเดิมที่ใช้สำหรับการแปลคือ [คะแนน BLEU](https://en.wikipedia.org/wiki/BLEU) ซึ่งนำมาใช้ใน [บทความปี 2002](https://aclanthology.org/P02-1040.pdf) โดย Kishore Papineni และคณะ คะแนน BLEU จะประเมินว่าคำแปลใกล้เคียงกับป้ายกำกับเพียงใด ไม่ได้วัดความชัดเจนหรือความถูกต้องทางไวยากรณ์ของผลลัพธ์ที่สร้างของแบบจำลอง แต่ใช้กฎทางสถิติเพื่อให้แน่ใจว่าคำทั้งหมดในผลลัพธ์ที่สร้างขึ้นจะปรากฏในเป้าหมายด้วย นอกจากนี้ ยังมีกฎที่ลงโทษการใช้คำเดียวกันซ้ำๆ หากไม่ซ้ำกันในเป้าหมายด้วย (เพื่อหลีกเลี่ยงรูปแบบที่ส่งออกประโยค เช่น `"the the the the"`) และประโยคเอาต์พุตที่สั้นกว่าที่อยู่ใน เป้าหมาย (เพื่อหลีกเลี่ยงโมเดลที่ส่งออกประโยคเช่น `"the"`)

จุดอ่อนประการหนึ่งของ BLEU คือคาดว่าข้อความจะถูกโทเค็นไนซ์แล้ว ซึ่งทำให้ยากต่อการเปรียบเทียบคะแนนระหว่างโมเดลที่ใช้โทเค็นไนเซอร์ที่แตกต่างกัน ดังนั้น เมตริกที่ใช้บ่อยที่สุดสำหรับการเปรียบเทียบโมเดลการแปลในปัจจุบันคือ [SacreBLEU](https://github.com/mjpost/sacrebleu) ซึ่งจัดการกับจุดอ่อนนี้ (และอื่นๆ) ด้วยการกำหนดขั้นตอนโทเค็นให้เป็นมาตรฐาน หากต้องการใช้ตัวชี้วัดนี้ เราต้องติดตั้งไลบรารี SacreBLEU ก่อน:

```py
!pip install sacrebleu
```

จากนั้นเราสามารถโหลดมันผ่าน `evaluate.load()` เหมือนที่เราทำใน [บทที่ 3](/course/th/chapter3):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

เมตริกนี้จะใช้ข้อความเป็นอินพุตและเป้าหมาย ได้รับการออกแบบมาเพื่อยอมรับเป้าหมายที่ยอมรับได้หลายเป้าหมาย เนื่องจากมักจะมีการแปลประโยคเดียวกันที่ยอมรับได้หลายรายการ ชุดข้อมูลที่เราใช้จะมีเพียงเป้าหมายเดียว แต่ไม่ใช่เรื่องแปลกใน NLP ที่จะค้นหาชุดข้อมูลที่มีหลายประโยคเป็นป้ายกำกับ ดังนั้นการคาดคะเนควรเป็นรายการประโยค แต่การอ้างอิงควรเป็นรายการประโยค

มาทดสอบตัวอย่างกัน:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

ซึ่งได้คะแนน BLEU ที่ 46.75 ซึ่งค่อนข้างดี สำหรับการอ้างอิง โมเดล Transformer ดั้งเดิมในเอกสาร ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) ได้รับ คะแนน BLEU 41.8 ในงานแปลที่คล้ายกันระหว่างภาษาอังกฤษและภาษาฝรั่งเศส! (สำหรับข้อมูลเพิ่มเติมเกี่ยวกับตัวชี้วัดแต่ละรายการ เช่น `counts` และ `bp` โปรดดูที่ [พื้นที่เก็บข้อมูล SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74 ).) ในทางกลับกัน ถ้าเราลองใช้การทำนายที่ไม่ดีสองประเภท (การซ้ำหลายครั้งหรือสั้นเกินไป) ซึ่งมักจะมาจากโมเดลการแปล เราจะได้คะแนน BLEU ที่ค่อนข้างแย่:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

คะแนนสามารถอยู่ระหว่าง 0 ถึง 100 และยิ่งสูงก็ยิ่งดี

{#if fw === 'tf'}

หากต้องการรับจากเอาต์พุตของโมเดลเป็นข้อความที่หน่วยวัดสามารถใช้ได้ เราจะใช้เมธอด `tokenizer.batch_decode()` เราแค่ต้องทำความสะอาด `-100` ทั้งหมดในฉลาก โทเค็นไนเซอร์จะทำเช่นเดียวกันกับโทเค็นการเติมโดยอัตโนมัติ เรามากำหนดฟังก์ชันที่ใช้โมเดลและชุดข้อมูลของเราและคำนวณเมตริกจากโมเดลนั้น นอกจากนี้เรายังจะใช้เคล็ดลับที่เพิ่มประสิทธิภาพได้อย่างมาก ด้วยการคอมไพล์โค้ดการสร้างของเราด้วย [XLA](https://www.tensorflow.org/xla) ซึ่งเป็นคอมไพเลอร์พีชคณิตเชิงเส้นแบบเร่งของ TensorFlow XLA ใช้การปรับให้เหมาะสมต่างๆ กับกราฟการคำนวณของโมเดล และส่งผลให้มีการปรับปรุงความเร็วและการใช้หน่วยความจำอย่างมีนัยสำคัญ ตามที่อธิบายไว้ใน Hugging Face [บล็อก](https://huggingface.co/blog/tf-xla-generate) XLA ทำงานได้ดีที่สุดเมื่อรูปร่างอินพุตของเราไม่แตกต่างกันมากเกินไป เพื่อจัดการสิ่งนี้ เราจะแพดอินพุตของเราให้เป็นทวีคูณของ 128 และสร้างชุดข้อมูลใหม่ด้วยตัวเปรียบเทียบการแพด จากนั้นเราจะใช้มัณฑนากร `@tf.function(jit_compile=True)` กับฟังก์ชันการสร้างของเรา ซึ่ง ทำเครื่องหมายฟังก์ชันทั้งหมดสำหรับการคอมไพล์ด้วย XLA

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []

    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

หากต้องการรับจากเอาต์พุตของโมเดลเป็นข้อความที่หน่วยวัดสามารถใช้ได้ เราจะใช้เมธอด `tokenizer.batch_decode()` เราเพียงแค่ต้องล้าง `-100` ทั้งหมดในป้ายกำกับ (โทเค็นจะทำเช่นเดียวกันกับโทเค็นการเติมโดยอัตโนมัติ):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # In case the model returns more than the prediction logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100s in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

เมื่อเสร็จแล้ว เราก็พร้อมที่จะปรับแต่งโมเดลของเรา!


### การปรับแต่งโมเดลอย่างละเอียด[[การปรับแต่งโมเดลอย่างละเอียด]]

ขั้นตอนแรกคือการเข้าสู่ระบบ Hugging Face เพื่อให้คุณสามารถอัปโหลดผลลัพธ์ของคุณไปยัง Model Hub มีฟังก์ชันอำนวยความสะดวกที่จะช่วยคุณในเรื่องนี้ในโน้ตบุ๊ก:

```python
from huggingface_hub import notebook_login

notebook_login()
```

นี่จะแสดงวิดเจ็ตที่คุณสามารถป้อนข้อมูลรับรองการเข้าสู่ระบบ Hugging Face ของคุณได้

หากคุณไม่ได้ทำงานในโน้ตบุ๊ก เพียงพิมพ์บรรทัดต่อไปนี้ในเทอร์มินัลของคุณ:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

ก่อนที่เราจะเริ่มต้น มาดูกันว่าเราได้รับผลลัพธ์ประเภทใดจากโมเดลของเราโดยไม่มีการฝึกอบรมใดๆ:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

เมื่อเสร็จแล้ว เราก็เตรียมทุกอย่างที่จำเป็นเพื่อคอมไพล์และฝึกโมเดลของเราได้ โปรดสังเกตการใช้ `tf.keras.mixed_precision.set_global_policy("mixed_float16")` -- ซึ่งจะเป็นการบอกให้ Keras ฝึกฝนการใช้ float16 ซึ่งสามารถเร่งความเร็วได้อย่างมากบน GPU ที่รองรับ (Nvidia 20xx/V100 หรือใหม่กว่า)

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ต่อไป เราจะกำหนด `PushToHubCallback` เพื่ออัปโหลดโมเดลของเราไปยัง Hub ในระหว่างการฝึก ดังที่เราเห็นใน [ส่วนที่ 2]((/course/th/chapter7/2)) จากนั้นเราก็ปรับโมเดลให้เข้ากับ callback นั้น:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

โปรดทราบว่าคุณสามารถระบุชื่อของพื้นที่เก็บข้อมูลที่คุณต้องการพุชไปได้ด้วยอาร์กิวเมนต์ `hub_model_id` (โดยเฉพาะ คุณจะต้องใช้อาร์กิวเมนต์นี้เพื่อพุชไปยังองค์กร) ตัวอย่างเช่น เมื่อเราผลักโมเดลไปที่ [`huggingface-course` Organization](https://huggingface.co/huggingface-course) เราได้เพิ่ม `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` ถึง `Seq2SeqTrainingArguments` ตามค่าเริ่มต้น พื้นที่เก็บข้อมูลที่ใช้จะอยู่ในเนมสเปซของคุณและตั้งชื่อตามไดเร็กทอรีเอาต์พุตที่คุณตั้งค่า ดังนั้นที่นี่จะเป็น `"sgugger/marian-finetuned-kde4-en-to-fr"` (ซึ่งเป็นโมเดลที่เราเชื่อมโยงด้วย ในตอนต้นของส่วนนี้)

<Tip>

💡 หากไดเร็กทอรีเอาต์พุตที่คุณใช้มีอยู่แล้ว จะต้องเป็นโคลนในเครื่องของที่เก็บที่คุณต้องการพุชไป หากไม่เป็นเช่นนั้น คุณจะได้รับข้อผิดพลาดเมื่อเรียก `model.fit()` และจะต้องตั้งชื่อใหม่

</Tip>

สุดท้ายนี้ เรามาดูกันว่าเมตริกของเราจะเป็นอย่างไรเมื่อการฝึกอบรมเสร็จสิ้นแล้ว:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

ในขั้นตอนนี้ คุณสามารถใช้วิดเจ็ตการอนุมานบน Model Hub เพื่อทดสอบโมเดลของคุณและแบ่งปันกับเพื่อนๆ ของคุณได้ คุณปรับแต่งโมเดลในงานแปลได้สำเร็จ ขอแสดงความยินดีด้วย!

{:else}

เมื่อเสร็จแล้ว เราก็สามารถกำหนด `Seq2SeqTrainingArguments` ของเราได้ เช่นเดียวกับ `Trainer` เราใช้คลาสย่อยของ `TrainingArguments` ที่มีตัวแปลเพิ่มเติมสองสามตัว:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

นอกเหนือจากไฮเปอร์พารามิเตอร์ตามปกติ (เช่น learning rate, number of epochs, batch size, and some weight decay), ต่อไปนี้คือการเปลี่ยนแปลงเล็กๆ น้อยๆ เมื่อเทียบกับสิ่งที่เราเห็นในส่วนที่แล้ว:

- เราไม่ได้กำหนดการประเมินเป็นประจำ เนื่องจากการประเมินจะใช้เวลาระยะหนึ่ง เราจะประเมินแบบจำลองของเราหนึ่งครั้งก่อนการฝึกและหลังการฝึก
- เราตั้งค่า `fp16=True` ซึ่งจะช่วยเร่งความเร็วการฝึกอบรมบน GPU สมัยใหม่
- เราตั้งค่า `predict_with_generate=True` ตามที่กล่าวไว้ข้างต้น
- เราใช้ `push_to_hub=True` เพื่ออัปโหลดโมเดลไปยัง Hub เมื่อสิ้นสุดแต่ละ epoch

โปรดทราบว่าคุณสามารถระบุชื่อเต็มของพื้นที่เก็บข้อมูลที่คุณต้องการพุชไปได้ด้วยอาร์กิวเมนต์ `hub_model_id` (โดยเฉพาะ คุณจะต้องใช้อาร์กิวเมนต์นี้เพื่อพุชไปยังองค์กร) ตัวอย่างเช่น เมื่อเราผลักโมเดลไปที่ [`huggingface-course` Organization](https://huggingface.co/huggingface-course) เราได้เพิ่ม `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` ถึง `Seq2SeqTrainingArguments` ตามค่าเริ่มต้น พื้นที่เก็บข้อมูลที่ใช้จะอยู่ในเนมสเปซของคุณและตั้งชื่อตามไดเร็กทอรีเอาต์พุตที่คุณตั้งค่า ดังนั้นในกรณีของเราจะเป็น `"sgugger/marian-finetuned-kde4-en-to-fr"` (ซึ่งเป็นโมเดลที่เรา เชื่อมโยงกับตอนต้นของส่วนนี้)

<Tip>

💡 หากไดเร็กทอรีเอาต์พุตที่คุณใช้มีอยู่แล้ว จะต้องเป็นโคลนในเครื่องของที่เก็บที่คุณต้องการพุชไป หากไม่เป็นเช่นนั้น คุณจะได้รับข้อผิดพลาดเมื่อกำหนด `Seq2SeqTrainer` ของคุณและจะต้องตั้งชื่อใหม่

</Tip>


ในที่สุด เราก็ส่งทุกอย่างไปที่ `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ก่อนการฝึก เราจะดูคะแนนที่โมเดลของเราได้รับก่อน เพื่อตรวจสอบอีกครั้งว่าเราไม่ได้ทำให้สิ่งต่าง ๆ แย่ลงด้วยการปรับแต่งของเรา คำสั่งนี้จะใช้เวลาสักครู่ ดังนั้นคุณจึงสามารถดื่มกาแฟในขณะที่ดำเนินการได้:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

คะแนน BLEU ที่ 39 ก็ไม่ได้แย่นัก ซึ่งสะท้อนให้เห็นว่าแบบจำลองของเราแปลประโยคภาษาอังกฤษเป็นภาษาฝรั่งเศสได้ดีอยู่แล้ว

ต่อไปคือการฝึกอบรมซึ่งจะใช้เวลาสักหน่อย:

```python
trainer.train()
```

โปรดทราบว่าในขณะที่การฝึกเกิดขึ้น แต่ละครั้งที่มีการบันทึกโมเดล (ที่นี่ ทุก epoch) โมเดลจะถูกอัปโหลดไปยัง Hub ในเบื้องหลัง ด้วยวิธีนี้ คุณจะสามารถกลับมาฝึกต่อในเครื่องอื่นได้หากจำเป็น

เมื่อการฝึกอบรมเสร็จสิ้น เราจะประเมินแบบจำลองของเราอีกครั้ง หวังว่าเราจะได้เห็นการปรับปรุงในคะแนน BLEU

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

นั่นเป็นการปรับปรุงเกือบ 14 แต้มซึ่งดีมาก

สุดท้ายนี้ เราใช้เมธอด `push_to_hub()` เพื่อให้แน่ใจว่าเราจะอัปโหลดโมเดลเวอร์ชันล่าสุด `Trainer` ยังร่างการ์ดโมเดลพร้อมผลการประเมินทั้งหมดแล้วอัปโหลด การ์ดโมเดลนี้มีข้อมูลเมตาที่ช่วยให้ Model Hub เลือกวิดเจ็ตสำหรับการสาธิตการอนุมาน โดยปกติแล้ว ไม่จำเป็นต้องพูดอะไร เนื่องจากสามารถอนุมานวิดเจ็ตที่ถูกต้องจากคลาสโมเดลได้ แต่ในกรณีนี้ สามารถใช้คลาสโมเดลเดียวกันสำหรับปัญหาลำดับต่อลำดับทุกประเภท ดังนั้นเราจึงระบุว่าเป็นการแปล แบบอย่าง:

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

คำสั่งนี้จะส่งคืน URL ของการคอมมิตที่เพิ่งทำไป หากคุณต้องการตรวจสอบ:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

ในขั้นตอนนี้ คุณสามารถใช้วิดเจ็ตการอนุมานบน Model Hub เพื่อทดสอบโมเดลของคุณและแบ่งปันกับเพื่อนๆ ของคุณได้ คุณปรับแต่งโมเดลในงานแปลได้สำเร็จ ขอแสดงความยินดีด้วย!

หากคุณต้องการเจาะลึกลงไปในลูปการฝึกซ้อมมากขึ้น ตอนนี้เราจะแสดงวิธีทำสิ่งเดียวกันโดยใช้ 🤗 Accelerate

{/if}

{#if fw === 'pt'}

## การวนลูปฝึกอบรมแบบกำหนดเอง[[การวนลูปฝึกอบรมแบบกำหนดเอง]]

ตอนนี้เรามาดูวงจรการฝึกซ้อมทั้งหมดกัน เพื่อให้คุณปรับแต่งส่วนต่างๆ ที่ต้องการได้อย่างง่ายดาย มันจะดูเหมือนกับสิ่งที่เราทำใน [ส่วนที 2](/course/th/chapter7/2) และ [บทที่ 3](/course/th/chapter3/4) มาก

### เตรียมทุกอย่างเพื่อการฝึก[[เตรียมทุกอย่างเพื่อการฝึก]]

คุณเคยเห็นทั้งหมดนี้มาสองสามครั้งแล้ว ดังนั้นเราจะอธิบายโค้ดอย่างรวดเร็ว ก่อนอื่น เราจะสร้าง `DataLoader`s จากชุดข้อมูลของเรา หลังจากตั้งค่าชุดข้อมูลเป็นรูปแบบ `"torch"` แล้ว เราก็จะได้ PyTorch tensors:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

ต่อไป เราจะสร้างโมเดลของเราขึ้นมาใหม่ เพื่อให้แน่ใจว่าเราจะไม่ทำการปรับแต่งแบบละเอียดจากเมื่อก่อนอีกต่อไป แต่เริ่มต้นจากโมเดลที่ได้รับการฝึกไว้ล่วงหน้าอีกครั้ง:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

จากนั้นเราจะต้องมีเครื่องมือเพิ่มประสิทธิภาพ:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

เมื่อเรามีอ็อบเจ็กต์ทั้งหมดแล้ว เราก็สามารถส่งมันไปที่เมธอด `accelerator.prepare()` ได้ โปรดทราบว่าหากคุณต้องการฝึก TPU ในสมุดบันทึก Colab คุณจะต้องย้ายโค้ดทั้งหมดนี้ไปยังฟังก์ชันการฝึก และไม่ควรเรียกใช้เซลล์ใดๆ ที่สร้างอินสแตนซ์ `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ตอนนี้เราได้ส่ง `train_dataloader` ไปที่ `accelerator.prepare()` แล้ว เราสามารถใช้ความยาวของมันเพื่อคำนวณจำนวนขั้นตอนการฝึกได้ โปรดจำไว้ว่าเราควรทำเช่นนี้เสมอหลังจากเตรียม dataloader เนื่องจากวิธีการดังกล่าวจะเปลี่ยนความยาวของ `DataLoader` เราใช้กำหนดการเชิงเส้นแบบคลาสสิกจากอัตราการเรียนรู้ถึง 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

สุดท้ายนี้ ในการผลักดันโมเดลของเราไปที่ Hub เราจะต้องสร้างออบเจ็กต์ `Repository` ในโฟลเดอร์ที่ใช้งานได้ ขั้นแรกให้เข้าสู่ระบบ Hugging Face Hub หากคุณยังไม่ได้เข้าสู่ระบบ เราจะกำหนดชื่อที่เก็บจาก ID โมเดลที่เราต้องการให้กับโมเดลของเรา (อย่าลังเลที่จะแทนที่ `repo_name` ด้วยตัวเลือกของคุณเอง เพียงต้องมีชื่อผู้ใช้ของคุณ ซึ่งเป็นสิ่งที่ฟังก์ชัน `get_full_repo_name()` ทำ ):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

จากนั้นเราสามารถโคลนพื้นที่เก็บข้อมูลนั้นในโฟลเดอร์ในเครื่องได้ หากมีอยู่แล้ว โฟลเดอร์ในเครื่องนี้ควรเป็นโคลนของพื้นที่เก็บข้อมูลที่เรากำลังทำงานด้วย:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ตอนนี้เราสามารถอัปโหลดทุกสิ่งที่เราบันทึกไว้ใน `output_dir` ได้โดยการเรียกเมธอด `repo.push_to_hub()` ซึ่งจะช่วยให้เราอัปโหลดโมเดลระดับกลางในตอนท้ายของแต่ละ epoch ได้

### วงจรการฝึกอบรม[[วงจรการฝึกอบรม]]

ตอนนี้เราพร้อมที่จะเขียนลูปการฝึกอบรมฉบับเต็มแล้ว เพื่อให้ส่วนการประเมินง่ายขึ้น เราได้กำหนดฟังก์ชัน `postprocess()` นี้ ซึ่งใช้การคาดการณ์และป้ายกำกับ และแปลงเป็นรายการสตริงที่ออบเจ็กต์ `metric` ของเราจะคาดหวัง:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

วงจรการฝึกอบรมมีลักษณะคล้ายกับใน [ส่วนที่ 2](/course/th/chapter7/2) และ [บทที่ 3](/course/th/chapter3) มาก โดยมีความแตกต่างเล็กน้อยในส่วนการประเมิน ดังนั้นเรามาเน้นที่เรื่องนั้นกันดีกว่า

สิ่งแรกที่ควรทราบคือเราใช้เมธอด `generate()` เพื่อคำนวณการคาดการณ์ แต่นี่เป็นวิธีการในโมเดลพื้นฐานของเรา ไม่ใช่โมเดลที่ห่อ 🤗 Accelerate สร้างขึ้นในเมธอด `prepare()` นั่นคือสาเหตุที่เราแกะโมเดลออกก่อน แล้วจึงเรียกเมธอดนี้

อย่างที่สองก็คือ เช่นเดียวกับ [การจำแนกโทเค็น](/course/th/chapter7/2) สองกระบวนการอาจมีการเสริมอินพุตและป้ายกำกับเป็นรูปร่างที่แตกต่างกัน ดังนั้นเราจึงใช้ `accelerator.pad_across_processes()` เพื่อทำการคาดการณ์และป้ายกำกับ รูปร่างเดียวกันก่อนที่จะเรียกเมธอด `gather()` หากเราไม่ทำเช่นนี้ การประเมินจะเกิดข้อผิดพลาดหรือหยุดทำงานตลอดไป

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

เมื่อเสร็จแล้ว คุณควรมีโมเดลที่มีผลลัพธ์ค่อนข้างคล้ายกับโมเดลที่ได้รับการฝึกด้วย `Seq2SeqTrainer` คุณสามารถตรวจสอบอันที่เราฝึกได้โดยใช้โค้ดนี้ที่ [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate) และหากคุณต้องการทดสอบการปรับแต่งใดๆ ในลูปการฝึก คุณสามารถนำไปใช้ได้โดยตรงโดยแก้ไขโค้ดที่แสดงด้านบน!

{/if}

## การใช้โมเดลที่ปรับแต่งแล้ว[[การใช้โมเดลที่ปรับแต่งแล้ว]]

เราได้แสดงให้คุณเห็นแล้วว่าคุณสามารถใช้โมเดลที่เราปรับแต่งอย่างละเอียดบน Model Hub ด้วยวิดเจ็ตการอนุมานได้อย่างไร หากต้องการใช้ภายในเครื่องใน `pipeline` เราเพียงแค่ต้องระบุตัวระบุโมเดลที่เหมาะสม:

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par défaut, développer les fils de discussion'}]
```

ตามที่คาดไว้ โมเดลที่ได้รับการฝึกล่วงหน้าของเราจะปรับความรู้ให้เข้ากับคลังข้อมูลที่เราปรับแต่ง และแทนที่จะปล่อยให้คำว่า "threads" ในภาษาอังกฤษเพียงอย่างเดียว ตอนนี้กลับแปลเป็นเวอร์ชันทางการของภาษาฝรั่งเศส เช่นเดียวกับ "plugin":

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

อีกตัวอย่างที่ดีของการปรับใช้โดเมน!

<Tip>

✏️ **ตาคุณแล้ว!** โมเดลส่งคืนอะไรในตัวอย่างที่มีคำว่า "email" ที่คุณระบุไว้ก่อนหน้านี้

</Tip>
