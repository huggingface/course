<FrameworkSwitchCourse {fw} />

# <i>Tokenizer</i> rapide dans le pipeline de QA

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3b_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3b_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3b_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3b_tf.ipynb"},
]} />

{/if}

Nous allons maintenant nous plonger dans le pipeline de `question-answering` et voir comment exploiter les *offsets* pour extraire d'u ncontexte la r√©ponse √† la question pos√©e. Nous verrons ensuite comment g√©rer les contextes tr√®s longs qui finissent par √™tre tronqu√©s. Vous pouvez sauter cette section si vous n'√™tes pas int√©ress√© par la t√¢che de r√©ponse aux questions.

{#if fw === 'pt'}

<Youtube id="_wxyB3j3mk4"/>

{:else}

<Youtube id="b3u8RzBCX9Y"/>

{/if}

## Utilisation du pipeline de `question-answering`

Comme nous l'avons vu dans le [chapitre 1](/course/fr/chapter1), nous pouvons utiliser le pipeline de `question-answering` comme ceci pour obtenir une r√©ponse √† une question :

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ü§ó Transformers is backed by the three most popular deep learning libraries
 ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration between them. 
It's straightforward to train your models with one before loading them for inference with the other.
"""
# ü§ó Transformers s'appuie sur les trois biblioth√®ques d'apprentissage profond les plus populaires
# (Jax, PyTorch et TensorFlow) avec une int√©gration transparente entre elles.
# C'est simple d'entra√Æner vos mod√®les avec l'une avant de les charger pour l'inf√©rence avec l'autre.
question = "Which deep learning libraries back ü§ó Transformers?"
# Quelles biblioth√®ques d'apprentissage profond derri√®re ü§ó Transformers ?
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Contrairement aux autres pipelines, qui ne peuvent pas tronquer et diviser les textes dont la longueur est sup√©rieure √† la longueur maximale accept√©e par le mod√®le (et qui peuvent donc manquer des informations √† la fin d'un document), ce pipeline peut traiter des contextes tr√®s longs et retournera la r√©ponse √† la question m√™me si elle se trouve √† la fin :

```py
long_context = """
ü§ó Transformers: State of the Art NLP

ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""

long_context - fr = """
ü§ó Transformers : l'√©tat de l'art du NLP

ü§ó Transformers fournit des milliers de mod√®les pr√©-entra√Æn√©s pour effectuer des t√¢ches sur des textes telles que la classification, 
l'extraction d'informations, la r√©ponse √† des questions, le r√©sum√© de textes, la traduction, la g√©n√©ration de texte et plus encore dans plus de 100 langues.
Son objectif est de rendre le traitement automatique des langues de pointe plus facile √† utiliser pour tout le monde.

ü§ó Transformers fournit des API permettant de t√©l√©charger et d'utiliser rapidement ces mod√®les pr√©-entra√Æn√©s sur un texte donn√©, de les affiner sur vos propres ensembles de donn√©es et de les partager avec la communaut√© sur notre site Web.
puis de les partager avec la communaut√© sur notre hub de mod√®les. En m√™me temps, chaque module python d√©finissant une architecture est enti√®rement autonome et peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.
peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.

Pourquoi devrais-je utiliser des transformateurs ?

1. Des mod√®les de pointe faciles √† utiliser :
  - Haute performance sur les t√¢ches NLU et NLG.
  - Faible barri√®re √† l'entr√©e pour les √©ducateurs et les praticiens.
  - Peu d'abstractions pour l'utilisateur avec seulement trois classes √† apprendre.
  - Une API unifi√©e pour utiliser tous nos mod√®les pr√©-entra√Æn√©s.
  - Des co√ªts de calcul plus faibles, une empreinte carbone r√©duite :

2. Les chercheurs peuvent partager les mod√®les form√©s au lieu de toujours les reformer.
  - Les praticiens peuvent r√©duire le temps de calcul et les co√ªts de production.
  - Des dizaines d'architectures avec plus de 10 000 mod√®les pr√©-form√©s, certains dans plus de 100 langues.

3. Choisissez le cadre appropri√© pour chaque √©tape de la vie d'un mod√®le :
  - Entra√Ænez des mod√®les de pointe en 3 lignes de code.
  - D√©placez un seul mod√®le entre les frameworks TF2.0/PyTorch √† volont√©.
  - Choisissez de mani√®re transparente le bon framework pour l'entra√Ænement, l'√©valuation et la production.

4. Adaptez facilement un mod√®le ou un exemple √† vos besoins :
  - Nous fournissons des exemples pour chaque architecture afin de reproduire les r√©sultats publi√©s par ses auteurs originaux.
  - Les √©l√©ments internes des mod√®les sont expos√©s de mani√®re aussi coh√©rente que possible.
  - Les fichiers de mod√®les peuvent √™tre utilis√©s ind√©pendamment de la biblioth√®que pour des exp√©riences rapides.

ü§ó Transformers s'appuie sur les trois biblioth√®ques d'apprentissage profond les plus populaires (Jax, PyTorch et TensorFlow) avec une int√©gration parfaite
entre elles. Il est simple d'entra√Æner vos mod√®les avec l'une avant de les charger pour l'inf√©rence avec l'autre.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Voyons comment il fait tout cela !

### Utilisation d'un mod√®le pour r√©pondre √† des questions

Comme avec n'importe quel autre pipeline, nous commen√ßons par tokeniser notre entr√©e et l'envoyons ensuite √† travers le mod√®le. Le *checkpoint* utilis√© par d√©faut pour le pipeline de `question-answering` est [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) (le ¬´ squad ¬ª dans le nom vient du jeu de donn√©es sur lequel le mod√®le a √©t√© *finetun√©*, nous parlerons davantage de ce jeu de donn√©es dans le [chapitre 7](/course/fr/chapter7/7)) :

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)
```

{/if}

Notez que nous tokenizons la question et le contexte comme une paire, la question en premier.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

Les mod√®les de r√©ponse aux questions fonctionnent un peu diff√©remment des mod√®les que nous avons vus jusqu'√† pr√©sent. En utilisant l'image ci-dessus comme exemple, le mod√®le a √©t√© entra√Æn√© √† pr√©dire l'index du *token* de d√©but de la r√©ponse (ici 21) et l'index du *token* o√π la r√©ponse se termine (ici 24). C'est pourquoi ces mod√®les ne retournent pas un tenseur de logits mais deux : un pour les logits correspondant au *token* de d√©but de la r√©ponse, et un pour les logits correspondant au *token* de fin de la r√©ponse. Puisque dans ce cas nous n'avons qu'une seule entr√©e contenant 66 *tokens*, nous obtenons :

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

{:else}

```python out
(1, 66) (1, 66)
```

{/if}

Pour convertir ces logits en probabilit√©s, nous allons appliquer une fonction softmax. Mais avant cela, nous devons nous assurer de masquer les indices qui ne font pas partie du contexte. Notre entr√©e est `[CLS] question [SEP] contexte [SEP]` donc nous devons masquer les *tokens* de la question ainsi que le *token* `[SEP]`. Nous garderons cependant le *token* `[CLS]` car certains mod√®les l'utilisent pour indiquer que la r√©ponse n'est pas dans le contexte.

Puisque nous appliquerons une fonction softmax par la suite, il nous suffit de remplacer les logits que nous voulons masquer par un grand nombre n√©gatif. Ici, nous utilisons `-10000` :

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
# Masque tout, sauf les tokens du contexte
mask = [i != 1 for i in sequence_ids]
# D√©masquer le token [CLS]
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Masque tout, sauf les tokens du contexte
mask = [i != 1 for i in sequence_ids]
# D√©masquer le token [CLS]
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Maintenant que nous avons correctement masqu√© les logits correspondant aux positions que nous ne voulons pas pr√©dire, nous pouvons appliquer la softmax :

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

A ce stade, nous pourrions prendre l'argmax des probabilit√©s de d√©but et de fin mais nous pourrions nous retrouver avec un indice de d√©but sup√©rieur √† l'indice de fin. Nous devons donc prendre quelques pr√©cautions suppl√©mentaires. Nous allons calculer les probabilit√©s de chaque `start_index` et `end_index` possible o√π `start_index<=end_index`, puis nous prendrons le *tuple* `(start_index, end_index)` avec la plus grande probabilit√©.

En supposant que les √©v√©nements ¬´ La r√©ponse commence √† `start_index` ¬ª et ¬´ La r√©ponse se termine √† `end_index` ¬ª sont ind√©pendants, la probabilit√© que la r√©ponse commence √† `end_index` et se termine √† `end_index` est :

$$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]$$ 

Ainsi, pour calculer tous les scores, il suffit de calculer tous les produits \\(\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]\\) o√π `start_index <= end_index`.

Calculons d'abord tous les produits possibles :

```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

{#if fw === 'pt'}

Ensuite, nous masquerons les valeurs o√π `start_index > end_index` en les mettant √† `0` (les autres probabilit√©s sont toutes des nombres positifs). La fonction `torch.triu()` renvoie la partie triangulaire sup√©rieure du tenseur 2D pass√© en argument, elle fera donc ce masquage pour nous :

```py
scores = torch.triu(scores)
```

{:else}

Ensuite, nous masquerons les valeurs o√π `start_index > end_index` en les mettant √† `0` (les autres probabilit√©s sont toutes des nombres positifs). La fonction `np.triu()` renvoie la partie triangulaire sup√©rieure du tenseur 2D pass√© en argument, elle fera donc ce masquage pour nous :

```py
scores = np.triu(scores)
```

{/if}

Il ne nous reste plus qu'√† obtenir l'indice du maximum. Puisque PyTorch retourne l'index dans le tenseur aplati, nous devons utiliser les op√©rations division `//` et modulo `%` pour obtenir le `start_index` et le `end_index` :

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

Nous n'avons pas encore tout √† fait termin√©, mais au moins nous avons d√©j√† le score correct pour la r√©ponse (vous pouvez le v√©rifier en le comparant au premier r√©sultat de la section pr√©c√©dente) :

```python out
0.97773
```

<Tip>

‚úèÔ∏è **Essayez !** Calculez les indices de d√©but et de fin pour les cinq r√©ponses les plus probables.

</Tip>

Nous avons les `start_index` et `end_index` de la r√©ponse en termes de *tokens*. Maintenant nous devons juste convertir en indices de caract√®res dans le contexte. C'est l√† que les *offsets* seront super utiles. Nous pouvons les saisir et les utiliser comme nous l'avons fait dans la t√¢che de classification des *tokens* :

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

Il ne nous reste plus qu'√† tout formater pour obtenir notre r√©sultat :

```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

Super ! C'est la m√™me chose que dans notre premier exemple !

<Tip>

‚úèÔ∏è **Essayez !** Utilisez les meilleurs scores que vous avez calcul√©s pr√©c√©demment pour afficher les cinq r√©ponses les plus probables. Pour v√©rifier vos r√©sultats, retournez au premier pipeline et passez dans `top_k=5` lorsque vous l'appelez.

</Tip>

## Gestion des contextes longs

Si nous essayons de tokeniser la question et le long contexte que nous avons utilis√© dans l'exemple pr√©c√©demment, nous obtenons un nombre de *tokens* sup√©rieur √† la longueur maximale utilis√©e dans le pipeline `question-answering` (qui est de 384) :

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

Nous devrons donc tronquer nos entr√©es √† cette longueur maximale. Il y a plusieurs fa√ßons de le faire mais nous ne voulons pas tronquer la question, seulement le contexte. Puisque le contexte est la deuxi√®me phrase, nous utilisons la strat√©gie de troncature `"only_second"`. Le probl√®me qui se pose alors est que la r√©ponse √† la question peut ne pas se trouver dans le contexte tronqu√©. Ici, par exemple, nous avons choisi une question dont la r√©ponse se trouve vers la fin du contexte, et lorsque nous la tronquons, cette r√©ponse n'est pas pr√©sente :

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""

"""
[CLS] Quelles sont les biblioth√®ques d'apprentissage profond qui soutiennent [UNK] Transformers ? [SEP] [UNK] Transformers : l'√©tat de l'art du NLP

[UNK] Transformers fournit des milliers de mod√®les pr√©-entra√Æn√©s pour effectuer des t√¢ches sur des textes telles que la classification, l'extraction d'informations, la r√©ponse √† des questions, le r√©sum√©, la traduction, la g√©n√©ration de textes, etc,
la r√©ponse √† des questions, le r√©sum√©, la traduction, la g√©n√©ration de texte et plus encore dans plus de 100 langues.
Son objectif est de rendre le traitement automatique des langues de pointe plus facile √† utiliser pour tous.

Transformers [UNK] fournit des API permettant de t√©l√©charger et d'utiliser rapidement ces mod√®les pr√©-entra√Æn√©s sur un texte donn√©, de les affiner sur vos propres ensembles de donn√©es et de les partager avec la communaut√© sur notre site Web.
puis de les partager avec la communaut√© sur notre hub de mod√®les. En m√™me temps, chaque module python d√©finissant une architecture est enti√®rement autonome et peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.
peut √™tre modifi√© pour permettre des exp√©riences de recherche rapides.

Pourquoi devrais-je utiliser des transformateurs ?

1. Des mod√®les de pointe faciles √† utiliser :
  - Haute performance sur les t√¢ches NLU et NLG.
  - Faible barri√®re √† l'entr√©e pour les √©ducateurs et les praticiens.
  - Peu d'abstractions pour l'utilisateur avec seulement trois classes √† apprendre.
  - Une API unifi√©e pour utiliser tous nos mod√®les pr√©-entra√Æn√©s.
  - Des co√ªts de calcul plus faibles, une empreinte carbone r√©duite :

2. Les chercheurs peuvent partager les mod√®les form√©s au lieu de toujours les reformer.
  - Les praticiens peuvent r√©duire le temps de calcul et les co√ªts de production.
  - Des dizaines d'architectures avec plus de 10 000 mod√®les pr√©-form√©s, certains dans plus de 100 langues.

3. Choisissez le cadre appropri√© pour chaque √©tape de la vie d'un mod√®le :
  - Entra√Ænez des mod√®les de pointe en 3 lignes de code.
  - D√©placez un seul mod√®le entre les frameworks TF2.0/PyTorch √† volont√©.
  - Choisissez de mani√®re transparente le bon framework pour l'entra√Ænement, l'√©valuation et la production.

4. Adaptez facilement un mod√®le ou un exemple √† vos besoins :
  - Nous fournissons des exemples pour chaque architecture afin de reproduire les r√©sultats publi√©s par ses auteurs originaux.
  - Mod√®le interne [SEP]
"""
```

Cela signifie que le mod√®le a du mal √† trouver la bonne r√©ponse. Pour r√©soudre ce probl√®me, le pipeline de `question-answering` nous permet de diviser le contexte en morceaux plus petits, en sp√©cifiant la longueur maximale. Pour s'assurer que nous ne divisons pas le contexte exactement au mauvais endroit pour permettre de trouver la r√©ponse, il inclut √©galement un certain chevauchement entre les morceaux.

Nous pouvons demander au *tokenizer* (rapide ou lent) de le faire pour nous en ajoutant `return_overflowing_tokens=True`, et nous pouvons sp√©cifier le chevauchement que nous voulons avec l'argument `stride`. Voici un exemple, en utilisant une phrase plus petite :

```py
sentence = "This sentence is not too long but we are going to split it anyway."
# "Cette phrase n'est pas trop longue mais nous allons la diviser quand m√™me."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

Comme on peut le voir, la phrase a √©t√© d√©coup√©e en morceaux de telle sorte que chaque entr√©e dans `inputs["input_ids"]` a au maximum 6 *tokens* (il faudrait ajouter du *padding* pour que la derni√®re entr√©e ait la m√™me taille que les autres) et il y a un chevauchement de 2 *tokens* entre chacune des entr√©es. 

Regardons de plus pr√®s le r√©sultat de la tok√©nisation :

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

Comme pr√©vu, nous obtenons les identifiants d'entr√©e et un masque d'attention. La derni√®re cl√©, `overflow_to_sample_mapping`, est une carte qui nous indique √† quelle phrase correspond chacun des r√©sultats. Ici nous avons 7 r√©sultats qui proviennent tous de la (seule) phrase que nous avons pass√©e au *tokenizer* :

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

C'est plus utile lorsque nous tokenisons plusieurs phrases ensemble. Par exemple :

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    # Cette phrase n'est pas trop longue mais nous allons la diviser quand m√™me.
    "This sentence is shorter but will still get split.",
    # Cette phrase est plus courte mais sera quand m√™me divis√©e.
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

nous donne :

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

ce qui signifie que la premi√®re phrase est divis√©e en 7 morceaux comme pr√©c√©demment et que les 4 morceaux suivants proviennent de la deuxi√®me phrase.

Revenons maintenant √† notre contexte long. Par d√©faut, le pipeline `question-answering` utilise une longueur maximale de 384 et un *stride* de 128, qui correspondent √† la fa√ßon dont le mod√®le a √©t√© *finetun√©* (vous pouvez ajuster ces param√®tres en passant les arguments `max_seq_len` et `stride` lorsque vous appelez le pipeline). Nous utiliserons donc ces param√®tres lors de la tokenisation. Nous ajouterons aussi du *padding* (pour avoir des √©chantillons de m√™me longueur afin de pouvoir construire des tenseurs) ainsi que demander les *offsets* :

```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

Ces `inputs` contiendront les identifiants d'entr√©e, les masques d'attention que le mod√®le attend, ainsi que les *offsets* et le `overflow_to_sample_mapping` dont on vient de parler. Puisque ces deux √©l√©ments ne sont pas des param√®tres utilis√©s par le mod√®le, nous allons les sortir des `inputs` (et nous ne stockerons pas la correspondance puisqu'elle n'est pas utile ici) avant de le convertir en tenseur :

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python out
(2, 384)
```

{/if}

Notre contexte long a √©t√© divis√© en deux, ce qui signifie qu'apr√®s avoir travers√© notre mod√®le, nous aurons deux ensembles de logits de d√©but et de fin :

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python out
(2, 384) (2, 384)
```

{/if}

Comme pr√©c√©demment, nous masquons d'abord les *tokens* qui ne font pas partie du contexte avant de prendre le softmax. Nous masquons √©galement tous les *tokens* de *padding* (tels que signal√©s par le masque d'attention) :

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
# Masque tout, sauf les tokens du contexte
mask = [i != 1 for i in sequence_ids]
# D√©masquer le jeton [CLS]
mask[0] = False
# Masquer tous les tokens [PAD]
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
# Masque tout, sauf les tokens du contexte
mask = [i != 1 for i in sequence_ids]
# D√©masquer le jeton [CLS]
mask[0] = False
# Masquer tous les tokens [PAD]
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Ensuite, nous pouvons utiliser la fonction softmax pour convertir nos logits en probabilit√©s :

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()
```

{/if}

L'√©tape suivante est similaire √† ce que nous avons fait pour le petit contexte mais nous la r√©p√©tons pour chacun de nos deux morceaux. Nous attribuons un score √† tous les espaces de r√©ponse possibles puis nous prenons l'espace ayant le meilleur score :

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[0]
    end_idx = idx % scores.shape[0]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[0]
    end_idx = idx % scores.shape[0]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

Ces deux candidats correspondent aux meilleures r√©ponses que le mod√®le a pu trouver dans chaque morceau. Le mod√®le est beaucoup plus confiant dans le fait que la bonne r√©ponse se trouve dans la deuxi√®me partie (ce qui est bon signe !). Il ne nous reste plus qu'√† faire correspondre ces deux espaces de *tokens* √† des espaces de caract√®res dans le contexte (nous n'avons besoin de faire correspondre que le second pour avoir notre r√©ponse, mais il est int√©ressant de voir ce que le mod√®le a choisi dans le premier morceau).

<Tip>

‚úèÔ∏è **Essayez !** Adaptez le code ci-dessus pour renvoyer les scores et les √©tendues des cinq r√©ponses les plus probables (au total, pas par morceau).

</Tip>

Le `offsets` que nous avons saisi plus t√¥t est en fait une liste d'*offsets* avec une liste par morceau de texte :

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python out
{'answer': '\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

Si nous ignorons le premier r√©sultat, nous obtenons le m√™me r√©sultat que notre pipeline pour ce long contexte !

<Tip>

‚úèÔ∏è **Essayez !** Utilisez les meilleurs scores que vous avez calcul√©s auparavant pour montrer les cinq r√©ponses les plus probables (pour l'ensemble du contexte, pas pour chaque morceau). Pour v√©rifier vos r√©sultats, retournez au premier pipeline et sp√©cifiez `top_k=5` en argument en l'appelant.

</Tip>

Ceci conclut notre plong√©e en profondeur dans les capacit√©s du *tokenizer*. Nous mettrons √† nouveau tout cela en pratique dans le prochain chapitre, lorsque nous vous montrerons comment *finetuner* un mod√®le sur une s√©rie de t√¢ches NLP courantes.