# Introduction

Dans le [chapitre 3](/course/fr/chapter3), nous avons vu comment *finetuner* un modÃ¨le sur une tÃ¢che donnÃ©e. Pour ce faire, nous utilisons le mÃªme *tokenizer* que celui avec lequel le modÃ¨le a Ã©tÃ© prÃ©-entraÃ®nÃ©. Mais que faisons-nous lorsque nous voulons entraÃ®ner un modÃ¨le Ã  partir de zÃ©ro ? Dans ces cas, l'utilisation d'un *tokenizer* qui a Ã©tÃ© prÃ©-entraÃ®nÃ© sur un corpus d'un autre domaine ou d'une autre langue est gÃ©nÃ©ralement sous-optimale. Par exemple, un *tokenizer* entraÃ®nÃ© sur un corpus anglais sera peu performant sur un corpus de textes japonais car l'utilisation des espaces et de la ponctuation est trÃ¨s diffÃ©rente entre les deux langues.

Dans ce chapitre, vous apprendrez Ã  entraÃ®ner un tout nouveau *tokenizer* sur un corpus de textes afin qu'il puisse ensuite Ãªtre utilisÃ© pour prÃ©-entraÃ®ner un modÃ¨le de langue. Tout cela se fera Ã  l'aide de la bibliothÃ¨que [ğŸ¤— *Tokenizers*](https://github.com/huggingface/tokenizers), qui fournit les *tokenizers* Â« rapides Â» de la bibliothÃ¨que [ğŸ¤— *Transformers*](https://github.com/huggingface/transformers). Nous examinerons de prÃ¨s les fonctionnalitÃ©s offertes par cette bibliothÃ¨que et nous Ã©tudierons comment les *tokenizers* rapides diffÃ¨rent des versions Â« lentes Â».

Les sujets que nous couvrirons comprennent :
* comment entraÃ®ner sur un nouveau corpus de textes, un nouveau *tokenizer* similaire Ã  celui utilisÃ© par un *checkpoint* donnÃ©,
* les caractÃ©ristiques spÃ©ciales des *tokenizers* rapides,
* les diffÃ©rences entre les trois principaux algorithmes de tokÃ©nisation utilisÃ©s aujourd'hui en NLP,
* comment construire un *tokenizer* Ã  partir de zÃ©ro avec la bibliothÃ¨que ğŸ¤— *Tokenizers* et l'entraÃ®ner sur des donnÃ©es.

Les techniques prÃ©sentÃ©es dans ce chapitre vous prÃ©pareront Ã  la section du [chapitre 7](/course/fr/chapter7/6) oÃ¹ nous verrons comment crÃ©er un modÃ¨le de langue pour le langage Python. CommenÃ§ons par examiner ce que signifie Â« entraÃ®ner Â» un *tokenizer*.