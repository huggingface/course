# TokÃ©nisation <i>Byte-Pair Encoding</i>

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section5.ipynb"},
]} />

Le *Byte-Pair Encoding* (BPE) a Ã©tÃ© initialement dÃ©veloppÃ© en tant qu'algorithme de compression de textes puis utilisÃ© par OpenAI pour la tokenisation du prÃ©-entraÃ®nement du modÃ¨le GPT. Il est utilisÃ© par de nombreux *transformers* dont GPT, GPT-2, RoBERTa, BART et DeBERTa.

<Youtube id="HEikzVL-lZU"/>

<Tip>

ğŸ’¡ Cette section couvre le BPE en profondeur, allant jusqu'Ã  montrer une implÃ©mentation complÃ¨te. Vous pouvez passer directement Ã  la fin si vous souhaitez simplement avoir un aperÃ§u gÃ©nÃ©ral de l'algorithme de tokenisation.

</Tip>

## Algorithme d'entraÃ®nement

L'entraÃ®nement du BPE commence par le calcul de l'unique ensemble de mots utilisÃ©s dans le corpus (aprÃ¨s les Ã©tapes de normalisation et de prÃ©tokÃ©nisation), puis la construction du vocabulaire en prenant tous les symboles utilisÃ©s pour Ã©crire ces mots. A titre d'exemple, disons que notre corpus utilise ces cinq mots :

```
"hug", "pug", "pun", "bun", "hugs" # "cÃ¢lin", "carlin", "jeu de mots", "brioche", "cÃ¢lins"
```

Le vocabulaire de base sera alors `["b", "g", "h", "n", "p", "s", "u"]`. Dans le monde rÃ©el, le vocabulaire de base contient au moins tous les caractÃ¨res ASCII et probablement aussi quelques caractÃ¨res Unicode. Si un exemple que vous tokenisez utilise un caractÃ¨re qui n'est pas dans le corpus d'entraÃ®nement, ce caractÃ¨re est converti en *token* inconnu. C'est l'une des raisons pour lesquelles de nombreux modÃ¨les de NLP sont par exemple trÃ¨s mauvais dans l'analyse de contenus contenant des emojis.

<Tip>

Les *tokenizers* du GPT-2 et de RoBERTa (qui sont assez similaires) ont une faÃ§on intelligente de gÃ©rer ce problÃ¨me : ils ne considÃ¨rent pas les mots comme Ã©tant Ã©crits avec des caractÃ¨res Unicode mais avec des octets. De cette faÃ§on, le vocabulaire de base a une petite taille (256) et tous les caractÃ¨res auxquels vous pouvez penser seront inclus dedans et ne finiront pas par Ãªtre convertis en un *token* inconnu. Cette astuce est appelÃ©e *byte-level BPE*.

</Tip>

AprÃ¨s avoir obtenu ce vocabulaire de base, nous ajoutons de nouveaux *tokens* jusqu'Ã  ce que la taille souhaitÃ©e du vocabulaire soit atteinte en apprenant les fusions qui sont des rÃ¨gles permettant de fusionner deux Ã©lÃ©ments du vocabulaire existant pour en crÃ©er un nouveau. Ainsi, au dÃ©but, ces fusions crÃ©eront des *tokens* de deux caractÃ¨res, puis au fur et Ã  mesure de l'entraÃ®nement, des sous-mots plus longs.

Ã€ chaque Ã©tape de l'entraÃ®nement du *tokenizer*, l'algorithme BPE recherche la paire la plus frÃ©quente de *tokens* existants (par Â« paire Â», nous entendons ici deux *tokens* consÃ©cutifs dans un mot). Cette paire la plus frÃ©quente est celle qui sera fusionnÃ©e. Nous rinÃ§ons et rÃ©pÃ©tons pour l'Ã©tape suivante.

Pour revenir Ã  notre exemple prÃ©cÃ©dent, supposons que les mots ont les frÃ©quences suivantes :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ce qui veut dire que `"hug"` Ã©tait prÃ©sent 10 fois dans le corpus, `"pug"` 5 fois, `"pun"` 12 fois, `"bun"` 4 fois et `"hugs"`" 5 fois. Nous commenÃ§ons l'entraÃ®nement en divisant chaque mot en caractÃ¨res (ceux qui forment notre vocabulaire initial) afin de voir chaque mot comme une liste de *tokens* :

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

Ensuite, nous regardons les paires. La paire `("h", "u")` est prÃ©sente dans les mots `"hug"` et `"hugs"`, donc 15 fois au total dans le corpus. Ce n'est cependant pas la paire la plus frÃ©quente. Cet honneur revient Ã  `("u", "g")` qui est prÃ©sent dans `"hug"`, `"pug"`, et `"hugs"`, pour un total de 20 fois dans le vocabulaire.

Ainsi, la premiÃ¨re rÃ¨gle de fusion apprise par le *tokenizer* est `("u", "g") -> "ug"`, ce qui signifie que `"ug"` est ajoutÃ© au vocabulaire et que la paire doit Ãªtre fusionnÃ©e dans tous les mots du corpus. A la fin de cette Ã©tape, le vocabulaire et le corpus ressemblent Ã  ceci :

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

Nous avons maintenant quelques paires qui aboutissent Ã  un *token* de plus de deux caractÃ¨res. Par exemple la paire `("h", "ug")` prÃ©sente 15 fois dans le corpus. La paire la plus frÃ©quente Ã  ce stade est `("u", "n")`, prÃ©sente 16 fois dans le corpus, donc la deuxiÃ¨me rÃ¨gle de fusion apprise est `("u", "n") -> "un"`. En ajoutant cela au vocabulaire et en fusionnant toutes les occurrences existantes, nous obtenons :

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

Maintenant la paire la plus frÃ©quente est `("h", "ug")` donc nous apprenons la rÃ¨gle de fusion `("h", "ug") -> "hug"`. Cela nous donne donc notre premier *token* de trois lettres. AprÃ¨s la fusion, le corpus ressemble Ã  ceci :

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

Et nous continuons ainsi jusqu'Ã  ce que nous atteignions la taille de vocabulaire souhaitÃ©e.

<Tip>

âœï¸ **A votre tour !** A votre avis, quelle sera la prochaine rÃ¨gle de fusion ?

</Tip>

## Algorithme de tokenisation

La tokenisation suit de prÃ¨s le processus d'entraÃ®nement, dans le sens oÃ¹ les nouvelles entrÃ©es sont tokenisÃ©es en appliquant les Ã©tapes suivantes :

1. Normalisation
2. PrÃ©tokÃ©nisation
3. DÃ©coupage des mots en caractÃ¨res individuels
4. Application des rÃ¨gles de fusion apprises dans l'ordre sur ces divisions.

Prenons l'exemple que nous avons utilisÃ© pendant l'entraÃ®nement, avec les trois rÃ¨gles de fusion apprises :

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

Le mot Â« bug Â»  sera traduit par Â« ["b", "ug"] Â». Par contre, le mot Â« mug Â» (tasse en franÃ§ais) sera traduit par Â« ["[UNK]", "ug"] Â» puisque la lettre Â« m Â» ne fait pas partie du vocabulaire de base. De la mÃªme faÃ§on, le mot Â« thug Â» (voyou en franÃ§ais) sera tokenisÃ© en Â« ["[UNK]", "hug"] Â» car la lettre Â« t Â» n'est pas dans le vocabulaire de base et l'application des rÃ¨gles de fusion rÃ©sulte d'abord en la fusion de Â« u Â» et Â« g Â» et ensuite en la fusion de Â« hu Â» et Â« g Â».

<Tip>

âœï¸ **A votre tour !** Comment pensez-vous que le mot Â« unhug Â» (dÃ©tacher en franÃ§ais) sera tokenized ?

</Tip>

## ImplÃ©mentation du BPE

Voyons maintenant une implÃ©mentation de l'algorithme BPE. Il ne s'agira pas d'une version optimisÃ©e que vous pourrez utiliser sur un grand corpus. Nous voulons simplement vous montrer le code afin que vous puissiez comprendre un peu mieux l'algorithme.

Tout d'abord, nous avons besoin d'un corpus, alors crÃ©ons un corpus simple avec quelques phrases :

```python
corpus = [
    "This is the Hugging Face Course.",
    # C'est le cours d'Hugging Face.
    "This chapter is about tokenization.",
    # Ce chapitre traite de la tokenisation.
    "This section shows several tokenizer algorithms.",
    # Cette section prÃ©sente plusieurs algorithmes de tokenizer.
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
    # Avec un peu de chance, vous serez en mesure de comprendre comment ils sont entraÃ®nÃ©s et gÃ©nÃ¨rent des tokens.
]
```

Ensuite, nous devons prÃ©tokeniser ce corpus en mots. Puisque nous rÃ©pliquons un *tokenizer* BPE (comme celui du GPT-2), nous utiliserons le *tokenizer* `gpt2` pour la prÃ©tokÃ©nisation :

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Ensuite, nous calculons les frÃ©quences de chaque mot dans le corpus comme nous le faisons pour la prÃ©tokÃ©nisation :

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

L'Ã©tape suivante consiste Ã  calculer le vocabulaire de base, formÃ© par tous les caractÃ¨res utilisÃ©s dans le corpus :

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

Nous ajoutons Ã©galement les *tokens* spÃ©ciaux utilisÃ©s par le modÃ¨le au dÃ©but de ce vocabulaire. Dans le cas du GPT-2, le seul *token* spÃ©cial est `"<|endoftext|>"` :

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

Nous devons maintenant diviser chaque mot en caractÃ¨res individuels pour pouvoir commencer l'entraÃ®nement :

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

Maintenant que nous sommes prÃªts pour l'entraÃ®nement, Ã©crivons une fonction qui calcule la frÃ©quence de chaque paire. Nous devrons l'utiliser Ã  chaque Ã©tape de l'entraÃ®nement :

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

Jetons un coup d'Å“il Ã  une partie de ce dictionnaire aprÃ¨s les premiÃ¨res divisions :

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

Maintenant, trouver la paire la plus frÃ©quente ne demande qu'une rapide boucle :

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ä ', 't') 7
```

Donc la premiÃ¨re fusion Ã  apprendre est `('Ä ', 't') -> 'Ä t'`, et on ajoute `'Ä t'` au vocabulaire :

```python
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

Pour continuer, nous devons appliquer cette fusion dans notre dictionnaire `splits`. Ã‰crivons une autre fonction pour cela :

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

Et nous pouvons regarder le rÃ©sultat de la premiÃ¨re fusion :

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```python out
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

Maintenant, nous avons tout ce dont nous avons besoin pour boucler jusqu'Ã  ce que nous ayons appris toutes les fusions que nous voulons. Visons une taille de vocabulaire de 50 :

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

En consÃ©quence, nous avons appris 19 rÃ¨gles de fusion (le vocabulaire initial avait une taille de 31 : 30 caractÃ¨res dans l'alphabet plus le *token* spÃ©cial) :

```py
print(merges)
```

```python out
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

Et le vocabulaire est composÃ© du *token* spÃ©cial, de l'alphabet initial, et de tous les rÃ©sultats des fusions :

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

<Tip>

ğŸ’¡ Utiliser `train_new_from_iterator()` sur le mÃªme corpus ne donnera pas exactement le mÃªme vocabulaire. C'est parce que lorsqu'il y a un choix de la paire la plus frÃ©quente, nous avons sÃ©lectionnÃ© la premiÃ¨re rencontrÃ©e, alors que la bibliothÃ¨que ğŸ¤— *Tokenizers* sÃ©lectionne la premiÃ¨re en fonction de ses identifiants internes.

</Tip>

Pour tokeniser un nouveau texte, on le prÃ©tokenise, on le divise, puis on applique toutes les rÃ¨gles de fusion apprises :

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

Nous pouvons essayer cela sur n'importe quel texte composÃ© de caractÃ¨res de l'alphabet :

```py
tokenize("This is not a token.")
```

```python out
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

<Tip warning={true}>

âš ï¸ Notre implÃ©mentation lancera une erreur s'il y a un caractÃ¨re inconnu puisque nous n'avons rien fait pour les gÃ©rer. GPT-2 n'a pas rÃ©ellement de <i>token</i> inconnu (il est impossible d'obtenir un caractÃ¨re inconnu en utilisant le BPE au niveau de l'octet) mais cela pourrait arriver ici car nous n'avons pas inclus tous les octets possibles dans le vocabulaire initial. Cet aspect du BPE dÃ©passe le cadre de cette section, nous avons donc laissÃ© ces dÃ©tails de cÃ´tÃ©.

</Tip>

C'est tout pour l'algorithme BPE ! Nous allons nous intÃ©resser Ã  WordPiece dans la suite.