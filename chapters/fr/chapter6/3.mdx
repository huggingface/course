<FrameworkSwitchCourse {fw} />

# Pouvoirs sp√©ciaux des <i>tokenizers</i> rapides

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},
]} />

{/if}

Dans cette section, nous allons examiner de plus pr√®s les capacit√©s des *tokenizers* dans ü§ó *Transformers*.
Jusqu'√† pr√©sent, nous ne les avons utilis√©s que pour tokeniser les entr√©es ou d√©coder les identifiants pour revenir √† du texte. Mais les *tokenizers*, et surtout ceux soutenus par la biblioth√®que ü§ó *Tokenizers*, peuvent faire beaucoup plus. Pour illustrer ces fonctionnalit√©s suppl√©mentaires, nous allons explorer comment reproduire les r√©sultats des pipelines `token-classification` (que nous avons appel√© `ner`) et `question-answering` que nous avons rencontr√©s pour la premi√®re fois dans le [chapitre 1](/course/fr/chapter1).

<Youtube id="g8quOxoqhHQ"/>

Dans la discussion qui suit, nous ferons souvent la distinction entre les *tokenizers* ¬´ lents ¬ª et les ¬´ rapides ¬ª. Les *tokenizers* lents sont ceux √©crits en Python √† l'int√©rieur de la biblioth√®que ü§ó *Transformers*, tandis que les rapides sont ceux fournis par ü§ó *Tokenizers* et sont cod√©s en Rust. Si vous vous souvenez du tableau du [chapitre 5](/course/fr/chapter5/3) qui indiquait combien de temps il fallait √† un *tokenizer* rapide et √† un *tokenizer* lent pour tokeniser le jeu de donn√©es *Drug Review*, vous devriez avoir une id√©e de la raison pour laquelle nous les appelons rapides et lents :

                | *Tokenizer* rapide | *Tokenizer* lent
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

‚ö†Ô∏è Lors de la tokenisation d'une seule phrase, vous ne verrez pas toujours une diff√©rence de vitesse entre les versions lente et rapide d'un m√™me *tokenizer*. En fait, la version rapide peut m√™me √™tre plus lente ! Ce n'est que lorsque vous tokenisez beaucoup de textes en parall√®le et en m√™me temps que vous pourrez clairement voir la diff√©rence.

</Tip>

## L'objet <i>BatchEncoding</i>

<Youtube id="3umI3tm27Vw"/>

La sortie d'un *tokenizer* n'est pas un simple dictionnaire Python. Ce que nous obtenons est en fait un objet sp√©cial `BatchEncoding`. C'est une sous-classe d'un dictionnaire (c'est pourquoi nous avons pu indexer ce r√©sultat sans probl√®me auparavant), mais avec des m√©thodes suppl√©mentaires qui sont principalement utilis√©es par les *tokenizers* rapides.

En plus de leurs capacit√©s de parall√©lisation, la fonctionnalit√© cl√© des *tokenizers* rapides est qu'ils gardent toujours la trace de l'√©tendue originale des textes d'o√π proviennent les *tokens* finaux, une fonctionnalit√© que nous appelons *mapping offset*. Cela permet de d√©bloquer des fonctionnalit√©s telles que le mappage de chaque mot aux *tokens* qu'il a g√©n√©r√©s ou le mappage de chaque caract√®re du texte original au *token* qu'il contient, et vice versa.

Prenons un exemple :

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
# Je m'appelle Sylvain et je travaille chez Hugging Face √† Brooklyn.
encoding = tokenizer(example)
print(type(encoding))
```

Comme mentionn√© pr√©c√©demment, nous obtenons un objet `BatchEncoding` dans la sortie du *tokenizer* :

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

Puisque la classe `AutoTokenizer` choisit un *tokenizer* rapide par d√©faut, nous pouvons utiliser les m√©thodes suppl√©mentaires que cet objet `BatchEncoding` fournit. Nous avons deux fa√ßons de v√©rifier si notre *tokenizer* est rapide ou lent. Nous pouvons soit v√©rifier l'attribut `is_fast` du *tokenizer* comme suit :

```python
tokenizer.is_fast
```

```python out
True
```

soit v√©rifier le m√™me attribut mais avec notre `encoding` :

```python
encoding.is_fast
```

```python out
True
```

Voyons ce qu'un *tokenizer* rapide nous permet de faire. Tout d'abord, nous pouvons acc√©der aux *tokens* sans avoir √† reconvertir les identifiants en *tokens* :

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

Dans ce cas, le *token* √† l'index 5 est `##yl` et fait partie du mot ¬´ Sylvain ¬ª dans la phrase originale. Nous pouvons √©galement utiliser la m√©thode `word_ids()` pour obtenir l'index du mot dont provient chaque *token* :

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

On peut voir que les *tokens* sp√©ciaux du *tokenizer*, `[CLS]` et `[SEP]`, sont mis en correspondance avec `None` et que chaque *token* est mis en correspondance avec le mot dont il provient. Ceci est particuli√®rement utile pour d√©terminer si un *token* est au d√©but d'un mot ou si deux *tokens* sont dans le m√™me mot. Nous pourrions nous appuyer sur le pr√©fixe `##` pour cela, mais il ne fonctionne que pour les *tokenizers* de type BERT. Cette m√©thode fonctionne pour n'importe quel type de *tokenizer*, du moment qu'il est rapide. Dans le chapitre suivant, nous verrons comment utiliser cette capacit√© pour appliquer correctement les √©tiquettes que nous avons pour chaque mot aux *tokens* dans des t√¢ches comme la reconnaissance d'entit√©s nomm√©es et le POS (*Part-of-speech*). Nous pouvons √©galement l'utiliser pour masquer tous les *tokens* provenant du m√™me mot dans la mod√©lisation du langage masqu√© (une technique appel√©e _whole word masking_).



La notion de ce qu'est un mot est compliqu√©e. Par exemple, est-ce que ¬´ I'll ¬ª (contraction de ¬´ I will ¬ª) compte pour un ou deux mots ? Cela d√©pend en fait du *tokenizer* et de l'op√©ration de pr√©tok√©nisation qu'il applique. Certains *tokenizer* se contentent de s√©parer les espaces et consid√®rent donc qu'il s'agit d'un seul mot. D'autres utilisent la ponctuation en plus des espaces et consid√®rent donc qu'il s'agit de deux mots.

<Tip>

‚úèÔ∏è **Essayez !** Cr√©ez un *tokenizer* √† partir des <i>checkpoints</i> `bert-base-cased` et `roberta-base` et tokenisez ¬´ 81s ¬ª avec. Qu'observez-vous ? Quels sont les identifiants des mots ?

</Tip>

De m√™me, il existe une m√©thode `sentence_ids()` que nous pouvons utiliser pour associer un *token* √† la phrase dont il provient (bien que dans ce cas, le `token_type_ids` retourn√© par le *tokenizer* peut nous donner la m√™me information).

Enfin, nous pouvons faire correspondre n'importe quel mot ou *token* aux caract√®res du texte d'origine (et vice versa) gr√¢ce aux m√©thodes `word_to_chars()` ou `token_to_chars()` et `char_to_word()` ou `char_to_token()`. Par exemple, la m√©thode `word_ids()` nous a dit que `##yl` fait partie du mot √† l'indice 3, mais de quel mot s'agit-il dans la phrase ? Nous pouvons le d√©couvrir comme ceci :

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

Comme nous l'avons mentionn√© pr√©c√©demment, tout ceci est rendu possible par le fait que le *tokenizer* rapide garde la trace de la partie du texte d'o√π provient chaque *token* dans une liste d'*offsets*. Pour illustrer leur utilisation, nous allons maintenant vous montrer comment reproduire manuellement les r√©sultats du pipeline `token-classification`.

<Tip>

‚úèÔ∏è **Essayez !** R√©digez votre propre texte et voyez si vous pouvez comprendre quels *tokens* sont associ√©s √† l'identifiant du mot et comment extraire les √©tendues de caract√®res pour un seul mot. Pour obtenir des points bonus, essayez d'utiliser deux phrases en entr√©e et voyez si les identifiants ont un sens pour vous.

</Tip>

## A l'int√©rieur du pipeline `token-classification`

Dans le [chapitre 1](/course/fr/chapter1), nous avons eu un premier aper√ßu de la NER (o√π la t√¢che est d'identifier les parties du texte qui correspondent √† des entit√©s telles que des personnes, des lieux ou des organisations) avec la fonction `pipeline()` de ü§ó *Transformers*. Puis, dans le [chapitre 2](/course/fr/chapter2), nous avons vu comment un pipeline regroupe les trois √©tapes n√©cessaires pour obtenir les pr√©dictions √† partir d'un texte brut : la tokenisation, le passage des entr√©es dans le mod√®le et le post-traitement. Les deux premi√®res √©tapes du pipeline de `token-classification` sont les m√™mes que dans tout autre pipeline mais le post-traitement est un peu plus complexe. Voyons comment !

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### Obtenir les r√©sultats de base avec le pipeline

Tout d'abord, prenons un pipeline de classification de *tokens* afin d'obtenir des r√©sultats √† comparer manuellement. Le mod√®le utilis√© par d√©faut est [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english). Il effectue une NER sur les phrases :

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Le mod√®le a correctement identifi√© chaque *token* g√©n√©r√© par ¬´ Sylvain ¬ª comme une personne, chaque *token* g√©n√©r√© par ¬´ Hugging Face ¬ª comme une organisation, et le *token* ¬´ Brooklyn ¬ª comme un lieu. Nous pouvons √©galement demander au pipeline de regrouper les *tokens* qui correspondent √† la m√™me entit√© :

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

La propri√©t√© `aggregation_strategy` choisie va changer les scores calcul√©s pour chaque entit√© group√©e. Avec `"simple"` le score est juste la moyenne des scores de chaque *token* dans l'entit√© donn√©e. Par exemple, le score de ¬´ Sylvain ¬ª est la moyenne des scores que nous avons vu dans l'exemple pr√©c√©dent pour les tokens `S`, `##yl`, `##va`, et `##in`. D'autres strat√©gies sont disponibles :

- `"first"`, o√π le score de chaque entit√© est le score du premier *token* de cette entit√© (donc pour ¬´ Sylvain ¬ª ce serait 0.993828, le score du token `S`)
- `"max"`, o√π le score de chaque entit√© est le score maximal des *tokens* de cette entit√© (ainsi, pour ¬´ Hugging Face ¬ª, le score de ¬´ Face ¬ª serait de 0,98879766).
- `"average"`, o√π le score de chaque entit√© est la moyenne des scores des mots qui composent cette entit√© (ainsi, pour ¬´ Sylvain ¬ª, il n'y aurait pas de diff√©rence avec la strat√©gie `"simple"`, mais "Hugging Face" aurait un score de 0,9819, la moyenne des scores de ¬´ Hugging ¬ª, 0,975, et ¬´ Face ¬ª, 0,98879).

Voyons maintenant comment obtenir ces r√©sultats sans utiliser la fonction `pipeline()` !

### Des entr√©es aux pr√©dictions

{#if fw === 'pt'}

D'abord, nous devons tokeniser notre entr√©e et la faire passer dans le mod√®le. Cela se fait exactement comme dans le [chapitre 2](/course/fr/chapter2). Nous instancions le *tokenizer* et le mod√®le en utilisant les classes `TFAutoXxx` et les utilisons ensuite dans notre exemple :

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

Puisque nous utilisons `AutoModelForTokenClassification`, nous obtenons un ensemble de logits pour chaque *token* dans la s√©quence d'entr√©e :

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

D'abord, nous devons tokeniser notre entr√©e et la faire passer dans le mod√®le. Cela se fait exactement comme dans le [chapitre 2](/course/fr/chapter2). Nous instancions le *tokenizer* et le mod√®le en utilisant les classes `TFAutoXxx` et les utilisons ensuite dans notre exemple :

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

Puisque nous utilisons `TFAutoModelForTokenClassification`, nous obtenons un ensemble de logits pour chaque *token* dans la s√©quence d'entr√©e :

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

Nous avons un batch avec 1 s√©quence de 19 *tokens* et le mod√®le a 9 √©tiquettes diff√©rentes. Ainsi, la sortie du mod√®le a une forme de 1 x 19 x 9. Comme pour le pipeline de classification de texte, nous utilisons une fonction softmax pour convertir ces logits en probabilit√©s et nous prenons l'argmax pour obtenir des pr√©dictions (notez que nous pouvons prendre l'argmax sur les logits car la fonction softmax ne change pas l'ordre) :

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

L'attribut `model.config.id2label` contient la correspondance entre les index et les √©tiquettes que nous pouvons utiliser pour donner un sens aux pr√©dictions :

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

Comme nous l'avons vu pr√©c√©demment, il y a 9 √©tiquettes : `O` est le label pour les *tokens* qui ne sont dans aucune entit√© nomm√©e (il signifie *outside* (en dehors)) et nous avons ensuite deux labels pour chaque type d'entit√© (divers, personne, organisation et lieu). L'√©tiquette `B-XXX` indique que le *token* est au d√©but d'une entit√© `XXX` et l'√©tiquette `I-XXX` indique que le *token* est √† l'int√©rieur de l'entit√© `XXX`. Par exemple, dans l'exemple actuel, nous nous attendons √† ce que notre mod√®le classe le *token* `S` comme `B-PER` (d√©but d'une entit√© personne) et les *tokens* `##yl`, `##va` et `##in` comme `I-PER` (√† l'int√©rieur d'une entit√© personne). 

Vous pourriez penser que le mod√®le s'est tromp√© ici car il a attribu√© l'√©tiquette `I-PER` √† ces quatre *tokens* mais ce n'est pas tout √† fait vrai. Il existe en fait deux formats pour ces √©tiquettes `B-` et `I-` : *IOB1* et *IOB2*. Le format IOB2 (en rose ci-dessous) est celui que nous avons introduit alors que dans le format IOB1 (en bleu), les √©tiquettes commen√ßant par `B-` ne sont jamais utilis√©es que pour s√©parer deux entit√©s adjacentes du m√™me type. Le mod√®le que nous utilisons a √©t√© *finetun√©* sur un jeu de donn√©es utilisant ce format, c'est pourquoi il attribue le label `I-PER` au *token* `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

Nous sommes √† pr√©sent pr√™ts √† reproduire (presque enti√®rement) les r√©sultats du premier pipeline. Nous pouvons simplement r√©cup√©rer le score et le label de chaque *token* qui n'a pas √©t√© class√© comme `O` :

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

C'est tr√®s similaire √† ce que nous avions avant, √† une exception pr√®s : le pipeline nous a aussi donn√© des informations sur le `d√©but` et la `fin` de chaque entit√© dans la phrase originale. C'est l√† que notre *offset mapping* va entrer en jeu. Pour obtenir les *offsets*, il suffit de d√©finir `return_offsets_mapping=True` lorsque nous appliquons le *tokenizer* √† nos entr√©es :

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

Chaque *tuple* est l'√©tendue de texte correspondant √† chaque *token* o√π `(0, 0)` est r√©serv√© aux *tokens* sp√©ciaux. Nous avons vu pr√©c√©demment que le *token* √† l'index 5 est `##yl`, qui a `(12, 14)` comme *offsets* ici. Si on prend la tranche correspondante dans notre exemple :


```py
example[12:14]
```

nous obtenons le bon espace de texte sans le `##` :

```python out
yl
```

En utilisant cela, nous pouvons maintenant compl√©ter les r√©sultats pr√©c√©dents :

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

C'est la m√™me chose que ce que nous avons obtenu avec le premier pipeline !

### Regroupement des entit√©s

L'utilisation des *offsets* pour d√©terminer les cl√©s de d√©but et de fin pour chaque entit√© est pratique mais cette information n'est pas strictement n√©cessaire. Cependant, lorsque nous voulons regrouper les entit√©s, les *offsets* nous √©pargnent un batch de code compliqu√©. Par exemple, si nous voulions regrouper les *tokens* `Hu`, `##gging`, et `Face`, nous pourrions √©tablir des r√®gles sp√©ciales disant que les deux premiers devraient √™tre attach√©s tout en enlevant le `##`, et le `Face` devrait √™tre ajout√© avec un espace puisqu'il ne commence pas par `##` mais cela ne fonctionnerait que pour ce type particulier de *tokenizer*. Il faudrait √©crire un autre ensemble de r√®gles pour un *tokenizer* de type SentencePiece ou *Byte-Pair-Encoding* (voir plus loin dans ce chapitre).

Avec les *offsets*, tout ce code personnalis√© dispara√Æt : il suffit de prendre l'intervalle du texte original qui commence par le premier *token* et se termine par le dernier *token*. Ainsi, dans le cas des *tokens* `Hu`, `##gging`, et `Face`, nous devrions commencer au caract√®re 33 (le d√©but de `Hu`) et finir avant le caract√®re 45 (la fin de `Face`) :

```py
example[33:45]
```

```python out
Hugging Face
```

Pour √©crire le code qui post-traite les pr√©dictions tout en regroupant les entit√©s, nous regrouperons les entit√©s qui sont cons√©cutives et √©tiquet√©es avec `I-XXX`, √† l'exception de la premi√®re, qui peut √™tre √©tiquet√©e comme `B-XXX` ou `I-XXX` (ainsi, nous arr√™tons de regrouper une entit√© lorsque nous obtenons un `O`, un nouveau type d'entit√©, ou un `B-XXX` qui nous indique qu'une entit√© du m√™me type commence) :

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Enlever le B- ou le I-
        label = label[2:]
        start, _ = offsets[idx]

        # R√©cup√©rer tous les tokens √©tiquet√©s avec I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # Le score est la moyenne de tous les scores des tokens dans cette entit√© group√©e
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

Et nous obtenons les m√™mes r√©sultats qu'avec notre deuxi√®me pipeline !

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Un autre exemple de t√¢che o√π ces *offsets* sont extr√™mement utiles est la r√©ponse aux questions. Plonger dans ce pipeline, ce que nous ferons dans la section suivante, nous permettra de jeter un coup d'≈ìil √† une derni√®re caract√©ristique des *tokenizers* de la biblioth√®que ü§ó *Transformers* : la gestion des *tokens* qui d√©bordent lorsque nous tronquons une entr√©e √† une longueur donn√©e.
