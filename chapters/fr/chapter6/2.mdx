# EntraÃ®ner un nouveau <i>tokenizer</i> Ã  partir d'un ancien

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section2.ipynb"},
]} />

Si un modÃ¨le de langue n'est pas disponible dans la langue qui vous intÃ©resse ou si votre corpus est trÃ¨s diffÃ©rent de celui sur lequel votre modÃ¨le de langue a Ã©tÃ© entraÃ®nÃ©, vous voudrez trÃ¨s probablement rÃ©entraÃ®ner le modÃ¨le Ã  partir de zÃ©ro en utilisant un *tokenizer* adaptÃ© Ã  vos donnÃ©es. Pour ce faire, vous devrez entraÃ®ner un nouveau *tokenizer* sur votre jeu de donnÃ©es. Mais qu'est-ce que cela signifie exactement ? Lorsque nous avons examinÃ© pour la premiÃ¨re fois les *tokenizers* dans le [chapitre 2](/course/fr/chapter2), nous avons vu que la plupart des *transformers* utilisent un _algorithme de tokenisation en sous-mots_. Pour identifier les sous-mots qui sont intÃ©ressants et qui apparaissent le plus frÃ©quemment dans un corpus donnÃ©, le *tokenizer* doit examiner attentivement tous les textes du corpus. C'est un processus que nous appelons *entraÃ®nement*. Les rÃ¨gles exactes qui rÃ©gissent cet apprentissage dÃ©pendent du type de *tokenizer* utilisÃ©. Nous passerons en revue les trois principaux algorithmes plus loin dans ce chapitre.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

âš ï¸ EntraÃ®ner un *tokenizer* n'est pas la mÃªme chose qu'entraÃ®ner un modÃ¨le ! L'entraÃ®nement du modÃ¨le utilise la descente de gradient stochastique pour rÃ©duire un peu plus la perte Ã  chaque batch. Il est par nature alÃ©atoire (ce qui signifie que vous devez dÃ©finir des graines pour obtenir les mÃªmes rÃ©sultats lorsque vous effectuez deux fois le mÃªme entraÃ®nement). EntraÃ®ner un *tokenizer* est un processus statistique qui identifie les meilleurs sous-mots Ã  choisir pour un corpus donnÃ©. Les rÃ¨gles exactes utilisÃ©es pour les choisir dÃ©pendent de l'algorithme de tokÃ©nisation. Le processus est dÃ©terministe, ce qui signifie que vous obtenez toujours les mÃªmes rÃ©sultats lorsque vous vous entraÃ®nez avec le mÃªme algorithme sur le mÃªme corpus.


</Tip>

## Assemblage d'un corpus

Il y a une API trÃ¨s simple dans ğŸ¤— *Transformers* que vous pouvez utiliser pour entraÃ®ner un nouveau *tokenizer* avec les mÃªmes caractÃ©ristiques qu'un dÃ©jÃ  existant : `AutoTokenizer.train_new_from_iterator()`. Pour illustrer cela, disons que nous voulons entraÃ®ner GPT-2 Ã  partir de zÃ©ro mais dans une langue autre que l'anglais. Notre premiÃ¨re tÃ¢che est de rassembler des batchs de donnÃ©es dans cette langue dans un corpus d'entraÃ®nement. Pour avoir des exemples que tout le monde puisse comprendre, nous n'utiliserons pas ici une langue comme le russe ou le chinois mais plutÃ´t une langue anglaise spÃ©cialisÃ©e : le langage Python.

La bibliothÃ¨que [ğŸ¤— *Datasets*](https://github.com/huggingface/datasets) peut nous aider Ã  assembler un corpus de code source Python. Nous allons utiliser la fonction habituelle `load_dataset()` pour tÃ©lÃ©charger et mettre en cache le jeu de donnÃ©es [CodeSearchNet](https://huggingface.co/datasets/code_search_net). Ce jeu de donnÃ©es a Ã©tÃ© crÃ©Ã© pour le [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) et contient des millions de fonctions provenant de bibliothÃ¨ques open source sur GitHub dans plusieurs langages de programmation. Ici, nous allons charger la partie Python de ce jeu de donnÃ©es :

```py
from datasets import load_dataset

# Cela peut prendre quelques minutes alors prenez un thÃ© ou un cafÃ© pendant que vous patientez !
raw_datasets = load_dataset("code_search_net", "python")
```

Nous pouvons jeter un coup d'Å“il au jeu d'entraÃ®nement pour voir quelles sont les colonnes auxquelles nous avons accÃ¨s :

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

Nous pouvons voir que le jeu de donnÃ©es sÃ©pare les chaÃ®nes de documents du code et suggÃ¨re une tokenization des deux. Ici, nous utiliserons simplement la colonne `whole_func_string` pour entraÃ®ner notre *tokenizer*. Nous pouvons regarder un exemple de la faÃ§on suivante :

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

qui nous affiche ce qui suit :

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

La premiÃ¨re chose Ã  faire est de transformer le jeu de donnÃ©es en un _itÃ©rateur_ de listes de textes. Par exemple, une liste de listes de textes. L'utilisation de listes de textes permet Ã  notre *tokenizer* d'aller plus vite (l'entraÃ®nement a alors lieu sur des batchs de textes au lieu de traiter des textes un par un). Et le fait que ce soit un itÃ©rateur permet d'Ã©viter d'avoir tout en mÃ©moire en mÃªme temps. Si votre corpus est Ã©norme, vous voudrez profiter du fait que ğŸ¤— *Datasets* ne charge pas tout en RAM mais stocke les Ã©lÃ©ments du jeu de donnÃ©es sur le disque. 

Faire ce qui suit crÃ©erait une liste de listes de 1 000 textes chacune mais chargerait tout en mÃ©moire :


```py
# Ne dÃ©commentez pas la ligne suivante Ã  moins que votre jeu de donnÃ©es soit petit !
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

En utilisant un gÃ©nÃ©rateur, nous pouvons Ã©viter que Python ne charge quoi que ce soit en mÃ©moire Ã  moins que cela soit rÃ©ellement nÃ©cessaire. Pour crÃ©er un tel gÃ©nÃ©rateur, il suffit de remplacer les crochets par des parenthÃ¨ses :

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

Cette ligne de code ne rÃ©cupÃ¨re aucun Ã©lÃ©ment du jeu de donnÃ©es. Elle crÃ©e simplement un objet que vous pouvez utiliser dans une boucle `for` Python. Les textes ne seront chargÃ©s que lorsque vous en aurez besoin (c'est-Ã -dire lorsque vous serez Ã  l'Ã©tape de la boucle `for` qui les requiert) et seulement 1 000 textes Ã  la fois. De cette faÃ§on, vous n'Ã©puiserez pas toute votre mÃ©moire, mÃªme si vous traitez un Ã©norme jeu de donnÃ©es.

Le problÃ¨me avec un objet gÃ©nÃ©rateur est qu'il ne peut Ãªtre utilisÃ© qu'une seule fois. Ainsi, au lieu que cet objet nous donne deux fois la liste des 10 premiers chiffres :


```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

on les reÃ§oit une fois et ensuite une liste vide :

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

C'est pourquoi nous dÃ©finissons une fonction qui renvoie un gÃ©nÃ©rateur Ã  la place :

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

Vous pouvez Ã©galement dÃ©finir votre gÃ©nÃ©rateur Ã  l'intÃ©rieur d'une boucle `for` en utilisant l'instruction `yield` :

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

qui produit exactement le mÃªme gÃ©nÃ©rateur que prÃ©cÃ©demment mais  permet d'utiliser une logique plus complexe que celle que vous pouvez utiliser dans une comprÃ©hension de liste.


## EntraÃ®nement d'un nouveau <i>tokenizer</i>

Maintenant que nous avons notre corpus sous la forme d'un itÃ©rateur de batchs de textes, nous sommes prÃªts Ã  entraÃ®ner un nouveau *tokenizer*. Pour ce faire, nous devons d'abord charger le *tokenizer* que nous voulons coupler avec notre modÃ¨le (ici, le GPT-2) :


```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

MÃªme si nous allons entraÃ®ner un nouveau *tokenizer*, c'est une bonne idÃ©e de faire Ã§a pour Ã©viter de partir entiÃ¨rement de zÃ©ro. De cette faÃ§on, nous n'aurons pas Ã  spÃ©cifier l'algorithme de tokÃ©nisation ou les jetons spÃ©ciaux que nous voulons utiliser. Notre nouveau *tokenizer* sera exactement le mÃªme que celui du GPT-2. La seule chose qui changera sera le vocabulaire qui sera dÃ©terminÃ© lors de l'entraÃ®nement sur notre corpus.

Voyons d'abord comment ce *tokenizer* traiterait un exemple de fonction :


```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two',
 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '."', '""', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

Ce *tokenizer* possÃ¨de quelques symboles spÃ©ciaux, comme `Ä ` et `ÄŠ`, qui dÃ©signent respectivement les espaces et les retours Ã  la ligne. Comme on peut le voir, ce n'est pas trÃ¨s efficace. Le *tokenizer* renvoie des jetons individuels pour chaque espace alors qu'il pourrait regrouper ceux des indentations (puisquâ€™avoir des ensembles de quatre ou huit espaces est trÃ¨s courant dans du code). Il divise Ã©galement le nom de la fonction de faÃ§on un peu bizarre car pas habituÃ© Ã  voir des mots avec le caractÃ¨re `_`.

EntraÃ®nons un nouveau *tokenizer* et voyons s'il rÃ©sout ces problÃ¨mes. Pour cela, nous allons utiliser la mÃ©thode `train_new_from_iterator()` :


```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

Cette commande peut prendre un peu de temps si votre corpus est trÃ¨s grand. Pour ce jeu de donnÃ©es de 1,6 Go de textes, elle est trÃ¨s rapide (1 minute 16 secondes sur un CPU AMD Ryzen 9 3900X avec 12 cÅ“urs).

Notez que `AutoTokenizer.train_new_from_iterator()` ne fonctionne que si le *tokenizer* que vous utilisez est un *tokenizer* Â« rapide Â». Comme vous le verrez dans la section suivante, la bibliothÃ¨que ğŸ¤— *Transformers* contient deux types de *tokenizers* : certains sont Ã©crits en pur Python et d'autres (les rapides) sont soutenus par la bibliothÃ¨que ğŸ¤— *Tokenizers* qui est Ã©crite dans le langage [Rust](https://www.rust-lang.org). Python est le langage le plus souvent utilisÃ© pour les applications de science des donnÃ©es et d'apprentissage profond, mais lorsque quelque chose doit Ãªtre parallÃ©lisÃ© pour Ãªtre rapide, il faut que cela soit Ã©crit dans un autre langage. Par exemple, les multiplications matricielles qui sont au cÅ“ur du calcul du modÃ¨le sont Ã©crites en CUDA, une bibliothÃ¨que en C optimisÃ©e pour les GPUs.

EntraÃ®ner un tout nouveau *tokenizer* en Python pur est atrocement lent, c'est pourquoi nous avons dÃ©veloppÃ© la bibliothÃ¨que ğŸ¤— *Tokenizers*. Notez que, tout comme vous n'avez pas eu Ã  apprendre le langage CUDA pour pouvoir exÃ©cuter votre modÃ¨le sur un batch d'entrÃ©es sur un GPU, vous n'aurez pas besoin d'apprendre Rust pour utiliser un *tokenizer* rapide. La bibliothÃ¨que ğŸ¤— *Tokenizers* fournit des liaisons Python pour de nombreuses mÃ©thodes qui appellent en interne un morceau de code en Rust. Par exemple, pour parallÃ©liser l'entraÃ®nement de votre nouveau *tokenizer* ou, comme nous l'avons vu dans le [Chapitre 3](/course/fr/chapter3), la tokenisation d'un lot d'entrÃ©es.

La plupart des *transformers* ont un *tokenizer* rapide de disponible. Il y a quelques exceptions que vous pouvez vÃ©rifier [ici](https://huggingface.co/transformers/#supported-frameworks). S'il est disponible, l'API `AutoTokenizer` sÃ©lectionne toujours pour vous le *tokenizer* rapide. Dans la prochaine section, nous allons jeter un coup d'oeil Ã  certaines des autres caractÃ©ristiques spÃ©ciales des *tokenizers* rapides, qui seront trÃ¨s utiles pour des tÃ¢ches comme la classification de *tokens* et la rÃ©ponse aux questions. Mais avant cela, essayons notre tout nouveau *tokenizer* sur l'exemple prÃ©cÃ©dent :

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `',
 'a', '`', 'Ä and', 'Ä `', 'b', '`."""', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

Ici, nous voyons Ã  nouveau les symboles spÃ©ciaux `Ä ` et `ÄŠ` qui indiquent les espaces et les retours Ã  la ligne. Nous pouvons Ã©galement voir que notre *tokenizer* a appris certains *tokens* qui sont trÃ¨s spÃ©cifiques Ã  un corpus de fonctions Python. Par exemple, il y a un token `ÄŠÄ Ä Ä ` qui reprÃ©sente une indentation et un *token* `Ä """` qui reprÃ©sente les trois guillemets qui commencent une *docstring*. Le *tokenizer* divise Ã©galement correctement le nom de la fonction sur `_`. Il s'agit d'une reprÃ©sentation assez compacte. En comparaison, l'utilisation du *tokenizer* en anglais Â« simple Â» sur le mÃªme exemple nous donnera une phrase plus longue :

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

Prenons un autre exemple :

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',',
 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',
 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']
```

En plus du *token* correspondant Ã  une indentation, on peut Ã©galement voir ici un *token* pour une double indentation : `ÄŠÄ Ä Ä Ä Ä Ä Ä Ä Ä `. Les mots spÃ©ciaux de Python comme `class`, `init`, `call`, `self`, et `return` sont tous tokenizÃ©s comme un seul *token*. Nous pouvons voir qu'en plus de sÃ©parer sur `_` et `.` le tokenizer sÃ©pare correctement mÃªme les noms en minuscules. Par exemple `LinearLayer` est tokenisÃ© comme `["Ä Linear", "Layer"]`.

## Sauvegarde du <i>tokenizer</i>

Pour Ãªtre sÃ»r de pouvoir l'utiliser plus tard, nous devons sauvegarder notre nouveau *tokenizer*. Comme pour les modÃ¨les, ceci est fait avec la mÃ©thode `save_pretrained()` :


```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

Cela crÃ©era un nouveau dossier nommÃ© *code-search-net-tokenizer* contenant tous les fichiers dont le *tokenizer* a besoin pour Ãªtre rechargÃ©. Si vous souhaitez partager ce *tokenizer* avec vos collÃ¨gues et amis, vous pouvez le tÃ©lÃ©charger sur le *Hub* en vous connectant Ã  votre compte. Si vous travaillez dans un *notebook*, il existe une fonction pratique pour vous aider Ã  le faire :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Cela affichera un *widget* oÃ¹ vous pourrez entrer vos identifiants de connexion Ã  Hugging Face. Si vous ne travaillez pas sur un ordinateur portable, tapez simplement la ligne suivante dans votre terminal :

```bash
huggingface-cli login
```

Une fois connectÃ©, vous pouvez pousser votre *tokenizer* en exÃ©cutant la commande suivante :

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

Cela crÃ©era un nouveau dÃ©pÃ´t dans votre espace avec le nom `code-search-net-tokenizer` contenant le fichier *tokenizer*. Vous pouvez ensuite charger le *tokenizer* de n'importe oÃ¹ avec la mÃ©thode `from_pretrained()` :

```py
# Remplacez "huggingface-course" ci-dessous par votre espace rÃ©el pour utiliser votre propre tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

Vous Ãªtes maintenant prÃªt Ã  entraÃ®ner un modÃ¨le de langue Ã  partir de zÃ©ro et Ã  le *finetuner* sur votre tÃ¢che ! Nous verrons cela dans le [chapitre 7](/course/fr/chapter7), mais d'abord, dans le reste de ce chapitre, nous allons examiner de plus prÃ¨s les *tokenizers* rapides et explorer en dÃ©tail ce qui se passe lorsque nous appelons la mÃ©thode `train_new_from_iterator()`.