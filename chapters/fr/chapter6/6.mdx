# Tok√©nisation <i>WordPiece</i>

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section6.ipynb"},
]} />

*WordPiece* est l'algorithme de tok√©nisation d√©velopp√© par Google pour pr√©tra√Æner BERT. Il a depuis √©t√© r√©utilis√© dans un grand nombre de mod√®les de *transformers* bas√©s sur BERT tels que DistilBERT, MobileBERT, Funnel Transformers et MPNET. Il est tr√®s similaire au BPE en termes d'entra√Ænement mais la tokenisation r√©elle est effectu√©e diff√©remment.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

üí° Cette section couvre le <i>WordPiece</i> en profondeur, allant jusqu'√† montrer une impl√©mentation compl√®te. Vous pouvez passer directement √† la fin si vous souhaitez simplement avoir un aper√ßu g√©n√©ral de l'algorithme de tok√©nisation.

</Tip>

## Algorithme d'entra√Ænement

<Tip warning={true}>

‚ö†Ô∏è Google n'a jamais mis en ligne son impl√©mentation de l'algorithme d'entra√Ænement du <i>WordPiece</i>. Ce qui suit est donc notre meilleure estimation bas√©e sur la litt√©rature publi√©e. Il se peut qu'elle ne soit pas exacte √† 100 %.

</Tip>

Comme le BPE, *WordPiece* part d'un petit vocabulaire comprenant les *tokens* sp√©ciaux utilis√©s par le mod√®le et l'alphabet initial. Puisqu'il identifie les sous-mots en ajoutant un pr√©fixe (comme `##` pour BERT), chaque mot est initialement d√©coup√© en ajoutant ce pr√©fixe √† tous les caract√®res du mot. Ainsi par exemple, `"word"` est divis√© comme ceci :

```
w ##o ##r ##d
```

Ainsi, l'alphabet initial contient tous les caract√®res pr√©sents au d√©but d'un mot et les caract√®res pr√©sents √† l'int√©rieur d'un mot pr√©c√©d√© du pr√©fixe de *WordPiece*.

Ensuite, toujours comme le BPE, *WordPiece* apprend des r√®gles de fusion. La principale diff√©rence r√©side dans la mani√®re dont la paire √† fusionner est s√©lectionn√©e. Au lieu de s√©lectionner la paire la plus fr√©quente, *WordPiece* calcule un score pour chaque paire en utilisant la formule suivante :

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

En divisant la fr√©quence de la paire par le produit des fr√©quences de chacune de ses parties, l'algorithme donne la priorit√© √† la fusion des paires dont les parties individuelles sont moins fr√©quentes dans le vocabulaire. Par exemple, il ne fusionnera pas n√©cessairement `("un", "##able")` m√™me si cette paire appara√Æt tr√®s fr√©quemment dans le vocabulaire car les deux paires `"un"`" et `"##able"` appara√Ætront probablement chacune dans un batch d'autres mots et auront une fr√©quence √©lev√©e. En revanche, une paire comme `("hu", "##gging")` sera probablement fusionn√©e plus rapidement (en supposant que le mot `"hugging"` apparaisse souvent dans le vocabulaire) puisque `"hu"` et `"##gging"` sont probablement moins fr√©quents individuellement.

Examinons le m√™me vocabulaire que celui utilis√© dans l'exemple d'entra√Ænement du BPE :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

Les divisions ici seront :

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

Si on oublie les *tokens* sp√©ciaux pour l'instant, le vocabulaire initial sera donc `["b", "h", "p", "##g", "##n", "##s", "##u"]`. La paire la plus fr√©quente est `("##u", "##g")` (pr√©sente 20 fois), mais la fr√©quence individuelle de `"##u"` est tr√®s √©lev√©e, donc son score n'est pas le plus √©lev√© (il est de 1 / 36). Toutes les paires avec un `"##u"` ont en fait le m√™me score (1 / 36). Ainsi le meilleur score va √† la paire `("##g", "##s")` qui est la seule sans un `"##u"` avec un score 1 / 20. Et la premi√®re fusion apprise est `("##g", "##s") -> ("##gs")`.

Notez que lorsque nous fusionnons, nous enlevons le `##` entre les deux *tokens*, donc nous ajoutons `"##gs"` au vocabulaire et appliquons la fusion dans les mots du corpus :

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

√Ä ce stade, `" ##u "` est dans toutes les paires possibles, donc elles finissent toutes par avoir le m√™me score. Disons que dans ce cas, la premi√®re paire est fusionn√©e, donc `("h", "##u") -> "hu"`. Cela nous am√®ne √† :

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

Ensuite, le meilleur score suivant est partag√© par `("hu", "##g")` et `("hu", "##gs")` (avec 1/15, compar√© √† 1/21 pour toutes les autres paires). Ainsi la premi√®re paire avec le plus grand score est fusionn√©e :

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

et nous continuons ainsi jusqu'√† ce que nous atteignions la taille de vocabulaire souhait√©e.

<Tip>

‚úèÔ∏è **A votre tour !** Quelle sera la prochaine r√®gle de fusion ?

</Tip>

## Algorithme de tokenisation

La tok√©nisation diff√®re dans *WordPiece* et BPE en ce que *WordPiece* ne sauvegarde que le vocabulaire final et non pas les r√®gles de fusion apprises. En partant du mot √† tokeniser, *WordPiece* trouve le sous-mot le plus long qui se trouve dans le vocabulaire, puis se s√©pare sur celui-ci. Par exemple, si nous utilisons le vocabulaire appris dans l'exemple ci-dessus, pour le mot `"hugs"` le plus long sous-mot en partant du d√©but qui est dans le vocabulaire est `"hug"`. Donc nous le divisons et obtenons `["hug", "##s"]`. On continue avec `"##s"`, qui est dans le vocabulaire, donc la tokenisation de `"hugs"` est `["hug", "##s"]`.

Avec BPE, nous aurions appliqu√© les fusions apprises dans l'ordre et la tok√©nisation aurait √©t√© `["hu", "##gs"]`, l'encodage est donc diff√©rent.

Comme autre exemple, voyons comment le mot `"bugs"` serait tokenis√©. `"b"` est le plus long sous-mot commen√ßant au d√©but du mot qui est dans le vocabulaire donc on le divise et on obtient `["b", "##ugs"]`. Ensuite, `"##u"` est le plus long sous-mot commen√ßant au d√©but de `"##ugs"` qui est dans le vocabulaire, donc on le s√©pare et on obtient `["b", "##u, "##gs"]`. Enfin, `"##gs"` est dans le vocabulaire, donc cette derni√®re liste est la tokenization de `"bugs"`.

Lorsque la tokenisation arrive √† un stade o√π il n'est pas possible de trouver un sous-mot dans le vocabulaire, le mot entier est tokenis√© comme inconnu. Par exemple, `"mug"` serait tokenis√© comme `["[UNK]"]`, tout comme `"bum"` (m√™me si on peut commencer par " b " et " ##u ", " ##m " ne fait pas partie du vocabulaire, et le *tokenizer* r√©sultant sera simplement `["[UNK]"]` " et non `["b", "##u", "[UNK]"]` "). C'est une autre diff√©rence avec le BPE qui classerait seulement les caract√®res individuels qui ne sont pas dans le vocabulaire comme inconnus.

<Tip>

‚úèÔ∏è **A votre tour !** Comment le mot `"pugs"` sera-t-il tokenis√© ?

</Tip>

## Impl√©mentation de <i>WordPiece</i>

Voyons maintenant une impl√©mentation de l'algorithme *WordPiece*. Comme pour le BPE, il s'agit d'un exemple p√©dagogique et vous ne pourrez pas l'utiliser sur un grand corpus.

Nous utiliserons le m√™me corpus que dans l'exemple BPE :

```python
corpus = [
    "This is the Hugging Face Course.",
    # C'est le cours d'Hugging Face.
    "This chapter is about tokenization.",
    # This chapter is about tokenization
    "This section shows several tokenizer algorithms.",
    # Cette section pr√©sente plusieurs algorithmes de *tokenizer*.
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
    # Avec un peu de chance, vous serez en mesure de comprendre comment ils sont entra√Æn√©s et g√©n√®rent des *tokens*.
]
```

Tout d'abord, nous devons pr√©tok√©niser le corpus en mots. Puisque nous r√©pliquons un *tokenizer WordPiece* (comme BERT), nous utiliserons le *tokenizer* `bert-base-cased` pour la pr√©tok√©nisation :

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Ensuite, nous calculons les fr√©quences de chaque mot dans le corpus comme nous le faisons pour la pr√©tok√©nisation :

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

Comme nous l'avons vu pr√©c√©demment, l'alphabet est l'unique ensemble compos√© de toutes les premi√®res lettres des mots, et de toutes les autres lettres qui apparaissent dans les mots pr√©fix√©s par `##` :

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

Nous ajoutons √©galement les *tokens* sp√©ciaux utilis√©s par le mod√®le au d√©but de ce vocabulaire. Dans le cas de BERT, il s'agit de la liste `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]` :

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

Ensuite, nous devons diviser chaque mot, avec toutes les lettres qui ne sont pas les premi√®res pr√©fix√©es par `##` :

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

Maintenant que nous sommes pr√™ts pour l'entra√Ænement, √©crivons une fonction qui calcule le score de chaque paire. Nous devrons l'utiliser √† chaque √©tape de l'entra√Ænement :

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

Jetons un coup d'≈ìil √† une partie de ce dictionnaire apr√®s les premi√®res divisions :

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

Maintenant, trouver la paire avec le meilleur score ne prend qu'une rapide boucle :

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

Ainsi, la premi√®re fusion √† apprendre est `('a', '##b') -> 'ab'` et nous ajoutons `'ab'` au vocabulaire :

```python
vocab.append("ab")
```

Pour continuer, nous devons appliquer cette fusion dans notre dictionnaire `splits`. √âcrivons une autre fonction pour cela :

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

Et nous pouvons regarder le r√©sultat de la premi√®re fusion :

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

Nous avons maintenant tout ce dont nous avons besoin pour boucler jusqu'√† ce que nous ayons appris toutes les fusions que nous voulons. Visons une taille de vocabulaire de 70 :

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

Nous pouvons ensuite examiner le vocabulaire g√©n√©r√© :

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

Comme nous pouvons le voir, compar√© √† BPE, ce *tokenizer* apprend les parties de mots comme des *tokens* un peu plus rapidement.

<Tip>

üí° Utiliser `train_new_from_iterator()` sur le m√™me corpus ne donnera pas exactement le m√™me vocabulaire. C'est parce que la biblioth√®que ü§ó *Tokenizers* n'impl√©mente pas *WordPiece* pour l'entra√Ænement (puisque nous ne sommes pas compl√®tement s√ªrs de son fonctionnement interne), mais utilise le BPE √† la place.

</Tip>

Pour tokeniser un nouveau texte, on le pr√©tokenise, on le divise, puis on applique l'algorithme de tokenisation sur chaque mot. En d'autres termes, nous recherchons le plus grand sous-mot commen√ßant au d√©but du premier mot et le divisons. Puis nous r√©p√©tons le processus sur la deuxi√®me partie et ainsi de suite pour le reste de ce mot et les mots suivants dans le texte :

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

Testons-le sur un mot qui fait partie du vocabulaire, et un autre qui n'en fait pas partie :

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

Maintenant, √©crivons une fonction qui permet de tokeniser un texte :

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

On peut l'essayer sur n'importe quel texte :

```python
tokenize("This is the Hugging Face Course!")  # C'est le cours d'Hugging Face
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

C'est tout pour l'algorithme *WordPiece* ! Maintenant, jetons un coup d'oeil √† *Unigram*.
