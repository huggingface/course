<!-- DISABLE-FRONTMATTER-SECTIONS -->

# Quiz de fin de chapitre

Testons ce que vous avez appris dans ce chapitre !

### 1. Quand devez-vous entra√Æner un nouveau <i>tokenizer</i> ?

<Question
	choices={[
		{
			text: "Lorsque votre jeu de donn√©es est similaire √† celui utilis√© par un mod√®le pr√©-entra√Æn√© existant et que vous voulez pr√©-entra√Æner un nouveau mod√®le",
			explain: "Dans ce cas, pour √©conomiser du temps et des ressources de calcul, il est pr√©f√©rable d'utiliser le m√™me <i>tokenizer</i> que le mod√®le pr√©-entra√Æn√© et de <i>finetuner</i> ce mod√®le √† la place."
		},
		{
			text: "Lorsque votre jeu de donn√©es est similaire √† celui utilis√© par un mod√®le pr√©-entra√Æn√© existant et que vous souhaitez <i>finetuner</i> un nouveau mod√®le en utilisant ce mod√®le pr√©-entra√Æn√©.",
			explain: "Pour <i>finetuner</i> un mod√®le √† partir d'un mod√®le pr√©-entra√Æn√©, vous devez toujours utiliser le m√™me <i>tokenizer</i>."
		},
		{
			text: "Lorsque votre jeu de donn√©es est diff√©rent de celui utilis√© par un mod√®le pr√©-entra√Æn√© existant et que vous souhaitez pr√©-entra√Æner un nouveau mod√®le.",
			explain: "Dans ce cas, il n'y a aucun avantage √† utiliser le m√™me <i>tokenizer</i>.",
            correct: true
		},
        {
			text: "Lorsque votre jeu de donn√©es est diff√©rent de celui utilis√© par un mod√®le pr√©-entra√Æn√© existant mais que vous souhaitez <i>finetuner</i> un nouveau mod√®le en utilisant ce mod√®le pr√©-entra√Æn√©.",
			explain: "Pour <i>finetuner</i> un mod√®le √† partir d'un mod√®le pr√©-entra√Æn√©, vous devez toujours utiliser le m√™me <i>tokenizer</i>."
		}
	]}
/>

### 2. Quel est l'avantage d'utiliser un g√©n√©rateur de listes par rapport √† une liste de listes lors de l'utilisation de <code>train_new_from_iterator()</code> ?

<Question
	choices={[
		{
			text: "C'est le seul type que la m√©thode <code>train_new_from_iterator()</code> accepte.",
			explain: "Une liste de listes de textes est un type particulier de g√©n√©rateur de listes de textes, la m√©thode l'acceptera donc aussi. Essayez √† nouveau !"
		},
		{
			text: "Vous √©viterez de charger l'ensemble des donn√©es en m√©moire en une seule fois.",
			explain: "Chaque batch de textes sera lib√©r√© de la m√©moire lorsque vous it√©rerez et le gain sera particuli√®rement visible si vous utilisez des ü§ó <i>Datasets</i> pour stocker vos textes.",
			correct: true
		},
		{
			text: "Cela permettra √† la biblioth√®que ü§ó <i>Tokenizers</i> d'utiliser le multitraitement.",
			explain: "Il utilisera le multiprocesseur dans tous les cas."
		},
        {
			text: "Le <i>tokenizer</i> que vous entra√Ænez g√©n√©rera de meilleurs textes.",
			explain: "Le <i>tokenizer</i> ne g√©n√®re pas de texte. Vous le confondez avec un mod√®le de langage ?"
		}
	]}
/>

### 3. Quels sont les avantages d'utiliser un <i>tokenizer</i> ¬´ rapide ¬ª ?

<Question
	choices={[
		{
			text: "Il peut traiter les entr√©es plus rapidement qu'un <i>tokenizer</i> lent lorsque vous faites des batchs d'entr√©es.",
			explain: "Gr√¢ce au parall√©lisme impl√©ment√© dans Rust, il sera plus rapide sur les batchs d'entr√©es. Quel autre avantage pouvez-vous imaginer ?",
			correct: true
		},
		{
			text: "Les <i>tokenizers</i> rapides sont toujours plus rapides que leurs homologues lents.",
			explain: "Un <i>tokenizer</i> rapide peut en fait √™tre plus lent si vous ne lui donnez qu'un seul ou tr√®s peu de textes, car il ne peut pas utiliser le parall√©lisme."
		},
		{
			text: "Il peut appliquer le <i>padding</i> et la troncature.",
			explain: "C'est vrai, mais les <i>tokenizers</i> lents le font aussi."
		},
        {
			text: "Il poss√®de des fonctionnalit√©s suppl√©mentaires qui vous permettent d'associer les <i>tokens</i> √† l'extrait de texte qui les a cr√©√©s.",
			explain: "En effet, c'est ce qu'on appelle des correspondances d'<i>offset</i>. Ce n'est pas le seul avantage, cependant.",
			correct: true
		}
	]}
/>

### 4. Comment le pipeline `token-classification` g√®re-t-il les entit√©s qui s'√©tendent sur plusieurs <i>tokens</i> ?

<Question
	choices={[
		{
			text: "Les entit√©s ayant la m√™me √©tiquette sont fusionn√©es en une seule entit√©.",
			explain: "C'est un peu trop simplifier les choses. Essayez encore !"
		},
		{
			text: "Il existe une √©tiquette pour le d√©but d'une entit√© et une √©tiquette pour la suite d'une entit√©.",
			explain: " ",
			correct: true
		},
		{
			text: "Dans un mot donn√©, tant que le premier <i>token</i> porte l'√©tiquette de l'entit√©, le mot entier est consid√©r√© comme √©tiquet√© avec cette entit√©.",
			explain: "C'est une strat√©gie pour g√©rer les entit√©s. Quelles autres r√©ponses s'appliquent ici ?",
			correct: true
		},
        {
			text: "Lorsqu'un <i>token</i> a l'√©tiquette d'une entit√© donn√©e, tout autre <i>token</i> suivant ayant la m√™me √©tiquette est consid√©r√© comme faisant partie de la m√™me entit√©, √† moins qu'il ne soit √©tiquet√© comme le d√©but d'une nouvelle entit√©.",
			explain: "C'est la fa√ßon la plus courante de regrouper des entit√©s, mais ce n'est pas la seule bonne r√©ponse.",
			correct: true
		}
	]}
/>

### 5. Comment le pipeline `question-answering` g√®re-t-il les contextes longs ?

<Question
	choices={[
		{
			text: "Ce n'est pas vraiment le cas car il tronque le long contexte √† la longueur maximale accept√©e par le mod√®le.",
			explain: "Il existe une astuce que vous pouvez utiliser pour g√©rer les longs contextes. Vous en souvenez-vous ?"
		},
		{
			text: "Il divise le contexte en plusieurs parties et fait la moyenne des r√©sultats obtenus.",
			explain: "Cela n'aurait pas de sens de faire la moyenne des r√©sultats car certaines parties du contexte n'incluront pas la r√©ponse."
		},
		{
			text: "Il divise le contexte en plusieurs parties (avec chevauchement) et trouve le score maximum pour une r√©ponse dans chaque partie.",
			explain: "C'est la bonne r√©ponse !",
			correct: true
		},
        {
			text: "Il divise le contexte en plusieurs parties (sans chevauchemen par souci d'efficacit√©) et trouve le score maximum pour une r√©ponse dans chaque partie.",
			explain: "Il comprend un certain chevauchement entre les parties pour √©viter une situation o√π la r√©ponse serait divis√©e en deux parties."
		}
	]}
/>

### 6. Qu'est-ce que la normalisation ?

<Question
	choices={[
		{
			text: "C'est le nettoyage que le <i>tokenizer</i> effectue sur les textes lors des √©tapes initiales.",
			explain: "Par exemple, il peut s'agir de supprimer les accents ou les espaces, ou de mettre les entr√©es en minuscules.",
			correct: true
		},
		{
			text: "Il s'agit d'une technique d'augmentation de donn√©es qui consiste √† rendre le texte plus normal en supprimant les mots rares.",
			explain: "Essayez encore."
		},
		{
			text: "C'est l'√©tape finale du post-traitement o√π le <i>tokenizer</i> ajoute les <i>tokens</i> sp√©ciaux.",
			explain: "Cette √©tape est simplement appel√©e post-traitement."
		},
        {
			text: "C'est lorsque les ench√¢ssements sont faits avec une moyenne nulle et un √©cart-type de 1, en soustrayant la moyenne et en divisant par l'√©cart-type.",
			explain: "Ce processus est commun√©ment appel√© normalisation lorsqu'il est appliqu√© aux valeurs des pixels en vision par ordinateur, mais ce n'est pas ce que signifie la normalisation en NLP."
		}
	]}
/>

### 7. Qu'est-ce que la pr√©-tok√©nisation pour un <i>tokenizer</i> en sous-mots ?

<Question
	choices={[
		{
			text: "C'est l'√©tape qui pr√©c√®de la tok√©nisation, o√π l'augmentation des donn√©es (comme le masquage al√©atoire) est appliqu√©e.",
			explain: "Cette √©tape fait partie du pr√©traitement."
		},
		{
			text: "C'est l'√©tape avant la tok√©nisation, o√π les op√©rations de nettoyage souhait√©es sont appliqu√©es au texte.",
			explain: "C'est l'√©tape de normalisation."
		},
		{
			text: "C'est l'√©tape qui pr√©c√®de l'application du mod√®le <i>tokenizer</i>, pour diviser l'entr√©e en mots.",
			explain: "C'est la bonne r√©ponse !",
			correct: true
		},
        {
			text: "Il s'agit de l'√©tape pr√©c√©dant l'application du mod√®le <i>tokenizer</i>, qui divise l'entr√©e en <i>tokens</i>.",
			explain: "La division en <i>tokens</i> est le travail du mod√®le <i>tokenizer</i>."
		}
	]}
/>

### 8. S√©lectionnez les phrases qui s'appliquent au <i>tokenizer</i> BPE.

<Question
	choices={[
		{
			text: "BPE est un algorithme de tok√©nisation en sous-mots qui part d'un petit vocabulaire et apprend des r√®gles de fusion.",
			explain: "C'est le cas en effet !",
			correct: true
		},
		{
			text: "BPE est un algorithme de tok√©nisation en sous-mots qui part d'un grand vocabulaire et en retire progressivement les <i>tokens</i>.",
			explain: "C'est l'approche adopt√©e par un algorithme de tok√©nisation diff√©rent."
		},
		{
			text: "Un <i>tokenizer</i> BPE apprend les r√®gles de fusion en fusionnant la paire de <i>tokens</i> la plus fr√©quente.",
			explain: "C'est exact !",
			correct: true
		},
		{
			text: "Un <i>tokenizer</i> BPE apprend une r√®gle de fusion en fusionnant la paire de <i>tokens</i> qui maximise un score qui privil√©gie les paires fr√©quentes avec des parties individuelles moins fr√©quentes.",
			explain: "C'est la strat√©gie appliqu√©e par un autre algorithme de tokenization."
		},
		{
			text: "BPE tokenise les mots en sous-mots en les divisant en caract√®res, puis en appliquant les r√®gles de fusion.",
			explain: " ",
			correct: true
		},
		{
			text: "BPE tokenise les mots en sous-mots en trouvant le plus long sous-mot du vocabulaire en commen√ßant par le d√©but, puis en r√©p√©tant le processus pour le reste du texte.",
			explain: "C'est la fa√ßon de faire d'un autre algorithme de tokenization."
		},
	]}
/>

### 9. S√©lectionnez les phrases qui s'appliquent au <i>tokenizer</i> WordPiece.

<Question
	choices={[
		{
			text: "WordPiece est un algorithme de tok√©nisation en sous-mots qui part d'un petit vocabulaire et apprend des r√®gles de fusion.",
			explain: "C'est le cas en effet !",
			correct: true
		},
		{
			text: "WordPiece est un algorithme de tok√©nisation en sous-mots qui part d'un grand vocabulaire et en retire progressivement les <i>tokens</i>.",
			explain: "C'est la fa√ßon de faire d'un autre algorithme de tokenization."
		},
		{
			text: "WordPiece Les <i>tokenizer</i> apprennent les r√®gles de fusion en fusionnant la paire de <i>tokens</i> la plus fr√©quente.",
			explain: "C'est la fa√ßon de faire d'un autre algorithme de tokenization."
		},
		{
			text: "Un <i>tokenizer</i> WordPiece apprend une r√®gle de fusion en fusionnant la paire de <i>tokens</i> qui maximise un score qui privil√©gie les paires fr√©quentes avec des parties individuelles moins fr√©quentes.",
			explain: " ",
			correct: true
		},
		{
			text: "WordPiece tokenise les mots en sous-mots en trouvant la segmentation en <i>tokens</i> la plus probable, selon le mod√®le.",
			explain: "C'est la fa√ßon de faire d'un autre algorithme de tokenization."
		},
		{
			text: "WordPiece tokenise les mots en sous-mots en trouvant le plus long sous-mot du vocabulaire en commen√ßant par le d√©but, puis en r√©p√©tant le processus pour le reste du texte.",
			explain: "C'est ainsi que WordPiece proc√®de pour l'encodage.",
			correct: true
		},
	]}
/>

### 10. S√©lectionnez les phrases qui s'appliquent au <i>tokenizer</i> Unigram.

<Question
	choices={[
		{
			text: "Unigram est un algorithme de tok√©nisation en sous-mots qui part d'un petit vocabulaire et apprend des r√®gles de fusion.",
			explain: "C'est la fa√ßon de faire d'un autre algorithme de tokenization."
		},
		{
			text: "Unigram est un algorithme de tok√©nisation en sous-mots qui part d'un grand vocabulaire et en retire progressivement les <i>tokens</i>.",
			explain: " ",
			correct: true
		},
		{
			text: "Unigram adapte son vocabulaire en minimisant une perte calcul√©e sur l'ensemble du corpus.",
			explain: " ",
			correct: true
		},
		{
			text: "Unigram adapte son vocabulaire en conservant les sous-mots les plus fr√©quents.",
			explain: " "
		},
		{
			text: "Unigram segmente les mots en sous-mots en trouvant la segmentation la plus probable en <i>tokens</i>, selon le mod√®le.",
			explain: " ",
			correct: true
		},
		{
			text: "Unigram d√©compose les mots en sous-mots en les divisant en caract√®res puis en appliquant les r√®gles de fusion.",
			explain: "C'est la fa√ßon de faire d'un autre algorithme de tokenization."
		},
	]}
/>
