# Tokenisation <i>Unigram</i>

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section7.ipynb"},
]} />

L'algorithme *Unigram* est souvent utilis√© dans *SentencePiece*, qui est l'algorithme de tokenization utilis√© par des mod√®les comme ALBERT, T5, mBART, Big Bird et XLNet.

<Youtube id="TGZfZVuF9Yc"/>

<Tip>

üí° Cette section couvre *Unigram* en profondeur, allant jusqu'√† montrer une impl√©mentation compl√®te. Vous pouvez passer directement √† la fin si vous souhaitez simplement avoir un aper√ßu g√©n√©ral de l'algorithme de tok√©nisation.

</Tip>

## Algorithme d'entra√Ænement

Compar√© au BPE et *WordPiece*, *Unigram* fonctionne dans l'autre sens : il part d'un grand vocabulaire et enl√®ve des *tokens* jusqu'√† atteindre la taille de vocabulaire d√©sir√©e. Il existe plusieurs options pour construire ce vocabulaire de base. Nous pouvons par exemple prendre les sous-cha√Ænes les plus courantes dans les mots pr√©tok√©nis√©s ou appliquer le BPE sur le corpus initial avec une grande taille de vocabulaire.

√Ä chaque √©tape de l'entra√Ænement, l'algorithme *Unigram* calcule une perte sur le corpus compte tenu du vocabulaire actuel. Ensuite, pour chaque symbole du vocabulaire, l'algorithme calcule de combien la perte globale augmenterait si le symbole √©tait supprim√© et recherche les symboles qui l'augmenteraient le moins. Ces symboles ont un effet moindre sur la perte globale du corpus, ils sont donc en quelque sorte ¬´ moins n√©cessaires ¬ª et sont les meilleurs candidats √† la suppression.

Comme il s'agit d'une op√©ration tr√®s co√ªteuse, nous ne nous contentons pas de supprimer le symbole unique associ√© √† la plus faible augmentation de la perte mais le \\(p\\) pourcent des symboles associ√©s √† la plus faible augmentation de la perte. \(p\\) est un hyperparam√®tre que vous pouvez contr√¥ler, valant g√©n√©ralement 10 ou 20. Ce processus est ensuite r√©p√©t√© jusqu'√† ce que le vocabulaire ait atteint la taille souhait√©e.

Notez que nous ne supprimons jamais les caract√®res de base, afin de nous assurer que tout mot peut √™tre tokenis√©.

Tout ceci peut para√Ætre encore un peu vague. En effet, la partie principale de l'algorithme est de calculer une perte sur le corpus et de voir comment elle change lorsque nous supprimons certains *tokens* du vocabulaire mais nous n'avons pas encore expliqu√© comment le faire. Cette √©tape repose sur l'algorithme de tok√©nisation *Unigram*, nous allons donc l'aborder √† pr√©sent.

Nous allons r√©utiliser le corpus des exemples pr√©c√©dents :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5) # "c√¢lin", "carlin", "jeu de mots", "brioche", "c√¢lins"...
```

et pour cet exemple, nous prendrons toutes les sous-cha√Ænes strictes pour le vocabulaire initial :

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Algorithme de tokenisation

Un mod√®le *Unigram* est un type de mod√®le de langage qui consid√®re que chaque *token* est ind√©pendant des *tokens* qui le pr√©c√®dent. Il s'agit du mod√®le de langage le plus simple, dans le sens o√π la probabilit√© du *token* X compte tenu du contexte pr√©c√©dent est simplement la probabilit√© du *token* X. Ainsi, si nous utilisions un mod√®le de langage *Unigram* pour g√©n√©rer du texte, nous pr√©dirions toujours le *token* le plus courant.

La probabilit√© d'un *token* donn√© est sa fr√©quence (le nombre de fois que nous le trouvons) dans le corpus original, divis√©e par la somme de toutes les fr√©quences de tous les *tokens* dans le vocabulaire (pour s'assurer que la somme des probabilit√©s est √©gale √† 1). Par exemple, `"ug"` est pr√©sent dans `"hug"`, `"pug"`, et `"hugs"`. Il a donc une fr√©quence de 20 dans notre corpus.

Voici les fr√©quences de tous les sous-mots possibles dans le vocabulaire :

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

Ainsi, la somme de toutes les fr√©quences est de 210 et la probabilit√© du sous-mot `"ug"` est donc de 20/210.

<Tip>

‚úèÔ∏è **A votre tour !** Ecrivez le code permettant de calculer les fr√©quences ci-dessus et v√©rifiez que les r√©sultats affich√©s sont corrects, de m√™me que la somme totale.

</Tip>

Maintenant, pour tokeniser un mot donn√©, nous examinons toutes les segmentations possibles en *tokens* et calculons la probabilit√© de chacune d'entre elles selon le mod√®le *Unigram*. Puisque tous les *tokens* sont consid√©r√©s comme ind√©pendants, cette probabilit√© est juste le produit de la probabilit√© de chaque *token*. Par exemple, la tokenisation `["p", "u", "g"]` de `"pug"` a la probabilit√© :

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

Comparativement, la tokenization `["pu", "g"]` a la probabilit√© :

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

donc celle-l√† est beaucoup plus probable. En g√©n√©ral, les tok√©nisations comportant le moins de *tokens* possible auront la probabilit√© la plus √©lev√©e (en raison de la division par 210 r√©p√©t√©e pour chaque *token*), ce qui correspond √† ce que nous voulons intuitivement : diviser un mot en un nombre de *tokens* le plus faible possible.

La tokenisation d'un mot avec le mod√®le *Unigram* est donc la tokenisation avec la plus haute probabilit√©. Dans l'exemple de `"pug"`, voici les probabilit√©s que nous obtiendrions pour chaque segmentation possible :

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

Ainsi, `"pug"` sera tokenis√© comme `["p", "ug"]` ou `["pu", "g"]`, selon la segmentation rencontr√©e en premier (notez que dans un corpus plus large, les cas d'√©galit√© comme celui-ci seront rares).

Dans ce cas-ci, cela a √©t√© facile de trouver toutes les segmentations possibles et de calculer leurs probabilit√©s, mais en g√©n√©ral ce sera un peu plus difficile. Il existe un algorithme classique utilis√© pour cela, appel√© *algorithme de Viterbi*. Essentiellement, on peut construire un graphe pour d√©tecter les segmentations possibles d'un mot donn√© en disant qu'il existe une branche du caract√®re _a_ au caract√®re _b_ si le sous-mot de _a_ √† _b_ est dans le vocabulaire, et attribuer √† cette branche la probabilit√© du sous-mot.

Pour trouver le chemin qui va avoir le meilleur score dans ce graphe, l'algorithme de Viterbi d√©termine, pour chaque position dans le mot, la segmentation avec le meilleur score qui se termine √† cette position. Puisque nous allons du d√©but √† la fin, ce meilleur score peut √™tre trouv√© en parcourant en boucle tous les sous-mots se terminant √† la position actuelle, puis en utilisant le meilleur score de tokenization de la position √† laquelle ce sous-mot commence. Ensuite, il suffit de d√©rouler le chemin emprunt√© pour arriver √† la fin.

Prenons un exemple en utilisant notre vocabulaire et le mot `"unhug"`. Pour chaque position, les sous-mots avec les meilleurs scores se terminant l√† sont les suivants :

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

Ainsi, `"unhug"` serait tokenis√© comme `["un", "hug"]`.

<Tip>

‚úèÔ∏è **A votre tour !** D√©terminer la tokenization du mot `"huggun"` et son score.

</Tip>

## Retour √† l'entra√Ænement

Maintenant que nous avons vu comment fonctionne la tokenisation, nous pouvons nous plonger un peu plus profond√©ment dans la perte utilis√©e pendant l'entra√Ænement. √Ä n'importe quelle √©tape, cette perte est calcul√©e en tokenisant chaque mot du corpus, en utilisant le vocabulaire courant et le mod√®le *Unigram* d√©termin√© par les fr√©quences de chaque *token* dans le corpus (comme vu pr√©c√©demment).

Chaque mot du corpus a un score, et la perte est le n√©gatif du logarithme de ces scores, c'est-√†-dire la somme pour tous les mots du corpus de tous les `-log(P(word))`.

Revenons √† notre exemple avec le corpus suivant :

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

La tokenisation de chaque mot avec leurs scores respectifs est :

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

Donc la perte est :

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Maintenant, nous devons calculer comment la suppression de chaque token affecte la perte. C'est plut√¥t fastidieux, donc nous allons le faire pour deux *tokens* ici et garder tout le processus pour quand nous aurons du code pour nous aider. Dans ce cas (tr√®s) particulier, nous avions deux tokenizations √©quivalentes de tous les mots. Par exmeple, comme nous l'avons vu pr√©c√©demment, `"pug"` pourrait √™tre tokenis√© en `["p", "ug"]` avec le m√™me score. Ainsi, enlever le token `"pu"` du vocabulaire donnera exactement la m√™me perte.

D'un autre c√¥t√©, supprimer le mot `"hug"` aggravera la perte, car la tokenisation de `"hug"` et `"hugs"` deviendra :

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Ces changements entra√Æneront une augmentation de la perte de :

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

Par cons√©quent, le token `"pu"` sera probablement retir√© du vocabulaire, mais pas `"hug"`.

## Impl√©mentation d'<i>Unigram</i>

Maintenant, impl√©mentons tout ce que nous avons vu jusqu'√† pr√©sent dans le code. Comme pour le BPE et *WordPiece*, ce n'est pas une impl√©mentation efficace de l'algorithme *Unigram* (bien au contraire), mais elle devrait vous aider √† le comprendre un peu mieux.

Nous allons utiliser le m√™me corpus que pr√©c√©demment comme exemple :

```python
corpus = [
    "This is the Hugging Face Course.",
    # C'est le cours d'Hugging Face.
    "This chapter is about tokenization.",
    # Ce chapitre traite de la tokenisation.
    "This section shows several tokenizer algorithms.",
    # Cette section pr√©sente plusieurs algorithmes de *tokenizer*.
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
    # Avec un peu de chance, vous serez en mesure de comprendre comment ils sont entra√Æn√©s et g√©n√®rent des *tokens*.
]
```

Cette fois, nous allons utiliser `xlnet-base-cased` comme mod√®le :

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

Comme pour le BPE et *WordPiece*, nous commen√ßons par compter le nombre d'occurrences de chaque mot dans le corpus :

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Ensuite, nous devons initialiser notre vocabulaire √† une taille plus grande que celle du vocabulaire que nous voudrons √† la fin. Nous devons inclure tous les caract√®res de base (sinon nous ne serons pas en mesure de tokeniser chaque mot), mais pour les sous-cha√Ænes plus grandes, nous ne garderons que les plus communs. AInsi nous les trions par fr√©quence :

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Boucle √† travers les sous-mots de longueur au moins √©gale √† 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Trier les sous-mots par fr√©quence
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('‚ñÅt', 7), ('is', 5), ('er', 5), ('‚ñÅa', 5), ('‚ñÅto', 4), ('to', 4), ('en', 4), ('‚ñÅT', 3), ('‚ñÅTh', 3), ('‚ñÅThi', 3)]
```

Nous regroupons les caract√®res avec les meilleurs sous-mots pour arriver √† un vocabulaire initial de taille 300 :

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>

üí° *SentencePiece* utilise un algorithme plus efficace appel√© *Enhanced Suffix Array* (ESA) pour cr√©er le vocabulaire initial.

</Tip>

Ensuite, nous calculons la somme de toutes les fr√©quences, pour convertir les fr√©quences en probabilit√©s. Pour notre mod√®le, nous allons stocker les logarithmes des probabilit√©s, car c'est plus stable num√©riquement d'additionner des logarithmes que de multiplier des petits nombres. Cela simplifiera aussi le calcul de la perte du mod√®le :

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Maintenant la fonction principale est celle qui tokenise les mots en utilisant l'algorithme de Viterbi. Comme nous l'avons vu pr√©c√©demment, cet algorithme calcule la meilleure segmentation de chaque sous-cha√Æne du mot que nous allons stocker dans une variable nomm√©e `best_segmentations`. Nous allons stocker un dictionnaire par position dans le mot (de 0 √† sa longueur totale), avec deux cl√©s : l'index du d√©but du dernier *token* dans la meilleure segmentation et le score de la meilleure segmentation. Avec l'index du d√©but du dernier *token*, nous serons en mesure de r√©cup√©rer la segmentation compl√®te une fois que la liste est compl√®tement remplie.

Le remplissage de la liste se fait √† l'aide de deux boucles seulement : la boucle principale passe en revue chaque position de d√©part et la seconde boucle essaie toutes les sous-cha√Ænes commen√ßant √† cette position de d√©part. Si la sous-cha√Æne est dans le vocabulaire, nous avons une nouvelle segmentation du mot jusqu'√† cette position finale que nous comparons √† ce qui est dans `best_segmentations`.

Une fois que la boucle principale est termin√©e, nous commen√ßons juste √† la fin et sautons d'une position de d√©part √† une autre, en enregistrant les *tokens* au fur et √† mesure, jusqu'√† ce que nous atteignions le d√©but du mot :

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # Doit √™tre correctement rempli par les √©tapes pr√©c√©dentes de la boucle
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # Si nous avons trouv√© une meilleure segmentation se terminant √† end_idx, nous mettons √† jour
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # Nous n'avons pas trouv√© de tokenization du mot -> inconnu (<unk>)
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

Nous pouvons d√©j√† essayer notre mod√®le initial sur quelques mots :

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Il est maintenant facile de calculer la perte du mod√®le sur le corpus !

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Nous pouvons v√©rifier que cela fonctionne sur le mod√®le que nous avons :

```python
compute_loss(model)
```

```python out
413.10377642940875
```

Le calcul des scores pour chaque *token* n'est pas tr√®s difficile non plus. Il suffit de calculer la perte pour les mod√®les obtenus en supprimant chaque *token* :

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # Nous gardons toujours les tokens de longueur 1.
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

Nous pouvons l'essayer sur un *token* donn√© :

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

Puisque `"ll"` est utilis√© dans la tokenisation de `"Hopefully"`, et que le supprimer nous fera probablement utiliser le token `"l"` deux fois √† la place, nous nous attendons √† ce qu'il ait une perte positive. `"his"` n'est utilis√© qu'√† l'int√©rieur du mot `"This"`, qui est tokenis√© comme lui-m√™me, donc nous nous attendons √† ce qu'il ait une perte nulle. Voici les r√©sultats :

```python out
6.376412403623874
0.0
```

<Tip>

üí° Cette approche est tr√®s inefficace, c'est pourquoi *SentencePiece* utilise une approximation de la perte du mod√®le sans le *token* X. Au lieu de partir de z√©ro, il remplace simplement le *token* X par sa segmentation dans le vocabulaire restant. De cette fa√ßon, tous les scores peuvent √™tre calcul√©s en une seule fois, en m√™me temps que la perte du mod√®le.

</Tip>

Une fois tout cela en place, la derni√®re chose √† faire est d'ajouter les *tokens* sp√©ciaux utilis√©s par le mod√®le au vocabulaire, puis de boucler jusqu'√† ce que nous ayons √©lagu√© suffisamment de *tokens* du vocabulaire pour atteindre la taille souhait√©e :

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Supprime les tokens percent_to_remove ayant les scores les plus bas
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Ensuite, pour tokeniser un texte, il suffit d'appliquer la pr√©tok√©nisation et d'utiliser la fonction `encode_word()` :

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅHugging', '‚ñÅFace', '‚ñÅ', 'c', 'ou', 'r', 's', 'e', '.']
```

C'est tout pour *Unigram* ! Avec un peu de chance, vous vous sentez √† pr√©sent √™tre un expert des *tokenizers*. Dans la prochaine section, nous allons nous plonger dans les blocs de construction de la biblioth√®que ü§ó *Tokenizers* et allons vous montrer comment vous pouvez les utiliser pour construire votre propre *tokenizer*.
