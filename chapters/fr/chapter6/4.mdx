# Normalisation et pr√©tokenization

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter6/section4.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter6/section4.ipynb"},
]} />

Avant de nous plonger plus profond√©ment dans les trois algorithmes de tok√©nisation en sous-mots les plus courants utilis√©s avec les *transformers* (*Byte-Pair Encoding* (BPE), *WordPiece* et *Unigram*), nous allons d'abord examiner le pr√©traitement que chaque *tokenizer* applique au texte. Voici un aper√ßu de haut niveau des √©tapes du pipeline de tokenisation :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

Avant de diviser un texte en sous-*tokens* (selon le mod√®le), le *tokenizer* effectue deux √©tapes : la _normalisation_ et la _pr√©tok√©nisation_.

## Normalisation

<Youtube id="4IIC2jI9CaU"/>

L'√©tape de normalisation implique un nettoyage g√©n√©ral, comme la suppression des espaces inutiles, la mise en minuscules et/ou la suppression des accents. Si vous √™tes familier avec la [normalisation Unicode](http://www.unicode.org/reports/tr15/) (comme NFC ou NFKC), c'est aussi quelque chose que le *tokenizer* peut appliquer.

Le `tokenizer` de ü§ó *Transformers* poss√®de un attribut appel√© `backend_tokenizer` qui donne acc√®s au *tokenizer* sous-jacent de la biblioth√®que ü§ó *Tokenizers* :

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

L'attribut `normalizer` de l'objet `tokenizer` poss√®de une m√©thode `normalize_str()` que nous pouvons utiliser pour voir comment la normalisation est effectu√©e :

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("H√©ll√≤ h√¥w are √º?"))
```

```python out
'hello how are u?'
```

Dans cet exemple, puisque nous avons choisi le *checkpoint* `bert-base-uncased`, la normalisation a mis le texte en minuscule et supprim√© les accents. 

<Tip>

‚úèÔ∏è **Essayez !** Chargez un *tokenizer* depuis le *checkpoint* `bert-base-cased` et passez-lui le m√™me exemple. Quelles sont les principales diff√©rences que vous pouvez voir entre les versions cas√©e et non cas√©e du *tokenizer* ?

</Tip>

## Pr√©tokenization

<Youtube id="grlLV8AIXug"/>

Comme nous le verrons dans les sections suivantes, un *tokenizer* ne peut pas √™tre entra√Æn√© uniquement sur du texte brut. Au lieu de cela, nous devons d'abord diviser les textes en petites entit√©s, comme des mots. C'est l√† qu'intervient l'√©tape de pr√©tok√©nisation. Comme nous l'avons vu dans le [chapitre 2](/course/fr/chapter2), un *tokenizer* bas√© sur les mots peut simplement diviser un texte brut en mots sur les espaces et la ponctuation. Ces mots constitueront les limites des sous-*tokens* que le *tokenizer* peut apprendre pendant son entra√Ænement.

Pour voir comment un *tokenizer* rapide effectue la pr√©tok√©nisation, nous pouvons utiliser la m√©thode `pre_tokenize_str()`de l'attribut `pre_tokenizer` de l'objet `tokenizer` :

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

Remarquez que le *tokenizer* garde d√©j√† la trace des *offsets*, ce qui lui permet de nous donner la correspondance des d√©calages que nous avons utilis√©e dans la section pr√©c√©dente. Ici, le *tokenizer* ignore les deux espaces et les remplace par un seul, mais le d√©calage saute entre `are` et `you` pour en tenir compte.

Puisque nous utilisons le *tokenizer* de BERT, la pr√©tok√©nisation implique la s√©paration des espaces et de la ponctuation. D'autres *tokenizers* peuvent avoir des r√®gles diff√©rentes pour cette √©tape. Par exemple, si nous utilisons le *tokenizer* du GPT-2 :

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

Il s√©parera aussi sur les espaces et la ponctuation mais gardera les espaces et les remplacera par un symbole `ƒ†`, ce qui lui permettra de r√©cup√©rer les espaces originaux si nous d√©codons les *tokens* :

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),
 ('?', (19, 20))]
```

Notez √©galement que, contrairement au *tokenizer* de BERT, ce *tokenizer* n'ignore pas les doubles espaces.

Pour un dernier exemple, regardons le *tokenizer* du 5, qui est bas√© sur l'algorithme SentencePiece :

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('‚ñÅHello,', (0, 6)), ('‚ñÅhow', (7, 10)), ('‚ñÅare', (11, 14)), ('‚ñÅyou?', (16, 20))]
```

Comme le *tokenizer* du GPT-2, celui-ci garde les espaces et les remplace par un *token* sp√©cifique (`_`), mais le *tokenizer* du T5 ne s√©pare que sur les espaces, pas sur la ponctuation. Notez √©galement qu'il a ajout√© un espace par d√©faut au d√©but de la phrase (avant `Hello`) et il ignore le double espace entre `are` et `you`.

Maintenant que nous avons vu un peu comment les diff√©rents *tokenizers* traitent le texte, nous pouvons commencer √† explorer les algorithmes sous-jacents eux-m√™mes. Nous commencerons par jeter un coup d'oeil rapide sur le tr√®s r√©pandu SentencePiece, puis au cours des trois sections suivantes nous examinerons le fonctionnement des trois principaux algorithmes utilis√©s pour la tokenisation en sous-mots.

## SentencePiece

[SentencePiece](https://github.com/google/sentencepiece) est un algorithme de tokenisation pour le pr√©traitement du texte que vous pouvez utiliser avec n'importe lequel des mod√®les que nous verrons dans les trois prochaines sections. Il consid√®re le texte comme une s√©quence de caract√®res Unicode et il remplace les espaces par un caract√®re sp√©cial : `‚ñÅ`. Utilis√© en conjonction avec l'algorithme Unigram (voir la [section 7](/course/fr/chapter7/7)), il ne n√©cessite m√™me pas d'√©tape de pr√©tok√©nisation, ce qui est tr√®s utile pour les langues o√π le caract√®re espace n'est pas utilis√© (comme le chinois ou le japonais).

L'autre caract√©ristique principale de SentencePiece est le *tokenisation r√©versible* : comme il n'y a pas de traitement sp√©cial des espaces, le d√©codage des *tokens* se fait simplement en les concat√©nant et en rempla√ßant les `_` par des espaces, ce qui donne le texte normalis√©. Comme nous l'avons vu pr√©c√©demment, le *tokenizer* de BERT supprime les espaces r√©p√©titifs, donc sa tokenisation n'est pas r√©versible.

## Vue d'ensemble des algorithmes

Dans les sections suivantes, nous allons nous plonger dans les trois principaux algorithmes de tokenisation en sous-mots : BPE (utilis√© par GPT-2 et autres), WordPiece (utilis√© par exemple par BERT), et Unigram (utilis√© par T5 et autres). Avant de commencer, voici un rapide aper√ßu du fonctionnement de chacun d'entre eux. N'h√©sitez pas √† revenir √† ce tableau apr√®s avoir lu chacune des sections suivantes si cela n'a pas encore de sens pour vous.


Mod√®le | BPE | WordPiece | Unigramme
:----:|:---:|:---------:|:------:
Entra√Ænement | Part d'un petit vocabulaire et apprend des r√®gles pour fusionner les *tokens* | Part d'un petit vocabulaire et apprend des r√®gles pour fusionner les *tokens* | Part d'un grand vocabulaire et apprend des r√®gles pour supprimer les *tokens*
√âtape d'entra√Ænement | Fusionne les *tokens* correspondant √† la paire la plus commune | Fusionne les *tokens* correspondant √† la paire ayant le meilleur score bas√© sur la fr√©quence de la paire, en privil√©giant les paires o√π chaque *token* individuel est moins fr√©quent | Supprime tous les *tokens* du vocabulaire qui minimiseront la perte calcul√©e sur le corpus entier
Apprend | A fusionner des r√®gles et un vocabulaire | Juste un vocabulaire | Un vocabulaire avec un score pour chaque *token*
Encodage | D√©coupe un mot en caract√®res et applique les fusions apprises pendant l'entra√Ænement | Trouve le plus long sous-mot depuis le d√©but qui est dans le vocabulaire puis fait de m√™me pour le reste du mot | Trouve la division la plus probable en tokens, en utilisant les scores appris pendant l'entra√Ænement

Maintenant, plongeons dans le BPE !
