# Construction d'un <i>tokenizer</i>, bloc par bloc


<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter6/section8.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter6/section8.ipynb"},
]} />

Comme nous l'avons vu dans les sections pr√©c√©dentes, la tokenisation comprend plusieurs √©tapes :

- normalisation (tout nettoyage du texte jug√© n√©cessaire, comme la suppression des espaces ou des accents, la normalisation Unicode, etc.),
- pr√©tok√©nisation (division de l'entr√©e en mots),
- passage de l'entr√©e dans le mod√®le (utilisation des mots pr√©tok√©nis√©s pour produire une s√©quence de *tokens*),
- post-traitement (ajout des *tokens* sp√©ciaux du *tokenizer*, g√©n√©ration du masque d'attention et des identifiants du type de *token*).

Pour m√©moire, voici un autre aper√ßu du processus global :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

La biblioth√®que ü§ó *Tokenizers* a √©t√© construite pour fournir plusieurs options pour chacune de ces √©tapes. Vous pouvez les m√©langer et assortir ensemble. Dans cette section, nous verrons comment nous pouvons construire un *tokenizer* √† partir de z√©ro, par opposition √† entra√Æner un nouveau *tokenizer* √† partir d'un ancien, comme nous l'avons fait dans [section 2](/course/fr/chapter6/2). Vous serez alors en mesure de construire n'importe quel type de *tokenizer* auquel vous pouvez penser !

<Youtube id="MR8tZm5ViWU"/>

Plus pr√©cis√©ment, la biblioth√®que est construite autour d'une classe centrale `Tokenizer` avec les blocs de construction regroup√©s en sous-modules :

- `normalizers` contient tous les types de `Normalizer` que vous pouvez utiliser (liste compl√®te [ici](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers)),
- `pre_tokenizers` contient tous les types de `PreTokenizer` que vous pouvez utiliser (liste compl√®te [ici](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers)),
- `models` contient les diff√©rents types de `Model` que vous pouvez utiliser, comme `BPE`, `WordPiece`, et `Unigram` (liste compl√®te [ici](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models)),
- `trainers` contient tous les diff√©rents types de `Trainer` que vous pouvez utiliser pour entra√Æner votre mod√®le sur un corpus (un par type de mod√®le ; liste compl√®te [ici](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers)),
- `post_processors` contient les diff√©rents types de `PostProcessor` que vous pouvez utiliser (liste compl√®te [ici](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors)),
- `decoders` contient les diff√©rents types de `Decoder` que vous pouvez utiliser pour d√©coder les sorties de tokenization (liste compl√®te [ici](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders)).

Vous pouvez trouver la liste compl√®te des blocs de construction [ici](https://huggingface.co/docs/tokenizers/python/latest/components.html).

## Acquisition d'un corpus

Pour entra√Æner notre nouveau *tokenizer*, nous utiliserons un petit corpus de texte (pour que les exemples soient rapides). Les √©tapes pour acqu√©rir ce corpus sont similaires √† celles que nous avons suivies au [d√©but du chapitre](/course/fr/chapter6/2), mais cette fois nous utiliserons le jeu de donn√©es [WikiText-2](https://huggingface.co/datasets/wikitext) :


```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

La fonction `get_training_corpus()` est un g√©n√©rateur qui donne des batchs de 1 000 textes, que nous utiliserons pour entra√Æner le *tokenizer*. 

ü§ó *Tokenizers* peut aussi √™tre entra√Æn√© directement sur des fichiers texte. Voici comment nous pouvons g√©n√©rer un fichier texte contenant tous les textes de WikiText-2 que nous pourrons ensuite utilis√© en local :

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

Ensuite, nous vous montrerons comment construire vos propres *tokenizers* pour BERT, GPT-2 et XLNet, bloc par bloc. Cela vous donnera un exemple de chacun des trois principaux algorithmes de tokenisation : *WordPiece*, BPE et *Unigram*. Commen√ßons par BERT !

## Construire un <i>tokenizer WordPiece</i> √† partir de z√©ro

Pour construire un *tokenizer* avec la biblioth√®que ü§ó *Tokenizers*, nous commen√ßons par instancier un objet `Tokenizer` avec un `model`. Puis nous d√©finissons ses attributs `normalizer`, `pre_tokenizer`, `post_processor` et `decoder` aux valeurs que nous voulons.

Pour cet exemple, nous allons cr√©er un `Tokenizer` avec un mod√®le *WordPiece* :

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

Nous devons sp√©cifier le `unk_token` pour que le mod√®le sache quoi retourner lorsqu'il rencontre des caract√®res qu'il n'a pas vu auparavant. D'autres arguments que nous pouvons d√©finir ici incluent le `vocab` de notre mod√®le (nous allons entra√Æner le mod√®le, donc nous n'avons pas besoin de le d√©finir) et `max_input_chars_per_word`, qui sp√©cifie une longueur maximale pour chaque mot (les mots plus longs que la valeur pass√©e seront s√©par√©s).

La premi√®re √©tape de la tok√©nisation est la normalisation. Puisque BERT est largement utilis√©, une fonction `BertNormalizer` a √©t√© cr√©√©e avec les options classiques que nous pouvons d√©finir pour BERT : `lowercase` pour mettre le texte en minuscule, `strip_accents` qui enl√®ve les accents, `clean_text` pour enlever tous les caract√®res de contr√¥le et fusionner des espaces r√©p√©t√©s par un seul, et `handle_chinese_chars` qui place des espaces autour des caract√®res chinois. Pour reproduire le *tokenizer* `bert-base-uncased`, nous pouvons simplement d√©finir ce *normalizer* :

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

Cependant, g√©n√©ralement, lorsque vous construisez un nouveau *tokenizer*, vous n'avez pas acc√®s √† un normaliseur aussi pratique d√©j√† impl√©ment√© dans la biblioth√®que ü§ó *Tokenizers*. Donc voyons comment cr√©er le normaliseur de BERT manuellement. La biblioth√®que fournit un normaliseur `Lowercase` et un normaliseur `StripAccents`. Il est possible de composer plusieurs normaliseurs en utilisant une `Sequence` :

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

Nous utilisons √©galement un normaliseur Unicode `NFD`, car sinon `StripAccents` ne reconna√Ætra pas correctement les caract√®res accentu√©s et ne les supprimera donc pas.

Comme nous l'avons vu pr√©c√©demment, nous pouvons utiliser la m√©thode `normalize_str()` du `normalizer` pour v√©rifier les effets qu'il a sur un texte donn√© :

```python
print(tokenizer.normalizer.normalize_str("H√©ll√≤ h√¥w are √º?"))
```

```python out
hello how are u?
```

<Tip>

**Pour aller plus loin** Si vous testez les deux versions des normaliseurs pr√©c√©dents sur une cha√Æne contenant le caract√®re unicode `u"\u0085"` vous remarquerez s√ªrement qu'ils ne sont pas exactement √©quivalents. 
Pour ne pas trop compliquer la version avec `normalizers.Sequence`, nous n'avons pas inclus les Regex que le `BertNormalizer` requiert quand l'argument `clean_text` est mis √† `True` ce qui est le comportement par d√©faut. Mais ne vous inqui√©tez pas : il est possible d'obtenir exactement la m√™me normalisation sans utiliser le tr√®s pratique `BertNormalizer` en ajoutant deux `normalizers.Replace` √† la s√©quence de normalisation. 

</Tip>

L'√©tape suivante est la pr√©tokenisation. Encore une fois, il y a un `BertPreTokenizer` pr√©construit que nous pouvons utiliser :

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

Ou nous pouvons le construire √† partir de z√©ro :

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

Notez que le `Whitespace` divise sur les espaces et tous les caract√®res qui ne sont pas des lettres, des chiffres ou le caract√®re de soulignement. Donc techniquement il divise sur les espaces et la ponctuation :

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

Si vous voulez seulement s√©parer sur les espaces, vous devez utiliser `WhitespaceSplit` √† la place :

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

Comme pour les normaliseurs, vous pouvez utiliser une `Sequence` pour composer plusieurs pr√©tokenizers :

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

L'√©tape suivante dans le pipeline de tok√©nisation est de faire passer les entr√©es par le mod√®le. Nous avons d√©j√† sp√©cifi√© notre mod√®le dans l'initialisation, mais nous devons encore l'entra√Æner, ce qui n√©cessitera un `WordPieceTrainer`. La principale chose √† retenir lors de l'instanciation d'un entra√Æneur dans ü§ó *Tokenizers* est que vous devez lui passer tous les *tokens* sp√©ciaux que vous avez l'intention d'utiliser. Sinon il ne les ajoutera pas au vocabulaire puisqu'ils ne sont pas dans le corpus d'entra√Ænement :

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

En plus de sp√©cifier la `vocab_size` et les `special_tokens`, nous pouvons d√©finir la `min_frequency` (le nombre de fois qu'un *token* doit appara√Ætre pour √™tre inclus dans le vocabulaire) ou changer le `continuing_subword_prefix` (si nous voulons utiliser quelque chose de diff√©rent de `##`).

Pour entra√Æner notre mod√®le en utilisant l'it√©rateur que nous avons d√©fini plus t√¥t, il suffit d'ex√©cuter cette commande :

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Nous pouvons √©galement utiliser des fichiers texte pour entra√Æner notre *tokenizer* qui ressemblerait alors √† ceci (nous r√©initialisons le mod√®le avec un `WordPiece` vide au pr√©alable) :

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Dans les deux cas, nous pouvons ensuite tester le *tokenizer* sur un texte en appelant la m√©thode `encode()` :

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

L'encodage obtenu est un `Encoding` contenant toutes les sorties n√©cessaires du *tokenizer* dans ses diff√©rents attributs : `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask` et `overflowing`.

La derni√®re √©tape du pipeline de tok√©nisation est le post-traitement. Nous devons ajouter le *token* `[CLS]` au d√©but et le *token* `[SEP]` √† la fin (ou apr√®s chaque phrase si nous avons une paire de phrases). Nous utiliserons `TemplateProcessor` pour cela, mais d'abord nous devons conna√Ætre les identifiants des *tokens* `[CLS]` et `[SEP]` dans le vocabulaire :

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

Pour √©crire le gabarit pour `TemplateProcessor`, nous devons sp√©cifier comment traiter une seule phrase et une paire de phrases. Pour les deux, nous √©crivons les *tokens* sp√©ciaux que nous voulons utiliser. La premi√®re (ou unique) phrase est repr√©sent√©e par `$A`, alors que la deuxi√®me phrase (si on code une paire) est repr√©sent√©e par `$B`. Pour chacun de ces √©l√©ments (*tokens* sp√©ciaux et phrases), nous sp√©cifions √©galement l'identifiant du *token* correspondant apr√®s un deux-points. 

Le gabarit classique de BERT est donc d√©fini comme suit :

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

Notez que nous devons transmettre les identifiants des *tokens* sp√©ciaux afin que le *tokenizer* puisse les convertir correctement.

Une fois cela ajout√©, revenons √† notre exemple pr√©c√©dent donnera :

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

Et sur une paire de phrases, on obtient le bon r√©sultat :

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

Nous avons presque fini de construire ce *tokenizer* √† partir de z√©ro. La derni√®re √©tape consiste √† inclure un d√©codeur : 

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

Testons-le sur notre pr√©c√©dent `encoding` :

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences." # Testons ce tokenizer... sur une paire de phrases.
```

G√©nial ! Nous pouvons enregistrer notre *tokenizer* dans un seul fichier JSON comme ceci :

```python
tokenizer.save("tokenizer.json")
```

Nous pouvons alors recharger ce fichier dans un objet `Tokenizer` avec la m√©thode `from_file()` :

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

Pour utiliser ce *tokenizer* dans ü§ó *Transformers*, nous devons l'envelopper dans un `PreTrainedTokenizerFast`. Nous pouvons soit utiliser la classe g√©n√©rique, soit, si notre *tokenizer* correspond √† un mod√®le existant, utiliser cette classe (ici, `BertTokenizerFast`). Si vous appliquez cette logique pour construire un tout nouveau *tokenizer*, vous devrez utiliser la premi√®re option.

Pour envelopper le *tokenizer* dans un `PreTrainedTokenizerFast`, nous pouvons soit passer le *tokenizer* que nous avons construit comme un `tokenizer_object`, soit passer le fichier de *tokenizer* que nous avons sauvegard√© comme `tokenizer_file`. Ce qu'il faut retenir, c'est que nous devons d√©finir manuellement tous les *tokens* sp√©ciaux car cette classe ne peut pas d√©duire de l'objet `tokenizer` quel *token* est le *token* de masque, quel est le *token*`[CLS]`, etc :

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # Vous pouvez charger √† partir du fichier du tokenizer, alternativement
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

Si vous utilisez une classe de *tokenizer* sp√©cifique (comme `BertTokenizerFast`), vous aurez seulement besoin de sp√©cifier les *tokens* sp√©ciaux qui sont diff√©rents de ceux par d√©faut (ici, aucun) :

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

Vous pouvez ensuite utiliser ce *tokenizer* comme n'importe quel autre *tokenizer* de ü§ó *Transformers*. Vous pouvez le sauvegarder avec la m√©thode `save_pretrained()` ou le t√©l√©charger sur le *Hub* avec la m√©thode `push_to_hub()`.

Maintenant que nous avons vu comment construire un *tokenizer WordPiece*, faisons de m√™me pour un *tokenizer* BPE. Nous irons un peu plus vite puisque vous connaissez toutes les √©tapes. Nous ne soulignerons que les diff√©rences.

## Construire un <i>tokenizer</i> BPE √† partir de z√©ro

Construisons maintenant un *tokenizer* BPE. Comme pour le *tokenizer* BERT, nous commen√ßons par initialiser un `Tokenizer` avec un mod√®le BPE :

```python
tokenizer = Tokenizer(models.BPE())
```

Comme pour BERT, nous pourrions initialiser ce mod√®le avec un vocabulaire si nous en avions un (nous aurions besoin de passer le `vocab` et le `merges` dans ce cas), mais puisque nous allons nous entra√Æner √† partir de z√©ro, nous n'avons pas besoin de le faire. Nous n'avons pas non plus besoin de sp√©cifier un `unk_token` parce que le GPT-2 utilise un BPE au niveau de l'octet.

GPT-2 n'utilise pas de normaliseur, donc nous sautons cette √©tape et allons directement √† la pr√©tok√©nisation :

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

L'option que nous avons ajout√©e √† `ByteLevel` ici est de ne pas ajouter d'espace en d√©but de phrase (ce qui est le cas par d√©faut). Nous pouvons jeter un coup d'oeil √† la pr√©tok√©nisation d'un texte d'exemple comme avant :

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('ƒ†test', (5, 10)), ('ƒ†pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

Vient ensuite le mod√®le, qui doit √™tre entra√Æn√©. Pour le GPT-2, le seul *token* sp√©cial est le *token* de fin de texte :

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Comme avec le `WordPieceTrainer`, ainsi que le `vocab_size` et le `special_tokens`, nous pouvons sp√©cifier la `min_frequency` si nous le voulons, ou si nous avons un suffixe de fin de mot (comme `</w>`), nous pouvons le d√©finir avec `end_of_word_suffix`. 

Ce *tokenizer* peut aussi √™tre entra√Æn√© sur des fichiers texte :

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Regardons la tokenisation d'un exemple de texte :

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'ƒ†test', 'ƒ†this', 'ƒ†to', 'ken', 'izer', '.']
```

Nous appliquons le post-traitement au niveau de l'octet pour le *tokenizer* du GPT-2 comme suit :

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

L'option `trim_offsets = False` indique au post-processeur que nous devons laisser les *offsets* des *tokens* qui commencent par 'ƒ†' tels quels : de cette fa√ßon, le d√©but des *offsets* pointera sur l'espace avant le mot, et non sur le premier caract√®re du mot (puisque l'espace fait techniquement partie du *token*). Regardons le r√©sultat avec le texte que nous venons de coder, o√π `'ƒ†test'` est le *token* √† l'index 4 :

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

Enfin, nous ajoutons un d√©codeur au niveau de l'octet :

```python
tokenizer.decoder = decoders.ByteLevel()
```

et nous pouvons v√©rifier qu'il fonctionne correctement :

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer." # Testons ce tokenizer
```

Super ! Maintenant que nous avons termin√©, nous pouvons sauvegarder le tokenizer comme avant, et l'envelopper dans un `PreTrainedTokenizerFast` ou un `GPT2TokenizerFast` si nous voulons l'utiliser dans ü§ó *Transformers* :

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

ou :

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

Comme dernier exemple, nous allons vous montrer comment construire un *tokenizer* *Unigram* √† partir de z√©ro.

## Construire un <i>tokenizer Unigram</i> √† partir de z√©ro

Construisons maintenant un *tokenizer* XLNet. Comme pour les *tokenizers* pr√©c√©dents, nous commen√ßons par initialiser un `Tokenizer` avec un mod√®le *Unigram* :

```python
tokenizer = Tokenizer(models.Unigram())
```

Encore une fois, nous pourrions initialiser ce mod√®le avec un vocabulaire si nous en avions un.

Pour la normalisation, XLNet utilise quelques remplacements (qui proviennent de *SentencePiece*) :

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

Il remplace <code>``</code> et <code>''</code> par <code>"</code> et toute s√©quence de deux espaces ou plus par un seul espace, de plus il supprime les accents.

Le pr√©tokenizer √† utiliser pour tout *tokenizer SentencePiece* est `Metaspace` :

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

Nous pouvons jeter un coup d'oeil √† la pr√©tok√©nisation sur le m√™me exemple de texte que pr√©c√©demment :

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("‚ñÅLet's", (0, 5)), ('‚ñÅtest', (5, 10)), ('‚ñÅthe', (10, 14)), ('‚ñÅpre-tokenizer!', (14, 29))]
```

Vient ensuite le mod√®le, qui doit √™tre entra√Æn√©. XLNet poss√®de un certain nombre de *tokens* sp√©ciaux :

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Un argument tr√®s important √† ne pas oublier pour le `UnigramTrainer` est le `unk_token`. Nous pouvons aussi passer d'autres arguments sp√©cifiques √† l'algorithme *Unigram*, comme le `shrinking_factor` pour chaque √©tape o√π nous enlevons des *tokens* (par d√©faut 0.75) ou le `max_piece_length` pour sp√©cifier la longueur maximale d'un *token* donn√© (par d√©faut 16).

Ce *tokenizer* peut aussi √™tre entra√Æn√© sur des fichiers texte :

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Regardons la tokenisation de notre exemple :

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['‚ñÅLet', "'", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.']
```

Une particularit√© de XLNet est qu'il place le *token* `<cls>` √† la fin de la phrase, avec un identifiant de 2 (pour le distinguer des autres *tokens*). Le r√©sultat est un remplissage √† gauche. Nous pouvons traiter tous les *tokens* sp√©ciaux et les types d'identifiant de *token* avec un mod√®le, comme pour BERT. Mais d'abord nous devons obtenir les identifiants des *tokens* `<cls>` et `<sep>` :

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

Le mod√®le ressemble √† ceci :

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

Et nous pouvons tester son fonctionnement en codant une paire de phrases :

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['‚ñÅLet', "'", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.', '.', '.', '<sep>', '‚ñÅ', 'on', '‚ñÅ', 'a', '‚ñÅpair', 
  '‚ñÅof', '‚ñÅsentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

Enfin, nous ajoutons un d√©codeur `Metaspace` :

```python
tokenizer.decoder = decoders.Metaspace()
```

et on en a fini avec ce *tokenizer* ! On peut le sauvegarder et l'envelopper dans un `PreTrainedTokenizerFast` ou `XLNetTokenizerFast` si on veut l'utiliser dans ü§ó *Transformers*. Une chose √† noter lors de l'utilisation de `PreTrainedTokenizerFast` est qu'en plus des *tokens* sp√©ciaux, nous devons dire √† la biblioth√®que ü§ó *Transformers* de rembourrer √† gauche :

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

Ou alternativement :

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

Maintenant que vous avez vu comment les diff√©rentes briques sont utilis√©es pour construire des *tokenizers* existants, vous devriez √™tre capable d'√©crire n'importe quel *tokenizer* que vous voulez avec la biblioth√®que ü§ó *Tokenizers* et pouvoir l'utiliser dans ü§ó *Transformers*.
