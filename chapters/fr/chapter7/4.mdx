<FrameworkSwitchCourse {fw} />

# Traduction

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
]} />

{/if}

Plongeons maintenant dans la traduction. Il s'agit d'une autre [t√¢che de s√©quence √† s√©quence](/course/fr/chapitre1/7), ce qui signifie que c'est un probl√®me qui peut √™tre formul√© comme le passage d'une s√©quence √† une autre. En ce sens, le probl√®me est assez proche de la t√¢che de [r√©sum√©](/course/fr/chapitre7/6) et vous pouvez adapter ce que nous allons voir ici √† d'autres probl√®mes de s√©quence √† s√©quence tels que :

- Le **transfert de style** ? c'est-√†-dire cr√©er un mod√®le qui *traduit* des textes √©crits dans un certain style vers un autre (par exemple, du formel au d√©contract√© ou de l'anglais shakespearien √† l'anglais moderne).
- La **g√©n√©ration de r√©ponse √† des questions** c'est-√†-dire cr√©er un mod√®le qui g√©n√®re des r√©ponses √† des questions compte tenu d'un contexte.

<Youtube id="1JvfrvZgi6c"/>

Si vous disposez d'un corpus de textes suffisamment important en deux langues diff√©rentes (ou plus), vous pouvez entra√Æner un nouveau mod√®le de traduction √† partir de z√©ro, comme nous le ferons dans la section sur la [mod√©lisation causale du langage](/course/fr/chapitre7/6). Il est toutefois plus rapide de *finetuner* un mod√®le de traduction existant, qu'il s'agisse d'un mod√®le multilingue comme mT5 ou mBART que vous souhaitez adapter √† une paire de langues sp√©cifique, ou m√™me d'un mod√®le sp√©cialis√© dans la traduction d'une langue vers une autre que vous souhaitez adapter √† votre corpus sp√©cifique.

Dans cette section, nous allons *finetuner* un mod√®le Marian pr√©-entra√Æn√© pour traduire de l'anglais au fran√ßais (puisque de nombreux employ√©s de Hugging Face parlent ces deux langues) sur le jeu de donn√©es [KDE4](https://huggingface.co/datasets/kde4) qui est un jeu de donn√©es de fichiers localis√©s pour les applications [KDE](https://apps.kde.org/). Le mod√®le que nous utiliserons a √©t√© pr√©-entra√Æn√© sur un large corpus de textes fran√ßais et anglais provenant du jeu de donn√©es [Opus](https://opus.nlpl.eu/) qui contient en fait le jeu de donn√©es KDE4. A noter que m√™me si le mod√®le pr√©-entra√Æn√© que nous utilisons a vu ces donn√©es pendant son pr√©-entra√Ænement, nous verrons que nous pouvons obtenir une meilleure version de ce mod√®le apr√®s un *finetuning*.

Une fois que nous aurons termin√©, nous aurons un mod√®le capable de faire des pr√©dictions comme celle-ci :

<iframe src="https://hf.space/gradioiframe/course-demos/marian-finetuned-kde4-en-to-fr/+" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://hf.space/gradioiframe/course-demos/marian-finetuned-kde4-en-to-fr-darkmode/+" frameBorder="0" height="350" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

Comme dans les sections pr√©c√©dentes, vous pouvez trouver, t√©l√©charger et v√©rifier les pr√©cisions de ce mod√®le sur le [*Hub*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## Pr√©paration des donn√©es

Pour *finetuner* ou entra√Æner un mod√®le de traduction √† partir de z√©ro, nous avons besoin d'un jeu de donn√©es adapt√© √† cette t√¢che. Comme mentionn√© pr√©c√©demment, nous utiliserons le jeu de donn√©es [KDE4](https://huggingface.co/datasets/kde4) dans cette section. Notez que vous pouvez adapter assez facilement le code pour utiliser vos propres donn√©es du moment que vous disposez de paires de phrases dans les deux langues que vous voulez traduire. Reportez-vous au [chapitre 5](/course/fr/chapter5) si vous avez besoin d'un rappel sur la fa√ßon de charger vos donn√©es personnalis√©es dans un `Dataset`.

### Le jeu de donn√©es KDE4

Comme d'habitude, nous t√©l√©chargeons notre jeu de donn√©es en utilisant la fonction `load_dataset()` :

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

Si vous souhaitez travailler avec une autre paire de langues, 92 langues sont disponibles au total pour ce jeu de donn√©es. Vous pouvez les voir dans la [carte du jeu de donn√©es](https://huggingface.co/datasets/kde4).

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

Jetons un coup d'≈ìil au jeu de donn√©es :

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

Nous avons 210 173 paires de phrases. Cependant regroup√©es dans un seul √©chantillon. Nous devrons donc cr√©er notre propre jeu de validation. Comme nous l'avons vu dans le [chapitre 5](/course/fr/chapter5), un `Dataset` poss√®de une m√©thode `train_test_split()` qui peut nous aider. Nous allons fournir une graine pour la reproductibilit√© :

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

Nous pouvons renommer la cl√© `test` en `validation` comme ceci :

```py
split_datasets["validation"] = split_datasets.pop("test")
```

Examinons maintenant un √©l√©ment de ce jeu de donn√©es :

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par d√©faut, d√©velopper les fils de discussion'}
```

Nous obtenons un dictionnaire contenant deux phrases dans la paire de langues qui nous int√©resse. 
Une particularit√© de ce jeu de donn√©es rempli de termes techniques informatiques est qu'ils sont tous enti√®rement traduits en fran√ßais. Cependant, les ing√©nieurs fran√ßais sont souvent paresseux et laissent la plupart des mots sp√©cifiques √† l'informatique en anglais lorsqu'ils parlent. Ici, par exemple, le mot ¬´ *threads* ¬ª pourrait tr√®s bien appara√Ætre dans une phrase fran√ßaise, surtout dans une conversation technique. Mais dans ce jeu de donn√©es, il a √©t√© traduit en ¬´ fils de discussion ¬ª. Le mod√®le pr√©-entra√Æn√© que nous utilisons (qui a √©t√© pr√©-entra√Æn√© sur un plus grand corpus de phrases fran√ßaises et anglaises) prend l'option de laisser le mot tel quel :

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par d√©faut pour les threads √©largis'}]
```

Un autre exemple de ce comportement peut √™tre observ√© avec le mot  ¬´ *plugin* ¬ª qui n'est pas officiellement un mot fran√ßais mais que la plupart des francophones comprendront et ne prendront pas la peine de traduire.
Dans le jeu de donn√©es KDE4, ce mot a √©t√© traduit en fran√ßais par le plus officiel ¬´ module d'extension ¬ª :

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

Notre mod√®le pr√©-entra√Æn√©, lui, s'en tient au mot anglais :

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

Il sera int√©ressant de voir si notre mod√®le *finetun√©* tient compte de ces particularit√©s (alerte *spoiler* : il le fera).

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

‚úèÔ∏è **A votre tour !** Un autre mot anglais souvent utilis√© en fran√ßais est ¬´ *email* ¬ª. Trouvez le premier √©chantillon dans l'√©chantillon d'entra√Ænement qui utilise ce mot. Comment est-il traduit ? Comment le mod√®le pr√©-entra√Æn√© traduit-il cette m√™me phrase ?

</Tip>

### Traitement des donn√©es

<Youtube id="XAR8jnZZuUs"/>

Vous devriez maintenant conna√Ætre le principe : les textes doivent tous √™tre convertis en ensembles d'ID de *tokens* pour que le mod√®le puisse leur donner un sens. Pour cette t√¢che, nous aurons besoin de tokeniser les entr√©es et les cibles. Notre premi√®re t√¢che est de cr√©er notre objet `tokenizer`. Comme indiqu√© pr√©c√©demment, nous utiliserons un mod√®le pr√©-entra√Æn√© Marian English to French. Si vous essayez ce code avec une autre paire de langues, assurez-vous d'adapter le *checkpoint* du mod√®le. L'organisation [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) fournit plus de mille mod√®les dans plusieurs langues.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="tf")
```

Vous pouvez remplacer le `model_checkpoint` par un tout autre mod√®le disponible sur le [*Hub*](https://huggingface.co/models) qui aurait votre pr√©f√©rence, ou par un dossier en local o√π vous avez sauvegard√© un mod√®le pr√©-entra√Æn√© et un *tokenizer*.

<Tip>

üí° Si vous utilisez un *tokenizer* multilingue tel que mBART, mBART-50 ou M2M100, vous devrez d√©finir les codes de langue de vos entr√©es et cibles dans le *tokenizer* en d√©finissant `tokenizer.src_lang` et `tokenizer.tgt_lang` aux bonnes valeurs.

</Tip>

La pr√©paration de nos donn√©es est assez simple. Il y a juste une chose √† retenir : vous traitez les entr√©es comme d'habitude, mais pour les cibles, vous devez envelopper le *tokenizer* dans le gestionnaire de contexte `as_target_tokenizer()`.

Un gestionnaire de contexte en Python est introduit avec l'instruction `with` et est utile lorsque vous avez deux op√©rations li√©es √† ex√©cuter en paire. L'exemple le plus courant est lorsque vous √©crivez ou lisez un fichier, ce qui est souvent fait dans une instruction comme :

```
with open(file_path) as f:
    content = f.read()
```

Ici, les deux op√©rations connexes qui sont ex√©cut√©es en paire sont les actions d'ouverture et de fermeture du fichier. L'objet correspondant au fichier ouvert `f` n'existe qu'√† l'int√©rieur du bloc indent√© sous le `with`. L'ouverture se produit avant ce bloc et la fermeture √† la fin du bloc.

Dans le cas pr√©sent, le gestionnaire de contexte `as_target_tokenizer()` va d√©finir le *tokenizer* dans la langue de sortie (ici, le fran√ßais) avant l'ex√©cution du bloc indent√©, puis le red√©finir dans la langue d'entr√©e (ici, l'anglais).

Ainsi, le pr√©traitement d'un √©chantillon ressemble √† ceci :

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence)
with tokenizer.as_target_tokenizer():
    targets = tokenizer(fr_sentence)
```

Si nous oublions de tokeniser les cibles dans le gestionnaire de contexte, elles seront tokenis√©es par le *tokenizer* d'entr√©e, ce qui dans le cas d'un mod√®le Marian, ne va pas du tout bien se passer :

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(targets["input_ids"]))
```

```python out
['‚ñÅPar', '‚ñÅd√©', 'f', 'aut', ',', '‚ñÅd√©', 've', 'lop', 'per', '‚ñÅles', '‚ñÅfil', 's', '‚ñÅde', '‚ñÅdiscussion', '</s>']
['‚ñÅPar', '‚ñÅd√©faut', ',', '‚ñÅd√©velopper', '‚ñÅles', '‚ñÅfils', '‚ñÅde', '‚ñÅdiscussion', '</s>']
```

Comme on peut le voir, utiliser le *tokenizer* anglais pour pr√©traiter une phrase fran√ßaise donne un batch de *tokens* plus important, puisque le *tokenizer* ne conna√Æt aucun mot fran√ßais (sauf ceux qui apparaissent aussi en anglais, comme ¬´ discussion ¬ª).

Les `inputs` et les `targets` sont des dictionnaires avec nos cl√©s habituelles (identifiants d'entr√©e, masque d'attention, etc.). La derni√®re √©tape est de d√©finir une cl√© `"labels"` dans les entr√©es. Nous faisons cela dans la fonction de pr√©traitement que nous allons appliquer sur les jeux de donn√©es :

```python
max_input_length = 128
max_target_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Configurer le tokenizer pour les cibles.
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

Notez que nous avons fix√© des longueurs maximales similaires pour nos entr√©es et nos sorties. Comme les textes que nous traitons semblent assez courts, nous utilisons 128.

<Tip>

üí° Si vous utilisez un mod√®le T5 (plus pr√©cis√©ment, un des *checkpoints* `t5-xxx`), le mod√®le s'attendra √† ce que les entr√©es aient un pr√©fixe indiquant la t√¢che √† accomplir, comme `translate: English to French:`.

</Tip>

<Tip warning={true}>

‚ö†Ô∏è Nous ne faisons pas attention au masque d'attention des cibles car le mod√®le ne s'y attend pas. Au lieu de cela, les √©tiquettes correspondant √† un *token* de *padding* doivent √™tre mises √† `-100` afin qu'elles soient ignor√©es dans le calcul de la perte. Cela sera fait par notre collateur de donn√©es plus tard puisque nous appliquons le *padding* dynamique, mais si vous utilisez le *padding* ici, vous devriez adapter la fonction de pr√©traitement pour mettre toutes les √©tiquettes qui correspondent au *token* de *padding* √† `-100`.

</Tip>

Nous pouvons maintenant appliquer ce pr√©traitement en une seule fois sur toutes les √©chantillons de notre jeu de donn√©es :

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

Maintenant que les donn√©es ont √©t√© pr√©trait√©es, nous sommes pr√™ts √† *finetuner* notre mod√®le pr√©-entra√Æn√© !

{#if fw === 'pt'}

## <i>Finetuner</i> le mod√®le avec l'API `Trainer`

Le code actuel utilisant `Trainer` sera le m√™me que pr√©c√©demment, avec juste un petit changement : nous utilisons ici [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) qui est une sous-classe de `Trainer` qui nous permet de traiter correctement l'√©valuation, en utilisant la m√©thode `generate()` pour pr√©dire les sorties √† partir des entr√©es. Nous y reviendrons plus en d√©tail lorsque nous parlerons du calcul de la m√©trique.

Tout d'abord, nous avons besoin d'un mod√®le √† *finetuner*. Nous allons utiliser l'API habituelle `AutoModel` :

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## <i>Finetuner</i> du mod√®le avec Keras

Tout d'abord, nous avons besoin d'un mod√®le √† *finetuner*. Nous allons utiliser l'API habituelle `AutoModel` :

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

üí° Le *checkpoint* `Helsinki-NLP/opus-mt-en-fr` ne dispose que de poids PyTorch, vous aurez donc une erreur si vous essayez de charger le mod√®le sans utiliser l'argument `from_pt=True` dans la m√©thode `from_pretrained()`. Lorsque vous sp√©cifiez `from_pt=True`, la biblioth√®que t√©l√©chargera et convertira automatiquement les poids PyTorch pour vous. Comme vous pouvez le constater, c'est tr√®s simple de passer d'un *framework* √† l'autre dans ü§ó *Transformers* !

</Tip>

{/if}

Notez que cette fois-ci, nous utilisons un mod√®le qui a √©t√© entra√Æn√© sur une t√¢che de traduction et qui peut d√©j√† √™tre utilis√©, donc il n'y a pas d'avertissement concernant les poids manquants ou ceux nouvellement initialis√©s.

### Collecte des donn√©es

Nous aurons besoin d'un assembleur de donn√©es pour g√©rer le rembourrage pour la mise en batchs dynamique. Ici, nous ne pouvons pas simplement utiliser un `DataCollatorWithPadding` comme dans le [chapitre 3](/course/fr/chapter3) car cela ne rembourre que les entr√©es (identifiants d'entr√©e, masque d'attention, et *token* de type identifiants). Nos √©tiquettes doivent √©galement √™tre rembourr√©es √† la longueur maximale rencontr√©e dans les √©tiquettes. Et, comme mentionn√© pr√©c√©demment, la valeur de remplissage utilis√©e pour remplir les √©tiquettes doit √™tre `-100` et non le *token* de *padding* du *tokenizer* afin de s'assurer que ces valeurs soient ignor√©es dans le calcul de la perte.

Tout ceci est r√©alis√© par un [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). Comme le `DataCollatorWithPadding`, il prend le `tokenizer` utilis√© pour pr√©traiter les entr√©es, mais √©galement le `model`. C'est parce que ce collateur de donn√©es est √©galement responsable de la pr√©paration des identifiants d'entr√©e du d√©codeur, qui sont des versions d√©cal√©es des √©tiquettes avec un *token* sp√©cial au d√©but. Comme ce d√©calage est effectu√© de mani√®re l√©g√®rement diff√©rente selon les architectures, le `DataCollatorForSeq2Seq` a besoin de conna√Ætre l'objet `model` :

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

Pour le tester sur quelques √©chantillons, nous l'appelons simplement sur une liste d'exemples de notre √©chantillon d'entrainement tok√©nis√© :

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

Nous pouvons v√©rifier que nos √©tiquettes ont √©t√© rembourr√©es √† la longueur maximale du batch, en utilisant `-100` :

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

Nous pouvons aussi jeter un coup d'≈ìil aux identifiants d'entr√©e du d√©codeur, pour voir qu'il s'agit de versions d√©cal√©es des √©tiquettes :

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

Voici les √©tiquettes des premier et deuxi√®me √©l√©ments de notre jeu de donn√©es :

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

Nous allons transmettre ce `data_collator` au `Seq2SeqTrainer`. Ensuite, jetons un coup d'oeil √† la m√©trique.

{:else}

Nous pouvons maintenant utiliser ce `data_collator` pour convertir chacun de nos jeux de donn√©es en un `tf.data.Dataset`, pr√™t pour l'entra√Ænement :

```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### M√©triques

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

La fonctionnalit√© que `Seq2SeqTrainer` ajoute √† sa superclasse `Trainer` est la possibilit√© d'utiliser la m√©thode `generate()` pendant l'√©valuation ou la pr√©diction. Pendant l'entra√Ænement, le mod√®le utilisera les `decoder_input_ids` avec un masque d'attention assurant qu'il n'utilise pas les *tokens* apr√®s le *token* qu'il essaie de pr√©dire, pour acc√©l√©rer l'entra√Ænement. Pendant l'inf√©rence, nous ne pourrons pas les utiliser puisque nous n'aurons pas d'√©tiquettes. Ainsi c'est une bonne id√©e d'√©valuer notre mod√®le avec la m√™me configuration.

Comme nous l'avons vu dans le [chapitre 1](/course/fr/chapter1/6), le d√©codeur effectue l'inf√©rence en pr√©disant les *tokens* un par un. C'est quelque chose qui est impl√©ment√© en coulisses dans ü§ó *Transformers* par la m√©thode `generate()`. Le `Seq2SeqTrainer` nous laissera utiliser cette m√©thode pour l'√©valuation si nous indiquons `predict_with_generate=True`.

{/if}

La m√©trique traditionnelle utilis√©e pour la traduction est le [score BLEU](https://en.wikipedia.org/wiki/BLEU), introduit dans [un article de 2002](https://aclanthology.org/P02-1040.pdf) par Kishore Papineni et al. Le score BLEU √©value dans quelle mesure les traductions sont proches de leurs √©tiquettes. Il ne mesure pas l'intelligibilit√© ou l'exactitude grammaticale des r√©sultats g√©n√©r√©s par le mod√®le, mais utilise des r√®gles statistiques pour garantir que tous les mots des r√©sultats g√©n√©r√©s apparaissent √©galement dans les cibles. En outre, il existe des r√®gles qui p√©nalisent les r√©p√©titions des m√™mes mots s'ils ne sont pas √©galement r√©p√©t√©s dans les cibles (pour √©viter que le mod√®le ne produise des phrases telles que ¬´ the the the the the the the ¬ª) et les phrases produites qui sont plus courtes que celles des cibles (pour √©viter que le mod√®le ne produise des phrases telles que ¬´ the ¬ª).

L'une des faiblesses de BLEU est qu'il s'attend √† ce que le texte soit d√©j√† tokenis√©, ce qui rend difficile la comparaison des scores entre les mod√®les qui utilisent diff√©rents *tokenizers*. Par cons√©quent, la mesure la plus couramment utilis√©e aujourd'hui pour √©valuer les mod√®les de traduction est [SacreBLEU](https://github.com/mjpost/sacrebleu) qui rem√©die √† cette faiblesse (et √† d'autres) en standardisant l'√©tape de tokenisation. Pour utiliser cette m√©trique, nous devons d'abord installer la biblioth√®que *SacreBLEU* :

```py
!pip install sacrebleu
```

Nous pouvons ensuite charger ce score via `evaluate.load()` comme nous l'avons fait dans le [chapitre 3](/course/fr/chapter3) :

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

Cette m√©trique prend des textes comme entr√©es et cibles. Elle est con√ßue pour accepter plusieurs cibles acceptables car il y a souvent plusieurs traductions possibles d'une m√™me phrase. Le jeu de donn√©es que nous utilisons n'en fournit qu'une seule, mais en NLP, il n'est pas rare de trouver des jeux de donn√©es ayant plusieurs phrases comme √©tiquettes. Ainsi, les pr√©dictions doivent √™tre une liste de phrases mais les r√©f√©rences doivent √™tre une liste de listes de phrases.

Essayons un exemple :

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

Cela donne un score BLEU de 46.75, ce qui est plut√¥t bon. A titre de comparaison, le *Transformer* original dans l'article [*Attention Is All You Need*](https://arxiv.org/pdf/1706.03762.pdf) a obtenu un score BLEU de 41.8 sur une t√¢che de traduction similaire entre l'anglais et le fran√ßais ! (Pour plus d'informations sur les m√©triques individuelles, comme `counts` et `bp`, voir le [d√©p√¥t SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74). D'autre part, si nous essayons avec les deux mauvais types de pr√©dictions (r√©p√©titions ou pr√©diction trop courte) qui sortent souvent des mod√®les de traduction, nous obtiendrons des scores BLEU plut√¥t mauvais :

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

Le score peut aller de 0 √† 100. Plus il est √©lev√©, mieux c'est.

{#if fw === 'tf'}

Pour passer des sorties du mod√®le aux textes que la m√©trique peut utiliser, nous allons utiliser la m√©thode `tokenizer.batch_decode()`. Nous devons juste nettoyer tous les `-100` dans les √©tiquettes. Le *tokenizer* fera automatiquement la m√™me chose pour le *token* de *padding*. D√©finissons une fonction qui prend notre mod√®le et un jeu de donn√©es et calcule des m√©triques sur ceux-ci. Comme la g√©n√©ration de longues s√©quences peut √™tre lente, nous sous-√©chantillonnons l'ensemble de validation pour nous assurer que cela ne prend pas une √©ternit√© :

```py
import numpy as np


def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch in tf_generate_dataset:
        predictions = model.generate(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = batch["labels"].numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

Pour passer des sorties du mod√®le aux textes utilisables par la m√©trique, nous allons utiliser la m√©thode `tokenizer.batch_decode()`. Nous devons juste nettoyer tous les `-100` dans les √©tiquettes. Le *tokenizer* fera automatiquement la m√™me chose pour le *token* de *padding* :

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # Dans le cas o√π le mod√®le retourne plus que les logits de pr√©diction
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Remplacer les -100 dans les √©tiquettes car nous ne pouvons pas les d√©coder
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Quelques post-traitements simples
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

Maintenant que c'est fait, nous sommes pr√™ts √† *finetuner* notre mod√®le !


### <i>Finetuner</i> le mod√®le

La premi√®re √©tape consiste √† se connecter √† Hugging Face, afin de pouvoir t√©l√©charger vos r√©sultats sur le *Hub*. Il y a une fonction pratique pour vous aider √† le faire dans un *notebook* :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Cela affichera un *widget* o√π vous pourrez entrer vos identifiants de connexion √† Hugging Face.

Si vous ne travaillez pas dans un *notebook*, tapez simplement la ligne suivante dans votre terminal :

```bash
huggingface-cli login
```

{#if fw === 'tf'}

Avant de commencer, voyons quel type de r√©sultats nous obtenons avec notre mod√®le sans entra√Ænement :

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

Une fois ceci fait, nous pouvons pr√©parer tout ce dont nous avons besoin pour compiler et entra√Æner notre mod√®le. Notez l'utilisation de `tf.keras.mixed_precision.set_global_policy("mixed_float16")`. Ceci indiquera √† Keras de s'entra√Æner en utilisant float16, ce qui peut donner un gain de vitesse significatif sur les GPUs qui le supportent (Nvidia 20xx/V100 ou plus r√©cent).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# Le nombre d'√©tapes d'entra√Ænement est le nombre d'√©chantillons dans le jeu de donn√©es, divis√© par la taille du batch,
# puis multipli√© par le nombre total d'√©poques. Notez que le jeu de donn√©es tf_train_dataset est ici un tf.data.Dataset,
# et non le jeu de donn√©es original donc son len() est d√©j√† num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Entra√Æner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Ensuite, nous d√©finissons un `PushToHubCallback` pour t√©l√©charger notre mod√®le sur le *Hub* pendant l'entra√Ænement, comme nous l'avons vu dans la [section 2](/course/fr/chapter7/2), puis nous entra√Ænons simplement le mod√®le avec ce *callback* :

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

Notez que vous pouvez sp√©cifier le nom du d√©p√¥t vers lequel vous voulez pousser le mod√®le avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss√© le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` dans `Seq2SeqTrainingArguments`. Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace et nomm√© apr√®s le r√©pertoire de sortie que vous avez d√©fini. Ici ce sera `"sgugger/marian-finetuned-kde4-en-to-fr"` (qui est le mod√®le que nous avons li√© au d√©but de cette section).

<Tip>

üí° Si le r√©pertoire de sortie que vous utilisez existe d√©j√†, il doit √™tre un clone local du d√©p√¥t vers lequel vous voulez pousser. S'il ne l'est pas, vous obtiendrez une erreur lors de l'appel de `model.fit()` et devrez d√©finir un nouveau nom.

</Tip>

Enfin, voyons √† quoi ressemblent nos m√©triques maintenant que l'entra√Ænement est termin√© :

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

√Ä ce stade, vous pouvez utiliser le *widget* d'inf√©rence sur le *Hub* pour tester votre mod√®le et le partager avec vos amis. Vous avez r√©ussi √† *finetuner* un mod√®le sur une t√¢che de traduction. F√©licitations !

{:else}

Une fois ceci fait, nous pouvons d√©finir notre `Seq2SeqTrainingArguments`. Comme pour le `Trainer`, nous utilisons une sous-classe de `TrainingArguments` qui contient quelques champs suppl√©mentaires :

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

En dehors des hyperparam√®tres habituels (comme le taux d'apprentissage, le nombre d'√©poques, la taille des batchs et une le taux de d√©croissance des poids), voici quelques changements par rapport √† ce que nous avons vu dans les sections pr√©c√©dentes :

- Nous ne d√©finissons pas d'√©valuation car elle prend du temps. Nous allons juste √©valuer une fois notre mod√®le avant l'entra√Ænement et apr√®s.
- Nous avons mis `fp16=True`, ce qui acc√©l√®re l'entra√Ænement sur les GPUs modernes.
- Nous d√©finissons `predict_with_generate=True`, comme discut√© ci-dessus.
- Nous utilisons `push_to_hub=True` pour t√©l√©charger le mod√®le sur le *Hub* √† la fin de chaque √©poque.

Notez que vous pouvez sp√©cifier le nom complet du d√©p√¥t vers lequel vous voulez pousser avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss√© le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` √† `Seq2SeqTrainingArguments`. Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace et nomm√© d'apr√®s le r√©pertoire de sortie que vous avez d√©fini. Dans notre cas ce sera `"sgugger/marian-finetuned-kde4-en-to-fr"` (qui est le mod√®le que nous avons li√© au d√©but de cette section).

<Tip>

üí° Si le r√©pertoire de sortie que vous utilisez existe d√©j√†, il doit √™tre un clone local du d√©p√¥t vers lequel vous voulez pousser. S'il ne l'est pas, vous obtiendrez une erreur lors de la d√©finition de votre `Seq2SeqTrainer` et devrez d√©finir un nouveau nom.

</Tip>


Enfin, nous passons tout au `Seq2SeqTrainer` :

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Avant d'entra√Æner, nous allons d'abord regarder le score obtenu par notre mod√®le, pour v√©rifier que nous n'aggravons pas les choses avec notre *finetuning*. Cette commande va prendre un peu de temps, vous pouvez donc prendre un caf√© pendant qu'elle s'ex√©cute :

```python
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

Un score BLEU de 39 n'est pas trop mauvais, ce qui refl√®te le fait que notre mod√®le est d√©j√† bon pour traduire des phrases anglaises en phrases fran√ßaises.

Vient ensuite l'entra√Ænement, qui prendra √©galement un peu de temps :

```python
trainer.train()
```

Notez que pendant l'entra√Ænement, chaque fois que le mod√®le est sauvegard√© (ici, √† chaque √©poque), il est t√©l√©charg√© sur le *Hub* en arri√®re-plan. De cette fa√ßon, vous serez en mesure de reprendre votre entra√Ænement sur une autre machine si n√©cessaire.

Une fois l'entra√Ænement termin√©, nous √©valuons √† nouveau notre mod√®le. Avec un peu de chance, nous verrons une am√©lioration du score BLEU !

```py
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

C'est une am√©lioration de pr√®s de 14 points, ce qui est formidable.

Enfin, nous utilisons la m√©thode `push_to_hub()` pour nous assurer que nous t√©l√©chargeons la derni√®re version du mod√®le. `Trainer` r√©dige √©galement une carte de mod√®le avec tous les r√©sultats de l'√©valuation et la t√©l√©charge. Cette carte de mod√®le contient des m√©tadonn√©es qui aident le *Hub* √† choisir le *widget* pour l'inf√©rence. Habituellement, il n'y a pas besoin de dire quoi que ce soit car il peut inf√©rer le bon *widget* √† partir de la classe du mod√®le, mais dans ce cas, la m√™me classe de mod√®le peut √™tre utilis√©e pour toutes sortes de probl√®mes de s√©quence √† s√©quence. Ainsi nous sp√©cifions que c'est un mod√®le de traduction :

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

Cette commande renvoie l'URL du commit qu'elle vient de faire, si vous voulez l'inspecter :

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

√Ä ce stade, vous pouvez utiliser le *widget* d'inf√©rence sur le *Hub* pour tester votre mod√®le et le partager avec vos amis. Vous avez r√©ussi √† *finetuner* un mod√®le sur une t√¢che de traduction. F√©licitations !

Si vous souhaitez vous plonger un peu plus profond√©ment dans la boucle d'entra√Ænement, nous allons maintenant vous montrer comment faire la m√™me chose en utilisant ü§ó *Accelerate*.

{/if}

{#if fw === 'pt'}

## Une boucle d'entra√Ænement personnalis√©e

Jetons maintenant un coup d'≈ìil √† la boucle d'entra√Ænement compl√®te afin que vous puissiez facilement personnaliser les parties dont vous avez besoin. Elle ressemblera beaucoup √† ce que nous avons fait dans la [section 2](/course/fr/chapter7/2) et dans le [chapitre 3](/course/fr/chapter3/4).

### Pr√©parer le tout pour l'entra√Ænement

Vous avez vu tout cela plusieurs fois maintenant, donc nous allons passer en revue le code assez rapidement. D'abord, nous allons construire le `DataLoader` √† partir de nos jeux de donn√©es, apr√®s avoir configur√© les jeux de donn√©es au format `"torch"` pour obtenir les tenseurs PyTorch :

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

Ensuite, nous r√©instantifions notre mod√®le pour nous assurer que nous ne poursuivons pas le *finetuning* pr√©c√©dent et que nous repartons du mod√®le pr√©-entra√Æn√© :

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

Nous aurons alors besoin d'un optimiseur :

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Une fois que nous avons tous ces objets, nous pouvons les envoyer √† la m√©thode `accelerator.prepare()`. Rappelez-vous que si vous voulez entra√Æner sur des TPUs dans un *notebook* de Colab, vous devez d√©placer tout ce code dans une fonction d'entra√Ænement et ne devrait pas ex√©cuter une cellule qui instancie un `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

Maintenant que nous avons envoy√© notre `train_dataloader` √† `accelerator.prepare()`, nous pouvons utiliser sa longueur pour calculer le nombre d'√©tapes d'entra√Ænement. Rappelez-vous que nous devrions toujours faire cela apr√®s avoir pr√©par√© le chargeur de donn√©es car cette m√©thode va changer la longueur du `DataLoader`. Nous utilisons un programme lin√©aire classique du taux d'apprentissage √† 0 :

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Enfin, pour pousser notre mod√®le vers le *Hub*, nous aurons besoin de cr√©er un objet `Repository` dans un dossier de travail. Tout d'abord, connectez-vous au *Hub* si vous n'√™tes pas d√©j√† connect√©. Nous d√©terminerons le nom du d√©p√¥t √† partir de l'identifiant du mod√®le que nous voulons donner √† notre mod√®le (n'h√©sitez pas √† remplacer le `repo_name` par votre propre choix, il doit juste contenir votre nom d'utilisateur, ce que fait la fonction `get_full_repo_name()`) :

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

Ensuite, nous pouvons cloner ce d√©p√¥t dans un dossier local. S'il existe d√©j√†, ce dossier local doit √™tre un clone du d√©p√¥t avec lequel nous travaillons :

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Nous pouvons maintenant t√©l√©charger tout ce que nous sauvegardons dans `output_dir` en appelant la m√©thode `repo.push_to_hub()`. Cela nous aidera √† t√©l√©charger les mod√®les interm√©diaires √† la fin de chaque √©poque.

### Boucle d'entra√Ænement

Nous sommes maintenant pr√™ts √† √©crire la boucle d'entra√Ænement compl√®te. Pour simplifier sa partie √©valuation, nous d√©finissons cette fonction `postprocess()` qui prend les pr√©dictions et les √©tiquettes et les convertit en listes de cha√Ænes de caract√®res que notre objet `metric` attend :

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Remplace -100 dans les √©tiquettes car nous ne pouvons pas les d√©coder
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Quelques post-traitements simples
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

La boucle d'entra√Ænement ressemble beaucoup √† celles de la [section 2](/course/fr/chapter7/2) et du [chapitre 3](/course/fr/chapter3), avec quelques diff√©rences dans la partie √©valuation. Donc concentrons-nous sur cela !

La premi√®re chose √† noter est que nous utilisons la m√©thode `generate()` pour calculer les pr√©dictions. C'est une m√©thode sur notre mod√®le de base et non pas le mod√®le envelopp√© cr√©√© dans la m√©thode `prepare()`. C'est pourquoi nous d√©ballons d'abord le mod√®le, puis nous appelons cette m√©thode.

La deuxi√®me chose est que, comme avec la classification de [*token*](/course/fr/chapter7/2), deux processus peuvent avoir rembourr√©s les entr√©es et les √©tiquettes √† des formes diff√©rentes. Ainsi nous utilisons `accelerator.pad_across_processes()` pour rendre les pr√©dictions et les √©tiquettes de la m√™me forme avant d'appeler la m√©thode `gather()`. Si nous ne faisons pas cela, l'√©valuation va soit se tromper, soit se bloquer pour toujours.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra√Ænement
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # N√©cessaire pour rembourrer les pr√©dictions et les √©tiquettes √† rassembler
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Sauvegarder et t√©l√©charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

Une fois que c'est fait, vous devriez avoir un mod√®le qui a des r√©sultats assez similaires √† celui entra√Æn√© avec `Seq2SeqTrainer`. Vous pouvez v√©rifier celui que nous avons entra√Æn√© en utilisant ce code sur [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). Et si vous voulez tester des modifications de la boucle d'entra√Ænement, vous pouvez les mettre en ≈ìuvre directement en modifiant le code ci-dessus !

{/if}

### Utilisation du mod√®le <i>finetun√©</i>

Nous vous avons d√©j√† montr√© comment vous pouvez utiliser le mod√®le que nous avons *finetun√©* sur le *Hub* avec le *widget* d'inf√©rence. Pour l'utiliser localement dans un `pipeline`, nous devons juste sp√©cifier l'identifiant de mod√®le appropri√© :

```py
from transformers import pipeline

# Remplacez ceci par votre propre checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par d√©faut, d√©velopper les fils de discussion'}]
```

Comme pr√©vu, notre mod√®le pr√©-entra√Æn√© a adapt√© ses connaissances au corpus sur lequel nous l'avons *finetun√©*. Et au lieu de laisser le mot anglais ¬´ *threads* ¬ª, le mod√®le le traduit maintenant par la version fran√ßaise officielle. Il en va de m√™me pour ¬´ *plugin* ¬ª :

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

Un autre excellent exemple d'adaptation au domaine !

<Tip>

‚úèÔ∏è **A votre tour !** Que retourne le mod√®le sur l'√©chantillon avec le mot ¬´ *email* ¬ª que vous avez identifi√© plus t√¥t ?

</Tip>
