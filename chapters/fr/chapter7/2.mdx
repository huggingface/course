<FrameworkSwitchCourse {fw} />

# Classification de <i>tokens</i>

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
]} />

{/if}

La premi√®re application que nous allons explorer est la classification de *tokens*. Cette t√¢che g√©n√©rique englobe tous les probl√®mes qui peuvent √™tre formul√©s comme l'attribution d'une √©tiquette √† chaque *token* d'une phrase, tels que :

- la **reconnaissance d'entit√©s nomm√©es (NER de l'anglais *Named Entity Recognition*)**, c'est-√†-dire trouver les entit√©s (telles que des personnes, des lieux ou des organisations) dans une phrase. Ce t√¢che peut √™tre formul√©e comme l'attribution d'une √©tiquette √† chaque *token* faisant parti d'une entit√© en ayant une classe sp√©cifique par entit√©, et une classe pour les *tokens* ne faisant pas parti d'entit√©.
- le ***part-of-speech tagging* (POS)**, c'est-√†-dire marquer chaque mot dans une phrase comme correspondant √† une partie particuli√®re (comme un nom, un verbe, un adjectif, etc.).
- le ***chunking***, c'est-√†-dire trouver les *tokens* qui appartiennent √† la m√™me entit√©. Cette t√¢che (qui peut √™tre combin√©e avec le POS ou la NER) peut √™tre formul√©e comme l'attribution d'une √©tiquette (habituellement `B-`) √† tous les *tokens* qui sont au d√©but d'un morceau, une autre √©tiquette (habituellement `I-`) aux *tokens* qui sont √† l'int√©rieur d'un morceau, et une troisi√®me √©tiquette (habituellement `O`) aux *tokens* qui n'appartiennent √† aucun morceau.

<Youtube id="wVHdVlPScxA"/>

Bien s√ªr, il existe de nombreux autres types de probl√®mes de classification de *tokens*. Ce ne sont l√† que quelques exemples repr√©sentatifs. Dans cette section, nous allons *finetuner* un mod√®le (BERT) sur la t√¢che de NER. Il sera alors capable de calculer des pr√©dictions comme celle-ci :

<iframe src="https://hf.space/gradioiframe/course-demos/bert-finetuned-ner/+" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://hf.space/gradioiframe/course-demos/bert-finetuned-ner-darkmode/+" frameBorder="0" height="350" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

Vous pouvez trouver, t√©l√©charger et v√©rifier les pr√©cisions de ce mod√®le sur le [*Hub*](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+nom+est+Sylvain+et+je+travaille+√†+Hugging+Face+in+Brooklyn) les pr√©dictions du mod√®le que nous allons entra√Æner.

## Pr√©paration des donn√©es

Tout d'abord, nous avons besoin d'un jeu de donn√©es adapt√© √† la classification des *tokens*. Dans cette section, nous utiliserons le jeu de donn√©es [CoNLL-2003](https://huggingface.co/datasets/conll2003), qui contient des articles de presse de Reuters. 

<Tip>

üí° Tant que votre jeu de donn√©es consiste en des textes divis√©s en mots avec leurs √©tiquettes correspondantes, vous pourrez adapter les proc√©dures de traitement des donn√©es d√©crites ici √† votre propre jeu de donn√©es. Reportez-vous au [chapitre 5](/course/fr/chapter5) si vous avez besoin d'un rafra√Æchissement sur la fa√ßon de charger vos propres donn√©es personnalis√©es dans un `Dataset`.

</Tip>

### Le jeu de donn√©es CoNLL-2003

Pour charger le jeu de donn√©es CoNLL-2003, nous utilisons la m√©thode `load_dataset()` de la biblioth√®que ü§ó *Datasets* :

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

Cela va t√©l√©charger et mettre en cache le jeu de donn√©es, comme nous l'avons vu dans [chapitre 3](/course/fr/chapter3) pour le jeu de donn√©es GLUE MRPC. L'inspection de cet objet nous montre les colonnes pr√©sentes dans ce jeu de donn√©es et la r√©partition entre les ensembles d'entra√Ænement, de validation et de test :

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

En particulier, nous pouvons voir que le jeu de donn√©es contient des √©tiquettes pour les trois t√¢ches que nous avons mentionn√©es pr√©c√©demment : NER, POS et *chunking*. Une grande diff√©rence avec les autres jeux de donn√©es est que les entr√©es textuelles ne sont pas pr√©sent√©s comme des phrases ou des documents, mais comme des listes de mots (la derni√®re colonne est appel√©e `tokens`, mais elle contient des mots dans le sens o√π ce sont des entr√©es pr√©tok√©nis√©es qui doivent encore passer par le *tokenizer* pour la tokenisation en sous-mots).

Regardons le premier √©l√©ment de l'ensemble d'entra√Ænement :

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

Puisque nous voulons effectuer reconna√Ætre des entit√©s nomm√©es, nous allons examiner les balises NER :

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

Ce sont les √©tiquettes sous forme d'entiers disponibles pour l'entra√Ænement mais ne sont pas n√©cessairement utiles lorsque nous voulons inspecter les donn√©es. Comme pour la classification de texte, nous pouvons acc√©der √† la correspondance entre ces entiers et les noms des √©tiquettes en regardant l'attribut `features` de notre jeu de donn√©es :

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

Cette colonne contient donc des √©l√©ments qui sont des s√©quences de `ClassLabel`. Le type des √©l√©ments de la s√©quence se trouve dans l'attribut `feature` de cette `ner_feature`, et nous pouvons acc√©der √† la liste des noms en regardant l'attribut `names` de cette `feature` :

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

Nous avons d√©j√† vu ces √©tiquettes au [chapitre 6](/course/fr/chapter6/3) lorsque nous nous sommes int√©ress√©s au pipeline `token-classification` mais nosu pouvons tout de m√™me faire un rapide rappel : 

- `O` signifie que le mot ne correspond √† aucune entit√©.
- `B-PER`/`I-PER` signifie que le mot correspond au d√©but/est √† l'int√©rieur d'une entit√© *personne*.
- `B-ORG`/`I-ORG` signifie que le mot correspond au d√©but/est √† l'int√©rieur d'une entit√© *organisation*.
- `B-LOC`/`I-LOC` signifie que le mot correspond au d√©but/est √† l'int√©rieur d'une entit√© *location*.
- `B-MISC`/`I-MISC` signifie que le mot correspond au d√©but/est √† l'int√©rieur d'une entit√© *divers*.

Maintenant, le d√©codage des √©tiquettes que nous avons vues pr√©c√©demment nous donne ceci :

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

Et pour un exemple m√©langeant les √©tiquettes `B-` et `I-`, voici ce que le m√™me code nous donne sur le quatri√®me √©l√©ment du jeu d'entra√Ænement :

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

Comme on peut le voir, les entit√©s couvrant deux mots, comme ¬´ European Union ¬ª et ¬´ Werner Zwingmann ¬ª, se voient attribuer une √©tiquette `B-` pour le premier mot et une √©tiquette `I-` pour le second.

<Tip>

‚úèÔ∏è *A votre tour !* Affichez les deux m√™mes phrases avec leurs √©tiquettes POS ou *chunking*.

</Tip>

### Traitement des donn√©es

<Youtube id="iY2AZYdZAr0"/>

Comme d'habitude, nos textes doivent √™tre convertis en identifiants de *tokens* avant que le mod√®le puisse leur donner un sens. Comme nous l'avons vu au [chapitre 6](/course/fr/chapter6/), une grande diff√©rence dans le cas des t√¢ches de classification de *tokens* est que nous avons des entr√©es pr√©tok√©nis√©es. Heureusement, l'API `tokenizer` peut g√©rer cela assez facilement. Nous devons juste avertir le `tokenizer` avec un drapeau sp√©cial.

Pour commencer, nous allons cr√©er notre objet `tokenizer`. Comme nous l'avons dit pr√©c√©demment, nous allons utiliser un mod√®le BERT pr√©-entra√Æn√©, donc nous allons commencer par t√©l√©charger et mettre en cache le *tokenizer* associ√© :

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Vous pouvez remplacer le `model_checkpoint` par tout autre mod√®le que vous pr√©f√©rez √† partir du [*Hub*](https://huggingface.co/models), ou par un dossier local dans lequel vous avez sauvegard√© un mod√®le pr√©-entra√Æn√© et un *tokenizer*. La seule contrainte est que le *tokenizer* doit √™tre soutenu par la biblioth√®que ü§ó *Tokenizers*. Il y a donc une version rapide disponible. Vous pouvez voir toutes les architectures qui ont une version rapide dans [ce tableau](https://huggingface.co/transformers/#supported-frameworks), et pour v√©rifier que l'objet `tokenizer` que vous utilisez est bien soutenu par ü§ó *Tokenizers* vous pouvez regarder son attribut `is_fast` :

```py
tokenizer.is_fast
```

```python out
True
```

Pour tokeniser une entr√©e pr√©tokenis√©e, nous pouvons utiliser notre `tokenizer` comme d'habitude et juste ajouter `is_split_into_words=True` :

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

Comme on peut le voir, le *tokenizer* a ajout√© les *tokens* sp√©ciaux utilis√©s par le mod√®le (`[CLS]` au d√©but et `[SEP]` √† la fin) et n'a pas touch√© √† la plupart des mots. Le mot `lamb`, cependant, a √©t√© tokenis√© en deux sous-mots, `la` et `##mb`. Cela introduit un d√©calage entre nos entr√©es et les √©tiquettes : la liste des √©tiquettes n'a que 9 √©l√©ments, alors que notre entr√©e a maintenant 12 *tokens*. Il est facile de tenir compte des *tokens* sp√©ciaux (nous savons qu'ils sont au d√©but et √† la fin), mais nous devons √©galement nous assurer que nous alignons toutes les √©tiquettes avec les mots appropri√©s.

Heureusement, comme nous utilisons un *tokenizer* rapide, nous avons acc√®s aux superpouvoirs des ü§ó *Tokenizers*, ce qui signifie que nous pouvons facilement faire correspondre chaque *token* au mot correspondant (comme on le voit au [chapitre 6](/course/fr/chapter6/3)) :

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

Avec un peu de travail, nous pouvons √©tendre notre liste d'√©tiquettes pour qu'elle corresponde aux *tokens*. La premi√®re r√®gle que nous allons appliquer est que les *tokens* sp√©ciaux re√ßoivent une √©tiquette de `-100`. En effet, par d√©faut, `-100` est un indice qui est ignor√© dans la fonction de perte que nous allons utiliser (l'entropie crois√©e). Ensuite, chaque *token* re√ßoit la m√™me √©tiquette que le *token* qui a commenc√© le mot dans lequel il se trouve puisqu'ils font partie de la m√™me entit√©. Pour les *tokens* √† l'int√©rieur d'un mot mais pas au d√©but, nous rempla√ßons le `B-` par `I-` (puisque le *token* ne commence pas l'entit√©) :

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # D√©but d'un nouveau mot !
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Token sp√©cial
            new_labels.append(-100)
        else:
            # M√™me mot que le token pr√©c√©dent
            label = labels[word_id]
            # Si l'√©tiquette est B-XXX, nous la changeons en I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

Essayons-le sur notre premi√®re phrase :

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

Comme nous pouvons le voir, notre fonction a ajout√© `-100` pour les deux *tokens* sp√©ciaux du d√©but et de fin, et un nouveau `0` pour notre mot qui a √©t√© divis√© en deux *tokens*.

<Tip>

‚úèÔ∏è *A votre tour !* Certains chercheurs pr√©f√®rent n'attribuer qu'une seule √©tiquette par mot et attribuer `-100` aux autres sous-*tokens* dans un mot donn√©. Ceci afin d'√©viter que les longs mots qui se divisent en plusieurs batchs ne contribuent fortement √† la perte. Changez la fonction pr√©c√©dente pour aligner les √©tiquettes avec les identifiants d'entr√©e en suivant cette r√®gle.
</Tip>

Pour pr√©traiter notre jeu de donn√©es, nous devons tokeniser toutes les entr√©es et appliquer `align_labels_with_tokens()` sur toutes les √©tiquettes. Pour profiter de la vitesse de notre *tokenizer* rapide, il est pr√©f√©rable de tokeniser beaucoup de textes en m√™me temps. Nous allons donc √©crire une fonction qui traite une liste d'exemples et utiliser la m√©thode `Dataset.map()` avec l'option `batched=True`. La seule chose qui diff√®re de notre exemple pr√©c√©dent est que la fonction `word_ids()` a besoin de r√©cup√©rer l'index de l'exemple dont nous voulons les identifiants de mots lorsque les entr√©es du *tokenizer* sont des listes de textes (ou dans notre cas, des listes de mots), donc nous l'ajoutons aussi :

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

Notez que nous n'avons pas encore rembourr√© nos entr√©es. Nous le ferons plus tard lors de la cr√©ation des batchs avec un collateur de donn√©es. 

Nous pouvons maintenant appliquer tout ce pr√©traitement en une seule fois sur les autres divisions de notre jeu de donn√©es :

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

Nous avons fait la partie la plus difficile ! Maintenant que les donn√©es ont √©t√© pr√©trait√©es, l'entra√Ænement ressemblera beaucoup √† ce que nous avons fait dans le [chapitre 3](/course/fr/chapter3).

{#if fw === 'pt'}

## <i>Finetuning</i> du mod√®le avec l'API `Trainer`

Le code utilisant `Trainer` sera le m√™me que pr√©c√©demment. Les seuls changements sont la fa√ßon dont les donn√©es sont rassembl√©es dans un batch ainsi que la fonction de calcul de la m√©trique.

{:else}

## <i>Finetuning</i> du mod√®le avec Keras

Le code utilisant Keras sera tr√®s similaire au pr√©c√©dent. Les seuls changements sont la fa√ßon dont les donn√©es sont rassembl√©es dans un batch ainsi que la fonction de calcul de la m√©trique.

{/if}


### Collation des donn√©es

Nous ne pouvons pas simplement utiliser un `DataCollatorWithPadding` comme dans le [chapitre 3](/course/fr/chapter3) car cela ne fait que rembourrer les entr√©es (identifiants d'entr√©e, masque d'attention et *token* de type identifiants). Ici, nos √©tiquettes doivent √™tre rembourr√©√©s exactement de la m√™me mani√®re que les entr√©es afin qu'elles gardent la m√™me taille, en utilisant `-100` comme valeur afin que les pr√©dictions correspondantes soient ignor√©es dans le calcul de la perte.

Tout ceci est fait par un [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification). Comme le `DataCollatorWithPadding`, il prend le `tokenizer` utilis√© pour pr√©traiter les entr√©es :

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

Pour tester cette fonction sur quelques √©chantillons, nous pouvons simplement l'appeler sur une liste d'exemples provenant de notre jeu d'entra√Ænement tok√©nis√© :

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

Comparons cela aux √©tiquettes des premier et deuxi√®me √©l√©ments de notre jeu de donn√©es :

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

Comme nous pouvons le voir, le deuxi√®me jeu d'√©tiquettes a √©t√© compl√©t√© √† la longueur du premier en utilisant des `-100`.

{:else}

Notre collateur de donn√©es est pr√™t √† fonctionner ! Maintenant, utilisons-le pour cr√©er un `tf.data.Dataset` avec la m√©thode `to_tf_dataset()`.

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```


Prochain arr√™t : le mod√®le lui-m√™me.

{/if}

{#if fw === 'tf'}

### D√©finir le mod√®le

Puisque nous travaillons sur un probl√®me de classification de *tokens*, nous allons utiliser la classe `TFAutoModelForTokenClassification`. La principale chose √† retenir lors de la d√©finition de ce mod√®le est de transmettre des informations sur le nombre d'√©tiquettes que nous avons. La fa√ßon la plus simple de le faire est de passer ce nombre avec l'argument `num_labels`, mais si nous voulons un joli *widget* d'inf√©rence fonctionnant comme celui que nous avons vu au d√©but de cette section, il est pr√©f√©rable de d√©finir les correspondances correctes des √©tiquettes √† la place.

Elles devraient √™tre d√©finies par deux dictionnaires, `id2label` et `label2id`, qui contiennent la correspondance de l'identifiant √† l'√©tiquette et vice versa :

```py
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

Maintenant, nous pouvons simplement les passer √† la m√©thode `TFAutoModelForTokenClassification.from_pretrained()`, et ils seront d√©finis dans la configuration du mod√®le puis correctement enregistr√©s et t√©l√©charg√©s vers le *Hub* :

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Comme lorsque nous avons d√©fini notre `TFAutoModelForSequenceClassification` au [chapitre 3](/course/fr/chapter3), la cr√©ation du mod√®le √©met un avertissement indiquant que certains poids n'ont pas √©t√© utilis√©s (ceux de la t√™te de pr√©-entra√Ænement) et que d'autres poids ont √©t√© initialis√©s de mani√®re al√©atoire (ceux de la t√™te de classification des nouveaux *tokens*), et que ce mod√®le doit √™tre entra√Æn√©. Nous ferons cela dans une minute mais v√©rifions d'abord que notre mod√®le a le bon nombre d'√©tiquettes :

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

‚ö†Ô∏è Si vous avez un mod√®le avec le mauvais nombre d'√©tiquettes, vous obtiendrez plus tard une erreur obscure lors de l'appel de `model.fit()`. Cela peut √™tre ennuyeux √† d√©boguer donc assurez-vous de faire cette v√©rification pour confirmer que vous avez le nombre d'√©tiquettes attendu.

</Tip>

### <i>Finetuning</i> du mod√®le

Nous sommes maintenant pr√™ts √† entra√Æner notre mod√®le ! Mais nous devons d'abord faire un peu de m√©nage : nous devons nous connecter √† Hugging Face et d√©finir nos hyperparam√®tres d'entra√Ænement. Si vous travaillez dans un *notebook*, il y a une fonction pratique pour vous aider √† le faire :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Cela affichera un *widget* o√π vous pourrez entrer vos identifiants de connexion √† Hugging Face.

Si vous ne travaillez pas dans un *notebook*, tapez simplement la ligne suivante dans votre terminal :

```bash
huggingface-cli login
```

Apr√®s s'√™tre connect√©, nous pouvons pr√©parer tout ce dont nous avons besoin pour compiler notre mod√®le. ü§ó *Transformers* fournit une fonction pratique `create_optimizer()` qui vous donnera un optimiseur `AdamW` avec des param√®tres appropri√©s pour le taux de d√©croissance des poids et le taux de d√©croissance de l'apprentissage, les deux am√©liorant les performances de votre mod√®le par rapport √† l'optimiseur `Adam` : 

```python
from transformers import create_optimizer
import tensorflow as tf

# Entra√Æner en mixed-precision float16
# Commentez cette ligne si vous utilisez un GPU qui ne b√©n√©ficiera pas de cette fonction
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# Le nombre d'√©tapes d'entra√Ænement est le nombre d'√©chantillons dans l'ensemble de donn√©es, divis√© par la taille du batch puis multipli√© par le nombre total d'√©poques
# par le nombre total d'√©poques. Notez que le jeu de donn√©es tf_train_dataset est ici un batchtf.data.Dataset,
# et non le jeu de donn√©es original Hugging Face Dataset, donc son len() est d√©j√† num_samples // batch_size
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

Notez √©galement que nous ne fournissons pas d'argument `loss` √† `compile()`. C'est parce que les mod√®les peuvent en fait calculer la perte en interne. Si vous compilez sans perte et fournissez vos √©tiquettes dans le dictionnaire d'entr√©e (comme nous le faisons dans nos jeux de donn√©es), alors le mod√®le s'entra√Ænera en utilisant cette perte interne, qui sera appropri√©e pour la t√¢che et le type de mod√®le que vous avez choisi.

Ensuite, nous d√©finissons un `PushToHubCallback` pour t√©l√©charger notre mod√®le vers le *Hub* pendant l'entra√Ænement, et nous ajustons le mod√®le avec ce *callback* :

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

Vous pouvez sp√©cifier le nom complet du d√©p√¥t vers lequel vous voulez pousser avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss√© le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/bert-finetuned-ner"`. Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace de noms et nomm√© apr√®s le r√©pertoire de sortie que vous avez d√©fini, par exemple `"cool_huggingface_user/bert-finetuned-ner"`.

<Tip>

üí° Si le r√©pertoire de sortie que vous utilisez existe d√©j√†, il doit √™tre un clone local du d√©p√¥t vers lequel vous voulez pousser. S'il ne l'est pas, vous obtiendrez une erreur lors de l'appel de `model.fit()` et devrez d√©finir un nouveau nom.

</Tip>

Notez que pendant l'entra√Ænement, chaque fois que le mod√®le est sauvegard√© (ici, √† chaque √©poque), il est t√©l√©charg√© sur le *Hub* en arri√®re-plan. De cette fa√ßon, vous pourrez reprendre votre entra√Ænement sur une autre machine si n√©cessaire.

A ce stade, vous pouvez utiliser le *widget* d'inf√©rence sur le *Hub* pour tester votre mod√®le et le partager avec vos amis. Vous avez r√©ussi √† *finetuner* un mod√®le sur une t√¢che de classification de *tokens*. F√©licitations ! Mais quelle est la qualit√© r√©elle de notre mod√®le ? Nous devons √©valuer certaines m√©triques pour le d√©couvrir.

{/if}


### M√©triques

{#if fw === 'pt'}

Pour que le `Trainer` calcule une m√©trique √† chaque √©poque, nous devrons d√©finir une fonction `compute_metrics()` qui prend les tableaux de pr√©dictions et d'√©tiquettes, et retourne un dictionnaire avec les noms et les valeurs des m√©triques. 

Le *framework* traditionnel utilis√© pour √©valuer la pr√©diction de la classification des *tokens* est [*seqeval*](https://github.com/chakki-works/seqeval). Pour utiliser cette m√©trique, nous devons d'abord installer la biblioth√®que *seqeval* :

```py
!pip install seqeval
```

Nous pouvons ensuite le charger via la fonction `evaluate.load()` comme nous l'avons fait dans le [chapitre 3](/course/fr/chapter3) :

{:else}

Le *framework*  traditionnel utilis√© pour √©valuer la pr√©diction de la classification des *tokens* est [*seqeval*](https://github.com/chakki-works/seqeval). Pour utiliser cette m√©trique, nous devons d'abord installer la biblioth√®que *seqeval* :

```py
!pip install seqeval
```

Nous pouvons ensuite le charger via la fonction `evaluate.load()` comme nous l'avons fait dans le [chapitre 3](/course/fr/chapter3) :

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

Cette m√©trique ne se comporte pas comme la pr√©cision standard : elle prend les listes d'√©tiquettes comme des cha√Ænes de caract√®res et non comme des entiers. Nous devrons donc d√©coder compl√®tement les pr√©dictions et les √©tiquettes avant de les transmettre √† la m√©trique. Voyons comment cela fonctionne. Tout d'abord, nous allons obtenir les √©tiquettes pour notre premier exemple d'entra√Ænement :

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

Nous pouvons alors cr√©er de fausses pr√©dictions pour celles-ci en changeant simplement la valeur de l'indice 2 :

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

Notez que la m√©trique prend une liste de pr√©dictions (pas seulement une) et une liste d'√©tiquettes. Voici la sortie :

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

Cela renvoie un batch d'informations ! Nous obtenons la pr√©cision, le rappel et le score F1 pour chaque entit√© s√©par√©e, ainsi que le score global. Pour notre calcul de m√©trique, nous ne garderons que le score global, mais n'h√©sitez pas √† modifier la fonction `compute_metrics()` pour retourner toutes les m√©triques que vous souhaitez.

Cette fonction `compute_metrics()` prend d'abord l'argmax des logits pour les convertir en pr√©dictions (comme d'habitude, les logits et les probabilit√©s sont dans le m√™me ordre, donc nous n'avons pas besoin d'appliquer la fonction softmax). Ensuite, nous devons convertir les √©tiquettes et les pr√©dictions des entiers en cha√Ænes de caract√®res. Nous supprimons toutes les valeurs dont l'√©tiquette est `-100`, puis nous passons les r√©sultats √† la m√©thode `metric.compute()` :

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Suppression de l'index ignor√© (tokens sp√©ciaux) et conversion en √©tiquettes
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

Maintenant que ceci est fait, nous sommes presque pr√™ts √† d√©finir notre `Trainer`. Nous avons juste besoin d'un objet `model` pour *finetuner* !

{:else}

Cela renvoie un batch d'informations ! Nous obtenons la pr√©cision, le rappel et le score F1 pour chaque entit√© s√©par√©e, ainsi que pour l'ensemble. Voyons maintenant ce qui se passe si nous essayons d'utiliser les pr√©dictions de notre mod√®le pour calculer des scores r√©els.

TensorFlow n'aime pas concat√©ner nos pr√©dictions ensemble car elles ont des longueurs de s√©quence variables. Cela signifie que nous ne pouvons pas simplement utiliser `model.predict()`. Mais cela ne va pas nous arr√™ter. Nous obtiendrons des pr√©dictions un batch √† la fois et les concat√©nerons en une grande liste longue au fur et √† mesure et en laissant de c√¥t√© les *tokens* `-100` qui indiquent le masquage/le remplissage. Puis nous calculerons les m√©triques sur la liste √† la fin :

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

Comment s'est comport√© votre mod√®le, compar√© au n√¥tre ? Si vous avez obtenu des chiffres similaires, votre entra√Ænement a √©t√© un succ√®s !

{/if}

{#if fw === 'pt'}

### D√©finir le mod√®le

Puisque nous travaillons sur un probl√®me de classification de *tokens*, nous allons utiliser la classe `AutoModelForTokenClassification`. La principale chose √† retenir lors de la d√©finition de ce mod√®le est de transmettre des informations sur le nombre d'√©tiquettes que nous avons. La fa√ßon la plus simple de le faire est de passer ce nombre avec l'argument `num_labels`, mais si nous voulons un joli *widget* d'inf√©rence fonctionnant comme celui que nous avons vu au d√©but de cette section, il est pr√©f√©rable de d√©finir les correspondances des √©tiquettes √† la place.

Elles devraient √™tre d√©finies par deux dictionnaires, `id2label` et `label2id`, qui contiennent les correspondances entre identifiants et √©tiquettes et vice versa :

```py
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

Maintenant nous pouvons simplement les passer √† la m√©thode `AutoModelForTokenClassification.from_pretrained()`, ils seront d√©finis dans la configuration du mod√®le puis correctement sauvegard√©s et t√©l√©charg√©s vers le *Hub* :

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Comme lorsque nous avons d√©fini notre `AutoModelForSequenceClassification` au [chapitre 3](/course/fr/chapter3), la cr√©ation du mod√®le √©met un avertissement indiquant que certains poids n'ont pas √©t√© utilis√©s (ceux de la t√™te de pr√©-entra√Ænement) et que d'autres poids ont √©t√© initialis√©s de mani√®re al√©atoire (ceux de la t√™te de classification des nouveaux *tokens*), et que ce mod√®le doit √™tre entra√Æn√©. Nous ferons cela dans une minute, mais v√©rifions d'abord que notre mod√®le a le bon nombre d'√©tiquettes :

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

‚ö†Ô∏è Si vous avez un mod√®le avec le mauvais nombre d'√©tiquettes, vous obtiendrez une erreur obscure lors de l'appel de la m√©thode `Trainer.train()` (quelque chose comme "CUDA error : device-side assert triggered"). C'est la premi√®re cause de bogues signal√©s par les utilisateurs pour de telles erreurs, donc assurez-vous de faire cette v√©rification pour confirmer que vous avez le nombre d'√©tiquettes attendu.

</Tip>

### <i>Finetuning</i> du mod√®le

Nous sommes maintenant pr√™ts √† entra√Æner notre mod√®le ! Nous devons juste faire deux derni√®res choses avant de d√©finir notre `Trainer` : se connecter √† Hugging Face et d√©finir nos arguments d'entra√Ænement. Si vous travaillez dans un *notebook*, il y a une fonction pratique pour vous aider √† le faire :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Cela affichera un *widget* o√π vous pourrez entrer vos identifiants de connexion √† Hugging Face.

Si vous ne travaillez pas dans un *notebook*, tapez simplement la ligne suivante dans votre terminal :

```bash
huggingface-cli login
```

Une fois ceci fait, nous pouvons d√©finir nos `TrainingArguments` :

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

Vous avez d√©j√† vu la plupart d'entre eux. Nous d√©finissons quelques hyperparam√®tres (comme le taux d'apprentissage, le nombre d'√©poques √† entra√Æner, et le taux de d√©croissance des poids), et nous sp√©cifions `push_to_hub=True` pour indiquer que nous voulons sauvegarder le mod√®le, l'√©valuer √† la fin de chaque √©poque, et que nous voulons t√©l√©charger nos r√©sultats vers le *Hub*. Notez que vous pouvez sp√©cifier le nom du d√©p√¥t vers lequel vous voulez pousser avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss√© le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/bert-finetuned-ner"``TrainingArguments`. Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace de noms et nomm√© d'apr√®s le r√©pertoire de sortie que vous avez d√©fini, donc dans notre cas ce sera `"sgugger/bert-finetuned-ner"`.

<Tip>

üí° Si le r√©pertoire de sortie que vous utilisez existe d√©j√†, il doit √™tre un clone local du d√©p√¥t vers lequel vous voulez pousser. S'il ne l'est pas, vous obtiendrez une erreur lors de la d√©finition de votre `Trainer` et devrez d√©finir un nouveau nom.

</Tip>

Enfin, nous passons tout au `Trainer` et lan√ßons l'entra√Ænement :

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

Notez que pendant l'entra√Ænement, chaque fois que le mod√®le est sauvegard√© (ici, √† chaque √©poque), il est t√©l√©charg√© sur le *Hub* en arri√®re-plan. De cette fa√ßon, vous serez en mesure de reprendre votre entra√Ænement sur une autre machine si n√©cessaire.

Une fois l'entra√Ænement termin√©, nous utilisons la m√©thode `push_to_hub()` pour nous assurer que nous t√©l√©chargeons la version la plus r√©cente du mod√®le :

```py
trainer.push_to_hub(commit_message="Training complete")
```

Cette commande renvoie l'URL du commit qu'elle vient de faire, si vous voulez l'inspecter :

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

Le `Trainer` r√©dige √©galement une carte mod√®le avec tous les r√©sultats de l'√©valuation et la t√©l√©charge. A ce stade, vous pouvez utiliser le *widget* d'inf√©rence sur le *Hub* pour tester votre mod√®le et le partager avec vos amis. Vous avez r√©ussi √† affiner un mod√®le sur une t√¢che de classification de *tokens*. F√©licitations !

Si vous voulez plonger un peu plus profond√©ment dans la boucle d'entra√Ænement, nous allons maintenant vous montrer comment faire la m√™me chose en utilisant ü§ó *Accelerate*.

## Une boucle d'entra√Ænement personnalis√©e

Jetons maintenant un coup d'≈ìil √† la boucle d'entra√Ænement compl√®te afin que vous puissiez facilement personnaliser les parties dont vous avez besoin. Elle ressemblera beaucoup √† ce que nous avons fait dans le [chapitre 3](/course/fr/chapter3/4) avec quelques changements pour l'√©valuation.

### Pr√©parer tout pour l'entra√Ænement

D'abord nous devons construire le `DataLoader`s √† partir de nos jeux de donn√©es. Nous r√©utilisons notre `data_collator` comme un `collate_fn` et m√©langer l'ensemble d'entra√Ænement, mais pas l'ensemble de validation :

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

Ensuite, nous r√©instantifions notre mod√®le pour nous assurer que nous ne continuons pas le *finetuning* d'avant et que nous repartons bien du mod√®le pr√©-entra√Æn√© de BERT :

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Ensuite, nous avons besoin d'un optimiseur. Nous utilisons le classique `AdamW`, qui est comme `Adam`, mais avec un correctif dans la fa√ßon dont le taux de d√©croissance des poids est appliqu√©e :

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Une fois que nous avons tous ces objets, nous pouvons les envoyer √† la m√©thode `accelerator.prepare()` :

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® Si vous entra√Ænez sur un TPU, vous devrez d√©placer tout le code √† partir de la cellule ci-dessus dans une fonction d'entra√Ænement d√©di√©e. Voir le [chapitre 3](/course/fr/chapter3) pour plus de d√©tails.

</Tip>

Maintenant que nous avons envoy√© notre `train_dataloader` √† `accelerator.prepare()`, nous pouvons utiliser sa longueur pour calculer le nombre d'√©tapes d'entra√Ænement. Rappelez-vous que nous devrions toujours faire cela apr√®s avoir pr√©par√© le *dataloader* car cette m√©thode modifiera sa longueur. Nous utilisons un programme lin√©aire classique du taux d'apprentissage √† 0 :

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Enfin, pour pousser notre mod√®le vers le *Hub*, nous avons besoin de cr√©er un objet `Repository` dans un dossier de travail. Tout d'abord, connectez-vous √† Hugging Face si vous n'√™tes pas d√©j√† connect√©. Nous d√©terminons le nom du d√©p√¥t √† partir de l'identifiant du mod√®le que nous voulons donner √† notre mod√®le (n'h√©sitez pas √† remplacer le `repo_name` par votre propre choix, il doit juste contenir votre nom d'utilisateur et ce que fait la fonction `get_full_repo_name()`) :

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

Ensuite, nous pouvons cloner ce d√©p√¥t dans un dossier local. S'il existe d√©j√†, ce dossier local doit √™tre un clone existant du d√©p√¥t avec lequel nous travaillons :

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Nous pouvons maintenant t√©l√©charger tout ce que nous sauvegardons dans `output_dir` en appelant la m√©thode `repo.push_to_hub()`. Cela nous aidera √† t√©l√©charger les mod√®les interm√©diaires √† la fin de chaque √©poque.

### Boucle d'entra√Ænement

Nous sommes maintenant pr√™ts √† √©crire la boucle d'entra√Ænement compl√®te. Pour simplifier sa partie √©valuation, nous d√©finissons cette fonction `postprocess()` qui prend les pr√©dictions et les √©tiquettes, et les convertit en listes de cha√Ænes de caract√®res comme notre objet `metric` l'attend :

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Suppression de l'index ignor√© (tokens sp√©ciaux) et conversion en √©tiquettes
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

Ensuite, nous pouvons √©crire la boucle d'entra√Ænement. Apr√®s avoir d√©fini une barre de progression pour suivre l'√©volution de l'entra√Ænement, la boucle comporte trois parties :

- L'entra√Ænement proprement dit, qui est l'it√©ration classique sur le `train_dataloader`, passage en avant, puis passage en arri√®re et √©tape d'optimisation.
- L'√©valuation, dans laquelle il y a une nouveaut√© apr√®s avoir obtenu les sorties de notre mod√®le sur un batch : puisque deux processus peuvent avoir padd√© les entr√©es et les √©tiquettes √† des formes diff√©rentes, nous devons utiliser `accelerator.pad_across_processes()` pour rendre les pr√©dictions et les √©tiquettes de la m√™me forme avant d'appeler la m√©thode `gather()`. Si nous ne le faisons pas, l'√©valuation va soit se tromper, soit se bloquer pour toujours. Ensuite, nous envoyons les r√©sultats √† `metric.add_batch()` et appelons `metric.compute()` une fois que la boucle d'√©valuation est termin√©e.
- Sauvegarde et t√©l√©chargement, o√π nous sauvegardons d'abord le mod√®le et le *tokenizer*, puis appelons `repo.push_to_hub()`. Remarquez que nous utilisons l'argument `blocking=False` pour indiquer √† la biblioth√®que ü§ó *Hub* de pousser dans un processus asynchrone. De cette fa√ßon, l'entra√Ænement continue normalement et cette (longue) instruction est ex√©cut√©e en arri√®re-plan.

Voici le code complet de la boucle d'entra√Ænement :

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra√Ænement
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # N√©cessaire pour rembourrer les pr√©dictions et les √©tiquettes √† rassembler
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Sauvegarder et t√©l√©charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

Au cas o√π ce serait la premi√®re fois que vous verriez un mod√®le enregistr√© avec ü§ó *Accelerate*, prenons un moment pour inspecter les trois lignes de code qui l'accompagnent :

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

La premi√®re ligne est explicite : elle indique √† tous les processus d'attendre que tout le monde soit √† ce stade avant de continuer. C'est pour s'assurer que nous avons le m√™me mod√®le dans chaque processus avant de sauvegarder. Ensuite, nous prenons le `unwrapped_model` qui est le mod√®le de base que nous avons d√©fini. La m√©thode `accelerator.prepare()` modifie le mod√®le pour qu'il fonctionne dans l'entra√Ænement distribu√©, donc il n'aura plus la m√©thode `save_pretrained()` ; la m√©thode `accelerator.unwrap_model()` annule cette √©tape. Enfin, nous appelons `save_pretrained()` mais nous disons √† cette m√©thode d'utiliser `accelerator.save()` au lieu de `torch.save()`. 

Une fois ceci fait, vous devriez avoir un mod√®le qui produit des r√©sultats assez similaires √† celui entra√Æn√© avec le `Trainer`. Vous pouvez v√©rifier le mod√®le que nous avons form√© en utilisant ce code √† [*huggingface-course/bert-finetuned-ner-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate). Et si vous voulez tester des modifications de la boucle d'entra√Ænement, vous pouvez les impl√©menter directement en modifiant le code ci-dessus !

{/if}

### Utilisation du mod√®le <i>finetun√©</i>

Nous vous avons d√©j√† montr√© comment vous pouvez utiliser le mod√®le *finetun√©* sur le *Hub* avec le *widget* d'inf√©rence. Pour l'utiliser localement dans un `pipeline`, vous devez juste sp√©cifier l'identifiant de mod√®le appropri√© :

```py
from transformers import pipeline

# Remplacez ceci par votre propre checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Super ! Notre mod√®le fonctionne aussi bien que le mod√®le par d√©faut pour ce pipeline !

