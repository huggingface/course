<FrameworkSwitchCourse {fw} />

# R√©ponse aux questions

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section7_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section7_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section7_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section7_tf.ipynb"},
]} />

{/if}

Il est temps de s'int√©resser √† la r√©ponse aux questions ! Cette t√¢che peut prendre plusieurs formes mais celle sur laquelle nous allons nous concentrer dans cette section est appel√©e r√©ponse aux questions *extractives*. Il s'agit de poser des questions sur un document et d'identifier les r√©ponses sous forme de ¬´ d'√©tendue de texte ¬ª dans le document lui-m√™me.

<Youtube id="ajPx5LwJD-I"/>

Nous allons *finetuner* un mod√®le BERT sur le [jeu de donn√©es SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), qui consiste en des questions pos√©es par des *crowdworkers* sur un ensemble d'articles de Wikipedia. Cela nous donnera un mod√®le capable de calculer des pr√©dictions comme celui-ci :

<iframe src="https://hf.space/gradioiframe/course-demos/bert-finetuned-squad/+" frameBorder="0" height="450" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://hf.space/gradioiframe/course-demos/bert-finetuned-squad-darkmode/+" frameBorder="0" height="450" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Il s'agit d'une pr√©sentation du mod√®le qui a √©t√© entra√Æn√© √† l'aide du code pr√©sent√© dans cette section et qui a ensuit√© √©t√© t√©l√©charg√© sur le *Hub*. Vous pouvez le trouver [ici](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F)

<Tip>

üí° Les mod√®les bas√© que sur l'encodeur comme BERT ont tendance √† √™tre excellents pour extraire les r√©ponses √† des questions factuelles comme ¬´ Qui a invent√© l'architecture Transformer ? ¬ª mais ne sont pas tr√®s performants lorsqu'on leur pose des questions ouvertes comme ¬´ Pourquoi le ciel est-il bleu ? ¬ª. Dans ces cas plus difficiles, les mod√®les encodeurs-d√©codeurs comme le T5 et BART sont g√©n√©ralement utilis√©s pour synth√©tiser les informations d'une mani√®re assez similaire au [r√©sum√© de texte](/course/fr/chapter7/5). Si vous √™tes int√©ress√© par ce type de r√©ponse aux questions *g√©n√©ratives*, nous vous recommandons de consulter notre [d√©mo](https://yjernite.github.io/lfqa.html) bas√©e sur le [jeu de donn√©es ELI5](https://huggingface.co/datasets/eli5).

</Tip>

## Pr√©paration des donn√©es

Le jeu de donn√©es le plus utilis√© comme r√©f√©rence acad√©mique pour la r√©ponse extractive aux questions est [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/). C'est donc celui que nous utiliserons ici. Il existe √©galement une version plus difficile [SQuAD v2](https://huggingface.co/datasets/squad_v2), qui comprend des questions sans r√©ponse. Tant que votre propre jeu de donn√©es contient une colonne pour les contextes, une colonne pour les questions et une colonne pour les r√©ponses, vous devriez √™tre en mesure d'adapter les √©tapes ci-dessous.

### Le jeu de donn√©es SQuAD

Comme d'habitude, nous pouvons t√©l√©charger et mettre en cache le jeu de donn√©es en une seule √©tape gr√¢ce √† `load_dataset()` :

```py
from datasets import load_dataset

raw_datasets = load_dataset("squad")
```

Nous pouvons jeter un coup d'≈ìil √† cet objet pour en savoir plus sur le jeu de donn√©es SQuAD :

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})
```

On dirait que nous avons tout ce dont nous avons besoin avec les champs `context`, `question` et `answers`. Affichons-les pour le premier √©l√©ment de notre ensemble d'entra√Ænement :

```py
print("Context: ", raw_datasets["train"][0]["context"])
print("Question: ", raw_datasets["train"][0]["question"])
print("Answer: ", raw_datasets["train"][0]["answers"])
```

```python out
Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'
# Sur le plan architectural, l'√©cole a un caract√®re catholique. Au sommet du d√¥me dor√© du b√¢timent principal se trouve une statue dor√©e de la Vierge Marie. Imm√©diatement devant le b√¢timent principal et face √† lui, se trouve une statue en cuivre du Christ, les bras lev√©s, avec la l√©gende "Venite Ad Me Omnes". √Ä c√¥t√© du b√¢timent principal se trouve la basilique du Sacr√©-C≈ìur. Imm√©diatement derri√®re la basilique se trouve la Grotte, un lieu marial de pri√®re et de r√©flexion. Il s'agit d'une r√©plique de la grotte de Lourdes, en France, o√π la Vierge Marie serait apparue √† Sainte Bernadette Soubirous en 1858. Au bout de l'all√©e principale (et dans une ligne directe qui passe par 3 statues et le D√¥me d'or), se trouve une statue de pierre simple et moderne de Marie'.
Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?' 
# A qui la Vierge Marie serait-elle apparue en 1858 √† Lourdes, en France ?
Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
```

Les champs `context` et `question` sont tr√®s simples √† utiliser. Le champ `answers` est un peu plus d√©licat car il compile un dictionnaire avec deux champs qui sont tous deux des listes. C'est le format qui sera attendu par la m√©trique `squad` lors de l'√©valuation. Si vous utilisez vos propres donn√©es, vous n'avez pas n√©cessairement besoin de vous soucier de mettre les r√©ponses dans le m√™me format. Le champ `text` est assez √©vident et le champ `answer_start` contient l'indice du caract√®re de d√©part de chaque r√©ponse dans le contexte.

Pendant l'entra√Ænement, il n'y a qu'une seule r√©ponse possible. Nous pouvons v√©rifier cela en utilisant la m√©thode `Dataset.filter()` :

```py
raw_datasets["train"].filter(lambda x: len(x["answers"]["text"]) != 1)
```

```python out
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
```

Pour l'√©valuation, cependant, il existe plusieurs r√©ponses possibles pour chaque √©chantillon, qui peuvent √™tre identiques ou diff√©rentes :

```py
print(raw_datasets["validation"][0]["answers"])
print(raw_datasets["validation"][2]["answers"])
```

```python out
{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}
{'text': ['Santa Clara, California', "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], 'answer_start': [403, 355, 355]}
```

Nous ne nous plongerons pas dans le script d'√©valuation car tout sera envelopp√© pour nous par une m√©trique de ü§ó *Datasets*. La version courte est que certaines des questions ont plusieurs r√©ponses possibles, et ce script va comparer une r√©ponse pr√©dite √† toutes les r√©ponses acceptables et prendre le meilleur score. Par exemple, si nous regardons l'√©chantillon de l'indice 2 :

```py
print(raw_datasets["validation"][2]["context"])
print(raw_datasets["validation"][2]["question"])
```

```python out
'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.'
# Le Super Bowl 50 √©tait un match de football am√©ricain visant √† d√©terminer le champion de la National Football League (NFL) pour la saison 2015. Les Denver Broncos, champions de la Conf√©rence de football am√©ricain (AFC), ont battu les Carolina Panthers, champions de la Conf√©rence nationale de football (NFC), 24 √† 10, pour remporter leur troisi√®me titre de Super Bowl. Le match s'est d√©roul√© le 7 f√©vrier 2016 au Levi\'s Stadium, dans la baie de San Francisco, √† Santa Clara, en Californie. Comme il s'agissait du 50e Super Bowl, la ligue a mis l'accent sur l'" anniversaire dor√© " avec diverses initiatives sur le th√®me de l'or, ainsi qu'en suspendant temporairement la tradition de nommer chaque match du Super Bowl avec des chiffres romains (en vertu de laquelle le match aurait √©t√© appel√© " Super Bowl L "), afin que le logo puisse mettre en √©vidence les chiffres arabes 50.''
'Where did Super Bowl 50 take place?' 
# O√π a eu lieu le Super Bowl 50 ?
```

nous pouvons voir que la r√©ponse peut effectivement √™tre l'une des trois possibilit√©s que nous avons vues pr√©c√©demment.

### Traitement des donn√©es d'entra√Ænement

<Youtube id="qgaM0weJHpA"/>

Commen√ßons par le pr√©traitement des donn√©es d'entra√Ænement. La partie la plus difficile est de g√©n√©rer des √©tiquettes pour la r√©ponse √† la question, c'est-√†-dire les positions de d√©but et de fin des *tokens* correspondant √† la r√©ponse dans le contexte.

Mais ne nous emballons pas. Tout d'abord, √† l'aide d'un *tokenizer*, nous devons convertir le texte d'entr√©e en identifiants que le mod√®le peut comprendre :

```py
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Comme mentionn√© pr√©c√©demment, nous allons *finetuner* un mod√®le BERT, mais vous pouvez utiliser n'importe quel autre type de mod√®le tant qu'il a un *tokenizer* rapide impl√©ment√©. Vous pouvez voir toutes les architectures qui sont livr√©es avec un *tokenizer* rapide dans [ce tableau](https://huggingface.co/transformers/#supported-frameworks), et pour v√©rifier que l'objet `tokenizer` que vous utilisez est bien soutenu par ü§ó *Tokenizers* vous pouvez regarder son attribut `is_fast` :

```py
tokenizer.is_fast
```

```python out
True
```

Nous pouvons transmettre √† notre *tokenizer* la question et le contexte ensemble. Il ins√©rera correctement les *tokens* sp√©ciaux pour former une phrase comme celle-ci :

```
[CLS] question [SEP] context [SEP]
```

V√©rifions √† nouveau :

```py
context = raw_datasets["train"][0]["context"]
question = raw_datasets["train"][0]["question"]

inputs = tokenizer(question, context)
tokenizer.decode(inputs["input_ids"])
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '
'the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin '
'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '
'upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred '
'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '
'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '
'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '
'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'

'[CLS] A qui la Vierge Marie serait-elle apparue en 1858 √† Lourdes en France ? [SEP] Architecturalement, '
'l √©cole a un caract√®re catholique. Au sommet du d√¥me dor√© du b√¢timent principal se trouve une statue dor√©e de la Vierge '
'Marie. Imm√©diatement devant le b√¢timent principal et face √† lui, se trouve une statue en cuivre du Christ, les bras '
'lev√©s avec la l√©gende " Venite Ad Me Omnes ". A c√¥t√© du b√¢timent principal se trouve la basilique du Sacr√© '
'C≈ìur. Imm√©diatement derri√®re la basilique se trouve la Grotte, un lieu marial de pri√®re et de r√©flexion. Il s'agit d'une '
'r√©plique de la grotte de Lourdes, en France, o√π la Vierge Marie serait apparue √† Sainte Bernadette '
'Soubirous en 1858. Au bout de l'all√©e principale ( et en ligne directe qui passe par 3 statues '
'et le D√¥me d'or), se trouve une statue de Marie en pierre, simple et moderne. [SEP]'
```

Les √©tiquettes sont l'index des *tokens* de d√©but et de fin de la r√©ponse. Le mod√®le sera charg√© de pr√©dire dans l'entr√©e un logit de d√©but et de fin par *token*, les √©tiquettes th√©oriques √©tant les suivantes :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels-dark.svg" alt="One-hot encoded labels for question answering."/>
</div>

Dans ce cas, le contexte n'est pas trop long, mais certains des exemples du jeu de donn√©es ont des contextes tr√®s longs qui d√©passeront la longueur maximale que nous avons fix√©e (qui est de 384 dans ce cas). Comme nous l'avons vu dans le [chapitre 6](/course/fr/chapter6/4) lorsque nous avons explor√© le pipeline de `question-answering`, nous allons traiter les contextes longs en cr√©ant plusieurs caract√©ristiques d'entra√Ænement √† partir d'un √©chantillon de notre jeu de donn√©es et avec une fen√™tre glissante entre eux.

Pour voir comment cela fonctionne sur notre exemple, nous pouvons limiter la longueur √† 100 et utiliser une fen√™tre glissante de 50 *tokens*. Pour rappel, nous utilisons :

- `max_length` pour d√©finir la longueur maximale (ici 100)
- `truncation="only_second"` pour tronquer le contexte (qui est en deuxi√®me position) quand la question avec son contexte est trop longue
- `stride` pour fixer le nombre de *tokens* se chevauchant entre deux morceaux successifs (ici 50)
- `return_overflowing_tokens=True` pour indiquer au *tokenizer* que l'on veut les *tokens* qui d√©bordent

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'
'[CLS] A qui la Vierge Marie serait-elle apparue en 1858 √† Lourdes en France ? [SEP] Sur le plan architectural, l √©cole a un caract√®re catholique. Au sommet du d√¥me dor√© du b√¢timent principal se trouve une statue dor√©e de la Vierge Marie. Imm√©diatement devant le b√¢timent principal et face √† lui, se trouve une statue en cuivre du Christ, les bras lev√©s, avec la l√©gende " Venite Ad Me Omnes ". √Ä c√¥t√© du b√¢timent principal se trouve la basilique du Sacr√©-C≈ìur. Imm√©diatement derri√®re la basi [SEP]'

'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'
'[CLS] A qui la Vierge Marie serait-elle apparue en 1858 √† Lourdes en France ? [SEP] le b√¢timent principal et face √† lui, une statue en cuivre du Christ aux bras lev√©s avec la l√©gende " Venite Ad Me Omnes ". √Ä c√¥t√© du b√¢timent principal se trouve la basilique du Sacr√©-C≈ìur. Imm√©diatement derri√®re la basilique se trouve la Grotte, un lieu marial de pri√®re et de r√©flexion. Il s agit d'une r√©plique de la grotte de Lourdes, en France, o√π la Vierge [SEP]'

'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'
'[CLS] A qui la Vierge Marie serait-elle apparue en 1858 √† Lourdes en France ? [SEP] A c√¥t√© du b√¢timent principal se trouve la basilique du Sacr√©-C≈ìur. Imm√©diatement derri√®re la basilique se trouve la Grotte, un lieu marial de pri√®re et de r√©flexion. Il s agit d une r√©plique de la grotte de Lourdes, en France, o√π la Vierge Marie serait apparue √† Sainte Bernadette Soubirous en 1858. Au bout de l all√©e principale ( et dans une ligne directe qui relie par 3 [SEP]'

'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
'[CLS] A qui la Vierge Marie est-elle pr√©tendument apparue en 1858 √† Lourdes France ? [SEP]. Il s agit d une r√©plique de la grotte de Lourdes, en France, o√π la Vierge Marie serait apparue √† Sainte Bernadette Soubirous en 1858. Au bout de l all√©e principale (et dans une ligne directe qui passe par 3 statues et le D√¥me d or), se trouve une simple statue de pierre moderne de Marie. [SEP]'
```

Comme nous pouvons le voir, notre exemple a √©t√© divis√© en quatre entr√©es, chacune d'entre elles contenant la question et une partie du contexte. Notez que la r√©ponse √† la question (¬´ Bernadette Soubirous ¬ª) n'appara√Æt que dans la troisi√®me et la derni√®re entr√©e. Donc en traitant les longs contextes de cette fa√ßon, nous allons cr√©er quelques exemples d'entra√Ænement o√π la r√©ponse n'est pas incluse dans le contexte. Pour ces exemples, les √©tiquettes seront `start_position = end_position = 0` (donc nous pr√©disons le *token* `[CLS]`). Nous d√©finirons √©galement ces √©tiquettes dans le cas malheureux o√π la r√©ponse a √©t√© tronqu√©e de sorte que nous n'avons que le d√©but (ou la fin) de celle-ci. Pour les exemples o√π la r√©ponse est enti√®rement dans le contexte, les √©tiquettes seront l'index du *token* o√π la r√©ponse commence et l'index du *token* o√π la r√©ponse se termine.

Le jeu de donn√©es nous fournit le caract√®re de d√©but de la r√©ponse dans le contexte, et en ajoutant la longueur de la r√©ponse, nous pouvons trouver le caract√®re de fin dans le contexte. Pour faire correspondre ces indices aux *tokens*, nous devrons utiliser les correspondances *offset* que nous avons √©tudi√©s au [chapitre 6](/course/fr/chapter6/4). Nous pouvons faire en sorte que notre *tokenizer* renvoie ces index en passant `return_offsets_mapping=True` :

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
inputs.keys()
```

```python out
dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])
```

Comme nous pouvons le voir, nous r√©cup√©rons les identifiants d'entr√©e, les *tokens* de type identifiant, le masque d'attention, ainsi que la correspondance *offset* dont nous avions besoin et une cl√© suppl√©mentaire, `overflow_to_sample_mapping`. La valeur correspondante nous sera utile lorsque nous tokeniserons plusieurs textes en m√™me temps (ce que nous devrions faire pour b√©n√©ficier du fait que notre *tokenizer* est en Rust). Puisqu'un √©chantillon peut donner plusieurs caract√©ristiques, il fait correspondre chaque caract√©ristique √† l'exemple d'o√π elle provient. Parce qu'ici nous avons seulement tokenis√© un exemple, nous obtenons une liste de `0` :

```py
inputs["overflow_to_sample_mapping"]
```

```python out
[0, 0, 0, 0]
```

Mais si nous tokenisons davantage d'exemples, cela deviendra plus utile :

```py
inputs = tokenizer(
    raw_datasets["train"][2:6]["question"],
    raw_datasets["train"][2:6]["context"],
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)

print(f"The 4 examples gave {len(inputs['input_ids'])} features.")
print(f"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.")
```

```python out
'The 4 examples gave 19 features.'
'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'
```

Comme nous pouvons le voir, les trois premiers exemples (aux indices 2, 3 et 4 de l'ensemble d'entra√Ænement) ont chacun donn√© quatre caract√©ristiques et le dernier exemple (√† l'indice 5 de l'ensemble d'entra√Ænement) a donn√© 7 caract√©ristiques.

Ces informations seront utiles pour associer chaque caract√©ristique obtenue √† son √©tiquette correspondante. Comme mentionn√© pr√©c√©demment, ces √©tiquettes sont :

- `(0, 0)` si la r√©ponse n'est pas dans l'espace correspondant du contexte.
- `(start_position, end_position)` si la r√©ponse est dans l'espace correspondant du contexte, avec `start_position` √©tant l'index du *token* (dans les identifiants d'entr√©e) au d√©but de la r√©ponse et `end_position` √©tant l'index du *token* (dans les identifiants d'entr√©e) o√π la r√©ponse se termine.

Pour d√©terminer ce qui est le cas et, le cas √©ch√©ant, les positions des *tokens*, nous trouvons d'abord les indices qui commencent et finissent le contexte dans les identifiants d'entr√©e. Nous pourrions utiliser les *tokens* de type identifiants pour le faire, mais puisque ceux-ci n'existent pas n√©cessairement pour tous les mod√®les (DistilBERT ne les requiert pas par exemple), nous allons plut√¥t utiliser la m√©thode `sequence_ids()` du `BatchEncoding` que notre *tokenizer* retourne. 

Une fois que nous avons ces indices de *tokens*, nous regardons les *offsets* correspondants, qui sont des *tuples* de deux entiers repr√©sentant l'√©tendue des caract√®res dans le contexte original. Nous pouvons ainsi d√©tecter si le morceau de contexte dans cette fonctionnalit√© commence apr√®s la r√©ponse ou se termine avant que la r√©ponse ne commence (dans ce cas, l'√©tiquette est `(0, 0)`). Si ce n'est pas le cas, nous bouclons pour trouver le premier et le dernier *token* de la r√©ponse :

```py
answers = raw_datasets["train"][2:6]["answers"]
start_positions = []
end_positions = []

for i, offset in enumerate(inputs["offset_mapping"]):
    sample_idx = inputs["overflow_to_sample_mapping"][i]
    answer = answers[sample_idx]
    start_char = answer["answer_start"][0]
    end_char = answer["answer_start"][0] + len(answer["text"][0])
    sequence_ids = inputs.sequence_ids(i)

    # Trouver le d√©but et la fin du contexte
    idx = 0
    while sequence_ids[idx] != 1:
        idx += 1
    context_start = idx
    while sequence_ids[idx] == 1:
        idx += 1
    context_end = idx - 1

    # Si la r√©ponse n'est pas enti√®rement dans le contexte, l'√©tiquette est (0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # Sinon, ce sont les positions de d√©but et de fin du token
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python out
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

Jetons un coup d'≈ìil  √† quelques r√©sultats pour v√©rifier que notre approche est correcte. Pour la premi√®re caract√©ristique, nous trouvons `(83, 85)` comme √©tiquettes. Comparons alors la r√©ponse th√©orique avec l'√©tendue d√©cod√©e des *tokens* de 83 √† 85 (inclus) :

```py
idx = 0
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs["input_ids"][idx][start : end + 1])

print(f"Theoretical answer: {answer}, labels give: {labeled_answer}")
```

```python out
'Theoretical answer: the Main Building, labels give: the Main Building'
```

Cela correspond ! Maintenant v√©rifions l'index 4, o√π nous avons mis les √©tiquettes √† `(0, 0)`, signifiant que la r√©ponse n'est pas dans le morceau de contexte de cette caract√©ristique :

```py
idx = 4
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

decoded_example = tokenizer.decode(inputs["input_ids"][idx])
print(f"Theoretical answer: {answer}, decoded example: {decoded_example}")
```

```python out
'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'
```

En effet, nous ne voyons pas la r√©ponse dans le contexte.

<Tip>

‚úèÔ∏è **A votre tour !** En utilisant l'architecture XLNet, le *padding* est appliqu√© √† gauche et la question et le contexte sont intervertis. Adaptez tout le code que nous venons de voir √† l'architecture XLNet (et ajoutez `padding=True`). Soyez conscient que le token `[CLS]` peut ne pas √™tre √† la position 0 avec le *padding* appliqu√©.

</Tip>

Maintenant que nous avons vu √©tape par √©tape comment pr√©traiter nos donn√©es d'entra√Ænement, nous pouvons les regrouper dans une fonction que nous appliquerons √† l'ensemble des donn√©es d'entra√Ænement. Nous allons rembourrer chaque caract√©ristique √† la longueur maximale que nous avons d√©finie, car la plupart des contextes seront longs (et les √©chantillons correspondants seront divis√©s en plusieurs caract√©ristiques). Il n'y a donc pas de r√©el avantage √† appliquer un rembourrage dynamique ici :

```py
max_length = 384
stride = 128


def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Trouver le d√©but et la fin du contexte
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # Si la r√©ponse n'est pas enti√®rement dans le contexte, l'√©tiquette est (0, 0)
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Sinon, ce sont les positions de d√©but et de fin du token
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs
```

Notez que nous avons d√©fini deux constantes pour d√©terminer la longueur maximale utilis√©e ainsi que la longueur de la fen√™tre glissante, et que nous avons ajout√© un petit nettoyage avant la tok√©nisation : certaines des questions dans SQuAD ont des espaces suppl√©mentaires au d√©but et √† la fin qui n'ajoutent rien (et prennent de la place lors de la tok√©nisation si vous utilisez un mod√®le comme RoBERTa), donc nous avons supprim√© ces espaces suppl√©mentaires.

Pour appliquer cette fonction √† l'ensemble de l'entra√Ænement, nous utilisons la m√©thode `Dataset.map()` avec le flag `batched=True`. C'est n√©cessaire ici car nous changeons la longueur du jeu de donn√©es (puisqu'un exemple peut donner plusieurs caract√©ristiques d'entra√Ænement) :

```py
train_dataset = raw_datasets["train"].map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
len(raw_datasets["train"]), len(train_dataset)
```

```python out
(87599, 88729)
```

Comme nous pouvons le voir, le pr√©traitement a ajout√© environ 1 000 caract√©ristiques. Notre ensemble d'entra√Ænement est maintenant pr√™t √† √™tre utilis√©. Passons au pr√©traitement de l'ensemble de validation !

### Traitement des donn√©es de validation

Le pr√©traitement des donn√©es de validation sera l√©g√®rement plus facile car nous n'avons pas besoin de g√©n√©rer des √©tiquettes (sauf si nous voulons calculer une perte de validation, mais elle ne nous aidera pas vraiment √† comprendre la qualit√© du mod√®le). Le r√©el plaisir sera d'interpr√©ter les pr√©dictions du mod√®le dans des √©tendues du contexte original. Pour cela, il nous suffit de stocker les correspondances d'*offset* et un moyen de faire correspondre chaque caract√©ristique cr√©√©e √† l'exemple original dont elle provient. Puisqu'il y a une colonne identifiant dans le jeu de donn√©es original, nous l'utiliserons.

La seule chose que nous allons ajouter ici est un petit nettoyage des correspondances d'*offset*. Elles contiendront les *offsets* pour la question et le contexte, mais une fois que nous serons √† la phase de post-traitement, nous n'aurons aucun moyen de savoir quelle partie des identifiants d'entr√©e correspondait au contexte et quelle partie √©tait la question (la m√©thode `sequence_ids()` que nous avons utilis√©e n'est disponible que pour la sortie du *tokenizer*). Donc, nous allons mettre les *offsets* correspondant √† la question √† `None` :

```py
def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs
```

Nous pouvons appliquer cette fonction sur l'ensemble de validation comme pr√©c√©demment :

```py
validation_dataset = raw_datasets["validation"].map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
len(raw_datasets["validation"]), len(validation_dataset)
```

```python out
(10570, 10822)
```

Dans ce cas, nous n'avons ajout√© que quelques centaines d'√©chantillons, il semble donc que les contextes dans l'ensemble de validation soient un peu plus courts.

Maintenant que nous avons pr√©trait√© toutes les donn√©es, nous pouvons passer √† l'entra√Ænement. 

{#if fw === 'pt'}

## <i>Finetuner</i> le mod√®le avec l'API `Trainer`

Le code d'entra√Ænement pour cet exemple ressemblera beaucoup au code des sections pr√©c√©dentes mais le calcul de la m√©trique avec la fonction `compute_metrics()` sera un d√©fi unique. Puisque nous avons rembourr√© tous les √©chantillons √† la longueur maximale que nous avons d√©finie, il n'y a pas de collateur de donn√©es √† d√©finir. Ainsi le calcul de la m√©trique est vraiment la seule chose dont nous devons nous soucier. La partie la plus difficile sera de post-traiter les pr√©dictions du mod√®le en √©tendues de texte dans les exemples originaux. Une fois que nous aurons fait cela, la m√©trique de la biblioth√®que ü§ó *Datasets* fera le gros du travail pour nous.

{:else}

## <i>Finetuner</i> fin du mod√®le avec Keras

Le code d'entra√Ænement de cet exemple ressemblera beaucoup au code des sections pr√©c√©dentes, mais le calcul de la m√©trique sera un d√©fi unique. Puisque nous avons rembourr√© tous les √©chantillons √† la longueur maximale que nous avons d√©finie, il n'y a pas de collateur de donn√©es √† d√©finir. Ainsi le calcul de la m√©trique est vraiment la seule chose dont nous devons nous soucier. La partie la plus difficile sera de post-traiter les pr√©dictions du mod√®le en √©tendues de texte dans les exemples originaux. Une fois que nous aurons fait cela, la m√©trique de la biblioth√®que ü§ó *Datasets* fera le gros du travail pour nous.

{/if}

### Post-traitement

{#if fw === 'pt'}

<Youtube id="BNy08iIWVJM"/>

{:else}

<Youtube id="VN67ZpN33Ss"/>

{/if}

Le mod√®le produira des logits pour les positions de d√©but et de fin de la r√©ponse dans les identifiants d'entr√©e, comme nous l'avons vu lors de notre exploration du pipeline de `question-answering` [au chapitre 6](/course/fr/chapter6/3b). L'√©tape de post-traitement sera similaire √† ce que nous avons fait √† ce chapitre l√†. Voici un rapide rappel des actions que nous avons prises :

- nous avons masqu√© les logits de d√©but et de fin correspondant aux *tokens* en dehors du contexte,
- nous avons ensuite converti les logits de d√©but et de fin en probabilit√©s en utilisant une fonction SoftMax,
- nous avons attribu√© un score √† chaque paire `(start_token, end_token)` en prenant le produit des deux probabilit√©s correspondantes,
- nous avons cherch√© la paire avec le score maximum qui donnait une r√©ponse valide (par exemple, un `start_token` inf√©rieur au `end_token`).

Ici, nous allons modifier l√©g√®rement ce processus car nous n'avons pas besoin de calculer les scores r√©els (juste la r√©ponse pr√©dite). Cela signifie que nous pouvons sauter l'√©tape de la SoftMax. Pour aller plus vite, nous ne donnerons pas non plus un score √† toutes les paires `(start_token, end_token)` possibles, mais seulement celles correspondant aux `n_best` logits les plus √©lev√©s (avec `n_best=20`). Puisque nous sautons la SoftMax, les scores seront des scores logi, et seront obtenus en prenant la somme des logits de d√©but et de fin (au lieu du produit, √† cause de la r√®gle \\(\log(ab) = \log(a) + \log(b)\\)).

Pour d√©montrer tout cela, nous aurons besoin d'un certain type de pr√©dictions. Puisque nous n'avons pas encore entra√Æn√© notre mod√®le, nous allons utiliser le mod√®le par d√©faut du pipeline de `question-answering` pour g√©n√©rer quelques pr√©dictions sur une petite partie de l'ensemble de validation. Nous pouvons utiliser la m√™me fonction de traitement que pr√©c√©demment car elle repose sur la constante globale `tokenizer`, nous devons juste changer cet objet par le *tokenizer* du mod√®le que nous voulons utiliser temporairement :

```python
small_eval_set = raw_datasets["validation"].select(range(100))
trained_checkpoint = "distilbert-base-cased-distilled-squad"

tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)
eval_set = small_eval_set.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
```

Maintenant que le pr√©traitement est termin√©, nous changeons le *tokenizer* pour celui que nous avons choisi √† l'origine :

```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Nous supprimons ensuite les colonnes de notre `eval_set` qui ne sont pas attendues par le mod√®le. Nous construisons un batch avec tout de ce petit ensemble de validation et le passons au mod√®le. Si un GPU est disponible, nous l'utilisons pour aller plus vite :

{#if fw === 'pt'}

```python
import torch
from transformers import AutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}
trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(
    device
)

with torch.no_grad():
    outputs = trained_model(**batch)
```

Puisque `Trainer` nous donne les pr√©dictions sous forme de tableaux NumPy, nous r√©cup√©rons les logits de d√©but et de fin et les convertissons dans ce format :

```python
start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()
```

{:else}

```python
import tensorflow as tf
from transformers import TFAutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("numpy")

batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}
trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)

outputs = trained_model(**batch)
```

Pour faciliter l'exp√©rimentation, nous allons convertir ces sorties en tableaux NumPy :

```python
start_logits = outputs.start_logits.numpy()
end_logits = outputs.end_logits.numpy()
```

{/if}

Maintenant, nous devons trouver la r√©ponse pr√©dite pour chaque exemple dans notre `small_eval_set`. Un exemple peut avoir √©t√© divis√© en plusieurs caract√©ristiques dans `eval_set`, donc la premi√®re √©tape est de faire correspondre chaque exemple dans `small_eval_set` aux caract√©ristiques correspondantes dans `eval_set` :

```python
import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(eval_set):
    example_to_features[feature["example_id"]].append(idx)
```

Avec cela, nous pouvons vraiment nous mettre au travail en bouclant tous les exemples et, pour chaque exemple, toutes les caract√©ristiques associ√©es. Comme nous l'avons dit pr√©c√©demment, nous allons regarder les scores logit pour les `n_best` logits de d√©but et logits de fin, en excluant les positions qui donnent :

- une r√©ponse qui ne serait pas dans le contexte
- une r√©ponse avec une longueur n√©gative
- une r√©ponse qui est trop longue (nous limitons les possibilit√©s √† `max_answer_length=30`)

Une fois que nous avons toutes les r√©ponses possibles not√©es pour un exemple, nous choisissons simplement celle qui a le meilleur score logit :

```python
import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in small_eval_set:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = eval_set["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Ignore les r√©ponses qui ne sont pas enti√®rement dans le contexte
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Ignore les r√©ponses dont la longueur est soit < 0 soit > max_answer_length
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
```

Le format final des r√©ponses pr√©dites est celui qui sera attendu par la m√©trique que nous allons utiliser. Comme d'habitude, nous pouvons la charger √† l'aide de la biblioth√®que ü§ó *Evaluate* :

```python
import evaluate

metric = evaluate.load("squad")
```

Cette m√©trique attend les r√©ponses pr√©dites dans le format que nous avons vu ci-dessus (une liste de dictionnaires avec une cl√© pour l'identifiant de l'exemple et une cl√© pour le texte pr√©dit) et les r√©ponses th√©oriques dans le format ci-dessous (une liste de dictionnaires avec une cl√© pour l'identifiant de l'exemple et une cl√© pour les r√©ponses possibles) :

```python
theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]} for ex in small_eval_set
]
```

Nous pouvons maintenant v√©rifier que nous obtenons des r√©sultats raisonnables en examinant le premier √©l√©ment des deux listes :

```python
print(predicted_answers[0])
print(theoretical_answers[0])
```

```python out
{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}
```

Pas trop mal ! Voyons maintenant le score que la m√©trique nous donne :

```python
metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

Encore une fois, c'est plut√¥t bon si l'on consid√®re que, d'apr√®s [le papier](https://arxiv.org/abs/1910.01108v2) de DistilBERT, *finetun√©* sur SQuAD, ce mod√®le obtient 79,1 et 86,9 pour ces scores sur l'ensemble du jeu de donn√©es.

{#if fw === 'pt'}

Maintenant, mettons tout ce que nous venons de faire dans une fonction `compute_metrics()` que nous utiliserons dans le `Trainer`. Normalement, cette fonction `compute_metrics()` re√ßoit seulement un *tuple* `eval_preds` avec les logits et les √©tiquettes. Ici, nous aurons besoin d'un peu plus, car nous devons chercher dans le jeu de donn√©es des caract√©ristiques pour le d√©calage et dans le jeu de donn√©es des exemples pour les contextes originaux. Ainsi nous ne serons pas en mesure d'utiliser cette fonction pour obtenir des r√©sultats d'√©valuation standards pendant l'entra√Ænement. Nous ne l'utiliserons qu'√† la fin de l'entra√Ænement pour v√©rifier les r√©sultats.

La fonction `compute_metrics()` regroupe les m√™mes √©tapes que pr√©c√©demment. Nous ajoutons juste une petite v√©rification au cas o√π nous ne trouverions aucune r√©ponse valide (dans ce cas nous pr√©disons une cha√Æne vide).

{:else}

Maintenant, mettons tout ce que nous venons de faire dans une fonction `compute_metrics()` que nous utiliserons apr√®s avoir entra√Æn√© notre mod√®le. Nous aurons besoin de passer un peu plus que juste les logits de sortie, car nous devons chercher dans le jeu de donn√©es des caract√©ristiques pour le d√©calage et dans le jeu de donn√©es des exemples pour les contextes originaux :

{/if}

```python
from tqdm.auto import tqdm


def compute_metrics(start_logits, end_logits, features, examples):
    example_to_features = collections.defaultdict(list)
    for idx, feature in enumerate(features):
        example_to_features[feature["example_id"]].append(idx)

    predicted_answers = []
    for example in tqdm(examples):
        example_id = example["id"]
        context = example["context"]
        answers = []

        # Parcourir en boucle toutes les fonctionnalit√©s associ√©es √† cet exemple
        for feature_index in example_to_features[example_id]:
            start_logit = start_logits[feature_index]
            end_logit = end_logits[feature_index]
            offsets = features[feature_index]["offset_mapping"]

            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Ignore les r√©ponses qui ne sont pas enti√®rement dans le contexte
                    if offsets[start_index] is None or offsets[end_index] is None:
                        continue
                    # Ignore les r√©ponses dont la longueur est soit < 0, soit > max_answer_length
                    if (
                        end_index < start_index
                        or end_index - start_index + 1 > max_answer_length
                    ):
                        continue

                    answer = {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                    answers.append(answer)

        # S√©lectionne la r√©ponse avec le meilleur score
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append(
                {"id": example_id, "prediction_text": best_answer["text"]}
            )
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    theoretical_answers = [{"id": ex["id"], "answers": ex["answers"]} for ex in examples]
    return metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

Nous pouvons v√©rifier que cela fonctionne sur nos pr√©dictions :

```python
compute_metrics(start_logits, end_logits, eval_set, small_eval_set)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

C'est bien ! Maintenant, utilisons ceci pour *finetuner* notre mod√®le.

### <i>Finetuning</i> du mod√®le

{#if fw === 'pt'}

Nous sommes maintenant pr√™ts √† entra√Æner notre mod√®le. Cr√©ons-le en utilisant la classe `AutoModelForQuestionAnswering` comme pr√©c√©demment :

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{:else}

Nous sommes maintenant pr√™ts √† entra√Æner notre mod√®le. Cr√©ons-le en utilisant la classe `TFAutoModelForQuestionAnswering` comme pr√©c√©demment :

```python
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{/if}

Comme d'habitude, nous recevons un avertissement indiquant que certains poids ne sont pas utilis√©s (ceux de la t√™te de pr√©-entra√Ænement) et que d'autres sont initialis√©s de mani√®re al√©atoire (ceux de la t√™te de r√©ponse aux questions). Vous devriez √™tre habitu√© √† cela maintenant, mais cela signifie que ce mod√®le n'est pas encore pr√™t √† √™tre utilis√© et qu'il a besoin d'√™tre *finetun√©*. Une bonne chose que nous soyons sur le point de le faire !

Pour pouvoir pousser notre mod√®le vers le *Hub*, nous devons nous connecter √† Hugging Face. Si vous ex√©cutez ce code dans un *notebook*, vous pouvez le faire avec la fonction utilitaire suivante, qui affiche un *widget* o√π vous pouvez entrer vos identifiants de connexion :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Si vous ne travaillez pas dans un *notebook*, tapez simplement la ligne suivante dans votre terminal :

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Une fois ceci fait, nous pouvons d√©finir nos `TrainingArguments`. Comme nous l'avons dit lorsque nous avons d√©fini notre fonction pour calculer la m√©trique, nous ne serons pas en mesure d'avoir une boucle d'√©valuation standard √† cause de la signature de la fonction `compute_metrics()`. Nous pourrions √©crire notre propre sous-classe de `Trainer` pour faire cela (une approche que vous pouvez trouver dans le [script d'exemple de r√©ponse aux questions](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)), mais c'est un peu trop long pour cette section. A la place, nous n'√©valuerons le mod√®le qu'√† la fin de l'entra√Ænement et nous vous montrerons comment faire une √©valuation cela dans le paragraphe ¬´ Une boucle d'entra√Ænement personnalis√©e ¬ª ci-dessous.

C'est l√† que l'API `Trainer` montre ses limites et que la biblioth√®que ü§ó *Accelerate* brille : personnaliser la classe pour un cas d'utilisation sp√©cifique peut √™tre p√©nible, mais modifier une boucle d'entra√Ænement est facile.

Jetons un coup d'≈ìil √† notre `TrainingArguments` :

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-squad",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)
```

Nous avons d√©j√† vu la plupart d'entre eux. Nous d√©finissons quelques hyperparam√®tres (comme le taux d'apprentissage, le nombre d'√©poques d'entra√Ænement, un taux de d√©croissance des poids) et nous indiquons que nous voulons sauvegarder le mod√®le √† la fin de chaque √©poque, sauter l'√©valuation, et t√©l√©charger nos r√©sultats vers le *Hub*. Nous activons √©galement l'entra√Ænement en pr√©cision mixte avec `fp16=True`, car cela peut acc√©l√©rer l'entra√Ænement sur un GPU r√©cent.

{:else}

Maintenant que c'est fait, nous pouvons cr√©er nos jeux de donn√©es TensorFlow. Nous pouvons utiliser le simple collateur de donn√©es par d√©faut cette fois-ci :

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

Et maintenant nous cr√©ons les jeux de donn√©es comme d'habitude.

```python
tf_train_dataset = train_dataset.to_tf_dataset(
    columns=[
        "input_ids",
        "start_positions",
        "end_positions",
        "attention_mask",
        "token_type_ids",
    ],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)
tf_eval_dataset = validation_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

Ensuite, nous configurons nos hyperparam√®tres d'entra√Ænement et compilons notre mod√®le :

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# Le nombre d'√©tapes d'entra√Ænement est le nombre d'√©chantillons dans le jeu de donn√©es, divis√© par la taille du batch,
# puis multipli√© par le nombre total d'√©poques. Notez que le jeu de donn√©es tf_train_dataset est ici un tf.data.Dataset,
# et non le jeu de donn√©es original donc son len() est d√©j√† num_samples // batch_size.
num_train_epochs = 3
num_train_steps = len(tf_train_dataset) * num_train_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Entra√Æner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Enfin, nous sommes pr√™ts √† entra√Æner avec `model.fit()`. Nous utilisons un `PushToHubCallback` pour t√©l√©charger le mod√®le sur le *Hub* apr√®s chaque √©poque.

{/if}

Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace et nomm√© apr√®s le r√©pertoire de sortie que vous avez d√©fini. Donc dans notre cas il sera dans `"sgugger/bert-finetuned-squad"`. Nous pouvons passer outre en passant un `hub_model_id`, par exemple, pour pousser le mod√®le dans l'organisation `huggingface_course` nous avons utilis√© `hub_model_id= "huggingface_course/bert-finetuned-squad"` (qui est le mod√®le que nous avons li√© au d√©but de cette section).

{#if fw === 'pt'}

<Tip>

üí° Si le r√©pertoire de sortie que vous utilisez existe, il doit √™tre un clone local du d√©p√¥t vers lequel vous voulez pousser (donc d√©finissez un nouveau nom si vous obtenez une erreur lors de la d√©finition de votre `Trainer`).

</Tip>

Enfin, nous passons tout √† la classe `Trainer` et lan√ßons l'entra√Ænement :

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

{:else}

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-squad", tokenizer=tokenizer)

# Nous allons faire la validation apr√®s, donc pas de validation au milieu de l'entra√Ænement.
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)
```

{/if}

Notez que pendant l'entra√Ænement, chaque fois que le mod√®le est sauvegard√© (ici, √† chaque √©poque), il est t√©l√©charg√© sur le *Hub* en arri√®re-plan. Ainsi, vous pourrez reprendre votre entra√Ænement sur une autre machine si n√©cessaire. L'ensemble de l'entra√Ænement prend un certain temps (un peu plus d'une heure sur une Titan RTX), vous pouvez donc prendre un caf√© ou relire les parties du cours qui vous ont sembl√© plus difficiles pendant qu'il se d√©roule. Notez √©galement que d√®s que la premi√®re √©poque est termin√©e, vous verrez des poids t√©l√©charg√©s sur le *Hub* et vous pourrez commencer √† jouer avec votre mod√®le sur sa page.

{#if fw === 'pt'}

Une fois l'entra√Ænement termin√©, nous pouvons enfin √©valuer notre mod√®le (et prier pour ne pas avoir d√©pens√© tout ce temps de calcul pour rien). La m√©thode `predict()` du `Trainer` retournera un *tuple* o√π les premiers √©l√©ments seront les pr√©dictions du mod√®le (ici une paire avec les logits de d√©but et de fin). Nous envoyons ceci √† notre fonction `compute_metrics()` :

```python
predictions, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets["validation"])
```

{:else}

Une fois l'entra√Ænement termin√©, nous pouvons enfin √©valuer notre mod√®le (et prier pour ne pas avoir d√©pens√© tout ce temps de calcul pour rien). La m√©thode `predict()` de notre `model` se chargera d'obtenir les pr√©dictions, et puisque nous avons fait tout le travail difficile de d√©finir une fonction `compute_metrics()` plus t√¥t, nous pouvons obtenir nos r√©sultats en une seule ligne :

```python
predictions = model.predict(tf_eval_dataset)
compute_metrics(
    predictions["start_logits"],
    predictions["end_logits"],
    validation_dataset,
    raw_datasets["validation"],
)
```

{/if}

```python out
{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}
```

Super ! √Ä titre de comparaison, les scores indiqu√©s dans l'article de BERT pour ce t√¢che sont de 80,8 et 88,5. Donc nous sommes exactement l√† o√π nous devrions √™tre.

{#if fw === 'pt'}

Enfin, nous utilisons la m√©thode `push_to_hub()` pour nous assurer que nous t√©l√©chargeons la derni√®re version du mod√®le :

```py
trainer.push_to_hub(commit_message="Training complete")
```

Cela renvoie l'URL du commit qu'il vient de faire, si vous voulez l'inspecter :

```python out
'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68'
```

Le `Trainer` r√©dige √©galement une carte de mod√®le avec tous les r√©sultats de l'√©valuation et la t√©l√©charge.

{/if}

√Ä ce stade, vous pouvez utiliser le *widget* d'inf√©rence sur le *Hub* du mod√®le pour tester le mod√®le et le partager avec vos amis, votre famille et vos animaux pr√©f√©r√©s. Vous avez r√©ussi √† *finetuner* un mod√®le sur une t√¢che de r√©ponse √† une question. F√©licitations !

<Tip>

‚úèÔ∏è **A votre tour** Essayez un autre mod√®le pour voir s'il est plus performant pour cette t√¢che !

</Tip>

{#if fw === 'pt'}

Si vous voulez plonger un peu plus profond√©ment dans la boucle d'entra√Ænement, nous allons maintenant vous montrer comment faire la m√™me chose en utilisant ü§ó *Accelerate*.

## Une boucle d'entra√Ænement personnalis√©e

Jetons maintenant un coup d'≈ìil √† la boucle d'entra√Ænement compl√®te, afin que vous puissiez facilement personnaliser les parties dont vous avez besoin. Elle ressemblera beaucoup √† la boucle d'entra√Ænement du [chapitre 3](/course/fr/chapter3/4), √† l'exception de la boucle d'√©valuation. Nous serons en mesure d'√©valuer le mod√®le r√©guli√®rement puisque nous ne sommes plus contraints par la classe `Trainer`.

### Pr√©parer tout pour l'entra√Ænement

Tout d'abord, nous devons construire le `DataLoader`s √† partir de nos jeux de donn√©es. Nous d√©finissons le format de ces jeux de donn√©es √† `"torch"` et supprimons les colonnes dans le jeu de validation qui ne sont pas utilis√©es par le mod√®le. Ensuite, nous pouvons utiliser le `default_data_collator` fourni par ü§ó *Transformers* comme `collate_fn` et m√©langer l'ensemble d'entra√Ænement mais pas celui de validation :

```py
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_dataset.set_format("torch")
validation_set = validation_dataset.remove_columns(["example_id", "offset_mapping"])
validation_set.set_format("torch")

train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=default_data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    validation_set, collate_fn=default_data_collator, batch_size=8
)
```

Ensuite, nous r√©instantifions notre mod√®le afin de nous assurer que nous ne poursuivons pas le *finetuning* pr√©c√©dent et que nous repartons du mod√®le BERT pr√©-entra√Æn√© :

```py
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

Ensuite, nous aurons besoin d'un optimiseur. Comme d'habitude, nous utilisons le classique `AdamW`, qui est comme Adam mais avec une correction dans la fa√ßon dont le taux de d√©croissance des poids est appliqu√© :

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Une fois que nous avons tous ces objets, nous pouvons les envoyer √† la m√©thode `accelerator.prepare()`. Rappelez-vous que si vous voulez entra√Æner sur des TPUs dans un *notebook* Colab, vous devrez d√©placer tout ce code dans une fonction d'entra√Ænement, et qui ne devrait pas ex√©cuter une cellule qui instancie un `Accelerator`. Nous pouvons forcer l'entra√Ænement en pr√©cision mixte en passant l'argument `fp16=True` √† `Accelerator` (ou, si vous ex√©cutez le code comme un script, assurez-vous de remplir la ü§ó *Accelerate* `config` de mani√®re appropri√©e).

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

Comme vous devez le savoir depuis les sections pr√©c√©dentes, nous ne pouvons utiliser la longueur de `train_dataloader` pour calculer le nombre d'√©tapes d'entra√Ænement qu'apr√®s qu'il soit pass√© par la m√©thode `accelerator.prepare()`. Nous utilisons le m√™me programme lin√©aire que dans les sections pr√©c√©dentes :

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Pour pousser notre mod√®le vers le *Hub*, nous aurons besoin de cr√©er un objet `Repository` dans un dossier de travail. Tout d'abord, connectez-vous au *Hub*, si vous n'√™tes pas d√©j√† connect√©. Nous d√©terminerons le nom du d√©p√¥t √† partir de l'identifiant du mod√®le que nous voulons donner √† notre mod√®le (n'h√©sitez pas √† remplacer le `repo_name` par votre propre choix. Il doit juste contenir votre nom d'utilisateur, ce que fait la fonction `get_full_repo_name()`) :

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-squad-accelerate'
```

Ensuite, nous pouvons cloner ce d√©p√¥t dans un dossier local. S'il existe d√©j√†, ce dossier local doit √™tre un clone du d√©p√¥t avec lequel nous travaillons :

```py
output_dir = "bert-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Nous pouvons maintenant t√©l√©charger tout ce que nous sauvegardons dans `output_dir` en appelant la m√©thode `repo.push_to_hub()`. Cela nous aidera √† t√©l√©charger les mod√®les interm√©diaires √† la fin de chaque √©poque.

## Boucle d'entra√Ænement

Nous sommes maintenant pr√™ts √† √©crire la boucle d'entra√Ænement compl√®te. Apr√®s avoir d√©fini une barre de progression pour suivre l'√©volution de l'entra√Ænement, la boucle comporte trois parties :

- l'entra√Ænement √† proprement dit, qui est l'it√©ration classique sur le `train_dataloader`, passage en avant du mod√®le, puis passage en arri√®re et √©tape d'optimisation.
- l'√©valuation, dans laquelle nous rassemblons toutes les valeurs pour `start_logits` et `end_logits` avant de les convertir en tableaux NumPy. Une fois la boucle d'√©valuation termin√©e, nous concat√©nons tous les r√©sultats. Notez que nous devons tronquer car `Accelerator` peut avoir ajout√© quelques √©chantillons √† la fin pour s'assurer que nous avons le m√™me nombre d'exemples dans chaque processus.
- sauvegarde et t√©l√©chargement, o√π nous sauvegardons d'abord le mod√®le et le *tokenizer*, puis appelons `repo.push_to_hub()`. Comme nous l'avons fait auparavant, nous utilisons l'argument `blocking=False` pour dire √† la biblioth√®que ü§ó *Hub* de pousser dans un processus asynchrone. De cette fa√ßon, l'entra√Ænement continue normalement et cette (longue) instruction est ex√©cut√©e en arri√®re-plan.

Voici le code complet de la boucle d'entra√Ænement :

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra√Ænement
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    start_logits = []
    end_logits = []
    accelerator.print("Evaluation!")
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())
        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())

    start_logits = np.concatenate(start_logits)
    end_logits = np.concatenate(end_logits)
    start_logits = start_logits[: len(validation_dataset)]
    end_logits = end_logits[: len(validation_dataset)]

    metrics = compute_metrics(
        start_logits, end_logits, validation_dataset, raw_datasets["validation"]
    )
    print(f"epoch {epoch}:", metrics)

    # Sauvegarder et t√©l√©charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

Au cas o√π ce serait la premi√®re fois que vous verriez un mod√®le enregistr√© avec ü§ó *Accelerate*, prenons un moment pour inspecter les trois lignes de code qui l'accompagnent :

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

La premi√®re ligne est explicite : elle indique √† tous les processus d'attendre que tout le monde soit √† ce stade avant de continuer. C'est pour s'assurer que nous avons le m√™me mod√®le dans chaque processus avant de sauvegarder. Ensuite, nous prenons le `unwrapped_model`, qui est le mod√®le de base que nous avons d√©fini. La m√©thode `accelerator.prepare()` modifie le mod√®le pour qu'il fonctionne dans l'entra√Ænement distribu√©. Donc il n'aura plus la m√©thode `save_pretrained()` car la m√©thode `accelerator.unwrap_model()` annule cette √©tape. Enfin, nous appelons `save_pretrained()` mais nous disons √† cette m√©thode d'utiliser `accelerator.save()` au lieu de `torch.save()`. 

Une fois ceci fait, vous devriez avoir un mod√®le qui produit des r√©sultats assez similaires √† celui entra√Æn√© avec `Trainer`. Vous pouvez v√©rifier le mod√®le que nous avons entra√Æn√© en utilisant ce code √† [*huggingface-course/bert-finetuned-squad-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate). Et si vous voulez tester des modifications de la boucle d'entra√Ænement, vous pouvez les impl√©menter directement en modifiant le code ci-dessus !

{/if}

### Utilisation du mod√®le <i>finetun√©</i>

Nous vous avons d√©j√† montr√© comment vous pouvez utiliser le mod√®le que nous avons *finetun√©* sur le *Hub* avec le *widget* d'inf√©rence. Pour l'utiliser localement dans un `pipeline`, il suffit de sp√©cifier l'identifiant du mod√®le :

```py
from transformers import pipeline

# Remplacez par votre propre checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-squad"
question_answerer = pipeline("question-answering", model=model_checkpoint)

context = """
ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ü§ó Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.9979003071784973,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Super ! Notre mod√®le fonctionne aussi bien que le mod√®le par d√©faut pour ce pipeline !
