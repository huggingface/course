<FrameworkSwitchCourse {fw} />

# Entra√Æner un mod√®le de langage causal √† partir de z√©ro

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
]} />

{/if}

Jusqu'√† pr√©sent, nous avons surtout r√©utilis√© des mod√®les pr√©-entra√Æn√©s et les avons *finetun√©s* sur de nouveaux cas d'usage. Comme nous l'avons vu dans le [chapitre 1](/course/fr/chapter1), ceci est commun√©ment appel√© _apprentissage par transfert_, et il s'agit d'une strat√©gie tr√®s efficace pour appliquer les *transformers* √† la plupart des applications du monde r√©el o√π les donn√©es √©tiquet√©es sont rares. Dans ce chapitre, nous allons adopter une approche diff√©rente consistant √† entra√Æner un mod√®le compl√®tement nouveau √† partir de z√©ro. C'est une bonne d√©marche √† adopter si vous avez beaucoup de donn√©es et qu'elles sont tr√®s diff√©rentes des donn√©es de pr√©-entra√Ænement utilis√©es par les mod√®les disponibles. Cependant, le pr√©-entra√Ænement d'un mod√®le de langue n√©cessite beaucoup plus de ressources informatiques que le simple *finetuning* d'un mod√®le existant. Parmi les exemples o√π il peut √™tre utile d'entra√Æner un nouveau mod√®le, citons les jeux de donn√©es constitu√©s de notes de musique, de s√©quences mol√©culaires telles que l'ADN, ou de langages de programmation. Ces derniers ont r√©cemment gagn√© en popularit√© gr√¢ce √† des outils tels que TabNine et Copilot de GitHub (aliment√©s par le mod√®le Codex d'OpenAI) qui peuvent g√©n√©rer de longues s√©quences de code. Cette t√¢che de g√©n√©ration de texte est mieux abord√©e avec des mod√®les de langage autor√©gressifs ou causaux tels que le GPT-2.

Dans cette section, nous allons construire une version r√©duite d'un mod√®le de g√©n√©ration de code Python. Nous nous concentrerons sur la compl√©tion d'une ligne de code au lieu de fonctions ou de classes compl√®tes. Lorsque vous travaillez sur des projets de science des donn√©es en Python, vous √™tes souvent en contact avec les biblioth√®ques `matplotlib`, `seaborn`, `pandas` et `scikit-learn`. Lors de l'utilisation de ces *frameworks*, il est fr√©quent d'avoir besoin de rechercher des commandes sp√©cifiques. Il serait donc bien d'utiliser un mod√®le pour compl√©ter ces appels pour nous.


<Youtube id="Vpjb1lu0MDk"/>

Dans le [chapitre 6](/course/fr/chapter6), nous avons cr√©√© un *tokenizer* efficace pour traiter du code Python. Nous avons besoin d'un jeu de donn√©es √† grande √©chelle pour pr√©-entra√Æner un mod√®le. Ici, nous allons appliquer notre *tokenizer* √† un corpus de code Python provenant des d√©p√¥ts GitHub. Nous utiliserons ensuite l'API `Trainer` et ü§ó *Accelerate* pour entra√Æner le mod√®le. C'est parti !

<iframe src="https://hf.space/gradioiframe/course-demos/codeparrot-ds/+" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://hf.space/gradioiframe/course-demos/codeparrot-ds-darkmode/+" frameBorder="0" height="300" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Il s'agit d'une pr√©sentation du mod√®le qui a √©t√© entra√Æn√© √† l'aide du code pr√©sent√© dans cette section et qui a ensuit√© √©t√© t√©l√©charg√© sur le *Hub*. Vous pouvez le trouver [ici](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). Notez qu'√©tant donn√© qu'il y a un certains al√©at dans la g√©n√©ration du texte, vous obtiendrez probablement un r√©sultat l√©g√®rement diff√©rent.
 
## Collecte des donn√©es

On peut trouver du code Python en abondance dans les d√©p√¥ts de code tels que GitHub, que nous pouvons utiliser pour cr√©er un jeu de donn√©es en r√©cup√©rant chaque d√©p√¥t Python. C'est l'approche adopt√©e dans le [livre *Natural Language Processing with Transformers*](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/) pour pr√©-entra√Æner un grand GPT-2. En utilisant un d√©p√¥t GitHub d'environ 180 Go contenant approximativement 20 millions de fichiers Python, les auteurs du livre ont construit un jeu de donn√©es appel√© `codeparrot` qu'ils ont ensuite partag√© sur le [*Hub*](https://huggingface.co/datasets/transformersbook/codeparrot).

Cependant, entra√Æner sur l'ensemble du corpus prend beaucoup de temps et demande beaucoup de ressources de calculs. Dans notre cas, nous n'avons besoin que du sous-ensemble du jeu de donn√©es qui est relatif aux codes portant sur la science des donn√©es. Commen√ßons donc par filtrer le jeu de donn√©es `codeparrot` en ne gardant que les fichiers incluant l'une des biblioth√®ques de science des donn√©es √©num√©r√©es pr√©c√©demment. En raison de la taille du jeu de donn√©es, nous voulons √©viter de le t√©l√©charger. Nous utiliserons donc la fonctionnalit√© de *streaming* de ü§ó *Datasets* afin de le filtrer √† la vol√©e. Pour nous aider √† filtrer les √©chantillons de code utilisant les biblioth√®ques que nous avons mentionn√©es pr√©c√©demment, nous utilisons la fonction suivante :

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

Testons-le sur deux exemples :

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

Nous pouvons l'utiliser pour cr√©er une fonction qui va *streamer* le jeu de donner et filtrer les √©l√©ments que nous voulons :

```py
def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

Ensuite, nous pouvons simplement appliquer cette fonction :

```py
# Cette cellule prendra beaucoup de temps √† s'ex√©cuter, donc vous devriez la sauter et aller √† la suivante !
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

Cela nous laisse avec environ 3 % du jeu de donn√©es original, ce qui est tout de m√™me assez important puisqu'il fait 6 Go et se compose de 600 000 scripts Python !

Le filtrage peut prendre de 2 √† 3 heures, selon votre machine et votre bande passante. Si vous ne voulez pas passer par ce long processus, nous fournissons sur le *Hub* le jeu de donn√©es filtr√© pour que vous puissiez le t√©l√©charger :

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="train")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

Le pr√©-entra√Ænement du mod√®le de langue prendra un certain temps. Nous vous sugg√©rons donc d'ex√©cuter d'abord la boucle d'entra√Ænement sur un petit √©chantillon des donn√©es en d√©commentant les deux lignes dans le code ci-dessus. Assurez-vous alors que l'entra√Ænement se termine avec succ√®s et que les mod√®les sont stock√©s. Rien n'est plus frustrant qu'un entra√Ænement qui √©choue √† la derni√®re √©tape car vous avez oubli√© de cr√©er un dossier ou parce qu'il y a une faute de frappe √† la fin de la boucle d'entra√Ænement !

</Tip>

Examinons un exemple tir√© du jeu de donn√©es. Nous ne montrerons que les 200 premiers caract√®res de chaque champ :

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

Nous pouvons voir que le champ `content` contient le code sur lequel nous voulons que notre mod√®le s'entra√Æne. Maintenant que nous avons un jeu de donn√©es, nous devons pr√©parer les textes afin qu'ils soient dans un format appropri√© pour le pr√©-entra√Ænement.

## Pr√©paration du jeu de donn√©es

<Youtube id="ma1TrR7gE7I"/>

La premi√®re √©tape est de tokeniser les donn√©es afin de pouvoir les utiliser pour l'entra√Ænement. Puisque notre objectif est d'autocompl√©ter de courts appels de fonctions, nous pouvons garder la taille du contexte relativement petite. L'avantage est que nous pouvons entra√Æner le mod√®le beaucoup plus rapidement et qu'il n√©cessite beaucoup moins de m√©moire. Si c'est important pour votre application d'avoir davantage de contexte (par exemple, si vous voulez que le mod√®le √©crive des tests unitaires bas√©s sur un fichier avec la d√©finition de la fonction), assurez-vous d'augmenter ce nombre. Gardez n√©anmoins √† l'esprit que cela s'accompagne d'une plus grande empreinte m√©moire du GPU. Pour l'instant, fixons la taille du contexte √† 128 *tokens*, par opposition aux 1 024 ou 2 048 utilis√©s respectivement dans le GPT-2 et le GPT-3.

La plupart des documents contiennent beaucoup plus de 128 *tokens*, donc le fait de tronquer les entr√©es √† la longueur maximale √©liminerait une grande partie de notre jeu de donn√©es. A la place, nous allons utiliser l'option `return_overflowing_tokens` pour tokeniser l'entr√©e enti√®re et la diviser en plusieurs morceaux, comme nous l'avons fait dans le [chapitre 6](/course/fr/chapter6/4). Nous utiliserons √©galement l'option `return_length` pour retourner automatiquement la longueur de chaque morceau cr√©√©. Souvent, le dernier morceau est plus petit que la taille du contexte et nous nous en d√©barrasserons pour √©viter les probl√®mes de *padding*. Nous n'en avons pas vraiment besoin puisque de toute fa√ßon nous avons beaucoup de donn√©es.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

Voyons comment cela fonctionne en examinant les deux premiers exemples :

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

Nous pouvons voir que nous obtenons 34 morceaux √† partir de ces deux exemples. En regardant leurs longueurs, nous pouvons voir qu'ils se terminent avec moins de 128 *tokens* (117 et 41, respectivement). Ils ne repr√©sentent qu'une petite fraction du total des morceaux que nous avons (2/34), donc nous pouvons les jeter sans risque. Avec le champ `overflow_to_sample_mapping`, nous pouvons aussi reconstruire quels morceaux appartenaient √† quels √©chantillons d'entr√©e.

Avec cette op√©ration, nous utilisons une fonctionnalit√© pratique de la fonction `Dataset.map()` de ü§ó *Datasets*. En effet, celle-ci ne n√©cessite pas une correspondance un √† un comme nous l'avons vu dans la [section 3](/course/fr/chapter7/3). Nous pouvons cr√©er des batchs avec plus ou moins d'√©l√©ments que le batch d'entr√©e. C'est utile lorsque l'on effectue des op√©rations telles que l'augmentation ou le filtrage des donn√©es qui modifient le nombre d'√©l√©ments. Dans notre cas, lors de la tokenisation de chaque √©l√©ment en morceaux de longeur de la taille de contexte sp√©cifi√©e, nous cr√©ons de nombreux √©chantillons de chaque document. Nous devons juste nous assurer de supprimer les colonnes existantes, car elles ont une taille conflictuelle. Si nous voulions les garder, nous pourrions les r√©p√©ter de mani√®re appropri√©e et les retourner dans l'appel `Dataset.map()` :

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

Nous avons maintenant 16,7 millions d'exemples avec 128 *tokens* chacun, ce qui correspond √† environ 2,1 milliards de *tokens* au total. A titre de comparaison, les mod√®les GPT-3 et Codex d'OpenAI sont entra√Æn√©s sur 300 et 100 milliards de *tokens*, respectivement. Les mod√®les Codex √©tant initialis√©s √† partir des *checkpoints* GPT-3. Notre objectif dans cette section n'est pas de rivaliser avec ces mod√®les, qui peuvent g√©n√©rer des textes longs et coh√©rents, mais de cr√©er une version r√©duite fournissant une fonction d'autocompl√©tion rapide.

Maintenant que le jeu de donn√©es est pr√™t, configurons le mod√®le !

<Tip>

‚úèÔ∏è **Essayez !** Se d√©barrasser de tous les morceaux qui sont plus petits que la taille du contexte n'√©tait pas un gros probl√®me ici parce que nous utilisons de petites fen√™tres de contexte. Si vous augmentez la taille du contexte (ou si vous avez un corpus de documents courts), la fraction des morceaux qui sont jet√©s augmentera. Une fa√ßon plus efficace de pr√©parer les donn√©es est de joindre tous les √©chantillons dans un batch avec un *token* `eos_token_id` entre les deux, puis d'effectuer le d√©coupage sur les s√©quences concat√©n√©es. Comme exercice, modifiez la fonction `tokenize()` pour utiliser cette approche. Notez que vous devrez mettre `truncation=False` et enlever les autres arguments du *tokenizer* pour obtenir la s√©quence compl√®te des identifiants des *tokens*.

</Tip>


## Initialisation d'un nouveau mod√®le

Notre premi√®re √©tape consiste √† initialiser un GPT-2. Pour notre mod√®le, nous utiliserons la m√™me configuration que pour le petit mod√®le GPT-2. Ainsi nous chargeons la configuration pr√©-entra√Æn√©e, nous nous assurons que la taille du *tokenizer* correspond √† la taille du vocabulaire du mod√®le et nous passons les identifiants des *tokens* `bos` et `eos` (d√©but et fin de s√©quence) :

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Avec cette configuration, nous pouvons charger un nouveau mod√®le. Notez que c'est la premi√®re fois que nous n'utilisons pas la fonction `from_pretrained()` puisque nous initialisons nous-m√™mes un mod√®le :

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Avec cette configuration, nous pouvons charger un nouveau mod√®le. Notez que c'est la premi√®re fois que nous n'utilisons pas la fonction `from_pretrained()` puisque nous initialisons nous-m√™mes un mod√®le :

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Construit le mod√®le
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

Notre mod√®le comporte 124 millions de param√®tres que nous devrons r√©gler. Avant de commencer l'entra√Ænement, nous devons configurer un collateur de donn√©es qui se chargera de cr√©er les batchs. Nous pouvons utiliser le collateur `DataCollatorForLanguageModeling`, qui est con√ßu sp√©cifiquement pour la mod√©lisation du langage (comme son nom le sugg√®re subtilement). En plus de l'empilage et du rembourrage des batchs, il s'occupe aussi de la cr√©ation des √©tiquettes du mod√®le de langage. Dans la mod√©lisation causale du langage, les entr√©es servent aussi d'√©tiquettes (juste d√©cal√©es d'un √©l√©ment) et que le collateur de donn√©es cr√©e √† la vol√©e pendant l'entra√Ænement pour ne pas avoir √† dupliquer les `input_ids`.

Notez que `DataCollatorForLanguageModeling` supporte √† la fois la mod√©lisation du langage masqu√© (MLM pour *masked language modeling*) et la mod√©lisation du langage causal (CLM pour *causal language modeling*). Par d√©faut, il pr√©pare les donn√©es pour la MLM mais nous pouvons passer √† la CLM en d√©finissant l'argument `mlm=False` :

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

Prenons un exemple :

```py
out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

Nous pouvons voir que les exemples ont √©t√© empil√©s et que tous les tenseurs ont la m√™me forme.

{#if fw === 'tf'}

Maintenant nous pouvons utiliser la m√©thode `to_tf_dataset()` pour convertir nos jeux de donn√©es en jeux de donn√©es TensorFlow avec le collateur de donn√©es que nous avons cr√©√© ci-dessus :

```python
tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

‚ö†Ô∏è Le d√©placement des entr√©es et des √©tiquettes pour les aligner se fait √† l'int√©rieur du mod√®le, de sorte que le collecteur de donn√©es ne fait que copier les entr√©es pour cr√©er les √©tiquettes.

</Tip>


Nous avons maintenant tout ce qu'il faut pour entra√Æner notre mod√®le. Ce n'√©tait pas si compliqu√© ! Avant de commencer l'entra√Ænement, nous devons nous connecter √† Hugging Face. Si vous travaillez dans un *notebook*, vous pouvez le faire avec la fonction utilitaire suivante :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Cela affichera un *widget* o√π vous pourrez entrer vos identifiants de connexion √† Hugging Face.

Si vous ne travaillez pas dans un *notebook*, tapez simplement la ligne suivante dans votre terminal :

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Tout ce qu'il reste √† faire est de configurer les arguments d'entra√Ænement et de lancer la fonction `Trainer`. Nous utiliserons un programme de taux d'apprentissage de type cosinus avec un r√©chauffement et une taille de batch de 256 (`per_device_train_batch_size` x `gradient_accumulation_steps`). L'accumulation du gradient est utilis√©e lorsqu'un seul batch ne tient pas en m√©moire, et construit le gradient de mani√®re incr√©mentale √† travers plusieurs passages en avant/en arri√®re. Nous verrons cela en action lorsque nous cr√©erons la boucle d'entra√Ænement avec ü§ó *Accelerate*.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

Maintenant, nous pouvons simplement lancer le `Trainer` et attendre que l'entra√Ænement se termine. Selon que vous l'ex√©cutez sur la totalit√© ou sur un sous-ensemble de l'√©chantillon d'entra√Ænement, cela prendra respectivement 20 ou 2 heures. Alors prenez quelques caf√©s et un bon livre √† lire !

```py
trainer.train()
```

Une fois l'entra√Ænement termin√©, nous pouvons pousser le mod√®le et le *tokenizer* vers le *Hub* :

```py
trainer.push_to_hub()
```

{:else}

Tout ce qu'il reste √† faire est de configurer les hyperparam√®tres d'entra√Ænement et d'appeler `compile()` et `fit()`. Nous utiliserons un programme de taux d'apprentissage avec un r√©chauffement pour am√©liorer la stabilit√© de l'entra√Ænement :

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Entra√Æner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Maintenant, nous pouvons simplement appeler `model.fit()` et attendre que l'entra√Ænement se termine. Selon que vous l'ex√©cutez sur la totalit√© ou sur un sous-ensemble de l'√©chantillon d'entra√Ænement, cela prendra respectivement 20 ou 2 heures. Alors prenez quelques caf√©s et un bon livre √† lire ! Une fois l'entra√Ænement termin√©, nous pouvons pousser le mod√®le et le *tokenizer* vers le *Hub* :

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

‚úèÔ∏è **Essayez !** Il ne nous a fallu qu'une trentaine de lignes de code en plus des `TrainingArguments` pour passer des textes bruts √† l'entra√Ænement du GPT-2. Essayez-le avec votre propre jeu de donn√©es et voyez si vous pouvez obtenir de bons r√©sultats ! 

</Tip>

<Tip>

{#if fw === 'pt'}

üí° Si vous avez acc√®s √† une machine avec plusieurs GPUs, essayez d'y ex√©cuter le code. `Trainer` g√®re automatiquement plusieurs machines ce qui peut acc√©l√©rer consid√©rablement l'entra√Ænement.

{:else}

üí° Si vous avez acc√®s √† une machine avec plusieurs GPUs, vous pouvez essayer d'utiliser `MirroredStrategy` pour acc√©l√©rer consid√©rablement l'entra√Ænement. Vous devrez cr√©er un objet `tf.distribute.MirroredStrategy` et vous assurer que les commandes `to_tf_dataset` ainsi que la cr√©ation du mod√®le et l'appel √† `fit()` sont tous ex√©cut√©s dans `scope()`. Vous pouvez consulter la documentation √† ce sujet [ici](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## G√©n√©ration de code avec le pipeline

C'est maintenant le moment de v√©rit√© : voyons comment le mod√®le entra√Æn√© fonctionne r√©ellement ! Nous pouvons voir dans les logs que la perte a diminu√© r√©guli√®rement, mais pour mettre le mod√®le √† l'√©preuve, regardons comment il fonctionne sur certains messages. Pour ce faire, nous allons envelopper le mod√®le dans un `pipeline` de g√©n√©ration de texte et, s'il y en a un de disponible, utiliser un GPU pour avoir des g√©n√©rations rapidement :

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

Let's start with the simple task of creating a scatter plot:

```py
txt = """\
# cr√©er des donn√©es
x = np.random.randn(100)
y = np.random.randn(100)

# cr√©er un nuage de points avec x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# cr√©er des donn√©es
x = np.random.randn(100)
y = np.random.randn(100)

# cr√©er un nuage de points avec x, y
plt.scatter(x, y)
```

Le r√©sultat semble correct. Est-ce que cela fonctionne aussi pour une op√©ration `pandas` ? Voyons si nous pouvons cr√©er un `DataFrame` √† partir de deux tableaux :

```py
txt = """\
# cr√©er des donn√©es
x = np.random.randn(100)
y = np.random.randn(100)

# cr√©er un tableau de donn√©es √† partir de x et y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# cr√©er des donn√©es
x = np.random.randn(100)
y = np.random.randn(100)

# cr√©er un tableau de donn√©es √† partir de x et y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

Bien, c'est la bonne r√©ponse. Bien qu'il ins√®re ensuite la colonne `x` √† nouveau. Comme le nombre de *tokens* g√©n√©r√©s est limit√©, la boucle `for` suivante est coup√©e. Voyons si nous pouvons faire quelque chose d'un peu plus complexe et faire en sorte que le mod√®le nous aide √† utiliser l'op√©ration `groupby` : 

```py
txt = """\
# tableau de donn√©es avec profession, revenu et nom
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculer le revenu moyen par profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# tableau de donn√©es avec profession, revenu et nom
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculer le revenu moyen par profession
profession = df.groupby(['profession']).mean()
```

Pas mal, c'est la bonne fa√ßon de faire. Enfin, voyons si nous pouvons aussi l'utiliser pour `scikit-learn` et utiliser un mod√®le *Random Forest* :

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra√Ænement du mod√®le de for√™t al√©atoire avec 300 estimateurs sur X, y :
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# entra√Ænement du mod√®le de for√™t al√©atoire avec 300 estimateurs sur X, y :
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

Au vu de ces quelques exemples, il semble que le mod√®le ait appris une partie de la syntaxe des biblioth√®ques Python de science des donn√©es. Bien s√ªr, nous devrions √©valuer le mod√®le de mani√®re plus approfondie avant de le d√©ployer dans le monde r√©el, mais il s'agit tout de m√™me d'un prototype impressionnant.

{:else}

Au vu de ces quelques exemples, il semble que le mod√®le ait appris une partie de la syntaxe des biblioth√®ques Python de science des donn√©es. Bien s√ªr, nous devrions √©valuer le mod√®le de mani√®re plus approfondie avant de le d√©ployer dans le monde r√©el, mais il s'agit tout de m√™me d'un prototype impressionnant. Parfois, il est n√©cessaire de personnaliser davantage l'entra√Ænement du mod√®le afin d'obtenir les performances n√©cessaires pour un cas d'utilisation donn√©. Par exemple, que se passe-t-il si l'on souhaite mettre √† jour dynamiquement la taille du batch ou si l'on dispose d'une boucle d'entra√Ænement conditionnelle qui ignore les mauvais exemples √† la vol√©e ? Une option serait de sous-classer le `Trainer` et d'ajouter les changements n√©cessaires, mais parfois il est plus simple d'√©crire la boucle d'entra√Ænement √† partir de z√©ro. C'est l√† qu'intervient ü§ó *Accelerate*.

{/if}

{#if fw === 'pt'}

## Entra√Æner avec ü§ó <i>Accelerate</i>

Nous avons vu comment entra√Æner un mod√®le avec le `Trainer`, qui permet une certaine personnalisation. Cependant, parfois nous voulons un contr√¥le total sur la boucle d'entra√Ænement ou nous souhaitons faire quelques changements exotiques. Dans ce cas, ü§ó *Accelerate* est un excellent choix, et dans cette section, nous allons suivre les √©tapes pour l'utiliser pour entra√Æner notre mod√®le. Pour rendre les choses plus int√©ressantes, nous allons √©galement ajouter une touche √† la boucle d'entra√Ænement.

<Youtube id="Hm8_PgVTFuc"/>

Puisque nous sommes principalement int√©ress√©s par l'autocompl√©tion pour les biblioth√®ques de science des donn√©es, il est logique de donner plus de poids aux √©chantillons d'entra√Ænement qui utilisent davantage ces biblioth√®ques. Nous pouvons facilement identifier ces exemples gr√¢ce √† l'utilisation de mots-cl√©s tels que `plt`, `pd`, `sk`, `fit`, et `predict`, qui sont les noms d'importation les plus fr√©quents pour `matplotlib.pyplot`, `pandas`, et `sklearn` ainsi que les fonctions `fit` et `predict` de cette derni√®re. Si chacun d'entre eux est repr√©sent√© par un seul *token*, nous pouvons facilement v√©rifier s'ils apparaissent dans la s√©quence d'entr√©e. Les *tokens* peuvent avoir un pr√©fixe d'espacement, donc nous v√©rifierons aussi ces versions dans le vocabulaire du *tokenizer*. Pour v√©rifier que cela fonctionne, nous ajouterons un *token* de test qui devrait √™tre divis√© en plusieurs *tokens* :

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

Super, √ßa a l'air de bien fonctionner ! Nous pouvons maintenant √©crire une fonction de perte personnalis√©e qui prend la s√©quence d'entr√©e, les logits et les *tokens* cl√©s que nous venons de s√©lectionner comme entr√©es. Tout d'abord, nous devons aligner les logits et les entr√©es : la s√©quence d'entr√©e d√©cal√©e d'une unit√© vers la droite forme les √©tiquettes, puisque le *token* suivant est l'√©tiquette du *token* actuel. Nous pouvons y parvenir en commen√ßant les √©tiquettes √† partir du deuxi√®me *token* de la s√©quence d'entr√©e, puisque le mod√®le ne fait pas de pr√©diction pour le premier *token* de toute fa√ßon. Ensuite, nous coupons le dernier logit, car nous n'avons pas d'√©tiquette pour le *token* qui suit la s√©quence d'entr√©e compl√®te. Avec cela, nous pouvons calculer la perte par √©chantillon et compter les occurrences de tous les mots-cl√©s dans chaque √©chantillon. Enfin, nous calculons la moyenne pond√©r√©e sur tous les √©chantillons en utilisant les occurrences comme poids. Comme nous ne voulons pas rejeter tous les √©chantillons qui ne contiennent pas de mots-cl√©s, nous ajoutons 1 aux poids :

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # D√©calage pour que tokens < n pr√©disent n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calcul de la perte par token
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Redimensionnement et perte moyenne par √©chantillon
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculer et √©chelonner la pond√©ration
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculer la moyenne pond√©r√©e
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

Avant de commencer √† entra√Æner avec cette nouvelle fonction de perte g√©niale, nous devons pr√©parer quelques √©l√©ments :

- Nous avons besoin de chargeurs de donn√©es pour charger les donn√©es par batch.
- Nous devons d√©finir les param√®tres de d√©croissance des poids.
- De temps en temps, nous voulons √©valuer, il est donc logique d'envelopper le code d'√©valuation dans une fonction.

Commen√ßons par les chargeurs de donn√©es. Nous avons seulement besoin de d√©finir le format du jeu de donn√©es √† `"torch"` et ensuite nous pouvons le passer √† un PyTorch `DataLoader` avec la taille de batch appropri√©e :

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

Ensuite, nous regroupons les param√®tres de fa√ßon √† ce que l'optimiseur sache lesquels b√©n√©ficieront d'une d√©croissance de poids suppl√©mentaire. Habituellement, tous les termes de biais et les poids de la *LayerNorm* en sont exempt√©s. Voici comment nous pouvons le faire :

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

Puisque nous voulons √©valuer le mod√®le r√©guli√®rement sur l'ensemble de validation pendant l'entra√Ænement, √©crivons une fonction pour cela aussi. Elle passe simplement par le *dataloader* d'√©valuation et rassemble toutes les pertes √† travers les processus :

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

Avec la fonction `evaluate()` nous pouvons rapporter la perte et la [perplexit√©](/course/fr/chapter7/3) √† intervalles r√©guliers. Ensuite, nous red√©finissons notre mod√®le pour nous assurer que nous entra√Ænons √† nouveau √† partir de z√©ro :

```py
model = GPT2LMHeadModel(config)
```

Nous pouvons ensuite d√©finir notre optimiseur, en utilisant la fonction pr√©c√©dente pour diviser les param√®tres de d√©croissance des poids :

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

Pr√©parons maintenant le mod√®le, l'optimiseur et les chargeurs de donn√©es pour pouvoir commencer l'entra√Ænement :

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® Si vous vous entra√Ænez sur un TPU, vous devrez d√©placer tout le code commen√ßant √† la cellule ci-dessus dans une fonction d'entra√Ænement d√©di√©e. Voir le [chapitre 3](/course/fr/chapter3) pour plus de d√©tails.

</Tip>

Maintenant que nous avons envoy√© notre `train_dataloader` √† `accelerator.prepare()`, nous pouvons utiliser sa longueur pour calculer le nombre d'√©tapes d'entra√Ænement. Rappelez-vous que nous devons toujours faire cela apr√®s avoir pr√©par√© le *dataloader* car cette m√©thode modifiera sa longueur. Nous utilisons un programme lin√©aire classique du taux d'apprentissage √† 0 :

```py
num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

Enfin, pour pousser notre mod√®le vers le *Hub*, nous aurons besoin de cr√©er un objet `Repository` dans un dossier de travail. Tout d'abord, connectez-vous au *Hub*, si vous n'√™tes pas d√©j√† connect√©. Nous d√©terminerons le nom du d√©p√¥t √† partir de l'identifiant du mod√®le que nous voulons donner √† notre mod√®le (n'h√©sitez pas √† remplacer le `repo_name` par votre propre choix. Il doit juste contenir votre nom d'utilisateur, ce que fait la fonction `get_full_repo_name()`) :

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

Ensuite, nous pouvons cloner ce d√©p√¥t dans un dossier local. S'il existe d√©j√†, ce dossier local doit √™tre un clone existant du d√©p√¥t avec lequel nous travaillons :

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Nous pouvons maintenant t√©l√©charger tout ce que nous sauvegardons dans `output_dir` en appelant la m√©thode `repo.push_to_hub()`. Cela nous aidera √† t√©l√©charger les mod√®les interm√©diaires √† la fin de chaque √©poque.

Avant de nous entra√Æner, ex√©cutons un test rapide pour voir si la fonction d'√©valuation fonctionne correctement :

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

Ce sont des valeurs tr√®s √©lev√©es pour la perte et la perplexit√©, mais ce n'est pas surprenant puisque nous n'avons pas encore entra√Æn√© le mod√®le. Avec cela, nous avons tout pr√©par√© pour √©crire la partie principale du script d'entra√Ænement : la boucle d'entra√Ænement. Dans celle-ci, nous it√©rons sur le chargeur de donn√©es et transmettons les batchs au mod√®le. Avec les logits, nous pouvons alors √©valuer notre fonction de perte personnalis√©e. Nous mettons √† l'√©chelle la perte par le nombre d'√©tapes d'accumulation du gradient afin de ne pas cr√©er de plus grandes pertes en agr√©geant plus d'√©tapes. Avant de proc√©der √† l'optimisation, nous d√©coupons √©galement les gradients pour une meilleure convergence. Enfin, tous les quelques pas, nous √©valuons le mod√®le sur l'ensemble d'√©valuation avec notre nouvelle fonction `evaluate()` :

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=len(train_dataloader)
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

Et voil√†, vous disposez maintenant de votre propre boucle d'entra√Ænement personnalis√©e pour les mod√®les de langage causal tels que le GPT-2. Vous pouvez encore l'adapter √† vos besoins. 

<Tip>

‚úèÔ∏è **Essayez !** Vous pouvez cr√©er votre propre fonction de perte personnalis√©e, adapt√©e √† votre cas d'utilisation, ou ajouter une autre √©tape personnalis√©e dans la boucle d'entra√Ænement.

</Tip>

<Tip>

‚úèÔ∏è **Essayez !** Lorsque vous effectuez de longues exp√©riences d'entra√Ænement, il est bon d'enregistrer les mesures importantes √† l'aide d'outils tels que *TensorBoard* ou *Weights & Biases*. Ajoutez l'un d'eux √† la boucle d'entra√Ænement afin de pouvoir toujours v√©rifier comment se d√©roule l'entra√Ænement.

</Tip>

{/if}
