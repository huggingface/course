<FrameworkSwitchCourse {fw} />

# R√©sum√© de textes

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},
]} />

{/if}


Dans cette section, nous allons voir comment les *transformers* peuvent √™tre utilis√©s pour condenser de longs documents en r√©sum√©s, une t√¢che connue sous le nom de _r√©sum√© de texte_. Il s'agit de l'une des t√¢ches de NLP les plus difficiles car elle requiert une s√©rie de capacit√©s, telles que la compr√©hension de longs passages et la g√©n√©ration d'un texte coh√©rent qui capture les sujets principaux d'un document. Cependant, lorsqu'il est bien fait, le r√©sum√© de texte est un outil puissant qui peut acc√©l√©rer divers processus commerciaux en soulageant les experts du domaine de la lecture d√©taill√©e de longs documents.

<Youtube id="yHnr5Dk2zCI"/>

Bien qu'il existe d√©j√† plusieurs mod√®les *finetun√©s* pour le r√©sum√© sur le [*Hub*](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), la plupart d'entre eux ne sont adapt√©s qu'aux documents en anglais. Ainsi, pour ajouter une touche d'originalit√© √† cette section, nous allons entra√Æner un mod√®le bilingue pour l'anglais et l'espagnol. √Ä la fin de cette section, vous disposerez d'un [mod√®le](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) capable de r√©sumer les commentaires des clients comme celui pr√©sent√© ici :

<iframe src="https://hf.space/gradioiframe/course-demos/mt5-small-finetuned-amazon-en-es/+" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://hf.space/gradioiframe/course-demos/mt5-small-finetuned-amazon-en-es-darkmode/+" frameBorder="0" height="400" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Comme nous allons le voir, ces r√©sum√©s sont concis car ils sont appris √† partir des titres que les clients fournissent dans leurs commentaires sur les produits. Commen√ßons par constituer un corpus bilingue appropri√© pour cette t√¢che.

## Pr√©paration d'un corpus multilingue

Nous allons utiliser le [*Multilingual Amazon Reviews Corpus*](https://huggingface.co/datasets/amazon_reviews_multi) pour cr√©er notre r√©sumeur bilingue. Ce corpus est constitu√© de critiques de produits Amazon en six langues et est g√©n√©ralement utilis√© pour √©valuer les classifieurs multilingues. Cependant, comme chaque critique est accompagn√©e d'un titre court, nous pouvons utiliser les titres comme r√©sum√©s cibles pour l'apprentissage de notre mod√®le ! Pour commencer, t√©l√©chargeons les sous-ensembles anglais et espagnols depuis le *Hub* :

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

Comme vous pouvez le voir, pour chaque langue, il y a 200 000 critiques pour la partie entra√Ænement et 5 000 critiques pour chacune des parties validation et test. Les informations qui nous int√©ressent sont contenues dans les colonnes `review_body` et `review_title`. Voyons quelques exemples en cr√©ant une fonction simple qui prend un √©chantillon al√©atoire de l'ensemble d'entra√Ænement avec les techniques apprises au [chapitre 5](/course/fr/chapter5) :

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
# Travaill√© en position avant, pas arri√®re
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'
# 3 √©toiles car ce ne sont pas des freins arri√®re comme indiqu√© dans la description de l'article. Au moins, l'adaptateur de montage ne fonctionnait que sur la fourche avant du v√©lo pour lequel je l'ai achet√©.

'>> Title: meh'
'>> Review: Does it‚Äôs job and it‚Äôs gorgeous but mine is falling apart, I had to basically put it together again with hot glue'
# Il fait son travail et il est magnifique mais le mien est en train de tomber en morceaux, j'ai d√ª le recoller avec de la colle chaude.

'>> Title: Can\'t beat these for the money' 
# On ne peut pas faire mieux pour le prix
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
# Je l'ai achet√© pour manipuler diverses pi√®ces d'avion et des "trucs" de hangar que je devais organiser ; il a vraiment fait l'affaire. L'unit√© est arriv√©e rapidement, √©tait bien emball√©e et est arriv√©e intacte (toujours un bon signe). Il y a cinq supports muraux - trois sur le dessus et deux sur le dessous. Je voulais le monter sur le mur, alors tout ce que j'ai eu √† faire √©tait d'enlever les deux couches sup√©rieures de tiroirs en plastique, ainsi que les tiroirs d'angle inf√©rieurs, de le placer o√π je voulais et de le marquer ; j'ai ensuite utilis√© quelques-uns des nouveaux ancrages muraux √† vis en plastique (la vari√©t√© de 50 livres) et il s'est facilement mont√© sur le mur. Certains ont fait remarquer qu'ils voulaient des s√©parateurs pour les tiroirs, et qu'ils les ont fabriqu√©s. Bonne id√©e. Pour ma part, j'avais besoin de quelque chose dont je pouvais voir le contenu √† hauteur des yeux, et je voulais donc des tiroirs plus grands. J'aime aussi le fait qu'il s'agisse du nouveau plastique qui ne se fragilise pas et ne se fend pas comme mes anciens tiroirs en plastique. J'aime la construction enti√®rement en plastique. Elle est suffisamment r√©sistante pour contenir des pi√®ces m√©talliques, mais √©tant en plastique, elle n'est pas aussi lourde qu'un cadre m√©tallique, ce qui permet de la fixer facilement au mur et de la charger d'objets lourds ou l√©gers. Aucun probl√®me. Pour le prix, c'est imbattable. C'est le meilleur que j'ai achet√© √† ce jour, et j'utilise des versions de ce type depuis plus de quarante ans.
```

<Tip>

‚úèÔ∏è **Essayez !** Changez la graine al√©atoire dans la commande `Dataset.shuffle()` pour explorer d'autres critiques dans le corpus. Si vous parlez espagnol, jetez un coup d'≈ìil  √† certaines des critiques dans `spanish_dataset` pour voir si les titres semblent aussi √™tre des r√©sum√©s raisonnables.

</Tip>

Cet √©chantillon montre la diversit√© des critiques que l'on trouve g√©n√©ralement en ligne, allant du positif au n√©gatif (et tout ce qui se trouve entre les deux !). Bien que l'exemple avec le titre ¬´ meh ¬ª ne soit pas tr√®s informatif, les autres titres semblent √™tre des r√©sum√©s d√©cents des critiques. Entra√Æner un mod√®le de r√©sum√© sur l'ensemble des 400 000 avis prendrait beaucoup trop de temps sur un seul GPU, nous allons donc nous concentrer sur la g√©n√©ration de r√©sum√©s pour un seul domaine de produits. Pour avoir une id√©e des domaines parmi lesquels nous pouvons choisir, convertissons `english_dataset` en `pandas.DataFrame` et calculons le nombre d'avis par cat√©gorie de produits :

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Afficher le compte des 20 premiers produits
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679   # maison
apparel                   15951   # v√™tements
wireless                  15717   # sans fil
other                     13418   # autres
beauty                    12091   # beaut√©
drugstore                 11730   # pharmacie
kitchen                   10382   # cuisine
toy                        8745   # jouets
sports                     8277   # sports
automotive                 7506   # automobile
lawn_and_garden            7327   # pelouse_et_jardin
home_improvement           7136   # am√©lioration_de_la_maison
pet_products               7082   # produits_pour_animaux_de_compagnie
digital_ebook_purchase     6749   # achat_de_livres_num√©riques 
pc                         6401   # ordinateur_personnel
electronics                6186   # √©lectronique
office_product             5521   # produits_de_bureau 
shoes                      5197   # chaussures 
grocery                    4730   # √©picerie
book                       3756   # livre
Name: product_category, dtype: int64
```

Les produits les plus populaires du jeu de donn√©es anglais concernent les articles m√©nagers, les v√™tements et l'√©lectronique sans fil. Pour rester dans le th√®me d'Amazon, nous allons nous concentrer sur le r√©sum√© des critiques de livres. Apr√®s tout, c'est la raison d'√™tre de l'entreprise ! Nous pouvons voir deux cat√©gories de produits qui correspondent √† nos besoins (`book` et `digital_ebook_purchase`). Nous allons donc filtrer les jeux de donn√©es dans les deux langues pour ces produits uniquement. Comme nous l'avons vu dans le [chapitre 5](/course/fr/chapter5), la fonction `Dataset.filter()` nous permet de d√©couper un jeu de donn√©es de mani√®re tr√®s efficace. Nous pouvons donc d√©finir une fonction simple pour le faire :

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

Maintenant, lorsque nous appliquons cette fonction √† `english_dataset` et `spanish_dataset`, le r√©sultat ne contient que les lignes impliquant les cat√©gories de livres. Avant d'appliquer le filtre, changeons le format de `english_dataset` de `"pandas"` √† `"arrow"` :

```python
english_dataset.reset_format()
```

Nous pouvons ensuite appliquer la fonction de filtrage et, √† titre de v√©rification, inspecter un √©chantillon de critiques pour voir si elles portent bien sur des livres :

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.' 
# Je suis d√©√ßu
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'
# Je suppose que j'avais de plus grandes attentes pour ce livre d'apr√®s les critiques. Je pensais vraiment que j'allais au moins l'aimer. L'id√©e de l'intrigue √©tait g√©niale. J'aimais Ash, mais √ßa n'allait nulle part. La plus grande partie du livre √©tait consacr√©e √† leur √©mission de radio et aux conversations avec les auditeurs. Je voulais que l'auteur creuse plus profond√©ment pour que nous puissions vraiment conna√Ætre les personnages. Tout ce que nous savons de Grace, c'est qu'elle est s√©duisante, qu'elle est latino et qu'elle est une sorte de garce. Je suis d√©√ßue.

'>> Title: Good art, good price, poor design' 
# Un bon art, un bon prix, un mauvais design
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'
# J'ai eu le calendrier DC Vintage ces deux derni√®res ann√©es, mais il √©tait en rupture de stock pour toujours cette ann√©e et j'ai vu qu'ils avaient r√©duit les dimensions sans raison valable. Celui-ci a de bons choix artistiques mais le design a le pli qui traverse l'image, donc c'est moins esth√©tique, surtout si vous voulez garder une image √† accrocher. Pour le prix, c'est un bon calendrier.

'>> Title: Helpful'
# Utile
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
# Presque tous les conseils sont utiles et. Je me consid√®re comme un utilisateur interm√©diaire √† avanc√© de OneNote. Je le recommande vivement.
```

D'accord, nous pouvons voir que les critiques ne concernent pas strictement les livres et peuvent se r√©f√©rer √† des choses comme des calendriers et des applications √©lectroniques telles que OneNote. N√©anmoins, le domaine semble appropri√© pour entra√Æner un mod√®le de r√©sum√©. Avant de regarder les diff√©rents mod√®les qui conviennent √† cette t√¢che, nous avons une derni√®re pr√©paration de donn√©es √† faire : combiner les critiques anglaises et espagnoles en un seul objet `DatasetDict`. ü§ó *Datasets* fournit une fonction pratique `concatenate_datasets()` qui (comme son nom l'indique) va empiler deux objets `Dataset` l'un sur l'autre. Ainsi, pour cr√©er notre jeu de donn√©es bilingue, nous allons boucler sur chaque division, concat√©ner les jeux de donn√©es pour cette division, et m√©langer le r√©sultat pour s'assurer que notre mod√®le ne s'adapte pas trop √† une seule langue :

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Quelques exemples
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!' 
# Facile √† suivre!!!!
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'
# J'ai ador√© The dash diet weight loss Solution. Jamais faim. Je recommande ce r√©gime. Les menus sont √©galement bien arrondis. Essayez-le. Il contient beaucoup d'informations, merci.

'>> Title: PARCIALMENTE DA√ëADO' 
# PARTIELLEMENT ENDOMMAG√â
'>> Review: Me lleg√≥ el d√≠a que tocaba, junto a otros libros que ped√≠, pero la caja lleg√≥ en mal estado lo cual da√±√≥ las esquinas de los libros porque ven√≠an sin protecci√≥n (forro).'
# Il est arriv√© le jour pr√©vu, avec d'autres livres que j'avais command√©s, mais la bo√Æte est arriv√©e en mauvais √©tat, ce qui a endommag√© les coins des livres car ils √©taient livr√©s sans protection (doublure).

'>> Title: no lo he podido descargar' 
# Je n'ai pas pu le t√©l√©charger
'>> Review: igual que el anterior' 
# m√™me chose que ci-dessus
```

Cela ressemble certainement √† un m√©lange de critiques anglaises et espagnoles ! Maintenant que nous avons un corpus d'entra√Ænement, une derni√®re chose √† v√©rifier est la distribution des mots dans les critiques et leurs titres. Ceci est particuli√®rement important pour les t√¢ches de r√©sum√©, o√π les r√©sum√©s de r√©f√©rence courts dans les donn√©es peuvent biaiser le mod√®le pour qu'il ne produise qu'un ou deux mots dans les r√©sum√©s g√©n√©r√©s. Les graphiques ci-dessous montrent les distributions de mots, et nous pouvons voir que les titres sont fortement biais√©s vers seulement 1 ou 2 mots :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

Pour y rem√©dier, nous allons filtrer les exemples avec des titres tr√®s courts afin que notre mod√®le puisse produire des r√©sum√©s plus int√©ressants. Puisque nous avons affaire √† des textes anglais et espagnols, nous pouvons utiliser une heuristique grossi√®re pour s√©parer les titres sur les espaces blancs, puis utiliser notre fid√®le m√©thode `Dataset.filter()` comme suit :

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

Maintenant que nous avons pr√©par√© notre corpus, voyons quelques *transformers* possibles que l'on pourrait *finetun√©* dessus !

## Mod√®les pour le r√©sum√© de texte

Si vous y pensez, le r√©sum√© de texte est une t√¢che similaire √† la traduction automatique. Nous avons un corps de texte, comme une critique, que nous aimerions ¬´ traduire ¬ª en une version plus courte qui capture les caract√©ristiques saillantes de l'entr√©e. En cons√©quence, la plupart des *transformers* pour le r√©sum√© adoptent l'architecture encodeur-d√©codeur que nous avons rencontr√©e pour la premi√®re fois dans le [chapitre 1](/course/fr/chapter1), bien qu'il y ait quelques exceptions comme la famille de mod√®les GPT qui peut √©galement √™tre utilis√©e pour le r√©sum√© dans des contextes peu complexes. Le tableau suivant pr√©sente quelques mod√®les pr√©-entra√Æn√©s populaires qui peuvent √™tre *finetun√©s* pour le r√©sum√©.

| *Transformers* | Description                                                                                                                                                                                                    | Multilingue ? |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | Bien qu'il soit entra√Æn√© comme un mod√®le de langage autor√©gressif, vous pouvez faire en sorte que le GPT-2 g√©n√®re des r√©sum√©s en ajoutant `TL;DR` √† la fin du texte d'entr√©e.                                                                          | ‚ùå |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | Utilise un objectif de pr√©-entra√Ænement pour pr√©dire les phrases masqu√©es dans les textes √† plusieurs phrases. Cet objectif de pr√©-entra√Ænement est plus proche du r√©sum√© que de la mod√©lisation du langage standard et obtient des scores √©lev√©s sur des *benchmarks* populaires. | ‚ùå |
|     [T5](https://huggingface.co/t5-base)      | Une architecture universelle de *transformer* qui formule toutes les t√¢ches dans un cadre texte √† texte. Par exemple, le format d'entr√©e du mod√®le pour r√©sumer un document est `summarize: ARTICLE`.                              | ‚ùå |
|     [mT5](https://huggingface.co/google/mt5-base)     | Une version multilingue de T5, pr√©-entra√Æn√©e sur le corpus multilingue Common Crawl (mC4), couvrant 101 langues.                                                                                                | ‚úÖ |
|    [BART](https://huggingface.co/facebook/bart-base)     | Une architecture de *transformer* avec une pile d'encodeurs et de d√©codeurs entra√Æn√©s pour reconstruire l'entr√©e corrompue qui combine les sch√©mas de pr√©-entra√Ænement de BERT et GPT-2.                                    | ‚ùå |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | Une version multilingue de BART, pr√©-entra√Æn√©e sur 50 langues.                                                                                                                                                     |      ‚úÖ       |

Comme vous pouvez le voir dans ce tableau, la majorit√© des *transformers* pour le r√©sum√© (et en fait la plupart des t√¢ches de NLP) sont monolingues. C'est une bonne chose si votre t√¢che se d√©roule dans une langue ¬´ √† haute ressource ¬ª comme l'anglais ou l'allemand, mais moins pour les milliers d'autres langues utilis√©es dans le monde. Heureusement, il existe une cat√©gorie de *transformers* multilingues, comme mT5 et mBART, qui viennent √† la rescousse. Ces mod√®les sont pr√©-entra√Æn√©s en utilisant la mod√©lisation du langage mais avec une particularit√© : au lieu d'√™tre entra√Æn√© sur un corpus d'une seule langue, ils sont entra√Æn√©s conjointement sur des textes dans plus de 50 langues !

Nous allons nous concentrer sur mT5, une architecture int√©ressante bas√©e sur T5 qui a √©t√© pr√©-entra√Æn√©e dans un cadre texte √† texte. Dans T5, chaque t√¢che de NLP est formul√©e en termes d'un pr√©fixe de *prompt* comme `summarize:` qui conditionne le mod√®le √† adapter le texte g√©n√©r√© au *prompt*. Comme le montre la figure ci-dessous, cela rend le T5 extr√™mement polyvalent car vous pouvez r√©soudre de nombreuses t√¢ches avec un seul mod√®le !

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

mT5 n'utilise pas de pr√©fixes mais partage une grande partie de la polyvalence de T5 et a l'avantage d'√™tre multilingue. Maintenant que nous avons choisi un mod√®le, voyons comment pr√©parer nos donn√©es pour l'entra√Ænement.


<Tip>

‚úèÔ∏è **Essayez !** Une fois que vous aurez termin√© cette section, comparez le mT5 √† mBART en *finetunant* ce dernier avec les m√™mes techniques. Pour des points bonus, vous pouvez aussi essayer de *finetuner* le T5 uniquement sur les critiques anglaises. Puisque le T5 a un pr√©fixe sp√©cial, vous devrez ajouter `summarize:` aux entr√©es dans les √©tapes de pr√©traitement ci-dessous.

</Tip>

## Pr√©traitement des donn√©es

<Youtube id="1m7BerpSq8A"/>

Notre prochaine t√¢che est de tokeniser et d'encoder nos critiques et leurs titres. Comme d'habitude, nous commen√ßons par charger le *tokenizer* associ√© au *checkpoint* du mod√®le pr√©-entra√Æn√©. Nous utiliserons `mt5-small` comme *checkpoint* afin de pouvoir *finetuner* le mod√®le en un temps raisonnable :

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

üí° Aux premiers stades de vos projets de NLP, une bonne pratique consiste √† entra√Æner une classe de ¬´ petits ¬ª mod√®les sur un petit √©chantillon de donn√©es. Cela vous permet de d√©boguer et d'it√©rer plus rapidement vers un flux de travail de bout en bout. Une fois que vous avez confiance dans les r√©sultats, vous pouvez toujours faire √©voluer le mod√®le en changeant simplement le *checkpoint* du mod√®le !

</Tip>

Testons le *tokenizer* de mT5 sur un petit exemple :

```python
inputs = tokenizer(
    "I loved reading the Hunger Games!"
)  # J'ai ador√© lire les Hunger Games !
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Ici nous pouvons voir les familiers `input_ids` et `attention_mask` que nous avons rencontr√©s dans nos premi√®res exp√©riences de *finetuning* au [chapitre 3](/course/fr/chapter3). D√©codons ces identifiants d'entr√©e avec la fonction `convert_ids_to_tokens()` du *tokenizer* pour voir √† quel type de *tokenizer* nous avons affaire :

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['‚ñÅI', '‚ñÅ', 'loved', '‚ñÅreading', '‚ñÅthe', '‚ñÅHung', 'er', '‚ñÅGames', '</s>']
```

Le caract√®re Unicode sp√©cial `‚ñÅ` et le *token* de fin de s√©quence `</s>` indiquent que nous avons affaire au *tokenizer* de SentencePiece, qui est bas√© sur l'algorithme de segmentation Unigram discut√© dans le [chapitre 6](/course/chapter6). Unigram est particuli√®rement utile pour les corpus multilingues car il permet √† SentencePiece d'√™tre agnostique vis-√†-vis des accents, de la ponctuation et du fait que de nombreuses langues, comme le japonais, n'ont pas de caract√®res d'espacement.

Pour tokeniser notre corpus, nous devons faire face √† une subtilit√© associ√©e au r√©sum√© : comme nos √©tiquettes sont √©galement du texte, il est possible qu'elles d√©passent la taille maximale du contexte du mod√®le. Cela signifie que nous devons appliquer une troncature √† la fois aux critiques et √† leurs titres pour nous assurer de ne pas transmettre des entr√©es trop longues √† notre mod√®le. Les tokenizers de ü§ó *Transformers* fournissent une fonction tr√®s pratique `as_target_tokenizer()` qui vous permet de tokeniser les √©tiquettes en parall√®le avec les entr√©es. Ceci est typiquement fait en utilisant un gestionnaire de contexte √† l'int√©rieur d'une fonction de pr√©traitement qui encode d'abord les entr√©es, et ensuite encode les √©tiquettes comme une colonne s√©par√©e. Voici un exemple d'une telle fonction pour mT5 :

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"], max_length=max_input_length, truncation=True
    )
    # Configurer le tokenizer pour les cibles
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["review_title"], max_length=max_target_length, truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

Parcourons ce code pour comprendre ce qui se passe. La premi√®re chose que nous avons faite est de d√©finir des valeurs pour `max_input_length` et `max_target_length`, qui fixent les limites sup√©rieures de la longueur des commentaires et des titres. Comme le corps de la critique est g√©n√©ralement beaucoup plus long que le titre, nous avons mis ces valeurs √† l'√©chelle en cons√©quence. Ensuite, dans la `preprocess_function()` elle-m√™me, nous pouvons voir que les commentaires sont d'abord tokeniz√©s, suivis par les titres avec `as_target_tokenizer()`.

Avec la fonction `preprocess_function()`, il est alors simple de tokeniser l'ensemble du corpus en utilisant la fonction pratique `Dataset.map()` que nous avons largement utilis√©e dans ce cours :

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

Maintenant que le corpus a √©t√© pr√©trait√©, examinons certaines m√©triques couramment utilis√©es pour le r√©sum√©. Comme nous allons le voir, il n'existe pas de solution miracle pour mesurer la qualit√© d'un texte g√©n√©r√© par une machine.

<Tip>

üí° Vous avez peut-√™tre remarqu√© que nous avons utilis√© `batched=True` dans notre fonction `Dataset.map()` ci-dessus. Cela permet de coder les exemples par lots de 1 000 (par d√©faut) et d'utiliser les capacit√©s de *multithreading* des *tokenizers* rapides de ü§ó *Transformers*. Lorsque cela est possible, essayez d'utiliser `batched=True` pour tirer le meilleur parti de votre pr√©traitement !

</Tip>


## M√©triques pour le r√©sum√© de texte

<Youtube id="TMshhnrEXlg"/>

Par rapport √† la plupart des autres t√¢ches que nous avons abord√©es dans ce cours, la mesure des performances des t√¢ches de g√©n√©ration de texte comme le r√©sum√© ou la traduction n'est pas aussi simple. Par exemple, pour une critique telle que ¬´ J'ai ador√© lire les Hunger Games ¬ª, il existe plusieurs r√©sum√©s valides, comme ¬´ J'ai ador√© Hunger Games ¬ª ou ¬´ Hunger Games est une excellente lecture ¬ª. Il est clair que l'application d'une sorte de correspondance exacte entre le r√©sum√© g√©n√©r√© et l'√©tiquette n'est pas une bonne solution. En effet, m√™me les humains auraient de mauvais r√©sultats avec une telle mesure, car nous avons tous notre propre style d'√©criture.

Pour le r√©sum√©, l'une des m√©triques les plus couramment utilis√©es est le [score ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (abr√©viation de *Recall-Oriented Understudy for Gisting Evaluation*). L'id√©e de base de cette m√©trique est de comparer un r√©sum√© g√©n√©r√© avec un ensemble de r√©sum√©s de r√©f√©rence qui sont g√©n√©ralement cr√©√©s par des humains. Pour √™tre plus pr√©cis, supposons que nous voulions comparer les deux r√©sum√©s suivants :

```python
generated_summary = "I absolutely loved reading the Hunger Games"
# "J'ai absolument ador√© lire les Hunger Games"
reference_summary = "I loved reading the Hunger Games"
# "J'ai ador√© lire les Hunger Games"
```

Une fa√ßon de les comparer pourrait √™tre de compter le nombre de mots qui se chevauchent, qui dans ce cas serait de 6. Cependant, cette m√©thode est un peu grossi√®re, c'est pourquoi ROUGE se base sur le calcul des scores de _pr√©cision_ et de _rappel_ pour le chevauchement.

<Tip>

üôã Ne vous inqui√©tez pas si c'est la premi√®re fois que vous entendez parler de pr√©cision et de rappel. Nous allons parcourir ensemble quelques exemples explicites pour que tout soit clair. Ces m√©triques sont g√©n√©ralement rencontr√©es dans les t√¢ches de classification, donc si vous voulez comprendre comment la pr√©cision et le rappel sont d√©finis dans ce contexte, nous vous recommandons de consulter les [guides de `scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html).

</Tip>

Pour ROUGE, le rappel mesure la proportion du r√©sum√© de r√©f√©rence qui est captur√©e par le r√©sum√© g√©n√©r√©. Si nous ne faisons que comparer des mots, le rappel peut √™tre calcul√© selon la formule suivante :

$$ \mathrm{Recall} = \frac{\mathrm{Nombre\,de\,mots\,qui\,se\,chevauchent}}{\mathrm{Nombre\, total\, de\, mots\, dans\, le\, r√©sum√©\, de\, r√©ference}} $$

Pour notre exemple simple ci-dessus, cette formule donne un rappel parfait de 6/6 = 1, c'est-√†-dire que tous les mots du r√©sum√© de r√©f√©rence ont √©t√© produits par le mod√®le. Cela peut sembler g√©nial, mais imaginez que le r√©sum√© g√©n√©r√© ait √©t√© ¬´ J'ai vraiment aim√© lire les Hunger Games toute la nuit ¬ª. Le rappel serait √©galement parfait, mais le r√©sum√© serait sans doute moins bon puisqu'il serait verbeux. Pour traiter ces sc√©narios, nous calculons √©galement la pr√©cision, qui dans le contexte de ROUGE, mesure la proportion du r√©sum√© g√©n√©r√© qui est pertinente :

$$ \mathrm{Precision} = \frac{\mathrm{Nombre\,de\,mots\,qui\,se\,chevauchent}}{\mathrm{Nombre\, total\, de\, mots\, dans\, le\, r√©sum√©\, g√©n√©r√©}} $$

En appliquant cela √† notre r√©sum√© verbeux, on obtient une pr√©cision de 6/10 = 0,6, ce qui est consid√©rablement moins bon que la pr√©cision de 6/7 = 0,86 obtenue par notre r√©sum√© plus court. En pratique, la pr√©cision et le rappel sont g√©n√©ralement calcul√©s, puis le score F1 (la moyenne harmonique de la pr√©cision et du rappel) est indiqu√©. Nous pouvons le faire facilement dans ü§ó *Datasets* en installant d'abord le *package* `rouge_score` :

```py
!pip install rouge_score
```

et ensuite charger la m√©trique ROUGE comme suit :

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

Ensuite, nous pouvons utiliser la fonction `rouge_score.compute()` pour calculer toutes les m√©triques en une seule fois :

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

Whoa, il y a pas mal d'informations dans cette sortie. Qu'est-ce que √ßa veut dire ? Tout d'abord, ü§ó *Datasets* calcule des intervalles de confiance pour la pr√©cision, le rappel et le score F1. Ce sont les attributs `low`, `mid`, et `high` que vous pouvez voir ici. De plus, ü§ó *Datasets* calcule une vari√©t√© de scores ROUGE qui sont bas√©s sur diff√©rents types de granularit√© du texte lors de la comparaison des r√©sum√©s g√©n√©r√©s et de r√©f√©rence. La variante `rouge1` est le chevauchement des unigrammes. C'est juste une fa√ßon fantaisiste de dire le chevauchement des mots et c'est exactement la m√©trique dont nous avons discut√© ci-dessus. Pour v√©rifier cela, nous allons extraire la valeur `mid` de nos scores :

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

Super, les chiffres de pr√©cision et de rappel correspondent ! Maintenant, qu'en est-il des autres scores ROUGE ? `rouge2` mesure le chevauchement entre les bigrammes (chevauchement des paires de mots), tandis que `rougeL` et `rougeLsum` mesurent les plus longues s√©quences de mots correspondants en recherchant les plus longues sous-souches communes dans les r√©sum√©s g√©n√©r√©s et de r√©f√©rence. Le ¬´ sum ¬ª dans `rougeLsum` fait r√©f√©rence au fait que cette m√©trique est calcul√©e sur un r√©sum√© entier, alors que `rougeL` est calcul√©e comme une moyenne sur des phrases individuelles.

<Tip>

‚úèÔ∏è **Essayez !** Cr√©ez votre propre exemple de r√©sum√© g√©n√©r√© et de r√©f√©rence et voyez si les scores ROUGE obtenus correspondent √† un calcul manuel bas√© sur les formules de pr√©cision et de rappel. Pour des points bonus, divisez le texte en bigrammes et comparez la pr√©cision et le rappel pour la m√©trique `rouge2`.

</Tip>

Nous utiliserons ces scores ROUGE pour suivre les performances de notre mod√®le, mais avant cela, faisons ce que tout bon praticien de NLP devrait faire : cr√©er une *baseline* solide, mais simple !

### Cr√©ation d'une base de r√©f√©rence solide

Une *baseline* commune pour le r√©sum√© de texte consiste √† prendre simplement les trois premi√®res phrases d'un article, souvent appel√©e la *baseline* _lead-3_. Nous pourrions utiliser les points pour tracker les limites des phrases mais cela √©chouera avec des acronymes comme ¬´ U.S. ¬ª ou ¬´ U.N. ¬ª. Nous allons donc utiliser la biblioth√®que `nltk`, qui inclut un meilleur algorithme pour g√©rer ces cas. Vous pouvez installer le *package* en utilisant `pip` comme suit :

```python
!pip install nltk
```

puis t√©l√©chargez les r√®gles de ponctuation :

```python
import nltk

nltk.download("punkt")
```

Ensuite, nous importons le *tokenizer* de `nltk` et cr√©ons une fonction simple pour extraire les trois premi√®res phrases d'une critique. La convention dans le r√©sum√© de texte est de s√©parer chaque r√©sum√© avec une nouvelle ligne, donc nous allons √©galement inclure ceci et tester le tout sur un exemple d'entra√Ænement :

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.' 
# J'ai grandi en lisant Koontz, et il y a des ann√©es, j'ai arr√™t√©, convaincu que je l'avais "d√©pass√©"
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.' 
# "Pourtant, quand une amie cherchait un livre √† suspense, je lui ai sugg√©r√© Koontz."
'She found Strangers.' 
# Elle a trouv√© Strangers.
```

Cela semble fonctionner, alors impl√©mentons maintenant une fonction qui extrait ces r√©sum√©s d'un jeu de donn√©es et calcule les scores ROUGE pour la ligne de base :

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

Nous pouvons ensuite utiliser cette fonction pour calculer les scores ROUGE sur l'ensemble de validation et les embellir un peu en utilisant Pandas :

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

Nous pouvons voir que le score de `rouge2` est significativement plus bas que le reste. Ceci refl√®te probablement le fait que les titres des critiques sont typiquement concis et donc que la *baseline* *lead-3* est trop verbeuse. Maintenant que nous disposons d'une bonne *baseline*, concentrons-nous sur le *finetuning* du mT5 !

{#if fw === 'pt'}

## <i>Finetuning</i> de mT5 avec l'API `Trainer`

Le *finetuning* d'un mod√®le pour le r√©sum√© est tr√®s similaire aux autres t√¢ches que nous avons couvertes dans ce chapitre. La premi√®re chose √† faire est de charger le mod√®le pr√©-entra√Æn√© √† partir du *checkpoint* `mt5-small`. Puisque la compression est une t√¢che de s√©quence √† s√©quence, nous pouvons charger le mod√®le avec la classe `AutoModelForSeq2SeqLM`, qui t√©l√©chargera automatiquement et mettra en cache les poids :

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## <i>Finetuning</i> de mT5 avec Keras

Le *finetuning* d'un mod√®le pour le r√©sum√© est tr√®s similaire aux autres t√¢ches que nous avons couvertes dans ce chapitre. La premi√®re chose √† faire est de charger le mod√®le pr√©-entra√Æn√© √† partir du *checkpoint* `mt5-small`. Puisque la compression est une t√¢che de s√©quence √† s√©quence, nous pouvons charger le mod√®le avec la classe `TFAutoModelForSeq2SeqLM`, qui t√©l√©chargera automatiquement et mettra en cache les poids :

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

üí° Si vous vous demandez pourquoi vous ne voyez aucun avertissement concernant le *finetuning* du mod√®le sur une t√¢che en aval, c'est parce que pour les t√¢ches de s√©quence √† s√©quence, nous conservons tous les poids du r√©seau. Comparez cela √† notre mod√®le de classification de texte du [chapitre 3](/course/fr/chapter3) o√π la t√™te du mod√®le pr√©-entra√Æn√© a √©t√© remplac√©e par un r√©seau initialis√© de mani√®re al√©atoire.

</Tip>

La prochaine chose que nous devons faire est de nous connecter au *Hub*. Si vous ex√©cutez ce code dans un *notebook*, vous pouvez le faire avec la fonction utilitaire suivante :

```python
from huggingface_hub import notebook_login

notebook_login()
```

qui affichera un *widget* o√π vous pourrez saisir vos informations d'identification. Vous pouvez √©galement ex√©cuter cette commande dans votre terminal et vous connecter √† partir de l√† :

```
huggingface-cli login
```

{#if fw === 'pt'}

Nous aurons besoin de g√©n√©rer des r√©sum√©s afin de calculer les scores ROUGE pendant l'entra√Ænement. Heureusement, ü§ó *Transformers* fournit des classes d√©di√©es `Seq2SeqTrainingArguments` et `Seq2SeqTrainer` qui peuvent faire cela pour nous automatiquement ! Pour voir comment cela fonctionne, d√©finissons d'abord les hyperparam√®tres et autres arguments pour nos exp√©riences :

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# La perte d'entra√Ænement √† chaque √©poque
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

Ici, l'argument `predict_with_generate` a √©t√© d√©fini pour indiquer que nous devons g√©n√©rer des r√©sum√©s pendant l'√©valuation afin de pouvoir calculer les scores ROUGE pour chaque √©poque. Comme discut√© au [chapitre 1](/course/fr/chapter1), le d√©codeur effectue l'inf√©rence en pr√©disant les *tokens* un par un, et ceci est impl√©ment√© par la m√©thode `generate()`. D√©finir `predict_with_generate=True` indique au `Seq2SeqTrainer` d'utiliser cette m√©thode pour l'√©valuation. Nous avons √©galement ajust√© certains des hyperparam√®tres par d√©faut, comme le taux d'apprentissage, le nombre d'√©poques, et le taux de d√©croissance des poids, et nous avons r√©gl√© l'option `save_total_limit` pour ne sauvegarder que jusqu'√† trois *checkpoints* pendant l'entra√Ænement. C'est parce que m√™me la plus petite version de mT5 utilise environ 1 Go d'espace disque, et nous pouvons gagner un peu de place en limitant le nombre de copies que nous sauvegardons.

L'argument `push_to_hub=True` nous permettra de pousser le mod√®le vers le *Hub* apr√®s l'entra√Ænement. Vous trouverez le d√©p√¥t sous votre profil utilisateur dans l'emplacement d√©fini par `output_dir`. Notez que vous pouvez sp√©cifier le nom du d√©p√¥t vers lequel vous voulez pousser avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss√© le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` √† `Seq2SeqTrainingArguments`.

La prochaine chose que nous devons faire est de fournir √† `Seq2SeqTrainer` une fonction `compute_metrics()` afin que nous puissions √©valuer notre mod√®le pendant l'entra√Ænement. Pour le r√©sum√©, c'est un peu plus compliqu√© que de simplement appeler `rouge_score.compute()` sur les pr√©dictions du mod√®le, puisque nous devons _d√©coder_ les sorties et les √©tiquettes en texte avant de pouvoir calculer les scores ROUGE. La fonction suivante fait exactement cela, et utilise √©galement la fonction `sent_tokenize()` de `nltk` pour s√©parer les phrases du r√©sum√© avec des nouvelles lignes :


```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # D√©coder les r√©sum√©s g√©n√©r√©s en texte
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Remplacer -100 dans les √©tiquettes car nous ne pouvons pas les d√©coder
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # D√©coder les r√©sum√©s de r√©f√©rence en texte
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE attend une nouvelle ligne apr√®s chaque phrase
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Calcul des scores ROUGE
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extraire les scores m√©dians
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

Ensuite, nous devons d√©finir un collateur de donn√©es pour notre t√¢che de s√©quence √† s√©quence. Comme mT5 est un *transformer* encodeur-d√©codeur, une des subtilit√©s de la pr√©paration de nos batchs est que, pendant le d√©codage, nous devons d√©caler les √©tiquettes d'une unit√© vers la droite. Ceci est n√©cessaire pour garantir que le d√©codeur ne voit que les √©tiquettes de v√©rit√© terrain pr√©c√©dentes et non les √©tiquettes actuelles ou futures, qui seraient faciles √† m√©moriser pour le mod√®le. Cela ressemble √† la fa√ßon dont l'auto-attention masqu√©e est appliqu√©e aux entr√©es dans une t√¢che comme [la mod√©lisation causale du langage](/course/fr/chapter7/6).

Heureusement, ü§ó *Transformers* fournit un collateur `DataCollatorForSeq2Seq` qui rembourrera dynamiquement les entr√©es et les √©tiquettes pour nous. Pour instancier ce collateur, nous devons simplement fournir le *tokenizer* et le *mod√®le* :

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

Voyons ce que produit ce collateur lorsqu'on lui donne un petit batch d'exemples. Tout d'abord, nous devons supprimer les colonnes contenant des cha√Ænes de caract√®res, car le collateur ne saura pas comment remplir ces √©l√©ments :

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

Comme le collateur attend une liste de `dict`, o√π chaque `dict` repr√©sente un seul exemple du jeu de donn√©es, nous devons √©galement mettre les donn√©es dans le format attendu avant de les transmettre au collateur de donn√©es :

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

La principale chose √† remarquer ici est que le premier exemple est plus long que le second, donc les `input_ids` et `attention_mask` du second exemple ont √©t√© compl√©t√©s sur la droite avec un *token* `[PAD]` (dont l'identifiant est `0`). De m√™me, nous pouvons voir que les `labels` ont √©t√© compl√©t√©s par des `-100`, pour s'assurer que les *tokens* de remplissage sont ignor√©s par la fonction de perte. Et enfin, nous pouvons voir un nouveau `decoder_input_ids` qui a d√©plac√© les √©tiquettes vers la droite en ins√©rant un *token* `[PAD]` dans la premi√®re entr√©e.

{#if fw === 'pt'}

Nous avons enfin tous les ingr√©dients dont nous avons besoin pour l'entra√Ænement ! Nous devons maintenant simplement instancier le `Seq2SeqTrainer` avec les arguments :

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

et lancer notre course d'entra√Ænement :

```python
trainer.train()
```

Pendant l'entra√Ænement, vous devriez voir la perte d'entra√Ænement diminuer et les scores ROUGE augmenter √† chaque √©poque. Une fois l'entra√Ænement termin√©, vous pouvez voir les scores ROUGE finaux en ex√©cutant `Trainer.evaluate()` :

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

D'apr√®s les scores, nous pouvons voir que notre mod√®le a largement surpass√© notre *baseline* *lead-3*. Bien ! La derni√®re chose √† faire est de pousser les poids du mod√®le vers le *Hub*, comme suit :

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

Ceci sauvegardera le *checkpoint* et les fichiers de configuration dans `output_dir`, avant de t√©l√©charger tous les fichiers sur le *Hub*. En sp√©cifiant l'argument `tags`, nous nous assurons √©galement que le *widget* sur le *Hub* sera celui d'un pipeline de r√©sum√© au lieu de celui de la g√©n√©ration de texte par d√©faut associ√© √† l'architecture mT5 (pour plus d'informations sur les balises de mod√®le, voir la [documentation du *Hub*](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). La sortie de `trainer.push_to_hub()` est une URL vers le hash du commit Git, donc vous pouvez facilement voir les changements qui ont √©t√© faits au d√©p√¥t de mod√®le !

Pour conclure cette section, voyons comment nous pouvons √©galement *finetuner* mT5 en utilisant les fonctionnalit√©s de bas niveau fournies par ü§ó *Accelerate*.

{:else}

Nous sommes presque pr√™ts √† nous entra√Æner ! Nous devons juste convertir nos jeux de donn√©es en `tf.data.Dataset` en utilisant le collateur de donn√©es que nous avons d√©fini ci-dessus, puis utiliser `compile()` et `fit()`. D'abord, les jeux de donn√©es :

```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

Maintenant, nous d√©finissons nos hyperparam√®tres d'entra√Ænement et nous compilons :

```python
from transformers import create_optimizer
import tensorflow as tf

# Le nombre d'√©tapes d'entra√Ænement est le nombre d'√©chantillons dans le jeu de donn√©es, divis√© par la taille du batch,
# puis multipli√© par le nombre total d'√©poques. Notez que le jeu de donn√©es tf_train_dataset est ici un tf.data.Dataset,
# et non le jeu de donn√©es original donc son len() est d√©j√† num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Entra√Æner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Et enfin, nous *finetunons* le mod√®le. Nous utilisons un `PushToHubCallback` pour sauvegarder le mod√®le sur le *Hub* apr√®s chaque √©poque, ce qui nous permettra de l'utiliser pour l'inf√©rence plus tard :

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

Nous avons obtenu quelques valeurs de perte pendant l'entra√Ænement, mais nous aimerions vraiment voir les m√©triques ROUGE que nous avons calcul√©es plus t√¥t. Pour obtenir ces m√©triques, nous devons g√©n√©rer les sorties du mod√®le et les convertir en cha√Ænes de caract√®res. Construisons quelques listes d'√©tiquettes et de pr√©dictions pour comparer la m√©trique ROUGE (notez que si vous obtenez des erreurs d'importation pour cette section, vous pouvez avoir besoin de "pip install tqdm") :

```python
from tqdm import tqdm
import numpy as np

all_preds = []
all_labels = []
for batch in tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = batch["labels"].numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

Une fois que nous avons nos listes d'√©tiquettes et de cha√Ænes de pr√©diction, le calcul du score ROUGE est facile :

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## <i>Finetuning</i> de mT5 avec ü§ó <i>Accelerate</i>

Le *finetuning* de notre mod√®le avec ü§ó *Accelerate* est tr√®s similaire √† l'exemple de classification de texte que nous avons rencontr√© dans le [chapitre 3](/course/fr/chapter3). Les principales diff√©rences seront la n√©cessit√© de g√©n√©rer explicitement nos r√©sum√©s pendant l'entra√Ænement et de d√©finir comment nous calculons les scores ROUGE (rappelons que le `Seq2SeqTrainer` s'est occup√© de la g√©n√©ration pour nous). Voyons comment nous pouvons mettre en ≈ìuvre ces deux exigences dans ü§ó *Accelerate* !

### Pr√©parer tout pour l'entra√Ænement

La premi√®re chose que nous devons faire est de cr√©er un `DataLoader` pour chacun de nos √©chantillons. Puisque les chargeurs de donn√©es PyTorch attendent des batchs de tenseurs, nous devons d√©finir le format √† `"torch"` dans nos jeux de donn√©es :

```python
tokenized_datasets.set_format("torch")
```

Maintenant que nous avons des jeux de donn√©es constitu√©s uniquement de tenseurs, la prochaine chose √† faire est d'instancier √† nouveau le `DataCollatorForSeq2Seq`. Pour cela, nous devons fournir une nouvelle version du mod√®le, donc chargeons-le √† nouveau depuis notre cache :

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

Nous pouvons ensuite instancier le collateur de donn√©es et l'utiliser pour d√©finir nos chargeurs de donn√©es :

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

La prochaine chose √† faire est de d√©finir l'optimiseur que nous voulons utiliser. Comme dans nos autres exemples, nous allons utiliser `AdamW`, qui fonctionne bien pour la plupart des probl√®mes :

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Enfin, nous introduisons notre mod√®le, notre optimiseur et nos chargeurs de donn√©es dans la m√©thode `accelerator.prepare()` :

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® Si vous vous entra√Ænez sur un TPU, vous devrez d√©placer tout le code ci-dessus dans une fonction d'entra√Ænement d√©di√©e. Voir le [chapitre 3](/course/fr/chapter3) pour plus de d√©tails.

</Tip>

Maintenant que nous avons pr√©par√© nos objets, il reste trois choses √† faire :

* d√©finir le programmeur du taux d'apprentissage,
* impl√©menter une fonction pour post-traiter les r√©sum√©s pour l'√©valuation,
* cr√©er un d√©p√¥t sur le *Hub* vers lequel nous pouvons pousser notre mod√®le.

Pour le programmeur de taux d'apprentissage, nous utiliserons le programmeur lin√©aire standard des sections pr√©c√©dentes :

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Pour le post-traitement, nous avons besoin d'une fonction qui divise les r√©sum√©s g√©n√©r√©s en phrases s√©par√©es par des nouvelles lignes. C'est le format attendu par la m√©trique ROUGE et nous pouvons y parvenir avec le bout de code suivant :

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE attend une nouvelle ligne apr√®s chaque phrase
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

Cela devrait vous sembler familier si vous vous rappelez comment nous avons d√©fini la fonction `compute_metrics()` du `Seq2SeqTrainer`. 

Enfin, nous devons cr√©er un d√©p√¥t de mod√®les sur le *Hub*. Pour cela, nous pouvons utiliser la biblioth√®que ü§ó *Hub*, qui porte le nom appropri√©. Nous avons juste besoin de d√©finir un nom pour notre d√©p√¥t, et la biblioth√®que a une fonction utilitaire pour combiner l'identifiant du d√©p√¥t avec le profil de l'utilisateur :

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

Nous pouvons maintenant utiliser ce nom de d√©p√¥t pour cloner une version locale dans notre r√©pertoire de r√©sultats qui stockera les artefacts d'entra√Ænement :

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Cela nous permettra de pousser les artefacts vers le *Hub* en appelant la m√©thode `repo.push_to_hub()` pendant l'entra√Ænement ! Concluons maintenant notre analyse en √©crivant la boucle d'entra√Ænement.

### Boucle d'entra√Ænement

La boucle d'entra√Ænement pour le r√©sum√© est assez similaire aux autres exemples ü§ó *Accelerate* que nous avons rencontr√©s et est grossi√®rement divis√©e en quatre √©tapes principales :

1. entra√Æner le mod√®le en it√©rant sur tous les exemples dans `train_dataloader` pour chaque √©poque,
2. g√©n√©rer les r√©sum√©s du mod√®le √† la fin de chaque √©poque, en g√©n√©rant d'abord les *tokens* puis en les d√©codant (ainsi que les r√©sum√©s de r√©f√©rence) en texte,
3. calculer les scores ROUGE en utilisant les m√™mes techniques que nous avons vues pr√©c√©demment,
4. sauvegarder les *checkpoints* et pousser le tout vers le *Hub*. Ici, nous nous appuyons sur l'argument `blocking=False` de l'objet `Repository` afin de pouvoir pousser les *checkpoints* par √©poque de mani√®re _asynchrone_. Cela nous permet de poursuivre l'entra√Ænement sans avoir √† attendre le t√©l√©chargement quelque peu lent associ√© √† un mod√®le de la taille d'1 Go !

Ces √©tapes peuvent √™tre vues dans le bloc de code suivant :

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra√Ænement
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # Si nous n'avons pas rempli la longueur maximale, nous devons √©galement remplir les √©tiquettes
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Remplacer -100 dans les √©tiquettes car nous ne pouvons pas les d√©coder
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Calculer les m√©triques
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Sauvegarder et t√©l√©charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

Et c'est tout ! Une fois que vous l'aurez ex√©cut√©, vous aurez un mod√®le et des r√©sultats assez similaires √† ceux que nous avons obtenus avec le `Trainer`.

{/if}

## Utilisation de votre mod√®le <i>finetun√©</i>

Une fois que vous avez pouss√© le mod√®le vers le *Hub*, vous pouvez jouer avec lui soit via le *widget* d'inf√©rence, soit avec un objet `pipeline`, comme suit :

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

Nous pouvons alimenter notre pipeline avec quelques exemples de l'ensemble de test (que le mod√®le n'a pas vu) pour avoir une id√©e de la qualit√© des r√©sum√©s. Tout d'abord, impl√©mentons une fonction simple pour afficher ensemble la critique, le titre et le r√©sum√© g√©n√©r√© :

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

Examinons l'un des exemples anglais que nous recevons :

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn‚Äôt come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It‚Äôs also really expensive for what it is.'
# Ce produit n'a rien de sp√©cial... le livre est trop petit et rigide et il est difficile d'y √©crire. L'√©norme autocollant au dos ne se d√©tache pas et a l'air super collant. Je n'ach√®terai plus jamais ce produit. J'aurais pu simplement acheter un journal dans un magasin √† un dollar et ce serait √† peu pr√®s la m√™me chose. Il est √©galement tr√®s cher pour ce qu'il est.

'>>> Title: Not impressed at all... buy something else' 
# Pas du tout impressionn√©... achetez autre chose.

'>>> Summary: Nothing special at all about this product' 
# Rien de sp√©cial √† propos de ce produit
```

Ce n'est pas si mal ! Nous pouvons voir que notre mod√®le a √©t√© capable d'effectuer un r√©sum√© _abstractif_ en augmentant certaines parties de la critique avec de nouveaux mots. Et peut-√™tre que l'aspect le plus cool de notre mod√®le est qu'il est bilingue, donc nous pouvons √©galement g√©n√©rer des r√©sum√©s de critiques en espagnol :

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada' 
# C'est une trilogie qui se lit tr√®s facilement. J'ai aim√©, je ne m'attendais pas du tout √† la fin.

'>>> Title: Buena literatura para adolescentes' 
# Bonne litt√©rature pour les adolescents

'>>> Summary: Muy facil de leer' 
# Tr√®s facile √† lire
```

Le r√©sum√© a √©t√© extrait directement de la critique. N√©anmoins, cela montre la polyvalence du mod√®le mT5 et vous a donn√© un aper√ßu de ce que c'est que de traiter un corpus multilingue !

Ensuite, nous allons nous int√©resser √† une t√¢che un peu plus complexe : entra√Æner un mod√®le de langue √† partir de z√©ro.
