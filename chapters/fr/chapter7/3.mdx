<FrameworkSwitchCourse {fw} />

# <i>Finetuner</i> un mod√®le de langage masqu√©

{#if fw === 'pt'}
<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_pt.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter7/section3_pt.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_pt.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter7/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_tf.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter7/section3_tf.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section3_tf.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter7/section3_tf.ipynb"},
]} />

{/if}

Pour de nombreuses applications de NLP impliquant des *transformers*, vous pouvez simplement prendre un mod√®le pr√©-entra√Æn√© du *Hub* et le *finetuner* directement sur vos donn√©es pour la t√¢che √† accomplir. Pour autant que le corpus utilis√© pour le pr√©-entra√Ænement ne soit pas trop diff√©rent du corpus utilis√© pour le *finetuning*. L'apprentissage par transfert produira g√©n√©ralement de bons r√©sultats. 

Cependant, il existe quelques cas o√π vous voudrez d'abord *finetuner* les mod√®les de langue sur vos donn√©es, avant d'entra√Æner une t√™te sp√©cifique √† la t√¢che. Par exemple, si votre jeu de donn√©es contient des contrats l√©gaux ou des articles scientifiques, un *transformer* classique comme BERT traitera g√©n√©ralement les mots sp√©cifiques au domaine dans votre corpus comme des *tokens* rares et les performances r√©sultantes peuvent √™tre moins que satisfaisantes. En *finetunant* le mod√®le de langage sur les donn√©es du domaine, vous pouvez am√©liorer les performances de nombreuses t√¢ches en aval, ce qui signifie que vous ne devez g√©n√©ralement effectuer cette √©tape qu'une seule fois !

Ce processus de *finetuning* d'un mod√®le de langage pr√©-entra√Æn√© sur des donn√©es *dans le domaine* est g√©n√©ralement appel√© _adaptation au domaine_. Il a √©t√© popularis√© en 2018 par [ULMFiT](https://arxiv.org/abs/1801.06146) qui a √©t√© l'une des premi√®res architectures neuronales (bas√©es sur des LSTMs) √† faire en sorte que l'apprentissage par transfert fonctionne r√©ellement pour le NLP. Un exemple d'adaptation de domaine avec ULMFiT est pr√©sent√© dans l'image ci-dessous. Dans cette section, nous ferons quelque chose de similaire mais avec un *transformer* au lieu d'une LSTM !

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg" alt="ULMFiT."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg" alt="ULMFiT."/>
</div>

√Ä la fin de cette section, vous aurez un [mod√®le de langage masqu√©](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) sur le *Hub* qui peut autocompl√©ter des phrases comme indiqu√© ci-dessous :

<iframe src="https://course-demos-distilbert-base-uncased-finetune-7400b54.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-distilbert-base-uncased-finetuned-imdb-darkmode.hf.space" frameBorder="0" height="300" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Allons-y !

<Youtube id="mqElG5QJWUg"/>

<Tip>

üôã Si les termes ¬´ mod√©lisation du langage masqu√© ¬ª et ¬´ mod√®le pr√©-entra√Æn√© ¬ª ne vous sont pas familiers, consultez le [chapitre 1](/course/fr/chapiter1), o√π nous expliquons tous ces concepts fondamentaux, vid√©os √† l'appui !

</Tip>

## Choix d'un mod√®le pr√©-entra√Æn√© pour la mod√©lisation du langage masqu√©

Pour commencer, nous allons choisir un mod√®le pr√©-entra√Æn√© appropri√© pour la mod√©lisation du langage masqu√©. Comme le montre la capture d'√©cran suivante, vous pouvez trouver une liste de candidats en appliquant le filtre ¬´ *Fill-Mask* ¬ª sur le [*Hub*](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads) :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png" alt="Hub models." width="80%"/>
</div>

Bien que les mod√®les de la famille BERT et RoBERTa soient les plus t√©l√©charg√©s, nous utiliserons un mod√®le appel√© [DistilBERT](https://huggingface.co/distilbert-base-uncased) qui peut √™tre entra√Æn√© beaucoup plus rapidement avec peu ou pas de perte de performance en aval. Ce mod√®le a √©t√© entra√Æn√© √† l'aide d'une technique sp√©ciale appel√©e [_distillation de connaissances_](https://en.wikipedia.org/wiki/Knowledge_distillation), o√π un grand mod√®le *enseignant* comme BERT est utilis√© pour guider l'entra√Ænement d'un mod√®le *√©tudiant* qui a beaucoup moins de param√®tres. Une explication des d√©tails de la distillation de connaissances nous m√®nerait trop loin dans cette section mais si vous √™tes int√©ress√©, vous pouvez lire tout cela dans le livre [_Natural Language Processing with Transformers_](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).

{#if fw === 'pt'}

Allons-y et t√©l√©chargeons DistilBERT en utilisant la classe `AutoModelForMaskedLM` :

```python
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Nous pouvons voir combien de param√®tres ce mod√®le poss√®de en appelant la m√©thode `num_parameters()` :

```python
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT nombre de param√®tres : {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT nombre de param√®tres : 110M'")
```

```python out
'>>> DistilBERT  nombre de param√®tres : 67M'
'>>> BERT nombre de param√®tres : 110M'
```

{:else}

Allons-y et t√©l√©chargeons DistilBERT en utilisant la classe `AutoModelForMaskedLM` :

```python
from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Nous pouvons voir combien de param√®tres ce mod√®le poss√®de en appelant la m√©thode `summary()` :

```python
model(model.dummy_inputs)  # Construire le mod√®le
model.summary()
```

```python out
Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________
```

{/if}

Avec environ 67 millions de param√®tres, DistilBERT est environ deux fois plus petit que le mod√®le de base de BERT, ce qui se traduit approximativement par une acc√©l√©ration de l'entra√Ænement d'un facteur deux. Voyons maintenant quels types de *tokens* ce mod√®le pr√©dit comme √©tant les compl√©ments les plus probables d'un petit √©chantillon de texte :

```python
text = "This is a great [MASK]."
```

En tant qu'√™tres humains, nous pouvons imaginer de nombreuses possibilit√©s pour le *token* `[MASK]`, telles que ¬´ jour ¬ª, ¬´ promenade ¬ª ou ¬´ peinture ¬ª. Pour les mod√®les pr√©-entra√Æn√©s, les pr√©dictions d√©pendent du corpus sur lequel le mod√®le a √©t√© entra√Æn√© puisqu'il apprend √† d√©tecter les mod√®les statistiques pr√©sents dans les donn√©es. Comme BERT, DistilBERT a √©t√© pr√©-entra√Æn√© sur les jeux de donn√©es [*English Wikipedia*](https://huggingface.co/datasets/wikipedia) et [*BookCorpus*](https://huggingface.co/datasets/bookcorpus), nous nous attendons donc √† ce que les pr√©dictions pour `[MASK]` refl√®tent ces domaines. Pour pr√©dire le masque, nous avons besoin du *tokenizer* de DistilBERT pour produire les entr√©es du mod√®le, alors t√©l√©chargeons-le √©galement depuis le *Hub* :

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Avec un *tokenizer* et un mod√®le, nous pouvons maintenant passer notre exemple de texte au mod√®le, extraire les logits, et afficher les 5 meilleurs candidats :

{#if fw === 'pt'}

```python
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# Trouve l'emplacement de [MASK] et extrait ses logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Choisissez les candidats [MASK] avec les logits les plus √©lev√©s
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")
```

{:else}

```python
import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# Trouve l'emplacement de [MASK] et extrait ses logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# On choisit les candidats [MASK] avec les logits les plus √©lev√©s
# Nous annulons le tableau avant argsort pour obtenir le plus grand, et non le plus petit, logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")
```

{/if}

```python out
'>>> This is a great deal.' # C'est une bonne affaire
'>>> This is a great success.' # C'est un grand succ√®s
'>>> This is a great adventure.' # C'est une grande aventure
'>>> This is a great idea.' # C'est une bonne id√©e
'>>> This is a great feat.' # C'est un grand exploit
```

Nous pouvons voir dans les sorties que les pr√©dictions du mod√®le se r√©f√®rent √† des termes de tous les jours, ce qui n'est peut-√™tre pas surprenant √©tant donn√© le fondement de Wikip√©dia. Voyons comment nous pouvons changer ce domaine pour quelque chose d'un peu plus sp√©cialis√© : des critiques de films !


## Le jeu de donn√©es

Pour illustrer l'adaptation au domaine, nous utiliserons le c√©l√®bre [*Large Movie Review Dataset*](https://huggingface.co/datasets/imdb) (ou IMDb en abr√©g√©), qui est un corpus de critiques de films souvent utilis√© pour √©valuer les mod√®les d'analyse de sentiments. En *finetunant* DistilBERT sur ce corpus, nous esp√©rons que le mod√®le de langage adaptera son vocabulaire des donn√©es factuelles de Wikip√©dia sur lesquelles il a √©t√© pr√©-entra√Æn√© aux √©l√©ments plus subjectifs des critiques de films. Nous pouvons obtenir les donn√©es du *Hub* avec la fonction `load_dataset()` de ü§ó *Datasets* :

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```

Nous pouvons voir que les parties `train` et `test` sont chacune compos√©es de 25 000 critiques, alors qu'il y a une partie non √©tiquet√©e appel√©e `unsupervised` qui contient 50 000 critiques. Jetons un coup d'≈ìil √† quelques √©chantillons pour avoir une id√©e du type de texte auquel nous avons affaire. Comme nous l'avons fait dans les chapitres pr√©c√©dents du cours, nous allons encha√Æner les fonctions `Dataset.shuffle()` et `Dataset.select()` pour cr√©er un √©chantillon al√©atoire :

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")
```

```python out

'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clich√©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'
```

Oui, ce sont bien des critiques de films, et si vous √™tes assez √¢g√©s, vous pouvez m√™me comprendre le commentaire dans la derni√®re critique sur le fait de poss√©der une version VHS üòú ! Bien que nous n'ayons pas besoin des √©tiquettes pour la mod√©lisation du langage, nous pouvons d√©j√† voir qu'un `0` d√©note une critique n√©gative, tandis qu'un `1` correspond √† une critique positive.

<Tip>

‚úèÔ∏è **Essayez !** Cr√©ez un √©chantillon al√©atoire de la r√©partition `unsupervised` et v√©rifiez que les √©tiquettes ne sont ni `0` ni `1`. Pendant que vous y √™tes, vous pouvez aussi v√©rifier que les √©tiquettes dans les √©chantillons `train` et `test` sont bien `0` ou `1`. C'est un contr√¥le utile que tout praticien en NLP devrait effectuer au d√©but d'un nouveau projet !

</Tip>

Maintenant que nous avons jet√© un coup d'≈ìil rapide aux donn√©es, plongeons dans leur pr√©paration pour la mod√©lisation du langage masqu√©. Comme nous allons le voir, il y a quelques √©tapes suppl√©mentaires √† suivre par rapport aux t√¢ches de classification de s√©quences que nous avons vues au [chapitre 3](/course/fr/chapter3). Allons-y !

## Pr√©traitement des donn√©es

<Youtube id="8PmhEIXhBvI"/>

Pour la mod√©lisation autor√©gressive et la mod√©lisation du langage masqu√©, une √©tape commune de pr√©traitement consiste √† concat√©ner tous les exemples, puis √† diviser le corpus entier en morceaux de taille √©gale. C'est tr√®s diff√©rent de notre approche habituelle, o√π nous nous contentons de *tokenizer* les exemples individuels. Pourquoi tout concat√©ner ? La raison est que les exemples individuels peuvent √™tre tronqu√©s s'ils sont trop longs, ce qui entra√Ænerait la perte d'informations qui pourraient √™tre utiles pour la t√¢che de mod√©lisation du langage !

Donc pour commencer, nous allons d'abord tokeniser notre corpus comme d'habitude, mais _sans_ mettre l'option `truncation=True` dans notre *tokenizer*. Nous allons aussi r√©cup√©rer les identifiants des mots s'ils sont disponibles (ce qui sera le cas si nous utilisons un *tokenizer* rapide, comme d√©crit dans le [chapitre 6](/course/fr/chapter6/3)), car nous en aurons besoin plus tard pour faire le masquage de mots entiers. Nous allons envelopper cela dans une simple fonction, et pendant que nous y sommes, nous allons supprimer les colonnes `text` et `label` puisque nous n'en avons plus besoin :

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# Utilisation de batched=True pour activer le multithreading rapide !
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})
```

Comme DistilBERT est un mod√®le de type BERT, nous pouvons voir que les textes encod√©s sont constitu√©s des `input_ids` et des `attention_mask` que nous avons vus dans d'autres chapitres, ainsi que des `word_ids` que nous avons ajout√©s. 

Maintenant que nos critiques de films ont √©t√© tokenis√©es, l'√©tape suivante consiste √† les regrouper et √† diviser le r√©sultat en chunks. Mais quelle taille doivent avoir ces *chunks* ? Cela sera finalement d√©termin√© par la quantit√© de m√©moire GPU dont vous disposez, mais un bon point de d√©part est de voir quelle est la taille maximale du contexte du mod√®le. Cela peut √™tre d√©duit en inspectant l'attribut `model_max_length` du *tokenizer* :

```python
tokenizer.model_max_length
```


```python out
512
```

Cette valeur est d√©riv√©e du fichier *tokenizer_config.json* associ√© √† un *checkpoint*. Dans ce cas, nous pouvons voir que la taille du contexte est de 512 *tokens*, tout comme avec BERT.

<Tip>

‚úèÔ∏è **Essayez !** Certains *transformers*, comme [BigBird](https://huggingface.co/google/bigbird-roberta-base) et [Longformer](hf.co/allenai/longformer-base-4096), ont une longueur de contexte beaucoup plus longue que BERT et les autres premiers *transformers*. Instanciez le *tokenizer* pour l'un de ces *checkpoints* et v√©rifiez que le `model_max_length` correspond √† ce qui est indiqu√© sur sa carte.

</Tip>

Ainsi, pour r√©aliser nos exp√©riences sur des GPUs comme ceux disponibles sur Google Colab, nous choisirons quelque chose d'un peu plus petit qui peut tenir en m√©moire :

```python
chunk_size = 128
```

<Tip warning={true}>

Notez que l'utilisation d'une petite taille peut √™tre pr√©judiciable dans les sc√©narios du monde r√©el. Vous devez donc utiliser une taille qui correspond au cas d'utilisation auquel vous appliquerez votre mod√®le.

</Tip>

Maintenant vient la partie amusante. Pour montrer comment la concat√©nation fonctionne, prenons quelques commentaires de notre ensemble d'entra√Ænement et affichons le nombre de *tokens* par commentaire :

```python
# Le d√©coupage produit une liste de listes pour chaque caract√©ristique
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python out
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

Nous pouvons ensuite concat√©ner tous ces exemples avec une simple compr√©hension du dictionnaire, comme suit :

```python
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Longueur des critiques concat√©n√©es : {total_length}'")
```

```python out
'>>> Longueur des critiques concat√©n√©es : 951'
```

Super, la longueur totale est correcte. Donc maintenant, nous allons diviser les exemples concat√©n√©s en morceaux de la taille donn√©e par `block_size`. Pour ce faire, nous it√©rons sur les caract√©ristiques de `concatenated_examples` et utilisons une compr√©hension de liste pour cr√©er des *chunks* de chaque caract√©ristique. Le r√©sultat est un dictionnaire de *chunks* pour chaque caract√©ristique :

```python
chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")
```

```python out
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

Comme vous pouvez le voir dans cet exemple, le dernier *chunk* sera g√©n√©ralement plus petit que la taille maximale des morceaux. Il y a deux strat√©gies principales pour g√©rer cela :

* Abandonner le dernier morceau s'il est plus petit que `chunk_size`.
* Rembourrer le dernier morceau jusqu'√† ce que sa longueur soit √©gale √† `chunk_size`.

Nous adopterons la premi√®re approche ici, donc nous allons envelopper toute la logique ci-dessus dans une seule fonction que nous pouvons appliquer √† nos jeux de donn√©es tokenis√©s :

```python
def group_texts(examples):
    # Concat√©nation de tous les textes
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # Calcule la longueur des textes concat√©n√©s
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # Nous laissons tomber le dernier morceau s'il est plus petit que chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Fractionnement par chunk de max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # Cr√©er une nouvelle colonne d'√©tiquettes
    result["labels"] = result["input_ids"].copy()
    return result
```

Notez que dans la derni√®re √©tape de `group_texts()` nous cr√©ons une nouvelle colonne `labels` qui est une copie de la colonne `input_ids`. Comme nous le verrons bient√¥t, c'est parce que dans la mod√©lisation du langage masqu√©, l'objectif est de pr√©dire des *tokens* masqu√©s al√©atoirement dans le batch d'entr√©e, et en cr√©ant une colonne `labels`, nous fournissons la v√©rit√© de base pour notre mod√®le de langage √† apprendre. 

Appliquons maintenant `group_texts()` √† nos jeux de donn√©es tokenis√©s en utilisant notre fid√®le fonction `Dataset.map()` :

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})
```

Vous pouvez voir que le regroupement puis le d√©coupage des textes a produit beaucoup plus d'exemples que nos 25 000 exemples initiaux pour les divisions `train` et `test`. C'est parce que nous avons maintenant des exemples impliquant des *tokens* contigus qui s'√©tendent sur plusieurs exemples du corpus original. Vous pouvez le voir explicitement en cherchant les *tokens* sp√©ciaux `[SEP]` et `[CLS]` dans l'un des *chunks* :

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

Dans cet exemple, vous pouvez voir deux critiques de films qui se chevauchent, l'une sur un film de lyc√©e et l'autre sur les sans-abri. Voyons √©galement √† quoi ressemblent les √©tiquettes pour la mod√©lisation du langage masqu√© :

```python out
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

Comme pr√©vu par notre fonction `group_texts()` ci-dessus, cela semble identique aux `input_ids` d√©cod√©s. Mais alors comment notre mod√®le peut-il apprendre quoi que ce soit ? Il nous manque une √©tape cl√© : ins√©rer des *tokens* √† des positions al√©atoires dans les entr√©es ! Voyons comment nous pouvons le faire √† la vol√©e pendant le *finetuning* en utilisant un assembleur de donn√©es sp√©cial.

## <i>Finetuning</i> de DistilBERT avec l'API `Trainer`

Le *finetuning* d'un mod√®le de langage masqu√© est presque identique au *finetuning* d'un mod√®le de classification de s√©quences, comme nous l'avons fait dans le [chapitre 3](/course/fr/chapter3). La seule diff√©rence est que nous avons besoin d'un collecteur de donn√©es sp√©cial qui peut masquer de mani√®re al√©atoire certains des *tokens* dans chaque batch de textes. Heureusement, ü§ó *Transformers* est livr√© pr√©par√© avec un `DataCollatorForLanguageModeling` d√©di√© √† cette t√¢che. Nous devons juste lui passer le *tokenizer* et un argument `mlm_probability` qui sp√©cifie quelle fraction des *tokens* √† masquer. Nous choisirons 15%, qui est la quantit√© utilis√©e pour BERT et un choix commun dans la litt√©rature :

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

Pour voir comment le masquage al√©atoire fonctionne, nous allons donner quelques exemples √† l'assembleur de donn√©es. Puisqu'il s'attend √† une liste de `dict` o√π chaque `dict` repr√©sente un seul morceau de texte contigu, nous it√©rons d'abord sur le jeu de donn√©es avant de donner le batch √† l'assembleur. Nous supprimons la cl√© `"word_ids"` pour cet assembleur de donn√©es car il ne l'attend pas :

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george ÂÆáin stated )ÂÖ¨ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

Super, √ßa a march√© ! Nous pouvons voir que le *token* `[MASK]` a √©t√© ins√©r√© de fa√ßon al√©atoire √† diff√©rents endroits dans notre texte. Ce seront les *tokens* que notre mod√®le devra pr√©dire pendant l'entra√Ænement. Et la beaut√© du collecteur de donn√©es est qu'il va rendre al√©atoire l'insertion du `[MASK]` √† chaque batch ! 

<Tip>

‚úèÔ∏è **Essayez** Ex√©cutez le code ci-dessus plusieurs fois pour voir le masquage al√©atoire se produire sous vos yeux ! Remplacez aussi la m√©thode `tokenizer.decode()` par `tokenizer.convert_ids_to_tokens()` pour voir que parfois un seul *token* d'un mot donn√© est masqu√© et pas les autres.

</Tip>

{#if fw === 'pt'}

Un effet secondaire du masquage al√©atoire est que nos m√©triques d'√©valuation ne seront pas d√©terministes lorsque nous utilisons la fonction `Trainer` puisque nous utilisons le m√™me assembleur de donn√©es pour les √©chantillons d'entra√Ænement et de test. Nous verrons plus tard, lorsque nous examinerons le *finetuning* avec ü§ó *Accelerate*, comment nous pouvons utiliser la flexibilit√© d'une boucle d'√©valuation personnalis√©e pour geler le caract√®re al√©atoire.

{/if}

Lors de l'entra√Ænement des mod√®les pour la mod√©lisation du langage masqu√©, une technique qui peut √™tre utilis√©e est de masquer des mots entiers ensemble et pas seulement des *tokens* individuels. Cette approche est appel√©e _masquage de mots entiers_. Si nous voulons utiliser le masquage de mots entiers, nous devons construire nous-m√™mes un assembleur de donn√©es. Un assembleur de donn√©es est simplement une fonction qui prend une liste d'√©chantillons et les convertit en un batch. Faisons-le ! Nous utiliserons les identifiants des mots calcul√©s plus t√¥t pour faire une correspondance entre les indices des mots et les *tokens*, puis nous d√©ciderons al√©atoirement quels mots masquer et appliquerons ce masque sur les entr√©es. Notez que les √©tiquettes sont toutes `-100` sauf celles qui correspondent aux mots masqu√©s.

{#if fw === 'pt'}

```py
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Cr√©ation d'une correspondance entre les mots et les indices des tokens correspondants
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Masquer des mots de fa√ßon al√©atoire
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return default_data_collator(features)
```

{:else}

```py
import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Cr√©ation d'une correspondance entre les mots et les indices des tokens correspondants
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Masquer des mots de fa√ßon al√©atoire
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return tf_default_data_collator(features)
```

{/if}

Ensuite, nous pouvons l'essayer sur les m√™mes √©chantillons que pr√©c√©demment :

```py
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python out
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

<Tip>

‚úèÔ∏è **Essayez** Ex√©cutez le code ci-dessus plusieurs fois pour voir le masquage al√©atoire se produire sous vos yeux ! Remplacez aussi la m√©thode `tokenizer.decode()` par `tokenizer.convert_ids_to_tokens()` pour voir que les *tokens* d'un mot donn√© sont toujours masqu√©s ensemble.

</Tip>

Maintenant que nous avons deux assembleurs de donn√©es, les √©tapes restantes du *finetuning* sont standards. L'entra√Ænement peut prendre un certain temps sur Google Colab si vous n'avez pas la chance de tomber sur un mythique GPU P100 üò≠. Ainsi nous allons d'abord r√©duire la taille du jeu d'entra√Ænement √† quelques milliers d'exemples. Ne vous inqui√©tez pas, nous obtiendrons quand m√™me un mod√®le de langage assez d√©cent ! Un moyen rapide de r√©duire la taille d'un jeu de donn√©es dans ü§ó *Datasets* est la fonction `Dataset.train_test_split()` que nous avons vue au [chapitre 5](/course/fr/chapter5) :

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})
```

Cela a automatiquement cr√©√© de nouvelles divisions `train` et `test` avec la taille du jeu d'entra√Ænement fix√©e √† 10.000 exemples et la validation fix√©e √† 10% de cela. N'h√©sitez pas √† augmenter la taille si vous avez un GPU puissant ! La prochaine chose que nous devons faire est de nous connecter au *Hub*. Si vous ex√©cutez ce code dans un *notebook*, vous pouvez le faire avec la fonction suivante :

```python
from huggingface_hub import notebook_login

notebook_login()
```

qui affichera un *widget* o√π vous pourrez saisir vos informations d'identification. Alternativement, vous pouvez ex√©cuter : 

```
huggingface-cli login
```

dans votre terminal pr√©f√©r√© et connectez-vous l√†. 

{#if fw === 'tf'}

Une fois que nous sommes connect√©s, nous pouvons cr√©er nos jeux de donn√©es `tf.data`. Pour ce faire, nous utiliserons la m√©thode `prepare_tf_dataset()`, qui utilise notre mod√®le pour d√©duire automatiquement quelles colonnes doivent aller dans le jeu de donn√©es. Si vous voulez contr√¥ler exactement les colonnes √† utiliser, vous pouvez utiliser la m√©thode `Dataset.to_tf_dataset()` √† la place. Pour garder les choses simples, nous n'utiliserons ici que le l‚Äôassembleur de donn√©es standard, mais vous pouvez aussi essayer l‚Äôassembleur masquant des mots entiers et comparer les r√©sultats √† titre d'exercice :

```python
tf_train_dataset = model.prepare_tf_dataset(
    downsampled_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = model.prepare_tf_dataset(
    downsampled_dataset["test"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

Ensuite, nous configurons nos hyperparam√®tres d'entra√Ænement et compilons notre mod√®le. Nous utilisons la fonction `create_optimizer()` de la biblioth√®que ü§ó *Transformers*, qui nous donne un optimiseur `AdamW` avec une d√©croissance lin√©aire du taux d'apprentissage. Nous utilisons √©galement la perte int√©gr√©e au mod√®le, qui est la perte par d√©faut lorsqu'aucune perte n'est sp√©cifi√©e comme argument de `compile()`, et nous d√©finissons la pr√©cision d'entra√Ænement √† `"mixed_float16"`. Notez que si vous utilisez un GPU Colab ou un autre GPU qui n'a pas le support acc√©l√©r√© en float16, vous devriez probablement commenter cette ligne.

De plus, nous mettons en place un `PushToHubCallback` qui sauvegardera le mod√®le sur le *Hub* apr√®s chaque √©poque. Vous pouvez sp√©cifier le nom du d√©p√¥t vers lequel vous voulez pousser avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, pour pousser le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/distilbert-finetuned-imdb"`. Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace de noms et nomm√© apr√®s le r√©pertoire de sortie que vous avez d√©fini, donc dans notre cas, ce sera `"lewtun/distilbert-finetuned-imdb"`.

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Entra√Æner en mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

model_name = model_checkpoint.split("/")[-1]
callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)
```

Nous sommes maintenant pr√™ts √† ex√©cuter `model.fit()`. Mais avant, regardons bri√®vement la _perplexit√©_ qui est une m√©trique commune pour √©valuer la performance des mod√®les de langage.

{:else}

Une fois que nous sommes connect√©s, nous pouvons sp√©cifier les arguments pour le `Trainer` :

```python
from transformers import TrainingArguments

batch_size = 64
# Montrer la perte d'entra√Ænement √† chaque √©poque
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)
```

Ici, nous avons modifi√© quelques options par d√©faut, y compris `logging_steps` pour s'assurer que nous suivons la perte d'entra√Ænement √† chaque √©poque. Nous avons √©galement utilis√© `fp16=True` pour activer l'entra√Ænement en pr√©cision mixte, ce qui nous donne un autre gain de vitesse. Par d√©faut, `Trainer` va supprimer toutes les colonnes qui ne font pas partie de la m√©thode `forward()` du mod√®le. Cela signifie que si vous utilisez l'assembleur de masquage de mots entiers, vous devrez √©galement d√©finir `remove_unused_columns=False` pour vous assurer que nous ne perdons pas la colonne `word_ids` pendant l'entra√Ænement.

Notez que vous pouvez sp√©cifier le nom du d√©p√¥t vers lequel vous voulez pousser avec l'argument `hub_model_id` (en particulier, vous devrez utiliser cet argument pour pousser vers une organisation). Par exemple, lorsque nous avons pouss√© le mod√®le vers l'organisation [`huggingface-course`](https://huggingface.co/huggingface-course), nous avons ajout√© `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` `TrainingArguments`. Par d√©faut, le d√©p√¥t utilis√© sera dans votre espace de noms et nomm√© apr√®s le r√©pertoire de sortie que vous avez d√©fini, donc dans notre cas ce sera `"lewtun/distilbert-finetuned-imdb"`.

Nous avons maintenant tous les ingr√©dients pour instancier le `Trainer`. Ici, nous utilisons juste l'assembleur standard `data_collator`, mais vous pouvez essayer l'assembleur de masquage de mots entiers et comparer les r√©sultats comme exercice : 

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Nous sommes maintenant pr√™ts √† ex√©cuter `trainer.train()`. Mais avant, regardons bri√®vement la _perplexit√©_ qui est une m√©trique commune pour √©valuer la performance des mod√®les de langage.

{/if}

### Perplexit√© pour les mod√®les de langage

<Youtube id="NURcDHhYe98"/>

Contrairement √† d'autres t√¢ches, comme la classification de textes ou la r√©ponse √† des questions, sur lesquelles nous disposons d'un corpus √©tiquet√© pour entra√Æner, la mod√©lisation du langage ne s'appuie sur aucune √©tiquette explicite. Alors comment d√©terminer ce qui fait un bon mod√®le de langage ? Comme pour la fonction de correction automatique de votre t√©l√©phone, un bon mod√®le de langage est celui qui attribue des probabilit√©s √©lev√©es aux phrases grammaticalement correctes et des probabilit√©s faibles aux phrases absurdes. Pour vous donner une meilleure id√©e de ce √† quoi cela ressemble, vous pouvez trouver en ligne des s√©ries enti√®res de ¬´ rat√©s d'autocorrection ¬ª o√π le mod√®le d'un t√©l√©phone produit des compl√©ments plut√¥t amusants (et souvent inappropri√©s) ! 

{#if fw === 'pt'}

En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa√ßon de mesurer la qualit√© de notre mod√®le de langage est de calculer les probabilit√©s qu'il attribue au mot suivant dans toutes les phrases de l'ensemble de test. Des probabilit√©s √©lev√©es indiquent que le mod√®le n'est pas ¬´ surpris ¬ª ou ¬´ perplexe ¬ª vis-√†-vis des exemples non vus, et sugg√®rent qu'il a appris les mod√®les de base de la grammaire de la langue. Il existe plusieurs d√©finitions math√©matiques de la perplexit√©. Celle que nous utiliserons la d√©finit comme l'exponentielle de la perte d'entropie crois√©e. Ainsi, nous pouvons calculer la perplexit√© de notre mod√®le pr√©-entra√Æn√© en utilisant la fonction `Trainer.evaluate()` pour calculer la perte d'entropie crois√©e sur l'ensemble de test, puis en prenant l'exponentielle du r√©sultat :

```python
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

En supposant que notre ensemble de test se compose principalement de phrases grammaticalement correctes, une fa√ßon de mesurer la qualit√© de notre mod√®le de langage est de calculer les probabilit√©s qu'il attribue au mot suivant dans toutes les phrases de l'ensemble de test. Des probabilit√©s √©lev√©es indiquent que le mod√®le n'est pas ¬´ surpris ¬ª ou ¬´ perplexe ¬ª vis-√†-vis des exemples non vus, et sugg√®rent qu'il a appris les mod√®les de base de la grammaire de la langue. Il existe plusieurs d√©finitions math√©matiques de la perplexit√©. Celle que nous utiliserons la d√©finit comme l'exponentielle de la perte d'entropie crois√©e. Ainsi, nous pouvons calculer la perplexit√© de notre mod√®le pr√©-entra√Æn√© en utilisant la fonction `model.evaluate()` pour calculer la perte d'entropie crois√©e sur l'ensemble de test, puis en prenant l'exponentielle du r√©sultat :

```python
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexit√© : {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexit√© : 21.75
```

Un score de perplexit√© faible signifie un meilleur mod√®le de langue. Nous pouvons voir ici que notre mod√®le de d√©part a une valeur assez √©lev√©e. Voyons si nous pouvons la r√©duire en l'affinant ! Pour ce faire, nous commen√ßons par ex√©cuter la boucle d'entra√Ænement :

{#if fw === 'pt'}

```python
trainer.train()
```

{:else}

```python
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

et ensuite calculer la perplexit√© r√©sultante sur l'ensemble de test comme pr√©c√©demment :

{#if fw === 'pt'}

```python
eval_results = trainer.evaluate()
print(f">>> Perplexit√© : {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

```python
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexit√© : {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexit√© : 11.32
```

Joli. C'est une r√©duction consid√©rable de la perplexit√©, ce qui nous indique que le mod√®le a appris quelque chose sur le domaine des critiques de films !

{#if fw === 'pt'}

Une fois l'entra√Ænement termin√©, nous pouvons pousser la carte de mod√®le avec les informations d'entra√Ænement vers le *Hub* (les *checkpoints* sont sauvegard√©s pendant l'entra√Ænement lui-m√™me) :

```python
trainer.push_to_hub()
```

{/if}

<Tip>

‚úèÔ∏è **A votre tour !** Ex√©cutez l'entra√Ænement ci-dessus apr√®s avoir remplac√© le collecteur de donn√©es par le collecteur de mots entiers masqu√©s. Obtenez-vous de meilleurs r√©sultats ?

</Tip>

{#if fw === 'pt'} 

Dans notre cas d'utilisation, nous n'avons pas eu besoin de faire quelque chose de sp√©cial avec la boucle d'entra√Ænement, mais dans certains cas, vous pourriez avoir besoin de mettre en ≈ìuvre une logique personnalis√©e. Pour ces applications, vous pouvez utiliser ü§ó *Accelerate*. Jetons un coup d'≈ìil !

## <i>Finetuning</i> de DistilBERT avec ü§ó <i>Accelerate</i>

Comme nous l'avons vu, avec `Trainer` le *finetuning* d'un mod√®le de langage masqu√© est tr√®s similaire √† l'exemple de classification de texte du [chapitre 3](/course/fr/chapter3). En fait, la seule subtilit√© est l'utilisation d'un assembleur de donn√©es sp√©cial, et nous l'avons d√©j√† couvert plus t√¥t dans cette section ! 

Cependant, nous avons vu que `DataCollatorForLanguageModeling` applique aussi un masquage al√©atoire √† chaque √©valuation. Nous verrons donc quelques fluctuations dans nos scores de perplexit√© √† chaque entrainement. Une fa√ßon d'√©liminer cette source d'al√©at est d'appliquer le masquage _une fois_ sur l'ensemble de test, puis d'utiliser l'assembleur de donn√©es par d√©faut dans ü§ó *Transformers* pour collecter les batchs pendant l'√©valuation. Pour voir comment cela fonctionne, impl√©mentons une fonction simple qui applique le masquage sur un batch, similaire √† notre premi√®re rencontre avec `DataCollatorForLanguageModeling` :

```python
def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # Cr√©er une nouvelle colonne "masqu√©e" pour chaque colonne du jeu de donn√©es
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}
```

Ensuite, nous allons appliquer cette fonction √† notre jeu de test et laisser tomber les colonnes non masqu√©es afin de les remplacer par les colonnes masqu√©es. Vous pouvez utiliser le masquage de mots entiers en rempla√ßant le `data_collator` ci-dessus par celui qui est appropri√©. Dans ce cas vous devez supprimer la premi√®re ligne ici :

```py
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)
```

Nous pouvons ensuite configurer les *dataloaders* comme d'habitude, mais nous utiliserons le `default_data_collator` de ü§ó *Transformers* pour le jeu d'√©valuation :

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)
```

Nous suivons les √©tapes standard avec ü§ó *Accelerate*. La premi√®re est de charger une version fra√Æche du mod√®le pr√©-entra√Æn√© :

```
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Ensuite, nous devons sp√©cifier l'optimiseur. Nous utiliserons le standard `AdamW` :

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Avec ces objets, nous pouvons maintenant tout pr√©parer pour l'entra√Ænement avec l'objet `Accelerator` :

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

Maintenant que notre mod√®le, notre optimiseur et nos chargeurs de donn√©es sont configur√©s, nous pouvons sp√©cifier le planificateur du taux d'apprentissage comme suit :

```python
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Il ne reste qu'une derni√®re chose √† faire avant de s'entra√Æner : cr√©er un d√©p√¥t de mod√®les sur le *Hub* d'Hugging Face ! Nous pouvons utiliser la biblioth√®que ü§ó *Hub* pour g√©n√©rer d'abord le nom complet de notre d√©p√¥t :

```python
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'
```

puis cr√©er et cloner le d√©p√¥t en utilisant la classe `Repository` du ü§ó *Hub* :

```python
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)
```

Une fois cela fait, il ne reste plus qu'√† r√©diger la boucle compl√®te d'entra√Ænement et d'√©valuation :

```python
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Entra√Ænement
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # Sauvegarder et t√©l√©charger
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409
```

Cool, nous avons √©t√© en mesure d'√©valuer la perplexit√© √† chaque √©poque et de garantir la reproductibilit√© des entra√Ænements multiples !

{/if}

### Utilisation de notre mod√®le <i>finetun√©</i>

Vous pouvez interagir avec votre mod√®le *finetun√©* soit en utilisant son *widget* sur le *Hub*, soit localement avec le `pipeline` de ü§ó *Transformers*. Utilisons ce dernier pour t√©l√©charger notre mod√®le en utilisant le pipeline `fill-mask` :

```python
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

Nous pouvons ensuite donner au pipeline notre exemple de texte ¬´ this is a great [MASK] ¬ª et voir quelles sont les 5 premi√®res pr√©dictions :

```python
preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")
```

```python out
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

Notre mod√®le a clairement adapt√© ses pond√©rations pour pr√©dire les mots qui sont plus fortement associ√©s aux films !

<Youtube id="0Oxphw4Q9fo"/>

Ceci conclut notre premi√®re exp√©rience d'entra√Ænement d'un mod√®le de langage. Dans la [section 6](/course/fr/chapter7/section6), vous apprendrez comment entra√Æner √† partir de z√©ro un mod√®le autor√©gressif comme GPT-2. Allez-y si vous voulez voir comment vous pouvez pr√©-entra√Æner votre propre *transformer* !

<Tip>

‚úèÔ∏è **Essayez !** Pour quantifier les avantages de l'adaptation au domaine, <i>finetunez</i> un classifieur sur le jeu de donn√©es IMDb pour √† la fois, le <i>checkpoint</i> de DistilBERT pr√©-entra√Æn√© et e <i>checkpoint</i> de DistilBERT <i>finetun√©</i>. Si vous avez besoin d'un rafra√Æchissement sur la classification de texte, consultez le [chapitre 3](/course/fr/chapter3). 

</Tip>
