# Introduction

## Bienvenue au cours ü§ó !

<Youtube id="00GKzGyWFEs" />

Ce cours vous apprendra √† utiliser les biblioth√®ques de NLP de l'√©cosyst√®me [Hugging Face](https://huggingface.co/) : [ü§ó *Transformers*](https://github.com/huggingface/transformers), [ü§ó *Datasets*](https://github.com/huggingface/datasets), [ü§ó *Tokenizers*](https://github.com/huggingface/tokenizers) et [ü§ó *Accelerate*](https://github.com/huggingface/accelerate), ainsi que le [*Hub*](https://huggingface.co/models). C'est totalement gratuit et sans publicit√©.

## √Ä quoi s'attendre ?

Voici un bref aper√ßu du cours :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

- Les chapitres 1 √† 4 pr√©sentent les principaux concepts de la biblioth√®que ü§ó *Transformers*. √Ä la fin de ce chapitre, vous serez familier avec le fonctionnement des *transformers* et vous saurez comment utiliser un mod√®le pr√©sent sur le [*Hub*](https://huggingface.co/models), le *finetuner* sur un jeu de donn√©es, et partager vos r√©sultats sur le *Hub* !
- Les chapitres 5 √† 8 pr√©sentent les bases des librairies ü§ó *Datasets* et ü§ó *Tokenizers* ainsi qu'une d√©couverte des probl√®mes classiques de NLP. √Ä la fin de ce chapitre, vous serez capable de r√©soudre les probl√®mes de NLP les plus communs par vous-m√™me.
- Les chapitres 9 √† 12 proposent d'aller plus loin et d'explorer comment les *transformers* peuvent √™tre utilis√©s pour r√©soudre des probl√®mes de traitement de la parole et de vision par ordinateur. En suivant ces chapitres, vous apprendrez √† construire et √† partager vos mod√®les via des d√©monstrateurs, et vous serez capable d'optimiser ces mod√®les pour des environnements de production. Enfin, vous serez pr√™t √† appliquer ü§ó *Transformers* √† (presque) n'importe quel probl√®me d'apprentissage automatique !

Ce cours :

* requiert un bon niveau en Python,
* se comprend mieux si vous avez d√©j√† suivi un cours d'introduction √† l'apprentissage profond comme [fast.ai's](https://www.fast.ai/), [*Practical Deep Learning for Coders*](https://course.fast.ai/) ou un des cours d√©velopp√©s par [*DeepLearning.AI*](https://www.deeplearning.ai/),
* n'attend pas une connaissance appronfondie de [PyTorch](https://pytorch.org/) ou de [TensorFlow](https://www.tensorflow.org/), bien qu'√™tre familiaris√© avec l'un d'entre eux peut aider.

Apr√®s avoir termin√© ce cours, nous vous recommandons de suivre la [Sp√©cialisation en NLP](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh) dispens√©e par DeepLearning.AI, qui couvre une grande partie des mod√®les traditionnels de NLP comme le Bay√©sien na√Øf et les LSTMs qui sont importants √† conna√Ætre!

## Qui sommes-nous ?

√Ä propos des auteurs de ce cours :

**Matthew Carrigan** est ing√©nieur en apprentissage machine chez Hugging Face. Il vit √† Dublin en Irlande. Il a travaill√© auparavant comme ing√©nieur en apprentissage machine chez Parse.ly et avant cela comme chercheur postdoctoral au Trinity College Dublin. Il ne croit pas que nous arrivions √† l'*AGI* en mettant √† l'√©chelle les architectures existantes mais a tout de m√™me beaucoup d'espoir dans l'immortalit√© des robots.

**Lysandre Debut** est ing√©nieur en apprentissage machine chez Hugging Face et a travaill√© sur la biblioth√®que ü§ó *Transformers* depuis les premi√®res phases de d√©veloppement. Son but est de rendre le NLP accessible √† tous en d√©veloppant des outils disposant d'une API tr√®s simple.

**Sylvain Gugger** est ing√©nieur recherche chez Hugging Face et un des principaux responsables de la biblioth√®que ü§ó *Transformers*. Avant cela, il √©tait chercheur en en apprentissage machine chez fast.ai et a √©crit le livre [*Deep Learning for Coders with fastai and PyTorch*](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/) avec Jeremy Howard. Son but est de rendre l'apprentissage profond plus accessible, en d√©veloppant et en am√©liorant des techniques permettant aux mod√®les d'apprendre rapidement sur des ressources limit√©es.

**Merve Noyan** est d√©veloppeuse *advocate* chez Hugging Face et travaille √† la cr√©ation d'outils et de contenus visant √† d√©mocratiser l'apprentissage machine pour tous.

**Lucile Saulnier** est ing√©nieure en apprentissage machine chez Hugging Face et travaille au d√©veloppement et √† l'impl√©mentation de nombreux outils *open source*. Elle est √©galement activement impliqu√©e dans de nombreux projets de recherche dans le domaine du NLP comme l'entra√Ænement collaboratif de mod√®les et le projet BigScience.

**Lewis Tunstall** est ing√©nieur en apprentissage machine chez Hugging Face et d√©vou√© au d√©veloppement d'outils open source avec la volont√© de les rendre accessibles √† une communaut√© plus large. Il est √©galement co-auteur du livre [*Natural Language Processing with Transformers*](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/).

**Leandro von Werra** est ing√©nieur en apprentissage machine dans l'√©quipe *open source* d'Hugging Face et √©galement co-auteur du livre [*Natural Language Processing with Transformers*](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/). Il a plusieurs ann√©es d'exp√©rience dans l'industrie o√π il a pu d√©ployer des projets de NLP en production et travailler sur toutes les √©tapes clefs du d√©ploiement.

√ätes-vous pr√™t √† commencer ? Dans ce chapitre, vous apprendrez :
* √† utiliser la fonction `pipeline()` pour r√©soudre des probl√®mes de NLP comme la g√©n√©ration de texte et la classification,
* l'architecture d'un *transformer*,
* comment faire la distinction entre les diff√©rentes architectures d'encodeur, de d√©codeur et d'encodeur-d√©codeur ainsi que leurs diff√©rents cas d'usage.
