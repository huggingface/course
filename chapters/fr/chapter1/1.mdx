# Introduction

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

## Bienvenue au cours ü§ó !

<Youtube id="00GKzGyWFEs" />

Ce cours vous apprendra √† utiliser les biblioth√®ques de NLP de l'√©cosyst√®me [Hugging Face](https://huggingface.co/) : [ü§ó *Transformers*](https://github.com/huggingface/transformers), [ü§ó *Datasets*](https://github.com/huggingface/datasets), [ü§ó *Tokenizers*](https://github.com/huggingface/tokenizers) et [ü§ó *Accelerate*](https://github.com/huggingface/accelerate), ainsi que le [*Hub*](https://huggingface.co/models). C'est totalement gratuit et sans publicit√©.

## √Ä quoi s'attendre ?

Voici un bref aper√ßu du cours :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

- Les chapitres 1 √† 4 pr√©sentent les principaux concepts de la biblioth√®que ü§ó *Transformers*. √Ä la fin de ces chapitres, vous serez familier avec le fonctionnement des *transformers* et vous saurez comment utiliser un mod√®le pr√©sent sur le [*Hub*](https://huggingface.co/models), le *finetuner* sur un jeu de donn√©es, et partager vos r√©sultats sur le *Hub* !
- Les chapitres 5 √† 8 pr√©sentent les bases des librairies ü§ó *Datasets* et ü§ó *Tokenizers* ainsi qu'une d√©couverte des probl√®mes classiques de NLP. √Ä la fin de ce chapitre, vous serez capable de r√©soudre les probl√®mes de NLP les plus communs par vous-m√™me.
- Les chapitres 9 √† 12 proposent d'aller plus loin et d'explorer comment les *transformers* peuvent √™tre utilis√©s pour r√©soudre des probl√®mes de traitement de la parole et de vision par ordinateur. En suivant ces chapitres, vous apprendrez √† construire et √† partager vos mod√®les via des d√©monstrateurs, et vous serez capable d'optimiser ces mod√®les pour des environnements de production. Enfin, vous serez pr√™t √† appliquer ü§ó *Transformers* √† (presque) n'importe quel probl√®me d'apprentissage automatique !

Ce cours :

* requiert un bon niveau en Python,
* se comprend mieux si vous avez d√©j√† suivi un cours d'introduction √† l'apprentissage profond comme [fast.ai's](https://www.fast.ai/), [*Practical Deep Learning for Coders*](https://course.fast.ai/) ou un des cours d√©velopp√©s par [*DeepLearning.AI*](https://www.deeplearning.ai/),
* n'attend pas une connaissance appronfondie de [PyTorch](https://pytorch.org/) ou de [TensorFlow](https://www.tensorflow.org/), bien qu'√™tre familiaris√© avec l'un d'entre eux peut aider.

Apr√®s avoir termin√© ce cours, nous vous recommandons de suivre la [Sp√©cialisation en NLP](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh) dispens√©e par DeepLearning.AI, qui couvre une grande partie des mod√®les traditionnels de NLP comme le Bay√©sien na√Øf et les LSTMs qui sont importants √† conna√Ætre !

## Qui sommes-nous ?

√Ä propos des auteurs de ce cours :

[**Abubakar Abid**](https://huggingface.co/abidlabs) a obtenu son doctorat en apprentissage automatique appliqu√© √† Stanford. Pendant son doctorat, il a fond√© [Gradio](https://github.com/gradio-app/gradio), une biblioth√®que Python *open source* qui a √©t√© utilis√©e pour construire plus de 600 000 d√©mos d'apprentissage automatique. Gradio a √©t√© rachet√©e par Hugging Face, o√π Abubakar occupe d√©sormais le poste de responsable de l'√©quipe d'apprentissage automatique.

[**Matthew Carrigan**](https://huggingface.co/Rocketknight1) est ing√©nieur en apprentissage machine chez Hugging Face. Il vit √† Dublin en Irlande. Il a travaill√© auparavant comme ing√©nieur en apprentissage machine chez Parse.ly et avant cela comme chercheur postdoctoral au Trinity College Dublin. Il ne croit pas que nous arrivions √† l'*AGI* en mettant √† l'√©chelle les architectures existantes mais a tout de m√™me beaucoup d'espoir dans l'immortalit√© des robots.

[**Lysandre Debut**](https://huggingface.co/lysandre) est ing√©nieur en apprentissage machine chez Hugging Face et a travaill√© sur la biblioth√®que ü§ó *Transformers* depuis les premi√®res phases de d√©veloppement. Son but est de rendre le NLP accessible √† tous en d√©veloppant des outils disposant d'une API tr√®s simple.

[**Sylvain Gugger**](https://huggingface.co/sgugger) est ing√©nieur de recherche chez Hugging Face et un des principaux responsables de la biblioth√®que ü§ó *Transformers*. Avant cela, il √©tait chercheur en apprentissage machine chez fast.ai et a √©crit le livre [*Deep Learning for Coders with fastai and PyTorch*](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/) avec Jeremy Howard. Son but est de rendre l'apprentissage profond plus accessible en d√©veloppant et en am√©liorant des techniques permettant aux mod√®les d'apprendre rapidement sur des ressources limit√©es.

[**Dawood Khan**](https://huggingface.co/dawoodkhan82) est un ing√©nieur en apprentissage automatique chez Hugging Face. Il vient de New York et est dipl√¥m√© en informatique de l‚ÄôUniversit√© de New York. Apr√®s avoir travaill√© comme ing√©nieur iOS pendant quelques ann√©es, Dawood a quitt√© son poste pour cr√©er Gradio avec ses cofondateurs. Gradio a finalement √©t√© acquis par Hugging Face.

[**Merve Noyan**](https://huggingface.co/merve) est d√©veloppeuse *advocate* chez Hugging Face et travaille √† la cr√©ation d'outils et de contenus visant √† d√©mocratiser l'apprentissage machine pour tous.

[**Lucile Saulnier**](https://huggingface.co/SaulLu) est ing√©nieure en apprentissage machine chez Hugging Face et travaille au d√©veloppement et √† l'impl√©mentation de nombreux outils *open source*. Elle est √©galement activement impliqu√©e dans de nombreux projets de recherche dans le domaine du NLP comme l'entra√Ænement collaboratif de mod√®les et le projet [BigScience](https://bigscience.huggingface.co/).

[**Lewis Tunstall**](https://huggingface.co/lewtun) est ing√©nieur en apprentissage machine chez Hugging Face et d√©vou√© au d√©veloppement d'outils *open source* avec la volont√© de les rendre accessibles √† une communaut√© plus large. Il est √©galement co-auteur du livre [*Natural Language Processing with Transformers*](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).

[**Leandro von Werra**](https://huggingface.co/lvwerra) est ing√©nieur en apprentissage machine dans l'√©quipe *open source* d'Hugging Face et √©galement co-auteur du livre [*Natural Language Processing with Transformers*](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). Il a plusieurs ann√©es d'exp√©rience dans l'industrie o√π il a pu d√©ployer des projets de NLP en production et travailler sur toutes les √©tapes clefs du d√©ploiement.


## FAQ

Voici quelques r√©ponses aux questions fr√©quemment pos√©es :

- **Suivre ce cours m√®ne-t-il √† une certification ?**  
Actuellement, nous n'avons pas de certification pour ce cours. Cependant, nous travaillons sur un programme de certification pour l'√©cosyst√®me *Hugging Face*. Restez √† l'√©coute !

- **Combien de temps dois-je consacrer √† ce cours ?**  
Chaque chapitre de ce cours est con√ßu pour √™tre compl√©t√© en une semaine, avec environ 6 √† 8 heures de travail par semaine. Cependant, vous pouvez prendre tout le temps n√©cessaire pour le suivre.

- **O√π puis-je poser une question si j'en ai une ?**  
Si vous avez une question sur l'une des sections du cours, il vous suffit de cliquer sur la banni√®re ¬´ *Ask a question* ¬ª en haut de la page pour √™tre automatiquement redirig√© vers la bonne section du [forum d‚Äô*Hugging Face*](https://discuss.huggingface.co/) :

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png" alt="Link to the Hugging Face forums" width="75%">

Notez qu'une liste d'[id√©es de projets](https://discuss.huggingface.co/c/course/course-event/25) est √©galement disponible sur le forum si vous souhaitez pratiquer davantage une fois le cours termin√©.

- **O√π puis-je obtenir le code du cours ?**  
Pour chaque section, vous pouvez cliquer sur la banni√®re en haut de la page pour ex√©cuter son code dans *Google Colab* ou *Amazon SageMaker Studio Lab* :

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png" alt="Link to the Hugging Face course notebooks" width="75%">

A noter que pour la version en fran√ßais du cours, deux choix s‚Äôoffrent √† vous lorsque vous cliquez sur la banni√®re. Le premier est de s√©lectionner le *notebook* utilisant des mod√®les en anglais. L‚Äôint√©r√™t est qu‚Äôil s‚Äôagit de celui sur lequel sont bas√©es les explications du cours (interpr√©tation des r√©sultats, etc.). Le second est de s√©lectionner le *notebook* utilisant des mod√®les en fran√ßais. Il s‚Äôagit alors d‚Äôune proposition d‚Äôadaptation (un mod√®le parmi tous ceux existant en fran√ßais est utilis√©).

Si vous souhaitez acc√©der √† l‚Äôensemble des *notebooks* Jupyter du cours, il existe deux possibilit√©s. La premi√®re est de cloner le d√©p√¥t [`huggingface/notebooks`](https://github.com/huggingface/notebooks) et de consulter les *notebooks* contenus dans le dossier *course*. La seconde est de g√©n√©rer les *notebooks* localement en suivant les instructions dans le *README* du d√©p√¥t [`course`](https://github.com/huggingface/course#-jupyter-notebooks) sur GitHub.

- **Comment puis-je contribuer au cours ?**  
Il existe de nombreuses fa√ßons de contribuer au cours ! Si vous trouvez une coquille ou un bug, veuillez ouvrir une ¬´ *Issue* ¬ª sur le d√©p√¥t [`course`](https://github.com/huggingface/course). Si vous souhaitez aider √† traduire le cours dans votre langue maternelle, consultez les instructions [ici](https://github.com/huggingface/course#translating-the-course-into-your-language).

- **Quels ont √©t√© les choix retenus pour la traduction ?**  
Vous pouvez consulter le [glossaire](https://huggingface.co/course/fr/glossary/1) d√©taillant les choix retenus pour la traduction vers le fran√ßais.

- **Peut-on r√©utiliser ce cours?**  
Bien s√ªr ! Le cours est publi√© sous la licence [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0.html). Cela signifie que vous devez cr√©diter de mani√®re appropri√©e, fournir un lien vers la licence et indiquer si des modifications ont √©t√© apport√©es. Vous pouvez le faire de toute mani√®re raisonnable, mais pas d'une fa√ßon qui sugg√®re que le distributeur de la licence vous approuve ou approuve votre utilisation. Si vous souhaitez citer le cours, veuillez utiliser le BibTeX suivant :

```
@misc{huggingfacecourse,
  author = {Hugging Face},
  title = {The Hugging Face Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year = {2022},
  note = "[Online; accessed <today>]"
}
```


## C'est parti !

√ätes-vous pr√™t √† commencer ? Dans ce chapitre, vous apprendrez :
* √† utiliser la fonction `pipeline()` pour r√©soudre des probl√®mes de NLP comme la g√©n√©ration de texte et la classification,
* l'architecture d'un *transformer*,
* comment faire la distinction entre les diff√©rentes architectures d'encodeur, de d√©codeur et d'encodeur-d√©codeur ainsi que leurs diff√©rents cas d'usage.
