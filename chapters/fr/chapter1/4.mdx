# Comment fonctionnent les <i>transformers</i> ?

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

Dans cette partie, nous allons jeter un coup d'≈ìil √† l'architecture des *transformers*.

## Court historique des <i>transformers</i>

Voici quelques dates clefs dans la courte histoire des *transformers* :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

[L'architecture *Transformer*](https://arxiv.org/abs/1706.03762) a √©t√© pr√©sent√©e en juin 2017. Initialement, la recherche portait sur la t√¢che de traduction. Elle a √©t√© suivie par l'introduction de plusieurs mod√®les influents, notamment :

- **Juin 2018** : [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), le premier *transformer* pr√©-entra√Æn√© et *finetun√©* sur diff√©rentes t√¢ches de NLP et ayant obtenu des r√©sultats √† l'√©tat de l'art,

- **Octobre 2018** : [BERT](https://arxiv.org/abs/1810.04805), autre grand mod√®le pr√©-entra√Æn√© ayant √©t√© construit pour produire de meilleurs r√©sum√©s de texte (plus de d√©tails dans le chapitre suivant !),

- **F√©vrier 2019** : [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), une version am√©lior√©e (et plus grande) de GPT qui n'a pas √©t√© directement rendu publique pour cause de raisons √©thiques,

- **Octobre 2019** : [DistilBERT](https://arxiv.org/abs/1910.01108), une version distill√©e de BERT √©tant 60% plus rapide, 40% plus l√©g√®re en m√©moire et conservant tout de m√™me 97% des performances initiales de BERT,

- **Octobre 2019** : [BART](https://arxiv.org/abs/1910.13461) et [T5](https://arxiv.org/abs/1910.10683), deux mod√®les pr√©-entra√Æn√©s utilisant la m√™me architecture que le *transformer* original (les premiers √† faire cela),

- **Mai 2020** : [GPT-3](https://arxiv.org/abs/2005.14165), une version encore plus grande que GPT-2 ayant des performances tr√®s bonnes sur une vari√©t√© de t√¢ches ne n√©cessitant pas de *finetuning* (appel√© _zero-shot learning_).

Cette liste est loin d'√™tre exhaustive et met en lumi√®re certains *transformers*. Plus largement, ces mod√®les peuvent √™tre regroup√©s en trois cat√©gories :

- ceux de type GPT (aussi appel√©s *transformers* _autor√©gressifs_)  
- ceux de type BERT (aussi appel√©s *transformers* _auto-encodeurs_)
- ceux de type BART/T5 (aussi appel√©s *transformers* _s√©quence-√†-s√©quence_)

Nous verrons plus en profondeur ces familles de mod√®les plus tard.

## Les <i>transformers</i> sont des mod√®les de langage

Tous les *transformers* mentionn√©s ci-dessus (GPT, BERT, BART, T5, etc.) ont √©t√© entra√Æn√©s comme des *mod√®les de langage*. Cela signifie qu'ils ont √©t√© entra√Æn√©s sur une large quantit√© de textes bruts de mani√®re autosupervis√©e. L'apprentissage autosupervis√© est un type d'entra√Ænement dans lequel l'objectif est automatiquement calcul√© √† partir des entr√©es du mod√®le. Cela signifie que les humains ne sont pas n√©cessaires pour √©tiqueter les donn√©es !

Ce type de mod√®le d√©veloppe une compr√©hension statistique de la langue sur laquelle il a √©t√© entra√Æn√©, mais il n'est pas tr√®s utile pour des t√¢ches pratiques sp√©cifiques. Pour cette raison, le mod√®le pr√©-entra√Æn√© passe ensuite par un processus appel√© apprentissage par transfert. Au cours de ce processus, le mod√®le est *finetun√©* de mani√®re supervis√©e (c'est-√†-dire en utilisant des √©tiquettes annot√©es par des humains) pour une t√¢che donn√©e.

Un exemple de t√¢che consiste √† pr√©dire le mot suivant dans une phrase apr√®s avoir lu les *n* mots pr√©c√©dents. Cette t√¢che est appel√©e *mod√©lisation causale du langage* car la sortie d√©pend des entr√©es pass√©es et pr√©sentes, mais pas des entr√©es futures.
<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

Un autre exemple est la *mod√©lisation du langage masqu√©*, dans laquelle le mod√®le pr√©dit un mot masqu√© dans la phrase.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## Les <i>transformers</i> sont √©normes

En dehors de quelques exceptions (comme DistilBERT), la strat√©gie g√©n√©rale pour obtenir de meilleure performance consiste √† augmenter la taille des mod√®les ainsi que la quantit√© de donn√©es utilis√©es pour l'entra√Ænement de ces derniers.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

Malheureusement, entra√Æner un mod√®le et particuli√®rement un tr√®s grand mod√®le, n√©cessite une importante quantit√© de donn√©es. Cela devient tr√®s co√ªteux en termes de temps et de ressources de calcul. Cela se traduit m√™me par un impact environnemental comme le montre le graphique suivant.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>

<Youtube id="ftWlj4FBHTg"/>

L'image montre l'empreinte carbone pour un projet d'entra√Ænement d'un (tr√®s grand) mod√®le men√© par une √©quipe qui pourtant essaie consciemment de r√©duire l'impact environnemental du pr√©-entra√Ænement. L'empreinte de l'ex√©cution de nombreux essais pour obtenir les meilleurs hyperparam√®tres serait encore plus √©lev√©e.

Imaginez qu'√† chaque fois qu'une √©quipe de recherche, une association d'√©tudiants ou une entreprise souhaite entra√Æner un mod√®le, elle le fasse en partant de z√©ro. Cela entra√Ænerait des co√ªts globaux √©normes et inutiles !

C'est pourquoi le partage des mod√®les du langage est primordial : partager les poids d'entra√Ænement et construire √† partir de ces poids permet de r√©duire les co√ªts de calcul globaux ainsi que l'empreinte carbone de toute la communaut√©.

A noter que vous pouvez d‚Äôailleurs √©valuer l‚Äôempreinte carbone de l‚Äôentra√Ænement de vos mod√®les √† travers plusieurs outils. Par exemple [ML CO2 Impact](https://mlco2.github.io/impact/) ou bien [Code Carbon]( https://codecarbon.io/) qui est int√©gr√© dans ü§ó Transformers. Pour en savoir plus √† ce sujet, vous pouvez lire cet [article de blog](https://huggingface.co/blog/carbon-emissions-on-the-hub) qui vous montrera comment faire pour g√©n√©rer un fichier `emissions.csv` comportant une estimation de l‚Äôempreinte de votre entra√Ænement, ainsi que la [documentation](https://huggingface.co/docs/hub/model-cards-co2) de ü§ó Transformers abordant ce th√®me.


## L'apprentissage par transfert

<Youtube id="BqqfQnyjmgg" />

Le pr√©-entra√Ænement consiste √† entra√Æner un mod√®le √† partir de z√©ro : les poids sont initialis√©s de mani√®re al√©atoire et l'entra√Ænement commence sans aucune connaissance pr√©alable.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

Ce pr√©-entra√Ænement est g√©n√©ralement effectu√© sur de tr√®s grandes quantit√©s de donn√©es. Il n√©cessite donc un tr√®s grand corpus de donn√©es et l'entra√Ænement peut prendre jusqu'√† plusieurs semaines.

Le *finetuning*, quant √† lui, est l'entrainement effectu√© apr√®s qu'un mod√®le ait √©t√© pr√©-entra√Æn√©. Pour effectuer un *finetuning*, vous devez d'abord acqu√©rir un mod√®le de langue pr√©-entra√Æn√©, puis effectuer un entra√Ænement suppl√©mentaire avec un jeu de donn√©es sp√©cifiques. Mais pourquoi ne pas entra√Æner directement pour la t√¢che finale ? Il y a plusieurs raisons √† cela :

*  Le mod√®le pr√©-entra√Æn√© a d√©j√† √©t√© entra√Æn√© sur un jeu de donn√©es qui pr√©sente certaines similitudes avec le jeu de donn√©es de *finetuning*. Le processus de *finetuning* est donc en mesure de tirer parti des connaissances acquises par le mod√®le initial lors du pr√©-entra√Ænement (par exemple, pour les probl√®mes de langage naturel, le mod√®le pr√©-entra√Æn√© aura une certaine compr√©hension statistique de la langue que vous utilisez pour votre t√¢che)
*  Comme le mod√®le pr√©-entra√Æn√© a d√©j√† √©t√© entra√Æn√© sur de nombreuses donn√©es, le *finetuning* n√©cessite beaucoup moins de donn√©es pour obtenir des r√©sultats d√©cents.
*  Pour la m√™me raison, le temps et les ressources n√©cessaires pour obtenir de bons r√©sultats sont beaucoup moins importants.

Par exemple, il est possible d'exploiter un mod√®le pr√©-entra√Æn√© entra√Æn√© sur la langue anglaise, puis de le *finetuner* sur un corpus arXiv, pour obtenir un mod√®le bas√© sur la science et la recherche. Le *finetuning* ne n√©cessitera qu'une quantit√© limit√©e de donn√©es : les connaissances acquises par le mod√®le pr√©-entra√Æn√© sont ¬´ transf√©r√©es ¬ª, d'o√π le terme d'apprentissage par transfert.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

Le *finetuning* d'un mod√®le a donc un co√ªt moindre en termes de temps, de donn√©es, de finances et d'environnement. Il est aussi plus rapide et plus facile d'it√©rer sur diff√©rents sch√©mas de *finetuning* car l'entra√Ænement est moins contraignant qu'un pr√©-entra√Ænement complet.

Ce processus permet √©galement d'obtenir de meilleurs r√©sultats que l'entra√Ænement √† partir de z√©ro (√† moins que vous ne disposiez d'un grand nombre de donn√©es). C'est pourquoi vous devez toujours essayer de tirer parti d'un mod√®le pr√©-entra√Æn√©, c'est-√†-dire un mod√®le aussi proche que possible de la t√¢che que vous avez √† accomplir, et de le *finetuner*.

## Architecture g√©n√©rale

Dans cette section, nous allons voir l'architecture g√©n√©rale des *transformers*. Pas d'inqui√©tudes si vous ne comprenez pas tous les concepts, des sections d√©taill√©es qui couvrent chaque composant seront abord√©es plus tard.

<Youtube id="H39Z_720T5s" />

## Introduction

Le mod√®le est principalement compos√© de deux blocs :

* **Encodeur (√† gauche)** : l'encodeur re√ßoit une entr√©e et construit une repr√©sentation de celle-ci (ses caract√©ristiques). Cela signifie que le mod√®le est optimis√© pour acqu√©rir une compr√©hension venant de ces entr√©es.
* **D√©codeur (√† droite)** : le d√©codeur utilise la repr√©sentation de l'encodeur (les caract√©ristiques) en plus des autres entr√©es pour g√©n√©rer une s√©quence cible. Cela signifie que le mod√®le est optimis√© pour g√©n√©rer des sorties.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

Chacun de ces blocs peuvent √™tre utilis√©s ind√©pendamment en fonction de la t√¢che que l'on souhaite traiter :

* **Mod√®les uniquement encodeurs** : adapt√©s pour des t√¢ches qui n√©cessitent une compr√©hension de l'entr√©e, comme la classification de phrases et la reconnaissance d'entit√©s nomm√©es.
* **Mod√®les uniquement d√©codeurs** : adapt√©s pour les t√¢ches g√©n√©ratives telles que la g√©n√©ration de texte.
* **Mod√®les encodeurs-d√©codeurs** (ou **mod√®les de s√©quence-√†-s√©quence**) : adapt√©s aux t√¢ches g√©n√©ratives qui n√©cessitent une entr√©e, telles que la traduction ou le r√©sum√© de texte.

Nous verrons plus en d√©tails chacune de ces architectures plus tard.

## Les couches d'attention

Une caract√©ristique cl√© des *transformers* est qu'ils sont construits avec des couches sp√©ciales appel√©es couches d'attention. En fait, le titre du papier introduisant l'architecture *transformer* se nomme [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) ! Nous explorerons les d√©tails des couches d'attention plus tard dans le cours. Pour l'instant, tout ce que vous devez savoir est que cette couche indique au mod√®le de pr√™ter une attention sp√©cifique √† certains mots de la phrase que vous lui avez pass√©e (et d'ignorer plus ou moins les autres) lors du traitement de la repr√©sentation de chaque mot.

Pour mettre cela en contexte, consid√©rons la t√¢che de traduire un texte de l'anglais au fran√ßais. √âtant donn√© l'entr√©e ¬´ *You like this course* ¬ª, un mod√®le de traduction devra √©galement s'int√©resser au mot adjacent ¬´ *You* ¬ª pour obtenir la traduction correcte du mot ¬´ *like* ¬ª, car en fran√ßais le verbe ¬´ *like* ¬ª se conjugue diff√©remment selon le sujet. Le reste de la phrase n'est en revanche pas utile pour la traduction de ce mot. Dans le m√™me ordre d'id√©es, pour traduire ¬´ *this* ¬ª, le mod√®le devra √©galement faire attention au mot ¬´ *course* ¬ª car ¬´ *this* ¬ª se traduit diff√©remment selon que le nom associ√© est masculin ou f√©minin. L√† encore, les autres mots de la phrase n'auront aucune importance pour la traduction de ¬´ *this* ¬ª. Avec des phrases plus complexes (et des r√®gles de grammaire plus complexes), le mod√®le devra pr√™ter une attention particuli√®re aux mots qui pourraient appara√Ætre plus loin dans la phrase pour traduire correctement chaque mot.

Le m√™me concept s'applique √† toute t√¢che associ√©e au langage naturel : un mot en lui-m√™me a un sens, mais ce sens est profond√©ment affect√© par le contexte, qui peut √™tre n'importe quel autre mot (ou mots) avant ou apr√®s le mot √©tudi√©.

Maintenant que vous avez une id√©e plus pr√©cise des couches d'attentions, nous allons regarder de plus pr√®s l'architecture des *transformers*.

## L'architecture originale

L'architecture du *transformer* a initialement √©t√© construite pour la t√¢che de traduction. Pendant l'entra√Ænement, l'encodeur re√ßoit des entr√©es (des phrases) dans une certaine langue, tandis que le d√©codeur re√ßoit la m√™me phrase traduite dans la langue cible. Pour l'encodeur, les couches d'attention peuvent utiliser tous les mots d'une phrase (puisque comme nous venons de le voir, la traduction d'un mot donn√© peut d√©pendre de ce qui le suit ou le pr√©c√®de dans la phrase). Le d√©codeur, quant √† lui, fonctionne de fa√ßon s√©quentielle et ne peut porter son attention qu'aux mots d√©j√† traduits dans la phrase (donc uniquement les mots g√©n√©r√©s avant le mot en cours). Par exemple, lorsqu'on a pr√©dit les trois premiers mots de la phrase cible, on les donne au d√©codeur qui utilise alors toutes les entr√©es de l'encodeur pour essayer de pr√©dire le quatri√®me mot.

Pour acc√©l√©rer les choses pendant l'apprentissage (lorsque le mod√®le a acc√®s aux phrases cibles), le d√©codeur est aliment√© avec la cible enti√®re, mais il n'est pas autoris√© √† utiliser les mots futurs (s'il avait acc√®s au mot en position 2 lorsqu'il essayait de pr√©dire le mot en position 2, le probl√®me ne serait pas tr√®s difficile !). Par exemple, en essayant de pr√©dire le quatri√®me mot, la couche d'attention n'aura acc√®s qu'aux mots des positions 1 √† 3.

L'architecture originale du *transformer* ressemble √† ceci, avec l'encodeur √† gauche et le d√©codeur √† droite :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

Notez que la premi√®re couche d'attention dans un bloc d√©codeur pr√™te attention √† toutes les entr√©es (pass√©es) du d√©codeur, mais que la deuxi√®me couche d'attention utilise la sortie de l'encodeur. Elle peut donc acc√©der √† l'ensemble de la phrase d'entr√©e pour pr√©dire au mieux le mot actuel. C'est tr√®s utile, car diff√©rentes langues peuvent avoir des r√®gles grammaticales qui placent les mots dans un ordre diff√©rent, ou un contexte fourni plus tard dans la phrase peut √™tre utile pour d√©terminer la meilleure traduction d'un mot donn√©.

Le *masque d'attention* peut √©galement √™tre utilis√© dans l'encodeur/d√©codeur pour emp√™cher le mod√®le de pr√™ter attention √† certains mots sp√©ciaux. Par exemple, le mot de remplissage sp√©cial (le *padding*) utilis√© pour que toutes les entr√©es aient la m√™me longueur lors du regroupement de phrases.

## Architectures contre <i>checkpoints</i>
 
En approfondissant l'√©tude des <i>transformers</i> dans ce cours, vous verrez des mentions d'<i>architectures</i> et de <i>checkpoints</i> ainsi que de <i>mod√®les</i>. Ces termes ont tous des significations l√©g√®rement diff√©rentes :

* **Architecture** : c'est le squelette du mod√®le, la d√©finition de chaque couche et chaque op√©ration qui se produit au sein du mod√®le.
* **Checkpoints** : ce sont les poids qui seront charg√©s dans une architecture donn√©e.
* **Mod√®le** : c'est un mot valise n'√©tant pas aussi pr√©cis que les mots ¬´ architecture ¬ª ou ¬´ *checkpoint* ¬ª. Il peut d√©signer l'un comme l'autre. Dans ce cours, il sera sp√©cifi√© *architecture* ou *checkpoint* lorsqu'il sera essentiel de r√©duire toute ambigu√Øt√©.

Par exemple, BERT est une architecture alors que `bert-base-cased` (un ensemble de poids entra√Æn√© par l'√©quipe de Google lors de la premi√®re sortie de BERT) est un *checkpoint*. Cependant, il est possible de dire ¬´ le mod√®le BERT ¬ª et ¬´ le mod√®le `bert-base-cased` ¬ª.
