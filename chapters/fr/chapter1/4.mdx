# Comment fonctionnent les modèles Transformers ?

Dans cette partie, nous allons jeter un coup d'oeil à l'architecture des modèles Transformers.

## Court historique des Transformers

Voici quelques dates clefs dans la courte histoire des modèles Transformers :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

[L'architecture Transformer](https://arxiv.org/abs/1706.03762) a été présentée en Juin 2017. La recherche initiale portait sur les tâches de traduction. Cela s'est suivi par la présentation de plusieurs modèles influents, dont :

- **Juin 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), le premier modèle Transformer pré-entraîné, utilisé pour fine-tuning sur différentes tâches de NLP et ayant obtenu des résultats à l'état de l'art

- **Octobre 2018**: [BERT](https://arxiv.org/abs/1810.04805), autre modèle large pré-entraîné, ayant été construit pour produire de meilleurs résumés de texte (plus de détails dans le chapitre suivant !)

- **Février 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), une version améliorée (et plus grande) de GPT qui n'a pas été directement rendu publique pour cause de raisons éthiques

- **Octobre 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108), une version distillée de BERT étant 60% plus rapide, 40% plus légère en mémoire, et conservant tout de même 97% performances initiales de BERT

- **Octobre 2019**: [BART](https://arxiv.org/abs/1910.13461) et [T5](https://arxiv.org/abs/1910.10683), deux modèles larges pré-entraînés utilisant la même architecture que le modèle Transformer originel (les premiers à faire cela)

- **Mai 2020**, [GPT-3](https://arxiv.org/abs/2005.14165), une version encore plus grande que GPT-2 ayant des performances très bonnes sur une variété de tâches ne nécessitant pas de fine-tuning (appellé _zero-shot learning_)

Cette liste est loin d'être exhaustive, et permet juste de mettre en lumière certains modèles Transformers. Plus largement, ces modèles peuvent être regroupés en trois catégories :

- GPT-like (aussi appelé modèles Transformers _auto-regressif_)  
- BERT-like (aussi appelé modèles Transformers _auto-encodeur_)
- BART/T5-like (aussi appelé modèles Transformers _séquence-à-séquence_)

Nous verrons plus en profondeur ces familles de modèles plus tard.

## Transformers, des modèles du langage

Tous les modèles Transformers mentionné ci-dessus (GPT, BERT, BART, T5, etc.) ont été entraînés comme des *modèles du langage*. Cela signifie qu'ils ont été entraînés sur une large quantité de textes bruts en mode auto-supervisé. L'apprentissage auto-supervisé est un type d'entraînement où l'objectif est automatiquement déterminé par les entrées du modèle. Cela veut dire qu'une labellisation des données par des humains n'est pas nécessaire !

Ce type de modèle développe une compréhension statistique du langage sur lequel il est entraîné, mais cela n'est pas adapté à une tâche spécifique. A cause de cela, le modèle pré-entraîné doit ensuite suivre une procédure de *transfer learning*. Durant cette procédure, le modèle est fine-tuné en mode supervisé -- en utilisant des données labellisées par un human -- sur une tâche donnée.

Un exemple de tâche est la prédiction du prochain mot de la phrase en ayant lu les *n* mots précédents. Ce procédé s'appelle *modélisation du langage causal* car les prédictions dépendent des entrées précédentes et actuelles, mais pas des suivantes.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

Un autre exemple est la *modélisation du langage contextuel*, où me modèle doit prédire un mot masqué dans une phrase.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## Les modèles Transformers sont énormes

En dehors de quelques exceptions (comme DistilBERT), la stratégie générale pour obtenir de meilleure performance consiste à augmenter la taille des modèles ainsi que la quantité de données utilisées pour l'entraînement de ces derniers.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

Malheureusement, entraîner un modèle, et particulièrement un modèle large, nécessite une importante quantité de données. Cela devient très coûteux en terme de temps et de ressources de calcul. Cela se traduit même par un impact environnemental, comme le montre le graphique suivant.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>

<Youtube id="ftWlj4FBHTg"/>

Et cela présente un projet pour un (gigantesque) modèle mené par une équipe qui essaye consciemment de réduire l'impact environnemental du pré-entraînement. L'empreinte environnemental serait encore plus conséquente s'il fallait lancer beaucoup d'entraînements pour obtenir les meilleurs hyper-paramètres.

Imaginez si à chaque fois qu'une équipe de recherche, qu'une organisation étudiante ou qu'une entreprise voulait entraîner un modèle, ils devaient repartir de zéro. Ceci menerait à des énormes et non-nécessaires coûts environnementaux.

C'est pourquoi le partage des modèles du langage est primordial : partager les poids d'entraînement et construire à partir de ces poids permet de réduire les coûts de calcul globaux ainsi que l'empreinte carbone de toute la communauté.

## Le Transfer Learning

<Youtube id="BqqfQnyjmgg" />

Le *Pré-entraînement* est le fait d'entraîner un modèle de zéro : les poids sont initialisés de manière aléatoire, et l'entraînement débute sans aucune connaissance préalable (ou heuristique).

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

Ce pré-entraînement est habituellement réalisé sur de grande quantité de données. Il nécessite donc un très grand corpus de données, et l'entraînement peut prendre jusqu'à plusieurs semaines.

Le *Fine-tuning*, d'un autre côté, est un entraînement effectué **après** qu'un modèle ait été pré-entraîné. Pour réaliser le fine-tuning, il faut d'abord récupèrer un modèle du langage pré-entraîné, puis lancé un entraînement sur un jeu de données adapté à la tâche cible. Mais attendez -- pourquoi ne pas simplement entraîné un modèle sur la tâche cible ? Voici plusieurs raisons :

*  Le modèle pré-entraîné a déjà été entraîné sur un jeu de données qui a des similarités avec le jeu de données utilisé pour le fine-tuning. Ainsi le processus de fine-tuning peut bénéficier du savoir acquis par le modèle initial lors du pré-entraînement (par exemple, avec des problèmes de NLP, le modèle pré-entraîné aura une sorte de compréhension statistique de la langue associée à la tâche cible).
*  Puisque le modèle pré-entraîné a déjà été entraîné sur beaucoup de données, le fine-tuning nécessite beaucoup moins de données pour obtenir des résultats décents.
*  Pour la même raison, la quantité de temps et de ressources nécessaires pour obtenir de bons résultats est beaucoup moins grande.

Par exemple, il serait possible de tirer prodit d'un modèle pré-entraîné sur la langue anglaise and ensuite fine-tuné sur un corpus arXiv, permettant d'obtenir un modèle basé sur la recherche et la science. Le fine-tuning ne nécessiterait qu'une quantité réduite de données : le savoir acquis par le modèle pré-entraîné est transféré au nouveau modèle, d'où le terme de *transfer learning*.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

Le fine-tuning d'un modèle nécessite ainsi moins de temps, de données, d'argent et a un impact environnemental réduit. Il est également possible d'itérer plus rapidement sur différentes stratégies de fine-tuning, puisque l'entraînement est moins contraignant qu'un entraînement complet.

Ce procédé permet également d'obtenir de meilleurs résultats qu'un entraînement de zéro (sauf si on dispose de beaucoup de données), ce qui explique pourquoi il faut toujours essayer de tirer profit d'un modèle pré-entraîné -- un qui est le plus proche de la tâche cible -- et de le fine-tuner.

## Architecture générale

Dans cette section, nous allons voir l'architecture générale des modèles Transformers. Pas d'inquiétudes si vous ne comprenez pas tous les concepts; il y a des sections détaillées qui couvrent chaque composants plus tard.

<Youtube id="H39Z_720T5s" />

## Introduction

Le modèle est principalement composé de deux blocs :

* **Encodeur (gauche)**: L'encodeur reçoit une entrée et contruit une réprésentation de celle-ci (ses caractéristiques). Cela signifie que le modèle est optimisé pour acquérir une compréhension venant de ces entrées.
* **Décodeur (droite)**: Le décodeur utilise la réprésentation de l'encodeur (caractéristiques) en plus des autres entrées pour générer une séquence cible. Cela signifie que le modèle est optimisé pour générer des sorties.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

Chacune de ces parties peuvent être utilisées indépendamment, en fonction de la tâche :

* **Modèles uniquement encodeurs** : Adaptés pour des tâches qui nécessitent une compréhension de l'entrée, comme la classification de phrases et la reconnaissance d'entités nommées.
* **Modèles uniquement décodeurs** : Adaptés pour les tâches génératives telles que la génération de texte.
* **Modèles encodeurs-décodeurs** ou **modèles de séquence-à-séquence** : Adaptés aux tâches génératives qui nécessitent une entrée, telles que la traduction ou le résumé de texte.

Nous verrons plus en détails chacune de ces architectures plus tard.

## Les couches d'attention

Une caractéristique clef des modèles Transformers est qu'ils sont construits avec des couches spéciales appellées *couches d'attention*. En fait, le titre de la publication qui a présenté l'architecture Transformer est ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) ! Nous allons explorer les détails des couches d'attention plus tard dans ce cours; pour le moment, ce qu'il faut retenir est que la couche d'attention va indiquer au modèle les mots, de la phrase d'entrée, sur lesquels porter son attention (et plus ou moins ignorer les autres) lorsqu'il s'agit de construire une représentation de chaque mot.

Pour mettre ceci en contexte, considérons la tâche de traduction de l'anglais vers le français. Étant donné l'entrée "You like this course", un modèle de traduction devra également s'occuper du mot adjacent "You" pour obtenir la traduction correcte du mot "like", car en français le verbe "aimer" est conjugué différemment selon l'objet. Le reste de la phrase, cependant, n'est pas utile pour la traduction de ce mot. De même, lors de la traduction de "this", le modèle devra également faire attention au mot "course", car "this" se traduit différemment selon que le nom associé est masculin ou féminin. Encore une fois, les autres mots de la phrase n'auront pas d'importance pour la traduction de "this". Avec des phrases plus complexes (et des règles de grammaire plus complexes), le modèle devrait accorder une attention particulière aux mots qui pourraient apparaître plus loin dans la phrase pour traduire correctement chaque mot.

Le même concept s'applique à n'importe quelle tâche associée au langage naturel : un mot seul est porteur de sens, mais ce sens est profondément affecté par le contexte, qui peut être un ou plusieurs mots se situant avant ou après le mot étudié.

Maintenant que vous avez une idée plus précise des couches d'attentions, nous allons regarder de plus près l'architecture des modèles Transformers.

## L'architecture originelle

L'architecture Transformer a initialement été construite pour des tâches de traduction. Pendant l'entraînement, l'encodeur reçoit des entrées (des phrases) dans une certaine langue, tandis que le décodeur reçoit la même phrase traduite dans la langue cible. Pour l'encodeur, les couches d'attention peuvent utiliser tous les mots d'une phrase (puisque, comme nous venons de le voir, la traduction d'un mot donné peut dépendre de ce qui le suit ou le précède dans la phrase). Le décodeur, quant à lui, fonctionne de façon séquentielle et ne peut porter son attention qu'aux mots déjà traduits dans la phrase (donc, uniquement les mots générés avant le mot en cours). Par exemple, lorsqu'on a prédit les trois premiers mots de la phrase cible, on les donne au décodeur qui utilise alors toutes les entrées de l'encodeur pour essayer de prédire le quatrième mot.

Pour accélérer les choses pendant l'apprentissage (lorsque le modèle a accès aux phrases cibles), le décodeur est alimenté avec la cible entière, mais il n'est pas autorisé à utiliser les mots futurs (s'il avait accès au mot en position 2 lorsqu'il essayait de prédire le mot en position 2, le problème ne serait pas très difficile !). Par exemple, en essayant de prédire le quatrième mot, la couche d'attention n'aura accès qu'aux mots des positions 1 à 3.

L'architecture originale des modèles Transformers ressemblait à ceci, avec l'encodeur à gauche et le décodeur à droite :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

Notez que la première couche d'attention dans un bloc décodeur porte sont attention à toutes les entrées (passées) du décodeur, mais la seconde couche d'attention utilise la sortie de l'encodeur. Il peut ainsi accéder à toute la phrase d'entrée pour prédire au mieux le mot actuel. Ceci est très utile puisque les différentes langues peuvent avoir des règles grammaticales qui placent les mots dans des ordres différents, ou un contexte fourni plus tard dans la phrase peut être utile pour déterminer la meilleure traduction d'un mot donné.

Le *masque d'attention* peut également être utilisé dans un modèle encodeur/décodeur pour l'empêcher de porter son attention sur certains mots spéciaux -- par exemple, le motde remplissage spécial utilisé pour que toutes les entrées aient la même longueur lors du regroupement de phrases.

## Architectures contre checkpoints
 
En approfondissant l'étude des modèles Transformers dans ce cours, vous verrez des mentions d'*architectures* et de *checkpoints* ainsi que de *modèles*. Ces termes ont tous des significations légèrement différentes :

* **Architecture**: C'est le squelette du modèle -- la définition de chaque couche et chaque opération qui se produit au sein du modèle.
* **Checkpoints**: Ce sont les poids qui seront chargés dans une architecture donnée.
* **Model**: C'est un mot valise n'étant pas aussi précis que les mots "architecture" ou "checkpoint": il peut désigner l'un comme l'autre. Dans ce cours, il sera spécifié *architecture* ou *checkpoint* lorsqu'il est nécessaire de réduire l'ambiguité.

Par exemple, BERT est une architecture alors que `bert-base-cased`, un ensemble de poids entraîné par l'équipe de Google lors de la première sortie de BERT, est un checkpoint. Cependant, il est possible de dire "le modèle BERT" et "le modèle `bert-base-cased`".
