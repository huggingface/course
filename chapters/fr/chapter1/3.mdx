# Que peuvent faire les <i>transformers</i> ?

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb"},
]} />

Dans cette section, nous allons voir ce que peuvent faire les *transformers* et utiliser notre premier outil de la biblioth√®que ü§ó *Transformers* : la fonction `pipeline()`.

<Tip>
üëÄ Vous voyez ce bouton <em>Open in Colab</em> en haut √† droite ? Cliquez dessus pour ouvrir un <i>notebook</i> Colab avec tous les exemples de code de cette section. Ce bouton sera pr√©sent dans n'importe quelle section contenant des exemples de code.

Si vous souhaitez ex√©cuter les codes en local, nous vous recommandons de jeter un ≈ìil au chapitre <a href="/course/fr/chapter0">configuration</a>.
</Tip>

## Les <i>transformers</i> sont partout !

Les *transformers* sont utilis√©s pour r√©soudre toute sorte de t√¢ches de NLP comme celles mentionn√©es dans la section pr√©c√©dente. Voici quelques-unes des entreprises et organisations qui utilisent Hugging Face, les *transformers* et qui contribuent aussi √† la communaut√© en partageant leurs mod√®les :

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Companies using Hugging Face" width="100%">

La biblioth√®que [ü§ó *Transformers*](https://github.com/huggingface/transformers) fournit toutes les fonctionnalit√©s n√©cessaires pour cr√©er et utiliser les mod√®les partag√©s. Le [*Hub*](https://huggingface.co/models) contient des milliers de mod√®les pr√©-entra√Æn√©s que n'importe qui peut t√©l√©charger et utiliser. Vous pouvez √©galement transf√©rer vos propres mod√®les vers le Hub !

<Tip>
	‚ö†Ô∏è Le <i>Hub</i> n'est pas limit√© aux <i>transformers</i>. Tout le monde peut partager n'importe quel mod√®le ou jeu de donn√©es s'il le souhaite ! <a href="https://huggingface.co/join">Cr√©ez un compte sur huggingface.co</a> pour b√©n√©ficier de toutes les fonctionnalit√©s disponibles !
</Tip>

Avant de d√©couvrir en d√©tail comment les *transformers* fonctionnent, nous allons voir quelques exemples de comment ils peuvent √™tre utilis√©s pour r√©soudre des probl√®mes int√©ressants de NLP.

## Travailler avec les pipelines

<Youtube id="tiZFewofSLM" />

L'outil le plus basique de la biblioth√®que ü§ó *Transformers* est la fonction `pipeline()`. Elle relie un mod√®le avec ses √©tapes de pr√©-traitement et de post-traitement, permettant d'entrer n'importe quel texte et d'obtenir une r√©ponse compr√©hensible :

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    "I've been waiting for a HuggingFace course my whole life."
)  # J'ai attendu un cours d'HuggingFace toute ma vie.
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

On peut m√™me passer plusieurs phrases !

```python
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]  # ¬´ J'ai attendu un cours d'HuggingFace toute ma vie. ¬ª,  ¬´ Je d√©teste tellement √ßa ! ¬ª
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Par d√©faut, ce pipeline s√©lectionne un mod√®le pr√©-entra√Æn√© qui a √©t√© sp√©cifiquement entra√Æn√© pour l'analyse de sentiment en anglais. Le mod√®le est t√©l√©charg√© et mis en cache lorsque vous cr√©ez l'objet `classifier`. Si vous r√©ex√©cutez la commande, c'est le mod√®le mis en cache qui sera utilis√© et il n'y a pas besoin de t√©l√©charger le mod√®le √† nouveau.

Il y a trois √©tapes principales lorsque vous passez du texte √† un pipeline :

1. le texte est pr√©trait√© pour qu'il ait un format compr√©hensible par le mod√®le,
2. les donn√©es pr√©trait√©es sont pass√©es au mod√®le,
3. les pr√©dictions du mod√®le sont post-trait√©es de sorte que vous puissiez les comprendre.


Voici une liste non-exhaustive des [pipelines disponibles](https://huggingface.co/transformers/main_classes/pipelines.html) :

- `feature-extraction` (pour obtenir la repr√©sentation vectorielle d'un texte)
- `fill-mask`
- `ner` (*named entity recognition* ou reconnaissance d'entit√©s nomm√©es en fran√ßais)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

Regardons de plus pr√®s certains d'entre eux !

## <i>Zero-shot classification</i>

Nous allons commencer par nous attaquer √† une t√¢che plus difficile o√π nous devons classer des textes qui n'ont pas √©t√© annot√©s. C'est un sc√©nario tr√®s r√©pandu dans les projets r√©els car l'annotation de textes est g√©n√©ralement longue et n√©cessite parfois une expertise dans un domaine. Pour ce cas d'usage, le pipeline `zero-shot-classification` est tr√®s puissant : il vous permet de sp√©cifier les labels √† utiliser pour la classification, de sorte que vous n'ayez pas √† vous soucier des labels du mod√®le pr√©-entra√Æn√©. Nous avons d√©j√† vu comment le mod√®le peut classer un texte comme positif ou n√©gatif en utilisant ces deux labels mais il peut √©galement classer le texte en utilisant n'importe quel autre ensemble de labels que vous souhaitez.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    # C'est un cours sur la biblioth√®que Transformers
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library', 
# C'est un cours sur la biblioth√®que Transformers
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Ce pipeline est appel√© _zero-shot_ car vous n'avez pas besoin d'entra√Æner sp√©cifiquement le mod√®le sur vos donn√©es pour l'utiliser. Il peut directement retourner des scores de probabilit√© pour n'importe quel ensemble de labels que vous choisissez !

<Tip>

‚úèÔ∏è **Essayez !** Jouez avec vos propres s√©quences et labels et voyez comment le mod√®le fonctionne.

</Tip>


## G√©n√©ration de texte

Maintenant, nous allons voir comment utiliser un pipeline pour g√©n√©rer du texte. L'id√©e principale ici est que vous fournissez seulement un extrait de texte qui va √™tre compl√©t√© par du texte g√©n√©r√© automatiquement par le mod√®le. Cette fonction est similaire √† la fonction de texte pr√©dictif que l'on trouve sur de nombreux t√©l√©phones portables. La g√©n√©ration de texte implique de l'al√©atoire, donc il est normal que vous n'obteniez pas les m√™mes r√©sultats que ceux pr√©sent√©s ci-dessous.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator(
    "In this course, we will teach you how to"
)  # Dans ce cours, nous vous enseignerons comment
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use ' 
					# Dans ce cours, nous vous enseignerons comment comprendre et utiliser
                    'data flow and data interchange when handling user data. We '
					# flux de donn√©es et l'√©change de donn√©es lors de la manipulation des donn√©es utilisateur. Nous 
                    'will be working with one or more of the most commonly used ' 
					# travailleront avec un ou plusieurs des plus couramment utilis√©s
                    'data flows ‚Äî data flows of various types, as seen by the ' 
					# flux de donn√©es - flux de donn√©es de diff√©rents types, tels qu'ils sont vus par
                    'HTTP'}] # HTTP
```

Il est possible de contr√¥ler le nombre de s√©quences g√©n√©r√©es avec l'argument `num_return_sequences` et la longueur totale du texte g√©n√©r√© avec l'argument `max_length`.

<Tip>

‚úèÔ∏è **Essayez !** Utilisez les arguments `num_return_sequences` et `max_length` pour g√©n√©rer deux phrases de 15 mots chacune.

</Tip>


## Utiliser n'importe quel mod√®le du <i>Hub</i> dans un pipeline

Les exemples pr√©c√©dents utilisaient le mod√®le par d√©faut pour la t√¢che en question mais vous pouvez aussi choisir un mod√®le particulier du *Hub* et l'utiliser dans un pipeline pour une t√¢che sp√©cifique comme par exemple la g√©n√©ration de texte. Rendez-vous sur le [*Hub*](https://huggingface.co/models) et cliquez sur le *filtre* correspondant sur la gauche pour afficher seulement les mod√®les support√©s pour cette t√¢che. Vous devriez arriver sur une page comme [celle-ci](https://huggingface.co/models?pipeline_tag=text-generation).

Essayons le mod√®le [`distilgpt2`](https://huggingface.co/distilgpt2) ! Voici comment charger le mod√®le dans le m√™me pipeline que pr√©c√©demment :

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    # Dans ce cours, nous vous enseignerons comment
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and ' 
					# Dans ce cours, nous vous enseignerons comment manipuler le monde et
                    'move your mental and physical capabilities to your advantage.'}, 
					# utiliser vos capacit√©s mentales et physiques √† votre avantage.
 {'generated_text': 'In this course, we will teach you how to become an expert and ' 
					# Dans ce cours, nous vous apprendrons comment devenir un expert et
                    'practice realtime, and with a hands on experience on both real ' 
					# pratique en temps r√©el, et avec une exp√©rience pratique √† la fois sur de vrais
                    'time and real'}] 
					# temps et r√©el
```

Vous pouvez am√©liorer votre recherche de mod√®le en cliquant sur les *filtres* de langue et choisir un mod√®le qui g√©n√®re du texte dans une autre langue. Le *Hub* contient √©galement des *checkpoints* pour des mod√®les multilingues qui supportent plusieurs langues.

Une fois que vous avez choisi un mod√®le, vous verrez que vous pouvez tester son fonctionnement en ligne directement. Cela vous permet de tester rapidement les capacit√©s du mod√®le avant de le t√©l√©charger.

<Tip>

‚úèÔ∏è **Essayez !** Utilisez les filtres pour trouver un mod√®le de g√©n√©ration de texte pour une autre langue. N'h√©sitez pas √† jouer avec le *widget* et l'utiliser dans un pipeline !

</Tip>

### L'API d'inf√©rence

Tous les mod√®les peuvent √™tre test√© directement depuis votre navigateur en utilisant l'API d'inf√©rence qui est disponible sur le site [Hugging Face](https://huggingface.co/). Vous pouvez jouer avec le mod√®le directement sur sa page en entrant du texte personnalis√© et en regardant le mod√®le traiter les donn√©es d'entr√©e.

L'API d'inf√©rence qui est utilis√©e par le *widget* est √©galement disponible en tant que produit payant si vous avez besoin de l'API pour votre travail. Consultez la [page des prix](https://huggingface.co/pricing) pour plus de d√©tails.

## Remplacement des mots manquants

Le prochain pipeline que vous allez essayer est celui de `fill-mask`. L'id√©e de cette t√¢che est de remplir les mots manquants d'un texte donn√© :

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.', 
# Ce cours vous apprendra tout sur les mod√®les math√©matiques.
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.', 
 # Ce cours vous apprendra tout sur les mod√®les math√©matiques.
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

L'argument `top_k` permet de contr√¥ler le nombre de possibilit√©s que vous souhaitez afficher. Notez que dans ce cas, le mod√®le remplace le mot sp√©cial `<mask>`, qui est souvent appel√© un *mot masqu√©*. D'autres mod√®les permettant de remplacer les mots manquants peuvent avoir des mots masqu√©s diff√©rents, donc il est toujours bon de v√©rifier le mot masqu√© appropri√© lorsque vous comparez d'autres mod√®les. Une fa√ßon de le v√©rifier est de regarder le mot masqu√© utilis√© dans l'outil de test de la page du mod√®le.

<Tip>

‚úèÔ∏è **Essayez !** Recherchez le mod√®le `bert-base-cased` sur le *Hub* et identifiez le mot masqu√© dans l'outil d'inf√©rence. Que pr√©dit le mod√®le pour la phrase dans notre exemple de pipeline au-dessus ?

</Tip>

## Reconnaissance d'entit√©s nomm√©es

La reconnaissance d'entit√©s nomm√©es ou NER (pour *Named Entity Recognition*) est une t√¢che o√π le mod√®le doit trouver les parties du texte d'entr√©e qui correspondent √† des entit√©s telles que des personnes, des lieux ou des organisations. Voyons un exemple :

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner(
    "My name is Sylvain and I work at Hugging Face in Brooklyn."
)  # Je m'appelle Sylvain et je travaille √† Hugging Face √† Brooklyn.
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

Nous pouvons voir que le mod√®le a correctement identifi√© Sylvain comme une personne (PER), Hugging Face comme une organisation (ORG) et Brooklyn comme un lieu (LOC).

Il est possible d'utiliser l'option `grouped_entities=True` lors de la cr√©ation du pipeline pour regrouper les parties du texte qui correspondent √† la m√™me entit√© : ici le mod√®le √† correctement regroup√© `Hugging` et `Face` comme une seule organisation, m√™me si le nom comporte plusieurs mots. En effet, comme nous allons voir dans le prochain chapitre, la pr√©traitement du texte s√©pare parfois certains mots en plus petites parties. Par exemple, `Sylvain` est s√©par√© en quatre morceaux : `S`, `##yl`, `##va`, et `##in`. Dans l'√©tape de post-traitement, le pipeline a r√©ussi √† regrouper ces morceaux.

<Tip>

‚úèÔ∏è **Essayez !** Recherchez sur le *Hub* un mod√®le capable de reconna√Ætre les diff√©rentes parties du langage (g√©n√©ralement abr√©g√© en POS pour *Part-of-speech*) en anglais. Que pr√©dit le mod√®le pour la phrase dans notre exemple du pipeline au-dessus ?

</Tip>

## R√©ponse √† des questions

Le pipeline `question-answering` r√©pond √† des questions en utilisant des informations donn√©es en contexte :

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",  # O√π est-ce que je travaille ?
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
    # Je m'appelle Sylvain et je travaille √† Hugging Face √† Brooklyn.
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Notez que ce pipeline fonctionne par extraction d'information depuis le contexte fourni, il ne g√©n√®re pas la r√©ponse.

## R√©sum√©

Le r√©sum√© est une t√¢che de r√©duction d'un texte en un texte plus court, tout en gardant tous (ou presque tous) les aspects importants r√©f√©renc√©s dans le texte. Voici un exemple :

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)

"""
    L'Am√©rique a chang√© de fa√ßon spectaculaire au cours des derni√®res ann√©es. Non seulement le nombre de 
    dipl√¥m√©s dans les disciplines traditionnelles de l'ing√©nierie telles que le g√©nie m√©canique, civil, 
    l'√©lectricit√©, la chimie et l'a√©ronautique a diminu√©, mais dans la plupart 
    des grandes universit√©s am√©ricaines, les programmes d'√©tudes d'ing√©nierie se concentrent d√©sormais sur 
    et encouragent largement l'√©tude des sciences de l'ing√©nieur. Par cons√©quent, il y a 
    de moins en moins d'offres dans les sujets d'ing√©nierie traitant de l'infrastructure, 
    l'environnement et les questions connexes, et une plus grande concentration sur les sujets de haute 
    technologie, qui soutiennent en grande partie des d√©veloppements scientifiques de plus en plus 
    complexes. Si cette derni√®re est importante, elle ne doit pas se faire au d√©triment
    de l'ing√©nierie plus traditionnelle.

    Les √©conomies en d√©veloppement rapide telles que la Chine et l'Inde, ainsi que d'autres 
    pays industrialis√©s d'Europe et d'Asie, continuent d'encourager et de promouvoir
    l'enseignement de l'ing√©nierie. La Chine et l'Inde, respectivement, dipl√¥ment 
    six et huit fois plus d'ing√©nieurs traditionnels que les √âtats-Unis. 
    Les autres pays industriels maintiennent au minimum leur production, tandis que l'Am√©rique 
    souffre d'une baisse de plus en plus importante du nombre de dipl√¥m√©s en ing√©nierie
    et un manque d'ing√©nieurs bien form√©s.
"""
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The ' 
				  # L'Am√©rique a chang√© de fa√ßon spectaculaire au cours des derni√®res ann√©es. Le
                  'number of engineering graduates in the U.S. has declined in ' 
				  # nombre de dipl√¥m√©s en ing√©nierie aux √âtats-Unis a diminu√© dans
                  'traditional engineering disciplines such as mechanical, civil ' 
				  # dans les disciplines traditionnelles de l'ing√©nierie, telles que le g√©nie m√©canique, civil
                  ', electrical, chemical, and aeronautical engineering . Rapidly ' 
				  # l'√©lectricit√©, la chimie et l'a√©ronautique. Les √©conomies
                  'developing economies such as China and India, as well as other ' 
				  # en d√©veloppement rapide comme la Chine et l'Inde, ainsi que d'autres
                  'industrial countries in Europe and Asia, continue to encourage ' 
				  # pays industriels d'Europe et d'Asie, continuent d'encourager
                  'and advance engineering.'}] 
				  # et √† faire progresser l'ing√©nierie.
```

Comme pour la g√©n√©ration de texte, vous pouvez sp√©cifier une `max_length` (longueur maximale) ou une `min_length` (longueur minimale) pour le r√©sultat.


## Traduction

Pour la traduction, vous pouvez utiliser un mod√®le par d√©faut si vous fournissez un couple de langues dans le nom de la t√¢che (comme `"translation_en_to_fr"`), mais le plus simple reste d'utiliser un mod√®le ad√©quat disponible sur le [*Hub*](https://huggingface.co/models). Ici, nous allons essayer de traduire du fran√ßais en anglais :

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Comme pour la g√©n√©ration de texte et le r√©sum√© de texte, il est possible de sp√©cifier une `max_length` (longueur maximale) ou une `min_length` (longueur minimale) pour le r√©sultat.

<Tip>

‚úèÔ∏è **Essayez !** Recherchez d'autres mod√®les de traduction sur le Hub et essayez de traduire la phrase pr√©c√©dente en plusieurs langues diff√©rentes.

</Tip>

Les pipelines pr√©sent√©s jusqu'ici sont principalement destin√©s √† des fins de d√©monstration. Ils ont √©t√© programm√©s pour des t√¢ches sp√©cifiques et ne peuvent pas effectuer de variations de celles-ci. Dans le chapitre suivant, vous apprendrez ce qu'il y a dans un `pipeline()` et comment modifier son comportement.
