# Que peuvent faire les mod√®les Transformers ?

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb"},
]} />

Dans cette section, nous allons voir ce que peuvent faire les mod√®les Transformers et utiliser notre premier outil de la librairie ü§ó Transformers: la fonction `pipeline()`.

<Tip>
üëÄ Vous voyez ce bouton <em>Open in Colab</em> en haut √† droite ? Cliquez dessus pour ouvrir un notebook Colab avec tous les exemples de code de cette section. Ce bouton sera pr√©sent dans n'importe quelle section contenant des exemples de code.

Si vous souhaitez ex√©cuter les exemples de code en local, nous vous recommandons de jetez un oeil √†: <a href="/course/chapter0">configuration</a>.
</Tip>

## Les mod√®les Transformers sont partout !

Les mod√®les Transformers sont utilis√©s pour r√©soudre toute sorte de t√¢ches de NLP, comme celles mentionn√©es dans la section pr√©c√©dente. Voici quelques-unes des entreprises et organisations qui utilisent Hugging Face et les mod√®les Transformers, et qui contribuent aussi √† la communaut√© en partageant leurs mod√®les :

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Companies using Hugging Face" width="100%">

La librairie [ü§ó Transformers](https://github.com/huggingface/transformers) fournit toutes les fonctionnalit√©s n√©cessaires pour cr√©er et utiliser les mod√®les partag√©s. Le [Model Hub](https://huggingface.co/models) contient des milliers de mod√®les pr√©-entra√Æn√©s que n'importe qui peut t√©l√©charger et utiliser. Vous pouvez √©galement transf√©rer vos propres mod√®les vers le Hub !

<Tip>
‚ö†Ô∏è Le Hub Hugging Face n'est pas limit√© aux mod√®les Transformers. Tout le monde peut partager n'importe quel mod√®le ou jeu de donn√©es s'il le souhaite ! <a href="https://huggingface.co/join">Cr√©ez un compte sur huggingface.co</a> pour b√©n√©ficier de toutes les fonctionnalit√©s disponibles !
</Tip>

Avant de d√©couvrir en d√©tail comment les mod√®les Transformers fonctionnent, nous allons voir quelques exemples de comment ils peuvent √™tre utilis√©s pour r√©soudre des probl√®mes NLP int√©ressants.

## Travailler avec des pipelines

<Youtube id="tiZFewofSLM" />

L'outil le plus basique de la librairie ü§ó Transformers est la fonction `pipeline()`. Elle relie un mod√®le avec ses √©tapes de pr√©-traitement et de post-traitement, permettant d'entrer n'importe quel texte et d'obtenir une r√©ponse compr√©hensible :

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

On peut m√™me passer plusieurs phrases !

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Par d√©faut, ce pipeline s√©lectionne un mod√®le pr√©-entra√Æn√© qui a √©t√© sp√©cifiquement entra√Æn√© pour l'analyse de sentiment en anglais. Le mod√®le est t√©l√©charg√© et mis en cache lorsque vous cr√©ez l'objet `classifier`. Si vous r√©ex√©cutez la commande, c'est le mod√®le mis en cache qui sera utilis√© et il n'y a pas besoin de t√©l√©charger le mod√®le √† nouveau.

Il y a trois √©tapes principales lorsque vous passez du texte √† un pipeline :

1. Le texte est pr√©-trait√© pour qu'il ait un format compr√©hensible par le mod√®le.
2. Les donn√©es pr√©-trait√©es sont pass√©es au mod√®le.
3. Les pr√©dictions du mod√®le sont post-trait√©s, de sorte que vous puissiez les comprendre.


Voici une liste non-exhaustive des [pipelines disponibles](https://huggingface.co/transformers/main_classes/pipelines.html) :

- `feature-extraction` (pour obtenir la repr√©sentation vectorielle d'un texte)
- `fill-mask`
- `ner` (named entity recognition = reconnaissance des entit√©s nomm√©es)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

Regardons de plus pr√®s certains d'entre-eux !

## Zero-shot classification

Nous allons commencer par nous attaquer √† une t√¢che plus difficile o√π nous devons classer des textes qui n'ont pas √©t√© annot√©s. C'est un sc√©nario tr√®s r√©pandu dans les projets r√©els car l'annotation de texte est g√©n√©ralement longue et n√©cessite parfois une expertise dans un domaine. Pour ce cas d'usage, le pipeline `zero-shot-classification` est tr√®s puissant : il vous permet de sp√©cifier les labels √† utiliser pour la classification, de sorte que vous n'ayez pas √† vous soucier des labels du mod√®le pr√©-entra√Æn√©. Nous avons d√©j√† vu comment le mod√®le peut classer un texte comme positif ou n√©gatif en utilisant ces deux labels, mais il peut √©galement classer le texte en utilisant n'importe quel autre ensemble de labels que vous souhaitez.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Ce pipeline est appel√© _zero-shot_ car vous n'avez pas besoin d'entra√Æner sp√©cifiquement le mod√®le sur vos donn√©es pour l'utiliser. Il peut directement retourner des scores de probabilit√© pour n'importe quel ensemble de labels que vous choisissez !

<Tip>

‚úèÔ∏è **Essayez-le !** Jouez avec vos propres s√©quences et labels et voyez comment le mod√®le fonctionne.

</Tip>


## G√©n√©ration de texte

Maintenant, nous allons voir comment utiliser un pipeline pour g√©n√©rer du texte. L'id√©e principale ici est que vous fournissez seulement un extrait de texte qui va √™tre compl√©t√© par du texte g√©n√©r√© automatiquement par le mod√®le. C'est similaire √† la fonctionnalit√© de pr√©diction de texte qui est tr√®s pr√©sente dans les appareils mobiles. La g√©n√©ration de texte implique de l'al√©atoire, donc il est normal que vous n'obteniez pas les m√™mes r√©sultats que ceux pr√©sent√©s ci-dessous.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows ‚Äî data flows of various types, as seen by the '
                    'HTTP'}]
```

Il est possible de contr√¥ler le nombre de s√©quences g√©n√©r√©es avec l'argument `num_return_sequences` et la longueur totale du texte g√©n√©r√© avec l'argument `max_length`.

<Tip>

‚úèÔ∏è **Essayez-le !** Utilisez les arguments `num_return_sequences` et `max_length` pour g√©n√©rer deux phrases de 15 mots chacune.

</Tip>


## Utiliser n'importe quel mod√®le du Hub dans un pipeline

Les exemples pr√©c√©dents utilisaient le mod√®le par d√©faut pour la t√¢che en question, mais vous pouvez aussi choisir un mod√®le particulier du Hub et l'utiliser dans un pipeline pour une t√¢che sp√©cifique ‚Äî par exemple, la g√©n√©ration de texte. Rendez-vous sur la [page du Hub de mod√®les](https://huggingface.co/models?pipeline_tag=text-generation) et cliquez sur le tag correspondant sur la gauche pour afficher seulement les mod√®les support√©s pour cette t√¢che. Vous devriez arriver sur une page comme [celle-ci](https://huggingface.co/models?pipeline_tag=text-generation).

Essayons le mod√®le [`distilgpt2`](https://huggingface.co/distilgpt2) ! Voici comment charger le mod√®le dans le m√™me pipeline que pr√©c√©demment :

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

Vous pouvez am√©liorer votre recherche de mod√®le en cliquant sur les tags de langue, et choisir un mod√®le qui g√©n√®re du texte dans une autre langue. Le Hub de mod√®les contient √©galement des sauvegardes pour des mod√®les multilingues qui supportent plusieurs langues.

Une fois que vous avez choisi un mod√®le, vous verrez que vous pouvez tester son fonctionnement en ligne directement. Cela vous permet de tester rapidement les capacit√©s du mod√®le avant de le t√©l√©charger.

<Tip>

‚úèÔ∏è **Essayez-le !** Utilisez les filtres pour trouver un mod√®le de g√©n√©ration de texte pour une autre langue. N'h√©sitez pas √† jouer avec le widget et l'utiliser dans un pipeline !

</Tip>

### L'API d'inf√©rence

Tous les mod√®les peuvent √™tre test√© directement depuis votre navigateur en utilisant l'API d'inf√©rence, qui est disponible sur le site [Hugging Face](https://huggingface.co/). Vous pouvez jouer avec le mod√®le directement sur sa page en entrant du texte personnalis√© et en regardant le mod√®le traiter les donn√©es d'entr√©e.

L'API d'inf√©rence qui est utilis√©e par le widget est √©galement disponible en tant que produit payant, qui est utile si vous avez besoin de l'API pour votre travail. Consultez la [page des prix](https://huggingface.co/pricing) pour plus de d√©tails.

## Remplacement des mots manquants

Le prochain pipeline que vous allez essayer est celui de `fill-mask`. L'id√©e de cette t√¢che est de remplir les mots manquants d'un texte donn√© :

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

L'argument `top_k` permet de contr√¥ler le nombre de possibilit√©s que vous souhaitez afficher. Notez que dans ce cas, le mod√®le remplace le mot sp√©cial `<mask>`, qui est souvent appel√© un *mot masqu√©*. D'autres mod√®les permettant de remplacer les mots manquants peuvent avoir des mots masqu√©s diff√©rents, donc il est toujours bon de v√©rifier le mot masqu√© appropri√© lorsque vous comparez d'autres mod√®les. Une fa√ßon de le v√©rifier est de regarder le mot masqu√© utilis√© dans l'outil de test de la page du mod√®le.

<Tip>

‚úèÔ∏è **Essayez-le !** Recherchez le mod√®le `bert-base-cased` sur le Hub et identifiez le mot masqu√© dans l'outil d'inf√©rence. Que pr√©dit le mod√®le pour la phrase dans notre exemple de pipeline au-dessus ?

</Tip>

## Reconnaissance d'entit√©s nomm√©es

La reconnaissance d'entit√©s nomm√©es (NER = Named Entity Recognition) est une t√¢che o√π le mod√®le doit trouver les parties du texte d'entr√©e qui correspondent √† des entit√©s telles que des personnes, des lieux ou des organisations. Voyons un exemple :

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

Nous pouvons voir que le mod√®le a correctement identifi√© Sylvain comme une personne (PER), Hugging Face comme une organisation (ORG) et Brooklyn comme un lieu (LOC).

Il est possible d'utiliser l'option `grouped_entities=True` lors de la cr√©ation du pipeline pour regrouper les parties du texte qui correspondent √† la m√™me entit√©: ici le mod√®le √† correctement regroup√© "Hugging" et "Face" comme une seule organisation, m√™me si le nom comporte plusieurs mots. En effet, comme nous allons voir dans la prochaine chapitre, la pr√©traitement du texte s√©pare parfois certains mots en plus petites parties. Par exemple, `Sylvain` est s√©par√© en quatre morceaux: `S`, `##yl`, `##va`, et `##in`. Dans l'√©tape de post-traitement, le pipeline a r√©ussi √† regrouper ces morceaux.

<Tip>

‚úèÔ∏è **Essayez-le !** Recherchez sur le Hub un mod√®le capable de reconna√Ætre les diff√©rentes parties du langage (Part-of-speech = POS) en Anglais. Que pr√©dit le mod√®le pour la phrase dans notre exemple du pipeline au-dessus ?

</Tip>

## R√©ponse √† des questions

Le pipeline `question-answering` r√©pond √† des questions en utilisant des informations donn√©es en contexte :

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Notez que ce pipeline fonctionne par extraction d'information depuis le contexte fourni, il ne g√©n√®re pas la r√©ponse.

## R√©sum√©

Le r√©sum√© est une t√¢che de r√©duction d'un texte en un texte plus court, tout en gardant tous (ou presque tous) les aspects importants r√©f√©renc√©s dans le texte. Voici un exemple :

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

Comme pour la g√©n√©ration de texte, vous pouvez sp√©cifier une `max_length` (longueur maximale) ou une `min_length` (longueur minimale) pour le r√©sultat.


## Traduction

Pour la traduction, vous pouvez utiliser un mod√®le par d√©faut si vous fournissez un couple de langues dans le nom de la t√¢che (comme `"translation_en_to_fr"`), mais le plus simple reste d'utiliser un mod√®le ad√©quat disponible sur le [Model Hub](https://huggingface.co/models). Ici, nous allons essayer de traduire du fran√ßais en anglais :

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Comme pour la g√©n√©ration de texte et le r√©sum√© de texte, il est possible de sp√©cifier une `max_length` (longueur maximale) ou une `min_length` (longueur minimale) pour le r√©sultat.

<Tip>

‚úèÔ∏è **Essayez-le !** Recherchez d'autres mod√®les de traduction sur le Hub et essayez de traduire la phrase pr√©c√©dente en plusieurs langues diff√©rentes.

</Tip>

Les pipelines pr√©sent√©s jusqu'ici sont principalement destin√©s √† des fins de d√©monstration. Ils ont √©t√© programm√©s pour des t√¢ches sp√©cifiques et ne peuvent pas effectuer de variations de celles-ci. Dans le chapitre suivant, vous apprendrez ce qu'il y a dans un `pipeline()` et comment modifier son comportement.
