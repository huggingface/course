# R√©sum√© du chapitre

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

Au cours de ce chapitre, vous avez vu comment approcher diff√©rents probl√®mes de NLP en utilisant la fonction `pipeline()` de la biblioth√®que ü§ó *Transformers*. Vous avez √©galement vu comment rechercher et utiliser des mod√®les dans le *Hub* ainsi que comment utiliser l'API d'inf√©rence pour tester les mod√®les directement dans votre navigateur.

Nous avons pu aborder le fonctionnement des *transformers* de fa√ßon g√©n√©rale et parler de l'importance de l'apprentissage par transfert et du *finetuning*. Un point important est que vous pouvez utiliser l'architecture compl√®te ou seulement l'encodeur ou le d√©codeur, selon le type de t√¢che que vous souhaitez r√©soudre. Le tableau suivant r√©sume ceci :

| Mod√®le            | Exemples                                   | T√¢ches                                                                                       |
|-------------------|--------------------------------------------|----------------------------------------------------------------------------------------------|
| Encodeur          | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Classification de phrase, reconnaissance d'entit√©s nomm√©es, extraction de question-r√©ponse |
| D√©codeur          | CTRL, GPT, GPT-2, Transformer XL           | G√©n√©ration de texte                                                                          |
| Encodeur-d√©codeur | BART, T5, Marian, mBART                    | R√©sum√©, traduction, g√©n√©ration de question-r√©ponse                                           |