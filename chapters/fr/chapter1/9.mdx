# R√©sum√© du chapitre

Au cours de ce chapitre, vous avez vu comment approcher diff√©rents probl√®mes de NLP en utilisant la fonction `pipeline()` de la librairie ü§ó Transformers. Vous avez aussi vu comment rechercher et utiliser des mod√®les dans le Hub, ainsi que comment utiliser l'API d'inf√©rence pour tester les mod√®les directement dans votre navigateur.

Nous avons pu aborder le fonctionnement des mod√®les Transformers de fa√ßon g√©n√©rale, et nous avons parl√© de l'importance du transfer learning et du fine-tuning. Un aspect important est que vous pouvez utiliser l'architecture compl√®te ou seulement l'encodeur ou le d√©codeur, selon le type de t√¢che que vous souhaitez r√©soudre. Le tableau suivante r√©sume ceci :

| Mod√®le            | Exemples                                   | T√¢ches                                                                                       |
|-------------------|--------------------------------------------|----------------------------------------------------------------------------------------------|
| Encodeur          | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Classification de phrase, reconnaissance des entit√©s nomm√©es, extraction de question-r√©ponse |
| D√©codeur          | CTRL, GPT, GPT-2, Transformer XL           | G√©n√©ration de texte                                                                          |
| Encodeur-d√©codeur | BART, T5, Marian, mBART                    | R√©sum√©, traduction, g√©n√©ration de question-r√©ponse                                           |
