# Les modèles de séquence-à-séquence

<Youtube id="0_4KEb08xrE" />

Les modèles encodeur-décodeur (également appelés modèles de séquence-à-séquence) utilisent les deux parties de l'architecture Transformer. À chaque étape, les couches d'attention de l'encodeur peuvent accéder à tous les mots de la phrase initiale, tandis que les couches d'attention du décodeur n'ont accès qu'aux mots positionnés avant un mot donné en entrée de ces couches d'attention.

Le pré-entraînement de ces modèles peut être fait en utilisant les objectifs des modèles d'encodeur ou de décodeur, mais en général cela implique quelque chose de plus complexe. Par exemple, le modèle [T5](https://huggingface.co/t5-base) est pré-entraîné en remplaçant des zones aléatoires de texte (qui peuvent contenir plusieurs mots) par un mot-masque spécial, et l'objectif est alors de prédire le texte que ce mot-masque remplace.

Les modèles de séquence-à-séquence sont les plus adaptés pour les tâches liées à la génération de nouvelles phrases en fonction d'une entrée donnée, comme le résumé de texte, la traduction ou la génération de question-réponse.

Les modèles qui représentent le mieux cette famille sont:

- [BART](https://huggingface.co/transformers/model_doc/bart.html)
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)
- [T5](https://huggingface.co/transformers/model_doc/t5.html)
