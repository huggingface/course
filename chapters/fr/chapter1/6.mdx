# Les modèles décodeurs

<Youtube id="d_ixlCubqQw" />

Les modèles décodeurs utilisent seulement le décodeur d'un modèle Transformer. À chaque étape, pour un mot donné, les couches d'attention ne peuvent strictement accéder qu'aux mots situés avant dans la phrase. Ces modèles sont souvent appelés *modèles auto-régressifs*.

Le pré-entraînement des modèles décodeurs se concentre généralement sur la prédiction du prochain mot dans la phrase.

Ces modèles sont vraiment adaptés aux tâches qui impliquent la génération de texte.

Les modèles qui représentent le mieux la famille des modèles décodeurs sont :

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)
- [GPT](https://huggingface.co/transformers/model_doc/gpt.html)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)
