# Biais et limitations

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter1/section8.ipynb"},
]} />

Si vous souhaitez utiliser un modèle pré-entraîné ou une version fine-tunée de celui-ci en production, il est important d'avoir conscience que, bien que ces modèles soient puissants, ils ont des limites. La plus importante de ces limitations est que, pour permettre le pré-entraînement des modèles sur de grandes quantités de données, les chercheurs récupèrent souvent tout le contenu qu'ils peuvent trouver, en prenant le meilleur et le pire de ce qui est disponible sur internet.

Pour illustrer cela rapidement, revenons au modèle de remplacement de mot-masque avec le modèle BERT:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

Lorsque l'on demande au modèle de remplacer le mot manquant dans ces deux phrases, il ne propose qu'un seul métier ne portant pas la marque du genre (waiter/waitress = serveur/serveuse). Les autres sont des métiers habituellement associés à un genre spécifique -- et oui malheureusement, prostituée a été retenu dans les 5 premiers choix du modèle, mot associé à "femme" et à "travail" par le modèle. Cela se produit même si BERT est l'un des rare modèles Transformer qui n'a pas été construit avec des données récupérées par scrapping sur internet, mais à l'aide de données en apparence neutres (il est entraîné sur les jeux de données suivants : [Wikipédia Anglais](https://huggingface.co/datasets/wikipedia) et [BookCorpus](https://huggingface.co/datasets/bookcorpus)).

Donc lorsque vous utilisez ce genre d'outils, il est important de garder en tête que le modèle que vous utilisez peut rapidement générer du contenu sexiste, raciste ou homophobe. Le fine-tuning de ce modèle sur vos données ne fera en aucun cas disparaître ce biais intrinsèque.
