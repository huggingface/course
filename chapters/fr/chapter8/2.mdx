# Que faire lorsque vous obtenez une erreur

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter8/section2.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter8/section2.ipynb"},
]} />

Dans cette section, nous allons examiner certaines erreurs courantes qui peuvent se produire lorsque vous essayez de g√©n√©rer des pr√©dictions √† partir de votre *transformer* fra√Æchement *finetun√©*. Cela vous pr√©parera pour la [section 4](/course/chapter8/fr/section4) de ce chapitre o√π nous explorerons comment d√©boguer la phase d'entra√Ænement elle-m√™me.

<Youtube id="DQ-CpJn6Rc4"/>

Nous avons pr√©par√© un gabarit de [d√©p√¥t de mod√®les](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) pour cette section et si vous voulez ex√©cuter le code de ce chapitre, vous devrez d'abord copier le mod√®le dans votre compte sur le [*Hub* d'Hugging Face](https://huggingface.co). Pour ce faire, connectez-vous d'abord en ex√©cutant l'une ou l'autre des commandes suivantes dans un *notebook* Jupyter :

```python
from huggingface_hub import notebook_login

notebook_login()
```

ou ce qui suit dans votre terminal pr√©f√©r√© :

```bash
huggingface-cli login
```

Cela vous demandera d'entrer votre nom d'utilisateur et votre mot de passe, et enregistrera un jeton sous *~/.cache/huggingface/*. Une fois que vous vous √™tes connect√©, vous pouvez copier le gabarit du d√©p√¥t avec la fonction suivante :

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Cloner le d√©p√¥t et extraire le chemin local
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Cr√©er un d√©p√¥t vide sur le Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Cloner le d√©p√¥t vide
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copier les fichiers
    copy_tree(template_repo_dir, new_repo_dir)
    # Pousser sur le Hub
    repo.push_to_hub()
```

Maintenant, lorsque vous appelez `copy_repository_template()`, cela va cr√©er une copie du gabarit du d√©p√¥t sous votre compte.

## D√©boguer le pipeline √† partir de ü§ó <i>Transformers</i>

Pour donner le coup d'envoi de notre voyage dans le monde merveilleux du d√©bogage de *transformers*, consid√©rez le sc√©nario suivant : vous travaillez avec un coll√®gue sur un projet de r√©ponse √† des questions pour aider les clients d'un site de commerce en ligne √† trouver des r√©ponses √† des produits. Votre coll√®gue vous envoie un message du genre :

> Bonjour ! Je viens de r√©aliser une exp√©rience en utilisant les techniques du [chapitre 7](/course/fr/chapiter7/7) du cours d'Hugging Face et j'ai obtenu d'excellents r√©sultats sur SQuAD ! Je pense que nous pouvons utiliser ce mod√®le comme point de d√©part pour notre projet. L'identifiant du mod√®le sur le *Hub* est "lewtun/distillbert-base-uncased-finetuned-squad-d5716d28". N'h√©site pas √† le tester :)

et la premi√®re chose √† laquelle on pense est de charger le mod√®le en utilisant le `pipeline` de ü§ó *Transformers* :

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Oh non, quelque chose semble s'√™tre mal pass√©e ! Si vous √™tes novice en programmation, ce genre d'erreurs peut sembler un peu cryptique au d√©but (qu'est-ce qu'une `OSError` ?!). L'erreur affich√©e ici n'est que la derni√®re partie d'un rapport d'erreur beaucoup plus long appel√© _Python traceback_ (alias *stack trace*). Par exemple, si vous ex√©cutez ce code sur Google Colab, vous devriez voir quelque chose comme la capture d'√©cran suivante :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png" alt="A Python traceback." width="100%"/>
</div>

Il y a beaucoup d'informations dans ces rapports, nous allons donc en parcourir ensemble les √©l√©ments cl√©s. La premi√®re chose √† noter est que les *tracebacks* doivent √™tre lus _de bas en haut_. Cela peut sembler bizarre si vous avez l'habitude de lire du texte fran√ßais de haut en bas mais cela refl√®te le fait que le *traceback* montre la s√©quence d'appels de fonction que le `pipeline` fait lors du t√©l√©chargement du mod√®le et du *tokenizer*. Consultez le [chapitre 2](/course/fr/chapter2) pour plus de d√©tails sur la fa√ßon dont le `pipeline` fonctionne sous le capot.

<Tip>

üö® Vous voyez le cadre bleu autour de ¬´ <i>6 frames</i> ¬ª dans le <i>traceback</i> de Google Colab ? Il s'agit d'une fonctionnalit√© sp√©ciale de Colab qui compresse le <i>traceback</i> en <i>frames</i>. Si vous ne parvenez pas √† trouver la source d'une erreur, d√©roulez le <i>traceback</i> en cliquant sur ces deux petites fl√®ches.

</Tip>

Cela signifie que la derni√®re ligne du <i>traceback</i> indique le dernier message d'erreur et donne le nom de l'exception qui a √©t√© lev√©e. Dans ce cas, le type d'exception est `OSError`, ce qui indique une erreur li√©e au syst√®me. Si nous lisons le message d'erreur qui l'accompagne, nous pouvons voir qu'il semble y avoir un probl√®me avec le fichier *config.json* du mod√®le et deux suggestions nous sont donn√©es pour le r√©soudre :

```python out
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

<Tip>

üí° Si vous rencontrez un message d'erreur difficile √† comprendre, copiez et collez le message dans Google ou sur [Stack Overflow](https://stackoverflow.com/) (oui, vraiment !). Il y a de fortes chances que vous ne soyez pas la premi√®re personne √† rencontrer cette erreur et c'est un bon moyen de trouver des solutions que d'autres membres de la communaut√© ont publi√©es. Par exemple, en recherchant `OSError : Can't load config for` sur Stack Overflow donne plusieurs [r√©ponses](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+) qui peuvent √™tre utilis√©es comme point de d√©part pour r√©soudre le probl√®me.

</Tip>

La premi√®re suggestion nous demande de v√©rifier si l'identifiant du mod√®le est effectivement correct, la premi√®re chose √† faire est donc de copier l'identifiant et de le coller dans la barre de recherche du *Hub* :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png" alt="The wrong model name." width="100%"/>
</div>

Hmm, il semble en effet que le mod√®le de notre coll√®gue ne soit pas sur le *Hub*... Mais il y a une faute de frappe dans le nom du mod√®le ! DistilBERT n'a qu'un seul ¬´ l ¬ª dans son nom alors corrigeons cela et cherchons ¬´ lewtun/distilbert-base-uncased-finetuned-squad-d5716d28 ¬ª √† la place :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png" alt="The right model name." width="100%"/>
</div>

Ok, √ßa a march√©. Maintenant essayons de t√©l√©charger √† nouveau le mod√®le avec le bon identifiant :

```python
model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

Argh, encore un √©chec. Bienvenue dans la vie quotidienne d'un ing√©nieur en apprentissage machine ! Puisque nous avons corrig√© l'identifiant du mod√®le, le probl√®me doit se situer dans le d√©p√¥t lui-m√™me. Une fa√ßon rapide d'acc√©der au contenu d'un d√©p√¥t sur le ü§ó *Hub* est via la fonction `list_repo_files()` de la biblioth√®que `huggingface_hub` :

```python
from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)
```

```python out
['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']
```

Int√©ressant. Il ne semble pas y avoir de fichier *config.json* dans le d√©p√¥t ! Pas √©tonnant que notre `pipeline` n'ait pas pu charger le mod√®le. Notre coll√®gue a d√ª oublier de pousser ce fichier vers le *Hub* apr√®s l'avoir *finetun√©*. Dans ce cas, le probl√®me semble assez simple √† r√©soudre : nous pouvons lui demander d'ajouter le fichier, ou, puisque nous pouvons voir √† partir de l'identifiant du mod√®le que le mod√®le pr√©-entra√Æn√© utilis√© est [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased), nous pouvons t√©l√©charger la configuration de ce mod√®le et la pousser dans notre d√©p√¥t pour voir si cela r√©sout le probl√®me. Essayons cela. En utilisant les techniques apprises dans le [chapitre 2](/course/fr/chapter2), nous pouvons t√©l√©charger la configuration du mod√®le avec la classe `AutoConfig` :

```python
from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)
```

<Tip warning={true}>

üö® L'approche que nous adoptons ici n'est pas infaillible puisque notre coll√®gue peut avoir modifi√© la configuration de `distilbert-base-uncased` avant de <i>finetuner</i> le mod√®le. Dans la vie r√©elle, nous voudrions v√©rifier avec lui d'abord, mais pour les besoins de cette section nous supposerons qu'il a utilis√© la configuration par d√©faut.

</Tip>

Nous pouvons ensuite le pousser vers notre d√©p√¥t de mod√®les avec la fonction `push_to_hub()` de la configuration :

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

Maintenant, nous pouvons tester si cela a fonctionn√© en chargeant le mod√®le depuis le dernier *commit* de la branche `main` :

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

context_fr = r"""
La r√©ponse √† des questions consiste √† extraire une r√©ponse d'un texte
√† partir d'une question. Un exemple de jeu de donn√©es de r√©ponse aux questions est le 
jeu de donn√©es SQuAD qui est enti√®rement bas√© sur cette t√¢che. Si vous souhaitez finetuner
un mod√®le sur une t√¢che SQuAD, vous pouvez utiliser le fichier
exemples/pytorch/question-answering/run_squad.py.

ü§ó Transformers est interop√©rable avec les frameworks PyTorch, TensorFlow et JAX.
de sorte que vous pouvez utiliser vos outils pr√©f√©r√©s pour une grande vari√©t√© de t√¢ches !
"""

question = "What is extractive question answering?"
# Qu'est-ce que la r√©ponse extractive aux questions ?
reader(question=question, context=context)
```

```python out
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'} 
 # la t√¢che consistant √† extraire une r√©ponse d'un texte √† partir d'une question.
```

Woohoo, √ßa a march√© ! R√©capitulons ce que vous venez d'apprendre :

- les messages d'erreur en Python sont appel√©s _tracebacks_ et sont lus de bas en haut. La derni√®re ligne du message d'erreur contient g√©n√©ralement les informations dont vous avez besoin pour localiser la source du probl√®me,
- si la derni√®re ligne ne contient pas suffisamment d'informations, remontez dans le *traceback* et voyez si vous pouvez identifier o√π l'erreur se produit dans le code source,
- si aucun des messages d'erreur ne peut vous aider √† d√©boguer le probl√®me, essayez de rechercher en ligne une solution √† un probl√®me similaire,
- l'`huggingface_hub` fournit une suite d'outils que vous pouvez utiliser pour interagir avec et d√©boguer les d√©p√¥ts sur le *Hub*.

Maintenant que vous savez comment d√©boguer un pipeline, examinons un exemple plus d√©licat dans la passe avant du mod√®le lui-m√™me.

## D√©boguer la passe avant de votre mod√®le

Bien que le `pipeline` soit parfait pour la plupart des applications o√π vous devez g√©n√©rer rapidement des pr√©dictions, vous aurez parfois besoin d'acc√©der aux logits du mod√®le (par exemple si vous avez un post-traitement personnalis√© que vous souhaitez appliquer). Pour voir ce qui peut mal tourner dans ce cas, commen√ßons par r√©cup√©rer le mod√®le et le *tokenizer* de notre `pipeline` :

```python
tokenizer = reader.tokenizer
model = reader.model
```

Ensuite, nous avons besoin d'une question, alors voyons si nos *frameworks* pr√©f√©r√©s sont support√©s :

```python
question = "Which frameworks can I use?"  # Quel frameworks puis-je utiliser ?
```

Comme nous l'avons vu dans le [chapitre 7](/course/fr/chapter7), les √©tapes habituelles que nous devons suivre sont la tok√©nisation des entr√©es, l'extraction des logits des *tokens* de d√©but et de fin, puis le d√©codage de l'√©tendue de la r√©ponse :

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Pour obtenir le d√©but de r√©ponse le plus probable avec l'argmax du score
answer_start = torch.argmax(answer_start_scores)
# Pour obtenir la fin de r√©ponse la plus probable avec l'argmax du score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""
```

Il semble que nous ayons un *bug* dans notre code ! Mais il ne nous fait pas peur. Nous pouvons utiliser le d√©bogueur Python dans un *notebook* :

<Youtube id="rSPyvPw0p9k"/>

ou dans un terminal :

<Youtube id="5PkZ4rbHL6c"/>

Ici, la lecture du message d'erreur nous indique que l'objet `'list' n'a pas d'attribut 'size'`, et nous pouvons voir une fl√®che `-->` pointant vers la ligne o√π le probl√®me a √©t√© soulev√© dans `model(**inputs)`. Vous pouvez d√©boguer ceci de mani√®re interactive en utilisant le d√©bogueur Python, mais pour l'instant nous allons simplement imprimer une tranche de `inputs` pour voir ce que nous avons :

```python
inputs["input_ids"][:5]
```

```python out
[101, 2029, 7705, 2015, 2064]
```

Cela ressemble certainement √† une `list` ordinaire en Python mais v√©rifions le type :

```python
type(inputs["input_ids"])
```

```python out
list
```

Oui, c'est bien une `list` Python. Alors, qu'est-ce qui a mal tourn√© ? Rappelez-vous que dans le [chapitre 2](/course/fr/chapter2) nous avons vu que les classes `AutoModelForXxx` op√®rent sur des _tenseurs_ (soit dans PyTorch ou TensorFlow) et qu'une op√©ration commune est d'extraire les dimensions d'un tenseur en utilisant `Tensor.size()`. Jetons un autre coup d'oeil au *traceback* pour voir quelle ligne a d√©clench√© l'exception :

```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

Il semble que notre code ait essay√© d'appeler `input_ids.size()`, mais cela ne fonctionne clairement pas pour une `list` Python qui est juste un conteneur. Comment pouvons-nous r√©soudre ce probl√®me ? La recherche du message d'erreur sur Stack Overflow donne quelques [r√©ponses](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f) pertinentes. En cliquant sur la premi√®re, une question similaire √† la n√¥tre s'affiche, avec la r√©ponse indiqu√©e dans la capture d'√©cran ci-dessous :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png" alt="An answer from Stack Overflow." width="100%"/>
</div>

La r√©ponse recommande d'ajouter `return_tensors='pt'` au *tokenizer*, voyons donc si cela fonctionne pour nous :

```python out
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Pour obtenir le d√©but de r√©ponse le plus probable avec l'argmax du score
answer_start = torch.argmax(answer_start_scores)
# Pour obtenir la fin de r√©ponse la plus probable avec l'argmax du score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
Question: Which frameworks can I use? # Quels frameworks puis-je utiliser ?
Answer: pytorch, tensorflow, and jax # pytorch, tensorflow et jax
"""
```

Super, √ßa a march√© ! Voil√† un excellent exemple de l'utilit√© de Stack Overflow : en identifiant un probl√®me similaire, nous avons pu b√©n√©ficier de l'exp√©rience d'autres membres de la communaut√©. Cependant, une recherche de ce type ne donne pas toujours une r√©ponse pertinente. Que faire alors dans ce cas ? Heureusement, il existe une communaut√© accueillante de d√©veloppeurs sur le [forum d'Hugging Face](https://discuss.huggingface.co/) qui peut vous aider ! Dans la prochaine section, nous verrons comment r√©diger de bonnes questions sur les forums pour avoir de bonnes chances d'obtenir une r√©ponse.
