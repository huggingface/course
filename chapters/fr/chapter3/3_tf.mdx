<FrameworkSwitchCourse {fw} />

# <i>Finetuner</i> un mod√®le avec Keras

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3_tf.ipynb"},
]} />

Une fois que vous avez fait tout le travail de pr√©traitement des donn√©es dans la derni√®re section, il ne vous reste que quelques √©tapes pour entra√Æner le mod√®le. Notez, cependant, que la commande `model.fit()` s'ex√©cutera tr√®s lentement sur un CPU. Si vous n'avez pas de GPU, vous pouvez avoir acc√®s √† des GPUs ou TPUs gratuits sur [Google Colab](https://colab.research.google.com/).

Les exemples de code ci-dessous supposent que vous avez d√©j√† ex√©cut√© les exemples de la section pr√©c√©dente. Voici un bref r√©sum√© de ce dont vous avez besoin :

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### Entra√Ænement

Les mod√®les TensorFlow import√©s depuis ü§ó *Transformers* sont d√©j√† des mod√®les Keras. Voici une courte introduction √† Keras.

<Youtube id="rnTGBy2ax1c"/>

Cela signifie qu'une fois que nous disposons de nos donn√©es, tr√®s peu de travail est n√©cessaire pour commencer √† entra√Æner sur celles-ci.

<Youtube id="AUozVp78dhk"/>

Comme dans le [chapitre pr√©c√©dent](/course/fr/chapter2), nous allons utiliser la classe `TFAutoModelForSequenceClassification`, avec deux √©tiquettes : 

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Vous remarquerez que, contrairement au [chapitre 2](/course/fr/chapter2), vous obtenez un message d'avertissement apr√®s l'instanciation de ce mod√®le pr√©-entra√Æn√©. Ceci est d√ª au fait que BERT n'a pas √©t√© pr√©-entra√Æn√© √† la classification de paires de phrases, donc la t√™te du mod√®le pr√©-entra√Æn√© a √©t√© supprim√©e et une nouvelle t√™te adapt√©e √† la classification de s√©quences a √©t√© ins√©r√©e √† la place. Les messages d'avertissement indiquent que certains poids n'ont pas √©t√© utilis√©s (ceux correspondant √† la t√™te de pr√©-entra√Ænement abandonn√©e) et que d'autres ont √©t√© initialis√©s de mani√®re al√©atoire (ceux pour la nouvelle t√™te). Il conclut en vous encourageant √† entra√Æner le mod√®le, ce qui est exactement ce que nous allons faire maintenant.

Pour *finetuner* le mod√®le sur notre jeu de donn√©es, nous devons simplement `compiler()` notre mod√®le et ensuite passer nos donn√©es √† la m√©thode `fit()`. Cela va d√©marrer le processus de *finetuning* (qui devrait prendre quelques minutes sur un GPU) et rapporter la perte d'entra√Ænement au fur et √† mesure, plus la perte de validation √† la fin de chaque √©poque.

<Tip>

Notez que les mod√®les ü§ó *Transformers* ont une capacit√© sp√©ciale que la plupart des mod√®les Keras n'ont pas. Ils peuvent automatiquement utiliser une perte appropri√©e qu'ils calculent en interne. Ils utiliseront cette perte par d√©faut si vous ne d√©finissez pas un argument de perte dans `compile()`. Notez que pour utiliser la perte interne, vous devrez passer vos labels comme faisant partie de l'entr√©e, et non pas comme un label s√©par√©, ce qui est la fa√ßon normale d'utiliser les labels avec les mod√®les Keras. Vous verrez des exemples de cela dans la partie 2 du cours, o√π la d√©finition de la fonction de perte correcte peut √™tre d√©licate. Pour la classification des s√©quences, cependant, une fonction de perte standard de Keras fonctionne bien, et c'est donc ce que nous utiliserons ici.

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

Notez un pi√®ge tr√®s commun ici. Vous *pouvez* simplement passer le nom de la perte comme une cha√Æne √† Keras, mais par d√©faut Keras supposera que vous avez d√©j√† appliqu√© une fonction softmax √† vos sorties. Cependant, de nombreux mod√®les produisent les valeurs juste avant l'application de la softmax, que l'on appelle aussi les *logits*. Nous devons indiquer √† la fonction de perte que c'est ce que fait notre mod√®le, et la seule fa√ßon de le faire est de l'appeler directement, plut√¥t que par son nom avec une cha√Æne.

</Tip>


### Am√©liorer les performances d'entra√Ænement

<Youtube id="cpzq6ESSM5c"/>

Si vous essayez le code ci-dessus, il fonctionne certainement, mais vous constaterez que la perte ne diminue que lentement ou sporadiquement. La cause principale est le *taux d'apprentissage*. Comme pour la perte, lorsque nous transmettons √† Keras le nom d'un optimiseur sous forme de cha√Æne de caract√®res, Keras initialise cet optimiseur avec des valeurs par d√©faut pour tous les param√®tres, y compris le taux d'apprentissage. Cependant, nous savons depuis longtemps que les *transformers* b√©n√©ficient d'un taux d'apprentissage beaucoup plus faible que celui par d√©faut d'Adam, qui est de 1e-3, √©galement √©crit comme 10 √† la puissance -3, ou 0,001. 5e-5 (0,00005), qui est environ vingt fois inf√©rieur, est un bien meilleur point de d√©part.

En plus de r√©duire le taux d'apprentissage, nous avons une deuxi√®me astuce dans notre manche : nous pouvons r√©duire lentement le taux d'apprentissage au cours de l'entra√Ænement. Dans la litt√©rature, on parle parfois de *d√©croissance* ou d'*annulation* du taux d'apprentissage.le taux d'apprentissage. Dans Keras, la meilleure fa√ßon de le faire est d'utiliser un *planificateur du taux d'apprentissage*. Un bon planificateur √† utiliser est `PolynomialDecay`. Malgr√© son nom, avec les param√®tres par d√©faut, il diminue simplement de fa√ßon lin√©aire le taux d'apprentissage de la valeur initiale √† la valeur finale au cours de l'entra√Ænement, ce qui est exactement ce que nous voulons. Afin d'utiliser correctement un planificateur, nous devons lui dire combien de temps l'entra√Ænement va durer. Nous calculons cela comme `num_train_steps` ci-dessous.

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# Le nombre d'√©tapes d'entra√Ænement est le nombre d'√©chantillons dans l'ensemble de donn√©es, divis√© par la taille du batch puis multipli√©
# par le nombre total d'√©poques. Notez que le jeu de donn√©es tf_train_dataset est ici un lot tf.data.Dataset
# et non le jeu de donn√©es original Hugging Face Dataset, donc son len() est d√©j√† num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

La biblioth√®que ü§ó *Transformers* poss√®de √©galement une fonction `create_optimizer()` qui cr√©era un optimiseur `AdamW` avec un taux d'apprentissage d√©croissant. Il s'agit d'un raccourci pratique que vous verrez en d√©tail dans les prochaines sections du cours.

</Tip>

Nous avons maintenant notre tout nouvel optimiseur et nous pouvons essayer de nous entra√Æner avec lui. Tout d'abord, rechargeons le mod√®le pour r√©initialiser les modifications apport√©es aux poids lors de l'entra√Ænement que nous venons d'effectuer, puis nous pouvons le compiler avec le nouvel optimiseur :

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

Maintenant, on *fit* :

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

üí° Si vous voulez t√©l√©charger automatiquement votre mod√®le sur le *Hub* pendant l'entra√Ænement, vous pouvez passer un `PushToHubCallback` dans la m√©thode `model.fit()`. Nous en apprendrons davantage √† ce sujet au [chapitre 4](/course/fr/chapter4/3).

</Tip>

### Pr√©dictions du mod√®le

<Youtube id="nx10eh4CoOs"/>


Entra√Æner et regarder la perte diminuer, c'est tr√®s bien, mais que faire si l'on veut r√©ellement obtenir des r√©sultats du mod√®le entra√Æn√©, soit pour calculer des m√©triques, soit pour utiliser le mod√®le en production ? Pour ce faire, nous pouvons simplement utiliser la m√©thode `predict()`. Ceci retournera les *logits* de la t√™te de sortie du mod√®le, un par classe.

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

Nous pouvons convertir ces logits en pr√©dictions de classe du mod√®le en utilisant `argmax` pour trouver le logit le plus √©lev√©, qui correspond √† la classe la plus probable :

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

Maintenant, utilisons ces `preds` pour calculer des m√©triques ! Nous pouvons charger les m√©triques associ√©es au jeu de donn√©es MRPC aussi facilement que nous avons charg√© le jeu de donn√©es, cette fois avec la fonction `load_metric()`. L'objet retourn√© a une m√©thode `compute()` que nous pouvons utiliser pour faire le calcul de la m√©trique :

```py
from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Les r√©sultats exacts que vous obtiendrez peuvent varier, car l'initialisation al√©atoire de la t√™te du mod√®le peut modifier les m√©triques obtenues. Ici, nous pouvons voir que notre mod√®le a une pr√©cision de 85,78% sur l'ensemble de validation et un score F1 de 89,97. Ce sont les deux m√©triques utilis√©es pour √©valuer les r√©sultats sur le jeu de donn√©es MRPC pour le benchmark GLUE. Le tableau du papier de [BERT](https://arxiv.org/pdf/1810.04805.pdf) indique un score F1 de 88,9 pour le mod√®le de base. Il s'agissait du mod√®le `uncased` alors que nous utilisons actuellement le mod√®le `cased`, ce qui explique le meilleur r√©sultat.

Ceci conclut l'introduction √† le *finetuning* en utilisant l'API Keras. Un exemple d'application de cette m√©thode aux t√¢ches les plus courantes du traitement automatique des langues sera pr√©sent√© au [chapitre 7](/course/fr/chapter7). Si vous souhaitez affiner vos connaissances de l'API Keras, essayez *finetuner* un mod√®le sur le jeu de donn√©es GLUE SST-2, en utilisant le traitement des donn√©es que vous avez effectu√© dans la section 2.
