<FrameworkSwitchCourse {fw} />

# Pr√©parer les donn√©es

{#if fw === 'pt'}


<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter3/section2_pt.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter3/section2_tf.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_tf.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}

En continuant avec l'exemple du [chapitre pr√©c√©dent](/course/fr/chapter2), voici comment entra√Æner un classifieur de s√©quences sur un batch avec PyTorch :

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# M√™me chose que pr√©c√©demment
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "This course is amazing!",  # Ce cours est incroyable !
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Ceci est nouveau
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}

En continuant avec l'exemple du [chapitre pr√©c√©dent](/course/fr/chapter2), voici comment entra√Æner un classifieur de s√©quences sur un batch avec TensorFlow :

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# M√™me chose que pr√©c√©demment
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "This course is amazing!",  # Ce cours est incroyable !
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Ceci est nouveau
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}
Evidemment, entra√Æner un mod√®le avec seulement deux phrases ne va pas donner de bons r√©sultats. Pour obtenir de meilleurs r√©sultats, vous allez avoir √† pr√©parer un plus grand jeu de donn√©es. 

Dans cette section, nous allons utiliser comme exemple le jeu de donn√©es MRPC (*Microsoft Research Paraphrase Corpus*) pr√©sent√© dans un [papier](https://www.aclweb.org/anthology/I05-5002.pdf) par William B. Dolan et Chris Brockett. Ce jeu de donn√©es contient 5801 paires de phrases avec un label indiquant si ces paires sont des paraphrases ou non (i.e. si elles ont la m√™me signification). Nous l'avons choisi pour ce chapitre parce que c'est un petit jeu de donn√©es et cela rend donc simples les exp√©riences d'entra√Ænement sur ce jeu de donn√©es. 

### Charger un jeu de donn√©es depuis le <i>Hub</i>

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Le *Hub* ne contient pas seulement des mod√®les mais aussi plusieurs jeux de donn√©es dans un tas de langues diff√©rentes. Vous pouvez explorer les jeux de donn√©es [ici](https://huggingface.co/datasets) et nous vous conseillons d'essayer de charger un nouveau jeu de donn√©es une fois que vous avez √©tudi√© cette section (voir la documentation g√©n√©rale [ici](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). Mais pour l'instant, concentrons-nous sur le jeu de donn√©es MRPC ! Il s'agit de l'un des 10 jeux de donn√©es qui constituent le [*benchmark* GLUE](https://gluebenchmark.com/) qui est un *benchmark* acad√©mique utilis√© pour mesurer les performances des mod√®les d'apprentissage automatique sur 10 diff√©rentes t√¢ches de classification de textes.  

La biblioth√®que ü§ó *Datasets* propose une commande tr√®s simple pour t√©l√©charger et mettre en cache un jeu de donn√©es √† partir du *Hub*. On peut t√©l√©charger le jeu de donn√©es MRPC comme ceci :   

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Comme vous le voyez, on obtient un objet de type `DatasetDict` qui contient le jeu de donn√©es d'entra√Ænement, celui de validation et celui de test. Chacun d'eux contient plusieurs colonnes (`sentence1`, `sentence2`, `label` et `idx`) et une variable nombre de lignes qui contient le nombre d'√©l√©ments dans chaque jeu de donn√©es (il y a donc 3.668 paires de phrases dans le jeu d'entra√Ænement, 408 dans celui de validation et 1.725 dans celui de test).

Cette commande t√©l√©charge et met en cache le jeu de donn√©es dans *~/.cache/huggingface/dataset*. Rappelez-vous que comme vu au chapitre 2, vous pouvez personnaliser votre dossier cache en modifiant la variable d'environnement `HF_HOME`.

Nous pouvons acc√©der √† chaque paire de phrase de notre objet `raw_datasets` par les indices, comme avec un dictionnaire :

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .', 
 # Amrozi a accus√© son fr√®re, qu'il a appel√© ¬´ le t√©moin ¬ª, de d√©former d√©lib√©r√©ment son t√©moignage.
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'} 
 # Se r√©f√©rant √† lui uniquement comme ¬´ le t√©moin ¬ª, Amrozi a accus√© son fr√®re de d√©former d√©lib√©r√©ment son t√©moignage.
```

Nous pouvons voir que les √©tiquettes sont d√©j√† des entiers, donc nous n'aurons pas √† faire de pr√©traitement ici. Pour savoir quel entier correspond √† quel label, nous pouvons inspecter les `features` de notre `raw_train_dataset`. Cela nous indiquera le type de chaque colonne :

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

En r√©alit√©, `label` est de type `ClassLabel` et la correspondance des entiers aux noms des labels est enregistr√©e le dossier *names*. `0` correspond √†  `not_equivalent` et `1` correspond √† `equivalent`.

<Tip>

‚úèÔ∏è **Essayez !** Regardez l'√©l√©ment 15 de l'ensemble d'entra√Ænement et l'√©l√©ment 87 de l'ensemble de validation. Quelles sont leurs √©tiquettes ?
</Tip>

### Pr√©traitement d'un jeu de donn√©es

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Pour pr√©traiter le jeu de donn√©es, nous devons convertir le texte en chiffres compr√©hensibles par le mod√®le. Comme vous l'avez vu dans le [chapitre pr√©c√©dent](/course/fr/chapter2), cette conversion est effectu√©e par un *tokenizer*. Nous pouvons fournir au *tokenizer* une phrase ou une liste de phrases, de sorte que nous pouvons directement tokeniser toutes les premi√®res phrases et toutes les secondes phrases de chaque paire comme ceci :

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Cependant, nous ne pouvons pas simplement passer deux s√©quences au mod√®le et obtenir une pr√©diction pour savoir si les deux phrases sont des paraphrases ou non. Nous devons traiter les deux s√©quences comme une paire, et appliquer le pr√©traitement appropri√©. Heureusement, le *tokenizer* peut √©galement prendre une paire de s√©quences et la pr√©parer de la mani√®re attendue par notre mod√®le BERT : 

```py
inputs = tokenizer(
    "This is the first sentence.", "This is the second one."
)  # "C'est la premi√®re phrase.", "C'est la deuxi√®me."
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Nous avons discut√© des cl√©s `input_ids` et `attention_mask` dans le [chapitre 2](/course/fr/chapter2), mais nous avons laiss√© de c√¥t√© les `token_type_ids`. Dans cet exemple, c'est ce qui indique au mod√®le quelle partie de l'entr√©e est la premi√®re phrase et quelle partie est la deuxi√®me phrase.

<Tip>

‚úèÔ∏è **Essayez !** Prenez l'√©l√©ment 15 de l'ensemble d'entra√Ænement et tokenisez les deux phrases s√©par√©ment et par paire. Quelle est la diff√©rence entre les deux r√©sultats ?

</Tip>

Si on d√©code les IDs dans `input_ids` en mots :

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

nous aurons :

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

Nous voyons donc que le mod√®le s'attend √† ce que les entr√©es soient de la forme `[CLS] phrase1 [SEP] phrase2 [SEP]` lorsqu'il y a deux phrases. En alignant cela avec les `token_type_ids`, on obtient :

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Comme vous pouvez le voir, les parties de l'entr√©e correspondant √† `[CLS] sentence1 [SEP]` ont toutes un *token* de type ID de `0`, tandis que les autres parties, correspondant √† `sentence2 [SEP]`, ont toutes un *token* de type ID de `1`.

Notez que si vous choisissez un autre *checkpoint*, vous n'aurez pas n√©cessairement les `token_type_ids` dans vos entr√©es tokenis√©es (par exemple, ils ne sont pas retourn√©s si vous utilisez un mod√®le DistilBERT). Ils ne sont retourn√©s que lorsque le mod√®le sait quoi faire avec eux, parce qu'il les a vus pendant son pr√©-entra√Ænement. 

Ici, BERT est pr√©-entra√Æn√© avec les *tokens* de type ID et en plus de l'objectif de mod√©lisation du langage masqu√© dont nous avons abord√© dans [chapitre 1](/course/fr/chapter1), il a un objectif suppl√©mentaire appel√© _pr√©diction de la phrase suivante_. Le but de cette t√¢che est de mod√©liser la relation entre des paires de phrases.

Avec la pr√©diction de la phrase suivante, on fournit au mod√®le des paires de phrases (avec des *tokens* masqu√©s de mani√®re al√©atoire) et on lui demande de pr√©dire si la deuxi√®me phrase suit la premi√®re. Pour rendre la t√¢che non triviale, la moiti√© du temps, les phrases se suivent dans le document d'origine dont elles ont √©t√© extraites, et l'autre moiti√© du temps, les deux phrases proviennent de deux documents diff√©rents. 

En g√©n√©ral, vous n'avez pas besoin de vous inqui√©ter de savoir s'il y a ou non des `token_type_ids` dans vos entr√©es tokenis√©es : tant que vous utilisez le m√™me *checkpoint* pour le *tokenizer* et le mod√®le, tout ira bien puisque le *tokenizer* sait quoi fournir √† son mod√®le.

Maintenant que nous avons vu comment notre *tokenizer* peut traiter une paire de phrases, nous pouvons l'utiliser pour tokeniser l'ensemble de notre jeu de donn√©es : comme dans le [chapitre pr√©c√©dent](/course/fr/chapter2), nous pouvons fournir au *tokenizer* une liste de paires de phrases en lui donnant la liste des premi√®res phrases, puis la liste des secondes phrases. Ceci est √©galement compatible avec les options de remplissage et de troncature que nous avons vues dans le [chapitre 2](/course/fr/chapter2). Voici donc une fa√ßon de pr√©traiter le jeu de donn√©es d'entra√Ænement :

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Cela fonctionne bien, mais a l'inconv√©nient de retourner un dictionnaire (avec nos cl√©s, `input_ids`, `attention_mask`, et `token_type_ids`, et des valeurs qui sont des listes de listes). Cela ne fonctionnera √©galement que si vous avez assez de RAM pour stocker l'ensemble de votre jeu de donn√©es pendant la tokenisation (alors que les jeux de donn√©es de la biblioth√®que ü§ó *Datasets* sont des fichiers [Apache Arrow](https://arrow.apache.org/) stock√©s sur le disque, vous ne gardez donc en m√©moire que les √©chantillons que vous demandez).

Pour conserver les donn√©es sous forme de jeu de donn√©es, nous utiliserons la m√©thode [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). Cela nous permet √©galement une certaine flexibilit√©, si nous avons besoin d'un pr√©traitement plus pouss√© que la simple tokenisation. La m√©thode `map()` fonctionne en appliquant une fonction sur chaque √©l√©ment de l'ensemble de donn√©es, donc d√©finissons une fonction qui tokenise nos entr√©es :

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Cette fonction prend un dictionnaire (comme les √©l√©ments de notre jeu de donn√©es) et retourne un nouveau dictionnaire avec les cl√©s `input_ids`, `attention_mask`, et `token_type_ids`. Notez que cela fonctionne √©galement si le dictionnaire `example` contient plusieurs √©chantillons (chaque cl√© √©tant une liste de phrases) puisque le `tokenizer` travaille sur des listes de paires de phrases, comme vu pr√©c√©demment. Cela nous permettra d'utiliser l'option `batched=True` dans notre appel √† `map()`, ce qui acc√©l√©rera grandement la tok√©nisation. Le `tokenizer` est soutenu par un *tokenizer* √©crit en Rust √† partir de la biblioth√®que [ü§ó *Tokenizers*](https://github.com/huggingface/tokenizers). Ce *tokenizer* peut √™tre tr√®s rapide, mais seulement si on lui donne beaucoup d'entr√©es en m√™me temps.

Notez que nous avons laiss√© l'argument `padding` hors de notre fonction de *tokenizer* pour le moment. C'est parce que le *padding* de tous les √©chantillons √† la longueur maximale n'est pas efficace : il est pr√©f√©rable de remplir les √©chantillons lorsque nous construisons un batch, car alors nous avons seulement besoin de remplir √† la longueur maximale dans ce batch, et non la longueur maximale dans l'ensemble des donn√©es. Cela peut permettre de gagner beaucoup de temps et de puissance de traitement lorsque les entr√©es ont des longueurs tr√®s variables ! 

Voici comment nous appliquons la fonction de tokenization sur tous nos jeux de donn√©es en m√™me temps. Nous utilisons `batched=True` dans notre appel √† `map` pour que la fonction soit appliqu√©e √† plusieurs √©l√©ments de notre jeu de donn√©es en une fois, et non √† chaque √©l√©ment s√©par√©ment. Cela permet un pr√©traitement plus rapide.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

La fa√ßon dont la biblioth√®que ü§ó *Datasets* applique ce traitement consiste √† ajouter de nouveaux champs aux jeux de donn√©es, un pour chaque cl√© du dictionnaire renvoy√© par la fonction de pr√©traitement :

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Vous pouvez m√™me utiliser le multitraitement lorsque vous appliquez votre fonction de pr√©traitement avec `map()` en passant un argument `num_proc`. Nous ne l'avons pas fait ici parce que la biblioth√®que ü§ó *Tokenizers* utilise d√©j√† plusieurs *threads* pour tokeniser nos √©chantillons plus rapidement, mais si vous n'utilisez pas un *tokenizer* rapide soutenu par cette biblioth√®que, cela pourrait acc√©l√©rer votre pr√©traitement.

Notre `tokenize_function` retourne un dictionnaire avec les cl√©s `input_ids`, `attention_mask`, et `token_type_ids`, donc ces trois champs sont ajout√©s √† toutes les divisions de notre jeu de donn√©es. Notez que nous aurions √©galement pu modifier des champs existants si notre fonction de pr√©traitement avait retourn√© une nouvelle valeur pour une cl√© existante dans l'ensemble de donn√©es auquel nous avons appliqu√© `map()`.

La derni√®re chose que nous devrons faire est de remplir tous les exemples √† la longueur de l'√©l√©ment le plus long lorsque nous regroupons les √©l√©ments, une technique que nous appelons le *padding dynamique*.

### <i>Padding</i> dynamique

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
La fonction qui est responsable de l'assemblage des √©chantillons dans un batch est appel√©e *fonction d'assemblement*. C'est un argument que vous pouvez passer quand vous construisez un `DataLoader`, la valeur par d√©faut √©tant une fonction qui va juste convertir vos √©chantillons en tenseurs PyTorch et les concat√©ner (r√©cursivement si vos √©l√©ments sont des listes, des *tuples* ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr√©es que nous avons ne seront pas toutes de la m√™me taille. Nous avons d√©lib√©r√©ment report√© le *padding*, pour ne l'appliquer que si n√©cessaire sur chaque batch et √©viter d'avoir des entr√©es trop longues avec beaucoup de remplissage. Cela acc√©l√®re consid√©rablement l'entra√Ænement, mais notez que si vous vous entra√Ænez sur un TPU, cela peut poser des probl√®mes. En effet, les TPU pr√©f√®rent les formes fixes, m√™me si cela n√©cessite un *padding* suppl√©mentaire.

{:else}
La fonction qui est responsable de l'assemblage des √©chantillons dans un batch est appel√©e *fonction d'assemblement*. C'est un argument que vous pouvez passer quand vous construisez un `DataLoader`, la valeur par d√©faut √©tant une fonction qui va juste convertir vos √©chantillons en type tf.Tensor et les concat√©ner (r√©cursivement si les √©l√©ments sont des listes, des *tuples* ou des dictionnaires). Cela ne sera pas possible dans notre cas puisque les entr√©es que nous avons ne seront pas toutes de la m√™me taille. Nous avons d√©lib√©r√©ment report√© le *padding*, pour ne l'appliquer que si n√©cessaire sur chaque batch et √©viter d'avoir des entr√©es trop longues avec beaucoup de remplissage. Cela acc√©l√®re consid√©rablement l'entra√Ænement, mais notez que si vous vous entra√Ænez sur un TPU, cela peut poser des probl√®mes. En effet, les TPU pr√©f√®rent les formes fixes, m√™me si cela n√©cessite un *padding* suppl√©mentaire.

{/if}
Pour faire cela en pratique, nous devons d√©finir une fonction d'assemblement qui appliquera la bonne quantit√© de *padding* aux √©l√©ments du jeu de donn√©es que nous voulons regrouper. Heureusement, la biblioth√®que ü§ó *Transformers* nous fournit une telle fonction via `DataCollatorWithPadding`. Elle prend un *tokenizer* lorsque vous l'instanciez (pour savoir quel *token* de *padding* utiliser et si le mod√®le s'attend √† ce que le *padding* soit √† gauche ou √† droite des entr√©es) et fera tout ce dont vous avez besoin :

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Pour tester notre nouveau jouet, prenons quelques √©l√©ments de notre jeu d'entra√Ænement avec lesquels nous allons former un batch. Ici, on supprime les colonnes `idx`, `sentence1` et `sentence2` puisque nous n'en aurons pas besoin et qu'elles contiennent des *strings* (et nous ne pouvons pas cr√©er des tenseurs avec des *strings*) et on regarde la longueur de chaque entr√©e du batch : 

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Sans surprise, nous obtenons des √©chantillons de longueur variable, de 32 √† 67. Le *padding* dynamique signifie que les √©chantillons de ce batch doivent tous √™tre rembourr√©s √† une longueur de 67, la longueur maximale dans le batch. Sans le *padding* dynamique, tous les √©chantillons devraient √™tre rembourr√©s √† la longueur maximale du jeu de donn√©es entier, ou √† la longueur maximale que le mod√®le peut accepter. V√©rifions √† nouveau que notre `data_collator` rembourre dynamiquement le batch correctement :

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

C'est beau ! Maintenant que nous sommes pass√©s du texte brut √† des batchs que notre mod√®le peut traiter, nous sommes pr√™ts √† le *finetuner* !

{/if}

<Tip>

‚úèÔ∏è **Essayez !** Reproduisez le pr√©traitement sur le jeu de donn√©es GLUE SST-2. C'est un peu diff√©rent puisqu'il est compos√© de phrases simples au lieu de paires, mais le reste de ce que nous avons fait devrait √™tre identique. Pour un d√©fi plus difficile, essayez d'√©crire une fonction de pr√©traitement qui fonctionne sur toutes les t√¢ches GLUE.

</Tip>

{#if fw === 'tf'}

Maintenant que nous disposons de notre jeu de donn√©es et d'un assembleur de donn√©es, nous devons les assembler. Nous pourrions charger manuellement des batchs et les assembler mais c'est beaucoup de travail et probablement pas tr√®s performant non plus. A la place, il existe une m√©thode simple qui offre une solution performante √† ce probl√®me : `to_tf_dataset()`. Cela va envelopper un `tf.data.Dataset` autour de votre jeu de donn√©es, avec une fonction de collation optionnelle. `tf.data.Dataset` est un format natif de TensorFlow que Keras peut utiliser pour `model.fit()`, donc cette seule m√©thode convertit imm√©diatement un *dataset* en un format pr√™t pour l'entra√Ænement. Voyons cela en action avec notre jeu de donn√©es !

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

Et c'est tout ! Nous pouvons utiliser ces jeux de donn√©es dans le prochain cours, o√π l'entra√Ænement sera agr√©ablement simple apr√®s tout le dur travail de pr√©traitement des donn√©es.

{/if}
