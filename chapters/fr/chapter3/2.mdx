<FrameworkSwitchCourse {fw} />

# Pr√©parer les donn√©es

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
Continuons avec [l'exemple pr√©c√©dent](/course/chapter2), voil√† comment on entra√Æne un classifieur de s√©quences sur un batch avec PyTorch :
s
```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Continuons avec [l'exemple pr√©c√©dent](/course/chapter2), voil√† comment on entra√Æne un classifieur de s√©quences sur un batch avec TensorFlow :

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}
Evidemment, entra√Æner un mod√®le avec seulement deux phrases ne va pas donner de bons r√©sultats. Pour obtenir de meilleurs r√©sultats, vous allez avoir √† pr√©parer un plus grand jeu de donn√©es. 

Dans cette section, nous allons utiliser comme exemple le jeu de donn√©es (dataset) MRPC (Microsoft Research Paraphrase Corpus) pr√©sent√© dans un [papier](https://www.aclweb.org/anthology/I05-5002.pdf) de William B. Dolan et Chris Brockett. Ce jeu de donn√©es contient 5801 paires de phrases avec un label indiquant si ces paires sont des paraphrases ou non (i.e. si elles ont la m√™me signification). Nous l'avons choisi pour ce chapitre parce que c'est un petit jeu de donn√©es, et cela rend donc facile les exp√©riences d'entra√Ænement sur ce dataset. 

### Charger un jeu de donn√©es depuis le Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Le hub ne contient pas juste des mod√®les; il contient aussi plusieurs jeux de donn√©es dans un tas de langages diff√©rents. Vous pouvez explorer les jeux de donn√©es [ici](https://huggingface.co/datasets), et nous vous conseillons d'essayer de charger un nouveau jeu de donn√©es une fois que vous avez √©tudi√© cette section (voir la documentation g√©n√©rale [ici](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). Mais pour l'instant, concentrons nous sur le jeu de donn√©es MRPC! Il s'agit de l'un des 10 jeux de donn√©es qui constituent le [benchmark GLUE](https://gluebenchmark.com/) qui est un benchmark acad√©mique utilis√© pour mesurer les performances des mod√®les de Machine Learning sur 10 t√¢ches de classification de textes.  

La librairie de datasets de ü§ó propose une commande tr√®s simple pour t√©l√©charger et mettre en cache un jeu de donn√©es √† partir du Hub. On peut t√©l√©charger le jeu de donn√©es MRPC comme ceci :   

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Comme vous le voyez, on obtient un objet de type `DatasetDict` qui contient le jeu de donn√©es d'entra√Ænement, celui de validation et celui de test. Chacun d'eux contient plusieurs colonnes (`sentence1`, `sentence2`, `label` et `idx`) et une variable nombre de lignes qui contient le nombre d'√©l√©ments dans chaque jeu de donn√©es (il y a donc 3.668 paires de phrases dans le jeu d'entra√Ænement, 408 dans celui de validation et 1.725 dans celui de test).

Cette commande t√©l√©charge et met en cache le jeu de donn√©es dans *~/.cache/huggingface/dataset*. Rappelez-vous que comme vu au chapitre 2, vous pouvez personnaliser votre dossier cache en modifiant la variable d'environnement `HF_HOME`.

Nous pouvons acc√©der √† chaque paire de phrase de notre objet `raw_datasets` par les indices, comme avec un dictionnaire :

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```
Nous pouvons voir que les labels sont d√©j√† des entiers, nous n'avons donc aucun preprocessing √† faire sur les labels. Pour savoir quel entier correspond √† quel label, on peut inspecter les `features` de notre `raw_train_dataset`. Cela nous dira le type de chaque colonne :   

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

En r√©alit√©, `label` est de type `ClassLabel`, et la correspondance des entiers aux noms des labels est enregistr√©e le dossier *names*. `0` correspond √†  `not_equivalent` (pas √©quivalent), et `1` correspond √† `equivalent`.

<Tip>

‚úèÔ∏è **Essayez ceci!** Regardez l'√©l√©ment 15 du jeu d'entra√Ænement et l'√©l√©ment 87 du jeu de validation. Quels sont leurs labels? 

</Tip>

### Preprocessing d'un jeu de donn√©e

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Pour pr√©traiter l'ensemble de donn√©es, nous devons convertir le texte en nombres que le mod√®le peut comprendre. Comme vous l'avez vu dans le [chapitre pr√©c√©dent](/course/chapter2), cela se fait avec un tokenizer. Nous pouvons passer au tokenizer une phrase ou une liste de phrases, ainsi nous pouvons directement tokeniser toutes les premi√®res phrases et toutes les deuxi√®mes phrases de chaque paire comme ceci :

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

N√©anmoins, nous ne pouvons pas simplement transmettre deux s√©quences au mod√®le et obtenir une pr√©diction indiquant si les deux phrases sont des paraphrases ou non. Nous devons g√©rer les deux s√©quences comme une paire et appliquer le pr√©traitement appropri√©. Heureusement, le tokenizer peut √©galement prendre une paire de s√©quences et la mettre dans le format attendu par notre mod√®le BERT¬†:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Nous avons vu les cl√©s `input_ids` et `attention_mask` au [chapitre 2](/course/chapter2), mais nous avons omis de parler des `token_type_ids`. Dans cet exemple, c'est ce qui indique au mod√®le quelle partie de l'entr√©e repr√©sente la premi√®re phrase et quelle partie repr√©sente la deuxi√®me phrase.

<Tip>

‚úèÔ∏è **Essayez ceci!** Prenez l'√©l√©ment 15 de l'ensemble d'entra√Ænement et tokenisez les deux phrases s√©par√©ment et comme une paire. Quelle est la diff√©rence entre les deux r√©sultats¬†?

</Tip>

Si nous d√©codons les IDs contenus dans `input_ids` pour r√©obtenir des mots :

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

Nous obtiendrons : 

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

Nous pouvons donc voir que le mod√®le attend une entr√©e de la forme  `[CLS] sentence1 [SEP] sentence2 [SEP]` lorsqu'il y a deux phrases. Aligner cette repr√©sentation avec `token_type_ids` nous donne :

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Comme vous pouvez le voir, les parties correspondant √† `[CLS] sentence1 [SEP]` ont toutes `0` comme ID de type de token, alors que les autres parties qui correspondent √† `sentence2 [SEP]`, ont toute `1` comme ID de type de token.

Notez que si vous s√©lectionnez un checkpoint diff√©rent, vous n'aurez pas n√©cessairement les `token_type_ids` dans vos entr√©es tokenis√©es (par exemple, ils ne sont pas retourn√©s lorsque vous utilisez le mod√®le DistilBERT). Ils ne sont renvoy√©s que lorsque le mod√®le sait les utiliser, pour les avoir vu pendant le pr√©-entra√Ænement. 

Ici, BERT est pr√©-entra√Æn√© avec des IDs de type de token, et en plus de la fonction de co√ªt MLM (Masked Language Modelling) dont nous avons parl√© au [chapitre 1](/course/chapter1), il a un co√ªt suppl√©mentaire appel√© _pr√©diction de la phrase suivante_(_next sentence prediction_). Le but de cette t√¢che est de mod√©liser la relation entre des paires de phrases.

Pour la pr√©diction de la phrase suivante, le mod√®le re√ßoit des paires de phrases (avec des tokens masqu√©s de mani√®re al√©atoire) et on lui demande de pr√©dire si la deuxi√®me phrase suit la premi√®re. Pour rendre la t√¢che non triviale, la moiti√© du temps les phrases se suivent dans le document original dont elles sont extraites, et l'autre moiti√© du temps les deux phrases proviennent de deux documents diff√©rents.

En g√©n√©ral, vous n'avez pas √† vous soucier de savoir s'il y a ou non des `token_type_ids` dans vos entr√©es tokenis√©es¬†: tant que vous utilisez le m√™me checkpoint pour le tokenizer et le mod√®le, tout ira bien car le tokenizer sait quoi fournir √† son mod√®le.

Maintenant que nous avons vu comment notre tokenizer peut traiter une paire de phrases, nous pouvons l'utiliser pour tokeniser l'ensemble de nos donn√©es¬†: comme dans le [chapitre pr√©c√©dent](/course/chapter2), nous pouvons fournir au tokenizer une liste de paires de phrases en lui donnant la liste des premi√®res phrases, puis la liste des secondes phrases. Ceci est √©galement compatible avec les options de padding et de troncature que nous avons vues au [chapitre 2](/course/chapter2). Ainsi, une fa√ßon de pr√©traiter l'ensemble de donn√©es d'entra√Ænement est¬†:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Cela marche bien, mais a le d√©savantage de retourner un dictionnaire (avec nos cl√©s, `input_ids`, `attention_mask` et `token_type_ids`, et des valeurs qui sont des listes de listes). Cela ne marchera que si vous avez assez de RAM pour contenir tout le jeu de donn√©es pendant la tokenisation (tandis que les jeux de donn√©es de la librairie de ü§ó sont des fichiers [Apache Arrow](https://arrow.apache.org/) charg√©es sur le disque, de sorte que vous ne gardez que les exemples dont vous avez besoin en m√©moire )

Pour garder les donn√©es sous forme de dataset (jeu de donn√©es), nous allons utiliser la m√©thode [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). Cela nous permet d'avoir plus de flexibilit√© si nous avons besoin de faire plus que juste tokeniser pendant la phase de pr√©-tra√Ætement. La m√©thode `map()` fonctionne en applicant une fonction √† chaque √©l√©ment du dataset, d√©finissons alors une fonction qui tokenise nos entr√©es :

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Cette prend en entr√©e un dictionnaire (comme les items de notre dataset) and renvioe un nouveau dictionnaire avec les cl√©s `input_ids`, `attention_mask` et `token_type_ids`. Notez que cela marche aussi si le dictionnaire `example` contient plusieurs exemples puisque le tokenizer fonctionne aussi sur des listes de paires de phrases, comme nous l'avons vu pr√©c√©demment. Cela permettra d'utiliser l'option `batched=True` dans notre appel de la m√©thode `map()`, ce qui permettra d'acc√©l√©rer la tokenisation.  Le `tokenizer` est aid√© par  un tokenizer √©crit en Rust  provenant de la librairie [Tokenizers](https://github.com/huggingface/tokenizers) de ü§ó. Ce tokenizer peut √™tre tr√®s rapide, mais seulement si on lui donne beaucoup d'entr√©es en m√™me temps.

Notez que nous avons ignorer l'argument `padding` dans notre fonction de tokenisation pour l'instant. Ceci parce que faire le padding de tous les exemples √† la longueur maximale n'est pas efficace : il vaut mieux faire le padding lorsque nous construisons un batch, puisque dans ce cas nous allons seulement faire le padding pour la longueur maximale dans ce batch, et non pour la longueur maximale de tout le dataset. Cela permet d'√©conomiser beaucoup de temps et de puissance de calcul lorsqu'on traite des entr√©es avec des longueurs tr√©s vari√©es!   

Voil√† comment on applique la fonction de tokenisation √† tous nos datasets en m√™me temps. Nous utilisons `batched=True` dans notre appel de `map` pour que la fonction soit appliqu√©e √† plusieurs √©l√©ments de notre dataset en m√™me temps, et non sur chaque √©l√©ment s√©par√©ment. Cela permet d'avoir un pr√©-tra√Ætement plus rapide.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

La librairie Datasets de ü§ó applique le processing en ajoutant de nouveaux champs aux datasets, un nouveau pour chaque cl√© retourn√©e par la fonction de preprocessing : 

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Vous pouvez m√™me utiliser le multiprocessing en appliquant votre pr√©-tra√Ætement avec `map()` en lui passant l'argument `num_proc`. Nous ne l'avons pas utilis√© ici parce que la librairie Tokenizers de ü§ó utilise plusieurs threads pour tokeniser nos exemples plus rapidement, mais si vous n'utilisez pas de tokenizer rapide qui s'aide de cette librairie, cela pourrait acc√©l√©rer votre pr√©-tra√Ætement.

Notre `tokenize_function` retourne un dictionnaire avec les cl√©s `input_ids`, `attention_mask` et `token_type_ids`, donc ces trois champs sont ajout√©s √† toutes les parties (entra√Ænement, validation et test) de notre dataset. Notez que nous aurions aussi pu changer des champs existants si notre pr√©-tra√Ætement retournait une nouvelle valeur pour une cl√© qui existait d√©j√† dans le dataset sur lequel nous avons appel√© `map()`.

La derni√®re chose que nous aurons besoin de faire est le padding de tous les √©l√©ments pour que leur longueur atteigne la longueur de la plus longue s√©quence du batch lorsque nous construisons les batchs ‚Äî une technique que nous appelons *padding dynamique*.

### Padding dynamique

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
La fonction responsable de mettre en ensemble les √©l√©ments pour en faire un batch est appel√©e *fonction d'assemblage* -*collate function*-. C'est un argument que vous pouvez fournir lorsque vous construisez un `DataLoader`, par d√©faut il s'agit d'une fonction qui va juste convertir les √©l√©ments en type tensor de Pytorch et les concat√©ner (r√©cursivement si les √©l√©ments sont des listes, des tuples ou des dictionnaires). Cela -la concat√©nation- ne sera pas possible dans notre cas puisque toutes les entr√©es n'auront pas la m√™me taille. Nous avons volontairement reporter √† plus tard le padding, pour n'appliquer que le padding n√©cessaire pour chaque batch et √©viter d'avoir des entr√©es excessivement longues avec beaucoup de padding. Cela va beaucoup acc√©l√©rer l'entra√Ænement, notez cependant que si vous faites l'entra√Ænement sur TPU cela peut poser des probl√®mes ‚Äî Les TPUs pr√©f√®rent une taille fixe m√™me si cela requiert beaucoup de padding. 

{:else}
La fonction responsable de mettre en ensemble les √©l√©ments pour en faire un batch est appel√©e *fonction d'assemblage* -*collate function*-. C'est un argument que vous pouvez fournir lorsque vous construisez un `DataLoader`, par d√©faut il s'agit d'une fonction qui va juste convertir les √©l√©ments en type tf.Tensor et les concat√©ner (r√©cursivement si les √©l√©ments sont des listes, des tuples ou des dictionnaires). Cela -la concat√©nation- ne sera pas possible dans notre cas puisque toutes les entr√©es n'auront pas la m√™me taille. Nous avons volontairement reporter √† plus tard le padding, pour n'appliquer que le padding n√©cessaire pour chaque batch et √©viter d'avoir des entr√©es excessivement longues avec beaucoup de padding. Cela va beaucoup acc√©l√©rer l'entra√Ænement, notez cependant que si vous faites l'entra√Ænement sur TPU cela peut poser des probl√®mes ‚Äî Les TPUs pr√©f√®rent une taille fixe m√™me si cela requiert beaucoup de padding.

{/if}
En pratique, pour faire cela, on utilise une fonction d'assemblage qui va mettre la bonne quantit√© de padding aux √©l√©ments du dataset que nous mettre ensemble pour former un batch.  Heureusement,  la librairie Transformers de ü§ó fournit une telle fonction via  `DataCollatorWithPadding`. Elle prend un tokenizer lorsqu'on l'instancie (pour savoir quel token utiliser pour le padding, et aussi s'il faut faire le padding √† gauche ou √† droite en fonction des attentes du  mod√®le) et va faire le n√©cessaire:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Pour tester notre nouveau jouet, prenons quelques √©l√©ments de notre jeu d'entra√Ænement avec lesquels nous allons former un batch. Ici, on supprime les colonnes `idx`, `sentence1` et `sentence2` puisque nous n'en aurons pas besoin et qu'elles contiennent des strings ( et nous ne pouvons pas cr√©er des tensors avec des strings) et on regarde la longueur de chaque entr√©e du batch : 

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Sans surprise, nous obtenons des √©l√©ments de longueurs diff√©rentes, allant de 32 √† 67. Le padding dynamique signifie que nous allons utiliser le padding sur tous les √©l√©ments du batch pour obtenir une longueur de 67, la longueur maximale pour ce batch. Sans le padding dynamique, on appliquerait un padding √† tous les √©l√©ments pour atteindre la longueur maximale de tout le dataset, ou la longueur maximale que le mod√®le peut accepter. V√©rifions que notre `data_collator` effectue correctement le padding dynamique : 

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```
Bien! Maintenant que nous sommes pass√©s du texte brut aux batchs que notre mod√®le peut exploiter, nous sommes pr√™t √† proc√©der au fine-tuning!

{/if}

<Tip>

‚úèÔ∏è **Essayez ceci!** Reproduisez le preprocessing  sur le dataset GLUE SST-2. Il est un peu diff√©rent puisqu'il est compos√© d'uniques phrases et non de paires de phrases, mais le reste de ce que nous avons fait devrait √™tre similaire. Pour un d√©fi plus cors√©, essayez d'√©crire une fonction de pr√©processing qui marche pour toutes les t√¢ches de GLUE.   

</Tip>

{#if fw === 'tf'}

Maintenant que nous avons notre dataset et notre assembleur de donn√©es, nous devons les mettre ensemble. On pourrait charger manuellement les batchs et les assembler, mais cela repr√©sente beaucoup de travail, et ce n'est pas tr√®s performant non plus. Au lieu de faire cela, il y a une simple m√©thode qui offre une solution performante √† ce probl√®me : `to_tf_dataset()`.  Cette m√©thode va passer `tf.data.Dataset` sur votre dataset, avec une fonction d'assemblage optionnelle. `tf.data.Dataset` est un format natif de Tensorflow que Keras peut utiliser avec `model.fit()`, donc cette m√©thode √† elle seule convertit un dataset de ü§ó √† un format pr√™t pour l'entra√Ænement. Voyons la √† l'≈ìuvre avec notre dataset!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

Et c'est tout! Nous pouvons utiliser ces datasets dans le prochain cours, o√π l'entra√Ænement va √™tre agr√©ablement facile apr√®s tout le difficile travail de pr√©processing.

{/if}
