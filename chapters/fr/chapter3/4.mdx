# Un entra√Ænement complet

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

Maintenant nous allons voir comment obtenir les m√™mes r√©sultats que dans la derni√®re section sans utiliser la classe `Trainer`. Encore une fois, nous supposons que vous avez fait le traitement des donn√©es dans la section 2. Voici un court r√©sum√© couvrant tout ce dont vous aurez besoin :

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Pr√©parer l'entra√Ænement

Avant d'√©crire r√©ellement notre boucle d'entra√Ænement, nous devons d√©finir quelques objets. Les premiers sont les *dataloaders* que nous utiliserons pour it√©rer sur les batchs. Mais avant de pouvoir d√©finir ces chargeurs de donn√©es, nous devons appliquer un peu de post-traitement √† nos `tokenized_datasets`, pour prendre soin de certaines choses que le `Trainer` fait pour nous automatiquement. Sp√©cifiquement, nous devons :

- supprimer les colonnes correspondant aux valeurs que le mod√®le n'attend pas (comme les colonnes `sentence1` et `sentence2`),
- renommer la colonne `label` en `labels` (parce que le mod√®le s'attend √† ce que l'argument soit nomm√© `labels`),
- d√©finir le format des jeux de donn√©es pour qu'ils retournent des tenseurs PyTorch au lieu de listes.

Notre `tokenized_datasets` a une m√©thode pour chacune de ces √©tapes :

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Nous pouvons alors v√©rifier que le r√©sultat ne comporte que des colonnes que notre mod√®le acceptera :

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Maintenant que cela est fait, nous pouvons facilement d√©finir nos *dataloaders* :

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

Pour v√©rifier rapidement qu'il n'y a pas d'erreur dans le traitement des donn√©es, nous pouvons inspecter un batch comme celui-ci :

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

Notez que les formes r√©elles seront probablement l√©g√®rement diff√©rentes pour vous puisque nous avons d√©fini `shuffle=True` pour le chargeur de donn√©es d'entra√Ænement et que nous *paddons* √† la longueur maximale dans le batch.

Maintenant que nous en avons termin√© avec le pr√©traitement des donn√©es (un objectif satisfaisant mais difficile √† atteindre pour tout praticien d'apprentissage automatique), passons au mod√®le. Nous l'instancions exactement comme nous l'avons fait dans la section pr√©c√©dente :

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Pour s'assurer que tout se passera bien pendant l'entra√Ænement, nous transmettons notre batch √† ce mod√®le :

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

Tous les mod√®les ü§ó *Transformers* renvoient la perte lorsque les `labels` sont fournis. Nous obtenons √©galement les logits (deux pour chaque entr√©e de notre batch, donc un tenseur de taille 8 x 2).

Nous sommes presque pr√™ts √† √©crire notre boucle d'entra√Ænement ! Il nous manque juste deux choses : un optimiseur et un planificateur de taux d'apprentissage. Puisque nous essayons de reproduire √† la main ce que fait la fonction `Trainer`, utilisons les m√™mes param√®tres par d√©faut. L'optimiseur utilis√© par `Trainer` est `AdamW`, qui est le m√™me qu'Adam, mais avec une torsion pour la r√©gularisation par d√©croissance de poids (voir [*Decoupled Weight Decay Regularization*](https://arxiv.org/abs/1711.05101) par Ilya Loshchilov et Frank Hutter) :

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Enfin, le planificateur du taux d'apprentissage utilis√© par d√©faut est juste une d√©croissance lin√©aire de la valeur maximale (5e-5) √† 0. Pour le d√©finir correctement, nous devons conna√Ætre le nombre d'√©tapes d'entra√Ænement que nous prendrons, qui est le nombre d'√©poques que nous voulons ex√©cuter multipli√© par le nombre de batch d'entra√Ænement (qui est la longueur de notre *dataloader* d'entra√Ænement). Le `Trainer` utilise trois √©poques par d√©faut, nous allons donc suivre √ßa :

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### La boucle d'entra√Ænement

Une derni√®re chose : nous voulons utiliser le GPU si nous en avons un (sur un CPU, l'entra√Ænement peut prendre plusieurs heures au lieu de quelques minutes). Pour ce faire, nous d√©finissons un `device` sur lequel nous allons placer notre mod√®le et nos batchs :

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

Nous sommes maintenant pr√™ts √† entra√Æner ! Pour avoir une id√©e du moment o√π l'entra√Ænement sera termin√©, nous ajoutons une barre de progression sur le nombre d'√©tapes d'entra√Ænement, en utilisant la biblioth√®que `tqdm` :

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Vous pouvez voir que le c≈ìur de la boucle d'entra√Ænement ressemble beaucoup √† celui de l'introduction. Nous n'avons pas demand√© de rapport, donc cette boucle d'entra√Ænement ne nous dira rien sur les r√©sultats du mod√®le. Pour cela, nous devons ajouter une boucle d'√©valuation.


### La boucle d'√©valuation

Comme nous l'avons fait pr√©c√©demment, nous allons utiliser une m√©trique fournie par la biblioth√®que ü§ó *Evaluate*. Nous avons d√©j√† vu la m√©thode `metric.compute()`, mais les m√©triques peuvent en fait accumuler des batchs pour nous au fur et √† mesure que nous parcourons la boucle de pr√©diction avec la m√©thode `add_batch()`. Une fois que nous avons accumul√© tous les batchs, nous pouvons obtenir le r√©sultat final avec `metric.compute()`. Voici comment impl√©menter tout cela dans une boucle d'√©valuation :

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Une fois encore, vos r√©sultats seront l√©g√®rement diff√©rents en raison du caract√®re al√©atoire de l'initialisation de la t√™te du mod√®le et du m√©lange des donn√©es, mais ils devraient se situer dans la m√™me fourchette.

<Tip>

‚úèÔ∏è **Essayez** Modifiez la boucle d'entra√Ænement pr√©c√©dente pour *finetuner* votre mod√®le sur le jeu de donn√©es SST-2.

</Tip>

### Optimisez votre boucle d'entra√Ænement avec ü§ó <i>Accelerate</i>

<Youtube id="s7dy8QRgjJ0" />

La boucle d'entra√Ænement que nous avons d√©finie pr√©c√©demment fonctionne bien sur un seul CPU ou GPU. Mais en utilisant la biblioth√®que [ü§ó *Accelerate*](https://github.com/huggingface/accelerate), il suffit de quelques ajustements pour permettre un entra√Ænement distribu√© sur plusieurs GPUs ou TPUs. En partant de la cr√©ation des *dataloaders* d'entra√Ænement et de validation, voici √† quoi ressemble notre boucle d'entra√Ænement manuel :

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Et voici les changements :

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

La premi√®re ligne √† ajouter est la ligne d'importation. La deuxi√®me ligne instancie un objet `Accelerator` qui va regarder l'environnement et initialiser la bonne configuration distribu√©e. ü§ó *Accelerate* g√®re le placement des p√©riph√©riques pour vous, donc vous pouvez enlever les lignes qui placent le mod√®le sur le p√©riph√©rique (ou, si vous pr√©f√©rez, les changer pour utiliser `accelerator.device` au lieu de `device`).

Ensuite, le gros du travail est fait dans la ligne qui envoie les *dataloaders*, le mod√®le, et l'optimiseur √† `accelerator.prepare()`. Cela va envelopper ces objets dans le conteneur appropri√© pour s'assurer que votre entra√Ænement distribu√© fonctionne comme pr√©vu. Les changements restants √† faire sont la suppression de la ligne qui met le batch sur le `device` (encore une fois, si vous voulez le garder, vous pouvez juste le changer pour utiliser `accelerator.device`) et le remplacement de `loss.backward()` par `accelerator.backward(loss)`.

<Tip>
‚ö†Ô∏è Afin de b√©n√©ficier de la rapidit√© offerte par les TPUs du Cloud, nous vous recommandons de rembourrer vos √©chantillons √† une longueur fixe avec les arguments `padding="max_length"` et `max_length` du <i>tokenizer</i>.
</Tip>

Si vous souhaitez faire un copier-coller pour jouer, voici √† quoi ressemble la boucle d'entra√Ænement compl√®te avec ü§ó <i>Accelerate</i> :

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

En pla√ßant ceci dans un script `train.py`, cela sera ex√©cutable sur n'importe quel type d'installation distribu√©e. Pour l'essayer dans votre installation distribu√©e, ex√©cutez la commande :

```bash
accelerate config
```

qui vous demandera de r√©pondre √† quelques questions et enregistrera vos r√©ponses dans un fichier de configuration utilis√© par cette commande :

```
accelerate launch train.py
```

qui lancera l'entra√Ænement distribu√©.

Si vous voulez essayer ceci dans un *notebook* (par exemple, pour le tester avec des TPUs sur Colab), collez simplement le code dans une `training_function()` et lancez une derni√®re cellule avec :

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

Vous trouverez d'autres exemples dans le d√©p√¥t d'[ü§ó *Accelerate*](https://github.com/huggingface/accelerate/tree/main/examples).
