<FrameworkSwitchCourse {fw} />

# <i>Finetuner</i> un mod√®le avec l'API Trainer

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

La biblioth√®que ü§ó *Transformers* fournit une classe `Trainer` pour vous aider √† *finetuner* n'importe lequel des mod√®les pr√©-entra√Æn√©s qu'elle met √† disposition sur votre jeu de donn√©es. Une fois que vous avez fait tout le travail de pr√©traitement des donn√©es dans la derni√®re section, il ne vous reste que quelques √©tapes pour d√©finir le `Trainer`. La partie la plus difficile sera probablement de pr√©parer l'environnement pour ex√©cuter `Trainer.train()`, car elle fonctionnera tr√®s lentement sur un CPU. Si vous n'avez pas de GPU, vous pouvez avoir acc√®s √† des GPUs ou TPUs gratuits sur [Google Colab](https://colab.research.google.com/).

Les exemples de code ci-dessous supposent que vous avez d√©j√† ex√©cut√© les exemples de la section pr√©c√©dente. Voici un bref r√©sum√© de ce dont vous avez besoin :

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Entra√Ænement

La premi√®re √©tape avant de pouvoir d√©finir notre `Trainer` est de d√©finir une classe `TrainingArguments` qui contiendra tous les hyperparam√®tres que le `Trainer` utilisera pour l'entra√Ænement et l'√©valuation. Le seul argument que vous devez fournir est un r√©pertoire o√π le mod√®le entra√Æn√© sera sauvegard√©, ainsi que les *checkpoints*. Pour tout le reste, vous pouvez laisser les valeurs par d√©faut, qui devraient fonctionner assez bien pour un *finetuning* de base.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<Tip>

üí° Si vous voulez t√©l√©charger automatiquement votre mod√®le sur le *Hub* pendant l'entra√Ænement, passez `push_to_hub=True` dans le `TrainingArguments`. Nous en apprendrons plus √† ce sujet au [chapitre 4](/course/fr/chapter4/3).

</Tip>

La deuxi√®me √©tape consiste √† d√©finir notre mod√®le. Comme dans le [chapitre pr√©c√©dent](/course/fr/chapter2), nous utiliserons la classe `AutoModelForSequenceClassification`, avec deux labels :

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Vous remarquerez que contrairement au [chapitre 2](/course/fr/chapter2), vous obtenez un message d'avertissement apr√®s l'instanciation de ce mod√®le pr√©-entra√Æn√©. C'est parce que BERT n'a pas √©t√© pr√©-entra√Æn√© √† la classification de paires de phrases, donc la t√™te du mod√®le pr√©-entra√Æn√© a √©t√© supprim√©e et une nouvelle t√™te adapt√©e √† la classification de s√©quences a √©t√© ajout√©e √† la place. Les messages d'avertissement indiquent que certains poids n'ont pas √©t√© utilis√©s (ceux correspondant √† la t√™te de pr√©-entra√Ænement abandonn√©e) et que d'autres ont √©t√© initialis√©s de mani√®re al√©atoire (ceux pour la nouvelle t√™te). Il conclut en vous encourageant √† entra√Æner le mod√®le, ce qui est exactement ce que nous allons faire maintenant.

Une fois que nous avons notre mod√®le, nous pouvons d√©finir un `Trainer` en lui passant tous les objets construits jusqu'√† pr√©sent : le `model`, le `training_args`, les jeux de donn√©es d'entra√Ænement et de validation, notre `data_collator`, et notre `tokenizer` :

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Notez que lorsque vous passez le `tokenizer` comme nous l'avons fait ici, le `data_collator` par d√©faut utilis√© par le `Trainer` sera un `DataCollatorWithPadding` comme d√©fini pr√©c√©demment. Ainsi, vous pouvez sauter la ligne `data_collator=data_collator` dans cet appel. Il √©tait quand m√™me important de vous montrer cette partie du traitement dans la section 2 !

Pour *finetuner* le mod√®le sur notre jeu de donn√©es, il suffit d'appeler la m√©thode `train()` de notre `Trainer` :

```py
trainer.train()
```

Cela lancera le *finetuning* (qui devrait prendre quelques minutes sur un GPU) et indiquera la perte d'entra√Ænement tous les 500 pas. Cependant, elle ne vous dira pas si votre mod√®le fonctionne bien (ou mal). Ceci est d√ª au fait que :

1. nous n'avons pas dit au `Trainer` d'√©valuer pendant l'entra√Ænement en r√©glant `evaluation_strategy` √† soit `"steps"` (√©valuer chaque `eval_steps`) ou `"epoch"` (√©valuer √† la fin de chaque *epoch*).
2. nous n'avons pas fourni au `Trainer` une fonction `compute_metrics()` pour calculer une m√©trique pendant ladite √©valuation (sinon l'√©valuation aurait juste affich√© la perte, qui n'est pas un nombre tr√®s intuitif).


### Evaluation

Voyons comment nous pouvons construire une fonction `compute_metrics()` utile et l'utiliser la prochaine fois que nous entra√Ænons. La fonction doit prendre un objet `EvalPrediction` (qui est un *tuple* nomm√© avec un champ `predictions` et un champ `label_ids`) et retournera un dictionnaire de cha√Ænes de caract√®res vers des flottants (les cha√Ænes de caract√®res √©tant les noms des m√©triques retourn√©es, et les flottants leurs valeurs). Pour obtenir des pr√©dictions de notre mod√®le, nous pouvons utiliser la commande `Trainer.predict()` :

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

La sortie de la m√©thode `predict()` est un autre *tuple* nomm√© avec trois champs : `predictions`, `label_ids`, et `metrics`. Le champ `metrics` contiendra juste la perte sur le jeu de donn√©es pass√©, ainsi que quelques mesures de temps (combien de temps il a fallu pour pr√©dire, au total et en moyenne). Une fois que nous aurons compl√©t√© notre fonction `compute_metrics()` et que nous l'aurons pass√© au `Trainer`, ce champ contiendra √©galement les m√©triques retourn√©es par `compute_metrics()`.

Comme vous pouvez le voir, `predictions` est un tableau bidimensionnel de forme 408 x 2 (408 √©tant le nombre d'√©l√©ments dans le jeu de donn√©es que nous avons utilis√©). Ce sont les logits pour chaque √©l√©ment du jeu de donn√©es que nous avons pass√© √† `predict()` (comme vous l'avez vu dans le [chapitre pr√©c√©dent](/course/fr/chapter2), tous les *transformers* retournent des logits). Pour les transformer en pr√©dictions que nous pouvons comparer √† nos √©tiquettes, nous devons prendre l'indice avec la valeur maximale sur le second axe :

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Nous pouvons maintenant comparer ces `preds` aux √©tiquettes. Pour construire notre fonction `compute_metric()`, nous allons nous appuyer sur les m√©triques de la biblioth√®que ü§ó [*Evaluate*](https://github.com/huggingface/evaluate/). Nous pouvons charger les m√©triques associ√©es au jeu de donn√©es MRPC aussi facilement que nous avons charg√© le jeu de donn√©es, cette fois avec la fonction `evaluate.load()`. L'objet retourn√© poss√®de une m√©thode `compute()` que nous pouvons utiliser pour effectuer le calcul de la m√©trique :

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Les r√©sultats exacts que vous obtiendrez peuvent varier, car l'initialisation al√©atoire de la t√™te du mod√®le peut modifier les m√©triques obtenues. Ici, nous pouvons voir que notre mod√®le a une pr√©cision de 85,78% sur l'ensemble de validation et un score F1 de 89,97. Ce sont les deux m√©triques utilis√©es pour √©valuer les r√©sultats sur le jeu de donn√©es MRPC pour le benchmark GLUE. Le tableau du papier de [BERT](https://arxiv.org/pdf/1810.04805.pdf) indique un score F1 de 88,9 pour le mod√®le de base. Il s'agissait du mod√®le `uncased` alors que nous utilisons actuellement le mod√®le `cased`, ce qui explique le meilleur r√©sultat.

En regroupant le tout, nous obtenons notre fonction `compute_metrics()` :

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Et pour le voir utilis√© en action pour rapporter les m√©triques √† la fin de chaque √©poque, voici comment nous d√©finissons un nouveau `Trainer` avec cette fonction `compute_metrics()` :

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Notez que nous cr√©ons un nouveau `TrainingArguments` avec sa `evaluation_strategy` d√©finie sur `"epoch"` et un nouveau mod√®le. Sinon, nous ne ferions que continuer l'entra√Ænement du mod√®le que nous avons d√©j√† entra√Æn√©. Pour lancer un nouveau cycle d'entra√Ænement, nous ex√©cutons :

```
trainer.train()
```

Cette fois, il indiquera la perte et les mesures de validation √† la fin de chaque √©poque, en plus de la perte d'entra√Ænement. Encore une fois, le score exact de pr√©cision/F1 que vous atteignez peut √™tre un peu diff√©rent de ce que nous avons trouv√©, en raison de l'initialisation al√©atoire de la t√™te du mod√®le, mais il devrait √™tre dans la m√™me fourchette.

Le `Trainer` fonctionnera sur plusieurs GPUs ou TPUs et fournit beaucoup d'options, comme l'entra√Ænement en pr√©cision mixte (utilisez `fp16 = True` dans vos arguments d'entra√Ænement). Nous passerons en revue tout ce qu'il supporte dans le chapitre 10.

Ceci conclut l'introduction au *fine-tuning* en utilisant l'API `Trainer`. Un exemple d'utilisation pour les t√¢ches de NLP les plus communes es donn√© dans le [chapitre 7](/course/fr/chapter7), mais pour l'instant regardons comment faire la m√™me chose en PyTorch pur.

<Tip>

‚úèÔ∏è **Essayez !** *Finetunez* un mod√®le sur le jeu de donn√©es GLUE SST-2, en utilisant le traitement des donn√©es que vous avez fait dans la section 2.

</Tip>
