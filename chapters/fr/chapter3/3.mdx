<FrameworkSwitchCourse {fw} />

# <i>Finetuner</i> un modÃ¨le avec l'API Trainer

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
    {label: "FranÃ§ais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter3/section3.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
    {label: "FranÃ§ais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

La bibliothÃ¨que ğŸ¤— *Transformers* fournit une classe `Trainer` pour vous aider Ã  *finetuner* n'importe lequel des modÃ¨les prÃ©-entraÃ®nÃ©s qu'elle met Ã  disposition sur votre jeu de donnÃ©es. Une fois que vous avez fait tout le travail de prÃ©traitement des donnÃ©es dans la derniÃ¨re section, il ne vous reste que quelques Ã©tapes pour dÃ©finir le `Trainer`. La partie la plus difficile sera probablement de prÃ©parer l'environnement pour exÃ©cuter `Trainer.train()`, car elle fonctionnera trÃ¨s lentement sur un CPU. Si vous n'avez pas de GPU, vous pouvez avoir accÃ¨s Ã  des GPUs ou TPUs gratuits sur [Google Colab](https://colab.research.google.com/).

Les exemples de code ci-dessous supposent que vous avez dÃ©jÃ  exÃ©cutÃ© les exemples de la section prÃ©cÃ©dente. Voici un bref rÃ©sumÃ© de ce dont vous avez besoin :

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### EntraÃ®nement

La premiÃ¨re Ã©tape avant de pouvoir dÃ©finir notre `Trainer` est de dÃ©finir une classe `TrainingArguments` qui contiendra tous les hyperparamÃ¨tres que le `Trainer` utilisera pour l'entraÃ®nement et l'Ã©valuation. Le seul argument que vous devez fournir est un rÃ©pertoire oÃ¹ le modÃ¨le entraÃ®nÃ© sera sauvegardÃ©, ainsi que les *checkpoints*. Pour tout le reste, vous pouvez laisser les valeurs par dÃ©faut, qui devraient fonctionner assez bien pour un *finetuning* de base.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

> [!TIP]
> ğŸ’¡ Si vous voulez tÃ©lÃ©charger automatiquement votre modÃ¨le sur le *Hub* pendant l'entraÃ®nement, passez `push_to_hub=True` dans le `TrainingArguments`. Nous en apprendrons plus Ã  ce sujet au [chapitre 4](/course/fr/chapter4/3).

La deuxiÃ¨me Ã©tape consiste Ã  dÃ©finir notre modÃ¨le. Comme dans le [chapitre prÃ©cÃ©dent](/course/fr/chapter2), nous utiliserons la classe `AutoModelForSequenceClassification`, avec deux labels :

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Vous remarquerez que contrairement au [chapitre 2](/course/fr/chapter2), vous obtenez un message d'avertissement aprÃ¨s l'instanciation de ce modÃ¨le prÃ©-entraÃ®nÃ©. C'est parce que BERT n'a pas Ã©tÃ© prÃ©-entraÃ®nÃ© Ã  la classification de paires de phrases, donc la tÃªte du modÃ¨le prÃ©-entraÃ®nÃ© a Ã©tÃ© supprimÃ©e et une nouvelle tÃªte adaptÃ©e Ã  la classification de sÃ©quences a Ã©tÃ© ajoutÃ©e Ã  la place. Les messages d'avertissement indiquent que certains poids n'ont pas Ã©tÃ© utilisÃ©s (ceux correspondant Ã  la tÃªte de prÃ©-entraÃ®nement abandonnÃ©e) et que d'autres ont Ã©tÃ© initialisÃ©s de maniÃ¨re alÃ©atoire (ceux pour la nouvelle tÃªte). Il conclut en vous encourageant Ã  entraÃ®ner le modÃ¨le, ce qui est exactement ce que nous allons faire maintenant.

Une fois que nous avons notre modÃ¨le, nous pouvons dÃ©finir un `Trainer` en lui passant tous les objets construits jusqu'Ã  prÃ©sent : le `model`, le `training_args`, les jeux de donnÃ©es d'entraÃ®nement et de validation, notre `data_collator`, et notre `tokenizer` :

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Notez que lorsque vous passez le `tokenizer` comme nous l'avons fait ici, le `data_collator` par dÃ©faut utilisÃ© par le `Trainer` sera un `DataCollatorWithPadding` comme dÃ©fini prÃ©cÃ©demment. Ainsi, vous pouvez sauter la ligne `data_collator=data_collator` dans cet appel. Il Ã©tait quand mÃªme important de vous montrer cette partie du traitement dans la section 2 !

Pour *finetuner* le modÃ¨le sur notre jeu de donnÃ©es, il suffit d'appeler la mÃ©thode `train()` de notre `Trainer` :

```py
trainer.train()
```

Cela lancera le *finetuning* (qui devrait prendre quelques minutes sur un GPU) et indiquera la perte d'entraÃ®nement tous les 500 pas. Cependant, elle ne vous dira pas si votre modÃ¨le fonctionne bien (ou mal). Ceci est dÃ» au fait que :

1. nous n'avons pas dit au `Trainer` d'Ã©valuer pendant l'entraÃ®nement en rÃ©glant `evaluation_strategy` Ã  soit `"steps"` (Ã©valuer chaque `eval_steps`) ou `"epoch"` (Ã©valuer Ã  la fin de chaque *epoch*).
2. nous n'avons pas fourni au `Trainer` une fonction `compute_metrics()` pour calculer une mÃ©trique pendant ladite Ã©valuation (sinon l'Ã©valuation aurait juste affichÃ© la perte, qui n'est pas un nombre trÃ¨s intuitif).


### Evaluation

Voyons comment nous pouvons construire une fonction `compute_metrics()` utile et l'utiliser la prochaine fois que nous entraÃ®nons. La fonction doit prendre un objet `EvalPrediction` (qui est un *tuple* nommÃ© avec un champ `predictions` et un champ `label_ids`) et retournera un dictionnaire de chaÃ®nes de caractÃ¨res vers des flottants (les chaÃ®nes de caractÃ¨res Ã©tant les noms des mÃ©triques retournÃ©es, et les flottants leurs valeurs). Pour obtenir des prÃ©dictions de notre modÃ¨le, nous pouvons utiliser la commande `Trainer.predict()` :

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

La sortie de la mÃ©thode `predict()` est un autre *tuple* nommÃ© avec trois champs : `predictions`, `label_ids`, et `metrics`. Le champ `metrics` contiendra juste la perte sur le jeu de donnÃ©es passÃ©, ainsi que quelques mesures de temps (combien de temps il a fallu pour prÃ©dire, au total et en moyenne). Une fois que nous aurons complÃ©tÃ© notre fonction `compute_metrics()` et que nous l'aurons passÃ© au `Trainer`, ce champ contiendra Ã©galement les mÃ©triques retournÃ©es par `compute_metrics()`.

Comme vous pouvez le voir, `predictions` est un tableau bidimensionnel de forme 408 x 2 (408 Ã©tant le nombre d'Ã©lÃ©ments dans le jeu de donnÃ©es que nous avons utilisÃ©). Ce sont les logits pour chaque Ã©lÃ©ment du jeu de donnÃ©es que nous avons passÃ© Ã  `predict()` (comme vous l'avez vu dans le [chapitre prÃ©cÃ©dent](/course/fr/chapter2), tous les *transformers* retournent des logits). Pour les transformer en prÃ©dictions que nous pouvons comparer Ã  nos Ã©tiquettes, nous devons prendre l'indice avec la valeur maximale sur le second axe :

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Nous pouvons maintenant comparer ces `preds` aux Ã©tiquettes. Pour construire notre fonction `compute_metric()`, nous allons nous appuyer sur les mÃ©triques de la bibliothÃ¨que ğŸ¤— [*Evaluate*](https://github.com/huggingface/evaluate/). Nous pouvons charger les mÃ©triques associÃ©es au jeu de donnÃ©es MRPC aussi facilement que nous avons chargÃ© le jeu de donnÃ©es, cette fois avec la fonction `evaluate.load()`. L'objet retournÃ© possÃ¨de une mÃ©thode `compute()` que nous pouvons utiliser pour effectuer le calcul de la mÃ©trique :

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Les rÃ©sultats exacts que vous obtiendrez peuvent varier, car l'initialisation alÃ©atoire de la tÃªte du modÃ¨le peut modifier les mÃ©triques obtenues. Ici, nous pouvons voir que notre modÃ¨le a une prÃ©cision de 85,78% sur l'ensemble de validation et un score F1 de 89,97. Ce sont les deux mÃ©triques utilisÃ©es pour Ã©valuer les rÃ©sultats sur le jeu de donnÃ©es MRPC pour le benchmark GLUE. Le tableau du papier de [BERT](https://arxiv.org/pdf/1810.04805.pdf) indique un score F1 de 88,9 pour le modÃ¨le de base. Il s'agissait du modÃ¨le `uncased` alors que nous utilisons actuellement le modÃ¨le `cased`, ce qui explique le meilleur rÃ©sultat.

En regroupant le tout, nous obtenons notre fonction `compute_metrics()` :

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Et pour le voir utilisÃ© en action pour rapporter les mÃ©triques Ã  la fin de chaque Ã©poque, voici comment nous dÃ©finissons un nouveau `Trainer` avec cette fonction `compute_metrics()` :

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Notez que nous crÃ©ons un nouveau `TrainingArguments` avec sa `evaluation_strategy` dÃ©finie sur `"epoch"` et un nouveau modÃ¨le. Sinon, nous ne ferions que continuer l'entraÃ®nement du modÃ¨le que nous avons dÃ©jÃ  entraÃ®nÃ©. Pour lancer un nouveau cycle d'entraÃ®nement, nous exÃ©cutons :

```
trainer.train()
```

Cette fois, il indiquera la perte et les mesures de validation Ã  la fin de chaque Ã©poque, en plus de la perte d'entraÃ®nement. Encore une fois, le score exact de prÃ©cision/F1 que vous atteignez peut Ãªtre un peu diffÃ©rent de ce que nous avons trouvÃ©, en raison de l'initialisation alÃ©atoire de la tÃªte du modÃ¨le, mais il devrait Ãªtre dans la mÃªme fourchette.

Le `Trainer` fonctionnera sur plusieurs GPUs ou TPUs et fournit beaucoup d'options, comme l'entraÃ®nement en prÃ©cision mixte (utilisez `fp16 = True` dans vos arguments d'entraÃ®nement). Nous passerons en revue tout ce qu'il supporte dans le chapitre 10.

Ceci conclut l'introduction au *fine-tuning* en utilisant l'API `Trainer`. Un exemple d'utilisation pour les tÃ¢ches de NLP les plus communes es donnÃ© dans le [chapitre 7](/course/fr/chapter7), mais pour l'instant regardons comment faire la mÃªme chose en PyTorch pur.

> [!TIP]
> âœï¸ **Essayez !** *Finetunez* un modÃ¨le sur le jeu de donnÃ©es GLUE SST-2, en utilisant le traitement des donnÃ©es que vous avez fait dans la section 2.
