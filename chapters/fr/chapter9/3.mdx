# Comprendre la classe <i>Interface</i>

Dans cette section, nous allons examiner de plus pr√®s la classe `Interface`, et comprendre les principaux param√®tres utilis√©s pour en cr√©er une.

## Comment cr√©er une interface

Vous remarquerez que la classe `Interface` a 3 param√®tres obligatoires :  

`Interface(fn, inputs, outputs, ...)`

Ces param√®tres sont :

  - `fn`: la fonction de pr√©diction qui est envelopp√©e par l'interface *Gradio*. Cette fonction peut prendre un ou plusieurs param√®tres et retourner une ou plusieurs valeurs.
  - `inputs`: le(s) type(s) de composant(s) d'entr√©e. *Gradio* fournit de nombreux composants pr√©construits tels que`"image"` ou `"mic"`. 
  - `outputs`: le(s) type(s) de composant(s) de sortie. Encore une fois, *Gradio* fournit de nombreux composants pr√©-construits, par ex. `"image"` ou `"label"`.

Pour une liste compl√®te des composants, [jetez un coup d'≈ìil √† la documentation de *Gradio*](https://gradio.app/docs). Chaque composant pr√©construit peut √™tre personnalis√© en instanciant la classe correspondant au composant. 

Par exemple, comme nous l'avons vu dans la [section pr√©c√©dente](/course/fr/chapter9/2), au lieu de passer le param√®tre `inputs` par `"textbox"`, vous pouvez passer un composant `Textbox(lines=7, label="Prompt")` pour cr√©er une zone de texte avec 7 lignes et un label.

Voyons un autre exemple, cette fois avec un composant `Audio`.

## Un exemple simple avec audio

Comme mentionn√© pr√©c√©demment,*Gradio* fournit de nombreuses entr√©es et sorties diff√©rentes. 
Construisons donc une `Interface` qui fonctionne avec l'audio.

Dans cet exemple, nous allons construire une fonction audio-vers-audio qui prend un fichier audio et l'inverse simplement.

Nous utiliserons comme entr√©e le composant `Audio`. Lorsque vous utilisez le composant `Audio`, vous pouvez sp√©cifier si vous voulez que la `source` de l'audio soit un fichier que l'utilisateur t√©l√©charge ou un microphone avec lequel l'utilisateur enregistre sa voix. Dans ce cas, nous allons choisir un "microphone". Juste pour le plaisir, nous allons ajouter une √©tiquette √† notre `Audio` qui dit ¬´ *Speak here...* ¬ª (Parler ici).

De plus, nous aimerions recevoir l'audio sous la forme d'un tableau numpy afin de pouvoir facilement l'inverser. Nous allons donc d√©finir le `"type"` comme √©tant `"numpy"`, ce qui permet de passer les donn√©es d'entr√©e comme un *tuple* de (`sample_rate`, `data`) dans notre fonction.

Nous utiliserons √©galement le composant de sortie `Audio` qui peut automatiquement rendre un *tuple* avec un taux d'√©chantillonnage et un tableau numpy de donn√©es comme un fichier audio lisible. 
Dans ce cas, nous n'avons pas besoin de faire de personnalisation, donc nous utiliserons le raccourci de la cha√Æne `"audio"`.


```py
import numpy as np
import gradio as gr


def reverse_audio(audio):
    sr, data = audio
    reversed_audio = (sr, np.flipud(data))
    return reversed_audio


mic = gr.Audio(source="microphone", type="numpy", label="Speak here...")
gr.Interface(reverse_audio, mic, "audio").launch()
```

Le code ci-dessus produira une interface comme celle qui suit (si votre navigateur ne vous demande pas l'autorisation pour acc√©der au microphone, <a href="https://huggingface.co/spaces/course-demos/audio-reverse" target="_blank">ouvrez la d√©mo dans un onglet s√©par√©</a>).

<iframe src="https://hf. space/gradioiframe/course-demos/audio-reverse/+" frameBorder="0" height="250" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer ; ambient-light-sensor ; autoplay ; battery ; camera ; document-domain ; encrypted-media ; fullscreen ; geolocation ; gyroscope ; layout-animations ; legacy-image-formats ; magn√©tom√®tre ; microphone ; midi ; oversized-images ; paiement ; image dans l'image ; publickey-credentials-get ; sync-xhr ; usb ; vr ; wake-lock ; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Vous devriez maintenant √™tre capable d'enregistrer votre voix et de vous entendre parler √† l'envers. Effrayant üëª !

## G√©rer les entr√©es et sorties multiples

Imaginons que nous ayons une fonction plus compliqu√©e, avec plusieurs entr√©es et sorties. 
Dans l'exemple ci-dessous, nous avons une fonction qui prend un index de liste d√©roulante, une valeur de curseur et un nombre, et renvoie un √©chantillon audio d'une tonalit√© musicale. 

Regardez comment nous passons une liste de composants d'entr√©e et de sortie, et voyez si vous pouvez suivre ce qui se passe.

La cl√© ici est que lorsque vous passez :
* une liste de composants d'entr√©e, chaque composant correspond √† un param√®tre dans l'ordre.
* une liste de composants de sortie, chaque composant correspond √† une valeur retourn√©e.

L'extrait de code ci-dessous montre comment trois composants d'entr√©e correspondent aux trois arguments de la fonction `generate_tone()` :

```py
import numpy as np
import gradio as gr

notes = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]


def generate_tone(note, octave, duration):
    sr = 48000
    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)
    frequency = a4_freq * 2 ** (tones_from_a4 / 12)
    duration = int(duration)
    audio = np.linspace(0, duration, duration * sr)
    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)
    return (sr, audio)


gr.Interface(
    generate_tone,
    [
        gr.Dropdown(notes, type="index"),
        gr.Slider(minimum=4, maximum=6, step=1),
        gr.Textbox(type="number", value=1, label="Duration in seconds"),
    ],
    "audio",
).launch()
```

<iframe src="https://hf.space/gradioiframe/course-demos/generate-tone/+" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


### La m√©thode `launch()`.

Jusqu'√† pr√©sent, nous avons utilis√© la m√©thode `launch()` pour lancer l'interface, mais nous n'avons pas vraiment discut√© de ce qu'elle fait. 

Par d√©faut, la m√©thode `launch()` lancera la d√©mo dans un serveur web qui tourne localement. Si vous ex√©cutez votre code dans un *notebook* Jupyter ou Colab, *Gradio* va int√©grer l'interface graphique de la d√©mo dans le *notebook* afin que vous puissiez l'utiliser facilement.

Vous pouvez personnaliser le comportement de `launch()` √† travers diff√©rents param√®tres :

  - `inline` : si vous voulez afficher l'interface en ligne sur les *notebooks* Python.
  - `inbrowser` : pour lancer automatiquement l'interface dans un nouvel onglet du navigateur par d√©faut.
  - `share` : si vous voulez cr√©er un lien public partageable depuis votre ordinateur pour l'interface. Un peu comme un lien Google Drive !

Nous couvrirons le param√®tre `share` plus en d√©tail dans la section suivante !

## ‚úèÔ∏è Appliquons-le !

Construisons une interface qui vous permette de faire la d√©monstration d'un mod√®le de **reconnaissance vocale**.
Pour rendre la chose int√©ressante, nous accepterons *soit* une entr√©e micro, soit un fichier t√©l√©charg√©.

Comme d'habitude, nous allons charger notre mod√®le de reconnaissance vocale en utilisant la fonction `pipeline()` de ü§ó *Transformers*. 
Si vous avez besoin d'un rafra√Æchissement rapide, vous pouvez revenir √† [cette section du chapitre 1](/course/fr/chapter1/3). Ensuite, nous allons impl√©menter une fonction `transcribe_audio()` qui traite l'audio et retourne la transcription (en anglais). Enfin, nous allons envelopper cette fonction dans une `Interface` avec les composants `Audio` pour les entr√©es et juste le texte pour la sortie. Au total, le code de cette application est le suivant :

```py
from transformers import pipeline
import gradio as gr

model = pipeline("automatic-speech-recognition")


def transcribe_audio(mic=None, file=None):
    if mic is not None:
        audio = mic
    elif file is not None:
        audio = file
    else:
        return "You must either provide a mic recording or a file"
    transcription = model(audio)["text"]
    return transcription


gr.Interface(
    fn=transcribe_audio,
    inputs=[
        gr.Audio(source="microphone", type="filepath", optional=True),
        gr.Audio(source="upload", type="filepath", optional=True),
    ],
    outputs="text",
).launch()
```

Si votre navigateur ne vous demande pas l'autorisation pour acc√©der au microphone, <a href="https://huggingface.co/spaces/course-demos/audio-reverse" target="_blank">ouvrez la d√©mo dans un onglet s√©par√©</a>.

<iframe src="https://hf. space/gradioiframe/course-demos/asr/+" frameBorder="0" height="550" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer ; ambient-light-sensor ; autoplay ; battery ; camera ; document-domain ; encrypted-media ; fullscreen ; geolocation ; gyroscope ; layout-animations ; legacy-image-formats ; magn√©tom√®tre ; microphone ; midi ; images surdimensionn√©es ; paiement ; image dans l'image ; publickey-credentials-get ; sync-xhr ; usb ; vr ; wake-lock ; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>


Voil√†, c'est fait ! Vous pouvez maintenant utiliser cette interface pour transcrire de l'audio. Remarquez ici qu'en passant le param√®tre `optional` √† `True`, nous permettons √† l'utilisateur de soit fournir un microphone ou un fichier audio (ou aucun des deux, mais cela retournera un message d'erreur).

Continuez pour voir comment partager votre interface avec d'autres !