# Il est temps de trancher et de dÃ©couper

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section3.ipynb"},
]} />

La plupart du temps, les donnÃ©es avec lesquelles vous travaillez ne seront pas parfaitement prÃ©parÃ©es pour les modÃ¨les de formation. Dans cette section, nous allons explorer les diffÃ©rentes fonctionnalitÃ©s fournies par ğŸ¤— Datasets pour nettoyer vos ensembles de donnÃ©es.

<Youtube id="tqfSFcPMgOI"/>

## Trancher et dÃ©couper nos donnÃ©es

Semblable Ã  Pandas, ğŸ¤— Datasets fournit plusieurs fonctions pour manipuler le contenu des objets `Dataset` et `DatasetDict`. Nous avons dÃ©jÃ  rencontrÃ© la mÃ©thode `Dataset.map()` dans le [Chapitre 3](/course/chapter3), et dans cette section nous allons explorer certaines des autres fonctions Ã  notre disposition.

Pour cet exemple, nous utiliserons le [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) qui est hÃ©bergÃ© sur [UC Irvine Machine Learning Repository] (https://archive.ics.uci.edu/ml/index.php), qui contient des avis de patients sur divers mÃ©dicaments, ainsi que la condition traitÃ©e et une note de 10 Ã©toiles sur la satisfaction du patient.

Nous devons d'abord tÃ©lÃ©charger et extraire les donnÃ©es, ce qui peut Ãªtre fait avec les commandes `wget` et `unzip`Â :

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

Ã‰tant donnÃ© que TSV n'est qu'une variante de CSV qui utilise des tabulations au lieu de virgules comme sÃ©parateurs, nous pouvons charger ces fichiers en utilisant le script de chargement `csv` et en spÃ©cifiant l'argument `delimiter` dans la fonction `load_dataset()` comme suitÂ :

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t is the tab character in Python
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

Une bonne pratique lors de toute sorte d'analyse de donnÃ©es consiste Ã  prÃ©lever un petit Ã©chantillon alÃ©atoire pour avoir une idÃ©e rapide du type de donnÃ©es avec lesquelles vous travaillez. Dans ğŸ¤— Datasets, nous pouvons crÃ©er un Ã©chantillon alÃ©atoire en enchaÃ®nant les fonctions `Dataset.shuffle()` et `Dataset.select()`Â :

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# Peek at the first few examples
drug_sample[:3]
```

```python out
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

Notez que nous avons corrigÃ© la graine dans `Dataset.shuffle()` Ã  des fins de reproductibilitÃ©. `Dataset.select()` attend un itÃ©rable d'indices, nous avons donc passÃ© `range(1000)` pour rÃ©cupÃ©rer les 1 000 premiers exemples de l'ensemble de donnÃ©es mÃ©langÃ©. Ã€ partir de cet Ã©chantillon, nous pouvons dÃ©jÃ  voir quelques bizarreries dans notre ensemble de donnÃ©esÂ :

* La colonne "Sans nomÂ :Â 0" ressemble Ã©trangement Ã  un identifiant anonyme pour chaque patient.
* La colonne "condition" comprend un mÃ©lange d'Ã©tiquettes en majuscules et en minuscules.
* Les avis sont de longueur variable et contiennent un mÃ©lange de sÃ©parateurs de lignes Python (`\r\n`) ainsi que des codes de caractÃ¨res HTML comme `&\#039;`.

Voyons comment nous pouvons utiliser ğŸ¤— Datasets pour traiter chacun de ces problÃ¨mes. Pour tester l'hypothÃ¨se de l'ID patient pour la colonne `UnnamedÂ : 0`, nous pouvons utiliser la fonction `Dataset.unique()` pour vÃ©rifier que le nombre d'ID correspond au nombre de lignes dans chaque divisionÂ :

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

Cela semble confirmer notre hypothÃ¨se, alors nettoyons un peu l'ensemble de donnÃ©es en renommant la colonne `Unnamed: 0` en quelque chose d'un peu plus interprÃ©table. Nous pouvons utiliser la fonction `DatasetDict.rename_column()` pour renommer la colonne sur les deux divisions en une seule foisÂ :

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<Tip>

âœï¸ **Essayez-leÂ !** Utilisez la fonction "Dataset.unique()" pour trouver le nombre de mÃ©dicaments et de conditions uniques dans les ensembles d'entraÃ®nement et de test.

</Tip>

Ensuite, normalisons toutes les Ã©tiquettes `condition` en utilisant `Dataset.map()`. Comme nous l'avons fait avec la tokenisation dans le [chapitre 3](/course/chapter3), nous pouvons dÃ©finir une fonction simple qui peut Ãªtre appliquÃ©e sur toutes les lignes de chaque division dans `drug_dataset`Â :

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}


drug_dataset.map(lowercase_condition)
```

```python out
AttributeError: 'NoneType' object has no attribute 'lower'
```

Oh non, nous avons rencontrÃ© un problÃ¨me avec notre fonction de carteÂ ! Ã€ partir de l'erreur, nous pouvons dÃ©duire que certaines des entrÃ©es de la colonne "condition" sont "Aucune", qui ne peuvent pas Ãªtre mises en minuscules car ce ne sont pas des chaÃ®nes. Supprimons ces lignes en utilisant `Dataset.filter()`, qui fonctionne de maniÃ¨re similaire Ã  `Dataset.map()` et attend une fonction qui reÃ§oit un seul exemple de l'ensemble de donnÃ©es. Au lieu d'Ã©crire une fonction explicite comme :

```py
def filter_nones(x):
    return x["condition"] is not None
```

puis en exÃ©cutant `drug_dataset.filter(filter_nones)`, nous pouvons le faire en une seule ligne en utilisant une _lambda function_. En Python, les fonctions lambda sont de petites fonctions que vous pouvez dÃ©finir sans les nommer explicitement. Ils prennent la forme gÃ©nÃ©rale :

```
lambda <arguments> : <expression>
```

oÃ¹ `lambda` est l'un des [mots clÃ©s] spÃ©ciaux de Python (https://docs.python.org/3/reference/lexical_analysis.html#keywords), `<arguments>` est une liste/ensemble de valeurs sÃ©parÃ©es par des virgules qui dÃ©finissent les entrÃ©es de la fonction et `<expression>` reprÃ©sente les opÃ©rations que vous souhaitez exÃ©cuter. Par exemple, nous pouvons dÃ©finir une simple fonction lambda qui met au carrÃ© un nombre comme suitÂ :

```
lambda x : x * x
```

Pour appliquer cette fonction Ã  une entrÃ©e, nous devons l'envelopper ainsi que l'entrÃ©e entre parenthÃ¨sesÂ :

```py
(lambda x: x * x)(3)
```

```python out
9
```

De mÃªme, nous pouvons dÃ©finir des fonctions lambda avec plusieurs arguments en les sÃ©parant par des virgules. Par exemple, nous pouvons calculer l'aire d'un triangle comme suit :

```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

Les fonctions Lambda sont pratiques lorsque vous souhaitez dÃ©finir de petites fonctions Ã  usage unique (pour plus d'informations Ã  leur sujet, nous vous recommandons de lire l'excellent [tutoriel Real Python](https://realpython.com/python-lambda/) d'AndrÃ© Burgaud) . Dans le contexte ğŸ¤— Datasets, nous pouvons utiliser des fonctions lambda pour dÃ©finir des opÃ©rations simples de mappage et de filtrage, alors utilisons cette astuce pour Ã©liminer les entrÃ©es "None" dans notre jeu de donnÃ©esÂ :

```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

Avec les entrÃ©es "None" supprimÃ©es, nous pouvons normaliser notre colonne "condition"Â :

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# Check that lowercasing worked
drug_dataset["train"]["condition"][:3]
```

```python out
['left ventricular dysfunction', 'adhd', 'birth control']
```

Ã‡a marche ! Maintenant que nous avons nettoyÃ© les Ã©tiquettes, examinons le nettoyage des avis eux-mÃªmes.

## CrÃ©ation de nouvelles colonnes

Chaque fois que vous avez affaire Ã  des avis de clients, une bonne pratique consiste Ã  vÃ©rifier le nombre de mots dans chaque avis. Une critique peut Ãªtre un simple mot comme "GÃ©nialÂ !" ou un essai complet avec des milliers de mots, et selon le cas d'utilisation, vous devrez gÃ©rer ces extrÃªmes diffÃ©remment. Pour calculer le nombre de mots dans chaque rÃ©vision, nous utiliserons une heuristique approximative basÃ©e sur la division de chaque texte par des espaces.

DÃ©finissons une fonction simple qui compte le nombre de mots dans chaque avisÂ :

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

Contrairement Ã  notre fonction `lowercase_condition()`, `compute_review_length()` renvoie un dictionnaire dont la clÃ© ne correspond pas Ã  l'un des noms de colonne de l'ensemble de donnÃ©es. Dans ce cas, lorsque `compute_review_length()` est passÃ© Ã  `Dataset.map()`, il sera appliquÃ© Ã  toutes les lignes du jeu de donnÃ©es pour crÃ©er une nouvelle colonne `review_length`Â :

```py
drug_dataset = drug_dataset.map(compute_review_length)
# Inspect the first training example
drug_dataset["train"][0]
```

```python out
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

Comme prÃ©vu, nous pouvons voir qu'une colonne "review_length" a Ã©tÃ© ajoutÃ©e Ã  notre ensemble d'entraÃ®nement. Nous pouvons trier cette nouvelle colonne avec `Dataset.sort()` pour voir Ã  quoi ressemblent les valeurs extrÃªmesÂ :

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python out
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

Comme nous le soupÃ§onnions, certaines critiques ne contiennent qu'un seul mot, ce qui, bien que cela puisse convenir Ã  l'analyse des sentiments, ne serait pas informatif si nous voulons prÃ©dire la condition.

<Tip>

ğŸ™‹ Une autre faÃ§on d'ajouter de nouvelles colonnes Ã  un ensemble de donnÃ©es consiste Ã  utiliser la fonction `Dataset.add_column()`. Cela vous permet de fournir la colonne sous forme de liste Python ou de tableau NumPy et peut Ãªtre utile dans les situations oÃ¹ `Dataset.map()` n'est pas bien adaptÃ© Ã  votre analyse.

</Tip>

Utilisons la fonction `Dataset.filter()` pour supprimer les avis contenant moins de 30 mots. De la mÃªme maniÃ¨re que nous l'avons fait avec la colonne "condition", nous pouvons filtrer les avis trÃ¨s courts en exigeant que les avis aient une longueur supÃ©rieure Ã  ce seuilÂ :

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python out
{'train': 138514, 'test': 46108}
```

Comme vous pouvez le constater, cela a supprimÃ© environ 15Â % des avis de nos ensembles d'entraÃ®nement et de test d'origine.

<Tip>

âœï¸ **Essayez-le !** Utilisez la fonction `Dataset.sort()` pour inspecter les avis avec le plus grand nombre de mots. Consultez la [documentation](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) pour voir quel argument vous devez utiliser pour trier les avis par longueur dans l'ordre dÃ©croissant.

</Tip>

La derniÃ¨re chose Ã  laquelle nous devons faire face est la prÃ©sence de codes de caractÃ¨res HTML dans nos avis. Nous pouvons utiliser le module `html` de Python pour supprimer ces caractÃ¨res, comme ceciÂ :

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python out
"I'm a transformer called BERT"
```

Nous utiliserons `Dataset.map()` pour dÃ©masquer tous les caractÃ¨res HTML de notre corpusÂ :

```python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

Comme vous pouvez le voir, la mÃ©thode `Dataset.map()` est trÃ¨s utile pour le traitement des donnÃ©es -- et nous n'avons mÃªme pas effleurÃ© la surface de tout ce qu'elle peut faireÂ !

## Les superpouvoirs de la mÃ©thode `map()`

La mÃ©thode `Dataset.map()` prend un argument `batched` qui, s'il est dÃ©fini sur `True`, l'amÃ¨ne Ã  envoyer un lot d'exemples Ã  la fonction map en une seule fois (la taille du lot est configurable mais par dÃ©faut Ã  1 000). Par exemple, la fonction de carte prÃ©cÃ©dente qui dÃ©gageait tout le code HTML prenait un peu de temps Ã  s'exÃ©cuter (vous pouvez lire le temps pris dans les barres de progression). On peut accÃ©lÃ©rer cela en traitant plusieurs Ã©lÃ©ments en mÃªme temps Ã  l'aide d'une liste en comprÃ©hension.

Lorsque vous spÃ©cifiez `batched=True`, la fonction reÃ§oit un dictionnaire avec les champs de l'ensemble de donnÃ©es, mais chaque valeur est maintenant une _liste de valeurs_, et non plus une seule valeur. La valeur de retour de `Dataset.map()` devrait Ãªtre la mÃªmeÂ : un dictionnaire avec les champs que nous voulons mettre Ã  jour ou ajouter Ã  notre ensemble de donnÃ©es, et une liste de valeurs. Par exemple, voici une autre faÃ§on de supprimer tous les caractÃ¨res HTML, mais en utilisant `batched=True`Â :

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

Si vous exÃ©cutez ce code dans un notebook, vous verrez que cette commande s'exÃ©cute beaucoup plus rapidement que la prÃ©cÃ©dente. Et ce n'est pas parce que nos critiques ont dÃ©jÃ  Ã©tÃ© sans Ã©chappement HTML -- si vous rÃ©-exÃ©cutez l'instruction de la section prÃ©cÃ©dente (sans `batched=True`), cela prendra le mÃªme temps qu'avant. En effet, les comprÃ©hensions de liste sont gÃ©nÃ©ralement plus rapides que l'exÃ©cution du mÃªme code dans une boucle "for", et nous gagnons Ã©galement en performances en accÃ©dant Ã  de nombreux Ã©lÃ©ments en mÃªme temps au lieu d'un par un.

L'utilisation de `Dataset.map()` avec `batched=True` sera essentielle pour dÃ©bloquer la vitesse des tokenizers "rapides" que nous rencontrerons dans [Chapitre 6](/course/chapter6), qui peuvent rapidement tokeniser de grandes listes de textes. Par exemple, pour tokeniser toutes les revues de mÃ©dicaments avec un tokenizer rapide, nous pourrions utiliser une fonction comme celle-ciÂ :

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

Comme vous l'avez vu dans le [Chapitre 3](/course/chapter3), nous pouvons passer un ou plusieurs exemples au tokenizer, nous pouvons donc utiliser cette fonction avec ou sans `batched=True`. Profitons-en pour comparer les performances des diffÃ©rentes options. Dans un cahier, vous pouvez chronomÃ©trer une instruction d'une ligne en ajoutant `%time` avant la ligne de code que vous souhaitez mesurerÂ :

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

Vous pouvez Ã©galement chronomÃ©trer une cellule entiÃ¨re en mettant `%%time` au dÃ©but de la cellule. Sur le matÃ©riel sur lequel nous avons exÃ©cutÃ© cela, il affichait 10,8 s pour cette instruction (c'est le nombre Ã©crit aprÃ¨s "Wall time").

<Tip>

âœï¸ **Essayez-leÂ !** ExÃ©cutez la mÃªme instruction avec et sans `batched=True`, puis essayez-le avec un tokenizer lent (ajoutez `use_fast=False` dans la mÃ©thode `AutoTokenizer.from_pretrained()`) afin que vous puissiez voir quels numÃ©ros vous obtenez sur votre matÃ©riel.

</Tip>

Voici les rÃ©sultats que nous avons obtenus avec et sans batching, avec un tokenizer rapide et un lent :

Options         | Tokenizer rapide | Tokenizer lent
:--------------:|:----------------:|:-----------------:
`batched=True`  | 10.8s            | 4min41s
`batched=False` | 59.2s            | 5min3s

Cela signifie que l'utilisation d'un tokenizer rapide avec l'option `batched=True` est 30 fois plus rapide que son homologue lent sans traitement par lot -- c'est vraiment incroyableÂ ! C'est la raison principale pour laquelle les tokenizers rapides sont la valeur par dÃ©faut lors de l'utilisation de `AutoTokenizer` (et pourquoi ils sont appelÃ©s "rapides"). Ils sont capables d'atteindre une telle accÃ©lÃ©ration car dans les coulisses, le code de tokenisation est exÃ©cutÃ© dans Rust, qui est un langage qui facilite la parallÃ©lisation de l'exÃ©cution du code.

La parallÃ©lisation est Ã©galement la raison de l'accÃ©lÃ©ration de prÃ¨s de 6 fois obtenue par le fast tokenizer avec le traitement par lotsÂ : vous ne pouvez pas parallÃ©liser une seule opÃ©ration de tokenisation, mais lorsque vous souhaitez tokeniser de nombreux textes en mÃªme temps, vous pouvez simplement rÃ©partir l'exÃ©cution sur plusieurs processus. chacun responsable de ses propres textes.

`Dataset.map()` possÃ¨de Ã©galement ses propres capacitÃ©s de parallÃ©lisation. Comme ils ne sont pas soutenus par Rust, ils ne laisseront pas un tokenizer lent rattraper un rapide, mais ils peuvent toujours Ãªtre utiles (surtout si vous utilisez un tokenizer qui n'a pas de version rapide). Pour activer le multitraitement, utilisez l'argument `num_proc` et spÃ©cifiez le nombre de processus Ã  utiliser dans votre appel Ã  `Dataset.map()`Â :

```py
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

Vous pouvez expÃ©rimenter un peu le timing pour dÃ©terminer le nombre optimal de processus Ã  utiliser ; dans notre cas 8 semblait produire le meilleur gain de vitesse. Voici les chiffres que nous avons obtenus avec et sans multitraitementÂ :

Options                       | Tokenizer rapide | Tokenizer lent
:----------------------------:|:----------------:|:---------------:
`batched=True`                | 10.8s            | 4min41s
`batched=False`               | 59.2s            | 5min3s
`batched=True`, `num_proc=8`  | 6.52s            | 41.3s
`batched=False`, `num_proc=8` | 9.49s            | 45.2s

Ce sont des rÃ©sultats beaucoup plus raisonnables pour le tokenizer lent, mais les performances du tokenizer rapide ont Ã©galement Ã©tÃ© considÃ©rablement amÃ©liorÃ©es. Notez, cependant, que ce ne sera pas toujours le cas - pour les valeurs de `num_proc` autres que 8, nos tests ont montrÃ© qu'il Ã©tait plus rapide d'utiliser `batched=True` sans cette option. En gÃ©nÃ©ral, nous ne recommandons pas d'utiliser le multitraitement Python pour les tokenizers rapides avec `batched=True`.

<Tip>

Utiliser `num_proc` pour accÃ©lÃ©rer votre traitement est gÃ©nÃ©ralement une bonne idÃ©e, tant que la fonction que vous utilisez n'effectue pas dÃ©jÃ  une sorte de multitraitement.

</Tip>

Toutes ces fonctionnalitÃ©s condensÃ©es en une seule mÃ©thode sont dÃ©jÃ  assez Ã©tonnantes, mais il y a plus ! Avec `Dataset.map()` et `batched=True` vous pouvez modifier le nombre d'Ã©lÃ©ments dans votre jeu de donnÃ©es. Ceci est trÃ¨s utile dans de nombreuses situations oÃ¹ vous souhaitez crÃ©er plusieurs fonctionnalitÃ©s d'entraÃ®nement Ã  partir d'un exemple, et nous devrons le faire dans le cadre du prÃ©traitement de plusieurs des tÃ¢ches NLP que nous entreprendrons dans [Chapitre 7](/course/ Chapitre 7).

<Tip>

ğŸ’¡ En machine learning, un _example_ est gÃ©nÃ©ralement dÃ©fini comme l'ensemble de _features_ que nous alimentons au modÃ¨le. Dans certains contextes, ces caractÃ©ristiques seront l'ensemble des colonnes d'un `Dataset`, mais dans d'autres (comme ici et pour la rÃ©ponse aux questions), plusieurs caractÃ©ristiques peuvent Ãªtre extraites d'un seul exemple et appartenir Ã  une seule colonne.

</Tip>

Voyons comment cela fonctionneÂ ! Ici, nous allons tokeniser nos exemples et les tronquer Ã  une longueur maximale de 128, mais nous demanderons au tokenizer de renvoyer *tous* les morceaux des textes au lieu du premier. Cela peut Ãªtre fait avec `return_overflowing_tokens=True`Â :

```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

Testons cela sur un exemple avant d'utiliser `Dataset.map()` sur l'ensemble de donnÃ©esÂ :

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python out
[128, 49]
```

Ainsi, notre premier exemple dans l'ensemble de formation est devenu deux fonctionnalitÃ©s car il a Ã©tÃ© segmentÃ© Ã  plus que le nombre maximum de jetons que nous avons spÃ©cifiÃ©Â : le premier de longueur 128 et le second de longueur 49. Faisons maintenant cela pour tous les Ã©lÃ©ments du base de donnÃ©es!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python out
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

Oh non! Cela n'a pas fonctionnÃ© ! Pourquoi pas? L'examen du message d'erreur nous donnera un indiceÂ : il y a une incompatibilitÃ© dans les longueurs de l'une des colonnes, l'une Ã©tant de longueur 1Â 463 et l'autre de longueur 1Â 000. Si vous avez consultÃ© la [documentation] `Dataset.map()`(https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map), vous vous souviendrez peut-Ãªtre qu'il s'agit du nombre d'Ã©chantillons passÃ©s Ã  la fonction que nous mapponsÂ ; ici, ces 1 000 exemples ont donnÃ© 1 463 nouvelles fonctionnalitÃ©s, entraÃ®nant une erreur de forme.

Le problÃ¨me est que nous essayons de mÃ©langer deux ensembles de donnÃ©es diffÃ©rents de tailles diffÃ©rentesÂ : les colonnes `drug_dataset` auront un certain nombre d'exemples (les 1 000 dans notre erreur), mais le `tokenized_dataset` que nous construisons en aura plus (le 1 463 dans le message d'erreur). Cela ne fonctionne pas pour un `Dataset`, nous devons donc soit supprimer les colonnes de l'ancien jeu de donnÃ©es, soit leur donner la mÃªme taille que dans le nouveau jeu de donnÃ©es. Nous pouvons faire le premier avec l'argument `remove_columns`Â :

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

Maintenant, cela fonctionne sans erreur. Nous pouvons vÃ©rifier que notre nouveau jeu de donnÃ©es contient beaucoup plus d'Ã©lÃ©ments que le jeu de donnÃ©es d'origine en comparant les longueursÂ :

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

Nous avons mentionnÃ© que nous pouvions Ã©galement rÃ©soudre le problÃ¨me de longueur non concordante en donnant aux anciennes colonnes la mÃªme taille que les nouvelles. Pour ce faire, nous aurons besoin du champ `overflow_to_sample_mapping` que le tokenizer renvoie lorsque nous dÃ©finissons `return_overflowing_tokens=True`. Il nous donne une correspondance entre un nouvel index de fonctionnalitÃ© et l'index de l'Ã©chantillon dont il est issu. GrÃ¢ce Ã  cela, nous pouvons associer chaque clÃ© prÃ©sente dans notre jeu de donnÃ©es d'origine Ã  une liste de valeurs de la bonne taille en rÃ©pÃ©tant les valeurs de chaque exemple autant de fois qu'il gÃ©nÃ¨re de nouvelles fonctionnalitÃ©sÂ :

```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extract mapping between new and old indices
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

Nous pouvons voir que cela fonctionne avec `Dataset.map()` sans que nous ayons besoin de supprimer les anciennes colonnesÂ :

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

Nous obtenons le mÃªme nombre de fonctionnalitÃ©s d'entraÃ®nement qu'auparavant, mais ici nous avons conservÃ© tous les anciens champs. Si vous en avez besoin pour un post-traitement aprÃ¨s l'application de votre modÃ¨le, vous pouvez utiliser cette approche.

Vous avez maintenant vu comment ğŸ¤— Datasets peut Ãªtre utilisÃ© pour prÃ©traiter un ensemble de donnÃ©es de diffÃ©rentes maniÃ¨res. Bien que les fonctions de traitement de ğŸ¤— Datasets couvrent la plupart de vos besoins de formation de modÃ¨les,
il peut arriver que vous deviez passer Ã  Pandas pour accÃ©der Ã  des fonctionnalitÃ©s plus puissantes, telles que `DataFrame.groupby()` ou des API de haut niveau pour la visualisation. Heureusement, ğŸ¤— Datasets est conÃ§u pour Ãªtre interopÃ©rable avec des bibliothÃ¨ques telles que Pandas, NumPy, PyTorch, TensorFlow et JAX. Voyons comment cela fonctionne.

## De `Dataset`s Ã  `DataFrame`s et vice versa

<Youtube id="tfcY1067A5Q"/>

Pour permettre la conversion entre diverses bibliothÃ¨ques tierces, ğŸ¤— Datasets fournit une fonction `Dataset.set_format()`. Cette fonction ne modifie que le _format de sortie_ de l'ensemble de donnÃ©es, vous pouvez donc facilement passer Ã  un autre format sans affecter le _format de donnÃ©es_ sous-jacent, qui est Apache Arrow. Le formatage se fait sur place. Pour dÃ©montrer, convertissons notre jeu de donnÃ©es en PandasÂ :

```py
drug_dataset.set_format("pandas")
```

Maintenant, lorsque nous accÃ©dons aux Ã©lÃ©ments du jeu de donnÃ©es, nous obtenons un `pandas.DataFrame` au lieu d'un dictionnaireÂ :

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

CrÃ©ons un `pandas.DataFrame` pour l'ensemble d'entraÃ®nement en sÃ©lectionnant tous les Ã©lÃ©ments de `drug_dataset["train"]`Â :

```py
train_df = drug_dataset["train"][:]
```

<Tip>

ğŸš¨ Sous le capot, `Dataset.set_format()` change le format de retour pour la mÃ©thode dunder `__getitem__()` de l'ensemble de donnÃ©es. Cela signifie que lorsque nous voulons crÃ©er un nouvel objet comme `train_df` Ã  partir d'un `Dataset` au format `"pandas"`, nous devons dÃ©couper tout l'ensemble de donnÃ©es pour obtenir un `pandas.DataFrame`. Vous pouvez vÃ©rifier par vous-mÃªme que le type de `drug_dataset["train"]` est `Dataset`, quel que soit le format de sortie.

</Tip>


De lÃ , nous pouvons utiliser toutes les fonctionnalitÃ©s Pandas que nous voulons. Par exemple, nous pouvons faire un chaÃ®nage sophistiquÃ© pour calculer la distribution de classe parmi les entrÃ©es `condition`Â :

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>


Et une fois que nous avons terminÃ© notre analyse Pandas, nous pouvons toujours crÃ©er un nouvel objet `Dataset` en utilisant la fonction `Dataset.from_pandas()` comme suitÂ :


```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python out
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<Tip>

âœï¸ **Essayez-leÂ !** Calculez la note moyenne par mÃ©dicament et stockez le rÃ©sultat dans un nouvel ensemble de donnÃ©es.

</Tip>

Ceci conclut notre visite des diffÃ©rentes techniques de prÃ©traitement disponibles dans ğŸ¤— Datasets. Pour complÃ©ter la section, crÃ©ons un ensemble de validation pour prÃ©parer l'ensemble de donnÃ©es pour la formation d'un classificateur. Avant cela, nous allons rÃ©initialiser le format de sortie de `drug_dataset` de `"pandas"` Ã  `"arrow"`Â :

```python
drug_dataset.reset_format()
```

## CrÃ©ation d'un ensemble de validation

Bien que nous ayons un jeu de test que nous pourrions utiliser pour l'Ã©valuation, il est recommandÃ© de ne pas toucher au jeu de test et de crÃ©er un jeu de validation sÃ©parÃ© pendant le dÃ©veloppement. Une fois que vous Ãªtes satisfait des performances de vos modÃ¨les sur l'ensemble de validation, vous pouvez effectuer une derniÃ¨re vÃ©rification d'intÃ©gritÃ© sur l'ensemble de test. Ce processus permet d'attÃ©nuer le risque de dÃ©passement de l'ensemble de test et de dÃ©ploiement d'un modÃ¨le qui Ã©choue sur des donnÃ©es du monde rÃ©el.

ğŸ¤— Datasets fournit une fonction `Dataset.train_test_split()` basÃ©e sur la cÃ©lÃ¨bre fonctionnalitÃ© de `scikit-learn`. Utilisons-le pour diviser notre ensemble d'entraÃ®nement en divisions "train" et "validation" (nous dÃ©finissons l'argument "seed" pour la reproductibilitÃ©)Â :

```py
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# Rename the default "test" split to "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# Add the "test" set to our `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

GÃ©nial, nous avons maintenant prÃ©parÃ© un jeu de donnÃ©es prÃªt pour l'entraÃ®nement de certains modÃ¨lesÂ ! Dans la [section 5](/course/chapter5/5), nous vous montrerons comment tÃ©lÃ©charger des ensembles de donnÃ©es sur le Hugging Face Hub, mais pour l'instant, terminons notre analyse en examinant quelques faÃ§ons d'enregistrer des ensembles de donnÃ©es sur votre ordinateur local. 

## Enregistrer un jeu de donnÃ©es

<Youtube id="blF9uxYcKHo"/>

Bien que ğŸ¤— Datasets mette en cache chaque jeu de donnÃ©es tÃ©lÃ©chargÃ© et les opÃ©rations qui y sont effectuÃ©es, il y a des moments oÃ¹ vous voudrez enregistrer un jeu de donnÃ©es sur le disque (par exemple, au cas oÃ¹ le cache serait supprimÃ©). Comme indiquÃ© dans le tableau ci-dessous, ğŸ¤— Datasets fournit trois fonctions principales pour enregistrer votre jeu de donnÃ©es dans diffÃ©rents formatsÂ :

| Format de donnÃ©es |         Fonction         |
| :---------------: | :----------------------: |
|      FlÃ¨che       | `Dataset.save_to_disk()` |
|       CSV         |    `Dataset.to_csv()`    |
|      JSON         |   `Dataset.to_json()`    |

Par exemple, enregistrons notre jeu de donnÃ©es nettoyÃ© au format ArrowÂ :

```py
drug_dataset_clean.save_to_disk("drug-reviews")
```

Cela crÃ©era un rÃ©pertoire avec la structure suivanteÂ :

```
drug-reviews/
â”œâ”€â”€ dataset_dict.json
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ dataset.arrow
â”‚   â”œâ”€â”€ dataset_info.json
â”‚   â””â”€â”€ state.json
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ dataset.arrow
â”‚   â”œâ”€â”€ dataset_info.json
â”‚   â”œâ”€â”€ indices.arrow
â”‚   â””â”€â”€ state.json
â””â”€â”€ validation
    â”œâ”€â”€ dataset.arrow
    â”œâ”€â”€ dataset_info.json
    â”œâ”€â”€ indices.arrow
    â””â”€â”€ state.json
```

oÃ¹ nous pouvons voir que chaque division est associÃ©e Ã  sa propre table * dataset.arrow * et Ã  certaines mÃ©tadonnÃ©es dans * dataset_info.json * et * state.json *. Vous pouvez considÃ©rer le format Arrow comme un tableau sophistiquÃ© de colonnes et de lignes optimisÃ© pour la crÃ©ation d'applications hautes performances qui traitent et transportent de grands ensembles de donnÃ©es.

Une fois le jeu de donnÃ©es enregistrÃ©, nous pouvons le charger en utilisant la fonction `load_from_disk()` comme suitÂ :

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

Pour les formats CSV et JSON, nous devons stocker chaque fractionnement dans un fichier sÃ©parÃ©. Pour ce faire, vous pouvez parcourir les clÃ©s et les valeurs de l'objet "DatasetDict"Â :

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

Cela enregistre chaque fractionnement au [format de lignes JSON] (https://jsonlines.org), oÃ¹ chaque ligne de l'ensemble de donnÃ©es est stockÃ©e sous la forme d'une seule ligne de JSON. Voici Ã  quoi ressemble le premier exempleÂ :

```py
!head -n 1 drug-reviews-train.jsonl
```

```python out
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

Nous pouvons ensuite utiliser les techniques de [section 2](/course/chapter5/2) pour charger les fichiers JSON comme suitÂ :

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

Et c'est tout pour notre excursion dans le data wrangling avec ğŸ¤— DatasetsÂ ! Maintenant que nous disposons d'un ensemble de donnÃ©es nettoyÃ© pour entraÃ®ner un modÃ¨le, voici quelques idÃ©es que vous pouvez essayerÂ :

1. Utilisez les techniques du [Chapitre 3](/course/chapter3) pour former un classificateur capable de prÃ©dire l'Ã©tat du patient en fonction de l'examen du mÃ©dicament.
2. Utilisez le pipeline `summarization` du [Chapitre 1](/course/chapter1) pour gÃ©nÃ©rer des rÃ©sumÃ©s des rÃ©visions.

Ensuite, nous verrons comment ğŸ¤— Datasets peut vous permettre de travailler avec d'Ã©normes ensembles de donnÃ©es sans faire exploser votre ordinateur portableÂ !
