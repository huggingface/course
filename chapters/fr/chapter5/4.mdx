# Donn√©es massives ? ü§ó <i>Datasets</i> √† la rescousse !

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},
]} />


De nos jours, il n'est pas rare de travailler avec des jeux de donn√©es de plusieurs gigaoctets surtout si vous envisagez de pr√©-entra√Æner un *transformer* comme BERT ou GPT-2 √† partir de z√©ro. Dans ces cas, m√™me _charger_ les donn√©es peut √™tre un d√©fi. Par exemple, le corpus WebText utilis√© pour pr√©-entra√Æner GPT-2 se compose de plus de 8 millions de documents et de 40 Go de texte. Le charger dans la RAM de votre ordinateur portable est susceptible de lui donner une crise cardiaque !

Heureusement, ü§ó *Datasets* a √©t√© con√ßu pour surmonter ces limitations. Il vous lib√®re des probl√®mes de gestion de la m√©moire en traitant les jeux de donn√©es comme des fichiers _mapp√©s en m√©moire_, ainsi que des limites du disque dur en faisant du _streaming_ sur les entr√©es dans un corpus.

<Youtube id="JwISwTCPPWo"/>

Dans cette section, nous allons explorer ces fonctionnalit√©s de ü§ó *Datasets* avec un √©norme corpus de 825 Go connu sous le nom de [*The Pile*](https://pile.eleuther.ai). Commen√ßons !

## Qu'est-ce que <i>The Pile</i> ?

*The Pile* est un corpus de texte en anglais cr√©√© par [EleutherAI](https://www.eleuther.ai) pour entra√Æner des mod√®les de langage √† grande √©chelle. Il comprend une gamme vari√©e de jeux de donn√©es, couvrant des articles scientifiques, des r√©f√©rentiels de code GitHub et du texte Web filtr√©. Le corpus d‚Äôentra√Ænement est disponible en [morceaux de 14 Go](https://mystic.the-eye.eu/public/AI/pile/) et vous pouvez aussi t√©l√©charger plusieurs des [composants individuels]( https://mystic.the-eye.eu/public/AI/pile_preliminary_components/). Commen√ßons par jeter un coup d'≈ìil au jeu de donn√©es *PubMed Abstracts*, qui est un corpus de r√©sum√©s de 15 millions de publications biom√©dicales sur [PubMed](https://pubmed.ncbi.nlm.nih.gov/). Le jeu de donn√©es est au [format JSON Lines](https://jsonlines.org) et est compress√© √† l'aide de la biblioth√®que `zstandard`. Nous devons donc d'abord installer cette biblioth√®que :

```py
!pip install zstandard
```

Ensuite, nous pouvons charger le jeu de donn√©es en utilisant la m√©thode pour les fichiers distants que nous avons apprise dans [section 2](/course/fr/chapter5/2) :

```py
from datasets import load_dataset

# Cela prend quelques minutes √† ex√©cuter, alors allez prendre un th√© ou un caf√© en attendant :)
data_files = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

Nous pouvons voir qu'il y a 15 518 009 lignes et 2 colonnes dans notre jeu de donn√©es. C'est beaucoup !

<Tip>

‚úé Par d√©faut, ü§ó *Datasets* d√©compresse les fichiers n√©cessaires pour charger un jeu de donn√©es. Si vous souhaitez conserver de l'espace sur le disque dur, vous pouvez passer `DownloadConfig(delete_extracted=True)` √† l'argument `download_config` de `load_dataset()`. Voir la [documentation](https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig) pour plus de d√©tails.

</Tip>

Inspectons le contenu du premier exemple :

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'
# √âpid√©miologie de l'hypox√©mie chez les enfants souffrant d'une infection aigu√´ des voies respiratoires inf√©rieures. D√©terminer la pr√©valence de l'hypox√©mie chez les enfants de moins de 5 ans souffrant d'une infection aigu√´ des voies respiratoires inf√©rieures (IAVI), les facteurs de risque de l'hypox√©mie chez les enfants de moins de 5 ans souffrant d'une IAVI, et l'association de l'hypox√©mie √† un risque accru de d√©c√®s chez les enfants du m√™me √¢ge ...
}
```

Cela ressemble au r√©sum√© d'un article m√©dical. Voyons maintenant combien de RAM nous avons utilis√© pour charger le jeu de donn√©es !

## La magie du <i>memory mapping</i>

Un moyen simple de mesurer l'utilisation de la m√©moire dans Python consiste √† utiliser la biblioth√®que [`psutil`](https://psutil.readthedocs.io/en/latest/) qui peut √™tre install√©e avec `pip` comme suit :

```python
!pip install psutil
```

Elle fournit une classe `Process` qui permet de v√©rifier l'utilisation de la m√©moire du processus en cours :

```py
import psutil

# Process.memory_info est exprim√© en octets, donc convertir en m√©gaoctets
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
RAM used: 5678.33 MB
```

Ici, l'attribut `rss` fait r√©f√©rence √† la _taille de l'ensemble r√©sident_, qui est la fraction de m√©moire qu'un processus occupe dans la RAM. Cette mesure inclut √©galement la m√©moire utilis√©e par l'interpr√©teur Python et les biblioth√®ques que nous avons charg√©es, de sorte que la quantit√© r√©elle de m√©moire utilis√©e pour charger le jeu de donn√©es est un peu plus petite. √Ä titre de comparaison, voyons la taille du jeu de donn√©es sur le disque en utilisant l'attribut `dataset_size`. Comme le r√©sultat est exprim√© en octets comme pr√©c√©demment, nous devons le convertir manuellement en gigaoctets :

```py
print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
```

Malgr√© sa taille de pr√®s de 20 Go, nous pouvons charger et acc√©der au jeu de donn√©es avec beaucoup moins de RAM !

<Tip>

‚úèÔ∏è **Essayez !** Choisissez l'un des [sous-ensembles](https://mystic.the-eye.eu/public/AI/pile_preliminary_components/) de *The Pile* qui est plus grand que la RAM de votre ordinateur portable ou de bureau. Chargez-le avec ü§ó *Datasets* et mesurez la quantit√© de RAM utilis√©e. Notez que pour obtenir une mesure pr√©cise, vous devrez le faire dans un nouveau processus. Vous pouvez trouver les tailles d√©compress√©es de chaque sous-ensemble dans le tableau 1 du papier de [*The Pile*](https://arxiv.org/abs/2101.00027).

</Tip>

Si vous √™tes familier avec Pandas, ce r√©sultat pourrait surprendre en raison de la c√©l√®bre [r√®gle d'or](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) de Wes Kinney selon laquelle vous avez g√©n√©ralement besoin de 5 √† 10 fois plus de RAM que la taille de votre jeu de donn√©es. Alors, comment ü§ó *Datasets* r√©sout-il ce probl√®me de gestion de la m√©moire ? ü§ó *Datasets* traite chaque jeu de donn√©es comme un [fichier mapp√© en m√©moire](https://en.wikipedia.org/wiki/Memory-mapped_file). Cela fournit un mappage entre la RAM et le stockage du syst√®me de fichiers permettant √† la biblioth√®que d'acc√©der et d'op√©rer sur des √©l√©ments du jeu de donn√©es sans avoir besoin de le charger enti√®rement en m√©moire.

Les fichiers mapp√©s en m√©moire peuvent √©galement √™tre partag√©s entre plusieurs processus ce qui permet de parall√©liser des m√©thodes telles que `Dataset.map()` sans avoir √† d√©placer ou copier le jeu de donn√©es. Sous le capot, ces capacit√©s sont toutes r√©alis√©es par le format de m√©moire [Apache Arrow](https://arrow.apache.org) et [`pyarrow`](https://arrow.apache.org/docs/python/index.html), qui acc√©l√®rent le chargement et le traitement des donn√©es. (Pour plus de d√©tails sur Apache Arrow et les comparaisons avec Pandas, consultez [l'article de blog de Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a)). Pour voir ceci en action, effectuons un petit test de vitesse en it√©rant sur tous les √©l√©ments du jeu de donn√©es *PubMed Abstracts* :

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

Ici, nous avons utilis√© le module `timeit` de Python pour mesurer le temps d'ex√©cution pris par `code_snippet`. Vous pourrez g√©n√©ralement it√©rer sur un jeu de donn√©es √† une vitesse de quelques dixi√®mes de Go/s √† plusieurs Go/s. Cela fonctionne tr√®s bien pour la grande majorit√© des applications, mais vous devrez parfois travailler avec un jeu de donn√©es trop volumineux pour √™tre m√™me stock√© sur le disque dur de votre ordinateur portable. Par exemple, si nous essayions de t√©l√©charger *The Pile* dans son int√©gralit√©, nous aurions besoin de 825 Go d'espace disque libre ! Pour g√©rer ces cas, ü§ó *Datasets* fournit une fonctionnalit√© de streaming qui nous permet de t√©l√©charger et d'acc√©der aux √©l√©ments √† la vol√©e, sans avoir besoin de t√©l√©charger l'int√©gralit√© du jeu de donn√©es. Voyons comment cela fonctionne.

<Tip>

üí° Dans les *notebooks* Jupyter, vous pouvez √©galement chronom√©trer les cellules √† l'aide de la fonction magique [`%%timeit`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).

</Tip>

## Jeux de donn√©es en continu

Pour activer le streaming du jeu de donn√©es, il vous suffit de passer l'argument `streaming=True` √† la fonction `load_dataset()`. Par exemple, chargeons √† nouveau le jeu de donn√©es *PubMed Abstracts* mais en mode streaming :

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Au lieu du familier `Dataset` que nous avons rencontr√© ailleurs dans ce chapitre, l'objet retourn√© avec `streaming=True` est un `IterableDataset`. Comme son nom l'indique, pour acc√©der aux √©l√©ments d'un `IterableDataset`, nous devons parcourir celui-ci. Nous pouvons acc√©der au premier √©l√©ment de notre jeu de donn√©es diffus√© comme suit :


```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Les √©l√©ments d'un jeu de donn√©es diffus√© en continu peuvent √™tre trait√©s √† la vol√©e √† l'aide de `IterableDataset.map()`, ce qui est utile pendant l‚Äôentra√Ænement si vous avez besoin de tokeniser les entr√©es. Le processus est exactement le m√™me que celui que nous avons utilis√© pour tokeniser notre jeu de donn√©es dans le [chapitre 3](/course/fr/chapter3), √† la seule diff√©rence que les sorties sont renvoy√©es une par une :

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<Tip>

üí° Pour acc√©l√©rer la tokenisation avec le streaming, vous pouvez passer `batched=True`, comme nous l'avons vu dans la derni√®re section. Il traitera les exemples batch par batch. La taille de batch par d√©faut est de 1 000 et peut √™tre sp√©cifi√©e avec l'argument `batch_size`.

</Tip>

Vous pouvez √©galement m√©langer un jeu de donn√©es diffus√© en continu √† l'aide de `IterableDataset.shuffle()`, mais contrairement √† `Dataset.shuffle()`, cela ne m√©lange que les √©l√©ments dans un `buffer_size` pr√©d√©fini :

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'
# √âtude randomis√©e sur la modification de la dose ou du calendrier d'administration du facteur de stimulation des colonies de granulocytes dans le cadre d'une chimioth√©rapie √† base de platine chez les patients √¢g√©s atteints de cancer du poumon ...
}
```

Dans cet exemple, nous avons s√©lectionn√© un exemple al√©atoire parmi les 10 000 premiers exemples du tampon. Une fois qu'un exemple est acc√©d√©, sa place dans le tampon est remplie avec l'exemple suivant dans le corpus (c'est-√†-dire le 10 001e exemple dans le cas ci-dessus). Vous pouvez √©galement s√©lectionner des √©l√©ments d'un jeu de donn√©es diffus√© en continu √† l'aide des fonctions `IterableDataset.take()` et `IterableDataset.skip()`, qui agissent de la m√™me mani√®re que `Dataset.select()`. Par exemple, pour s√©lectionner les 5 premiers exemples dans le jeu de donn√©es *PubMed Abstracts*, nous pouvons proc√©der comme suit :

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'
# √âpid√©miologie de l'hypox√©mie chez les enfants atteints d'une infection aigu√´ des voies respiratoires inf√©rieures ...},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'
# Signes cliniques d'hypox√©mie chez les enfants atteints d'une infection aigu√´ des voies respiratoires inf√©rieures : indicateurs de l'oxyg√©noth√©rapie ...},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."
# Hypox√©mie chez les enfants atteints de pneumonie grave en Papouasie-Nouvelle-Guin√©e ...},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'
# Concentrateurs et bouteilles d'oxyg√®ne...},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'
# L'approvisionnement en oxyg√®ne dans les zones rurales africaines : une exp√©rience personnelle ...}]
```

De m√™me, vous pouvez utiliser la fonction `IterableDataset.skip()` pour cr√©er des fractionnements d'entra√Ænement et de validation √† partir d'un jeu de donn√©es m√©lang√© comme suit :

```py
# Ignorer les 1 000 premiers exemples et inclure le reste dans l'ensemble d'apprentissage.
train_dataset = shuffled_dataset.skip(1000)
# Prendre les 1 000 premiers exemples pour l'ensemble de validation.
validation_dataset = shuffled_dataset.take(1000)
```

Terminons notre exploration du streaming des jeux de donn√©es avec une application commune : combiner plusieurs jeux de donn√©es pour cr√©er un seul corpus. ü§ó *Datasets* fournit une fonction `interleave_datasets()` qui convertit une liste d'objets `IterableDataset` en un seul `IterableDataset`, o√π les √©l√©ments du nouveau jeu de donn√©es sont obtenus en alternant entre les exemples source. Cette fonction est particuli√®rement utile lorsque vous essayez de combiner de grands jeux de donn√©es. Par exemple, streamons FreeLaw, un sous-ensemble de *The Pile* et qui est un jeu de donn√©es de 51 Go d'avis juridiques de tribunaux am√©ricains :

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

Ce jeu de donn√©es est suffisamment volumineux pour solliciter la RAM de la plupart des ordinateurs portables, mais nous avons pu le charger et y acc√©der sans transpirer ! Combinons maintenant les jeux de donn√©es FreeLaw et *PubMed Abstracts* avec la fonction `interleave_datasets()` :

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

Ici, nous avons utilis√© la fonction `islice()` du module `itertools` de Python pour s√©lectionner les deux premiers exemples du jeu de donn√©es combin√©. Nous pouvons voir qu'ils correspondent aux premiers exemples de chacun des deux jeux de donn√©es source.

Enfin, si vous souhaitez streamer *The Pile* dans son int√©gralit√© de 825 Go, vous pouvez r√©cup√©rer tous les fichiers pr√©par√©s comme suit :

```py
base_url = "https://mystic.the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play ‚ÄúSurvival of the Tastiest‚Äù on Android, and on the web...'}
```

<Tip>

‚úèÔ∏è **Essayez !** Utilisez l'un des grands corpus Common Crawl comme [`mc4`](https://huggingface.co/datasets/mc4) ou [`oscar`](https://huggingface.co/datasets/oscar) pour cr√©er en streaming un jeu de donn√©es multilingue repr√©sentant les proportions de langues parl√©es dans un pays de votre choix. Par exemple, les quatre langues nationales en Suisse sont l'allemand, le fran√ßais, l'italien et le romanche. Vous pouvez donc essayer de cr√©er un corpus suisse en √©chantillonnant les sous-ensembles Oscar en fonction de leur proportion parl√©e.

</Tip>

Vous disposez maintenant de tous les outils dont vous avez besoin pour charger et traiter des jeux de donn√©es de toutes formes et tailles. Cependant √† moins que vous ne soyez exceptionnellement chanceux, il arrivera un moment dans votre cheminement en traitement du langage naturel o√π vous devrez r√©ellement cr√©er un jeu de donn√©es pour r√©soudre un probl√®me donn√©. C'est le sujet de la section suivante !
