# Big Data? ü§ó Datasets √† la rescousse¬†!

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},
]} />


De nos jours, il n'est pas rare de travailler avec des ensembles de donn√©es de plusieurs gigaoctets, surtout si vous envisagez de pr√©-entra√Æner un transformateur comme BERT ou GPT-2 √† partir de z√©ro. Dans ces cas, m√™me _charger_ les donn√©es peut √™tre un d√©fi. Par exemple, le corpus WebText utilis√© pour pr√©-entra√Æner GPT-2 se compose de plus de 8 millions de documents et de 40 Go de texte - le charger dans la RAM de votre ordinateur portable est susceptible de lui donner une crise cardiaque !

Heureusement, ü§ó Datasets a √©t√© con√ßu pour surmonter ces limitations. Il vous lib√®re des probl√®mes de gestion de la m√©moire en traitant les ensembles de donn√©es comme des fichiers _mapp√©s en m√©moire_, et des limites du disque dur en _streaming_ les entr√©es dans un corpus.

<Youtube id="JwISwTCPPWo"/>

Dans cette section, nous allons explorer ces fonctionnalit√©s de ü§ó Datasets avec un √©norme corpus de 825 Go connu sous le nom de [the Pile](https://pile.eleuther.ai). Commen√ßons!

## Qu'est-ce que The Pile ?

The Pile est un corpus de texte en anglais cr√©√© par [EleutherAI](https://www.eleuther.ai) pour entra√Æner des mod√®les de langage √† grande √©chelle. Il comprend une gamme vari√©e d'ensembles de donn√©es, couvrant des articles scientifiques, des r√©f√©rentiels de code GitHub et du texte Web filtr√©. Le corpus de formation est disponible en [morceaux de 14 Go](https://mystic.the-eye.eu/public/AI/pile/), et vous pouvez √©galement t√©l√©charger plusieurs des [composants individuels](https://mystic .the-eye.eu/public/AI/pile_preliminary_components/). Commen√ßons par jeter un coup d'≈ìil √† l'ensemble de donn√©es PubMed Abstracts, qui est un corpus de r√©sum√©s de 15 millions de publications biom√©dicales sur [PubMed](https://pubmed.ncbi.nlm.nih.gov/). L'ensemble de donn√©es est au [format JSON Lines](https://jsonlines.org) et est compress√© √† l'aide de la biblioth√®que `zstandard`, nous devons donc d'abord l'installer¬†:

```py
!pip install zstandard
```

Ensuite, nous pouvons charger le jeu de donn√©es en utilisant la m√©thode pour les fichiers distants que nous avons apprise dans [section¬†2](/course/chapter5/2)¬†:

```py
from datasets import load_dataset

# Cela prend quelques minutes √† ex√©cuter, alors allez prendre un th√© ou un caf√© en attendant :)
data_files = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

Nous pouvons voir qu'il y a 15 518 009 lignes et 2 colonnes dans notre ensemble de donn√©es -- c'est beaucoup¬†!

<Tip>

‚úé Par d√©faut, ü§ó Datasets d√©compressera les fichiers n√©cessaires pour charger un jeu de donn√©es. Si vous souhaitez conserver de l'espace sur le disque dur, vous pouvez passer `DownloadConfig(delete_extracted=True)` √† l'argument `download_config` de `load_dataset()`. Voir la [documentation](https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig) pour plus de d√©tails.

</Tip>

Inspectons le contenu du premier exemple¬†:

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

OK, √ßa ressemble au r√©sum√© d'un article m√©dical. Voyons maintenant combien de RAM nous avons utilis√© pour charger le jeu de donn√©es¬†!

## La magie de la cartographie m√©moire

Un moyen simple de mesurer l'utilisation de la m√©moire dans Python consiste √† utiliser la biblioth√®que [`psutil`](https://psutil.readthedocs.io/en/latest/), qui peut √™tre install√©e avec `pip` comme suit¬†:

```python
!pip install psutil
```

Il fournit une classe `Process` qui nous permet de v√©rifier l'utilisation de la m√©moire du processus en cours comme suit¬†:

```py
import psutil

# Process.memory_info is expressed in bytes, so convert to megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
RAM used: 5678.33 MB
```

Ici, l'attribut `rss` fait r√©f√©rence √† la _taille de l'ensemble r√©sident_, qui est la fraction de m√©moire qu'un processus occupe dans la RAM. Cette mesure inclut √©galement la m√©moire utilis√©e par l'interpr√©teur Python et les biblioth√®ques que nous avons charg√©es, de sorte que la quantit√© r√©elle de m√©moire utilis√©e pour charger l'ensemble de donn√©es est un peu plus petite. √Ä titre de comparaison, voyons la taille de l'ensemble de donn√©es sur le disque, en utilisant l'attribut `dataset_size`. Comme le r√©sultat est exprim√© en octets comme pr√©c√©demment, nous devons le convertir manuellement en gigaoctets¬†:

```py
print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
```

Bien - malgr√© sa taille de pr√®s de 20 Go, nous pouvons charger et acc√©der √† l'ensemble de donn√©es avec beaucoup moins de RAM¬†!

<Tip>

‚úèÔ∏è **Essayez-le¬†!** Choisissez l'un des [sous-ensembles](https://mystic.the-eye.eu/public/AI/pile_preliminary_components/) de la Pile qui est plus grand que la RAM de votre ordinateur portable ou de bureau, chargez avec ü§ó Datasets, et mesurez la quantit√© de RAM utilis√©e. Notez que pour obtenir une mesure pr√©cise, vous devrez le faire dans un nouveau processus. Vous pouvez trouver les tailles d√©compress√©es de chaque sous-ensemble dans le tableau 1 de [the Pile paper](https://arxiv.org/abs/2101.00027).

</Tip>

Si vous √™tes familier avec les pandas, ce r√©sultat pourrait surprendre en raison de la c√©l√®bre [r√®gle d'or](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) de Wes Kinney selon laquelle vous avez g√©n√©ralement besoin de 5 √† 10 fois plus de RAM que la taille de votre jeu de donn√©es. Alors, comment ü§ó Datasets r√©sout-il ce probl√®me de gestion de la m√©moire¬†? ü§ó Datasets traite chaque ensemble de donn√©es comme un [fichier mapp√© en m√©moire] (https://en.wikipedia.org/wiki/Memory-mapped_file), qui fournit un mappage entre la RAM et le stockage du syst√®me de fichiers qui permet √† la biblioth√®que d'acc√©der et d'op√©rer sur des √©l√©ments du jeu de donn√©es sans avoir besoin de le charger enti√®rement en m√©moire.

Les fichiers mapp√©s en m√©moire peuvent √©galement √™tre partag√©s entre plusieurs processus, ce qui permet de parall√©liser des m√©thodes telles que `Dataset.map()` sans avoir √† d√©placer ou copier l'ensemble de donn√©es. Sous le capot, ces capacit√©s sont toutes r√©alis√©es par le format de m√©moire [Apache Arrow](https://arrow.apache.org) et [`pyarrow`](https://arrow.apache.org/docs/python/index .html), qui acc√©l√®rent le chargement et le traitement des donn√©es. (Pour plus de d√©tails sur Apache Arrow et les comparaisons avec Pandas, consultez [l'article de blog de Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) Pour voir ceci en action, effectuons un petit test de vitesse en it√©rant sur tous les √©l√©ments du jeu de donn√©es PubMed Abstracts¬†:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

Ici, nous avons utilis√© le module `timeit` de Python pour mesurer le temps d'ex√©cution pris par `code_snippet`. Vous pourrez g√©n√©ralement it√©rer sur un ensemble de donn√©es √† une vitesse de quelques dixi√®mes de Go/s √† plusieurs Go/s. Cela fonctionne tr√®s bien pour la grande majorit√© des applications, mais vous devrez parfois travailler avec un ensemble de donn√©es trop volumineux pour √™tre m√™me stock√© sur le disque dur de votre ordinateur portable. Par exemple, si nous essayions de t√©l√©charger la Pile dans son int√©gralit√©, nous aurions besoin de 825 Go d'espace disque libre ! Pour g√©rer ces cas, ü§ó Datasets fournit une fonctionnalit√© de streaming qui nous permet de t√©l√©charger et d'acc√©der aux √©l√©ments √† la vol√©e, sans avoir besoin de t√©l√©charger l'int√©gralit√© du jeu de donn√©es. Voyons comment cela fonctionne.

<Tip>

üí° Dans les notebooks Jupyter, vous pouvez √©galement chronom√©trer les cellules √† l'aide de la fonction magique [`%%timeit`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).

</Tip>

## Ensembles de donn√©es en continu

Pour activer le streaming de l'ensemble de donn√©es, il vous suffit de passer l'argument `streaming=True` √† la fonction `load_dataset()`. Par exemple, chargeons √† nouveau le jeu de donn√©es PubMed Abstracts, mais en mode streaming¬†:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Au lieu du familier `Dataset` que nous avons rencontr√© ailleurs dans ce chapitre, l'objet retourn√© avec `streaming=True` est un `IterableDataset`. Comme son nom l'indique, pour acc√©der aux √©l√©ments d'un `IterableDataset`, nous devons parcourir celui-ci. Nous pouvons acc√©der au premier √©l√©ment de notre jeu de donn√©es diffus√© comme suit¬†:


```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Les √©l√©ments d'un ensemble de donn√©es diffus√© en continu peuvent √™tre trait√©s √† la vol√©e √† l'aide de `IterableDataset.map()`, ce qui est utile pendant la formation si vous avez besoin de tokeniser les entr√©es. Le processus est exactement le m√™me que celui que nous avons utilis√© pour tokeniser notre jeu de donn√©es dans [Chapitre 3](/course/chapter3), √† la seule diff√©rence que les sorties sont renvoy√©es une par une¬†:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<Tip>

üí° Pour acc√©l√©rer la tokenisation avec le streaming, vous pouvez passer `batched=True`, comme nous l'avons vu dans la derni√®re section. Il traitera les exemples lot par lot ; la taille de lot par d√©faut est de 1 000 et peut √™tre sp√©cifi√©e avec l'argument `batch_size`.

</Tip>

Vous pouvez √©galement m√©langer un ensemble de donn√©es diffus√© en continu √† l'aide de `IterableDataset.shuffle()`, mais contrairement √† `Dataset.shuffle()`, cela ne m√©lange que les √©l√©ments dans un `buffer_size` pr√©d√©fini¬†:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

Dans cet exemple, nous avons s√©lectionn√© un exemple al√©atoire parmi les 10 000 premiers exemples du tampon. Une fois qu'un exemple est acc√©d√©, sa place dans le tampon est remplie avec l'exemple suivant dans le corpus (c'est-√†-dire le 10 001e exemple dans le cas ci-dessus). Vous pouvez √©galement s√©lectionner des √©l√©ments d'un ensemble de donn√©es diffus√© en continu √† l'aide des fonctions `IterableDataset.take()` et `IterableDataset.skip()`, qui agissent de la m√™me mani√®re que `Dataset.select()`. Par exemple, pour s√©lectionner les 5 premiers exemples dans l'ensemble de donn√©es PubMed Abstracts, nous pouvons proc√©der comme suit¬†:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

De m√™me, vous pouvez utiliser la fonction `IterableDataset.skip()` pour cr√©er des fractionnements d'entra√Ænement et de validation √† partir d'un ensemble de donn√©es m√©lang√© comme suit¬†:

```py
# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)
```

Terminons notre exploration du streaming d'ensembles de donn√©es avec une application commune¬†: combiner plusieurs ensembles de donn√©es pour cr√©er un seul corpus. ü§ó Datasets fournit une fonction `interleave_datasets()` qui convertit une liste d'objets `IterableDataset` en un seul `IterableDataset`, o√π les √©l√©ments du nouveau jeu de donn√©es sont obtenus en alternant entre les exemples source. Cette fonction est particuli√®rement utile lorsque vous essayez de combiner de grands ensembles de donn√©es. Par exemple, diffusons le sous-ensemble FreeLaw de la pile, qui est un ensemble de donn√©es de 51¬†Go d'avis juridiques de tribunaux am√©ricains¬†:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

Cet ensemble de donn√©es est suffisamment volumineux pour solliciter la RAM de la plupart des ordinateurs portables, mais nous avons pu le charger et y acc√©der sans transpirer¬†! Combinons maintenant les exemples des jeux de donn√©es FreeLaw et PubMed Abstracts avec la fonction `interleave_datasets()`¬†:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

Ici, nous avons utilis√© la fonction `islice()` du module `itertools` de Python pour s√©lectionner les deux premiers exemples de l'ensemble de donn√©es combin√©, et nous pouvons voir qu'ils correspondent aux premiers exemples de chacun des deux ensembles de donn√©es source.

Enfin, si vous souhaitez diffuser le Pile dans son int√©gralit√© de 825 Go, vous pouvez r√©cup√©rer tous les fichiers pr√©par√©s comme suit :

```py
base_url = "https://mystic.the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play ‚ÄúSurvival of the Tastiest‚Äù on Android, and on the web...'}
```

<Tip>

‚úèÔ∏è **Essayez-le¬†!** Utilisez l'un des grands corpus Common Crawl comme [`mc4`](https://huggingface.co/datasets/mc4) ou [`oscar`](https://huggingface.co /datasets/oscar) pour cr√©er un jeu de donn√©es multilingue en continu qui repr√©sente les proportions de langues parl√©es dans un pays de votre choix. Par exemple, les quatre langues nationales en Suisse sont l'allemand, le fran√ßais, l'italien et le romanche, vous pouvez donc essayer de cr√©er un corpus suisse en √©chantillonnant les sous-ensembles Oscar en fonction de leur proportion parl√©e.

</Tip>

Vous disposez maintenant de tous les outils dont vous avez besoin pour charger et traiter des ensembles de donn√©es de toutes formes et tailles - mais √† moins que vous ne soyez exceptionnellement chanceux, il arrivera un moment dans votre parcours PNL o√π vous devrez r√©ellement cr√©er un ensemble de donn√©es pour r√©soudre le probl√®me √† port√©e de main. C'est le sujet de la section suivante !
