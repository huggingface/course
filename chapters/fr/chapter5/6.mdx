<FrameworkSwitchCourse {fw} />

# Recherche s√©mantique avec FAISS

{#if fw === 'pt'}
<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter5/section6_pt.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter5/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter5/section6_tf.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter5/section6_tf.ipynb"},
]} />

{/if}

Dans [section 5](/course/fr/chapter5/5), nous avons cr√©√© un jeu de donn√©es de probl√®mes et de commentaires GitHub √† partir du d√©p√¥t ü§ó *Datasets*. Dans cette section, nous utilisons ces informations pour cr√©er un moteur de recherche qui peut nous aider √† trouver des r√©ponses √† nos questions les plus urgentes sur la biblioth√®que !

<Youtube id="OATCgQtNX2o"/>

## Utilisation des ench√¢ssements pour la recherche s√©mantique

Comme nous l'avons vu dans le [chapitre 1](/course/fr/chapter1), les mod√®les de langage bas√©s sur les *transformers* repr√©sentent chaque *token* dans une √©tendue de texte sous la forme d'un _ench√¢ssement_. Il s'av√®re que l'on peut regrouper les ench√¢ssements individuels pour cr√©er une repr√©sentation vectorielle pour des phrases enti√®res, des paragraphes ou (dans certains cas) des documents. Ces ench√¢ssements peuvent ensuite √™tre utilis√©s pour trouver des documents similaires dans le corpus en calculant la similarit√© du produit scalaire (ou une autre m√©trique de similarit√©) entre chaque ench√¢ssement et en renvoyant les documents avec le plus grand chevauchement.

Dans cette section, nous utilisons les ench√¢ssements pour d√©velopper un moteur de recherche s√©mantique. Ces moteurs de recherche offrent plusieurs avantages par rapport aux approches conventionnelles bas√©es sur la correspondance des mots-cl√©s dans une requ√™te avec les documents.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Recherche s√©mantique."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Recherche s√©mantique."/>
</div>

## Chargement et pr√©paration du jeu de donn√©es

La premi√®re chose que nous devons faire est de t√©l√©charger notre jeu de donn√©es de probl√®mes GitHub. Utilisons la biblioth√®que ü§ó *Hub* pour r√©soudre l'URL o√π notre fichier est stock√© sur le *Hub* d‚ÄôHugging Face :

```py
from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-comments.jsonl",
    repo_type="dataset",
)
```

Avec l'URL stock√© dans `data_files`, nous pouvons ensuite charger le jeu de donn√©es distant en utilisant la m√©thode introduite dans [section 2](/course/fr/chapter5/2) :

```py
from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

Ici, nous avons sp√©cifi√© l‚Äô√©chantillon `train` par d√©faut dans `load_dataset()`, de sorte que cela renvoie un `Dataset` au lieu d'un `DatasetDict`. La premi√®re chose √† faire est de filtrer les *pull requests* car celles-ci ont tendance √† √™tre rarement utilis√©es pour r√©pondre aux requ√™tes des utilisateurs et introduiront du bruit dans notre moteur de recherche. Comme cela devrait √™tre familier maintenant, nous pouvons utiliser la fonction `Dataset.filter()` pour exclure ces lignes de notre jeu de donn√©es. Pendant que nous y sommes, filtrons √©galement les lignes sans commentaires, car celles-ci ne fournissent aucune r√©ponse aux requ√™tes des utilisateurs :

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

Nous pouvons voir qu'il y a beaucoup de colonnes dans notre jeu de donn√©es, dont la plupart n'ont pas besoin de construire notre moteur de recherche. Du point de vue de la recherche, les colonnes les plus informatives sont `title`, `body` et `comments`, tandis que `html_url` nous fournit un lien vers le probl√®me source. Utilisons la fonction `Dataset.remove_columns()` pour supprimer le reste :

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

Pour cr√©er nos ench√¢ssements, nous ajoutons √† chaque commentaire le titre et le corps du probl√®me, car ces champs contiennent des informations contextuelles utiles. √âtant donn√© que notre colonne `comments` est actuellement une liste de commentaires pour chaque probl√®me, nous devons ¬´ √©clater ¬ª la colonne afin que chaque ligne se compose d'un *tuple* `(html_url, title, body, comment)`. Dans Pandas, nous pouvons le faire avec la fonction [`DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), qui cr√©e une nouvelle ligne pour chaque √©l√©ment dans une colonne de type liste, tout en r√©pliquant toutes les autres valeurs de colonne. Pour voir cela en action, passons d'abord au format `DataFrame` de Pandas :

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

Si nous inspectons la premi√®re ligne de ce `DataFrame`, nous pouvons voir qu'il y a quatre commentaires associ√©s √† ce probl√®me :

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in Ôºö\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Lorsque nous d√©composons `df`, nous nous attendons √† obtenir une ligne pour chacun de ces commentaires. V√©rifions si c'est le cas :

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in Ôºö\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

G√©nial, nous pouvons voir que les lignes ont √©t√© r√©pliqu√©es, avec la colonne `comments` contenant les commentaires individuels ! Maintenant que nous en avons fini avec Pandas, nous pouvons rapidement revenir √† un `Dataset` en chargeant le `DataFrame` en m√©moire :

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

D'accord, cela nous a donn√© quelques milliers de commentaires avec lesquels travailler !


<Tip>

‚úèÔ∏è **Essayez !** Voyez si vous pouvez utiliser `Dataset.map()` pour exploser la colonne `comments` de `issues_dataset` _sans_ recourir √† l'utilisation de Pandas. C'est un peu d√©licat. La section [¬´ Batch mapping ¬ª](https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping) de la documentation ü§ó *Datasets* peut √™tre utile pour cette t√¢che.

</Tip>

Maintenant que nous avons un commentaire par ligne, cr√©ons une nouvelle colonne `comments_length` contenant le nombre de mots par commentaire :

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

Nous pouvons utiliser cette nouvelle colonne pour filtrer les commentaires courts incluant g√©n√©ralement des √©l√©ments tels que ¬´ cc @lewtun ¬ª ou ¬´ Merci ! ¬ª qui ne sont pas pertinents pour notre moteur de recherche. Il n'y a pas de nombre pr√©cis √† s√©lectionner pour le filtre mais 15 mots semblent √™tre un bon d√©but :

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

Apr√®s avoir un peu nettoy√© notre jeu de donn√©es, concat√©nons le titre, la description et les commentaires du probl√®me dans une nouvelle colonne `text`. Comme d'habitude, nous allons √©crire une fonction simple que nous pouvons passer √† `Dataset.map()` :

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)
```

Nous sommes enfin pr√™ts √† cr√©er des ench√¢ssements ! Jetons un coup d'≈ìil.

## Cr√©ation d‚Äôench√¢ssements pour les textes

Nous avons vu dans [chapitre 2](/course/fr/chapter2) que nous pouvons obtenir des ench√¢ssements de *tokens* en utilisant la classe `AutoModel`. Tout ce que nous avons √† faire est de choisir un *checkpoint* appropri√© √† partir duquel charger le mod√®le. Heureusement, il existe une biblioth√®que appel√©e `sentence-transformers` d√©di√©e √† la cr√©ation d‚Äôench√¢ssements. Comme d√©crit dans la [documentation de la biblioth√®que](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), notre cas d'utilisation est un exemple de _recherche s√©mantique asym√©trique_. En effet, nous avons une requ√™te courte dont nous aimerions trouver la r√©ponse dans un document plus long, par exemple un commentaire √† un probl√®me. Le [tableau de pr√©sentation des mod√®les](https://www.sbert.net/docs/pretrained_models.html#model-overview) de la documentation indique que le *checkpoint* `multi-qa-mpnet-base-dot-v1` a les meilleures performances pour la recherche s√©mantique. Utilisons donc le pour notre application. Nous allons √©galement charger le *tokenizer* en utilisant le m√™me *checkpoint* :

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

Pour acc√©l√©rer le processus, il est utile de placer le mod√®le et les entr√©es sur un p√©riph√©rique GPU, alors faisons-le maintenant :

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

Notez que nous avons d√©fini `from_pt=True` comme argument de la m√©thode `from_pretrained()`. C'est parce que le point de contr√¥le `multi-qa-mpnet-base-dot-v1` n'a que des poids PyTorch. Donc d√©finir `from_pt=True` converti automatiquement au format TensorFlow pour nous. Comme vous pouvez le voir, il est tr√®s simple de passer d'un *framework* √† l'autre dans ü§ó *Transformers* !

{/if}

Comme nous l'avons mentionn√© pr√©c√©demment, nous aimerions repr√©senter chaque entr√©e dans notre corpus de probl√®mes GitHub comme un vecteur unique. Nous devons donc regrouper ou faire la moyenne de nos ench√¢ssements de *tokens* d'une mani√®re ou d'une autre. Une approche populaire consiste √† effectuer un *regroupement CLS* sur les sorties de notre mod√®le, o√π nous collectons simplement le dernier √©tat cach√© pour le *token* sp√©cial `[CLS]`. La fonction suivante fait √ßa pour nous :

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

Ensuite, nous allons cr√©er une fonction utile qui va tokeniser une liste de documents, placer les tenseurs dans le GPU, les donner au mod√®le et enfin appliquer le regroupement CLS aux sorties :

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Nous pouvons tester le fonctionnement de la fonction en lui donnant la premi√®re entr√©e textuelle de notre corpus et en inspectant la forme de sortie :

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

Super ! Nous avons converti la premi√®re entr√©e de notre corpus en un vecteur √† 768 dimensions. Nous pouvons utiliser `Dataset.map()` pour appliquer notre fonction `get_embeddings()` √† chaque ligne de notre corpus. Cr√©ons donc une nouvelle colonne `embeddings` comme suit :

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Nous pouvons tester le fonctionnement de la fonction en lui donnant la premi√®re entr√©e textuelle de notre corpus et en inspectant la forme de sortie :

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

Super ! Nous avons converti la premi√®re entr√©e de notre corpus en un vecteur √† 768 dimensions. Nous pouvons utiliser `Dataset.map()` pour appliquer notre fonction `get_embeddings()` √† chaque ligne de notre corpus. Cr√©ons donc une nouvelle colonne `embeddings` comme suit :

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}


Notez que nous avons converti les ench√¢ssements en tableaux NumPy. C'est parce que ü§ó *Datasets* n√©cessite ce format lorsque nous essayons de les indexer avec FAISS, ce que nous ferons ensuite.

## Utilisation de FAISS pour une recherche de similarit√© efficace

Maintenant que nous avons un jeu de donn√©es d'incorporations, nous avons besoin d'un moyen de les rechercher. Pour ce faire, nous utiliserons une structure de donn√©es sp√©ciale dans ü§ó *Datasets* appel√©e _FAISS index_. [FAISS](https://faiss.ai/) (abr√©viation de *Facebook AI Similarity Search*) est une biblioth√®que qui fournit des algorithmes efficaces pour rechercher et regrouper rapidement des vecteurs d'int√©gration.

L'id√©e de base derri√®re FAISS est de cr√©er une structure de donn√©es sp√©ciale appel√©e un _index_ qui permet de trouver quels plongements sont similaires √† un plongement d'entr√©e. Cr√©er un index FAISS dans ü§ó *Datasets* est simple -- nous utilisons la fonction `Dataset.add_faiss_index()` et sp√©cifions quelle colonne de notre jeu de donn√©es nous aimerions indexer :

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

Nous pouvons maintenant effectuer des requ√™tes sur cet index en effectuant une recherche des voisins les plus proches avec la fonction `Dataset.get_nearest_examples()`. Testons cela en ench√¢ssant d'abord une question comme suit :

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

Tout comme avec les documents, nous avons maintenant un vecteur de 768 dimensions repr√©sentant la requ√™te. Nous pouvons le comparer √† l‚Äôensemble du corpus pour trouver les ench√¢ssements les plus similaires :

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

La fonction `Dataset.get_nearest_examples()` renvoie un *tuple* de scores qui classent le chevauchement entre la requ√™te et le document, et un jeu correspondant d'√©chantillons (ici, les 5 meilleures correspondances). Collectons-les dans un `pandas.DataFrame` afin de pouvoir les trier facilement :

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Nous pouvons maintenant parcourir les premi√®res lignes pour voir dans quelle mesure notre requ√™te correspond aux commentaires disponibles :

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Pas mal ! Notre deuxi√®me r√©sultat semble correspondre √† la requ√™te.

<Tip>

‚úèÔ∏è **Essayez !** Cr√©ez votre propre requ√™te et voyez si vous pouvez trouver une r√©ponse dans les documents r√©cup√©r√©s. Vous devrez peut-√™tre augmenter le param√®tre `k` dans `Dataset.get_nearest_examples()` pour √©largir la recherche.

</Tip>
