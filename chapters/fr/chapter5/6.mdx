<FrameworkSwitchCourse {fw} />

# Recherche s√©mantique avec FAISS

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},
]} />

{/if}

Dans [section 5](/course/chapter5/5), nous avons cr√©√© un ensemble de donn√©es de probl√®mes et de commentaires GitHub √† partir du r√©f√©rentiel ü§ó Datasets. Dans cette section, nous utiliserons ces informations pour cr√©er un moteur de recherche qui peut nous aider √† trouver des r√©ponses √† nos questions les plus urgentes sur la biblioth√®que¬†!

<Youtube id="OATCgQtNX2o"/>

## Utilisation des repr√©sentations vectorielles continues pour la recherche s√©mantique

Comme nous l'avons vu dans le [Chapitre 1](/course/chapter1), les mod√®les de langage bas√©s sur Transformer repr√©sentent chaque jeton dans une √©tendue de texte sous la forme d'un _vecteur d'int√©gration_. Il s'av√®re que l'on peut "regrouper" les incorporations individuelles pour cr√©er une repr√©sentation vectorielle pour des phrases enti√®res, des paragraphes ou (dans certains cas) des documents. Ces int√©grations peuvent ensuite √™tre utilis√©es pour trouver des documents similaires dans le corpus en calculant la similarit√© du produit scalaire (ou une autre m√©trique de similarit√©) entre chaque int√©gration et en renvoyant les documents avec le plus grand chevauchement.

Dans cette section, nous utiliserons les incorporations pour d√©velopper un moteur de recherche s√©mantique. Ces moteurs de recherche offrent plusieurs avantages par rapport aux approches conventionnelles bas√©es sur la correspondance des mots-cl√©s dans une requ√™te avec les documents.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Recherche s√©mantique."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Recherche s√©mantique."/>
</div>

## Chargement et pr√©paration du jeu de donn√©es

La premi√®re chose que nous devons faire est de t√©l√©charger notre ensemble de donn√©es de probl√®mes GitHub, alors utilisons la biblioth√®que ü§ó Hub pour r√©soudre l'URL o√π notre fichier est stock√© sur le Hugging Face Hub :

```py
from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-comments.jsonl",
    repo_type="dataset",
)
```

Avec l'URL stock√©e dans `data_files`, nous pouvons ensuite charger le jeu de donn√©es distant en utilisant la m√©thode introduite dans [section 2](/course/chapter5/2) :

```py
from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

Ici, nous avons sp√©cifi√© la division `train` par d√©faut dans `load_dataset()`, de sorte qu'elle renvoie un `Dataset` au lieu d'un `DatasetDict`. La premi√®re chose √† faire est de filtrer les demandes d'extraction, car celles-ci ont tendance √† √™tre rarement utilis√©es pour r√©pondre aux requ√™tes des utilisateurs et introduiront du bruit dans notre moteur de recherche. Comme cela devrait √™tre familier maintenant, nous pouvons utiliser la fonction `Dataset.filter()` pour exclure ces lignes de notre ensemble de donn√©es. Pendant que nous y sommes, filtrons √©galement les lignes sans commentaires, car celles-ci ne fournissent aucune r√©ponse aux requ√™tes des utilisateurs¬†:

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

Nous pouvons voir qu'il y a beaucoup de colonnes dans notre ensemble de donn√©es, dont la plupart n'ont pas besoin de construire notre moteur de recherche. Du point de vue de la recherche, les colonnes les plus informatives sont `title`, `body` et `comments`, tandis que `html_url` nous fournit un lien vers le probl√®me source. Utilisons la fonction `Dataset.remove_columns()` pour supprimer le reste¬†:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

Pour cr√©er nos int√©grations, nous ajouterons √† chaque commentaire le titre et le corps du probl√®me, car ces champs contiennent souvent des informations contextuelles utiles. √âtant donn√© que notre colonne `comments` est actuellement une liste de commentaires pour chaque probl√®me, nous devons "√©clater" la colonne afin que chaque ligne se compose d'un tuple `(html_url, title, body, comment)`. Dans Pandas, nous pouvons le faire avec la fonction [`DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), qui cr√©e une nouvelle ligne pour chaque √©l√©ment dans une colonne de type liste, tout en r√©pliquant toutes les autres valeurs de colonne. Pour voir cela en action, passons d'abord au format "DataFrame" de Pandas¬†:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

Si nous inspectons la premi√®re ligne de ce `DataFrame`, nous pouvons voir qu'il y a quatre commentaires associ√©s √† ce probl√®me¬†:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in Ôºö\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Lorsque nous d√©composons `df`, nous nous attendons √† obtenir une ligne pour chacun de ces commentaires. V√©rifions si c'est le cas :

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in Ôºö\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

G√©nial, nous pouvons voir que les lignes ont √©t√© r√©pliqu√©es, avec la colonne "commentaires" contenant les commentaires individuels¬†! Maintenant que nous en avons fini avec Pandas, nous pouvons rapidement revenir √† un `Dataset` en chargeant le `DataFrame` en m√©moire¬†:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

D'accord, cela nous a donn√© quelques milliers de commentaires avec lesquels travailler¬†!


<Tip>

‚úèÔ∏è **Essayez-le !** Voyez si vous pouvez utiliser `Dataset.map()` pour exploser la colonne `comments` de `issues_dataset` _sans_ recourir √† l'utilisation de Pandas. C'est un peu d√©licat; vous pourriez trouver la section ["Batch mapping"](https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping) de la documentation ü§ó Datasets utile pour cette t√¢che.

</Tip>

Maintenant que nous avons un commentaire par ligne, cr√©ons une nouvelle colonne `comments_length` contenant le nombre de mots par commentaire¬†:

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

Nous pouvons utiliser cette nouvelle colonne pour filtrer les commentaires courts, qui incluent g√©n√©ralement des √©l√©ments tels que "cc @lewtun" ou "Merci¬†!" qui ne sont pas pertinents pour notre moteur de recherche. Il n'y a pas de nombre pr√©cis √† s√©lectionner pour le filtre, mais environ 15 mots semblent √™tre un bon d√©but :

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

Apr√®s avoir un peu nettoy√© notre ensemble de donn√©es, concat√©nons le titre, la description et les commentaires du probl√®me dans une nouvelle colonne "texte". Comme d'habitude, nous allons √©crire une fonction simple que nous pouvons passer √† `Dataset.map()`¬†:

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)
```

Nous sommes enfin pr√™ts √† cr√©er des embeddings¬†! Nous allons jeter un coup d'oeil.

## Cr√©ation d'incorporations de texte

Nous avons vu dans [Chapitre 2](/course/chapter2) que nous pouvons obtenir des incorporations de jetons en utilisant la classe `AutoModel`. Tout ce que nous avons √† faire est de choisir un point de contr√¥le appropri√© √† partir duquel charger le mod√®le. Heureusement, il existe une biblioth√®que appel√©e `sentence-transformers` d√©di√©e √† la cr√©ation d'incorporations. Comme d√©crit dans la [documentation] de la biblioth√®que (https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), notre cas d'utilisation est un exemple de _asymmetric recherche s√©mantique_ car nous avons une requ√™te courte dont nous aimerions trouver la r√©ponse dans un document plus long, comme un commentaire sur un probl√®me. Le [tableau de pr√©sentation des mod√®les] (https://www.sbert.net/docs/pretrained_models.html#model-overview) pratique de la documentation indique que le point de contr√¥le `multi-qa-mpnet-base-dot-v1` a le meilleures performances pour la recherche s√©mantique, nous l'utiliserons donc pour notre application. Nous allons √©galement charger le tokenizer en utilisant le m√™me point de contr√¥le¬†:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

Pour acc√©l√©rer le processus d'int√©gration, il est utile de placer le mod√®le et les entr√©es sur un p√©riph√©rique GPU, alors faisons-le maintenant¬†:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

Notez que nous avons d√©fini `from_pt=True` comme argument de la m√©thode `from_pretrained()`. C'est parce que le point de contr√¥le `multi-qa-mpnet-base-dot-v1` n'a que des poids PyTorch, donc d√©finir `from_pt=True` les convertira automatiquement au format TensorFlow pour nous. Comme vous pouvez le voir, il est tr√®s simple de passer d'un framework √† l'autre dans ü§ó Transformers !

{/if}

Comme nous l'avons mentionn√© pr√©c√©demment, nous aimerions repr√©senter chaque entr√©e dans notre corpus de probl√®mes GitHub comme un vecteur unique, nous devons donc "regrouper" ou faire la moyenne de nos incorporations de jetons d'une mani√®re ou d'une autre. Une approche populaire consiste √† effectuer un * regroupement CLS * sur les sorties de notre mod√®le, o√π nous collectons simplement le dernier √©tat cach√© pour le jeton sp√©cial `[CLS]`. La fonction suivante fait l'affaire pour nous :

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

Ensuite, nous allons cr√©er une fonction d'assistance qui va tokeniser une liste de documents, placer les tenseurs sur le GPU, les alimenter au mod√®le et enfin appliquer le regroupement CLS aux sorties¬†:

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Nous pouvons tester le fonctionnement de la fonction en lui fournissant la premi√®re entr√©e de texte dans notre corpus et en inspectant la forme de sortie¬†:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

Super, nous avons converti la premi√®re entr√©e de notre corpus en un vecteur √† 768 dimensions ! Nous pouvons utiliser `Dataset.map()` pour appliquer notre fonction `get_embeddings()` √† chaque ligne de notre corpus, cr√©ons donc une nouvelle colonne `embeddings` comme suit¬†:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Nous pouvons tester le fonctionnement de la fonction en lui fournissant la premi√®re entr√©e de texte dans notre corpus et en inspectant la forme de sortie¬†:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

Super, nous avons converti la premi√®re entr√©e de notre corpus en un vecteur √† 768 dimensions ! Nous pouvons utiliser `Dataset.map()` pour appliquer notre fonction `get_embeddings()` √† chaque ligne de notre corpus, cr√©ons donc une nouvelle colonne `embeddings` comme suit¬†:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}


Notez que nous avons converti les int√©grations en tableaux NumPy -- c'est parce que ü§ó Datasets n√©cessite ce format lorsque nous essayons de les indexer avec FAISS, ce que nous ferons ensuite.

## Utilisation de FAISS pour une recherche de similarit√© efficace

Maintenant que nous avons un ensemble de donn√©es d'incorporations, nous avons besoin d'un moyen de les rechercher. Pour ce faire, nous utiliserons une structure de donn√©es sp√©ciale dans ü§ó Datasets appel√©e _FAISS index_. [FAISS](https://faiss.ai/) (abr√©viation de Facebook AI Similarity Search) est une biblioth√®que qui fournit des algorithmes efficaces pour rechercher et regrouper rapidement des vecteurs d'int√©gration.

L'id√©e de base derri√®re FAISS est de cr√©er une structure de donn√©es sp√©ciale appel√©e un _index_ qui permet de trouver quels plongements sont similaires √† un plongement d'entr√©e. Cr√©er un index FAISS dans ü§ó Datasets est simple -- nous utilisons la fonction `Dataset.add_faiss_index()` et sp√©cifions quelle colonne de notre jeu de donn√©es nous aimerions indexer¬†:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

Nous pouvons maintenant effectuer des requ√™tes sur cet index en effectuant une recherche de voisin le plus proche avec la fonction `Dataset.get_nearest_examples()`. Testons cela en int√©grant d'abord une question comme suit¬†:

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

Tout comme avec les documents, nous avons maintenant un vecteur de 768 dimensions repr√©sentant la requ√™te, que nous pouvons comparer √† l'ensemble du corpus pour trouver les plongements les plus similaires¬†:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

La fonction `Dataset.get_nearest_examples()` renvoie un tuple de scores qui classent le chevauchement entre la requ√™te et le document, et un ensemble correspondant d'√©chantillons (ici, les 5 meilleures correspondances). Collectons-les dans un `pandas.DataFrame` afin de pouvoir les trier facilement¬†:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Nous pouvons maintenant parcourir les premi√®res lignes pour voir dans quelle mesure notre requ√™te correspond aux commentaires disponibles¬†:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Pas mal! Notre deuxi√®me r√©sultat semble correspondre √† la requ√™te.

<Tip>

‚úèÔ∏è **Essayez-le¬†!** Cr√©ez votre propre requ√™te et voyez si vous pouvez trouver une r√©ponse dans les documents r√©cup√©r√©s. Vous devrez peut-√™tre augmenter le param√®tre `k` dans `Dataset.get_nearest_examples()` pour √©largir la recherche.

</Tip>