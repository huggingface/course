# Introduction

Dans le [Chapitre 3](/course/chapter3) vous avez eu un premier aperÃ§u de la bibliothÃ¨que ğŸ¤— Datasets et vous avez vu qu'il y avait trois Ã©tapes principales pour affiner un modÃ¨le:

1. Chargez un jeu de donnÃ©es Ã  partir de Hugging Face Hub.
2. PrÃ©traitez les donnÃ©es avec `Dataset.map()`.
3. Charger et calculer des mÃ©triques.

Mais ce n'est qu'effleurer la surface de ce que ğŸ¤— Datasets peut faireÂ ! Dans ce chapitre, nous allons plonger profondÃ©ment dans la bibliothÃ¨que. En cours de route, nous trouverons des rÃ©ponses aux questions suivantes:

* Que faites-vous lorsque votre jeu de donnÃ©es n'est pas sur le Hub ?
* Comment pouvez-vous dÃ©couper et trancher un ensemble de donnÃ©esÂ ? (Et si vous avez _vraiment_ besoin d'utiliser PandasÂ ?)
* Que faites-vous lorsque votre ensemble de donnÃ©es est Ã©norme et va faire fondre la RAM de votre ordinateur portableÂ ?
* Qu'est-ce que c'est que le "mappage de la mÃ©moire" et Apache ArrowÂ ?
* Comment pouvez-vous crÃ©er votre propre ensemble de donnÃ©es et le pousser vers le HubÂ ?

Les techniques que vous apprenez ici vous prÃ©pareront aux tÃ¢ches avancÃ©es de tokenisation et de rÃ©glage fin du [Chapitre 6](/course/chapter6) et du [Chapitre 7](/course/chapter7) -- alors prenez un cafÃ© et commenÃ§onsÂ !