# Introduction

Dans le [chapitre 3](/course/fr/chapter3) vous avez eu un premier aperÃ§u de la bibliothÃ¨que ğŸ¤— *Datasets* et des trois Ã©tapes principales pour *finetuner* un modÃ¨le :

1. chargement d'un jeu de donnÃ©es Ã  partir du *Hub* dâ€™Hugging Face,
2. prÃ©traitement des donnÃ©es avec `Dataset.map()`,
3. chargement et calcul des mÃ©triques.

Mais ce n'est qu'effleurer la surface de ce que ğŸ¤— *Datasets* peut faire ! Dans ce chapitre, nous allons plonger profondÃ©ment dans cette bibliothÃ¨que. En cours de route, nous trouverons des rÃ©ponses aux questions suivantes :

* que faire lorsque votre jeu de donnÃ©es n'est pas sur le *Hub* ?
* comment dÃ©couper et trancher un jeu de donnÃ©es ? (Et si on a _vraiment_ besoin d'utiliser Pandas ?)
* que faire lorsque votre jeu de donnÃ©es est Ã©norme et va monopoliser la RAM de votre ordinateur portable ?
* qu'est-ce que c'est que le Â« *memory mapping* Â» et Apache Arrow ?
* comment crÃ©er votre propre jeu de donnÃ©es et le pousser sur le *Hub* ?

Les techniques apprises dans ce chapitre vous prÃ©pareront aux tÃ¢ches avancÃ©es de tokenisation du [chapitre 6](/course/fr/chapter6) et de *finetuning* du [chapitre 7](/course/fr/chapter7). Alors prenez un cafÃ© et commenÃ§ons !