# Introduction

Dans le [Chapitre 3](/course/fr/chapter3) vous avez eu un premier aperÃ§u de la bibliothÃ¨que ğŸ¤— *Datasets* et qu'il y a trois Ã©tapes principales pour *finetuner* un modÃ¨le :

1. charger un jeu de donnÃ©es Ã  partir du *Hub* dâ€™Hugging Face.
2. PrÃ©traiter les donnÃ©es avec `Dataset.map()`.
3. Charger et calculer des mÃ©triques.

Mais ce n'est qu'effleurer la surface de ce que ğŸ¤— *Datasets* peut faire ! Dans ce chapitre, nous allons plonger profondÃ©ment dans cette bibliothÃ¨que. En cours de route, nous trouverons des rÃ©ponses aux questions suivantes :

* que faire lorsque votre jeu de donnÃ©es n'est pas sur le *Hub* ?
* comment dÃ©couper et trancher un jeu de donnÃ©es ? (Et si on a _vraiment_ besoin d'utiliser Pandas ?)
* que faire lorsque votre jeu de donnÃ©es est Ã©norme et va monopoliser la RAM de votre ordinateur portable ?
* qu'est-ce que c'est que le Â« *memory mapping* Â» et Apache Arrow ?
* comment crÃ©er votre propre jeu de donnÃ©es et le pousser sur le *Hub* ?

Les techniques apprises dans ce chapitre vous prÃ©pareront aux tÃ¢ches avancÃ©es de tokenisation et de *finetuning* du [Chapitre 6](/course/fr/chapter6) et du [Chapitre 7](/course/fr/chapter7). Alors prenez un cafÃ© et commenÃ§ons !
