# Que faire si mon jeu de donn√©es n'est pas sur le <i>Hub</i> ?

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter5/section2.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter5/section2.ipynb"},
]} />

Vous savez comment utiliser le [*Hub*](https://huggingface.co/datasets) pour t√©l√©charger des jeux de donn√©es mais en pratique vous vous retrouverez souvent √† travailler avec des donn√©es stock√©es sur votre ordinateur portable ou sur un serveur distant. Dans cette section, nous allons vous montrer comment ü§ó *Datasets* peut √™tre utilis√© pour charger des jeux de donn√©es qui ne sont pas disponibles sur le *Hub* d‚ÄôHugging Face.

<Youtube id="HyQgpJTkRdE"/>

## Travailler avec des jeux de donn√©es locaux et distants

ü§ó *Datasets* fournit des scripts de chargement de jeux de donn√©es locaux et distants. La biblioth√®que prend en charge plusieurs formats de donn√©es courants, tels que :

| Format des donn√©es  | Script de chargement |                         Exemple                         |
| :----------------: | :------------------: | :-----------------------------------------------------: |
|     CSV & TSV      |        `csv`         |     `load_dataset("csv", data_files="my_file.csv")`     |
|   Fichiers texte   |        `text`        |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines |        `json`        |   `load_dataset("json", data_files="my_file.jsonl")`    |
| DataFrames en Pickle |       `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

Comme indiqu√© dans le tableau, pour chaque format de donn√©es, nous avons juste besoin de sp√©cifier le type de script de chargement dans la fonction `load_dataset()`, ainsi qu'un argument `data_files` qui sp√©cifie le chemin vers un ou plusieurs fichiers. Commen√ßons par charger un jeu de donn√©es √† partir de fichiers locaux puis plus tard comment faire la m√™me chose avec des fichiers distants.

## Charger un jeu de donn√©es local

Pour cet exemple, nous utilisons le jeu de donn√©es [SQuAD-it](https://github.com/crux82/squad-it/) qui est un grand jeu de donn√©es pour la t√¢che de *Question Awnswering* en italien.

Les √©chantillons d‚Äôentra√Ænement et de test sont h√©berg√©s sur GitHub, nous pouvons donc les t√©l√©charger avec une simple commande `wget` :

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Cela t√©l√©charge deux fichiers compress√©s appel√©s *SQuAD_it-train.json.gz* et *SQuAD_it-test.json.gz* que nous pouvons d√©compresser avec la commande Linux `gzip` :

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

Nous pouvons voir que les fichiers compress√©s ont √©t√© remplac√©s par _SQuAD_it-train.json_ et _SQuAD_it-text.json_, et que les donn√©es sont stock√©es au format JSON.

<Tip>

‚úé Si vous vous demandez pourquoi il y a un caract√®re `!` dans les commandes *shell* ci-dessus, c'est parce que nous les ex√©cutons dans un *notebook* Jupyter. Supprimez simplement le pr√©fixe si vous souhaitez t√©l√©charger et d√©compresser le jeu de donn√©es dans un terminal.

</Tip>

Pour charger un fichier JSON avec la fonction `load_dataset()`, nous avons juste besoin de savoir si nous avons affaire √† du JSON ordinaire (similaire √† un dictionnaire imbriqu√©) ou √† des lignes JSON (JSON s√©par√© par des lignes). Comme de nombreux jeux de donn√©es de questions-r√©ponses, SQuAD-it utilise le format imbriqu√© o√π tout le texte est stock√© dans un champ `data`. Cela signifie que nous pouvons charger le jeu de donn√©es en sp√©cifiant l'argument `field` comme suit :

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Par d√©faut, le chargement de fichiers locaux cr√©e un objet `DatasetDict` avec un √©chantillon `train`. Nous pouvons le voir en inspectant l'objet `squad_it_dataset` :

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Cela nous montre le nombre de lignes et les noms de colonnes associ√©s √† l‚Äô√©chantillon d‚Äôentra√Ænement. Nous pouvons afficher l'un des exemples en indexant l‚Äô√©chantillon `train` comme suit :

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008", # S√©isme du Sichuan 2008
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
			# Le tremblement de terre du Sichuan de 2008 ou le...
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si √® verificato il terremoto nel Sichuan?", 
					# En quelle ann√©e le tremblement de terre du Sichuan a-t-il eu lieu ?
                },
                ...
            ],
        },
        ...
    ],
}
```

Super, nous avons charg√© notre premier jeu de donn√©es local ! Mais ce que nous voulons vraiment, c'est inclure √† la fois les √©chantillons `train` et `test` dans un seul objet `DatasetDict` afin que nous puissions appliquer les fonctions `Dataset.map()` sur les deux √† la fois . Pour ce faire, nous pouvons fournir un dictionnaire √† l'argument `data_files` qui associe chaque nom des √©chantillons √† un fichier associ√© √† cet √©chantillon :

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

C'est exactement ce que nous voulions. D√©sormais, nous pouvons appliquer diverses techniques de pr√©traitement pour nettoyer les donn√©es, tokeniser les avis, etc.

<Tip>

L'argument `data_files` de la fonction `load_dataset()` est assez flexible et peut √™tre soit un chemin de fichier unique, une liste de chemins de fichiers, ou un dictionnaire qui fait correspondre les noms des √©chantillons aux chemins de fichiers. Vous pouvez √©galement regrouper les fichiers correspondant √† un motif sp√©cifi√© selon les r√®gles utilis√©es par le shell Unix. Par exemple, vous pouvez regrouper tous les fichiers JSON d'un r√©pertoire en une seule division en d√©finissant `data_files="*.json"`. Voir la [documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) de ü§ó *Datasets* pour plus de d√©tails.

</Tip>

Les scripts de chargement de ü§ó *Datasets* prennent en charge la d√©compression automatique des fichiers d'entr√©e. Nous aurions donc pu ignorer l'utilisation de `gzip` en pointant l'argument `data_files` directement sur les fichiers compress√©s :

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Cela peut √™tre utile si vous ne souhaitez pas d√©compresser manuellement de nombreux fichiers GZIP. La d√©compression automatique s'applique √©galement √† d'autres formats courants tels que ZIP et TAR. Il vous suffit donc de pointer `data_files` vers les fichiers compress√©s et vous √™tes pr√™t √† partir !

Maintenant que vous savez comment charger des fichiers locaux sur votre ordinateur portable ou de bureau, examinons le chargement de fichiers distants.

## Charger un jeu de donn√©es distant

Si vous travaillez en tant que *data scientist* ou codeur dans une entreprise, il y a de fortes chances que les jeux de donn√©es que vous souhaitez analyser soient stock√©s sur un serveur distant. Heureusement, charger des fichiers distants est aussi simple que de charger des fichiers locaux ! Au lieu de fournir un chemin vers les fichiers locaux, nous pointons l'argument `data_files` de `load_dataset()` vers une ou plusieurs URL o√π les fichiers distants sont stock√©s. Par exemple, pour le jeu de donn√©es SQuAD-it h√©berg√© sur GitHub, nous pouvons simplement faire pointer `data_files` vers les URL _SQuAD_it-*.json.gz_ comme suit :

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Cela renvoie le m√™me objet `DatasetDict` obtenu ci-dessus mais nous √©vite de t√©l√©charger et de d√©compresser manuellement les fichiers _SQuAD_it-*.json.gz_. Ceci conclut notre incursion dans les diff√©rentes fa√ßons de charger des jeux de donn√©es qui ne sont pas h√©berg√©s sur le *Hub*. Maintenant que nous avons un jeu de donn√©es avec lequel jouer, mettons la main √† la p√¢te avec diverses techniques de gestion des donn√©es !

<Tip>

‚úèÔ∏è **Essayez !** Choisissez un autre jeu de donn√©es h√©berg√© sur GitHub ou dans le [*UCI Machine Learning Repository*](https://archive.ics.uci.edu/ml/index.php) et essayez de le charger localement et √† distance en utilisant les techniques pr√©sent√©es ci-dessus. Pour obtenir des points bonus, essayez de charger un jeu de donn√©es stock√© au format CSV ou texte (voir la [documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) pour plus d'informations sur ces formats).

</Tip>
