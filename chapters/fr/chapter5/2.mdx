# Que faire si mon ensemble de donn√©es n'est pas sur le Hub¬†?

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section2.ipynb"},
]} />

Vous savez comment utiliser le [Hugging Face Hub](https://huggingface.co/datasets) pour t√©l√©charger des ensembles de donn√©es, mais vous vous retrouverez souvent √† travailler avec des donn√©es stock√©es sur votre ordinateur portable ou sur un serveur distant. Dans cette section, nous allons vous montrer comment ü§ó Datasets peut √™tre utilis√© pour charger des ensembles de donn√©es qui ne sont pas disponibles sur le Hugging Face Hub.

<Youtube id="HyQgpJTkRdE"/>

## Travailler avec des ensembles de donn√©es locaux et distants

ü§ó Datasets fournit des scripts de chargement pour g√©rer le chargement des ensembles de donn√©es locaux et distants. Il prend en charge plusieurs formats de donn√©es courants, tels que¬†:

| Format de donn√©es  | Chargement du script |                         Exemple                         |
| :----------------: | :------------------: | :-----------------------------------------------------: |
|     CSV & TSV      |        `csv`         |     `load_dataset("csv", data_files="my_file.csv")`     |
|   Fichiers texte   |        `text`        |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & Lignes JSON |        `json`        |   `load_dataset("json", data_files="my_file.jsonl")`    |
| DataFrames marin√©s |       `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

Comme indiqu√© dans le tableau, pour chaque format de donn√©es, nous avons juste besoin de sp√©cifier le type de script de chargement dans la fonction `load_dataset()`, ainsi qu'un argument `data_files` qui sp√©cifie le chemin vers un ou plusieurs fichiers. Commen√ßons par charger un jeu de donn√©es √† partir de fichiers locaux¬†; plus tard, nous verrons comment faire la m√™me chose avec des fichiers distants.

## Charger un jeu de donn√©es local

Pour cet exemple, nous utiliserons l'ensemble de donn√©es [SQuAD-it](https://github.com/crux82/squad-it/), qui est un ensemble de donn√©es √† grande √©chelle pour r√©pondre aux questions en Italien.

Les fractionnements de formation et de test sont h√©berg√©s sur GitHub, nous pouvons donc les t√©l√©charger avec une simple commande `wget`¬†:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Cela t√©l√©chargera deux fichiers compress√©s appel√©s *SQuAD_it-train.json.gz* et *SQuAD_it-test.json.gz*, que nous pouvons d√©compresser avec la commande Linux `gzip`¬†:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

Nous pouvons voir que les fichiers compress√©s ont √©t√© remplac√©s par _SQuAD_it-train.json_ et _SQuAD_it-text.json_, et que les donn√©es sont stock√©es au format JSON.

<Tip>

‚úé Si vous vous demandez pourquoi il y a un caract√®re `!` dans les commandes shell ci-dessus, c'est parce que nous les ex√©cutons dans un cahier Jupyter. Supprimez simplement le pr√©fixe si vous souhaitez t√©l√©charger et d√©compresser l'ensemble de donn√©es dans un terminal.

</Tip>

Pour charger un fichier JSON avec la fonction `load_dataset()`, nous avons juste besoin de savoir si nous avons affaire √† du JSON ordinaire (similaire √† un dictionnaire imbriqu√©) ou √† des lignes JSON (JSON s√©par√© par des lignes). Comme de nombreux ensembles de donn√©es de questions-r√©ponses, SQuAD-it utilise le format imbriqu√©, avec tout le texte stock√© dans un champ "donn√©es". Cela signifie que nous pouvons charger le jeu de donn√©es en sp√©cifiant l'argument `field` comme suit¬†:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Par d√©faut, le chargement de fichiers locaux cr√©e un objet `DatasetDict` avec une division `train`. Nous pouvons le voir en inspectant l'objet `squad_it_dataset`¬†:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Cela nous montre le nombre de lignes et les noms de colonnes associ√©s √† l'ensemble d'apprentissage. Nous pouvons afficher l'un des exemples en indexant la division "train" comme suit¬†:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si √® verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Super, nous avons charg√© notre premier jeu de donn√©es local¬†! Mais bien que cela ait fonctionn√© pour l'ensemble d'entra√Ænement, ce que nous voulons vraiment, c'est inclure √† la fois les divisions `train` et `test` dans un seul objet `DatasetDict` afin que nous puissions appliquer les fonctions `Dataset.map()` sur les deux divisions √† la fois . Pour ce faire, nous pouvons fournir un dictionnaire √† l'argument `data_files` qui associe chaque nom de division √† un fichier associ√© √† cette division¬†:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

C'est exactement ce que nous voulions. D√©sormais, nous pouvons appliquer diverses techniques de pr√©traitement pour nettoyer les donn√©es, tokeniser les avis, etc.

<Tip>

The `data_files` argument of the `load_dataset()` function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also group files matching a specified pattern according to the rules used by the Unix shell (for example, you can group all JSON files in a directory into a single division by setting `data_files="*.json"` ). See ü§ó Datasets [documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) for details.

</Tip>

Les scripts de chargement dans ü§ó Datasets prend en charge la d√©compression automatique des fichiers d'entr√©e, nous aurions donc pu ignorer l'utilisation de `gzip` en pointant l'argument `data_files` directement sur les fichiers compress√©s¬†:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Cela peut √™tre utile si vous ne souhaitez pas d√©compresser manuellement de nombreux fichiers GZIP. La d√©compression automatique s'applique √©galement √† d'autres formats courants tels que ZIP et TAR, il vous suffit donc de pointer `data_files` vers les fichiers compress√©s et vous √™tes pr√™t √† partir¬†!

Maintenant que vous savez comment charger des fichiers locaux sur votre ordinateur portable ou de bureau, examinons le chargement de fichiers distants.

## Charger un jeu de donn√©es distant

Si vous travaillez en tant que data scientist ou codeur dans une entreprise, il y a de fortes chances que les ensembles de donn√©es que vous souhaitez analyser soient stock√©s sur un serveur distant. Heureusement, charger des fichiers distants est aussi simple que de charger des fichiers locaux¬†! Au lieu de fournir un chemin vers les fichiers locaux, nous pointons l'argument `data_files` de `load_dataset()` vers une ou plusieurs URL o√π les fichiers distants sont stock√©s. Par exemple, pour l'ensemble de donn√©es SQuAD-it h√©berg√© sur GitHub, nous pouvons simplement faire pointer `data_files` vers les URL _SQuAD_it-*.json.gz_ comme suit¬†:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Cela renvoie le m√™me objet `DatasetDict` obtenu ci-dessus, mais nous √©vite de t√©l√©charger et de d√©compresser manuellement les fichiers _SQuAD_it-*.json.gz_. Ceci conclut notre incursion dans les diff√©rentes fa√ßons de charger des ensembles de donn√©es qui ne sont pas h√©berg√©s sur le Hugging Face Hub. Maintenant que nous avons un ensemble de donn√©es avec lequel jouer, mettons-nous la main √† la p√¢te avec diverses techniques de gestion des donn√©es¬†!

<Tip>

‚úèÔ∏è **Essayez-le !** Choisissez un autre ensemble de donn√©es h√©berg√© sur GitHub ou le [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) et essayez de le charger localement et √† distance en utilisant les techniques pr√©sent√©es ci-dessus. Pour obtenir des points bonus, essayez de charger un ensemble de donn√©es stock√© au format CSV ou texte (voir la [documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) pour plus d'informations sur ces formats).

</Tip>


