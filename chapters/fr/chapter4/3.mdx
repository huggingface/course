<FrameworkSwitchCourse {fw} />

# Partage de mod√®les pr√©-entra√Æn√©s

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section3_tf.ipynb"},
]} />

{/if}

Dans les √©tapes ci-dessous, nous allons examiner les moyens les plus simples de partager des mod√®les pr√©-entra√Æn√©s sur le ü§ó *Hub*. Il existe des outils et des services disponibles qui permettent de simplifier le partage et la mise √† jour des mod√®les directement sur le *Hub*, que nous allons explorer ci-dessous.

<Youtube id="9yY3RB_GSPM"/>

Nous encourageons tous les utilisateurs qui entra√Ænent des mod√®les √† contribuer en les partageant avec la communaut√©. Le partage des mod√®les, m√™me s'ils ont √©t√© entra√Æn√©s sur des jeux de donn√©es tr√®s sp√©cifiques, aidera les autres, en leur faisant gagner du temps, des ressources de calcul et en leur donnant acc√®s √† des artefacts entra√Æn√©s utiles. √Ä votre tour, vous pourrez b√©n√©ficier du travail effectu√© par les autres !

Il y a trois fa√ßons de cr√©er de nouveaux d√©p√¥ts de mod√®les :

- en utilisant l'API `push_to_hub`,
- en utilisant la biblioth√®que Python `huggingface_hub`,
- en utilisant l'interface web.

Une fois que vous avez cr√©√© un d√©p√¥t, vous pouvez y charger des fichiers via git et git-lfs. Nous allons vous guider dans la cr√©ation de d√©p√¥ts de mod√®les et le t√©l√©chargement de fichiers dans les sections suivantes.


## Utilisation de l'API `push_to_hub`

{#if fw === 'pt'}

<Youtube id="Zh0FfmVrKX0"/>

{:else}

<Youtube id="pUh5cGmNV8Y"/>

{/if}

La fa√ßon la plus simple de t√©l√©charger des fichiers vers le *Hub* est d'utiliser l'API `push_to_hub`.

Avant d'aller plus loin, vous devrez g√©n√©rer un jeton d'authentification afin que l'API `huggingface_hub` sache qui vous √™tes et √† quels espaces de noms vous avez acc√®s en √©criture. Assurez-vous que vous √™tes dans un environnement o√π vous avez install√© `transformers` (voir la [Configuration](/course/fr/chapter0)). Si vous √™tes dans un *notebook*, vous pouvez utiliser la fonction suivante pour vous connecter :

```python
from huggingface_hub import notebook_login

notebook_login()
```

Dans un terminal, vous pouvez ex√©cuter :

```bash
huggingface-cli login
```

Dans les deux cas, vous serez invit√© √† saisir votre nom d'utilisateur et votre mot de passe, qui sont les m√™mes que ceux que vous utilisez pour vous connecter au *Hub*. Si vous n'avez pas encore de profil pour le Hub, vous devez en cr√©er un [ici](https://huggingface.co/join).

Super ! Votre jeton d'authentification est maintenant stock√© dans votre dossier de cache. Cr√©ons quelques d√©p√¥ts !

{#if fw === 'pt'}

Si vous avez jou√© avec l'API `Trainer` pour entra√Æner un mod√®le, le moyen le plus simple de le t√©l√©charger sur le *Hub* est de d√©finir `push_to_hub=True` lorsque vous d√©finissez vos `TrainingArguments` :

```py
from transformers import TrainingArguments

training_args = TrainingArguments(
    "bert-finetuned-mrpc", save_strategy="epoch", push_to_hub=True
)
```

Lorsque vous appelez `trainer.train()`, le `Trainer` t√©l√©chargera alors votre mod√®le vers le *Hub* √† chaque fois qu'il sera sauvegard√© (ici √† chaque √©poque) dans un d√©p√¥t dans votre espace personnel. Ce d√©p√¥t sera nomm√© comme le r√©pertoire de sortie que vous avez choisi (ici `bert-finetuned-mrpc`) mais vous pouvez choisir un nom diff√©rent avec `hub_model_id = "a_different_name"`.

Pour t√©l√©charger votre mod√®le vers une organisation dont vous √™tes membre, passez-le simplement avec `hub_model_id = "my_organization/my_repo_name"`.

Une fois que votre entra√Ænement est termin√©, vous devriez faire un dernier `trainer.push_to_hub()` pour t√©l√©charger la derni√®re version de votre mod√®le. Cela g√©n√©rera √©galement une carte pour le mod√®le avec toutes les m√©tadonn√©es pertinentes, rapportant les hyperparam√®tres utilis√©s et les r√©sultats d'√©valuation ! Voici un exemple du contenu que vous pourriez trouver dans une telle carte de mod√®le :
<div class="flex justify-center">
  <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png" alt="An example of an auto-generated model card." width="100%"/>
</div>

{:else}

Si vous utilisez Keras pour entra√Æner votre mod√®le, le moyen le plus simple de le t√©l√©charger sur le *Hub* est de passer un `PushToHubCallback` lorsque vous appelez `model.fit()` :

```py
from transformers import PushToHubCallback

callback = PushToHubCallback(
    "bert-finetuned-mrpc", save_strategy="epoch", tokenizer=tokenizer
)
```

Ensuite, vous devez ajouter `callbacks=[callback]` dans votre appel √† `model.fit()`. Le *callback* t√©l√©chargera alors votre mod√®le vers le *Hub* √† chaque fois qu'il sera sauvegard√© (ici √† chaque √©poque) dans un d√©p√¥t dans votre espace de noms. Ce d√©p√¥t sera nomm√© comme le r√©pertoire de sortie que vous avez choisi (ici `bert-finetuned-mrpc`) mais vous pouvez choisir un nom diff√©rent avec `hub_model_id = "a_different_name"`.

Pour t√©l√©charger votre mod√®le dans une organisation dont vous √™tes membre, passez-le simplement avec `hub_model_id = "my_organization/my_repo_name"`.

{/if}

A un niveau inf√©rieur, l'acc√®s au *Hub* peut √™tre fait directement sur les mod√®les, les *tokenizers* et les objets de configuration via leur m√©thode `push_to_hub()`. Cette m√©thode s'occupe √† la fois de la cr√©ation du d√©p√¥t et de l'envoi les fichiers du mod√®le et du *tokenizer* directement dans le d√©p√¥t. Aucune manipulation manuelle n'est n√©cessaire, contrairement √† l'API que nous verrons plus loin.

Pour avoir une id√©e de son fonctionnement, commen√ßons par initialiser un mod√®le et un *tokenizer* :

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{/if}

Vous √™tes libre de faire ce que vous voulez avec ces objets : ajouter des *tokens* au *tokenizer*, entra√Æner le mod√®le, le *finetuner*. Une fois que vous √™tes satisfait du mod√®le, des poids et du *tokenizer* obtenus, vous pouvez utiliser la m√©thode `push_to_hub()` directement disponible sur l'objet `model` :

```py
model.push_to_hub("dummy-model")
```

Cela va cr√©er le nouveau d√©p√¥t `dummy-model` dans votre profil et le remplir avec les fichiers du mod√®le.
Faites la m√™me chose avec le *tokenizer*, de sorte que tous les fichiers sont maintenant disponibles dans ce d√©p√¥t :

```py
tokenizer.push_to_hub("dummy-model")
```

Si vous appartenez √† une organisation, il suffit de sp√©cifier l'argument `organization` pour t√©l√©charger dans l'espace de cette organisation :

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface")
```

Si vous souhaitez utiliser un jeton Hugging Face sp√©cifique, vous pouvez √©galement le sp√©cifier √† la m√©thode `push_to_hub()` :

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface", use_auth_token="<TOKEN>")
```

Maintenant, dirigez-vous sur *Hub* pour trouver votre mod√®le nouvellement t√©l√©charg√© : *https://huggingface.co/user-or-organization/dummy-model*.

Cliquez sur l'onglet ¬´ Fichiers et versions ¬ª et vous devriez voir les fichiers visibles dans la capture d'√©cran suivante :

{#if fw === 'pt'}
<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</div>
{:else}
<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model_tf.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</div>
{/if}

<Tip>

‚úèÔ∏è **Essayez** Prenez le mod√®le et le *tokenizer* associ√©s au *checkpoint* `bert-base-cased` et t√©l√©chargez-les vers un d√©p√¥t dans votre espace en utilisant la m√©thode `push_to_hub()`. V√©rifiez que le d√©p√¥t appara√Æt correctement sur votre page avant de le supprimer.

</Tip>

Comme vous l'avez vu, la m√©thode `push_to_hub()` accepte plusieurs arguments, ce qui permet de t√©l√©charger vers un d√©p√¥t ou un espace d'organisation sp√©cifique, ou d'utiliser un jeton d'API diff√©rent. Nous vous recommandons de jeter un coup d'≈ìil √† la sp√©cification de la m√©thode disponible directement dans la documentation de [ü§ó *Transformers*](https://huggingface.co/transformers/model_sharing.html) pour avoir une id√©e de ce qui est possible.

La m√©thode `push_to_hub()` est soutenue par le *package* Python [`huggingface_hub`](https://github.com/huggingface/huggingface_hub), qui offre une API directe au *Hub*. C'est int√©gr√© √† ü§ó *Transformers* et √† plusieurs autres biblioth√®ques d'apprentissage automatique, comme [`allenlp`](https://github.com/allenai/allennlp). Bien que nous nous concentrions sur l'int√©gration via ü§ó *Transformers* dans ce chapitre, son int√©gration dans votre propre code ou biblioth√®que est simple.

Passez √† la derni√®re section pour voir comment t√©l√©charger des fichiers dans votre d√©p√¥t nouvellement cr√©√© !

## Utilisation de la biblioth√®que Python `huggingface_hub`

La biblioth√®que Python `huggingface_hub` est un *package* qui offre un ensemble d'outils pour les hubs des mod√®les et des jeux de donn√©es. Elle fournit des m√©thodes et des classes simples pour des t√¢ches courantes telles qu'obtenir et g√©rer des informations √† propos des d√©p√¥ts sur le *Hub*. Elle fournit des APIs simples qui fonctionnent au-dessus de git pour g√©rer le contenu de ces d√©p√¥ts et pour int√©grer le *Hub* dans vos projets et biblioth√®ques.

De la m√™me mani√®re que pour l'utilisation de l'API `push_to_hub`, vous devrez avoir votre jeton d'API enregistr√© dans votre cache. Pour ce faire, vous devrez utiliser la commande `login` de la CLI, comme mentionn√© dans la section pr√©c√©dente (encore une fois, assurez-vous de faire pr√©c√©der ces commandes du caract√®re `!` si vous les ex√©cutez dans Google Colab) :

```bash
huggingface-cli login
```

Le *package* `huggingface_hub` offre plusieurs m√©thodes et classes qui sont utiles pour notre objectif. Tout d'abord, il y a quelques m√©thodes pour g√©rer la cr√©ation, la suppression des d√©p√¥ts, et autres :

```python no-format
from huggingface_hub import (
    # Gestion des utilisateurs
    login,
    logout,
    whoami,

    # Cr√©ation et gestion du d√©p√¥t
    create_repo,
    delete_repo,
    update_repo_visibility,

    # Et quelques m√©thodes pour r√©cup√©rer/changer des informations sur le contenu
    list_models,
    list_datasets,
    list_metrics,
    list_repo_files,
    upload_file,
    delete_file,
)
```


De plus, elle offre la tr√®s puissante classe `Repository` pour g√©rer un d√©p√¥t local. Nous allons explorer ces m√©thodes et cette classe dans les prochaines sections pour comprendre comment les exploiter.

La m√©thode `create_repo` peut √™tre utilis√©e pour cr√©er un nouveau d√©p√¥t sur le *Hub* :

```py
from huggingface_hub import create_repo

create_repo("dummy-model")
```

Ceci cr√©era le d√©p√¥t `dummy-model` dans votre espace. Si vous le souhaitez, vous pouvez sp√©cifier √† quelle organisation le d√©p√¥t doit appartenir en utilisant l'argument `organization` :

```py
from huggingface_hub import create_repo

create_repo("dummy-model", organization="huggingface")
```

Cela cr√©era le d√©p√¥t `dummy-model` dans l'espace de nom `huggingface`, en supposant que vous appartenez √† cette organisation.
D'autres arguments qui peuvent √™tre utiles sont :

- `private`, afin de sp√©cifier si le d√©p√¥t doit √™tre visible des autres ou non,
- `token`, si vous voulez remplacer le jeton stock√© dans votre cache par un jeton donn√©,
- `repo_type`, si vous souhaitez cr√©er un `dataset` ou un `space` au lieu d'un mod√®le. Les valeurs accept√©es sont `"dataset"` et `"space"`.

Une fois que le d√©p√¥t est cr√©√©, nous devons y ajouter des fichiers ! Passez √† la section suivante pour voir les trois fa√ßons dont cela peut √™tre g√©r√©.


## Utilisation de l'interface web

L'interface web offre des outils pour g√©rer les d√©p√¥ts directement dans le *Hub*. En utilisant l'interface, vous pouvez facilement cr√©er des d√©p√¥ts, ajouter des fichiers (m√™me de grande taille !), explorer des mod√®les, visualiser les diff√©rences, et bien plus encore.

Pour cr√©er un nouveau d√©p√¥t, visitez [huggingface.co/new](https://huggingface.co/new) :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png" alt="Page showcasing the model used for the creation of a new model repository." width="80%"/>
</div>

Tout d'abord, indiquez le propri√©taire du d√©p√¥t : il peut s'agir de vous ou de l'une des organisations auxquelles vous √™tes affili√©. Si vous choisissez une organisation, le mod√®le sera pr√©sent√© sur la page de l'organisation et chaque membre de l'organisation aura la possibilit√© de contribuer au d√©p√¥t.

Ensuite, saisissez le nom de votre mod√®le. Ce sera √©galement le nom du d√©p√¥t. Enfin, vous pouvez pr√©ciser si vous souhaitez que votre mod√®le soit public ou priv√©. Les mod√®les priv√©s sont cach√©s de la vue du public.

Apr√®s avoir cr√©√© votre d√©p√¥t de mod√®les, vous devriez voir une page comme celle-ci :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/empty_model.png" alt="An empty model page after creating a new repository." width="80%"/>
</div>

C'est l√† que votre mod√®le sera h√©berg√©. Pour commencer √† le remplir, vous pouvez ajouter un fichier README directement depuis l'interface web.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/dummy_model.png" alt="The README file showing the Markdown capabilities." width="80%"/>
</div>

Le fichier README est en Markdown. N'h√©sitez pas √† vous l√¢cher avec lui ! La troisi√®me partie de ce chapitre est consacr√©e √† la construction d'une carte de mod√®le. Celles-ci sont d'une importance capitale pour valoriser votre mod√®le, car c'est par elles que vous indiquez aux autres ce qu'il peut faire.

Si vous regardez l'onglet ¬´ *Files and versions* ¬ª, vous verrez qu'il n'y a pas encore beaucoup de fichiers : juste le *README.md* que vous venez de cr√©er et le fichier *.gitattributes* qui garde la trace des gros fichiers.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/files.png" alt="The 'Files and versions' tab only shows the .gitattributes and README.md files." width="80%"/>
</div>

Nous allons maintenant voir comment ajouter de nouveaux fichiers.

## T√©l√©chargement des fichiers du mod√®le

Le syst√®me de gestion des fichiers sur le *Hub* est bas√© sur git pour les fichiers ordinaires et git-lfs (qui signifie [Git Large File Storage](https://git-lfs.github.com/)) pour les fichiers plus importants. 

Dans la section suivante, nous passons en revue trois fa√ßons diff√©rentes de t√©l√©charger des fichiers sur le *Hub* : par `huggingface_hub` et par des commandes git.

### L'approche `upload_file'

L'utilisation de `upload_file` ne n√©cessite pas que git et git-lfs soient install√©s sur votre syst√®me. Il pousse les fichiers directement vers le ü§ó *Hub* en utilisant des requ√™tes HTTP POST. Une limitation de cette approche est qu'elle ne g√®re pas les fichiers dont la taille est sup√©rieure √† 5 Go.
Si vos fichiers ont une taille sup√©rieure √† 5 Go, veuillez suivre les deux autres m√©thodes d√©taill√©es ci-dessous.

L'API peut √™tre utilis√©e comme suit :

```py
from huggingface_hub import upload_file

upload_file(
    "<path_to_file>/config.json",
    path_in_repo="config.json",
    repo_id="<namespace>/dummy-model",
)
```

Ceci t√©l√©chargera le fichier `config.json` disponible √† `<path_to_file>` √† la racine du d√©p√¥t en tant que `config.json`, vers le d√©p√¥t `dummy-model`.
D'autres arguments qui peuvent √™tre utiles sont :

- `token`, si vous souhaitez remplacer le jeton stock√© dans votre cache par un jeton donn√©,
- `repo_type`, si vous souhaitez t√©l√©charger vers un `dataset` ou un `space` au lieu d'un mod√®le. Les valeurs accept√©es sont `"dataset"` et `"space"`.


### La classe `Repository`

La classe `Repository` g√®re un d√©p√¥t local d'une mani√®re similaire √† git. Elle abstrait la plupart des probl√®mes que l'on peut rencontrer avec git pour fournir toutes les fonctionnalit√©s dont nous avons besoin. 

L'utilisation de cette classe n√©cessite l'installation de git et de git-lfs, donc assurez-vous que git-lfs est install√© (voir [ici](https://git-lfs.github.com/) pour les instructions d'installation) et configur√© avant de commencer. 

Afin de commencer √† jouer avec le d√©p√¥t que nous venons de cr√©er, nous pouvons commencer par l'initialiser dans un dossier local en clonant le d√©p√¥t distant :

```py
from huggingface_hub import Repository

repo = Repository("<path_to_dummy_folder>", clone_from="<namespace>/dummy-model")
```

Cela a cr√©√© le dossier `<path_to_dummy_folder>` dans notre r√©pertoire de travail. Ce dossier ne contient que le fichier `.gitattributes` car c'est le seul fichier cr√©√© lors de l'instanciation du d√©p√¥t par `create_repo`.

A partir de maintenant, nous pouvons utiliser plusieurs des m√©thodes traditionnelles de git :

```py
repo.git_pull()
repo.git_add()
repo.git_commit()
repo.git_push()
repo.git_tag()
```

Et d'autres encore ! Nous vous recommandons de jeter un coup d‚Äô≈ìil  √† la documentation de `Repository` disponible [ici](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub#advanced-programmatic-repository-management) pour une vue d'ensemble de toutes les m√©thodes disponibles.

Actuellement, nous avons un mod√®le et un *tokenizer* que nous voulons pousser vers le *Hub*. Nous avons r√©ussi √† cloner le d√©p√¥t, nous pouvons donc enregistrer les fichiers dans ce d√©p√¥t.

Nous nous assurons d'abord que notre clone local est √† jour en r√©cup√©rant les derni√®res modifications :

```py
repo.git_pull()
```

Une fois que c'est fait, nous sauvegardons les fichiers du mod√®le et du *tokenizer* :

```py
model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```

Le `<path_to_dummy_folder>` contient maintenant tous les fichiers du mod√®le et du *tokenizer*. Nous suivons le flux de travail git habituel en ajoutant des fichiers √† la zone de transit, en les validant et en les poussant vers le *Hub* :

```py
repo.git_add()
repo.git_commit("Add model and tokenizer files")
repo.git_push()
```

F√©licitations ! Vous venez de pousser vos premiers fichiers sur le *Hub*.

### L'approche bas√©e sur git

Il s'agit de l'approche la plus basique pour t√©l√©charger des fichiers : nous le ferons directement avec git et git-lfs. La plupart des difficult√©s sont abstraites par les approches pr√©c√©dentes, mais il y a quelques r√©serves avec la m√©thode suivante, nous allons donc suivre un cas d'utilisation plus complexe.

L'utilisation de cette classe n√©cessite l'installation de git et de git-lfs, donc assurez-vous d'avoir [git-lfs](https://git-lfs.github.com/) install√© et configur√© avant de commencer. 

Commencez par initialiser git-lfs :

```bash
git lfs install
```

```bash
Updated git hooks.
Git LFS initialized.
```

Une fois que c'est fait, la premi√®re √©tape consiste √† cloner votre d√©p√¥t de mod√®les :

```bash
git clone https://huggingface.co/<namespace>/<your-model-id>
```

Mon nom d'utilisateur est `lysandre` et j'ai utilis√© le nom de mod√®le `dummy`, donc pour moi la commande ressemble √† ce qui suit :

```
git clone https://huggingface.co/lysandre/dummy
```

J'ai maintenant un dossier nomm√© *dummy* dans mon r√©pertoire de travail. Je peux `cd` dans ce dossier et jeter un coup d'oeil √† son contenu :

```bash
cd dummy && ls
```

```bash
README.md
```

Si vous venez de cr√©er votre d√©p√¥t en utilisant la m√©thode `create_repo` du *Hub*, ce dossier devrait seulement contenir un fichier cach√© `.gitattributes`. Si vous avez suivi les instructions de la section pr√©c√©dente pour cr√©er un d√©p√¥t en utilisant l'interface web, le dossier devrait contenir un seul fichier *README.md* √† c√¥t√© du fichier cach√© `.gitattributes`, comme indiqu√© ici.

L'ajout d'un fichier de taille normale, comme un fichier de configuration, un fichier de vocabulaire, ou tout autre fichier de moins de quelques m√©gaoctets, est fait exactement comme on le ferait dans n'importe quel syst√®me bas√© sur git. Cependant, les fichiers plus volumineux doivent √™tre enregistr√©s via git-lfs afin de les pousser vers *huggingface.co*. 

Revenons un peu √† Python pour g√©n√©rer un mod√®le et un *tokenizer* que nous souhaitons ¬´ commiter ¬ª dans notre d√©p√¥t fictif :

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Faites ce que vous voulez avec le mod√®le, entra√Ænez-le, finetunez-le...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Faites ce que vous voulez avec le mod√®le, entra√Ænez-le, finetunez-le...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{/if}

Maintenant que nous avons sauvegard√© quelques artefacts de mod√®le et de *tokenizer*, regardons √† nouveau le dossier *dummy* :

```bash
ls
```

{#if fw === 'pt'}
```bash
config.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json
```

Si vous regardez la taille des fichiers (par exemple, avec `ls -lh`), vous devriez voir que le fichier d'√©tat du mod√®le (*pytorch_model.bin*) est la seule exception, avec plus de 400 Mo.

{:else}
```bash
config.json  README.md  sentencepiece.bpe.model  special_tokens_map.json  tf_model.h5  tokenizer_config.json  tokenizer.json
```

Si vous regardez la taille des fichiers (par exemple, avec `ls -lh`), vous devriez voir que le fichier dict de l'√©tat du mod√®le (*t5_model.h5*) est la seule aberration, avec plus de 400 Mo.

{/if}

<Tip>
‚úèÔ∏è Lors de la cr√©ation du d√©p√¥t √† partir de l'interface web, le fichier <i>.gitattributes</i> est automatiquement configur√© pour consid√©rer les fichiers avec certaines extensions, comme <i>.bin</i> et <i>.h5</i>, comme des fichiers volumineux, et git-lfs les suivra sans aucune configuration n√©cessaire de votre part.
</Tip> 

Nous pouvons maintenant aller de l'avant et proc√©der comme nous le ferions habituellement avec des d√©p√¥ts Git traditionnels. Nous pouvons ajouter tous les fichiers √† l'environnement Git en utilisant la commande `git add` :

```bash
git add .
```

Nous pouvons alors jeter un coup d'≈ìil aux fichiers :

```bash
git status
```

{#if fw === 'pt'}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
	new file:   config.json
	new file:   pytorch_model.bin
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{:else}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
  	new file:   config.json
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tf_model.h5
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{/if}

De m√™me, nous pouvons nous assurer que git-lfs suit les bons fichiers en utilisant sa commande `status` :

```bash
git lfs status
```

{#if fw === 'pt'}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	pytorch_model.bin (LFS: 35686c2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

Nous pouvons voir que tous les fichiers ont `Git` comme gestionnaire, sauf *pytorch_model.bin* et *sentencepiece.bpe.model*, qui ont `LFS`. Super !

{:else}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tf_model.h5 (LFS: 86fce29)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

Nous pouvons voir que tous les fichiers ont `Git` comme gestionnaire, sauf *t5_model.h5* qui a `LFS`. Super !

{/if}

Passons aux √©tapes finales, *committing* et *pushing* vers le d√©p√¥t distant *huggingface.co* :

```bash
git commit -m "First model version"
```

{#if fw === 'pt'}
```bash
[main b08aab1] First model version
 7 files changed, 29027 insertions(+)
  6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 pytorch_model.bin
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{:else}
```bash
[main b08aab1] First model version
 6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tf_model.h5
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{/if}

Le chargement peut prendre un peu de temps, en fonction de la vitesse de votre connexion Internet et de la taille de vos fichiers :

```bash
git push
```

```bash
Uploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.
Enumerating objects: 11, done.
Counting objects: 100% (11/11), done.
Delta compression using up to 12 threads
Compressing objects: 100% (9/9), done.
Writing objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.
Total 9 (delta 1), reused 0 (delta 0), pack-reused 0
To https://huggingface.co/lysandre/dummy
   891b41d..b08aab1  main -> main
```

{#if fw === 'pt'}
Si nous jetons un coup d'≈ìil au d√©p√¥t du mod√®le, lorsque cette op√©ration est termin√©e, nous pouvons voir tous les fichiers r√©cemment ajout√©s :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</div>

L'interface utilisateur vous permet d'explorer les fichiers du mod√®le et les *commits* et de voir la diff√©rence introduite par chaque *commit* :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffs.gif" alt="The diff introduced by the recent commit." width="80%"/>
</div>
{:else}
Si nous jetons un coup d'≈ìil au d√©p√¥t du mod√®le, lorsque cette op√©ration est termin√©e, nous pouvons voir tous les fichiers r√©cemment ajout√©s :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model_tf.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</div>

L'interface utilisateur vous permet d'explorer les fichiers du mod√®le et les *commits* et de voir la diff√©rence introduite par chaque *commit* :

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffstf.gif" alt="The diff introduced by the recent commit." width="80%"/>
</div>
{/if}
