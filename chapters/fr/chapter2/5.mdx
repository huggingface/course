<FrameworkSwitchCourse {fw} />

# Manipulation de plusieurs s√©quences

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

Dans la section pr√©c√©dente, nous avons explor√© le cas d'utilisation le plus simple : faire une inf√©rence sur une seule s√©quence de petite longueur. Cependant, certaines questions √©mergent d√©j√† :

- comment g√©rer de plusieurs s√©quences ?
- comment g√©rer de plusieurs s√©quences *de longueurs diff√©rentes* ?
- les indices du vocabulaire sont-ils les seules entr√©es qui permettent √† un mod√®le de bien fonctionner ?
- existe-t-il une s√©quence trop longue ?

Voyons quels types de probl√®mes ces questions posent et comment nous pouvons les r√©soudre en utilisant l'API ü§ó *Transformers*.

## Les mod√®les attendent un batch d'entr√©es

Dans l'exercice pr√©c√©dent, vous avez vu comment les s√©quences sont traduites en listes de nombres. 
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod√®le :

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d‚ÄôHuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# Cette ligne va √©chouer.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d‚ÄôHuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

Pourquoi cela a √©chou√© ? Nous avons suivi les √©tapes du pipeline de la section 2.

Le probl√®me est que nous avons envoy√© une seule s√©quence au mod√®le, alors que les mod√®les de l‚ÄôAPI ü§ó *Transformers* attendent plusieurs phrases par d√©faut. Ici, nous avons essay√© de faire ce que le *tokenizer* fait en coulisses lorsque nous l'avons appliqu√© √† une `s√©quence`. Cependant si vous regardez de pr√®s, vous verrez qu'il n'a pas seulement converti la liste des identifiants d'entr√©e en un tenseur mais aussi ajout√© une dimension par-dessus :


{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

Essayons √† nouveau en ajoutant une nouvelle dimension :

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d‚ÄôHuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d‚ÄôHuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

Nous affichons les identifiants d'entr√©e ainsi que les logits r√©sultants. Voici la sortie :

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

Le ¬´ *batching* ¬ª est l'acte d'envoyer plusieurs phrases √† travers le mod√®le, toutes en m√™me temps. Si vous n'avez qu'une seule phrase, vous pouvez simplement construire un batch avec une seule s√©quence : 

```
batched_ids = [ids, ids]
```

Il s'agit d'un batch de deux s√©quences identiques !

<Tip>

‚úèÔ∏è **Essayez !** Convertissez cette liste `batched_ids` en un tenseur et passez-la dans votre mod√®le. V√©rifiez que vous obtenez les m√™mes logits que pr√©c√©demment (mais deux fois) !

</Tip>

Utiliser des *batchs* permet au mod√®le de fonctionner lorsque vous lui donnez plusieurs s√©quences. Utiliser plusieurs s√©quences est aussi simple que de construire un batch avec une seule s√©quence. Il y a cependant un deuxi√®me probl√®me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent √™tre de longueurs diff√©rentes. Si vous avez d√©j√† travaill√© avec des tenseurs, vous savez qu'ils doivent √™tre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d'entr√©e en un tenseur. Pour contourner ce probl√®me, nous avons l'habitude de *rembourrer*/*remplir* (le *padding* en anglais) les entr√©es.

## <i>Padding</i> des entr√©es

La liste de listes suivante ne peut pas √™tre convertie en un tenseur :


```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Afin de contourner ce probl√®me, nous utilisons le *padding* pour que nos tenseurs aient une forme rectangulaire. Le *padding* permet de s'assurer que toutes nos phrases ont la m√™me longueur en ajoutant un mot sp√©cial appel√© *padding token* aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le *padding* fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r√©sultant ressemble √† ceci :

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

L'identifiant du jeton de *padding* peut √™tre trouv√© dans `tokenizer.pad_token_id`. Utilisons-le et envoyons nos deux phrases √† travers le mod√®le premi√®rement individuellement puis en √©tant mises dans un m√™me batch :

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

Il y a quelque chose qui ne va pas avec les logits de notre pr√©diction avec les s√©quences mises dans un m√™me batch. La deuxi√®me ligne devrait √™tre la m√™me que les logits pour la deuxi√®me phrase, mais nous avons des valeurs compl√®tement diff√©rentes !

C'est parce que dans un *transformer* les couches d‚Äôattention *contextualisent* chaque *token*. Celles-ci prennent en compte les *tokens* de *padding* puisqu'elles analysent tous les *tokens* d'une s√©quence. Pour obtenir le m√™me r√©sultat lorsque l'on passe dans notre mod√®le des phrases individuelles de diff√©rentes longueurs ou un batch compos√© de m√™mes phrases avec *padding*, nous devons dire √† ces couches d'attention d'ignorer les jetons de *padding*. Ceci est fait en utilisant un masque d'attention.


## Masques d'attention

Les masques d'attention sont des tenseurs ayant exactement la m√™me forme que le tenseur d'identifiants d'entr√©e, remplis de 0 et de 1 :
-	1 indique que les *tokens* correspondants doivent √™tre analys√©s
-	0 indique que les *tokens* correspondants ne doivent pas √™tre analys√©s (c'est-√†-dire qu'ils doivent √™tre ignor√©s par les couches d'attention du mod√®le).

Compl√©tons l'exemple pr√©c√©dent avec un masque d'attention :


{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

Nous obtenons maintenant les m√™mes logits pour la deuxi√®me phrase du batch.

Remarquez comment la derni√®re valeur de la deuxi√®me s√©quence est un identifiant de *padding* valant 0 dans le masque d'attention.


<Tip>

‚úèÔ∏è **Essayez !** Appliquez la tokenisation manuellement sur les deux phrases utilis√©es dans la section 2 (¬´ <i>I've been waiting for a HuggingFace course my whole life.</i> ¬ª et ¬´ <i>I hate this so much!</i> ¬ª). Passez-les dans le mod√®le et v√©rifiez que vous obtenez les m√™mes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de *padding* et cr√©ez le masque d'attention appropri√©. V√©rifiez que vous obtenez les m√™mes r√©sultats qu‚Äôen passant par le mod√®le !

</Tip>


## S√©quences plus longues

Les *transformers* acceptent en entr√©e que des s√©quences d‚Äôune longueur limit√©e. La plupart des mod√®les traitent des s√©quences allant jusqu'√† 512 ou 1024 *tokens* et plantent lorsqu'on leur demande de traiter des s√©quences plus longues. Il existe deux solutions √† ce probl√®me :

- utiliser un mod√®le avec une longueur de s√©quence support√©e plus longue,
- tronquer les s√©quences.

Certains mod√®les sont sp√©cialis√©s dans le traitement de tr√®s longues s√©quences comme par exemple le [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) ou le [LED](https://huggingface.co/transformers/model_doc/led.html). Si vous travaillez sur une t√¢che qui n√©cessite de tr√®s longues s√©quences, nous vous recommandons de jeter un coup d'≈ìil √† ces mod√®les.

Sinon, nous vous recommandons de tronquer vos s√©quences en sp√©cifiant le param√®tre `max_sequence_length` :


```py
sequence = sequence[:max_sequence_length]
```
