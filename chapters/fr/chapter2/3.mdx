<FrameworkSwitchCourse {fw} />

# Les mod√®les

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
Dans cette section, nous allons examiner de plus pr√®s la cr√©ation et l'utilisation d'un mod√®le. Nous utiliserons la classe `AutoModel` qui est pratique lorsque vous voulez instancier n'importe quel mod√®le √† partir d'un checkpoint.

La classe `AutoModel` et toutes les classes apparent√©es sont en fait de simples *wrappers* sur la grande vari√©t√© de mod√®les disponibles dans la biblioth√®que. C'est une enveloppe intelligente car elle peut automatiquement deviner l'architecture appropri√©e pour votre *checkpoint* et ensuite instancier un mod√®le avec cette architecture.

{:else}
Dans cette section, nous allons examiner de plus pr√®s la cr√©ation et l'utilisation d'un mod√®le. Nous utiliserons la classe `TFAutoModel` qui est pratique lorsque vous voulez instancier n'importe quel mod√®le √† partir d'un checkpoint.

La classe `TFAutoModel` et toutes les classes apparent√©es sont en fait de simples *wrappers* sur la grande vari√©t√© de mod√®les disponibles dans la biblioth√®que. C'est une enveloppe intelligente car elle peut automatiquement deviner l'architecture appropri√©e pour votre *checkpoint* et ensuite instancier un mod√®le avec cette architecture.

{/if}

Cependant, si vous connaissez le type de mod√®le que vous voulez utiliser, vous pouvez utiliser directement la classe qui d√©finit son architecture. Voyons comment cela fonctionne avec un mod√®le BERT.

## Cr√©ation d‚Äôun <i>transformer</i>

La premi√®re chose que nous devons faire pour initialiser un mod√®le BERT est de charger un objet configuration :

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Construire la configuration
config = BertConfig()

# Construire le mod√®le √† partir de la configuration
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Construire la configuration
config = BertConfig()

# Construire le mod√®le √† partir de la configuration
model = TFBertModel(config)
```
{/if}

La configuration contient de nombreux attributs qui sont utilis√©s pour construire le mod√®le :
```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

Bien que vous n'ayez pas encore vu ce que font tous ces attributs, vous devriez en reconna√Ætre certains : l'attribut `hidden_size` d√©finit la taille du vecteur `hidden_states`, et `num_hidden_layers` d√©finit le nombre de couches que le *transformer* poss√®de.

### Diff√©rentes m√©thodes de chargement

Un mod√®le cr√©√© √† partir de la configuration par d√©faut est initialis√© avec des valeurs al√©atoires :


{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Le mod√®le est initialis√© de fa√ßon al√©atoire !
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Le mod√®le est initialis√© de fa√ßon al√©atoire !
```
{/if}

Le mod√®le peut √™tre utilis√© tel quel mais il produira du charabia. En effet, il doit d'abord √™tre entra√Æn√©. Nous pourrions entra√Æner le mod√®le √† partir de z√©ro sur la t√¢che qui nous int√©resse mais comme vous l'avez vu dans le [chapitre 1](/course/fr/chapter1) cela n√©cessiterait beaucoup de temps et de donn√©es. De plus cela aurait un impact environnemental non n√©gligeable. Pour √©viter les efforts inutiles et redondants, il est imp√©ratif de pouvoir partager et r√©utiliser les mod√®les qui ont d√©j√† √©t√© entra√Æn√©s.

Charger un *transformer* qui a d√©j√† √©t√© entra√Æn√© est simple : nous pouvons le faire en utilisant la m√©thode `from_pretrained()` :


{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

Comme vu pr√©c√©demment, nous pouvons remplacer `BertModel` par la classe √©quivalente `AutoModel`. A partir de maintenant nous ferons cela car cela produit un code agnostique *checkpoint*, c‚Äôest-√†-dire que si votre code fonctionne pour un *checkpoint* donn√©, il devrait fonctionner sans probl√®me avec un autre. Cela s'applique m√™me si l'architecture est diff√©rente du moment que le *checkpoint* a √©t√© entra√Æn√© pour une t√¢che similaire (par exemple, une t√¢che d'analyse de sentiments).

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

Comme vu pr√©c√©demment, nous pouvons remplacer `TFBertModel` par la classe √©quivalente `TFAutoModel`. A partir de maintenant nous ferons cela car cela produit un code agnostique *checkpoint*, c‚Äôest-√†-dire que si votre code fonctionne pour un *checkpoint* donn√©, il devrait fonctionner sans probl√®me avec un autre. Cela s'applique m√™me si l'architecture est diff√©rente du moment que le *checkpoint* a √©t√© entra√Æn√© pour une t√¢che similaire (par exemple, une t√¢che d'analyse de sentiments).

{/if}

Dans l'exemple de code ci-dessus, nous n'avons pas utilis√© `BertConfig` et avons √† la place charg√© un mod√®le pr√©-entra√Æn√© via l'identifiant `bert-base-cased`. Il s'agit d'un *checkpoint* qui a √©t√© entra√Æn√© par les auteurs de BERT eux-m√™mes. Vous pouvez trouver davantage de d√©tails √† son sujet dans la [fiche du mod√®le](https://huggingface.co/bert-base-cased).

Ce mod√®le est maintenant initialis√© avec tous les poids du *checkpoint*. Il peut √™tre utilis√© directement pour l'inf√©rence sur les t√¢ches sur lesquelles il a √©t√© entra√Æn√©. Il peut √©galement √™tre *finetun√©* sur une nouvelle t√¢che. En entra√Ænant avec des poids pr√©-entra√Æn√©s plut√¥t qu'√† partir de z√©ro, nous pouvons rapidement obtenir de bons r√©sultats.

Les poids ont √©t√© t√©l√©charg√©s et mis en cache (afin que les futurs appels √† la m√©thode `from_pretrained()` ne les ret√©l√©chargent pas) dans le dossier cache, qui est par d√©faut *~/.cache/huggingface/transformers*. Vous pouvez personnaliser votre dossier de cache en d√©finissant la variable d'environnement `HF_HOME`.

L'identifiant utilis√© pour charger le mod√®le peut √™tre l'identifiant de n'importe quel mod√®le sur le *Model Hub* du moment qu'il est compatible avec l'architecture BERT. La liste compl√®te des *checkpoints* de BERT disponibles peut √™tre trouv√©e [ici](https://huggingface.co/models?filter=bert).

### M√©thodes de sauvegarde

Sauvegarder un mod√®le est aussi facile que d'en charger un. Nous utilisons la m√©thode `save_pretrained()`, qui est analogue √† la m√©thode `from_pretrained()` :


```py
model.save_pretrained("directory_on_my_computer")
```

Cela enregistre deux fichiers sur votre disque :

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

Si vous jetez un coup d'≈ìil au fichier *config.json*, vous reconna√Ætrez les attributs n√©cessaires pour construire l'architecture du mod√®le. Ce fichier contient √©galement certaines m√©tadonn√©es, comme l'origine du *checkpoint* et la version de la biblioth√®que ü§ó *Transformers* que vous utilisiez lors du dernier enregistrement du point *checkpoint*.

{#if fw === 'pt'}
Le fichier *pytorch_model.bin* est connu comme le *dictionnaire d'√©tat*. Il contient tous les poids de votre mod√®le. Les deux fichiers vont de pair : la configuration est n√©cessaire pour conna√Ætre l'architecture de votre mod√®le, tandis que les poids du mod√®le sont les param√®tres de votre mod√®le.

{:else}
Le fichier *tf_model.h5* est connu comme le *dictionnaire d'√©tat*. Il contient tous les poids de votre mod√®le. Les deux fichiers vont de pair : la configuration est n√©cessaire pour conna√Ætre l'architecture de votre mod√®le, tandis que les poids du mod√®le sont les param√®tres de votre mod√®le.

{/if}

### Utilisation d'un <i>transformer</i> pour l'inf√©rence

Maintenant que vous savez comment charger et sauvegarder un mod√®le, essayons de l'utiliser pour faire quelques pr√©dictions. Les *transformers* ne peuvent traiter que des nombres. Des nombres que le *tokenizer* g√©n√®re. Mais avant de parler des *tokenizers*, explorons les entr√©es que le mod√®le accepte.

Les *tokenizers* se chargent de passer les entr√©es vers les tenseurs du *framework* appropri√©. Pour vous aider √† comprendre ce qui se passe, jetons un coup d'≈ìil rapide √† ce qui doit √™tre fait avant d'envoyer les entr√©es au mod√®le.

Disons que nous avons les s√©quences suivantes :


```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

Le *tokenizer* les convertit en indices de vocabulaire qui sont g√©n√©ralement appel√©s *input IDs*. Chaque s√©quence est maintenant une liste de nombres ! La sortie r√©sultante est :

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

Il s'agit d'une liste de s√©quences encod√©es : une liste de listes. Les tenseurs n'acceptent que des formes rectangulaires (pensez aux matrices). Ce ¬´ tableau ¬ª est d√©j√† de forme rectangulaire, donc le convertir en tenseur est facile :

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### Utilisation des tenseurs comme entr√©es du mod√®le

L'utilisation des tenseurs avec le mod√®le est extr√™mement simple, il suffit d'appeler le mod√®le avec les entr√©es :


```py
output = model(model_inputs)
```

Bien que le mod√®le accepte un grand nombre d'arguments diff√©rents, seuls les identifiants d'entr√©e sont n√©cessaires. Nous expliquerons plus tard ce que font les autres arguments et quand ils sont n√©cessaires. Avant cela, regardons de plus pr√®s les *tokenizers*, cet outil qui construit les entr√©es qu'un *transformer* peut comprendre.
