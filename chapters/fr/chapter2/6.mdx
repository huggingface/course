<FrameworkSwitchCourse {fw} />

# Tout assembler

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"},
]} />

{/if}

Dans les derni√®res sections, nous avons fait de notre mieux pour effectuer la plupart du travail manuellement. Nous avons explor√© le fonctionnement des *tokenizers* et examin√© la tokenisation, la conversion en identifiants d'entr√©e, le *padding*, la troncature et les masques d'attention.

Cependant, comme nous l'avons vu dans la section 2, l'API ü§ó *Transformers* peut g√©rer tout cela pour nous via une fonction dans laquelle nous allons nous plonger ici. Lorsque vous appelez votre `tokenizer` directement sur la phrase, vous r√©cup√©rez des entr√©es qui sont pr√™tes √† √™tre pass√©es dans votre mod√®le :


```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d‚ÄôHuggingFace toute ma vie.

model_inputs = tokenizer(sequence)
```

Ici, la variable `model_inputs` contient tout ce qui est n√©cessaire au bon fonctionnement d'un mod√®le. Pour DistilBERT, cela inclut les identifiants d'entr√©e ainsi que le masque d'attention. D'autres mod√®les qui acceptent des entr√©es suppl√©mentaires sont √©galement fournis par l'objet `tokenizer`.

Comme nous allons le voir dans les quelques exemples ci-dessous, cette m√©thode est tr√®s puissante. Premi√®rement, elle peut tokeniser une seule s√©quence :


```py
sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d‚ÄôHuggingFace toute ma vie.


model_inputs = tokenizer(sequence)
```

Elle g√®re √©galement plusieurs s√©quences √† la fois, sans modification de l'API :


```py
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!",
]  # ¬´ J'ai attendu un cours de HuggingFace toute ma vie. ¬ª, ¬´ Moi aussi ! ¬ª

model_inputs = tokenizer(sequences)
```

Il est possible de faire du *padding* selon plusieurs objectifs :

```py
# Remplit les s√©quences jusqu'√† la longueur maximale de la s√©quence
model_inputs = tokenizer(sequences, padding="longest")

# Remplit les s√©quences jusqu'√† la longueur maximale du mod√®le (512 pour BERT ou DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Remplit les s√©quences jusqu'√† la longueur maximale sp√©cifi√©e
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

La fonction peut √©galement tronquer les s√©quences :

```py
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!",
]  # ¬´ J'ai attendu un cours de HuggingFace toute ma vie. ¬ª, ¬´ Moi aussi ! ¬ª

# Tronque les s√©quences qui sont plus longues que la longueur maximale du mod√®le
# (512 pour BERT ou DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Tronque les s√©quences qui sont plus longues que la longueur maximale sp√©cifi√©e
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

L'objet `tokenizer` peut g√©rer la conversion en des tenseurs de *frameworks* sp√©cifiques. Ils peuvent ensuite √™tre directement envoy√©s au mod√®le. Par exemple, dans le code suivant, nous demandons au *tokenizer* de retourner des tenseurs PyTorch lorsque l‚Äôon sp√©cifie `"pt"`, de retourner des tenseurs TensorFlow lorsque l‚Äôon sp√©cifie `"tf"` et des tableaux NumPy lorsque l‚Äôon indique `"np"` :

```py
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!",
]  # ¬´ J'ai attendu un cours de HuggingFace toute ma vie. ¬ª, ¬´ Moi aussi ! ¬ª

# Retourne des tenseurs PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Retourne des tenseurs TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Retourne des tableaux NumPy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Jetons sp√©ciaux

Si nous jetons un coup d'≈ìil aux identifiants d'entr√©e renvoy√©s par le *tokenizer*, nous verrons qu'ils sont un peu diff√©rents de ceux que nous avions pr√©c√©demment :


```py
sequence = "I've been waiting for a HuggingFace course my whole life."
# ¬´ J'ai attendu un cours de HuggingFace toute ma vie. ¬ª

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

Un identifiant symbolique a √©t√© ajout√© au d√©but ainsi qu‚Äôun autre √† la fin. D√©codons les deux s√©quences d'identifiants ci-dessus pour voir de quoi il s'agit :

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

Le *tokenizer* a ajout√© le mot sp√©cial `[CLS]` au d√©but et le mot sp√©cial `[SEP]` √† la fin. C'est parce que le mod√®le a √©t√© pr√©-entra√Æn√© avec ces mots, donc pour obtenir les m√™mes r√©sultats pour l'inf√©rence, nous devons √©galement les ajouter. Notez que certains mod√®les n'ajoutent pas de mots sp√©ciaux, ou en ajoutent des diff√©rents. Les mod√®les peuvent aussi ajouter ces mots sp√©ciaux seulement au d√©but, ou seulement √† la fin. Dans tous les cas, le *tokenizer* sait lesquels sont attendus et s'en occupe pour vous.

## Conclusion : du <i>tokenizer</i> au mod√®le

Maintenant que nous avons vu toutes les √©tapes individuelles que l'objet `tokenizer` utilise lorsqu'il est appliqu√© sur des textes, voyons une derni√®re fois comment il peut g√©rer plusieurs s√©quences (*padding*), de tr√®s longues s√©quences (*troncation*) et plusieurs types de tenseurs avec son API principale :


{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!",
]  # ¬´ J'ai attendu un cours de HuggingFace toute ma vie. ¬ª, ¬´ Moi aussi ! ¬ª


tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!",
]  # ¬´ J'ai attendu un cours de HuggingFace toute ma vie. ¬ª, ¬´ Moi aussi ! ¬ª

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
