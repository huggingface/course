# Introduction

Comme vous l'avez vu dans le [chapitre 1](/course/fr/chapter1), les *transformers* sont g√©n√©ralement tr√®s grands. Pouvant aller de plusieurs millions √† des dizaines de milliards de param√®tres, l'entra√Ænement et le d√©ploiement de ces mod√®les est une entreprise compliqu√©e. De plus, avec de nouveaux mod√®les publi√©s presque quotidiennement et ayant chacun sa propre impl√©mentation, les essayer tous n'est pas une t√¢che facile.

La biblioth√®que ü§ó *Transformers* a √©t√© cr√©√©e pour r√©soudre ce probl√®me. Son objectif est de fournir une API unique √† travers laquelle tout mod√®le de *transformers* peut √™tre charg√©, entra√Æn√© et sauvegard√©. Les principales caract√©ristiques de la biblioth√®que sont :

- **La facilit√© d'utilisation** : en seulement deux lignes de code il est possible de t√©l√©charger, charger et utiliser un mod√®le de NLP √† l'√©tat de l'art pour faire de l'inf√©rence,
- **La flexibilit√©** : au fond, tous les mod√®les sont de simples classes PyTorch `nn.Module` ou TensorFlow `tf.keras.Model` et peuvent √™tre manipul√©s comme n'importe quel autre mod√®le dans leurs *frameworks* d'apprentissage automatique respectifs,
- **La simplicit√©** : pratiquement aucune abstraction n'est faite dans la biblioth√®que. Avoir tout dans un fichier est un concept central : la passe avant d'un mod√®le est enti√®rement d√©finie dans un seul fichier afin que le code lui-m√™me soit compr√©hensible et piratable.

Cette derni√®re caract√©ristique rend ü§ó *Transformers* tr√®s diff√©rent des autres biblioth√®ques d'apprentissage automatique. 
Les mod√®les ne sont pas construits sur des modules partag√©s entre plusieurs fichiers. Au lieu de cela, chaque mod√®le poss√®de ses propres couches. 
En plus de rendre les mod√®les plus accessibles et compr√©hensibles, cela vous permet d'exp√©rimenter des choses facilement sur un mod√®le sans affecter les autres.

Ce chapitre commence par un exemple de bout en bout o√π nous utilisons un mod√®le et un *tokenizer* ensemble pour reproduire la fonction `pipeline()` introduite dans le [chapitre 1](/course/fr/chapter1). 
Ensuite, nous aborderons l'API *model* : nous nous plongerons dans les classes de mod√®le et de configuration, nous verrons comment charger un mod√®le et enfin comment il traite les entr√©es num√©riques pour produire des pr√©dictions. 

Nous examinerons ensuite l'API *tokenizer* qui est l'autre composant principal de la fonction `pipeline()`. 
Les *tokenizers* s'occupent de la premi√®re et de la derni√®re √©tape du traitement en g√©rant la conversion du texte en entr√©es num√©riques pour le r√©seau neuronal et la reconversion en texte lorsqu'elle est n√©cessaire. 
Enfin, nous montrerons comment g√©rer l'envoi de plusieurs phrases √† travers un mod√®le dans un batch pr√©par√© et nous conclurons le tout en examinant de plus pr√®s la fonction `tokenizer()`.

<Tip>
  ‚ö†Ô∏è Afin de b√©n√©ficier de toutes les fonctionnalit√©s disponibles avec le <i>Hub</i> et la biblioth√®que ü§ó <i>Transformers</i>, nous vous recommandons <a href="https://huggingface.co/join">de cr√©er un compte</a>.
</Tip>
