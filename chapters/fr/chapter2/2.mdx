<FrameworkSwitchCourse {fw} />

# Derri√®re le pipeline

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter2/section2_pt.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
    {label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/fr/chapter2/section2_tf.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
Il s'agit de la premi√®re section dont le contenu est l√©g√®rement diff√©rent selon que vous utilisez PyTorch ou TensorFlow. Cliquez sur le bouton situ√© au-dessus du titre pour s√©lectionner la plateforme que vous pr√©f√©rez !
</Tip>

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

Commen√ßons par un exemple complet en regardant ce qui s'est pass√© en coulisses lorsque nous avons ex√©cut√© le code suivant dans le [chapitre 1](/course/chapter1) :

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        # J'ai attendu un cours de HuggingFace toute ma vie.
        "I hate this so much!",  # Je d√©teste tellement √ßa !
    ]
)
```

la sortie :

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Comme nous l'avons vu dans le [chapitre 1](/course/fr/chapter1), ce pipeline regroupe trois √©tapes : le pr√©traitement, le passage des entr√©es dans le mod√®le et le post-traitement.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."/>
</div>

Passons rapidement en revue chacun de ces √©l√©ments.

## Pr√©traitement avec un <i>tokenizer</i>

Comme d'autres r√©seaux de neurones, les *transformers* ne peuvent pas traiter directement le texte brut, donc la premi√®re √©tape de notre pipeline est de convertir les entr√©es textuelles en nombres afin que le mod√®le puisse les comprendre. Pour ce faire, nous utilisons un *tokenizer*, qui sera responsable de :
- diviser l'entr√©e en mots, sous-mots, ou symboles (comme la ponctuation) qui sont appel√©s *tokens*,
- associer chaque *token* √† un nombre entier,
- ajouter des entr√©es suppl√©mentaires qui peuvent √™tre utiles au mod√®le.

Tout ce pr√©traitement doit √™tre effectu√© exactement de la m√™me mani√®re que celui appliqu√© lors du pr√©-entra√Ænement du mod√®le. Nous devons donc d'abord t√©l√©charger ces informations depuis le [*Hub*](https://huggingface.co/models). Pour ce faire, nous utilisons la classe `AutoTokenizer` et sa m√©thode `from_pretrained()`. En utilisant le nom du *checkpoint* de notre mod√®le, elle va automatiquement r√©cup√©rer les donn√©es associ√©es au *tokenizer* du mod√®le et les mettre en cache (afin qu'elles ne soient t√©l√©charg√©es que la premi√®re fois que vous ex√©cutez le code ci-dessous).

Puisque le *checkpoint* par d√©faut du pipeline `sentiment-analysis` (analyse de sentiment) est `distilbert-base-uncased-finetuned-sst-2-english` (vous pouvez voir la carte de ce mod√®le [ici](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), nous ex√©cutons ce qui suit :

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

Une fois que nous avons le *tokenizer* nous pouvons lui passer directement nos phrases et obtenir un dictionnaire pr√™t √† √™tre donn√© √† notre mod√®le ! La seule chose qui reste √† faire est de convertir en tenseurs la liste des identifiants d'entr√©e.

Vous pouvez utiliser ü§ó *Transformers* sans avoir √† vous soucier du *framework* utilis√© comme *backend*. Il peut s'agir de PyTorch, de TensorFlow ou de Flax pour certains mod√®les. Cependant, les *transformers* n'acceptent que les *tenseurs* en entr√©e. Si c'est la premi√®re fois que vous entendez parler de tenseurs, vous pouvez les consid√©rer comme des tableaux NumPy. Un tableau NumPy peut √™tre un scalaire (0D), un vecteur (1D), une matrice (2D), ou avoir davantage de dimensions. Les tenseurs des autres *frameworks* d'apprentissage machine se comportent de mani√®re similaire et sont g√©n√©ralement aussi simples √† instancier que les tableaux NumPy.

Pour sp√©cifier le type de tenseurs que nous voulons r√©cup√©rer (PyTorch, TensorFlow, ou simplement NumPy), nous utilisons l'argument `return_tensors` :

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "I hate this so much!",  # Je d√©teste tellement √ßa !
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "I hate this so much!",  # Je d√©teste tellement √ßa !
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

Ne vous pr√©occupez pas encore du remplissage (*padding*) et de la troncature, nous les expliquerons plus tard. Les principales choses √† retenir ici sont que vous pouvez passer une phrase ou une liste de phrases, ainsi que sp√©cifier le type de tenseurs que vous voulez r√©cup√©rer (si aucun type n'est pass√©, par d√©faut vous obtiendrez une liste de listes comme r√©sultat).

{#if fw === 'pt'}

Voici √† quoi ressemblent les r√©sultats sous forme de tenseurs PyTorch :

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

Voici √† quoi ressemblent les r√©sultats sous forme de tenseurs TensorFlow :

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

La sortie elle-m√™me est un dictionnaire contenant deux cl√©s : `input_ids` et `attention_mask`. `input_ids` contient deux lignes d'entiers (une pour chaque phrase) qui sont les identifiants uniques des *tokens* dans chaque phrase. Nous expliquerons ce qu'est l'`attention_mask` plus tard dans ce chapitre. 

## Passage au mod√®le

{#if fw === 'pt'}
Nous pouvons t√©l√©charger notre mod√®le pr√©-entra√Æn√© de la m√™me mani√®re que nous l'avons fait avec notre *tokenizer*. ü§ó *Transformers* fournit une classe `AutoModel` qui poss√®de √©galement une m√©thode `from_pretrained()` :

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
Nous pouvons t√©l√©charger notre mod√®le pr√©tra√Æn√© de la m√™me mani√®re que nous l'avons fait avec notre *tokenizer*. ü§ó *Transformers* fournit une classe `TFAutoModel` qui poss√®de √©galement une m√©thode `from_pretrained()` :

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

Dans cet extrait de code, nous avons t√©l√©charg√© le m√™me *checkpoint* que nous avons utilis√© dans notre pipeline auparavant (il devrait en fait avoir d√©j√† √©t√© mis en cache) et instanci√© un mod√®le avec lui.

Cette architecture ne contient que le module de *transformer* de base : √©tant donn√© certaines entr√©es, il produit ce que nous appellerons des *√©tats cach√©s*, √©galement connus sous le nom de *caract√©ristiques*. 
Pour chaque entr√©e du mod√®le, nous r√©cup√©rons un vecteur en grande dimension repr√©sentant la **compr√©hension contextuelle de cette entr√©e par le *transformer***.

Si cela ne fait pas sens, ne vous inqui√©tez pas. Nous expliquons tout plus tard.

Bien que ces √©tats cach√©s puissent √™tre utiles en eux-m√™mes, ils sont g√©n√©ralement les entr√©es d'une autre partie du mod√®le, connue sous le nom de *t√™te*. Dans le [chapitre 1](/course/fr/chapter1), les diff√©rentes t√¢ches auraient pu √™tre r√©alis√©es avec la m√™me architecture mais en ayant chacune d'elles une t√™te diff√©rente.

### Un vecteur de grande dimension ?

Le vecteur produit en sortie par le *transformer* est g√©n√©ralement de grande dimension. Il a g√©n√©ralement trois dimensions :

- **la taille du lot** : le nombre de s√©quences trait√©es √† la fois (2 dans notre exemple),
- **la longueur de la s√©quence** : la longueur de la repr√©sentation num√©rique de la s√©quence (16 dans notre exemple),
- **la taille cach√©e** : la dimension du vecteur de chaque entr√©e du mod√®le.

On dit qu'il est de ¬´ grande dimension ¬ª en raison de la derni√®re valeur. La taille cach√©e peut √™tre tr√®s grande (g√©n√©ralement 768 pour les petits mod√®les et pour les grands mod√®les cela peut atteindre 3072 voire plus).

Nous pouvons le constater si nous alimentons notre mod√®le avec les entr√©es que nous avons pr√©trait√©es :


{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

Notez que les sorties des mod√®les de la biblioth√®que ü§ó *Transformers* se comportent comme des `namedtuples` ou des dictionnaires. Vous pouvez acc√©der aux √©l√©ments par attributs (comme nous l'avons fait), par cl√© (`outputs["last_hidden_state"]`), ou m√™me par l‚Äôindex si vous savez exactement o√π se trouve la chose que vous cherchez (`outputs[0]`).

### Les t√™tes des mod√®les : donner du sens aux chiffres
Les t√™tes des mod√®les prennent en entr√©e le vecteur de grande dimension des √©tats cach√©s et le projettent sur une autre dimension. Elles sont g√©n√©ralement compos√©es d'une ou de quelques couches lin√©aires :
<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

La sortie du *transformer* est envoy√©e directement √† la t√™te du mod√®le pour √™tre trait√©e.
Dans ce diagramme, le mod√®le est repr√©sent√© par sa couche d‚Äôench√¢ssement et les couches suivantes. La couche d‚Äôench√¢ssement convertit chaque identifiant d'entr√©e dans l'entr√©e tokenis√©e en un vecteur qui repr√©sente le *token* associ√©. Les couches suivantes manipulent ces vecteurs en utilisant le m√©canisme d'attention pour produire la repr√©sentation finale des phrases.
Il existe de nombreuses architectures diff√©rentes disponibles dans la biblioth√®que ü§ó *Transformers*, chacune √©tant con√ßue autour de la prise en charge d'une t√¢che sp√©cifique. En voici une liste non exhaustive :
- `*Model` (r√©cup√©rer les √©tats cach√©s)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- et autres ü§ó

{#if fw === 'pt'}
Pour notre exemple, nous avons besoin d'un mod√®le avec une t√™te de classification de s√©quence (pour pouvoir classer les phrases comme positives ou n√©gatives). Donc, nous n'utilisons pas r√©ellement la classe `AutoModel` mais plut√¥t `AutoModelForSequenceClassification` :
```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
Pour notre exemple, nous avons besoin d'un mod√®le avec une t√™te de classification de s√©quence (pour pouvoir classer les phrases comme positives ou n√©gatives). Donc, nous n'utilisons pas r√©ellement la classe ` TFAutoModel` mais plut√¥t ` TFAutoModelForSequenceClassification` :
```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

Maintenant, si nous examinons la forme de nos entr√©es, la dimensionnalit√© est beaucoup plus faible. La t√™te du mod√®le prend en entr√©e les vecteurs de grande dimension que nous avons vus pr√©c√©demment et elle produit des vecteurs contenant deux valeurs (une par √©tiquette) :
```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

Comme nous n'avons que deux phrases et deux √©tiquettes, le r√©sultat que nous obtenons est de forme 2 x 2

## Post-traitement de la sortie

Les valeurs que nous obtenons en sortie de notre mod√®le n'ont pas n√©cessairement de sens en elles-m√™mes. Jetons-y un coup d‚Äô≈ìil  :

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

Notre mod√®le a pr√©dit `[-1.5607, 1.6123]` pour la premi√®re phrase et `[ 4.1692, -3.3464]` pour la seconde. Ce ne sont pas des probabilit√©s mais des *logits*, les scores bruts, non normalis√©s, produits par la derni√®re couche du mod√®le. Pour √™tre convertis en probabilit√©s, ils doivent passer par une couche [SoftMax](https://fr.wikipedia.org/wiki/Fonction_softmax) (tous les mod√®les de la biblioth√®que ü§ó *Transformers* sortent les logits car la fonction de perte de l'entra√Ænement fusionne g√©n√©ralement la derni√®re fonction d'activation, comme la SoftMax, avec la fonction de perte r√©elle, comme l'entropie crois√©e) :

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

Maintenant nous pouvons voir que le mod√®le a pr√©dit `[0.0402, 0.9598]` pour la premi√®re phrase et `[0.9995, 0.0005]` pour la seconde. Ce sont des scores de probabilit√© reconnaissables.

Pour obtenir les √©tiquettes correspondant √† chaque position, nous pouvons inspecter l'attribut `id2label` de la configuration du mod√®le (plus de d√©tails dans la section suivante) :

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

Nous pouvons maintenant conclure que le mod√®le a pr√©dit ce qui suit :
 
- premi√®re phrase : NEGATIVE: 0.0402, POSITIVE: 0.9598
- deuxi√®me phrase : NEGATIVE: 0.9995, POSITIVE: 0.0005

Nous avons reproduit avec succ√®s les trois √©tapes du pipeline : pr√©traitement avec les *tokenizers*, passage des entr√©es dans le mod√®le et post-traitement ! Prenons maintenant le temps de nous plonger plus profond√©ment dans chacune de ces √©tapes.

<Tip>

‚úèÔ∏è **Essayez !** Choisissez deux (ou plus) textes de votre choix (en anglais) et faites-les passer par le pipeline `sentiment-analysis`. Reproduisez ensuite vous-m√™me les √©tapes vues ici et v√©rifiez que vous obtenez les m√™mes r√©sultats !
  
</Tip>
