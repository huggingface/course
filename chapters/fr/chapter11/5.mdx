# Évaluation

Avec un modèle finetuné par SFT ou LoRA, nous devrions l'évaluer sur des jeux d'évaluation standards. En tant qu'ingénieurs en apprentissage automatique, vous devez maintenir une série d'évaluations pertinentes pour votre domaine d'intérêt ciblé. Dans cette page, nous examinons certains des jeux d'évaluation les plus courants et la manière de les utiliser pour évaluer votre modèle. Nous verrons également comment créer des jeux d'évaluation personnalisés pour votre cas d'utilisation spécifique.

## Jeux d'évaluation automatiques

Les jeux d'évaluation (*benchmark* en anglais) automatiques servent d'outils standardisés pour l'évaluation des modèles dans différentes tâches et capacités. S'ils constituent un point de départ utile pour comprendre les performances des modèles, il est important de reconnaître qu'ils ne représentent qu'un élément d'une stratégie d'évaluation globale.

## Comprendre les jeux d'évaluation automatiques

Les jeux d'évaluation automatiques consistent généralement en des jeux de données nettoyés avec des tâches et des mesures d'évaluation prédéfinies. Ils visent à évaluer divers aspects de la capacité des modèles, de la compréhension du langage de base au raisonnement complexe. Leur avantage principal est leur standardisation, permettant une comparaison cohérente entre différents modèles et permettent d'obtenir des résultats reproductibles.

Toutefois, il est essentiel de comprendre que les performances des jeux d'évaluation ne se traduisent pas toujours directement par une efficacité dans le monde réel. Un modèle qui excelle sur des jeux académiques peut encore éprouver des difficultés avec des applications de domaines spécifiques ou des cas d'utilisation pratiques.

## Jeux d'évaluation de connaissances générales

[MMLU](https://huggingface.co/datasets/cais/mmlu) (*Massive Multitask Language Understanding*) permet de tester les connaissances d'un modèle dans 57 matières, des sciences aux sciences humaines. Bien qu'il soit complet, il peut ne pas refléter la profondeur de l'expertise nécessaire pour des domaines spécifiques. TruthfulQA évalue la tendance d'un modèle à reproduire les idées fausses les plus courantes, bien qu'il ne puisse pas saisir toutes les formes de désinformation.

## Jeux d'évaluation du raisonnement

[BBH](https://huggingface.co/datasets/lukaemon/bbh) (*Big Bench Hard*) et [GSM8K](https://huggingface.co/datasets/openai/gsm8k) se concentrent sur des tâches de raisonnement complexes. BBH teste la pensée logique et la planification, tandis que le GSM8K cible spécifiquement la résolution de problèmes mathématiques. Ces jeux d'évaluation permettent d'évaluer les capacités d'analyse, mais peuvent ne pas rendre compte du raisonnement nuancé requis dans les scénarios du monde réel.

## Compréhension du langage

[HELM](https://github.com/stanford-crfm/helm) fournit un cadre d'évaluation holistique. Les jeux d'évaluation comme HELM donnent un aperçu des capacités de traitement du langage sur des aspects tels que le bon sens, la connaissance du monde et le raisonnement. Mais ils peuvent ne pas représenter pleinement la complexité d'une conversation naturelle ou d'une terminologie spécifique à un domaine.

## Jeux d'évaluation spécifiques à un domaine

Examinons quelques jeux d'évaluation axés sur des domaines spécifiques tels que les mathématiques, le codage et le chat.

Le [MATH](https://huggingface.co/papers/2103.03874) est un autre outil d'évaluation important pour le raisonnement mathématique. Il se compose de 12 500 problèmes issus de concours de mathématiques, couvrant l'algèbre, la géométrie, la théorie des nombres, le comptage, les probabilités, etc. Ce qui rend MATH particulièrement difficile, c'est qu'il exige un raisonnement en plusieurs étapes, la compréhension de la notation mathématique formelle et la capacité à générer des solutions étape par étape. Contrairement aux tâches arithmétiques plus simples, les problèmes de MATH exigent souvent des stratégies de résolution de problèmes sophistiquées et l'application de concepts mathématiques.

[HumanEval](https://github.com/openai/human-eval) est un jeu de données d'évaluation axé sur le codage et composé de 164 problèmes de programmation. Il teste la capacité d'un modèle à générer un code Python fonctionnellement correct qui résout les tâches de programmation données. Ce qui rend HumanEval particulièrement précieux, c'est qu'il évalue à la fois les capacités de génération de code et la correction fonctionnelle par le biais de l'exécution de cas de test réels, plutôt que par une simple similitude superficielle avec des solutions de référence. Les problèmes vont de la simple manipulation de chaînes de caractères à des algorithmes et des structures de données plus complexes.

[Alpaca Eval](https://tatsu-lab.github.io/alpaca_eval/) est un cadre d'évaluation automatisé conçu pour évaluer la qualité des modèles de langage à suivre des instructions. Il utilise GPT-4 comme juge pour évaluer les résultats du modèle selon diverses dimensions, notamment l'utilité, l'honnêteté et l'innocuité. Le cadre comprend un jeu de données de 805 d'instructions soigneusement sélectionnées et peut évaluer les réponses par rapport à plusieurs modèles de référence tels que Claude, GPT-4 et d'autres. Ce qui rend Alpaca Eval particulièrement utile, c'est sa capacité à fournir des évaluations cohérentes et évolutives sans nécessiter d'annotateurs humains, tout en capturant des aspects nuancés de la performance du modèle que les mesures traditionnelles pourraient manquer.

## Autres méthodes d'évaluation

De nombreuses organisations ont développé des méthodes d'évaluation alternatives pour répondre aux limites des jeux d'évaluation standards :

### *LLM-as-Judge*

L'utilisation d'un modèle de langage pour évaluer les résultats d'un autre modèle est de plus en plus populaire. Cette approche peut fournir un retour d'information plus nuancé que les mesures traditionnelles, bien qu'elle s'accompagne de ses propres biais et limites.

### Arènes d'évaluation

Les arènes d'évaluation telles que [Chatbot Arena](https://lmarena.ai/) offrent une approche unique pour évaluer les LLM via des retours humains massifs. Sur ces plateformes, les utilisateurs s'engagent dans des « batailles » anonymes entre deux LLM, en posant des questions et en votant pour le modèle qui fournit les meilleures réponses. Cette approche permet de saisir les modes d'utilisation et les préférences du monde réel grâce à des questions variées et stimulantes, et les études montrent une forte concordance entre les votes de la foule et les évaluations d'experts. Bien que puissantes, ces plateformes présentent des limites, notamment un biais potentiel de la base d'utilisateurs, des distributions d'instructions biaisées et un accent mis sur l'utilité plutôt que sur les considérations de sécurité.

### Jeux d'évaluation personnalisés

Les organisations développent souvent des suites d'évaluations internes adaptées à leurs besoins spécifiques et à leurs cas d'utilisation. Il peut s'agir de tests de connaissances spécifiques à un domaine ou de scénarios d'évaluation qui reflètent les conditions réelles de déploiement.

## CÉvaluation personnalisée

Si les jeux d'évaluation standards constituent une base de référence utile, ils ne doivent pas être votre seule méthode d'évaluation. Voici comment développer une approche plus complète :

1. Commencez par des jeux d'évaluation standards pertinents pour établir une base de référence et permettre la comparaison avec d'autres modèles.

2. Identifiez les exigences et les défis spécifiques de votre cas d'utilisation. Quelles tâches votre modèle va-t-il réellement accomplir ? Quels types d'erreurs seraient les plus problématiques ?

3. Développez des jeux de données d'évaluation personnalisés qui reflètent votre cas d'utilisation réel. Cela pourrait inclure :
   - Des requêtes d'utilisateurs réels provenant de votre domaine
   - Des cas de figure courants que vous avez rencontrés
   - Des exemples de scénarios particulièrement difficiles.

4. Envisagez de mettre en œuvre une stratégie d'évaluation à plusieurs niveaux :
   - Mesures automatisées pour un retour d'information rapide
   - Évaluation humaine pour une compréhension nuancée
   - Examen par des experts du domaine pour les applications spécialisées
   - Tests A/B dans des environnements contrôlés

## Implémentation d'évaluations personnalisées

Dans cette section, nous allons implémenter l'évaluation de notre modèle finetuné. Nous pouvons utiliser [`lighteval`](https://github.com/huggingface/lighteval) pour évaluer notre modèle sur des jeux d'évaluation standards, car contient une large gamme de tâches intégrées dans la bibliothèque. Il suffit de définir les tâches que nous voulons évaluer et les paramètres de l'évaluation.  

Les tâches de LightEval sont définies selon un format spécifique :

```
{suite}|{task}|{num_few_shot}|{auto_reduce}
```

| Paramètres | Description |
|-----------|-------------|
| `suite` | La suite de jeux d'évaluation (ex : 'mmlu', 'truthfulqa') |
| `task` | Tâche spécifique au sein de la suite (ex : 'abstract_algebra') |
| `num_few_shot` | Nombre d'exemples à inclure dans l'instruction |
| `auto_reduce` | Possibilité de réduire automatiquement les exemples si l'instruction est trop longue (0 ou 1) |

Exemple : `"mmlu|abstract_algebra|0|0"` évalue le modèle sur la tâche d'algèbre abstraite de MMLU via une inférence avec zéro exemple.

## Exemple de pipeline d'évaluation

Mettons en place un pipeline d'évaluation pour notre modèle finetuné. Nous évaluerons le modèle sur un ensemble de sous-tâches liées au domaine de la médecine. 

Voici un exemple complet d'évaluation sur des jeux d'évaluation automatiques relatifs à un domaine spécifique en utilisant Lighteval avec le *backend* VLLM :

```bash
lighteval accelerate \
    "pretrained=your-model-name" \
    "mmlu|anatomy|0|0" \
    "mmlu|high_school_biology|0|0" \
    "mmlu|high_school_chemistry|0|0" \
    "mmlu|professional_medicine|0|0" \
    --max_samples 40 \
    --batch_size 1 \
    --output_path "./results" \
    --save_generations true
```

Les résultats sont affichés sous forme de tableau :

```
|                  Task                  |Version|Metric|Value |   |Stderr|
|----------------------------------------|------:|------|-----:|---|-----:|
|all                                     |       |acc   |0.3333|±  |0.1169|
|leaderboard:mmlu:_average:5             |       |acc   |0.3400|±  |0.1121|
|leaderboard:mmlu:anatomy:5              |      0|acc   |0.4500|±  |0.1141|
|leaderboard:mmlu:high_school_biology:5  |      0|acc   |0.1500|±  |0.0819|
```

Lighteval inclut également une API python pour des tâches d'évaluation plus détaillées, ce qui est utile pour manipuler les résultats d'une manière plus flexible. Consultez la [documentation de Lighteval](https://huggingface.co/docs/lighteval/using-the-python-api) pour plus d'informations.
<Tip>
✏️ **Essayez !** Évaluez votre modèle finetuné sur une tâche spécifique de lighteval.
</Tip>

# Quiz de fin de chapitre

<CourseFloatingBanner
    chapter={11}
    classNames="absolute z-10 right-0 top-0"
/>

### 1. Quels sont les principaux avantages de l'utilisation de jeux d'évaluation automatiques pour l'évaluation des modèles ?

<Question
	choices={[
		{
			text: "Ils fournissent des métriques de performance parfaites dans le monde réel",
			explain: "Incorrect ! Si les jeux d'évaluation automatiques sont utiles, ils ne se traduisent pas toujours directement par des performances réelles.."
		},
		{
			text: "Ils permettent une comparaison standardisée entre les modèles et fournissent des résultats reproductibles.",
			explain: "Correct ! C'est l'un des principaux avantages des jeux d'évaluation automatiques.",
			correct: true
		},
		{
			text: "Ils éliminent la nécessité de toute autre forme d'évaluation",
			explain: "Incorrect ! Les jeux d'évaluation automatiques doivent faire partie d'une stratégie d'évaluation globale et ne doivent pas constituer la seule méthode.."
		}
	]}
/>

### 2. Quel jeu d'évaluation teste les connaissances dans 57 matières différentes ??

<Question
	choices={[
		{
			text: "BBH (Big Bench Hard)",
			explain: "Incorrect ! BBH se concentre sur des tâches de raisonnement complexes et non sur des connaissances générales."
		},
		{
			text: "GSM8K",
			explain: "Incorrect ! GSM8K vise spécifiquement la résolution de problèmes mathématiques."
		},
		{
			text: "MMLU",
			explain: "Correct ! MMLU (Massive Multitask Language Understanding) teste les connaissances dans 57 matières, des sciences aux sciences humaines.",
			correct: true
		}
	]}
/>

### 3. Qu'est-ce qu'un <i>LLM-as-Judge</i> ?

<Question
	choices={[
		{
			text: "Utilisation d'un modèle de langage pour évaluer les résultats d'un autre modèle",
			explain: "Correct ! Il s'agit d'une approche d'évaluation alternative qui peut fournir un retour d'information plus nuancé.",
			correct: true
		},
		{
			text: "Un jeu d'évaluation qui teste le raisonnement judiciaire",
			explain: "Incorrect ! Il s'agit de l'utilisation d'un modèle pour en évaluer un autre, et non pour tester le raisonnement judiciaire."
		},
		{
			text: "Une méthode pour entraîner des modèles sur des jeux de données juridiques",
			explain: "Incorrect ! Cela n'a rien à voir avec l'entraînement sur des données juridiques mais plutôt avec l'utilisation d'un modèle pour évaluer les résultats d'un autre.."
		}
	]}
/>

### 4. Quels sont les éléments à inclure dans une stratégie d'évaluation globale ?

<Question
	choices={[
		{
			text: "Uniquement des jeux d'évaluation standards",
			explain: "Incorrect ! A comprehensive strategy should include multiple evaluation methods."
		},
		{
			text: "Des jeux d'évaluation standard, des jeux de données d'évaluation personnalisés et des tests spécifiques à un domaine",
			explain: "Correct ! Une stratégie globale doit comporter plusieurs niveaux d'évaluation.",
			correct: true
		},
		{
			text: "Uniquement des jeux de données personnalisés spécifiques à votre cas d'usage",
			explain: "Incorrect ! Si les jeux de données personnalisés sont importants, ils ne doivent pas constituer la seule méthode d'évaluation."
		}
	]}
/>

### 5. Qu'est-ce qu'une limitation des jeux d'évaluation automatiques ?

<Question
	choices={[
		{
			text: "Ils sont trop chers à éxécuter",
			explain: "Incorrect ! Le coût n'est généralement pas leur principale limite."
		},
		{
			text: "Les performances des jeux d'évaluation ne traduisent pas toujours directement une efficacité réelle",
			explain: "Correct ! Il s'agit d'une limitation essentielle à garder à l'esprit.",
			correct: true
		},
		{
			text: "Ils ne peuvent évaluer que de petits modèles",
			explain: "Incorrect ! Ils peuvent être utilisés pour évaluer des modèles de différentes tailles."
		}
	]}
/>

### 6. Quel est l'objectif de la création de jeux de données d'évaluation personnalisés ?

<Question
	choices={[
		{
			text: "Pour refléter votre cas d'utilisation spécifique et inclure des requêtes d'utilisateurs réelles liées à votre domaine.",
			explain: "Correct ! Des jeux de données personnalisés permettent de s'assurer que l'évaluation répond à vos besoins.",
			correct: true
		},
		{
			text: "Remplacer totalement les jeux d'évaluation standards",
			explain: "Incorrect ! Les jeux de données personnalisés doivent compléter, et non remplacer, les jeux d'évaluation standards."
		},
		{
			text: "Pour faciliter l'évaluation",
			explain: "Incorrect ! La création de jeux de données personnalisés nécessite un effort supplémentaire mais permet d'obtenir une évaluation plus pertinente."
		}
	]}
/>
