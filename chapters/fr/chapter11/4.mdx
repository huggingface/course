<CourseFloatingBanner chapter={11}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "English", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter11/section4.ipynb"},
	{label: "Fran√ßais", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter10/section4.ipynb"},
    {label: "English", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter11/section4.ipynb"},
    {label: "Fran√ßais", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/fr/chapter11/section4.ipynb"},
]} />

# LoRA (Low-Rank Adaptation)

Le *finetuning* de LLM est un processus gourmand en ressources. LoRA est une technique qui nous permet de finetuner de tels mod√®les avec un petit nombre de param√®tres. Elle fonctionne en ajoutant et en optimisant des matrices plus petites aux poids d'attention, ce qui r√©duit g√©n√©ralement les param√®tres entra√Ænables d'environ 90 %.

## Comprendre la m√©thode LoRA

LoRA (Low-Rank Adaptation) est une technique de *finetuning* efficace en termes de param√®tres qui g√®le les poids du mod√®le pr√©-entra√Æn√© et injecte des matrices de d√©composition de rangs entra√Ænables dans les couches du mod√®le. Au lieu d'entra√Æner tous les param√®tres du mod√®le pendant le *finetuning*, LoRA d√©compose les mises √† jour des poids en matrices plus petites par le biais d'une d√©composition de rang inf√©rieur, r√©duisaint ainsi consid√©rablement le nombre de param√®tres √† entra√Æner tout en maintenant les performances du mod√®le. Par exemple, lorsqu'il est appliqu√© au GPT-3 175B, LoRA r√©duit de 10 000 fois le nombre de param√®tres entra√Ænables et de trois fois les besoins en m√©moire du par rapport √† un *finetuning* complet. Pour en savoir plus sur cette m√©thode, consultez ce [papier](https://arxiv.org/pdf/2106.09685).

LoRA fonctionne en ajoutant, g√©n√©ralement sur les poids des couches dattention du *transformer*, des paires de matrices de d√©composition des rangs. Pendant l'inf√©rence, ces poids d'adaptateur peuvent √™tre fusionn√©s avec le mod√®le de base, ce qui n'entra√Æne aucune latence suppl√©mentaire. LoRA est particuli√®rement utile pour adapter de grands mod√®les de langage √† des t√¢ches ou domaines sp√©cifiques tout en conservant des besoins en ressources g√©rables.

## Principaux avantages du LoRA

1. **Efficacit√© de la m√©moire** : 
   - Seuls les param√®tres de l`adaptateur sont stock√©s dans la m√©moire du GPU
   - Les poids du mod√®le de base restent gel√©s et peuvent √™tre charg√©s avec une pr√©cision inf√©rieure.
   - Permet un *finetuning* des grands mod√®les sur des GPU grand public.

2. **Caract√©ristiques d'entra√Ænement** :
   - Int√©gration native des PEFT/LoRA avec une configuration minimale
   - Prise en charge de QLoRA (Quantized LoRA) pour une meilleure efficacit√© de la m√©moire.

3. **Gestion des adaptateurs** :
   - R√©duction du poids de l'adaptateur lors des *checkpoints*
   - Fonctionnalit√©s permettant de fusionner les adaptateurs dans le mod√®le de base

## Chargement des adaptateurs LoRA avec PEFT

[*PEFT*](https://github.com/huggingface/peft) est une biblioth√®que qui fournit une interface unifi√©e pour charger et g√©rer les m√©thodes PEFT (*Parameter-Efficient Fine-Tuning*), y compris LoRA. Elle permet de charger et de passer facilement d'une m√©thode PEFT √† l'autre, ce qui facilite l'exp√©rimentation de diff√©rentes techniques de *finetuning*. 

Les adaptateurs peuvent √™tre charg√©s sur un mod√®le pr√©-entra√Æn√© avec `load_adapter()`, ce qui est utile pour en essayer diff√©rents dont les poids ne sont pas fusionn√©s. D√©finissez les poids de l'adaptateur actif avec la fonction `set_adapter()`. Pour retourner le mod√®le de base, vous pouvez utiliser `unload()` pour d√©charger tous les modules LoRA. Cela facilite le basculement entre diff√©rents poids sp√©cifiques √† une t√¢che.

```python
from peft import PeftModel, PeftConfig

config = PeftConfig.from_pretrained("ybelkada/opt-350m-lora")
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
lora_model = PeftModel.from_pretrained(model, "ybelkada/opt-350m-lora")
```

![lora_load_adapter](https://github.com/huggingface/smol-course/raw/main/3_parameter_efficient_finetuning/images/lora_adapter.png)

## Finetuner un LLM en utilisant `trl` et `SFTTrainer` avec LoRA

[SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) de `trl` fournit une int√©gration avec les adaptateurs LoRA √† travers la biblioth√®que [PEFT](https://huggingface.co/docs/peft/en/index). Cela signifie que nous pouvons finetuner un mod√®le de la m√™me mani√®re que nous l'avons fait avec le SFT, mais en utilisant LoRA pour r√©duire le nombre de param√®tres que nous avons besoin d'entra√Æner.

Nous utiliserons la classe `LoRAConfig` de PEFT dans notre exemple. L'installation ne n√©cessite que quelques √©tapes de configuration :

1. D√©finir la configuration LoRA (rang, alpha, *dropout*).
2. Cr√©er le SFTTrainer avec la configuration PEFT
3. Entra√Æner et sauvegarder les poids de l'adaptateur.

## Configuration du LoRA

Passons en revue la configuration et les param√®tres cl√©s du LoRA.
*** Traduit avec www.DeepL.com/Translator (version gratuite) ***


| Param√®tres | Description |
|-----------|-------------|
| `r` (rank) | Dimension des matrices de rang inf√©rieur utilis√©es pour la mise √† jour des poids. Elle est g√©n√©ralement comprise entre 4 et 32. Des valeurs plus faibles permettent une plus grande compression mais potentiellement moins d'expressivit√©. |
| `lora_alpha` | Facteur d'√©chelle pour les couches LoRA, g√©n√©ralement fix√© √† deux fois la valeur du rang. Des valeurs plus √©lev√©es se traduisent par des effets d'adaptation plus forts. |
| `lora_dropout` | Probabilit√© de *dropout* pour les couches LoRA, g√©n√©ralement de 0,05 √† 0,1. Des valeurs plus √©lev√©es permettent d'√©viter un surentra√Ænement pendant l'entra√Ænement. |
| `bias` | Contr√¥le l'entra√Ænement des termes de biais. Les options sont ¬´ none ¬ª, ¬´ all ¬ª ou ¬´ lora_only ¬ª. L'option ¬´ none ¬ª est la plus courante pour des raisons d'efficacit√© de m√©moire. |
| `target_modules` | Sp√©cifie les modules du mod√®le auxquels appliquer la m√©thode LoRA. Il peut s'agir de ¬´ tous les modules lin√©aires ¬ª ou de modules sp√©cifiques tels que ¬´ q_proj, v_proj ¬ª. Un plus grand nombre de modules permet une plus grande adaptabilit√© mais augmente l'utilisation de la m√©moire. |

<Tip>
Lors de l'impl√©mentation des m√©thodes PEFT, commencez par de petites valeurs de rang (4-8) pour LoRA et surveillez la perte d'entra√Ænement. Utilisez des ensembles de validation pour √©viter le surentra√Æne√πent et comparez les r√©sultats avec des *baselines* via *finetuning* complet lorsque cela est possible. L'efficacit√© des diff√©rentes m√©thodes peut varier en fonction de la t√¢che, c'est pourquoi l'exp√©rimentation est essentielle.
</Tip>

## Utilisation de TRL avec PEFT

Les m√©thodes PEFT peuvent √™tre combin√©es avec TRL pour un *finetuning* permettant de r√©duire les besoins de m√©moire. Nous pouvons passer `LoraConfig` au mod√®le lors de son chargement.  

```python
from peft import LoraConfig

# TODO : Configurer les param√®tres de LoRA
# r : dimension du rang des matrices LoRA (plus petite = plus de compression)
rank_dimension = 6
# lora_alpha : facteur d'√©chelle pour les couches LoRA (plus √©lev√© = adaptation plus forte)
lora_alpha = 8
# lora_dropout : probabilit√© de dropout pour les couches LoRA (aide √† pr√©venir le surentra√Ænement)
lora_dropout = 0.05

peft_config = LoraConfig(
    r=rank_dimension,  # Dimension du rang, g√©n√©ralement entre 4 et 32
    lora_alpha=lora_alpha,  # Facteur d'√©chelle LoRA, g√©n√©ralement 2x le rang
    lora_dropout=lora_dropout,  # Probabilit√© de dropout probability pour les couches de LoRA
    bias="none",  # Type de biais pour le LoRA. Les biais correspondants seront mis √† jour pendant l'entra√Ænement
    target_modules="all-linear",  # Modules auxquels appliquer le LoRA
    task_type="CAUSAL_LM",  # Type de t√¢che pour l'architecture du mod√®le
)
```

Ci-dessus, nous avons utilis√© `device_map="auto"` pour assigner automatiquement le mod√®le au bon appareil. Vous pouvez √©galement assigner manuellement le mod√®le √† un appareil sp√©cifique en utilisant `device_map={"": device_index}`. 

Nous aurons √©galement besoin de d√©finir le `SFTTrainer` avec la configuration du LoRA.

```python
# Cr√©er un SFTTrainer avec la configuration LoRA
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    peft_config=peft_config,  # Configuration LoRA 
    max_seq_length=max_seq_length,  # Longueur maximale de la s√©quence
    processing_class=tokenizer,
)
```

<Tip>
‚úèÔ∏è **Essayez !** Reprenez votre mod√®le de la section pr√©c√©dente, mais finetun√©-le avec LoRA. Utilisez le jeu de donn√©es `HuggingFaceTB/smoltalk` pour finetuner un mod√®le `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`, en utilisant la configuration LoRA que nous avons d√©finie ci-dessus.
</Tip>

## Fusion des adaptateurs LoRA

Apr√®s l'entra√Ænement avec LoRA, vous pouvez souhaiter fusionner les poids des adaptateurs dans le mod√®le de base pour faciliter le d√©ploiement. Cela permet de cr√©er un mod√®le unique avec les poids combin√©s, √©liminant ainsi la n√©cessit√© de charger les adaptateurs s√©par√©ment pendant l'inf√©rence.

Le processus de fusion n√©cessite une attention particuli√®re de la gestion de la m√©moire et de la pr√©cision. Comme vous devrez charger simultan√©ment le mod√®le de base et les poids des adaptateurs, assurez-vous que la m√©moire du GPU/CPU est suffisante. L'utilisation de `device_map="auto"` dans ü§ó *Transformers* trouvera le bon p√©riph√©rique pour le mod√®le en fonction de votre mat√©riel. 

Maintenez une pr√©cision coh√©rente tout au long du processus, en utilisant la pr√©cision utilis√©e pendant l'entra√Ænement et en sauvegardant le mod√®le fusionn√© dans le m√™me format pour le d√©ploiement. 

## Mise en ≈ìuvre de la fusion

Apr√®s l'entra√Ænement d'un adaptateur LoRA, vous pouvez fusionner les poids de l'adaptateur dans le mod√®le de base. Voici comment proc√©der :

```python
import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel

# 1. Charger le mod√®le de basel
base_model = AutoModelForCausalLM.from_pretrained(
    "base_model_name", torch_dtype=torch.float16, device_map="auto"
)

# 2. Charger le mod√®le PEFT avec l'adaptateur
peft_model = PeftModel.from_pretrained(
    base_model, "path/to/adapter", torch_dtype=torch.float16
)

# 3. Fusionner les poids de l'adaptateur avec le mod√®le de base
merged_model = peft_model.merge_and_unload()
```

Si vous constatez des diff√©rences de taille dans le mod√®le sauvegard√©, assurez-vous que vous sauvegardez √©galement le *tokenizer* :

```python
# Sauvegarde du mod√®le et du tokenizer
tokenizer = AutoTokenizer.from_pretrained("base_model_name")
merged_model.save_pretrained("path/to/save/merged_model")
tokenizer.save_pretrained("path/to/save/merged_model")
```

<Tip>
‚úèÔ∏è **Essayez !** Fusionner les poids de l'adaptateur dans le mod√®le de base. Utiliser le jeu de donn√©es `HuggingFaceTB/smoltalk` pour finetuner un mod√®le `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` gr√¢ce √† l'approche LoRA que nous avons d√©finie ci-dessus.
</Tip>


# Ressources

- [Papier de la m√©thode LoRA (*Low-Rank Adaptation of Large Language Models*)](https://arxiv.org/pdf/2106.09685)
- [Documentation de la biblioth√®que *PEFT*](https://huggingface.co/docs/peft)
- [Article de blog d'Hugging Face sur *PEFT-](https://huggingface.co/blog/peft)