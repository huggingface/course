<FrameworkSwitchCourse {fw} />

# Vorbereitung der Daten

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/de/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/de/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/de/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/de/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
Wir fahren mit dem Beispiel aus dem [vorigen Kapitel](/course/chapter2) fort. Folgenderweise w√ºrden wir einen Sequenzklassifikator mit einem Batch in PyTorch trainieren:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Genau wie vorher
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",  # Ich habe mein ganzes Leben auf einen HuggingFace-Kurs gewartet.
    "This course is amazing!",  # Dieser Kurs ist fantastisch!
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# Dies ist neu
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Wir fahren mit dem Beispiel aus dem [vorigen Kapitel](/course/chapter2) fort. Folgenderweise w√ºrden wir einen Sequenzklassifikator mit einem Batch in Tensorflow trainieren:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Genau wie vorher
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",  # Ich habe mein ganzes Leben auf einen HuggingFace-Kurs gewartet.
    "This course is amazing!",  # Dieser Kurs ist fantastisch!
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# Dies ist neu
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

Nat√ºrlich w√ºrde das Training von Modellen mit nur zwei S√§tzen keine sonderlich guten Ergebnisse liefern. Um bessere Ergebnisse zu erzielen, m√ºssen wir einen gr√∂√üeren Datensatz vorbereiten.

In diesem Abschnitt verwenden wir den MRPC-Datensatz (Microsoft Research Paraphrase Corpus) als Beispiel. Dieser wurde in einem [Paper](https://www.aclweb.org/anthology/I05-5002.pdf) von William B. Dolan und Chris Brockett ver√∂ffentlicht. Der Datensatz besteht aus insgesamt 5.801 Satzpaaren und enth√§lt ein Label, das angibt, ob es sich bei einem Paar um Paraphrasen handelt (d.h. ob beide S√§tze dasselbe bedeuten). Wir haben diesen Datensatz f√ºr dieses Kapitel ausgew√§hlt, weil es sich um einen kleinen Datensatz handelt, sodass es einfach ist, w√§hrend dem Training zu experimentieren.

### Laden eines Datensatzes vom Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Das Hub enth√§lt nicht nur Modelle; Es hat auch mehrere Datens√§tze in vielen verschiedenen Sprachen. Du kannst die Datens√§tze [hier](https://huggingface.co/datasets) durchsuchen, und wir empfehlen, einen weiteren Datensatz zu laden und zu verarbeiten, sobald Sie diesen Abschnitt abgeschlossen haben (die Dokumentation befindet sich [hier](https: //huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). Aber jetzt konzentrieren wir uns auf den MRPC-Datensatz! Dies ist einer der 10 Datens√§tze, aus denen sich das [GLUE-Benchmark](https://gluebenchmark.com/) zusammensetzt. Dies ist ein akademisches Benchmark, das verwendet wird, um die Performance von ML-Modellen in 10 verschiedenen Textklassifizierungsaufgaben zu messen.

Die Bibliothek ü§ó Datasets bietet einen leichten Befehl zum Herunterladen und Caching eines Datensatzes aus dem Hub. Wir k√∂nnen den MRPC-Datensatz wie folgt herunterladen:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Wie du sehen kannst, erhalten wir ein `DatasetDict`-Objekt, das die Trainingsdaten, die Validierungsdaten und die Testdaten enth√§lt. Jedes Objekt enth√§lt mehrere Spalten (`sentence1`, `sentence2`, `label` und `idx`) und eine unterschiedliche Anzahl an Zeilen, dies ist die Anzahl der Elemente in jedem Datensatz (also gibt es 3.668 Satzpaare in den Trainingsdaten, 408 in den Validierungsdaten und 1.725 in den Testdaten).

Dieser Befehl l√§dt das Dataset herunter und speichert es im Cache, standardm√§√üig in *~/.cache/huggingface/dataset*. Wir Erinnern uns an Kapitel 2, dass der Cache-Ordner anpasst werden kann, indem man die Umgebungsvariable `HF_HOME` setzt.

Wir k√∂nnen auf jedes Satzpaar in unserem `raw_datasets`-Objekt zugreifen, indem wir wie bei einem Dictionary einen Schl√ºsselwert als Index verwenden:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

Wir stellen fest, dass die Labels bereits Ganzzahlen sind, sodass wir dort keine Vorverarbeitung durchf√ºhren m√ºssen. Wir k√∂nnen die `features` von `raw_train_dataset` untersuchen, um zu erfahren, welche Ganzzahl welchem Label entspricht. Der folgende Befehl gibt uns den Variablentyp zur√ºck:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

Hinter den Kulissen ist `label` vom Typ `ClassLabel`, und die Zuordnung von Ganzzahlen zum Labelnamen wird im Ordner *names* gespeichert. `0` entspricht `not_equivalent`, also "nicht √§quivalent", und `1` entspricht `equivalent`, also "√§quivalent".

<Tip>

‚úèÔ∏è **Probier es aus!** Sieh dir das Element 15 der Trainingsdaten und Element 87 des Validierungsdaten an. Was sind ihre Labels?

</Tip>

### Vorverarbeitung eines Datensatzes

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Um den Datensatz vorzubereiten, m√ºssen wir den Text in Zahlen umwandeln, die das Modell sinnvoll verarbeiten kann. Im [vorherigen Kapitel](/course/chapter2) haben wir gesehen, dass dies mit einem Tokenizer gemacht wird. Wir k√∂nnen den Tokenizer mit einem Satz oder einer Liste von S√§tzen f√ºttern, sodass wir die ersten und zweiten S√§tze jedes Paares wie folgt direkt tokenisieren k√∂nnen:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Wir k√∂nnen jedoch nicht einfach zwei Sequenzen an das Modell √ºbergeben und eine Vorhersage erhalten, ob die beiden S√§tze paraphrasiert sind oder nicht. Wir m√ºssen die beiden Sequenzen als Paar behandeln und die entsprechende Vorverarbeitung anwenden. Gl√ºcklicherweise kann der Tokenizer auch ein Sequenzpaar nehmen und es so vorbereiten, wie es unser BERT-Modell erwartet:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

In [Kapitel 2](/course/chapter2) haben wir die Schl√ºsselwerte `input_ids` und `attention_mask` behandelt, allerdings haben wir es aufgeschoben, √ºber `token_type_ids` zu sprechen. In diesem Beispiel teilt diese dem Modell mit, welcher Teil des Input der erste Satz und welcher der zweite Satz ist.

<Tip>

‚úèÔ∏è **Probier es aus!** Nimm Element 15 der Trainingsdaten und tokenisiere die beiden S√§tze separat und als Paar. Wo liegt der Unterschied zwischen den beiden Ergebnissen?

</Tip>

Wenn wir die IDs in `input_ids` zur√ºck in Worte dekodieren:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

dann bekommen wir:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

Wir sehen also wenn es zwei S√§tze gibt, dass das Modell erwartet, dass die Inputs die Form "[CLS] Satz1 [SEP] Satz2 [SEP]" haben. Wenn wir dies mit den `token_type_ids` abgleichen, erhalten wir:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Wie du sehen kannst, haben die Teile der Eingabe, die `[CLS] Satz1 [SEP]` entsprechen, alle eine Token-Typ-ID von `0`, w√§hrend die anderen Teile, die `Satz2 [SEP]` entsprechen, alle einer Token-Typ-ID von `1` enthalten.

Beachte, dass die Auswahl eines anderen Checkpoints nicht unbedingt die `token_type_ids` in Ihren tokenisierten Inputs haben (z.B. werden sie nicht zur√ºckgegeben, wenn ein DistilBERT-Modell verwendet wird). Sie werden nur zur√ºckgegeben, wenn das Modell wei√ü was damit zu tun ist, weil es die Toke-Typ-Ids w√§hrend des Vortrainings gesehen hat.

In diesem Fall ist BERT mit Token-Typ-IDs vortrainiert worden, und zus√§tzlich zu dem maskierten Sprachmodellierungsziel aud [Kapitel 1](/course/chapter1), hat es ein zus√§tzliches Vorhersageziel namens _next sentence prediction_ (d.h. Vorhersage des n√§chsten Satzes). Das Ziel dieser Aufgabe ist es, die Beziehung zwischen Satzpaaren zu modellieren.

Bei der Vorhersage des n√§chsten Satzes werden dem Modell Satzpaare (mit zuf√§llig maskierten Token) bereitgestellt und erwartet, vorherzusagen, ob auf den ersten Satz der zweite Satz folgt. Um die Aufgabe non-trivial zu machen, folgen sich die H√§lfte der S√§tze in dem Originaldokument, aus dem sie extrahiert wurden, aufeinander, und in der anderen H√§lfte stammen die beiden S√§tze aus zwei verschiedenen Dokumenten.

Im Allgemeinen muss man sich keine Gedanken dar√ºber machen, ob Ihre tokenisierten Inputs `token_type_ids` enthalten oder nicht: Solange du denselben Checkpoint f√ºr den Tokenizer und das Modell verwendest, ist alles in Ordnung, da der Tokenizer wei√ü, was er dem Modell bereitstellen soll.

Nachdem wir nun gesehen haben, wie unser Tokenizer mit einem Satzpaar umgehen kann, k√∂nnen wir damit unseren gesamten Datensatz tokenisieren: Wie im [vorherigen Kapitel](/course/chapter2) k√∂nnen wir dem Tokenizer eine Liste von Satzpaaren einspeisen, indem du ihm die Liste der ersten S√§tze und dann die Liste der zweiten S√§tze gibst. Dies ist auch kompatibel mit den Optionen zum Padding und Trunkieren, die wir in [Kapitel 2](/course/chapter2) gesehen haben. Eine M√∂glichkeit, den Trainingsdatensatz vorzuverarbeiten, ist also:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Das funktioniert gut, hat aber den Nachteil, dass ein Dictionary zur√ºckgegeben wird (mit unseren Schl√ºsselw√∂rtern `input_ids`, `attention_mask` und `token_type_ids` und Werten aus Listen von Listen). Es funktioniert auch nur, wenn du gen√ºgend RAM hast, um den gesamten Datensatz w√§hrend der Tokenisierung zu im RAM zwischen zu speichern (w√§hrend die Datens√§tze aus der Bibliothek ü§ó Datasets [Apache Arrow](https://arrow.apache.org/) Dateien sind, die auf der Festplatte gespeichert sind, sodass nur die gew√ºnschten Samples im RAM geladen sind).

Um die Daten als Datensatz zu speichern, verwenden wir die Methode [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). Dies gew√§hrt uns zus√§tzliche Flexibilit√§t, wenn wir zus√§tzliche Vorverarbeitung als nur die Tokenisierung ben√∂tigen. Die `map()`-Methode funktioniert, indem sie eine Funktion auf jedes Element des Datensatzes anwendet, also definieren wir eine Funktion, die unsere Inputs tokenisiert:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

Diese Funktion nimmt ein Dictionary (wie die Elemente unseres Datensatzes) und gibt ein neues Dictionary mit den Schl√ºsselwerten `input_ids`, `attention_mask` und `token_type_ids` zur√ºck. Beachte, dass es auch funktioniert, wenn das `example`-Dictionary mehrere Beispiele enth√§lt (jeder Schl√ºsselwert als Liste von S√§tzen), da der `Tokenizer`, wie zuvor gesehen, mit Listen von Satzpaaren arbeitet. Dadurch k√∂nnen wir die Option `batched=True` in unserem Aufruf von `map()` verwenden, was die Tokenisierung erheblich beschleunigt. Der `tokenizer` wurde in Rust geschriebenen und ist in der Bibliothek [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) verf√ºgbar. Dieser Tokenizer kann sehr schnell arbeiten, wenn wir ihm viele Inputs auf einmal zum Verarbeiten geben. Note that we've left the `padding` argument out in our tokenization function for now. 

Beachte, dass wir das `padding`-Argument vorerst in unserer Tokenisierungsfunktion ausgelassen haben. Dies liegt daran, dass das Anwenden von Padding auf alle Elemente unserer Daten auf die maximale L√§nge nicht effizient ist: Es ist besser, die Proben aufzuf√ºllen, wenn wir ein Batch erstellen, da wir dann nur auf die maximale L√§nge in diesem Batch auff√ºllen m√ºssen und nicht auf die maximale L√§nge in den gesamten Datensatz. Dies kann viel Zeit und Rechenleistung sparen, besonders wenn die Eingaben stark variable L√§ngen haben!

So wenden wir die Tokenisierungsfunktion auf alle unsere Datens√§tze gleichzeitig an. In unserem Aufruf von `map` verwenden wir `batched=True`, damit die Funktion auf mehrere Elemente des Datensatzes gleichzeitig angewendet wird und nicht auf jedes Element separat. Dies erm√∂glicht eine schnellere Vorverarbeitung.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

Die Bibliothek ü§ó Datasets verarbeitet Datens√§tzen indem sie neue Felder hinzuzuf√ºgen, eines f√ºr jeden Schl√ºssel im Dictionary, der von der Vorverarbeitungsfunktion zur√ºckgegeben wird:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Du kannst sogar Multiprocessing verwenden, wenn du die Vorverarbeitungsfunktion mit `map()` anwendest, indem du ein `num_proc`-Argument √ºbergiebst. Wir haben dies hier nicht getan, weil die ü§ó Tokenizers-Bibliothek bereits mehrere Threads verwendet, um unsere Samples schneller zu tokenisieren. Wenn du keinen schnellen Tokenizer verwendest, der von dieser Bibliothek unterst√ºtzt wird, w√ºrde dies allerdings die Vorverarbeitung beschleunigen.

Unsere `tokenize_function` gibt ein Dictionary mit den Schl√ºsselwerten `input_ids`, `attention_mask` und `token_type_ids` zur√ºck, also werden diese drei Felder zu allen Splits unseres Datensatzes hinzugef√ºgt. Beachte, dass wir auch vorhandene Felder √§ndern k√∂nnten, wenn unsere Vorverarbeitungsfunktion einen neuen Wert f√ºr einen vorhandenen Schl√ºsselwert in dem Datensatz zur√ºckgegeben h√§tte, auf den wir `map()` angewendet haben.

Zuletzt, m√ºssen wir alle Beispiele auf die L√§nge des l√§ngsten Elements aufzuf√ºllen, wenn wir Elemente zusammenfassen ‚Äì eine Technik, die wir als *Dynamisches Padding* bezeichnen.

### Dynamisches Padding

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}

Die Funktion, die f√ºr das Zusammenstellen von Samples innerhalb eines Batches verantwortlich ist, wird als *Collate-Funktion* bezeichnet. Es ist ein Argument, das du √ºbergeben kannst, wenn du einen `DataLoader` baust, wobei es standardm√§√üig eine Funktion ist, die die Daten in PyTorch-Tensoren umwandelt und zusammenf√ºgt (rekursiv wenn die Elemente Listen, Tupel oder Dictionaries sind). Dies ist in unserem Fall nicht m√∂glich, da die Inputs nicht alle gleich gro√ü sind. Das Padding haben wir bewusst aufgeschoben, um es bei jedem Batch nur bei Bedarf anzuwenden und √ºberlange Inputs mit massivem Padding zu vermeiden. Dies beschleunigt das Training zwar, aber beachte, dass das Training auf einer TPU Probleme verursachen kann ‚Äì TPUs bevorzugen feste Formen, auch wenn das ein zus√§tzliches Padding erfordert.

{:else}

Die Funktion, die f√ºr das Zusammenstellen von Samples innerhalb eines Batches verantwortlich ist, wird als *Collate-Funktion* bezeichnet. Es ist ein Argument, das du √ºbergeben kannst, wenn du einen `DataLoader` baust, wobei es standardm√§√üig eine Funktion ist, die die Daten in tf.Tensor umwandelt und zusammenf√ºgt (rekursiv wenn die Elemente Listen, Tupel oder Dictionaries sind). Dies ist in unserem Fall nicht m√∂glich, da die Inputs nicht alle gleich gro√ü sind. Das Padding haben wir bewusst aufgeschoben, um es bei jedem Batch nur bei Bedarf anzuwenden und √ºberlange Inputs mit massivem Padding zu vermeiden. Dies beschleunigt das Training zwar, aber beachte, dass das Training auf einer TPU Probleme verursachen kann ‚Äì TPUs bevorzugen feste Formen, auch wenn das ein zus√§tzliches Padding erfordert.

{/if}

In der Praxis m√ºssen wir eine Collate-Funktion definieren, die die korrekte Menge an Padding auf die Elemente des Datensatzes anwendet, die wir in einem Batch haben m√∂chten. Gl√ºcklicherweise stellt uns die ü§ó Transformers-Bibliothek √ºber `DataCollatorWithPadding` eine solche Funktion zur Verf√ºgung. Wenn sie instanziert wird, braucht es einen Tokenizer (um zu wissen, welches Padding-token verwendet werden soll und ob das Modell erwartet, dass sich das Padding links oder rechts von den Inputs befindet) und √ºbernimmt alles was wir brauchen:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Um dieses neue Werkzeug zu testen, nehmen wir einige Elemente aus den Trainingsdaten, die wir als Batch verwenden m√∂chten. Hier entfernen wir die Spalten `idx`, `sentence1` und `sentence2`, da sie nicht ben√∂tigt werden und Strings enthalten (wir k√∂nnen keine Tensoren mit Strings erstellen) und sehen uns die L√§nge jedes Eintrags im Batch an:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Wenig √ºberraschen erhalten wir Samples unterschiedlicher L√§nge von 32 bis 67. Dynamisches Padding bedeutet, dass die Elemente in diesem Batch alle auf eine L√§nge von 67 aufgef√ºllt werden, die maximale L√§nge innerhalb des Batches. Ohne dynamisches Auff√ºllen m√ºssten alle Eintr√§ge auf die maximale L√§nge im gesamten Datensatz oder auf die maximale L√§nge die das Modell akzeptiert, aufgef√ºllt werden. Lass uns noch einmal √ºberpr√ºfen, ob unser `data_collator` den Stapel dynamisch richtig auff√ºllt:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Das sieht gut aus! Jetzt, da wir vom Rohtext zu Batches √ºbergegangen sind, mit denen unser Modell umgehen kann, sind wir bereit zum fein-tunen!

{/if}

<Tip>

‚úèÔ∏è **Probier es aus!** Repliziere die Vorverarbeitung auf dem GLUE SST-2-Datensatz. Es ist ein bisschen anders, da es aus einzelnen S√§tzen statt aus Paaren besteht, aber der Rest von dem, was wir gemacht haben, sollte gleich aussehen. Alternative w√§re eine schwierigere Herausforderung, eine Vorverarbeitungsfunktion zu schreiben, die bei allen GLUE-Aufgaben funktioniert.

</Tip>

{#if fw === 'tf'}

Jetzt, da wir unseren Datensatz und einen DataCollator haben, m√ºssen wir sie verbinden. Wir k√∂nnten Batches manuell laden und sortieren, aber das ist eine Menge Arbeit und wahrscheinlich auch nicht sehr sonderlich performant. Stattdessen gibt es eine einfache Methode, die dieses Problem performant l√∂st: `to_tf_dataset()`. Dadurch wird ein `tf.data.Dataset` um den Datensatz gewickelt, mit einer optionalen Kollatierungsfunktion. `tf.data.Dataset` ist ein natives TensorFlow-Format, das Keras f√ºr `model.fit()` verwenden kann. Diese Methode kann einen ü§ó-Datensatz ohne Umst√§nde in ein f√ºrs Training vorbereitetes Format konvertieren. Sehen wir es uns nun mit unserem Datensatz in Aktion an!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```
Und das was's! Wir k√∂nnen Datens√§tze in das n√§chste Kapitel mitnehmen, wo das Training nach all der harten Arbeit der Datenvorverarbeitung angenehm unkompliziert sein wird.

{/if}
