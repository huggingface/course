- title: 0. Einrichtung
  sections:
  - local: chapter0/1
    title: Einf√ºhrung

- title: 1. Transformer-Modelle
  sections:
  - local: chapter1/1
    title: Einf√ºhrung
  - local: chapter1/2
    title: Natural Language Processing
  - local: chapter1/3
    title: Transformer-Modelle - wozu sind sie imstande?
  - local: chapter1/4
    title: Wie funktionieren Transformer-Modelle?
  - local: chapter1/5
    title: Encoder-Modelle
  - local: chapter1/6
    title: Decoder-Modelle
  - local: chapter1/7
    title: Sequence-to-Sequence-Modelle
  - local: chapter1/8
    title: Bias und Einschr√§nkungen
  - local: chapter1/9
    title: Zusammenfassung
  - local: chapter1/10
    title: Quiz am Ende des Kapitels
    quiz: 1

- title: 2. ü§ó Transformers verwenden
  sections:
  - local: chapter2/1
    title: Einf√ºhrung
  - local: chapter2/2
    title: Die Pipeline
  - local: chapter2/3
    title: Modelle
  - local: chapter2/4
    title: Tokenizer
  - local: chapter2/5
    title: Handling multiple sequences
  - local: chapter2/6
    title: Putting it all together
  - local: chapter2/7
    title: Basislektion abgeschlossen!
  - local: chapter2/8
    title: End-of-chapter quiz
    quiz: 2

- title: 3. Fine-tuning von vortrainierten Modellen
  sections:
  - local: chapter3/1
    title: Einf√ºhrung
  - local: chapter3/2
    title: Datenbearbeitung
  - local: chapter3/3
    title: Fine-tuning von Modellen mit der Trainer API oder Keras
    local_fw: { pt: chapter3/3, tf: chapter3/3_tf }
  - local: chapter3/4
    title: Komplettes Training
  - local: chapter3/5
    title: Fine-tuning, Check!
  - local: chapter3/6
    title: Quiz am Ende des Kapitels
    quiz: 3

- title: 4. Teilen von Modellen und Tokenizers
  sections:
  - local: chapter4/1
    title: Der Hugging Face Hub
  - local: chapter4/2
    title: Verwendung vortrainierter Modelle
  - local: chapter4/3
    title: Vortrainierte Modelle teilen

- title: W√∂rterverzeichnis
  sections:
  - local: glossary/1
    title: W√∂rterverzeichnis
