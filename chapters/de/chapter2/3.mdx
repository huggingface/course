<FrameworkSwitchCourse {fw} />

# Modelle[[models]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
In diesem Abschnitt werden wir uns die Erstellung und Verwendung eines Modells genauer ansehen. Wir werden die Klasse `AutoModel` verwenden, die praktisch ist, wenn du ein beliebiges Modell von einem Checkpoint aus instanziieren willst.

Die Klasse `AutoModel` und alle ihre verwandten Klassen sind eigentlich einfache Wrapper f√ºr die Vielzahl der in der Bibliothek verf√ºgbaren Modelle. Es ist ein smarter Wrapper, da er automatisch die passende Modellarchitektur f√ºr deinen Checkpoint erkennen kann und dann ein Modell mit dieser Architektur instanziiert.

{:else}
In diesem Abschnitt werden wir uns die Erstellung und Verwendung eines Modells genauer ansehen. Wir werden die Klasse `TFAutoModel` verwenden, die praktisch ist, wenn du ein beliebiges Modell von einem Checkpoint aus instanziieren willst.

Die Klasse `TFAutoModel` und alle ihre verwandten Klassen sind eigentlich einfache Wrapper f√ºr die Vielzahl der in der Bibliothek verf√ºgbaren Modelle. Es ist ein smarter Wrapper, da er automatisch die passende Modellarchitektur f√ºr deinen Checkpoint erkennen kann und dann ein Modell mit dieser Architektur instanziiert.

{/if}

Wenn du jedoch den Typ des Modells kennst, das du verwenden m√∂chtest, kannst du die Klasse der Architektur direkt verwenden. Schauen wir uns an, wie das mit einem BERT-Modell funktioniert.

## Einen Transformer erstellen[[creating-a-transformer]]

Das erste, was wir tun m√ºssen, um ein BERT-Modell zu initialisieren, ist das Laden eines Konfigurationsobjekts:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```
{/if}

Die Konfiguration enth√§lt viele Attribute, die zur Erstellung des Modells verwendet werden:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

Du hast zwar noch nicht gesehen, was all diese Attribute bewirken, aber du solltest einige von ihnen erkennen: Das Attribut `hidden_size` definiert die Gr√∂√üe des Vektors `hidden_states`, und `num_hidden_layers` definiert die Anzahl der Ebenen, die das Transformer-Modell hat.

### Die unterschiedlichen Lademethoden[[different-loading-methods]]

Bei der Erstellung eines Modells aus der Standardkonfiguration wird es mit Zufallswerten initialisiert:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}

Das Modell kann in diesem Zustand verwendet werden, aber es wird Unsinn ausgeben; es muss erst trainiert werden. Wir k√∂nnten das Modell von Grund auf f√ºr die vorliegende Aufgabe trainieren, aber wie du in [Kapitel 1](/course/chapter1) gesehen hast, w√ºrde dies viel Zeit und eine Menge Daten erfordern, und es h√§tte nicht unerhebliche Auswirkungen auf die Umwelt. Um unn√∂tigen und doppelten Aufwand zu vermeiden, ist es unerl√§sslich, bereits trainierte Modelle gemeinsam zu nutzen und wiederzuverwenden.

Das Laden eines bereits trainierten Transformer-Modells ist einfach - wir k√∂nnen dies mit der Methode `from_pretrained()` tun:

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

Wie du bereits gesehen hast, k√∂nnen wir `BertModel` durch die entsprechende Klasse `AutoModel` ersetzen. Wir werden dies von nun an tun, da dies zu Checkpoint-spezifischem Code f√ºhrt; wenn dein Code f√ºr einen Checkpoint funktioniert, sollte er nahtlos mit einem anderen funktionieren. Dies gilt selbst dann, wenn die Architektur unterschiedlich ist, solange der Checkpoint f√ºr eine √§hnliche Aufgabe trainiert wurde (z.B. f√ºr eine Sentiment-Analyse).

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

Wie du bereits gesehen hast, k√∂nnen wir `TFBertModel` durch die entsprechende Klasse `TFAutoModel` ersetzen. Wir werden dies von nun an tun, da dies zu Checkpoint-spezifischem Code f√ºhrt; wenn dein Code f√ºr einen Checkpoint funktioniert, sollte er nahtlos mit einem anderen funktionieren. Dies gilt selbst dann, wenn die Architektur unterschiedlich ist, solange der Checkpoint f√ºr eine √§hnliche Aufgabe trainiert wurde (z.B. f√ºr eine Sentiment-Analyse).

{/if}

Im obigen Codebeispiel haben wir `BertConfig` nicht verwendet und stattdessen ein trainiertes Modell √ºber die Kennung `bert-base-cased` geladen. Dies ist ein Modell-Checkpoint, der von den Autoren von BERT selbst trainiert wurde; du kannst mehr Details dar√ºber in seiner [model card](https://huggingface.co/bert-base-cased) finden.

Dieses Modell wird nun mit allen Gewichten des Checkpoints initialisiert. Es kann direkt f√ºr die Inferenz f√ºr die Aufgaben verwendet werden, auf die es trainiert wurde, und es kann auch auf eine neue Aufgabe feingetunt werden. Indem wir mit vortrainierten Gewichten trainieren und nicht von Grund auf neu, k√∂nnen wir schnell gute Ergebnisse erzielen.

Die Gewichte wurden heruntergeladen und im Cache gespeichert (so dass sie bei zuk√ºnftigen Aufrufen der Methode `from_pretrained()` nicht erneut heruntergeladen werden), der standardm√§√üig auf *~/.cache/huggingface/transformers* liegt. Du kannst deinen Cache-Ordner anpassen, indem du die Umgebungsvariable `HF_HOME` setzt.

Die zum Laden des Modells verwendete Kennung kann die eines beliebigen Modells im Model Hub sein, solange es mit der BERT-Architektur kompatibel ist. Die gesamte Liste der verf√ºgbaren BERT-Checkpoints findest du [hier](https://huggingface.co/models?filter=bert).

### Methoden zum Speichern[[saving-methods]]

Das Speichern eines Modells ist so einfach wie das Laden eines Modells - wir verwenden die Methode `save_pretrained()`, die analog zur Methode `from_pretrained()` ist:

```py
model.save_pretrained("directory_on_my_computer")
```

Dadurch werden zwei Dateien auf Ihrer Festplatte gespeichert:

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

Wenn du einen Blick auf die Datei *config.json* wirfst, wirst du die Attribute erkennen, die zum Aufbau der Modellarchitektur erforderlich sind. Diese Datei enth√§lt auch einige Metadaten, z. B. woher der Checkpoint stammt und welche ü§ó Transformers-Version du beim letzten Speichern des Checkpoints verwendet hast.

{#if fw === 'pt'}
Die Datei *pytorch_model.bin* wird als *State Dictionary* bezeichnet; sie enth√§lt alle Gewichte deines Modells. Die beiden Dateien gehen Hand in Hand; die Konfiguration ist notwendig, um die Architektur deines Modells zu kennen, w√§hrend die Modellgewichte die Parameter deines Modells sind.

{:else}
Die Datei *tf_model.h5* wird als *State Dictionary* bezeichnet; sie enth√§lt alle Gewichte deines Modells. Die beiden Dateien gehen Hand in Hand; die Konfiguration ist notwendig, um die Architektur deines Modells zu kennen, w√§hrend die Modellgewichte die Parameter deines Modells sind.

{/if}

## Verwendung eines Transformer-Modells f√ºr die Inferenz[[using-a-transformer-model-for-inference]]

Nachdem du nun wei√üt, wie man ein Modell l√§dt und speichert, lass uns versuchen, damit einige Vorhersagen zu machen. Transformer-Modelle k√∂nnen nur Zahlen verarbeiten - Zahlen, die der Tokenizer erzeugt. Doch bevor wir uns mit Tokenizern befassen, sollten wir uns ansehen, welche Eingaben das Modell akzeptiert.

Tokenizer k√∂nnen sich um die Umwandlung der Eingaben in die Tensoren des entsprechenden Frameworks k√ºmmern, aber damit du besser verstehst, was vor sich geht, werfen wir einen kurzen Blick darauf, was getan werden muss, bevor die Eingaben an das Modell gesendet werden.

Nehmen wir an, wir haben ein paar Sequenzen:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

Der Tokenizer wandelt diese in Vokabularindizes um, die √ºblicherweise als *Input IDs* bezeichnet werden. Jede Sequenz ist nun eine Liste von Zahlen! Die resultierende Ausgabe ist:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

Dies ist eine Liste von kodierten Sequenzen: eine Liste von Listen. Tensoren akzeptieren nur rechteckige Formen (√§hnlich wie bei Matrizen). Dieses "Array" hat bereits eine rechteckige Form, so dass die Umwandlung in einen Tensor einfach ist:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### Verwendung der Tensoren als Input f√ºr das Modell[[using-the-tensors-as-inputs-to-the-model]]

Die Verwendung der Tensoren mit dem Modell ist extrem einfach - wir rufen das Modell einfach mit den Eingaben auf:

```py
output = model(model_inputs)
```

Obwohl das Modell viele verschiedene Argumente akzeptiert, sind nur die Eingabe-IDs erforderlich. Wir werden sp√§ter erkl√§ren, was die anderen Argumente bewirken und wann sie erforderlich sind, 
aber zun√§chst m√ºssen wir einen genaueren Blick auf die Tokenizer werfen, die die Eingaben erstellen, die ein Transformer-Modell verstehen kann.
