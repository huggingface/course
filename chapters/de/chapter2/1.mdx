# Einf√ºhrung[[introduction]]

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

Wie du in [Kapitel 1](/course/chapter1) erfahren hast, handelt es sich bei Transformer-Modellen √ºblicherweise um sehr gro√üe Architekturen. Mit Millionen und bis zu √ºber mehreren *Milliarden* Parametern ist das Trainieren und der Einsatz dieser Modelle √§u√üerst kompliziert. Da fast tagt√§glich neue Modelle mit anderen Implementierungen hinzukommen, ist es eine gro√üe Herausforderung alle Modelle auszuprobieren.

Die ü§ó Transformers Bibliothek wurde erstellt, um dieses Problem zu l√∂sen. Mit ihr soll eine einzige API bereitgestellt werden, √ºber die jedes Transformer Modell geladen, trainiert und gespeichert werden kann. Die Haupt-Features der Bibliothek sind:

- **Einfache Bedienung**: Das Herunterladen, Laden und Verwenden eines hochmodernen CL-Modells f√ºr die Inferenz l√§sst sich mithilfe von nur zwei Codezeilen erledigen.
- **Flexibilit√§t**: Im Kern sind alle Modelle herk√∂mmliche PyTorch `nn.Module` oder TensorFlow `tf.keras.Model` Klassen und k√∂nnen wie alle anderen Modelle auch in den jeweiligen Frameworks f√ºr maschinelles Lernen (ML) benutzt werden.
- **Einfachheit**: Es werden kaum Abstraktionen in der Bibliothek vorgenommen. Als Kernkonzept gilt, dass alles in einer Datei liegt: Die Berechnung des Vorw√§rtsalgorithmus eines Modells wird vollst√§ndig in einer einzigen Datei definiert, so dass der Code verst√§ndlich und bearbeitbar ist.

Dieses letzte Feature unterscheidet ü§ó Transformers deutlich von anderen ML-Bibliotheken. Die Modelle sind nicht in Modulen aufgebaut, bei denen es Abh√§ngigkeiten zu anderen Komponenten gibt. Stattdessen hat jedes Modell seine eigenen Ebenen. Dadurch werden die Modelle nicht nur leichter zug√§nglich und verst√§ndlicher, sondern erlaubt es dir auch problemlos an einem Modell zu experimentieren, ohne andere zu beeintr√§chtigen.

Dieses Kapitel beginnt mit einem End-to-End-Beispiel, in dem wir ein Modell und einen Tokenizer zusammen verwenden, um die in [Kapitel 1](/course/chapter1) vorgestellte `pipeline()` Funktion besser zu verstehen. Als N√§chstes besprechen wir die Modell-API: Wir schauen uns die die Modell- und Konfigurationsklassen genauer an und zeigen dir, wie du ein Modell l√§dst und wie es Eingaben in numerischer Form verarbeitet, um Vorhersagen auszugeben.

Dann schauen wir uns die Tokenizer-API an, die die andere Hauptkomponente der `pipeline()` Funktion ist. Tokenizer √ºbernehmen die ersten und letzten Verarbeitungsschritte, die Umwandlung von Text in seine numerische Form f√ºr das neuronale Netzwerk und die Umwandlung zur√ºck in Text, falls erforderlich. Abschlie√üend zeigen wir dir, wie du mehrere S√§tze gleichzeitig als vorbereitete Batch durch das Modell sendest, und beenden diesen Teil mit einem genaueren Blick auf die High-Level "tokenizer()" Funktion ab.

<Tip>
‚ö†Ô∏è Um von allen Funktionen des Model Hubs und der ü§ó Transformers zu profitieren, empfehlen wir dir ein <a href="https://huggingface.co/join">Konto zu erstellen</a>.
</Tip>
