<FrameworkSwitchCourse {fw} />

# Tokenizer[[tokenizers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb"},
]} />

{/if}

<Youtube id="VFp38yj8h3A"/>

Tokenizer sind eine der Kernkomponenten der NLP-Pipeline. Sie dienen einem Zweck: Sie übersetzen Text in ein Datenformat, das von einem Modell verarbeitet werden kann. Modelle können nur Zahlen verarbeiten, daher müssen Tokenizer unsere Texteingaben in numerische Daten umwandeln. In diesem Abschnitt werden wir genau untersuchen, was in der Tokenisierungspipeline passiert. 

Bei NLP-Aufgaben handelt es sich bei den Daten, die verarbeitet werden, in der Regel um Rohtext. Hier ist ein Beispiel für einen solchen Text:

```
Jim Henson war ein Puppenspieler
```

Modelle können jedoch nur Zahlen verarbeiten, also müssen wir einen Weg finden, den Rohtext in Zahlen umzuwandeln. Das ist es, was die Tokenizer tun, und es gibt viele Möglichkeiten, dies zu tun. Ziel ist es, die aussagekräftigste Darstellung zu finden - d. h. diejenige, die für das Modell am sinnvollsten ist - und, wenn möglich, die kleinstmögliche Darstellung.

Schauen wir uns einige Beispiele für Tokenisierungs-Algorithmen an und versuchen wir, einige der Fragen zu beantworten, die du vielleicht zur Tokenisierung hast.

## Wörter-basierend[[word-based]]

<Youtube id="nhJxYji1aho"/>

Der erste Typ von Tokenizer ist _wortbasiert_. Er ist im Allgemeinen sehr einfach einzurichten und mit nur wenigen Regeln zu verwenden, und er liefert oft gute Ergebnisse. In der folgenden Abbildung besteht das Ziel beispielsweise darin, den Rohtext in Wörter zu zerlegen und für jedes dieser Wörter eine numerische Darstellung zu finden:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" alt="Ein Beispiel für wortbasierte Tokenisierung."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg" alt="An example of word-based tokenization."/>
</div>

Es gibt verschiedene Möglichkeiten, den Text aufzuteilen. Wir könnten zum Beispiel Leerzeichen verwenden, um den Text in Wörter aufzuteilen, indem wir die Python-Funktion `split()` anwenden:

```py
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

Es gibt auch Varianten von Wort-Tokenizern, die zusätzliche Regeln für die Zeichensetzung enthalten. Mit dieser Art von Tokenizer können wir ziemlich große "Vokabulare" erhalten, wobei ein Vokabular durch die Gesamtzahl der einzigartigen Token in unserem Korpus definiert ist.

Jedem Wort wird eine ID zugewiesen, beginnend bei 0 und aufsteigend hin bis zur Größe des Vokabulars. Das Modell verwendet diese IDs, um jedes Wort zu identifizieren.

Wenn wir eine Sprache vollständig mit einem wortbasierten Tokenizer abdecken wollen, brauchen wir eine Kennung für jedes Wort in der Sprache, was eine riesige Menge an Token erzeugt. Zum Beispiel gibt es über 500.000 Wörter in der englischen Sprache, so dass wir, um eine Zuordnung von jedem Wort zu einer Input-ID zu erstellen, so viele IDs im Auge behalten müssten. Außerdem werden Wörter wie "dog" anders dargestellt als Wörter wie "dogs", und das Modell kann zunächst nicht wissen, dass "dog" und "dogs" ähnlich sind: Es wird die beiden Wörter als nicht verwandt identifizieren. Das Gleiche gilt für andere ähnliche Wörter wie "run" und "running", die das Modell anfangs nicht als ähnlich erkennt.

Schließlich benötigen wir ein benutzerdefiniertes Token, um Wörter zu repräsentieren, die nicht in unserem Wortschatz enthalten sind. Dies wird als "unbekanntes" Token bezeichnet und oft als "[UNK]" oder "&lt;unk&gt;" dargestellt. Es ist in der Regel ein schlechtes Zeichen, wenn der Tokenizer viele dieser Token produziert, da er keine sinnvolle Darstellung eines Wortes finden konnte und auf diesem Weg Informationen verloren gehen. Das Ziel bei der Erstellung des Vokabulars ist es, so vorzugehen, dass der Tokenizer so wenige Wörter wie möglich in das unbekannte Token einfügt.

Eine Möglichkeit, die Menge der unbekannten Token zu reduzieren, besteht darin, eine Ebene tiefer zu gehen und einen _zeichenbasierten_ Tokenizer zu verwenden.

## Zeichenbasierend[[character-based]]

<Youtube id="ssLq_EK2jLE"/>

Zeichenbasierte Tokenizer teilen den Text in Zeichen und nicht in Wörter auf. Dies hat zwei Hauptvorteile:

- Das Vokabular ist viel kleiner.
- Es gibt viel weniger Token außerhalb des Vokabulars (unbekannte Token), da jedes Wort aus Zeichen gebildet werden kann.

Aber auch hier stellen sich einige Fragen zu Leerzeichen und Zeichensetzung:

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" alt="An example of character-based tokenization."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg" alt="An example of character-based tokenization."/>
</div>

Auch dieser Ansatz ist nicht perfekt. Da die Darstellung nun auf Zeichen und nicht mehr auf Wörtern basiert, könnte man argumentieren, dass sie intuitiv weniger aussagekräftig ist: Jedes Zeichen bedeutet für sich genommen nicht viel, während das bei Wörtern der Fall ist. Dies ist jedoch je nach Sprache unterschiedlich; im Chinesischen zum Beispiel enthält jedes Zeichen mehr Informationen als ein Zeichen in einer lateinischen Sprache.

Außerdem ist zu bedenken, dass wir am Ende eine sehr große Menge an Token haben, die von unserem Modell verarbeitet werden müssen: Während ein Wort mit einem wortbasierten Tokenizer nur ein einziges Token wäre, kann es bei der Umwandlung in Zeichen leicht zu 10 oder mehr Token werden.

Um das Beste aus beiden Welten zu erhalten, können wir eine dritte Technik verwenden, die beide Ansätze kombiniert: *Teilwort-Tokenisierung*.

## Teilwort-Tokenisierung[[subword-tokenization]]

<Youtube id="zHvTiHr506c"/>

Algorithmen zur Tokenisierung von Teilwörtern beruhen auf dem Prinzip, dass häufig verwendete Wörter nicht in kleinere Teilwörter aufgeteilt werden sollten, seltene Wörter jedoch in sinnvolle Teilwörter zerlegt werden sollten.

Zum Beispiel könnte "ärgerlich" als seltenes Wort betrachtet und in "ärgerlich" und "ly" zerlegt werden. Beide werden als eigenständige Teilwörter wahrscheinlich häufiger vorkommen, während gleichzeitig die Bedeutung von "ärgerlich" durch die zusammengesetzte Bedeutung von "ärgerlich" und "ly" erhalten bleibt.

Das folgende Beispiel zeigt, wie ein Algorithmus zur Tokenisierung von Teilwörtern die Sequenz "Let's do tokenization!":

<div class="flex justify-center">
  <img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" alt="A subword tokenization algorithm."/>
  <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg" alt="A subword tokenization algorithm."/>
</div>

Im obigen Beispiel wurde "tokenization" in "token" und "ization" aufgeteilt, zwei Token, die eine semantische Bedeutung haben und gleichzeitig platzsparend sind (es werden nur zwei Token benötigt, um ein langes Wort darzustellen). Auf diese Weise können wir eine relativ gute Abdeckung mit kleinen Vokabularen und nahezu keinen unbekannten Token erreichen.

Dieser Ansatz ist besonders nützlich in agglutinierenden Sprachen wie Türkisch, wo man (fast) beliebig lange komplexe Wörter durch Aneinanderreihung von Teilwörtern bilden kann.

### Und noch mehr![[and-more]]

Es ist keine Überraschung, dass es noch viele weitere Techniken gibt. Um nur einige zu nennen:

- BPE auf Byte-Ebene, wie in GPT-2 verwendet
- WordPiece, wie es in BERT verwendet wird
- SentencePiece oder Unigram, wie sie in mehreren mehrsprachigen Modellen verwendet werden

Sie sollten nun genügend Wissen über die Funktionsweise von Tokenizern haben, um mit der API beginnen zu können.

## Laden und Speichern[[loading-and-saving]]

Das Laden und Speichern von Tokenizern ist genauso einfach wie bei Modellen. Eigentlich basiert es auf denselben zwei Methoden: `from_pretrained()` und `save_pretrained()`. Diese Methoden laden oder speichern den vom Tokenizer verwendeten Algorithmus (ein bisschen wie die *Architektur* des Modells) sowie sein Vokabular (ein bisschen wie die *Gewichte* des Modells).

Das Laden des BERT-Tokenizers, der mit demselben Checkpoint wie BERT trainiert wurde, erfolgt auf die gleiche Weise wie das Laden des Modells, nur dass wir die Klasse `BertTokenizer` verwenden:

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

{#if fw === 'pt'}
Ähnlich wie `AutoModel` greift die Klasse `AutoTokenizer` die richtige Tokenizer-Klasse in der Bibliothek auf der Grundlage des Checkpoint-Namens und kann direkt mit jedem Checkpoint verwendet werden:

{:else}
Ähnlich wie `TFAutoModel` greift die Klasse `AutoTokenizer` die richtige Tokenizer-Klasse in der Bibliothek auf der Grundlage des Checkpoint-Namens und kann direkt mit jedem Checkpoint verwendet werden:

{/if}

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Wir können nun den Tokenizer wie im vorherigen Abschnitt gezeigt verwenden:

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Das Speichern eines Tokenizers ist identisch mit dem Speichern eines Modells:

```py
tokenizer.save_pretrained("directory_on_my_computer")
```

Wir werden mehr über `token_type_ids` in [Kapitel 3](/course/chapter3) sprechen, und wir werden den Schlüssel `attention_mask` ein wenig später erklären. Zuerst wollen wir sehen, wie die `input_ids` generiert werden. Dazu müssen wir uns die Zwischenmethoden des Tokenizers ansehen.

## Enkodierung[[encoding]]

<Youtube id="Yffk5aydLzg"/>

Die Umwandlung von Text in Zahlen wird als _Enkodierung_ bezeichnet. Die Enkodierung erfolgt in einem zweistufigen Prozess: der Tokenisierung, gefolgt von der Umwandlung in Input-IDs.

Wie wir gesehen haben, besteht der erste Schritt darin, den Text in Wörter (oder Teile von Wörtern, Satzzeichen usw.) aufzuteilen, die gewöhnlich *Token* genannt werden. Es gibt mehrere Regeln, die diesen Prozess steuern können. Deshalb müssen wir den Tokenizer mit dem Namen des Modells instanziieren, um sicherzustellen, dass wir die gleichen Regeln verwenden, die beim Pre-Training des Modells verwendet wurden.

Der zweite Schritt besteht darin, diese Token in Zahlen umzuwandeln, so dass wir einen Tensor aus ihnen erstellen und sie in das Modell einspeisen können. Um dies zu tun, hat der Tokenizer ein *Vokabular*, das ist der Teil, den wir herunterladen, wenn wir ihn mit der Methode `from_pretrained()` instanziieren. Auch hier müssen wir das gleiche Vokabular verwenden, das beim Pretraining des Modells verwendet wurde.

Um ein besseres Verständnis für die beiden Schritte zu bekommen, werden wir sie getrennt voneinander untersuchen. Beachte aber, dass wir einige Methoden verwenden werden, die Teile der Tokenisierungspipeline separat ausführen, um dir die Zwischenergebnisse dieser Schritte zu zeigen, aber in der Praxis sollten Sie den Tokenizer direkt auf Ihren Inputs aufrufen (wie in Abschnitt 2 gezeigt).

### Tokenisierung[[tokenization]]

Der Tokenisierungsprozess wird durch die Methode `tokenize()` des Tokenizers durchgeführt:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

Die Ausgabe dieser Methode ist eine Liste von Zeichenketten oder Token:

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

Dieser Tokenizer ist ein Teilwort-Tokenizer: Er zerlegt die Wörter, bis er Token erhält, die durch sein Vokabular dargestellt werden können. Das ist hier der Fall mit `transformer`, das in zwei Token aufgeteilt wird: `transform` und `##er`.

### Von Token zu Input-IDs[[from-tokens-to-input-ids]]

Die Konvertierung in Input-IDs wird von der Tokenizer-Methode `convert_tokens_to_ids()` vorgenommen:

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier in this chapter.

Diese Ausgaben können dann, nachdem sie in den entsprechenden Tensor des jeweiligen Frameworks umgewandelt wurden, als Inputs für ein Modell verwendet werden, wie weiter oben in diesem Kapitel beschrieben.

<Tip>

✏️ **Versuche es selbst!** Wiederhole die beiden letzten Schritte (Tokenisierung und Umwandlung in Input-IDs) mit den Sätzen, die wir in Abschnitt 2 verwendet haben ("I've been waiting for a HuggingFace course my whole life." und "I hate this so much!"). Überprüfen Sie, ob Sie dieselben Input-IDs erhalten, die wir zuvor erhalten haben!

</Tip>

## Dekodierung[[decoding]]

Die *Dekodierung* erfolgt in umgekehrter Richtung: Aus Vokabular-Indexen wollen wir eine Zeichenkette erhalten. Dies kann mit der Methode `decode()` wie folgt geschehen:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

Beachte, dass die Methode `decode` nicht nur die Indexe wieder in Token umwandelt, sondern auch die Token, die Teil derselben Wörter waren, zu einem lesbaren Satz zusammenfasst. Dies wird äußerst nützlich sein, wenn wir Modelle verwenden, die neuen Text vorhersagen (entweder Text, der aus einer Eingabeaufforderung generiert wird, oder für Sequence-to-Sequence-Probleme wie Übersetzung oder Zusammenfassung).

Inzwischen solltest du die grundlegenden Operationen verstehen, die ein Tokenizer durchführen kann: Tokenisierung, Konvertierung in IDs und Konvertierung von IDs zurück in eine Zeichenkette. Wir haben jedoch nur die Spitze des Eisbergs angekratzt. Im folgenden Abschnitt werden wir diesen Ansatz an seine Grenzen bringen und uns ansehen, wie man diese überwinden kann.
