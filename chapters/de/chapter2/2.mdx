<FrameworkSwitchCourse {fw} />

# Die Pipeline[[behind-the-pipeline]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
Dies ist der erste Abschnitt, in dem sich der Inhalt leicht unterscheidet, je nachdem, ob du PyTorch oder TensorFlow verwendest. Du kannst den Schalter √ºber dem Titel verwenden, um die von dir bevorzugte Plattform auszuw√§hlen!
</Tip>

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

Beginnen wir mit einem Beispiel und sehen uns an, was hinter den Kulissen passiert, wenn wir den Code aus [Kapitel 1](/course/chapter1) ausf√ºhren:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

dabei erhalten wir:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Wie wir in [Kapitel 1](/course/chapter1) gesehen haben, fasst diese Pipeline drei Schritte zusammen: Die Vorverarbeitung des Textes, das Senden der Eingaben durch das Modell und die Nachbearbeitung:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="Die komplette CL Pipeline: Tokenisierung des Textes, Umwandlung zu IDs und die Inferenz mithilfe des Transformer Modell-Heads."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="Die komplette CL Pipeline: Tokenisierung des Textes, Umwandlung zu IDs und die Inferenz mithilfe des Transformer Modell-Heads."/>
</div>

Im folgenden gehen wir auf jeden dieser Schritte ein.

## Vorverarbeitung mit einem Tokenizer[[preprocessing-with-a-tokenizer]]

Wie andere neuronale Netze k√∂nnen Transformer-Modelle Rohtext nicht direkt verarbeiten. Daher besteht der erste Schritt unserer Pipeline darin, die Texteingaben in Zahlen umzuwandeln, die das Modell verstehen kann. Dazu verwenden wir einen *Tokenizer*, der verantwortlich ist f√ºr:

- Die Trennung der Eingabe in W√∂rter, Unterw√∂rter oder Symbole (wie zum Beispiel Satzzeichen), die als *Tokens* bezeichnet werden
- Die Zuordnung jedes Tokens zu einem Integer (ganze Zahl)
- Das Hinzuf√ºgen zus√§tzlicher Eingaben, die f√ºr das Modell n√ºtzlich sein k√∂nnen

Die gesamte Vorverarbeitung muss auf genau dieselbe Weise erfolgen wie beim Vortrainieren des Modells, daher m√ºssen wir diese Informationen zuerst vom [Model Hub](https://huggingface.co/models) herunterladen. Dazu verwenden wir die `from_pretrained()` Methode der Klasse `AutoTokenizer`. Unter Angabe des Checkpoint-Namens unseres Modells werden die Daten des Modell-Tokenizers automatisch abgerufen und zwischengespeichert (und werden daher nur bei der ersten Ausf√ºhrung des unten stehenden Codes heruntergeladen).

Da der Standard-Checkpoint der `sentiment-analysis` Pipeline `distilbert-base-uncased-finetuned-sst-2-english` ist (du findest die Modellkarte [hier](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), f√ºhren wir Folgendes aus:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

Sobald wir den Tokenizer geladen haben, k√∂nnen wir unsere S√§tze direkt an ihn √ºbergeben und erhalten ein Dictionary zur√ºck, das wir in unser Modell einspeisen k√∂nnen! Dann ist es nur noch N√∂tig die Liste der Eingabe-IDs in Tensoren umzuwandeln.

Du kannst ü§ó Transformers verwenden, ohne dir dabei Gedanken dar√ºber zu machen, welches ML-Framework als Backend verwendet wird; bei manchen Modellen kann es PyTorch oder TensorFlow sein, bei anderen Flax. Transformer-Modelle akzeptieren jedoch nur *Tensoren* als Eingabe. Wenn du zum ersten Mal von Tensoren h√∂rst, kannst du dir stattdessen NumPy-Arrays vorstellen. Ein NumPy-Array kann ein Skalar (0D), ein Vektor (1D), eine Matrix (2D) sein oder noch mehrere Dimensionen haben. Es handelt sich im Endeffekt um einen Tensor. Die Tensoren anderer ML-Frameworks verhalten sich √§hnlich und sind normalerweise so einfach zu instanziieren wie NumPy-Arrays.

Um anzugeben welche Art von Tensoren wir zur√ºckbekommen wollen (PyTorch, TensorFlow oder einfach NumPy), verwenden wir das Argument `return_tensors`:

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

Die Konzepte Padding und Truncation musst du zum jetzigen Zeitpunkt noch nicht begreifen; wir werden dir diese sp√§ter erkl√§ren. Am wichtigsten ist es f√ºr dich zu verstehen, dass du einen einzelnen Satz oder eine Liste von S√§tzen √ºbergeben kannst, sowie die Art der Tensoren, die du zur√ºckerhalten m√∂chtest. Solltest du keinen Typ angeben, erh√§ltst du als Ergebnis eine Liste von Listen.

{#if fw === 'pt'}

So sehen die Ergebnisse als PyTorch-Tensoren aus:

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

So sehen die Ergebnisse als TensorFlow-Tensoren aus:

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

Die Ausgabe selbst ist ein Dictionary bestehend aus den zwei Schl√ºsseln `input_ids` und `attention_mask`. `input_ids` enth√§lt zwei Zeilen mit Integers (eine f√ºr jeden Satz), mit denen sich die Tokens in jedem Satz eindeutig identifizieren lassen. Wir werden sp√§ter in diesem Kapitel erkl√§ren, um was es sich bei `attention_mask` handelt. 

## Das Modell: Schritt f√ºr Schritt[[going-through-the-model]]

{#if fw === 'pt'}
Wir k√∂nnen unser vortrainiertes Modell auf die selbe Weise herunterladen, wie wir es mit unserem Tokenizer getan haben. ü§ó Transformers verf√ºgt √ºber eine `AutoModel` Klasse, bei der es auch eine `from_pretrained()` Methode gibt:

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
Wir k√∂nnen unser vortrainiertes Modell auf die selbe Weise herunterladen, wie wir es mit unserem Tokenizer getan haben. ü§ó Transformers verf√ºgt √ºber eine `TFAutoModel` Klasse, bei der es auch eine `from_pretrained` Methode gibt:

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

In diesem Code-Snippet haben wir denselben Checkpoint heruntergeladen, den wir zuvor in unserer Pipeline verwendet haben (er sollte daher bereits gecached sein) und damit ein Modell instanziiert.

Diese Architektur enth√§lt nur das Transformer-Basismodul: Bei einer Eingabe gibt es das aus, was wir *Hidden States* oder auch *Features* nennen. F√ºr jede Modelleingabe erhalten wir ein hochdimensionalen Vektor, der das **kontextuelle Verst√§ndnis dieser Eingabe durch das Transformer-Modell** darstellt.

Mache dir keine Sorgen, wenn das noch keinen Sinn f√ºr dich ergibt. Wir werden alles sp√§ter erkl√§ren.

Auch wenn diese Hidden States f√ºr sich genommen n√ºtzlich sein k√∂nnen, sind sie in der Regel Eingaben f√ºr einen anderen Teil des Modells, den sogenannten *Kopf*. In [Kapitel 1](/course/chapter1) h√§tten die verschiedenen Aufgaben mit derselben Architektur ausgef√ºhrt werden k√∂nnen, aber jeder dieser Aufgaben ist ein anderer Kopf zugeordnet.

### Ein hochdimensionaler Vektor?[[a-high-dimensional-vector]]

Der vom Transformer-Modul ausgegebene Vektor ist normalerweise gro√ü. Er hat im Allgemeinen drei Dimensionen:

- **Batch size**: Die Anzahl der gleichzeitig verarbeiteten Sequenzen (in unserem Beispiel 2).
- **Sequenzl√§nge**: Die L√§nge der numerischen Darstellung der Sequenz (16 in unserem Beispiel).
- **Hidden size**: Die Vektordimension der einzelnen Modelleingaben.

Wegen des letzten Wertes wird sie als "hochdimensional" bezeichnet. Die Dimension der Hidden size kann sehr gro√ü sein (768 ist bei kleineren Modellen √ºblich, und bei gr√∂√üeren Modellen kann sie bis zu 3072 sein).

Wir k√∂nnen dies sehen, wenn wir die vorverarbeiteten Eingaben in unser Modell einspeisen:

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

Beachte, dass sich die Ausgaben von ü§ó Transformers-Modellen wie `nameduples` oder Dictionaries verhalten. Du kannst auf die Elemente √ºber Attribute zugreifen (wie wir es getan haben) oder √ºber Schl√ºssel (`outputs["last_hidden_state"]`), oder sogar √ºber den Index (`outputs[0]`), wenn du genau wei√üt, wonach du suchst.

### Modell-K√∂pfe[[model-heads-making-sense-out-of-numbers]]

Die Modell-K√∂pfe nehmen den hochdimensionalen Vektor der Hidden States als Eingabe und projizieren sie auf eine andere Dimension. Sie bestehen in der Regel aus einer oder mehreren linearen Ebenen:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

Die Ausgabe des Transformer-Modells wird direkt an den Modell-Kopf zur Verarbeitung weitergeleitet.

In diesem Diagramm wird das Modell durch seine Embedding-Ebene und die nachfolgenden Ebenen dargestellt. Die Embedding-Ebene wandelt jede Eingabe-ID in der tokenisierten Eingabe in einen Vektor um, der das zugeh√∂rige Token darstellt. Die nachfolgenden Ebenen manipulieren diese Vektoren mit Hilfe des Attention-Mechanismus, um die endg√ºltige Darstellung der S√§tze zu erzeugen.

Es gibt viele verschiedene Architekturen in ü§ó Transformers, die jeweils auf eine bestimmte Aufgabe zugeschnitten sind. Um nur einige davon zu nennen:

- `*Model` (retrieve the hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- and others ü§ó

{#if fw === 'pt'}
F√ºr unser Beispiel ben√∂tigen wir ein Modell mit einem Sequenzklassifizierungskopf (um die S√§tze als positiv oder negativ klassifizieren zu k√∂nnen). Wir werden also nicht die Klasse `AutoModel` verwenden, sondern `AutoModelForSequenceClassification`:

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
F√ºr unser Beispiel ben√∂tigen wir ein Modell mit einem Sequenzklassifizierungskopf (um die S√§tze als positiv oder negativ klassifizieren zu k√∂nnen). Wir werden also nicht die Klasse `TFAutoModel` verwenden, sondern `TFAutoModelForSequenceClassification`:

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

Wenn wir nun die Form unserer Eingaben betrachten, ist die Dimensionalit√§t deutlich geringer: Der Modell-Kopf nimmt als Eingabe die hochdimensionalen Vektoren, die wir zuvor gesehen haben, und gibt Vektoren aus, die zwei Werte enthalten (einen pro Label):

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

Da wir nur zwei S√§tze und zwei Bezeichnungen haben, hat das Ergebnis unseres Modells die Form 2 x 2.

## Nachbearbeitung der Ausgabe[[postprocessing-the-output]]

Die Werte, die wir als Ausgabe unseres Modells erhalten, ergeben auf den ersten Blick nicht unbedingt einen Sinn. Schauen wir uns das mal an:

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

Unser Modell gibt `[-1.5607, 1.6123]` f√ºr den ersten Satz und `[ 4.1692, -3.3464]` f√ºr den zweiten Satz aus. Dabei handelt es sich nicht um Wahrscheinlichkeiten, sondern um *Logits*, d. h. die rohen, nicht normierten Werte, die von der letzten Ebene des Modells ausgegeben werden. Um diese in Wahrscheinlichkeiten umzuwandeln, muss auf sie die [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) Funktion angewendet werden (alle ü§ó Transformers-Modelle geben die Logits aus, da die Verlustfunktion f√ºr das Training im Allgemeinen die letzte Aktivierungsfunktion, wie SoftMax, mit der eigentlichen Verlustfunktion, wie Kreuzentropie, kombiniert):

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

Jetzt k√∂nnen wir sehen, dass das Modell `[0.0402, 0.9598]` f√ºr den ersten Satz und `[0.9995, 0.0005]` f√ºr den zweiten Satz vorausgesagt hat. Dabei handelt es sich um Wahrscheinlichkeitswerte.

Um das dazugeh√∂rige Label f√ºr jede Position zu erhalten, k√∂nnen wir uns das Attribut `id2label` der Modellkonfiguration ansehen (mehr dazu im n√§chsten Abschnitt):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

Wir k√∂nnen nun feststellen, dass das Modell Folgendes vorausgesagt hat:
 
- Erster Satz: NEGATIV: 0.0402, POSITIV: 0.9598
- Zweiter Satz: NEGATIV: 0.9995, POSITIV: 0.0005

Wir haben die drei Schritte der Pipeline erfolgreich reproduziert: Die Vorverarbeitung mit Tokenizern, das Senden der Eingaben durch das Modell und die Nachverarbeitung! Jetzt werden wir uns etwas Zeit nehmen, um in jeden dieser Schritte noch tiefer einzutauchen.

<Tip>

‚úèÔ∏è **Probiere es selbst aus!** W√§hle zwei (oder mehr) deiner eigenen Texte und lasse sie durch die `sentiment-analysis` Pipeline laufen. Dann wiederhole die Schritte, die wir dir hier gezeigt haben und pr√ºfe, ob du die gleichen Ergebnisse erh√§ltst!

</Tip>
