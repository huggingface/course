<FrameworkSwitchCourse {fw} />

# Vortrainierte Modelle teilen

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section3_tf.ipynb"},
]} />

{/if}

Demn√§chst schauen wir uns an, wie man am einfachsten vortrainierte Modelle auf dem ü§ó Hub teilen kann.
Es gibt schon Tools und Hilfsmittel, die das Teilen und Updaten von Modellen auf dem Hub vereinfachen. Die werden wir gleich unten explorieren.

<Youtube id="9yY3RB_GSPM"/>

Wir empfehlen allen Nutzer*innen, die Modelle trainieren, dass sie der Communinity beitragen, indem sie Modelle teilen. Selbst die Modelle, die auf sehr spezifische Datens√§tze trainiert wurden, werden anderen Nutzer*innen helfen, weil man Zeit und Rechenressourcen spart und Zugang zu n√ºtzlichen Trainingsartifakten bekommt. Also eventuell kannst du auch von der Arbeit anderer Nutzer*innen auch profitieren!

Es gibt drei Wege, um Repositories zu neuen Modellen zu kreieren:

- Mittels der `push_to_hub` API
- Mittels der `huggingface_hub` Python Bibliothek
- Mittels der Web-Oberfl√§che

Nachdem du einen Repository erstellst hast, kannst du die Dateien √ºber git und git-lfs hochladen. Demn√§chst zeigen wir dir die genauen Schritte, um Modell-Repositories zu erstellenund Dateien hochzuladen.


## Hochladen mit der `push_to_hub` API

{#if fw === 'pt'}

<Youtube id="Zh0FfmVrKX0"/>

{:else}

<Youtube id="pUh5cGmNV8Y"/>

{/if}

Die einfachste Variante, um Dateien auf den Hub hochzuladen, ist mittels der `push_to_hub` API. Bevor du weitermachst, must du einen Autentifizierungstoken generieren, damit die `huggingface_hub` API wei√üt, wer du bist und auf welche Namespaces du zugreifen darfst. Stell sicher, dass du in einer Umgebung mit `transformers` installiert bist (siehe [Setup](/course/chapter0)). Wenn du auf einem Notebook bist, kannst du diese Funktion benutzen, um dich einzuloggen:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Im Terminal kannst folgendes ausf√ºhren:

```bash
huggingface-cli login
```

In beiden F√§llen solltest du nach deinem Username und Passwort gefragt werden. Das sind die selben, mit denen du dich auf dem Hub einloggst. Solltest du noch kein Hub-Profil haben, musst du erstmal eins [hier](https://huggingface.co/join) erstellen.

Gro√üartig! Nun hast du deinen Autentifizierungstoken in deinem Cache-Ordner gespeichert. Lass uns ein paar Repositories erstellen!

{#if fw === 'pt'}

If you have played around with the  API to train a model, the easiest way to upload it to the Hub is to set  when you define your 
Wenn du schon Modelle mit der `Trainer` API trainiert hast, dann ist der einfachste Weg, um Modelle hochzuladen, das Argument `push_to_hub=True` in `TrainingArguments` einzustellen.

```py
from transformers import TrainingArguments

training_args = TrainingArguments(
    "bert-finetuned-mrpc", save_strategy="epoch", push_to_hub=True
)
```

Wenn du `trainer.train()` aufrufst, l√§dt der `Trainer` das Modell auf den Hub zu dem Repository in deinem Namespace hoch. Das passiert jedes Mal, wenn das Modell gespeichert wird (in diesem Beispiel jede Epoche). Der Repository wird so benannt werden, wie der Output-Ordner, den du gew√§hlt hast (hier `bert-finetuned-mrpc`). Nat√ºrlich kannst du dir aber einen anderen Namen ausdenken und mit `hub_model_id = "a_different_name"` setzen.

Um dein Modell zu einer Organisation, wovon du Mitglied bist, hochzuladen, kannst du einfach `hub_model_id = "my_organization/my_repo_name"` mit eingeben.

Wenn das Training durch ist, must du noch einmal `trainer.push_to_hub()` ausf√ºhren, um die letzte Version deines Modells hochzuladen. Das wird auch eine Modell-Karte generieren, auf der die relevanten Metadaten mit den benutzten Hyperparametern und Evaluierungsergebnissen! Hier ist ein Beispiel von dem Inhalt, den du auf so einer Modell-Karte finden kannst:

<div class="flex justify-center">
  <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png" alt="An example of an auto-generated model card." width="100%"/>
</div>

{:else}

Wenn du f√ºr das Modell-Training Keras benutzt, ist der einfachste Weg, um das Modell aud den Hub hochzuladen, den `PushToHubCallback` zu setzen, wenn du `model.fit()` aufrufst.

```py
from transformers import PushToHubCallback

callback = PushToHubCallback(
    "bert-finetuned-mrpc", save_strategy="epoch", tokenizer=tokenizer
)
```

Danach must du noch `callbacks=[callback]` beim `model.fit()` Aufruf setzen. 
Der Callback wird das Modell auf den Hub hochladen und zwar zu einem Repository in deinem Namespace. Das passiert jedes Mal, wenn das Modell gespeichert wird (in diesem Beispiel jede Epoche). Der Repository wird so benannt werden, wie der Output-Ordner, den du gew√§hlt hast (hier `bert-finetuned-mrpc`). Nat√ºrlich kannst du dir aber einen anderen Namen ausdenken und mit `hub_model_id = "a_different_name"` setzen.

Um dein Modell zu einer Organisation, wovon du Mitglied bist, hochzuladen, kannst du einfach `hub_model_id = "my_organization/my_repo_name"` mit eingeben.


{/if}

Auf einer tieferen Ebene kann man auf Modelle, Tokenizers und Konfigurationen auf dem Model-Hub direkt zugreifen, indem man die Methode `push_to_hub()` benutzt. 
Diese Methode k√ºmmert sich sowohl um das Erstellen vom Repository als auch das Pushen (Hochladen) von Modell- und Tokenizer-Dateien auf den Repository. Also da ist kein manueller Schritt notwendig (im Gegensatz zu den APIs, die wir demn√§chst sehen werden).

Um uns eine Vorstellung zu schaffen, wie es funktioniert, lass uns zuerst ein Modell und einen Tokenizer initialisieren:

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{/if}

Dir steht frei, was du mit diesen machst, z.B. Tokens zum Tokenizer hinzuzuf√ºgen, das Modell zu trainineren oder zu finetunen. Wenn du mit dem Modell, Gewichten und Tokenizer zufrieden bist, kannst du die Methode `push_to_hub()` vom `model` Objekt benutzten:

```py
model.push_to_hub("dummy-model")
```
Das wird den neuen Repository `dummy-model` in deinem Profil erstellen und den mit deinen Model-Dateien bef√ºllen. Mach das gliche mit dem Tokenizer, sodass jetzt alle Dateien in diesem Repository verf√ºgbar sind.

```py
tokenizer.push_to_hub("dummy-model")
```
Wenn du Teil einer Organisation bist, kannst du einfach das Argument `organization` mit eingeben, um die Artifakte auf den Namespace dieser Organisation hochzuladen. 

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface")
```

Wenn du einen bestimmten Hugging Face Token benutzten m√∂chtest, kannst du ihn auch in der Methode `push_to_hub()` spezifizieren:

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface", use_auth_token="<TOKEN>")
```

Nun geh auf den Model Hub, um dein hochgeladenes Modell zu finden: *https://huggingface.co/user-or-organization/dummy-model*.

Click auf den Tab "Files and versions" und da solltest du die Dateien finden, die auf diesem Screenshot zu sehen sind:

{#if fw === 'pt'}
<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</div>
{:else}
<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model_tf.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</div>
{/if}

<Tip>

‚úèÔ∏è **Probier das selber aus!** Lade das Modell und den Tokenizer vom Checkpoint `bert-base-cased` mit der Methode `push_to_hub()` hoch. √úberpr√ºfe, dass der Repository auf deiner Seite richtig erscheint, bevor du den l√∂schst.

</Tip>

Wie du schon gesehen hast, akzeptiert die Methode `push_to_hub()` mehrere Argumente. Dies erlaub das Hochladen auf den Namespace eines spezifischen Repositorys oder einer Organisation, sowie die M√∂glichkeit, einen anderen API Token zu benutzten. Wir empfehlen dir, die Dokumentation der Methode direkt auf [ü§ó Transformers documentation](https://huggingface.co/transformers/model_sharing.html) zu lesen, um dir eine Vorstellung zu schaffen, was alles damit m√∂glich ist.

Die `push_to_hub()` Methode funktioniert im Hintergrund mit der Python Bibliothek [`huggingface_hub`](https://github.com/huggingface/huggingface_hub), die eine direkte API zum Hugging Face Hub anbietet. Sie ist auch drin in der ü§ó Transformers Bibliothek und mehreren anderen Machine Learning Bibliotheken, z.B. [`allenlp`](https://github.com/allenai/allennlp). Obwohl wir in diesem Kapitel den Fokus auf die Integration mit ü§ó Transformers legen, kannst du es in deinen eigenen Code bzw. eigene Bibliothek relativ einfach integrieren. Spring auf den letzten Part, um zu erfahren, wie man Dateien auf einen frisch erstellten Repository hochladen kann!

## Verwendung der `huggingface_hub` Python Bibliothek
Die `huggingface_hub` Python Bibliothek ist ein Python Packet, das einige Werkzeuge f√ºr das Nutzen von Modell- und Datasethub anbietet.  Es bietet simple Methoden und Klassen f√ºr g√§ngige Aufgaben, z.B. um Information zu Repositories auf dem Hub zu bekommen oder um sie zu Verwalten. Es bietet auch simple auf git basierende APIs, um die Inhalte von solchen Repositories zu verwalten sowie um den Hub in deine Projekte und Bibliotheken zu integrieren.

√Ñhnlich wie bei der Verwendung der`push_to_hub` API ist es bei diesen Aktionen erforderlich, dass dein API Token schon in deinem Cache gespeichert ist. Daf√ºr musst du den `login` Befehl aus der CLI ausf√ºhren so wie in dem vorherigen Teil erkl√§rt wurde (nochmal: Vergiss nicht, das `!` Zeichen vor die Befehle zu setzen, wenn du im Google Colab arbeitest).

```bash
huggingface-cli login
```

Die `huggingface_hub` Bibliothek bietet mehrere n√ºtzliche Methoden und Klassen an. Erstens gibt es einige Methoden, um das Erstellen, L√∂schen, usw. von Repositories durchzuf√ºhren:


```python no-format
from huggingface_hub import (
    # User-Management
    login,
    logout,
    whoami,

    # Repository erstellen und managen
    create_repo,
    delete_repo,
    update_repo_visibility,

    # Methoden, um inhaltliche Information abzufragen/abzu√§ndern
    list_models,
    list_datasets,
    list_metrics,
    list_repo_files,
    upload_file,
    delete_file,
)
```

Au√üerdem gibt es die sehr m√§chtige `Repository` Klasse, um einen lokalen Repository zu managen. Demn√§chst werden wir uns mit diesen Methoden und dieser Klasse besch√§ftigen, um zu verstehen, wie man die am besten nutzt.

Mit der `create_repo` Methode kann ein neuer Repository auf dem Hub erstellt werden:

```py
from huggingface_hub import create_repo

create_repo("dummy-model")
```

Das erstellt den Repository `dummy-model` unter deinem Namespace. Wenn du m√∂chtest, kannst du auch die Organisation spezifizieren, zu der der Repository geh√∂ren sollte, indem du das `organization` Argument setzt:

```py
from huggingface_hub import create_repo

create_repo("dummy-model", organization="huggingface")
```

Das erstellt den Repository `dummy-model` unter dem `huggingface` Namespace ‚Äì angenommen du geh√∂rst zu dieser Organisation.
Andere eventuell n√ºtzliche Argumente sind:

- `private`: um zu spezifizieren, ob der Repository f√ºr andere sichtbar sein sollte oder nicht.
- `token`: um den Token, der im Zwischenspeicher (Cache) liegt, mit einem neuen Token zu √ºberscheiben.
- `repo_type`: zum Ausw√§hlen, ob du einen `dataset` oder einen `space` anstatt von einem Modell kreieren m√∂chtest. Erlaubte Werte sind `"dataset"` und `"space"`.

Nachdem der Repository erstellt wurde, k√∂nnen wir Dateien hinzuf√ºgen! Spring zum n√§chsten Abschnitt, um drei Varianten dazu zu lernen, wie man das machen kann.

## Mit der Webinterface

Die Webinterface bietet Tools an, um Repositories direkt auf dem Hub zu managen. Damit kannst du ganz einfach Repositories erstellen, Dateien hinzuf√ºgen (sogar gro√üe Dateien), Modelle explorieren, Unterschiede ("diffs") visualisieren und viel mehr.

Um einen Repository zu erstellen, geh auf [huggingface.co/new](https://huggingface.co/new):

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png" alt="Beispiel vom Modell, mit dem man einen Repository erstellen kann." width="80%"/>
</div>

Erstens muss man den Besitzer vom Repository eingeben: Das kannst entweder du selbst oder jede andere Person von der Organisation sein, zu der du geh√∂rst. Wenn du eine Organisation ausw√§hlst, wird das Modell auf der Seite der Organisation pr√§sentiert und jedes Mitglied der Organisation wird zu diesem Repository beitragen k√∂nnen.

Als n√§chstes gib den Namen deines Modells ein. So wird der Repository auch hei√üen. Zuletzt kannst du spezifizieren, ob das Modell √∂ffentlich oder privat sein soll. Private Modelle sind von der √ñffentlichkeit unsichtbar.

Nach der Erstellung des Repositorys solltest du so eine Seite sehen k√∂nnen:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/empty_model.png" alt="Leeres Modell nach der Erstellung des Repositorys." width="80%"/>
</div>

Hier wird dein Modell gehostet. Um mit dem Auff√ºllen zu beginnen, kannst du direkt √ºber die Weboberfl√§che eine README-Datei hinzuf√ºgen.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/dummy_model.png" alt="The README file showing the Markdown capabilities." width="80%"/>
</div>

Die README-Datei ist im Markdown Format ‚Äî du kannst dich damit gerne austoben!
Der dritte Teil dieses Kapitels zielt darauf hin, eine "model card" (Steckbrief) zu bauen. Steckbriefe haben eine entscheidende Relevanz, um dein Modell wertvoll zu machen, denn du kannst dort anderen erz√§hlen, was das Modell kann.

Wenn du dir den "Files and versions" Tab anschaust, wirst du sehen, dass noch nicht viele Dateien darauf sind ‚Äì n√§mlich nur die von dir eben kreierte *README.md* und die *.gitattributes* (wo gro√üe Dateien geloggt werden).


<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/files.png" alt="The 'Files and versions' tab only shows the .gitattributes and README.md files." width="80%"/>
</div>

Gleich werden wir sehen, wie wir neue Dateien hinzuf√ºgen k√∂nnen.

## Hochladen von Modell-Dateien

Das System zum Managen der Dateien auf Hugging Face Hub basiert auf git f√ºr normale Dateien und auf git-lfs ([Git Large File Storage](https://git-lfs.github.com/)) f√ºr gr√∂√üere Dateien.

Im n√§chsten Teil schauen wir uns drei M√∂glichkeitein an, um Dateien mittels `huggingface_hub` und git-Befehle auf den Hub hochzuladen.

### Die `upload_file` Variante

Um `upload_file` zu verwenden, muss man nicht unbedingt git und git-lfs installiert haben. Die Funktion l√§dt Dateien auf den ü§ó Hub mittels HTTP POST Anfragen. Eine Einschr√§nkunf dieser Variante ist, dass man nur mit Dateien unter 5GB gro√ü arbeiten kann.
Wenn deine Dateien gr√∂√üer als 5GB sind, nutz eine von den folgenden zwei Methoden.

Die API kann folgenderma√üen benutzt werden:

```py
from huggingface_hub import upload_file

upload_file(
    "<path_to_file>/config.json",
    path_in_repo="config.json",
    repo_id="<namespace>/dummy-model",
)
```
Das wird die `config.json` Datei in `<path_to_file>` auf das Root-Verzeichnis vom Repository als `config.json` vom `dummy-model` Repository.
Andere n√ºtzliche Argumente :

- `token`, um den Token zu √ºberscheiben, der in deinem Cache gespeichert ist
- `repo_type`, wenn du anstatt von einem Modell Dateien auf einen `dataset` oder `space` hochladen m√∂chtest. Valide Werte sind `"dataset"` und `"space"`.


### Die `Repository` Klasse

Die `Repository` Klasse verwaltet einen lokalen Repository so wie git. Sie abstrahiert aber die meisten schwierigen Punkte, auf die man sto√üen w√ºrde, wenn man eine √§hnliche Funktionalit√§t mit git erreichen m√∂chte.

Diese Klasse braucht git und git-lfs im System schon installiert. Also stell sicher, dass du git-lfs installiert hast (siehe [hier](https://git-lfs.github.com/) f√ºr Installationsanweisungen) und richte alles ein, bevor du loslegst.

Um mit dem Repository rumspielen zu starten, k√∂nnen wir den in einem lokalen Ordner initialisieren, in dem wir den Remote-Repository klonen:

```py
from huggingface_hub import Repository

repo = Repository("<path_to_dummy_folder>", clone_from="<namespace>/dummy-model")
```

Das hat den Ordner `<path_to_dummy_folder>` in unserem Arbeitsverzeichnis erstellt. Dieser Ordner enth√§lt bisher nur die `.gitattributes` Datel, da diese die einzige Datei ist, die wir mit `create_repo` kreiert haben.

Ab jetzt k√∂nnen mehrere g√§ngige Methoden benutzten:

```py
repo.git_pull()
repo.git_add()
repo.git_commit()
repo.git_push()
repo.git_tag()
```

Und andere Optionen auch! Wir empfehlen, dass du dir die Dokumentation zu `Repository`, die dir [hier](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub#advanced-programmatic-repository-management) zur Verf√ºgung steht, anschaust, um dir eine √úbersicht aller verf√ºgbaren Methoden zu verschaffen.

Bisher haben wir ein Modell und einen Tokenizer, die wir gerne auf den Hub pushen w√ºrden. Wir haben auch den Repository geklont, sodass wir die Dateien in dem Repository speichern k√∂nnen.

Zuerst stellen wir sicher, dass unser lokaler Repository einen aktuellen Stand hat, in dem wir die letzten √Ñnderungen pullen:

```py
repo.git_pull()
```

Wenn das durch ist, speichern wir die Dateien vom Modell und Tokenizer:

```py
model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```

Der Pfad `<path_to_dummy_folder>` beinhaltet jetzt alle Modell- und Tokenizerdateien. Wir folgen dem g√§ngigen Git-Workflow, indem wir die Dateien in die "staging area" bringen, wir committen und pushen sie auf den hub:

```py
repo.git_add()
repo.git_commit("Add model and tokenizer files")
repo.git_push()
```

Gl√ºckwunsch! Du hast gerade deine ersten Dateien auf den Hub hochgeladen.

# TODO
### The git-based approach

This is the very barebones approach to uploading files: we'll do so with git and git-lfs directly. Most of the difficulty is abstracted away by previous approaches, but there are a few caveats with the following method so we'll follow a more complex use-case.

Using this class requires having git and git-lfs installed, so make sure you have [git-lfs](https://git-lfs.github.com/) installed (see here for installation instructions) and set up before you begin. 

First start by initializing git-lfs:

```bash
git lfs install
```

```bash
Updated git hooks.
Git LFS initialized.
```

Once that's done, the first step is to clone your model repository:

```bash
git clone https://huggingface.co/<namespace>/<your-model-id>
```

My username is `lysandre` and I've used the model name `dummy`, so for me the command ends up looking like the following:

```
git clone https://huggingface.co/lysandre/dummy
```

I now have a folder named *dummy* in my working directory. I can `cd` into the folder and have a look at the contents:

```bash
cd dummy && ls
```

```bash
README.md
```

If you just created your repository using Hugging Face Hub's `create_repo` method, this folder should only contain a hidden `.gitattributes` file. If you followed the instructions in the previous section to create a repository using the web interface, the folder should contain a single *README.md* file alongside the hidden `.gitattributes` file, as shown here.

Adding a regular-sized file, such as a configuration file, a vocabulary file, or basically any file under a few megabytes, is done exactly as one would do it in any git-based system. However, bigger files must be registered through git-lfs in order to push them to *huggingface.co*. 

Let's go back to Python for a bit to generate a model and tokenizer that we'd like to commit to our dummy repository:

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Do whatever with the model, train it, fine-tune it...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Do whatever with the model, train it, fine-tune it...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{/if}

Now that we've saved some model and tokenizer artifacts, let's take another look at the *dummy* folder:

```bash
ls
```

{#if fw === 'pt'}
```bash
config.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json
```

If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*pytorch_model.bin*) is the only outlier, at more than 400 MB.

{:else}
```bash
config.json  README.md  sentencepiece.bpe.model  special_tokens_map.json  tf_model.h5  tokenizer_config.json  tokenizer.json
```

If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*t5_model.h5*) is the only outlier, at more than 400 MB.

{/if}

<Tip>
‚úèÔ∏è When creating the repository from the web interface, the *.gitattributes* file is automatically set up to consider files with certain extensions, such as *.bin* and *.h5*, as large files, and git-lfs will track them with no necessary setup on your side.
</Tip> 

We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:

```bash
git add .
```

We can then have a look at the files that are currently staged:

```bash
git status
```

{#if fw === 'pt'}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
	new file:   config.json
	new file:   pytorch_model.bin
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{:else}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
  	new file:   config.json
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tf_model.h5
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{/if}

Similarly, we can make sure that git-lfs is tracking the correct files by using its `status` command:

```bash
git lfs status
```

{#if fw === 'pt'}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	pytorch_model.bin (LFS: 35686c2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

We can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiece.bpe.model*, which have `LFS`. Great!

{:else}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tf_model.h5 (LFS: 86fce29)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

We can see that all files have `Git` as a handler, except *t5_model.h5*, which has `LFS`. Great!

{/if}

Let's proceed to the final steps, committing and pushing to the *huggingface.co* remote repository:

```bash
git commit -m "First model version"
```

{#if fw === 'pt'}
```bash
[main b08aab1] First model version
 7 files changed, 29027 insertions(+)
  6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 pytorch_model.bin
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{:else}
```bash
[main b08aab1] First model version
 6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tf_model.h5
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{/if}

Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:

```bash
git push
```

```bash
Uploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.
Enumerating objects: 11, done.
Counting objects: 100% (11/11), done.
Delta compression using up to 12 threads
Compressing objects: 100% (9/9), done.
Writing objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.
Total 9 (delta 1), reused 0 (delta 0), pack-reused 0
To https://huggingface.co/lysandre/dummy
   891b41d..b08aab1  main -> main
```

{#if fw === 'pt'}
If we take a look at the model repository when this is finished, we can see all the recently added files:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</div>

The UI allows you to explore the model files and commits and to see the diff introduced by each commit:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffs.gif" alt="The diff introduced by the recent commit." width="80%"/>
</div>
{:else}
If we take a look at the model repository when this is finished, we can see all the recently added files:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model_tf.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</div>

The UI allows you to explore the model files and commits and to see the diff introduced by each commit:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffstf.gif" alt="The diff introduced by the recent commit." width="80%"/>
</div>
{/if}
