<FrameworkSwitchCourse {fw} />

# နိဒါန်း[[introduction]]

<CourseFloatingBanner
    chapter={7}
    classNames="absolute z-10 right-0 top-0"
/>

[Chapter 3](/course/chapter3) မှာ သင်ဟာ text classification အတွက် model တစ်ခုကို fine-tune လုပ်နည်းကို တွေ့ခဲ့ရပါတယ်။ ဒီအခန်းမှာတော့၊ traditional NLP models တွေနဲ့ modern LLMs တွေ နှစ်ခုလုံးနဲ့ အလုပ်လုပ်ဖို့အတွက် မရှိမဖြစ်လိုအပ်တဲ့ အောက်ပါ common language tasks တွေကို ကျွန်တော်တို့ ကိုင်တွယ်ဖြေရှင်းသွားမှာပါ။ 

- Token classification
- Masked language modeling (BERT ကဲ့သို့)
- အနှစ်ချုပ်ဖော်ပြခြင်း (Summarization)
- ဘာသာပြန်ခြင်း (Translation)
- Causal language modeling pretraining (GPT-2 ကဲ့သို့)
- မေးခွန်းဖြေဆိုခြင်း (Question answering)

ဒီအခြေခံ tasks တွေက Large Language Models (LLMs) တွေ ဘယ်လိုအလုပ်လုပ်တယ်ဆိုတာရဲ့ အခြေခံအုတ်မြစ်တွေကို ဖွဲ့စည်းထားပြီး၊ ဒါတွေကို နားလည်ထားခြင်းက ဒီနေ့ခေတ်ရဲ့ အဆင့်မြင့်ဆုံး language models တွေနဲ့ ထိထိရောက်ရောက် အလုပ်လုပ်ဖို့အတွက် အရေးကြီးပါတယ်။

{#if fw === 'pt'}

ဒါတွေကို လုပ်ဆောင်ဖို့အတွက်၊ [Chapter 3](/course/chapter3) မှာ သင်သင်ယူခဲ့တဲ့ `Trainer` API နဲ့ 🤗 Accelerate library အကြောင်း၊ [Chapter 5](/course/chapter5) မှာ 🤗 Datasets library အကြောင်း၊ ပြီးတော့ [Chapter 6](/course/chapter6) မှာ 🤗 Tokenizers library အကြောင်း သင်သင်ယူခဲ့တဲ့ အရာအားလုံးကို အကျိုးယူဖို့ လိုအပ်ပါလိမ့်မယ်။ [Chapter 4](/course/chapter4) မှာ လုပ်ခဲ့သလို ကျွန်တော်တို့ရဲ့ ရလဒ်တွေကို Model Hub ကိုလည်း upload လုပ်သွားမှာဖြစ်တဲ့အတွက်၊ ဒါဟာ အရာအားလုံး ပေါင်းစပ်လာမယ့် အခန်းပါပဲ!

အပိုင်းတစ်ခုစီကို သီးခြားစီ ဖတ်ရှုနိုင်ပြီး `Trainer` API ဒါမှမဟုတ် သင်ကိုယ်တိုင်ရဲ့ training loop ကို 🤗 Accelerate အသုံးပြုပြီး model တစ်ခုကို ဘယ်လို train လုပ်ရမယ်ဆိုတာ ပြသပေးပါလိမ့်မယ်။ အပိုင်းနှစ်ခုထဲက စိတ်ဝင်စားရာ တစ်ခုကို ကျော်သွားပြီး အဲဒီတစ်ခုတည်းကိုပဲ အာရုံစိုက်နိုင်ပါတယ်။ `Trainer` API က model ကို fine-tuning လုပ်တာ ဒါမှမဟုတ် နောက်ကွယ်မှာ ဘာတွေဖြစ်နေလဲဆိုတာကို စိုးရိမ်စရာမလိုဘဲ train လုပ်တာအတွက် အကောင်းဆုံးဖြစ်ပြီး၊ `Accelerate` နဲ့ training loop ကတော့ သင်လိုချင်တဲ့ အစိတ်အပိုင်းတိုင်းကို ပိုမိုလွယ်ကူစွာ customize လုပ်နိုင်စေပါလိမ့်မယ်။

{:else}

ဒါတွေကို လုပ်ဆောင်ဖို့အတွက်၊ [Chapter 3](/course/chapter3) မှာ Keras API နဲ့ models တွေကို train လုပ်တာအကြောင်း သင်သင်ယူခဲ့တဲ့ အရာအားလုံး၊ [Chapter 5](/course/chapter5) မှာ 🤗 Datasets library အကြောင်း၊ ပြီးတော့ [Chapter 6](/course/chapter6) မှာ 🤗 Tokenizers library အကြောင်း သင်သင်ယူခဲ့တဲ့ အရာအားလုံးကို အကျိုးယူဖို့ လိုအပ်ပါလိမ့်မယ်။ [Chapter 4](/course/chapter4) မှာ လုပ်ခဲ့သလို ကျွန်တော်တို့ရဲ့ ရလဒ်တွေကို Model Hub ကိုလည်း upload လုပ်သွားမှာဖြစ်တဲ့အတွက်၊ ဒါဟာ အရာအားလုံး ပေါင်းစပ်လာမယ့် အခန်းပါပဲ!

အပိုင်းတစ်ခုစီကို သီးခြားစီ ဖတ်ရှုနိုင်ပါတယ်။

{/if}

> [!TIP]
> အကယ်၍ သင်သည် အပိုင်းများကို အစီအစဉ်အတိုင်း ဖတ်ပါက၊ ၎င်းတို့တွင် ကုတ်ဒ်နှင့် စာသားများ အတန်အသင့် တူညီနေသည်ကို သတိပြုမိပါလိမ့်မည်။ ထပ်ခါတလဲလဲ ဖော်ပြခြင်းမှာ သင်စိတ်ဝင်စားသော မည်သည့်လုပ်ငန်းအတွက်မဆို (သို့မဟုတ် နောက်မှ ပြန်လာရန်) အပြည့်အစုံသော လုပ်ဆောင်နိုင်သည့် ဥပမာတစ်ခုကို တွေ့ရှိနိုင်စေရန် ရည်ရွယ်ပါသည်။

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Fine-tune**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနဲ့ ထပ်မံလေ့ကျင့်ပေးခြင်းကို ဆိုလိုပါတယ်။
*   **Text Classification**: စာသားကို သတ်မှတ်ထားသော အမျိုးအစားများထဲသို့ ခွဲခြားသတ်မှတ်ခြင်းနှင့် သက်ဆိုင်သော ပြဿနာ။
*   **Traditional NLP Models**: Neural Networks များ မပေါ်မီက အသုံးပြုခဲ့သော Natural Language Processing (NLP) models များ (ဥပမာ- rule-based systems, statistical models)။
*   **Modern LLMs (Large Language Models)**: Transformer Architecture ပေါ်တွင် အခြေခံပြီး ဒေတာအမြောက်အမြားဖြင့် လေ့ကျင့်ထားသော ခေတ်မီဘာသာစကားမော်ဒယ်ကြီးများ။
*   **Token Classification**: စာသား sequence တစ်ခုအတွင်းရှိ token တစ်ခုစီကို အမျိုးအစားခွဲခြားသတ်မှတ်ခြင်း လုပ်ငန်း (ဥပမာ- Named Entity Recognition)။
*   **Masked Language Modeling (MLM)**: စာကြောင်းတစ်ခုထဲမှ စကားလုံးအချို့ကို ဝှက်ထားပြီး ၎င်းတို့ကို ခန့်မှန်းစေခြင်းဖြင့် model ကို လေ့ကျင့်သော task (BERT ကဲ့သို့)။
*   **Summarization**: ရှည်လျားသော စာသားတစ်ခု၏ အနှစ်ချုပ်ကို ထုတ်လုပ်ခြင်း။
*   **Translation**: ဘာသာစကားတစ်ခုမှ အခြားဘာသာစကားတစ်ခုသို့ စာသားများကို ဘာသာပြန်ခြင်း။
*   **Causal Language Modeling Pretraining**: စာကြောင်းတစ်ခု၏ နောက်ဆက်တွဲ token (စကားလုံး) ကို ခန့်မှန်းခြင်းဖြင့် model ကို လေ့ကျင့်သော task (GPT-2 ကဲ့သို့)။
*   **Question Answering**: ပေးထားသော စာသားတစ်ခုမှ မေးခွန်းတစ်ခု၏ အဖြေကို ရှာဖွေခြင်း။
*   **Tasks**: Artificial Intelligence (AI) သို့မဟုတ် Machine Learning (ML) မော်ဒယ်တစ်ခုက လုပ်ဆောင်ရန် ဒီဇိုင်းထုတ်ထားသော သီးခြားအလုပ်။
*   **Large Language Models (LLMs)**: လူသားဘာသာစကားကို နားလည်ပြီး ထုတ်လုပ်ပေးနိုင်တဲ့ အလွန်ကြီးမားတဲ့ Artificial Intelligence (AI) မော်ဒယ်တွေ ဖြစ်ပါတယ်။
*   **`Trainer` API**: Hugging Face Transformers library မှ model များကို ထိရောက်စွာ လေ့ကျင့်ရန်အတွက် ဒီဇိုင်းထုတ်ထားသော မြင့်မားသောအဆင့် API။
*   **🤗 Accelerate Library**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး PyTorch code တွေကို မတူညီတဲ့ training environment (ဥပမာ - GPU အများအပြား၊ distributed training) တွေမှာ အလွယ်တကူ run နိုင်အောင် ကူညီပေးပါတယ်။
*   **🤗 Datasets Library**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် ဒေတာအစုအဝေး (datasets) တွေကို လွယ်လွယ်ကူကူ ဝင်ရောက်ရယူ၊ စီမံခန့်ခွဲပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **🤗 Tokenizers Library**: Rust ဘာသာနဲ့ ရေးသားထားတဲ့ Hugging Face library တစ်ခုဖြစ်ပြီး မြန်ဆန်ထိရောက်တဲ့ tokenization ကို လုပ်ဆောင်ပေးသည်။
*   **Model Hub**: Hugging Face Hub ကို ရည်ညွှန်းပြီး AI မော်ဒယ်များ ရှာဖွေ၊ မျှဝေ၊ အသုံးပြုနိုင်သော ဗဟို platform။
*   **Upload Results**: လေ့ကျင့်ထားသော model ၏ ရလဒ်များ သို့မဟုတ် model files များကို Hugging Face Hub သို့ တင်ခြင်း။
*   **Training Loop**: model တစ်ခုကို လေ့ကျင့်ရန်အတွက် iterations များစွာဖြင့် လုပ်ဆောင်သော အဓိက code အပိုင်း။
*   **Customize**: မိမိနှစ်သက်ရာ သို့မဟုတ် လိုအပ်ချက်များအတိုင်း ပြင်ဆင်ခြင်း။
*   **Keras API**: TensorFlow library တွင် ပါဝင်သော မြင့်မားသောအဆင့် (high-level) API တစ်ခုဖြစ်ပြီး deep learning models များကို လွယ်ကူလျင်မြန်စွာ တည်ဆောက်ရန် အသုံးပြုသည်။
*   **Prose**: ပုံမှန်စာသား သို့မဟုတ် စကားပြေ။