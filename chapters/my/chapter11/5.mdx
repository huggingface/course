# အကဲဖြတ်ခြင်း (Evaluation)

SFT (Supervised Fine-Tuning) သို့မဟုတ် LoRA SFT နည်းလမ်းတွေနဲ့ fine-tune လုပ်ထားတဲ့ model တစ်ခုကို standard benchmarks တွေပေါ်မှာ ကျွန်တော်တို့ အကဲဖြတ်သင့်ပါတယ်။ Machine Learning Engineers တွေအနေနဲ့၊ သင်စိတ်ဝင်စားတဲ့ domain အတွက် သက်ဆိုင်ရာ evaluation တွေ စုစည်းထားသင့်ပါတယ်။ ဒီစာမျက်နှာမှာ၊ အသုံးအများဆုံး benchmarks တွေနဲ့ သင့် model ကို အကဲဖြတ်ဖို့ ဘယ်လိုအသုံးပြုရမလဲဆိုတာ ကြည့်သွားပါမယ်။ သင့်ရဲ့ သီးခြား use case အတွက် custom benchmarks တွေကို ဘယ်လိုဖန်တီးရမလဲဆိုတာကိုလည်း ကြည့်သွားပါမယ်။

## အလိုအလျောက် Benchmarks များ

Automatic benchmarks တွေဟာ မတူညီတဲ့ tasks တွေနဲ့ capabilities တွေပေါ်မှာ language models တွေကို အကဲဖြတ်ဖို့အတွက် စံပြုကိရိယာတွေအဖြစ် လုပ်ဆောင်ပါတယ်။ Model စွမ်းဆောင်ရည်ကို နားလည်ဖို့အတွက် အသုံးဝင်တဲ့ အစမှတ်တစ်ခုကို ပံ့ပိုးပေးပေမယ့်၊ ၎င်းတို့ဟာ ပြည့်စုံတဲ့ evaluation strategy ရဲ့ အစိတ်အပိုင်းတစ်ခုသာ ဖြစ်တယ်ဆိုတာကို နားလည်ဖို့ အရေးကြီးပါတယ်။

## Automatic Benchmarks တွေကို နားလည်ခြင်း

Automatic benchmarks တွေမှာ ပုံမှန်အားဖြင့် ကြိုတင်သတ်မှတ်ထားတဲ့ tasks တွေနဲ့ evaluation metrics တွေပါတဲ့ curated datasets တွေ ပါဝင်ပါတယ်။ ဒီ benchmarks တွေက အခြေခံဘာသာစကား နားလည်မှုကနေ ရှုပ်ထွေးတဲ့ reasoning အထိ model ရဲ့ capability အမျိုးမျိုးကို အကဲဖြတ်ဖို့ ရည်ရွယ်ပါတယ်။ Automatic benchmarks တွေကို အသုံးပြုခြင်းရဲ့ အဓိကအားသာချက်ကတော့ ၎င်းတို့ရဲ့ Standardization ပါပဲ၊ ၎င်းတို့က မတူညီတဲ့ models တွေကြား တသမတ်တည်း နှိုင်းယှဉ်နိုင်စေပြီး reproducible results တွေကို ပံ့ပိုးပေးပါတယ်။

ဒါပေမယ့်၊ benchmark စွမ်းဆောင်ရည်ဟာ တကယ့်လက်တွေ့ကမ္ဘာမှာ ထိရောက်မှုနဲ့ အမြဲတမ်း တိုက်ရိုက်ဆက်စပ်မှု မရှိဘူးဆိုတာကို နားလည်ဖို့ အရေးကြီးပါတယ်။ Academic benchmarks တွေမှာ ထူးချွန်တဲ့ model တစ်ခုဟာ သီးခြား domain applications တွေ ဒါမှမဟုတ် လက်တွေ့ use cases တွေနဲ့ ရုန်းကန်နေရဆဲ ဖြစ်နိုင်ပါတယ်။

## အထွေထွေဗဟုသုတ Benchmarks များ

[MMLU](https://huggingface.co/datasets/cais/mmlu) (Massive Multitask Language Understanding) က သိပ္ပံပညာကနေ လူမှုရေးဘာသာရပ်တွေအထိ ဘာသာရပ် ၅၇ ခုမှာ ဗဟုသုတကို စစ်ဆေးပါတယ်။ ဒါက ပြည့်စုံပေမယ့်၊ သီးခြား domains တွေအတွက် လိုအပ်တဲ့ ကျွမ်းကျင်မှုအတိမ်အနက်ကိုတော့ မဖော်ပြနိုင်ပါဘူး။ TruthfulQA က model တစ်ခုရဲ့ အသုံးများတဲ့ အယူအဆမှားတွေကို ပြန်လည်ထုတ်လုပ်နိုင်တဲ့ လမ်းကြောင်းကို အကဲဖြတ်ပါတယ်၊ ဒါပေမယ့် သတင်းမှားအမျိုးအစားအားလုံးကိုတော့ ဖမ်းယူနိုင်ခြင်း မရှိပါဘူး။

## Reasoning Benchmarks များ

[BBH](https://huggingface.co/datasets/lukaemon/bbh) (Big Bench Hard) နဲ့ [GSM8K](https://huggingface.co/datasets/openai/gsm8k) တို့က ရှုပ်ထွေးတဲ့ reasoning tasks တွေကို အာရုံစိုက်ပါတယ်။ BBH က logical thinking နဲ့ planning ကို စစ်ဆေးပြီး၊ GSM8K ကတော့ သင်္ချာပြဿနာဖြေရှင်းခြင်းကို အထူးပစ်မှတ်ထားပါတယ်။ ဒီ benchmarks တွေက analytical capabilities တွေကို အကဲဖြတ်ဖို့ ကူညီပေးပေမယ့် တကယ့်လက်တွေ့ကမ္ဘာ အခြေအနေတွေမှာ လိုအပ်တဲ့ နက်နဲသိမ်မွေ့တဲ့ reasoning တွေကိုတော့ မဖမ်းယူနိုင်ပါဘူး။

## ဘာသာစကား နားလည်မှု

[HELM](https://github.com/stanford-crfm/helm) က ပြည့်စုံတဲ့ evaluation framework တစ်ခုကို ပံ့ပိုးပေးပါတယ်။ HELM လို benchmarks တွေက commonsense, world knowledge, နဲ့ reasoning လို ကဏ္ဍတွေမှာ ဘာသာစကားလုပ်ဆောင်နိုင်စွမ်းအပေါ် ထိုးထွင်းသိမြင်မှုတွေကို ပေးပါတယ်။ ဒါပေမယ့် သဘာဝအတိုင်း စကားပြောဆိုမှုရဲ့ ရှုပ်ထွေးမှု ဒါမှမဟုတ် domain-specific terminology တွေကို အပြည့်အဝ ကိုယ်စားပြုနိုင်ခြင်း မရှိပါဘူး။

## Domain-Specific Benchmarks များ

သင်္ချာ၊ coding နဲ့ chat လို သီးခြား domains တွေကို အာရုံစိုက်ထားတဲ့ benchmarks အချို့ကို ကြည့်ကြရအောင်။

[MATH benchmark](https://huggingface.co/papers/2103.03874) ဟာ သင်္ချာဆိုင်ရာ reasoning အတွက် အရေးကြီးတဲ့ evaluation tool တစ်ခုလည်း ဖြစ်ပါတယ်။ ဒါက algebra, geometry, number theory, counting, probability နဲ့ အခြားအရာတွေ အပါအဝင် သင်္ချာပြိုင်ပွဲတွေက ပြဿနာ ၁၂,၅၀၀ ပါဝင်ပါတယ်။ MATH ကို အထူးစိန်ခေါ်မှုဖြစ်စေတာက multi-step reasoning, formal mathematical notation ကို နားလည်မှုနဲ့ step-by-step solutions တွေကို ထုတ်လုပ်နိုင်စွမ်းတို့ လိုအပ်တာပဲ ဖြစ်ပါတယ်။ ရိုးရှင်းတဲ့ arithmetic tasks တွေနဲ့ မတူဘဲ၊ MATH ပြဿနာတွေက ရှုပ်ထွေးတဲ့ ပြဿနာဖြေရှင်းနည်း ဗျူဟာတွေနဲ့ သင်္ချာသဘောတရားတွေကို အသုံးချဖို့ တောင်းဆိုလေ့ရှိပါတယ်။

[HumanEval Benchmark](https://github.com/openai/human-eval) ကတော့ coding ကို အာရုံစိုက်ထားတဲ့ evaluation dataset တစ်ခုဖြစ်ပြီး programming ပြဿနာ ၁၆၄ ခု ပါဝင်ပါတယ်။ ဒီ benchmark က model တစ်ခုရဲ့ ပေးထားတဲ့ programming tasks တွေကို ဖြေရှင်းနိုင်မယ့် functionally correct Python code ကို ထုတ်လုပ်နိုင်စွမ်းကို စစ်ဆေးပါတယ်။ HumanEval ကို အထူးတန်ဖိုးရှိစေတာက code generation capabilities နဲ့ functional correctness နှစ်ခုလုံးကို တကယ့် test case execution မှတစ်ဆင့် အကဲဖြတ်တာပဲ ဖြစ်ပါတယ်။ ရည်ညွှန်း solutions တွေနဲ့ အပေါ်ယံဆင်တူမှုကိုပဲ မကြည့်ပါဘူး။ ပြဿနာတွေက အခြေခံ string manipulation ကနေ ပိုရှုပ်ထွေးတဲ့ algorithms နဲ့ data structures တွေအထိ ပါဝင်ပါတယ်။

[Alpaca Eval](https://tatsu-lab.github.io/alpaca_eval/) က instruction-following language models တွေရဲ့ အရည်အသွေးကို အကဲဖြတ်ဖို့ ဒီဇိုင်းထုတ်ထားတဲ့ automated evaluation framework တစ်ခု ဖြစ်ပါတယ်။ ဒါက GPT-4 ကို judge အဖြစ် အသုံးပြုပြီး helpfulness, honesty, နဲ့ harmlessness အပါအဝင် မတူညီတဲ့ dimensions တွေပေါ်မှာ model outputs တွေကို အကဲဖြတ်ပါတယ်။ framework မှာ ဂရုတစိုက် curated လုပ်ထားတဲ့ prompts ၈၀၅ ခုပါတဲ့ dataset တစ်ခု ပါဝင်ပြီး Claude, GPT-4 နဲ့ အခြား reference models များစွာနဲ့ responses တွေကို အကဲဖြတ်နိုင်ပါတယ်။ Alpaca Eval ကို အထူးအသုံးဝင်စေတာက လူသား annotators တွေ မလိုအပ်ဘဲ တသမတ်တည်းဖြစ်တဲ့၊ scalable evaluations တွေကို ပံ့ပိုးပေးနိုင်ပြီး၊ traditional metrics တွေက လွဲချော်နိုင်တဲ့ model စွမ်းဆောင်ရည်ရဲ့ နက်နဲသိမ်မွေ့တဲ့ ကဏ္ဍတွေကို ဖမ်းယူနိုင်တာပဲ ဖြစ်ပါတယ်။

## အခြားသော Evaluation ချဉ်းကပ်မှုများ

အဖွဲ့အစည်းများစွာက standard benchmarks တွေရဲ့ ကန့်သတ်ချက်တွေကို ဖြေရှင်းဖို့ အခြား evaluation နည်းလမ်းတွေကို ဖန်တီးခဲ့ကြပါတယ်။

### LLM-as-Judge

language model တစ်ခုရဲ့ outputs တွေကို အခြား language model တစ်ခုကို အသုံးပြုပြီး အကဲဖြတ်တာက ပိုပြီး ရေပန်းစားလာပါတယ်။ ဒီချဉ်းကပ်မှုက traditional metrics တွေထက် ပိုမိုနက်နဲတဲ့ feedback တွေကို ပေးနိုင်ပေမယ့်၊ သူ့မှာလည်း ဘက်လိုက်မှုတွေနဲ့ ကန့်သတ်ချက်တွေ ရှိပါတယ်။

### Evaluation Arenas

[Chatbot Arena](https://lmarena.ai/) လို Evaluation arenas တွေက crowdsourced feedback မှတစ်ဆင့် LLM အကဲဖြတ်ခြင်းအတွက် ထူးခြားတဲ့ ချဉ်းကပ်မှုတစ်ခုကို ပေးပါတယ်။ ဒီ platform တွေမှာ၊ အသုံးပြုသူတွေဟာ LLMs နှစ်ခုကြား အမည်မဖော်ဘဲ "battles" တွေမှာ ပါဝင်ပြီး မေးခွန်းတွေမေးကာ ဘယ် model က ပိုကောင်းတဲ့ response တွေ ပေးသလဲဆိုတာကို မဲပေးကြပါတယ်။ ဒီချဉ်းကပ်မှုက မတူညီတဲ့၊ စိန်ခေါ်မှုရှိတဲ့ မေးခွန်းတွေမှတစ်ဆင့် တကယ့်လက်တွေ့ကမ္ဘာ အသုံးပြုမှုပုံစံတွေနဲ့ နှစ်သက်မှုတွေကို ဖမ်းယူပါတယ်။ Crowdsourced votes တွေနဲ့ expert evaluations တွေကြား ခိုင်မာတဲ့ သဘောတူညီမှုရှိတယ်လို့ လေ့လာမှုတွေက ပြသထားပါတယ်။ ဒါပေမယ့်၊ ဒီ platform တွေမှာ user base bias, skewed prompt distributions နဲ့ safety considerations တွေထက် helpfulness ကို အဓိကထားတဲ့ ကန့်သတ်ချက်တွေ ပါဝင်ပါတယ်။

### Custom Benchmark Suites

အဖွဲ့အစည်းတွေက သူတို့ရဲ့ သီးခြားလိုအပ်ချက်တွေနဲ့ use cases တွေအတွက် စိတ်ကြိုက် benchmark suites တွေကို မကြာခဏ ဖန်တီးကြပါတယ်။ ဒါတွေမှာ domain-specific knowledge tests တွေ ဒါမှမဟုတ် တကယ့် deployment အခြေအနေတွေကို ထင်ဟပ်စေတဲ့ evaluation scenarios တွေ ပါဝင်နိုင်ပါတယ်။

## Custom Evaluation

Standard benchmarks တွေက အသုံးဝင်တဲ့ baseline တစ်ခုကို ပံ့ပိုးပေးပေမယ့်၊ ဒါတွေက သင့်ရဲ့ တစ်ခုတည်းသော evaluation method မဖြစ်သင့်ပါဘူး။ ပိုပြီး ပြည့်စုံတဲ့ ချဉ်းကပ်မှုတစ်ခုကို ဘယ်လိုတည်ဆောက်ရမလဲဆိုတာ ဒီမှာပါ။

၁။ baseline တစ်ခုကို တည်ဆောက်ပြီး အခြား models တွေနဲ့ နှိုင်းယှဉ်နိုင်ဖို့ သက်ဆိုင်ရာ standard benchmarks တွေနဲ့ စတင်ပါ။

၂။ သင့်ရဲ့ use case ရဲ့ သီးခြားလိုအပ်ချက်တွေနဲ့ စိန်ခေါ်မှုတွေကို ဖော်ထုတ်ပါ။ သင့် model က တကယ်တမ်း ဘယ် tasks တွေကို လုပ်ဆောင်ရမလဲ။ ဘယ်လိုအမှားမျိုးတွေက အပြဿနာအရှိဆုံးလဲ။

၃။ သင့်ရဲ့ တကယ့် use case ကို ထင်ဟပ်စေမယ့် custom evaluation datasets တွေကို ဖန်တီးပါ။ ဒါတွေမှာ အောက်ပါတို့ ပါဝင်နိုင်ပါတယ်-
   - သင့် domain ကနေ ရရှိတဲ့ တကယ့် user queries များ
   - သင်ကြုံတွေ့ခဲ့ရတဲ့ common edge cases များ
   - အထူးစိန်ခေါ်မှုရှိတဲ့ အခြေအနေများရဲ့ ဥပမာများ

၄။ multi-layered evaluation strategy ကို အကောင်အထည်ဖော်တာကို ထည့်သွင်းစဉ်းစားပါ။
   - လျင်မြန်တဲ့ feedback အတွက် Automated metrics များ
   - နက်နဲသိမ်မွေ့တဲ့ နားလည်မှုအတွက် Human evaluation များ
   - သီးခြား applications တွေအတွက် Domain expert review များ
   - ထိန်းချုပ်ထားတဲ့ environments တွေမှာ A/B testing များ

## Custom Evaluations တွေကို အကောင်အထည်ဖော်ခြင်း

ဒီအပိုင်းမှာ၊ ကျွန်တော်တို့ရဲ့ finetune လုပ်ထားတဲ့ model အတွက် evaluation ကို အကောင်အထည်ဖော်ပါမယ်။ Hugging Face library ထဲမှာ တည်ဆောက်ထားတဲ့ tasks များစွာပါဝင်တဲ့ standard benchmarks တွေပေါ်မှာ ကျွန်တော်တို့ရဲ့ finetune လုပ်ထားတဲ့ model ကို အကဲဖြတ်ဖို့ [`lighteval`](https://github.com/huggingface/lighteval) ကို အသုံးပြုနိုင်ပါတယ်။ ကျွန်တော်တို့ အကဲဖြတ်ချင်တဲ့ tasks တွေနဲ့ evaluation အတွက် parameters တွေကို သတ်မှတ်ဖို့ပဲ လိုအပ်ပါတယ်။

LightEval tasks တွေကို သီးခြား format တစ်ခုကို အသုံးပြုပြီး သတ်မှတ်ထားပါတယ်။

```
{suite}|{task}|{num_few_shot}|{auto_reduce}
```

| Parameter | Description |
|-----------|-------------|
| `suite` | benchmark suite (ဥပမာ- 'mmlu', 'truthfulqa') |
| `task` | suite အတွင်းရှိ သီးခြား task (ဥပမာ- 'abstract_algebra') |
| `num_few_shot` | prompt တွင် ထည့်သွင်းရန် ဥပမာအရေအတွက် (zero-shot အတွက် 0) |
| `auto_reduce` | prompt အလွန်ရှည်လျားပါက few-shot examples များကို အလိုအလျောက်လျှော့ချမလား (0 သို့မဟုတ် 1) |

ဥပမာ - `"mmlu|abstract_algebra|0|0"` က MMLU ရဲ့ abstract algebra task ကို zero-shot inference ဖြင့် အကဲဖြတ်ပါတယ်။

## Example Evaluation Pipeline

ကျွန်တော်တို့ရဲ့ finetune လုပ်ထားတဲ့ model အတွက် evaluation pipeline တစ်ခုကို တည်ဆောက်ကြရအောင်။ ဆေးဘက်ဆိုင်ရာ domain နဲ့ သက်ဆိုင်တဲ့ sub tasks တွေပေါ်မှာ model ကို အကဲဖြတ်သွားပါမယ်။

VLLM backend ကို အသုံးပြုပြီး Lighteval ဖြင့် သီးခြား domain တစ်ခုနဲ့ သက်ဆိုင်တဲ့ automatic benchmarks တွေကို အကဲဖြတ်တဲ့ ပြည့်စုံတဲ့ ဥပမာတစ်ခုကတော့ အောက်ပါအတိုင်းပါ။

```bash
lighteval accelerate \
    "pretrained=your-model-name" \
    "mmlu|anatomy|0|0" \
    "mmlu|high_school_biology|0|0" \
    "mmlu|high_school_chemistry|0|0" \
    "mmlu|professional_medicine|0|0" \
    --max_samples 40 \
    --batch_size 1 \
    --output_path "./results" \
    --save_generations true
```

ရလဒ်တွေကို tabular format နဲ့ ပြသပေးပါတယ်။

```
|                  Task                  |Version|Metric|Value |   |Stderr|
|----------------------------------------|------:|------|-----:|---|-----:|
|all                                     |       |acc   |0.3333|±  |0.1169|
|leaderboard:mmlu:_average:5             |       |acc   |0.3400|±  |0.1121|
|leaderboard:mmlu:anatomy:5              |      0|acc   |0.4500|±  |0.1141|
|leaderboard:mmlu:high_school_biology:5  |      0|acc   |0.1500|±  |0.0819|
```

Lighteval မှာ Python API လည်း ပါဝင်ပြီး ပိုမိုအသေးစိတ်တဲ့ evaluation tasks တွေအတွက် အသုံးဝင်ပါတယ်။ ဒါက results တွေကို ပိုမိုပြောင်းလွယ်ပြင်လွယ် ကိုင်တွယ်နိုင်စေပါတယ်။ အသေးစိတ်အချက်အလက်တွေအတွက် [Lighteval documentation](https://huggingface.co/docs/lighteval/using-the-python-api) ကို ကြည့်ရှုပါ။

> [!TIP]
> ✏️ **စမ်းသပ်ကြည့်ပါ!** သင့်ရဲ့ finetune လုပ်ထားတဲ့ model ကို lighteval မှာ သီးခြား task တစ်ခုပေါ်မှာ အကဲဖြတ်ကြည့်ပါ။

# အခန်း (၁၁) ဆိုင်ရာ မေးခွန်းများ[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={11}
    classNames="absolute z-10 right-0 top-0"
/>

### ၁။ model evaluation အတွက် automatic benchmarks တွေကို အသုံးပြုခြင်းရဲ့ အဓိကအားသာချက်တွေက ဘာတွေလဲ။

<Question
	choices={[
		{
			text: "၎င်းတို့သည် ပြီးပြည့်စုံသော တကယ့်လက်တွေ့ကမ္ဘာ စွမ်းဆောင်ရည် metrics များကို ပံ့ပိုးပေးသည်။",
			explain: "မမှန်ပါဘူး! automatic benchmarks တွေက အသုံးဝင်ပေမယ့်၊ ၎င်းတို့ဟာ တကယ့်လက်တွေ့ကမ္ဘာ စွမ်းဆောင်ရည်နဲ့ တိုက်ရိုက်အမြဲတမ်း ဆက်စပ်မှု မရှိပါဘူး။"
		},
		{
			text: "၎င်းတို့သည် models များကြား စံပြုနှိုင်းယှဉ်မှုကို ခွင့်ပြုပြီး reproducible results များကို ပံ့ပိုးပေးသည်။",
			explain: "မှန်ပါတယ်။ ဒါက automatic benchmarks တွေရဲ့ အဓိက အကျိုးကျေးဇူးတွေထဲက တစ်ခုပါပဲ။",
			correct: true
		},
		{
			text: "၎င်းတို့သည် အခြားမည်သည့် evaluation ပုံစံကိုမဆို မလိုအပ်အောင် ဖယ်ရှားပေးသည်။",
			explain: "မမှန်ပါဘူး! automatic benchmarks တွေဟာ ပြည့်စုံတဲ့ evaluation strategy ရဲ့ အစိတ်အပိုင်းတစ်ခု ဖြစ်သင့်ပြီး၊ တစ်ခုတည်းသော နည်းလမ်းတော့ မဟုတ်ပါဘူး။"
		}
	]}
/>

### ၂။ ဘာသာရပ် ၅၇ ခုမှာ ဗဟုသုတကို သီးခြားစစ်ဆေးတဲ့ benchmark က ဘာလဲ။

<Question
	choices={[
		{
			text: "BBH (Big Bench Hard)",
			explain: "မမှန်ပါဘူး! BBH က ရှုပ်ထွေးတဲ့ reasoning tasks တွေကို အာရုံစိုက်တာဖြစ်ပြီး၊ ကျယ်ပြန့်တဲ့ ဘာသာရပ် ဗဟုသုတကို မဟုတ်ပါဘူး။"
		},
		{
			text: "GSM8K",
			explain: "မမှန်ပါဘူး! GSM8K က သင်္ချာပြဿနာဖြေရှင်းခြင်းကို အထူးပစ်မှတ်ထားပါတယ်။"
		},
		{
			text: "MMLU",
			explain: "မှန်ပါတယ်။ MMLU (Massive Multitask Language Understanding) က သိပ္ပံပညာကနေ လူမှုရေးဘာသာရပ်တွေအထိ ဘာသာရပ် ၅၇ ခုမှာ ဗဟုသုတကို စစ်ဆေးပါတယ်။",
			correct: true
		}
	]}
/>

### ၃။ LLM-as-Judge ဆိုတာ ဘာလဲ။

<Question
	choices={[
		{
			text: "language model တစ်ခုရဲ့ outputs တွေကို အခြား language model တစ်ခုကို အသုံးပြုပြီး အကဲဖြတ်ခြင်း။",
			explain: "မှန်ပါတယ်။ ဒါက ပိုမိုနက်နဲတဲ့ feedback တွေကို ပေးနိုင်တဲ့ အခြား evaluation ချဉ်းကပ်မှုတစ်ခုပါပဲ။",
			correct: true
		},
		{
			text: "တရားရေးရာ reasoning ကို စစ်ဆေးတဲ့ benchmark တစ်ခု။",
			explain: "မမှန်ပါဘူး! LLM-as-Judge ဆိုတာ model တစ်ခုက အခြားတစ်ခုရဲ့ outputs တွေကို အကဲဖြတ်တာကို ရည်ညွှန်းတာဖြစ်ပြီး တရားရေးရာ reasoning ကို စစ်ဆေးတာ မဟုတ်ပါဘူး။"
		},
		{
			text: "ဥပဒေရေးရာ datasets တွေပေါ်မှာ models တွေကို train လုပ်တဲ့ နည်းလမ်းတစ်ခု။",
			explain: "မမှန်ပါဘူး! ဒါက ဥပဒေရေးရာ data တွေပေါ်မှာ train လုပ်တာနဲ့ မသက်ဆိုင်ပါဘူး၊ ဒါပေမယ့် model တစ်ခုက အခြားတစ်ခုရဲ့ outputs တွေကို အကဲဖြတ်တာနဲ့ ပိုဆိုင်ပါတယ်။"
		}
	]}
/>

### ၄။ ပြည့်စုံတဲ့ evaluation strategy မှာ ဘာတွေပါဝင်သင့်လဲ။

<Question
	choices={[
		{
			text: "standard benchmarks တွေချည်းသာ။",
			explain: "မမှန်ပါဘူး! ပြည့်စုံတဲ့ strategy မှာ evaluation နည်းလမ်းများစွာ ပါဝင်သင့်ပါတယ်။"
		},
		{
			text: "standard benchmarks တွေ၊ custom evaluation datasets တွေနဲ့ domain-specific testing တွေ။",
			explain: "မှန်ပါတယ်။ ပြည့်စုံတဲ့ strategy မှာ evaluation ရဲ့ layers များစွာ ပါဝင်သင့်ပါတယ်။",
			correct: true
		},
		{
			text: "သင့်ရဲ့ use case နဲ့ သီးခြားသက်ဆိုင်တဲ့ custom datasets တွေချည်းသာ။",
			explain: "မမှန်ပါဘူး! custom datasets တွေက အရေးကြီးပေမယ့်၊ ၎င်းတို့ဟာ တစ်ခုတည်းသော evaluation method မဖြစ်သင့်ပါဘူး။"
		}
	]}
/>

### ၅။ automatic benchmarks တွေရဲ့ ကန့်သတ်ချက်က ဘာလဲ။

<Question
	choices={[
		{
			text: "၎င်းတို့သည် run ရန် အလွန်စျေးကြီးသည်။",
			explain: "မမှန်ပါဘူး! ကုန်ကျစရိတ်က automatic benchmarks တွေရဲ့ အဓိကကန့်သတ်ချက်တော့ မဟုတ်ပါဘူး။"
		},
		{
			text: "benchmark စွမ်းဆောင်ရည်ဟာ တကယ့်လက်တွေ့ကမ္ဘာ ထိရောက်မှုနဲ့ အမြဲတမ်း တိုက်ရိုက်ဆက်စပ်မှု မရှိဘူး။",
			explain: "မှန်ပါတယ်။ ဒါက automatic benchmarks တွေကို အသုံးပြုတဲ့အခါ မှတ်သားထားရမယ့် အဓိကကန့်သတ်ချက်တစ်ခုပါပဲ။",
			correct: true
		},
		{
			text: "၎င်းတို့သည် models အသေးစားများကိုသာ အကဲဖြတ်နိုင်သည်။",
			explain: "မမှန်ပါဘူး! automatic benchmarks တွေကို အရွယ်အစားမျိုးစုံရှိတဲ့ models တွေကို အကဲဖြတ်ဖို့ အသုံးပြုနိုင်ပါတယ်။"
		}
	]}
/>

### ၆။ custom evaluation datasets တွေ ဖန်တီးခြင်းရဲ့ ရည်ရွယ်ချက်က ဘာလဲ။

<Question
	choices={[
		{
			text: "သင့်ရဲ့ သီးခြား use case ကို ထင်ဟပ်စေပြီး သင့် domain ကနေ တကယ့် user queries တွေ ပါဝင်စေဖို့။",
			explain: "မှန်ပါတယ်။ Custom datasets တွေက evaluation ဟာ သင့်ရဲ့ သီးခြားလိုအပ်ချက်တွေနဲ့ သက်ဆိုင်ကြောင်း သေချာစေဖို့ ကူညီပေးပါတယ်။",
			correct: true
		},
		{
			text: "standard benchmarks တွေကို လုံးဝအစားထိုးဖို့။",
			explain: "မမှန်ပါဘူး! Custom datasets တွေဟာ standard benchmarks တွေကို ဖြည့်စွက်ပေးသင့်ပြီး၊ အစားထိုးတာတော့ မဟုတ်ပါဘူး။"
		},
		{
			text: "evaluation ကို ပိုမိုလွယ်ကူစေဖို့။",
			explain: "မမှန်ပါဘူး! Custom datasets တွေ ဖန်တီးတာက အပိုအားထုတ်မှုတွေ လိုအပ်ပေမယ့် ပိုမိုသက်ဆိုင်ရာ evaluation ကို ပံ့ပိုးပေးပါတယ်။"
		}
	]}
/>

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Fine-tuned Model**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနဲ့ ထပ်မံလေ့ကျင့်ပေးထားသော မော်ဒယ်။
*   **SFT (Supervised Fine-Tuning)**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို တိကျသောလုပ်ငန်းဆောင်တာများ (specific tasks) အတွက် label ပါသော ဒေတာများကို အသုံးပြု၍ ထပ်မံလေ့ကျင့်ခြင်းနည်းလမ်း။
*   **LoRA SFT (Low-Rank Adaptation Supervised Fine-Tuning)**: LoRA နည်းပညာကို အသုံးပြု၍ SFT လုပ်ဆောင်ခြင်း။
*   **Standard Benchmarks**: မော်ဒယ်များ၏ စွမ်းဆောင်ရည်ကို နှိုင်းယှဉ်တိုင်းတာရန်အတွက် အများသိ၊ စံပြုထားသော datasets များ သို့မဟုတ် လုပ်ငန်းများ။
*   **Machine Learning Engineers**: Machine Learning စနစ်များကို ဒီဇိုင်းဆွဲ၊ တည်ဆောက်ပြီး အကောင်အထည်ဖော်သူများ။
*   **Suite of Evaluations**: သီးခြား domain တစ်ခုအတွက် သက်ဆိုင်ရာ evaluation နည်းလမ်းများ သို့မဟုတ် ကိရိယာများ စုစည်းမှု။
*   **Targeted Domain of Interest**: စိတ်ဝင်စားသော သို့မဟုတ် ပစ်မှတ်ထားသော သီးခြားနယ်ပယ် (ဥပမာ- ဆေးပညာ၊ ဘဏ္ဍာရေး)။
*   **Automatic Benchmarks**: လူသားရဲ့ ကြားဝင်ဆောင်ရွက်မှု အနည်းဆုံးဖြင့် မော်ဒယ်များကို အလိုအလျောက် အကဲဖြတ်နိုင်သော စံပြုကိရိယာများ။
*   **Language Models**: လူသားဘာသာစကား၏ ဖြန့်ဝေမှုကို နားလည်ရန် လေ့ကျင့်ထားသော AI မော်ဒယ်တစ်ခု။ ၎င်းသည် စာသားထုတ်လုပ်ခြင်း၊ ဘာသာပြန်ခြင်း စသည့်လုပ်ငန်းများတွင် အသုံးပြုနိုင်သည်။
*   **Tasks**: Artificial Intelligence (AI) သို့မဟုတ် Machine Learning (ML) မော်ဒယ်တစ်ခုက လုပ်ဆောင်ရန် ဒီဇိုင်းထုတ်ထားသော သီးခြားအလုပ်။
*   **Capabilities**: မော်ဒယ်တစ်ခု၏ လုပ်ဆောင်နိုင်စွမ်းများ။
*   **Comprehensive Evaluation Strategy**: မော်ဒယ်၏ စွမ်းဆောင်ရည်ကို အကဲဖြတ်ရန်အတွက် နည်းလမ်းမျိုးစုံ (benchmarks, human evaluation, custom datasets) ကို ပေါင်းစပ်အသုံးပြုသော ဗျူဟာ။
*   **Curated Datasets**: သီးခြားရည်ရွယ်ချက်အတွက် ဂရုတစိုက် ရွေးချယ်၊ စုစည်းပြီး ပြင်ဆင်ထားသော datasets များ။
*   **Predefined Tasks**: ကြိုတင်သတ်မှတ်ထားသော လုပ်ငန်းများ။
*   **Evaluation Metrics**: မော်ဒယ်၏ စွမ်းဆောင်ရည်ကို တိုင်းတာရန် အသုံးပြုသော တန်ဖိုးများ (ဥပမာ- accuracy, F1 score, BLEU)။
*   **Language Understanding**: မော်ဒယ်တစ်ခုက လူသားဘာသာစကားကို မည်မျှနားလည်နိုင်ခြင်း။
*   **Complex Reasoning**: ရှုပ်ထွေးသော ပြဿနာများကို ဖြေရှင်းရန်အတွက် ဆင်ခြင်တွေးခေါ်နိုင်စွမ်း။
*   **Standardization**: မတူညီသော entities များကြား နှိုင်းယှဉ်နိုင်စေရန်အတွက် တသမတ်တည်းသော စည်းမျဉ်းများ သို့မဟုတ် နည်းလမ်းများကို ချမှတ်ခြင်း။
*   **Reproducible Results**: တူညီသော input များဖြင့် တူညီသောလုပ်ငန်းစဉ်ကို ပြန်လည်လုပ်ဆောင်သောအခါ တူညီသောရလဒ်များကို ပြန်လည်ရရှိနိုင်ခြင်း။
*   **Real-world Effectiveness**: လက်တွေ့အခြေအနေများတွင် model တစ်ခု၏ အသုံးဝင်မှုနှင့် စွမ်းဆောင်ရည်။
*   **Academic Benchmarks**: ပညာရပ်ဆိုင်ရာ သုတေသနများတွင် အသုံးပြုသော စံပြု benchmarks များ။
*   **Domain Applications**: သီးခြားနယ်ပယ်တစ်ခု (ဥပမာ- ဆေးဘက်ဆိုင်ရာ၊ ဘဏ္ဍာရေး) တွင် အသုံးပြုသော application များ။
*   **Practical Use Cases**: လက်တွေ့အသုံးချနိုင်သော အခြေအနေများ။
*   **MMLU (Massive Multitask Language Understanding)**: ဘာသာရပ် ၅၇ ခုတွင် ဘာသာစကားနားလည်မှုနှင့် ဗဟုသုတကို စစ်ဆေးသည့် benchmark။
*   **TruthfulQA**: မော်ဒယ်တစ်ခု၏ အမှန်တရားကို ပြောဆိုနိုင်စွမ်းနှင့် အယူအဆမှားများကို ရှောင်ရှားနိုင်စွမ်းကို အကဲဖြတ်သည့် benchmark။
*   **Common Misconceptions**: အများအားဖြင့် မှားယွင်းစွာ နားလည်ထားသော အယူအဆများ။
*   **Misinformation**: မှားယွင်းသော သို့မဟုတ် မမှန်ကန်သော အချက်အလက်များ။
*   **BBH (Big Bench Hard)**: ရှုပ်ထွေးသော reasoning tasks များကို အကဲဖြတ်သည့် benchmark။
*   **GSM8K**: သင်္ချာပြဿနာဖြေရှင်းခြင်းစွမ်းရည်ကို အထူးပစ်မှတ်ထားသည့် benchmark (Grade School Math 8K)။
*   **Logical Thinking**: အကြောင်းအကျိုးဆင်ခြင်ခြင်း။
*   **Planning**: အလုပ်တစ်ခုကို လုပ်ဆောင်ရန်အတွက် အဆင့်ဆင့် စီမံခြင်း။
*   **Analytical Capabilities**: အချက်အလက်များကို ခွဲခြမ်းစိတ်ဖြာပြီး နားလည်နိုင်စွမ်း။
*   **Nuanced Reasoning**: နက်နဲသိမ်မွေ့ပြီး အသေးစိတ်ဆင်ခြင်နိုင်စွမ်း။
*   **HELM (Holistic Evaluation of Language Models)**: မော်ဒယ်များ၏ စွမ်းဆောင်ရည်ကို ကဏ္ဍစုံမှ အကဲဖြတ်သည့် ဘက်စုံသုံး evaluation framework။
*   **Commonsense**: လူအများစု သိရှိနားလည်ထားသော သာမန်အသိပညာ။
*   **World Knowledge**: ကမ္ဘာကြီးအကြောင်း အထွေထွေဗဟုသုတ။
*   **Natural Conversation**: လူသားများ ပုံမှန်အတိုင်း ပြောဆိုဆက်ဆံခြင်း။
*   **Domain-Specific Terminology**: သီးခြားနယ်ပယ်တစ်ခုတွင် အသုံးပြုသော အသုံးအနှုန်းများ။
*   **MATH Benchmark**: သင်္ချာပြဿနာဖြေရှင်းခြင်းနှင့် reasoning စွမ်းရည်ကို အကဲဖြတ်သည့် benchmark။
*   **Multi-step Reasoning**: ပြဿနာတစ်ခုကို ဖြေရှင်းရန်အတွက် အဆင့်များစွာ ဆင်ခြင်တွေးခေါ်ခြင်း။
*   **Formal Mathematical Notation**: သင်္ချာဆိုင်ရာ သင်္ကေတများနှင့် ပုံစံများ။
*   **Step-by-step Solutions**: ပြဿနာတစ်ခု၏ ဖြေရှင်းနည်းအဆင့်ဆင့်။
*   **Sophisticated Problem-solving Strategies**: ရှုပ်ထွေးသော ပြဿနာဖြေရှင်းနည်း ဗျူဟာများ။
*   **Mathematical Concept Applications**: သင်္ချာသဘောတရားများကို လက်တွေ့အသုံးချခြင်း။
*   **HumanEval Benchmark**: programming ပြဿနာများကို ဖြေရှင်းရန် Python code ထုတ်လုပ်နိုင်စွမ်းကို အကဲဖြတ်သည့် benchmark။
*   **Functionally Correct Python Code**: ပေးထားသော task ကို မှန်ကန်စွာ လုပ်ဆောင်နိုင်သော Python code။
*   **Code Generation Capabilities**: code များကို ဖန်တီးထုတ်လုပ်နိုင်စွမ်း။
*   **Functional Correctness**: ဆော့ဖ်ဝဲလ်တစ်ခု၏ လုပ်ဆောင်ချက်များသည် မျှော်လင့်ထားသည့်အတိုင်း မှန်ကန်စွာ အလုပ်လုပ်ခြင်း။
*   **Test Case Execution**: ပရိုဂရမ်တစ်ခု၏ လုပ်ဆောင်ချက်များကို စမ်းသပ်ရန်အတွက် သီးခြား input များကို အသုံးပြုခြင်း။
*   **Superficial Similarity**: အပေါ်ယံဆင်တူမှု။
*   **Reference Solutions**: မှန်ကန်သည်ဟု သတ်မှတ်ထားသော အဖြေများ။
*   **String Manipulation**: စာသားကြိုးများကို ပြောင်းလဲခြင်း သို့မဟုတ် စီမံဆောင်ရွက်ခြင်း။
*   **Algorithms**: ပြဿနာတစ်ခုကို ဖြေရှင်းရန်အတွက် အဆင့်ဆင့် ညွှန်ကြားချက်များ။
*   **Data Structures**: ကွန်ပျူတာထဲတွင် ဒေတာများကို စုစည်းပြီး သိမ်းဆည်းရန် နည်းလမ်းများ။
*   **Alpaca Eval**: instruction-following language models များ၏ အရည်အသွေးကို အကဲဖြတ်ရန် ဒီဇိုင်းထုတ်ထားသော automated evaluation framework။
*   **Instruction-following Language Models**: ပေးထားသော ညွှန်ကြားချက်များကို လိုက်နာ၍ တုံ့ပြန်မှုများ ထုတ်လုပ်နိုင်သော language models များ။
*   **GPT-4**: OpenAI မှ ထုတ်လုပ်ထားသော အဆင့်မြင့် Large Language Model။
*   **Judge**: Evaluation လုပ်ရာတွင် model outputs များကို အကဲဖြတ်သူ။
*   **Helpfulness**: model ၏ အဖြေများသည် အထောက်အကူဖြစ်ခြင်း။
*   **Honesty**: model ၏ အဖြေများသည် မှန်ကန်ခြင်း။
*   **Harmlessness**: model ၏ အဖြေများသည် အန္တရာယ်မရှိခြင်း။
*   **Curated Prompts**: ဂရုတစိုက် ရွေးချယ်ပြီး ပြင်ဆင်ထားသော prompts များ။
*   **Reference Models**: နှိုင်းယှဉ်ရန်အတွက် အသုံးပြုသော အခြား model များ။
*   **Claude**: Anthropic မှ ထုတ်လုပ်ထားသော AI assistant model။
*   **Scalable Evaluations**: ပိုမိုများပြားသော models များ သို့မဟုတ် datasets များကို ထိရောက်စွာ အကဲဖြတ်နိုင်ခြင်း။
*   **Human Annotators**: ဒေတာများကို labels များ ထည့်သွင်းပေးရန် ငှားရမ်းထားသော လူများ။
*   **Nuanced Aspects**: နက်နဲသိမ်မွေ့ပြီး အသေးစိတ်ကျသော ကဏ္ဍများ။
*   **LLM-as-Judge**: language model တစ်ခု၏ output များကို အခြား language model တစ်ခုကို အသုံးပြု၍ အကဲဖြတ်ခြင်းနည်းလမ်း။
*   **Nuanced Feedback**: နက်နဲသိမ်မွေ့ပြီး အသေးစိတ်ကျသော တုံ့ပြန်ချက်များ။
*   **Evaluation Arenas**: LLM များကို crowdsourced feedback မှတစ်ဆင့် အကဲဖြတ်ရန်အတွက် အွန်လိုင်း platform များ (ဥပမာ- Chatbot Arena)။
*   **Chatbot Arena**: LLM များကို crowdsourced feedback မှတစ်ဆင့် အကဲဖြတ်ရန် အသုံးပြုသော platform တစ်ခု။
*   **Crowdsourced Feedback**: အွန်လိုင်းလူအဖွဲ့အစည်းမှ လူအများအပြားထံမှ စုဆောင်းရရှိသော တုံ့ပြန်ချက်များ။
*   **Anonymous "Battles"**: အမည်မဖော်ဘဲ LLM နှစ်ခုကြား နှိုင်းယှဉ်စစ်ဆေးခြင်း။
*   **User Base Bias**: အသုံးပြုသူအဖွဲ့အစည်း၏ ဝိသေသလက္ခဏာများကြောင့် ဖြစ်ပေါ်လာနိုင်သော ဘက်လိုက်မှု။
*   **Skewed Prompt Distributions**: prompts များ၏ ဖြန့်ဝေမှုသည် မမျှတခြင်း။
*   **Safety Considerations**: AI စနစ်များ၏ အန္တရာယ်ကင်းရှင်းမှုနှင့် သက်ဆိုင်သော အချက်များ။
*   **Custom Benchmark Suites**: အဖွဲ့အစည်းတစ်ခု၏ သီးခြားလိုအပ်ချက်များနှင့် use cases များကို ဖြည့်ဆည်းရန် ဒီဇိုင်းထုတ်ထားသော benchmarks များ။
*   **Domain-Specific Knowledge Tests**: သီးခြားနယ်ပယ်တစ်ခုရှိ ဗဟုသုတကို စစ်ဆေးသည့် စမ်းသပ်မှုများ။
*   **Deployment Conditions**: model တစ်ခုကို လက်တွေ့ပတ်ဝန်းကျင်တွင် အသုံးပြုသည့် အခြေအနေများ။
*   **Baseline**: နှိုင်းယှဉ်မှုအတွက် အသုံးပြုသော စတင်မှတ် သို့မဟုတ် ရည်ညွှန်းချက်။
*   **Real User Queries**: တကယ့်အသုံးပြုသူများ၏ မေးမြန်းချက်များ။
*   **Edge Cases**: ပုံမှန်မဟုတ်သော သို့မဟုတ် ရှားပါးသော အခြေအနေများ။
*   **Multi-layered Evaluation Strategy**: မတူညီသော evaluation နည်းလမ်းများစွာကို ပေါင်းစပ်အသုံးပြုသော ဗျူဟာ။
*   **Automated Metrics**: ကွန်ပျူတာပရိုဂရမ်များဖြင့် အလိုအလျောက် တွက်ချက်နိုင်သော metrics များ။
*   **Human Evaluation**: လူသားများက model ၏ output များကို အကဲဖြတ်ခြင်း။
*   **Domain Expert Review**: သီးခြားနယ်ပယ်တစ်ခုရှိ ကျွမ်းကျင်သူများက model ကို ပြန်လည်စစ်ဆေးခြင်း။
*   **A/B Testing**: မတူညီသော models သို့မဟုတ် features နှစ်ခု၏ စွမ်းဆောင်ရည်ကို နှိုင်းယှဉ်ရန် အသုံးပြုသော စမ်းသပ်မှု။
*   **Controlled Environments**: ပြောင်းလဲနိုင်သော အကြောင်းအရာများကို ဂရုတစိုက် ထိန်းချုပ်ထားသော ပတ်ဝန်းကျင်။
*   **`lighteval`**: Hugging Face မှ ထုတ်လုပ်ထားသော library တစ်ခုဖြစ်ပြီး LLM များကို standard benchmarks များပေါ်တွင် အကဲဖြတ်ရန် အသုံးပြုသည်။
*   **VLLM Backend**: vLLM library ကို အသုံးပြု၍ LLM inference ကို အရှိန်မြှင့်တင်ရန်အတွက် backend။
*   **`pretrained=your-model-name`**: evaluate လုပ်မည့် pretrained model ၏ နာမည်ကို သတ်မှတ်သည်။
*   **`mmlu|anatomy|0|0`**: MMLU benchmark အတွင်းရှိ 'anatomy' task ကို zero-shot inference ဖြင့် အကဲဖြတ်ရန် LightEval task format။
*   **`--max_samples`**: evaluation အတွက် အများဆုံး samples အရေအတွက်။
*   **`--batch_size`**: evaluation လုပ်ငန်းစဉ်အတွင်း တစ်ပြိုင်နက်တည်း လုပ်ဆောင်မည့် samples အရေအတွက်။
*   **`--output_path`**: evaluation results များကို သိမ်းဆည်းမည့် လမ်းကြောင်း။
*   **`--save_generations`**: model မှ ထုတ်လုပ်သော generations များကို သိမ်းဆည်းမလား။
*   **Tabular Format**: ဇယားပုံစံဖြင့် ပြသထားသော အချက်အလက်များ။
*   **Metric**: Model ၏ စွမ်းဆောင်ရည်ကို တိုင်းတာရန် အသုံးပြုသော တန်ဖိုး (ဥပမာ- `acc` for accuracy)။
*   **Value**: Metric ၏ တန်ဖိုး။
*   **Stderr (Standard Error)**: ခန့်မှန်းထားသော တန်ဖိုး၏ မမှန်ကန်မှုပမာဏကို တိုင်းတာခြင်း။