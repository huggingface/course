# Supervised Fine-Tuning

[Chapter 2 အပိုင်း ၂](/course/chapter2/2) မှာ၊ generative language models တွေကို summarization နဲ့ question answering လို သီးခြား tasks တွေမှာ fine-tune လုပ်နိုင်တယ်ဆိုတာကို ကျွန်တော်တို့ တွေ့ခဲ့ရပါတယ်။ ဒါပေမယ့် ဒီနေ့ခေတ်မှာတော့ language models တွေကို tasks အမျိုးမျိုးမှာ တစ်ပြိုင်နက်တည်း fine-tune လုပ်တာက ပိုပြီး အဖြစ်များပါတယ်။ ဒီနည်းလမ်းကို supervised fine-tuning (SFT) လို့ ခေါ်ပါတယ်။ ဒီလုပ်ငန်းစဉ်က models တွေ ပိုမိုစွမ်းဆောင်နိုင်ပြီး မတူညီတဲ့ use cases တွေကို ကိုင်တွယ်ဖြေရှင်းနိုင်ဖို့ ကူညီပေးပါတယ်။ ChatGPT လို platform တွေမှာ လူတွေ အသုံးပြုနေတဲ့ LLMs အများစုဟာ လူသားတွေရဲ့ နှစ်သက်မှုနဲ့ ပိုမိုကိုက်ညီပြီး အထောက်အကူဖြစ်အောင် SFT ကို ဖြတ်သန်းပြီးသားတွေ ဖြစ်ပါတယ်။ ဒီအခန်းကို အပိုင်းလေးပိုင်း ခွဲခြားပြီး လေ့လာသွားပါမယ်။

## ၁။ Chat Templates များ

Chat templates တွေက အသုံးပြုသူတွေနဲ့ AI models တွေကြား အပြန်အလှန်ဆက်သွယ်မှုတွေကို စနစ်တကျ ပြုလုပ်ပေးပါတယ်။ ဒါက တသမတ်တည်းဖြစ်ပြီး အခြေအနေနဲ့ကိုက်ညီတဲ့ တုံ့ပြန်မှုတွေကို သေချာစေပါတယ်။ ၎င်းတို့မှာ system prompts နဲ့ role-based messages တွေလို အစိတ်အပိုင်းတွေ ပါဝင်ပါတယ်။

## ၂။ Supervised Fine-Tuning

Supervised Fine-Tuning (SFT) ဟာ pre-trained language models တွေကို သီးခြား tasks တွေနဲ့ လိုက်လျောညီထွေဖြစ်အောင် ပြုလုပ်တဲ့ အရေးကြီးတဲ့ လုပ်ငန်းစဉ်တစ်ခု ဖြစ်ပါတယ်။ ဒီအတွက် task-specific dataset တစ်ခုကို labeled examples တွေနဲ့ train လုပ်ရပါတယ်။ SFT အကြောင်း အသေးစိတ်လမ်းညွှန်ချက်တွေ၊ အဓိကအဆင့်တွေနဲ့ အကောင်းဆုံးကျင့်စဉ်တွေအတွက် [TRL documentation ရဲ့ supervised fine-tuning အပိုင်း](https://huggingface.co/docs/trl/en/sft_trainer) ကို ကြည့်ရှုနိုင်ပါတယ်။

## ၃။ LoRA (Low-Rank Adaptation)

Low Rank Adaptation (LoRA) ဟာ model ရဲ့ layers တွေမှာ low-rank matrices တွေကို ထည့်သွင်းခြင်းဖြင့် language models တွေကို fine-tuning လုပ်တဲ့ နည်းပညာတစ်ခု ဖြစ်ပါတယ်။ ဒါက model ရဲ့ pre-trained knowledge ကို ထိန်းသိမ်းထားရင်း ထိရောက်တဲ့ fine-tuning ကို ခွင့်ပြုပါတယ်။ LoRA ရဲ့ အဓိက အကျိုးကျေးဇူးတွေထဲက တစ်ခုကတော့ memory ကို သိသိသာသာ ချွေတာနိုင်တာပဲ ဖြစ်ပါတယ်။ ဒါကြောင့် limited resources ရှိတဲ့ hardware တွေမှာတောင် large models တွေကို fine-tune လုပ်နိုင်ပါတယ်။

## ၄။ Evaluation

Evaluation ဟာ fine-tuning လုပ်ငန်းစဉ်ရဲ့ အရေးကြီးတဲ့ အဆင့်တစ်ခု ဖြစ်ပါတယ်။ ဒါက task-specific dataset ပေါ်မှာ model ရဲ့ စွမ်းဆောင်ရည်ကို တိုင်းတာနိုင်စေပါတယ်။

> [!TIP]
> ⚠️ Model Hub နဲ့ 🤗 Transformers မှာ ရရှိနိုင်တဲ့ features တွေအားလုံးကနေ အကျိုးအပြည့်အဝရရှိဖို့အတွက် [account တစ်ခု ဖန်တီးဖို့](https://huggingface.co/join) ကျွန်တော်တို့ အကြံပြုပါတယ်။

## References

- [Transformers documentation on chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating)
- [Script for Supervised Fine-Tuning in TRL](https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py)
- [`SFTTrainer` in TRL](https://huggingface.co/docs/trl/main/en/sft_trainer)
- [Direct Preference Optimization Paper](https://arxiv.org/abs/2305.18290)
- [Supervised Fine-Tuning with TRL](https://huggingface.co/docs/trl/sft_trainer)
- [How to fine-tune Google Gemma with ChatML and Hugging Face TRL](https://github.com/huggingface/alignment-handbook)
- [Fine-tuning LLM to Generate Persian Product Catalogs in JSON Format](https://huggingface.co/learn/cookbook/en/fine_tuning_llm_to_generate_persian_product_catalogs_in_json_format)

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Supervised Fine-Tuning (SFT)**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို တိကျသောလုပ်ငန်းဆောင်တာများ (specific tasks) အတွက် label ပါသော ဒေတာများကို အသုံးပြု၍ ထပ်မံလေ့ကျင့်ခြင်းနည်းလမ်း။ ၎င်းသည် မော်ဒယ်ကို ပိုမိုစွမ်းဆောင်နိုင်ပြီး ဘက်စုံသုံးနိုင်စေသည်။
*   **Generative Language Models**: စာသားအသစ်များ၊ code သို့မဟုတ် အခြားဒေတာပုံစံများကို ဖန်တီးထုတ်လုပ်နိုင်သော ဘာသာစကားမော်ဒယ်များ။
*   **Summarization**: ရှည်လျားသော စာသားတစ်ခု၏ အနှစ်ချုပ်ကို ထုတ်လုပ်ခြင်း။
*   **Question Answering**: ပေးထားသော စာသားတစ်ခုမှ မေးခွန်းတစ်ခု၏ အဖြေကို ရှာဖွေခြင်း။
*   **Language Models**: လူသားဘာသာစကား၏ ဖြန့်ဝေမှုကို နားလည်ရန် လေ့ကျင့်ထားသော AI မော်ဒယ်တစ်ခု။ ၎င်းသည် စာသားထုတ်လုပ်ခြင်း၊ ဘာသာပြန်ခြင်း စသည့်လုပ်ငန်းများတွင် အသုံးပြုနိုင်သည်။
*   **Tasks**: Artificial Intelligence (AI) သို့မဟုတ် Machine Learning (ML) မော်ဒယ်တစ်ခုက လုပ်ဆောင်ရန် ဒီဇိုင်းထုတ်ထားသော သီးခြားအလုပ်။
*   **Versatile**: ကွဲပြားသော အလုပ်များ သို့မဟုတ် အခြေအနေများစွာကို ကိုင်တွယ်နိုင်စွမ်းရှိခြင်း။
*   **Use Cases**: ထုတ်ကုန် သို့မဟုတ် စနစ်တစ်ခုကို သီးခြားအခြေအနေတစ်ခုတွင် မည်သို့အသုံးပြုသည်ကို ဖော်ပြခြင်း။
*   **LLMs (Large Language Models)**: လူသားဘာသာစကားကို နားလည်ပြီး ထုတ်လုပ်ပေးနိုင်တဲ့ အလွန်ကြီးမားတဲ့ Artificial Intelligence (AI) မော်ဒယ်တွေ ဖြစ်ပါတယ်။
*   **ChatGPT**: OpenAI မှ ဖန်တီးထားသော လူသားနှင့်ဆင်တူသော စာသားများကို ဖန်တီးနိုင်သည့် conversational AI မော်ဒယ်။
*   **Human Preferences**: လူသားများ၏ နှစ်သက်မှုများ သို့မဟုတ် ရွေးချယ်မှုများ။
*   **Chat Templates**: အသုံးပြုသူနှင့် AI မော်ဒယ်များကြား အပြန်အလှန်ဆက်သွယ်မှုများကို စနစ်တကျ ပြုလုပ်ပေးသည့် ဖွဲ့စည်းပုံများ။ ၎င်းတို့သည် တသမတ်တည်းဖြစ်ပြီး အခြေအနေနှင့်ကိုက်ညီသော တုံ့ပြန်မှုများကို သေချာစေသည်။
*   **System Prompts**: AI မော်ဒယ်တစ်ခုအား ၎င်း၏ အခန်းကဏ္ဍ၊ ပုံစံ သို့မဟုတ် လုပ်ဆောင်ရမည့်အရာများကို လမ်းညွှန်ပေးသည့် မူလညွှန်ကြားချက်များ။
*   **Role-based Messages**: AI model နှင့် အသုံးပြုသူတို့၏ သတ်မှတ်ထားသော အခန်းကဏ္ဍများ (ဥပမာ- user, assistant) အပေါ် အခြေခံ၍ ပေးပို့သော messages များ။
*   **Pre-trained Language Models**: အကြီးစား ဒေတာအမြောက်အမြားဖြင့် ကြိုတင်လေ့ကျင့်ထားပြီးဖြစ်သော ဘာသာစကားမော်ဒယ်များ။
*   **Task-specific Dataset**: သီးခြားလုပ်ငန်းတစ်ခု (ဥပမာ- sentiment analysis) အတွက် အထူးပြင်ဆင်ထားသော ဒေတာအစုအဝေး။
*   **Labeled Examples**: labels များ သို့မဟုတ် မှန်ကန်သောအဖြေများ ပါဝင်သော training data များ။
*   **TRL Documentation**: Hugging Face Transformes Reinforcement Learning (TRL) library ၏ တရားဝင် မှတ်တမ်းများ (documentation)။
*   **LoRA (Low-Rank Adaptation)**: Transformer မော်ဒယ်များကဲ့သို့သော large models များကို fine-tuning လုပ်ရာတွင် ထိရောက်မှုရှိစေရန်အတွက် model ၏ layers တွေမှာ low-rank matrices တွေကို ထပ်ထည့်သည့် နည်းပညာ။ ၎င်းသည် memory အသုံးပြုမှုကို သိသိသာသာ လျှော့ချနိုင်သည်။
*   **Low-Rank Matrices**: သင်္ချာပိုင်းဆိုင်ရာ matrix တစ်မျိုးဖြစ်ပြီး ၎င်း၏ rank သည် ၎င်း၏ dimensions များထက် သိသိသာသာ နည်းပါးသည်။ Machine Learning တွင် parameters အရေအတွက်ကို လျှော့ချရန် အသုံးပြုသည်။
*   **Model's Layers**: Neural network model တစ်ခု၏ အဆင့်များ။
*   **Pre-trained Knowledge**: မော်ဒယ်အား မူလ pre-training လုပ်ငန်းစဉ်မှ သင်ယူထားသော ဗဟုသုတများ။
*   **Memory Savings**: ကွန်ပျူတာ၏ RAM အသုံးပြုမှုကို လျှော့ချနိုင်ခြင်း။
*   **Hardware with Limited Resources**: ကွန်ပျူတာ၏ memory (RAM) သို့မဟုတ် processing power (GPU) အစွမ်းအစ အကန့်အသတ်ရှိသော devices များ။
*   **Evaluation**: fine-tuning လုပ်ငန်းစဉ်ပြီးနောက် model ၏ စွမ်းဆောင်ရည်ကို တိုင်းတာခြင်း။ ၎င်းသည် model ၏ ထိရောက်မှုနှင့် တိကျမှုကို ဆုံးဖြတ်ရန် ကူညီပေးသည်။
*   **Model Hub**: Hugging Face Hub ကို ရည်ညွှန်းပြီး AI မော်ဒယ်များ ရှာဖွေ၊ မျှဝေ၊ အသုံးပြုနိုင်သော ဗဟို platform။
*   **🤗 Transformers**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး Transformer မော်ဒယ်တွေကို အသုံးပြုပြီး Natural Language Processing (NLP), computer vision, audio processing စတဲ့ နယ်ပယ်တွေမှာ အဆင့်မြင့် AI မော်ဒယ်တွေကို တည်ဆောက်ပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **Account**: Hugging Face Hub ပေါ်ရှိ သုံးစွဲသူအကောင့်။
*   **Script**: အလိုအလျောက်လုပ်ဆောင်ရန် ရေးသားထားသော code များ။
*   **SFTTrainer**: TRL library မှ `Trainer` class ၏ extension တစ်ခုဖြစ်ပြီး Supervised Fine-Tuning လုပ်ငန်းစဉ်ကို ရိုးရှင်းစေသည်။
*   **Direct Preference Optimization (DPO)**: Reinforcement Learning from Human Feedback (RLHF) အတွက် simplified algorithm တစ်ခုဖြစ်ပြီး model output များကို လူသားနှစ်သက်မှုနှင့် ပိုမိုကိုက်ညီအောင် လုပ်ဆောင်ပေးသည်။
*   **Google Gemma**: Google မှ ထုတ်လုပ်ထားသော open-source LLM တစ်မျိုး။
*   **ChatML**: OpenAI မှ ထုတ်လုပ်ထားသော chat conversation များကို ကိုယ်စားပြုရန်အတွက် markup format တစ်ခု။
*   **Alignment Handbook**: Hugging Face မှ LLM များကို လူသားနှစ်သက်မှုနှင့် ကိုက်ညီအောင် လေ့ကျင့်ရန်အတွက် လမ်းညွှန်စာတမ်း။
*   **Persian Product Catalogs**: ပါရှန်ဘာသာစကားဖြင့် ထုတ်ကုန်စာရင်းများ။
*   **JSON Format**: ဒေတာများကို ပေါ့ပေါ့ပါးပါး ဖလှယ်နိုင်သော format ဖြစ်ပြီး လူသားများ ဖတ်ရှုရလွယ်ကူပြီး စက်များ စီမံဆောင်ရွက်ရလွယ်ကူသည်။