<!-- DISABLE-FRONTMATTER-SECTIONS -->

# အခန်း (၅) ဆိုင်ရာ မေးခွန်းများ[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={5}
    classNames="absolute z-10 right-0 top-0"
/>

ဒီအခန်းမှာ အချက်အလက်များစွာကို ဖော်ပြခဲ့ပါတယ်။ အသေးစိတ်အချက်အလက်အားလုံးကို နားမလည်သေးရင်လည်း စိတ်မပူပါနဲ့၊ နောက်အခန်းတွေက အတွင်းပိုင်းလုပ်ဆောင်မှုတွေကို နားလည်အောင် ကူညီပေးပါလိမ့်မယ်။

ဒါပေမယ့် ဆက်မသွားခင်၊ ဒီအခန်းမှာ သင်ယူခဲ့တာတွေကို စစ်ဆေးကြည့်ရအောင်။

### ၁။ 🤗 Datasets မှာရှိတဲ့ `load_dataset()` function က အောက်ပါနေရာတွေထဲက ဘယ်နေရာကနေ dataset တစ်ခုကို load လုပ်နိုင်စေသလဲ။

<Question
	choices={[
		{
			text: "Locally၊ ဥပမာ သင့် laptop ပေါ်ကနေ",
			explain: "မှန်ပါတယ်။ local datasets တွေကို load လုပ်ဖို့အတွက် local files တွေရဲ့ paths တွေကို `load_dataset()` ရဲ့ `data_files` argument ကို ပေးနိုင်ပါတယ်။",
			correct: true
		},
		{
			text: "Hugging Face Hub ကနေ",
			explain: "မှန်ပါတယ်။ dataset ID ကို ပေးခြင်းဖြင့် Hub ပေါ်က datasets တွေကို load လုပ်နိုင်ပါတယ်။ ဥပမာ- <code>load_dataset('emotion')</code>။",
			correct: true
		},
		{
			text: "Remote server တစ်ခုကနေ",
			explain: "မှန်ပါတယ်။ remote files တွေကို load လုပ်ဖို့အတွက် URLs တွေကို `load_dataset()` ရဲ့ `data_files` argument ကို ပေးနိုင်ပါတယ်။",
			correct: true
		},
	]}
/>

### ၂။ အောက်ပါအတိုင်း GLUE tasks ထဲက တစ်ခုကို load လုပ်တယ်လို့ ယူဆပါစို့-

```py
from datasets import load_dataset

dataset = load_dataset("glue", "mrpc", split="train")
```

အောက်ပါ commands တွေထဲက ဘယ်ဟာက `dataset` ကနေ elements ၅၀ ကို random sample အဖြစ် ထုတ်လုပ်ပေးမလဲ။

<Question
	choices={[
		{
			text: "<code>dataset.sample(50)</code>",
			explain: "ဒါက မမှန်ပါဘူး -- <code>Dataset.sample()</code> method မရှိပါဘူး။"
		},
		{
			text: "<code>dataset.shuffle().select(range(50))</code>",
			explain: "မှန်ပါတယ်။ ဒီအခန်းမှာ သင်တွေ့ခဲ့ရတဲ့အတိုင်း၊ သင်ဟာ dataset ကို အရင် shuffle လုပ်ပြီးမှ ၎င်းကနေ samples တွေကို ရွေးထုတ်တာပါ။",
			correct: true
		},
		{
			text: "<code>dataset.select(range(50)).shuffle()</code>",
			explain: "ဒါက မမှန်ပါဘူး -- code က run မှာဖြစ်ပေမယ့်၊ dataset ထဲက ပထမဆုံး elements ၅၀ ကိုပဲ shuffle လုပ်ပါလိမ့်မယ်။"
		}
	]}
/>

### ၃။ `pets_dataset` လို့ခေါ်တဲ့ အိမ်မွေးတိရစ္ဆာန်တွေနဲ့ ပတ်သက်တဲ့ dataset တစ်ခုရှိပြီး၊ တိရစ္ဆာန်တစ်ခုစီရဲ့ နာမည်ကို ဖော်ပြတဲ့ `name` column ပါဝင်တယ်လို့ ယူဆပါ။ အောက်ပါနည်းလမ်းတွေထဲက ဘယ်ဟာက နာမည် "L" စာလုံးနဲ့ စတင်တဲ့ တိရစ္ဆာန်တွေအားလုံးအတွက် dataset ကို filter လုပ်နိုင်စေမှာလဲ။

<Question
	choices={[
		{
			text: "<code>pets_dataset.filter(lambda x : x['name'].startswith('L'))</code>",
			explain: "မှန်ပါတယ်။ ဒီလို မြန်ဆန်တဲ့ filters တွေအတွက် Python lambda function ကို အသုံးပြုတာက အကောင်းဆုံးပါပဲ။ တခြားဖြေရှင်းနည်းတစ်ခုကို စဉ်းစားနိုင်မလား။",
			correct: true
		},
		{
			text: "<code>pets_dataset.filter(lambda x['name'].startswith('L'))</code>",
			explain: "ဒါက မမှန်ပါဘူး -- lambda function တစ်ခုက <code>lambda *arguments* : *expression*</code> ပုံစံရှိတာကြောင့်၊ ဒီကိစ္စမှာ arguments တွေ ပေးဖို့လိုပါတယ်။"
		},
		{
			text: "<code>def filter_names(x): return x['name'].startswith('L')</code> လို function တစ်ခု ဖန်တီးပြီး <code>pets_dataset.filter(filter_names)</code> ကို run ပါ။",
			explain: "မှန်ပါတယ်။ <code>Dataset.map()</code> နဲ့ အတူတူပါပဲ၊ သင်ဟာ <code>Dataset.filter()</code> ကို explicit functions တွေ ပေးနိုင်ပါတယ်။ ဒါက တိုတောင်းတဲ့ lambda function နဲ့ မသင့်လျော်တဲ့ ရှုပ်ထွေးတဲ့ logic တွေရှိတဲ့အခါ အသုံးဝင်ပါတယ်။ တခြားဘယ်ဖြေရှင်းနည်းတွေ အလုပ်ဖြစ်မလဲ။",
			correct: true
		}
	]}
/>

### ၄။ Memory mapping ဆိုတာ ဘာလဲ။

<Question
	choices={[
		{
			text: "CPU နဲ့ GPU RAM ကြားက mapping တစ်ခု",
			explain: "ဒါက မဟုတ်ပါဘူး -- ထပ်ကြိုးစားပါ။",
		},
		{
			text: "RAM နဲ့ filesystem storage ကြားက mapping တစ်ခု",
			explain: "မှန်ပါတယ်။ 🤗 Datasets က dataset တစ်ခုစီကို memory-mapped file တစ်ခုလို သတ်မှတ်ပါတယ်။ ဒါက library ကို dataset ရဲ့ elements တွေကို memory ထဲကို အပြည့်အဝ load လုပ်ဖို့ မလိုဘဲ ဝင်ရောက်ကြည့်ရှုပြီး လုပ်ဆောင်နိုင်စေပါတယ်။",
			correct: true
		},
		{
			text: "🤗 Datasets cache ထဲက files နှစ်ခုကြားက mapping တစ်ခု",
			explain: "ဒါက မမှန်ပါဘူး -- ထပ်ကြိုးစားပါ။"
		}
	]}
/>

### ၅။ အောက်ပါတို့ထဲက ဘယ်အရာတွေက memory mapping ရဲ့ အဓိက အကျိုးကျေးဇူးတွေလဲ။

<Question
	choices={[
		{
			text: "memory-mapped files တွေကို ဝင်ရောက်ကြည့်ရှုတာက disk ကနေ ဖတ်တာ ဒါမှမဟုတ် disk ကို ရေးတာထက် ပိုမြန်ပါတယ်။",
			explain: "မှန်ပါတယ်။ ဒါက 🤗 Datasets ကို အလွန်လျင်မြန်စေပါတယ်။ ဒါပေမယ့် ဒါတစ်ခုတည်းသော အကျိုးကျေးဇူးတော့ မဟုတ်ပါဘူး။",
			correct: true
		},
		{
			text: "Applications တွေဟာ အလွန်ကြီးမားတဲ့ file တစ်ခုထဲက data segments တွေကို file တစ်ခုလုံးကို RAM ထဲကို အရင်ဖတ်စရာ မလိုဘဲ ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်။",
			explain: "မှန်ပါတယ်။ ဒါက 🤗 Datasets ကို multi-gigabyte datasets တွေကို သင့် laptop ပေါ်မှာ CPU ကို မပိတ်မိစေဘဲ load လုပ်နိုင်စေပါတယ်။ memory mapping က တခြားဘာ အကျိုးကျေးဇူးတွေ ပေးလဲ။",
			correct: true
		},
		{
			text: "စွမ်းအင် နည်းနည်းပဲ သုံးစွဲတာကြောင့် သင့်ဘက်ထရီက ပိုကြာကြာ ခံပါတယ်။",
			explain: "ဒါက မမှန်ပါဘူး -- ထပ်ကြိုးစားပါ။"
		}
	]}
/>

### ၆။ အောက်ပါ code က ဘာကြောင့် အလုပ်မလုပ်တာလဲ။

```py
from datasets import load_dataset

dataset = load_dataset("allocine", streaming=True, split="train")
dataset[0]
```

<Question
	choices={[
		{
			text: "RAM ထဲကို မဆံ့လောက်အောင် ကြီးမားတဲ့ dataset ကို stream လုပ်ဖို့ ကြိုးစားနေတာ။",
			explain: "ဒါက မမှန်ပါဘူး -- streaming datasets တွေကို on the fly မှာ decompress လုပ်တာဖြစ်ပြီး၊ terabyte-sized datasets တွေကို RAM အနည်းငယ်နဲ့ လုပ်ဆောင်နိုင်ပါတယ်!",
		},
		{
			text: "<code>IterableDataset</code> ကို ဝင်ရောက်ကြည့်ရှုဖို့ ကြိုးစားနေတာ။",
			explain: "မှန်ပါတယ်။ <code>IterableDataset</code> ဆိုတာ generator တစ်ခုဖြစ်ပြီး container မဟုတ်ပါဘူး၊ ဒါကြောင့် ၎င်းရဲ့ elements တွေကို <code>next(iter(dataset))</code> ကို အသုံးပြုပြီး ဝင်ရောက်ကြည့်ရှုသင့်ပါတယ်။",
			correct: true
		},
		{
			text: "<code>allocine</code> dataset မှာ <code>train</code> split မရှိပါဘူး။",
			explain: "ဒါက မမှန်ပါဘူး -- Hub ပေါ်က [<code>allocine</code> dataset card](https://huggingface.co/datasets/allocine) ကို ကြည့်ပြီး ၎င်းမှာ ဘယ် splits တွေ ပါဝင်လဲဆိုတာ စစ်ဆေးပါ။"
		}
	]}
/>

### ၇။ Dataset card တစ်ခု ဖန်တီးခြင်းရဲ့ အဓိက အကျိုးကျေးဇူးတွေက ဘာတွေလဲ။

<Question
	choices={[
		{
			text: "ဒါက dataset ရဲ့ ရည်ရွယ်အသုံးပြုပုံနဲ့ ထောက်ပံ့ထားတဲ့ tasks တွေအကြောင်း အချက်အလက်တွေ ပေးတာကြောင့် community ထဲက တခြားသူတွေက ဒါကို အသုံးပြုဖို့ အသိပေးဆုံးဖြတ်ချက် ချနိုင်ပါတယ်။",
			explain: "မှန်ပါတယ်။ မှတ်တမ်းမရှိတဲ့ datasets တွေဟာ dataset ဖန်တီးသူတွေရဲ့ ရည်ရွယ်ချက်တွေကို ထင်ဟပ်နိုင်ခြင်းမရှိတဲ့ models တွေကို train လုပ်ဖို့ အသုံးပြုနိုင်ပါတယ်၊ ဒါမှမဟုတ် privacy သို့မဟုတ် licensing ကန့်သတ်ချက်တွေကို ချိုးဖောက်တဲ့ data တွေပေါ်မှာ train လုပ်ထားရင် ဥပဒေရေးရာ မရှင်းလင်းတဲ့ models တွေကို ထုတ်လုပ်နိုင်ပါတယ်။ ဒါပေမယ့် ဒါတစ်ခုတည်းသော အကျိုးကျေးဇူးတော့ မဟုတ်ပါဘူး!",
			correct : true
		},
		{
			text: "corpus ထဲမှာ ရှိနေတဲ့ ဘက်လိုက်မှုတွေကို အာရုံစိုက်မိစေဖို့ ကူညီပေးပါတယ်။",
			explain: "မှန်ပါတယ်။ datasets အားလုံးနီးပါးမှာ ဘက်လိုက်မှုပုံစံအချို့ ပါဝင်ပြီး၊ ဒါတွေက အောက်ဘက်မှာ အနုတ်လက္ခဏာဆောင်တဲ့ အကျိုးဆက်တွေ ဖြစ်ပေါ်စေနိုင်ပါတယ်။ ဒါတွေကို သိရှိနားလည်ခြင်းက model တည်ဆောက်သူတွေကို မွေးရာပါ ဘက်လိုက်မှုတွေကို ဘယ်လိုဖြေရှင်းရမလဲဆိုတာ နားလည်စေပါတယ်။ dataset cards တွေက တခြားဘာတွေ ကူညီပေးသေးလဲ။",
			correct : true
		},
		{
			text: "community ထဲက တခြားသူတွေက ကျွန်ုပ်ရဲ့ dataset ကို အသုံးပြုနိုင်ခြေကို တိုးတက်စေပါတယ်။",
			explain: "မှန်ပါတယ်။ ကောင်းကောင်းရေးထားတဲ့ dataset card တစ်ခုက သင့်ရဲ့ တန်ဖိုးရှိတဲ့ dataset ကို ပိုမိုအသုံးပြုလာစေဖို့ ဦးဆောင်ပါလိမ့်မယ်။ တခြားဘာ အကျိုးကျေးဇူးတွေ ပေးသေးလဲ။",
			correct: true
		},
	]}
/>

### ၈။ Semantic search ဆိုတာ ဘာလဲ။

<Question
	choices={[
		{
			text: "query ထဲက စကားလုံးတွေနဲ့ corpus ထဲက documents တွေကြား တိကျတဲ့ ကိုက်ညီမှုတွေကို ရှာဖွေတဲ့ နည်းလမ်းတစ်ခု။",
			explain: "ဒါက မမှန်ပါဘူး -- ဒီလို ရှာဖွေမှုကို *lexical search* လို့ခေါ်ပြီး၊ ရိုးရာ search engines တွေမှာ သင်ပုံမှန်တွေ့ရတာပါ။"
		},
		{
			text: "query ရဲ့ contextual meaning ကို နားလည်ခြင်းဖြင့် ကိုက်ညီတဲ့ documents တွေကို ရှာဖွေတဲ့ နည်းလမ်းတစ်ခု။",
			explain: "မှန်ပါတယ်။ Semantic search က embedding vectors တွေကို အသုံးပြုပြီး queries နဲ့ documents တွေကို ကိုယ်စားပြုကာ၊ ၎င်းတို့ကြားက ထပ်နေမှုပမာဏကို တိုင်းတာဖို့ similarity metric ကို အသုံးပြုပါတယ်။ တခြားဘယ်လို ဖော်ပြနိုင်သေးလဲ။",
			correct: true
		},
		{
			text: "search accuracy ကို တိုးတက်စေတဲ့ နည်းလမ်းတစ်ခု။",
			explain: "မှန်ပါတယ်။ Semantic search engines တွေက keyword matching ထက် query ရဲ့ ရည်ရွယ်ချက်ကို ပိုကောင်းကောင်း နားလည်နိုင်ပြီး ပုံမှန်အားဖြင့် ပိုမိုမြင့်မားတဲ့ precision နဲ့ documents တွေကို ပြန်လည်ရယူပါတယ်။ ဒါပေမယ့် ဒါတစ်ခုတည်းသော အဖြေမှန် မဟုတ်ပါဘူး -- semantic search က တခြားဘာတွေ ပံ့ပိုးပေးသေးလဲ။",
			correct: true
		}
	]}
/>

### ၉။ Asymmetric semantic search အတွက်၊ သင်အများအားဖြင့် ဘာတွေရှိလဲ။

<Question
	choices={[
		{
			text: "တိုတောင်းသော query တစ်ခုနဲ့ query ကို ဖြေကြားပေးတဲ့ ပိုရှည်တဲ့ paragraph တစ်ခု။",
			explain: "မှန်ပါတယ်!",
			correct : true
		},
		{
			text: "query တွေနဲ့ paragraphs တွေက အရှည်တူညီလောက်ပါတယ်။",
			explain: "ဒါက symmetric semantic search ရဲ့ ဥပမာတစ်ခုပါ -- ထပ်ကြိုးစားပါ။"
		},
		{
			text: "ရှည်လျားသော query တစ်ခုနဲ့ query ကို ဖြေကြားပေးတဲ့ ပိုတိုတောင်းတဲ့ paragraph တစ်ခု။",
			explain: "ဒါက မမှန်ပါဘူး -- ထပ်ကြိုးစားပါ။"
		}
	]}
/>

### ၁၀။ 🤗 Datasets ကို speech processing လိုမျိုး အခြား domains တွေမှာ အသုံးပြုဖို့ data တွေ load လုပ်ဖို့ အသုံးပြုနိုင်မလား။

<Question
	choices={[
		{
			text: "မရပါဘူး။",
			explain: "ဒါက မမှန်ပါဘူး -- 🤗 Datasets က လက်ရှိမှာ tabular data, audio နဲ့ computer vision တွေကို ထောက်ပံ့ပေးပါတယ်။ computer vision ဥပမာအတွက် Hub ပေါ်က <a  href='https://huggingface.co/datasets/mnist'>MNIST dataset</a> ကို ကြည့်ပါ။"
		},
		{
			text: "ရပါတယ်။",
			explain: "မှန်ပါတယ်။ 🤗 Transformers library မှာ speech နဲ့ vision နဲ့ပတ်သက်တဲ့ စိတ်လှုပ်ရှားဖွယ် တိုးတက်မှုတွေကို ကြည့်ပြီး 🤗 Datasets ကို ဒီ domains တွေမှာ ဘယ်လိုအသုံးပြုလဲဆိုတာ ကြည့်ပါ။",
			correct : true
		},
	]}
/>

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **🤗 Datasets Library**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် ဒေတာအစုအဝေး (datasets) တွေကို လွယ်လွယ်ကူကူ ဝင်ရောက်ရယူ၊ စီမံခန့်ခွဲပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **`load_dataset()` Function**: Hugging Face Datasets library မှ dataset များကို download လုပ်ပြီး cache လုပ်ရန် အသုံးပြုသော function။
*   **Locally**: သင့်ကွန်ပျူတာ (laptop သို့မဟုတ် desktop) ၏ hard drive ပေါ်တွင်။
*   **Laptop**: သယ်ဆောင်ရလွယ်ကူသော ကိုယ်ပိုင်ကွန်ပျူတာ။
*   **`data_files` Argument**: `load_dataset()` function တွင် dataset files (local သို့မဟုတ် remote) ၏ path (သို့မဟုတ် URL) ကို သတ်မှတ်ရန် အသုံးပြုသော argument။
*   **Hugging Face Hub**: AI မော်ဒယ်တွေ၊ datasets တွေနဲ့ demo တွေကို အခြားသူတွေနဲ့ မျှဝေဖို့၊ ရှာဖွေဖို့နဲ့ ပြန်လည်အသုံးပြုဖို့အတွက် အွန်လိုင်း platform တစ်ခု ဖြစ်ပါတယ်။
*   **Dataset ID**: Hugging Face Hub ပေါ်ရှိ dataset တစ်ခု၏ ထူးခြားသော ဖော်ထုတ်ကိန်း (identifier)။
*   **Remote Server**: ကွန်ရက်တစ်ခုပေါ်တွင် ဝန်ဆောင်မှုများ သို့မဟုတ် အရင်းအမြစ်များကို ပံ့ပိုးပေးသော ကွန်ပျူတာ။
*   **URL (Uniform Resource Locator)**: web ပေါ်ရှိ အရင်းအမြစ်တစ်ခု (ဥပမာ- web page, file) ၏ လိပ်စာ။
*   **GLUE Tasks (General Language Understanding Evaluation Tasks)**: စာသားခွဲခြားသတ်မှတ်ခြင်း လုပ်ငန်း ၁၀ ခုတွင် ML model များ၏ စွမ်းဆောင်ရည်ကို တိုင်းတာရန် အသုံးပြုသည့် academic benchmark တစ်ခု။
*   **`mrpc` (Microsoft Research Paraphrase Corpus)**: GLUE benchmark ထဲက paraphrase detection task တစ်ခု။
*   **`split="train"`**: dataset ရဲ့ training split ကို ရွေးချယ်ခြင်း။
*   **Random Sample**: dataset တစ်ခုမှ ကျပန်းရွေးချယ်ထားသော elements များ။
*   **`Dataset.sample()` Method**: `Dataset` object မှာ မရှိပါ။
*   **`Dataset.shuffle()` Method**: dataset အတွင်းရှိ elements များကို ကျပန်းရောနှော (shuffle) ရန် အသုံးပြုသော method။
*   **`Dataset.select(range(50))` Method**: dataset ၏ ပထမဆုံး elements ၅၀ ကို ရွေးထုတ်ရန် အသုံးပြုသော method။
*   **`Dataset.filter()` Method**: 🤗 Datasets library မှာ ပါဝင်တဲ့ method တစ်ခုဖြစ်ပြီး သတ်မှတ်ထားသော အခြေအနေများနှင့် ကိုက်ညီသော ဒေတာများကိုသာ dataset မှ ရွေးထုတ်ရန် အသုံးပြုသည်။
*   **Lambda Function (Python Lambda)**: အမည်မရှိသော (anonymous) function တစ်ခုဖြစ်ပြီး code လိုင်းတစ်ကြောင်းတည်းဖြင့် သတ်မှတ်နိုင်သည်။
*   **`x['name'].startswith('L')`**: dictionary `x` အတွင်းရှိ `name` key ၏ value သည် 'L' ဖြင့် စတင်ခြင်းရှိမရှိ စစ်ဆေးသော Python expression။
*   **Memory Mapping**: ဖိုင်တစ်ခု၏ အကြောင်းအရာများကို ကွန်ပျူတာ၏ virtual memory နေရာသို့ တိုက်ရိုက်ချိတ်ဆက်ပေးသည့် နည်းလမ်း။
*   **CPU (Central Processing Unit)**: ကွန်ပျူတာ၏ ပင်မ processor။
*   **GPU (Graphics Processing Unit)**: ဂရပ်ဖစ်လုပ်ဆောင်မှုအတွက် အထူးဒီဇိုင်းထုတ်ထားသော processor တစ်မျိုးဖြစ်သော်လည်း AI/ML လုပ်ငန်းများတွင် အရှိန်မြှင့်ရန် အသုံးများသည်။
*   **RAM (Random Access Memory)**: ကွန်ပျူတာ၏ ယာယီမှတ်ဉာဏ်သိုလှောင်ရာနေရာ။
*   **Filesystem Storage**: ကွန်ပျူတာ၏ hard disk သို့မဟုတ် solid-state drive (SSD) ကဲ့သို့သော အမြဲတမ်းသိုလှောင်ရာနေရာ။
*   **🤗 Datasets Cache**: 🤗 Datasets library မှ download လုပ်ထားသော datasets များနှင့် processing လုပ်ထားသော ဒေတာများကို ယာယီသိမ်းဆည်းထားသော နေရာ။
*   **Blazing Fast**: အလွန်လျင်မြန်စွာ လုပ်ဆောင်ခြင်း။
*   **Multi-gigabyte Datasets**: gigabyte အရွယ်အစားများစွာရှိသော datasets များ။
*   **CPU (Central Processing Unit)**: ကွန်ပျူတာ၏ ပင်မ processor။ (ဤနေရာတွင် "blowing up your CPU" ဆိုသည်မှာ CPU ကို အလွန်အမင်း ဝန်ပိစေခြင်းကို ဆိုလိုသည်)။
*   **`streaming=True`**: `load_dataset()` function တွင် dataset ကို memory ထဲသို့ တစ်ခါတည်း အားလုံး load မလုပ်ဘဲ၊ လိုအပ်သလို အပိုင်းလိုက် stream လုပ်ရန် သတ်မှတ်ခြင်း။
*   **`IterableDataset`**: 🤗 Datasets library ၏ class တစ်ခုဖြစ်ပြီး dataset ကို generator တစ်ခုအဖြစ် လုပ်ဆောင်စေကာ memory ထဲသို့ ဒေတာအားလုံးကို တစ်ခါတည်း load မလုပ်ဘဲ လိုအပ်သလို ထုတ်ပေးသည်။
*   **Generator**: Python တွင် iteration လုပ်နိုင်သော object တစ်ခုဖြစ်ပြီး ၎င်းသည် အရာအားလုံးကို memory ထဲသို့ တစ်ပြိုင်နက်တည်း သိမ်းဆည်းမထားဘဲ လိုအပ်သလို တန်ဖိုးများကို ထုတ်ပေးသည်။
*   **Container**: Python တွင် elements များကို သိမ်းဆည်းထားနိုင်သော object (ဥပမာ- list, tuple, dictionary)။
*   **`next(iter(dataset))`**: `IterableDataset` မှ နောက်ထပ် element တစ်ခုကို ရယူရန် အသုံးပြုသော Python code။
*   **`allocine` Dataset**: Hugging Face Hub ပေါ်ရှိ dataset တစ်ခု (ပြင်သစ်ရုပ်ရှင် reviews များ ပါဝင်နိုင်သည်)။
*   **Dataset Card**: Hugging Face Hub တွင် dataset တစ်ခုစီအတွက် ပါရှိသော အချက်အလက်များပါသည့် စာမျက်နှာ။
*   **Intended Use**: Dataset ကို အသုံးပြုရန် ရည်ရွယ်ထားသော ကိစ္စရပ်များ။
*   **Supported Tasks**: Dataset ကို အသုံးပြု၍ လုပ်ဆောင်နိုင်သော လုပ်ငန်းများ။
*   **Informed Decision**: အချက်အလက်အပြည့်အစုံကို အခြေခံပြီး ဆုံးဖြတ်ချက်ချခြင်း။
*   **Undocumented Datasets**: အသုံးပြုပုံ၊ ကန့်သတ်ချက်များ သို့မဟုတ် ဘက်လိုက်မှုများအတွက် တရားဝင်မှတ်တမ်းမရှိသော datasets များ။
*   **Legal Status**: ဥပဒေရေးရာ အခြေအနေ။
*   **Murky**: ရှင်းရှင်းလင်းလင်း မရှိခြင်း၊ မရေမရာဖြစ်ခြင်း။
*   **Privacy Restrictions**: ကိုယ်ရေးကိုယ်တာ အချက်အလက်များနှင့် ပတ်သက်သော ကန့်သတ်ချက်များ။
*   **Licensing Restrictions**: လိုင်စင်နှင့် ပတ်သက်သော ကန့်သတ်ချက်များ။
*   **Corpus**: စာသား (သို့မဟုတ် အခြားဒေတာ) အစုအဝေးကြီးတစ်ခု။
*   **Bias**: Model တစ်ခု၏ ခန့်မှန်းချက်များတွင် ဒေတာ သို့မဟုတ် သင်္ချာဆိုင်ရာ အကြောင်းများကြောင့် ဖြစ်ပေါ်လာသော ဘက်လိုက်မှုများ။
*   **Negative Consequences Downstream**: နောက်ဆက်တွဲအဆင့်များတွင် ဖြစ်ပေါ်လာနိုင်သော အနုတ်လက္ခဏာဆောင်သည့် ရလဒ်များ။
*   **Semantic Search**: စာလုံးများကို ကိုက်ညီမှု ရှာဖွေခြင်းထက် အဓိပ္ပာယ်ပေါ်မူတည်၍ ရှာဖွေနိုင်သော search engine။
*   **Lexical Search**: စကားလုံးများကို တိကျသော ကိုက်ညီမှုအပေါ် အခြေခံ၍ ရှာဖွေခြင်း။
*   **Query**: search engine တွင် ရှာဖွေရန် ထည့်သွင်းသော စကားလုံး သို့မဟုတ် စာကြောင်း။
*   **Documents**: ရှာဖွေရန် စုစည်းထားသော စာသားအချက်အလက်များ။
*   **Contextual Meaning**: စာသားတစ်ခု၏ အကြောင်းအရာအလိုက် အဓိပ္ပာယ်။
*   **Embedding Vectors**: စာသား သို့မဟုတ် အခြားဒေတာများကို ဂဏန်းဆိုင်ရာ vector များအဖြစ် ကိုယ်စားပြုခြင်း။
*   **Similarity Metric**: elements နှစ်ခုကြား ဆင်တူမှုပမာဏကို တိုင်းတာသော တန်ဖိုး။
*   **Overlap**: အရာနှစ်ခုကြား တူညီသော သို့မဟုတ် ထပ်နေသော အစိတ်အပိုင်းများ။
*   **Search Accuracy**: ရှာဖွေမှုရလဒ်များ၏ မှန်ကန်မှုပမာဏ။
*   **Keyword Matching**: search query ထဲက စကားလုံးတွေနဲ့ document ထဲက စကားလုံးတွေ တိကျစွာ ကိုက်ညီမှုကို ရှာဖွေခြင်း။
*   **Precision**: search results များထဲမှ သက်ဆိုင်ရာရလဒ်များ၏ ရာခိုင်နှုန်း။
*   **Asymmetric Semantic Search**: Query နှင့် Document များ၏ အရှည် သို့မဟုတ် ပုံစံ ကွာခြားသည့် semantic search အမျိုးအစား (ဥပမာ- တိုတိုလေး query နှင့် ရှည်လျားသော document)။
*   **Symmetric Semantic Search**: Query နှင့် Document များ၏ အရှည် သို့မဟုတ် ပုံစံ တူညီသည့် semantic search အမျိုးအစား။
*   **Tabular Data**: جداول ပုံစံဖြင့် စုစည်းထားသော ဒေတာ (rows and columns)။
*   **Audio (Speech Processing)**: အသံအချက်အလက်များကို AI စနစ်များဖြင့် စီမံဆောင်ရွက်ခြင်း။
*   **Computer Vision**: ကွန်ပျူတာများကို ပုံရိပ်များ သို့မဟုတ် ဗီဒီယိုများမှ အချက်အလက်များ နားလည်စေရန် သင်ကြားပေးခြင်း။
*   **MNIST Dataset**: handwritten digits များပါဝင်သော computer vision dataset တစ်ခု။