<FrameworkSwitchCourse {fw} />

# FAISS á€–á€¼á€„á€·á€º Semantic Search á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸[[semantic-search-with-faiss]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
]} />

{/if}

[á€¡á€á€”á€ºá€¸ á…](/course/chapter5/5) á€™á€¾á€¬ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· ğŸ¤— Datasets repository á€€á€”á€± GitHub issues á€”á€²á€· comments á€á€½á€±á€›á€²á€· dataset á€á€…á€ºá€á€¯á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€á€²á€·á€•á€«á€á€šá€ºá‹ á€’á€®á€¡á€•á€­á€¯á€„á€ºá€¸á€™á€¾á€¬á€á€±á€¬á€· á€’á€®á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€á€½á€±á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ library á€”á€²á€· á€•á€á€ºá€á€€á€ºá€á€²á€· á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· á€¡á€›á€±á€¸á€¡á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ á€™á€±á€¸á€á€½á€”á€ºá€¸á€á€½á€±á€›á€²á€· á€¡á€–á€¼á€±á€á€½á€±á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€ºá€™á€šá€·á€º search engine á€á€…á€ºá€á€¯á€€á€­á€¯ á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€½á€¬á€¸á€™á€¾á€¬á€•á€«á‹

<Youtube id="OATCgQtNX2o"/>

## Semantic Search á€¡á€á€½á€€á€º Embeddings á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸[[using-embeddings-for-semantic-search]]

[Chapter 1](/course/chapter1) á€™á€¾á€¬ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€™á€¼á€„á€ºá€á€²á€·á€›á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸áŠ Transformer-based language models á€á€½á€±á€€ á€…á€¬á€á€¬á€¸á€¡á€•á€­á€¯á€„á€ºá€¸á€¡á€…á€á€…á€ºá€á€¯á€‘á€²á€€ token á€á€…á€ºá€á€¯á€…á€®á€€á€­á€¯ _embedding vector_ á€¡á€–á€¼á€…á€º á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹ á€á€…á€ºá€á€«á€á€…á€ºá€›á€¶á€™á€¾á€¬ sentences á€á€½á€±áŠ paragraphs á€á€½á€± á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º (á€¡á€á€»á€­á€¯á€·á€€á€­á€…á€¹á€…á€á€½á€±á€™á€¾á€¬) documents á€á€½á€±á€¡á€á€½á€€á€º vector representation á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€–á€­á€¯á€· individual embeddings á€á€½á€±á€€á€­á€¯ "pool" á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ á€‘á€­á€¯á€·á€”á€±á€¬á€€á€º á€¤ embeddings á€™á€»á€¬á€¸á€€á€­á€¯ dot-product similarity (á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€á€¼á€¬á€¸ similarity metric á€á€…á€ºá€á€¯á€á€¯) á€€á€­á€¯ á€á€½á€€á€ºá€á€»á€€á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º corpus á€‘á€²á€›á€¾á€­ á€†á€„á€ºá€á€°á€á€±á€¬ documents á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€ºá€•á€¼á€®á€¸ á€¡á€á€°á€†á€¯á€¶á€¸á€á€±á€¬ documents á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¼á€”á€ºá€•á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

á€’á€®á€¡á€•á€­á€¯á€„á€ºá€¸á€™á€¾á€¬á€á€±á€¬á€· embeddings á€á€½á€±á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ semantic search engine á€á€…á€ºá€á€¯á€€á€­á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€½á€¬á€¸á€™á€¾á€¬á€•á€«á‹ á€’á€® search engines á€á€½á€±á€€ query á€‘á€²á€€ keywords á€á€½á€±á€€á€­á€¯ documents á€á€½á€±á€”á€²á€· á€€á€­á€¯á€€á€ºá€Šá€®á€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€²á€· á€›á€­á€¯á€¸á€›á€¬á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€½á€±á€‘á€€á€º á€¡á€¬á€¸á€á€¬á€á€»á€€á€ºá€™á€»á€¬á€¸á€…á€½á€¬á€€á€­á€¯ á€•á€±á€¸á€…á€½á€™á€ºá€¸á€•á€«á€á€šá€ºá‹

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Semantic search."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Semantic search."/>
</div>

## Dataset á€€á€­á€¯ Loading á€œá€¯á€•á€ºá€•á€¼á€®á€¸ á€•á€¼á€„á€ºá€†á€„á€ºá€á€¼á€„á€ºá€¸[[loading-and-preparing-the-dataset]]

á€•á€‘á€™á€†á€¯á€¶á€¸ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€œá€¯á€•á€ºá€›á€™á€šá€·á€ºá€¡á€›á€¬á€€ GitHub issues á€á€½á€±á€›á€²á€· dataset á€€á€­á€¯ download á€œá€¯á€•á€ºá€–á€­á€¯á€·á€•á€«á€•á€²áŠ á€’á€«á€€á€¼á€±á€¬á€„á€·á€º á€•á€¯á€¶á€™á€¾á€”á€ºá€¡á€á€­á€¯á€„á€ºá€¸ `load_dataset()` function á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€¡á€±á€¬á€„á€ºá‹

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

á€’á€®á€”á€±á€›á€¬á€™á€¾á€¬ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· `load_dataset()` á€™á€¾á€¬ default `train` split á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€¬á€€á€¼á€±á€¬á€„á€·á€º `DatasetDict` á€¡á€…á€¬á€¸ `Dataset` á€€á€­á€¯ á€•á€¼á€”á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹ á€•á€‘á€™á€¦á€¸á€†á€¯á€¶á€¸ á€œá€¯á€•á€ºá€›á€™á€šá€·á€ºá€¡á€›á€¬á€€ pull requests á€á€½á€±á€€á€­á€¯ á€…á€…á€ºá€‘á€¯á€á€ºá€•á€…á€ºá€–á€­á€¯á€·á€•á€«á€•á€²áŠ á€˜á€¬á€œá€­á€¯á€·á€œá€²á€†á€­á€¯á€á€±á€¬á€· á€’á€«á€á€½á€±á€€ user queries á€á€½á€±á€€á€­á€¯ á€–á€¼á€±á€–á€­á€¯á€·á€¡á€á€½á€€á€º á€›á€¾á€¬á€¸á€›á€¾á€¬á€¸á€•á€«á€¸á€•á€«á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¬á€€á€¼á€±á€¬á€„á€·á€º search engine á€™á€¾á€¬ noise á€á€½á€± á€–á€¼á€…á€ºá€•á€±á€«á€ºá€…á€±á€•á€«á€œá€­á€™á€·á€ºá€™á€šá€ºá‹ á€¡á€á€¯á€†á€­á€¯ á€›á€„á€ºá€¸á€”á€¾á€®á€¸á€”á€±á€•á€¼á€®á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸áŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· dataset á€‘á€²á€€ á€’á€® rows á€á€½á€±á€€á€­á€¯ á€–á€šá€ºá€‘á€¯á€á€ºá€–á€­á€¯á€· `Dataset.filter()` function á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ á€¡á€²á€’á€«á€€á€­á€¯ á€œá€¯á€•á€ºá€”á€±á€›á€„á€ºá€¸á€”á€²á€·áŠ user queries á€á€½á€±á€¡á€á€½á€€á€º á€¡á€–á€¼á€±á€™á€•á€±á€¸á€”á€­á€¯á€„á€ºá€á€²á€· comments á€™á€›á€¾á€­á€á€²á€· rows á€á€½á€±á€€á€­á€¯á€œá€Šá€ºá€¸ á€…á€…á€ºá€‘á€¯á€á€ºá€•á€…á€ºá€›á€¡á€±á€¬á€„á€ºá‹

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· dataset á€™á€¾á€¬ columns á€á€½á€± á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸á€•á€«á€á€¬á€€á€­á€¯ á€á€½á€±á€·á€›á€•á€«á€á€šá€ºá‹ á€’á€«á€á€½á€±á€‘á€²á€€ á€¡á€™á€»á€¬á€¸á€…á€¯á€€á€­á€¯ search engine á€á€Šá€ºá€†á€±á€¬á€€á€ºá€–á€­á€¯á€· á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€™á€œá€­á€¯á€¡á€•á€ºá€•á€«á€˜á€°á€¸á‹ search á€›á€¾á€¯á€‘á€±á€¬á€„á€·á€ºá€€á€€á€¼á€Šá€·á€ºá€™á€šá€ºá€†á€­á€¯á€›á€„á€ºáŠ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ columns á€á€½á€±á€€ `title`áŠ `body` á€”á€²á€· `comments` á€á€½á€±á€–á€¼á€…á€ºá€•á€¼á€®á€¸áŠ `html_url` á€€á€á€±á€¬á€· á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€€á€­á€¯ source issue á€€á€­á€¯ á€•á€¼á€”á€ºá€œá€Šá€ºá€Šá€½á€¾á€”á€ºá€•á€¼á€á€²á€· link á€€á€­á€¯ á€•á€±á€¸á€•á€«á€á€šá€ºá‹ á€€á€»á€”á€ºá€á€¬á€á€½á€±á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€–á€­á€¯á€· `Dataset.remove_columns()` function á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€¡á€±á€¬á€„á€ºá‹

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· embeddings á€á€½á€± á€–á€”á€ºá€á€®á€¸á€–á€­á€¯á€·á€¡á€á€½á€€á€º issue á€›á€²á€· title á€”á€²á€· body á€€á€­á€¯ comment á€á€…á€ºá€á€¯á€…á€®á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ á€•á€±á€«á€„á€ºá€¸á€‘á€Šá€·á€ºá€•á€«á€™á€šá€ºáŠ á€˜á€¬á€œá€­á€¯á€·á€œá€²á€†á€­á€¯á€á€±á€¬á€· á€’á€® fields á€á€½á€±á€€ á€™á€€á€¼á€¬á€á€á€†á€­á€¯á€á€œá€­á€¯ á€¡á€á€¯á€¶á€¸á€á€„á€ºá€á€²á€· context information á€á€½á€± á€•á€«á€á€„á€ºá€œá€­á€¯á€·á€•á€«á‹ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· `comments` column á€€ á€œá€€á€ºá€›á€¾á€­á€™á€¾á€¬ issue á€á€…á€ºá€á€¯á€…á€®á€¡á€á€½á€€á€º comments á€á€½á€±á€›á€²á€· list á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€”á€±á€á€¬á€€á€¼á€±á€¬á€„á€·á€ºáŠ row á€á€…á€ºá€á€¯á€…á€®á€™á€¾á€¬ `(html_url, title, body, comment)` tuple á€á€…á€ºá€á€¯á€•á€«á€á€„á€ºá€¡á€±á€¬á€„á€º column á€€á€­á€¯ "explode" á€œá€¯á€•á€ºá€–á€­á€¯á€· á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹ Pandas á€™á€¾á€¬ á€’á€«á€€á€­á€¯ [`DataFrame.explode()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) á€”á€²á€· á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€€ list-like column á€á€…á€ºá€á€¯á€…á€®á€™á€¾á€¬ element á€á€…á€ºá€á€¯á€…á€®á€¡á€á€½á€€á€º new row á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€•á€±á€¸á€•á€¼á€®á€¸ á€€á€»á€”á€ºá€á€²á€· column values á€á€½á€±á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ á€•á€½á€¬á€¸á€•á€±á€¸á€•á€«á€á€šá€ºá‹ á€’á€«á€€á€­á€¯ á€œá€€á€ºá€á€½á€±á€·á€™á€¼á€„á€ºá€›á€–á€­á€¯á€·áŠ á€•á€‘á€™á€†á€¯á€¶á€¸ Pandas `DataFrame` format á€á€­á€¯á€· á€•á€¼á€±á€¬á€„á€ºá€¸á€›á€¡á€±á€¬á€„á€ºá‹

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

á€’á€® `DataFrame` á€‘á€²á€€ á€•á€‘á€™á€†á€¯á€¶á€¸ row á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€€á€¼á€Šá€·á€ºá€™á€šá€ºá€†á€­á€¯á€›á€„á€º á€’á€® issue á€”á€²á€· á€†á€€á€ºá€…á€•á€ºá€”á€±á€á€²á€· comments á€œá€±á€¸á€á€¯á€›á€¾á€­á€á€¬á€€á€­á€¯ á€á€½á€±á€·á€›á€•á€«á€á€šá€º-

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in ï¼š\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· `df` á€€á€­á€¯ explode á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€«áŠ á€’á€® comments á€á€…á€ºá€á€¯á€…á€®á€¡á€á€½á€€á€º row á€á€…á€ºá€á€¯á€›á€›á€¾á€­á€–á€­á€¯á€· á€™á€»á€¾á€±á€¬á€ºá€œá€„á€·á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€Ÿá€¯á€á€ºá€™á€Ÿá€¯á€á€º á€…á€…á€ºá€€á€¼á€Šá€·á€ºá€›á€¡á€±á€¬á€„á€ºá‹

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in ï¼š\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

á€€á€±á€¬á€„á€ºá€¸á€•á€«á€•á€¼á€®áŠ rows á€á€½á€± á€•á€½á€¬á€¸á€”á€±á€á€¬á€€á€­á€¯ á€á€½á€±á€·á€›á€•á€¼á€®á€¸ `comments` column á€™á€¾á€¬ individual comments á€á€½á€± á€•á€«á€á€„á€ºá€á€¬á€€á€­á€¯ á€™á€¼á€„á€ºá€›á€•á€«á€á€šá€ºá‹ Pandas á€”á€²á€· á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€¬ á€•á€¼á€®á€¸á€…á€®á€¸á€á€½á€¬á€¸á€•á€¼á€®á€†á€­á€¯á€á€±á€¬á€· `DataFrame` á€€á€­á€¯ memory á€‘á€²á€™á€¾á€¬ loading á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º `Dataset` á€á€­á€¯á€· á€œá€»á€„á€ºá€™á€¼á€”á€ºá€…á€½á€¬ á€•á€¼á€”á€ºá€•á€¼á€±á€¬á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

á€€á€±á€¬á€„á€ºá€¸á€•á€«á€•á€¼á€®áŠ á€’á€«á€€ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€€á€­á€¯ á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€–á€­á€¯á€· comments á€¡á€”á€Šá€ºá€¸á€„á€šá€º á€‘á€±á€¬á€„á€ºá€á€»á€®á€•á€¼á€®á€¸ á€•á€±á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹

> [!TIP]
> âœï¸ **á€…á€™á€ºá€¸á€á€•á€ºá€€á€¼á€Šá€·á€ºá€•á€«á‹** Pandas á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€™á€•á€¼á€¯á€˜á€² `issues_dataset` á€›á€²á€· `comments` column á€€á€­á€¯ explode á€œá€¯á€•á€ºá€–á€­á€¯á€· `Dataset.map()` á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€™á€œá€¬á€¸ á€€á€¼á€Šá€·á€ºá€•á€«á‹ á€’á€«á€€ á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸á€œá€±á€¸ á€á€€á€ºá€•á€«á€á€šá€ºáŠ á€’á€® task á€¡á€á€½á€€á€º ğŸ¤— Datasets documentation á€›á€²á€· ["Batch mapping"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) á€¡á€•á€­á€¯á€„á€ºá€¸á€€ á€¡á€‘á€±á€¬á€€á€ºá€¡á€€á€° á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

á€¡á€á€¯ row á€á€…á€ºá€á€¯á€…á€®á€™á€¾á€¬ comment á€á€…á€ºá€á€¯á€…á€® á€›á€¾á€­á€•á€¼á€®á€†á€­á€¯á€á€±á€¬á€·áŠ comment á€á€…á€ºá€á€¯á€…á€®á€™á€¾á€¬á€›á€¾á€­á€á€²á€· á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€¡á€›á€±á€¡á€á€½á€€á€º á€•á€«á€á€„á€ºá€á€²á€· `comments_length` column á€¡á€á€…á€ºá€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€›á€¡á€±á€¬á€„á€ºá‹

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

á€’á€® column á€¡á€á€…á€ºá€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ "cc @lewtun" á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º "Thanks!" á€œá€­á€¯á€™á€»á€­á€¯á€¸ search engine á€”á€²á€· á€™á€á€€á€ºá€†á€­á€¯á€„á€ºá€á€²á€· á€á€­á€¯á€á€­á€¯á€á€±á€¬á€„á€ºá€¸á€á€±á€¬á€„á€ºá€¸ comments á€á€½á€±á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ filter á€¡á€á€½á€€á€º á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€²á€· á€”á€¶á€•á€«á€á€ºá€™á€›á€¾á€­á€•á€±á€™á€šá€·á€ºáŠ á€…á€€á€¬á€¸á€œá€¯á€¶á€¸ áá… á€œá€¯á€¶á€¸á€á€”á€ºá€¸á€€á€»á€„á€ºá€€ á€€á€±á€¬á€„á€ºá€¸á€™á€½á€”á€ºá€á€²á€· á€¡á€…á€á€…á€ºá€á€¯ á€–á€¼á€…á€ºá€•á€«á€œá€­á€™á€·á€ºá€™á€šá€ºá‹

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· dataset á€€á€­á€¯ á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸ á€á€”á€·á€ºá€›á€¾á€„á€ºá€¸á€›á€±á€¸ á€œá€¯á€•á€ºá€•á€¼á€®á€¸á€•á€¼á€®á€†á€­á€¯á€á€±á€¬á€·áŠ issue titleáŠ description á€”á€²á€· comments á€á€½á€±á€€á€­á€¯ `text` column á€¡á€á€…á€ºá€á€…á€ºá€á€¯á€‘á€²á€™á€¾á€¬ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€œá€­á€¯á€€á€ºá€›á€¡á€±á€¬á€„á€ºá‹ á€•á€¯á€¶á€™á€¾á€”á€ºá€¡á€á€­á€¯á€„á€ºá€¸á€•á€²áŠ `Dataset.map()` á€€á€­á€¯ á€•á€±á€¸á€•á€­á€¯á€·á€”á€­á€¯á€„á€ºá€™á€šá€·á€º á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€á€²á€· function á€á€…á€ºá€á€¯á€€á€­á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€›á€±á€¸á€•á€«á€™á€šá€ºá‹

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)
```

á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€á€±á€¬á€· embeddings á€á€½á€± á€–á€”á€ºá€á€®á€¸á€–á€­á€¯á€· á€¡á€†á€„á€ºá€á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€•á€¼á€®á‹ á€€á€¼á€Šá€·á€ºá€›á€¡á€±á€¬á€„á€ºá‹

## Text Embeddings á€™á€»á€¬á€¸ á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸[[creating-text-embeddings]]

[Chapter 2](/course/chapter2) á€™á€¾á€¬ `AutoModel` class á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ token embeddings á€á€½á€±á€›á€”á€­á€¯á€„á€ºá€á€šá€ºá€†á€­á€¯á€á€¬ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€á€½á€±á€·á€á€²á€·á€›á€•á€«á€á€šá€ºá‹ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€œá€¯á€•á€ºá€–á€­á€¯á€·á€œá€­á€¯á€á€¬á€€ model á€€á€­á€¯ load á€œá€¯á€•á€ºá€–á€­á€¯á€· á€á€„á€·á€ºá€œá€»á€±á€¬á€ºá€á€²á€· checkpoint á€á€…á€ºá€á€¯á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€–á€­á€¯á€·á€•á€«á€•á€²á‹ á€€á€¶á€€á€±á€¬á€„á€ºá€¸á€…á€½á€¬á€”á€²á€·á€•á€²áŠ embeddings á€á€½á€± á€–á€”á€ºá€á€®á€¸á€–á€­á€¯á€·á€¡á€á€½á€€á€º á€á€®á€¸á€á€”á€·á€º library á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€á€²á€· `sentence-transformers` á€›á€¾á€­á€•á€«á€á€šá€ºá‹ library á€›á€²á€· [documentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) á€™á€¾á€¬ á€–á€±á€¬á€ºá€•á€¼á€‘á€¬á€¸á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸áŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· use case á€€ _asymmetric semantic search_ á€›á€²á€· á€¥á€•á€™á€¬á€á€…á€ºá€á€¯á€•á€«á€•á€²áŠ á€˜á€¬á€œá€­á€¯á€·á€œá€²á€†á€­á€¯á€á€±á€¬á€· á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€™á€¾á€¬ á€á€­á€¯á€á€±á€¬á€„á€ºá€¸á€á€²á€· query á€á€…á€ºá€á€¯á€›á€¾á€­á€•á€¼á€®á€¸ á€¡á€²á€’á€® query á€›á€²á€· á€¡á€–á€¼á€±á€€á€­á€¯ issue comment á€œá€­á€¯á€™á€»á€­á€¯á€¸ á€•á€­á€¯á€›á€¾á€Šá€ºá€á€²á€· document á€á€…á€ºá€á€¯á€‘á€²á€™á€¾á€¬ á€›á€¾á€¬á€–á€½á€±á€œá€­á€¯á€á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€•á€«á‹ documentation á€‘á€²á€€ á€¡á€á€¯á€¶á€¸á€á€„á€ºá€á€²á€· [model overview table](https://www.sbert.net/docs/pretrained_models.html#model-overview) á€€ `multi-qa-mpnet-base-dot-v1` checkpoint á€Ÿá€¬ semantic search á€¡á€á€½á€€á€º á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€º á€›á€¾á€­á€á€šá€ºá€œá€­á€¯á€· á€Šá€½á€¾á€”á€ºá€•á€¼á€‘á€¬á€¸á€á€¬á€€á€¼á€±á€¬á€„á€·á€º á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· application á€¡á€á€½á€€á€º á€’á€«á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€™á€šá€ºá‹ tokenizer á€€á€­á€¯á€œá€Šá€ºá€¸ á€¡á€œá€¬á€¸á€á€° checkpoint á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ load á€œá€¯á€•á€ºá€•á€«á€™á€šá€ºá‹

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

embedding á€œá€¯á€•á€ºá€„á€”á€ºá€¸á€…á€‰á€ºá€€á€­á€¯ á€™á€¼á€”á€ºá€†á€”á€ºá€…á€±á€–á€­á€¯á€·áŠ model á€”á€²á€· inputs á€á€½á€±á€€á€­á€¯ GPU device á€•á€±á€«á€ºá€™á€¾á€¬ á€‘á€¬á€¸á€á€¬á€€ á€¡á€‘á€±á€¬á€€á€ºá€¡á€€á€°á€–á€¼á€…á€ºá€…á€±á€•á€«á€á€šá€ºáŠ á€’á€«á€€á€¼á€±á€¬á€„á€·á€º á€¡á€á€¯á€•á€² á€œá€¯á€•á€ºá€›á€¡á€±á€¬á€„á€ºá‹

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

`from_pretrained()` method á€›á€²á€· argument á€¡á€–á€¼á€…á€º `from_pt=True` á€€á€­á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€¬á€€á€­á€¯ á€á€á€­á€•á€¼á€¯á€•á€«á‹ á€’á€«á€€ `multi-qa-mpnet-base-dot-v1` checkpoint á€™á€¾á€¬ PyTorch weights á€á€½á€±á€•á€² á€›á€¾á€­á€á€¬á€€á€¼á€±á€¬á€„á€·á€º `from_pt=True` á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€á€¼á€„á€ºá€¸á€€ áá€„á€ºá€¸á€á€­á€¯á€·á€€á€­á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€¡á€á€½á€€á€º TensorFlow format á€á€­á€¯á€· á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€±á€¸á€•á€«á€œá€­á€™á€·á€ºá€™á€šá€ºá‹ á€á€„á€ºá€á€½á€±á€·á€›á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸áŠ ğŸ¤— Transformers á€™á€¾á€¬ frameworks á€á€½á€±á€€á€¼á€¬á€¸ á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¬á€€ á€¡á€œá€½á€”á€ºá€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

{/if}

á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€¡á€…á€±á€¬á€•á€­á€¯á€„á€ºá€¸á€€ á€–á€±á€¬á€ºá€•á€¼á€á€²á€·á€á€²á€·á€¡á€á€­á€¯á€„á€ºá€¸áŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· GitHub issues corpus á€‘á€²á€€ entry á€á€…á€ºá€á€¯á€…á€®á€€á€­á€¯ single vector á€¡á€–á€¼á€…á€º á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€á€»á€„á€ºá€á€¬á€€á€¼á€±á€¬á€„á€·á€ºáŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· token embeddings á€á€½á€±á€€á€­á€¯ á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€…á€ºá€á€¯á€á€¯á€”á€²á€· "pool" á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º average á€œá€¯á€•á€ºá€–á€­á€¯á€· á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹ á€œá€°á€€á€¼á€­á€¯á€€á€ºá€™á€»á€¬á€¸á€á€²á€· á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€…á€ºá€á€¯á€€á€á€±á€¬á€· model á€›á€²á€· outputs á€á€½á€±á€•á€±á€«á€ºá€™á€¾á€¬ *CLS pooling* á€€á€­á€¯ á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€¬á€•á€«á€•á€²áŠ á€’á€®á€”á€±á€›á€¬á€™á€¾á€¬ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€Ÿá€¬ special `[CLS]` token á€¡á€á€½á€€á€º last hidden state á€€á€­á€¯ á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€…á€½á€¬ á€…á€¯á€†á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹ á€¡á€±á€¬á€€á€ºá€•á€« function á€€ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€¡á€á€½á€€á€º á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

á€”á€±á€¬á€€á€ºá€‘á€•á€ºá€¡á€”á€±á€”á€²á€·áŠ documents á€á€½á€±á€›á€²á€· list á€á€…á€ºá€á€¯á€€á€­á€¯ tokenize á€œá€¯á€•á€ºá€•á€±á€¸á€™á€šá€·á€ºáŠ tensors á€á€½á€±á€€á€­á€¯ GPU á€•á€±á€«á€ºá€™á€¾á€¬ á€‘á€¬á€¸á€•á€±á€¸á€™á€šá€·á€ºáŠ model á€€á€­á€¯ feed á€œá€¯á€•á€ºá€•á€±á€¸á€™á€šá€·á€ºáŠ á€•á€¼á€®á€¸á€á€±á€¬á€· á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€™á€¾á€¬ outputs á€á€½á€±á€€á€­á€¯ CLS pooling á€œá€¯á€•á€ºá€•á€±á€¸á€™á€šá€·á€º helper function á€á€…á€ºá€á€¯á€€á€­á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€–á€”á€ºá€á€®á€¸á€•á€«á€™á€šá€ºá‹

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

function á€€ á€¡á€œá€¯á€•á€ºá€–á€¼á€…á€ºá€™á€–á€¼á€…á€º á€…á€…á€ºá€†á€±á€¸á€–á€­á€¯á€·á€¡á€á€½á€€á€º á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· corpus á€‘á€²á€€ á€•á€‘á€™á€†á€¯á€¶á€¸ text entry á€€á€­á€¯ feed á€œá€¯á€•á€ºá€•á€¼á€®á€¸ output shape á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

á€€á€±á€¬á€„á€ºá€¸á€•á€«á€•á€¼á€®áŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· corpus á€‘á€²á€€ á€•á€‘á€™á€†á€¯á€¶á€¸ entry á€€á€­á€¯ 768-dimensional vector á€á€…á€ºá€á€¯á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€²á€·á€•á€«á€•á€¼á€®á‹ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· `get_embeddings()` function á€€á€­á€¯ corpus á€‘á€²á€€ row á€á€…á€ºá€á€¯á€…á€®á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€–á€­á€¯á€· `Dataset.map()` á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€á€¬á€€á€¼á€±á€¬á€„á€·á€ºáŠ á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ `embeddings` column á€¡á€á€…á€ºá€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€›á€¡á€±á€¬á€„á€ºá‹

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

function á€€ á€¡á€œá€¯á€•á€ºá€–á€¼á€…á€ºá€™á€–á€¼á€…á€º á€…á€…á€ºá€†á€±á€¸á€–á€­á€¯á€·á€¡á€á€½á€€á€º á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· corpus á€‘á€²á€€ á€•á€‘á€™á€†á€¯á€¶á€¸ text entry á€€á€­á€¯ feed á€œá€¯á€•á€ºá€•á€¼á€®á€¸ output shape á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

á€€á€±á€¬á€„á€ºá€¸á€•á€«á€•á€¼á€®áŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· corpus á€‘á€²á€€ á€•á€‘á€™á€†á€¯á€¶á€¸ entry á€€á€­á€¯ 768-dimensional vector á€á€…á€ºá€á€¯á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€²á€·á€•á€«á€•á€¼á€®á‹ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· `get_embeddings()` function á€€á€­á€¯ corpus á€‘á€²á€€ row á€á€…á€ºá€á€¯á€…á€®á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€–á€­á€¯á€· `Dataset.map()` á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€á€¬á€€á€¼á€±á€¬á€„á€·á€ºáŠ á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ `embeddings` column á€¡á€á€…á€ºá€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€›á€¡á€±á€¬á€„á€ºá‹

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

embeddings á€á€½á€±á€€á€­á€¯ NumPy arrays á€á€½á€±á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€‘á€¬á€¸á€á€¬á€€á€­á€¯ á€á€á€­á€•á€¼á€¯á€•á€«á‹ á€’á€«á€€ ğŸ¤— Datasets á€€ áá€„á€ºá€¸á€á€­á€¯á€·á€€á€­á€¯ FAISS á€”á€²á€· index á€œá€¯á€•á€ºá€–á€­á€¯á€· á€€á€¼á€­á€¯á€¸á€…á€¬á€¸á€á€²á€·á€¡á€á€« á€’á€® format á€€á€­á€¯ á€œá€­á€¯á€¡á€•á€ºá€œá€­á€¯á€·á€•á€«á‹ á€’á€«á€€á€­á€¯ á€”á€±á€¬á€€á€ºá€á€…á€ºá€†á€„á€·á€ºá€™á€¾á€¬ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€•á€«á€™á€šá€ºá‹

## FAISS á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á á€‘á€­á€›á€±á€¬á€€á€ºá€á€±á€¬ Similarity Search[[using-faiss-for-efficient-similarity-search]]

á€¡á€á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€™á€¾á€¬ embeddings á€á€½á€±á€›á€²á€· dataset á€á€…á€ºá€á€¯á€›á€¾á€­á€•á€¼á€®á€†á€­á€¯á€á€±á€¬á€· áá€„á€ºá€¸á€á€­á€¯á€·á€•á€±á€«á€ºá€™á€¾á€¬ search á€œá€¯á€•á€ºá€–á€­á€¯á€· á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€á€…á€ºá€á€¯ á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€€á€­á€¯á€œá€¯á€•á€ºá€–á€­á€¯á€·áŠ ğŸ¤— Datasets á€™á€¾á€¬ _FAISS index_ á€œá€­á€¯á€·á€á€±á€«á€ºá€á€²á€· á€¡á€‘á€°á€¸ data structure á€á€…á€ºá€á€¯á€€á€­á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€™á€šá€ºá‹ [FAISS](https://faiss.ai/) (Facebook AI Similarity Search á€›á€²á€· á€¡á€á€­á€¯á€€á€±á€¬á€€á€º) á€€ embedding vectors á€á€½á€±á€€á€­á€¯ á€œá€»á€„á€ºá€™á€¼á€”á€ºá€…á€½á€¬ á€›á€¾á€¬á€–á€½á€±á€•á€¼á€®á€¸ cluster á€œá€¯á€•á€ºá€–á€­á€¯á€· á€‘á€­á€›á€±á€¬á€€á€ºá€á€²á€· algorithms á€á€½á€±á€€á€­á€¯ á€•á€¶á€·á€•á€­á€¯á€¸á€•á€±á€¸á€á€²á€· library á€á€…á€ºá€á€¯á€•á€«á‹

FAISS á€›á€²á€· á€¡á€á€¼á€±á€á€¶á€á€˜á€±á€¬á€á€›á€¬á€¸á€€ input embedding á€á€…á€ºá€á€¯á€”á€²á€· á€†á€„á€ºá€á€°á€á€²á€· embeddings á€á€½á€±á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€ºá€…á€±á€™á€šá€·á€º _index_ á€œá€­á€¯á€·á€á€±á€«á€ºá€á€²á€· á€¡á€‘á€°á€¸ data structure á€á€…á€ºá€á€¯á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€–á€­á€¯á€·á€•á€«á€•á€²á‹ ğŸ¤— Datasets á€™á€¾á€¬ FAISS index á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€á€¬á€€ á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€º â€” á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· `Dataset.add_faiss_index()` function á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· dataset á€‘á€²á€€ á€˜á€šá€º column á€€á€­á€¯ index á€œá€¯á€•á€ºá€á€»á€„á€ºá€á€šá€ºá€†á€­á€¯á€á€¬ á€á€á€ºá€™á€¾á€á€ºá€•á€±á€¸á€›á€¯á€¶á€•á€«á€•á€²á‹

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

á€¡á€á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· `Dataset.get_nearest_examples()` function á€”á€²á€· nearest neighbor lookup á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º á€’á€® index á€•á€±á€«á€ºá€™á€¾á€¬ queries á€á€½á€± á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€”á€­á€¯á€„á€ºá€•á€«á€•á€¼á€®á‹ á€’á€«á€€á€­á€¯ á€•á€‘á€™á€†á€¯á€¶á€¸ á€™á€±á€¸á€á€½á€”á€ºá€¸á€á€…á€ºá€á€¯á€€á€­á€¯ á€¡á€±á€¬á€€á€ºá€•á€«á€¡á€á€­á€¯á€„á€ºá€¸ embedding á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º á€…á€™á€ºá€¸á€€á€¼á€Šá€·á€ºá€›á€¡á€±á€¬á€„á€ºá‹

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

documents á€á€½á€±á€”á€²á€·á€¡á€á€°á€á€°á€•á€²áŠ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€™á€¾á€¬ á€¡á€á€¯ query á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€á€²á€· 768-dimensional vector á€á€…á€ºá€á€¯ á€›á€¾á€­á€•á€«á€á€šá€ºá‹ á€’á€«á€€á€­á€¯ á€¡á€á€°á€†á€¯á€¶á€¸ embeddings á€á€½á€±á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€–á€­á€¯á€· corpus á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€”á€²á€· á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

`Dataset.get_nearest_examples()` function á€€ query á€”á€²á€· document á€€á€¼á€¬á€¸ á€á€°á€Šá€®á€™á€¾á€¯á€¡á€†á€„á€·á€ºá€€á€­á€¯ á€¡á€†á€„á€·á€ºá€á€á€ºá€™á€¾á€á€ºá€•á€±á€¸á€á€²á€· scores á€á€½á€±á€›á€²á€· tuple á€á€…á€ºá€á€¯á€”á€²á€· á€á€€á€ºá€†á€­á€¯á€„á€ºá€›á€¬ samples á€¡á€…á€¯á€¡á€á€±á€¸á€á€…á€ºá€á€¯ (á€’á€®á€”á€±á€›á€¬á€™á€¾á€¬á€á€±á€¬á€· á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ á€€á€­á€¯á€€á€ºá€Šá€®á€™á€¾á€¯ á… á€á€¯) á€€á€­á€¯ á€•á€¼á€”á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹ á€’á€«á€á€½á€±á€€á€­á€¯ `pandas.DataFrame` á€‘á€²á€™á€¾á€¬ á€…á€¯á€†á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ á€¡á€œá€½á€šá€ºá€á€€á€° á€…á€®á€…á€‰á€ºá€”á€­á€¯á€„á€ºá€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€›á€¡á€±á€¬á€„á€ºá‹

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

á€¡á€á€¯ á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· query á€€ á€›á€›á€¾á€­á€”á€­á€¯á€„á€ºá€á€²á€· comments á€á€½á€±á€”á€²á€· á€˜á€šá€ºá€œá€±á€¬á€€á€º á€€á€±á€¬á€„á€ºá€¸á€€á€±á€¬á€„á€ºá€¸ á€€á€­á€¯á€€á€ºá€Šá€®á€œá€²á€†á€­á€¯á€á€¬á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€–á€­á€¯á€· á€•á€‘á€™á€†á€¯á€¶á€¸ rows á€¡á€”á€Šá€ºá€¸á€„á€šá€ºá€€á€­á€¯ iterate á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€•á€¼á€®á‹

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

á€™á€†á€­á€¯á€¸á€•á€«á€˜á€°á€¸! á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·á€›á€²á€· á€’á€¯á€á€­á€š hit á€€ query á€”á€²á€· á€€á€­á€¯á€€á€ºá€Šá€®á€•á€¯á€¶á€›á€•á€«á€á€šá€ºá‹

> [!TIP]
> âœï¸ **á€…á€™á€ºá€¸á€á€•á€ºá€€á€¼á€Šá€·á€ºá€•á€«á‹** á€á€„á€·á€ºá€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º query á€á€…á€ºá€á€¯á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€•á€¼á€®á€¸ á€•á€¼á€”á€ºá€œá€Šá€ºá€›á€›á€¾á€­á€‘á€¬á€¸á€á€²á€· documents á€á€½á€±á€‘á€²á€™á€¾á€¬ á€¡á€–á€¼á€±á€›á€¾á€¬á€”á€­á€¯á€„á€ºá€™á€œá€¬á€¸ á€€á€¼á€Šá€·á€ºá€•á€«á‹ search á€€á€­á€¯ á€•á€­á€¯á€™á€­á€¯á€€á€»á€šá€ºá€•á€¼á€”á€·á€ºá€…á€±á€–á€­á€¯á€· `Dataset.get_nearest_examples()` á€™á€¾á€¬á€›á€¾á€­á€á€²á€· `k` parameter á€€á€­á€¯ á€á€­á€¯á€¸á€™á€¼á€¾á€„á€·á€ºá€›á€•á€«á€œá€­á€™á€·á€ºá€™á€šá€ºá‹

## á€á€±á€«á€Ÿá€¬á€› á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º (Glossary)

*   **FAISS (Facebook AI Similarity Search)**: embedding vectors á€™á€»á€¬á€¸á€€á€­á€¯ á€œá€»á€„á€ºá€™á€¼á€”á€ºá€…á€½á€¬ á€›á€¾á€¬á€–á€½á€±á€•á€¼á€®á€¸ cluster á€œá€¯á€•á€ºá€›á€”á€ºá€¡á€á€½á€€á€º á€‘á€­á€›á€±á€¬á€€á€ºá€á€±á€¬ algorithms á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¶á€·á€•á€­á€¯á€¸á€•á€±á€¸á€á€±á€¬ library á€á€…á€ºá€á€¯á‹
*   **Semantic Search**: á€¡á€“á€­á€•á€¹á€•á€¬á€šá€ºá€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€•á€¼á€®á€¸ query á€á€…á€ºá€á€¯á á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€ºá€”á€¾á€„á€·á€º á€€á€­á€¯á€€á€ºá€Šá€®á€á€±á€¬ documents á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€•á€±á€¸á€á€Šá€·á€º search engine á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á‹
*   **Embeddings**: á€…á€¬á€á€¬á€¸ (á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€á€¼á€¬á€¸á€’á€±á€á€¬) á€€á€­á€¯ multi-dimensional vector space á€‘á€²á€›á€¾á€­ á€‚á€á€”á€ºá€¸á€™á€»á€¬á€¸á€¡á€–á€¼á€…á€º á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸á‹
*   **GitHub Issues**: GitHub repository á€™á€»á€¬á€¸á€á€½á€„á€º á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸áŠ bug á€™á€»á€¬á€¸ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€„á€ºá€¹á€‚á€«á€›á€•á€º á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€™á€¾á€¯á€™á€»á€¬á€¸á€€á€­á€¯ á€™á€¾á€á€ºá€á€™á€ºá€¸á€á€„á€ºá€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ á€¡á€„á€ºá€¹á€‚á€«á€›á€•á€ºá‹
*   **Comments**: GitHub issue á€á€…á€ºá€á€¯ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º pull request á€á€…á€ºá€á€¯á€¡á€±á€¬á€€á€ºá€á€½á€„á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á€™á€»á€¬á€¸á€€ á€•á€±á€«á€„á€ºá€¸á€‘á€Šá€·á€ºá€á€±á€¬ á€…á€¬á€á€¬á€¸á€™á€¾á€á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á‹
*   **Search Engine**: á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á query á€”á€¾á€„á€·á€º á€€á€­á€¯á€€á€ºá€Šá€®á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€•á€±á€¸á€á€Šá€·á€º á€…á€”á€…á€ºá‹
*   **Transformer-based Language Models**: Transformer architecture á€•á€±á€«á€ºá€á€½á€„á€º á€¡á€á€¼á€±á€á€¶á€‘á€¬á€¸á€á€±á€¬ language models á€™á€»á€¬á€¸á‹
*   **Embedding Vector**: á€…á€¬á€á€¬á€¸á€¡á€•á€­á€¯á€„á€ºá€¸á€¡á€…á€á€…á€ºá€á€¯ (token, sentence, paragraph) á€€á€­á€¯ á€‚á€á€”á€ºá€¸á€á€”á€ºá€–á€­á€¯á€¸á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€‘á€¬á€¸á€á€±á€¬ vectorá‹
*   **Pooling**: individual embeddings á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€•á€¼á€®á€¸ á€•á€­á€¯á€€á€¼á€®á€¸á€á€±á€¬ text unit (á€¥á€•á€™á€¬- sentence, document) á€¡á€á€½á€€á€º single vector representation á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸á‹
*   **Corpus**: á€á€¯á€á€±á€á€”á€•á€¼á€¯á€›á€”á€ºá€¡á€á€½á€€á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ á€…á€¬á€á€¬á€¸á€¡á€…á€¯á€¡á€á€±á€¸á€€á€¼á€®á€¸á‹
*   **Dot-product Similarity**: vectors á€”á€¾á€…á€ºá€á€¯á€€á€¼á€¬á€¸á€›á€¾á€­ similarity á€€á€­á€¯ á€á€½á€€á€ºá€á€»á€€á€ºá€á€±á€¬ metric á€á€…á€ºá€á€¯á‹
*   **Similarity Metric**: á€¡á€›á€¬á€á€á€¹á€‘á€¯á€”á€¾á€…á€ºá€á€¯ (á€¥á€•á€™á€¬- embeddings) á€™á€Šá€ºá€™á€»á€¾á€á€°á€Šá€®á€á€Šá€ºá€€á€­á€¯ á€á€­á€¯á€„á€ºá€¸á€á€¬á€á€±á€¬ á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á‹
*   **Query**: search engine á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º database á€™á€¾ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸ á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€á€¼á€„á€ºá€¸á‹
*   **Conventional Approaches**: á€›á€­á€¯á€¸á€›á€¬ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€á€™á€¬á€¸á€›á€­á€¯á€¸á€€á€» á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€™á€»á€¬á€¸á‹
*   **Keywords**: Search query á€á€½á€„á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ á€¡á€“á€­á€€á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€™á€»á€¬á€¸á‹
*   **`load_dataset()` Function**: Hugging Face Datasets library á€™á€¾ dataset á€™á€»á€¬á€¸á€€á€­á€¯ download á€œá€¯á€•á€ºá€•á€¼á€®á€¸ cache á€œá€¯á€•á€ºá€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ functioná‹
*   **`split="train"`**: `load_dataset()` function á€á€½á€„á€º training split á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€”á€ºá€¡á€á€½á€€á€º argumentá‹
*   **`Dataset` Object**: Hugging Face Datasets library á€™á€¾ dataset á€á€…á€ºá€á€¯á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€á€±á€¬ objectá‹
*   **`DatasetDict` Object**: Training set, validation set, á€”á€¾á€„á€·á€º test set á€€á€²á€·á€á€­á€¯á€·á€á€±á€¬ dataset á€¡á€™á€»á€¬á€¸á€¡á€•á€¼á€¬á€¸á€€á€­á€¯ dictionary á€•á€¯á€¶á€…á€¶á€–á€¼á€„á€·á€º á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ objectá‹
*   **Pull Requests**: GitHub á€á€½á€„á€º code á€¡á€•á€¼á€±á€¬á€„á€ºá€¸á€¡á€œá€²á€™á€»á€¬á€¸á€€á€­á€¯ project á main branch á€á€­á€¯á€· á€•á€±á€«á€„á€ºá€¸á€…á€Šá€ºá€¸á€›á€”á€º á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€á€¼á€„á€ºá€¸á‹
*   **`Dataset.filter()` Function**: Dataset á€™á€¾ á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€á€¼á€±á€¡á€”á€±á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€™á€€á€­á€¯á€€á€ºá€Šá€®á€á€±á€¬ rows á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ functioná‹
*   **`is_pull_request`**: GitHub issue á€á€…á€ºá€á€¯á€á€Šá€º pull request á€Ÿá€¯á€á€ºá€™á€Ÿá€¯á€á€ºá€€á€­á€¯ á€–á€±á€¬á€ºá€•á€¼á€á€±á€¬ feature (boolean value)á‹
*   **`len(x["comments"]) > 0`**: comment list á á€¡á€›á€¾á€Šá€ºá€á€Šá€º á€á€¯á€Šá€‘á€€á€º á€€á€¼á€®á€¸á€™á€¬á€¸á€á€¼á€„á€ºá€¸á€›á€¾á€­á€™á€›á€¾á€­ á€…á€…á€ºá€†á€±á€¸á€á€¼á€„á€ºá€¸á‹
*   **`title` Column**: Issue á á€á€±á€«á€„á€ºá€¸á€…á€‰á€ºá€€á€­á€¯ á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ columná‹
*   **`body` Column**: Issue á á€–á€±á€¬á€ºá€•á€¼á€á€»á€€á€ºá€€á€­á€¯ á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ columná‹
*   **`comments` Column**: Issue á€”á€¾á€„á€·á€º á€á€€á€ºá€†á€­á€¯á€„á€ºá€á€±á€¬ comments á€™á€»á€¬á€¸á€€á€­á€¯ á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ column (list of strings)á‹
*   **`html_url` Column**: Issue á GitHub URL á€€á€­á€¯ á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ columná‹
*   **`Dataset.remove_columns()` Function**: Dataset á€™á€¾ á€™á€œá€­á€¯á€¡á€•á€ºá€á€±á€¬ columns á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ functioná‹
*   **`set()`**: Python á€á€½á€„á€º item á€™á€»á€¬á€¸á€€á€­á€¯ á€…á€¯á€…á€Šá€ºá€¸á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ unordered collection á€–á€¼á€…á€ºá€•á€¼á€®á€¸ duplicate á€™á€»á€¬á€¸ á€™á€•á€«á€á€„á€ºá€•á€«á‹
*   **`symmetric_difference()`**: set á€”á€¾á€…á€ºá€á€¯á€€á€¼á€¬á€¸á€›á€¾á€­ á€™á€á€°á€Šá€®á€á€±á€¬ items á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€á€±á€¬ methodá‹
*   **Contextual Information**: á€¡á€á€¼á€±á€¡á€”á€±á€á€…á€ºá€á€¯ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€…á€¬á€á€¬á€¸á€á€…á€ºá€á€¯á á€¡á€“á€­á€•á€¹á€•á€¬á€šá€ºá€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€›á€”á€º á€€á€°á€Šá€®á€•á€±á€¸á€á€±á€¬ á€”á€±á€¬á€€á€ºá€á€¶á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á‹
*   **"Explode" a Column**: Pandas DataFrame á€á€½á€„á€º list-like column á€á€…á€ºá€á€¯á€›á€¾á€­ element á€á€…á€ºá€á€¯á€…á€®á€¡á€á€½á€€á€º new row á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸á‹
*   **Pandas `DataFrame`**: Python á€á€½á€„á€º tabular data (á€‡á€šá€¬á€¸á€•á€¯á€¶á€…á€¶á€’á€±á€á€¬) á€€á€­á€¯ á€€á€­á€¯á€„á€ºá€á€½á€šá€ºá€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ two-dimensional data structureá‹
*   **`DataFrame.explode()` Function**: Pandas á€™á€¾ list-like column á€á€…á€ºá€á€¯á€›á€¾á€­ element á€á€…á€ºá€á€¯á€…á€®á€¡á€á€½á€€á€º new row á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€•á€±á€¸á€á€±á€¬ functioná‹
*   **`issues_dataset.set_format("pandas")`**: Dataset á€€á€­á€¯ Pandas DataFrame format á€á€­á€¯á€· á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸á‹
*   **`issues_dataset[:]`**: Dataset á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ (Pandas format á€á€½á€„á€º) selection á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á‹
*   **`comments_df.head(4)`**: DataFrame á á€•á€‘á€™á€†á€¯á€¶á€¸ rows á€œá€±á€¸á€á€¯á€€á€­á€¯ á€•á€¼á€á€á€¼á€„á€ºá€¸á‹
*   **`ignore_index=True`**: `explode()` function á€á€½á€„á€º original index á€€á€­á€¯ á€™á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€˜á€² new index á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€›á€”á€º argumentá‹
*   **`Dataset.from_pandas()`**: Pandas DataFrame á€á€…á€ºá€á€¯á€™á€¾ Hugging Face Dataset object á€á€…á€ºá€á€¯á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€á€±á€¬ methodá‹
*   **`comments_length` Column**: Comment á€á€…á€ºá€á€¯á€…á€®á€›á€¾á€­ á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€¡á€›á€±á€¡á€á€½á€€á€ºá€€á€­á€¯ á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€±á€¬ columná‹
*   **`x["comments"].split()`**: Comment á€…á€¬á€á€¬á€¸á€€á€­á€¯ á€…á€€á€¬á€¸á€œá€¯á€¶á€¸á€™á€»á€¬á€¸á€¡á€–á€¼á€…á€º á€•á€­á€¯á€„á€ºá€¸á€á€¼á€¬á€¸á€á€¼á€„á€ºá€¸á‹
*   **`Dataset.map()`**: ğŸ¤— Datasets library á€™á€¾á€¬ á€•á€«á€á€„á€ºá€á€²á€· method á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€•á€¼á€®á€¸ dataset á€›á€²á€· element á€á€…á€ºá€á€¯á€…á€® á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º batch á€á€…á€ºá€á€¯á€…á€®á€•á€±á€«á€ºá€™á€¾á€¬ function á€á€…á€ºá€á€¯á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€…á€±á€á€Šá€ºá‹
*   **`AutoModel` Class**: Hugging Face Transformers library á€™á€¾ á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€™á€Šá€ºá€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ á€á€€á€ºá€†á€­á€¯á€„á€ºá€›á€¬ model class á€€á€­á€¯ á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º load á€œá€¯á€•á€ºá€•á€±á€¸á€á€±á€¬ classá‹
*   **Checkpoint**: á€™á€±á€¬á€ºá€’á€šá€ºá weights á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€¡á€á€¼á€¬á€¸á€–á€½á€²á€·á€…á€Šá€ºá€¸á€•á€¯á€¶á€™á€»á€¬á€¸ (configuration) á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€á€»á€­á€”á€ºá€á€…á€ºá€á€¯á€á€½á€„á€º á€á€­á€™á€ºá€¸á€†á€Šá€ºá€¸á€‘á€¬á€¸á€á€¼á€„á€ºá€¸á‹
*   **`sentence-transformers` Library**: Sentence embeddings á€™á€»á€¬á€¸ á€–á€”á€ºá€á€®á€¸á€›á€”á€ºá€¡á€á€½á€€á€º á€’á€®á€‡á€­á€¯á€„á€ºá€¸á€‘á€¯á€á€ºá€‘á€¬á€¸á€á€±á€¬ Python libraryá‹
*   **Asymmetric Semantic Search**: query á€á€Šá€º á€á€­á€¯á€á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ document á€á€Šá€º á€›á€¾á€Šá€ºá€œá€»á€¬á€¸á€á€±á€¬ semantic search á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ (á€¥á€•á€™á€¬- á€™á€±á€¸á€á€½á€”á€ºá€¸á€á€…á€ºá€á€¯á€€á€­á€¯ á€¡á€–á€¼á€±á€›á€¾á€¬á€á€¼á€„á€ºá€¸)á‹
*   **`multi-qa-mpnet-base-dot-v1`**: Semantic search á€¡á€á€½á€€á€º á€…á€½á€™á€ºá€¸á€†á€±á€¬á€„á€ºá€›á€Šá€ºá€€á€±á€¬á€„á€ºá€¸á€™á€½á€”á€ºá€á€±á€¬ sentence-transformer model checkpointá‹
*   **`AutoTokenizer`**: Hugging Face Transformers library á€™á€¾á€¬ á€•á€«á€á€„á€ºá€á€²á€· class á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€•á€¼á€®á€¸ á€™á€±á€¬á€ºá€’á€šá€ºá€¡á€™á€Šá€ºá€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ á€á€€á€ºá€†á€­á€¯á€„á€ºá€›á€¬ tokenizer á€€á€­á€¯ á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º load á€œá€¯á€•á€ºá€•á€±á€¸á€á€Šá€ºá‹
*   **GPU (Graphics Processing Unit)**: á€‚á€›á€•á€ºá€–á€…á€ºá€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€™á€¾á€¯á€¡á€á€½á€€á€º á€¡á€‘á€°á€¸á€’á€®á€‡á€­á€¯á€„á€ºá€¸á€‘á€¯á€á€ºá€‘á€¬á€¸á€á€±á€¬ processor á€á€…á€ºá€™á€»á€­á€¯á€¸á€–á€¼á€…á€ºá€á€±á€¬á€ºá€œá€Šá€ºá€¸ AI/ML á€œá€¯á€•á€ºá€„á€”á€ºá€¸á€™á€»á€¬á€¸á€á€½á€„á€º á€¡á€›á€¾á€­á€”á€ºá€™á€¼á€¾á€„á€·á€ºá€›á€”á€º á€¡á€á€¯á€¶á€¸á€™á€»á€¬á€¸á€á€Šá€ºá‹
*   **`torch.device("cuda")`**: PyTorch á€á€½á€„á€º GPU device á€€á€­á€¯ á€›á€Šá€ºá€Šá€½á€¾á€”á€ºá€¸á€á€Šá€ºá‹
*   **`model.to(device)`**: PyTorch model á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€±á€¬ device (GPU) á€á€­á€¯á€· á€›á€½á€¾á€±á€·á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸á‹
*   **`TFAutoModel`**: TensorFlow framework á€¡á€á€½á€€á€º `AutoModel` á€”á€¾á€„á€·á€º á€á€°á€Šá€®á€á€±á€¬ á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€›á€¾á€­á€á€Šá€ºá‹
*   **`from_pt=True`**: `TFAutoModel.from_pretrained()` á€á€½á€„á€º PyTorch weights á€™á€»á€¬á€¸á€€á€­á€¯ TensorFlow format á€á€­á€¯á€· á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€›á€”á€º argumentá‹
*   **CLS Pooling**: Transformer model á output á€™á€¾ `[CLS]` token á last hidden state á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á text sequence á€¡á€á€½á€€á€º single vector representation á€á€…á€ºá€á€¯ á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸á‹
*   **`[CLS]` Token**: BERT model á€á€½á€„á€º sequence á á€¡á€…á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€•á€¼á€¯á€á€±á€¬ special tokená‹
*   **Last Hidden State**: Transformer model á á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸ layer á€™á€¾ output embeddings á€™á€»á€¬á€¸á‹
*   **`encoded_input`**: Tokenizer á€™á€¾ á€‘á€¯á€á€ºá€•á€±á€¸á€á€±á€¬ input IDs, attention masks á€…á€á€Šá€ºá€á€­á€¯á€· á€•á€«á€á€„á€ºá€á€±á€¬ dictionaryá‹
*   **`padding=True`**: Tokenization á€œá€¯á€•á€ºá€›á€¬á€á€½á€„á€º sequence á€¡á€›á€¾á€Šá€ºá€™á€»á€¬á€¸ á€€á€½á€²á€•á€¼á€¬á€¸á€•á€«á€€ á€¡á€›á€¾á€Šá€ºá€†á€¯á€¶á€¸ sequence á€¡á€›á€¾á€Šá€ºá€¡á€á€­á€¯á€„á€ºá€¸ á€–á€¼á€Šá€·á€ºá€•á€±á€¸á€á€¼á€„á€ºá€¸á‹
*   **`truncation=True`**: sequence á€¡á€›á€¾á€Šá€ºá€á€Šá€º model á á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ input á€¡á€›á€¾á€Šá€ºá€‘á€€á€º á€›á€¾á€Šá€ºá€œá€»á€¬á€¸á€•á€«á€€ á€–á€¼á€á€ºá€á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸á‹
*   **`return_tensors="pt"`**: PyTorch tensors á€™á€»á€¬á€¸á€¡á€–á€¼á€…á€º output á€•á€¼á€”á€ºá€•á€±á€¸á€›á€”á€º argumentá‹
*   **`encoded_input.items()`**: dictionary á€™á€¾ key-value pairs á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€šá€°á€á€¼á€„á€ºá€¸á‹
*   **`k: v.to(device)`**: dictionary comprehension á€–á€¼á€„á€·á€º input tensors á€™á€»á€¬á€¸á€€á€­á€¯ GPU á€á€­á€¯á€· á€›á€½á€¾á€±á€·á€á€¼á€„á€ºá€¸á‹
*   **`model(**encoded_input)`**: model á€€á€­á€¯ encoded inputs á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º run á€á€¼á€„á€ºá€¸á‹
*   **`embedding.shape`**: embedding vector á á€•á€¯á€¶á€á€á€¹á€á€¬á€”á€º (dimensions) á€€á€­á€¯ á€•á€¼á€á€á€¼á€„á€ºá€¸á‹
*   **768-dimensional Vector**: dimensions á‡á†áˆ á€á€¯á€•á€«á€á€„á€ºá€á€±á€¬ vectorá‹
*   **`detach().cpu().numpy()[0]`**: PyTorch tensor á€€á€­á€¯ detach (computation graph á€™á€¾ á€–á€¼á€á€ºá€á€±á€¬á€€á€º) á€•á€¼á€®á€¸ CPU á€á€­á€¯á€· á€›á€½á€¾á€±á€·áŠ á€‘á€­á€¯á€·á€”á€±á€¬á€€á€º NumPy array á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸á‹
*   **NumPy Arrays**: Python á€á€½á€„á€º á€‚á€á€”á€ºá€¸á€†á€­á€¯á€„á€ºá€›á€¬ á€á€½á€€á€ºá€á€»á€€á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ array objectá‹
*   **FAISS Index**: FAISS library á€™á€¾ efficient similarity search á€¡á€á€½á€€á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ data structureá‹
*   **`Dataset.add_faiss_index()` Function**: Hugging Face Dataset á€á€½á€„á€º FAISS index á€á€…á€ºá€á€¯á€€á€­á€¯ á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ functioná‹
*   **Nearest Neighbor Lookup**: input query á€”á€¾á€„á€·á€º á€¡á€á€°á€†á€¯á€¶á€¸á€á€±á€¬ item á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€á€¼á€„á€ºá€¸á‹
*   **`Dataset.get_nearest_examples()` Function**: Dataset á FAISS index á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ query á€”á€¾á€„á€·á€º á€¡á€á€°á€†á€¯á€¶á€¸á€á€±á€¬ examples á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€¼á€”á€ºá€•á€±á€¸á€á€±á€¬ functioná‹
*   **`question_embedding`**: Query á€…á€¬á€á€¬á€¸á embedding vectorá‹
*   **`k` Parameter**: `get_nearest_examples()` function á€á€½á€„á€º á€¡á€”á€®á€¸á€†á€¯á€¶á€¸ examples á€¡á€›á€±á€¡á€á€½á€€á€º (k) á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ parameterá‹
*   **`pd.DataFrame.from_dict()`**: dictionary á€á€…á€ºá€á€¯á€™á€¾ Pandas DataFrame á€á€…á€ºá€á€¯á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€á€±á€¬ methodá‹
*   **`samples_df.sort_values("scores", ascending=False, inplace=True)`**: DataFrame á€€á€­á€¯ `scores` column á€¡á€œá€­á€¯á€€á€º á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸á€™á€¾ á€¡á€”á€Šá€ºá€¸á€†á€¯á€¶á€¸á€á€­á€¯á€· á€…á€®á€…á€‰á€ºá€á€¼á€„á€ºá€¸á‹
*   **`iterrows()`**: DataFrame á rows á€™á€»á€¬á€¸á€€á€­á€¯ iterate á€œá€¯á€•á€ºá€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ methodá‹