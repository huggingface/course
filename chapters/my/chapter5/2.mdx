# ကျွန်ုပ်၏ Dataset သည် Hub တွင် မရှိလျှင် ဘာလုပ်ရမလဲ။[[what-if-my-dataset-isnt-on-the-hub]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

Datasets တွေကို download လုပ်ဖို့ [Hugging Face Hub](https://huggingface.co/datasets) ကို ဘယ်လိုအသုံးပြုရမယ်ဆိုတာ သင်သိပါတယ်။ ဒါပေမယ့် သင့် laptop မှာ ဒါမှမဟုတ် remote server တစ်ခုမှာ သိမ်းဆည်းထားတဲ့ data တွေနဲ့ အလုပ်လုပ်ရတာမျိုး မကြာခဏ ကြုံတွေ့ရမှာပါ။ ဒီအပိုင်းမှာ 🤗 Datasets ကို အသုံးပြုပြီး Hugging Face Hub မှာ မရရှိနိုင်တဲ့ datasets တွေကို ဘယ်လို load လုပ်ရမလဲဆိုတာ ပြသပေးပါမယ်။

<Youtube id="HyQgpJTkRdE"/>

## Local နှင့် Remote Datasets များနှင့် အလုပ်လုပ်ခြင်း[[working-with-local-and-remote-datasets]]

🤗 Datasets က local နဲ့ remote datasets တွေကို load လုပ်ရာမှာ အသုံးပြုနိုင်တဲ့ loading scripts တွေကို ပံ့ပိုးပေးပါတယ်။ ၎င်းက အောက်ပါ common data formats များစွာကို ထောက်ပံ့ပေးပါတယ်။

|    Data format     | Loading script |                         Example                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Text files     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

ဇယားမှာ ပြသထားတဲ့အတိုင်း၊ data format တစ်ခုစီအတွက် `load_dataset()` function မှာ loading script အမျိုးအစားကို သတ်မှတ်ဖို့ပဲ လိုပါတယ်။ ဒါ့အပြင် file တစ်ခု သို့မဟုတ် တစ်ခုထက်ပိုတဲ့ files တွေရဲ့ path ကို သတ်မှတ်ပေးမယ့် `data_files` argument လည်း လိုပါတယ်။ local files ကနေ dataset တစ်ခုကို load လုပ်ခြင်းဖြင့် စတင်ပါမယ်၊ နောက်ပိုင်းမှာ remote files တွေနဲ့ ဘယ်လိုလုပ်ရမလဲဆိုတာ ကြည့်ရပါမယ်။

## Local Dataset တစ်ခုကို Loading လုပ်ခြင်း[[loading-a-local-dataset]]

ဒီဥပမာအတွက် [SQuAD-it dataset](https://github.com/crux82/squad-it/) ကို ကျွန်တော်တို့ အသုံးပြုပါမယ်။ ဒါက Italian ဘာသာစကားနဲ့ မေးခွန်းဖြေဆိုခြင်း (question answering) အတွက် large-scale dataset တစ်ခု ဖြစ်ပါတယ်။

training နဲ့ test splits တွေကို GitHub မှာ host လုပ်ထားတာကြောင့်၊ ရိုးရှင်းတဲ့ `wget` command တစ်ခုနဲ့ download လုပ်နိုင်ပါတယ်။

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```
 
ဒါက *SQuAD_it-train.json.gz* နဲ့ *SQuAD_it-test.json.gz* လို့ခေါ်တဲ့ compressed files နှစ်ခုကို download လုပ်ပါလိမ့်မယ်။ ဒါတွေကို Linux ရဲ့ `gzip` command နဲ့ decompress လုပ်နိုင်ပါတယ်။

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

compressed files တွေဟာ _SQuAD_it-train.json_ နဲ့ _SQuAD_it-test.json_ တို့နဲ့ အစားထိုးခံရပြီး data တွေကို JSON format နဲ့ သိမ်းဆည်းထားတာကို တွေ့ရပါတယ်။

> [!TIP]
> ✎ အပေါ်က shell commands တွေမှာ `!` character ဘာကြောင့် ပါလဲဆိုတာ သင်တွေးနေမယ်ဆိုရင်၊ ဒါတွေက Jupyter notebook ထဲမှာ run နေလို့ပါပဲ။ သင် terminal ထဲမှာ dataset ကို download လုပ်ပြီး unzip လုပ်ချင်တယ်ဆိုရင် prefix ကို ရိုးရှင်းစွာ ဖယ်ရှားလိုက်ပါ။

`load_dataset()` function နဲ့ JSON file တစ်ခုကို load လုပ်ဖို့၊ ကျွန်တော်တို့ဟာ သာမန် JSON (nested dictionary နဲ့ ဆင်တူ) ဒါမှမဟုတ် JSON Lines (line-separated JSON) နဲ့ အလုပ်လုပ်နေတာ ဟုတ်မဟုတ် သိဖို့ပဲ လိုပါတယ်။ question answering datasets အများစုလိုပဲ၊ SQuAD-it က nested format ကို အသုံးပြုပြီး text အားလုံးကို `data` field ထဲမှာ သိမ်းဆည်းထားပါတယ်။ ဒါက `field` argument ကို အောက်ပါအတိုင်း သတ်မှတ်ပေးခြင်းဖြင့် dataset ကို load လုပ်နိုင်တယ်လို့ ဆိုလိုပါတယ်။

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

default အားဖြင့်၊ local files တွေကို load လုပ်ခြင်းက `train` split ပါဝင်တဲ့ `DatasetDict` object တစ်ခုကို ဖန်တီးပါတယ်။ ဒါကို `squad_it_dataset` object ကို စစ်ဆေးကြည့်ခြင်းဖြင့် မြင်နိုင်ပါတယ်။

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

ဒါက ကျွန်တော်တို့ကို rows အရေအတွက်နဲ့ training set နဲ့ ဆက်စပ်နေတဲ့ column names တွေကို ပြသပါတယ်။ `train` split ထဲက ဥပမာတစ်ခုကို အောက်ပါအတိုင်း indexing လုပ်ခြင်းဖြင့် ကြည့်ရှုနိုင်ပါတယ်။

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si è verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

ကောင်းပါပြီ၊ ကျွန်တော်တို့ရဲ့ ပထမဆုံး local dataset ကို load လုပ်ခဲ့ပါပြီ။ ဒါပေမယ့် ဒါက training set အတွက် အလုပ်ဖြစ်ခဲ့ပေမယ့်၊ ကျွန်တော်တို့ တကယ်လိုချင်တာက `train` နဲ့ `test` splits နှစ်ခုလုံးကို `DatasetDict` object တစ်ခုတည်းမှာ ထည့်သွင်းဖို့ပါပဲ။ ဒါမှ `Dataset.map()` functions တွေကို splits နှစ်ခုလုံးပေါ်မှာ တစ်ပြိုင်နက်တည်း အသုံးပြုနိုင်မှာပါ။ ဒါကိုလုပ်ဖို့၊ `data_files` argument ကို dictionary တစ်ခု ပေးနိုင်ပါတယ်။ အဲဒီ dictionary က split name တစ်ခုစီကို အဲဒီ split နဲ့ ဆက်စပ်နေတဲ့ file တစ်ခုသို့ map လုပ်ပေးပါတယ်။

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

ဒါက ကျွန်တော်တို့ လိုချင်တာ အတိအကျပါပဲ။ အခုဆိုရင် data ကို သန့်ရှင်းရေးလုပ်ဖို့၊ reviews တွေကို tokenize လုပ်ဖို့ စတဲ့ preprocessing techniques မျိုးစုံကို အသုံးပြုနိုင်ပါပြီ။

> [!TIP]
> `load_dataset()` function ရဲ့ `data_files` argument က အတော်လေး ပြောင်းလွယ်ပြင်လွယ်ရှိပြီး file path တစ်ခုတည်း၊ file paths စာရင်းတစ်ခု ဒါမှမဟုတ် split names တွေကို file paths တွေနဲ့ map လုပ်တဲ့ dictionary တစ်ခု ဖြစ်နိုင်ပါတယ်။ Unix shell က အသုံးပြုတဲ့ စည်းမျဉ်းတွေအရ သတ်မှတ်ထားတဲ့ pattern နဲ့ ကိုက်ညီတဲ့ files တွေကိုလည်း glob လုပ်နိုင်ပါတယ်။ (ဥပမာ- directory တစ်ခုထဲက JSON files အားလုံးကို `data_files="*.json"` လို့ သတ်မှတ်ခြင်းဖြင့် single split အဖြစ် glob လုပ်နိုင်ပါတယ်။) အသေးစိတ်အချက်အလက်တွေအတွက် 🤗 Datasets [documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) ကို ကြည့်ပါ။

🤗 Datasets မှာရှိတဲ့ loading scripts တွေက input files တွေကို အလိုအလျောက် decompress လုပ်ခြင်းကို ထောက်ပံ့ပေးပါတယ်၊ ဒါကြောင့် `data_files` argument ကို compressed files တွေဆီ တိုက်ရိုက်ညွှန်ပြခြင်းဖြင့် `gzip` ကို အသုံးပြုတာကို ကျော်ဖြတ်နိုင်ခဲ့ပါတယ်။

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

ဒါက GZIP files အများအပြားကို ကိုယ်တိုင် decompress လုပ်ချင်ခြင်းမရှိတဲ့အခါ အသုံးဝင်ပါတယ်။ အလိုအလျောက် decompression က ZIP နဲ့ TAR လိုမျိုး အခြား common formats တွေနဲ့လည်း သက်ဆိုင်တာကြောင့်၊ သင် `data_files` ကို compressed files တွေဆီ ညွှန်ပြဖို့ပဲ လိုအပ်ပြီး အဆင်သင့်ပါပဲ!

သင့် laptop ဒါမှမဟုတ် desktop ပေါ်မှာ local files တွေကို ဘယ်လို load လုပ်ရမယ်ဆိုတာ သိပြီဆိုတော့၊ remote files တွေကို loading လုပ်တာကို ကြည့်ရအောင်။

## Remote Dataset တစ်ခုကို Loading လုပ်ခြင်း[[loading-a-remote-dataset]]

သင်က ကုမ္ပဏီတစ်ခုမှာ data scientist ဒါမှမဟုတ် coder အဖြစ် အလုပ်လုပ်နေတယ်ဆိုရင်၊ သင် analyze လုပ်ချင်တဲ့ datasets တွေဟာ remote server တစ်ခုမှာ သိမ်းဆည်းထားနိုင်ခြေ များပါတယ်။ ကံကောင်းစွာနဲ့ပဲ၊ remote files တွေကို load လုပ်တာက local files တွေကို load လုပ်တာလောက် ရိုးရှင်းပါတယ်။ local files တွေရဲ့ path ကို ပေးမယ့်အစား၊ `load_dataset()` function ရဲ့ `data_files` argument ကို remote files တွေ သိမ်းဆည်းထားတဲ့ URL တစ်ခု သို့မဟုတ် တစ်ခုထက်ပိုတဲ့ URL တွေဆီ ညွှန်ပြပါတယ်။ ဥပမာ၊ GitHub မှာ host လုပ်ထားတဲ့ SQuAD-it dataset အတွက်၊ ကျွန်တော်တို့ `data_files` ကို _SQuAD_it-*.json.gz_ URLs တွေဆီ အောက်ပါအတိုင်း ညွှန်ပြနိုင်ပါတယ်။

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```
ဒါက အပေါ်မှာ ရရှိခဲ့တဲ့ `DatasetDict` object တူတူကို ပြန်ပေးပါတယ်၊ ဒါပေမယ့် _SQuAD_it-*.json.gz_ files တွေကို ကိုယ်တိုင် download လုပ်ပြီး decompress လုပ်ရမယ့်အဆင့်ကို ကျွန်တော်တို့အတွက် သက်သာစေပါတယ်။ ဒါက Hugging Face Hub မှာ host လုပ်ထားတာ မဟုတ်တဲ့ datasets တွေကို load လုပ်တဲ့ နည်းလမ်းအမျိုးမျိုးကို ကျွန်တော်တို့ရဲ့ လေ့လာမှုကို နိဂုံးချုပ်လိုက်ပါပြီ။ အခု ကျွန်တော်တို့ ကစားစရာ dataset တစ်ခု ရရှိပြီဆိုတော့၊ data-wrangling techniques အမျိုးမျိုးနဲ့ လက်တွေ့လုပ်ဆောင်ကြည့်ရအောင်။

> [!TIP]
> ✏️ **စမ်းသပ်ကြည့်ပါ။** GitHub မှာ ဒါမှမဟုတ် [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) မှာ host လုပ်ထားတဲ့ အခြား dataset တစ်ခုကို ရွေးပြီး အထက်မှာ မိတ်ဆက်ခဲ့တဲ့ နည်းလမ်းတွေကို အသုံးပြုပြီး local နဲ့ remote နှစ်မျိုးလုံး load လုပ်ကြည့်ပါ။ bonus အမှတ်များအတွက်၊ CSV ဒါမှမဟုတ် text format နဲ့ သိမ်းဆည်းထားတဲ့ dataset တစ်ခုကို load လုပ်ကြည့်ပါ (ဒီ formats တွေအကြောင်း အသေးစိတ်အချက်အလက်တွေအတွက် [documentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) ကို ကြည့်ပါ)။

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Hugging Face Hub**: AI မော်ဒယ်တွေ၊ datasets တွေနဲ့ demo တွေကို အခြားသူတွေနဲ့ မျှဝေဖို့၊ ရှာဖွေဖို့နဲ့ ပြန်လည်အသုံးပြုဖို့အတွက် အွန်လိုင်း platform တစ်ခု ဖြစ်ပါတယ်။
*   **Local Dataset**: သင့်ကွန်ပျူတာ (laptop သို့မဟုတ် desktop) ၏ hard drive ပေါ်တွင် သိမ်းဆည်းထားသော dataset။
*   **Remote Dataset**: အွန်လိုင်းဆာဗာ သို့မဟုတ် cloud storage တွင် သိမ်းဆည်းထားသော dataset။
*   **🤗 Datasets**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် ဒေတာအစုအဝေး (datasets) တွေကို လွယ်လွယ်ကူကူ ဝင်ရောက်ရယူ၊ စီမံခန့်ခွဲပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **Loading Scripts**: 🤗 Datasets library အတွင်းရှိ code များဖြစ်ပြီး မတူညီသော data formats (CSV, JSON, Text, Pandas) မှ datasets များကို load လုပ်ရန်အတွက် တာဝန်ယူသည်။
*   **CSV (Comma-Separated Values)**: ကော်မာများဖြင့် ပိုင်းခြားထားသော ဒေတာတန်ဖိုးများပါဝင်သည့် plain text file format။
*   **TSV (Tab-Separated Values)**: Tab များဖြင့် ပိုင်းခြားထားသော ဒေတာတန်ဖိုးများပါဝင်သည့် plain text file format။
*   **Text Files**: စာသားအချက်အလက်များသာ ပါဝင်သော ဖိုင်များ။
*   **JSON (JavaScript Object Notation)**: ဒေတာများကို ပေါ့ပေါ့ပါးပါး ဖလှယ်နိုင်သော format ဖြစ်ပြီး လူသားများ ဖတ်ရှုရလွယ်ကူပြီး စက်များ စီမံဆောင်ရွက်ရလွယ်ကူသည်။
*   **JSON Lines**: JSON objects များကို line တစ်ကြောင်းစီတွင် တစ်ခုစီ ထားရှိသော JSON format ၏ ပုံစံတစ်မျိုး။
*   **Pickled DataFrames**: Python ၏ `pickle` module ကို အသုံးပြု၍ Python object (ဥပမာ- Pandas DataFrame) ကို binary format ဖြင့် သိမ်းဆည်းထားသော ဖိုင်။
*   **`load_dataset()` Function**: Hugging Face Datasets library မှ dataset များကို download လုပ်ပြီး cache လုပ်ရန် အသုံးပြုသော function။
*   **`data_files` Argument**: `load_dataset()` function တွင် dataset files (local သို့မဟုတ် remote) ၏ path (သို့မဟုတ် URL) ကို သတ်မှတ်ရန် အသုံးပြုသော argument။
*   **SQuAD-it Dataset**: Italian ဘာသာစကားဖြင့် မေးခွန်းဖြေဆိုခြင်း (question answering) အတွက် အသုံးပြုသော dataset။
*   **GitHub**: Version control အတွက် Git ကို အသုံးပြုသည့် web-based platform တစ်ခုဖြစ်ပြီး code များနှင့် project များကို host လုပ်သည်။
*   **`wget` Command**: ကွန်ရက်မှ ဖိုင်များကို download လုပ်ရန်အတွက် Unix/Linux command-line utility။
*   **Compressed Files**: ဖိုင်အရွယ်အစားကို လျှော့ချရန်အတွက် ဖိသိပ်ထားသော ဖိုင်များ (ဥပမာ- `.gz`, `.zip`, `.tar`)။
*   **`gzip` Command**: GZIP format ဖြင့် ဖိုင်များကို compress (ဖိသိပ်) သို့မဟုတ် decompress (ဖိသိပ်မှုဖြေလျှော့) ရန်အတွက် Linux command-line utility။
*   **`!` Character**: Jupyter/Colab Notebooks များတွင် shell commands များကို run ရန်အတွက် အသုံးပြုသော prefix။
*   **Jupyter Notebook**: code, text, images, နှင့် mathematical equations တို့ကို ပေါင်းစပ်နိုင်သော interactive computing environment။
*   **Terminal**: command-line interface မှတစ်ဆင့် ကွန်ပျူတာကို ထိန်းချုပ်ရန် အသုံးပြုသော interface။
*   **Nested Dictionary**: dictionary အတွင်း၌ အခြား dictionary များ ပါဝင်သော dictionary။
*   **`field` Argument**: `load_dataset()` function တွင် JSON file အတွင်းရှိ မည်သည့် field မှ data များကို load လုပ်ရမည်ကို သတ်မှတ်ရန် အသုံးပြုသော argument။
*   **`DatasetDict` Object**: Training set, validation set, နှင့် test set ကဲ့သို့သော dataset အများအပြားကို dictionary ပုံစံဖြင့် သိမ်းဆည်းထားသော object။
*   **`train` Split**: Model ကို လေ့ကျင့်ရန်အတွက် အသုံးပြုသော dataset အပိုင်း။
*   **`test` Split**: Model ၏ နောက်ဆုံး စွမ်းဆောင်ရည်ကို တိုင်းတာရန် အသုံးပြုသော dataset အပိုင်း။
*   **Rows (Examples)**: dataset အတွင်းရှိ တစ်ခုချင်းစီသော data entry များ။
*   **Column Names (Features)**: dataset အတွင်းရှိ attributes များ သို့မဟုတ် ကဏ္ဍများ။
*   **Indexing**: dataset (သို့မဟုတ် list, dictionary) အတွင်းရှိ သီးခြား element တစ်ခုကို ၎င်း၏ index (သို့မဟုတ် key) ကို အသုံးပြု၍ ဝင်ရောက်ကြည့်ရှုခြင်း။
*   **Preprocessing Techniques**: ဒေတာများကို model က နားလည်ပြီး လုပ်ဆောင်နိုင်တဲ့ ပုံစံအဖြစ် ပြောင်းလဲပြင်ဆင်ခြင်း လုပ်ငန်းစဉ်အတွက် အသုံးပြုသော နည်းလမ်းများ။
*   **Tokenize**: စာသား (သို့မဟုတ် အခြားဒေတာ) ကို AI မော်ဒယ်များ စီမံဆောင်ရွက်နိုင်ရန် tokens တွေအဖြစ် ပိုင်းခြားပေးသည့် လုပ်ငန်းစဉ်။
*   **Glob Files**: Unix shell တွင် wildcard (ဥပမာ- `*`, `?`) များကို အသုံးပြု၍ pattern တစ်ခုနှင့် ကိုက်ညီသော ဖိုင်များကို ရှာဖွေခြင်း။
*   **URL (Uniform Resource Locator)**: web ပေါ်ရှိ အရင်းအမြစ်တစ်ခု (ဥပမာ- web page, file) ၏ လိပ်စာ။
*   **Data-wrangling Techniques**: ကုန်ကြမ်းဒေတာ (raw data) များကို ပိုမိုအသုံးဝင်ပြီး သန့်ရှင်းသော ပုံစံသို့ ပြောင်းလဲရန်အတွက် လုပ်ဆောင်သော လုပ်ငန်းစဉ်များ။
*   **UCI Machine Learning Repository**: Machine learning datasets များစွာကို စုစည်းထားသော repository တစ်ခု။
