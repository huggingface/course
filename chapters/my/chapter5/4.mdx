# Big Data လား။ 🤗 Datasets က ကယ်တင်ပါလိမ့်မယ်![[big-data-datasets-to-the-rescue]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
]} />

ဒီဘက်ခေတ်မှာ multi-gigabyte datasets တွေနဲ့ အလုပ်လုပ်ရတာ မဆန်းပါဘူး။ အထူးသဖြင့် BERT ဒါမှမဟုတ် GPT-2 လို transformer တစ်ခုကို အစကနေ pretrain လုပ်ဖို့ စီစဉ်နေတယ်ဆိုရင်ပေါ့။ ဒီလိုကိစ္စတွေမှာ၊ data တွေကို _loading_ လုပ်တာကတောင် စိန်ခေါ်မှုတစ်ခု ဖြစ်နိုင်ပါတယ်။ ဥပမာအားဖြင့်၊ GPT-2 ကို pretrain လုပ်ဖို့ အသုံးပြုခဲ့တဲ့ WebText corpus မှာ document ပေါင်း ၈ သန်းကျော်နဲ့ 40 GB စာသားတွေ ပါဝင်ပါတယ်၊ ဒါကို သင့် laptop ရဲ့ RAM ထဲကို loading လုပ်တာက သင့် laptop ကို ဒုက္ခပေးနိုင်ပါတယ်။

ကံကောင်းစွာနဲ့ပဲ၊ 🤗 Datasets ကို ဒီကန့်သတ်ချက်တွေကို ကျော်လွှားနိုင်ဖို့ ဒီဇိုင်းထုတ်ထားတာပါ။ ၎င်းက datasets တွေကို _memory-mapped_ files တွေအဖြစ် သတ်မှတ်ပြီး memory management ပြဿနာတွေကနေ သင့်ကို ကင်းဝေးစေပါတယ်။ ပြီးတော့ corpus ထဲက entries တွေကို _streaming_ လုပ်ခြင်းအားဖြင့် hard drive ကန့်သတ်ချက်တွေကနေလည်း ကင်းဝေးစေပါတယ်။

<Youtube id="JwISwTCPPWo"/>

ဒီအပိုင်းမှာတော့ 🤗 Datasets ရဲ့ ဒီ features တွေကို the Pile (825 GB corpus) နဲ့အတူ လေ့လာသွားမှာပါ။ စတင်လိုက်ရအောင်!

## The Pile ဆိုတာ ဘာလဲ။[[what-is-the-pile]]

The Pile ဟာ large-scale language models တွေကို training လုပ်ဖို့အတွက် [EleutherAI](https://www.eleuther.ai) က ဖန်တီးခဲ့တဲ့ English text corpus တစ်ခုပါ။ ၎င်းမှာ သိပ္ပံဆိုင်ရာဆောင်းပါးတွေ၊ GitHub code repositories တွေနဲ့ filter လုပ်ထားတဲ့ web text တွေ ပါဝင်တဲ့ မတူကွဲပြားတဲ့ datasets များစွာ ပါဝင်ပါတယ်။ training corpus ကို [14 GB chunks](https://the-eye.eu/public/AI/pile/) အဖြစ် ရရှိနိုင်ပြီး၊ [individual components](https://the-eye.eu/public/AI/pile_preliminary_components/) အချို့ကိုလည်း download လုပ်နိုင်ပါတယ်။ PubMed Abstracts dataset ကို အရင်ကြည့်ရအောင်။ ဒါက [PubMed](https://pubmed.ncbi.nlm.nih.gov/) က biomedical publications ၁၅ သန်းက abstracts တွေရဲ့ corpus တစ်ခုပါ။ dataset က [JSON Lines format](https://jsonlines.org) နဲ့ `zstandard` library ကို အသုံးပြုပြီး compressed လုပ်ထားတာကြောင့်၊ အရင်ဆုံး အဲဒါကို install လုပ်ဖို့ လိုအပ်ပါတယ်။

```py
!pip install zstandard
```

နောက်တစ်ခုကတော့ [section 2](/course/chapter5/2) မှာ ကျွန်တော်တို့ သင်ယူခဲ့တဲ့ remote files တွေအတွက် method ကို အသုံးပြုပြီး dataset ကို load လုပ်နိုင်ပါတယ်။

```py
from datasets import load_dataset

# ဒါက အချိန်အနည်းငယ် ကြာနိုင်ပါတယ်၊ ဒါကြောင့် သင်စောင့်နေတုန်း လက်ဖက်ရည် ဒါမှမဟုတ် ကော်ဖီတစ်ခွက်လောက် သွားယူလိုက်ပါ :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

ကျွန်တော်တို့ရဲ့ dataset မှာ rows ပေါင်း ၁၅,၅၁၈,၀၀၉ ခုနဲ့ columns ၂ ခု ရှိတယ်ဆိုတာ တွေ့နိုင်ပါတယ်၊ ဒါက အများကြီးပါပဲ!

> [!TIP]
> ✎ default အားဖြင့်၊ 🤗 Datasets က dataset တစ်ခုကို load လုပ်ဖို့ လိုအပ်တဲ့ files တွေကို decompress လုပ်ပါလိမ့်မယ်။ hard drive space ကို ချွေတာချင်တယ်ဆိုရင်၊ `DownloadConfig(delete_extracted=True)` ကို `load_dataset()` ရဲ့ `download_config` argument သို့ ပေးပို့နိုင်ပါတယ်။ အသေးစိတ်အချက်အလက်တွေအတွက် [documentation](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig) ကို ကြည့်ပါ။

ပထမဆုံး ဥပမာရဲ့ အကြောင်းအရာတွေကို စစ်ဆေးကြည့်ရအောင်...

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

ကောင်းပါပြီ၊ ဒါက ဆေးဘက်ဆိုင်ရာ ဆောင်းပါးတစ်စောင်ရဲ့ abstract လိုပါပဲ။ အခု dataset ကို load လုပ်ဖို့ ကျွန်တော်တို့ ဘယ်လောက် RAM အသုံးပြုခဲ့လဲဆိုတာ ကြည့်ရအောင်!

## Memory Mapping ၏ မှော်ပညာ[[the-magic-of-memory-mapping]]

Python မှာ memory အသုံးပြုမှုကို တိုင်းတာဖို့ ရိုးရှင်းတဲ့နည်းလမ်းက [`psutil`](https://psutil.readthedocs.io/en/latest/) library နဲ့ပါ။ ဒါကို `pip` နဲ့ အောက်ပါအတိုင်း install လုပ်နိုင်ပါတယ်။

```python
!pip install psutil
```

ဒါက ကျွန်တော်တို့ လက်ရှိ process ရဲ့ memory အသုံးပြုမှုကို အောက်ပါအတိုင်း စစ်ဆေးနိုင်တဲ့ `Process` class တစ်ခုကို ပံ့ပိုးပေးပါတယ်။

```py
import psutil

# Process.memory_info ကို bytes ဖြင့် ဖော်ပြထားသောကြောင့် megabytes သို့ ပြောင်းပါ။
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
RAM used: 5678.33 MB
```

ဒီနေရာမှာ `rss` attribute က _resident set size_ ကို ရည်ညွှန်းပြီး၊ ဒါက process တစ်ခုက RAM မှာ နေရာယူထားတဲ့ memory ရဲ့ အစိတ်အပိုင်းပါ။ ဒီတိုင်းတာမှုမှာ Python interpreter နဲ့ ကျွန်တော်တို့ load လုပ်ထားတဲ့ libraries တွေ အသုံးပြုတဲ့ memory လည်း ပါဝင်တာကြောင့်၊ dataset ကို load လုပ်ဖို့ အသုံးပြုခဲ့တဲ့ အမှန်တကယ် memory ပမာဏက နည်းနည်း ပိုနည်းပါတယ်။ နှိုင်းယှဉ်ဖို့အတွက်၊ `dataset_size` attribute ကို အသုံးပြုပြီး disk ပေါ်မှာ dataset က ဘယ်လောက်ကြီးလဲဆိုတာ ကြည့်ရအောင်။ ရလဒ်က အရင်ကလို bytes နဲ့ ဖော်ပြထားတာကြောင့်၊ ဒါကို gigabytes သို့ ကိုယ်တိုင် ပြောင်းဖို့ လိုအပ်ပါတယ်။

```py
print(f"Dataset size in bytes: {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Dataset size in bytes : 20979437051
Dataset size (cache file) : 19.54 GB
```

ကောင်းပါပြီ။ 20 GB နီးပါး ကြီးမားပေမယ့်၊ ကျွန်တော်တို့ dataset ကို RAM နည်းနည်းလေးနဲ့ load လုပ်ပြီး ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်။

> [!TIP]
> ✏️ **စမ်းသပ်ကြည့်ပါ။** သင့် laptop သို့မဟုတ် desktop ရဲ့ RAM ထက် ပိုကြီးတဲ့ Pile က [subsets](https://the-eye.eu/public/AI/pile_preliminary_components/) တစ်ခုကို ရွေးပါ၊ 🤗 Datasets နဲ့ load လုပ်ပါ၊ ပြီးတော့ အသုံးပြုထားတဲ့ RAM ပမာဏကို တိုင်းတာပါ။ တိကျတဲ့ တိုင်းတာမှုတစ်ခု ရဖို့အတွက်၊ ဒါကို process အသစ်တစ်ခုမှာ လုပ်ဖို့ လိုအပ်ပါလိမ့်မယ်။ subset တစ်ခုစီရဲ့ decompress လုပ်ပြီးသား အရွယ်အစားတွေကို [the Pile paper](https://arxiv.org/abs/2101.00027) ရဲ့ Table 1 မှာ ရှာတွေ့နိုင်ပါတယ်။

သင် Pandas နဲ့ ရင်းနှီးတယ်ဆိုရင်၊ ဒီရလဒ်က Wes Kinney ရဲ့ နာမည်ကြီး [rule of thumb](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) ကြောင့် အံ့သြစရာ ဖြစ်နိုင်ပါတယ်။ ဒါက ပုံမှန်အားဖြင့် သင့် dataset ရဲ့ အရွယ်အစားထက် ၅ ဆမှ ၁၀ ဆအထိ RAM လိုအပ်တယ်လို့ ဆိုလိုပါတယ်။ ဒါဆို 🤗 Datasets က ဒီ memory management ပြဿနာကို ဘယ်လိုဖြေရှင်းတာလဲ။ 🤗 Datasets က dataset တစ်ခုစီကို [memory-mapped file](https://en.wikipedia.org/wiki/Memory-mapped_file) တစ်ခုအဖြစ် သတ်မှတ်ပါတယ်။ ဒါက RAM နဲ့ filesystem storage ကြား mapping တစ်ခုကို ပံ့ပိုးပေးပြီး၊ library က dataset ကို memory ထဲကို အပြည့်အဝ load လုပ်ဖို့မလိုဘဲ dataset ရဲ့ elements တွေကို ဝင်ရောက်ကြည့်ရှုပြီး အလုပ်လုပ်နိုင်စေပါတယ်။

Memory-mapped files တွေကို processes များစွာကြား မျှဝေနိုင်တာကြောင့်၊ `Dataset.map()` လို methods တွေကို dataset ကို ရွှေ့ပြောင်းခြင်း ဒါမှမဟုတ် copy လုပ်ဖို့ မလိုဘဲ parallelize လုပ်နိုင်ပါတယ်။ အောက်ခံမှာ၊ ဒီစွမ်းဆောင်ရည်တွေ အားလုံးကို [Apache Arrow](https://arrow.apache.org) memory format နဲ့ [`pyarrow`](https://arrow.apache.org/docs/python/index.html) library တွေကနေ အကောင်အထည်ဖော်ထားတာဖြစ်ပြီး၊ ဒါတွေက data loading နဲ့ processing ကို လျှပ်စီးလက်သလို မြန်ဆန်စေပါတယ်။ (Apache Arrow အကြောင်းနဲ့ Pandas နဲ့ နှိုင်းယှဉ်မှုတွေအတွက် [Dejan Simic ရဲ့ blog post](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a) ကို ကြည့်ပါ။) ဒါကို လက်တွေ့မြင်ရဖို့အတွက်၊ PubMed Abstracts dataset ထဲက elements အားလုံးကို iterate လုပ်ပြီး speed test အနည်းငယ် run ကြည့်ရအောင်။

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

ဒီနေရာမှာ ကျွန်တော်တို့ Python ရဲ့ `timeit` module ကို အသုံးပြုပြီး `code_snippet` က ယူခဲ့တဲ့ execution time ကို တိုင်းတာခဲ့ပါတယ်။ သင်ဟာ ပုံမှန်အားဖြင့် GB/s ရဲ့ ဒဿမဂဏန်း အနည်းငယ်ကနေ GB/s အများအပြားအထိ မြန်နှုန်းနဲ့ dataset တစ်ခုကို iterate လုပ်နိုင်ပါလိမ့်မယ်။ ဒါက အသုံးချပရိုဂရမ်အများစုအတွက် ကောင်းမွန်စွာ အလုပ်လုပ်ပါတယ်၊ ဒါပေမယ့် တစ်ခါတစ်ရံမှာ သင့် laptop ရဲ့ hard drive မှာ သိမ်းဆည်းဖို့တောင် အရမ်းကြီးမားတဲ့ dataset တစ်ခုနဲ့ အလုပ်လုပ်ရပါလိမ့်မယ်။ ဥပမာအားဖြင့်၊ ကျွန်တော်တို့ Pile ကို အပြည့်အစုံ download လုပ်ဖို့ ကြိုးစားခဲ့မယ်ဆိုရင်၊ 825 GB free disk space လိုအပ်ပါလိမ့်မယ်! ဒီကိစ္စတွေကို ကိုင်တွယ်ဖို့အတွက်၊ 🤗 Datasets က streaming feature တစ်ခုကို ပံ့ပိုးပေးပြီး dataset တစ်ခုလုံးကို download လုပ်ဖို့မလိုဘဲ elements တွေကို on the fly download လုပ်ပြီး ဝင်ရောက်ကြည့်ရှုနိုင်စေပါတယ်။ ဒါက ဘယ်လိုအလုပ်လုပ်လဲဆိုတာ ကြည့်ရအောင်။

> [!TIP]
> 💡 Jupyter notebooks တွေမှာ [`%%timeit` magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) ကို အသုံးပြုပြီး cells တွေကိုလည်း time လုပ်နိုင်ပါတယ်။

## Streaming Datasets များ[[streaming-datasets]]

dataset streaming ကို enable လုပ်ဖို့အတွက် `load_dataset()` function သို့ `streaming=True` argument ကို ပေးပို့ဖို့ပဲ လိုပါတယ်။ ဥပမာအားဖြင့်၊ PubMed Abstracts dataset ကို streaming mode မှာ ထပ်မံ load လုပ်ကြည့်ရအောင်။

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

ဒီအခန်းမှာ ကျွန်တော်တို့ ကြုံတွေ့ခဲ့ရတဲ့ ရင်းနှီးတဲ့ `Dataset` အစား၊ `streaming=True` နဲ့ ပြန်ပေးတဲ့ object ဟာ `IterableDataset` ဖြစ်ပါတယ်။ နာမည်က ဖော်ပြသလိုပဲ၊ `IterableDataset` ရဲ့ elements တွေကို ဝင်ရောက်ကြည့်ရှုဖို့အတွက် ကျွန်တော်တို့ ဒါကို iterate လုပ်ဖို့ လိုအပ်ပါတယ်။ ကျွန်တော်တို့ရဲ့ streamed dataset ရဲ့ ပထမဆုံး element ကို အောက်ပါအတိုင်း ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်။

```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

streamed dataset မှ elements များကို `IterableDataset.map()` ကို အသုံးပြုပြီး on the fly process လုပ်နိုင်ပါတယ်။ ဒါက inputs တွေကို tokenize လုပ်ဖို့ လိုအပ်ရင် training လုပ်နေစဉ်အတွင်း အသုံးဝင်ပါတယ်။ လုပ်ငန်းစဉ်က [Chapter 3](/course/chapter3) မှာ ကျွန်တော်တို့ dataset ကို tokenize လုပ်ဖို့ အသုံးပြုခဲ့တဲ့ လုပ်ငန်းစဉ်အတိုင်း အတိအကျပါပဲ၊ ကွာခြားချက်ကတော့ outputs တွေကို တစ်ခုပြီးတစ်ခု ပြန်ပေးတာပါပဲ။

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

> [!TIP]
> 💡 streaming နဲ့ tokenization ကို အရှိန်မြှင့်ဖို့အတွက်၊ ယခင်အပိုင်းမှာ ကျွန်တော်တို့ တွေ့ခဲ့ရတဲ့အတိုင်း `batched=True` ကို ပေးပို့နိုင်ပါတယ်။ ဒါက examples တွေကို batch by batch လုပ်ဆောင်ပါလိမ့်မယ်။ default batch size က ၁,၀၀၀ ဖြစ်ပြီး `batch_size` argument နဲ့ သတ်မှတ်နိုင်ပါတယ်။

သင် `IterableDataset.shuffle()` ကို အသုံးပြုပြီး streamed dataset တစ်ခုကို shuffle လုပ်နိုင်ပေမယ့်၊ `Dataset.shuffle()` နဲ့မတူတာက ဒါက သတ်မှတ်ထားတဲ့ `buffer_size` ထဲက elements တွေကိုသာ shuffle လုပ်ပါတယ်။

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

ဒီဥပမာမှာ၊ ကျွန်တော်တို့ buffer ထဲက ပထမဆုံး ၁၀,၀၀၀ ဥပမာထဲက random example တစ်ခုကို ရွေးခဲ့ပါတယ်။ example တစ်ခုကို ဝင်ရောက်ကြည့်ရှုပြီးတာနဲ့၊ buffer ထဲက ၎င်းရဲ့နေရာကို corpus ထဲက နောက်ဆုံး example (အထက်ဖော်ပြပါကိစ္စတွင် ၁၀,၀၀၁ ခုမြောက် example) နဲ့ ဖြည့်ပေးပါလိမ့်မယ်။ `IterableDataset.take()` နဲ့ `IterableDataset.skip()` functions တွေကို အသုံးပြုပြီး streamed dataset ကနေ elements တွေကိုလည်း ရွေးချယ်နိုင်ပါတယ်။ ဒါတွေက `Dataset.select()` နဲ့ အလားတူ လုပ်ဆောင်ပါတယ်။ ဥပမာအားဖြင့်၊ PubMed Abstracts dataset ထဲက ပထမဆုံး ၅ ဥပမာကို ရွေးချယ်ဖို့ ကျွန်တော်တို့ အောက်ပါအတိုင်း လုပ်ဆောင်နိုင်ပါတယ်။

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

အလားတူပဲ၊ `IterableDataset.skip()` function ကို အသုံးပြုပြီး shuffled dataset ကနေ training နဲ့ validation splits တွေ ဖန်တီးနိုင်ပါတယ်။

```py
# ပထမဆုံး 1,000 examples တွေကို ကျော်ပြီး ကျန်တာတွေကို training set မှာ ထည့်သွင်းပါ
train_dataset = shuffled_dataset.skip(1000)
# validation set အတွက် ပထမဆုံး 1,000 examples တွေကို ယူပါ
validation_dataset = shuffled_dataset.take(1000)
```

multiple datasets တွေကို ပေါင်းစပ်ပြီး single corpus တစ်ခု ဖန်တီးခြင်းဆိုတဲ့ common application နဲ့ dataset streaming ကို ကျွန်တော်တို့ရဲ့ လေ့လာမှုကို နိဂုံးချုပ်လိုက်ရအောင်။ 🤗 Datasets က `interleave_datasets()` function တစ်ခုကို ပံ့ပိုးပေးပြီး၊ ဒါက `IterableDataset` objects စာရင်းတစ်ခုကို single `IterableDataset` တစ်ခုအဖြစ် ပြောင်းလဲပေးပါတယ်။ အဲဒီမှာ new dataset ရဲ့ elements တွေကို source examples တွေကြားမှာ alternating လုပ်ခြင်းဖြင့် ရရှိပါတယ်။ ဒီ function က large datasets တွေကို ပေါင်းစပ်ဖို့ ကြိုးစားတဲ့အခါ အထူးအသုံးဝင်ပါတယ်၊ ဒါကြောင့် ဥပမာအနေနဲ့ FreeLaw subset of the Pile ကို stream လုပ်ကြည့်ရအောင်။ ဒါက US courts တွေက ဥပဒေရေးရာအမြင် ၅၁ GB dataset တစ်ခုပါ။

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

ဒီ dataset က laptops အများစုရဲ့ RAM ကို ဒုက္ခပေးလောက်အောင် ကြီးမားပေမယ့်၊ ကျွန်တော်တို့ ဒါကို ချွေးတစ်စက်မှ မကျဘဲ load လုပ်ပြီး ဝင်ရောက်ကြည့်ရှုနိုင်ခဲ့ပါတယ်။ အခု FreeLaw နဲ့ PubMed Abstracts datasets တွေက examples တွေကို `interleave_datasets()` function နဲ့ ပေါင်းစပ်ကြည့်ရအောင်။

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

ဒီနေရာမှာ ကျွန်တော်တို့ Python ရဲ့ `itertools` module က `islice()` function ကို အသုံးပြုပြီး combined dataset ကနေ ပထမဆုံး ဥပမာ နှစ်ခုကို ရွေးခဲ့ပါတယ်၊ ပြီးတော့ ဒါတွေဟာ source datasets နှစ်ခုလုံးရဲ့ ပထမဆုံး ဥပမာတွေနဲ့ ကိုက်ညီတာကို တွေ့ရပါတယ်။

နောက်ဆုံးအနေနဲ့၊ သင် Pile ကို ၎င်းရဲ့ 825 GB အပြည့်အစုံ stream လုပ်ချင်တယ်ဆိုရင်၊ ပြင်ဆင်ထားတဲ့ files တွေအားလုံးကို အောက်ပါအတိုင်း ရယူနိုင်ပါတယ်။

```py
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web...'}
```

> [!TIP]
> ✏️ **စမ်းသပ်ကြည့်ပါ။** [`mc4`](https://huggingface.co/datasets/mc4) ဒါမှမဟုတ် [`oscar`](https://huggingface.co/datasets/oscar) လိုမျိုး ကြီးမားတဲ့ Common Crawl corpora တွေထဲက တစ်ခုကို အသုံးပြုပြီး သင်ရွေးချယ်ထားတဲ့ နိုင်ငံတစ်ခုမှာ ဘာသာစကားတွေ ပြောဆိုတဲ့ အချိုးအစားကို ကိုယ်စားပြုတဲ့ streaming multilingual dataset တစ်ခု ဖန်တီးပါ။ ဥပမာအားဖြင့်၊ Switzerland မှာ ဘာသာစကား လေးမျိုး (German, French, Italian, Romansh) ရှိတာကြောင့်၊ Oscar subsets တွေကို ၎င်းတို့ရဲ့ ပြောဆိုတဲ့အချိုးအစားအတိုင်း sampling လုပ်ပြီး Swiss corpus တစ်ခု ဖန်တီးကြည့်နိုင်ပါတယ်။

အခုဆိုရင် သင်ဟာ datasets အမျိုးအစားပေါင်းစုံကို load လုပ်ပြီး process လုပ်နိုင်ဖို့ လိုအပ်တဲ့ tools တွေ အားလုံးကို ပိုင်ဆိုင်နေပါပြီ။ ဒါပေမယ့် သင် အရမ်းကံကောင်းနေမှသာ၊ သင့် NLP ခရီးစဉ်မှာ လက်ရှိပြဿနာကို ဖြေရှင်းဖို့ dataset တစ်ခုကို တကယ်တမ်း ဖန်တီးရမယ့်အချိန် ရောက်လာပါလိမ့်မယ်။ ဒါကတော့ နောက်အပိုင်းရဲ့ ခေါင်းစဉ်ပါပဲ!

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Big Data**: ပုံမှန်ဒေတာစီမံခန့်ခွဲမှုကိရိယာများဖြင့် ကိုင်တွယ်ရန် ခက်ခဲသော အလွန်ကြီးမားသော ဒေတာအစုအဝေးများ။
*   **🤗 Datasets**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် ဒေတာအစုအဝေး (datasets) တွေကို လွယ်လွယ်ကူကူ ဝင်ရောက်ရယူ၊ စီမံခန့်ခွဲပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **Pretrain**: Model တစ်ခုကို အကြီးစားဒေတာများဖြင့် အစောပိုင်းကတည်းက လေ့ကျင့်ထားခြင်း။
*   **Transformer**: Natural Language Processing (NLP) မှာ အောင်မြင်မှုများစွာရရှိခဲ့တဲ့ deep learning architecture တစ်မျိုးပါ။
*   **BERT (Bidirectional Encoder Representations from Transformers)**: Google က ဖန်တီးခဲ့သော Transformer-based language model တစ်ခု။
*   **GPT-2 (Generative Pre-trained Transformer 2)**: OpenAI က ဖန်တီးခဲ့သော Transformer-based language model တစ်ခု။
*   **WebText Corpus**: GPT-2 ကို pretrain လုပ်ရာတွင် အသုံးပြုခဲ့သော ကြီးမားသော စာသားအစုအဝေး။
*   **RAM (Random Access Memory)**: ကွန်ပျူတာ၏ ယာယီမှတ်ဉာဏ်သိုလှောင်ရာနေရာ။
*   **Memory Management**: ကွန်ပျူတာ၏ memory အသုံးပြုမှုကို ထိန်းချုပ်ခြင်းနှင့် စီမံခန့်ခွဲခြင်း။
*   **Memory-mapped Files**: ဖိုင်တစ်ခု၏ အကြောင်းအရာများကို ကွန်ပျူတာ၏ virtual memory နေရာသို့ တိုက်ရိုက်ချိတ်ဆက်ပေးသည့် နည်းလမ်း။ ၎င်းသည် ကြီးမားသောဖိုင်များကို disk ပေါ်ကနေ လိုအပ်သလောက်သာ memory ထဲသို့ load လုပ်စေပြီး memory အသုံးပြုမှုကို လျှော့ချသည်။
*   **Streaming**: ဒေတာများကို တစ်စိတ်တစ်ပိုင်းစီ download လုပ်ပြီး လုပ်ဆောင်နိုင်ခြင်း၊ dataset တစ်ခုလုံးကို memory ထဲသို့ load လုပ်ရန် မလိုဘဲ။
*   **Corpus**: စာသားအမြောက်အမြား စုစည်းထားခြင်း။
*   **Hard Drive Limits**: ကွန်ပျူတာ၏ hard drive (disk storage) ၏ သိုလှောင်နိုင်စွမ်း ကန့်သတ်ချက်များ။
*   **The Pile**: EleutherAI မှ ဖန်တီးထားသော ကြီးမားသည့် English text corpus တစ်ခု။
*   **EleutherAI**: Large-scale open-source AI models များ ဖန်တီးရန် အဓိကထားသော collective တစ်ခု။
*   **PubMed Abstracts Dataset**: PubMed မှ biomedical publications ၏ abstracts များပါဝင်သော dataset။
*   **JSON Lines Format**: JSON objects များကို line တစ်ကြောင်းစီတွင် တစ်ခုစီ ထားရှိသော JSON format ၏ ပုံစံတစ်မျိုး။
*   **`zstandard` Library**: Zstandard compression algorithm ကို အသုံးပြု၍ ဖိုင်များကို compress (ဖိသိပ်) သို့မဟုတ် decompress (ဖိသိပ်မှုဖြေလျှော့) ရန်အတွက် Python library။
*   **`pip install zstandard`**: `zstandard` library ကို Python package manager `pip` ဖြင့် install လုပ်ခြင်း။
*   **`load_dataset()` Function**: Hugging Face Datasets library မှ dataset များကို download လုပ်ပြီး cache လုပ်ရန် အသုံးပြုသော function။
*   **`data_files` Argument**: `load_dataset()` function တွင် dataset files (local သို့မဟုတ် remote) ၏ path (သို့မဟုတ် URL) ကို သတ်မှတ်ရန် အသုံးပြုသော argument။
*   **`split` Argument**: `load_dataset()` function တွင် dataset ၏ မည်သည့် split (ဥပမာ- "train", "validation", "test") ကို load လုပ်ရမည်ကို သတ်မှတ်ရန် အသုံးပြုသော argument။
*   **`Dataset` Object**: 🤗 Datasets library မှ data များကို ကိုယ်စားပြုသော object။
*   **`DownloadConfig(delete_extracted=True)`**: `DownloadConfig` class ၏ object တစ်ခုဖြစ်ပြီး download လုပ်ပြီးသား extracted files များကို delete လုပ်ရန် configure လုပ်ထားသည်။
*   **`download_config` Argument**: `load_dataset()` function တွင် download configuration ကို သတ်မှတ်ရန် အသုံးပြုသော argument။
*   **`psutil` Library**: Python တွင် process နှင့် system utility များကို ဝင်ရောက်ကြည့်ရှုရန် အသုံးပြုသော library။
*   **`Process` Class**: `psutil` library မှ လက်ရှိ Python process ကို ကိုယ်စားပြုသော class။
*   **`memory_info().rss`**: Process တစ်ခုက RAM မှာ နေရာယူထားတဲ့ memory ပမာဏ (resident set size) ကို bytes ဖြင့် ပြန်ပေးသော method။
*   **`dataset_size` Attribute**: `Dataset` object ၏ disk ပေါ်ရှိ အရွယ်အစားကို bytes ဖြင့် ပြန်ပေးသော attribute။
*   **Wes McKinney**: Pandas library ကို ဖန်တီးသူ။
*   **Rule of Thumb**: အတွေ့အကြုံအရ အသုံးများသော သို့မဟုတ် လက်တွေ့ကျသော လမ်းညွှန်ချက်။
*   **Apache Arrow**: In-memory data format တစ်ခုဖြစ်ပြီး data analytics applications တွေကြား ဒေတာဖလှယ်မှုကို မြန်ဆန်စေပြီး ထိရောက်စေသည်။
*   **`pyarrow` Library**: Python တွင် Apache Arrow format ကို အသုံးပြုရန်အတွက် library။
*   **`timeit` Module**: Python code snippets များ၏ execution time ကို တိုင်းတာရန်အတွက် built-in module။
*   **`code_snippet`**: တိုင်းတာလိုသော Python code အပိုင်း။
*   **`%%timeit` Magic Function**: Jupyter notebooks တွင် cells များ၏ execution time ကို တိုင်းတာရန်အတွက် magic function။
*   **`streaming=True` Argument**: `load_dataset()` function တွင် dataset streaming ကို enable လုပ်ရန် အသုံးပြုသော argument။
*   **`IterableDataset`**: 🤗 Datasets library မှ streaming mode တွင် data များကို ကိုယ်စားပြုသော object။ ၎င်း၏ elements များကို iterate လုပ်ခြင်းဖြင့်သာ ဝင်ရောက်ကြည့်ရှုနိုင်သည်။
*   **`next(iter(dataset))`**: `IterableDataset` ၏ ပထမဆုံး element ကို ယူရန်အတွက် Python idiom။
*   **`IterableDataset.map()`**: `IterableDataset` ၏ elements များကို process လုပ်ရန်အတွက် method။
*   **`AutoTokenizer`**: Hugging Face Transformers library မှာ ပါဝင်တဲ့ class တစ်ခုဖြစ်ပြီး မော်ဒယ်အမည်ကို အသုံးပြုပြီး သက်ဆိုင်ရာ tokenizer ကို အလိုအလျောက် load လုပ်ပေးသည်။
*   **`distilbert-base-uncased`**: DistilBERT model ၏ base uncased version အတွက် checkpoint identifier။
*   **`input_ids`**: Tokenizer မှ ထုတ်ပေးသော tokens တစ်ခုစီ၏ ထူးခြားသော ဂဏန်းဆိုင်ရာ ID များ။
*   **`attention_mask`**: မော်ဒယ်ကို အာရုံစိုက်သင့်သည့် tokens များနှင့် လျစ်လျူရှုသင့်သည့် (padding) tokens များကို ခွဲခြားပေးသည့် binary mask။
*   **`batched=True`**: `map()` method မှာ အသုံးပြုသော argument တစ်ခုဖြစ်ပြီး function ကို dataset ရဲ့ element အများအပြားပေါ်မှာ တစ်ပြိုင်နက်တည်း အသုံးပြုစေသည်။
*   **`batch_size`**: training လုပ်ငန်းစဉ်တစ်ခုစီတွင် model သို့ ပေးပို့သော input samples အရေအတွက်။
*   **`IterableDataset.shuffle()`**: streamed dataset ၏ elements များကို shuffle လုပ်ရန်အတွက် method။ ၎င်းသည် သတ်မှတ်ထားသော `buffer_size` အတွင်းရှိ elements များကိုသာ shuffle လုပ်သည်။
*   **`buffer_size`**: `IterableDataset.shuffle()` တွင် elements များကို shuffle လုပ်ရန် အသုံးပြုသော memory buffer ၏ အရွယ်အစား။
*   **`seed`**: Random number generator ၏ အစမှတ်ကို သတ်မှတ်ခြင်း။
*   **`IterableDataset.take()`**: streamed dataset မှ ပထမဆုံး `n` elements များကို ယူရန်အတွက် method။
*   **`IterableDataset.skip()`**: streamed dataset မှ ပထမဆုံး `n` elements များကို ကျော်သွားရန်အတွက် method။
*   **`Dataset.select()`**: regular `Dataset` မှ သီးခြား index ဖြင့် elements များကို ရွေးချယ်ရန်အတွက် method။
*   **Training Dataset**: Model ကို လေ့ကျင့်ရန်အတွက် အသုံးပြုသော dataset အပိုင်း။
*   **Validation Dataset**: Training လုပ်နေစဉ် model ၏ စွမ်းဆောင်ရည်ကို အကဲဖြတ်ရန် အသုံးပြုသော dataset အပိုင်း။
*   **`interleave_datasets()` Function**: Hugging Face Datasets library မှ function တစ်ခုဖြစ်ပြီး `IterableDataset` objects စာရင်းကို single `IterableDataset` တစ်ခုအဖြစ် ပြောင်းလဲပေးသည်။
*   **`itertools` Module**: Python ၏ built-in module တစ်ခုဖြစ်ပြီး iterator များကို ဖန်တီးရန်နှင့် ကိုင်တွယ်ရန်အတွက် အသုံးဝင်သော functions များကို ပံ့ပိုးပေးသည်။
*   **`islice()` Function**: `itertools` module မှ iterator တစ်ခု၏ element များကို slice လုပ်ရန်အတွက် function။
*   **FreeLaw Subset**: The Pile dataset ၏ အစိတ်အပိုင်းတစ်ခုဖြစ်ပြီး US courts မှ legal opinions များ ပါဝင်သည်။
*   **Common Crawl Corpora**: အင်တာနက်မှ web data များကို စုစည်းထားသော ကြီးမားသည့် dataset များ (ဥပမာ- `mc4`, `oscar`)။
*   **Multilingual Dataset**: ဘာသာစကားမျိုးစုံ ပါဝင်သော dataset။
*   **Sampling**: ကြီးမားသော dataset တစ်ခုမှ သေးငယ်သော အစိတ်အပိုင်းတစ်ခုကို ရွေးချယ်ခြင်း။
*   **NLP Journey**: Natural Language Processing (NLP) နယ်ပယ်တွင် လေ့လာသင်ယူခြင်းနှင့် အလုပ်လုပ်ခြင်း ခရီးစဉ်။
*   **Problem at Hand**: လက်ရှိ ဖြေရှင်းရန် လိုအပ်သော ပြဿနာ။
