# ဘက်လိုက်မှုနှင့် ကန့်သတ်ချက်များ[[bias-and-limitations]]

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
]} />

အကယ်၍ သင်က pre-trained model တစ်ခုကို ဒါမှမဟုတ် fine-tuned version တစ်ခုကို ထုတ်လုပ်မှု (production) မှာ အသုံးပြုဖို့ ရည်ရွယ်တယ်ဆိုရင်၊ ဒီမော်ဒယ်တွေဟာ အစွမ်းထက်တဲ့ ကိရိယာတွေဖြစ်ပေမယ့် ကန့်သတ်ချက်တွေနဲ့ လာတယ်ဆိုတာကို သတိပြုသင့်ပါတယ်။ အကြီးဆုံး ကန့်သတ်ချက်ကတော့ ဒေတာပမာဏများစွာပေါ်မှာ pre-training လုပ်နိုင်ဖို့အတွက် သုတေသီတွေဟာ အင်တာနက်ပေါ်က တွေ့သမျှ အကြောင်းအရာအားလုံးကို ရယူကြပြီး၊ အကောင်းဆုံးအရာတွေရော အဆိုးဆုံးအရာတွေရော ပါဝင်လာတတ်ပါတယ်။

ဥပမာအနေနဲ့ ရှင်းပြရရင် BERT မော်ဒယ်ကို အသုံးပြုထားတဲ့ `fill-mask` pipeline ဥပမာကို ပြန်သွားကြည့်ရအောင်။

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

ဒီစာကြောင်းနှစ်ကြောင်းမှာ ပျောက်ဆုံးနေတဲ့ စကားလုံးကို ဖြည့်ဖို့ မေးတဲ့အခါ၊ မော်ဒယ်က လိင်ကွဲပြားမှုမရှိတဲ့ အဖြေတစ်ခု (waiter/waitress) ကိုသာ ပေးပါတယ်။ ကျန်တဲ့ အလုပ်အကိုင်တွေကတော့ သီးခြားလိင်နဲ့ ပုံမှန်အားဖြင့် ဆက်စပ်နေတဲ့ အလုပ်အကိုင်တွေ ဖြစ်ပါတယ်—ဟုတ်ပါတယ်၊ "prostitute" က "woman" နဲ့ "work" တို့နဲ့ မော်ဒယ်က ဆက်စပ်တဲ့ ဖြစ်နိုင်ခြေ ထိပ်ဆုံး ၅ ခုထဲမှာ ပါဝင်ခဲ့ပါတယ်။ BERT ဟာ အင်တာနက်တစ်လျှောက်ကနေ ဒေတာတွေကို ရယူပြီး တည်ဆောက်ထားတဲ့ ရှားပါး Transformer မော်ဒယ်တွေထဲက တစ်ခု မဟုတ်ဘဲ၊ ကြည့်ရတာ ကြားနေတဲ့ ဒေတာ (English Wikipedia နဲ့ BookCorpus datasets တွေနဲ့ လေ့ကျင့်ထားပါတယ်) ကို အသုံးပြုထားတာ ဖြစ်ပေမယ့်လည်း ဒီလိုဖြစ်တတ်ပါတယ်။

ဒီကိရိယာတွေကို အသုံးပြုတဲ့အခါ သင်သုံးနေတဲ့ မူရင်းမော်ဒယ်ဟာ Sexist၊ Racist ဒါမှမဟုတ် homophobic အကြောင်းအရာတွေကို အလွန်လွယ်ကူစွာ ထုတ်လုပ်နိုင်တယ်ဆိုတာကို သတိရနေဖို့ လိုအပ်ပါတယ်။ သင်ရဲ့ ဒေတာပေါ်မှာ မော်ဒယ်ကို fine-tuning လုပ်တာဟာ ဒီအတွင်းပိုင်း ဘက်လိုက်မှုကို ပျောက်ကွယ်သွားစေမှာ မဟုတ်ပါဘူး။

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Bias**: ဒေတာအစုအဝေး (dataset) သို့မဟုတ် မော်ဒယ်၏ လေ့ကျင့်မှုပုံစံကြောင့် ဖြစ်ပေါ်လာသော ဘက်လိုက်မှုများ။ ဥပမာ - လူမျိုး၊ လိင်၊ ဘာသာ စသည်တို့ကို ခွဲခြားဆက်ဆံခြင်း။
*   **Limitations**: AI မော်ဒယ်များ၏ လုပ်ဆောင်နိုင်စွမ်းနှင့် ပတ်သက်သော ကန့်သတ်ချက်များ၊ အားနည်းချက်များ။
*   **Pretrained Model**: ဒေတာအမြောက်အမြားပေါ်တွင် ကြိုတင်လေ့ကျင့်ထားပြီးသား Artificial Intelligence (AI) မော်ဒယ်တစ်ခု။ ၎င်းတို့ကို အခြားလုပ်ငန်းများအတွက် အခြေခံအဖြစ် ပြန်လည်အသုံးပြုနိုင်သည်။
*   **Fine-tuned Version**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနဲ့ ထပ်မံလေ့ကျင့်ပေးထားသော မော်ဒယ်၏ ပုံစံ။
*   **Production**: ဆော့ဖ်ဝဲလ် သို့မဟုတ် မော်ဒယ်တစ်ခုကို အမှန်တကယ် အသုံးပြုနေသော လက်တွေ့ပတ်ဝန်းကျင် သို့မဟုတ် စနစ်။
*   **Transformer Models**: Natural Language Processing (NLP) မှာ အောင်မြင်မှုများစွာရရှိခဲ့တဲ့ deep learning architecture တစ်မျိုးပါ။ ၎င်းတို့ဟာ စာသားတွေထဲက စကားလုံးတွေရဲ့ ဆက်နွယ်မှုတွေကို "attention mechanism" သုံးပြီး နားလည်အောင် သင်ကြားပေးပါတယ်။
*   **Scrape**: အင်တာနက်ပေါ်မှ ဒေတာများကို အလိုအလျောက် စုဆောင်းခြင်း။
*   **`fill-mask` pipeline**: Hugging Face Transformers library မှာ ပါဝင်တဲ့ function တစ်ခုဖြစ်ပြီး input text ထဲက `[MASK]` နေရာမှာ ပျောက်ဆုံးနေတဲ့ စကားလုံးကို ခန့်မှန်းပြီး ဖြည့်စွက်ပေးတဲ့ လုပ်ငန်းဆောင်တာ။
*   **BERT (Bidirectional Encoder Representations from Transformers)**: Google မှ တီထွင်ထားသော Transformer-based NLP မော်ဒယ်တစ်ခု။
*   **`bert-base-uncased`**: BERT မော်ဒယ်၏ အခြေခံဗားရှင်း (base version) ဖြစ်ပြီး စာလုံးအကြီးအသေး ခွဲခြားခြင်းမရှိ (uncased) ဘဲ လေ့ကျင့်ထားသည်။
*   **`token_str`**: ထုတ်လုပ်လိုက်သော token ကို ကိုယ်စားပြုသော စာသား string။
*   **English Wikipedia**: အင်္ဂလိပ်ဘာသာစကားဖြင့် ရေးသားထားသော Wikipedia စွယ်စုံကျမ်း၏ အချက်အလက်များ။
*   **BookCorpus**: စာအုပ်များစွာမှ စုဆောင်းထားသော စာသားဒေတာအစုအဝေးတစ်ခု။
*   **Dataset**: AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် အသုံးပြုတဲ့ ဒေတာအစုအဝေးတစ်ခုပါ။
*   **Sexist**: လိင်အပေါ်အခြေခံပြီး ခွဲခြားဆက်ဆံခြင်း သို့မဟုတ် ဘက်လိုက်ခြင်း။
*   **Racist**: လူမျိုးအပေါ်အခြေခံပြီး ခွဲခြားဆက်ဆံခြင်း သို့မဟုတ် ဘက်လိုက်ခြင်း။
*   **Homophobic**: လိင်တူချစ်သူများကို မနှစ်သက်ခြင်း သို့မဟုတ် ခွဲခြားဆက်ဆံခြင်း။