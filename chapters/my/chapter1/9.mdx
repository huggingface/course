# အနှစ်ချုပ်(Summary)

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

ဤအခန်းတွင် 🤗 Transformers မှ high-level `pipeline()` function ကို အသုံးပြု၍ မတူညီသော NLP လုပ်ငန်းများကို မည်သို့ချဉ်းကပ်ရမည်ကို သင်တွေ့မြင်ခဲ့ရသည်။ Hub တွင် မော်ဒယ်များကို မည်သို့ရှာဖွေရမည်နှင့် အသုံးပြုရမည်၊ သင်၏ browser တွင် မော်ဒယ်များကို တိုက်ရိုက်စမ်းသပ်ရန် Inference API ကို မည်သို့အသုံးပြုရမည်ကိုလည်း သင်တွေ့မြင်ခဲ့ရသည်။

Transformer မော်ဒယ်များ မည်သို့အလုပ်လုပ်ပုံကို high level တွင် ကျွန်ုပ်တို့ ဆွေးနွေးခဲ့ပြီး transfer learning နှင့် fine-tuning ၏ အရေးပါမှုအကြောင်း ပြောဆိုခဲ့သည်။ အဓိကအချက်မှာ သင်ဖြေရှင်းလိုသော လုပ်ငန်းအမျိုးအစားပေါ် မူတည်၍ architecture(ပုံစံ ဖွဲစည်းပုံ) သို့မဟုတ် encoder သို့မဟုတ် decoder ကိုသာ အသုံးပြုနိုင်သည်။ အောက်ပါဇယားတွင် ၎င်းကို အနှစ်ချုပ်ဖော်ပြထားသည်။

| Model           | Examples                                   | Tasks                                                                            |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, named entity recognition, extractive question answering |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Text generation                                                                  |
| Encoder-decoder | BART, T5, Marian, mBART                    | Summarization, translation, generative question answering                        |
