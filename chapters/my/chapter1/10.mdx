# အနှစ်ချုပ်[[summary]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

ဒီအခန်းမှာ Transformer မော်ဒယ်တွေ၊ Large Language Models (LLMs) တွေရဲ့ အခြေခံသဘောတရားတွေနဲ့ ၎င်းတို့က Artificial Intelligence (AI) နယ်ပယ်အပြင် အခြားနယ်ပယ်တွေကိုပါ ဘယ်လို တော်လှန်ပြောင်းလဲနေတယ်ဆိုတာကို သင်လေ့လာခဲ့ပြီးပါပြီ။

## အဓိက သဘောတရားများ[[key-concepts-covered]]

### Natural Language Processing (NLP) နှင့် LLMs များ

Natural Language Processing (NLP) ဆိုတာ ဘာလဲ၊ Large Language Models (LLMs) တွေက ဒီနယ်ပယ်ကို ဘယ်လို ပြောင်းလဲပစ်ခဲ့လဲဆိုတာကို ကျွန်တော်တို့ လေ့လာခဲ့ပါတယ်။ သင်လေ့လာခဲ့တဲ့ အချက်တွေကတော့-
- NLP ဟာ classification ကနေ generation အထိ လုပ်ငန်းတာဝန်အမျိုးမျိုးကို လွှမ်းခြုံထားပါတယ်။
- LLMs တွေဟာ ဒေတာအမြောက်အမြားနဲ့ လေ့ကျင့်ထားတဲ့ အစွမ်းထက်တဲ့ မော်ဒယ်တွေ ဖြစ်ပါတယ်။
- ဒီမော်ဒယ်တွေက architecture တစ်ခုတည်းနဲ့ လုပ်ငန်းတာဝန်မျိုးစုံကို လုပ်ဆောင်နိုင်ပါတယ်။
- ၎င်းတို့ရဲ့ စွမ်းရည်တွေရှိနေပေမယ့်လည်း LLMs တွေမှာ hallucinations နဲ့ bias တွေလို ကန့်သတ်ချက်တွေ ရှိပါတယ်။

### Transformer ရဲ့ စွမ်းရည်များ[[transformer-capabilities]]

🤗 Transformers library ထဲက `pipeline()` function က pre-trained model တွေကို လုပ်ငန်းတာဝန်အမျိုးမျိုးအတွက် ဘယ်လိုလွယ်ကူစွာ အသုံးပြုနိုင်လဲဆိုတာကို သင်တွေ့ခဲ့ရပါတယ်-
- Text classification, token classification, နဲ့ question answering
- Text generation နဲ့ summarization
- Translation နဲ့ အခြား sequence-to-sequence လုပ်ငန်းတာဝန်များ
- Speech recognition နဲ့ image classification

### Transformer architecture[[transformer-architecture]]

Transformer မော်ဒယ်တွေဟာ အမြင့်ဆုံးအဆင့်မှာ ဘယ်လိုအလုပ်လုပ်တယ်ဆိုတာကို ကျွန်တော်တို့ ဆွေးနွေးခဲ့ပါတယ်-
- Attention mechanism ရဲ့ အရေးပါမှု
- Transfer learning က မော်ဒယ်တွေကို သီးခြားလုပ်ငန်းတာဝန်များအတွက် ဘယ်လို လိုက်လျောညီထွေဖြစ်အောင် ကူညီပေးတယ်ဆိုတာ
- အဓိက architecture ပုံစံသုံးမျိုး- encoder-only, decoder-only, နဲ့ encoder-decoder

### မော်ဒယ် architecture များနှင့် ၎င်းတို့၏ အသုံးချမှုများ[[model-architectures-and-their-applications]]
ဒီအခန်းရဲ့ အဓိကအချက်ကတော့ မတူညီတဲ့ လုပ်ငန်းတာဝန်တွေအတွက် ဘယ် architecture ကို အသုံးပြုရမလဲဆိုတာကို နားလည်ခြင်း ဖြစ်ပါတယ်-

| မော်ဒယ်           | ဥပမာများ                                   | လုပ်ငန်းတာဝန်များ                                                                            |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder-only    | BERT, DistilBERT, ModernBERT               | စာကြောင်းခွဲခြားသတ်မှတ်ခြင်း (Sentence classification), အမည်သတ်မှတ်ခြင်း (named entity recognition), စာသားမှ အဖြေထုတ်ယူခြင်း (extractive question answering) |
| Decoder-only    | GPT, LLaMA, Gemma, SmolLM                  | စာသားထုတ်လုပ်ခြင်း (Text generation), conversational AI, ဖန်တီးမှုစာရေးခြင်း (creative writing)                             |
| Encoder-decoder | BART, T5, Marian, mBART                    | အကျဉ်းချုပ်ခြင်း (Summarization), ဘာသာပြန်ခြင်း (translation), ထုတ်လုပ်မှုမေးခွန်းဖြေခြင်း (generative question answering)                        |

### ခေတ်မီ LLM ဖွံ့ဖြိုးတိုးတက်မှုများ[[modern-llm-developments]]
နယ်ပယ်ရဲ့ မကြာသေးခင် ဖွံ့ဖြိုးတိုးတက်မှုတွေအကြောင်းကိုလည်း သင်လေ့လာခဲ့ပါတယ်-
- LLMs တွေဟာ အချိန်နဲ့အမျှ အရွယ်အစားနဲ့ စွမ်းရည် ဘယ်လိုတိုးတက်လာခဲ့လဲဆိုတာ
- Scaling laws သဘောတရားနဲ့ ၎င်းတို့က မော်ဒယ်ဖွံ့ဖြိုးတိုးတက်မှုကို ဘယ်လိုလမ်းညွှန်ပေးတယ်ဆိုတာ
- မော်ဒယ်တွေကို ပိုမိုရှည်လျားတဲ့ sequences တွေကို လုပ်ဆောင်နိုင်အောင် ကူညီပေးတဲ့ သီးခြား attention mechanism များ
- Pretraining နဲ့ instruction tuning တို့ရဲ့ နှစ်ဆင့်လေ့ကျင့်မှု ချဉ်းကပ်ပုံ

### လက်တွေ့အသုံးချမှုများ[[practical-applications]]
ဒီအခန်းတစ်လျှောက်လုံးမှာ ဒီမော်ဒယ်တွေကို လက်တွေ့ဘဝပြဿနာတွေမှာ ဘယ်လိုအသုံးချနိုင်လဲဆိုတာကို သင်တွေ့ခဲ့ရပါတယ်-
- Hugging Face Hub ကို အသုံးပြုပြီး pre-trained model တွေ ရှာဖွေအသုံးပြုခြင်း
- Inference API ကို အသုံးပြုပြီး browser ထဲမှာ မော်ဒယ်တွေကို တိုက်ရိုက်စမ်းသပ်ခြင်း
- သီးခြားလုပ်ငန်းတာဝန်များအတွက် ဘယ်မော်ဒယ်တွေက အသင့်တော်ဆုံးလဲဆိုတာကို နားလည်ခြင်း

## ရှေ့ဆက်မျှော်ကြည့်ခြင်း[[looking-ahead]]

Transformer မော်ဒယ်တွေဆိုတာ ဘာလဲ၊ ၎င်းတို့က အမြင့်ဆုံးအဆင့်မှာ ဘယ်လိုအလုပ်လုပ်တယ်ဆိုတာကို သင်သေချာနားလည်သွားပြီဆိုတော့၊ ၎င်းတို့ကို ထိထိရောက်ရောက် ဘယ်လိုအသုံးပြုရမလဲဆိုတာကို နက်နက်နဲနဲ လေ့လာဖို့ သင်အဆင်သင့်ဖြစ်ပါပြီ။ နောက်အခန်းတွေမှာ သင်လေ့လာရမယ့်အရာတွေကတော့-

- Transformers library ကို အသုံးပြုပြီး မော်ဒယ်တွေ တင်သွင်းခြင်းနဲ့ fine-tune လုပ်ခြင်း
- မော်ဒယ် input အတွက် မတူညီတဲ့ ဒေတာအမျိုးအစားတွေကို လုပ်ဆောင်ခြင်း
- Pre-trained model တွေကို သင်ရဲ့ သီးခြားလုပ်ငန်းတာဝန်များအတွက် လိုက်လျောညီထွေဖြစ်အောင် ပြုလုပ်ခြင်း
- လက်တွေ့အသုံးချမှုများအတွက် မော်ဒယ်များ တပ်ဆင်အသုံးပြုခြင်း (deploy)

ဒီအခန်းမှာ သင်တည်ဆောက်ခဲ့တဲ့ အခြေခံအုတ်မြစ်က လာမယ့်အပိုင်းတွေမှာ ပိုမိုအဆင့်မြင့်တဲ့ အကြောင်းအရာတွေနဲ့ နည်းစနစ်တွေကို သင်လေ့လာတဲ့အခါမှာ အလွန်အသုံးဝင်ပါလိမ့်မယ်။

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Transformer Models**: Natural Language Processing (NLP) မှာ အောင်မြင်မှုများစွာရရှိခဲ့တဲ့ deep learning architecture တစ်မျိုးပါ။ ၎င်းတို့ဟာ စာသားတွေထဲက စကားလုံးတွေရဲ့ ဆက်နွယ်မှုတွေကို "attention mechanism" သုံးပြီး နားလည်အောင် သင်ကြားပေးပါတယ်။
*   **Large Language Models (LLMs)**: လူသားဘာသာစကားကို နားလည်ပြီး ထုတ်လုပ်ပေးနိုင်တဲ့ အလွန်ကြီးမားတဲ့ Artificial Intelligence (AI) မော်ဒယ်တွေ ဖြစ်ပါတယ်။ ၎င်းတို့ဟာ ဒေတာအမြောက်အမြားနဲ့ သင်ကြားလေ့ကျင့်ထားပြီး စာရေးတာ၊ မေးခွန်းဖြေတာ စတဲ့ ဘာသာစကားဆိုင်ရာ လုပ်ငန်းမျိုးစုံကို လုပ်ဆောင်နိုင်ပါတယ်။
*   **Natural Language Processing (NLP)**: ကွန်ပျူတာတွေ လူသားဘာသာစကားကို နားလည်၊ အဓိပ္ပာယ်ဖော်ပြီး၊ ဖန်တီးနိုင်အောင် လုပ်ဆောင်ပေးတဲ့ Artificial Intelligence (AI) ရဲ့ နယ်ပယ်ခွဲတစ်ခု ဖြစ်ပါတယ်။ ဥပမာအားဖြင့် စာသားခွဲခြမ်းစိတ်ဖြာခြင်း၊ ဘာသာပြန်ခြင်း စသည်တို့ ပါဝင်ပါတယ်။
*   **Artificial Intelligence (AI)**: လူသားတွေရဲ့ ဉာဏ်ရည်ဉာဏ်သွေးလိုမျိုး တွေးခေါ်နိုင်စွမ်း၊ သင်ယူနိုင်စွမ်းနဲ့ ပြဿနာဖြေရှင်းနိုင်စွမ်းရှိတဲ့ စက်တွေကို ဖန်တီးတဲ့ သိပ္ပံနယ်ပယ်တစ်ခုပါ။
*   **Classification**: ဒေတာအချက်အလက်များကို သတ်မှတ်ထားသော အမျိုးအစားများ သို့မဟုတ် အတန်းများထဲသို့ ခွဲခြားသတ်မှတ်ခြင်း။
*   **Generation**: AI မော်ဒယ်များကို အသုံးပြု၍ အချက်အလက်အသစ်များ (ဥပမာ - စာသား၊ ပုံများ) ဖန်တီးခြင်း။
*   **Hallucinations**: Artificial Intelligence (AI) မော်ဒယ်များမှ မှန်ကန်မှုမရှိသော သို့မဟုတ် အဓိပ္ပာယ်မရှိသော အချက်အလက်များကို ယုံကြည်မှုရှိရှိ ထုတ်လုပ်ပေးခြင်း။
*   **Bias**: ဒေတာအစုအဝေး (dataset) သို့မဟုတ် မော်ဒယ်၏ လေ့ကျင့်မှုပုံစံကြောင့် ဖြစ်ပေါ်လာသော ဘက်လိုက်မှုများ။
*   **`pipeline()` function**: Hugging Face Transformers library မှာ ပါဝင်တဲ့ လုပ်ဆောင်ချက်တစ်ခုဖြစ်ပြီး မော်ဒယ်တွေကို သီးခြားလုပ်ငန်းတာဝန်များ (ဥပမာ- စာသားခွဲခြားသတ်မှတ်ခြင်း၊ စာသားထုတ်လုပ်ခြင်း) အတွက် အသုံးပြုရလွယ်ကူအောင် ပြုလုပ်ပေးပါတယ်။
*   **🤗 Transformers**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး Transformer မော်ဒယ်တွေကို အသုံးပြုပြီး Natural Language Processing (NLP), computer vision, audio processing စတဲ့ နယ်ပယ်တွေမှာ အဆင့်မြင့် AI မော်ဒယ်တွေကို တည်ဆောက်ပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **Pre-trained Models**: ဒေတာအမြောက်အမြားပေါ်တွင် ကြိုတင်လေ့ကျင့်ထားပြီးသား Artificial Intelligence (AI) မော်ဒယ်တစ်ခု။ ၎င်းတို့ကို အခြားလုပ်ငန်းများအတွက် အခြေခံအဖြစ် ပြန်လည်အသုံးပြုနိုင်သည်။
*   **Text Classification**: စာသားတစ်ခုကို သတ်မှတ်ထားသော အမျိုးအစားများ သို့မဟုတ် အတန်းများထဲသို့ ခွဲခြားသတ်မှတ်ခြင်း။
*   **Token Classification**: စာသားတစ်ခုရှိ token (စကားလုံး သို့မဟုတ် စာလုံးတစ်ပိုင်း) တစ်ခုစီကို အမျိုးအစားခွဲခြားသတ်မှတ်ခြင်း။
*   **Question Answering**: မေးခွန်းတစ်ခုကို ပေးထားသော စာသားအကြောင်းအရာမှ အဖြေထုတ်ပေးခြင်း။
*   **Text Generation**: AI မော်ဒယ်များကို အသုံးပြု၍ လူသားကဲ့သို့သော စာသားအသစ်များ ဖန်တီးခြင်း။
*   **Summarization**: စာသားရှည်ကြီးတစ်ခုကို အဓိကအချက်အလက်များ မပျောက်ပျက်စေဘဲ အကျဉ်းချုံးဖော်ပြခြင်း။
*   **Translation**: ဘာသာစကားတစ်ခုမှ အခြားဘာသာစကားတစ်ခုသို့ စာသားဘာသာပြန်ခြင်း။
*   **Sequence-to-sequence Tasks**: input sequence တစ်ခုမှ output sequence တစ်ခုကို ထုတ်လုပ်ပေးသော လုပ်ငန်းတာဝန်များ။
*   **Speech Recognition**: ပြောဆိုသော ဘာသာစကားကို ကွန်ပျူတာက စာသားအဖြစ် ပြောင်းလဲနားလည်နိုင်သည့် နည်းပညာ။
*   **Image Classification**: ရုပ်ပုံတစ်ခုကို သတ်မှတ်ထားသော အမျိုးအစားများထဲသို့ ခွဲခြားသတ်မှတ်ခြင်း။
*   **Attention Mechanism**: Transformer မော်ဒယ်များတွင် အသုံးပြုသော နည်းစနစ်တစ်ခုဖြစ်ပြီး input sequence အတွင်းရှိ အရေးပါသော အစိတ်အပိုင်းများကို အာရုံစိုက်ပြီး ဆက်နွယ်မှုများကို သင်ယူစေသည်။
*   **Transfer Learning**: ကြိုတင်လေ့ကျင့်ထားပြီးသား မော်ဒယ်တစ်ခုမှ သင်ယူထားသော အသိပညာများကို အခြားဆက်စပ်လုပ်ငန်းတစ်ခုအတွက် အသုံးပြုခြင်း။
*   **Encoder-only**: Transformer architecture အမျိုးအစားတစ်ခုဖြစ်ပြီး input ကို နားလည်ပြီး ကိုယ်စားပြုတဲ့ အချက်အလက်ကို ထုတ်ပေးတဲ့ encoder အစိတ်အပိုင်းတစ်ခုတည်း ပါဝင်ပါတယ်။
*   **Decoder-only**: Transformer architecture အမျိုးအစားတစ်ခုဖြစ်ပြီး စာသားထုတ်လုပ်ခြင်းအတွက် အသုံးပြုတဲ့ decoder အစိတ်အပိုင်းတစ်ခုတည်း ပါဝင်ပါတယ်။
*   **Encoder-decoder**: Transformer architecture အမျိုးအစားတစ်ခုဖြစ်ပြီး input sequence မှ output sequence သို့ ပြောင်းလဲခြင်း လုပ်ငန်းများအတွက် encoder နှင့် decoder နှစ်ခုစလုံး ပါဝင်ပါတယ်။
*   **BERT**: Google မှ တီထွင်ထားသော Encoder-only Transformer မော်ဒယ်ဥပမာ။
*   **DistilBERT**: BERT ၏ ပိုမိုသေးငယ်ပြီး မြန်ဆန်သော ဗားရှင်း။
*   **ModernBERT**: BERT မော်ဒယ်နှင့် ဆင်တူသော နောက်ဆုံးပေါ် ဗားရှင်းတစ်ခု (ဤနေရာတွင် ဥပမာအဖြစ် ရည်ညွှန်းခြင်း)။
*   **GPT (Generative Pre-trained Transformer)**: OpenAI မှ တီထွင်ထားသော Decoder-only Transformer မော်ဒယ်ဥပမာ။
*   **LLaMA**: Meta မှ တီထွင်ထားသော Decoder-only Transformer မော်ဒယ်ဥပမာ။
*   **Gemma**: Google မှ တီထွင်ထားသော Decoder-only Transformer မော်ဒယ်ဥပမာ။
*   **SmolLM**: Decoder-only Transformer မော်ဒယ်ဥပမာ (ဤနေရာတွင် ဥပမာအဖြစ် ရည်ညွှန်းခြင်း)။
*   **BART**: Facebook (ယခု Meta) မှ တီထွင်ထားသော Encoder-Decoder Transformer မော်ဒယ်ဥပမာ။
*   **T5**: Google မှ တီထွင်ထားသော Encoder-Decoder Transformer မော်ဒယ်ဥပမာ။
*   **Marian**: Encoder-Decoder Transformer မော်ဒယ်ဥပမာ (အဓိကအားဖြင့် ဘာသာပြန်ခြင်းအတွက်)။
*   **mBART**: Facebook (ယခု Meta) မှ တီထွင်ထားသော Encoder-Decoder Transformer မော်ဒယ်ဥပမာ (ဘာသာစကားမျိုးစုံအတွက်)။
*   **Scaling Laws**: မော်ဒယ်အရွယ်အစား၊ ဒေတာပမာဏနှင့် ကွန်ပျူတာအရင်းအမြစ်များ တိုးလာသည်နှင့်အမျှ AI မော်ဒယ်များ၏ စွမ်းဆောင်ရည်ကို ခန့်မှန်းဖော်ပြသော ဆက်နွယ်မှုများ။
*   **Instruction Tuning**: မော်ဒယ်ကို သီးခြားညွှန်ကြားချက်များ (instructions) ကို နားလည်ပြီး လိုက်နာရန် ထပ်မံလေ့ကျင့်ပေးသော လုပ်ငန်းစဉ်။
*   **Hugging Face Hub**: AI မော်ဒယ်တွေ၊ datasets တွေနဲ့ demo တွေကို အခြားသူတွေနဲ့ မျှဝေဖို့၊ ရှာဖွေဖို့နဲ့ ပြန်လည်အသုံးပြုဖို့အတွက် အွန်လိုင်း platform တစ်ခု ဖြစ်ပါတယ်။
*   **Inference API**: Hugging Face Hub ပေါ်ရှိ မော်ဒယ်များကို web request များမှတစ်ဆင့် တိုက်ရိုက်အသုံးပြုနိုင်စေသည့် Application Programming Interface (API)။
*   **Fine-tune**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနဲ့ ထပ်မံလေ့ကျင့်ပေးခြင်းကို ဆိုလိုပါတယ်။
*   **Deploy**: Machine Learning မော်ဒယ်တစ်ခုကို အမှန်တကယ် အသုံးပြုနိုင်သော စနစ် သို့မဟုတ် environment တစ်ခုထဲသို့ ထည့်သွင်းခြင်း။