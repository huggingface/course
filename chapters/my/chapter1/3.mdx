# Transformers များ၊ ဘာတွေလုပ်ဆောင်နိုင်လဲ။

<CourseFloatingBanner chapter={1}
    classNames="absolute z-10 right-0 top-0"
    notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
        {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb"},
]} />

ဤအပိုင်းတွင် Transformer မော်ဒယ်များ ဘာတွေလုပ်ဆောင်နိုင်သည်ကို ကျွန်ုပ်တို့ လေ့လာပြီး 🤗 Transformers library မှ ကျွန်ုပ်တို့၏ ပထမဆုံး tool ဖြစ်သော `pipeline()` function ကို အသုံးပြုပါမည်။

<Tip>
👀 ညာဘက်အပေါ်ထောင့်ရှိ <em>Open in Colab</em> ခလုတ်ကို မြင်ပါသလား။ ဤအပိုင်း၏ ကုဒ်နမူနာအားလုံးပါဝင်သည့် Google Colab notebook တစ်ခုဖွင့်ရန် ၎င်းကို နှိပ်ပါ။ ဤခလုတ်သည် ကုဒ်ဥပမာများပါဝင်သည့် မည်သည့်အပိုင်းတွင်မဆို ရှိနေပါမည်။

ဥပမာများကို ဒေသတွင်းတွင် လုပ်ဆောင်လိုပါက <a href="/course/chapter0">setup</a> ကို ကြည့်ရှုရန် အကြံပြုလိုပါသည်။
</Tip>

The [🤗 Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!
## Transformers များသည် နေရာအနှံ့အပြားမှာ ရှိပါတယ်။

Transformer မော်ဒယ်များကို ယခင်အပိုင်းတွင် ဖော်ပြခဲ့သည့် NLP လုပ်ငန်းအမျိုးမျိုးကို ဖြေရှင်းရန် အသုံးပြုကြသည်။ Hugging Face နှင့် Transformer မော်ဒယ်များကို အသုံးပြုနေသော ကုမ္ပဏီများနှင့် အဖွဲ့အစည်းအချို့မှာ ၎င်းတို့၏ မော်ဒယ်များကို မျှဝေခြင်းဖြင့် အသိုင်းအဝိုင်းသို့ ပြန်လည် ပံ့ပိုးကူညီကြသည်။

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Hugging Face ကို အသုံးပြုနေသော ကုမ္ပဏီများ" width="100%">

[🤗 Transformers library](https://github.com/huggingface/transformers) သည် မျှဝေထားသော မော်ဒယ်များကို ဖန်တီးရန်နှင့် အသုံးပြုရန် လုပ်ဆောင်နိုင်စွမ်းကို ပေးစွမ်းသည်။ [Model Hub](https://huggingface.co/models) တွင် မည်သူမဆို ဒေါင်းလုဒ်လုပ်၍ အသုံးပြုနိုင်သော ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်ထောင်ပေါင်းများစွာ ပါဝင်သည်။ သင်၏ကိုယ်ပိုင် မော်ဒယ်များကို Hub စီသို့လည်း အပ်လုဒ်တင်နိုင်ပါသည်။

<Tip>
⚠️ Hugging Face Hub သည် Transformer မော်ဒယ်များအတွက်သာ ကန့်သတ်မထားပါ။ မည်သူမဆို ၎င်းတို့ လိုချင်သော မော်ဒယ် သို့မဟုတ် ဒေတာစုများကို မျှဝေနိုင်ပါသည်။ ရရှိနိုင်သော feature အားလုံးမှ အကျိုးကျေးဇူး(benefit) ရရှိရန် <a href="https://huggingface.co/join">huggingface.co အကောင့်တစ်ခု ဖန်တီးပါ</a>။
</Tip>

Transformer မော်ဒယ်များ မည်သို့အလုပ်လုပ်ပုံကို မလေ့လာမီ NLP ပြဿနာအချို့ကို ဖြေရှင်းရန် မည်ကဲ့သို့အသုံးပြုနိုင်ပုံ ဥပမာအချို့ကို ကြည့်ကြပါစို့။

## Pipeline များနှင့် အလုပ်လုပ်ခြင်း

<Youtube id="tiZFewofSLM" />

🤗 Transformers library ရှိ အခြေခံအကျဆုံး object မှာ `pipeline()` function ဖြစ်သည်။ ၎င်းသည် မော်ဒယ်တစ်ခုကို ၎င်း၏ လိုအပ်သော preprocessing နှင့် postprocessing အဆင့်များနှင့် ချိတ်ဆက်ပေးပြီး မည်သည့်စာသားကိုမဆို တိုက်ရိုက်ထည့်သွင်း၍ နားလည်နိုင်သော အဖြေတစ်ခု ရရှိစေပါသည်။

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

စာကြောင်းများစွာကိုတောင် ပေးပို့နိုင်ပါတယ်။

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

default အားဖြင့် ဤ pipeline သည် အင်္ဂလိပ်ဘာသာဖြင့် sentiment analysis အတွက် fine-tune လုပ်ထားသော ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တစ်ခုကို ရွေးချယ်သည်။ classifier object ကို ဖန်တီးသောအခါ မော်ဒယ်ကို ဒေါင်းလုဒ်လုပ်ပြီး cache လုပ်သည်။ command ကို ပြန်လည် run ပါက cache လုပ်ထားသော မော်ဒယ်ကို အသုံးပြုမည်ဖြစ်ပြီး မော်ဒယ်ကို ထပ်မံ download လုပ်ရန် မလိုအပ်ပါ။

စာသားတစ်ခုကို pipeline သို့ ပေးပို့သောအခါ အဓိက အဆင့်သုံးဆင့် ပါဝင်သည်။

၁။ စာသားကို မော်ဒယ် နားလည်နိုင်သော format သို့ ကြိုတင် စီမံဆောင်ရွက်သည်။
၂။ ကြိုတင် စီမံဆောင်ရွက်ထားသော input များကို မော်ဒယ်သို့ ပေးပို့သည်။
၃။ မော်ဒယ်၏ ခန့်မှန်းချက်များကို နောက်ဆက်တွဲ စီမံဆောင်ရွက်သည်၊ ထို့ကြောင့် သင် ၎င်းတို့ကို နားလည်နိုင်သည်။

လက်ရှိ [ရရှိနိုင်သော pipeline များ] (https://huggingface.co/transformers/main_classes/pipelines) အချို့မှာ -

- `feature-extraction` (စာသား၏ vector ကိုယ်စားပြုမှုကို ရယူပါ)
- `fill-mask`
- `ner` (အမည်ပေး entity recognition)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

၎င်းတို့ထဲမှ အနည်းငယ်ကို ကြည့်ကြပါစို့။

## Zero-shot classification

Label မပါသော စာသားများကို အမျိုးအစား ခွဲခြားရန် လိုအပ်သည့် ပိုမိုစိန်ခေါ်မှုရှိသော လုပ်ငန်းတစ်ခုဖြင့် စတင်ကြပါစို့။ စာသားကို annotation ပြုလုပ်ခြင်းသည် အချိန်ကုန်ပြီး domain နှင့်ပတ်သက်သော အတွေ့အကြုံ လိုအပ်သောကြောင့် ၎င်းသည် လက်တွေ့ကမ္ဘာ ပရောဂျက်များတွင် အဖြစ်များသော အခြေအနေတစ်ခုဖြစ်သည်။ ဤအသုံးပြုမှုအတွက် `zero-shot-classification` pipeline သည် အလွန်အစွမ်းထက်သည်။ ၎င်းသည် အမျိုးအစားခွဲခြားရန် အသုံးပြုမည့် label များကို သတ်မှတ်နိုင်စေပြီး ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်၏ label များကို မှီခိုရန် မလိုအပ်ပါ။ ထို label နှစ်ခုကို အသုံးပြု၍ မော်ဒယ်သည် စာကြောင်းတစ်ကြောင်းကို positive သို့မဟုတ် negative အဖြစ် မည်သို့အမျိုးအစား ခွဲခြားနိုင်သည်ကို သင် တွေ့မြင်ပြီးဖြစ်သည် - သို့သော် ၎င်းသည် သင်နှစ်သက်သော အခြား label အစုံကို အသုံးပြု၍ စာသားကို အမျိုးအစား ခွဲခြားနိုင်သည်။

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

ဤ pipeline ကို zero-shot ဟုခေါ်သည်၊ အဘယ်ကြောင့်ဆိုသော် ၎င်းကိုအသုံးပြုရန် သင်၏ဒေတာတွင် မော်ဒယ်ကို fine-tune လုပ်ရန် မလိုအပ်ပါ။ သင်လိုချင်သော မည်သည့် label စာရင်းအတွက်မဆို ၎င်းသည် တိုက်ရိုက် ဖြစ်နိုင်ခြေရမှတ်များကို ပြန်ပေးနိုင်သည်။

<Tip>

✏️ စမ်းကြည့်ပါ။ သင်၏ကိုယ်ပိုင် sequence များနှင့် label များဖြင့် Play ပြီး မော်ဒယ် မည်သို့ပြုမူလုပ်ဆောင်သည်ကို ကြည့်ပါ။

</Tip>


# စာသားထုတ်လုပ်ခြင်း (Text generation)

စာသားအချို့ကို ထုတ်လုပ်(generation)ရန် pipeline ကို မည်သို့အသုံးပြုရမည်ကို ကြည့်ကြပါစို့။ ဤနေရာတွင် အဓိကအချက်မှာ သင်သည် prompt တစ်ခု ပေး၍ မော်ဒယ်သည် ကျန်ရှိသော စာသားကို ထုတ်လုပ်ခြင်း(generation)ဖြင့် ၎င်းကို အလိုအလျောက် ဖြည့်စွက်ပေးမည် ဖြစ်သည်။ ၎င်းသည် ဖုန်းများစွာတွင် တွေ့ရှိရသော predictive text feature နှင့် ဆင်တူသည်။ စာသားထုတ်လုပ်ခြင်း(Text generation)တွင် ကျပန်းဖြစ်မှု(randomness) ပါဝင်သောကြောင့် အောက်တွင် ပြထားသည့်အတိုင်း ရလဒ်များကို သင် မရရှိပါက ပုံမှန်ဖြစ်သည်။

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows — data flows of various types, as seen by the '
                    'HTTP'}]
```

`num_return_sequences` argument ဖြင့် မည်သည့် sequence များကို ထုတ်လုပ်ရမည်ကို ထိန်းချုပ်နိုင်ပြီး `max_length argument ဖြင့် output စာသား၏ စုစုပေါင်း အရှည်ကို ထိန်းချုပ်နိုင်သည်။

<Tip>

✏️ စမ်းကြည့်ပါ။ စကားလုံး ၁၅ လုံးစီပါဝင်သော စာကြောင်းနှစ်ကြောင်းကို ထုတ်လုပ်ရန် `num_return_sequences` နှင့် `max_length` argument များကို အသုံးပြုပါ။

</Tip>

## Hub မှ မည်သည့်မော်ဒယ်ကိုမဆို pipeline တွင် အသုံးပြုခြင်း

ယခင် ဥပမာများတွင် လုပ်ငန်းတစ်ခုအတွက် default မော်ဒယ်ကို အသုံးပြုခဲ့သော်လည်း သတ်မှတ်ထားသော လုပ်ငန်းတစ်ခုအတွက် - ဥပမာ စာသားထုတ်လုပ်ခြင်း - pipeline တွင် အသုံးပြုရန် Hub မှ မော်ဒယ်တစ်ခုကိုလည်း ရွေးချယ်နိုင်သည်။ [Model Hub](https://huggingface.co/models) သို့သွား၍ ဘယ်ဘက်ရှိ သက်ဆိုင်ရာ tag ကို နှိပ်ခြင်းဖြင့် ထိုလုပ်ငန်းအတွက် ပံ့ပိုးပေးထားသော မော်ဒယ်များကိုသာ ပြသပါ။ သင်သည် [ဤကဲ့သို့သော စာမျက်နှာ] (https://huggingface.co/models?pipeline_tag=text-generation) သို့ ရောက်ရှိသင့်သည်။

[`distilgpt2`](https://huggingface.co/distilgpt2) မော်ဒယ်ကို စမ်းကြည့်ကြပါစို့။ ယခင်ကဲ့သို့ pipeline တွင် ၎င်းကို load လုပ်ရမည့်နည်း -


```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

ဘာသာစကား tag များကို နှိပ်ခြင်းဖြင့် မော်ဒယ်တစ်ခုအတွက် သင်၏ရှာဖွေမှုကို ပိုမိုကောင်းမွန်အောင် ပြုလုပ်နိုင်ပြီး အခြားဘာသာစကားဖြင့် စာသားကို ထုတ်လုပ်ပေးမည့် မော်ဒယ်တစ်ခုကို ရွေးချယ်နိုင်သည်။ Model Hub တွင် ဘာသာစကားများစွာကို ပံ့ပိုးပေးသော multilingual မော်ဒယ်များအတွက် checkpoint များပင် ပါဝင်သည်။

မော်ဒယ်တစ်ခုကို နှိပ်ခြင်းဖြင့် ရွေးချယ်ပြီးသည်နှင့် ၎င်းကို online တွင် တိုက်ရိုက်စမ်းသပ်နိုင်စေမည့် widget တစ်ခုရှိသည်ကို သင်တွေ့လိမ့်မည်။ ဤနည်းအားဖြင့် မော်ဒယ်ကို ဒေါင်းလုဒ်မလုပ်မီ ၎င်း၏စွမ်းရည်ကို လျင်မြန်စွာ စမ်းသပ်နိုင်သည်။

<Tip>

✏️ **စမ်းကြည့်ပါ။** အခြားဘာသာစကားအတွက် စာသားထုတ်လုပ်မှု မော်ဒယ်တစ်ခုကို ရှာဖွေရန် filter များကို အသုံးပြုပါ။ widget ဖြင့် ကစားပြီး pipeline တွင် အသုံးပြုပါ။

</Tip>

### Inference API

Hugging Face [ဝက်ဘ်ဆိုက်](https://huggingface.co/) တွင် ရရှိနိုင်သော Inference API ကို အသုံးပြု၍ မော်ဒယ်အားလုံးကို သင်၏ browser မှတစ်ဆင့် တိုက်ရိုက် စမ်းသပ်နိုင်သည်။ ဤစာမျက်နှာတွင် စိတ်ကြိုက်စာသားကို ထည့်သွင်းခြင်းဖြင့် မော်ဒယ်ကို တိုက်ရိုက် ကစားနိုင်ပြီး မော်ဒယ်သည် input data ကို မည်သို့ စီမံဆောင်ရွက်သည်ကို ကြည့်ရှုနိုင်သည်။

widget ကို စွမ်းဆောင်ပေးသည့် Inference API ကိုလည်း အခကြေးငွေပေးရသော ထုတ်ကုန်တစ်ခုအဖြစ် ရရှိနိုင်ပြီး သင်၏ workflow များအတွက် လိုအပ်ပါက အသုံးဝင်ပါသည်။ အသေးစိတ်အချက်အလက်များအတွက် [ဈေးနှုန်းစာမျက်နှာ] (https://huggingface.co/pricing) ကို ကြည့်ပါ။

## Mask ဖြည့်ခြင်း

သင် စမ်းသပ်မည့် နောက်ထပ် pipeline မှာ `fill-mask` ဖြစ်သည်။ ဤလုပ်ငန်း၏ အဓိကအချက်မှာ ပေးထားသော စာသားတွင် ကွက်လပ်များကို ဖြည့်စွက်ရန်ဖြစ်သည်။

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

`top_k` argument သည် သင်ပြသလိုသော ဖြစ်နိုင်ခြေမည်မျှကို ထိန်းချုပ်သည်။ ဤနေရာတွင် မော်ဒယ်သည် mask token ဟု မကြာခဏ ရည်ညွှန်းလေ့ရှိသော အထူး `<mask>` စကားလုံးကို ဖြည့်သွင်းကြောင်း သတိပြုပါ။ အခြား mask-filling မော်ဒယ်များတွင် မတူညီသော *mask token* များ ရှိနိုင်သောကြောင့် အခြားမော်ဒယ်များကို စူးစမ်းလေ့လာသည့်အခါ မှန်ကန်သော mask စကားလုံးကို အတည်ပြုခြင်းသည် အမြဲတမ်း ကောင်းမွန်ပါသည်။ ၎င်းကိုစစ်ဆေးရန် နည်းလမ်းတစ်ခုမှာ widget တွင် အသုံးပြုထားသော mask စကားလုံးကို ကြည့်ရှုခြင်းဖြစ်သည်။

<Tip>

✏️ **စမ်းကြည့်ပါ။** Hub တွင် `bert-base-cased` မော်ဒယ်ကို ရှာဖွေပြီး Inference API widget တွင် ၎င်း၏ mask စကားလုံးကို ရှာဖွေပါ။ အထက်ဖော်ပြပါ ကျွန်ုပ်တို့၏ `pipeline` ဥပမာတွင် ဤမော်ဒယ်သည် စာကြောင်းအတွက် ဘာကို ခန့်မှန်းသနည်း။

</Tip>

## အမည်ပေး entity recognition (Named entity recognition)

Named entity recognition (NER) သည် input စာသား၏ မည်သည့်အပိုင်းများသည် လူပုဂ္ဂိုလ်များ၊ နေရာများ သို့မဟုတ် အဖွဲ့အစည်းများကဲ့သို့သော entity များနှင့် ကိုက်ညီသည်ကို မော်ဒယ်က ရှာဖွေရမည့် လုပ်ငန်းတစ်ခုဖြစ်သည်။ ဥပမာတစ်ခုကို ကြည့်ကြပါစို့။

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```
ဤနေရာတွင် မော်ဒယ်သည် Sylvain သည် လူတစ်ဦး (PER)၊ Hugging Face သည် အဖွဲ့အစည်းတစ်ခု (ORG) နှင့် Brooklyn သည် နေရာတစ်ခု (LOC) ဖြစ်ကြောင်း မှန်ကန်စွာ ဖော်ထုတ်ခဲ့သည်။

စာကြောင်း၏ အစိတ်အပိုင်းများကို တူညီသော entity နှင့် ကိုက်ညီသည့်အပိုင်းများကို ပြန်လည်စုစည်းရန် pipeline ကို ပြောပြရန် pipeline ဖန်တီးမှု function တွင် `grouped_entities=True` option ကို ကျွန်ုပ်တို့ ပေးပို့သည်။ ဤနေရာတွင် မော်ဒယ်သည် "Hugging" နှင့် "Face" ကို အဖွဲ့အစည်းတစ်ခုတည်းအဖြစ် မှန်ကန်စွာ စုစည်းထားသော်လည်း အမည်တွင် စကားလုံးများစွာ ပါဝင်သည်။ တကယ်တော့ နောက်အခန်းတွင် ကျွန်ုပ်တို့ တွေ့မြင်ရမည့်အတိုင်း preprocessing သည် စကားလုံးအချို့ကို အပိုင်းငယ်များအဖြစ်ပင် ပိုင်းခြားထားသည်။ ဥပမာအားဖြင့် `Sylvain` ကို အပိုင်းလေးပိုင်း ပိုင်းခြားထားသည်။ `S`, `##yl`, `##va`, နှင့် `##in။` post-processing အဆင့်တွင် pipeline သည် ထိုအပိုင်းများကို အောင်မြင်စွာ ပြန်လည်စုစည်းခဲ့သည်။

<Tip>

✏️ **စမ်းကြည့်ပါ။** အင်္ဂလိပ်လို Part-of-speech tagging (POS လို့ အတိုကောက်ခေါ်တယ်) လုပ်နိုင်တဲ့ မော်ဒယ်ကို Model Hub မှာ ရှာပါ။ အပေါ်က ဥပမာစာကြောင်းအတွက် ဒီမော်ဒယ်က ဘာကို ခန့်မှန်းပါသလဲ။

</Tip>

## မေးခွန်းဖြေဆိုခြင်း (Question answering)

`question-answering` pipeline သည် ပေးထားသော အကြောင်းအရာမှ အချက်အလက်များကို အသုံးပြု၍ မေးခွန်းများကို ဖြေကြားသည်။

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

ဤ pipeline သည် ပေးထားသော context မှ အချက်အလက်များကို ထုတ်ယူခြင်းဖြင့် အလုပ်လုပ်ကြောင်း သတိပြုပါ။ ၎င်းသည် အဖြေကို ထုတ်လုပ်ခြင်း မဟုတ်ပါ။

## အနှစ်ချုပ်ရေးသားခြင်း (Summarization)

အနှစ်ချုပ်ရေးသားခြင်း(Summarization)သည် စာသားတွင် ရည်ညွှန်းထားသော အရေးကြီးသော အချက်များအားလုံး (သို့မဟုတ် အများစု) ကို ထိန်းသိမ်းထားစဉ် စာသားတစ်ခုကို ပိုတိုသော စာသားအဖြစ် လျှော့ချခြင်း လုပ်ငန်းတစ်ခုဖြစ်သည်။ ဥပမာတစ်ခုကို ဤတွင် ဖော်ပြထားသည်။

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

စာသားထုတ်လုပ်ခြင်းကဲ့သို့ပင် ရလဒ်အတွက် `max_length` သို့မဟုတ် `min_length` ကို သတ်မှတ်နိုင်သည်။


## ဘာသာပြန်ခြင်း (Translation)

ဘာသာပြန်ခြင်း(Translation) အတွက်၊ သင်သည် task name တွင် ဘာသာစကား အတွဲတစ်ခု ပေးပါက (ဥပမာ `"translation_en_to_fr"`) default မော်ဒယ်ကို အသုံးပြုနိုင်သော်လည်း အလွယ်ကူဆုံး နည်းလမ်းမှာ [Model Hub](https://huggingface.co/models) တွင် သင်အသုံးပြုလိုသော မော်ဒယ်ကို ရွေးချယ်ခြင်း ဖြစ်သည်။ ဤတွင် ပြင်သစ်မှ အင်္ဂလိပ်သို့ ဘာသာပြန်ဆိုခြင်းကို ကျွန်ုပ်တို့ စမ်းကြည့်ပါမည်။

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

စာသားထုတ်လုပ်ခြင်း(Text generation) နှင့် အနှစ်ချုပ်ရေးသားခြင်း(Summarization) ကဲ့သို့ပင် ရလဒ်အတွက် max_length သို့မဟုတ် min_length ကို သတ်မှတ်နိုင်သည်။

<Tip>

✏️ စမ်းကြည့်ပါ။ အခြားဘာသာစကားများဖြင့် ဘာသာပြန်ဆိုသော မော်ဒယ်များကို ရှာဖွေပြီး ယခင်စာကြောင်းကို မတူညီသော ဘာသာစကားအနည်းငယ်ဖြင့် ဘာသာပြန်ဆိုရန် ကြိုးစားပါ။

</Tip>

ယခုအချိန်အထိ ပြသထားသော pipeline များသည် သရုပ်ပြရန် ရည်ရွယ်ချက်များအတွက် အများစုဖြစ်သည်။ ၎င်းတို့ကို သတ်မှတ်ထားသော လုပ်ငန်းများအတွက် ပရိုဂရမ်ရေးဆွဲထားပြီး ၎င်းတို့၏ မျိုးကွဲများကို မလုပ်ဆောင်နိုင်ပါ။ နောက်အခန်းတွင် `pipeline() function` တွင် ဘာတွေပါဝင်သည်နှင့် ၎င်း၏ အပြုအမူကို မည်သို့စိတ်ကြိုက်ပြင်ဆင်ရမည်ကို သင်လေ့လာရပါမည်။