<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

# Transformer Architectures များ[[transformer-architectures]]

ယခင်အပိုင်းတွေမှာ Transformer architecture အကြောင်းကို မိတ်ဆက်ပေးခဲ့ပြီး ဒီမော်ဒယ်တွေက လုပ်ငန်းတာဝန်အမျိုးမျိုးကို ဘယ်လိုဖြေရှင်းပေးနိုင်လဲဆိုတာကို လေ့လာခဲ့ပါတယ်။ အခုတော့ Transformer မော်ဒယ်တွေရဲ့ အဓိက architectured ပုံစံသုံးမျိုးကို ပိုမိုနက်နဲစွာ လေ့လာပြီး တစ်ခုချင်းစီကို ဘယ်အချိန်မှာ အသုံးပြုသင့်လဲဆိုတာကို နားလည်အောင် လုပ်ဆောင်ကြပါစို့။ ထို့နောက်၊ အဲဒီ architecture တွေကို မတူညီတဲ့ ဘာသာစကားလုပ်ငန်းတာဝန်တွေမှာ ဘယ်လိုအသုံးပြုလဲဆိုတာကို ကြည့်ရှုခဲ့ကြပါတယ်။

ဒီအပိုင်းမှာတော့ Transformer မော်ဒယ်တွေရဲ့ အဓိက architectured ပုံစံသုံးမျိုးကို ပိုမိုနက်နဲစွာ လေ့လာပြီး တစ်ခုချင်းစီကို ဘယ်အချိန်မှာ အသုံးပြုသင့်လဲဆိုတာကို နားလည်အောင် လုပ်ဆောင်သွားပါမယ်။


> [!TIP]
> Transformer မော်ဒယ်အများစုဟာ architecture သုံးမျိုးထဲက တစ်ခုကို အသုံးပြုတယ်ဆိုတာ သတိရပါ။ အဲဒါတွေကတော့ encoder-only, decoder-only, ဒါမှမဟုတ် encoder-decoder (sequence-to-sequence) တို့ ဖြစ်ပါတယ်။ ဒီကွာခြားချက်တွေကို နားလည်ထားခြင်းက သင့်ရဲ့ သီးခြားလုပ်ငန်းတာဝန်အတွက် မှန်ကန်တဲ့ မော်ဒယ်ကို ရွေးချယ်နိုင်ဖို့ ကူညီပါလိမ့်မယ်။

## Encoder မော်ဒယ်များ[[encoder-models]]

<Youtube id="MUqNwgPjJvQ" />

Encoder မော်ဒယ်တွေဟာ Transformer မော်ဒယ်ရဲ့ encoder အပိုင်းကိုသာ အသုံးပြုပါတယ်။ အဆင့်တိုင်းမှာ attention layers တွေက မူလစာကြောင်းထဲက စကားလုံးအားလုံးကို ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်။ ဒီမော်ဒယ်တွေကို "bi-directional" attention ရှိတယ်လို့ မကြာခဏ ဖော်ပြလေ့ရှိပြီး *auto-encoding models* လို့လည်း ခေါ်ကြပါတယ်။

ဒီမော်ဒယ်တွေရဲ့ pretraining က များသောအားဖြင့် ပေးထားတဲ့ စာကြောင်းတစ်ခုကို တစ်နည်းတစ်ဖုံ ဖျက်ဆီးပြီး (ဥပမာ- ကျပန်းစကားလုံးတွေကို ဝှက်ထားခြင်းဖြင့်) မူလစာကြောင်းကို ရှာဖွေခြင်း သို့မဟုတ် ပြန်လည်တည်ဆောက်ခြင်းတို့ကို မော်ဒယ်ကို တာဝန်ပေးခြင်းအပေါ် အခြေခံပါတယ်။

Encoder မော်ဒယ်တွေဟာ စာကြောင်းအပြည့်အစုံကို နားလည်ဖို့ လိုအပ်တဲ့ လုပ်ငန်းတာဝန်များအတွက် အသင့်တော်ဆုံးဖြစ်ပြီး ဥပမာအားဖြင့် sentence classification, named entity recognition (နဲ့ ပိုမိုယေဘုယျအားဖြင့် word classification) နဲ့ extractive question answering တို့ပဲ ဖြစ်ပါတယ်။

> [!TIP]
> "[🤗 Transformers တွေက လုပ်ငန်းတာဝန်တွေကို ဘယ်လိုဖြေရှင်းပေးလဲ။](/chapter1/5)" မှာ ကျွန်တော်တို့ မြင်တွေ့ခဲ့ရသလို BERT လို encoder မော်ဒယ်တွေဟာ စာသားကို နားလည်ရာမှာ ထူးချွန်ပါတယ်။ ဘာကြောင့်လဲဆိုတော့ ၎င်းတို့ဟာ input တစ်ခုလုံးရဲ့ အကြောင်းအရာ (context) ကို နှစ်ဖက်စလုံးကနေ ကြည့်ရှုနိုင်လို့ပါ။ ဒါကြောင့် input တစ်ခုလုံးရဲ့ နားလည်မှုက အရေးကြီးတဲ့ လုပ်ငန်းတာဝန်တွေအတွက် ၎င်းတို့ဟာ အကောင်းဆုံးပါပဲ။

ဒီမော်ဒယ်မိသားစုရဲ့ ကိုယ်စားပြုမော်ဒယ်တွေကတော့:

- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ModernBERT](https://huggingface.co/docs/transformers/en/model_doc/modernbert)

## Decoder မော်ဒယ်များ[[decoder-models]]

<Youtube id="d_ixlCubqQw" />

Decoder မော်ဒယ်တွေဟာ Transformer မော်ဒယ်ရဲ့ decoder အပိုင်းကိုသာ အသုံးပြုပါတယ်။ အဆင့်တိုင်းမှာ ပေးထားတဲ့ စကားလုံးတစ်ခုအတွက် attention layers တွေက စာကြောင်းထဲမှာ အရင်ရှိနေတဲ့ စကားလုံးတွေကိုသာ ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်။ ဒီမော်ဒယ်တွေကို မကြာခဏ *auto-regressive models* လို့ ခေါ်ကြပါတယ်။

Decoder မော်ဒယ်တွေရဲ့ pretraining က များသောအားဖြင့် စာကြောင်းထဲက နောက်ထပ်စကားလုံးကို ခန့်မှန်းခြင်းအပေါ် အခြေခံပါတယ်။

ဒီမော်ဒယ်တွေဟာ စာသားဖန်တီးခြင်း (text generation) နဲ့ပတ်သက်တဲ့ လုပ်ငန်းတာဝန်တွေအတွက် အသင့်တော်ဆုံး ဖြစ်ပါတယ်။

> [!TIP]
> GPT လို decoder မော်ဒယ်တွေကို တစ်ကြိမ်လျှင် token တစ်ခုချင်းစီကို ခန့်မှန်းခြင်းဖြင့် စာသားတွေ ဖန်တီးဖို့ ဒီဇိုင်းထုတ်ထားပါတယ်။ "[🤗 Transformers တွေက လုပ်ငန်းတာဝန်တွေကို ဘယ်လိုဖြေရှင်းပေးလဲ။](/chapter1/5)" မှာ ကျွန်တော်တို့ လေ့လာခဲ့ရသလို၊ ၎င်းတို့ဟာ ယခင် tokens တွေကိုသာ မြင်နိုင်တာကြောင့် ဖန်တီးမှုဆိုင်ရာ စာသားဖန်တီးခြင်းအတွက် အလွန်ကောင်းမွန်ပေမယ့် bi-directional နားလည်မှု လိုအပ်တဲ့ လုပ်ငန်းတာဝန်တွေအတွက်တော့ သိပ်မသင့်လျော်ပါဘူး။

ဒီမော်ဒယ်မိသားစုရဲ့ ကိုယ်စားပြုမော်ဒယ်တွေကတော့:

- [Hugging Face SmolLM Series](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)
- [Meta's Llama Series](https://huggingface.co/docs/transformers/en/model_doc/llama4)
- [Google's Gemma Series](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3)
- [DeepSeek's V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)

### ခေတ်မီ Large Language Models (LLMs) များ

ခေတ်မီ Large Language Models (LLMs) အများစုဟာ decoder-only architecture ကို အသုံးပြုပါတယ်။ ဒီမော်ဒယ်တွေဟာ လွန်ခဲ့တဲ့ နှစ်အနည်းငယ်အတွင်း အရွယ်အစားနဲ့ စွမ်းရည်တွေ သိသိသာသာ ကြီးထွားလာခဲ့ပြီး အကြီးဆုံးမော်ဒယ်အချို့မှာ parameters ဘီလီယံပေါင်း ရာနဲ့ချီ ပါဝင်ပါတယ်။

ခေတ်မီ LLMs တွေကို ပုံမှန်အားဖြင့် အဆင့်နှစ်ဆင့်နဲ့ လေ့ကျင့်ပါတယ်။
1. **Pretraining**: မော်ဒယ်ဟာ များပြားလှတဲ့ စာသားဒေတာတွေပေါ်မှာ နောက်ထပ် token ကို ခန့်မှန်းဖို့ သင်ယူပါတယ်။
2. **Instruction tuning**: မော်ဒယ်ဟာ ညွှန်ကြားချက်တွေကို လိုက်နာပြီး အထောက်အကူဖြစ်စေတဲ့ တုံ့ပြန်မှုတွေကို ဖန်တီးဖို့ fine-tune လုပ်ခံရပါတယ်။

ဒီချဉ်းကပ်မှုက ကျယ်ပြန့်တဲ့ ခေါင်းစဉ်တွေနဲ့ လုပ်ငန်းတာဝန်မျိုးစုံမှာ လူသားဆန်တဲ့ စာသားတွေကို နားလည်ပြီး ဖန်တီးနိုင်တဲ့ မော်ဒယ်တွေကို ဖြစ်ပေါ်စေခဲ့ပါတယ်။

#### ခေတ်မီ LLMs တွေရဲ့ အဓိက စွမ်းရည်များ

ခေတ်မီ decoder-based LLMs တွေဟာ အထင်ကြီးစရာကောင်းတဲ့ စွမ်းရည်တွေကို ပြသခဲ့ပါတယ်။

| စွမ်းရည်           | ဖော်ပြချက်                                            | ဥပမာ                                    |
|--------------------|--------------------------------------------------------|------------------------------------------|
| စာသားဖန်တီးခြင်း    | ဆက်စပ်မှုရှိပြီး အကြောင်းအရာနှင့်ကိုက်ညီသော စာသားများ ဖန်တီးခြင်း | စာစီစာကုံး၊ ပုံပြင်များ သို့မဟုတ် emails များ ရေးသားခြင်း |
| အကျဉ်းချုပ်ခြင်း     | စာရွက်စာတမ်းရှည်များကို ပိုမိုတိုတောင်းသော ပုံစံများဖြင့် အကျဉ်းချုံးခြင်း | အစီရင်ခံစာများ၏ အနှစ်ချုပ်များ ဖန်တီးခြင်း |
| ဘာသာပြန်ခြင်း      | ဘာသာစကားများအကြား စာသားများကို ပြောင်းလဲခြင်း           | အင်္ဂလိပ်မှ စပိန်သို့ ဘာသာပြန်ခြင်း        |
| မေးခွန်းဖြေဆိုခြင်း | အမှန်တကယ်မေးခွန်းများကို အဖြေပေးခြင်း                   | "ပြင်သစ်နိုင်ငံရဲ့ မြို့တော်က ဘာလဲ"      |
| Code ဖန်တီးခြင်း   | Code snippets များကို ရေးသားခြင်း သို့မဟုတ် ဖြည့်စွက်ခြင်း | ဖော်ပြချက်အပေါ် အခြေခံ၍ function တစ်ခု ဖန်တီးခြင်း |
| ဆင်ခြင်တုံတရား      | ပြဿနာများကို အဆင့်ဆင့်ဖြေရှင်းခြင်း                     | သင်္ချာပြဿနာများ သို့မဟုတ် ယုတ္တိပဟေဠိများ ဖြေရှင်းခြင်း |
| Few-shot learning  | prompt တွင် ဥပမာအနည်းငယ်မှ သင်ယူခြင်း                 | ဥပမာ ၂-၃ ခုသာ မြင်ပြီးနောက် စာသားများကို အမျိုးအစားခွဲခြားခြင်း |

Hub ပေါ်ရှိ model repo စာမျက်နှာများမှတစ်ဆင့် သင်၏ browser ထဲတွင် decoder-based LLMs များကို တိုက်ရိုက် စမ်းသပ်နိုင်ပါတယ်။ classic [GPT-2](https://huggingface.co/openai-community/gpt2) (OpenAI ရဲ့ အကောင်းဆုံး open source မော်ဒယ်!) ဥပမာတစ်ခုကတော့ ဒီမှာပါ။

<iframe
	src="https://huggingface.co/openai-community/gpt2"
	frameborder="0"
	width="100%"
	height="450"
></iframe>

## Sequence-to-sequence မော်ဒယ်များ[[sequence-to-sequence-models]]

<Youtube id="0_4KEb08xrE" />

Encoder-decoder မော်ဒယ်တွေ (သို့မဟုတ် *sequence-to-sequence models* လို့လည်း ခေါ်ကြပါတယ်) ဟာ Transformer architecture ရဲ့ အစိတ်အပိုင်းနှစ်ခုလုံးကို အသုံးပြုပါတယ်။ အဆင့်တိုင်းမှာ encoder ရဲ့ attention layers တွေက မူလစာကြောင်းထဲက စကားလုံးအားလုံးကို ဝင်ရောက်ကြည့်ရှုနိုင်ပြီး၊ decoder ရဲ့ attention layers တွေကတော့ input ထဲမှာ ပေးထားတဲ့ စကားလုံးတစ်ခုရဲ့ အရင်ရှိနေတဲ့ စကားလုံးတွေကိုသာ ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်။

ဒီမော်ဒယ်တွေရဲ့ pretraining က ပုံစံအမျိုးမျိုး ရှိနိုင်ပေမယ့် များသောအားဖြင့် input က တစ်နည်းတစ်ဖုံ ဖျက်ဆီးခံထားရတဲ့ စာကြောင်းတစ်ခုကို ပြန်လည်တည်ဆောက်ခြင်း (ဥပမာ- ကျပန်းစကားလုံးတွေကို ဝှက်ထားခြင်းဖြင့်) တို့ ပါဝင်ပါတယ်။ T5 မော်ဒယ်ရဲ့ pretraining ကတော့ စာသားအပိုင်းအစများ (စကားလုံးများစွာ ပါဝင်နိုင်ပါတယ်) ကို mask special token တစ်ခုတည်းနဲ့ အစားထိုးပြီး အဲဒီ mask token က အစားထိုးထားတဲ့ စာသားကို ခန့်မှန်းဖို့ လုပ်ငန်းတာဝန်ပေးခြင်းတို့ ပါဝင်ပါတယ်။

Sequence-to-sequence မော်ဒယ်တွေဟာ ပေးထားတဲ့ input အပေါ် မူတည်ပြီး စာကြောင်းအသစ်တွေ ဖန်တီးခြင်းနဲ့ပတ်သက်တဲ့ လုပ်ငန်းတာဝန်တွေအတွက် အသင့်တော်ဆုံးဖြစ်ပြီး ဥပမာအားဖြင့် summarization, translation, ဒါမှမဟုတ် generative question answering တို့ပဲ ဖြစ်ပါတယ်။

> [!TIP]
> "[🤗 Transformers တွေက လုပ်ငန်းတာဝန်တွေကို ဘယ်လိုဖြေရှင်းပေးလဲ။](/chapter1/5)" မှာ ကျွန်တော်တို့ မြင်တွေ့ခဲ့ရသလို BART နဲ့ T5 လို encoder-decoder မော်ဒယ်တွေဟာ architecture နှစ်ခုလုံးရဲ့ အားသာချက်တွေကို ပေါင်းစပ်ထားပါတယ်။ encoder က input ကို နှစ်ဖက်စလုံးကနေ နက်ရှိုင်းစွာ နားလည်မှုကို ပေးပြီး၊ decoder ကတော့ သင့်လျော်တဲ့ output စာသားကို ဖန်တီးပေးပါတယ်။ ဒါကြောင့် ဘာသာပြန်ခြင်း ဒါမှမဟုတ် အကျဉ်းချုပ်ခြင်းလို sequence တစ်ခုကို အခြားတစ်ခုသို့ ပြောင်းလဲပေးတဲ့ လုပ်ငန်းတာဝန်တွေအတွက် ၎င်းတို့ဟာ အကောင်းဆုံးပါပဲ။

### လက်တွေ့အသုံးချမှုများ

Sequence-to-sequence မော်ဒယ်တွေဟာ စာသားတစ်ခုရဲ့ ပုံစံကို အဓိပ္ပာယ်မပျက်စီးစေဘဲ အခြားပုံစံတစ်ခုသို့ ပြောင်းလဲဖို့ လိုအပ်တဲ့ လုပ်ငန်းတာဝန်တွေမှာ ထူးချွန်ပါတယ်။ လက်တွေ့အသုံးချမှုအချို့ကတော့:

| အသုံးချမှု           | ဖော်ပြချက်                                            | ဥပမာ မော်ဒယ် |
|--------------------|--------------------------------------------------------|---------------|
| စက်ဘာသာပြန်ခြင်း    | ဘာသာစကားများအကြား စာသားများကို ပြောင်းလဲခြင်း           | Marian, T5    |
| စာသားအကျဉ်းချုပ်ခြင်း | စာသားရှည်များကို အကျဉ်းချုံး ဖန်တီးခြင်း                  | BART, T5      |
| ဒေတာမှ စာသားဖန်တီးခြင်း | ဖွဲ့စည်းထားသော ဒေတာများကို သဘာဝဘာသာစကားအဖြစ် ပြောင်းလဲခြင်း | T5            |
| သဒ္ဒါပြင်ဆင်ခြင်း      | စာသားရှိ သဒ္ဒါအမှားများကို ပြင်ဆင်ခြင်း                   | T5            |
| မေးခွန်းဖြေဆိုခြင်း | အကြောင်းအရာ (context) အပေါ် အခြေခံ၍ အဖြေများ ဖန်တီးခြင်း | BART, T5      |

ဘာသာပြန်ခြင်းအတွက် sequence-to-sequence မော်ဒယ်ရဲ့ အပြန်အလှန်တုံ့ပြန်နိုင်တဲ့ demo တစ်ခုကတော့ ဒီမှာပါ။

<iframe
	src="https://course-demos-speech-to-speech-translation.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

ဒီမော်ဒယ်မိသားစုရဲ့ ကိုယ်စားပြုမော်ဒယ်တွေကတော့:

- [BART](https://huggingface.co/docs/transformers/model_doc/bart)
- [mBART](https://huggingface.co/docs/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/docs/transformers/model_doc/marian)
- [T5](https://huggingface.co/docs/transformers/model_doc/t5)

## မှန်ကန်သော Architecture ကို ရွေးချယ်ခြင်း[[choosing-the-right-architecture]]

တိကျတဲ့ NLP လုပ်ငန်းတာဝန်တစ်ခုကို လုပ်ဆောင်တဲ့အခါ ဘယ် architecture ကို အသုံးပြုရမယ်ဆိုတာ ဘယ်လိုဆုံးဖြတ်မလဲ။ အောက်ပါကတော့ အမြန်လမ်းညွှန်ချက် ဖြစ်ပါတယ်။

| လုပ်ငန်းတာဝန်                      | အကြံပြုထားသော Architecture | ဥပမာများ       |
|-------------------------------------|--------------------------------|-------------------|
| စာသားခွဲခြားသတ်မှတ်ခြင်း (sentiment, topic) | Encoder                        | BERT, RoBERTa     |
| စာသားဖန်တီးခြင်း (creative writing) | Decoder                        | GPT, LLaMA        |
| ဘာသာပြန်ခြင်း                        | Encoder-Decoder                | T5, BART          |
| အကျဉ်းချုပ်ခြင်း                     | Encoder-Decoder                | BART, T5          |
| Named entity recognition            | Encoder                        | BERT, RoBERTa     |
| မေးခွန်းဖြေဆိုခြင်း (extractive)   | Encoder                        | BERT, RoBERTa     |
| မေးခွန်းဖြေဆိုခြင်း (generative)   | Encoder-Decoder သို့မဟုတ် Decoder | T5, GPT           |
| Conversational AI                   | Decoder                        | GPT, LLaMA        |

> [!TIP]
> ဘယ်မော်ဒယ်ကို အသုံးပြုရမယ်ဆိုတာ မသေချာတဲ့အခါ အောက်ပါအချက်တွေကို ထည့်သွင်းစဉ်းစားပါ။
>
> 1.  သင့်လုပ်ငန်းတာဝန်က ဘယ်လိုနားလည်မှုမျိုး လိုအပ်လဲ။ (Bi-directional သို့မဟုတ် Uni-directional)
> 2.  သင်ဟာ စာသားအသစ် ဖန်တီးမှာလား ဒါမှမဟုတ် ရှိပြီးသားစာသားကို ခွဲခြမ်းစိတ်ဖြာမှာလား။
> 3.  Sequence တစ်ခုကို အခြားတစ်ခုသို့ ပြောင်းလဲဖို့ လိုအပ်ပါသလား။
>
> ဒီမေးခွန်းတွေရဲ့ အဖြေတွေက သင့်ကို မှန်ကန်တဲ့ architecture ဆီ ဦးတည်စေပါလိမ့်မယ်။

## LLMs တွေရဲ့ ဆင့်ကဲပြောင်းလဲမှု

Large Language Models တွေဟာ မကြာသေးခင်နှစ်များအတွင်းမှာ အလျင်အမြန် ဆင့်ကဲပြောင်းလဲလာခဲ့ပြီး မျိုးဆက်သစ်တိုင်းမှာ စွမ်းရည်တွေ သိသိသာသာ တိုးတက်လာခဲ့ပါတယ်။

## Attention Mechanisms[[attention-mechanisms]]

Transformer မော်ဒယ်အများစုက attention matrix က စတုရန်းပုံ (square) ဖြစ်တဲ့ full attention ကို အသုံးပြုပါတယ်။ စာသားတွေ ရှည်လျားလာတဲ့အခါမှာ ဒါဟာ တွက်ချက်မှုဆိုင်ရာ အဓိက အဟန့်အတားတစ်ခု ဖြစ်နိုင်ပါတယ်။ Longformer နဲ့ Reformer တို့ဟာ ပိုမိုထိရောက်အောင် ကြိုးစားပြီး လေ့ကျင့်မှုမြန်ဆန်စေရန် attention matrix ရဲ့ sparse version ကို အသုံးပြုကြပါတယ်။

> [!TIP]
> Standard attention mechanisms တွေမှာ computational complexity က O(n²) ရှိပါတယ်။ n ဆိုတာ sequence length ပါ။ ဒီဟာက ရှည်လျားလွန်းတဲ့ sequences တွေအတွက် ပြဿနာ ဖြစ်လာပါတယ်။ အောက်မှာဖော်ပြထားတဲ့ specialized attention mechanisms တွေက ဒီကန့်သတ်ချက်ကို ဖြေရှင်းပေးပါတယ်။

### LSH attention

[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer) က LSH attention ကို အသုံးပြုပါတယ်။ softmax(QK^t) မှာ QK^t matrix ရဲ့ အကြီးဆုံး elements (softmax dimension မှာ) တွေကသာ အသုံးဝင်တဲ့ အထောက်အကူတွေကို ပေးပါလိမ့်မယ်။ ဒါကြောင့် Q မှာရှိတဲ့ query q တစ်ခုစီအတွက် q နဲ့ နီးစပ်တဲ့ K မှာရှိတဲ့ key k တွေကိုသာ ထည့်သွင်းစဉ်းစားနိုင်ပါတယ်။ q နဲ့ k နီးစပ်မှုရှိမရှိကို ဆုံးဖြတ်ဖို့ hash function တစ်ခုကို အသုံးပြုပါတယ်။ attention mask ကို လက်ရှိ token ကို ဝှက်ထားဖို့ ပြင်ဆင်ထားပါတယ် (ပထမဆုံး position မှာမှလွဲ၍)။ ဘာလို့လဲဆိုတော့ ဒါဟာ query နဲ့ key ကို တူညီစေပြီး (ဒါကြောင့် အချင်းချင်း အလွန်ဆင်တူပါတယ်)။ hash က ကျပန်းဖြစ်နိုင်တာကြောင့် လက်တွေ့မှာ hash function အများအပြားကို အသုံးပြုပါတယ် (n_rounds parameter နဲ့ ဆုံးဖြတ်ပါတယ်) ပြီးတော့ ၎င်းတို့ကို ပျမ်းမျှယူပါတယ်။

### Local attention

[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) က local attention ကို အသုံးပြုပါတယ်- များသောအားဖြင့် ဒေသတွင်း အကြောင်းအရာ (ဥပမာ- ဘယ်ဘက်နဲ့ ညာဘက်က tokens နှစ်ခုက ဘာတွေလဲ) က ပေးထားတဲ့ token တစ်ခုအတွက် အရေးယူဖို့ လုံလောက်ပါတယ်။ ဒါ့အပြင် ပြတင်းပေါက်ငယ်တစ်ခုရှိတဲ့ attention layers တွေကို စုစည်းခြင်းဖြင့်၊ နောက်ဆုံး layer က ပြတင်းပေါက်ထဲရှိ tokens တွေထက် ပိုမိုကျယ်ပြန့်တဲ့ receptive field ကို ပိုင်ဆိုင်နိုင်ပြီး စာကြောင်းတစ်ခုလုံးကို ကိုယ်စားပြုတဲ့ အချက်အလက် (representation) ကို တည်ဆောက်နိုင်စေပါတယ်။

ကြိုတင်ရွေးချယ်ထားသော input tokens အချို့ကိုလည်း global attention ပေးထားပါတယ်။ ဒီ tokens အနည်းငယ်အတွက် attention matrix က tokens အားလုံးကို ဝင်ရောက်ကြည့်ရှုနိုင်ပြီး ဒီလုပ်ငန်းစဉ်က symmetric ဖြစ်ပါတယ်- အခြား tokens အားလုံးက အဲဒီသီးခြား tokens တွေကို ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ် (၎င်းတို့ရဲ့ local window ထဲမှာရှိတဲ့ tokens တွေအပြင်)။ ဒါကို စာတမ်းရဲ့ ပုံ 2d မှာ ပြသထားပါတယ်၊ အောက်မှာ attention mask ဥပမာကို ကြည့်ပါ။

<div class="flex justify-center">
    <img scale="50 %" align="center" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png"/>
</div>

parameters နည်းတဲ့ attention matrices တွေကို အသုံးပြုခြင်းအားဖြင့် မော်ဒယ်က sequence length ပိုကြီးတဲ့ inputs တွေကို လက်ခံနိုင်စေပါတယ်။

### Axial positional encodings

[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer) က axial positional encodings တွေကို အသုံးပြုပါတယ်။ ရိုးရာ Transformer မော်ဒယ်တွေမှာ positional encoding E က \\(l\\) by \\(d\\) matrix တစ်ခုဖြစ်ပြီး \\(l\\) က sequence length ဖြစ်ကာ \\(d\\) က hidden state ရဲ့ dimension ဖြစ်ပါတယ်။ စာသားတွေ အလွန်ရှည်လျားရင် ဒီ matrix က အလွန်ကြီးမားပြီး GPU ပေါ်မှာ နေရာအများကြီး ယူနိုင်ပါတယ်။ ဒါကို ဖြေလျှော့ဖို့အတွက် axial positional encodings တွေက အဲဒီကြီးမားတဲ့ matrix E ကို E1 နဲ့ E2 ဆိုတဲ့ သေးငယ်တဲ့ matrices နှစ်ခုအဖြစ် ခွဲထုတ်တာကို ဆိုလိုပါတယ်။ E1 နဲ့ E2 ရဲ့ dimensions တွေကတော့ \\(l_{1} \times d_{1}\\) နဲ့ \\(l_{2} \times d_{2}\\) ဖြစ်ပြီး \\(l_{1} \times l_{2} = l\\) နဲ့ \\(d_{1} + d_{2} = d\\) ဖြစ်ပါတယ်။ (အလျားတွေအတွက် မြှောက်လဒ်နဲ့ဆိုရင် ဒါက အများကြီး သေးငယ်သွားပါလိမ့်မယ်)။ E မှာရှိတဲ့ time step \\(j\\) အတွက် embedding ကို E1 မှာရှိတဲ့ time step \\(j \% l1\\) အတွက် embedding နဲ့ E2 မှာရှိတဲ့ time step \\(j // l1\\) အတွက် embedding တွေကို ပေါင်းစပ်ခြင်းဖြင့် ရရှိပါတယ်။

## နိဂုံးချုပ်[[conclusion]]

ဒီအပိုင်းမှာ ကျွန်တော်တို့ Transformer architectures သုံးမျိုးနဲ့ အထူးပြု attention mechanisms အချို့ကို လေ့လာခဲ့ပါတယ်။ ဒီ architecture တွေရဲ့ ကွာခြားချက်တွေကို နားလည်ထားတာက သင့်ရဲ့ သီးခြား NLP လုပ်ငန်းတာဝန်အတွက် မှန်ကန်တဲ့ မော်ဒယ်ကို ရွေးချယ်ဖို့အတွက် အရေးကြီးပါတယ်။

သင်တန်းမှာ ဆက်လက်လုပ်ဆောင်သွားတဲ့အခါ ဒီမတူညီတဲ့ architecture တွေနဲ့ လက်တွေ့အတွေ့အကြုံတွေ ရရှိလာမှာဖြစ်ပြီး သင့်ရဲ့ သီးခြားလိုအပ်ချက်တွေအတွက် ဘယ်လို fine-tune လုပ်ရမယ်ဆိုတာကို သင်ယူရမှာ ဖြစ်ပါတယ်။ နောက်အပိုင်းမှာတော့ ဒီမော်ဒယ်တွေမှာ ရှိနေတဲ့ ကန့်သတ်ချက်တွေနဲ့ ဘက်လိုက်မှုအချို့ကို လေ့လာသွားမှာဖြစ်ပြီး ၎င်းတို့ကို အသုံးပြုတဲ့အခါ သတိထားသင့်တဲ့ အချက်တွေပဲ ဖြစ်ပါတယ်။

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   ** Architecture**: ကွန်ပျူတာစနစ်တစ်ခု၊ ဆော့ဖ်ဝဲလ်တစ်ခု သို့မဟုတ် မော်ဒယ်တစ်ခု၏ အစိတ်အပိုင်းများ စုစည်းပုံနှင့် ၎င်းတို့အချင်းချင်း ဆက်စပ်လုပ်ဆောင်ပုံကို ဖော်ပြသည့် အခြေခံဒီဇိုင်း သို့မဟုတ် ဖွဲ့စည်းပုံ။
*   **Transformer Architecture**: Natural Language Processing (NLP) မှာ အောင်မြင်မှုများစွာရရှိခဲ့တဲ့ deep learning architecture တစ်မျိုးပါ။ ၎င်းတို့ဟာ စာသားတွေထဲက စကားလုံးတွေရဲ့ ဆက်နွယ်မှုတွေကို "attention mechanism" သုံးပြီး နားလည်အောင် သင်ကြားပေးပါတယ်။
*   **Encoder-only**: Transformer မော်ဒယ်ရဲ့ encoder အစိတ်အပိုင်းကိုသာ အသုံးပြုထားသော architecture အမျိုးအစား။ စာသားနားလည်မှုလုပ်ငန်းများအတွက် သင့်တော်သည်။
*   **Decoder-only**: Transformer မော်ဒယ်ရဲ့ decoder အစိတ်အပိုင်းကိုသာ အသုံးပြုထားသော architecture အမျိုးအစား။ စာသားဖန်တီးမှုလုပ်ငန်းများအတွက် သင့်တော်သည်။
*   **Encoder-Decoder (Sequence-to-sequence)**: Transformer မော်ဒယ်ရဲ့ encoder နှင့် decoder နှစ်ခုစလုံးကို အသုံးပြုထားသော architecture အမျိုးအစား။ စာသားတစ်ခုကို အခြားစာသားတစ်ခုအဖြစ် ပြောင်းလဲခြင်းလုပ်ငန်းများ (ဥပမာ- ဘာသာပြန်ခြင်း) အတွက် သင့်တော်သည်။
*   **Sentence Classification**: စာကြောင်းတစ်ခုလုံး၏ အဓိပ္ပာယ် သို့မဟုတ် ရည်ရွယ်ချက်ကို အမျိုးအစားခွဲခြားခြင်း (ဥပမာ- စိတ်ခံစားမှု၊ ခေါင်းစဉ်)။
*   **Named Entity Recognition (NER)**: စာသားထဲက လူအမည်၊ နေရာအမည်၊ အဖွဲ့အစည်းအမည် စတဲ့ သီးခြားအမည်တွေကို ရှာဖွေဖော်ထုတ်ခြင်း။
*   **Word Classification**: စာကြောင်းတစ်ခုရှိ စကားလုံးတစ်လုံးချင်းစီကို ၎င်း၏ သဒ္ဒါ သို့မဟုတ် အခြားအဓိပ္ပာယ်အရ အမျိုးအစားခွဲခြားခြင်း။
*   **Extractive Question Answering**: ပေးထားသော စာသားအပိုင်းအစမှ မေးခွန်း၏ အဖြေကို တိုက်ရိုက်ထုတ်ယူခြင်း။
*   **Bi-directional Attention**: မော်ဒယ်က စာသားတစ်ခုလုံး၏ အကြောင်းအရာ (context) ကို ရှေ့ဘက်နှင့် နောက်ဘက် နှစ်ဖက်စလုံးမှ ကြည့်ရှုနားလည်နိုင်ခြင်း။
*   **Auto-encoding Models**: စာသားကို ဖျက်ဆီးပြီးနောက် မူလစာသားကို ပြန်လည်တည်ဆောက်ရန် သင်ကြားထားသော မော်ဒယ်များ။
*   **BERT (Bidirectional Encoder Representations from Transformers)**: Google မှ တီထွင်ထားသော encoder-only Transformer မော်ဒယ်။
*   **DistilBERT**: BERT မော်ဒယ်ကို သေးငယ်ပြီး ပိုမိုမြန်ဆန်အောင် ပြုလုပ်ထားသော မော်ဒယ်။
*   **Text Generation**: AI မော်ဒယ်များကို အသုံးပြု၍ လူသားကဲ့သို့သော စာသားအသစ်များ ဖန်တီးခြင်း။
*   **Auto-regressive Models**: နောက်ထပ် token ကို ခန့်မှန်းရန် ယခင် tokens များကိုသာ အသုံးပြု၍ စာသားများကို တစ်ကြိမ်လျှင် token တစ်ခုချင်းစီ ဖန်တီးသော မော်ဒယ်များ။
*   **GPT (Generative Pre-trained Transformer)**: OpenAI မှ တီထွင်ထားသော decoder-only Transformer မော်ဒယ်။
*   **Llama**: Meta မှ တီထွင်ထားသော decoder-only Large Language Model (LLM) အမျိုးအစား။
*   **Gemma**: Google မှ တီထွင်ထားသော decoder-only Large Language Model (LLM) အမျိုးအစား။
*   **DeepSeek**: DeepSeek AI မှ တီထွင်ထားသော decoder-only Large Language Model (LLM) အမျိုးအစား။
*   **Pretraining**: မော်ဒယ်ကို များပြားလှသော အထွေထွေဒေတာများဖြင့် အစောပိုင်းသင်ကြားမှု။
*   **Instruction Tuning**: မော်ဒယ်ကို သီးခြားညွှန်ကြားချက်များကို လိုက်နာပြီး အထောက်အကူဖြစ်စေသော တုံ့ပြန်မှုများ ထုတ်လုပ်ရန် fine-tune လုပ်ခြင်း။
*   **Token**: စာသားကို ပိုင်းခြားထားသော အသေးငယ်ဆုံးယူနစ် (ဥပမာ- စကားလုံး၊ စာလုံးအစိတ်အပိုင်း)။
*   **Summarization**: စာသားရှည်များကို အကျဉ်းချုပ်ဖော်ပြခြင်း။
*   **Translation**: ဘာသာစကားတစ်ခုမှ အခြားတစ်ခုသို့ စာသားများကို ပြောင်းလဲခြင်း။
*   **Generative Question Answering**: မေးခွန်း၏ အဖြေကို ပေးထားသော အကြောင်းအရာ (context) အပေါ် အခြေခံ၍ စာသားအသစ်များ ဖန်တီးခြင်းဖြင့် ထုတ်ပေးခြင်း။
*   **BART (Bidirectional and Auto-Regressive Transformers)**: Encoder-Decoder Transformer မော်ဒယ်တစ်မျိုး။
*   **T5 (Text-to-Text Transfer Transformer)**: Encoder-Decoder Transformer မော်ဒယ်တစ်မျိုးဖြစ်ပြီး လုပ်ငန်းတာဝန်အားလုံးကို "text-to-text" ပုံစံဖြင့် ဖြေရှင်းရန် ဒီဇိုင်းထုတ်ထားသည်။
*   **Marian**: အဓိကအားဖြင့် machine translation အတွက် အသုံးပြုသော encoder-decoder မော်ဒယ်။
*   **mBART**: Multilingual BART (ဘာသာစကားမျိုးစုံအတွက် BART)။
*   **Data-to-text Generation**: ဖွဲ့စည်းထားသော ဒေတာများကို သဘာဝဘာသာစကားစာသားအဖြစ် ပြောင်းလဲခြင်း။
*   **Grammar Correction**: စာသားရှိ သဒ္ဒါအမှားများကို ပြင်ဆင်ခြင်း။
*   **Conversational AI**: လူသားများနှင့် သဘာဝဘာသာစကားဖြင့် အပြန်အလှန်ပြောဆိုနိုင်သော AI စနစ်များ။
*   **RoBERTa**: BERT ကို ပိုမိုကောင်းမွန်အောင် လေ့ကျင့်ထားသော encoder-only မော်ဒယ်။
*   **Attention Matrix**: Transformer မော်ဒယ်များတွင် အသုံးပြုသော matrix တစ်ခုဖြစ်ပြီး input sequence အတွင်းရှိ token များအချင်းချင်း မည်မျှဆက်စပ်နေသည်ကို ဖော်ပြသည်။
*   **Computational Bottleneck**: စနစ်တစ်ခု၏ စွမ်းဆောင်ရည်ကို ကန့်သတ်ထားသော အရင်းအမြစ် သို့မဟုတ် လုပ်ငန်းစဉ်။
*   **Sparse Attention**: attention matrix ၏ အရေးမကြီးသော အစိတ်အပိုင်းများကို လျစ်လျူရှုခြင်းဖြင့် တွက်ချက်မှု ထိရောက်အောင် ပြုလုပ်ထားသော attention mechanism အမျိုးအစား။
*   **LSH (Locality Sensitive Hashing) Attention**: Reformer မော်ဒယ်တွင် အသုံးပြုသော attention အမျိုးအစားဖြစ်ပြီး ဆင်တူသော query နှင့် key များကို ရှာဖွေရန် hash function များကို အသုံးပြုသည်။
*   **Longformer**: ရှည်လျားသော input sequences များကို ကိုင်တွယ်နိုင်ရန် local attention နှင့် global attention တို့ကို ပေါင်းစပ်အသုံးပြုထားသော Transformer မော်ဒယ်။
*   **Local Attention**: ပေးထားသော token တစ်ခုအတွက် အနီးအနားရှိ tokens များကိုသာ အာရုံစိုက်သော attention mechanism။
*   **Receptive Field**: neural network layer တစ်ခု၏ output ယူနစ်တစ်ခုကို လွှမ်းမိုးသော input data ၏ အရွယ်အစား။
*   **Global Attention**: အချို့သော input tokens များအတွက် input sequence ရှိ tokens အားလုံးကို အာရုံစိုက်ခွင့်ပြုသော attention mechanism။
*   **Axial Positional Encodings**: ရှည်လျားသော sequences များအတွက် positional encoding ကို ပိုမိုထိရောက်အောင် ပြုလုပ်ရန် matrix တစ်ခုကို သေးငယ်သော matrices နှစ်ခုအဖြစ် ခွဲထုတ်ခြင်းနည်းလမ်း။
*   **Hidden State**: Transformer မော်ဒယ်များတွင် layer တစ်ခုမှ အခြားတစ်ခုသို့ လက်ဆင့်ကမ်းပေးသော အတွင်းပိုင်း ကိုယ်စားပြုအချက်အလက်။
*   **Dimension**: vector သို့မဟုတ် matrix တစ်ခု၏ အတိုင်းအတာအရေအတွက်။