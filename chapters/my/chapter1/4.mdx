# Transformers များ ဘယ်လိုအလုပ်လုပ်သလဲ။

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

ဤအပိုင်းတွင် Transformer မော်ဒယ်များ၏ ဗိသုကာလက်ရာကို high-level မှ ကြည့်ရှုပါမည်။

## Transformer ရာဇဝင်အကျဉ်း

Transformer မော်ဒယ်များ၏ (တိုတောင်းသော) ရာဇဝင်တွင် အောက်ပါ အချက်အလက်များ ပါဝင်သည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg"   
 alt="Transformers   
 မော်ဒယ်များ၏ ရာဇဝင်အကျဉ်း">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg"   
  
 alt="Transformers မော်ဒယ်များ၏ ရာဇဝင်အကျဉ်း">
</div>

[Transformer architecture] (https://arxiv.org/abs/1706.03762) ကို ၂၀၁၇ ခုနှစ် ဇွန်လတွင် မိတ်ဆက်ခဲ့သည်။ မူလသုတေသန၏ အဓိကအာရုံစိုက်မှုမှာ ဘာသာပြန်လုပ်ငန်းများပေါ်တွင်ဖြစ်သည်။ ၎င်းနောက်တွင် သြဇာညောင်းသော မော်ဒယ်များစွာကို မိတ်ဆက်ခဲ့ပြီး အောက်ပါတို့ ပါဝင်သည်။

- **၂၀၁၈ ခုနှစ် ဇွန်လ။** [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)၊ ပထမဆုံး ကြိုတင်လေ့ကျင့်ထားသော Transformer မော်ဒယ်၊ NLP လုပ်ငန်းအမျိုးမျိုးတွင် fine-tuning ပြုလုပ်ရန် အသုံးပြုခဲ့ပြီး state-of-the-art ရလဒ်များ ရရှိခဲ့သည်။

- **၂၀၁၈ ခုနှစ် အောက်တိုဘာလ။** [BERT](https://arxiv.org/abs/1810.04805)၊ နောက်ထပ် ကြီးမားသော ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်၊ ၎င်းသည် စာကြောင်းများ၏ အနှစ်ချုပ်များ(summaries)ကို ပိုမိုကောင်းမွန်စွာ ထုတ်လုပ်ရန် ဒီဇိုင်းထုတ်ထားသည် (နောက်အခန်းတွင် ပိုမိုဖော်ပြပါမည်။)

- **၂၀၁၉ ခုနှစ် ဖေဖော်ဝါရီလ။** [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)၊ GPT ၏ ပိုမိုကောင်းမွန်သော (နှင့် ပိုကြီးသော) ဗားရှင်း၊ ကျင့်ဝတ်ဆိုင်ရာ စိုးရိမ်မှုများကြောင့် ချက်ချင်း အများပြည်သူသို့ ထုတ်ပြန်ခြင်း မပြုခဲ့ပါ။

- **၂၀၁၉ ခုနှစ် အောက်တိုဘာလ။** [DistilBERT](https://arxiv.org/abs/1910.01108)၊ BERT ၏ distilled ဗားရှင်း၊ ၎င်းသည် 60% ပိုမိုမြန်ဆန်ပြီး memory တွင် 40% ပိုမိုပေါ့ပါးပြီး BERT ၏ စွမ်းဆောင်ရည်၏ 97% ကို ဆက်လက်ထိန်းသိမ်းထားသည်။

- **၂၀၁၉ ခုနှစ် အောက်တိုဘာလ။** [BART](https://arxiv.org/abs/1910.13461) နှင့် [T5](https://arxiv.org/abs/1910.10683)၊ မူလ Transformer မော်ဒယ်ကဲ့သို့ ဗိသုကာလက်ရာတူညီသော ကြီးမားသော ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်နှစ်ခု (ထိုသို့ပြုလုပ်သည့် ပထမဆုံးမော်ဒယ်များ)

- **၂၀၂၀ ခုနှစ် မေလ။** [GPT-3](https://arxiv.org/abs/2005.14165)၊ GPT-2 ၏ ပိုမိုကြီးမားသော ဗားရှင်း၊ fine-tuning မလိုအပ်ဘဲ လုပ်ငန်းအမျိုးမျိုးတွင် ကောင်းမွန်စွာ လုပ်ဆောင်နိုင်သည် (_zero-shot learning_ ဟုခေါ်သည်)

ဤစာရင်းသည် အပြည့်အစုံမဟုတ်ပါ၊ Transformer မော်ဒယ် အမျိုးအစားအချို့ကိုသာ မီးမောင်းထိုးပြ(ဖော်ပြ)ရန် ရည်ရွယ်ပါသည်။ ယေဘုယျအားဖြင့် ၎င်းတို့ကို အမျိုးအစား သုံးမျိုး ခွဲခြားနိုင်သည်။

- GPT ကဲ့သို့ ( _auto-regressive_ Transformer မော်ဒယ်များ ဟုလည်းခေါ်သည်)
- BERT ကဲ့သို့ ( _auto-encoding_ Transformer မော်ဒယ်များ ဟုလည်းခေါ်သည်)
- BART/T5 ကဲ့သို့ ( _sequence-to-sequence_ Transformer မော်ဒယ်များ ဟုလည်းခေါ်သည်)

နောက်ပိုင်းတွင် ဤအမျိုးအစားများကို ပိုမိုနက်ရှိုင်းစွာ လေ့လာပါမည်။

## Transformers များသည် language မော်ဒယ်များ ဖြစ်သည်

အထက်တွင် ဖော်ပြခဲ့သော Transformer မော်ဒယ်အားလုံး (GPT၊ BERT၊ BART၊ T5 စသည်) ကို *language မော်ဒယ်များ* အဖြစ် လေ့ကျင့်ပေးထားသည်။ ဆိုလိုသည်မှာ ၎င်းတို့ကို ကြီးမားသော ပမာဏရှိသော ကုန်ကြမ်းစာသားများဖြင့် self-supervised နည်းလမ်းဖြင့် လေ့ကျင့်ပေးထားခြင်း ဖြစ်သည်။ Self-supervised learning သည် ရည်ရွယ်ချက်ကို မော်ဒယ်၏ input များမှ အလိုအလျောက် တွက်ချက်သည့် လေ့ကျင့်မှု အမျိုးအစားတစ်ခုဖြစ်သည်။ ဆိုလိုသည်မှာ လူသားများသည် ဒေတာကို label လုပ်ရန် မလိုအပ်ပါ။

ဤအမျိုးအစား မော်ဒယ်သည် ၎င်းကို လေ့ကျင့်ပေးထားသော ဘာသာစကား၏ စာရင်းအင်းဆိုင်ရာ(statistical) နားလည်မှုကို တီထွင်ထားသော်လည်း ၎င်းသည် သီးခြား လက်တွေ့ကျသော လုပ်ငန်းများအတွက် အသုံးမဝင်ပါ။ ထို့ကြောင့်၊ ယေဘုယျ ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်သည် *transfer learning* ဟုခေါ်သော လုပ်ငန်းစဉ်ကို ဖြတ်သန်းသည်။ ဤလုပ်ငန်းစဉ်အတွင်း မော်ဒယ်ကို supervised နည်းလမ်းဖြင့် - ဆိုလိုသည်မှာ လူသားများ annotation ပြုလုပ်ထားသော label များကို အသုံးပြု၍ - ပေးထားသော လုပ်ငန်းတစ်ခုတွင် fine-tune လုပ်သည်။

လုပ်ငန်းတစ်ခု၏ ဥပမာမှာ *n* ယခင်စကားလုံးများကို ဖတ်ပြီး စာကြောင်းတွင် နောက်ထပ်စကားလုံးကို ခန့်မှန်းခြင်းဖြစ်သည်။ ၎င်းကို *causal language modeling* ဟုခေါ်သည်၊ အဘယ်ကြောင့်ဆိုသော် output သည် ယခင်နှင့် လက်ရှိ input များပေါ်တွင် မူတည်သော်လည်း အနာဂတ် input များပေါ်တွင် မူတည်ခြင်း မဟုတ်ပါ။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="စာကြောင်းတစ်ကြောင်းမှ နောက်ထပ်စကားလုံးကို ခန့်မှန်းသည့် causal language modeling ၏ ဥပမာ">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="စာကြောင်းတစ်ကြောင်းမှ နောက်ထပ်စကားလုံးကို ခန့်မှန်းသည့် causal language modeling ၏ ဥပမာ">
</div>

နောက်ထပ် ဥပမာတစ်ခုမှာ *masked language modeling* ဖြစ်ပြီး မော်ဒယ်သည် စာကြောင်းတွင် mask လုပ်ထားသော စကားလုံးကို ခန့်မှန်းသည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="စာကြောင်းတစ်ကြောင်းမှ mask လုပ်ထားသော စကားလုံးကို ခန့်မှန်းသည့် masked language modeling ၏ ဥပမာ">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="စာကြောင်းတစ်ကြောင်းမှ mask လုပ်ထားသော စကားလုံးကို ခန့်မှန်းသည့် masked language modeling ၏ ဥပမာ">
</div>

## Transformers များသည် ကြီးမားသော မော်ဒယ်များ ဖြစ်သည်

အနည်းငယ်သော ခြွင်းချက်များ (DistilBERT ကဲ့သို့) မှလွဲ၍ ပိုမိုကောင်းမွန်သော စွမ်းဆောင်ရည်ကို ရရှိရန် ယေဘုယျ ဗျူဟာမှာ မော်ဒယ်များ၏ အရွယ်အစားအပြင် ၎င်းတို့ကို ကြိုတင်လေ့ကျင့်ပေးသည့် ဒေတာပမာဏကို တိုးမြှင့်ခြင်း ဖြစ်သည်။

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="မကြာသေးမီက   
 Transformers မော်ဒယ်များ၏ parameter အရေအတွက်" width="90%">
</div>

ကံမကောင်းစွာဖြင့်၊ မော်ဒယ်တစ်ခုကို လေ့ကျင့်ခြင်း၊ အထူးသဖြင့် ကြီးမားသော မော်ဒယ်တစ်ခုကို လေ့ကျင့်ခြင်းသည် ဒေတာများစွာ လိုအပ်သည်။ ၎င်းသည် အချိန်နှင့် တွက်ချက်မှု အရင်းအမြစ်များအရ အလွန်စျေးကြီးသည်။ အောက်ပါ ဂရပ်တွင် မြင်တွေ့ရသည့်အတိုင်း ပတ်ဝန်းကျင်ဆိုင်ရာ သက်ရောက်မှုကိုပင် ဖြစ်ပေါ်စေသည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg"   
  
 alt="ကြီးမားသော language မော်ဒယ်၏ ကာဗွန်ခြေရာ">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg"   
  
 alt="ကြီးမားသော language မော်ဒယ်၏ ကာဗွန်ခြေရာ">
</div>

ပြီးတော့ ဒါက ကြိုတင်လေ့ကျင့်ရေးရဲ့ ပတ်ဝန်းကျင်ဆိုင်ရာ သက်ရောက်မှုကို လျှော့ချဖို့ ကြိုးစားနေတဲ့ အဖွဲ့တစ်ဖွဲ့က ဦးဆောင်တဲ့ (အရမ်းကြီးတဲ့) မော်ဒယ်တစ်ခုအတွက် ပရောဂျက်တစ်ခုကို ပြသနေတာပါ။ အကောင်းဆုံး hyperparameter များရရှိရန် ሙከራများစွာကို လုပ်ဆောင်ခြင်း၏ သက်ရောက်မှုသည် ပိုမိုမြင့်မားလိမ့်မည်။

သုတေသနအဖွဲ့၊ ကျောင်းသားအဖွဲ့အစည်း သို့မဟုတ် ကုမ္ပဏီတစ်ခုသည် မော်ဒယ်တစ်ခုကို လေ့ကျင့်လိုသည့်အခါတိုင်း ၎င်းကို သုညမှ စတင်လေ့ကျင့်ပါက ကမ္ဘာလုံးဆိုင်ရာ ကုန်ကျစရိတ်များစွာ မလိုအပ်ဘဲ ဖြစ်ပေါ်လာလိမ့်မည်ဟု မြင်ယောင်ကြည့်ပါ။

ဒါကြောင့် language မော်ဒယ်များကို မျှဝေခြင်းက အရေးကြီးပါတယ်။ လေ့ကျင့်ပြီးသား weight များကို မျှဝေခြင်းနှင့် လေ့ကျင့်ပြီးသား weight များအပေါ်တွင် တည်ဆောက်ခြင်းသည် အသိုင်းအဝိုင်း၏ စုစုပေါင်း တွက်ချက်မှု ကုန်ကျစရိတ်နှင့် ကာဗွန်ခြေရာကို လျှော့ချပေးသည်။

စကားမစပ်၊ သင်၏မော်ဒယ်များ၏ လေ့ကျင့်မှု၏ ကာဗွန်ခြေရာကို tool များစွာဖြင့် အကဲဖြတ်နိုင်သည်။ ဥပမာ [ML CO2 Impact](https://mlco2.github.io/impact/) သို့မဟုတ် 🤗 Transformers တွင် ပေါင်းစပ်ထားသော [Code Carbon](https://codecarbon.io/) ။ ၎င်းအကြောင်း ပိုမိုလေ့လာရန် သင်၏လေ့ကျင့်မှု၏ ခြေရာကို ခန့်မှန်းခြေဖြင့် `emissions.csv` ဖိုင်တစ်ခု မည်သို့ထုတ်လုပ်ရမည်ကို ပြသမည့် ဤ [ဘလော့ဂ်ပို့စ်] (https://huggingface.co/blog/carbon-emissions-on-the-hub) နှင့် ဤခေါင်းစဉ်ကို ဖော်ပြသည့် 🤗 Transformers ၏ [စာရွက်စာတမ်း] (https://huggingface.co/docs/hub/model-cards-co2) ကို ဖတ်ရှုနိုင်ပါသည်။

## Transfer Learning[[transfer-learning]]

<Youtube id="BqqfQnyjmgg" />

*Pretraining* ဆိုသည်မှာ မော်ဒယ်တစ်ခုကို သုညမှ စတင်လေ့ကျင့်ခြင်း ဖြစ်သည်။ weight များကို ကျပန်း စတင်သတ်မှတ်ပြီး မည်သည့် ကြိုတင်ဗဟုသုတမှ မပါဘဲ လေ့ကျင့်မှု စတင်သည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg"   
 alt="Language မော်ဒယ်၏ ကြိုတင်လေ့ကျင့်မှုသည် အချိန်နှင့်ငွေနှစ်မျိုးလုံးတွင် ကုန်ကျစရိတ်များသည်။">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="Language   
 မော်ဒယ်၏ ကြိုတင်လေ့ကျင့်မှုသည် အချိန်နှင့်ငွေနှစ်မျိုးလုံးတွင် ကုန်ကျစရိတ်များသည်။">
</div>

ဤ pretraining ကို ပုံမှန်အားဖြင့် အလွန်ကြီးမားသော ဒေတာပမာဏဖြင့် ပြုလုပ်လေ့ရှိသည်။ ထို့ကြောင့် ၎င်းသည် အလွန်ကြီးမားသော ဒေတာ corpus တစ်ခု လိုအပ်ပြီး လေ့ကျင့်မှုသည် ရက်သတ္တပတ် အတော်ကြာ ကြာနိုင်သည်။

တစ်ဖက်တွင် *Fine-tuning* သည် မော်ဒယ်ကို ကြိုတင်လေ့ကျင့်ပြီးနောက် ပြုလုပ်သော လေ့ကျင့်မှုဖြစ်သည်။ fine-tuning ပြုလုပ်ရန် သင်သည် ဦးစွာ ကြိုတင်လေ့ကျင့်ထားသော language မော်ဒယ်ကို ရယူပြီးနောက် သင်၏လုပ်ငန်းနှင့် သက်ဆိုင်သော ဒေတာစုဖြင့် နောက်ထပ်လေ့ကျင့်မှုကို လုပ်ဆောင်သည်။ စောင့်ပါ - အစမှ (**သုည**) မှ သင်၏နောက်ဆုံးအသုံးပြုမှုအတွက် မော်ဒယ်ကို လေ့ကျင့်ရန် အဘယ်ကြောင့် မလုပ်ဆောင်သနည်း။ အကြောင်းရင်း အနည်းငယ်ရှိပါသည်။

* ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်ကို fine-tuning ဒေတာစုနှင့် ဆင်တူမှုအချို့ရှိသော ဒေတာစုတွင် လေ့ကျင့်ပြီးသားဖြစ်သည်။ ထို့ကြောင့် fine-tuning လုပ်ငန်းစဉ်သည် ကြိုတင်လေ့ကျင့်မှုအတွင်း ကနဦးမော်ဒယ်မှ ရရှိသော ဗဟုသုတကို အခွင့်ကောင်းယူနိုင်သည် (ဥပမာ၊ NLP ပြဿနာများဖြင့် ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်သည် သင်၏လုပ်ငန်းအတွက် အသုံးပြုနေသော ဘာသာစကား၏ စာရင်းအင်းဆိုင်ရာ နားလည်မှု အမျိုးအစားတစ်ခု ရှိလိမ့်မည်)။
* ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်ကို ဒေတာများစွာဖြင့် လေ့ကျင့်ပြီးသားဖြစ်သောကြောင့် fine-tuning သည် ကောင်းမွန်သော ရလဒ်များရရှိရန် ဒေတာ နည်းပါးစွာသာ လိုအပ်သည်။
* အကြောင်းရင်းချင်း တူညီသောကြောင့် ကောင်းမွန်သော ရလဒ်များ ရရှိရန် လိုအပ်သော အချိန်နှင့် အရင်းအမြစ် ပမာဏသည် သိသိသာသာ နည်းပါးသည်။

ဥပမာအားဖြင့် အင်္ဂလိပ်ဘာသာဖြင့် လေ့ကျင့်ထားသော ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်ကို အသုံးပြုနိုင်ပြီး ၎င်းကို arXiv corpus တွင် fine-tune လုပ်နိုင်သည်၊ ရလဒ်အနေဖြင့် သိပ္ပံ/သုတေသနအခြေခံ မော်ဒယ်တစ်ခု ဖြစ်လာသည်။ fine-tuning သည် ဒေတာ အကန့်အသတ်ရှိသော ပမာဏကိုသာ လိုအပ်လိမ့်မည်။ ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်မှ ရရှိထားသော ဗဟုသုတကို "လွှဲပြောင်း" ပေးသည်၊ ထို့ကြောင့် *transfer learning* ဟူသော ဝေါဟာရကို အသုံးပြုသည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="Language မော်ဒယ်ကို fine-tuning လုပ်ခြင်းသည် ကြိုတင်လေ့ကျင့်ခြင်းထက် အချိန်နှင့်ငွေနှစ်မျိုးလုံးတွင် စျေးသက်သာသည်။">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="Language မော်ဒယ်ကို fine-tuning လုပ်ခြင်းသည် ကြိုတင်လေ့ကျင့်ခြင်းထက် အချိန်နှင့်ငွေနှစ်မျိုးလုံးတွင် စျေးသက်သာသည်။">
</div>

ထို့ကြောင့် မော်ဒယ်တစ်ခုကို fine-tuning လုပ်ခြင်းသည် အချိန်၊ ဒေတာ၊ ငွေကြေးနှင့် ပတ်ဝန်းကျင်ဆိုင်ရာ ကုန်ကျစရိတ်များ လျော့နည်းစေသည်။ လေ့ကျင့်မှုသည် အပြည့်အဝ ကြိုတင်လေ့ကျင့်ခြင်းထက် ပိုမိုလွယ်ကူသောကြောင့် မတူညီသော fine-tuning ပုံစံများကို ပြန်လည်လုပ်ဆောင်ရန်လည်း ပိုမိုမြန်ဆန်လွယ်ကူသည်။

ဤလုပ်ငန်းစဉ်သည် သုညမှ လေ့ကျင့်ခြင်းထက် ပိုမိုကောင်းမွန်သော ရလဒ်များကို ရရှိစေမည်ဖြစ်သည် (သင့်မှာ ဒေတာများစွာ မရှိပါက)၊ ထို့ကြောင့် သင်သည် ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်ကို အမြဲတမ်း အသုံးပြုရန် ကြိုးစားသင့်သည် - သင်လုပ်ဆောင်ရမည့် လုပ်ငန်းနှင့် တတ်နိုင်သမျှ နီးစပ်သော တစ်ခု - နှင့် ၎င်းကို fine-tune လုပ်ပါ။

## ယေဘုယျ architecture

ဤအပိုင်းတွင် Transformer မော်ဒယ်၏ ယေဘုယျ architecture ကို လေ့လာပါမည်။ အယူအဆအချို့ကို သင်နားမလည်ပါက စိတ်မပူပါနှင့်။ အစိတ်အပိုင်းတစ်ခုစီကို လွှမ်းခြုံထားသော အသေးစိတ် အပိုင်းများကို နောက်ပိုင်းတွင် ဖော်ပြထားပါသည်။

<Youtube id="H39Z_720T5s" />

## မိတ်ဆက်

မော်ဒယ်ကို အဓိကအားဖြင့် block နှစ်ခုဖြင့် ဖွဲ့စည်းထားသည်။

* **Encoder (ဘယ်ဘက်)**: encoder သည် input တစ်ခုကို လက်ခံရရှိပြီး ၎င်း၏ ကိုယ်စားပြုမှု (၎င်း၏ features) ကို တည်ဆောက်သည်။ ဆိုလိုသည်မှာ မော်ဒယ်ကို input မှ နားလည်မှု ရရှိရန် ပိုမိုကောင်းမွန်အောင် ပြုလုပ်ထားခြင်း ဖြစ်သည်။
* **Decoder (ညာဘက်)**: decoder သည် encoder ၏ ကိုယ်စားပြုမှု (features) နှင့်အတူ အခြား input များကို အသုံးပြု၍ target sequence တစ်ခုကို ထုတ်လုပ်သည်။ ဆိုလိုသည်မှာ မော်ဒယ်ကို output များ ထုတ်လုပ်ရန် ပိုမိုကောင်းမွန်အောင် ပြုလုပ်ထားခြင်း ဖြစ်သည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Transformers မော်ဒယ်များ၏ architecture">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Transformers မော်ဒယ်များ၏ architecture">
</div>

ဤအစိတ်အပိုင်းတစ်ခုစီကို လုပ်ငန်းပေါ်မူတည်၍ သီးခြားစီ အသုံးပြုနိုင်သည်။

* **Encoder-only မော်ဒယ်များ**: စာကြောင်း အမျိုးအစားခွဲခြားခြင်းနှင့် အမည်ပေး entity recognition ကဲ့သို့သော input ကို နားလည်ရန် လိုအပ်သည့် လုပ်ငန်းများအတွက် ကောင်းမွန်သည်။
* **Decoder-only မော်ဒယ်များ**: စာသားထုတ်လုပ်ခြင်းကဲ့သို့သော ထုတ်လုပ်မှု လုပ်ငန်းများအတွက် ကောင်းမွန်သည်။
* **Encoder-decoder မော်ဒယ်များ** သို့မဟုတ် **sequence-to-sequence မော်ဒယ်များ**: ဘာသာပြန်ခြင်း သို့မဟုတ် အနှစ်ချုပ်ရေးသားခြင်းကဲ့သို့သော input တစ်ခု လိုအပ်သည့် ထုတ်လုပ်မှု လုပ်ငန်းများအတွက် ကောင်းမွန်သည်။

နောက်ပိုင်း အပိုင်းများတွင် ဤ architecture များကို သီးခြားစီ လေ့လာပါမည်။

## Attention layer များ

Transformer မော်ဒယ်များ၏ အဓိကအင်္ဂါရပ်မှာ *attention layer* များဟုခေါ်သော အထူး layer များဖြင့် တည်ဆောက်ထားခြင်းဖြစ်သည်။ တကယ်တော့ Transformer architecture ကို မိတ်ဆက်သည့် စာတမ်း၏ ခေါင်းစဉ်မှာ ["Attention Is All You Need"] (https://arxiv.org/abs/1706.03762) ဖြစ်သည်။ သင်တန်း၏ နောက်ပိုင်းအပိုင်းများတွင် attention layer များ၏ အသေးစိတ်ကို ကျွန်ုပ်တို့ လေ့လာပါမည်။ လောလောဆယ်တွင် သင်သိထားရန် လိုအပ်သည်မှာ ဤ layer သည် စကားလုံးတစ်လုံးချင်းစီ၏ ကိုယ်စားပြုမှုကို ကိုင်တွယ်သည့်အခါ သင်ပေးပို့သော စာကြောင်းရှိ စကားလုံးအချို့ကို အထူးဂရုပြုရန် (နှင့် အခြားစကားလုံးများကို လျစ်လျူရှုရန်) မော်ဒယ်ကို ပြောပြလိမ့်မည် ဖြစ်သည်။

၎င်းကို context ထဲသို့ ထည့်သွင်းရန် အင်္ဂလိပ်မှ ပြင်သစ်သို့ စာသားကို ဘာသာပြန်ဆိုသည့် လုပ်ငန်းကို စဉ်းစားပါ။ "You like this course" ဟူသော input ကို ပေးထားလျှင် ဘာသာပြန်မော်ဒယ်သည် "like" စကားလုံးအတွက် မှန်ကန်သော ဘာသာပြန်ဆိုချက်ကို ရရှိရန် ကပ်လျက်စကားလုံး "You" ကိုလည်း ဂရုပြုရန် လိုအပ်မည်ဖြစ်သည်၊ အဘယ်ကြောင့်ဆိုသော် ပြင်သစ်ဘာသာတွင် "like" ကြိယာကို subject ပေါ်မူတည်၍ မတူညီသော ပုံစံဖြင့် ပေါင်းစပ်ထားသောကြောင့်ဖြစ်သည်။ သို့သော် စာကြောင်း၏ ကျန်အပိုင်းသည် ထိုစကားလုံးကို ဘာသာပြန်ရန်အတွက် အသုံးမဝင်ပါ။ အလားတူပင် "this" ကို ဘာသာပြန်သည့်အခါ မော်ဒယ်သည် "course" စကားလုံးကိုလည်း ဂရုပြုရန် လိုအပ်မည်ဖြစ်သည်၊ အဘယ်ကြောင့်ဆိုသော် ဆက်စပ်နာမ်သည် masculine သို့မဟုတ် feminine ဖြစ်မဖြစ်ပေါ် မူတည်၍ "this" သည် မတူညီသော ပုံစံဖြင့် ဘာသာပြန်သောကြောင့်ဖြစ်သည်။ တစ်ဖန် စာကြောင်းရှိ အခြားစကားလုံးများသည် "course" ကို ဘာသာပြန်ရန်အတွက် အရေးမကြီးပါ။ ပိုမိုရှုပ်ထွေးသော စာကြောင်းများ (နှင့် ပိုမိုရှုပ်ထွေးသော သဒ္ဒါစည်းမျဉ်းများ) ဖြင့် မော်ဒယ်သည် စကားလုံးတစ်လုံးချင်းစီကို မှန်ကန်စွာ ဘာသာပြန်ရန် စာကြောင်းတွင် ဝေးဝေးတွင် ပေါ်လာနိုင်သော စကားလုံးများကို အထူးဂရုပြုရန် လိုအပ်လိမ့်မည်။

သဘာဝဘာသာစကားနှင့် ဆက်စပ်သော မည်သည့်လုပ်ငန်းတွင်မဆို အယူအဆတူညီသည်။ စကားလုံးတစ်လုံးတည်းသည် အဓိပ္ပါယ်တစ်ခုရှိသော်လည်း ထိုအဓိပ္ပါယ်ကို context မှ နက်ရှိုင်းစွာ သက်ရောက်မှုရှိသည်၊ ၎င်းသည် လေ့လာနေသော စကားလုံး၏ ရှေ့ သို့မဟုတ် နောက်ရှိ အခြားစကားလုံး (သို့မဟုတ် စကားလုံးများ) မည်သည့်စကားလုံးမဆို ဖြစ်နိုင်သည်။

ယခု သင်သည် attention layer များသည် ဘာအကြောင်းဖြစ်သည်ကို သိရှိပြီးဖြစ်သောကြောင့် Transformer architecture ကို ပိုမိုနီးကပ်စွာ လေ့လာကြည့်ကြပါစို့။

## မူလ architecture

Transformer architecture ကို မူလက ဘာသာပြန်ခြင်းအတွက် ဒီဇိုင်းထုတ်ခဲ့သည်။ လေ့ကျင့်မှုအတွင်း encoder သည် သတ်မှတ်ထားသော ဘာသာစကားဖြင့် input (စာကြောင်းများ) ကို လက်ခံရရှိပြီး decoder သည် လိုချင်သော target ဘာသာစကားဖြင့် တူညီသော စာကြောင်းများကို လက်ခံရရှိသည်။ encoder တွင် attention layer များသည် စာကြောင်းရှိ စကားလုံးအားလုံးကို အသုံးပြုနိုင်သည် (ကျွန်ုပ်တို့ မြင်တွေ့ခဲ့ရသည့်အတိုင်း ပေးထားသော စကားလုံးတစ်လုံး၏ ဘာသာပြန်ဆိုချက်သည် စာကြောင်းတွင် ၎င်းမတိုင်မီနှင့် ၎င်းပြီးနောက် အရာများပေါ်တွင် မူတည်နိုင်သည်)။ သို့သော် decoder သည် အစဉ်လိုက် အလုပ်လုပ်ပြီး ဘာသာပြန်ပြီးသား စာကြောင်းရှိ စကားလုံးများကိုသာ ဂရုပြုနိုင်သည် (ထို့ကြောင့် လက်ရှိ ထုတ်လုပ်နေသော စကားလုံးမတိုင်မီ စကားလုံးများကိုသာ)။ ဥပမာအားဖြင့် ဘာသာပြန်ထားသော target ၏ ပထမစကားလုံးသုံးလုံးကို ကျွန်ုပ်တို့ ခန့်မှန်းပြီးသောအခါ ၎င်းတို့ကို decoder သို့ ပေးသည်၊ ထို့နောက် စတုတ္ထစကားလုံးကို ခန့်မှန်းရန် ကြိုးစားရန် encoder ၏ input အားလုံးကို အသုံးပြုသည်။

လေ့ကျင့်မှုအတွင်း အရာများကို မြန်ဆန်စေရန် (မော်ဒယ်သည် target စာကြောင်းများကို ဝင်ရောက်ကြည့်ရှုနိုင်သောအခါ) decoder ကို target တစ်ခုလုံးကို ပေးသော်လည်း အနာဂတ်စကားလုံးများကို အသုံးပြုခွင့် မပြုပါ (၎င်းသည် နေရာ 2 ရှိ စကားလုံးကို ခန့်မှန်းရန် ကြိုးစားသည့်အခါ နေရာ 2 ရှိ စကားလုံးကို ဝင်ရောက်ကြည့်ရှုခွင့် ရှိပါက ပြဿနာသည် သိပ်မခက်ခဲပါ။)။ ဥပမာအားဖြင့် စတုတ္ထစကားလုံးကို ခန့်မှန်းရန် ကြိုးစားသည့်အခါ attention layer သည် နေရာ 1 မှ 3 အထိ စကားလုံးများကိုသာ ဝင်ရောက်ကြည့်ရှုနိုင်မည်ဖြစ်သည်။

မူလ Transformer architecture သည် ဘယ်ဘက်တွင် encoder နှင့် ညာဘက်တွင် decoder ဖြင့် ဤကဲ့သို့ ပုံစံရှိသည်။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg"   
 alt="Transformers   
 မော်ဒယ်များ၏ architecture">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Transformers   
 မော်ဒယ်များ၏ architecture">
</div>

Decoder block ရှိ ပထမဆုံး attention layer သည် decoder သို့ input အားလုံး (အတိတ်) ကို ဂရုပြုသော်လည်း ဒုတိယ attention layer သည် encoder ၏ output ကို အသုံးပြုကြောင်း သတိပြုပါ။ ထို့ကြောင့် လက်ရှိစကားလုံးကို အကောင်းဆုံး ခန့်မှန်းရန် input စာကြောင်းတစ်ခုလုံးကို ဝင်ရောက်ကြည့်ရှုနိုင်သည်။ ဘာသာစကား အမျိုးမျိုးတွင် မတူညီသော စကားလုံး အစီအစဉ်များ ထားရှိသည့် သဒ္ဒါစည်းမျဉ်းများ ရှိနိုင်သည် သို့မဟုတ် စာကြောင်းတွင် နောက်ပိုင်းတွင် ပေးထားသော context အချို့သည် ပေးထားသော စကားလုံး၏ အကောင်းဆုံး ဘာသာပြန်ဆိုချက်ကို ဆုံးဖြတ်ရန် အထောက်အကူ ဖြစ်နိုင်သောကြောင့် ၎င်းသည် အလွန်အသုံးဝင်ပါသည်။

*Attention mask* ကို encoder/decoder တွင် မော်ဒယ်အား အထူးစကားလုံးအချို့ကို ဂရုမပြုစေရန် အသုံးပြုနိုင်သည် - ဥပမာ စာကြောင်းများကို အတူတကွ batching လုပ်သည့်အခါ input အားလုံးကို အရှည်တူညီစေရန် အသုံးပြုသည့် အထူး padding စကားလုံး။

##  Architectures vs. checkpoints

ဤသင်တန်းတွင် Transformer မော်ဒယ်များကို လေ့လာသည့်အခါ *architectures* နှင့် *checkpoint* များအပြင် *မော်ဒယ်(models)* များ ဖော်ပြခြင်းကို သင်တွေ့မြင်ရလိမ့်မည်။ ဤဝေါဟာရများအားလုံးသည် အနည်းငယ် ကွဲပြားသော အဓိပ္ပါယ်များရှိသည်။

* **Architecture**: မော်ဒယ်ရဲ့ အခြေခံ တည်ဆောက်ပုံပါ။ အလွှာတစ်ခုချင်းစီရဲ့ လုပ်ဆောင်ချက် အဆင့်ဆင့်ကို ဖော်ပြပါတယ်။
* **Checkpoint**: ၎င်းတို့သည် ပေးထားသော Architecture တွင် load လုပ်မည့် weight များဖြစ်သည်။
* **Model**: ဒီ အသုံးအနှုန်းကတော့ "architecture" ဒါမှမဟုတ် "checkpoint" ဆိုတဲ့ အသုံးအနှုန်းတွေလို တိကျတဲ့ အဓိပ္ပာယ် မရှိပါဘူး။ နှစ်မျိုးလုံးကို ဆိုလိုနိုင်ပါတယ်။ ဒီသင်တန်းမှာတော့ အဓိပ္ပာယ် ပိုရှင်းလင်းစေဖို့အတွက် လိုအပ်တဲ့အခါ "architecture" ဒါမှမဟုတ် "checkpoint" ဆိုတဲ့ အသုံးအနှုန်းတွေကို သီးခြားစီ သုံးသွားပါမယ်။

ဥပမာအားဖြင့် BERT သည် architecture တစ်ခုဖြစ်ပြီး BERT ၏ ပထမဆုံးထုတ်ဝေမှုအတွက် Google အဖွဲ့မှ လေ့ကျင့်ပေးထားသော weight အစုံဖြစ်သည့် `bert-base-cased` သည် checkpoint တစ်ခုဖြစ်သည်။ သို့သော် "BERT မော်ဒယ်" နှင့် "`bert-base-cased` မော်ဒယ်" ဟု ပြောနိုင်သည်။
