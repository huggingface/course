# Sequence-to-sequence မော်ဒယ်များ

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

Encoder-decoder မော်ဒယ်များ (*sequence-to-sequence မော်ဒယ်များ* ဟုလည်းခေါ်သည်) သည် Transformer ဗိသုကာလက်ရာ၏ အစိတ်အပိုင်းနှစ်ခုလုံးကို အသုံးပြုသည်။ အဆင့်တိုင်းတွင် encoder ၏ attention layer များသည် ကနဦးစာကြောင်းရှိ စကားလုံးအားလုံးကို ဝင်ရောက်ကြည့်ရှုနိုင်သော်လည်း decoder ၏ attention layer များသည် input ရှိ ပေးထားသော စကားလုံးတစ်လုံးမတိုင်မီ နေရာယူထားသော စကားလုံးများကိုသာ ဝင်ရောက်ကြည့်ရှုနိုင်သည်။

ဤမော်ဒယ်များ၏ ကြိုတင်လေ့ကျင့်မှုကို encoder သို့မဟုတ် decoder မော်ဒယ်များ၏ ရည်ရွယ်ချက်များကို အသုံးပြု၍ ပြုလုပ်နိုင်သော်လည်း ပုံမှန်အားဖြင့် ပိုမိုရှုပ်ထွေးသော အရာတစ်ခု ပါဝင်သည်။ ဥပမာအားဖြင့် [T5](https://huggingface.co/t5-base) ကို စာသား၏ ကျပန်းအပိုင်းအစများ (စကားလုံးများစွာ ပါဝင်နိုင်သည်) ကို mask အထူးစကားလုံးတစ်လုံးတည်းဖြင့် အစားထိုးခြင်းဖြင့် ကြိုတင်လေ့ကျင့်ထားပြီး ရည်ရွယ်ချက်မှာ ထို mask စကားလုံး အစားထိုးသည့် စာသားကို ခန့်မှန်းရန်ဖြစ်သည်။

Sequence-to-sequence မော်ဒယ်များသည် အနှစ်ချုပ်ရေးသားခြင်း၊ ဘာသာပြန်ခြင်း သို့မဟုတ် မေးခွန်းဖြေဆိုခြင်းကဲ့သို့သော ပေးထားသော input ပေါ် မူတည်၍ စာကြောင်းအသစ်များ ထုတ်လုပ်ခြင်းနှင့် ပတ်သက်သည့် လုပ်ငန်းများအတွက် အသင့်တော်ဆုံးဖြစ်သည်။

ဒီ မော်ဒယ်အမျိုးအစားတွေထဲမှာ ဥပမာအနေနဲ့ ပြောရရင် -

- [BART](https://huggingface.co/transformers/model_doc/bart)
- [mBART](https://huggingface.co/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/transformers/model_doc/marian)
- [T5](https://huggingface.co/transformers/model_doc/t5)
