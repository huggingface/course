<!-- DISABLE-FRONTMATTER-SECTIONS -->

# အမှတ်မပေးသော Quiz[[ungraded-quiz]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

ဒီအခန်းက သင်ကြားရမယ့် အကြောင်းအရာတွေ အများကြီးကို ဖော်ပြခဲ့ပြီးပါပြီ။ အသေးစိတ် အချက်အလက်အားလုံးကို နားမလည်သေးရင်လည်း စိတ်မပူပါနဲ့။ ဒါပေမယ့် ဒီ quiz နဲ့ သင် သင်ယူခဲ့တာတွေကို ပြန်လည်သုံးသပ်ကြည့်ရအောင်။

ဒီ quiz က အမှတ်မပေးတဲ့အတွက် သင်နှစ်သက်သလောက် အကြိမ်ကြိမ် ကြိုးစားဖြေဆိုနိုင်ပါတယ်။ မေးခွန်းအချို့နဲ့ ရုန်းကန်ရရင် အကြံပြုချက်တွေကို လိုက်နာပြီး သင်ခန်းစာတွေကို ပြန်လည်လေ့လာပါ။ ဒီအကြောင်းအရာတွေကို အသိအမှတ်ပြု စာမေးပွဲမှာ ထပ်မံဖြေဆိုရမှာ ဖြစ်ပါတယ်။

### 1. Hub ကို ရှာဖွေပြီး `roberta-large-mnli` checkpoint ကို ရှာပါ။ ၎င်းသည် မည်သည့်လုပ်ငန်းကို လုပ်ဆောင်ပါသနည်း။

<Question
	choices={[
		{
			text: "အကျဉ်းချုပ်ခြင်း (Summarization)",
			explain: "roberta-large-mnli စာမျက်နှာကို <a href=\"https://huggingface.co/roberta-large-mnli\">ပြန်လည်ကြည့်ရှုပါ။</a>"
		},
		{
			text: "စာသားခွဲခြားသတ်မှတ်ခြင်း (Text classification)",
			explain: " ပိုတိတိကျကျပြောရရင် ၎င်းသည် စာကြောင်းနှစ်ကြောင်းက ယုတ္တိရှိရှိ ဆက်စပ်မှုရှိမရှိကို အဆင့်သုံးဆင့် (contradiction, neutral, entailment) နဲ့ ခွဲခြားသတ်မှတ်ပါတယ်။ ဒီလုပ်ငန်းကို <em>natural language inference</em> လို့လည်း ခေါ်ပါတယ်။",
			correct: true
		},
		{
			text: "စာသားထုတ်လုပ်ခြင်း (Text generation)",
			explain: "roberta-large-mnli စာမျက်နှာကို <a href=\"https://huggingface.co/roberta-large-mnli\">ပြန်လည်ကြည့်ရှုပါ။</a>"
		}
	]}
/>

### 2. အောက်ပါ code သည် မည်သည့်အရာကို ပြန်ပေးမည်နည်း။

```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

<Question
	choices={[
		{
			text: "၎င်းသည် ဤစာကြောင်းအတွက် classification scores \"positive\" သို့မဟုတ် \"negative\" အညွှန်းများဖြင့် ကို ပြန်ပေးပါလိမ့်မည်။",
			explain: "ဒါက မမှန်ကန်ပါဘူး — ဒါက `sentiment-analysis` pipeline ဖြစ်ပါလိမ့်မယ်။"
		},
		{
			text: "၎င်းသည် ဤစာကြောင်းကို ဖြည့်စွက်ထားသော ဖန်တီးထားသည့် စာသားကို ပြန်ပေးပါလိမ့်မည်။",
			explain: "ဒါက မမှန်ကန်ပါဘူး — ဒါက `text-generation` pipeline ဖြစ်ပါလိမ့်မယ်။",
		},
		{
			text: "၎င်းသည် လူပုဂ္ဂိုလ်များ၊ အဖွဲ့အစည်းများ သို့မဟုတ် နေရာများကို ကိုယ်စားပြုသည့် စကားလုံးများကို ပြန်ပေးပါလိမ့်မည်။",
			explain: "ထို့အပြင် `grouped_entities=True` ကို အသုံးပြုထားသောကြောင့် ၎င်းသည် 'Hugging Face' ကဲ့သို့သော တူညီသည့် entity နှင့် သက်ဆိုင်သည့် စကားလုံးများကို အုပ်စုဖွဲ့ပေးပါလိမ့်မည်။",
			correct: true
		}
	]}
/>

### 3. ဤ code နမူနာတွင် ... နေရာ၌ မည်သည့်အရာကို အစားထိုးသင့်သနည်း။

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

<Question
	choices={[
		{
			text: "This &#60;mask> has been waiting for you.",
			explain: "ဒါက မမှန်ကန်ပါဘူး။ `bert-base-cased` မော်ဒယ်ကတ်ကို ကြည့်ပြီး သင့်ရဲ့ အမှားကို ရှာဖွေကြည့်ပါ။"
		},
		{
			text: "This [MASK] has been waiting for you.",
			explain: "ဒီမော်ဒယ်ရဲ့ mask token က `[MASK]` ဖြစ်ပါတယ်။",
			correct: true
		},
		{
			text: "This man has been waiting for you.",
			explain: "ဒါက မမှန်ကန်ပါဘူး။ ဒီ pipeline က ဝှက်ထားတဲ့ စကားလုံးတွေကို ဖြည့်ဆည်းပေးတာဖြစ်တဲ့အတွက် mask token တစ်ခုခု လိုအပ်ပါတယ်။"
		}
	]}
/>

### 4. ဤ code သည် အဘယ်ကြောင့် အလုပ်မလုပ်နိုင်သနည်း။

```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

<Question
	choices={[
		{
			text: "ဤ pipeline သည် ဤစာသားကို အမျိုးအစားခွဲခြားရန်အတွက် အညွှန်းများ (labels) ပေးရန် လိုအပ်ပါသည်။",
			explain: "မှန်ပါတယ် — မှန်ကန်တဲ့ code မှာ `candidate_labels=[...]` ကို ထည့်သွင်းဖို့ လိုအပ်ပါတယ်။",
			correct: true
		},
		{
			text: "ဤ pipeline သည် စာကြောင်းတစ်ကြောင်းတည်းမဟုတ်ဘဲ စာကြောင်းများစွာ လိုအပ်ပါသည်။",
			explain: "ဒါက မမှန်ကန်ပါဘူး။ ဒါပေမယ့် မှန်ကန်စွာ အသုံးပြုပါက ဒီ pipeline ဟာ စာကြောင်းစာရင်းတစ်ခုကို လုပ်ဆောင်နိုင်ပါတယ်။ (အခြား pipelines အားလုံးလိုပဲပေါ့)"
		},
		{
			text: "🤗 Transformers library သည် အမြဲတမ်းလိုလို ပျက်နေပါသည်။",
			explain: "ဒီအဖြေကို ကျွန်တော်တို့ မှတ်ချက်မပေးတော့ပါဘူး။"
		},
		{
			text: "ဤ pipeline သည် ပိုရှည်သော inputs များ လိုအပ်ပါသည်။ ဤ input သည် အလွန်တိုတောင်းပါသည်။",
			explain: "ဒါက မမှန်ကန်ပါဘူး။ အလွန်ရှည်လျားသော စာသားကို ဒီ pipeline က လုပ်ဆောင်တဲ့အခါ ဖြတ်တောက်သွားမှာ ဖြစ်ပါတယ်။"
		}
	]}
/>

### 5. "Transfer learning" ဆိုတာ ဘာကိုဆိုလိုတာလဲ။

<Question
	choices={[
		{
			text: "ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တစ်ခု၏ အသိပညာကို တူညီသော dataset ဖြင့် ထပ်မံလေ့ကျင့်ခြင်းအားဖြင့် မော်ဒယ်အသစ်သို့ လွှဲပြောင်းပေးခြင်း။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက မော်ဒယ်တစ်ခုတည်းရဲ့ ဗားရှင်းနှစ်ခု ဖြစ်သွားပါလိမ့်မယ်။"
		},
		{
			text: "ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တစ်ခု၏ အသိပညာကို ဒုတိယမော်ဒယ်ကို ပထမမော်ဒယ်၏ weights များဖြင့် စတင်ခြင်းအားဖြင့် မော်ဒယ်အသစ်သို့ လွှဲပြောင်းပေးခြင်း။",
			explain: "ဒုတိယမော်ဒယ်ကို လုပ်ငန်းအသစ်တစ်ခုအတွက် လေ့ကျင့်သောအခါ ၎င်းသည် ပထမမော်ဒယ်၏ အသိပညာကို 'လွှဲပြောင်း' ပေးပါတယ်။",
			correct: true
		},
		{
			text: "ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တစ်ခု၏ အသိပညာကို ဒုတိယမော်ဒယ်ကို ပထမမော်ဒယ်နှင့် တူညီသော architecture ဖြင့် တည်ဆောက်ခြင်းအားဖြင့် မော်ဒယ်အသစ်သို့ လွှဲပြောင်းပေးခြင်း။",
			explain: "Architecture က မော်ဒယ်ကို ဘယ်လိုတည်ဆောက်ထားလဲဆိုတာကိုပဲ ပြောတာပါ။ ဒီကိစ္စမှာ အသိပညာကို မျှဝေတာ ဒါမှမဟုတ် လွှဲပြောင်းပေးတာ မရှိပါဘူး။"
		}
	]}
/>

### 6. မှန်လား မှားလား။ Language Model တစ်ခုသည် ၎င်း၏ pretraining အတွက် အညွှန်းများ (labels) မလိုအပ်ပါ။

<Question
	choices={[
		{
			text: "မှန်သည်",
			explain: "Pretraining က များသောအားဖြင့် *self-supervised* ဖြစ်ပါတယ်။ ဒါက အညွှန်းတွေကို inputs တွေကနေ အလိုအလျောက် ဖန်တီးပေးတယ်လို့ ဆိုလိုပါတယ်။ (ဥပမာ- နောက်စကားလုံးကို ခန့်မှန်းတာ သို့မဟုတ် ဝှက်ထားတဲ့ စကားလုံးတွေကို ဖြည့်ဆည်းပေးတာ)။",
			correct: true
		},
		{
			text: "မှားသည်",
			explain: "ဒါက မှန်ကန်တဲ့အဖြေ မဟုတ်ပါဘူး။"
		}
	]}
/>

### 7. "model"၊ "architecture" နှင့် "weights" ဟူသော ဝေါဟာရများကို အကောင်းဆုံး ဖော်ပြသည့် စာကြောင်းကို ရွေးပါ။

<Question
	choices={[
		{
			text: "အကယ်၍ model တစ်ခုသည် အဆောက်အအုံတစ်ခုဖြစ်ပါက ၎င်း၏ architecture သည် ပုံစံထုတ်ဒီဇိုင်း (blueprint) ဖြစ်ပြီး weights များသည် အတွင်း၌ နေထိုင်သူများ ဖြစ်သည်။",
			explain: "ဒီဥပမာအတိုင်းဆိုရင် weights တွေက အဆောက်အအုံကို ဆောက်လုပ်ဖို့အတွက် အသုံးပြုတဲ့ အုတ်တွေနဲ့ အခြားပစ္စည်းတွေ ဖြစ်ပါလိမ့်မယ်။"
		},
		{
			text: "Architecture တစ်ခုသည် model တစ်ခုကို တည်ဆောက်ရန် မြေပုံတစ်ခုဖြစ်ပြီး ၎င်း၏ weights များသည် မြေပုံပေါ်တွင် ဖော်ပြထားသော မြို့များ ဖြစ်သည်။",
			explain: "ဒီဥပမာရဲ့ ပြဿနာကတော့ မြေပုံတစ်ခုက များသောအားဖြင့် ရှိပြီးသား အဖြစ်မှန်တစ်ခုကို ကိုယ်စားပြုပါတယ်။ (ပြင်သစ်မှာ Paris လို့ အမည်ရတဲ့ မြို့တစ်မြို့ပဲ ရှိပါတယ်)။ သတ်မှတ်ထားတဲ့ architecture တစ်ခုအတွက် weights များစွာ ဖြစ်နိုင်ပါတယ်။"
		},
		{
			text: "Architecture တစ်ခုသည် model တစ်ခုကို တည်ဆောက်ရန် သင်္ချာဆိုင်ရာ functions များ၏ ဆက်တိုက်ဖြစ်စဉ်တစ်ခုဖြစ်ပြီး ၎င်း၏ weights များသည် ထို functions များ၏ parameters များ ဖြစ်သည်။",
			explain: "တူညီသော သင်္ချာဆိုင်ရာ functions အစုံ (architecture) ကို မတူညီသော parameters (weights) များကို အသုံးပြုခြင်းဖြင့် မတူညီသော model များကို တည်ဆောက်ရန် အသုံးပြုနိုင်ပါသည်။",
			correct: true
		}
	]}
/>

### 8. ဖန်တီးထားသော စာသားများဖြင့် prompts များကို ဖြည့်စွက်ရန်အတွက် မည်သည့်မော်ဒယ်အမျိုးအစားများကို အသုံးပြုမည်နည်း။

<Question
	choices={[
		{
			text: "Encoder model တစ်ခု",
			explain: "Encoder model တစ်ခုသည် စာကြောင်းတစ်ခုလုံး၏ ကိုယ်စားပြုမှုကို ထုတ်ပေးပြီး ၎င်းသည် classification ကဲ့သို့သော လုပ်ငန်းများအတွက် ပိုမိုသင့်လျော်ပါသည်။"
		},
		{
			text: "Decoder model တစ်ခု",
			explain: "Decoder model များသည် prompt တစ်ခုမှ စာသားထုတ်လုပ်ရန်အတွက် အပြည့်အဝ သင့်လျော်ပါသည်။",
			correct: true
		},
		{
			text: "Sequence-to-sequence model တစ်ခု",
			explain: "Sequence-to-sequence model များသည် input စာကြောင်းများနှင့် ဆက်စပ်ပြီး စာကြောင်းများကို ဖန်တီးလိုသည့် လုပ်ငန်းများအတွက် ပိုမိုသင့်လျော်ပြီး၊ သတ်မှတ်ထားသော prompt တစ်ခုအတွက် မဟုတ်ပါ။"
		}
	]}
/>

### 9. စာသားများကို အကျဉ်းချုပ်ရန်အတွက် မည်သည့်မော်ဒယ်အမျိုးအစားများကို အသုံးပြုမည်နည်း။

<Question
	choices={[
		{
			text: "Encoder model တစ်ခု",
			explain: "Encoder model တစ်ခုသည် စာကြောင်းတစ်ခုလုံး၏ ကိုယ်စားပြုမှုကို ထုတ်ပေးပြီး ၎င်းသည် classification ကဲ့သို့သော လုပ်ငန်းများအတွက် ပိုမိုသင့်လျော်ပါသည်။"
		},
		{
			text: "Decoder model တစ်ခု",
			explain: "Decoder model များသည် output text (ဥပမာ- အကျဉ်းချုပ်များ) ကို ထုတ်လုပ်ရန် ကောင်းမွန်သော်လည်း၊ ၎င်းတို့တွင် အကျဉ်းချုပ်ရန် စာသားတစ်ခုလုံးကဲ့သို့သော context ကို အသုံးချနိုင်သည့် စွမ်းရည် မရှိပါ။"
		},
		{
			text: "Sequence-to-sequence model တစ်ခု",
			explain: "Sequence-to-sequence model များသည် အကျဉ်းချုပ်ခြင်း လုပ်ငန်းတစ်ခုအတွက် အပြည့်အဝ သင့်လျော်ပါသည်။",
			correct: true
		}
	]}
/>

### 10. သတ်မှတ်ထားသော အညွှန်းများ (labels) အတိုင်း စာသား inputs များကို အမျိုးအစားခွဲခြားရန်အတွက် မည်သည့်မော်ဒယ်အမျိုးအစားများကို အသုံးပြုမည်နည်း။

<Question
	choices={[
		{
			text: "Encoder model တစ်ခု",
			explain: "Encoder model တစ်ခုသည် စာကြောင်းတစ်ခုလုံး၏ ကိုယ်စားပြုမှုကို ထုတ်ပေးပြီး ၎င်းသည် classification ကဲ့သို့သော လုပ်ငန်းတစ်ခုအတွက် အပြည့်အဝ သင့်လျော်ပါသည်။",
			correct: true
		},
		{
			text: "Decoder model တစ်ခု",
			explain: "Decoder model များသည် output text များကို ထုတ်လုပ်ရန် ကောင်းမွန်ပြီး၊ စာကြောင်းတစ်ခုမှ အညွှန်းတစ်ခုကို ထုတ်ယူရန်အတွက် မဟုတ်ပါ။"
		},
		{
			text: "Sequence-to-sequence model တစ်ခု",
			explain: "Sequence-to-sequence model များသည် input စာကြောင်းတစ်ခုအပေါ် အခြေခံပြီး စာသားကို ဖန်တီးလိုသည့် လုပ်ငန်းများအတွက် ပိုမိုသင့်လျော်ပြီး၊ အညွှန်းတစ်ခုအတွက် မဟုတ်ပါ။",
		}
	]}
/>

### 11. မော်ဒယ်တစ်ခုတွင် တွေ့ရသော ဘက်လိုက်မှု (bias) သည် မည်သည့်ရင်းမြစ်မှ ဖြစ်ပေါ်လာနိုင်သနည်း။

<Question
	choices={[
		{
			text: "မော်ဒယ်သည် ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်၏ fine-tuned version တစ်ခုဖြစ်ပြီး ၎င်းမှ ဘက်လိုက်မှုကို ရယူခဲ့ခြင်း။",
			explain: "Transfer Learning ကို အသုံးပြုသောအခါ ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တွင် ပါဝင်သည့် ဘက်လိုက်မှုသည် fine-tuned မော်ဒယ်တွင် ဆက်လက်တည်ရှိနေပါသည်။",
			correct: true
		},
		{
			text: "မော်ဒယ်ကို လေ့ကျင့်ရာတွင် အသုံးပြုခဲ့သော ဒေတာသည် ဘက်လိုက်မှု ရှိခြင်း။",
			explain: "ဒါက ဘက်လိုက်မှုရဲ့ အထင်ရှားဆုံး ရင်းမြစ်တစ်ခုဖြစ်ပေမယ့် တစ်ခုတည်းတော့ မဟုတ်ပါဘူး။",
			correct: true
		},
		{
			text: "မော်ဒယ်က အကောင်းဆုံးဖြစ်အောင် လုပ်ဆောင်နေတဲ့ metric မှာ ဘက်လိုက်မှု ရှိခြင်း။",
			explain: "ဘက်လိုက်မှုရဲ့ သိသာထင်ရှားမှု နည်းတဲ့ ရင်းမြစ်တစ်ခုကတော့ မော်ဒယ်ကို လေ့ကျင့်တဲ့ နည်းလမ်းပါ။ သင်ရွေးချယ်တဲ့ metric ကို မော်ဒယ်က မျက်စိမှိတ်ပြီး အကောင်းဆုံးဖြစ်အောင် လုပ်ဆောင်သွားမှာ ဖြစ်ပါတယ်။",
			correct: true
		}
	]}
/>

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Quiz**: သင်ယူခဲ့သည့် အကြောင်းအရာများကို ပြန်လည်စစ်ဆေးရန် မေးခွန်းများ။
*   **Transformer Models**: Natural Language Processing (NLP) မှာ အောင်မြင်မှုများစွာရရှိခဲ့တဲ့ deep learning architecture တစ်မျိုးပါ။ ၎င်းတို့ဟာ စာသားတွေထဲက စကားလုံးတွေရဲ့ ဆက်နွယ်မှုတွေကို "attention mechanism" သုံးပြီး နားလည်အောင် သင်ကြားပေးပါတယ်။
*   **Hugging Face Hub**: AI မော်ဒယ်တွေ၊ datasets တွေနဲ့ demo တွေကို အခြားသူတွေနဲ့ မျှဝေဖို့၊ ရှာဖွေဖို့နဲ့ ပြန်လည်အသုံးပြုဖို့အတွက် အွန်လိုင်း platform တစ်ခု ဖြစ်ပါတယ်။
*   **Checkpoint**: မော်ဒယ်တစ်ခုကို လေ့ကျင့်နေစဉ်အတွင်း အချိန်အတန်ကြာပြီးနောက် အခြေအနေတစ်ခုကို သိမ်းဆည်းထားသော အမှတ်။
*   **Task**: AI မော်ဒယ်တစ်ခုက လုပ်ဆောင်ရန် လေ့ကျင့်ထားသော သီးခြားလုပ်ငန်း (ဥပမာ- စာသားခွဲခြားသတ်မှတ်ခြင်း၊ စာသားထုတ်လုပ်ခြင်း)။
*   **Summarization**: စာသားတစ်ခုကို အဓိကအချက်အလက်များ မပျောက်ပျက်စေဘဲ ပိုမိုတိုတောင်းသော ပုံစံဖြင့် အကျဉ်းချုပ်ခြင်း။
*   **Text Classification**: စာသားတစ်ခုကို ကြိုတင်သတ်မှတ်ထားသော အမျိုးအစားများ သို့မဟုတ် အညွှန်းများထဲသို့ ခွဲခြားသတ်မှတ်ခြင်း။
*   **Natural Language Inference (NLI)**: စာကြောင်းနှစ်ကြောင်းကြားရှိ ယုတ္တိဆိုင်ရာ ဆက်နွယ်မှုကို ဆုံးဖြတ်သည့် လုပ်ငန်း။ (ဥပမာ- contradiction, neutral, entailment)
*   **Text Generation**: AI မော်ဒယ်များကို အသုံးပြု၍ လူသားကဲ့သို့သော စာသားအသစ်များ ဖန်တီးခြင်း။
*   **`pipeline()` function**: Hugging Face Transformers library မှာ ပါဝင်တဲ့ လုပ်ဆောင်ချက်တစ်ခုဖြစ်ပြီး မော်ဒယ်တွေကို သီးခြားလုပ်ငန်းတာဝန်များ (ဥပမာ- စာသားခွဲခြားသတ်မှတ်ခြင်း၊ စာသားထုတ်လုပ်ခြင်း) အတွက် အသုံးပြုရလွယ်ကူအောင် ပြုလုပ်ပေးပါတယ်။
*   **`ner` (Named Entity Recognition)**: စာသားထဲက လူအမည်၊ နေရာအမည်၊ အဖွဲ့အစည်းအမည် စတဲ့ သီးခြားအမည်တွေကို ရှာဖွေဖော်ထုတ်ခြင်း။
*   **`grouped_entities=True`**: `ner` pipeline တွင် အသုံးပြုသည့် parameter တစ်ခုဖြစ်ပြီး တူညီသော entity နှင့် သက်ဆိုင်သည့် စကားလုံးများကို အုပ်စုဖွဲ့ပေးသည်။
*   **`sentiment-analysis` pipeline**: စာသားတစ်ခု၏ စိတ်ခံစားမှု (အပြုသဘော၊ အနုတ်သဘော) ကို ခွဲခြမ်းစိတ်ဖြာရန် အသုံးပြုသော pipeline။
*   **`text-generation` pipeline**: input prompt အပေါ် အခြေခံ၍ စာသားအသစ်များကို ဖန်တီးရန် အသုံးပြုသော pipeline။
*   **`fill-mask` pipeline**: စာသားတစ်ခုရှိ ဝှက်ထားသော စကားလုံးများ (mask tokens) ကို ဖြည့်ဆည်းပေးရန် အသုံးပြုသော pipeline။
*   **`bert-base-cased`**: BERT (Bidirectional Encoder Representations from Transformers) မော်ဒယ်၏ ဗားရှင်းတစ်ခုဖြစ်ပြီး အင်္ဂလိပ်စာလုံးအကြီးအသေးကို ခွဲခြားသိမြင်သည်။ ၎င်း၏ mask token သည် `[MASK]` ဖြစ်သည်။
*   **`zero-shot-classification` pipeline**: လေ့ကျင့်မှုဒေတာတွင် မမြင်ဖူးသေးသော အညွှန်းများဖြင့် စာသားများကို အမျိုးအစားခွဲခြားနိုင်သော pipeline။
*   **`candidate_labels`**: `zero-shot-classification` pipeline တွင် အသုံးပြုသည့် parameter တစ်ခုဖြစ်ပြီး စာသားကို ခွဲခြားသတ်မှတ်ရန်အတွက် ဖြစ်နိုင်ခြေရှိသော အညွှန်းများ (labels) စာရင်းကို ပေးပို့သည်။
*   **Transfer Learning**: ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တစ်ခု၏ အသိပညာကို အခြားလုပ်ငန်းတစ်ခု (new task) အတွက် မော်ဒယ်အသစ်သို့ လွှဲပြောင်းပေးခြင်း။
*   **Pretrained Model**: ကြီးမားသော ဒေတာအစုအဝေးများဖြင့် အစောပိုင်းကတည်းက လေ့ကျင့်ထားသော မော်ဒယ်။
*   **Fine-tuned Model**: ကြိုတင်လေ့ကျင့်ထားသော မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနှင့် ထပ်မံလေ့ကျင့်ပေးထားသော မော်ဒယ်။
*   **Weights**: Machine Learning မော်ဒယ်တစ်ခု၏ သင်ယူနိုင်သော အစိတ်အပိုင်းများ။ ၎င်းတို့သည် လေ့ကျင့်နေစဉ်အတွင်း ဒေတာများမှ ပုံစံများကို သင်ယူကာ ချိန်ညှိပေးသည်။
*   **Architecture**: Machine Learning မော်ဒယ်တစ်ခု၏ တည်ဆောက်ပုံ သို့မဟုတ် ဒီဇိုင်း။ ၎င်းသည် သင်္ချာဆိုင်ရာ functions များ၏ အစီအစဉ်နှင့် ၎င်းတို့ မည်သို့ချိတ်ဆက်ထားသည်ကို သတ်မှတ်သည်။
*   **Self-supervised Learning**: အညွှန်းများ (labels) ကို inputs များမှ အလိုအလျောက် ထုတ်လုပ်နိုင်သည့် သင်ယူမှုပုံစံတစ်မျိုး။
*   **Encoder Model**: Transformer Architecture ၏ အစိတ်အပိုင်းတစ်ခုဖြစ်ပြီး input data (ဥပမာ- စာသား) ကို နားလည်ပြီး ကိုယ်စားပြုတဲ့ အချက်အလက် (representation) အဖြစ် ပြောင်းလဲပေးကာ classification ကဲ့သို့သော လုပ်ငန်းများအတွက် သင့်လျော်သည်။
*   **Decoder Model**: Transformer Architecture ၏ အစိတ်အပိုင်းတစ်ခုဖြစ်ပြီး encoder ကနေ ရရှိတဲ့ အချက်အလက် (representation) ကို အသုံးပြုပြီး output data (ဥပမာ- ဘာသာပြန်ထားတဲ့ စာသား သို့မဟုတ် စာသားထုတ်လုပ်ခြင်း) ကို ထုတ်ပေးသည်။
*   **Sequence-to-sequence Model**: Encoder နှင့် Decoder နှစ်ခုစလုံး ပါဝင်သော Transformer architecture တစ်မျိုးဖြစ်ပြီး input sequence မှ output sequence တစ်ခုသို့ ပြောင်းလဲခြင်း (ဥပမာ- ဘာသာပြန်ခြင်း၊ အကျဉ်းချုပ်ခြင်း) လုပ်ငန်းများအတွက် အသုံးပြုပါတယ်။
*   **Bias**: ဒေတာအစုအဝေး (dataset) သို့မဟုတ် မော်ဒယ်၏ လေ့ကျင့်မှုပုံစံကြောင့် ဖြစ်ပေါ်လာသော ဘက်လိုက်မှုများ။
*   **Metric**: မော်ဒယ်တစ်ခု၏ စွမ်းဆောင်ရည်ကို တိုင်းတာရန် အသုံးပြုသော တိုင်းတာမှုစနစ်။