# Time to slice and dice

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section3.ipynb"},
]} />

–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –¥–∞–Ω–Ω—ã–µ, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤—ã –±—É–¥–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–µ –±—É–¥—É—Ç –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –∏—Å—Å–ª–µ–¥—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö. 

<Youtube id="tqfSFcPMgOI"/>

## Slicing and dicing our data

–ö–∞–∫ –∏ –≤ Pandas, ü§ó Datasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –æ–±—ä–µ–∫—Ç–æ–≤ `Dataset` –∏ `DatasetDict`. –ú—ã —É–∂–µ –ø–æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å –º–µ—Ç–æ–¥–æ–º  `Dataset.map()` –≤ [–≥–ª–∞–≤–µ 3](/course/ru/chapter3), –∞ –¥–∞–ª–µ–µ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∏–º–µ—é—â–∏–µ—Å—è –≤ –Ω–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏. 

–î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29), —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã–π –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) –∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ—Ç–∑—ã–≤—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞, —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–π—Ç–∏–Ω–≥ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–π –≤ 10-–±–∞–ª–ª—å–Ω–æ–π —à–∫–∞–ª–µ. 

–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∫–∞—á–∞—Ç—å –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–º–∞–Ω–¥—ã `wget` –∏ `unzip`: 

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

–§–∞–π–ª TSV - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—å CSV —Ñ–∞–π–ª–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–∞–±—É–ª—è—Ü–∏–∏ –≤–º–µ—Å—Ç–æ –∑–∞–ø—è—Ç—ã—Ö –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è, –∞ –∑–Ω–∞—á–∏—Ç –º—ã –º–æ–∂–µ–º –µ–≥–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –ø–æ–º–æ—â—å—é —Å–∫—Ä–∏–ø—Ç–∞ `csv` –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `delimiter` —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é `load_dataset()`: 

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t is the tab character in Python
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

–•–æ—Ä–æ—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π –ø—Ä–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –≤–∑—è—Ç–∏–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π. –í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Datasets –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –ø—É—Ç–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–π `Dataset.shuffle()` –∏ `Dataset.select()`: 

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# Peek at the first few examples
drug_sample[:3]
```

```python out
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

–ó–∞–º–µ—Ç—å—Ç–µ, —á—Ç–æ –º—ã –∑–∞—Ñ–∏–∫–∏—Å–∏—Ä–æ–≤–∞–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `seed`¬†–¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.  `Dataset.select()` –æ–∂–∏–¥–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏–Ω–¥–µ–∫—Å—ã, –ø–æ—ç—Ç–æ–º—É –º—ã –ø–µ—Ä–µ–¥–∞–ª–∏ `range(1000)` –¥–ª—è –≤–∑—è—Ç–∏—è –ø–µ—Ä–≤—ã—Ö 1000 –æ–±—ä–µ–∫—Ç–æ–≤ –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –î–ª—è —ç—Ç–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –º—ã –º–æ–∂–µ–º —Å—Ä–∞–∑—É —É–≤–∏–¥–µ—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: 

* –ö–æ–ª–æ–Ω–∫–∞ `Unnamed: 0` –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –æ–±–µ–∑–ª–∏—á–µ–Ω–Ω—ã–π ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ü–∏–µ–Ω—Ç–∞.
* –ö–æ–ª–æ–Ω–∫–∞ `condition` –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–º–µ—Å—å –ª–µ–π–±–ª–æ–≤ –≤ –Ω–∏–∂–Ω–µ–º –∏ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ.
* –û—Ç–∑—ã–≤—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–º–µ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π —Ç–µ–∫—Å—Ç–∞ (`\r\n`) –∏ HTML-–∫–æ–¥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `&\#039;`). 

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó Datasets –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç—Ç–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π. –ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –Ω–∞—à–∞ –≥–∏–ø–æ—Ç–µ–∑–∞ –æ–± —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `Dataset.unique()` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —á—Ç–æ —á–∏—Å–ª–æ ID —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —á–∏—Å–ª–æ–º —Å—Ç—Ä–æ–∫ –≤ –æ–±–æ–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö (–æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º): 

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

–ü–æ –≤—Å–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏, –Ω–∞—à–∞ –≥–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∞—Å—å, —Ç–∞–∫ —á—Ç–æ –ø–µ—Ä–µ–π–¥–µ–º –∫ –æ—á–∏—Å—Ç–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞. –î–ª—è –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º `Unnamed: 0` –≤–æ —á—Ç–æ-—Ç–æ –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `DatasetDict.rename_column()` –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü—ã –Ω–∞ –æ–±–æ–∏—Ö —Å–ø–ª–∏—Ç–∞—Ö (–æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º): 

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `Dataset.unique()` –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–∏—Å–ª–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–∫–∞—Ä—Å—Ç–≤ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º —Å–ø–ª–∏—Ç–∞—Ö.

</Tip>

–î–∞–ª–µ–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –ª–µ–π–±–ª—ã —Å—Ç–æ–ª–±—Ü–∞ `condition` —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º `Dataset.map()`. –¢–∞–∫ –∂–µ, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤ [–≥–ª–∞–≤–µ 3](/course/ru/chapter3), –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ –∫–∞–∂–¥–æ–≥–æ —Å–ø–ª–∏—Ç–∞ –≤ `drug_dataset`:

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}


drug_dataset.map(lowercase_condition)
```

```python out
AttributeError: 'NoneType' object has no attribute 'lower'
```

–û –Ω–µ—Ç! –ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º—ã —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π! –ò–∑ –æ—à–∏–±–∫–∏ –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ `condition` —è–≤–ª—è—é—Ç—Å—è `None`, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö. –î–∞–≤–∞–π—Ç–µ —É–¥–∞–ª–∏–º —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏ —Å –ø–æ–º–æ—â—å—é `Dataset.filter()`, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ö–æ–∂–∏–º —Å `Dataset.map()` –æ–±—Ä–∞–∑–æ–º –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞. –í–º–µ—Å—Ç–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: 

```py
def filter_nones(x):
    return x["condition"] is not None
```

–∏ –≤—ã–∑–æ–≤–∞ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ `drug_dataset.filter(filter_nones)`, –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —Ç–æ –∂–µ —Å–∞–º–æ–µ —Å –ø–æ–º–æ—â—å—é _lambda-—Ñ—É–Ω–∫—Ü–∏–∏_. –í Python –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏–∏ - —ç—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±–µ–∑ —è–≤–Ω–æ–≥–æ –∏—Ö –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è. –û–±—â–∏–π –≤–∏–¥, –∫–æ—Ç–æ—Ä—ã–º –∏—Ö –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å: 

```
lambda <arguments> : <expression>
```

–≥–¥–µ `lambda` - –æ–¥–Ω–æ –∏–∑ [–∫–ª—é—á–µ–≤—ã—Ö](https://docs.python.org/3/reference/lexical_analysis.html#keywords) —Å–ª–æ–≤ Python, –∞ `<arguments>` - —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—è—Ç–æ–π –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–π–¥—É—Ç –Ω–∞ –≤—Ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏, –∏ `<expression>` –∑–∞–¥–∞–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º. –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤–æ–¥–∏—Ç –≤ –∫–≤–∞–¥—Ä–∞—Ç —á–∏—Å–ª–∞: 

```
lambda x : x * x
```

–ß—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é, –º—ã –¥–æ–ª–∂–Ω—ã –∑–∞–∫–ª—é—á–∏—Ç—å –µ–µ –∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤ —Å–∫–æ–±–∫–∏: 

```py
(lambda x: x * x)(3)
```

```python out
9
```

–ü–æ –∞–Ω–∞–ª–æ–≥–∏–∏ –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏—é —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∑–∞–ø—è—Ç—ã–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å –ø–ª–æ—â–∞–¥—å —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

–õ—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏–∏ —É–¥–æ–±–Ω—ã, –∫–æ–≥–¥–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–∏—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏–∑—É—á–∏—Ç—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é [Real Python tutorial](https://realpython.com/python-lambda/) –∑–∞ –∞–≤—Ç–æ—Ä—Å—Ç–≤–æ–º Andre Burgaud). –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π `map` –∏ `filter`, –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —É—Å—Ç—Ä–∞–Ω–∏—Ç—å `None`-–∑–∞–ø–∏—Å–∏ –∏–∑ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:

```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è `None` –∑–∞–ø–∏—Å–µ–π, –º—ã –º–æ–∂–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É `condition`: 

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# Check that lowercasing worked
drug_dataset["train"]["condition"][:3]
```

```python out
['left ventricular dysfunction', 'adhd', 'birth control']
```

–ó–∞—Ä–∞–±–æ—Ç–∞–ª–æ! –°–µ–π—á–∞—Å –º—ã –æ—á–∏—Å—Ç–∏–ª–∏ –ª–µ–π–±–ª—ã, –¥–∞–≤–∞–π—Ç–µ —Ç–µ–ø–µ—Ä—å –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ, –∫–∞–∫ –º–æ–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–∑—ã–≤—ã. 

## –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤

–í—Å—è–∫–∏–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –≤—ã –∏–º–µ–µ—Ç–µ –¥–µ–ª–æ —Å –æ—Ç–∑—ã–≤–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤, —Ö–æ—Ä–æ—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ—Ç–∑—ã–≤–µ. –û–±–∑–æ—Ä –º–æ–∂–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –≤—Å–µ–≥–æ –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä ¬´–û—Ç–ª–∏—á–Ω–æ!¬ª –∏–ª–∏ –±—ã—Ç—å –ø–æ–ª–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–º —ç—Å—Å–µ —Å —Ç—ã—Å—è—á–∞–º–∏ —Å–ª–æ–≤, –∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ-—Ä–∞–∑–Ω–æ–º—É —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —ç—Ç–∏–º–∏ —Å–ª—É—á–∞—è–º–∏. –ß—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ–±–∑–æ—Ä–µ, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä—É–±—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º.

–ó–∞–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—á–∏—Å–ª—è–µ—Ç —á–∏—Å–ª–æ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ—Ç–∑—ã–≤–µ: 

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ `lowercase_condition()`, `compute_review_length()` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, —á—å–∏ –∫–ª—é—á–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –Ω–∏ –æ–¥–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–ª–æ–Ω–∫–∏ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø—Ä–∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–∏  `compute_review_length()` (–ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –≤ `Dataset.map()`) —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ —Å–æ–∑–¥–∞—Å—Ç –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü —Å –∏–º–µ–Ω–µ–º `review_length`:  

```py
drug_dataset = drug_dataset.map(compute_review_length)
# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–µ—Ä–≤—ã–π –æ–±—ä–µ–∫—Ç –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
drug_dataset["train"][0]
```

```python out
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

–ö–∞–∫ –∏ –æ–∂–∞–¥–∞–ª–æ—Å—å, –º—ã –≤–∏–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –∏–º–µ–Ω–µ–º `review_length`, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∫ –Ω–∞—à–µ–º—É –æ–±—É—á–∞—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ú—ã –º–æ–∂–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —ç—Ç–æ–π –∫–æ–ª–æ–Ω–∫–µ –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `Dataset.sort()` –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ ¬´—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ¬ª –∑–Ω–∞—á–µ–Ω–∏—è: 

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python out
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

–ö–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∑—ã–≤—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–¥–Ω–æ —Å–ª–æ–≤–æ, —Ö–æ—Ç—è —ç—Ç–æ –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è –∑–∞–¥–∞—á–∏ –æ—Ü–µ–Ω–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –≤—Ä—è–¥ –ª–∏ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞. 

<Tip>

üôã –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `Dataset.add_column()`. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü –∏–∑ Python-—Å–ø–∏—Å–∫–∞ –∏–ª–∏ NumPy-–º–∞—Å—Å–∏–≤–∞, —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–æ–±–Ω–æ, –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è `Dataset.map()` –Ω–µ –æ—á–µ–Ω—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è.

</Tip>

–î–∞–≤–∞–π—Ç–µ –ø—Ä–∏–º–µ–Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é `Dataset.filter()` –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –º–µ–Ω—å—à–µ 30 —Å–ª–æ–≤. –°—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º –º—ã –ø—Ä–∏–º–µ–Ω—è–ª–∏ –µ—ë –¥–ª—è —Å—Ç–æ–ª–±—Ü–∞  `condition`: –º—ã –º–æ–∂–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —á–∏—Å–ª–æ —Å–ª–æ–≤ –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞:

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python out
{'train': 138514, 'test': 46108}
```

–ö–∞–∫ –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å, —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–∏–ª–∞ –æ–∫–æ–ª–æ 15% –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ –Ω–∞—à–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `Dataset.sort()` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–∏–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤. –ò–∑—É—á–∏—Ç–µ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ —Ñ—É–Ω–∫—Ü–∏—é, —á—Ç–æ–±—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤ —É–±—ã–≤–∞—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ.

</Tip>

–ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—â—å, –∫–æ—Ç–æ—Ä—É—é –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ–º HTML-–∫–æ–¥–∞–º–∏ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –Ω–∞—à–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å `html` –∏ –º–µ—Ç–æ–¥ `unescape()` —á—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –Ω–∏—Ö: 

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python out
"I'm a transformer called BERT"
```

–î–ª—è —ç—Ç–æ–≥–æ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Dataset.map()` –Ω–∞ –≤—Å–µ–º –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤: 

```python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

–ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –º–µ—Ç–æ–¥ `Dataset.map()` –∫—Ä–∞–π–Ω–µ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö -- —Ö–æ—Ç—è –º—ã –∏ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –º–∞–ª–æ–π —á–∞—Å—Ç—å—é –µ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π!

## –°—É–ø–µ—Ä—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ `map()`

The `Dataset.map()` method takes a `batched` argument that, if set to `True`, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000). For instance, the previous map function that unescaped all the HTML took a bit of time to run (you can read the time taken from the progress bars). We can speed this up by processing several elements at the same time using a list comprehension.

When you specify `batched=True` the function receives a dictionary with the fields of the dataset, but each value is now a _list of values_, and not just a single value. The return value of `Dataset.map()` should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. For example, here is another way to unescape all HTML characters, but using `batched=True`:

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

If you're running this code in a notebook, you'll see that this command executes way faster than the previous one. And it's not because our reviews have already been HTML-unescaped -- if you re-execute the instruction from the previous section (without `batched=True`), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a `for` loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.

Using `Dataset.map()` with `batched=True` will be essential to unlock the speed of the "fast" tokenizers that we'll encounter in [Chapter 6](/course/chapter6), which can quickly tokenize big lists of texts. For instance, to tokenize all the drug reviews with a fast tokenizer, we could use a function like this:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

As you saw in [Chapter 3](/course/chapter3), we can pass one or several examples to the tokenizer, so we can use this function with or without `batched=True`. Let's take this opportunity to compare the performance of the different options. In a notebook, you can time a one-line instruction by adding `%time` before the line of code you wish to measure:

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

You can also time a whole cell by putting `%%time` at the beginning of the cell. On the hardware we executed this on, it showed 10.8s for this instruction (it's the number written after "Wall time").

<Tip>

‚úèÔ∏è **Try it out!** Execute the same instruction with and without `batched=True`, then try it with a slow tokenizer (add `use_fast=False` in the `AutoTokenizer.from_pretrained()` method) so you can see what numbers you get on your hardware.

</Tip>

Here are the results we obtained with and without batching, with a fast and a slow tokenizer:

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

This means that using a fast tokenizer with the `batched=True` option is 30 times faster than its slow counterpart with no batching -- this is truly amazing! That's the main reason why fast tokenizers are the default when using `AutoTokenizer` (and why they are called "fast"). They're able to achieve such a speedup because behind the scenes the tokenization code is executed in Rust, which is a language that makes it easy to parallelize code execution.

Parallelization is also the reason for the nearly 6x speedup the fast tokenizer achieves with batching: you can't parallelize a single tokenization operation, but when you want to tokenize lots of texts at the same time you can just split the execution across several processes, each responsible for its own texts.

`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by Rust, they won't let a slow tokenizer catch up with a fast one, but they can still be helpful (especially if you're using a tokenizer that doesn't have a fast version). To enable multiprocessing, use the `num_proc` argument and specify the number of processes to use in your call to `Dataset.map()`:

```py
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

You can experiment a little with timing to determine the optimal number of processes to use; in our case 8 seemed to produce the best speed gain. Here are the numbers we got with and without multiprocessing:

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s
`batched=True`, `num_proc=8`  | 6.52s          | 41.3s
`batched=False`, `num_proc=8` | 9.49s          | 45.2s

Those are much more reasonable results for the slow tokenizer, but the performance of the fast tokenizer was also substantially improved. Note, however, that won't always be the case -- for values of `num_proc` other than 8, our tests showed that it was faster to use `batched=True` without that option. In general, we don't recommend using Python multiprocessing for fast tokenizers with `batched=True`.

<Tip>

Using `num_proc` to speed up your processing is usually a great idea, as long as the function you are using is not already doing some kind of multiprocessing of its own.

</Tip>

All of this functionality condensed into a single method is already pretty amazing, but there's more! With `Dataset.map()` and `batched=True` you can change the number of elements in your dataset. This is super useful in many situations where you want to create several training features from one example, and we will need to do this as part of the preprocessing for several of the NLP tasks we'll undertake in [Chapter 7](/course/chapter7).

<Tip>

üí° In machine learning, an _example_ is usually defined as the set of _features_ that we feed to the model. In some contexts, these features will be the set of columns in a `Dataset`, but in others (like here and for question answering), multiple features can be extracted from a single example and belong to a single column.

</Tip>

Let's have a look at how it works! Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return *all* the chunks of the texts instead of just the first one. This can be done with `return_overflowing_tokens=True`:

```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

Let's test this on one example before using `Dataset.map()` on the whole dataset:

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python out
[128, 49]
```

So, our first example in the training set became two features because it was tokenized to more than the maximum number of tokens we specified: the first one of length 128 and the second one of length 49. Now let's do this for all elements of the dataset!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python out
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

Oh no! That didn't work! Why not? Looking at the error message will give us a clue: there is a mismatch in the lengths of one of the columns, one being of length 1,463 and the other of length 1,000. If you've looked at the `Dataset.map()` [documentation](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map), you may recall that it's the number of samples passed to the function that we are mapping; here those 1,000 examples gave 1,463 new features, resulting in a shape error.

The problem is that we're trying to mix two different datasets of different sizes: the `drug_dataset` columns will have a certain number of examples (the 1,000 in our error), but the `tokenized_dataset` we are building will have more (the 1,463 in the error message). That doesn't work for a `Dataset`, so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset. We can do the former with the `remove_columns` argument:

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

Now this works without error. We can check that our new dataset has many more elements than the original dataset by comparing the lengths:

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

We mentioned that we can also deal with the mismatched length problem by making the old columns the same size as the new ones. To do this, we will need the `overflow_to_sample_mapping` field the tokenizer returns when we set `return_overflowing_tokens=True`. It gives us a mapping from a new feature index to the index of the sample it originated from. Using this, we can associate each key present in our original dataset with a list of values of the right size by repeating the values of each example as many times as it generates new features:

```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extract mapping between new and old indices
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

We can see it works with `Dataset.map()` without us needing to remove the old columns:

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

We get the same number of training features as before, but here we've kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach.

You've now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs,
there may be times when you'll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, ü§ó Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let's take a look at how this works.

## From `Dataset`s to `DataFrame`s and back

<Youtube id="tfcY1067A5Q"/>

To enable the conversion between various third-party libraries, ü§ó Datasets provides a `Dataset.set_format()` function. This function only changes the _output format_ of the dataset, so you can easily switch to another format without affecting the underlying _data format_, which is Apache Arrow. The formatting is done in place. To demonstrate, let's convert our dataset to Pandas:

```py
drug_dataset.set_format("pandas")
```

Now when we access elements of the dataset we get a `pandas.DataFrame` instead of a dictionary:

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

Let's create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_dataset["train"]`:

```py
train_df = drug_dataset["train"][:]
```

<Tip>

üö® Under the hood, `Dataset.set_format()` changes the return format for the dataset's `__getitem__()` dunder method. This means that when we want to create a new object like `train_df` from a `Dataset` in the `"pandas"` format, we need to slice the whole dataset to obtain a `pandas.DataFrame`. You can verify for yourself that the type of `drug_dataset["train"]` is `Dataset`, irrespective of the output format.

</Tip>


From here we can use all the Pandas functionality that we want. For example, we can do fancy chaining to compute the class distribution among the `condition` entries:

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>


And once we're done with our Pandas analysis, we can always create a new `Dataset` object by using the `Dataset.from_pandas()` function as follows:


```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python out
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<Tip>

‚úèÔ∏è **Try it out!** Compute the average rating per drug and store the result in a new `Dataset`.

</Tip>

This wraps up our tour of the various preprocessing techniques available in ü§ó Datasets. To round out the section, let's create a validation set to prepare the dataset for training a classifier on. Before doing so, we'll reset the output format of `drug_dataset` from `"pandas"` to `"arrow"`:

```python
drug_dataset.reset_format()
```

## Creating a validation set

Although we have a test set we could use for evaluation, it's a good practice to leave the test set untouched and create a separate validation set during development. Once you are happy with the performance of your models on the validation set, you can do a final sanity check on the test set. This process helps mitigate the risk that you'll overfit to the test set and deploy a model that fails on real-world data.

ü§ó Datasets provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn`. Let's use it to split our training set into `train` and `validation` splits (we set the `seed` argument for reproducibility):

```py
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# Rename the default "test" split to "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# Add the "test" set to our `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

Great, we've now prepared a dataset that's ready for training some models on! In [section 5](/course/chapter5/5) we'll show you how to upload datasets to the Hugging Face Hub, but for now let's cap off our analysis by looking at a few ways you can save datasets on your local machine.

## Saving a dataset

<Youtube id="blF9uxYcKHo"/>

Although ü§ó Datasets will cache every downloaded dataset and the operations performed on it, there are times when you'll want to save a dataset to disk (e.g., in case the cache gets deleted). As shown in the table below, ü§ó Datasets provides three main functions to save your dataset in different formats:

| Data format |        Function        |
| :---------: | :--------------------: |
|    Arrow    | `Dataset.save_to_disk()` |
|     CSV     |    `Dataset.to_csv()`    |
|    JSON     |   `Dataset.to_json()`    |

For example, let's save our cleaned dataset in the Arrow format:

```py
drug_dataset_clean.save_to_disk("drug-reviews")
```

This will create a directory with the following structure:

```
drug-reviews/
‚îú‚îÄ‚îÄ dataset_dict.json
‚îú‚îÄ‚îÄ test
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îú‚îÄ‚îÄ train
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îú‚îÄ‚îÄ indices.arrow
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îî‚îÄ‚îÄ validation
    ‚îú‚îÄ‚îÄ dataset.arrow
    ‚îú‚îÄ‚îÄ dataset_info.json
    ‚îú‚îÄ‚îÄ indices.arrow
    ‚îî‚îÄ‚îÄ state.json
```

where we can see that each split is associated with its own *dataset.arrow* table, and some metadata in *dataset_info.json* and *state.json*. You can think of the Arrow format as a fancy table of columns and rows that is optimized for building high-performance applications that process and transport large datasets.

Once the dataset is saved, we can load it by using the `load_from_disk()` function as follows:

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

For the CSV and JSON formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object:

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

This saves each split in [JSON Lines format](https://jsonlines.org), where each row in the dataset is stored as a single line of JSON. Here's what the first example looks like:

```py
!head -n 1 drug-reviews-train.jsonl
```

```python out
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

We can then use the techniques from [section 2](/course/chapter5/2) to load the JSON files as follows:

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

And that's it for our excursion into data wrangling with ü§ó Datasets! Now that we have a cleaned dataset for training a model on, here are a few ideas that you could try out:

1. Use the techniques from [Chapter 3](/course/chapter3) to train a classifier that can predict the patient condition based on the drug review.
2. Use the `summarization` pipeline from [Chapter 1](/course/chapter1) to generate summaries of the reviews.

Next, we'll take a look at how ü§ó Datasets can enable you to work with huge datasets without blowing up your laptop!
