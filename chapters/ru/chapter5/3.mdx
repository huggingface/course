# –ü—Ä–µ–ø–∞—Ä–∏—Ä—É–µ–º ü§ó Datasets

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section3.ipynb"},
]} />

–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –¥–∞–Ω–Ω—ã–µ, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤—ã –±—É–¥–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–µ –±—É–¥—É—Ç –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –∏—Å—Å–ª–µ–¥—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö. 

<Youtube id="tqfSFcPMgOI"/>

## –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏

–ö–∞–∫ –∏ –≤ Pandas, ü§ó Datasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –æ–±—ä–µ–∫—Ç–æ–≤ `Dataset` –∏ `DatasetDict`. –ú—ã —É–∂–µ –ø–æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å –º–µ—Ç–æ–¥–æ–º  `Dataset.map()` –≤ [–≥–ª–∞–≤–µ 3](/course/ru/chapter3), –∞ –¥–∞–ª–µ–µ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∏–º–µ—é—â–∏–µ—Å—è –≤ –Ω–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏. 

–î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29), —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã–π –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) –∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ—Ç–∑—ã–≤—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞, —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–π—Ç–∏–Ω–≥ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–π –≤ 10-–±–∞–ª–ª—å–Ω–æ–π —à–∫–∞–ª–µ. 

–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∫–∞—á–∞—Ç—å –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–º–∞–Ω–¥—ã `wget` –∏ `unzip`: 

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

–§–∞–π–ª TSV - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—å CSV —Ñ–∞–π–ª–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–∞–±—É–ª—è—Ü–∏–∏ –≤–º–µ—Å—Ç–æ –∑–∞–ø—è—Ç—ã—Ö –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è, –∞ –∑–Ω–∞—á–∏—Ç –º—ã –º–æ–∂–µ–º –µ–≥–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –ø–æ–º–æ—â—å—é —Å–∫—Ä–∏–ø—Ç–∞ `csv` –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `delimiter` —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é `load_dataset()`: 

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t is the tab character in Python
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

–•–æ—Ä–æ—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π –ø—Ä–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –≤–∑—è—Ç–∏–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π. –í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Datasets –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –ø—É—Ç–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–π `Dataset.shuffle()` –∏ `Dataset.select()`: 

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# Peek at the first few examples
drug_sample[:3]
```

```python out
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

–ó–∞–º–µ—Ç—å—Ç–µ, —á—Ç–æ –º—ã –∑–∞—Ñ–∏–∫–∏—Å–∏—Ä–æ–≤–∞–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `seed`¬†–¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.  `Dataset.select()` –æ–∂–∏–¥–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏–Ω–¥–µ–∫—Å—ã, –ø–æ—ç—Ç–æ–º—É –º—ã –ø–µ—Ä–µ–¥–∞–ª–∏ `range(1000)` –¥–ª—è –≤–∑—è—Ç–∏—è –ø–µ—Ä–≤—ã—Ö 1000 –æ–±—ä–µ–∫—Ç–æ–≤ –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –î–ª—è —ç—Ç–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –º—ã –º–æ–∂–µ–º —Å—Ä–∞–∑—É —É–≤–∏–¥–µ—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: 

* –ö–æ–ª–æ–Ω–∫–∞ `Unnamed: 0` –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –æ–±–µ–∑–ª–∏—á–µ–Ω–Ω—ã–π ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ü–∏–µ–Ω—Ç–∞.
* –ö–æ–ª–æ–Ω–∫–∞ `condition` –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–º–µ—Å—å –ª–µ–π–±–ª–æ–≤ –≤ –Ω–∏–∂–Ω–µ–º –∏ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ.
* –û—Ç–∑—ã–≤—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–º–µ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π —Ç–µ–∫—Å—Ç–∞ (`\r\n`) –∏ HTML-–∫–æ–¥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `&\#039;`). 

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó Datasets –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç—Ç–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π. –ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –Ω–∞—à–∞ –≥–∏–ø–æ—Ç–µ–∑–∞ –æ–± —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `Dataset.unique()` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —á—Ç–æ —á–∏—Å–ª–æ ID —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —á–∏—Å–ª–æ–º —Å—Ç—Ä–æ–∫ –≤ –æ–±–æ–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö (–æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º): 

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

–ü–æ –≤—Å–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏, –Ω–∞—à–∞ –≥–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∞—Å—å, —Ç–∞–∫ —á—Ç–æ –ø–µ—Ä–µ–π–¥–µ–º –∫ –æ—á–∏—Å—Ç–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞. –î–ª—è –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º `Unnamed: 0` –≤–æ —á—Ç–æ-—Ç–æ –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `DatasetDict.rename_column()` –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü—ã –Ω–∞ –æ–±–æ–∏—Ö —Å–ø–ª–∏—Ç–∞—Ö (–æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º): 

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `Dataset.unique()` –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–∏—Å–ª–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–∫–∞—Ä—Å—Ç–≤ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–º –∏ —Ç–µ—Å—Ç–æ–≤–æ–º —Å–ø–ª–∏—Ç–∞—Ö.

</Tip>

–î–∞–ª–µ–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –ª–µ–π–±–ª—ã —Å—Ç–æ–ª–±—Ü–∞ `condition` —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º `Dataset.map()`. –¢–∞–∫ –∂–µ, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤ [–≥–ª–∞–≤–µ 3](/course/ru/chapter3), –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ –∫–∞–∂–¥–æ–≥–æ —Å–ø–ª–∏—Ç–∞ –≤ `drug_dataset`:

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}


drug_dataset.map(lowercase_condition)
```

```python out
AttributeError: 'NoneType' object has no attribute 'lower'
```

–û –Ω–µ—Ç! –ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º—ã —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π! –ò–∑ –æ—à–∏–±–∫–∏ –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ `condition` —è–≤–ª—è—é—Ç—Å—è `None`, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö. –î–∞–≤–∞–π—Ç–µ —É–¥–∞–ª–∏–º —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏ —Å –ø–æ–º–æ—â—å—é `Dataset.filter()`, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ö–æ–∂–∏–º —Å `Dataset.map()` –æ–±—Ä–∞–∑–æ–º –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞. –í–º–µ—Å—Ç–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: 

```py
def filter_nones(x):
    return x["condition"] is not None
```

–∏ –≤—ã–∑–æ–≤–∞ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ `drug_dataset.filter(filter_nones)`, –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —Ç–æ –∂–µ —Å–∞–º–æ–µ —Å –ø–æ–º–æ—â—å—é _lambda-—Ñ—É–Ω–∫—Ü–∏–∏_. –í Python –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏–∏ - —ç—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±–µ–∑ —è–≤–Ω–æ–≥–æ –∏—Ö –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è. –û–±—â–∏–π –≤–∏–¥, –∫–æ—Ç–æ—Ä—ã–º –∏—Ö –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å: 

```
lambda <arguments> : <expression>
```

–≥–¥–µ `lambda` - –æ–¥–Ω–æ –∏–∑ [–∫–ª—é—á–µ–≤—ã—Ö](https://docs.python.org/3/reference/lexical_analysis.html#keywords) —Å–ª–æ–≤ Python, –∞ `<arguments>` - —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—è—Ç–æ–π –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–π–¥—É—Ç –Ω–∞ –≤—Ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏, –∏ `<expression>` –∑–∞–¥–∞–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º. –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤–æ–¥–∏—Ç –≤ –∫–≤–∞–¥—Ä–∞—Ç —á–∏—Å–ª–∞: 

```
lambda x : x * x
```

–ß—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é, –º—ã –¥–æ–ª–∂–Ω—ã –∑–∞–∫–ª—é—á–∏—Ç—å –µ–µ –∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤ —Å–∫–æ–±–∫–∏: 

```py
(lambda x: x * x)(3)
```

```python out
9
```

–ü–æ –∞–Ω–∞–ª–æ–≥–∏–∏ –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏—é —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∑–∞–ø—è—Ç—ã–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å –ø–ª–æ—â–∞–¥—å —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

–õ—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏–∏ —É–¥–æ–±–Ω—ã, –∫–æ–≥–¥–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–∏—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏–∑—É—á–∏—Ç—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é [Real Python tutorial](https://realpython.com/python-lambda/) –∑–∞ –∞–≤—Ç–æ—Ä—Å—Ç–≤–æ–º Andre Burgaud). –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—è–º–±–¥–∞-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π `map` –∏ `filter`, –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —É—Å—Ç—Ä–∞–Ω–∏—Ç—å `None`-–∑–∞–ø–∏—Å–∏ –∏–∑ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:

```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è `None` –∑–∞–ø–∏—Å–µ–π, –º—ã –º–æ–∂–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É `condition`: 

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# Check that lowercasing worked
drug_dataset["train"]["condition"][:3]
```

```python out
['left ventricular dysfunction', 'adhd', 'birth control']
```

–ó–∞—Ä–∞–±–æ—Ç–∞–ª–æ! –°–µ–π—á–∞—Å –º—ã –æ—á–∏—Å—Ç–∏–ª–∏ –ª–µ–π–±–ª—ã, –¥–∞–≤–∞–π—Ç–µ —Ç–µ–ø–µ—Ä—å –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ, –∫–∞–∫ –º–æ–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–∑—ã–≤—ã. 

## –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤

–í—Å—è–∫–∏–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –≤—ã –∏–º–µ–µ—Ç–µ –¥–µ–ª–æ —Å –æ—Ç–∑—ã–≤–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤, —Ö–æ—Ä–æ—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ—Ç–∑—ã–≤–µ. –û–±–∑–æ—Ä –º–æ–∂–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –≤—Å–µ–≥–æ –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä ¬´–û—Ç–ª–∏—á–Ω–æ!¬ª –∏–ª–∏ –±—ã—Ç—å –ø–æ–ª–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–º —ç—Å—Å–µ —Å —Ç—ã—Å—è—á–∞–º–∏ —Å–ª–æ–≤, –∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ-—Ä–∞–∑–Ω–æ–º—É —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —ç—Ç–∏–º–∏ —Å–ª—É—á–∞—è–º–∏. –ß—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ–±–∑–æ—Ä–µ, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä—É–±—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º.

–ó–∞–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—á–∏—Å–ª—è–µ—Ç —á–∏—Å–ª–æ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ—Ç–∑—ã–≤–µ: 

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ `lowercase_condition()`, `compute_review_length()` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, —á—å–∏ –∫–ª—é—á–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –Ω–∏ –æ–¥–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–ª–æ–Ω–∫–∏ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø—Ä–∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–∏  `compute_review_length()` (–ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –≤ `Dataset.map()`) —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ —Å–æ–∑–¥–∞—Å—Ç –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü —Å –∏–º–µ–Ω–µ–º `review_length`:  

```py
drug_dataset = drug_dataset.map(compute_review_length)
# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–µ—Ä–≤—ã–π –æ–±—ä–µ–∫—Ç –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
drug_dataset["train"][0]
```

```python out
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

–ö–∞–∫ –∏ –æ–∂–∞–¥–∞–ª–æ—Å—å, –º—ã –≤–∏–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –∏–º–µ–Ω–µ–º `review_length`, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∫ –Ω–∞—à–µ–º—É –æ–±—É—á–∞—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ú—ã –º–æ–∂–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —ç—Ç–æ–π –∫–æ–ª–æ–Ω–∫–µ –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `Dataset.sort()` –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ ¬´—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ¬ª –∑–Ω–∞—á–µ–Ω–∏—è: 

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python out
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

–ö–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∑—ã–≤—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–¥–Ω–æ —Å–ª–æ–≤–æ, —Ö–æ—Ç—è —ç—Ç–æ –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è –∑–∞–¥–∞—á–∏ –æ—Ü–µ–Ω–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –≤—Ä—è–¥ –ª–∏ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞. 

<Tip>

üôã –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `Dataset.add_column()`. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü –∏–∑ Python-—Å–ø–∏—Å–∫–∞ –∏–ª–∏ NumPy-–º–∞—Å—Å–∏–≤–∞, —á—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–æ–±–Ω–æ, –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è `Dataset.map()` –Ω–µ –æ—á–µ–Ω—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è.

</Tip>

–î–∞–≤–∞–π—Ç–µ –ø—Ä–∏–º–µ–Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é `Dataset.filter()` –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –º–µ–Ω—å—à–µ 30 —Å–ª–æ–≤. –°—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º –º—ã –ø—Ä–∏–º–µ–Ω—è–ª–∏ –µ—ë –¥–ª—è —Å—Ç–æ–ª–±—Ü–∞  `condition`: –º—ã –º–æ–∂–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —á–∏—Å–ª–æ —Å–ª–æ–≤ –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞:

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python out
{'train': 138514, 'test': 46108}
```

–ö–∞–∫ –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å, —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–∏–ª–∞ –æ–∫–æ–ª–æ 15% –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ –Ω–∞—à–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `Dataset.sort()` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–∏–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤. –ò–∑—É—á–∏—Ç–µ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ —Ñ—É–Ω–∫—Ü–∏—é, —á—Ç–æ–±—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤ —É–±—ã–≤–∞—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ.

</Tip>

–ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—â—å, –∫–æ—Ç–æ—Ä—É—é –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ–º HTML-–∫–æ–¥–∞–º–∏ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –Ω–∞—à–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å `html` –∏ –º–µ—Ç–æ–¥ `unescape()` —á—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –Ω–∏—Ö: 

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python out
"I'm a transformer called BERT"
```

–î–ª—è —ç—Ç–æ–≥–æ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Dataset.map()` –Ω–∞ –≤—Å–µ–º –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤: 

```python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

–ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –º–µ—Ç–æ–¥ `Dataset.map()` –∫—Ä–∞–π–Ω–µ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö -- —Ö–æ—Ç—è –º—ã –∏ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –º–∞–ª–æ–π —á–∞—Å—Ç—å—é –µ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π!

## –°—É–ø–µ—Ä—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ `map()`

–ú–µ—Ç–æ–¥ `Dataset.map()` –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç `batched`, –∫–æ—Ç–æ—Ä—ã–π, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –∑–Ω–∞—á–µ–Ω–∏–µ `True`, –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –µ–≥–æ —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –±–∞—Ç—á —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Ñ—É–Ω–∫—Ü–∏—é `map()` (—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å, –Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–Ω —Ä–∞–≤–µ–Ω 1000). –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è `map()`, –∫–æ—Ç–æ—Ä–∞—è —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–ª–∞ –≤–µ—Å—å HTML-–∫–æ–¥, —Ç—Ä–µ–±–æ–≤–∞–ª–∞ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (–≤—ã –º–æ–∂–µ—Ç–µ —É–∑–Ω–∞—Ç—å –≤—Ä–µ–º—è –≤–∑–≥–ª—è–Ω—É–≤ –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞). –ú—ã –º–æ–∂–µ–º —É—Å–∫–æ—Ä–∏—Ç—å —ç—Ç–æ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è list comprehension.

–ö–æ–≥–¥–∞ –≤—ã —É–∫–∞–∑—ã–≤–∞–µ—Ç–µ `batched=True`, —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –∫–∞–∂–¥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π _—Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π_, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ `Dataset.map()` –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º: —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –æ–±–Ω–æ–≤–∏—Ç—å –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –≤ –Ω–∞—à –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∏ —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–æ—Ç –µ—â–µ –æ–¥–∏–Ω —Å–ø–æ—Å–æ–± —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã HTML, –Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `batched=True`:

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

–ï—Å–ª–∏ –≤—ã –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –≤—ã —É–≤–∏–¥–∏—Ç–µ, —á—Ç–æ —ç—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∞—è. –ò —ç—Ç–æ –Ω–µ –ø–æ—Ç–æ–º—É, —á—Ç–æ –Ω–∞—à–∏ –æ—Ç–∑—ã–≤—ã —É–∂–µ –±—ã–ª–∏ HTML-—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ ‚Äî –µ—Å–ª–∏ –≤—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ (–±–µ–∑ `batched=True`), —ç—Ç–æ –∑–∞–π–º–µ—Ç —Å—Ç–æ–ª—å–∫–æ –∂–µ –≤—Ä–µ–º–µ–Ω–∏, —Å–∫–æ–ª—å–∫–æ –∏ —Ä–∞–Ω—å—à–µ. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–æ–≤ –æ–±—ã—á–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–æ–≥–æ –∂–µ –∫–æ–¥–∞ –≤ —Ü–∏–∫–ª–µ `for`, –º—ã —Ç–∞–∫–∂–µ –ø–æ–≤—ã—à–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–Ω–æ–∂–µ—Å—Ç–≤—É —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∞ –Ω–µ –ø–æ –æ–¥–Ω–æ–º—É.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `Dataset.map()` —Å `batched=True` ‚Äì —Ö–æ—Ä–æ—à–∏–π —Å–ø–æ—Å–æ–± ¬´—Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å¬ª —Å–∫–æ—Ä–æ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è "–±—ã—Å—Ç—Ä—ã—Ö" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–º—Å—è –≤ [–≥–ª–∞–≤–µ 6](/course/chapter6), –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Å—Ç—Ä–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ —Å–ø–∏—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –æ—Ç–∑—ã–≤—ã –Ω–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –ø–æ–¥–æ–±–Ω—É—é —ç—Ç–æ–π:  

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

–ö–∞–∫ –≤—ã –≤–∏–¥–µ–ª–∏ –≤ [–≥–ª–∞–≤–µ 3](/course/ru/chapter3), –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —Ç–∞–∫ —á—Ç–æ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ `batched=True`. –î–∞–≤–∞–π—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —ç—Ç–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∏ —Å—Ä–∞–≤–Ω–∏–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –í –Ω–æ—É—Ç–±—É–∫–µ –º–æ–∂–Ω–æ –∑–∞–º–µ—Ä–∏—Ç—å –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è `%time` –ø–µ—Ä–µ–¥ —Å—Ç—Ä–æ–∫–æ–π –∫–æ–¥–∞, –≤—Ä–µ–º—è –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ—Ç–æ—Ä–æ–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏–∑–º–µ—Ä–∏—Ç—å: 

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

–¢–∞–∫–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–∑–º–µ—Ä–∏—Ç—å –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ–π —è—á–µ–π–∫–∏: –Ω—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å `%time` –Ω–∞ `%%time` –≤ –Ω–∞—á–∞–ª–µ —è—á–µ–π–∫–∏. –ù–∞ –Ω–∞—à–µ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ —ç—Ç–æ –∑–∞–Ω—è–ª–æ 10.8 —Å–µ–∫—É–Ω–¥. –≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–æ –ø–æ—Å–ª–µ —Å–ª–æ–≤ "Wall time".

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –í—ã–ø–æ–ª–Ω–∏—Ç–µ —ç—Ç—É –∂–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å –∏ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ `batched=True`, –∑–∞—Ç–µ–º –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —Å "–º–µ–¥–ª–µ–Ω–Ω—ã–º" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º (–¥–æ–±–∞–≤—å—Ç–µ `use_fast=False` –≤ –º–µ—Ç–æ–¥ `AutoTokenizer.from_pretrained()`) –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –Ω–∞ —Å–≤–æ–µ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏.

</Tip>

–í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø–æ–ª—É—á–∏–ª–∏ –±–µ–∑ –∏ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –±–∞—Ç—á–∏–Ω–≥–∞, –∏ –¥–≤—É–º—è —Ä–∞–∑–Ω—ã–º–∏ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏: 

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

–ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≤–∏–¥–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `batched=True` –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ 30 —Ä–∞–∑ ‚Äì —ç—Ç–æ –ø–æ—Ç—Ä—è—Å–∞—é—â–µ! –≠—Ç–æ –≥–ª–∞–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞, –ø–æ—á–µ–º—É –±—ã—Å—Ç—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫–ª–∞—Å—Å–∞ `AutoTokenizer` (–∏ –ø–æ—á–µ–º—É –æ–Ω–∏ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è "–±—ã—Å—Ç—Ä—ã–º–∏"). –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏—á—å —Ç–∞–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞—Å—á–µ—Ç –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ —è–∑—ã–∫–µ Rust, –∫–æ—Ç–æ—Ä—ã–π –ª–µ–≥–∫–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞. 

–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—á—Ç–∏ –≤ 6 —Ä–∞–∑ —É—Å–∫–æ—Ä–∏—Ç—å –±—ã—Å—Ç—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `batched=True`: –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ –ø–∞—Ä–µ–ª–ª–µ–ª–∏–∑–æ–≤–∞—Ç—å –µ–¥–Ω–∏—á–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –Ω–æ –∫–æ–≥–¥–∞ –≤—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç–µ –º–Ω–æ–≥–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –≤—ã –º–æ–∂–µ—Ç–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. 

`Dataset.map()` —Ç–∞–∫–∂–µ –æ–±–ª–∞–¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏. –ü–æ—Å–∫–æ–ª—å–∫—É –º–µ—Ç–æ–¥ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –Ω–∞ Rust, –æ–Ω –Ω–µ –ø–æ–∑–≤–æ–ª—è—Ç "–º–µ–¥–ª–µ–Ω–Ω–æ–º—É" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É "–¥–æ–≥–Ω–∞—Ç—å" –±—ã—Å—Ç—Ä—ã–π, –Ω–æ –≤—Å–µ –∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω (–æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —É –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –±—ã—Å—Ç—Ä–æ–π –≤–µ—Ä—Å–∏–∏). –ß—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç `num_proc` –∏ —É–∫–∞–∂–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –≤–∞—à–µ–º –≤—ã–∑–æ–≤–µ `Dataset.map()`:

```py
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

–í—ã –º–æ–∂–µ—Ç–µ –ø–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã—è—Å–Ω–∏—Ç—å, –∫–∞–∫–æ–µ —á–∏—Å–ª–æ `num_proc` –¥–∞—Å—Ç –Ω–∞–∏–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –∑–Ω–∞—á–µ–Ω–∏–µ 8 —Å—Ç–∞–ª–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º. –í–æ—Ç –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø–æ–ª—É—á–∏–ª–∏ —Å –∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞: 

Options         | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s
`batched=True`, `num_proc=8`  | 6.52s          | 41.3s
`batched=False`, `num_proc=8` | 9.49s          | 45.2s

–≠—Ç–æ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è "–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –Ω–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Ç–∞–∫–∂–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤—ã—Ä–æ—Å–ª–∞. –û–¥–Ω–∞–∫–æ, –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —ç—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ —Ç–∞–∫ ‚Äî –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏–π `num_proc`, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç 8, –Ω–∞—à–∏ —Ç–µ—Å—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±—ã—Å—Ç—Ä–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `batched=True` –±–µ–∑ —ç—Ç–æ–π –æ–ø—Ü–∏–∏. –ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –º—ã –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ Python –¥–ª—è "–±—ã—Å—Ç—Ä—ã—Ö" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `batched=True`.

<Tip>

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `num_proc` –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—ã—á–Ω–æ –æ—Ç–ª–∏—á–Ω–∞—è –∏–¥–µ—è, –Ω–æ —Ç–æ–ª—å–∫–æ –≤ —Ç–µ—Ö —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å–∞–º–∞ –ø–æ —Å–µ–±–µ –Ω–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –Ω–∏–∫–∞–∫–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏. 

</Tip>

–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ–π —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–æ –≤—Å–µ–≥–æ –ª–∏—à—å –æ–¥–∏–Ω –º–µ—Ç–æ–¥ —Å–∞–º–æ –ø–æ —Å–µ–±–µ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ, –Ω–æ —ç—Ç–æ –µ—â–µ –Ω–µ –≤—Å–µ! –ò—Å–ø–æ–ª—å–∑—É—è `Dataset.map()` –∏ `batched=True` –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–º–µ–Ω—è—Ç—å —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ. –≠—Ç–æ –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ —Å–∏—Ç—É–∞—Ü–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–æ–∑–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Ç–µ–∫—Å—Ç–∞. –ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ—Å—è —ç—Ç–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö NLP-–∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤ [–≥–ª–∞–≤–µ 7](/course/ru/chapter7)

<Tip>

üí° –í –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–º (–æ–±—ä–µ–∫—Ç–æ–º, —ç–ª–µ–º–µ–Ω—Ç–æ–º –≤—ã–±–æ—Ä–∫–∏) —è–≤–ª—è–µ—Ç—Å—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ _–ø—Ä–∏–∑–Ω–∞–∫–æ–≤_, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –¥–æ–ª–∂–Ω—ã –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö —ç—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±—É–¥–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∫–æ–ª–æ–Ω–æ–∫ –≤ `Dataset`, –∞ –≤ –¥—Ä—É–≥–∏—Ö (–∫–∞–∫ –≤ —Ç–µ–∫—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ –∏–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã) –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—É–¥—É—Ç —Å–æ—Ñ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã –∏–∑ –æ–¥–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞. 

</Tip>

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç! –í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –Ω–∞—à–∏ —Ç–µ–∫—Å—Ç—ã –∏ –æ–±—Ä–µ–∂–µ–º –∏—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤ 128, –æ–¥–Ω–∞–∫–æ –º—ã –ø–æ–ø—Ä–æ—Å–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–µ—Ä–Ω—É—Ç—å –Ω–∞–º *–≤—Å–µ* –ø–æ–ª—É—á–∏–≤—à–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª—å–Ω—ã–µ. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–¥–µ–ª–∞–Ω–æ —Å –ø–æ–º–æ—â—å—é –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ `return_overflowing_tokens=True`: 

```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º —ç—Ç–æ –Ω–∞ –æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–∂–¥–µ, —á–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Dataset.map()` –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ: 

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python out
[128, 49]
```

–ò—Ç–∞–∫, –Ω–∞—à –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç –≤ –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏ —Å—Ç–∞–ª —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ –¥–≤—É—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ç.–∫. —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª –Ω–µ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 128 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –Ω–æ –∏ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 49 —Ç–æ–∂–µ. –î–∞–≤–∞–π—Ç–µ –ø—Ä–∏–º–µ–Ω–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫–æ –≤—Å–µ–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python out
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

–û, –Ω–µ—Ç! –ù–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ! –ü–æ—á–µ–º—É? –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ—à–∏–±–∫—É: –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–∞—Ö, –æ–¥–∏–Ω –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –¥–ª–∏–Ω–æ–π 1463, –∞ –¥—Ä—É–≥–æ–π ‚Äì 1000. –ï—Å–ª–∏ –≤—ã –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) `Dataset.map()`, –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –æ–¥–Ω–æ –∏–∑ —ç—Ç–∏—Ö —á–∏—Å–µ–ª ‚Äì —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—Ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏, –∞ –¥—Ä—É–≥–æ–µ ‚Äì

Oh no! That didn't work! Why not? Looking at the error message will give us a clue: there is a mismatch in the lengths of one of the columns, one being of length 1,463 and the other of length 1,000. If you've looked at the  [documentation](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map), you may recall that it's the number of samples passed to the function that we are mapping; here those 1,000 examples gave 1,463 new features, resulting in a shape error.

–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –º—ã –ø—ã—Ç–∞–µ–º—Å—è —Å–º–µ—à–∞—Ç—å –¥–≤–∞ —Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ `drug_dataset` —Ä–∞–≤–Ω—è–µ—Ç—Å—è 1000, –∞ –Ω—É–∂–Ω—ã–π –Ω–∞–º `tokenized_dataset` –∏–º–µ–µ—Ç 1463 –∫–æ–ª–æ–Ω–∫–∏. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —ç—Ç–æ–π –æ—à–∏–±–∫–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–¥–∞–ª–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å–¥–µ–ª–∞—Ç—å –æ–±–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ú—ã –º–æ–∂–µ–º –¥–æ—Å—Ç–∏—á—å —ç—Ç–æ–≥–æ —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `remove_columns`: 

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

–¢–µ–ø–µ—Ä—å —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –æ—à–∏–±–æ–∫. –ú—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –Ω–∞—à –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–º–µ–µ—Ç –±–û–ª—å—à–µ–µ —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø—Ä–æ—Å—Ç–æ —Å—Ä–∞–≤–Ω–∏–º –¥–ª–∏–Ω—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: 

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

–ú—ã —É–ø–æ–º–∏–Ω–∞–ª–∏ –æ —Ç–æ–º, —á—Ç–æ –º—ã –º–æ–∂–µ–º —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º –¥–ª–∏–Ω –ø—É—Ç–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —á–∏—Å–ª–∞ –∫–æ–ª–æ–Ω–æ–∫ —Å—Ç–∞—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–µ `overflow_to_sample_mapping`, –∫–æ—Ç–æ—Ä–æ–µ –≤–µ—Ä–Ω–µ—Ç –Ω–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –µ—Å–ª–∏ –º—ã –∑–∞–¥–∞–¥–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç `return_overflowing_tokens=True`. –≠—Ç–æ –¥–∞—Å—Ç –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–Ω–¥–µ–∫—Å–æ–º –Ω–æ–≤—ã—Ö –∏ —Å—Ç–∞—Ä—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã —Å–º–æ–∂–µ–º –∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –∫–ª—é—á –Ω–∞—à–µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –∑–Ω–∞—á–µ–Ω–∏–π –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –ø–æ–≤—Ç–æ—Ä—è—è –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —Å—Ç–æ–ª—å–∫–æ —Ä–∞–∑, —Å–∫–æ–ª—å–∫–æ –æ–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:

–î–ª—è —ç—Ç–æ–≥–æ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –ø–æ–ª–µ `overflow_to_sample_mapping`, –∫–æ—Ç–æ—Ä–æ–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ–≥–¥–∞ –º—ã —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º `return_overflowing_tokens=True`. –≠—Ç–æ –¥–∞–µ—Ç –Ω–∞–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å –∏–Ω–¥–µ–∫—Å–æ–º –≤—ã–±–æ—Ä–∫–∏, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω –ø—Ä–æ–∏–∑–æ—à–µ–ª. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–æ, –º—ã –º–æ–∂–µ–º —Å–≤—è–∑–∞—Ç—å –∫–∞–∂–¥—ã–π –∫–ª—é—á, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –≤ –Ω–∞—à–µ–º –∏—Å—Ö–æ–¥–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —Å–æ —Å–ø–∏—Å–∫–æ–º –∑–Ω–∞—á–µ–Ω–∏–π –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –ø–æ–≤—Ç–æ—Ä—è—è –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —Å—Ç–æ–ª—å–∫–æ —Ä–∞–∑, —Å–∫–æ–ª—å–∫–æ –æ–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:

```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extract mapping between new and old indices
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

–ú—ã –º–æ–∂–µ–º —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —ç—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –≤ `Dataset.map()` –∏ –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤: 

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

–ú—ã –ø–æ–ª—É—á–∞–µ–º —Ç–æ –∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ, –Ω–æ –∑–¥–µ—Å—å –º—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤—Å–µ —Å—Ç–∞—Ä—ã–µ –ø–æ–ª—è. –ï—Å–ª–∏ –æ–Ω–∏ –≤–∞–º –Ω—É–∂–Ω—ã –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥.

–¢–µ–ø–µ—Ä—å –≤—ã –≤–∏–¥–µ–ª–∏, –∫–∞–∫ ü§ó Datasets –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏. –•–æ—Ç—è —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ü§ó Datasets –ø–æ–∫—Ä–æ—é—Ç –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –≤–∞—à–∏—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏,
–º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ Pandas, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ `DataFrame.groupby()` –∏–ª–∏ API –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. –ö —Å—á–∞—Å—Ç—å—é, ü§ó Datasets –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Ç–∞–∫–∏–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏, –∫–∞–∫ Pandas, NumPy, PyTorch, TensorFlow –∏ JAX. –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç.

## –û—Ç `Dataset`–∞ –∫ `DataFrame`–∞–º –∏ –Ω–∞–∑–∞–¥

<Youtube id="tfcY1067A5Q"/>

–î–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ ü§ó Datasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é `Dataset.set_format()`. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—è–µ—Ç _–≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç_ –¥–∞—Ç–∞—Å–µ—Ç–∞, —Ç–∞–∫ —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç –Ω–µ –∏–∑–º–µ–Ω—è—è —Å–∞–º—É _—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö_, –∫–æ—Ç–æ—Ä–∞—è –æ—Å—Ç–∞–µ—Ç—Å—è Apache Arrow. –°–º–µ–Ω–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç in place. –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç Pandas: 

```py
drug_dataset.set_format("pandas")
```

–¢–µ–ø–µ—Ä—å –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —ç–ª–µ–º–µ–Ω—Ç–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞ –º—ã –±—É–¥–µ–º –ø–æ–ª—É—á–∞—Ç—å `pandas.DataFrame` –≤–º–µ—Å—Ç–æ —Å–ª–æ–≤–∞—Ä—è: 

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

–î–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º `pandas.DataFrame` –¥–ª—è –≤—Å–µ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞, –≤—ã–±—Ä–∞–≤ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ `drug_dataset["train"]`:

```py
train_df = drug_dataset["train"][:]
```

<Tip>

üö® –í–Ω—É—Ç—Ä–∏ `Dataset.set_format()` –∏–∑–º–µ–Ω—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π –º–µ—Ç–æ–¥–æ–º `__getitem__()`. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–æ–≥–¥–∞ –º—ã —Ö–æ—Ç–∏–º —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, `train_df`, –∏–∑ `Dataset`, —Ñ–æ—Ä–º–∞—Ç–∞ `"pandas"`, –º—ã –¥–æ–ª–∂–Ω—ã —Å–¥–µ–ª–∞—Ç—å slice –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø–æ–ª—É—á–∏—Ç—å `pandas.DataFrame`. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ç–∏–ø `drug_dataset["train"]` ‚Äì —Ñ–æ—Ä–º–∞—Ç–∞ `Dataset`, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç (–∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–Ω–µ—Ç `pandas.DataFrame`). 

</Tip>

–ù–∞—á–∏–Ω–∞—è —Å —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å Pandas. –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –º–æ–∂–µ–º –∏–Ω–∞—á–µ –ø–æ—Å—á–∏—Ç–∞—Ç—å —Ä–∞—Å—Ä–µ–¥–µ–ª–µ–Ω–∏–µ `condition` —Å—Ä–µ–¥–∏ –Ω–∞—à–µ–π –≤—ã–±–æ—Ä–∫–∏: 

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>


–ò –∫–∞–∫ —Ç–æ–ª—å–∫–æ –º—ã –∑–∞–∫–æ–Ω—á–∏–º –Ω–∞—à –∞–Ω–∞–ª–∏–∑ Pandas, –º—ã –≤—Å–µ–≥–¥–∞ –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç `Dataset` —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `Dataset.from_pandas()` —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python out
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –í—ã—á–∏—Å–ª–∏—Ç–µ —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ –ø–æ–¥–Ω–æ–º—É –ª–µ–∫–∞—Ä—Å—Ç–≤—É –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Ç–∏–ø–∞ `Dataset`.

</Tip>

–ù–∞ —ç—Ç–æ–º –º—ã –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –Ω–∞—à –æ–±–∑–æ—Ä —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞, –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤ ü§ó Datasets. –ß—Ç–æ–±—ã –∑–∞–≤–µ—Ä—à–∏—Ç—å —ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª, –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é —á–∞—Å—Ç—å –≤—ã–±–æ—Ä–∫–∏. –ü—Ä–µ–∂–¥–µ, —á–µ–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ, –º—ã —Å–±—Ä–æ—Å–∏–º —Ñ–æ—Ä–º–∞—Ç `drug_dataset` –æ–±—Ä–∞—Ç–Ω–æ –∫ `"arrow"`: 

```python
drug_dataset.reset_format()
```

## –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏

–•–æ—Ç—è —É –Ω–∞—Å –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤–∞—è —á–∞—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞, –∫–æ—Ç–æ—Ä—É—é –º—ã –º–æ–≥–ª–∏ –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏, —Ö–æ—Ä–æ—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π —è–≤–ª—è–µ—Ç—Å—è –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–µ—Ç—Ä–æ–Ω—É—Ç—ã–º –∏ —Å–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –ö–∞–∫ —Ç–æ–ª—å–∫–æ –≤—ã –±—É–¥–µ—Ç–µ –¥–æ–≤–æ–ª—å–Ω—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é —Å–≤–æ–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, –≤—ã –º–æ–∂–µ—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–º–æ–≥–∞–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

ü§ó –ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—É–Ω–∫—Ü–∏—é `Dataset.train_test_split()`, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ `scikit-learn`. –î–∞–≤–∞–π—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, —á—Ç–æ–±—ã —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞—à –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ –æ–±—É—á–∞—é—â–∏–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π (–º—ã —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç `seed` –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏):

```py
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º "test"  –≤ "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# –î–æ–±–∞–≤–∏–º "test" –≤ –Ω–∞—à `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

–û—Ç–ª–∏—á–Ω–æ, —Ç–µ–ø–µ—Ä—å –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏. –í [—Ä–∞–∑–¥–µ–ª–µ 5](/course/ru/chapter5/5) –º—ã –ø–æ–∫–∞–∂–µ–º, –∫–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–∞ Hugging Face Hub, –∞ –ø–æ–∫–∞ –∑–∞–∫–æ–Ω—á–∏–º –Ω–∞—à –æ–±–∑–æ—Ä –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä. 

## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

<Youtube id="blF9uxYcKHo"/>

–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ ü§ó Datasets –±—É–¥—É—Ç –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–¥ –Ω–∏–º–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è, –±—É–¥—É—Ç —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –≤–∞–º –±—É–¥–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –¥–∏—Å–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –∫—ç—à –±—ã–ª –æ—á–∏—â–µ–Ω). –ö–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–∏–∂–µ,  ü§ó Datasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—Ä–∏ –≥–ª–∞–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö. 

| Data format |        Function        |
| :---------: | :--------------------: |
|    Arrow    | `Dataset.save_to_disk()` |
|     CSV     |    `Dataset.to_csv()`    |
|    JSON     |   `Dataset.to_json()`    |

–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–∏–º –Ω–∞—à –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Arrow: 

```py
drug_dataset_clean.save_to_disk("drug-reviews")
```

–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞—Å—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: 

```
drug-reviews/
‚îú‚îÄ‚îÄ dataset_dict.json
‚îú‚îÄ‚îÄ test
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îú‚îÄ‚îÄ train
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îú‚îÄ‚îÄ indices.arrow
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îî‚îÄ‚îÄ validation
    ‚îú‚îÄ‚îÄ dataset.arrow
    ‚îú‚îÄ‚îÄ dataset_info.json
    ‚îú‚îÄ‚îÄ indices.arrow
    ‚îî‚îÄ‚îÄ state.json
```

–≥–¥–µ –º—ã –º–æ–∂–µ–º —É–≤–∏–¥–µ—Ç—å –∫–∞–∂–¥—ã–π —Å–ø–ª–∏—Ç –¥–∞–Ω–Ω—ã—Ö, –∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ–π *dataset.arrow*, –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, —Ö—Ä–∞–Ω—è—â–∏–º–∏—Å—è –≤ —Ñ–∞–π–ª–∞—Ö *dataset_info.json* –∏ *state.json*. –í—ã –º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç Arrow –ø—Ä–æ—Å—Ç–æ –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–µ—Ä–µ–¥–∞—á–∏ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. 

–ü–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ–≥–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ `load_from_disk()` —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

–î–ª—è —Ñ–æ—Ä–º–∞—Ç–æ–≤ CSV –∏ JSON –º—ã –¥–æ–ª–∂–Ω—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–π —Å–ø–ª–∏—Ç –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª. –û–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å ‚Äì –ø—Ä–æ–∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ –∫–ª—é—á–∞–º –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º –≤ –æ–±—ä–µ–∫—Ç–µ `DatasetDict`: 

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

–≠—Ç–æ—Ç –∫–æ–¥ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –∫–∞–∂–¥—ã–π –±–ª–æ–∫ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [JSON Lines](https://jsonlines.org), –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ JSON-–æ–±—ä–µ–∫—Ç. –í–æ—Ç –∫–∞–∫ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –Ω–∞—à–µ–π –≤—ã–±–æ—Ä–∫–∏: 

```py
!head -n 1 drug-reviews-train.jsonl
```

```python out
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

–ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏—ë–º—ã –∏–∑ [—Ä–∞–∑–¥–µ–ª–∞ 2](/course/ru/chapter5/2) –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ JSON-—Ñ–∞–π–ª–æ–≤: 

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

–í–æ—Ç –∏ –≤—Å–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –Ω–∞—à–µ–≥–æ —ç–∫—Å–∫—É—Ä—Å–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å ü§ó Datasets! –ú—ã –æ—á–∏—Å—Ç–∏–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –≤–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–¥–µ–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–≥–ª–∏ –±—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ: 

1. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –∑–Ω–∞–Ω–∏—è –∏–∑ [—Ä–∞–∑–¥–µ–ª–∞ 3](/course/ru/chapter3) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞ –ø–æ –æ—Ç–∑—ã–≤—É –Ω–∞ –ª–µ–∫–∞—Ä—Å—Ç–≤–æ.
2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ pipeline `summarization` –∏–∑ [—Ä–∞–∑–¥–µ–ª–∞ 1](/course/ru/chapter1)–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏ –æ—Ç–∑—ã–≤–æ–≤. 

–î–∞–ª–µ–µ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ ü§ó Datasets –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –≤–∞–º –≤ —Ä–∞–±–æ—Ç–µ —Å –≥—Ä–æ–º–∞–¥–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ _–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ_ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –≤–∞—à–µ–º –Ω–æ—É—Ç–±—É–∫–µ!

