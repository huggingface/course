<FrameworkSwitchCourse {fw} />

# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–µ–¥–∏–Ω–æ[[putting-it-all-together]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
]} />

{/if}

–í –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö –º—ã —Å—Ç–∞—Ä–∞–ª–∏—Å—å –¥–µ–ª–∞—Ç—å –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã –≤—Ä—É—á–Ω—É—é. –ú—ã –∏–∑—É—á–∏–ª–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã, —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–æ –≤—Ö–æ–¥–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–∏, —É—Å–µ—á–µ–Ω–∏–∏ –∏ –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è.

–û–¥–Ω–∞–∫–æ, –∫–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ 2, ü§ó Transformers API –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —ç—Ç–æ –¥–ª—è –Ω–∞—Å —Å –ø–æ–º–æ—â—å—é –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–¥–µ—Å—å. –ö–æ–≥–¥–∞ –≤—ã –≤—ã–∑—ã–≤–∞–µ—Ç–µ —Å–≤–æ–π `tokenizer` –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –≤—Ö–æ–¥—ã, –≥–æ—Ç–æ–≤—ã–µ –∫ –ø–µ—Ä–µ–¥–∞—á–µ –≤ –≤–∞—à—É –º–æ–¥–µ–ª—å:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

–ó–¥–µ—Å—å –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è `model_inputs` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ, —á—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏. –î–ª—è DistilBERT —ç—Ç–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤—Ö–æ–¥–æ–≤, –∞ —Ç–∞–∫–∂–µ –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è. –í –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—Ö–æ–¥—ã, –æ–Ω–∏ —Ç–∞–∫–∂–µ –±—É–¥—É—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤—ã—Ö–æ–¥–∞–º–∏ –æ–±—ä–µ–∫—Ç–∞ `tokenizer`.

–ö–∞–∫ –º—ã —É–≤–∏–¥–∏–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –Ω–∏–∂–µ, —ç—Ç–æ –æ—á–µ–Ω—å –º–æ—â–Ω—ã–π –º–µ—Ç–æ–¥. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω –º–æ–∂–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–¥–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

–û–Ω —Ç–∞–∫–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ API:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

–û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ø–æ–ª–Ω–µ–Ω –∏—Å—Ö–æ–¥—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ü–µ–ª–µ–π:

```py
# –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
model_inputs = tokenizer(sequences, padding="longest")

# –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –º–æ–¥–µ–ª–∏
# (512 –¥–ª—è BERT –∏–ª–∏ DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ –∑–∞–¥–∞–Ω–Ω–æ–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

–û–Ω —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —É—Å–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# –£—Å–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –º–æ–¥–µ–ª–∏
# (512 –¥–ª—è BERT –∏–ª–∏ DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# –£—Å–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∑–∞–¥–∞–Ω–Ω—É—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

–û–±—ä–µ–∫—Ç `tokenizer` –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–∞–ø—Ä—è–º—É—é –ø–µ—Ä–µ–¥–∞–Ω—ã –≤ –º–æ–¥–µ–ª—å. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø—Ä–∏–º–µ—Ä–µ –∫–æ–¥–∞ –º—ã –∑–∞–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ - `"pt"` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã PyTorch, `"tf"` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã TensorFlow, –∞ `"np"` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤—ã NumPy:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# –í–µ—Ä–Ω—É—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# –í–µ—Ä–Ω—É—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# –í–µ—Ä–Ω—É—Ç—å –º–∞—Å—Å–∏–≤—ã NumPy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã[[special-tokens]]

–ï—Å–ª–∏ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤—Ö–æ–¥–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º, —Ç–æ —É–≤–∏–¥–∏–º, —á—Ç–æ –æ–Ω–∏ –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Ç–µ—Ö, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∞–ª–∏ —Ä–∞–Ω–µ–µ:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

–û–¥–∏–Ω –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–∞ –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–µ, –∞ –¥—Ä—É–≥–æ–π - –≤ –∫–æ–Ω—Ü–µ. –î–∞–≤–∞–π—Ç–µ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –¥–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤, –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ –≤—ã—à–µ, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –≤ —á–µ–º –¥–µ–ª–æ:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–±–∞–≤–∏–ª —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ `[CLS]` –≤ –Ω–∞—á–∞–ª–µ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ `[SEP]` –≤ –∫–æ–Ω—Ü–µ. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ —Å –Ω–∏–º–∏, –ø–æ—ç—Ç–æ–º—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ—Ö –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –Ω–∞–º –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ –∏—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –¥–æ–±–∞–≤–ª—è—é—Ç –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞; –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –¥–æ–±–∞–≤–ª—è—Ç—å —ç—Ç–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ —Ç–æ–ª—å–∫–æ –≤ –Ω–∞—á–∞–ª–µ –∏–ª–∏ —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ. –í –ª—é–±–æ–º —Å–ª—É—á–∞–µ, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–Ω–∞–µ—Ç, –∫–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –æ–∂–∏–¥–∞—é—Ç—Å—è, –∏ —Å–ø—Ä–∞–≤–∏—Ç—Å—è —Å —ç—Ç–∏–º —Å–∞–º.

## –ü–æ–¥–≤–µ–¥–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤: –û—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∫ –º–æ–¥–µ–ª–∏[[wrapping-up-from-tokenizer-to-model]]

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ –≤—Å–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç `tokenizer` –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏, –¥–∞–≤–∞–π—Ç–µ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –æ–Ω –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ (–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ!), –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ (—É—Å–µ—á–µ–Ω–∏–µ!) –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–∏–ø–∞–º–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–≤–æ–µ–≥–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ API:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
