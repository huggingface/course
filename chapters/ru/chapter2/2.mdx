<FrameworkSwitchCourse {fw} />

# –í–Ω—É—Ç—Ä–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Tip>
–≠—Ç–æ –ø–µ—Ä–≤—ã–π —Ä–∞–∑–¥–µ–ª, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥–µ—Ç –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ª–∏ –≤—ã PyTorch –∏–ª–∏ TensorFlow. –ù–∞–∂–º–∏—Ç–µ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å –Ω–∞–¥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É!
</Tip>

{#if fw === 'pt'}
<Youtube id="1pedAIvTWXk"/>
{:else}
<Youtube id="wVN12smEvqg"/>
{/if}

–î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Å –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞, –≤–∑–≥–ª—è–Ω—É–≤ –Ω–∞ —Ç–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –∑–∞ –∫—É–ª–∏—Å–∞–º–∏, –∫–æ–≥–¥–∞ –º—ã –≤—ã–ø–æ–ª–Ω—è–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥ –≤ [–ì–ª–∞–≤–µ 1](/course/chapter1):

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

–∏ –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–ª–∏:

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

–ö–∞–∫ –º—ã —É–∂–µ —É–≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 1](/course/chapter1), –¥–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç—Ä–∏ —à–∞–≥–∞: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –ø–µ—Ä–µ–¥–∞—á–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="–ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä NLP: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏ –≤—ã–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ Transformer –∏ "–≥–æ–ª–æ–≤—ã" –º–æ–¥–µ–ª–∏."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg" alt="–ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä NLP: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏ –≤—ã–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ Transformer –∏ "–≥–æ–ª–æ–≤—ã" –º–æ–¥–µ–ª–∏."/>
</div>

–î–∞–≤–∞–π—Ç–µ –≤–∫—Ä–∞—Ç—Ü–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –∫–∞–∂–¥—ã–π –∏–∑ —ç—Ç–∏—Ö —ç—Ç–∞–ø–æ–≤.

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

–ö–∞–∫ –∏ –¥—Ä—É–≥–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –º–æ–¥–µ–ª–∏ Transformer –Ω–µ –º–æ–≥—É—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–≤—ã–º —à–∞–≥–æ–º –Ω–∞—à–µ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —á–∏—Å–ª–∞, –ø–æ–Ω—è—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º *—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä*, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞:

- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–ª–æ–≤–∞, –ø–æ–¥—Å–ª–æ–≤–∞ –∏–ª–∏ —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è), –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è *—Ç–æ–∫–µ–Ω–∞–º–∏*
- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ
- –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏

–í—Å—é —ç—Ç—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ [Model Hub](https://huggingface.co/models). –î–ª—è —ç—Ç–æ–≥–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å `AutoTokenizer` –∏ –µ–≥–æ –º–µ—Ç–æ–¥ `from_pretrained()`. –ò—Å–ø–æ–ª—å–∑—É—è –∏–º—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏, –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª–∏, –∏ –∫—ç—à–∏—Ä—É–µ—Ç –∏—Ö (–ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ –∫–æ–¥–∞ –Ω–∏–∂–µ).

–ü–æ—Å–∫–æ–ª—å–∫—É –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–æ–π –∫–æ–Ω–≤–µ–π—Ä–∞ `sentiment-analysis` –ø–æ-—É–º–æ–ª—á–∞–Ω–∏—é —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å `distilbert-base-uncased-finetuned-sst-2-english` (–≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏ [–∑–¥–µ—Å—å](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), –º—ã –≤—ã–ø–æ–ª–Ω–∏–º —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–∞–Ω–¥—ã:

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

–ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã —Å–æ—Å–∑–¥–∞–¥–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –º—ã —Å–º–æ–∂–µ–º –Ω–∞–ø—Ä—è–º—É—é –ø–µ—Ä–µ–¥–∞—Ç—å –µ–º—É –Ω–∞—à–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏ –ø–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å, –≥–æ—Ç–æ–≤—ã–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏! –û—Å—Ç–∞–ª–æ—Å—å —Ç–æ–ª—å–∫–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä—ã.

–í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É ü§ó Transformers, –Ω–µ –±–µ—Å–ø–æ–∫–æ—è—Å—å –æ —Ç–æ–º, –∫–∞–∫–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ ML –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –±—ç–∫—ç–Ω–¥–∞; —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å PyTorch –∏–ª–∏ TensorFlow –∏–ª–∏ Flax –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–¥–Ω–∞–∫–æ –º–æ–¥–µ–ª–∏ Transformer –ø—Ä–∏–Ω–∏–º–∞—é—Ç —Ç–æ–ª—å–∫–æ *—Ç–µ–Ω–∑–æ—Ä—ã* –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ï—Å–ª–∏ –≤—ã –≤–ø–µ—Ä–≤—ã–µ —Å–ª—ã—à–∏—Ç–µ –æ —Ç–µ–Ω–∑–æ—Ä–∞—Ö, –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –∏—Ö –∫–∞–∫ –º–∞—Å—Å–∏–≤—ã NumPy. –ú–∞—Å—Å–∏–≤ NumPy –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∫–∞–ª—è—Ä–æ–º (0D), –≤–µ–∫—Ç–æ—Ä–æ–º (1D), –º–∞—Ç—Ä–∏—Ü–æ–π (2D) –∏–ª–∏ –∏–º–µ—Ç—å –±–æ–ª—å—à–µ –∏–∑–º–µ—Ä–µ–Ω–∏–π. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä; —Ç–µ–Ω–∑–æ—Ä—ã –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–µ–¥—É—Ç —Å–µ–±—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –∏ –æ–±—ã—á–Ω–æ –∏—Ö —Ç–∞–∫ –∂–µ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å, –∫–∞–∫ –º–∞—Å—Å–∏–≤—ã NumPy.


–ß—Ç–æ–±—ã —É–∫–∞–∑–∞—Ç—å —Ç–∏–ø —Ç–µ–Ω–∑–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –ø–æ–ª—É—á–∏—Ç—å (PyTorch, TensorFlow, –∏–ª–∏ –æ–±—ã—á–Ω—ã–π NumPy), –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç `return_tensors`:

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

–ù–µ –±–µ—Å–ø–æ–∫–æ–π—Ç–µ—Å—å –ø–æ–∫–∞ –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è (padding) –∏ —É—Å–µ—á–µ–Ω–∏—è (truncation); –º—ã –æ–±—ä—è—Å–Ω–∏–º —ç—Ç–æ –ø–æ–∑–∂–µ. –ó–¥–µ—Å—å –≥–ª–∞–≤–Ω–æ–µ –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ —É–∫–∞–∑–∞—Ç—å —Ç–∏–ø —Ç–µ–Ω–∑–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ (–µ—Å–ª–∏ —Ç–∏–ø –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–æ–≤).

{#if fw === 'pt'}

–í–æ—Ç –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–≥–ª—è–¥—è—Ç –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ PyTorch:

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

–í–æ—Ç –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–≥–ª—è–¥—è—Ç –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ TensorFlow:

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

–°–∞–º –≤—ã–≤–æ–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–≤–∞—Ä—å, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–≤–∞ –∫–ª—é—á–∞, `input_ids` –∏ `attention_mask`. `input_ids` —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–µ —Å—Ç—Ä–æ–∫–∏ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª (–ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ú—ã –æ–±—ä—è—Å–Ω–∏–º, —á—Ç–æ —Ç–∞–∫–æ–µ `attention_mask` –ø–æ–∑–∂–µ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ. 

## –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å

{#if fw === 'pt'}
–ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞—à—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–ª–∞—Å—Å `AutoModel` –∫–æ—Ç–æ—Ä—ã–π —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥ `from_pretrained()`:

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
–ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞—à—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–ª–∞—Å—Å `TFAutoModel` –∫–æ—Ç–æ—Ä—ã–π —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥ `from_pretrained`:

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

–í —ç—Ç–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –∫–æ–¥–∞ –º—ã –∑–∞–≥—Ä—É–∑–∏–ª–∏ —Ç—É –∂–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É, –∫–æ—Ç–æ—Ä—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ –Ω–∞—à–µ–º –∫–æ–Ω–≤–µ–π–µ—Ä–µ —Ä–∞–Ω–µ–µ (–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –æ–Ω–∞ —É–∂–µ –¥–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –±—ã—Ç—å –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∞) –∏ —Å–æ–∑–¥–∞–ª–∏ —Å –µ–µ –ø–æ–º–æ—â—å—é —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏.

–≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–π –º–æ–¥—É–ª—å Transformer: –ø—Ä–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–Ω –≤—ã–≤–æ–¥–∏—Ç —Ç–æ, —á—Ç–æ –º—ã –±—É–¥–µ–º –Ω–∞–∑—ã–≤–∞—Ç—å *—Å–∫—Ä—ã—Ç—ã–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏*, —Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –∫–∞–∫ *–ø–∞—Ä–∞–º–µ—Ç—Ä—ã*. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏ –º—ã –ø–æ–ª—É—á–∏–º –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–æ–≥–æ –≤—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º–æ–¥–µ–ª—å—é Transformer**.

–ï—Å–ª–∏ –≤—ã –ø–æ–∫–∞ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ –≤ —á–µ–º —Å–º—ã—Å–ª, –Ω–µ –±–µ—Å–ø–æ–∫–æ–π—Ç–µ—Å—å –æ–± —ç—Ç–æ–º. –ú—ã –æ–±—ä—è—Å–Ω–∏–º –≤—Å–µ —ç—Ç–æ –ø–æ–∑–∂–µ.

–•–æ—Ç—è —ç—Ç–∏ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã —Å–∞–º–∏ –ø–æ —Å–µ–±–µ, –æ–Ω–∏ –æ–±—ã—á–Ω–æ —è–≤–ª—è—é—Ç—Å—è –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –¥—Ä—É–≥–æ–π —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∫–∞–∫ *–≥–æ–ª–æ–≤–∞*. –í [–ì–ª–∞–≤–µ 1](/course/chapter1) —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –º–æ–≥–ª–∏ –±—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è —Å –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –Ω–æ —Å –∫–∞–∂–¥–æ–π –∏–∑ —ç—Ç–∏—Ö –∑–∞–¥–∞—á –±—É–¥–µ—Ç —Å–≤—è–∑–∞–Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–∞—è –≥–æ–ª–æ–≤–∞.

### –ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä, —á—Ç–æ —ç—Ç–æ?

The vector output by the Transformer module is usually large. It generally has three dimensions:

- **Batch size**: The number of sequences processed at a time (2 in our example).
- **Sequence length**: The length of the numerical representation of the sequence (16 in our example).
- **Hidden size**: The vector dimension of each model input.

It is said to be "high dimensional" because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).

We can see this if we feed the inputs we preprocessed to our model:

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

Note that the outputs of ü§ó Transformers models behave like `namedtuple`s or dictionaries. You can access the elements by attributes (like we did) or by key (`outputs["last_hidden_state"]`), or even by index if you know exactly where the thing you are looking for is (`outputs[0]`).

### Model heads: Making sense out of numbers

The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" alt="A Transformer network alongside its head."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg" alt="A Transformer network alongside its head."/>
</div>

The output of the Transformer model is sent directly to the model head to be processed.

In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.

There are many different architectures available in ü§ó Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:

- `*Model` (retrieve the hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- and others ü§ó

{#if fw === 'pt'}
For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won't actually use the `AutoModel` class, but `AutoModelForSequenceClassification`:

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won't actually use the `TFAutoModel` class, but `TFAutoModelForSequenceClassification`:

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}
```python out
torch.Size([2, 2])
```
{:else}
```python out
(2, 2)
```
{/if}

Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.

## Postprocessing the output

The values we get as output from our model don't necessarily make sense by themselves. Let's take a look:

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

Our model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` for the second one. Those are not probabilities but *logits*, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

Now we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0.9995,  0.0005]` for the second one. These are recognizable probability scores.

To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config (more on this in the next section):

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

Now we can conclude that the model predicted the following:
 
- First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598
- Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005

We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let's take some time to dive deeper into each of those steps.

<Tip>

‚úèÔ∏è **Try it out!** Choose two (or more) texts of your own and run them through the `sentiment-analysis` pipeline. Then replicate the steps you saw here yourself and check that you obtain the same results!

</Tip>
