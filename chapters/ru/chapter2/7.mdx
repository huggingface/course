# –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!

–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞, –≤—ã –ø—Ä–æ—à–ª–∏ –∫—É—Ä—Å –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞! –ù–∞–ø–æ–º–Ω–∏–º, —á—Ç–æ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ –≤—ã:

- –ò–∑—É—á–∏–ª –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –º–æ–¥–µ–ª–∏  Transformer.
- Learned what makes up a tokenization pipeline.
- Saw how to use a Transformer model in practice.
- Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.
- Set up a tokenizer and a model together to get from text to predictions.
- Learned the limitations of input IDs, and learned about attention masks.
- Played around with versatile and configurable tokenizer methods.

From now on, you should be able to freely navigate the ü§ó Transformers docs: the vocabulary will sound familiar, and you've already seen the methods that you'll use the majority of the time.
