# –í–≤–µ–¥–µ–Ω–∏–µ

–ö–∞–∫ –≤—ã –º–æ–≥–ª–∏ –∑–∞–º–µ—Ç–∏—Ç—å –≤ [–ì–ª–∞–≤–µ 1](/course/chapter1), –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –æ–±—ã—á–Ω–æ –±—ã–≤–∞—é—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ. –û–±—É—á–µ–Ω–∏–µ –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –∏ –¥–∞–∂–µ –¥–µ—Å—è—Ç–∫–∞–º–∏ *–º–∏–ª–ª–∏–∞—Ä–¥–æ–≤* –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã–ø—É—Å–∫–∞—é—Ç—Å—è –ø–æ—á—Ç–∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ, –∏ –∫–∞–∂–¥–∞—è –∏–∑ –Ω–∏—Ö –∏–º–µ–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é, –æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Ö –≤—Å–µ ‚Äî –Ω–µ–ø—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞.

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Transformers –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã. –ï—ë —Ü–µ–ª—å ‚Äî –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –µ–¥–∏–Ω—ã–π API, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å, –æ–±—É—á–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—é–±—É—é –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –û—Å–Ω–æ–≤–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —è–≤–ª—è—é—Ç—Å—è:

- **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ, –∑–∞–≥—Ä—É–∑–∫—É –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ NLP –¥–ª—è –≤—ã–≤–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö, –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤—Å–µ–≥–æ –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏ –∫–æ–¥–∞.
- **–ì–∏–±–∫–æ—Å—Ç—å**: –ü–æ —Å–≤–æ–µ–π —Å—É—Ç–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø—Ä–æ—Å—Ç—ã–µ –∫–ª–∞—Å—Å—ã –±–∏–±–ª–∏–æ—Ç–µ–∫ PyTorch `nn.Module` –∏–ª–∏ TensorFlow `tf.keras.Model` –∏ –º–æ–≥—É—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è, –∫–∞–∫ –∏ –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏, –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Ä–µ–¥–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–ú–û).
- **–ü—Ä–æ—Å—Ç–æ—Ç–∞**: Hardly any abstractions are made across the library. The "All in one file" is a core concept: a model's forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.

This last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules 
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.

This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in [Chapter 1](/course/chapter1). Next, we'll discuss the model API: we'll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions. 

Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.

<Tip>
‚ö†Ô∏è In order to benefit from all features available with the Model Hub and ü§ó Transformers, we recommend <a href="https://huggingface.co/join">creating an account</a>.
</Tip>