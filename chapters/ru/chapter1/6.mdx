# Модели декодеров

<Youtube id="d_ixlCubqQw" />

Декодировщики используют только компонент декодер трансформера. На каждом этапе для текущего слова слой внимания может получить доступ только к словам, которые были расположены до текущего в предложении. Такие модели часто называются *авторегрессионными моделями*.

Процесс предобучения декодеров обычно заключается в предсказании следующего слова в предложении. ё

Такие модели лучше всего подходят для задач, связанных с генерацией текста. 

Представителями этого семейства моделей являются:

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)
- [GPT](https://huggingface.co/transformers/model_doc/gpt.html)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)
