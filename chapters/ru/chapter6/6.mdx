# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è WordPiece[[wordpiece-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"},
]} />

WordPiece - —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π Google –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è BERT. –í–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ –æ–Ω –±—ã–ª –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤–æ –º–Ω–æ–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ BERT, —Ç–∞–∫–∏—Ö –∫–∞–∫ DistilBERT, MobileBERT, Funnel Transformers –∏ MPNET. –û–Ω –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ BPE –≤ –ø–ª–∞–Ω–µ –æ–±—É—á–µ–Ω–∏—è, –Ω–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ-–¥—Ä—É–≥–æ–º—É.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

üí° –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è WordPiece, –≤–ø–ª–æ—Ç—å –¥–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ–≥–æ, –µ—Å–ª–∏ –≤–∞–º –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–±—â–∏–π –æ–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è[[training-algorithm]]

<Tip warning={true}>

‚ö†Ô∏è Google –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–ª –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–≤–æ–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—É—á–µ–Ω–∏—è WordPiece, –ø–æ—ç—Ç–æ–º—É –≤—Å–µ –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–µ - —ç—Ç–æ –Ω–∞—à–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö. –í–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ —Ç–æ—á–Ω—ã –Ω–µ –Ω–∞ 100 %.

</Tip>

–ö–∞–∫ –∏ BPE, WordPiece –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –≤–∫–ª—é—á–∞—é—â–µ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª—å—é, –∏ –Ω–∞—á–∞–ª—å–Ω—ã–π –∞–ª—Ñ–∞–≤–∏—Ç. –ü–æ—Å–∫–æ–ª—å–∫—É –º–æ–¥–µ–ª—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–æ–¥—Å–ª–æ–≤–∞ –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–∞ (–∫–∞–∫ `##` –¥–ª—è BERT), –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏ –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –∫–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞. –¢–∞–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, `"word"` —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```
w ##o ##r ##d
```

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞—á–∞–ª—å–Ω—ã–π –∞–ª—Ñ–∞–≤–∏—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞, –∏ —Å–∏–º–≤–æ–ª—ã, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å WordPiece.

–ó–∞—Ç–µ–º, –∫–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å BPE, WordPiece –∏–∑—É—á–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–ø–æ—Å–æ–±–µ –≤—ã–±–æ—Ä–∞ –ø–∞—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –ø–∞—Ä—É, WordPiece —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –ø–æ —Å–ª–µ–¥—É—é—â–µ–π —Ñ–æ—Ä–º—É–ª–µ:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

–î–µ–ª—è —á–∞—Å—Ç–æ—Ç—É –ø–∞—Ä—ã –Ω–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –∫–∞–∂–¥–æ–π –∏–∑ –µ–µ —á–∞—Å—Ç–µ–π, –∞–ª–≥–æ—Ä–∏—Ç–º –æ—Ç–¥–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Å–ª–∏—è–Ω–∏—é –ø–∞—Ä, –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –∫–æ—Ç–æ—Ä—ã—Ö –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ —Ä–µ–∂–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç `("un", "##able")`, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–∞ –ø–∞—Ä–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ –æ—á–µ–Ω—å —á–∞—Å—Ç–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–≤–µ –ø–∞—Ä—ã `"un"` –∏ `"##able"`, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤ –∏ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫—É—é —á–∞—Å—Ç–æ—Ç—É. –ù–∞–ø—Ä–æ—Ç–∏–≤, —Ç–∞–∫–∞—è –ø–∞—Ä–∞, –∫–∞–∫ `("hu", "##gging")`, –≤–µ—Ä–æ—è—Ç–Ω–æ, –±—É–¥–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ –±—ã—Å—Ç—Ä–µ–µ (–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ —Å–ª–æ–≤–æ  "hugging" —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ), –ø–æ—Å–∫–æ–ª—å–∫—É `"hu"` –∏ `"##gging"` –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ.

–î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç–æ—Ç –∂–µ —Å–ª–æ–≤–∞—Ä—å, –∫–æ—Ç–æ—Ä—ã–π –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ —É—á–µ–±–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

–†–∞–±–∏–µ–Ω–∏–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

–ø–æ—ç—Ç–æ–º—É –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç –∏–º–µ—Ç—å –≤–∏–¥ `["b", "h", "p", "##g", "##n", "##s", "##u"]` (–µ—Å–ª–∏ –º—ã –ø–æ–∫–∞ –∑–∞–±—É–¥–µ–º –æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö). –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞ - `("##u", "##g")` (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 20 —Ä–∞–∑), –Ω–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ `"##u"` –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞, –ø–æ—ç—Ç–æ–º—É –µ–µ –æ—Ü–µ–Ω–∫–∞ –Ω–µ —Å–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è (–æ–Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1/36). –í—Å–µ –ø–∞—Ä—ã —Å `"##u"` —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–º–µ—é—Ç —Ç–∞–∫—É—é –∂–µ –æ—Ü–µ–Ω–∫—É (1/36), –ø–æ—ç—Ç–æ–º—É –ª—É—á—à—É—é –æ—Ü–µ–Ω–∫—É –ø–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞ `("##g", "##s")` - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è, –≤ –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç `"##u"` - —Å –æ—Ü–µ–Ω–∫–æ–π 1/20, –∏ –ø–µ—Ä–≤—ã–º –≤—ã—É—á–µ–Ω–Ω—ã–º —Å–ª–∏—è–Ω–∏–µ–º –±—É–¥–µ—Ç `("##g", "##s") -> ("##gs")`.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø—Ä–∏ —Å–ª–∏—è–Ω–∏–∏ –º—ã —É–¥–∞–ª—è–µ–º `##` –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ–∫–µ–Ω–∞–º–∏, –ø–æ—ç—Ç–æ–º—É –º—ã –¥–æ–±–∞–≤–ª—è–µ–º `"##gs"` –≤ —Å–ª–æ–≤–∞—Ä—å –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–ª–∏—è–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ö –∫–æ—Ä–ø—É—Å–∞:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

–í —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç `"##u"` –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–æ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–∞—Ä–∞—Ö, –ø–æ—ç—Ç–æ–º—É –≤—Å–µ –æ–Ω–∏ –ø–æ–ª—É—á–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –±–∞–ª–ª. –î–æ–ø—É—Å—Ç–∏–º, –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø–µ—Ä–≤–∞—è –ø–∞—Ä–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è, —Ç–∞–∫ —á—Ç–æ `("h", "##u") -> "hu"`. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –Ω–∞—Å –∫:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

–ó–∞—Ç–µ–º —Å–ª–µ–¥—É—é—â—É—é –ª—É—á—à—É—é –æ—Ü–µ–Ω–∫—É —Ä–∞–∑–¥–µ–ª—è—é—Ç `("hu", "##g")` –∏ `("hu", "##gs")` (1/15, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 1/21 –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–∞—Ä), –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–≤–∞—è –ø–∞—Ä–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –æ—Ü–µ–Ω–∫–æ–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

–∏ –º—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–∞–∫ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è.

<Tip>

‚úèÔ∏è **–¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –æ—á–µ—Ä–µ–¥—å!** –ö–∞–∫–∏–º –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ —Å–ª–∏—è–Ω–∏—è?

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏[[tokenization-algorithm]]

–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤ WordPiece –∏ BPE –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è —Ç–µ–º, —á—Ç–æ WordPiece —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –∫–æ–Ω–µ—á–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å, –∞ –Ω–µ –≤—ã—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è. –ù–∞—á–∏–Ω–∞—è —Å–æ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, WordPiece –Ω–∞—Ö–æ–¥–∏—Ç —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –ø–æ–¥—Å–ª–æ–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, –∞ –∑–∞—Ç–µ–º —Ä–∞–∑–±–∏–≤–∞–µ—Ç –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å, –∏–∑—É—á–µ–Ω–Ω—ã–π –≤ –ø—Ä–∏–º–µ—Ä–µ –≤—ã—à–µ, –¥–ª—è —Å–ª–æ–≤–∞ `" hugs"` —Å–∞–º—ã–º –¥–ª–∏–Ω–Ω—ã–º –ø–æ–¥—Å–ª–æ–≤–æ–º, –Ω–∞—á–∏–Ω–∞—è —Å –Ω–∞—á–∞–ª–∞, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ, —è–≤–ª—è–µ—Ç—Å—è `"hug"`, –ø–æ—ç—Ç–æ–º—É –º—ã –¥–µ–ª–∏–º –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏ –∏ –ø–æ–ª—É—á–∞–µ–º `["hug", "##s"]`. –ó–∞—Ç–µ–º –º—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å `"##s"`, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ, –ø–æ—ç—Ç–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è `"hugs"` –±—É–¥–µ—Ç `["hug", "##s"]`.

–í BPE –º—ã –±—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Å–ª–∏—è–Ω–∏—è, –≤—ã—É—á–µ–Ω–Ω—ã–µ –ø–æ –ø–æ—Ä—è–¥–∫—É, –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —ç—Ç–æ –∫–∞–∫ `["hu", "##gs"]`, –ø–æ—ç—Ç–æ–º—É –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è.

–í –∫–∞—á–µ—Å—Ç–≤–µ –¥—Ä—É–≥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤–æ `"bugs"`. `"b"` - —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –ø–æ–¥—Å–ª–æ–≤–æ, –Ω–∞—á–∏–Ω–∞—é—â–µ–µ—Å—è —Å –Ω–∞—á–∞–ª–∞ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä–æ–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, –ø–æ—ç—Ç–æ–º—É –º—ã –¥–µ–ª–∏–º –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏ –∏ –ø–æ–ª—É—á–∞–µ–º `["b", "##ugs"]`. –ó–∞—Ç–µ–º `"##u"` - —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –ø–æ–¥—Å–ª–æ–≤–æ, –Ω–∞—á–∏–Ω–∞—é—â–µ–µ—Å—è –≤ –Ω–∞—á–∞–ª–µ `"##ugs"`, –∫–æ—Ç–æ—Ä–æ–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, –ø–æ—ç—Ç–æ–º—É –º—ã –¥–µ–ª–∏–º –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏ –∏ –ø–æ–ª—É—á–∞–µ–º `["b", "##u", "##gs"]`. –ù–∞–∫–æ–Ω–µ—Ü, `"##gs"` –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ä–µ, —Ç–∞–∫ —á—Ç–æ —ç—Ç–æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ø–∏—Å–æ–∫ —è–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–æ–º `"bugs"`.

–ö–æ–≥–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–æ—Ö–æ–¥–∏—Ç –¥–æ —Å—Ç–∞–¥–∏–∏, –∫–æ–≥–¥–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ–¥—Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ, –≤—Å–µ —Å–ª–æ–≤–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ - —Ç–∞–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, `"mug"` –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–∫ `["[UNK]"]`, –∫–∞–∫ –∏ `"bum"` (–¥–∞–∂–µ –µ—Å–ª–∏ –º—ã –º–æ–∂–µ–º –Ω–∞—á–∞—Ç—å —Å `"b"` –∏ `"##u"`, `"##m"` –Ω–µ –≤—Ö–æ–¥–∏—Ç –≤ —Å–ª–æ–≤–∞—Ä—å, –∏ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π —Ç–æ–∫–µ–Ω –±—É–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ `["[UNK]"]`, –∞ –Ω–µ `["b", "##u", "[UNK]"]`). –≠—Ç–æ –µ—â–µ –æ–¥–Ω–æ –æ—Ç–ª–∏—á–∏–µ –æ—Ç BPE, –∫–æ—Ç–æ—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∫–∞–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–æ–ª—å–∫–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ.

<Tip>

‚úèÔ∏è **–¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –æ—á–µ—Ä–µ–¥—å!** –ö–∞–∫ –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤–æ `"pugs"`?

</Tip>

## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è WordPiece[[implementing-wordpiece]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ WordPiece. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å BPE, —ç—Ç–æ –≤—Å–µ–≥–æ –ª–∏—à—å —É—á–µ–±–Ω—ã–π –ø—Ä–∏–º–µ—Ä, –∏ –≤—ã –Ω–µ —Å–º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ.

–ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ –∫–æ—Ä–ø—É—Å, —á—Ç–æ –∏ –≤ –ø—Ä–∏–º–µ—Ä–µ —Å BPE:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

–í–æ-–ø–µ—Ä–≤—ã—Ö, –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä–ø—É—Å –≤ —Å–ª–æ–≤–∞. –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä WordPiece (–Ω–∞–ø—Ä–∏–º–µ—Ä, BERT), –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `bert-base-cased`:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

–ó–∞—Ç–µ–º –º—ã –≤—ã—á–∏—Å–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ, –∫–∞–∫ –∏ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

–ö–∞–∫ –º—ã —É–∂–µ –≤–∏–¥–µ–ª–∏, –∞–ª—Ñ–∞–≤–∏—Ç - —ç—Ç–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ, —Å–æ—Å—Ç–æ—è—â–µ–µ –∏–∑ –≤—Å–µ—Ö –ø–µ—Ä–≤—ã—Ö –±—É–∫–≤ —Å–ª–æ–≤ –∏ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –±—É–∫–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Å–ª–æ–≤–∞—Ö —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

–ú—ã —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª—å—é, –≤ –Ω–∞—á–∞–ª–æ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –í —Å–ª—É—á–∞–µ BERT —ç—Ç–æ —Å–ø–∏—Å–æ–∫ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

–î–∞–ª–µ–µ –Ω–∞–º –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –Ω–∞ —á–∞—Å—Ç–∏, –ø—Ä–∏ —ç—Ç–æ–º –≤—Å–µ –±—É–∫–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø–µ—Ä–≤—ã–º–∏, –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é, –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã. –ù–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —á–∞—Å—Ç—å —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –ø–æ—Å–ª–µ –ø–µ—Ä–≤—ã—Ö —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

–¢–µ–ø–µ—Ä—å –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø–∞—Ä—É —Å –Ω–∞–∏–ª—É—á—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º, –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ –ª–∏—à—å —Å–¥–µ–ª–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ü–∏–∫–ª:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

–ò—Ç–∞–∫, –ø–µ—Ä–≤–æ–µ —Å–ª–∏—è–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤—ã—É—á–∏—Ç—å, —ç—Ç–æ `('a', '##b') -> 'ab'`, –∏ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º `'ab'` –≤ —Å–ª–æ–≤–∞—Ä—å:

```python
vocab.append("ab")
```

–ß—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å, –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–æ —Å–ª–∏—è–Ω–∏–µ –≤ –Ω–∞—à–µ–º —Å–ª–æ–≤–∞—Ä–µ `splits`. –î–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º –¥–ª—è —ç—Ç–æ–≥–æ –µ—â–µ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

–ò –º—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–≤–æ–≥–æ —Å–ª–∏—è–Ω–∏—è:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ, —á—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –∑–∞—Ü–∏–∫–ª–∏–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –º—ã –Ω–µ –≤—ã—É—á–∏–º –≤—Å–µ —Å–ª–∏—è–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω—É–∂–Ω—ã. –î–∞–≤–∞–π—Ç–µ –Ω–∞—Ü–µ–ª–∏–º—Å—è –Ω–∞ —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ä–∞–≤–Ω—ã–π 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å BPE —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—ã—Å—Ç—Ä–µ–µ –≤—ã—É—á–∏–≤–∞–µ—Ç —á–∞—Å—Ç–∏ —Å–ª–æ–≤ –∫–∞–∫ —Ç–æ–∫–µ–Ω—ã.

<Tip>

üí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `train_new_from_iterator()` –Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ –Ω–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–º—É –∂–µ —Å–ª–æ–≤–∞—Ä—é. –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Ç–æ–º—É, —á—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –Ω–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç WordPiece –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–≤–µ—Ä–µ–Ω—ã –≤ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ), –∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–º–µ—Å—Ç–æ –Ω–µ–≥–æ BPE.

</Tip>

–ß—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç, –º—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –µ–≥–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É. –¢–æ –µ—Å—Ç—å –Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –º—ã –∏—â–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ –ø–æ–¥—Å–ª–æ–≤–æ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏, –∑–∞—Ç–µ–º –º—ã –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç–∏, –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ –¥–ª—è –æ—Å—Ç–∞–≤—à–µ–π—Å—è —á–∞—Å—Ç–∏ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞ –∏ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –æ–¥–Ω–æ–º —Å–ª–æ–≤–µ, –∫–æ—Ç–æ—Ä–æ–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, –∏ –Ω–∞ –¥—Ä—É–≥–æ–º, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

–ú—ã –º–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –µ–≥–æ –Ω–∞ –ª—é–±–æ–º —Ç–µ–∫—Å—Ç–µ:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

–í–æ—Ç –∏ –≤—Å–µ –æ–± –∞–ª–≥–æ—Ä–∏—Ç–º–µ WordPiece! –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ Unigram.
