# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –±–ª–æ–∫ –∑–∞ –±–ª–æ–∫–æ–º[[building-a-tokenizer-block-by-block]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb"},
]} />

–ö–∞–∫ –º—ã —É–∂–µ –≤–∏–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤:

- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ª—é–±–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –∏–ª–∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode –∏ —Ç. –¥.)
- –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞).
- –ü—Ä–æ–≥–æ–Ω –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤)
- –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤)

–í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–æ—Ç –µ—â–µ –æ–¥–∏–Ω –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —à–∞–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ —Å–º–µ—à–∏–≤–∞—Ç—å –∏ —Å–æ—á–µ—Ç–∞—Ç—å –º–µ–∂–¥—É —Å–æ–±–æ–π. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –Ω—É–ª—è, –∞ –Ω–µ –æ–±—É—á–∞—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ä–æ–≥–æ, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ –≤ [—Ä–∞–∑–¥–µ–ª–µ 2](../chapter6/2). –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –≤—ã —Å–º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞—Ç—å –ª—é–±–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ–ª—å–∫–æ —Å–º–æ–∂–µ—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å!

<Youtube id="MR8tZm5ViWU"/>

–¢–æ—á–Ω–µ–µ, –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –≤–æ–∫—Ä—É–≥ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ `Tokenizer`, –∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –≤ –ø–æ–¥–º–æ–¥—É–ª–∏:

- `normalizers` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–∏–ø—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–∞ `Normalizer`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/api/normalizers)).
- `pre_tokenizers` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ `PreTokenizer`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)).
- `models` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –º–æ–¥–µ–ª–µ–π `Model`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —Ç–∞–∫–∏–µ –∫–∞–∫ `BPE`, `WordPiece` –∏ `Unigram` (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/api/models)).
- `trainers` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã `Trainer`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏; –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/api/trainers)).
- `post_processors` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ `PostProcessor`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/api/post-processors)).
- `decoders` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –¥–µ–∫–æ–¥–µ—Ä–æ–≤ `Decoder`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/components#decoders)).

–í–µ—Å—å —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ [–∑–¥–µ—Å—å](https://huggingface.co/docs/tokenizers/components).

## –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–∞[[acquiring-a-corpus]]

–î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ (—á—Ç–æ–±—ã –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω—è–ª–∏—Å—å –±—ã—Å—Ç—Ä–æ). –®–∞–≥–∏ –ø–æ —Å–±–æ—Ä—É –∫–æ—Ä–ø—É—Å–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã —Ç–µ–º, —á—Ç–æ –º—ã –¥–µ–ª–∞–ª–∏ –≤ [–Ω–∞—á–∞–ª–µ —ç—Ç–æ–π –≥–ª–∞–≤—ã](../chapter6/2), –Ω–æ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

–§—É–Ω–∫—Ü–∏—è `get_training_corpus()` - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–¥–∞–µ—Ç –±–∞—Ç—á –∏–∑ 1000 —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. 

ü§ó –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö. –í–æ—Ç –∫–∞–∫ –º—ã –º–æ–∂–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤—Å–µ —Ç–µ–∫—Å—Ç—ã/–≤—Ö–æ–¥—ã –∏–∑ WikiText-2, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

–î–∞–ª–µ–µ –º—ã –ø–æ–∫–∞–∂–µ–º –≤–∞–º, –∫–∞–∫ –±–ª–æ–∫ –∑–∞ –±–ª–æ–∫–æ–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã BERT, GPT-2 –∏ XLNet. –≠—Ç–æ –¥–∞—Å—Ç –Ω–∞–º –ø—Ä–∏–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: WordPiece, BPE –∏ Unigram. –ù–∞—á–Ω–µ–º —Å BERT!

## –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ WordPiece —Å –Ω—É–ª—è[[building-a-wordpiece-tokenizer-from-scratch]]

–ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Tokenizers, –º—ã –Ω–∞—á–Ω–µ–º —Å –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ `Tokenizer` –∏ `model`, –∑–∞—Ç–µ–º —É—Å—Ç–∞–Ω–æ–≤–∏–º –¥–ª—è –∏—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ `normalizer`, `pre_tokenizer`, `post_processor` –∏ `decoder` –Ω—É–∂–Ω—ã–µ –Ω–∞–º –∑–Ω–∞—á–µ–Ω–∏—è.

–î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã —Å–æ–∑–¥–∞–¥–∏–º `Tokenizer` —Å –º–æ–¥–µ–ª—å—é WordPiece:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

–ú—ã –¥–æ–ª–∂–Ω—ã —É–∫–∞–∑–∞—Ç—å `unk_token`, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –∑–Ω–∞–ª–∞, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å, –∫–æ–≥–¥–∞ –æ–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–Ω—å—à–µ –Ω–µ –≤–∏–¥–µ–ª–∞. –î—Ä—É–≥–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –∑–¥–µ—Å—å, –≤–∫–ª—é—á–∞—é—Ç `vocab` –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –µ–≥–æ –∑–∞–¥–∞–≤–∞—Ç—å) –∏ `max_input_chars_per_word`, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (—Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –±—É–¥—É—Ç —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ —á–∞—Å—Ç–∏).

–ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –ø–æ—ç—Ç–æ–º—É –Ω–∞—á–Ω–µ–º —Å –Ω–µ–µ. –ü–æ—Å–∫–æ–ª—å–∫—É BERT —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç `BertNormalizer` —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–ª—è BERT: `lowercase` –∏ `strip_accents`, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –ø–æ—è—Å–Ω–µ–Ω–∏–π; `clean_text` –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –≤—Å–µ—Ö —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∑–∞–º–µ–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω; –∏ `handle_chinese_chars`, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤. –ß—Ç–æ–±—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `bert-base-uncased`, –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç—Ç–æ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

–û–¥–Ω–∞–∫–æ, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —É –≤–∞—Å –Ω–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç–∞–∫–æ–º—É —É–¥–æ–±–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä—É, —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Tokenizers, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä BERT –≤—Ä—É—á–Ω—É—é. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä `Lowercase` –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä `StripAccents`, –∏ –≤—ã –º–æ–∂–µ—Ç–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

–ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä Unicode `NFD`, –ø–æ—Å–∫–æ–ª—å–∫—É –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä `StripAccents` –Ω–µ —Å–º–æ–∂–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω–µ —É–¥–∞–ª–∏—Ç –∏—Ö.

–ö–∞–∫ –º—ã —É–∂–µ –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `normalize_str()` –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ –æ–Ω –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:

```python
print(tokenizer.normalizer.normalize_str("H√©ll√≤ h√¥w are √º?"))
```

```python out
hello how are u?
```

<Tip>

**–î–∞–ª–µ–µ** –µ—Å–ª–∏ –≤—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç–µ –¥–≤–µ –≤–µ—Ä—Å–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π —Å–∏–º–≤–æ–ª Unicode `u"\u0085"`, —Ç–æ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ –∑–∞–º–µ—Ç–∏—Ç–µ, —á—Ç–æ —ç—Ç–∏ –¥–≤–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ —Å–æ–≤—Å–µ–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã.  
–ß—Ç–æ–±—ã –Ω–µ —É—Å–ª–æ–∂–Ω—è—Ç—å –≤–µ—Ä—Å–∏—é —Å `normalizers.Sequence`, –º—ã –Ω–µ –≤–∫–ª—é—á–∏–ª–∏ –≤ –Ω–µ–µ Regex-–∑–∞–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É–µ—Ç `BertNormalizer`, –∫–æ–≥–¥–∞ –∞—Ä–≥—É–º–µ–Ω—Ç `clean_text` —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ `True`, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. –ù–æ –Ω–µ –≤–æ–ª–Ω—É–π—Ç–µ—Å—å: –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ—á–Ω–æ —Ç–∞–∫—É—é –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É–¥–æ–±–Ω–æ–≥–æ `BertNormalizer`, –¥–æ–±–∞–≤–∏–≤ –¥–≤–∞ `normalizers.Replace` –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤.

</Tip>

–î–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç —ç—Ç–∞–ø –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –û–ø—è—Ç—å –∂–µ, –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–π `BertPreTokenizer`, –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

–ò–ª–∏ –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –µ–≥–æ —Å –Ω—É–ª—è:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `Whitespace` —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –±—É–∫–≤–∞–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ —Å–∏–º–≤–æ–ª–æ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è, –ø–æ—ç—Ç–æ–º—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –æ–Ω —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ –ø—Ä–æ–±–µ–ª—å–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º, —Ç–æ –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —Å–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `WhitespaceSplit`:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–∞–º–∏, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Sequence` –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏. –ú—ã —É–∂–µ —É–∫–∞–∑–∞–ª–∏ –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –Ω–æ –Ω–∞–º –≤—Å–µ –µ—â–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –µ–µ, –¥–ª—è —á–µ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è `WordPieceTrainer`. –ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å –ø—Ä–∏ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç—Ä–µ–Ω–µ—Ä–∞ –≤ ü§ó Tokenizers, —ç—Ç–æ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –µ–º—É –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Å–æ–±–∏—Ä–∞–µ—Ç–µ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å - –∏–Ω–∞—á–µ –æ–Ω –Ω–µ –¥–æ–±–∞–≤–∏—Ç –∏—Ö –≤ —Å–ª–æ–≤–∞—Ä—å, –ø–æ—Å–∫–æ–ª—å–∫—É –∏—Ö –Ω–µ—Ç –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

–ü–æ–º–∏–º–æ —É–∫–∞–∑–∞–Ω–∏—è `vocab_size` –∏ `special_tokens`, –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å `min_frequency` (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, –∫–æ—Ç–æ—Ä–æ–µ –¥–æ–ª–∂–µ–Ω –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è —Ç–æ–∫–µ–Ω, —á—Ç–æ–±—ã –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º –≤ —Å–ª–æ–≤–∞—Ä—å) –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å `continuing_subword_prefix` (–µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ –æ—Ç–ª–∏—á–Ω–æ–µ –æ—Ç `##`).

–ß—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Ä–∞–Ω–µ–µ, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

–ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –º—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –ø—É—Å—Ç—ã–º `WordPiece`):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

–í –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö –º—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —Ç–µ–∫—Å—Ç–µ, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `encode()`:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

–ü–æ–ª—É—á–µ–Ω–Ω–æ–µ `encoding` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π `Encoding`, –∫–æ—Ç–æ—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–∞—Ö: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask` –∏ `overflowing`.

–ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞. –ù–∞–º –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω `[CLS]` –≤ –Ω–∞—á–∞–ª–µ –∏ —Ç–æ–∫–µ–Ω `[SEP]` –≤ –∫–æ–Ω—Ü–µ (–∏–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –ø–∞—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π). –î–ª—è —ç—Ç–æ–≥–æ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `TemplateProcessor`, –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —É–∑–Ω–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤ `[CLS]` –∏ `[SEP]` –≤ —Å–ª–æ–≤–∞—Ä–µ:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

–ß—Ç–æ–±—ã –Ω–∞–ø–∏—Å–∞—Ç—å —à–∞–±–ª–æ–Ω –¥–ª—è `TemplateProcessor`, –º—ã –¥–æ–ª–∂–Ω—ã —É–∫–∞–∑–∞—Ç—å, –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –î–ª—è –æ–±–æ–∏—Ö —Å–ª—É—á–∞–µ–≤ –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å; –ø–µ—Ä–≤–æ–µ (–∏–ª–∏ –æ–¥–∏–Ω–æ—á–Ω–æ–µ) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ `$A`, –∞ –≤—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–µ—Å–ª–∏ –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è –ø–∞—Ä–∞) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ `$B`. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö (—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) –º—ã —Ç–∞–∫–∂–µ —É–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∞ (token type ID) –ø–æ—Å–ª–µ –¥–≤–æ–µ—Ç–æ—á–∏—è. 

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —à–∞–±–ª–æ–Ω BERT –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–≥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –∏—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã.

–ö–∞–∫ —Ç–æ–ª—å–∫–æ —ç—Ç–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ, –≤–µ—Ä–Ω–µ–º—Å—è –∫ –Ω–∞—à–µ–º—É –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É –ø—Ä–∏–º–µ—Ä—É:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

–ò –Ω–∞ –ø–∞—Ä–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

–ú—ã –ø–æ—á—Ç–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å –Ω—É–ª—è - –æ—Å—Ç–∞–ª—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - –¥–æ–±–∞–≤–∏—Ç—å –¥–µ–∫–æ–¥–µ—Ä:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏–º –µ–≥–æ –Ω–∞ –Ω–∞—à–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–º `encoding`:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

–û—Ç–ª–∏—á–Ω–æ! –ú—ã –º–æ–∂–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º JSON-—Ñ–∞–π–ª–µ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
tokenizer.save("tokenizer.json")
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç—Ç–æ—Ç —Ñ–∞–π–ª –≤ –æ–±—ä–µ–∫—Ç `Tokenizer` —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `from_file()`:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

–ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ ü§ó Transformers, –º—ã –¥–æ–ª–∂–Ω—ã –æ–±–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ `PreTrainedTokenizerFast`. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏–±–æ –æ–±—â–∏–π –∫–ª–∞—Å—Å, –ª–∏–±–æ, –µ—Å–ª–∏ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∫–ª–∞—Å—Å (–∑–¥–µ—Å—å `BertTokenizerFast`). –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —ç—Ç–æ—Ç —É—Ä–æ–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç.

–ß—Ç–æ–±—ã –æ–±–µ—Ä–Ω—É—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ `PreTrainedTokenizerFast`, –º—ã –º–æ–∂–µ–º –ª–∏–±–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫–∞–∫ `tokenizer_object`, –ª–∏–±–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∫–∞–∫ `tokenizer_file`. –ì–ª–∞–≤–Ω–æ–µ –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –Ω–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é –∑–∞–¥–∞–≤–∞—Ç—å –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å –Ω–µ –º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ –æ–±—ä–µ–∫—Ç–∞ `tokenizer`, –∫–∞–∫–æ–π —Ç–æ–∫–µ–Ω —è–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–æ–º –º–∞—Å–∫–∏, —Ç–æ–∫–µ–Ω–æ–º `[CLS]` –∏ —Ç. –¥.:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # –í –∫–∞—á–µ—Å—Ç–≤–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

–ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `BertTokenizerFast`), –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —É–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∑–¥–µ—Å—å –∏—Ö –Ω–µ—Ç):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

–ó–∞—Ç–µ–º –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–∞–∫ –∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ü§ó Transformers. –í—ã –º–æ–∂–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –µ–≥–æ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `save_pretrained()` –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ —Ö–∞–± —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `push_to_hub()`.

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏, –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä WordPiece, –¥–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º —Ç–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BPE. –ú—ã –±—É–¥–µ–º –¥–≤–∏–≥–∞—Ç—å—Å—è –Ω–µ–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ, –ø–æ—Å–∫–æ–ª—å–∫—É –≤—ã –∑–Ω–∞–µ—Ç–µ –≤—Å–µ —à–∞–≥–∏, –∏ –ø–æ–¥—á–µ—Ä–∫–Ω–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–ª–∏—á–∏—è.

## –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BPE —Å –Ω—É–ª—è[[building-a-bpe-tokenizer-from-scratch]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä GPT-2. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º BERT, –º—ã –Ω–∞—á–Ω–µ–º —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ `Tokenizer` —Å –º–æ–¥–µ–ª—å—é BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

–¢–∞–∫–∂–µ, –∫–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å BERT, –º—ã –º–æ–≥–ª–∏ –±—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å —Å–ª–æ–≤–∞—Ä–µ–º, –µ—Å–ª–∏ –±—ã –æ–Ω —É –Ω–∞—Å –±—ã–ª (–≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –Ω–∞–º –Ω—É–∂–Ω–æ –±—ã–ª–æ –±—ã –ø–µ—Ä–µ–¥–∞—Ç—å `vocab` –∏ `merges`), –Ω–æ –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å —Å –Ω—É–ª—è, –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ —ç—Ç–æ–≥–æ –¥–µ–ª–∞—Ç—å. –ù–∞–º —Ç–∞–∫–∂–µ –Ω–µ –Ω—É–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å `unk_token`, –ø–æ—Ç–æ–º—É —á—Ç–æ GPT-2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç byte-level BPE, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ–≥–æ.

GPT-2 –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä, –ø–æ—ç—Ç–æ–º—É –º—ã –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —à–∞–≥ –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∫ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

–û–ø—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º—ã –¥–æ–±–∞–≤–∏–ª–∏ –∫ `ByteLevel`, –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –ø—Ä–æ–±–µ–ª –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é). –ú—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞, –∫–∞–∫ –±—ã–ª–æ –ø–æ–∫–∞–∑–∞–Ω–æ —Ä–∞–Ω–µ–µ:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('ƒ†test', (5, 10)), ('ƒ†pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

–î–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å. –î–ª—è GPT-2 –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º —è–≤–ª—è–µ—Ç—Å—è —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å `WordPieceTrainer`, –∞ —Ç–∞–∫–∂–µ `vocab_size` –∏ `special_tokens`, –º—ã –º–æ–∂–µ–º —É–∫–∞–∑–∞—Ç—å `min_frequency`, –µ—Å–ª–∏ —Ö–æ—Ç–∏–º, –∏–ª–∏ –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å –∫–æ–Ω—Ü–∞ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `</w>`), –º—ã –º–æ–∂–µ–º –∑–∞–¥–∞—Ç—å –µ–≥–æ —Å –ø–æ–º–æ—â—å—é `end_of_word_suffix`. 

–≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—É—á–µ–Ω –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'ƒ†test', 'ƒ†this', 'ƒ†to', 'ken', 'izer', '.']
```

–ú—ã –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ GPT-2 —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

–û–ø—Ü–∏—è `trim_offsets = False` —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É, —á—Ç–æ –º—ã –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å 'ƒ†', –∫–∞–∫ –µ—Å—Ç—å: —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞—á–∞–ª–æ —Å–º–µ—â–µ–Ω–∏—è –±—É–¥–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ —Å–ª–æ–≤–æ–º, –∞ –Ω–µ –Ω–∞ –ø–µ—Ä–≤—ã–π —Å–∏–º–≤–æ–ª —Å–ª–æ–≤–∞ (–ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–æ–±–µ–ª —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Ç–æ–∫–µ–Ω–∞). –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ç–µ–∫—Å—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–ª–∏, –≥–¥–µ `'ƒ†test'` - —ç—Ç–æ —Ç–æ–∫–µ–Ω —Å –∏–Ω–¥–µ–∫—Å–æ–º 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ–∫–æ–¥–µ—Ä –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤:

```python
tokenizer.decoder = decoders.ByteLevel()
```

–∏ –º—ã —Å–º–æ–∂–µ–º –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏, –º—ã –º–æ–∂–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–∞–∫ —Ä–∞–Ω—å—à–µ, –∏ –æ–±–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ `PreTrainedTokenizerFast` –∏–ª–∏ `GPT2TokenizerFast`, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ ü§ó Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

–∏–ª–∏:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

–í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã –ø–æ–∫–∞–∂–µ–º –≤–∞–º, –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Unigram —Å –Ω—É–ª—è.

## –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ Unigram —Å –Ω—É–ª—è[[building-a-unigram-tokenizer-from-scratch]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä XLNet. –ö–∞–∫ –∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞—Ö, –º—ã –Ω–∞—á–Ω–µ–º —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ `Tokenizer` —Å –º–æ–¥–µ–ª—å—é Unigram:

```python
tokenizer = Tokenizer(models.Unigram())
```

–û–ø—è—Ç—å –∂–µ, –º—ã –º–æ–≥–ª–∏ –±—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å —Å–ª–æ–≤–∞—Ä–µ–º, –µ—Å–ª–∏ –±—ã –æ–Ω —É –Ω–∞—Å –±—ã–ª.

–î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ XLNet –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–º–µ–Ω (–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—à–ª–∏ –∏–∑ SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

–û–Ω –∑–∞–º–µ–Ω—è–µ—Ç <code>``</code> –∏ <code>''</code> with <code>"</code> –∏ –ª—é–±—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –¥–≤—É—Ö –∏–ª–∏ –±–æ–ª–µ–µ –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª, –∞ —Ç–∞–∫–∂–µ —É–¥–∞–ª—è–µ—Ç —É–¥–∞—Ä–µ–Ω–∏—è –≤ —Ç–æ–∫–µ–Ω–µ–∑–∏—Ä—É–µ–º—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö.

–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ª—é–±–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ SentencePiece, - —ç—Ç–æ `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

–ú—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞, –∫–∞–∫ –±—ã–ª–æ –ø–æ–∫–∞–∑–∞–Ω–æ —Ä–∞–Ω–µ–µ:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("‚ñÅLet's", (0, 5)), ('‚ñÅtest', (5, 10)), ('‚ñÅthe', (10, 14)), ('‚ñÅpre-tokenizer!', (14, 29))]
```

–î–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å. –í XLNet –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```
–û—á–µ–Ω—å –≤–∞–∂–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Å—Ç–æ–∏—Ç –∑–∞–±—ã–≤–∞—Ç—å –¥–ª—è `UnigramTrainer` - —ç—Ç–æ `unk_token`. –ú—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Unigram, —Ç–∞–∫–∏–µ –∫–∞–∫ `shrinking_factor` –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.75) –∏–ª–∏ `max_piece_length` –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –¥–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 16).

–≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—É—á–µ–Ω –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```
–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['‚ñÅLet', "'", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.']
```

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å—é XLNet —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ –æ–Ω –ø–æ–º–µ—â–∞–µ—Ç —Ç–æ–∫–µ–Ω `<cls>` –≤ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º —Ç–∏–ø–∞ 2 (—á—Ç–æ–±—ã –æ—Ç–ª–∏—á–∏—Ç—å –µ–≥–æ –æ—Ç –¥—Ä—É–≥–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤). –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –æ–Ω –ø–æ–º–µ—â–∞–µ—Ç—Å—è —Å–ª–µ–≤–∞. –ú—ã –º–æ–∂–µ–º —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å–æ –≤—Å–µ–º–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é —à–∞–±–ª–æ–Ω–∞, –∫–∞–∫ –≤ BERT, –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤ `<cls>` –∏ `<sep>`:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

–®–∞–±–ª–æ–Ω –≤—ã–≥–ª—è–¥–∏—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

–ò –º—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–≤ –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['‚ñÅLet', "'", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.', '.', '.', '<sep>', '‚ñÅ', 'on', '‚ñÅ', 'a', '‚ñÅpair', 
  '‚ñÅof', '‚ñÅsentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ–∫–æ–¥–µ—Ä `Metaspace`:

```python
tokenizer.decoder = decoders.Metaspace()
```

–∏ –º—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏ —Ä–∞–±–æ—Ç—É —Å —ç—Ç–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º! –ú—ã –º–æ–∂–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ, –∏ –æ–±–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ `PreTrainedTokenizerFast` –∏–ª–∏ `XLNetTokenizerFast`, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ ü§ó Transformers. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ `PreTrainedTokenizerFast` —Å–ª–µ–¥—É–µ—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ –ø–æ–º–∏–º–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –Ω–∞–º –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers –Ω–∞ —Ç–æ, —á—Ç–æ–±—ã –æ–Ω–∏ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–ª–∏—Å—å —Å–ª–µ–≤–∞:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

–ò–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –≤—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–ª–æ–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –≤—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –ª—é–±–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ, —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Tokenizers –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ ü§ó Transformers.
