# –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ä–æ–≥–æ[[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

–ï—Å–ª–∏ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–º –≤–∞—Å —è–∑—ã–∫–µ –∏–ª–∏ –≤–∞—à –∫–æ—Ä–ø—É—Å —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç–æ–≥–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ–±—É—á–∞–ª–∞—Å—å —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –≤–∞–º, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ø—Ä–∏–¥–µ—Ç—Å—è –∑–∞–Ω–æ–≤–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫ –≤–∞—à–∏–º –¥–∞–Ω–Ω—ã–º. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –≤–∞—à–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –ù–æ —á—Ç–æ –∏–º–µ–Ω–Ω–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç? –ö–æ–≥–¥–∞ –º—ã –≤–ø–µ—Ä–≤—ã–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –≤ [–ì–ª–∞–≤–µ 2](../chapter2/1), –º—ã —É–≤–∏–¥–µ–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç _–∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ –ø–æ–¥—Å–ª–æ–≤–∞–º_. –ß—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –ø–æ–¥—Å–ª–æ–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∏–Ω—Ç–µ—Ä–µ—Å –∏ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∫–æ—Ä–ø—É—Å–µ, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –∫–æ—Ä–ø—É—Å–µ - —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –º—ã –Ω–∞–∑—ã–≤–∞–µ–º *–æ–±—É—á–µ–Ω–∏–µ–º*. –¢–æ—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–∏–ø–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –¥–∞–ª–µ–µ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ - —ç—Ç–æ –Ω–µ —Ç–æ –∂–µ —Å–∞–º–æ–µ, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏! –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø–æ—Ç–µ—Ä–∏ –Ω–µ–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞. –û–Ω–æ —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ (—ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ –∑–∞–¥–∞—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —á–∏—Å–ª–æ seed, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏). –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ - —ç—Ç–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –ø–æ–¥—Å–ª–æ–≤–∞ –ª—É—á—à–µ –≤—Å–µ–≥–æ –≤—ã–±—Ä–∞—Ç—å –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞, –∞ —Ç–æ—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –∏—Ö –≤—ã–±–æ—Ä–∞, –∑–∞–≤–∏—Å—è—Ç –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –≠—Ç–æ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å, —Ç–æ –µ—Å—Ç—å –≤—ã –≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∏—Ç–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ.

</Tip>

## –°–±–æ—Ä –∫–æ—Ä–ø—É—Å–∞ —Å–ª–æ–≤[[assembling-a-corpus]]

–í ü§ó Transformers –µ—Å—Ç—å –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π API, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å —Ç–µ–º–∏ –∂–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏, —á—Ç–æ –∏ —É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ: `AutoTokenizer.train_new_from_iterator()`. –ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —ç—Ç–æ –≤ –¥–µ–π—Å—Ç–≤–∏–∏, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º –æ–±—É—á–∏—Ç—å GPT-2 —Å –Ω—É–ª—è, –Ω–æ –Ω–∞ —è–∑—ã–∫–µ, –æ—Ç–ª–∏—á–Ω–æ–º –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ù–∞—à–µ–π –ø–µ—Ä–≤–æ–π –∑–∞–¥–∞—á–µ–π –±—É–¥–µ—Ç —Å–æ–±—Ä–∞—Ç—å –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–æ–º —è–∑—ã–∫–µ –≤ –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å. –ß—Ç–æ–±—ã –ø—Ä–∏–º–µ—Ä—ã –±—ã–ª–∏ –ø–æ–Ω—è—Ç–Ω—ã –≤—Å–µ–º, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ —Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫, –∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: Python-–∫–æ–¥.

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ [ü§ó Datasets](https://github.com/huggingface/datasets) –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –Ω–∞–º —Å–æ–±—Ä–∞—Ç—å –∫–æ—Ä–ø—É—Å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ Python. –ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –æ–±—ã—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π `load_dataset()` –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö [CodeSearchNet](https://huggingface.co/datasets/code_search_net). –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–ª—è –∫–æ–Ω–∫—É—Ä—Å–∞ [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∏–ª–ª–∏–æ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º —Å GitHub –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ó–¥–µ—Å—å –º—ã –∑–∞–≥—Ä—É–∑–∏–º Python-—á–∞—Å—Ç—å —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:

```py
from datasets import load_dataset

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç, —Ç–∞–∫ —á—Ç–æ –≤—ã–ø–µ–π—Ç–µ –∫–æ—Ñ–µ –∏–ª–∏ —á–∞–π, –ø–æ–∫–∞ –∂–¥–µ—Ç–µ!
raw_datasets = load_dataset("code_search_net", "python")
```

–ú—ã –º–æ–∂–µ–º –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é —á–∞—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å, –∫ –∫–∞–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º —É –Ω–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç–¥–µ–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –æ—Ç –∫–æ–¥–∞ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —Ç–æ, –∏ –¥—Ä—É–≥–æ–µ. –ó–¥–µ—Å—å –º—ã –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É `whole_func_string` –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ú—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∏–º–µ—Ä –æ–¥–Ω–æ–π –∏–∑ —ç—Ç–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π, –æ–±—Ä–∞—Ç–∏–≤—à–∏—Å—å –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É –≤ —á–∞—Å—Ç–∏ `train`:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

–∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –≤—ã–≤–µ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–µ:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

–ü–µ—Ä–≤–æ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ _–∏—Ç–µ—Ä–∞—Ç–æ—Ä_ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ -- –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞—à–µ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É —Ä–∞–±–æ—Ç–∞—Ç—å –±—ã—Å—Ç—Ä–µ–µ (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É), –∏ —ç—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏—Ç–µ—Ä–∞—Ç–æ—Ä, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–µ—Ä–∂–∞—Ç—å –≤ –ø–∞–º—è—Ç–∏ –≤—Å–µ —Å—Ä–∞–∑—É. –ï—Å–ª–∏ –≤–∞—à –∫–æ—Ä–ø—É—Å –æ–≥—Ä–æ–º–µ–Ω, –≤—ã –∑–∞—Ö–æ—Ç–∏—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–µ–º, —á—Ç–æ ü§ó Datasets –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –≤ RAM, –∞ —Ö—Ä–∞–Ω–∏—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫–µ.

–°–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ —Å–æ–∑–¥–∞—Å—Ç —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –ø–æ 1 000 —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º, –Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç –≤—Å–µ –≤ –ø–∞–º—è—Ç—å:

```py
# –ù–µ —Ä–∞—Å–∫–æ–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –∫–æ–¥–∞, –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –≤–∞—à –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–µ –º–∞–ª–µ–Ω—å–∫–∏–π!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

–ò—Å–ø–æ–ª—å–∑—É—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä Python, –º—ã –º–æ–∂–µ–º –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –Ω–∏—á–µ–≥–æ –≤ –ø–∞–º—è—Ç—å Python –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è. –ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–∞–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–∏—Ç—å —Å–∫–æ–±–∫–∏ –Ω–∞ –∫—Ä—É–≥–ª—ã–µ —Å–∫–æ–±–∫–∏:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

–≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –∫–æ–¥–∞ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∏–∫–∞–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö; –æ–Ω–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ —Ü–∏–∫–ª–µ Python `for`. –¢–µ–∫—Å—Ç—ã –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –æ–Ω–∏ –≤–∞–º –Ω—É–∂–Ω—ã (—Ç–æ –µ—Å—Ç—å –∫–æ–≥–¥–∞ –≤—ã –Ω–∞—Ö–æ–¥–∏—Ç–µ—Å—å –Ω–∞ —Ç–æ–º —à–∞–≥–µ —Ü–∏–∫–ª–∞ `for`, –≥–¥–µ –æ–Ω–∏ —Ç—Ä–µ–±—É—é—Ç—Å—è), –∏ –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–æ–ª—å–∫–æ 1 000 —Ç–µ–∫—Å—Ç–æ–≤. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤—ã –Ω–µ –∏—Å—á–µ—Ä–ø–∞–µ—Ç–µ –≤—Å—é –ø–∞–º—è—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ –æ–≥—Ä–æ–º–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.

–ü—Ä–æ–±–ª–µ–º–∞ —Å –æ–±—ä–µ–∫—Ç–æ–º-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑. –ü–æ—ç—Ç–æ–º—É –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤—ã–¥–∞—Ç—å –Ω–∞–º —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–≤—ã—Ö 10 —Ü–∏—Ñ—Ä –¥–≤–∞–∂–¥—ã:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

–º—ã –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –æ–¥–∏–Ω —Ä–∞–∑, –∞ –∑–∞—Ç–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

–ü–æ—ç—Ç–æ–º—É –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞ `for`, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ç–æ—Ä `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

–∫–æ—Ç–æ—Ä—ã–π –≤—ã–¥–∞–µ—Ç —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–∞–∫ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π, –Ω–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É, —á–µ–º –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å list comprehension.

## –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞[[training-a-new-tokenizer]]

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –∫–æ—Ä–ø—É—Å –≤ –≤–∏–¥–µ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –±–∞—Ç—á–µ–π —Ç–µ–∫—Å—Ç–æ–≤, –º—ã –≥–æ—Ç–æ–≤—ã –æ–±—É—á–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –î–ª—è —ç—Ç–æ–≥–æ –Ω–∞–º —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø–∞—Ä–µ —Å –Ω–∞—à–µ–π –º–æ–¥–µ–ª—å—é (–∑–¥–µ—Å—å GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–±—É—á–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —ç—Ç–æ —Ö–æ—Ä–æ—à–∞—è –∏–¥–µ—è —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ, –Ω–µ –Ω–∞—á–∏–Ω–∞—è –≤—Å–µ —Å –Ω—É–ª—è. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞–º –Ω–µ –ø—Ä–∏–¥–µ—Ç—Å—è –Ω–∏—á–µ–≥–æ —É—Ç–æ—á–Ω—è—Ç—å –æ–± –∞–ª–≥–æ—Ä–∏—Ç–º–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å; –Ω–∞—à –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–º –∂–µ, –∫–∞–∫ GPT-2, –∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ, —á—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—Å—è, - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ.

–°–Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–∏–º–µ—Ä–æ–º —Ñ—É–Ω–∫—Ü–∏–∏:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'n', 'umbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two',
 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`', '."', '""', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

–≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ `ƒ†` –∏ `ƒä`, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ö–∞–∫ –º—ã –≤–∏–¥–∏–º, —ç—Ç–æ –Ω–µ —Å–ª–∏—à–∫–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–±–µ–ª–∞, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ–Ω –º–æ–≥ –±—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —É—Ä–æ–≤–Ω–∏ –æ—Ç—Å—Ç—É–ø–æ–≤ (–ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∞–±–æ—Ä—ã –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –∏–ª–∏ –≤–æ—Å—å–º–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –±—É–¥—É—Ç –æ—á–µ–Ω—å —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –≤ –∫–æ–¥–µ). –û–Ω —Ç–∞–∫–∂–µ –Ω–µ–º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–ª –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏, –Ω–µ –æ–∂–∏–¥–∞—è —É–≤–∏–¥–µ—Ç—å —Å–ª–æ–≤–∞ —Å —Å–∏–º–≤–æ–ª–æ–º `_`.

–î–∞–≤–∞–π—Ç–µ –æ–±—É—á–∏–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Ä–µ—à–∏—Ç –ª–∏ –æ–Ω —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –º–µ—Ç–æ–¥–æ–º `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –µ—Å–ª–∏ –≤–∞—à –∫–æ—Ä–ø—É—Å –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π, –Ω–æ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å 1,6 –ì–ë —Ç–µ–∫—Å—Ç–æ–≤ –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ (1 –º–∏–Ω—É—Ç–∞ 16 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ AMD Ryzen 9 3900X —Å 12 —è–¥—Ä–∞–º–∏).

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ `AutoTokenizer.train_new_from_iterator()` —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ —Ç–æ–º —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —è–≤–ª—è–µ—Ç—Å—è "–±—ã—Å—Ç—Ä—ã–º" —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º. –ö–∞–∫ –≤—ã —É–≤–∏–¥–∏—Ç–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ, –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Transformers —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–∞ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤: –æ–¥–Ω–∏ –Ω–∞–ø–∏—Å–∞–Ω—ã –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ Python, –∞ –¥—Ä—É–≥–∏–µ (–±—ã—Å—Ç—Ä—ã–µ) –æ–ø–∏—Ä–∞—é—Ç—Å—è –Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫—É ü§ó Tokenizers, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø–∏—Å–∞–Ω–∞ –Ω–∞ —è–∑—ã–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è [Rust](https://www.rust-lang.org). Python - —ç—Ç–æ —è–∑—ã–∫, –∫–æ—Ç–æ—Ä—ã–π —á–∞—â–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π data science –∏ deep learning, –Ω–æ –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –Ω—É–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã, —ç—Ç–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–∏—Å–∞—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–∞—Ç—Ä–∏—á–Ω—ã–µ —É–º–Ω–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–∂–∞—Ç –≤ –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–æ–¥–µ–ª–∏, –Ω–∞–ø–∏—Å–∞–Ω—ã –Ω–∞ CUDA, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ —è–∑—ã–∫–∞ C –¥–ª—è GPU.

–û–±—É—á–µ–Ω–∏–µ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —á–∏—Å—Ç–æ–º Python –±—ã–ª–æ –±—ã –º—É—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–º, –ø–æ—ç—Ç–æ–º—É –º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫—É ü§ó Tokenizers. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç–∞–∫ –∂–µ –∫–∞–∫ –≤–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±—ã–ª–æ –∏–∑—É—á–∞—Ç—å —è–∑—ã–∫ CUDA, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å –Ω–∞ –±–∞—Ç—á–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ GPU, –≤–∞–º –Ω–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –∏–∑—É—á–∞—Ç—å Rust, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–≤—è–∑–∫–∏ –∫ Python –¥–ª—è –º–Ω–æ–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ –≤—ã–∑—ã–≤–∞—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–∏ –∫–æ–¥–∞ –Ω–∞ Rust; –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–ª–∏, –∫–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1), —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±–∞—Ç—á–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –º–æ–¥–µ–ª–µ–π Transformer –¥–æ—Å—Ç—É–ø–µ–Ω –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –æ –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã –º–æ–∂–µ—Ç–µ —É–∑–Ω–∞—Ç—å [–∑–¥–µ—Å—å](https://huggingface.co/transformers/#supported-frameworks)), –∞ API `AutoTokenizer` –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω. –í —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥—Ä—É–≥–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á, –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –û–¥–Ω–∞–∫–æ –ø—Ä–µ–∂–¥–µ —á–µ–º –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ —ç—Ç—É —Ç–µ–º—É, –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞—à –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`',
 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`."""', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

–ó–¥–µ—Å—å –º—ã —Å–Ω–æ–≤–∞ –≤–∏–¥–∏–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã `ƒ†` –∏ `ƒä`, –æ–±–æ–∑–Ω–∞—á–∞—é—â–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏, –Ω–æ –º—ã —Ç–∞–∫–∂–µ –≤–∏–¥–∏–º, —á—Ç–æ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—ã—É—á–∏–ª –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—á–µ–Ω—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –∫–æ—Ä–ø—É—Å–∞ —Ñ—É–Ω–∫—Ü–∏–π Python: –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å—Ç—å —Ç–æ–∫–µ–Ω `ƒäƒ†ƒ†ƒ†`, –∫–æ—Ç–æ—Ä—ã–π –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –æ—Ç—Å—Ç—É–ø, –∏ —Ç–æ–∫–µ–Ω `ƒ†"""`, –∫–æ—Ç–æ—Ä—ã–π –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç —Ç—Ä–∏ –∫–∞–≤—ã—á–∫–∏, —Å –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è doc-—Å—Ç—Ä–æ–∫–∞. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–ª –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞ `_`. –≠—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ; –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è —Ç–æ–≥–æ –∂–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞—Å—Ç –Ω–∞–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

–î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –¥—Ä—É–≥–æ–π –ø—Ä–∏–º–µ—Ä:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ƒ†Linear', 'Layer', '():', 'ƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'init', '__(', 'self', ',', 'ƒ†input', '_', 'size', ',',
 'ƒ†output', '_', 'size', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'weight', 'ƒ†=', 'ƒ†torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'ƒ†output', '_', 'size', ')', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'bias', 'ƒ†=', 'ƒ†torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ƒäƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'call', '__(', 'self', ',', 'ƒ†x', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',
 'ƒ†return', 'ƒ†x', 'ƒ†@', 'ƒ†self', '.', 'weights', 'ƒ†+', 'ƒ†self', '.', 'bias', 'ƒäƒ†ƒ†ƒ†ƒ†']
```

–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ç–æ–∫–µ–Ω—É, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É –æ—Ç—Å—Ç—É–ø—É, –∑–¥–µ—Å—å –º—ã —Ç–∞–∫–∂–µ –≤–∏–¥–∏–º —Ç–æ–∫–µ–Ω –¥–ª—è –¥–≤–æ–π–Ω–æ–≥–æ –æ—Ç—Å—Ç—É–ø–∞: `ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†`. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ Python, —Ç–∞–∫–∏–µ –∫–∞–∫ `class`, `init`, `call`, `self` –∏ `return`, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω, –∏ –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –Ω–∞—Ä—è–¥—É —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ `_` –∏ `.` —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–∂–µ –∏–º–µ–Ω–∞ —Å camel-case: `LinearLayer` –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ `["ƒ†Linear", "Layer"]`.

## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞[[saving-the-tokenizer]]

–ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã —Å–º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –ø–æ–∑–∂–µ, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—à –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ö–∞–∫ –∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π, —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –ø–∞–ø–∫–∞ —Å –∏–º–µ–Ω–µ–º *code-search-net-tokenizer*, –≤ –∫–æ—Ç–æ—Ä–æ–π –±—É–¥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å—Å—è –≤—Å–µ —Ñ–∞–π–ª—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è —ç—Ç–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º —Å–æ —Å–≤–æ–∏–º–∏ –∫–æ–ª–ª–µ–≥–∞–º–∏ –∏ –¥—Ä—É–∑—å—è–º–∏, –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ–≥–æ –Ω–∞ Hub, –≤–æ–π–¥—è –≤ —Å–≤–æ—é —É—á–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –µ—Å—Ç—å —É–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –≤ —ç—Ç–æ–º:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–ü–æ—è–≤–∏—Ç—Å—è –≤–∏–¥–∂–µ—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Hugging Face. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```bash
huggingface-cli login
```

–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –≤—ã –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–ª–∏—Å—å, –≤—ã –º–æ–∂–µ—Ç–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å —Å–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –≤—ã–ø–æ–ª–Ω–∏–≤ —Å–ª–µ–¥—É—é—â—É—é –∫–æ–º–∞–Ω–¥—É:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

–≠—Ç–æ —Å–æ–∑–¥–∞—Å—Ç –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ –≤–∞—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º–µ–Ω —Å –∏–º–µ–Ω–µ–º `code-search-net-tokenizer`, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —Ñ–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ó–∞—Ç–µ–º –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≥–¥–µ —É–≥–æ–¥–Ω–æ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `from_pretrained()`:

```py
# –ó–∞–º–µ–Ω–∏—Ç–µ "huggingface-course" –Ω–∏–∂–µ —Å–≤–æ–∏–º —Ä–µ–∞–ª—å–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –∏–º–µ–Ω, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

–¢–µ–ø–µ—Ä—å –≤—ã –≥–æ—Ç–æ–≤—ã –æ–±—É—á–∏—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è –∏ –¥–æ–æ–±—É—á–∏—Ç—å –µ–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ–π! –ú—ã –∑–∞–π–º–µ–º—Å—è —ç—Ç–∏–º –≤ [–ì–ª–∞–≤–µ 7](../chapter7), –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –±—ã—Å—Ç—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∏–∑—É—á–∏–º, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–µ—Ç–æ–¥–∞ `train_new_from_iterator()`.
