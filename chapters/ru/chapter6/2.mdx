# –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

–ï—Å–ª–∏ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–º –≤–∞—Å —è–∑—ã–∫–µ –∏–ª–∏ –µ—Å–ª–∏ –≤–∞—à –∫–æ—Ä–ø—É—Å —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ç–æ–≥–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ–±—É—á–∞–ª–∞—Å—å –≤–∞—à–∞ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –≤—ã, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –∑–∞—Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫ –≤–∞—à–∏–º –¥–∞–Ω–Ω—ã–º. –≠—Ç–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –≤–∞—à–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –ù–æ —á—Ç–æ –∏–º–µ–Ω–Ω–æ —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç? –ö–æ–≥–¥–∞ –º—ã –≤–ø–µ—Ä–≤—ã–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –≤ [–ì–ª–∞–≤–µ 2](/course/chapter2), –º—ã —É–≤–∏–¥–µ–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Transformer –∏—Å–ø–æ–ª—å–∑—É—é—Ç _–∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Å–ª–æ–≤_ (_subword tokenization algorithm_). –ß—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –ø–æ–¥—Å–ª–æ–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∏–Ω—Ç–µ—Ä–µ—Å –∏ —á–∞—â–µ –≤—Å–µ–≥–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∏–º–µ—é—â–µ–º—Å—è –∫–æ—Ä–ø—É—Å–µ, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –∫–æ—Ä–ø—É—Å–µ ‚Äî –ø—Ä–æ—Ü–µ—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –º—ã –Ω–∞–∑—ã–≤–∞–µ–º ¬´–æ–±—É—á–µ–Ω–∏–µ–º¬ª. –¢–æ—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —ç—Ç–∏–º –æ–±—É—á–µ–Ω–∏–µ–º, –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–∏–ø–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –∏ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ø–æ–∑–∂–µ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ‚Äî —ç—Ç–æ –Ω–µ —Ç–æ –∂–µ —Å–∞–º–æ–µ, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏! –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, —á—Ç–æ–±—ã  —É–º–µ–Ω—å—à–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ (—ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤—ã –¥–æ–ª–∂–Ω—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–≤–∞–∂–¥—ã). –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ‚Äî —ç—Ç–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ –ø–æ–¥—Å–ª–æ–≤–∞ –ª—É—á—à–µ –≤—Å–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞, –∞ —Ç–æ—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –∏—Ö –≤—ã–±–æ—Ä–∞, –∑–∞–≤–∏—Å—è—Ç –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –û–Ω –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω, —Ç–æ –µ—Å—Ç—å –≤—ã –≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞–µ—Ç–µ –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –æ–¥–Ω–∏–º –∏ —Ç–µ–º –∂–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ.

</Tip>

## –°–±–æ—Ä –∫–æ—Ä–ø—É—Å–∞ —Å–ª–æ–≤

–í ü§ó Transformers –µ—Å—Ç—å –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π API, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å —Ç–µ–º–∏ –∂–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏, —á—Ç–æ –∏ —É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ: `AutoTokenizer.train_new_from_iterator()`. –ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —ç—Ç–æ –≤ –¥–µ–π—Å—Ç–≤–∏–∏, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º –æ–±—É—á–∏—Ç—å GPT-2 —Å –Ω—É–ª—è, –Ω–æ –Ω–∞ —è–∑—ã–∫–µ, –æ—Ç–ª–∏—á–Ω–æ–º –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ù–∞—à–µ–π –ø–µ—Ä–≤–æ–π –∑–∞–¥–∞—á–µ–π –±—É–¥–µ—Ç —Å–±–æ—Ä –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–æ–º —è–∑—ã–∫–µ –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ. –ß—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã, –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–∞–∂–¥–æ–º—É, –º—ã –Ω–µ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–¥–µ—Å—å —è–∑—ã–∫, –ø–æ–¥–æ–±–Ω—ã–π —Ä—É—Å—Å–∫–æ–º—É –∏–ª–∏ –∫–∏—Ç–∞–π—Å–∫–æ–º—É, –∞ —Å–∫–æ—Ä–µ–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫: –∫–æ–¥ Python.

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ [ü§ó Datasets](https://github.com/huggingface/datasets) –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –Ω–∞–º —Å–æ–±—Ä–∞—Ç—å –∫–æ—Ä–ø—É—Å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ Python. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—ã—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `load_dataset()` –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö [CodeSearchNet](https://huggingface.co/datasets/code_search_net). –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–ª—è [—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∏–ª–ª–∏–æ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –Ω–∞ GitHub –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ó–¥–µ—Å—å –º—ã –∑–∞–≥—Ä—É–∑–∏–º —á–∞—Å—Ç—å Python —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:

```py
from datasets import load_dataset

# –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è ‚Äì –∑–∞–≤–∞—Ä–∏—Ç–µ —Å–µ–±–µ —á–∞—é!
raw_datasets = load_dataset("code_search_net", "python")
```

–ú—ã –º–æ–∂–µ–º –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–∏–π —Å–ø–ª–∏—Ç –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫ –∫–∞–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º —É –Ω–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```
–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Ç–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç –∫–æ–¥–∞ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –æ–±–æ–∏—Ö. –ó–¥–µ—Å—å –º—ã –ø—Ä–æ—Å—Ç–æ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü `whole_func_string` –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ú—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä –æ–¥–Ω–æ–π –∏–∑ —ç—Ç–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π, –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–≤ —Ä–∞–∑–¥–µ–ª `train`:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞—Å–ø–µ—á–∞—Ç–∞–Ω–æ —Å–ª–µ–¥—É—é—â–µ–µ: 

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

–ü–µ—Ä–≤–æ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ _–∏—Ç–µ—Ä–∞—Ç–æ—Ä_ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞—à–µ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É —Ä–∞–±–æ—Ç–∞—Ç—å –±—ã—Å—Ç—Ä–µ–µ (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–∞–∫–µ—Ç–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É), –∏ –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–º –æ–±—ä–µ–∫—Ç–æ–º, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏–∑–±–µ–∂–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏. –ï—Å–ª–∏ –≤–∞—à –∫–æ—Ä–ø—É—Å –æ–≥—Ä–æ–º–µ–Ω, –≤—ã –∑–∞—Ö–æ—Ç–∏—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–µ–º —Ñ–∞–∫—Ç–æ–º, —á—Ç–æ ü§ó Datasets –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç –≤—Å–µ –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å, –∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫–µ.

–°–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ —Å–æ–∑–¥–∞—Å—Ç —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –ø–æ 1000 —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º, –Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç –≤—Å–µ –≤ –ø–∞–º—è—Ç—å:

```py
# –ï—Å–ª–∏ –≤–∞—à –¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π ‚Äì –æ—Å—Ç–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

–ò—Å–ø–æ–ª—å–∑—É—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä Python, –º—ã –º–æ–∂–µ–º –∏–∑–±–µ–∂–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ Python —á–µ–≥–æ-–ª–∏–±–æ –≤ –ø–∞–º—è—Ç—å –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ. –ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ç–∞–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –≤–∞–º –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ –ª–∏—à—å –∑–∞–º–µ–Ω–∏—Ç—å –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏ –∫—Ä—É–≥–ª—ã–º–∏:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

–≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –∫–æ–¥–∞ –Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∏–∫–∞–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö; –æ–Ω –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ —Ü–∏–∫–ª–µ for Python. –¢–µ–∫—Å—Ç—ã –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –æ–Ω–∏ –≤–∞–º –Ω—É–∂–Ω—ã (—Ç–æ –µ—Å—Ç—å, –∫–æ–≥–¥–∞ –≤—ã –Ω–∞—Ö–æ–¥–∏—Ç–µ—Å—å –Ω–∞ —ç—Ç–∞–ø–µ —Ü–∏–∫–ª–∞ `for`, –∫–æ—Ç–æ—Ä—ã–π –∏—Ö —Ç—Ä–µ–±—É–µ—Ç), –∏ –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–æ–ª—å–∫–æ 1000 —Ç–µ–∫—Å—Ç–æ–≤. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤—ã –Ω–µ –∏—Å—á–µ—Ä–ø–∞–µ—Ç–µ –≤—Å—é —Å–≤–æ—é –ø–∞–º—è—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ –æ–≥—Ä–æ–º–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.

–ü—Ä–æ–±–ª–µ–º–∞ —Å –æ–±—ä–µ–∫—Ç–æ–º-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –µ–≥–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑. –ò—Ç–∞–∫, –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–≤–∞–∂–¥—ã –¥–∞–≤–∞—Ç—å –Ω–∞–º —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–≤—ã—Ö 10 —Ü–∏—Ñ—Ä:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

–º—ã –ø–æ–ª—É—á–∏–º –∏—Ö —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑, –¥–∞–ª—å—à–µ —Å–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–µ—Ç –ø—É—Å—Ç—ã–º:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

–í–æ—Ç –ø–æ—á–µ–º—É –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞ `for`, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø–µ—Ä–∞—Ç–æ—Ä `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

–∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ, –Ω–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É, —á–µ–º –≤ –æ–±—ã—á–Ω–æ–º list comprehension.

## –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –∫–æ—Ä–ø—É—Å –≤ –≤–∏–¥–µ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –ø–∞–∫–µ—Ç–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤, –º—ã –≥–æ—Ç–æ–≤—ã –æ–±—É—á–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –î–ª—è —ç—Ç–æ–≥–æ –Ω–∞–º —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–∏–º —Å–≤—è–∑–∞—Ç—å —Å –Ω–∞—à–µ–π –º–æ–¥–µ–ª—å—é (–∑–¥–µ—Å—å, GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–±—É—á–∏—Ç—å –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º (–∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ GPT-2). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å —á—Ç–æ-–ª–∏–±–æ –æ–± –∞–ª–≥–æ—Ä–∏—Ç–º–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å; –Ω–∞—à –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–º –∂–µ, –∫–∞–∫ GPT-2, –∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ, —á—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—Å—è, ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ.

–°–Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏–∏:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'n', 'umbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two',
 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`', '."', '""', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

–≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ `ƒ†` –∏ `ƒä`, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ö–∞–∫ –º—ã –≤–∏–¥–∏–º, —ç—Ç–æ –Ω–µ —Å–ª–∏—à–∫–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–±–µ–ª–∞, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ–Ω –º–æ–≥ –±—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —É—Ä–æ–≤–Ω–∏ –æ—Ç—Å—Ç—É–ø–∞ (–ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∞–±–æ—Ä—ã –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –∏–ª–∏ –≤–æ—Å—å–º–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –±—É–¥—É—Ç –æ—á–µ–Ω—å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω—ã –≤ –∫–æ–¥–µ). –û–Ω —Ç–∞–∫–∂–µ –Ω–µ–º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–Ω–æ —Ä–∞–∑–¥–µ–ª—è–ª –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑-–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ `_`.

–î–∞–≤–∞–π—Ç–µ –æ–±—É—á–∏–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Ä–µ—à–∏—Ç –ª–∏ –æ–Ω —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã. –î–ª—è —ç—Ç–æ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –º–µ—Ç–æ–¥–æ–º `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

–≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è, –µ—Å–ª–∏ –≤–∞—à –∫–æ—Ä–ø—É—Å –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π, –Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 1,6 –ì–ë —Ç–µ–∫—Å—Ç–æ–≤ —ç—Ç–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –±—ã—Å—Ç—Ä–æ (1 –º–∏–Ω—É—Ç–∞ 16 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ AMD Ryzen 9 3900X —Å 12 —è–¥—Ä–∞–º–∏).

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ `AutoTokenizer.train_new_from_iterator()` —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ —Ç–æ–º —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —è–≤–ª—è–µ—Ç—Å—è ¬´–±—ã—Å—Ç—Ä—ã–º¬ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º. –ö–∞–∫ –≤—ã —É–≤–∏–¥–∏—Ç–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ, –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Transformers —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–∞ —Ç–∏–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤: –æ–¥–Ω–∏ –Ω–∞–ø–∏—Å–∞–Ω—ã –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ Python, –∞ –¥—Ä—É–≥–∏–µ (–±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–µ) –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π ü§ó Tokenizers, –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–π –Ω–∞ [Rust]( https://www.rust-lang.org). Python ‚Äî —ç—Ç–æ —è–∑—ã–∫, –∫–æ—Ç–æ—Ä—ã–π —á–∞—â–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –∫–æ–≥–¥–∞ —á—Ç–æ-—Ç–æ –Ω—É–∂–Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å, —á—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞—Ç—å –±—ã—Å—Ç—Ä–æ, —ç—Ç–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–∏—Å–∞—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, —É–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü, –ª–µ–∂–∞—â–µ–µ –≤ –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ CUDA, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ C –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤.

–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —á–∏—Å—Ç–æ–º Python –±—ã–ª–æ –±—ã –º—É—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–º, –ø–æ—ç—Ç–æ–º—É –º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫—É ü§ó Tokenizers. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±—ã–ª–æ –∏–∑—É—á–∞—Ç—å —è–∑—ã–∫ CUDA, —á—Ç–æ–±—ã –∏–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å –Ω–∞ –ø–∞–∫–µ—Ç–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ, –≤–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏–∑—É—á–∞—Ç—å Rust, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–≤—è–∑–∫–∏ Python –¥–ª—è –º–Ω–æ–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ –≤—ã–∑—ã–≤–∞—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞ –≤ Rust; –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–ª–∏, –∫–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](/course/ru/chapter3), —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–∞–∫–µ—Ç–∞ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö.

–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Transformer –∏–º–µ—é—Ç –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å [–∑–¥–µ—Å—å](https://huggingface.co/transformers/#supported-frameworks)), –∞ API `AutoTokenizer` –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤–∞—Å, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω. –í —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥—Ä—É–≥–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á, –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –û–¥–Ω–∞–∫–æ, –ø—Ä–µ–∂–¥–µ —á–µ–º —É–≥–ª—É–±–ª—è—Ç—å—Å—è –≤ —ç—Ç–æ, –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞—à –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`',
 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`."""', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

–ó–¥–µ—Å—å –º—ã —Å–Ω–æ–≤–∞ –≤–∏–¥–∏–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã `ƒ†` –∏ `ƒä`, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏, –Ω–æ –º—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –≤–∏–¥–µ—Ç—å, —á—Ç–æ –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑—É—á–∏–ª –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã, –æ—á–µ–Ω—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–æ—Ä–ø—É—Å–∞ —Ñ—É–Ω–∫—Ü–∏–π Python: –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å—Ç—å `ƒäƒ†ƒ†ƒ† ` —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç—Å—Ç—É–ø, –∏ —Ç–æ–∫–µ–Ω `ƒ†"""`, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—Ä–∏ –∫–∞–≤—ã—á–∫–∏, —Å –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ —Å–∏–º–≤–æ–ª—É `_`. –≠—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ; –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç–æ–º –∂–µ –ø—Ä–∏–º–µ—Ä–µ –¥–∞—Å—Ç –Ω–∞–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:


```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

–î–∞–≤–∞–π—Ç–µ –≤–∑–≥–ª—è–Ω–µ–º –Ω–∞ –µ—â–µ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä: 

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ƒ†Linear', 'Layer', '():', 'ƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'init', '__(', 'self', ',', 'ƒ†input', '_', 'size', ',',
 'ƒ†output', '_', 'size', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'weight', 'ƒ†=', 'ƒ†torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'ƒ†output', '_', 'size', ')', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'bias', 'ƒ†=', 'ƒ†torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ƒäƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'call', '__(', 'self', ',', 'ƒ†x', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',
 'ƒ†return', 'ƒ†x', 'ƒ†@', 'ƒ†self', '.', 'weights', 'ƒ†+', 'ƒ†self', '.', 'bias', 'ƒäƒ†ƒ†ƒ†ƒ†']
```

–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ç–æ–∫–µ–Ω—É, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É –æ—Ç—Å—Ç—É–ø—É, –∑–¥–µ—Å—å –º—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –≤–∏–¥–µ—Ç—å —Ç–æ–∫–µ–Ω –¥–ª—è –¥–≤–æ–π–Ω–æ–≥–æ –æ—Ç—Å—Ç—É–ø–∞: `ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†`. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ Python, —Ç–∞–∫–∏–µ –∫–∞–∫ `class`, `init`, `call`, `self` –∏ `return` —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏–º–µ–Ω–∞ –¥–∞–∂–µ –≤ –≤–µ—Ä–±–ª—é–∂—å–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ: `LinearLayer` —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ `["ƒ†Linear", "Layer"]`.

## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞

–ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã —Å–º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –ø–æ–∑–∂–µ, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—à –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å –º–æ–¥–µ–ª—è–º–∏, —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

–ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –ø–∞–ø–∫–∞ —Å –∏–º–µ–Ω–µ–º *code-search-net-tokenizer*, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è —ç—Ç–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º —Å–æ —Å–≤–æ–∏–º–∏ –∫–æ–ª–ª–µ–≥–∞–º–∏ –∏ –¥—Ä—É–∑—å—è–º–∏, –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ–≥–æ –≤ Hub, –≤–æ–π–¥—è –≤ —Å–≤–æ—é —É—á–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –µ—Å—Ç—å —É–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –≤ —ç—Ç–æ–º:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–≠—Ç–æ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç –≤–∏–¥–∂–µ—Ç, –≥–¥–µ –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Hugging Face. –ï—Å–ª–∏ –≤—ã –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É:

```bash
huggingface-cli login
```

–ü–æ—Å–ª–µ –≤—Ö–æ–¥–∞ –≤ —Å–∏—Å—Ç–µ–º—É –≤—ã –º–æ–∂–µ—Ç–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –≤—ã–ø–æ–ª–Ω–∏–≤ —Å–ª–µ–¥—É—é—â—É—é –∫–æ–º–∞–Ω–¥—É:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

–≠—Ç–æ —Å–æ–∑–¥–∞—Å—Ç –Ω–æ–≤—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≤ –≤–∞—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º–µ–Ω —Å –∏–º–µ–Ω–µ–º `code-search-net-tokenizer`, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ñ–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ó–∞—Ç–µ–º –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –ª—é–±–æ–≥–æ –º–µ—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞  `from_pretrained()`:

```py
# –ò–∑–º–µ–Ω–∏—Ç–µ "huggingface-course" –Ω–∞ –≤–∞—à–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –≤—Å–µ –≥–æ—Ç–æ–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è –∏ –µ–µ —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏! –ú—ã –≤–µ—Ä–Ω–µ–º—Å—è –∫ —ç—Ç–æ–º—É –≤ [–ì–ª–∞–≤–µ 7](/course/ru/chapter7), –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –≤ –æ—Å—Ç–∞–≤—à–µ–π—Å—è —á–∞—Å—Ç–∏ —ç—Ç–æ–π –≥–ª–∞–≤—ã –º—ã –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –±—ã—Å—Ç—Ä—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –∏ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –∫–æ–≥–¥–∞ –º—ã –≤—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ ` train_new_from_iterator()`.
