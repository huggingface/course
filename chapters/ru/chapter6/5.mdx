# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è Byte-Pair Encoding[[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

Byte-Pair Encoding (BPE) –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–æ–≤, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å OpenAI –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ GPT. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–æ –º–Ω–æ–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è GPT, GPT-2, RoBERTa, BART –∏ DeBERTa.

<Youtube id="HEikzVL-lZU"/>

<Tip>

üí° –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è BPE, –≤–ø–ª–æ—Ç—å –¥–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª, –µ—Å–ª–∏ –≤–∞–º –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–±—â–∏–π –æ–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è[[training-algorithm]]

–û–±—É—á–µ–Ω–∏–µ BPE –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –∫–æ—Ä–ø—É—Å–µ (–ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–∞–ø–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏), –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä—ã–π –∑–∞–Ω–æ—Å—è—Ç—Å—è –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ —ç—Ç–∏—Ö —Å–ª–æ–≤. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –≤ –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –ø—è—Ç—å —Å–ª–æ–≤:

```
"hug", "pug", "pun", "bun", "hugs"
```

–¢–æ–≥–¥–∞ –±–∞–∑–æ–≤—ã–º —Å–ª–æ–≤–∞—Ä–µ–º –±—É–¥–µ—Ç `["b", "g", "h", "n", "p", "s", "u"]`. –í —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ —ç—Ç–æ—Ç –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å, –∫–∞–∫ –º–∏–Ω–∏–º—É–º, –≤—Å–µ —Å–∏–º–≤–æ–ª—ã ASCII, –∞ –≤–æ–∑–º–æ–∂–Ω–æ, –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∏–º–≤–æ–ª—ã Unicode. –ï—Å–ª–∏ –≤ –ø—Ä–∏–º–µ—Ä–µ, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏–º–≤–æ–ª, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ, —ç—Ç–æ—Ç —Å–∏–º–≤–æ–ª –±—É–¥–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω. –≠—Ç–æ –æ–¥–Ω–∞ –∏–∑ –ø—Ä–∏—á–∏–Ω, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ NLP –æ—á–µ–Ω—å –ø–ª–æ—Ö–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å —ç–º–æ–¥–∂–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä.

<Tip>

–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã GPT-2 –∏ RoBERTa (–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–≤–æ–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏) –∏–º–µ—é—Ç —É–º–Ω—ã–π —Å–ø–æ—Å–æ–± —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã: –æ–Ω–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Å–ª–æ–≤–∞ –Ω–µ –∫–∞–∫ —Å–∏–º–≤–æ–ª—ã Unicode, –∞ –∫–∞–∫ –±–∞–π—Ç—ã. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–º–µ–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä (256), –Ω–æ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å, –≤—Å–µ —Ä–∞–≤–Ω–æ –±—É–¥—É—Ç –≤–∫–ª—é—á–µ–Ω—ã –∏ –Ω–µ –±—É–¥—É—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω. –≠—Ç–æ—Ç —Ç—Ä—é–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è *byte-level BPE*.

</Tip>

–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –∂–µ–ª–∞–µ–º–æ–≥–æ –æ–±—ä–µ–º–∞ —Å–ª–æ–≤–∞—Ä—è, –æ–±—É—á–∞—è—Å—å *—Å–ª–∏—è–Ω–∏—è–º*, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è –¥–≤—É—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è –≤ –Ω–æ–≤—ã–π. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–∏ —Å–ª–∏—è–Ω–∏—è –±—É–¥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã —Å –¥–≤—É–º—è —Å–∏–º–≤–æ–ª–∞–º–∏, –∞ –∑–∞—Ç–µ–º, –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è, –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ–¥—Å–ª–æ–≤–∞.

–ù–∞ –ª—é–±–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∞–ª–≥–æ—Ä–∏—Ç–º BPE –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –ø–∞—Ä—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ–¥ "–ø–∞—Ä–æ–π" –∑–¥–µ—Å—å –ø–æ–Ω–∏–º–∞—é—Ç—Å—è –¥–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–µ). –≠—Ç–∞ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∞—è—Å—è –ø–∞—Ä–∞ –∏ –±—É–¥–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞, –ø–æ—Å–ª–µ —á–µ–≥–æ –≤—Å–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞.

–í–æ–∑–≤—Ä–∞—â–∞—è—Å—å –∫ –Ω–∞—à–µ–º—É –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É –ø—Ä–∏–º–µ—Ä—É, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —Å–ª–æ–≤–∞ –∏–º–µ—é—Ç —Å–ª–µ–¥—É—é—â—É—é —á–∞—Å—Ç–æ—Ç—É:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

–∑–Ω–∞—á–µ–Ω–∏–µ `" hug"` –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤ –∫–æ—Ä–ø—É—Å–µ 10 —Ä–∞–∑, `"pug"` - 5 —Ä–∞–∑, `"pun"` - 12 —Ä–∞–∑, `"bun"` - 4 —Ä–∞–∑–∞, –∏ `"hugs"` - 5 —Ä–∞–∑. –ú—ã –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–∞ —á–∞—Å—Ç–∏ —Å–∏–º–≤–æ–ª–æ–≤ (—Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –Ω–∞—à –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å), —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

–ó–∞—Ç–µ–º –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–∞—Ä—ã. –ü–∞—Ä–∞ `("h", "u")` –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ö `"hug"` –∏ `"hugs"`, –≤—Å–µ–≥–æ 15 —Ä–∞–∑ –≤ –∫–æ—Ä–ø—É—Å–µ. –û–¥–Ω–∞–∫–æ —ç—Ç–æ –Ω–µ —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞: —ç—Ç–∞ —á–µ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç `("u", "g")`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ö `"hug"`, `"pug"` –∏ `"hugs"`, –≤ –æ–±—â–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ 20 —Ä–∞–∑ –≤ —Å–ª–æ–≤–∞—Ä–µ.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–µ—Ä–≤–æ–µ –ø—Ä–∞–≤–∏–ª–æ —Å–ª–∏—è–Ω–∏—è, –≤—ã—É—á–µ–Ω–Ω–æ–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º, - `("u", "g") -> "ug"`, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ `"ug"` –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä—å, –∏ —ç—Ç–∞ –ø–∞—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ –≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ö –∫–æ—Ä–ø—É—Å–∞. –í –∫–æ–Ω—Ü–µ —ç—Ç–æ–≥–æ —ç—Ç–∞–ø–∞ —Å–ª–æ–≤–∞—Ä—å –∏ –∫–æ—Ä–ø—É—Å –≤—ã–≥–ª—è–¥—è—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω –¥–ª–∏–Ω–Ω–µ–µ –¥–≤—É—Ö —Å–∏–º–≤–æ–ª–æ–≤: –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–∞—Ä–∞ `("h", "ug")` (–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –∫–æ—Ä–ø—É—Å–µ 15 —Ä–∞–∑). –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –ø–∞—Ä–∞ –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ - `("u", "n")`, –æ–¥–Ω–∞–∫–æ –æ–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –∫–æ—Ä–ø—É—Å–µ 16 —Ä–∞–∑, –ø–æ—ç—Ç–æ–º—É –≤—Ç–æ—Ä–æ–µ –≤—ã—É—á–µ–Ω–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ —Å–ª–∏—è–Ω–∏—è - `("u", "n") -> "un"`. –î–æ–±–∞–≤–∏–≤ —ç—Ç–æ –≤ —Å–ª–æ–≤–∞—Ä—å –∏ –æ–±—ä–µ–¥–∏–Ω–∏–≤ –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è, –º—ã –ø–æ–ª—É—á–∞–µ–º:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

–¢–µ–ø–µ—Ä—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–π –ø–∞—Ä–æ–π —è–≤–ª—è–µ—Ç—Å—è `("h", "ug")`, –ø–æ—ç—Ç–æ–º—É –º—ã –∏–∑—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª–æ —Å–ª–∏—è–Ω–∏—è `("h", "ug") -> "hug"`, —á—Ç–æ –¥–∞–µ—Ç –Ω–∞–º –ø–µ—Ä–≤—ã–π —Ç—Ä–µ—Ö–±—É–∫–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω. –ü–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è –∫–æ—Ä–ø—É—Å –≤—ã–≥–ª—è–¥–∏—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

–ò –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω–µ–º –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è.

<Tip>

‚úèÔ∏è **–¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –æ—á–µ—Ä–µ–¥—å!** –ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, –∫–∞–∫–∏–º –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ —Å–ª–∏—è–Ω–∏—è?

</Tip>

## –ê–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏[[tokenization-algorithm]]

–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–ª–µ–¥—É–µ—Ç –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –≤ —Ç–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ –Ω–æ–≤—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—É—Ç–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤:

1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
2. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª —Å–ª–∏—è–Ω–∏—è, –∏–∑—É—á–µ–Ω–Ω—ã—Ö –ø–æ –ø–æ—Ä—è–¥–∫—É, –∫ —ç—Ç–∏–º —á–∞—Å—Ç—è–º

–í–æ–∑—å–º–µ–º –ø—Ä–∏–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —Å —Ç—Ä–µ–º—è –≤—ã—É—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ —Å–ª–∏—è–Ω–∏—è:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

–°–ª–æ–≤–æ `"bug"` –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–∫ `["b", "ug"]`. –°–ª–æ–≤–æ `"mug"`, –æ–¥–Ω–∞–∫–æ, –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–∫ `["[UNK]", "ug"]`, –ø–æ—Å–∫–æ–ª—å–∫—É –±—É–∫–≤–∞ `"m"` –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–æ–≤–æ–º —Å–ª–æ–≤–∞—Ä–µ. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, —Å–ª–æ–≤–æ `"thug" –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–∫ `["[UNK]", "hug"]`: –±—É–∫–≤–∞ `"t" –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–æ–≤–æ–º —Å–ª–æ–≤–∞—Ä–µ, –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª —Å–ª–∏—è–Ω–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–Ω–∞—á–∞–ª–∞ –∫ —Å–ª–∏—è–Ω–∏—é `"u"` –∏ `"g"`, –∞ –∑–∞—Ç–µ–º –∫ —Å–ª–∏—è–Ω–∏—é `"h"` –∏ `"ug"`.

<Tip>

‚úèÔ∏è ** –¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –æ—á–µ—Ä–µ–¥—å!** –ö–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ, –∫–∞–∫ –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤–æ `'unhug'`?

</Tip>

## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è BPE[[implementing-bpe]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ BPE. –≠—Ç–æ –Ω–µ –±—É–¥–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –∫–æ—Ç–æ—Ä—É—é –≤—ã —Å–º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ; –º—ã –ø—Ä–æ—Å—Ç–æ —Ö–æ—Ç–∏–º –ø–æ–∫–∞–∑–∞—Ç—å –≤–∞–º –∫–æ–¥, —á—Ç–æ–±—ã –≤—ã –º–æ–≥–ª–∏ –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º.

–î–ª—è –Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–µ–Ω –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–∞, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç–æ–π –∫–æ—Ä–ø—É—Å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

–î–∞–ª–µ–µ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä–ø—É—Å –≤ —Å–ª–æ–≤–∞. –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä BPE (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT-2), –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `gpt2`:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

–ó–∞—Ç–µ–º –º—ã –≤—ã—á–∏—Å–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ, –∫–∞–∫ –∏ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'ƒ†is': 2, 'ƒ†the': 1, 'ƒ†Hugging': 1, 'ƒ†Face': 1, 'ƒ†Course': 1, '.': 4, 'ƒ†chapter': 1,
    'ƒ†about': 1, 'ƒ†tokenization': 1, 'ƒ†section': 1, 'ƒ†shows': 1, 'ƒ†several': 1, 'ƒ†tokenizer': 1, 'ƒ†algorithms': 1,
    'Hopefully': 1, ',': 1, 'ƒ†you': 1, 'ƒ†will': 1, 'ƒ†be': 1, 'ƒ†able': 1, 'ƒ†to': 1, 'ƒ†understand': 1, 'ƒ†how': 1,
    'ƒ†they': 1, 'ƒ†are': 1, 'ƒ†trained': 1, 'ƒ†and': 1, 'ƒ†generate': 1, 'ƒ†tokens': 1})
```

–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ - —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, —Å–æ—Å—Ç–æ—è—â–µ–≥–æ –∏–∑ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –∫–æ—Ä–ø—É—Å–µ:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'ƒ†']
```

–ú—ã —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª—å—é, –≤ –Ω–∞—á–∞–ª–æ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –í —Å–ª—É—á–∞–µ GPT-2 –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º —è–≤–ª—è–µ—Ç—Å—è `"<|endoftext|>"`:

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

–¢–µ–ø–µ—Ä—å –Ω–∞–º –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é, –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤—ã—á–∏—Å–ª—è–µ—Ç —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã. –ù–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —á–∞—Å—Ç—å —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –ø–æ—Å–ª–µ –ø–µ—Ä–≤—ã—Ö —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('ƒ†', 'i'): 2
('ƒ†', 't'): 7
('t', 'h'): 3
```

–¢–µ–ø–µ—Ä—å, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â—É—é—Å—è –ø–∞—Ä—É, –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ –ª–∏—à—å —Å–¥–µ–ª–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ü–∏–∫–ª:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('ƒ†', 't') 7
```

–ò—Ç–∞–∫, –ø–µ—Ä–≤–æ–µ —Å–ª–∏—è–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤—ã—É—á–∏—Ç—å, —ç—Ç–æ `('ƒ†', 't') -> 'ƒ†t'`, –∏ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º `'ƒ†t'` –≤ —Å–ª–æ–≤–∞—Ä—å:

```python
merges = {("ƒ†", "t"): "ƒ†t"}
vocab.append("ƒ†t")
```

–ß—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å, –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ –Ω–∞—à–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä–µ `splits` —Å–ª–æ–≤–∞—Ä—è. –î–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º –¥–ª—è —ç—Ç–æ–≥–æ –µ—â–µ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

–ò –º—ã –º–æ–∂–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–≤–æ–≥–æ —Å–ª–∏—è–Ω–∏—è:

```py
splits = merge_pair("ƒ†", "t", splits)
print(splits["ƒ†trained"])
```

```python out
['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd']
```

–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ, —á—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø—Ä–æ–∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –º—ã –Ω–µ –≤—ã—É—á–∏–º –≤—Å–µ —Å–ª–∏—è–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω—É–∂–Ω—ã. –ü—É—Å—Ç—å —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –±—É–¥–µ—Ç 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –º—ã –≤—ã—É—á–∏–ª–∏ 19 –ø—Ä–∞–≤–∏–ª —Å–ª–∏—è–Ω–∏—è (–∏—Å—Ö–æ–¥–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–º–µ–ª —Ä–∞–∑–º–µ—Ä 31 - 30 —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∞–ª—Ñ–∞–≤–∏—Ç–µ –ø–ª—é—Å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω):

```py
print(merges)
```

```python out
{('ƒ†', 't'): 'ƒ†t', ('i', 's'): 'is', ('e', 'r'): 'er', ('ƒ†', 'a'): 'ƒ†a', ('ƒ†t', 'o'): 'ƒ†to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ƒ†to', 'k'): 'ƒ†tok',
 ('ƒ†tok', 'en'): 'ƒ†token', ('n', 'd'): 'nd', ('ƒ†', 'is'): 'ƒ†is', ('ƒ†t', 'h'): 'ƒ†th', ('ƒ†th', 'e'): 'ƒ†the',
 ('i', 'n'): 'in', ('ƒ†a', 'b'): 'ƒ†ab', ('ƒ†token', 'i'): 'ƒ†tokeni'}
```

–ê —Å–ª–æ–≤–∞—Ä—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∞–ª—Ñ–∞–≤–∏—Ç–∞ –∏ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–ª–∏—è–Ω–∏—è:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†', 'ƒ†t', 'is', 'er', 'ƒ†a', 'ƒ†to', 'en', 'Th', 'This', 'ou', 'se',
 'ƒ†tok', 'ƒ†token', 'nd', 'ƒ†is', 'ƒ†th', 'ƒ†the', 'in', 'ƒ†ab', 'ƒ†tokeni']
```

<Tip>

üí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `train_new_from_iterator()` –Ω–∞ —Ç–æ–º –∂–µ –∫–æ—Ä–ø—É—Å–µ –Ω–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞—Ä—è. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –ø–∞—Ä—ã –º—ã –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é –ø–æ–ø–∞–≤—à—É—é—Å—è, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Tokenizers –≤—ã–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–≤—É—é –ø–∞—Ä—É, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –µ–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö ID.

</Tip>

–ß—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç, –º—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –µ–≥–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –∏–∑—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

–ú—ã –º–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —ç—Ç–æ –Ω–∞ –ª—é–±–æ–º —Ç–µ–∫—Å—Ç–µ, —Å–æ—Å—Ç–æ—è—â–µ–º –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ –∞–ª—Ñ–∞–≤–∏—Ç–∞:

```py
tokenize("This is not a token.")
```

```python out
['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.']
```

<Tip warning={true}>

‚ö†Ô∏è –ù–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –≤—ã–±—Ä–∞—Å—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –Ω–∏—á–µ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–ª–∏ –¥–ª—è –∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –≤ GPT-2 –Ω–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ BPE –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤), –Ω–æ –∑–¥–µ—Å—å —ç—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –Ω–µ –≤–∫–ª—é—á–∏–ª–∏ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –±–∞–π—Ç—ã –≤ –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å. –≠—Ç–æ—Ç –∞—Å–ø–µ–∫—Ç BPE –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞, –ø–æ—ç—Ç–æ–º—É –º—ã –æ–ø—É—Å—Ç–∏–ª–∏ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏.

</Tip>

–í–æ—Ç –∏ –≤—Å–µ –æ–± –∞–ª–≥–æ—Ä–∏—Ç–º–µ BPE! –î–∞–ª–µ–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º WordPiece.