<FrameworkSwitchCourse {fw} />

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤[[token-classification]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb"},
]} />

{/if}

–ü–µ—Ä–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, - —ç—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–∞ –æ–±—â–∞—è –∑–∞–¥–∞—á–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ª—é–±—É—é –ø—Ä–æ–±–ª–µ–º—É, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ "–ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –º–µ—Ç–∫–∏ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏", –Ω–∞–ø—Ä–∏–º–µ—Ä:

- **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (Named entity recognition - NER)**: –ü–æ–∏—Å–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏—Ü, –º–µ—Å—Ç –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π) –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –≠—Ç–æ –º–æ–∂–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –ø—Ä–∏–ø–∏—Å—ã–≤–∞–Ω–∏–µ –º–µ—Ç–∫–∏ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É, –∏–º–µ—è –æ–¥–∏–Ω –∫–ª–∞—Å—Å –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –æ–¥–∏–Ω –∫–ª–∞—Å—Å –¥–ª—è "–Ω–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏".
- **–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ (Part-of-speech tagging - POS)**: –ü–æ–º–µ—Ç–∏—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –∫–∞–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏ —Ä–µ—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, –≥–ª–∞–≥–æ–ª, –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ –∏ —Ç. –¥.).
- **–í—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (Chunking)**: –ü–æ–∏—Å–∫ —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏—Ö –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Å—É—â–Ω–æ—Å—Ç–∏. –≠—Ç–∞ –∑–∞–¥–∞—á–∞ (–∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∞ —Å POS –∏–ª–∏ NER) –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–µ—Ç–∫–∏ (–æ–±—ã—á–Ω–æ `B-`) –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞, –¥—Ä—É–≥–æ–π –º–µ—Ç–∫–∏ (–æ–±—ã—á–Ω–æ `I-`) - —Ç–æ–∫–µ–Ω–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞, –∏ —Ç—Ä–µ—Ç—å–µ–π –º–µ—Ç–∫–∏ (–æ–±—ã—á–Ω–æ `O`) - —Ç–æ–∫–µ–Ω–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –Ω–∏ –∫ –æ–¥–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É.

<Youtube id="wVHdVlPScxA"/>

–ö–æ–Ω–µ—á–Ω–æ, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤; —ç—Ç–æ –ª–∏—à—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –¥–æ–æ–±—É—á–∏–º –º–æ–¥–µ–ª—å (BERT) –¥–ª—è –∑–∞–¥–∞—á–∏ NER, –∫–æ—Ç–æ—Ä–∞—è –∑–∞—Ç–µ–º —Å–º–æ–∂–µ—Ç –≤—ã—á–∏—Å–ª—è—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã, –ø–æ–¥–æ–±–Ω—ã–µ —ç—Ç–æ–º—É:

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

–í—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–±—É—á–∏–º –∏ –∑–∞–≥—Ä—É–∑–∏–º –Ω–∞ —Ö–∞–±, –∏ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è [–∑–¥–µ—Å—å](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö[[preparing-the-data]]

–ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, –Ω–∞–º –Ω—É–∂–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [–Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö CoNLL-2003](https://huggingface.co/datasets/conll2003), –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç Reuters.

<Tip>

üí° –ï—Å–ª–∏ –≤–∞—à –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤, —á–∞—Å—Ç—å –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–æ–≤ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–∫–∞–º–∏, –≤—ã —Å–º–æ–∂–µ—Ç–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –∑–¥–µ—Å—å –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫ —Å–≤–æ–µ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ [–ì–ª–∞–≤–µ 5](../chapter5/1), –µ—Å–ª–∏ –≤–∞–º –Ω—É–∂–Ω–æ –æ—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ —Ç–æ, –∫–∞–∫ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ `Dataset`.

</Tip>

### –î–∞—Ç–∞—Å–µ—Ç CoNLL-2003[[the-conll-2003-dataset]]

–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ CoNLL-2003 –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ `load_dataset()` –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets:

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –∫–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1) –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ GLUE MRPC. –ò–∑—É—á–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–º –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã –∏ —á–∞—Å—Ç–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ, –ø—Ä–æ–≤–µ—Ä–æ—á–Ω–æ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∫–∏ –¥–ª—è —Ç—Ä–µ—Ö –∑–∞–¥–∞—á, –æ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ —Ä–∞–Ω–µ–µ: NER, POS –∏ chunking. –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ç–ª–∏—á–∏–µ–º –æ—Ç –¥—Ä—É–≥–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–µ –∫–∞–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∞ –∫–∞–∫ —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è `tokens`, –Ω–æ –æ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–∞ –≤ —Ç–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ —ç—Ç–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–∏–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–π—Ç–∏ —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ –ø–æ–¥—Å–ª–æ–≤–∞–º).

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞:

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ö–æ—Ç–∏–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π, –º—ã –∏–∑—É—á–∏–º —Ç–µ–≥–∏ NER:

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

–≠—Ç–æ –º–µ—Ç–∫–∏ –≤ –≤–∏–¥–µ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –≥–æ—Ç–æ–≤—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –æ–Ω–∏ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω—ã, –∫–æ–≥–¥–∞ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —Ü–µ–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –º–µ—Ç–æ–∫, –ø–æ—Å–º–æ—Ç—Ä–µ–≤ –Ω–∞ –∞—Ç—Ä–∏–±—É—Ç `features` –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —ç—Ç–æ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å–æ–¥–µ—Ä–∂–∏—Ç —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ `ClassLabel`. –¢–∏–ø —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–∫–∞–∑–∞–Ω –≤ –∞—Ç—Ä–∏–±—É—Ç–µ `feature` —ç—Ç–æ–≥–æ `ner_feature`, –∏ –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–ø–∏—Å–∫—É –∏–º–µ–Ω, –ø–æ—Å–º–æ—Ç—Ä–µ–≤ –Ω–∞ –∞—Ç—Ä–∏–±—É—Ç `names` —ç—Ç–æ–≥–æ `feature`:

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

–ú—ã —É–∂–µ –≤–∏–¥–µ–ª–∏ —ç—Ç–∏ –º–µ—Ç–∫–∏ –ø—Ä–∏ –∏–∑—É—á–µ–Ω–∏–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ `token-classification` –≤ [–ì–ª–∞–≤–µ 6](../chapter6/3), –Ω–æ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –Ω–∞–ø–æ–º–Ω–∏–º:  

- `O` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–æ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–∞–∫–æ–π-–ª–∏–±–æ —Å—É—â–Ω–æ—Å—Ç–∏.
- `B-PER`/`I-PER` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—á–∞–ª—É/–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω—ã *person*.
- `B-ORG`/`I-ORG` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—á–∞–ª—É/–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ *organization*.
- `B-LOC`/`I-LOC` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—á–∞–ª—É/–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ *location*.
- `B-MISC`/`I-MISC` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–ª–æ–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—á–∞–ª—É/–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–Ω–æ—Å—Ç–∏ *miscellaneous*.

–¢–µ–ø–µ—Ä—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ, –¥–∞–µ—Ç –Ω–∞–º —Å–ª–µ–¥—É—é—â–µ–µ:

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ `B-` –∏ `I-`, –≤–æ—Ç —á—Ç–æ –¥–∞–µ—Ç —Ç–æ—Ç –∂–µ –∫–æ–¥ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞ –æ–±—É—á–∞—é—â–µ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º 4:

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, —Å—É—â–Ω–æ—Å—Ç—è–º, —Å–æ—Å—Ç–æ—è—â–∏–º –∏–∑ –¥–≤—É—Ö —Å–ª–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä "European Union" –∏ "Werner Zwingmann", –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –º–µ—Ç–∫–∞ `B-` –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –∏ –º–µ—Ç–∫–∞ `I-` –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –í—ã–≤–µ–¥–∏—Ç–µ —Ç–µ –∂–µ –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –º–µ—Ç–∫–∞–º–∏ POS –∏–ª–∏ chunking.

</Tip>

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö[[processing-the-data]]

<Youtube id="iY2AZYdZAr0"/>

–ö–∞–∫ –æ–±—ã—á–Ω–æ, –Ω–∞—à–∏ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–µ–∂–¥–µ —á–µ–º –º–æ–¥–µ–ª—å —Å–º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å –∏—Ö —Å–º—ã—Å–ª. –ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 6](../chapter6/), —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ç–ª–∏—á–∏–µ–º –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ö —Å—á–∞—Å—Ç—å—é, API —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —ç—Ç–∏–º –¥–æ–≤–æ–ª—å–Ω–æ –ª–µ–≥–∫–æ; –Ω–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏—Ç—å `tokenizer` —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ñ–ª–∞–≥–æ–º.

–î–ª—è –Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –æ–±—ä–µ–∫—Ç `tokenizer`. –ö–∞–∫ –º—ã —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª–∏, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å BERT, –ø–æ—ç—Ç–æ–º—É –Ω–∞—á–Ω–µ–º —Å –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

–í—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ–Ω–∏—Ç—å `model_checkpoint` –Ω–∞ –ª—é–±—É—é –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –∏–∑ [Hub](https://huggingface.co/models) –∏–ª–∏ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä. –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Tokenizers, –ø–æ—ç—Ç–æ–º—É —Å—É—â–µ—Å—Ç–≤—É–µ—Ç "–±—ã—Å—Ç—Ä–∞—è" –≤–µ—Ä—Å–∏—è. –í—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –≤—Å–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Å –±—ã—Å—Ç—Ä–æ–π –≤–µ—Ä—Å–∏–µ–π –≤ [—ç—Ç–æ–π –±–æ–ª—å—à–æ–π —Ç–∞–±–ª–∏—Ü–µ](https://huggingface.co/transformers/#supported-frameworks), –∞ —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤–∞–º–∏ –æ–±—ä–µ–∫—Ç `tokenizer` –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è ü§ó Tokenizers, –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –µ–≥–æ –∞—Ç—Ä–∏–±—É—Ç `is_fast`:

```py
tokenizer.is_fast
```

```python out
True
```

–î–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–∏–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–≤–æ–¥–∞ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à `tokenizer`, –∫–∞–∫ –æ–±—ã—á–Ω–æ, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–≤ `is_split_into_words=True`:

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–±–∞–≤–∏–ª —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª—å—é (`[CLS]` –≤ –Ω–∞—á–∞–ª–µ –∏ `[SEP]` –≤ –∫–æ–Ω—Ü–µ), –∏ –æ—Å—Ç–∞–≤–∏–ª –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–æ–≤ –Ω–µ—Ç—Ä–æ–Ω—É—Ç—ã–º–∏. –°–ª–æ–≤–æ `lamb`, –æ–¥–Ω–∞–∫–æ, –±—ã–ª–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ –¥–≤–∞ –ø–æ–¥—Å–ª–æ–≤–∞, `la` –∏ `##mb`. –≠—Ç–æ –≤–Ω–æ—Å–∏—Ç –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –Ω–∞—à–∏–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏: —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ —Å–æ—Å—Ç–æ–∏—Ç –≤—Å–µ–≥–æ –∏–∑ 9 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∞—Ç 12 —Ç–æ–∫–µ–Ω–æ–≤. –£—á–µ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ª–µ–≥–∫–æ (–º—ã –∑–Ω–∞–µ–º, —á—Ç–æ –æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –∏ –≤ –∫–æ–Ω—Ü–µ), –Ω–æ –Ω–∞–º —Ç–∞–∫–∂–µ –Ω—É–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã –≤—ã—Ä–æ–≤–Ω—è–ª–∏ –≤—Å–µ –º–µ—Ç–∫–∏ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.

–ö —Å—á–∞—Å—Ç—å—é, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —É –Ω–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å—É–ø–µ—Ä—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º ü§ó Tokenizers, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —Å–ª–æ–≤–æ–º (–∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ [–ì–ª–∞–≤–∞ 6](../chapter6/3)):

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

–ù–µ–º–Ω–æ–≥–æ –ø–æ—Ä–∞–±–æ—Ç–∞–≤, –º—ã —Å–º–æ–∂–µ–º —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫, —á—Ç–æ–±—ã –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª —Ç–æ–∫–µ–Ω–∞–º. –ü–µ—Ä–≤–æ–µ –ø—Ä–∞–≤–∏–ª–æ, –∫–æ—Ç–æ—Ä–æ–µ –º—ã –ø—Ä–∏–º–µ–Ω–∏–º, –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç –º–µ—Ç–∫—É `-100`. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `-100` - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å, –∫–æ—Ç–æ—Ä—ã–π –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è). –ó–∞—Ç–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–∞–µ—Ç —Ç—É –∂–µ –º–µ—Ç–∫—É, —á—Ç–æ –∏ —Ç–æ–∫–µ–Ω, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∞–ª–æ—Å—å —Å–ª–æ–≤–æ, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ —è–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç—å—é –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Å—É—â–Ω–æ—Å—Ç–∏. –î–ª—è —Ç–æ–∫–µ–Ω–æ–≤, –Ω–∞—Ö–æ–¥—è—â–∏—Ö—Å—è –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞, –Ω–æ –Ω–µ –≤ –µ–≥–æ –Ω–∞—á–∞–ª–µ, –º—ã –∑–∞–º–µ–Ω—è–µ–º `B-` –Ω–∞ `I-` (–ø–æ—Å–∫–æ–ª—å–∫—É —Ç–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –Ω–µ —è–≤–ª—è—é—Ç—Å—è –Ω–∞—á–∞–ª–æ–º —Å—É—â–Ω–æ—Å—Ç–∏):

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
            new_labels.append(-100)
        else:
            # –¢–æ –∂–µ —Å–ª–æ–≤–æ, —á—Ç–æ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ–∫–µ–Ω
            label = labels[word_id]
            # –ï—Å–ª–∏ –º–µ—Ç–∫–∞ B-XXX, –∑–∞–º–µ–Ω—è–µ–º –µ–µ –Ω–∞ I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

–î–∞–≤–∞–π—Ç–µ –æ–ø—Ä–æ–±—É–µ–º —ç—Ç–æ –Ω–∞ –Ω–∞—à–µ–º –ø–µ—Ä–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏:

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–±–∞–≤–∏–ª–∞ `-100` –¥–ª—è –¥–≤—É—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –∏ –≤ –∫–æ–Ω—Ü–µ –∏ –Ω–æ–≤—ã–π `0` –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä–æ–µ –±—ã–ª–æ —Ä–∞–∑–±–∏—Ç–æ –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç –Ω–∞–∑–Ω–∞—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–µ—Ç–∫—É –Ω–∞ —Å–ª–æ–≤–æ –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—Ç—å `-100` –¥—Ä—É–≥–∏–º –ø–æ–¥—Ç–æ–∫–µ–Ω–∞–º –≤ –¥–∞–Ω–Ω–æ–º —Å–ª–æ–≤–µ. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, —á–∞—Å—Ç—å –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å—É–±—Ç–æ–∫–µ–Ω–æ–≤, –Ω–µ –≤–Ω–æ—Å–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥ –≤ –ø–æ—Ç–µ—Ä–∏.

</Tip>

–ß—Ç–æ–±—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–µ—Å—å –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤—Å–µ—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å `align_labels_with_tokens()` –∫–æ –≤—Å–µ–º –º–µ—Ç–∫–∞–º. –ß—Ç–æ–±—ã –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞–º–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –Ω–∞—à–µ–≥–æ –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –ø–æ—ç—Ç–æ–º—É –º—ã –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ `Dataset.map()` —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `batched=True`. –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –Ω–∞—à–µ–≥–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è `word_ids()` –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–≤ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞–º –Ω—É–∂–Ω—ã, —Å —É—á—ë—Ç–æ–º —Ç–æ–≥–æ —á—Ç–æ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ (–∏–ª–∏, –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ, —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤), –ø–æ—ç—Ç–æ–º—É –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –∏ —ç—Ç–æ:

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª—è–ª–∏ –≤–æ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã; –º—ã —Å–¥–µ–ª–∞–µ–º —ç—Ç–æ –ø–æ–∑–∂–µ, –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞—Ç—á–µ–π —Å –ø–æ–º–æ—â—å—é –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤—Å—é —ç—Ç—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∫ –¥—Ä—É–≥–∏–º —á–∞—Å—Ç—è–º –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

–ú—ã —Å–¥–µ–ª–∞–ª–∏ —Å–∞–º—É—é —Å–ª–æ–∂–Ω—É—é —á–∞—Å—Ç—å! –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, —Å–∞–º–æ –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —ç—Ç–æ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1).

{#if fw === 'pt'}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é API `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π `Trainer`, –±—É–¥–µ—Ç —Ç–∞–∫–∏–º –∂–µ, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ; –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è - —ç—Ç–æ —Å–ø–æ—Å–æ–± –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á –∏ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏.

{:else}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é Keras[[fine-tuning-the-model-with-keras]]

–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π Keras, –±—É–¥–µ—Ç –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π; –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è - —ç—Ç–æ —Å–ø–æ—Å–æ–± –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á –∏ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏.

{/if}


### –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö[[data-collation]]

–ú—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `DataCollatorWithPadding`, –∫–∞–∫ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1), –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –¥–æ–ø–æ–ª–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤—Ö–æ–¥–æ–≤, –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤). –ó–¥–µ—Å—å –Ω–∞—à–∏ –º–µ—Ç–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ–ø–æ–ª–Ω–µ–Ω—ã —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤—Ö–æ–¥—ã, —á—Ç–æ–±—ã –æ–Ω–∏ –æ—Å—Ç–∞–≤–∞–ª–∏—Å—å –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É—è `-100` –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ç–µ—Ä—å.

–í—Å–µ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification). –ö–∞–∫ –∏ `DataCollatorWithPadding`, –æ–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç `—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä`, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

–ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö, –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –≤—ã–∑–≤–∞—Ç—å –µ–≥–æ –Ω–∞ —Å–ø–∏—Å–∫–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –Ω–∞—à–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

–î–∞–≤–∞–π—Ç–µ —Å—Ä–∞–≤–Ω–∏–º —ç—Ç–æ —Å –º–µ—Ç–∫–∞–º–∏ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, –≤—Ç–æ—Ä–æ–π –Ω–∞–±–æ—Ä –º–µ—Ç–æ–∫ –±—ã–ª –¥–æ–ø–æ–ª–Ω–µ–Ω –¥–æ –¥–ª–∏–Ω—ã –ø–µ—Ä–≤–æ–≥–æ —Å –ø–æ–º–æ—â—å—é –∑–Ω–∞—á–µ–Ω–∏—è `-100`.

{:else}

–ù–∞—à –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ! –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ `tf.data.Dataset` —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `to_tf_dataset()`. –í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `model.prepare_tf_dataset()`, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ–¥–∞ - –≤—ã —É–≤–∏–¥–∏—Ç–µ —ç—Ç–æ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –¥—Ä—É–≥–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö —ç—Ç–æ–π –≥–ª–∞–≤—ã.

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```


 –°–ª–µ–¥—É—é—â–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: —Å–∞–º–∞ –º–æ–¥–µ–ª—å.

{/if}

{#if fw === 'tf'}

### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏[[defining-the-model]]

–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –ø—Ä–æ–±–ª–µ–º–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å `TFAutoModelForTokenClassification`. –ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏, - —ç—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏–º–µ—é—â–∏—Ö—Å—è —É –Ω–∞—Å –º–µ—Ç–æ–∫. –ü—Ä–æ—â–µ –≤—Å–µ–≥–æ –ø–µ—Ä–µ–¥–∞—Ç—å —ç—Ç–æ —á–∏—Å–ª–æ —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `num_labels`, –Ω–æ –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Å–∏–≤—ã–π –≤–∏–¥–∂–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –ø–æ–¥–æ–±–Ω—ã–π —Ç–æ–º—É, —á—Ç–æ –º—ã –≤–∏–¥–µ–ª–∏ –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞, —Ç–æ –ª—É—á—à–µ –∑–∞–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–∫.

–û–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–¥–∞–Ω—ã –¥–≤—É–º—è —Å–ª–æ–≤–∞—Ä—è–º–∏, `id2label` –∏ `label2id`, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –≤ –º–µ—Ç–∫—É –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏—Ö –≤ –º–µ—Ç–æ–¥ `TFAutoModelForTokenClassification.from_pretrained()`, –∏ –æ–Ω–∏ –±—É–¥—É—Ç –∑–∞–¥–∞–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, –∑–∞—Ç–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Hub:

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

–ö–∞–∫ –∏ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ `TFAutoModelForSequenceClassification` –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1), –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –≤—ã–¥–∞–µ—Ç—Å—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Å–∞ –Ω–µ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã (–≤–µ—Å–∞ –∏–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã), –∞ –¥—Ä—É–≥–∏–µ –≤–µ—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å–ª—É—á–∞–π–Ω–æ (–≤–µ—Å–∞ –∏–∑ –Ω–æ–≤–æ–π –≥–æ–ª–æ–≤—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤), –∏ —á—Ç–æ —ç—Ç—É –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å. –ú—ã —Å–¥–µ–ª–∞–µ–º —ç—Ç–æ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É, –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫:

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

‚ö†Ô∏è –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –º–æ–¥–µ–ª—å —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –º–µ—Ç–æ–∫, —Ç–æ –ø—Ä–∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –≤—ã–∑–æ–≤–µ `model.fit()` –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É. –≠—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å —Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ, –ø–æ—ç—Ç–æ–º—É –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —ç—Ç—É –ø—Ä–æ–≤–µ—Ä–∫—É, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫.

</Tip>

### –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏[[fine-tuning-the-model]]

–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏! –û–¥–Ω–∞–∫–æ —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã: –≤–æ–π—Ç–∏ –≤ Hugging Face –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –µ—Å—Ç—å —É–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –≤ —ç—Ç–æ–º:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–ü–æ—è–≤–∏—Ç—Å—è –≤–∏–¥–∂–µ—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Hugging Face.

–ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```bash
huggingface-cli login
```

–ü–æ—Å–ª–µ –≤—Ö–æ–¥–∞ –≤ –∞–∫–∫–∞—É–Ω—Ç –º—ã –º–æ–∂–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏. ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `create_optimizer()`, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—Å—Ç –≤–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä `AdamW` —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Å–æ–≤ –∏ –∑–∞—Ç—É—Ö–∞–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º `Adam`: 

```python
from transformers import create_optimizer
import tensorflow as tf

# –û–±—É—á–µ–Ω–∏–µ —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é float16
# –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GPU, –∫–æ—Ç–æ—Ä–æ–º—É —ç—Ç–æ –Ω–µ –ø—Ä–∏–Ω–µ—Å–µ—Ç –Ω–∏–∫–∞–∫–∏—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è - —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω–æ–µ –Ω–∞ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, –∑–∞—Ç–µ–º —É–º–Ω–æ–∂–µ–Ω–Ω–æ–µ
# –Ω–∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ tf_train_dataset –∑–¥–µ—Å—å - —ç—Ç–æ —Ä–∞–∑–±–∏—Ç–æ–µ –Ω–∞ –±–∞—Ç—á–∏ tf.data.Dataset,
# –∞ –Ω–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç Hugging Face, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ len() —É–∂–µ —Ä–∞–≤–µ–Ω num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç `loss` –≤ `compile()`. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã—á–∏—Å–ª—è—Ç—å –ø–æ—Ç–µ—Ä–∏ –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è - –µ—Å–ª–∏ –≤—ã –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç–µ –±–µ–∑ –ø–æ—Ç–µ—Ä—å –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç–µ —Å–≤–æ–∏ –º–µ—Ç–∫–∏ –≤–æ –≤—Ö–æ–¥–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ (–∫–∞–∫ –º—ã –¥–µ–ª–∞–µ–º –≤ –Ω–∞—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö), —Ç–æ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –∑–∞–¥–∞—á–µ –∏ —Ç–∏–ø—É –≤—ã–±—Ä–∞–Ω–Ω–æ–π –≤–∞–º–∏ –º–æ–¥–µ–ª–∏.

–î–∞–ª–µ–µ –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º `PushToHubCallback` –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –≤ Hub –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

–° –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `hub_model_id` –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω–æ–µ –∏–º—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—å (–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é). –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª—å –≤ [–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é `huggingface-course`](https://huggingface.co/huggingface-course), –º—ã –¥–æ–±–∞–≤–∏–ª–∏ `hub_model_id="huggingface-course/bert-finetuned-ner"`. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –≤–∞—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º–µ–Ω –∏ –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω–æ–π –≤–∞–º–∏ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä `"cool_huggingface_user/bert-finetuned-ner"`.

<Tip>

üí° –ï—Å–ª–∏ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ, —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–º –∫–ª–æ–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å push. –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–∞–∫, –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ—à–∏–±–∫—É –ø—Ä–∏ –≤—ã–∑–æ–≤–µ `model.fit()` –∏ –¥–æ–ª–∂–Ω—ã –±—É–¥–µ—Ç–µ –∑–∞–¥–∞—Ç—å –Ω–æ–≤–æ–µ –∏–º—è.

</Tip>

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è (–∑–¥–µ—Å—å - –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É), –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ —Ö–∞–± –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—ã —Å–º–æ–∂–µ—Ç–µ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–æ–π –º–∞—à–∏–Ω–µ.

–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ Model Hub, —á—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å –∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –µ—é —Å –¥—Ä—É–∑—å—è–º–∏. –í—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ - –ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –ù–æ –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∞ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ? –ß—Ç–æ–±—ã –≤—ã—è—Å–Ω–∏—Ç—å —ç—Ç–æ, –Ω–∞–º —Å–ª–µ–¥—É–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏.

{/if}


### –ú–µ—Ç—Ä–∏–∫–∏[[metrics]]

{#if fw === 'pt'}

–ß—Ç–æ–±—ã `Trainer` –≤—ã—á–∏—Å–ª—è–ª –º–µ—Ç—Ä–∏–∫—É –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É, –Ω–∞–º –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `compute_metrics()`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –º–∞—Å—Å–∏–≤—ã –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ –º–µ—Ç–æ–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∏–º–µ–Ω–∞–º–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –º–µ—Ç—Ä–∏–∫. 

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ [*seqeval*](https://github.com/chakki-works/seqeval). –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–µ—Ç—Ä–∏–∫—É, —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É *seqeval*:

```py
!pip install seqeval
```

–ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ–µ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `evaluate.load()`, –∫–∞–∫ –º—ã —ç—Ç–æ –¥–µ–ª–∞–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1):

{:else}

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ [*seqeval*](https://github.com/chakki-works/seqeval). –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–µ—Ç—Ä–∏–∫—É, —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É *seqeval*:

```py
!pip install seqeval
```

–ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ–µ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `evaluate.load()`, –∫–∞–∫ –º—ã —ç—Ç–æ –¥–µ–ª–∞–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1):

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

–≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤–µ–¥–µ—Ç —Å–µ–±—è –Ω–µ —Ç–∞–∫, –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è accuracy: –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –æ–Ω–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–∫–∏ –º–µ—Ç–æ–∫ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏, –∞ –Ω–µ –∫–∞–∫ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –∏ –º–µ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –∏—Ö –≤ –º–µ—Ç—Ä–∏–∫—É. –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –°–Ω–∞—á–∞–ª–∞ –º—ã –ø–æ–ª—É—á–∏–º –º–µ—Ç–∫–∏ –¥–ª—è –Ω–∞—à–µ–≥–æ –ø–µ—Ä–≤–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–ª—å—à–∏–≤—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è –Ω–∏—Ö, –ø—Ä–æ—Å—Ç–æ –∏–∑–º–µ–Ω–∏–≤ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å–µ 2:

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (–Ω–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω) –∏ —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫. –í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

–û–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏! –ú—ã –ø–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ precision, recall –∏ F1 –¥–ª—è –∫–∞–∂–¥–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –≤ —Ü–µ–ª–æ–º. –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –º—ã —Å–æ—Ö—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É, –Ω–æ –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `compute_metrics()` —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–ª–∞ –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å.

–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è `compute_metrics()` —Å–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ—Ç argmax –ª–æ–≥–∏—Ç–æ–≤, —á—Ç–æ–±—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ–≥–Ω–æ–∑—ã (–∫–∞–∫ –æ–±—ã—á–Ω–æ, –ª–æ–≥–∏—Ç—ã –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å softmax). –ó–∞—Ç–µ–º –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –º–µ—Ç–∫–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏–∑ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ —Å—Ç—Ä–æ–∫–∏. –ú—ã —É–¥–∞–ª—è–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –º–µ—Ç–∫–∞ —Ä–∞–≤–Ω–∞ `-100`, –∞ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –º–µ—Ç–æ–¥ `metric.compute()`:

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # –£–¥–∞–ª—è–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–π –∏–Ω–¥–µ–∫—Å (—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–µ—Ç–∫–∏
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —ç—Ç–æ —Å–¥–µ–ª–∞–Ω–æ, –º—ã –ø–æ—á—Ç–∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –Ω–∞—à–µ–≥–æ `Trainer`. –ù–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–∞ `model`, —á—Ç–æ–±—ã –¥–æ–æ–±—É—á–∏—Ç—å –µ–µ!

{:else}

–û–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏! –ú—ã –ø–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ precision, recall –∏ F1 –¥–ª—è –∫–∞–∂–¥–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –≤ —Ü–µ–ª–æ–º. –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ –º—ã –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫.

TensorFlow –Ω–µ –ª—é–±–∏—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à–∏ –ø—Ä–æ–≥–Ω–æ–∑—ã, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ –∏–º–µ—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `model.predict()` - –Ω–æ —ç—Ç–æ –Ω–∞—Å –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç. –ú—ã –±—É–¥–µ–º –ø–æ–ª—É—á–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ –±–∞—Ç—á—É –∑–∞ —Ä–∞–∑ –∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –¥–ª–∏–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è, –æ—Ç–±—Ä–∞—Å—ã–≤–∞—è —Ç–æ–∫–µ–Ω—ã `-100`, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ/–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ, –∞ –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ –≤ –∫–æ–Ω—Ü–µ:

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict_on_batch(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

–ö–∞–∫ –≤–∞—à–∞ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ —Å–µ–±—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–∞—à–µ–π? –ï—Å–ª–∏ –≤—ã –ø–æ–ª—É—á–∏–ª–∏ –ø–æ—Ö–æ–∂–∏–µ —Ü–∏—Ñ—Ä—ã, –∑–Ω–∞—á–∏—Ç, –≤–∞—à–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ!

{/if}

{#if fw === 'pt'}

### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏[[defining-the-model]]

–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –ø—Ä–æ–±–ª–µ–º–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å `AutoModelForTokenClassification`. –ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏, - —ç—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏–º–µ—é—â–∏—Ö—Å—è —É –Ω–∞—Å –º–µ—Ç–æ–∫. –ü—Ä–æ—â–µ –≤—Å–µ–≥–æ –ø–µ—Ä–µ–¥–∞—Ç—å —ç—Ç–æ —á–∏—Å–ª–æ —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `num_labels`, –Ω–æ –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Å–∏–≤—ã–π –≤–∏–¥–∂–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –ø–æ–¥–æ–±–Ω—ã–π —Ç–æ–º—É, —á—Ç–æ –º—ã –≤–∏–¥–µ–ª–∏ –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞, —Ç–æ –ª—É—á—à–µ –∑–∞–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫.

–û–Ω–æ –¥–æ–ª–∂–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å—Å—è –¥–≤—É–º—è —Å–ª–æ–≤–∞—Ä—è–º–∏, `id2label` –∏ `label2id`, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –∏ –º–µ—Ç–∫–æ–π –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏—Ö –≤ –º–µ—Ç–æ–¥ `AutoModelForTokenClassification.from_pretrained()`, –∏ –æ–Ω–∏ –±—É–¥—É—Ç –∑–∞–¥–∞–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Hub:

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è `AutoModelForSequenceClassification` –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1), –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –≤—ã–¥–∞–µ—Ç—Å—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Å–∞ –Ω–µ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã (—Ç–µ, —á—Ç–æ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã), –∞ –¥—Ä—É–≥–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å–ª—É—á–∞–π–Ω–æ (—Ç–µ, —á—Ç–æ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –Ω–æ–≤–æ–π –≥–æ–ª–æ–≤—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤), –∏ —á—Ç–æ —ç—Ç—É –º–æ–¥–µ–ª—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±—É—á–∏—Ç—å. –ú—ã —Å–¥–µ–ª–∞–µ–º —ç—Ç–æ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É, –Ω–æ —Å–Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫:

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

‚ö†Ô∏è –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –º–æ–¥–µ–ª—å —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –º–µ—Ç–æ–∫, —Ç–æ –ø—Ä–∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º –≤—ã–∑–æ–≤–µ –º–µ—Ç–æ–¥–∞ `Trainer.train()` –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É (—á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ "CUDA error: device-side assert triggered"). –≠—Ç–æ –≥–ª–∞–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ –æ—à–∏–±–æ–∫, –æ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–æ–±—â–∞—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —ç—Ç—É –ø—Ä–æ–≤–µ—Ä–∫—É, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫.

</Tip>

### –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏[[fine-tuning-the-model]]

–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏! –ù–∞–º –æ—Å—Ç–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å –¥–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–µ—â–∏, –ø—Ä–µ–∂–¥–µ —á–µ–º –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º –Ω–∞—à `Trainer`: –≤–æ–π—Ç–∏ –≤ Hugging Face –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—à–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –µ—Å—Ç—å —É–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –≤ —ç—Ç–æ–º:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–ü–æ—è–≤–∏—Ç—Å—è –≤–∏–¥–∂–µ—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Hugging Face.

–ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–µ –≤ –Ω–æ—É—Ç–±—É–∫–µ, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```bash
huggingface-cli login
```

–ö–∞–∫ —Ç–æ–ª—å–∫–æ —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ, –º—ã —Å–º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—à–∏ `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∏–∑ –Ω–∏—Ö –≤—ã —É–∂–µ –≤–∏–¥–µ–ª–∏: –º—ã –∑–∞–¥–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–æ–≤) –∏ —É–∫–∞–∑—ã–≤–∞–µ–º `push_to_hub=True`, —á—Ç–æ–±—ã —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –æ—Ü–µ–Ω–∏—Ç—å –µ–µ –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏, –∞ —Ç–∞–∫–∂–µ —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Model Hub. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `hub_model_id` –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –∏–º—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—å (–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é). –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –ø–µ—Ä–µ–¥–∞–≤–∞–ª–∏ –º–æ–¥–µ–ª—å –≤ [–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é `huggingface-course`](https://huggingface.co/huggingface-course), –º—ã –¥–æ–±–∞–≤–∏–ª–∏ `hub_model_id="huggingface-course/bert-finetuned-ner"` –≤ `TrainingArguments`. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –≤–∞—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º–µ–Ω –∏ –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –≤–∞–º–∏ –≤—ã—Ö–æ–¥–Ω—ã–º –∫–∞—Ç–∞–ª–æ–≥–æ–º, —Ç–∞–∫ —á—Ç–æ –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –±—É–¥–µ—Ç `"sgugger/bert-finetuned-ner"`.

<Tip>

üí° –ï—Å–ª–∏ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ, —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–º –∫–ª–æ–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—å. –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–∞–∫, –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ—à–∏–±–∫—É –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≤–∞—à–µ–≥–æ `Trainer` –∏ –¥–æ–ª–∂–Ω—ã –±—É–¥–µ—Ç–µ –∑–∞–¥–∞—Ç—å –Ω–æ–≤–æ–µ –∏–º—è.

</Tip>

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º –≤—Å–µ –≤ `Trainer` –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è (–∑–¥–µ—Å—å - –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É), –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ Hub –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—ã —Å–º–æ–∂–µ—Ç–µ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–æ–π –º–∞—à–∏–Ω–µ.

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ `push_to_hub()`, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–∞–º–∞—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏:

```py
trainer.push_to_hub(commit_message="Training complete")
```

–≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL —Ç–æ–ª—å–∫–æ —á—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ commit, –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –µ–≥–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å:

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

 `Trainer` —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ—Ç —á–µ—Ä–Ω–æ–≤–∏–∫ –∫–∞—Ä—Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–∏ —Å–æ –≤—Å–µ–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ–≥–æ. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ Model Hub, —á—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å –∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –µ—é —Å –¥—Ä—É–∑—å—è–º–∏. –í—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ - –ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!

 –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è, –º—ã –ø–æ–∫–∞–∂–µ–º –≤–∞–º, –∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–æ –∂–µ —Å–∞–º–æ–µ —Å –ø–æ–º–æ—â—å—é ü§ó Accelerate.

##  –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è[[a-custom-training-loop]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤—ã –º–æ–≥–ª–∏ –ª–µ–≥–∫–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω—É–∂–Ω—ã–µ –≤–∞–º —á–∞—Å—Ç–∏. –û–Ω –±—É–¥–µ—Ç –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ —Ç–æ—Ç, —á—Ç–æ –º—ã –¥–µ–ª–∞–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/4), —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏.

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ–≥–æ –∫ –æ–±—É—á–µ–Ω–∏—é[[preparing-everything-for-training]]

–°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å `DataLoader` –¥–ª—è –Ω–∞—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à `data_collator` –≤ –∫–∞—á–µ—Å—Ç–≤–µ `collate_fn` –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä, –Ω–æ –Ω–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

–ó–∞—Ç–µ–º –º—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–æ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å, –∞ —Å–Ω–æ–≤–∞ –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT:

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

–¢–æ–≥–¥–∞ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π `AdamW`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Ö–æ–∂ –Ω–∞ `Adam`, –Ω–æ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ –≤ —Å–ø–æ—Å–æ–±–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Å–æ–≤:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

–ö–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ —ç—Ç–∏ –æ–±—ä–µ–∫—Ç—ã, –º—ã –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∏—Ö –≤ –º–µ—Ç–æ–¥ `accelerator.prepare()`:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® –ï—Å–ª–∏ –≤—ã –æ–±—É—á–∞–µ—Ç–µ—Å—å –Ω–∞ TPU, –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤–µ—Å—å –∫–æ–¥, –Ω–∞—á–∏–Ω–∞—è —Å —è—á–µ–π–∫–∏ –≤—ã—à–µ, –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è. –ü–æ–¥—Ä–æ–±–Ω–µ–µ —Å–º–æ—Ç—Ä–∏—Ç–µ [–ì–ª–∞–≤—É 3](../chapter3/1).

</Tip>

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –Ω–∞—à `train_dataloader` –≤ `accelerator.prepare()`, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª–∏–Ω—É –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–º–µ–Ω–∏—Ç –µ–≥–æ –¥–ª–∏–Ω—É. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ª–∏–Ω–µ–π–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—à–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–æ 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

–ù–∞–∫–æ–Ω–µ—Ü, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤ Hub, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `Repository` –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–ø–∫–µ. –°–Ω–∞—á–∞–ª–∞ –∞–≤—Ç–æ—Ä–∏–∑—É–π—Ç–µ—Å—å –≤ Hugging Face, –µ—Å–ª–∏ –≤—ã –µ—â–µ –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã. –ú—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º –∏–º—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏—Å–≤–æ–∏—Ç—å –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–Ω–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –∑–∞–º–µ–Ω–∏—Ç—å `repo_name` –Ω–∞ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä; –æ–Ω –ø—Ä–æ—Å—Ç–æ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–∞—à–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ –∏ –¥–µ–ª–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∫–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É. –ï—Å–ª–∏ –æ–Ω–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —ç—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–æ–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã —Ä–∞–±–æ—Ç–∞–µ–º:

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤ `output_dir`, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `repo.push_to_hub()`. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏.

### –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è[[training-loop]]

–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –Ω–∞–ø–∏—Å–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è. –ß—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å –µ–≥–æ –æ—Ü–µ–Ω–æ—á–Ω—É—é —á–∞—Å—Ç—å, –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é `postprocess()`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ —Å–ø–∏—Å–∫–∏ —Å—Ç—Ä–æ–∫, –∫–∞–∫ —Ç–æ–≥–æ –æ–∂–∏–¥–∞–µ—Ç –Ω–∞—à –æ–±—ä–µ–∫—Ç `metric`:

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # –£–¥–∞–ª—è–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–π –∏–Ω–¥–µ–∫—Å (—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–µ—Ç–∫–∏
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è. –ü–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞, —á—Ç–æ–±—ã —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Ö–æ–¥–æ–º –æ–±—É—á–µ–Ω–∏—è, —Ü–∏–∫–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö —á–∞—Å—Ç–µ–π:

- –°–∞–º–æ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é –ø–æ `train_dataloader`, –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –º–æ–¥–µ–ª–∏, –∑–∞—Ç–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞.
- –û—Ü–µ–Ω–∫–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –µ—Å—Ç—å –Ω–æ–≤—à–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—ã—Ö–æ–¥–æ–≤ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –±–∞—Ç—á–µ: –ø–æ—Å–∫–æ–ª—å–∫—É –¥–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º–æ–≥–ª–∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –≤—Ö–æ–¥—ã –∏ –º–µ—Ç–∫–∏ –¥–æ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º, –Ω–∞–º –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `accelerator.pad_across_processes()`, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ñ–æ—Ä–º—ã –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º –º–µ—Ç–æ–¥–∞ `gather()`. –ï—Å–ª–∏ –º—ã —ç—Ç–æ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–µ–º, –æ—Ü–µ–Ω–∫–∞ –ª–∏–±–æ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è —Å –æ—à–∏–±–∫–æ–π, –ª–∏–±–æ –∑–∞–≤–∏—Å–Ω–µ—Ç –Ω–∞–≤—Å–µ–≥–¥–∞. –ó–∞—Ç–µ–º –º—ã –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ `metric.add_batch()` –∏ –≤—ã–∑—ã–≤–∞–µ–º `metric.compute()` –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ü–∏–∫–ª–∞ –æ—Ü–µ–Ω–∫–∏.
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞, –≥–¥–µ –º—ã —Å–Ω–∞—á–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∞ –∑–∞—Ç–µ–º –≤—ã–∑—ã–≤–∞–µ–º `repo.push_to_hub()`. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç `blocking=False`, —á—Ç–æ–±—ã —É–∫–∞–∑–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Hub –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ push –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –∞ —ç—Ç–∞ (–¥–ª–∏–Ω–Ω–∞—è) –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.

–í–æ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–¥ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # –û–±—É—á–µ–Ω–∏–µ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # –û—Ü–µ–Ω–∫–∞
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏ –¥–ª—è gather
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

–ï—Å–ª–∏ –≤—ã –≤–ø–µ—Ä–≤—ã–µ –≤–∏–¥–∏—Ç–µ –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é ü§ó Accelerate, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –∏–¥—É—Ç –≤–º–µ—Å—Ç–µ —Å –Ω–∏–º:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ—è—Å–Ω–µ–Ω–∏–π: –æ–Ω–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –≤—Å–µ–º –ø—Ä–æ—Ü–µ—Å—Å–∞–º –ø–æ–¥–æ–∂–¥–∞—Ç—å, –ø–æ–∫–∞ –≤—Å–µ –Ω–µ –æ–∫–∞–∂—É—Ç—Å—è –Ω–∞ —ç—Ç–æ–π —Å—Ç–∞–¥–∏–∏, –ø—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Ä–∞–±–æ—Ç—É. –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —É –Ω–∞—Å –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –º–æ–¥–µ–ª—å –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º. –ó–∞—Ç–µ–º –º—ã –±–µ—Ä–µ–º `unwrapped_model`, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏. –ú–µ—Ç–æ–¥ `accelerator.prepare()` –∏–∑–º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –ø–æ—ç—Ç–æ–º—É —É –Ω–µ–µ –±–æ–ª—å—à–µ –Ω–µ –±—É–¥–µ—Ç –º–µ—Ç–æ–¥–∞ `save_pretrained()`; –º–µ—Ç–æ–¥ `accelerator.unwrap_model()` –æ—Ç–º–µ–Ω—è–µ—Ç —ç—Ç–æ—Ç —à–∞–≥. –ù–∞–∫–æ–Ω–µ—Ü, –º—ã –≤—ã–∑—ã–≤–∞–µ–º `save_pretrained()`, –Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —ç—Ç–æ–º—É –º–µ—Ç–æ–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `accelerator.save()` –≤–º–µ—Å—Ç–æ `torch.save()`. 

–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ, —É –≤–∞—Å –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—å—Å—è –º–æ–¥–µ–ª—å, –≤—ã–¥–∞—é—â–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Ç–µ, —á—Ç–æ –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é `Trainer`. –í—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–±—É—á–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –∫–æ–¥–∞, –Ω–∞ [*huggingface-course/bert-finetuned-ner-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate). –ê –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è, –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞–ø—Ä—è–º—É—é —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏—Ö, –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–≤ –∫–æ–¥, –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–π –≤—ã—à–µ!

{/if}

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏[[using-the-fine-tuned-model]]

–ú—ã —É–∂–µ –ø–æ–∫–∞–∑–∞–ª–∏ –≤–∞–º, –∫–∞–∫ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –¥–æ–æ–±—É—á–∏–ª–∏ –Ω–∞ Model Hub, —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–∂–µ—Ç–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –ª–æ–∫–∞–ª—å–Ω–æ –≤ `pipeline`, –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ —É–∫–∞–∑–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏:

```py
from transformers import pipeline

# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç–æ –Ω–∞ —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

–û—Ç–ª–∏—á–Ω–æ! –ù–∞—à–∞ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫ –∂–µ —Ö–æ—Ä–æ—à–æ, –∫–∞–∫ –∏ –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞!
