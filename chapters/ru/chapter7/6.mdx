<FrameworkSwitchCourse {fw} />

# –û–±—É—á–µ–Ω–∏–µ –∫–∞—É–∑–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è[[training-a-causal-language-model-from-scratch]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_tf.ipynb"},
]} />

{/if}

–î–æ —Å–∏—Ö –ø–æ—Ä –º—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–ª–∏ –∏—Ö –¥–æ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ—Å–∞, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤ —Ö–æ–¥–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 1](../chapter1/1), —ç—Ç–æ –ø—Ä–∏–Ω—è—Ç–æ –Ω–∞–∑—ã–≤–∞—Ç—å _—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º_, –∏ —ç—Ç–æ –æ—á–µ–Ω—å —É—Å–ø–µ—à–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Transformer –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ. –í —ç—Ç–æ–π –≥–ª–∞–≤–µ –º—ã –ø—Ä–∏–º–µ–Ω–∏–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥ –∏ –æ–±—É—á–∏–º —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è. –≠—Ç–æ —Ö–æ—Ä–æ—à–∏–π –ø–æ–¥—Ö–æ–¥, –µ—Å–ª–∏ —É –≤–∞—Å –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, –∏ –æ–Ω–∏ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –∏–º–µ—é—â–∏—Ö—Å—è –º–æ–¥–µ–ª–µ–π. –û–¥–Ω–∞–∫–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —á–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π. –ü—Ä–∏–º–µ—Ä—ã, –∫–æ–≥–¥–∞ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –æ–±—É—á–∞—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å, –≤–∫–ª—é—á–∞—é—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –Ω–æ—Ç, –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –î–ù–ö, –∏–ª–∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤ –Ω–µ–¥–∞–≤–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–ª—É—á–∏–ª–∏ —à–∏—Ä–æ–∫–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–∏–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º, –∫–∞–∫ TabNine –∏ GitHub's Copilot, —Ä–∞–±–æ—Ç–∞—é—â–∏–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Codex –æ—Ç OpenAI, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –∏–ª–∏ –∫–∞—É–∑–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-2.

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø–æ—Å—Ç—Ä–æ–∏–º —É–º–µ–Ω—å—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞: –º—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è—Ö –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏–ª–∏ –∫–ª–∞—Å—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–æ–¥–∞ Python. –†–∞–±–æ—Ç–∞—è —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ Python, –≤—ã —á–∞—Å—Ç–æ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç–µ—Å—å —Å–æ —Å—Ç–µ–∫–æ–º Data Science –Ω–∞ Python, —Å–æ—Å—Ç–æ—è—â–µ–º –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫ `matplotlib`, `seaborn`, `pandas` –∏ `scikit-learn`. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —ç—Ç–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ —á–∞—Å—Ç–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–∞–Ω–¥, –ø–æ—ç—Ç–æ–º—É –±—ã–ª–æ –±—ã –Ω–µ–ø–ª–æ—Ö–æ, –µ—Å–ª–∏ –±—ã –º—ã –º–æ–≥–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤—ã–ø–æ–ª–Ω–µ–Ω—è—é—â—É—é —ç—Ç–∏ –≤—ã–∑–æ–≤–æ–≤—ã –∑–∞ –Ω–∞—Å.

<Youtube id="Vpjb1lu0MDk"/>

–í [–ì–ª–∞–≤–µ 6](../chapter6/1) –º—ã —Å–æ–∑–¥–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ Python, –Ω–æ –Ω–∞–º –≤—Å–µ –µ—â–µ –Ω—É–∂–µ–Ω –∫—Ä—É–ø–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ó–¥–µ—Å—å –º—ã –ø—Ä–∏–º–µ–Ω–∏–º –Ω–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫ –∫–æ—Ä–ø—É—Å—É –∫–æ–¥–∞ Python, –ø–æ–ª—É—á–µ–Ω–Ω–æ–º—É –∏–∑ —Ä–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ GitHub. –ó–∞—Ç–µ–º –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è API `Trainer` –∏ ü§ó Accelerate –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ü—Ä–∏—Å—Ç—É–ø–∏–º!

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

–ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —ç—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ Hub —Å –ø–æ–º–æ—â—å—é –∫–æ–¥–∞, –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ. –í—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –µ–µ [–∑–¥–µ—Å—å](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–µ–∫–æ—Ç–æ—Ä–∞—è —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è, –≤—ã, –≤–µ—Ä–æ—è—Ç–Ω–æ, –ø–æ–ª—É—á–∏—Ç–µ –Ω–µ–º–Ω–æ–≥–æ –¥—Ä—É–≥–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
 
## –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö[[gathering-the-data]]

–ö–æ–¥ –Ω–∞ Python –≤ –∏–∑–æ–±–∏–ª–∏–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–∞–∫–∏—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –∫–æ–¥–∞, –∫–∞–∫ GitHub, –∏ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—É—Ç–µ–º –ø–æ–∏—Å–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ä–æ–∑–∏—Ç–æ—Ä–∏—è Python. –ò–º–µ–Ω–Ω–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ –∫–Ω–∏–≥–µ [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ GPT-2. –ò—Å–ø–æ–ª—å–∑—É—è –¥–∞–º–ø GitHub –æ–±—ä–µ–º–æ–º –æ–∫–æ–ª–æ 180 –ì–ë, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø—Ä–∏–º–µ—Ä–Ω–æ 20 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ñ–∞–π–ª–æ–≤ Python –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º `codeparrot`, –∞–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–º –∑–∞—Ç–µ–º –ø–æ–¥–µ–ª–∏–ª–∏—Å—å –Ω–∞ [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot).

–û–¥–Ω–∞–∫–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–ª–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ —Ç—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∞ –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç–µ–∫–æ–º data science –Ω–∞ Python. –ò—Ç–∞–∫, –¥–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ `codeparrot` –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º, –≤–∫–ª—é—á–∞—é—â–∏–º –ª—é–±—É—é –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏–∑ —ç—Ç–æ–≥–æ —Å—Ç–µ–∫–∞. –ò–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –º—ã —Ö–æ—Ç–∏–º –∏–∑–±–µ–∂–∞—Ç—å –µ–≥–æ –∑–∞–≥—Ä—É–∑–∫–∏; –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ (streaming), —á—Ç–æ–±—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –µ–≥–æ –Ω–∞ –ª–µ—Ç—É. –ß—Ç–æ–±—ã –ø–æ–º–æ—á—å –Ω–∞–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫, –æ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ —Ä–∞–Ω–µ–µ, –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–ª–µ–¥—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–µ–π:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –µ–µ –Ω–∞ –¥–≤—É—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

–ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—Ç—å –Ω—É–∂–Ω—ã–µ –Ω–∞–º —ç–ª–µ–º–µ–Ω—Ç—ã:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –∫ –ø–æ—Ç–æ–∫–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É:

```py
# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–æ–π —è—á–µ–π–∫–∏ –∑–∞–π–º–µ—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –ø–æ—ç—Ç–æ–º—É –µ–µ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏ –ø–µ—Ä–µ–π—Ç–∏ –∫
# —Å–ª–µ–¥—É—é—â–µ–π!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —É –Ω–∞—Å –æ—Å—Ç–∞–µ—Ç—Å—è –æ–∫–æ–ª–æ 3% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, —á—Ç–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ - —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–Ω–∏–º–∞–µ—Ç 6 –ì–ë –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 600 000 Python-—Å–∫—Ä–∏–ø—Ç–æ–≤!

–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 2-3 —á–∞—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–µ–π –º–∞—à–∏–Ω—ã –∏ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ç–∏. –ï—Å–ª–∏ –≤—ã –Ω–µ —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç–æ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, –º—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ Hub –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–π–º–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è. –ú—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–≤ –¥–≤–µ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤—ã—à–µ, –∏ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã. –ù–µ—Ç –Ω–∏—á–µ–≥–æ –æ–±–∏–¥–Ω–µ–µ, —á–µ–º –Ω–µ—É–¥–∞—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º —ç—Ç–∞–ø–µ –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã –∑–∞–±—ã–ª–∏ —Å–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É –∏–ª–∏ –∏–∑-–∑–∞ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ –∫–æ–Ω—Ü–µ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è!

</Tip>

–î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞. –ú—ã –ø–æ–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –ø–æ–ª–µ `content` —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–¥, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –º—ã —Ö–æ—Ç–∏–º –æ–±—É—á–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å. –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç–µ–∫—Å—Ç—ã, —á—Ç–æ–±—ã –æ–Ω–∏ –±—ã–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ–¥—Ö–æ–¥—è—â–µ–º –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞[[preparing-the-dataset]]

<Youtube id="ma1TrR7gE7I"/>

–ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É –Ω–∞—à–∞ —Ü–µ–ª—å - –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π, –º—ã –º–æ–∂–µ–º –æ—Å—Ç–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–∏–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É –º—ã —Å–º–æ–∂–µ–º –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –≥–æ—Ä–∞–∑–¥–æ –±—ã—Å—Ç—Ä–µ–µ, –∏ –æ–Ω–∞ –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏. –ï—Å–ª–∏ –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—ã–ª –±–æ–ª—å—à–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø–∏—Å–∞–ª–∞ —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–π–ª–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏), –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á—å—Ç–µ —ç—Ç–æ —á–∏—Å–ª–æ, –Ω–æ –Ω–µ –∑–∞–±—ã–≤–∞–π—Ç–µ, —á—Ç–æ —ç—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏ GPU. –ü–æ–∫–∞ —á—Ç–æ –¥–∞–≤–∞–π—Ç–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ 128 —Ç–æ–∫–µ–Ω–æ–≤, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç 1 024 –∏–ª–∏ 2 048, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ GPT-2 –∏–ª–∏ GPT-3 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.

–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ 128 —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —Ç–æ–º—É, —á—Ç–æ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–∞. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä `return_overflowing_tokens` –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Å–µ–≥–æ –≤–≤–æ–¥–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ –≤ [–ì–ª–∞–≤–µ 6](../chapter6/4). –ú—ã —Ç–∞–∫–∂–µ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä `return_length`, —á—Ç–æ–±—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞. –ß–∞—Å—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –±—É–¥–µ—Ç –º–µ–Ω—å—à–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏ –º—ã –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º; –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –æ–Ω–∏ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω—ã, –ø–æ—Å–∫–æ–ª—å–∫—É —É –Ω–∞—Å –∏ —Ç–∞–∫ –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–∞ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

–ò–∑ —ç—Ç–∏—Ö –¥–≤—É—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–∏–¥–Ω–æ, —á—Ç–æ –≤ –æ–±—â–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º—ã –ø–æ–ª—É—á–∏–ª–∏ 34 —Å–µ–≥–º–µ–Ω—Ç–∞. –í–∑–≥–ª—è–Ω—É–≤ –Ω–∞ –¥–ª–∏–Ω—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –º—ã –≤–∏–¥–∏–º, —á—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω—Ü–µ –æ–±–æ–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–µ–Ω–µ–µ 128 —Ç–æ–∫–µ–Ω–æ–≤ (117 –∏ 41, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ). –≠—Ç–æ –ª–∏—à—å –º–∞–ª–∞—è —á–∞—Å—Ç—å –≤—Å–µ—Ö –∏–º–µ—é—â–∏—Ö—Å—è —É –Ω–∞—Å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É –º—ã –º–æ–∂–µ–º —Å–º–µ–ª–æ –æ—Ç–±—Ä–æ—Å–∏—Ç—å –∏—Ö. –° –ø–æ–º–æ—â—å—é –ø–æ–ª—è `overflow_to_sample_mapping` –º—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å, –∫–∞–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞–ª–∏ –∫–∞–∫–∏–º –≤—Ö–æ–¥–Ω—ã–º –ø—Ä–∏–º–µ—Ä–∞–º.

–í —ç—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–¥–æ–±–Ω—É—é –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ `Dataset.map()` –∏–∑ ü§ó Datasets, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–¥–∏–Ω –∫ –æ–¥–Ω–æ–º—É; –∫–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [—Ä–∞–∑–¥–µ–ª–µ 3](../chapter7/3), –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–∞—Ç—á —Å –±–æ–ª—å—à–∏–º –∏–ª–∏ –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —á–µ–º –≤—Ö–æ–¥–Ω–æ–π –±–∞—Ç—á. –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–∞–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π, –∫–∞–∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–º–µ–Ω—è—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º—ã —Å–æ–∑–¥–∞–µ–º –º–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ù–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã —É–¥–∞–ª–∏–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ –∏–º–µ—é—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–π —Ä–∞–∑–º–µ—Ä. –ï—Å–ª–∏ –±—ã –º—ã —Ö–æ—Ç–µ–ª–∏ –∏—Ö —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å, —Ç–æ –º–æ–≥–ª–∏ –±—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º –∏ –≤–µ—Ä–Ω—É—Ç—å –≤ —Ä–∞–º–∫–∞—Ö –≤—ã–∑–æ–≤–∞ `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å 16,7 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–æ 128 —Ç–æ–∫–µ–Ω–∞–º–∏ –≤ –∫–∞–∂–¥–æ–º, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 2,1 –º–∏–ª–ª–∏–∞—Ä–¥–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–±—â–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –º–æ–¥–µ–ª–∏ OpenAI GPT-3 –∏ Codex –æ–±—É—á–µ–Ω—ã –Ω–∞ 300 –∏ 100 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –ø—Ä–∏—á–µ–º –º–æ–¥–µ–ª–∏ Codex –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ GPT-3. –ù–∞—à–∞ —Ü–µ–ª—å –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ - –Ω–µ –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —ç—Ç–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ, —Å–≤—è–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, –∞ —Å–æ–∑–¥–∞—Ç—å —É–º–µ–Ω—å—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â—É—é –±—ã—Å—Ç—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö.

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –≥–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç, –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –º–æ–¥–µ–ª—å!

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò–∑–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–º–µ—Ä –∫–æ—Ç–æ—Ä—ã—Ö –º–µ–Ω—å—à–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –±–æ–ª—å—à–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ–∫–Ω–∞. –ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∏–ª–∏ –µ—Å–ª–∏ —É –≤–∞—Å –∫–æ—Ä–ø—É—Å –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) –¥–æ–ª—è –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–∞–∫–∂–µ –±—É–¥–µ—Ç —Ä–∞—Å—Ç–∏. –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö - –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤ –±–∞—Ç—á —Å –º–∞—Ä–∫–µ—Ä–æ–º `eos_token_id` –º–µ–∂–¥—É –Ω–∏–º–∏, –∞ –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö. –í –∫–∞—á–µ—Å—Ç–≤–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é `tokenize()`, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å `truncation=False` –∏ —É–¥–∞–ª–∏—Ç—å –¥—Ä—É–≥–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.

</Tip>


## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏[[initializing-a-new-model]]

–ù–∞—à –ø–µ—Ä–≤—ã–π —à–∞–≥ - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–≤–µ–∂–µ–π –º–æ–¥–µ–ª–∏ GPT-2. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—É –∂–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –∏ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª–∏ GPT-2, –ø–æ—ç—Ç–æ–º—É –∑–∞–≥—Ä—É–∑–∏–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–º–µ—Ä—É —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞ –º–æ–¥–µ–ª–∏, –∏ –ø–µ—Ä–µ–¥–∞–¥–∏–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤ `bos` –∏ `eos` (–Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

–° —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–ø–µ—Ä–≤—ã–µ –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é `from_pretrained()`, –ø–æ—Å–∫–æ–ª—å–∫—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

–° —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–ø–µ—Ä–≤—ã–µ –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é `from_pretrained()`, –ø–æ—Å–∫–æ–ª—å–∫—É —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

–ù–∞—à–∞ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç 124 –º–∏–ª–ª–∏–æ–Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –¥–æ–æ–±—É—á–∏—Ç—å. –ü—Ä–µ–∂–¥–µ —á–µ–º –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –Ω–∞–º –Ω—É–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–π–º–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º –±–∞—Ç—á–µ–π. –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–ª–ª–∞—Ç–æ—Ä `DataCollatorForLanguageModeling`, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–æ —á–µ–º –Ω–µ–¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ). –ü–æ–º–∏–º–æ —Å—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–µ–π, –æ–Ω —Ç–∞–∫–∂–µ –∑–∞–±–æ—Ç–∏—Ç—Å—è –æ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–µ—Ç–æ–∫ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ - –≤ –∫–∞—É–∑–∞–ª—å–Ω–æ–º —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ö–æ–¥—ã —Ç–æ–∂–µ —Å–ª—É–∂–∞—Ç –º–µ—Ç–∫–∞–º–∏ (–ø—Ä–æ—Å—Ç–æ —Å–¥–≤–∏–Ω—É—Ç—ã–º–∏ –Ω–∞ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç), –∏ —ç—Ç–æ—Ç –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–µ—Ç –∏—Ö –Ω–∞ –ª–µ—Ç—É –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ —á—Ç–æ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å `input_ids`.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ `DataCollatorForLanguageModeling` –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (masked language modeling - MLM), —Ç–∞–∫ –∏ –∫–∞—É–∑–∞–ª—å–Ω–æ–µ —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (causal language modeling - CLM). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–Ω –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è MLM, –Ω–æ –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ CLM, –∑–∞–¥–∞–≤ –∞—Ä–≥—É–º–µ–Ω—Ç `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –ø—Ä–∏–º–µ—Ä—ã –±—ã–ª–∏ —Å—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω—ã, –∏ –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Ñ–æ—Ä–º—É.

{#if fw === 'tf'}

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `prepare_tf_dataset()` –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–∞—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç—ã TensorFlow —Å –ø–æ–º–æ—â—å—é –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Å–æ–∑–¥–∞–ª–∏ –≤—ã—à–µ:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_dataset["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

‚ö†Ô∏è –°–¥–≤–∏–≥ –≤—Ö–æ–¥–æ–≤ –∏ –º–µ—Ç–æ–∫ –¥–ª—è –∏—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç –≤—Ö–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫.

</Tip>


–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ - –≤ –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤, —ç—Ç–æ –±—ã–ª–æ –Ω–µ —Ç–∞–∫ —É–∂ –∏ —Å–ª–æ–∂–Ω–æ! –ü—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é, –º—ã –¥–æ–ª–∂–Ω—ã –≤–æ–π—Ç–∏ –≤ Hugging Face. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –≤—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —Å –ø–æ–º–æ—â—å—é —Å–ª–µ–¥—É—é—â–µ–π —Å–ª—É–∂–µ–±–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–ü–æ—è–≤–∏—Ç—Å—è –≤–∏–¥–∂–µ—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Hugging Face.

–ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

–û—Å—Ç–∞–ª–æ—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å `Trainer`. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —Ä–∞–∑–æ–≥—Ä–µ–≤–æ–º (warmup) –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ –≤ 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). –ê–∫–∫—É–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –æ–¥–∏–Ω –±–∞—Ç—á –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç—å, –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ö–æ–¥–æ–≤ –≤–ø–µ—Ä–µ–¥/–Ω–∞–∑–∞–¥. –ú—ã —É–≤–∏–¥–∏–º —ç—Ç–æ –≤ –¥–µ–π—Å—Ç–≤–∏–∏, –∫–æ–≥–¥–∞ —Å–æ–∑–¥–∞–¥–∏–º —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ü§ó Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å `Trainer` –∏ –¥–æ–∂–¥–∞—Ç—å—Å—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è. –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ª–∏ –≤—ã –µ–≥–æ –Ω–∞ –ø–æ–ª–Ω–æ–º –∏–ª–∏ –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞, —ç—Ç–æ –∑–∞–π–º–µ—Ç 20 –∏–ª–∏ 2 —á–∞—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —Ç–∞–∫ —á—Ç–æ –∑–∞—Ö–≤–∞—Ç–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—à–µ–∫ –∫–æ—Ñ–µ –∏ —Ö–æ—Ä–æ—à—É—é –∫–Ω–∏–≥—É –¥–ª—è —á—Ç–µ–Ω–∏—è!

```py
trainer.train()
```

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º—ã –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ Hub:

```py
trainer.push_to_hub()
```

{:else}

–û—Å—Ç–∞–ª–æ—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–∑–≤–∞—Ç—å `compile()` –∏ `fit()`. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —Ä–∞–∑–æ–≥—Ä–µ–≤–æ–º, —á—Ç–æ–±—ã –ø–æ–≤—ã—Å–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# –û–±—É—á–µ–Ω–∏–µ —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –≤—ã–∑–≤–∞—Ç—å `model.fit()` –∏ –¥–æ–∂–¥–∞—Ç—å—Å—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è. –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ª–∏ –≤—ã –µ–≥–æ –Ω–∞ –ø–æ–ª–Ω–æ–º –∏–ª–∏ –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞, —ç—Ç–æ –∑–∞–π–º–µ—Ç 20 –∏–ª–∏ 2 —á–∞—Å–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —Ç–∞–∫ —á—Ç–æ –∑–∞—Ö–≤–∞—Ç–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—à–µ–∫ –∫–æ—Ñ–µ –∏ —Ö–æ—Ä–æ—à—É—é –∫–Ω–∏–≥—É –¥–ª—è —á—Ç–µ–Ω–∏—è! –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º—ã –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ Hub:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –í—Å–µ–≥–æ –æ–∫–æ–ª–æ 30 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ `TrainingArguments` –ø–æ–Ω–∞–¥–æ–±–∏–ª–æ—Å—å –Ω–∞–º, —á—Ç–æ–±—ã –ø–µ—Ä–µ–π—Ç–∏ –æ—Ç —Å—ã—Ä—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é GPT-2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —ç—Ç–æ –Ω–∞ —Å–≤–æ–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, —Å–º–æ–∂–µ—Ç–µ –ª–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!

</Tip>

<Tip>

{#if fw === 'pt'}

üí° –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∫–æ–º–ø—å—é—Ç–µ—Ä—É —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ GPU, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–¥ –Ω–∞ –Ω–µ–º. `Trainer` –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º–∏, –∏ —ç—Ç–æ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ.

{:else}

üí° –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∫–æ–º–ø—å—é—Ç–µ—Ä—É —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ GPU, –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç `MirroredStrategy` –¥–ª—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è. –í–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `tf.distribute.MirroredStrategy` –∏ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ –º–µ—Ç–æ–¥—ã `to_tf_dataset()` –∏–ª–∏ `prepare_tf_dataset()`, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –≤—ã–∑–æ–≤ `fit()` –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ –µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ `scope()`. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ —ç—Ç—É —Ç–µ–º—É –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å [–∑–¥–µ—Å—å](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω–≤–µ–π–µ—Ä–∞[[code-generation-with-a-pipeline]]

–ù–∞—Å—Ç–∞–ª –º–æ–º–µ–Ω—Ç –∏—Å—Ç–∏–Ω—ã: –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å! –í –ª–æ–≥–∞—Ö –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –ø–æ—Ç–µ—Ä–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Å–Ω–∏–∂–∞—é—Ç—Å—è, –Ω–æ —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–¥—Å–∫–∞–∑–∫–∞—Ö. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –æ–±–µ—Ä–Ω–µ–º –º–æ–¥–µ–ª—å –≤ `pipeline` –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–º–µ—Å—Ç–∏–º –µ–µ –Ω–∞ GPU –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ —Ç–∞–∫–æ–≤–æ–π –¥–æ—Å—Ç—É–ø–µ–Ω:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

–î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–∏ - —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å—Å–µ–∏–≤–∞–Ω–∏—è (scatter plot):

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —ç—Ç–æ —Ç–∞–∫–∂–µ –¥–ª—è `pandas` –æ–ø–µ—Ä–∞—Ü–∏–∏? –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–º–æ–∂–µ–º –ª–∏ –º—ã —Å–æ–∑–¥–∞—Ç—å `DataFrame` –∏–∑ –¥–≤—É—Ö –º–∞—Å—Å–∏–≤–æ–≤:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

–û—Ç–ª–∏—á–Ω–æ, —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç -- —Ö–æ—Ç—è –∑–∞—Ç–µ–º –æ–Ω —Å–Ω–æ–≤–∞ –≤—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–æ–ª–±–µ—Ü `x`. –ü–æ—Å–∫–æ–ª—å–∫—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ, —Å–ª–µ–¥—É—é—â–∏–π —Ü–∏–∫–ª `for` –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è. –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–º–æ–∂–µ–º –ª–∏ –º—ã —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ-—Ç–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–µ, –∏ —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø–æ–º–æ–≥–ª–∞ –Ω–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø–µ—Ä–∞—Ü–∏—é `groupby`: 

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

–ù–µ–ø–ª–æ—Ö–æ; —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ. –ù–∞–∫–æ–Ω–µ—Ü, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–º–æ–∂–µ–º –ª–∏ –º—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è `scikit-learn` –∏ —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å Random Forest:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

–ì–ª—è–¥—è –Ω–∞ —ç—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å —É—Å–≤–æ–∏–ª–∞ —á–∞—Å—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ —Å—Ç–µ–∫–∞ Python Data Science. –ö–æ–Ω–µ—á–Ω–æ, –Ω–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å, –ø—Ä–µ–∂–¥–µ —á–µ–º –≤–Ω–µ–¥—Ä—è—Ç—å –µ–µ –≤ —Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä, –Ω–æ –≤—Å–µ –∂–µ —ç—Ç–æ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–π –ø—Ä–æ—Ç–æ—Ç–∏–ø.

{:else}

–ì–ª—è–¥—è –Ω–∞ —ç—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å —É—Å–≤–æ–∏–ª–∞ —á–∞—Å—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ —Å—Ç–µ–∫–∞ Python Data Science (–∫–æ–Ω–µ—á–Ω–æ, –Ω–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —ç—Ç–æ –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ, –ø—Ä–µ–∂–¥–µ —á–µ–º —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ). –û–¥–Ω–∞–∫–æ –∏–Ω–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –¥–æ–±–∏—Ç—å—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª—É—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏–ª–∏ –∏–º–µ—Ç—å —É—Å–ª–æ–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –ø–ª–æ—Ö–∏–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∞ –ª–µ—Ç—É? –û–¥–Ω–∏–º –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–¥–∫–ª–∞—Å—Å `Trainer` –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ –∏–Ω–æ–≥–¥–∞ –ø—Ä–æ—â–µ –Ω–∞–ø–∏—Å–∞—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è. –í–æ—Ç —Ç—É—Ç-—Ç–æ –∏ –ø—Ä–∏—Ö–æ–¥–∏—Ç –Ω–∞ –ø–æ–º–æ—â—å ü§ó Accelerate.

{/if}

{#if fw === 'pt'}

## –û–±—É—á–µ–Ω–∏–µ —Å ü§ó Accelerate[[training-with-accelerate]]

–ú—ã —É–∂–µ –≤–∏–¥–µ–ª–∏, –∫–∞–∫ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é `Trainer`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–µ–ª–∞—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –û–¥–Ω–∞–∫–æ –∏–Ω–æ–≥–¥–∞ –Ω–∞–º –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ —Ü–∏–∫–ª–æ–º –æ–±—É—á–µ–Ω–∏—è, –∏–ª–∏ –º—ã —Ö–æ—Ç–∏–º –≤–Ω–µ—Å—Ç–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–∫–∑–æ—Ç–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ ü§ó Accelerate - –æ—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä, –∏ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —à–∞–≥–∏ –ø–æ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏. –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –≤—Å–µ –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º, –º—ã —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏–º –∏–∑—é–º–∏–Ω–∫—É –≤ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è.

<Youtube id="Hm8_PgVTFuc"/>

–ü–æ—Å–∫–æ–ª—å–∫—É –Ω–∞—Å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —Ä–∞–∑—É–º–Ω–æ–µ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫ data science, –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –ø—Ä–∏–¥–∞—Ç—å –±–æ–ª—å—à–∏–π –≤–µ—Å –æ–±—É—á–∞—é—â–∏–º –ø—Ä–∏–º–µ—Ä–∞–º, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —ç—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏. –ú—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —ç—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ç–∞–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∫–∞–∫ `plt`, `pd`, `sk`, `fit` –∏ `predict`, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ `matplotlib.pyplot`, `pandas` –∏ `sklearn`, –∞ —Ç–∞–∫–∂–µ —à–∞–±–ª–æ–Ω–∞–º fit/predict –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π. –ï—Å–ª–∏ –∫–∞–∂–¥—ã–π –∏–∑ –Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –º—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –ª–∏ –æ–Ω–∏ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –¢–æ–∫–µ–Ω—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–µ–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å, –ø–æ—ç—Ç–æ–º—É –º—ã —Ç–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–º –Ω–∞–ª–∏—á–∏–µ —Ç–∞–∫–∏—Ö –≤–µ—Ä—Å–∏–π –≤ —Å–ª–æ–≤–∞—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –º—ã –¥–æ–±–∞–≤–∏–º –æ–¥–∏–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

–û—Ç–ª–∏—á–Ω–æ, –ø–æ—Ö–æ–∂–µ, —ç—Ç–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç! –¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –ª–æ–≥–∏—Ç—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ –≤—ã–±—Ä–∞–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω—è—Ç—å –ª–æ–≥–∏—Ç—ã –∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–¥–≤–∏–Ω—É—Ç–∞—è –Ω–∞ –µ–¥–∏–Ω–∏—Ü—É –≤–ø—Ä–∞–≤–æ, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –º–µ—Ç–∫–∏, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ç–∫–æ–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ú—ã –º–æ–∂–µ–º –¥–æ–±–∏—Ç—å—Å—è —ç—Ç–æ–≥–æ, –Ω–∞—á–∏–Ω–∞—è –º–µ—Ç–∫–∏ —Å–æ –≤—Ç–æ—Ä–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –º–æ–¥–µ–ª—å –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–µ –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ó–∞—Ç–µ–º –º—ã –æ—Ç—Å–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ª–æ–≥–∏—Ç, –ø–æ—Å–∫–æ–ª—å–∫—É —É –Ω–∞—Å –Ω–µ—Ç –º–µ—Ç–∫–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–ª–µ–¥—É–µ—Ç –∑–∞ –≤—Å–µ–π –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å –ø–æ—Ç–µ—Ä–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –∏ –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–∏–º–µ—Ä–µ. –ù–∞–∫–æ–Ω–µ—Ü, –º—ã –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º –ø—Ä–∏–º–µ—Ä–∞–º, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Å–æ–≤. –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –Ω–µ —Ö–æ—Ç–∏–º –æ—Ç–±—Ä–∞—Å—ã–≤–∞—Ç—å –≤—Å–µ –≤—ã–±–æ—Ä–∫–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –º—ã –¥–æ–±–∞–≤–ª—è–µ–º 1 –∫ –≤–µ—Å–∞–º:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # –°–¥–≤–∏–≥–∞–µ–º —Ç–∞–∫, —á—Ç–æ–±—ã —Ç–æ–∫–µ–Ω—ã < n –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–∏ n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # –†–∞—Å—á–µ—Ç –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # –†–∞—Å—á–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

–ü—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é —Å —ç—Ç–æ–π –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–π –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å, –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—â–µ–π:

- –ù–∞–º –Ω—É–∂–Ω—ã –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∂–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏.
- –ù–∞–º –Ω—É–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Å–∞ (weight decay).
- –í—Ä–µ–º—è –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ—Ü–µ–Ω–∫—É, –ø–æ—ç—Ç–æ–º—É –∏–º–µ–µ—Ç —Å–º—ã—Å–ª –æ–±–µ—Ä–Ω—É—Ç—å –∫–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≤ —Ñ—É–Ω–∫—Ü–∏—é.

–î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Å –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –ù–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—Ç—å –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ñ–æ—Ä–º–∞—Ç `"torch"`, –∞ –∑–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –µ–≥–æ –≤ PyTorch `DataLoader` —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

–î–∞–ª–µ–µ –º—ã –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ–±—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∑–Ω–∞–ª, –∫–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –ø–æ–ª—É—á–∞—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–∞. –û–±—ã—á–Ω–æ –≤—Å–µ —Å–º–µ—â–µ–Ω–∏—è (bias) –∏ –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã LayerNorm (LayerNorm weights) –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –∏–∑ —ç—Ç–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞; –≤–æ—Ç –∫–∞–∫ –º—ã –º–æ–∂–µ–º —ç—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ö–æ—Ç–∏–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏ –¥–ª—è —ç—Ç–æ–≥–æ. –û–Ω–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –ø–æ—Ç–µ—Ä–∏:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

–° –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `evaluate()` –º—ã –º–æ–∂–µ–º —Å–æ–æ–±—â–∞—Ç—å –æ –ø–æ—Ç–µ—Ä—è—Ö –∏ [–ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏](../chapter7/3) —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏. –î–∞–ª–µ–µ –º—ã –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–º –Ω–∞—à—É –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã —Å–Ω–æ–≤–∞ –æ–±—É—á–∞–µ–º—Å—è —Å –Ω—É–ª—è:

```py
model = GPT2LMHeadModel(config)
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—à –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —á–∞—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Å–∞:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® –ï—Å–ª–∏ –≤—ã –ø—Ä–æ–≤–æ–¥–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ TPU, –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤–µ—Å—å –∫–æ–¥, –Ω–∞—á–∏–Ω–∞—è —Å —è—á–µ–π–∫–∏ –≤—ã—à–µ, –≤ –≤—ã–¥–µ–ª–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è. –ü–æ–¥—Ä–æ–±–Ω–µ–µ —Å–º–æ—Ç—Ä–∏—Ç–µ [–ì–ª–∞–≤—É 3](../chapter3/1).

</Tip>

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –Ω–∞—à `train_dataloader` –≤ `accelerator.prepare()`, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª–∏–Ω—É –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–º–µ–Ω–∏—Ç –µ–≥–æ –¥–ª–∏–Ω—É. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ª–∏–Ω–µ–π–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–æ 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

–ù–∞–∫–æ–Ω–µ—Ü, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤ Hub, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `Repository` –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–ø–∫–µ. –°–Ω–∞—á–∞–ª–∞ –≤–æ–π–¥–∏—Ç–µ –≤ Hub Hugging Face, –µ—Å–ª–∏ –≤—ã –µ—â–µ –Ω–µ –≤–æ—à–ª–∏ –≤ –Ω–µ–≥–æ. –ú—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º –∏–º—è —Ä–æ–∑–∏—Ç–æ—Ä–∏—è –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏—Å–≤–æ–∏—Ç—å –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–Ω–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –∑–∞–º–µ–Ω–∏—Ç—å `repo_name` –Ω–∞ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç; –æ–Ω –ø—Ä–æ—Å—Ç–æ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–∞—à–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ –∏ –¥–µ–ª–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∫–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ä–æ–∑–∏—Ç–æ—Ä–∏–π –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É. –ï—Å–ª–∏ –æ–Ω–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —ç—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–æ–Ω–æ–º —Ä–æ–∑–∏—Ç–æ—Ä–∏—è, —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã —Ä–∞–±–æ—Ç–∞–µ–º:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤ `output_dir`, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `repo.push_to_hub()`. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏.

–ü–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –¥–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ–¥–µ–º –±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

–≠—Ç–æ –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–æ—Ç–µ—Ä—å –∏ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏, –Ω–æ —ç—Ç–æ –Ω–µ—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –≤–µ–¥—å –º—ã –µ—â–µ –Ω–µ –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å. –ò—Ç–∞–∫, —É –Ω–∞—Å –≤—Å–µ –≥–æ—Ç–æ–≤–æ –¥–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è. –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è –º—ã –≤—ã–ø–æ–ª–Ω—è–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –∑–∞–≥—Ä—É–∑—á–∏–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–¥–∞–µ–º –±–∞—Ç—á–∏ –≤ –º–æ–¥–µ–ª—å. –° –ø–æ–º–æ—â—å—é –ª–æ–≥–∏—Ç–æ–≤ –º—ã –º–æ–∂–µ–º –æ—Ü–µ–Ω–∏—Ç—å –Ω–∞—à—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å. –ú—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø–æ—Ç–µ—Ä–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —à–∞–≥–æ–≤ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–∏—Ö –ø–æ—Ç–µ—Ä—å –ø—Ä–∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤. –ü–µ—Ä–µ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –º—ã —Ç–∞–∫–∂–µ –æ–±—Ä–µ–∑–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –ù–∞–∫–æ–Ω–µ—Ü, –∫–∞–∂–¥—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –º—ã –æ—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Ü–µ–Ω–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–π –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ `evaluate()`:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

–í–æ—Ç –∏ –≤—Å–µ -- —Ç–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞—É–∑–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-2, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –õ–∏–±–æ —Å–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, –ø–æ–¥—Ö–æ–¥—è—â—É—é –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è, –ª–∏–±–æ –¥–æ–±–∞–≤—å—Ç–µ –µ—â–µ –æ–¥–∏–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–≥ –≤ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è.

</Tip>

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ü—Ä–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –æ–±—É—á–µ–Ω–∏—é –ø–æ–ª–µ–∑–Ω–æ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é —Ç–∞–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∫–∞–∫ TensorBoard –∏–ª–∏ Weights & Biases. –î–æ–±–∞–≤—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤—ã –≤—Å–µ–≥–¥–∞ –º–æ–≥–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ –ø—Ä–æ—Ö–æ–¥–∏—Ç –æ–±—É—á–µ–Ω–∏–µ.

</Tip>

{/if}
