<FrameworkSwitchCourse {fw} />

# –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è[[summarization]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},
]} />

{/if}


–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º–æ–¥–µ–ª–∏ Transformer –º–æ–≥—É—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è —Å–∂–∞—Ç–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ - –∑–∞–¥–∞—á–∞, –∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞–∫ _—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (text summarization)_. –≠—Ç–æ –æ–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á NLP, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ —Ç—Ä–µ–±—É–µ—Ç —Ü–µ–ª–æ–≥–æ —Ä—è–¥–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç—Ä—ã–≤–∫–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤—è–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –æ—Ç—Ä–∞–∂–∞—é—â–µ–≥–æ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞. –û–¥–Ω–∞–∫–æ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã, –∏–∑–±–∞–≤–∏–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–æ–º–µ–Ω–∞ –æ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—á—Ç–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

<Youtube id="yHnr5Dk2zCI"/>

–•–æ—Ç—è –Ω–∞ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads) —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, –ø–æ—á—Ç–∏ –≤—Å–µ –æ–Ω–∏ –ø–æ–¥—Ö–æ–¥—è—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ—ç—Ç–æ–º—É, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –∏–∑—é–º–∏–Ω–∫—É –≤ —ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª, –º—ã –æ–±—É—á–∏–º –¥–≤—É—Ö—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –ö –∫–æ–Ω—Ü—É —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ —É –≤–∞—Å –±—É–¥–µ—Ç [–º–æ–¥–µ–ª—å](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es), —Å–ø–æ—Å–æ–±–Ω–∞—è –∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –∑–¥–µ—Å—å:

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

–ö–∞–∫ –º—ã —É–≤–∏–¥–∏–º, —ç—Ç–∏ —Ä–µ–∑—é–º–µ –∫—Ä–∞—Ç–∫–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫—É–ø–∞—Ç–µ–ª–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –≤ —Å–≤–æ–∏—Ö –æ—Ç–∑—ã–≤–∞—Ö –æ —Ç–æ–≤–∞—Ä–µ. –î–ª—è –Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ —Å–æ–±–µ—Ä–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–≤—É—è–∑—ã–∫–æ–≤–æ–π –∫–æ—Ä–ø—É—Å –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏.

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞[[preparing-a-multilingual-corpus]]

–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—à–µ–π –¥–≤—É—è–∑—ã–∫–æ–≤–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi). –≠—Ç–æ—Ç –∫–æ—Ä–ø—É—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ—Ç–∑—ã–≤–æ–≤ –æ —Ç–æ–≤–∞—Ä–∞—Ö Amazon –Ω–∞ —à–µ—Å—Ç–∏ —è–∑—ã–∫–∞—Ö –∏ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –û–¥–Ω–∞–∫–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –∫–æ—Ä–æ—Ç–∫–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö —Ä–µ–∑—é–º–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏! –ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É, –¥–∞–≤–∞–π—Ç–µ –∑–∞–≥—Ä—É–∑–∏–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏ –∏—Å–ø–∞–Ω—Å–∫–∏–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–∑ Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

–ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞ –µ—Å—Ç—å 200 000 –æ—Ç–∑—ã–≤–æ–≤ –≤ —á–∞—Å—Ç–∏ `train` –∏ –ø–æ 5 000 –æ—Ç–∑—ã–≤–æ–≤ –≤ —á–∞—Å—Ç—è—Ö `validation` –∏ `test`. –ò–Ω—Ç–µ—Ä–µ—Å—É—é—â–∞—è –Ω–∞—Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ—Ü–µ–Ω–∑–∏—è—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö `review_body` –∏ `review_title`. –î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤, —Å–æ–∑–¥–∞–≤ –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±–µ—Ä–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤, –∏–∑—É—á–µ–Ω–Ω—ã—Ö –≤ [–ì–ª–∞–≤–µ 5](../chapter5/1):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does it‚Äôs job and it‚Äôs gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò–∑–º–µ–Ω–∏—Ç–µ random seed –≤ –∫–æ–º–∞–Ω–¥–µ `Dataset.shuffle()`, —á—Ç–æ–±—ã –∏–∑—É—á–∏—Ç—å –¥—Ä—É–≥–∏–µ –æ—Ç–∑—ã–≤—ã –≤ –∫–æ—Ä–ø—É—Å–µ. –ï—Å–ª–∏ –≤—ã –≤–ª–∞–¥–µ–µ—Ç–µ –∏—Å–ø–∞–Ω—Å–∫–∏–º —è–∑—ã–∫–æ–º, –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∑—ã–≤—ã –≤ `spanish_dataset`, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –ø–æ—Ö–æ–∂–∏ –ª–∏ –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑—É–º–Ω—ã–µ —Ä–µ–∑—é–º–µ.

</Tip>

–≠—Ç–∞ –≤—ã–±–æ—Ä–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ—Ç–∑—ã–≤–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ —Å–µ—Ç–∏, - –æ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö (–∏ –≤—Å–µ, —á—Ç–æ –º–µ–∂–¥—É –Ω–∏–º–∏!). –•–æ—Ç—è –ø—Ä–∏–º–µ—Ä —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º "meh" –Ω–µ –æ—á–µ–Ω—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–µ–Ω, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –¥–æ—Å—Ç–æ–π–Ω—ã–µ —Ä–µ–∑—é–º–µ —Å–∞–º–∏—Ö –æ—Ç–∑—ã–≤–æ–≤. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö 400 000 –æ—Ç–∑—ã–≤–æ–≤ –∑–∞–Ω—è–ª–æ –±—ã —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º GPU, –ø–æ—ç—Ç–æ–º—É –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑—é–º–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –∫–∞–∫–∏–µ –¥–æ–º–µ–Ω—ã –º—ã –º–æ–∂–µ–º –≤—ã–±—Ä–∞—Ç—å, –¥–∞–≤–∞–π—Ç–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º `english_dataset` –≤ `pandas.DataFrame` –∏ –≤—ã—á–∏—Å–ª–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

–°–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ - —ç—Ç–æ –±—ã—Ç–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã, –æ–¥–µ–∂–¥–∞ –∏ –±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞. –û–¥–Ω–∞–∫–æ, —á—Ç–æ–±—ã –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–º—É Amazon, –¥–∞–≤–∞–π—Ç–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ –æ –∫–Ω–∏–≥–∞—Ö - –≤ –∫–æ–Ω—Ü–µ –∫–æ–Ω—Ü–æ–≤, –∏–º–µ–Ω–Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–º–ø–∞–Ω–∏—è –∏ –±—ã–ª–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞! –ú—ã –≤–∏–¥–∏–º –¥–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —ç—Ç–æ–π —Ü–µ–ª–∏ (`book` –∏ `digital_ebook_purchase`), –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –Ω–∞ –æ–±–æ–∏—Ö —è–∑—ã–∫–∞—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤. –ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 5](../chapter5/1), —Ñ—É–Ω–∫—Ü–∏—è `Dataset.filter()` –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ —á–∞—Å—Ç–∏, –ø–æ—ç—Ç–æ–º—É –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —ç—Ç–æ–≥–æ:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –ø—Ä–∏–º–µ–Ω–∏–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –∫ `english_dataset` –∏ `spanish_dataset`, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ —Å—Ç—Ä–æ–∫–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–Ω–∏–≥. –ü—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä, –¥–∞–≤–∞–π—Ç–µ –∏–∑–º–µ–Ω–∏–º —Ñ–æ—Ä–º–∞—Ç `english_dataset` —Å `"pandas"` –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ `"arrow"`:

```python
english_dataset.reset_format()
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤—ã–±–æ—Ä–∫—É –æ—Ç–∑—ã–≤–æ–≤, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Å–≤—è—â–µ–Ω—ã –∫–Ω–∏–≥–∞–º:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

–•–æ—Ä–æ—à–æ, –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –æ—Ç–∑—ã–≤—ã –Ω–µ —Å–æ–≤—Å–µ–º –æ –∫–Ω–∏–≥–∞—Ö –∏ –º–æ–≥—É—Ç –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫ —Ç–∞–∫–∏–º –≤–µ—â–∞–º, –∫–∞–∫ –∫–∞–ª–µ–Ω–¥–∞—Ä–∏ –∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä OneNote. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –¥–æ–º–µ–Ω –∫–∞–∂–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–µ–∂–¥–µ —á–µ–º –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏, –Ω–∞–º –æ—Å—Ç–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏ –∏—Å–ø–∞–Ω—Å–∫–∏–µ –æ—Ç–∑—ã–≤—ã –≤ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç `DatasetDict`. ü§óDatasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `concatenate_datasets()`, –∫–æ—Ç–æ—Ä–∞—è (–∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è) —Å—Ç–µ–∫–∏—Ä—É–µ—Ç –¥–≤–∞ –æ–±—ä–µ–∫—Ç–∞ `Dataset` –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å –¥–≤—É—è–∑—ã–∫–æ–≤–æ–π –¥–∞—Ç–∞—Å–µ—Ç, –º—ã –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏, –æ–±—ä–µ–¥–∏–Ω–∏–º –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è —ç—Ç–æ–π —á–∞—Å—Ç–∏ –∏ –ø–µ—Ä–µ–º–µ—à–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —á—Ç–æ–±—ã –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —Å–ª–∏—à–∫–æ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DA√ëADO'
'>> Review: Me lleg√≥ el d√≠a que tocaba, junto a otros libros que ped√≠, pero la caja lleg√≥ en mal estado lo cual da√±√≥ las esquinas de los libros porque ven√≠an sin protecci√≥n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

–≠—Ç–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Å–º–µ—Å—å –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∏ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –æ–±–∑–æ—Ä–æ–≤! –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∫–æ—Ä–ø—É—Å, –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –≤ —Ä–µ—Ü–µ–Ω–∑–∏—è—Ö –∏ –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –∑–∞–¥–∞—á —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, –≥–¥–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ –≤ –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç —Å–∫–ª–æ–Ω—è—Ç—å –º–æ–¥–µ–ª—å –≤ —Å—Ç–æ—Ä–æ–Ω—É –≤—ã–≤–æ–¥–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –¥–≤—É—Ö —Å–ª–æ–≤ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ. –ù–∞ –≥—Ä–∞—Ñ–∏–∫–∞—Ö –Ω–∏–∂–µ –ø–æ–∫–∞–∑–∞–Ω—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤, –∏ –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∫–æ—à–µ–Ω—ã –≤ —Å—Ç–æ—Ä–æ–Ω—É 1-2 —Å–ª–æ–≤:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

–ß—Ç–æ–±—ã —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π, –º—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, —á—Ç–æ–±—ã –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ä–µ–∑—é–º–µ. –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º–∏ –∏ –∏—Å–ø–∞–Ω—Å–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä—É–±—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º –ø—Ä–æ–±–µ–ª–∞, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à –Ω–∞–¥–µ–∂–Ω—ã–π –º–µ—Ç–æ–¥ `Dataset.filter()` —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –∫–æ—Ä–ø—É—Å, –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Transformer, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ!

## –ú–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞[[models-for-text-summarization]]

–ï—Å–ª–∏ –∑–∞–¥—É–º–∞—Ç—å—Å—è, —Ç–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ - —ç—Ç–æ –∑–∞–¥–∞—á–∞, –ø–æ—Ö–æ–∂–∞—è –Ω–∞ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥: —É –Ω–∞—Å –µ—Å—Ç—å —Ç–µ–∫—Å—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä —Ä–µ—Ü–µ–Ω–∑–∏—è, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–µ–ª–∏ –±—ã "–ø–µ—Ä–µ–≤–µ—Å—Ç–∏" –≤ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫—É—é –≤–µ—Ä—Å–∏—é, –ø–µ—Ä–µ–¥–∞—é—â—É—é –æ—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Transformer –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä, —Å –∫–æ—Ç–æ—Ä–æ–π –º—ã –≤–ø–µ—Ä–≤—ã–µ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å –≤ [–ì–ª–∞–≤–∞ 1](../chapter1/1), —Ö–æ—Ç—è –µ—Å—Ç—å –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π GPT, –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö few-shot –Ω–∞—Å—Ç—Ä–æ–µ–∫. –í —Å–ª–µ–¥—É—é—â–µ–π —Ç–∞–±–ª–∏—Ü–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.

| –ú–æ–¥–µ–ª—å Transformer | –û–ø–∏—Å–∞–Ω–∏–µ                                                                                                                                                                                                    | –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è? |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | –•–æ—Ç—è GPT-2 –æ–±—É—á–µ–Ω –∫–∞–∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ, –¥–æ–±–∞–≤–ª—è—è "TL;DR" –≤ –∫–æ–Ω—Ü–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.                                                                          |      ‚ùå       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–∞—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. –≠—Ç–∞ –∑–∞–¥–∞—á–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–ª–∏–∂–µ –∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, —á–µ–º –∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–º—É —è–∑—ã–∫–æ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.|      ‚ùå       |
|     [T5](https://huggingface.co/t5-base)      | –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –≤—Å–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç; –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ - `summarize: ARTICLE`.                              |      ‚ùå       |
|     [mT5](https://huggingface.co/google/mt5-base)     | –ú–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤–∞—è –≤–µ—Ä—Å–∏—è T5, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤–æ–º –∫–æ—Ä–ø—É—Å–µ Common Crawl (mC4), –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–º 101 —è–∑—ã–∫.                                                                                                |      ‚úÖ       |
|    [BART](https://huggingface.co/facebook/bart-base)     | –ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer —Å –∫–æ–¥–µ—Ä–æ–º –∏ —Å—Ç–µ–∫–æ–º –¥–µ–∫–æ–¥–µ—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π –≤—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª, —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ —Å—Ö–µ–º—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è BERT –∏ GPT-2.                                    |      ‚ùå       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | –ú–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤–∞—è –≤–µ—Ä—Å–∏—è BART, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 50 —è–∑—ã–∫–∞—Ö.                                                                                                                                                     |      ‚úÖ       |

–ö–∞–∫ –≤–∏–¥–Ω–æ –∏–∑ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Transformer –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–∏ –≤–æ–æ–±—â–µ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á NLP) —è–≤–ª—è—é—Ç—Å—è –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏. –≠—Ç–æ —Ö–æ—Ä–æ—à–æ, –µ—Å–ª–∏ –≤–∞—à–∞ –∑–∞–¥–∞—á–∞ –Ω–∞ "–≤—ã—Å–æ–∫–æ—Ä–µ—Å—É—Ä—Å–Ω–æ–º" —è–∑—ã–∫–µ, —Ç–∞–∫–æ–º –∫–∞–∫ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏–ª–∏ –Ω–µ–º–µ—Ü–∫–∏–π, –Ω–æ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ç—ã—Å—è—á –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. –ö —Å—á–∞—Å—Ç—å—é, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∫–ª–∞—Å—Å –º–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Transformer, —Ç–∞–∫–∏—Ö –∫–∞–∫ mT5 –∏ mBART, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –Ω–∞ –ø–æ–º–æ—â—å. –≠—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–æ —Å –∏–∑—é–º–∏–Ω–∫–æ–π: –≤–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞ –æ–Ω–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 50 —è–∑—ã–∫–∞—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ!

–ú—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ mT5, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ T5, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ "—Ç–µ–∫—Å-–≤-—Ç–µ–∫—Å—Ç" (text-to-text). –í T5 –∫–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ NLP —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç—Å—è –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø–æ–¥—Å–∫–∞–∑–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä `summarize:`, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫ –ø–æ–¥—Å–∫–∞–∑–∫–µ. –ö–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ –Ω–∏–∂–µ, —ç—Ç–æ –¥–µ–ª–∞–µ—Ç T5 —á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º, –ø–æ—Å–∫–æ–ª—å–∫—É –≤—ã –º–æ–∂–µ—Ç–µ —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

–í mT5 –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–µ—Ñ–∏–∫—Å—ã, –Ω–æ –æ–Ω–∞ –æ–±–ª–∞–¥–∞–µ—Ç –º–Ω–æ–≥–∏–º–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ T5 –∏ –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ. –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –≤—ã–±—Ä–∞–ª–∏ –º–æ–¥–µ–ª—å, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.


<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –≤—ã –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–µ—Ç–µ —ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª, –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ mT5 —Å—Ä–∞–≤–Ω–∏—Ç—Å—è —Å mBART, –¥–æ–æ–±—É—á–∏–≤ –µ–≥–æ —Ç–µ–º –∂–µ –º–µ—Ç–æ–¥–∞–º. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–æ–Ω—É—Å–Ω—ã–µ –æ—á–∫–∏, –≤—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–æ–æ–±—É—á–∏—Ç—å T5 —Ç–æ–ª—å–∫–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ä–µ—Ü–µ–Ω–∑–∏—è—Ö. –ü–æ—Å–∫–æ–ª—å–∫—É –≤ T5 –µ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å –∑–∞–ø—Ä–æ—Å–∞, –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å `summarize:` –∫ –≤—Ö–æ–¥–Ω—ã–º –ø—Ä–∏–º–µ—Ä–∞–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–∞—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.

</Tip>

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö[[preprocessing-the-data]]

<Youtube id="1m7BerpSq8A"/>

–ù–∞—à–∞ —Å–ª–µ–¥—É—é—â–∞—è –∑–∞–¥–∞—á–∞ - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –∏ –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤. –ö–∞–∫ –æ–±—ã—á–Ω–æ, –º—ã –Ω–∞—á–∏–Ω–∞–µ–º —Å –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–æ–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `mt5-small`, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∑–∞ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

üí° –ù–∞ —Ä–∞–Ω–Ω–∏—Ö —Å—Ç–∞–¥–∏—è—Ö –≤–∞—à–∏—Ö NLP-–ø—Ä–æ–µ–∫—Ç–æ–≤ —Ö–æ—Ä–æ—à–µ–π –ø—Ä–∞–∫—Ç–∏–∫–æ–π —è–≤–ª—è–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ "–º–∞–ª–µ–Ω—å–∫–∏—Ö" –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –≤–∞–º –±—ã—Å—Ç—Ä–µ–µ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å –∏ –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Å–∫–≤–æ–∑–Ω–æ–π —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å. –ö–æ–≥–¥–∞ –≤—ã –±—É–¥–µ—Ç–µ —É–≤–µ—Ä–µ–Ω—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö, –≤—ã –≤—Å–µ–≥–¥–∞ —Å–º–æ–∂–µ—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å –º–∞—Å—à—Ç–∞–± –º–æ–¥–µ–ª–∏, –ø—Ä–æ—Å—Ç–æ –∏–∑–º–µ–Ω–∏–≤ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏!

</Tip>

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä mT5 –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –ø—Ä–∏–º–µ—Ä–µ:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

–ó–¥–µ—Å—å –º—ã –≤–∏–¥–∏–º –∑–Ω–∞–∫–æ–º—ã–µ –Ω–∞–º `input_ids` –∏ `attention_mask`, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å –≤ –Ω–∞—à–∏—Ö –ø–µ—Ä–≤—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –ø–æ –¥–æ–æ–±—É—á–µ–Ω–∏—é –µ—â–µ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1). –î–∞–≤–∞–π—Ç–µ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º —ç—Ç–∏ –≤—Ö–æ–¥–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ `convert_ids_to_tokens()`, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —Å –∫–∞–∫–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['‚ñÅI', '‚ñÅ', 'loved', '‚ñÅreading', '‚ñÅthe', '‚ñÅHung', 'er', '‚ñÅGames', '</s>']
```

–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª –Æ–Ω–∏–∫–æ–¥–∞ `‚ñÅ` –∏ —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ `</s>` —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º SentencePiece, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ Unigram, —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ–º –≤ [–ì–ª–∞–≤–µ 6](../chapter6/1). Unigram –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç SentencePiece –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —É–¥–∞—Ä–µ–Ω–∏–π, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ç–æ–≥–æ —Ñ–∞–∫—Ç–∞, —á—Ç–æ –≤–æ –º–Ω–æ–≥–∏—Ö —è–∑—ã–∫–∞—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä –≤ —è–ø–æ–Ω—Å–∫–æ–º, –Ω–µ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.

–î–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞—à–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –Ω–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è —Å –æ–¥–Ω–æ–π —Ç–æ–Ω–∫–æ—Å—Ç—å—é, —Å–≤—è–∑–∞–Ω–Ω–æ–π —Å —Å—É–º—Ä–∏–∑–∞—Ü–∏–µ–π: –ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∞—à–∏ –º–µ—Ç–∫–∏ —Ç–∞–∫–∂–µ —è–≤–ª—è—é—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–º, –≤–æ–∑–º–æ–∂–Ω–æ, —á—Ç–æ –æ–Ω–∏ –ø—Ä–µ–≤—ã—à–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —É—Å–µ—á–µ–Ω–∏–µ –∫–∞–∫ –∫ –æ–±–∑–æ—Ä–∞–º, —Ç–∞–∫ –∏ –∫ –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞–º, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –≤ ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç `text_target`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –í–æ—Ç –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Ö–æ–¥–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ —ç—Ç–æ–º—É –∫–æ–¥—É, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç. –ü–µ—Ä–≤–æ–µ, —á—Ç–æ –º—ã —Å–¥–µ–ª–∞–ª–∏, —ç—Ç–æ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è `max_input_length` –∏ `max_target_length`, –∫–æ—Ç–æ—Ä—ã–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç –≤–µ—Ä—Ö–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª–∏–Ω—ã –Ω–∞—à–∏—Ö –æ–±–∑–æ—Ä–æ–≤ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤. –ü–æ—Å–∫–æ–ª—å–∫—É —Ç–µ–ª–æ –æ–±–∑–æ—Ä–∞ –æ–±—ã—á–Ω–æ –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –º—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è.

–° –ø–æ–º–æ—â—å—é `preprocess_function()` –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞ —Å –ø–æ–º–æ—â—å—é —É–¥–æ–±–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ `Dataset.map()`, –∫–æ—Ç–æ—Ä—É—é –º—ã —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –∫—É—Ä—Å–∞:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –∫–æ—Ä–ø—É—Å –±—ã–ª –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –ö–∞–∫ –º—ã —É–≤–∏–¥–∏–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å–µ—Ä–µ–±—Ä—è–Ω–æ–π –ø—É–ª–∏, –∫–æ–≥–¥–∞ –¥–µ–ª–æ –¥–æ—Ö–æ–¥–∏—Ç –¥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–æ–π —Ç–µ–∫—Å—Ç–∞.

<Tip>

üí° –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ –≤—ã—à–µ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ `Dataset.map()` –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ `batched=True`. –≠—Ç–æ –∫–æ–¥–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤ –±–∞—Ç—á–∞—Ö –ø–æ 1 000 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ –±—ã—Å—Ç—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ ü§ó Transformers. –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `batched=True`, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –æ—Ç–¥–∞—á—É –æ—Ç –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞!

</Tip>


## –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞[[metrics-for-text-summarization]]

<Youtube id="TMshhnrEXlg"/>

–ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –≤ —ç—Ç–æ–º –∫—É—Ä—Å–µ, –∏–∑–º–µ—Ä–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –∑–∞–¥–∞—á –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥, –Ω–µ —Ç–∞–∫ –ø—Ä–æ—Å—Ç–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è —Ä–µ—Ü–µ–Ω–∑–∏–∏ —Ç–∏–ø–∞ "I loved reading the Hunger Games" —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–µ–∑—é–º–µ, —Ç–∞–∫–∏—Ö –∫–∞–∫ "I loved the Hunger Games" –∏–ª–∏ "Hunger Games is a great read". –û—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–∞–∫–æ–≥–æ-—Ç–æ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–µ–∑—é–º–µ –∏ –º–µ—Ç–∫–æ–π –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ö–æ—Ä–æ—à–∏–º —Ä–µ—à–µ–Ω–∏–µ–º - –¥–∞–∂–µ –ª—é–¥–∏ –Ω–µ —Å–ø—Ä–∞–≤—è—Ç—Å—è —Å —Ç–∞–∫–æ–π –º–µ—Ç—Ä–∏–∫–æ–π, –ø–æ—Ç–æ–º—É —á—Ç–æ —É –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∞—Å —Å–≤–æ–π —Å—Ç–∏–ª—å –Ω–∞–ø–∏—Å–∞–Ω–∏—è.

–î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–µ—Ç—Ä–∏–∫ —è–≤–ª—è–µ—Ç—Å—è [–æ—Ü–µ–Ω–∫–∞ ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ—Ç Recall-Oriented Understudy for Gisting Evaluation). –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —ç—Ç–æ–π –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ —Å –Ω–∞–±–æ—Ä–æ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —Å–æ–∑–¥–∞—é—Ç—Å—è –ª—é–¥—å–º–∏. –ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –µ–µ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º —Å—Ä–∞–≤–Ω–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –¥–≤–∞ —Ä–µ–∑—é–º–µ:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

–û–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –∏—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –±—É–¥–µ—Ç 6. –û–¥–Ω–∞–∫–æ —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä—É–±–æ–≤–∞—Ç–æ, –ø–æ—ç—Ç–æ–º—É –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ ROUGE –æ—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –æ—Ü–µ–Ω–æ–∫ _precision_ –∏ _recall_ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è.

<Tip>

üôã –ù–µ –≤–æ–ª–Ω—É–π—Ç–µ—Å—å, –µ—Å–ª–∏ –≤—ã –≤–ø–µ—Ä–≤—ã–µ —Å–ª—ã—à–∏—Ç–µ –æ precision –∏ recall - –º—ã –≤–º–µ—Å—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–≥–ª—è–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ–±—ã –≤—Å–µ —Å—Ç–∞–ª–æ –ø–æ–Ω—è—Ç–Ω–æ. –≠—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—ã—á–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –ø–æ—ç—Ç–æ–º—É, –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è precision –∏ recall –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å `scikit-learn` [—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html).

</Tip>

–î–ª—è ROUGE recall –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ª–æ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É. –ï—Å–ª–∏ –º—ã –ø—Ä–æ—Å—Ç–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–ª–æ–≤–∞, recall –º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø–æ —Å–ª–µ–¥—É—é—â–µ–π —Ñ–æ—Ä–º—É–ª–µ:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

–î–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤—ã—à–µ —ç—Ç–∞ —Ñ–æ—Ä–º—É–ª–∞ –¥–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω—ã–π recall 6/6 = 1; —Ç–æ –µ—Å—Ç—å –≤—Å–µ —Å–ª–æ–≤–∞ –≤ —ç—Ç–∞–ª–æ–Ω–Ω–æ–º —Ä–µ–∑—é–º–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –º–æ–¥–µ–ª—å—é. –≠—Ç–æ –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å—Å—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–º, –Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, –µ—Å–ª–∏ –±—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–º–∏ —Ä–µ–∑—é–º–µ –±—ã–ª–æ "I really really loved reading the Hunger Games all night". –≠—Ç–æ —Ç–æ–∂–µ –¥–∞–ª–æ –±—ã –∏–¥–µ–∞–ª—å–Ω—ã–π recall, –Ω–æ, –≤–æ–∑–º–æ–∂–Ω–æ, –±—ã–ª–æ –±—ã —Ö—É–∂–µ, –ø–æ—Å–∫–æ–ª—å–∫—É –±—ã–ª–æ –±—ã –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–º. –ß—Ç–æ–±—ã —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏, –º—ã —Ç–∞–∫–∂–µ –≤—ã—á–∏—Å–ª—è–µ–º precision, –∫–æ—Ç–æ—Ä–∞—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ROUGE –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –±—ã–ª–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

–ï—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–æ –∫ –Ω–∞—à–µ–º—É –ø–æ–¥—Ä–æ–±–Ω–æ–º—É —Ä–µ–∑—é–º–µ, —Ç–æ precision —Å–æ—Å—Ç–∞–≤–∏—Ç 6/10 = 0,6, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ, —á–µ–º precision 6/7 = 0,86, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –æ–±—ã—á–Ω–æ –≤—ã—á–∏—Å–ª—è—é—Ç –∏ precision, –∏ recall, –∞ –∑–∞—Ç–µ–º F1-score (—Å—Ä–µ–¥–Ω–µ–µ –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ –∏–∑ precision –∏ recall). –ú—ã –º–æ–∂–µ–º –ª–µ–≥–∫–æ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é ü§ó Datasets, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏–≤ –ø–∞–∫–µ—Ç `rouge_score`:

```py
!pip install rouge_score
```

–∞ –∑–∞—Ç–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É ROUGE —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `rouge_score.compute()`, —á—Ç–æ–±—ã —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ä–∞–∑—É:

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

–û–≥–æ, –≤ —ç—Ç–æ–º –≤—ã–≤–æ–¥–µ –º–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - —á—Ç–æ –∂–µ –æ–Ω–∞ –æ–∑–Ω–∞—á–∞–µ—Ç? –í–æ-–ø–µ—Ä–≤—ã—Ö, ü§ó Datasets –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è precision, recall –∏ F1-score; —ç—Ç–æ `low`, `mid`, –∏ `high` –∞—Ç—Ä–∏–±—É—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∑–¥–µ—Å—å —É–≤–∏–¥–µ—Ç—å. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, ü§ó Dataset –≤—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ ROUGE, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–∞—Ö –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ. –í–∞—Ä–∏–∞–Ω—Ç `rouge1` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —É–Ω–∏–≥—Ä–∞–º–º - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –º–æ–¥–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–∫–∞–∑–∞—Ç—å –æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–∏ —Å–ª–æ–≤, –∏ —ç—Ç–æ –∏–º–µ–Ω–Ω–æ —Ç–∞ –º–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏ –≤—ã—à–µ. –ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ —ç—Ç–æ–º, –¥–∞–≤–∞–π—Ç–µ –∏–∑–≤–ª–µ—á–µ–º `—Å—Ä–µ–¥–Ω–µ–µ` –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞—à–∏—Ö –æ—Ü–µ–Ω–æ–∫:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

–û—Ç–ª–∏—á–Ω–æ, –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ precision –∏ recall —Å–æ–≤–ø–∞–¥–∞—é—Ç! –ê –∫–∞–∫ –Ω–∞—Å—á–µ—Ç –¥—Ä—É–≥–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π ROUGE? `rouge2` –∏–∑–º–µ—Ä—è–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –±–∏–≥—Ä–∞–º–º (—Å—á–∏—Ç–∞–π—Ç–µ, —á—Ç–æ —ç—Ç–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –ø–∞—Ä —Å–ª–æ–≤), –∞ `rougeL` –∏ `rougeLsum` –∏–∑–º–µ—Ä—è—é—Ç —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤, –∏—â–∞ —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ –æ–±—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ. –°–ª–æ–≤–æ "sum" –≤ `rougeLsum` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —ç—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –¥–ª—è –≤—Å–µ–≥–æ —Ä–µ–∑—é–º–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ `rougeL` –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, —Å–æ–≥–ª–∞—Å—É—é—Ç—Å—è –ª–∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ ROUGE —Å —Ä—É—á–Ω—ã–º —Ä–∞—Å—á–µ—Ç–æ–º –ø–æ —Ñ–æ—Ä–º—É–ª–∞–º precision –∏ recall. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–Ω—É—Å–Ω—ã—Ö –æ—á–∫–æ–≤ —Ä–∞–∑–±–µ–π—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –±–∏–≥—Ä–∞–º–º—ã –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ precision –∏ recall –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ `rouge2`.

</Tip>

–ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –æ—Ü–µ–Ω–∫—É ROUGE –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏, –Ω–æ –ø–µ—Ä–µ–¥ —ç—Ç–∏–º –¥–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º —Ç–æ, —á—Ç–æ –¥–æ–ª–∂–µ–Ω —Å–¥–µ–ª–∞—Ç—å –∫–∞–∂–¥—ã–π —Ö–æ—Ä–æ—à–∏–π NLP-–ø—Ä–∞–∫—Ç–∏–∫: —Å–æ–∑–¥–∞–¥–∏–º —Å–∏–ª—å–Ω—É—é, –Ω–æ –ø—Ä–æ—Å—Ç—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å!

### –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–ª—å–Ω–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è[[creating-a-strong-baseline]]

–î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –æ–±—ã—á–Ω–æ –±–µ—Ä—É—Ç –ø–µ—Ä–≤—ã–µ —Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏, —á—Ç–æ —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞—é—Ç –±–∞–∑–≤—ã–º —É—Ä–æ–≤–Ω–µ–º _lead-3_. –ú—ã –º–æ–≥–ª–∏ –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–º–≤–æ–ª—ã –ø–æ–ª–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –Ω–æ —ç—Ç–æ –Ω–µ –ø–æ–º–æ–∂–µ—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–∞–∫–∏—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä, –∫–∞–∫ " U.S." –∏–ª–∏ "U.N.". -- –ø–æ—ç—Ç–æ–º—É –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π `nltk`, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ª—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞–µ–≤. –í—ã –º–æ–∂–µ—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞–∫–µ—Ç —Å –ø–æ–º–æ—â—å—é `pip` —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
!pip install nltk
```

–∞ –∑–∞—Ç–µ–º —Å–∫–∞—á–∞–π—Ç–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏:

```python
import nltk

nltk.download("punkt")
```

–î–∞–ª–µ–µ –º—ã –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ `nltk` –∏ —Å–æ–∑–¥–∞–¥–∏–º –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–µ—Ä–≤—ã—Ö —Ç—Ä–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–±–∑–æ—Ä–µ. –ü—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏–Ω—è—Ç–æ –æ—Ç–¥–µ–ª—è—Ç—å –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –≤–∫–ª—é—á–∏–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –µ–µ –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –ø—Ä–∏–º–µ—Ä–µ:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

–ü–æ—Ö–æ–∂–µ, —á—Ç–æ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Ç–∞–∫ —á—Ç–æ —Ç–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ —Ä–µ–∞–ª–∏–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç —ç—Ç–∏ "—Ä–µ–∑—é–º–µ" –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É ROUGE –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è:

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ ROUGE –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ –∏ –Ω–µ–º–Ω–æ–≥–æ –ø—Ä–∏—É–∫—Ä–∞—Å–∏—Ç—å –∏—Ö —Å –ø–æ–º–æ—â—å—é Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –æ—Ü–µ–Ω–∫–∞ `rouge2` –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ, —á–µ–º —É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö; —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, —ç—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–µ—Ü–µ–Ω–∑–∏–π –æ–±—ã—á–Ω–æ –ª–∞–∫–æ–Ω–∏—á–Ω—ã, –∏ –ø–æ—ç—Ç–æ–º—É –±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å lead-3 —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ—Å–ª–æ–≤–µ–Ω. –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å —Ö–æ—Ä–æ—à–∏–π –±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è —Ä–∞–±–æ—Ç—ã, –¥–∞–≤–∞–π—Ç–µ –¥–æ–æ–±—É—á–∏–º mT5!

{#if fw === 'pt'}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ mT5 —Å API `Trainer`[[fine-tuning-mt5-with-the-trainer-api]]

–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ –¥—Ä—É–≥–∏–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ. –ü–µ—Ä–≤–æ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ `mt5-small`. –ü–æ—Å–∫–æ–ª—å–∫—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - —ç—Ç–æ –∑–∞–¥–∞—á–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∞ `AutoModelForSeq2SeqLM`, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –≤–µ—Å–∞:

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ mT5 —Å Keras[[fine-tuning-mt5-with-keras]]

–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ –¥—Ä—É–≥–∏–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ. –ü–µ—Ä–≤–æ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ `mt5-small`. –ü–æ—Å–∫–æ–ª—å–∫—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - —ç—Ç–æ –∑–∞–¥–∞—á–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∞ `TFAutoModelForSeq2SeqLM`, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –≤–µ—Å–∞:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

üí° –ï—Å–ª–∏ –≤—ã –∑–∞–¥–∞–µ—Ç–µ—Å—å –≤–æ–ø—Ä–æ—Å–æ–º, –ø–æ—á–µ–º—É –≤—ã –Ω–µ –≤–∏–¥–∏—Ç–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏, —Ç–æ —ç—Ç–æ –ø–æ—Ç–æ–º—É, —á—Ç–æ –¥–ª—è –∑–∞–¥–∞—á "–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å-–≤-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å" –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤–µ—Å–∞ —Å–µ—Ç–∏. –°—Ä–∞–≤–Ω–∏—Ç–µ —ç—Ç–æ —Å –Ω–∞—à–µ–π –º–æ–¥–µ–ª—å—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ [–ì–ª–∞–≤—ã 3](../chapter3/1), –≥–¥–µ –≥–æ–ª–æ–≤–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–µ—Ç—å.

</Tip>

–°–ª–µ–¥—É—é—â–µ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –≤–æ–π—Ç–∏ –≤ Hugging Face Hub. –ï—Å–ª–∏ –≤—ã –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –≤ –Ω–æ—É—Ç–±—É–∫–µ, –≤—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —Å –ø–æ–º–æ—â—å—é —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ª–µ–∑–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–∫–æ—Ç–æ—Ä–∞—è –æ—Ç–æ–±—Ä–∞–∑–∏—Ç –≤–∏–¥–∂–µ—Ç, –≥–¥–µ –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –¢–∞–∫–∂–µ –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∏ –≤–æ–π—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º—É —Ç–∞–º:

```
huggingface-cli login
```

{#if fw === 'pt'}

–î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ ROUGE –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ. –ö —Å—á–∞—Å—Ç—å—é, ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã `Seq2SeqTrainingArguments` –∏ `Seq2SeqTrainer`, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –∑–∞ –Ω–∞—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏! –ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –¥–∞–≤–∞–π—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –¥—Ä—É–≥–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –Ω–∞—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# –í—ã–≤–æ–¥–∏–º –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ø–æ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

–ó–¥–µ—Å—å –∞—Ä–≥—É–º–µ–Ω—Ç `predict_with_generate` –±—ã–ª –∑–∞–¥–∞–Ω, —á—Ç–æ–±—ã —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ –º—ã –¥–æ–ª–∂–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ –≤–æ –≤—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –≤—ã—á–∏—Å–ª–∏—Ç—å –±–∞–ª–ª—ã ROUGE –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏. –ö–∞–∫ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å –≤ [–ì–ª–∞–≤–µ 1](../chapter1/1), –¥–µ–∫–æ–¥–µ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è —Ç–æ–∫–µ–Ω—ã –ø–æ –æ–¥–Ω–æ–º—É, –∏ —ç—Ç–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –º–æ–¥–µ–ª–∏ `generate()`. –ó–∞–¥–∞–Ω–∏–µ `predict_with_generate=True` —É–∫–∞–∑—ã–≤–∞–µ—Ç `Seq2SeqTrainer` –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –ú—ã —Ç–∞–∫–∂–µ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –∏ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–æ–≤, –∏ –∑–∞–¥–∞–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä `save_total_limit`, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ 3 –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è - —ç—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –ø–æ—Ç–æ–º—É, —á—Ç–æ –¥–∞–∂–µ "–º–∞–ª–µ–Ω—å–∫–∞—è" –≤–µ—Ä—Å–∏—è mT5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–∫–æ–ª–æ –ì–∏–≥–∞–±–∞–π—Ç–∞ –º–µ—Å—Ç–∞ –Ω–∞ –∂–µ—Å—Ç–∫–æ–º –¥–∏—Å–∫–µ, –∏ –º—ã –º–æ–∂–µ–º —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –Ω–µ–º–Ω–æ–≥–æ –º–µ—Å—Ç–∞, –æ–≥—Ä–∞–Ω–∏—á–∏–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ø–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º.

–ê—Ä–≥—É–º–µ–Ω—Ç `push_to_hub=True` –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –≤ Hub –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è; –≤—ã –Ω–∞–π–¥–µ—Ç–µ —Ä–æ–∑–∏—Ç–æ—Ä–∏–π –ø–æ–¥ —Å–≤–æ–∏–º –ø—Ä–æ—Ñ–∏–ª–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –º–µ—Å—Ç–µ, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º `output_dir`. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ —É–∫–∞–∑–∞—Ç—å –∏–º—è —Ä–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ç–∏—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å, —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `hub_model_id` (–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –≤–∞–º –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é). –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é [`huggingface-course`](https://huggingface.co/huggingface-course), –º—ã –¥–æ–±–∞–≤–∏–ª–∏ `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` –≤ `Seq2SeqTrainingArguments`.

–°–ª–µ–¥—É—é—â–µ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ç—Ä–µ–Ω–µ—Ä—É —Ñ—É–Ω–∫—Ü–∏—é `compute_metrics()`, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –æ—Ü–µ–Ω–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –Ω–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ –≤—ã–∑–≤–∞—Ç—å `rouge_score.compute()` –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –º–æ–¥–µ–ª–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∞–º –Ω—É–∂–Ω–æ _–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å_ –≤—ã–≤–æ–¥—ã –∏ –º–µ—Ç–∫–∏ –≤ —Ç–µ–∫—Å—Ç, –ø—Ä–µ–∂–¥–µ —á–µ–º –º—ã —Å–º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å –æ—Ü–µ–Ω–∫—É ROUGE. –°–ª–µ–¥—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–ª–∞–µ—Ç –∏–º–µ–Ω–Ω–æ —ç—Ç–æ, –∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é `sent_tokenize()` –∏–∑ `nltk` –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Ä–µ–∑—é–º–µ —Å–∏–º–≤–æ–ª–æ–º –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏:

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ –≤ —Ç–µ–∫—Å—Ç
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # –ó–∞–º–µ–Ω—è–µ–º -100 –≤ –º–µ—Ç–∫–∞—Ö, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –Ω–µ –º–æ–∂–µ–º –∏—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ –≤ —Ç–µ–∫—Å—Ç
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE –æ–∂–∏–¥–∞–µ—Ç —Å–∏–º–≤–æ–ª –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ ROUGE
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

–î–∞–ª–µ–µ –Ω–∞–º –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ü–æ—Å–∫–æ–ª—å–∫—É mT5 —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä, –æ–¥–Ω–∞ –∏–∑ —Ç–æ–Ω–∫–æ—Å—Ç–µ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –Ω–∞—à–∏—Ö –±–∞—Ç—á–µ–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–≤–∏–Ω—É—Ç—å –º–µ—Ç–∫–∏ –≤–ø—Ä–∞–≤–æ –Ω–∞ –µ–¥–∏–Ω–∏—Ü—É. –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–µ–∫–æ–¥–µ—Ä –≤–∏–¥–µ–ª —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–∫–∏, –∞ –Ω–µ —Ç–µ–∫—É—â–∏–µ –∏–ª–∏ –±—É–¥—É—â–∏–µ, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–æ –±—ã –ª–µ–≥–∫–æ –∑–∞–ø–æ–º–Ω–∏—Ç—å. –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ç–æ, –∫–∞–∫ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –≤ –∑–∞–¥–∞—á–µ —Ç–∏–ø–∞ [–∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è](../chapter7/6).

–ö —Å—á–∞—Å—Ç—å—é, ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–ª–ª–∞—Ç–æ—Ä `DataCollatorForSeq2Seq`, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–µ—Ç–∫–∏ –∑–∞ –Ω–∞—Å. –ß—Ç–æ–±—ã –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∫–æ–ª–ª–∞—Ç–æ—Ä, –Ω–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å `tokenizer` –∏ `model`:

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –≤—ã–¥–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–ª–ª–∞—Ç–æ—Ä, –∫–æ–≥–¥–∞ –µ–º—É –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –Ω–µ–±–æ–ª—å—à–æ–π –±–∞—Ç—á –ø—Ä–∏–º–µ—Ä–æ–≤. –í–æ-–ø–µ—Ä–≤—ã—Ö, –Ω–∞–º –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ª–±—Ü—ã —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–æ–ª–ª–∞—Ç–æ—Ä –Ω–µ –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å, –∫–∞–∫ –≤—Å—Ç–∞–≤–ª—è—Ç—å —ç—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

–ü–æ—Å–∫–æ–ª—å–∫—É –∫–æ–ª–ª–∞—Ç–æ—Ä –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π `dict`, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å `dict` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –Ω–∞–º —Ç–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∫ –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É, –ø—Ä–µ–∂–¥–µ —á–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏—Ö –∫–æ–ª–ª–∞—Ç–æ—Ä—É:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

–ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –∑–¥–µ—Å—å –Ω—É–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, - —ç—Ç–æ —Ç–æ, —á—Ç–æ –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –¥–ª–∏–Ω–Ω–µ–µ –≤—Ç–æ—Ä–æ–≥–æ, –ø–æ—ç—Ç–æ–º—É `input_ids` –∏ `attention_mask` –≤—Ç–æ—Ä–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –±—ã–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω—ã —Å–ø—Ä–∞–≤–∞ —Ç–æ–∫–µ–Ω–æ–º `[PAD]` (—á–µ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ä–∞–≤–µ–Ω `0`). –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –º—ã –≤–∏–¥–∏–º, —á—Ç–æ `labels` –±—ã–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–µ–º `-100`, —á—Ç–æ–±—ã —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª–∞ —Ç–æ–∫–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è. –ò –Ω–∞–∫–æ–Ω–µ—Ü, –º—ã –≤–∏–¥–∏–º –Ω–æ–≤—ã–π `decoder_input_ids`, –≤ –∫–æ—Ç–æ—Ä–æ–º –º–µ—Ç–∫–∏ —Å–¥–≤–∏–Ω—É—Ç—ã –≤–ø—Ä–∞–≤–æ –∑–∞ —Å—á–µ—Ç –≤—Å—Ç–∞–≤–∫–∏ —Ç–æ–∫–µ–Ω–∞ `[PAD]` –≤ –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å.

{#if fw === 'pt'}

–ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è! –¢–µ–ø–µ—Ä—å –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞—Ç—å —Ç—Ä–µ–Ω–µ—Ä —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞—à —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:

```python
trainer.train()
```

–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤—ã –¥–æ–ª–∂–Ω—ã –≤–∏–¥–µ—Ç—å, –∫–∞–∫ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —É–º–µ–Ω—å—à–∞—é—Ç—Å—è, –∞ –æ—Ü–µ–Ω–∫–∞ ROUGE —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–æ–π. –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É ROUGE, –≤—ã–ø–æ–ª–Ω–∏–≤ –∫–æ–º–∞–Ω–¥—É `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

–ò–∑ –æ—Ü–µ–Ω–æ–∫ –≤–∏–¥–Ω–æ, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–∑–æ—à–ª–∞ –±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å lead-3 - –æ—Ç–ª–∏—á–Ω–æ! –û—Å—Ç–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –≤ Hub, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∏–∂–µ:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É –∏ —Ñ–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ `output_dir`, –∞ –∑–∞—Ç–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –•–∞–±. –£–∫–∞–∑–∞–≤ –∞—Ä–≥—É–º–µ–Ω—Ç `tags`, –º—ã —Ç–∞–∫–∂–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –≤–∏–¥–∂–µ—Ç –Ω–∞ —Ö–∞–±–µ –±—É–¥–µ—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, –∞ –Ω–µ –¥–ª—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π mT5 (–±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–≥–∞—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ [ü§ó –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ Hub](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). –í—ã–≤–æ–¥ `trainer.push_to_hub()` - —ç—Ç–æ URL –Ω–∞ —Ö—ç—à Git-–∫–æ–º–º–∏—Ç–∞, —Ç–∞–∫ —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ –ª–µ–≥–∫–æ —É–≤–∏–¥–µ—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–¥–µ–ª–∞–Ω—ã –≤ —Ä–æ–∑–∏—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–∏!

–í –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å mT5 —Å –ø–æ–º–æ—â—å—é –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã—Ö ü§ó Accelerate.

{:else}

–ú—ã –ø–æ—á—Ç–∏ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é! –ù–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–∞—à–∏ –¥–∞—Ç–∞—Å–µ—Ç—ã –≤ `tf.data.Dataset` —Å –ø–æ–º–æ—â—å—é –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –≤—ã—à–µ, –∞ –∑–∞—Ç–µ–º –≤—ã–æ–ª–Ω–∏—Ç—å `compile()` –∏ `fit()` –º–æ–¥–µ–ª–∏. –°–Ω–∞—á–∞–ª–∞ –¥–∞—Ç–∞—Å–µ—Ç—ã:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

–¢–µ–ø–µ—Ä—å –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –∏ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ–º:

```python
from transformers import create_optimizer
import tensorflow as tf

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è - —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω–æ–µ –Ω–∞ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, –∑–∞—Ç–µ–º —É–º–Ω–æ–∂–µ–Ω–Ω–æ–µ
# –Ω–∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ tf_train_dataset –∑–¥–µ—Å—å - —ç—Ç–æ –±–∞—Ç—á tf.data.Dataset,
# –∞ –Ω–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç Hugging Face, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ len() —É–∂–µ —Ä–∞–≤–µ–Ω num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# –û–±—É—á–µ–Ω–∏–µ —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

–ò –Ω–∞–∫–æ–Ω–µ—Ü, –º—ã –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `PushToHubCallback` –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ Hub –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –ø–æ–∑–∂–µ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

–ú—ã –ø–æ–ª—É—á–∏–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –Ω–∞–º —Ö–æ—Ç–µ–ª–æ—Å—å –±—ã —É–≤–∏–¥–µ—Ç—å –º–µ—Ç—Ä–∏–∫—É ROUGE, –∫–æ—Ç–æ—Ä—É—é –º—ã –≤—ã—á–∏—Å–ª—è–ª–∏ —Ä–∞–Ω–µ–µ. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —ç—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ —Å—Ç—Ä–æ–∫–∏. –î–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–≤ –º–µ—Ç–æ–∫ –∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –º–µ—Ç—Ä–∏–∫–æ–π ROUGE (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –µ—Å–ª–∏ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ, –≤–∞–º –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è –∫–æ–º–∞–Ω–¥–∞ `!pip install tqdm`). –ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—Ä—é–∫, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, - –∫–æ–º–ø–∏–ª—è—Ü–∏—é –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é [XLA](https://www.tensorflow.org/xla), —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã TensorFlow. XLA –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫ –≥—Ä–∞—Ñ—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –ö–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ Hugging Face [–±–ª–æ–≥–µ](https://huggingface.co/blog/tf-xla-generate), XLA —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –≤—Å–µ–≥–æ, –∫–æ–≥–¥–∞ —Ñ–æ—Ä–º—ã –Ω–∞—à–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è. –ß—Ç–æ–±—ã —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º, –º—ã –¥–æ–ø–æ–ª–Ω–∏–º –Ω–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ –∫—Ä–∞—Ç–Ω—ã—Ö 128 –∏ —Å–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –ø–æ–º–æ—â—å—é –¥–æ–ø–æ–ª–Ω—è—é—â–µ–≥–æ –∫–æ–ª–ª–∞—Ç–æ—Ä–∞, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏–º –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä `@tf.function(jit_compile=True)` –∫ –Ω–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–µ—á–∞–µ—Ç –≤—Å—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é XLA.

```python
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

–ö–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å —Å–ø–∏—Å–∫–∏ —Å—Ç—Ä–æ–∫ –º–µ—Ç–æ–∫ –∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, –≤—ã—á–∏—Å–ª–∏—Ç—å –æ—Ü–µ–Ω–∫—É ROUGE –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ:

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ mT5 —Å ü§ó Accelerate[[fine-tuning-mt5-with-accelerate]]

–î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é ü§ó Accelerate –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1). –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è –∑–∞–∫–ª—é—á–∞—é—Ç—Å—è –≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —è–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–ø–æ—Å–æ–±–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ ROUGE (–Ω–∞–ø–æ–º–Ω–∏–º, —á—Ç–æ `Seq2SeqTrainer` –ø–æ–∑–∞–±–æ—Ç–∏–ª—Å—è –æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞ –Ω–∞—Å). –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –º—ã –º–æ–∂–µ–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –¥–≤–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤ ü§ó Accelerate!

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ–≥–æ –∫ –æ–±—É—á–µ–Ω–∏—é[[preparing-everything-for-training]]

–ü–µ—Ä–≤–æ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ —Å–æ–∑–¥–∞—Ç—å `DataLoader` –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –Ω–∞—à–∏—Ö —á–∞—Å—Ç–µ–π. –ü–æ—Å–∫–æ–ª—å–∫—É –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö PyTorch –æ–∂–∏–¥–∞—é—Ç –±–∞—Ç—á —Ç–µ–Ω–∑–æ—Ä–æ–≤, –Ω–∞–º –Ω—É–∂–Ω–æ –∑–∞–¥–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç `"torch"` –≤ –Ω–∞—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö:

```python
tokenized_datasets.set_format("torch")
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Ç–µ–Ω–∑–æ—Ä–æ–≤, —Å–ª–µ–¥—É—é—â–µ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, - —ç—Ç–æ —Å–Ω–æ–≤–∞ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å `DataCollatorForSeq2Seq`. –î–ª—è —ç—Ç–æ–≥–æ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Å–≤–µ–∂—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ —Å–Ω–æ–≤–∞ –∑–∞–≥—Ä—É–∑–∏–º –µ–µ –∏–∑ –Ω–∞—à–µ–≥–æ –∫—ç—à–∞:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—à–∏—Ö –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

–°–ª–µ–¥—É—é—â–µ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å. –ö–∞–∫ –∏ –≤ –¥—Ä—É–≥–∏—Ö –Ω–∞—à–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `AdamW`, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –ø–µ—Ä–µ–¥–∞–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –º–µ—Ç–æ–¥ `accelerator.prepare()`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

üö® –ï—Å–ª–∏ –≤—ã –æ–±—É—á–∞–µ—Ç–µ –Ω–∞ TPU, –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤–µ—Å—å –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –≤—ã—à–µ –∫–æ–¥ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è. –ü–æ–¥—Ä–æ–±–Ω–µ–µ —Å–º–æ—Ç—Ä–∏—Ç–µ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1).

</Tip>

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –Ω–∞—à–∏ –æ–±—ä–µ–∫—Ç—ã, –æ—Å—Ç–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å —Ç—Ä–∏ –≤–µ—â–∏:

* –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.
* –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—é–º–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏.
* –°–æ–∑–¥–∞—Ç—å —Ä–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ Hub, –≤ –∫–æ—Ç–æ—Ä—ã–π –º—ã –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å.

–í –∫–∞—á–µ—Å—Ç–≤–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

–î–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–º –Ω—É–∂–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª–∞–º–∏ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏. –ò–º–µ–Ω–Ω–æ —Ç–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –æ–∂–∏–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∞ ROUGE, –∏ –º—ã –º–æ–∂–µ–º –¥–æ—Å—Ç–∏—á—å —ç—Ç–æ–≥–æ —Å –ø–æ–º–æ—â—å—é —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∫–æ–¥–∞:

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE –æ–∂–∏–¥–∞–µ—Ç —Å–∏–º–≤–æ–ª –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

–≠—Ç–æ –¥–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å—Å—è –≤–∞–º –∑–Ω–∞–∫–æ–º—ã–º, –µ—Å–ª–∏ –≤—ã –ø–æ–º–Ω–∏—Ç–µ, –∫–∞–∫ –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–ª–∏ —Ñ—É–Ω–∫—Ü–∏—é `compute_metrics()` –¥–ª—è `Seq2SeqTrainer`.

–ù–∞–∫–æ–Ω–µ—Ü, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ä–æ–∑–∏—Ç–æ—Ä–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face Hub. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É ü§óHub —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–º. –ù–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—Ç—å –∏–º—è –Ω–∞—à–µ–≥–æ —Ä–æ–∑–∏—Ç–æ—Ä–∏—è, –∞ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –µ—Å—Ç—å —Å–ª—É–∂–µ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ä–æ–∑–∏—Ç–æ—Ä–∏—è —Å –ø—Ä–æ—Ñ–∏–ª–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–º—è —ç—Ç–æ–≥–æ —Ä–æ–∑–∏—Ç–æ—Ä–∏—è –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Hub, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `repo.push_to_hub()` –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è! –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –∑–∞–≤–µ—Ä—à–∏–º –Ω–∞—à –∞–Ω–∞–ª–∏–∑, –Ω–∞–ø–∏—Å–∞–≤ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è.

### –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è[[training-loop]]

–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ –¥—Ä—É–≥–∏–µ –ø—Ä–∏–º–µ—Ä—ã ü§ó Accelerate, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã —Å—Ç–∞–ª–∫–∏–≤–∞–ª–∏—Å—å, –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤:

1. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –≤—Å–µ–º –ø—Ä–∏–º–µ—Ä–∞–º –≤ `train_dataloader` –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ.
2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –º–æ–¥–µ–ª—å—é –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏, —Å–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è —Ç–æ–∫–µ–Ω—ã, –∞ –∑–∞—Ç–µ–º –æ–Ω–∏ (–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ) –¥–µ–∫–æ–¥–∏—Ä—É—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç.
3. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ ROUGE —Å –ø–æ–º–æ—â—å—é —Ç–µ—Ö –∂–µ –ø—Ä–∏–µ–º–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ —Ä–∞–Ω–µ–µ.
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤—Å–µ–≥–æ –≤ Hub. –ó–¥–µ—Å—å –º—ã –ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç `blocking=False` –æ–±—ä–µ–∫—Ç–∞ `Repository`, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ _–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ_. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –Ω–µ –¥–æ–∂–∏–¥–∞—è—Å—å –º–µ–¥–ª–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏, —Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –º–æ–¥–µ–ª—å—é —Ä–∞–∑–º–µ—Ä–æ–º –≤ –≥–∏–≥–∞–±–∞–π—Ç!

–≠—Ç–∏ —à–∞–≥–∏ –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å –≤ —Å–ª–µ–¥—É—é—â–µ–º –±–ª–æ–∫–µ –∫–æ–¥–∞:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # –û–±—É—á–µ–Ω–∏–µ
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # –û—Ü–µ–Ω–∫–∞
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # –ï—Å–ª–∏ –º—ã –Ω–µ –¥–æ–ø–æ–ª–Ω–∏–ª–∏ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã, –Ω–∞–º –Ω—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –∏ –º–µ—Ç–∫–∏
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # –ó–∞–º–µ–Ω—è–µ–º -100 –≤ –º–µ—Ç–∫–∞—Ö, –ø–æ—Å–∫–æ–ª—å–∫—É –º—ã –Ω–µ –º–æ–∂–µ–º –∏—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    result = rouge_score.compute()
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ ROUGE
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

–í–æ—Ç –∏ –≤—Å–µ! –ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ —É –≤–∞—Å –±—É–¥–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Ç–µ, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é `Trainer`.

{/if}

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –≤–∞–º–∏ –º–æ–¥–µ–ª–∏[[using-your-fine-tuned-model]]

–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –≤—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª—å –≤ Hub, –≤—ã –º–æ–∂–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ–π –ª–∏–±–æ —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–∂–µ—Ç–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –ª–∏–±–æ —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–∫—Ç–∞ `pipeline`, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∏–∂–µ:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

–ú—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –Ω–∞—à –∫–æ–Ω–≤–µ–π–µ—Ä –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –Ω–µ –≤–∏–¥–µ–ª–∞), —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–∑—é–º–µ. –î–ª—è –Ω–∞—á–∞–ª–∞ –¥–∞–≤–∞–π—Ç–µ —Ä–µ–∞–ª–∏–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ–±–∑–æ—Ä, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –≤–º–µ—Å—Ç–µ:

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ–¥–∏–Ω –∏–∑ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø–æ–ª—É—á–∞–µ–º:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn‚Äôt come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It‚Äôs also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

–≠—Ç–æ –Ω–µ —Ç–∞–∫ —É–∂ –ø–ª–æ—Ö–æ! –ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–ø–æ—Å–æ–±–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—Ç—å _–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—É—é_ —Å—É–º–º—É—Ä–∏–∑–∞—Ü–∏—é, –¥–æ–ø–æ–ª–Ω—è—è —á–∞—Å—Ç–∏ –æ–±–∑–æ—Ä–∞ –Ω–æ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏. –ò, –ø–æ–∂–∞–ª—É–π, —Å–∞–º—ã–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∞—Å–ø–µ–∫—Ç –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ - —ç—Ç–æ —Ç–æ, —á—Ç–æ –æ–Ω–∞ –±–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è, —Ç–∞–∫ —á—Ç–æ –º—ã –º–æ–∂–µ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ –∏ –¥–ª—è –∏—Å–ø–∞–Ω—Å–∫–∏—Ö —Ä–µ—Ü–µ–Ω–∑–∏–π:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

–†–µ–∑—é–º–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ "Very easy to read" –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, —á—Ç–æ, –∫–∞–∫ –º—ã –≤–∏–¥–∏–º, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –±—ã–ª–æ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤–∑—è—Ç–æ –∏–∑ –æ–±–∑–æ—Ä–∞. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, —ç—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ mT5 –∏ –¥–∞–µ—Ç –≤–∞–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –∫–∞–∫–æ–≤–æ —ç—Ç–æ - —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º!

–î–∞–ª–µ–µ –º—ã –æ–±—Ä–∞—Ç–∏–º—Å—è –∫ –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ: –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è.
