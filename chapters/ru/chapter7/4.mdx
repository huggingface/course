<FrameworkSwitchCourse {fw} />

# –ü–µ—Ä–µ–≤–æ–¥[[translation]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section4_tf.ipynb"},
]} />

{/if}

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ–≥—Ä—É–∑–∏–º—Å—è –≤ –ø–µ—Ä–µ–≤–æ–¥. –≠—Ç–æ –µ—â–µ –æ–¥–Ω–∞ [–∑–∞–¥–∞—á–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (sequence-to-sequence)](../chapter1/7), —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –¥—Ä—É–≥–æ–π. –í —ç—Ç–æ–º —Å–º—ã—Å–ª–µ –∑–∞–¥–∞—á–∞ –æ—á–µ–Ω—å –±–ª–∏–∑–∫–∞ –∫ [—Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏—é (summarization)](../chapter7/6), –∏ –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–∏—Å–ø–æ—Å–æ–±–∏—Ç—å —Ç–æ, —á—Ç–æ –º—ã –∑–¥–µ—Å—å —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫ –¥—Ä—É–≥–∏–º –∑–∞–¥–∞—á–∞–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Ç–∞–∫–∏–º –∫–∞–∫:

- **–ü–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—è (Style transfer)**: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è *–ø–µ—Ä–µ–≤–æ–¥–∏—Ç* —Ç–µ–∫—Å—Ç—ã, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–µ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —Å—Ç–∏–ª–µ, –≤ –¥—Ä—É–≥–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–π –∏–ª–∏ –∏–∑ —à–µ–∫—Å–ø–∏—Ä–æ–≤—Å–∫–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π).
- **–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã (Generative question answering)**: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —É—á–∏—Ç—ã–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç

<Youtube id="1JvfrvZgi6c"/>

–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –¥–≤—É—Ö (–∏–ª–∏ –±–æ–ª–µ–µ) —è–∑—ã–∫–∞—Ö, –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –Ω—É–ª—è, –∫–∞–∫ –º—ã —ç—Ç–æ —Å–¥–µ–ª–∞–µ–º –≤ —Ä–∞–∑–¥–µ–ª–µ –ø–æ [–∫–∞–∑—É–∞–ª—å–Ω–æ–º—É —è–∑—ã–∫–æ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é (causal language modeling)](../chapter7/6). –û–¥–Ω–∞–∫–æ –±—ã—Å—Ç—Ä–µ–µ –±—É–¥–µ—Ç –¥–æ–æ–±—É—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞, –±—É–¥—å —Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ mT5 –∏–ª–∏ mBART, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã —è–∑—ã–∫–æ–≤, –∏–ª–∏ –¥–∞–∂–µ –º–æ–¥–µ–ª—å, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –æ–¥–Ω–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ –¥—Ä—É–≥–æ–π, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞.

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –¥–æ–æ–±—É—á–∏–º –º–æ–¥–µ–ª—å Marian, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –ø–µ—Ä–µ–≤–æ–¥—É —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π (–ø–æ—Å–∫–æ–ª—å–∫—É –º–Ω–æ–≥–∏–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ Hugging Face –≥–æ–≤–æ—Ä—è—Ç –Ω–∞ –æ–±–æ–∏—Ö —ç—Ç–∏—Ö —è–∑—ã–∫–∞—Ö), –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ [KDE4](https://huggingface.co/datasets/kde4), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π [KDE](https://apps.kde.org/). –ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏—Ö –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, –≤–∑—è—Ç—ã—Ö –∏–∑ [Opus dataset](https://opus.nlpl.eu/), –∫–æ—Ç–æ—Ä—ã–π —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞—Ç–∞—Å–µ—Ç KDE4. –ù–æ –¥–∞–∂–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º, –≤–∏–¥–µ–ª–∞ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –º—ã —É–≤–∏–¥–∏–º, —á—Ç–æ –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º—ã —Å–º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –µ–µ –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é.

–ö–æ–≥–¥–∞ –º—ã –∑–∞–∫–æ–Ω—á–∏–º, —É –Ω–∞—Å –±—É–¥–µ—Ç –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã, –ø–æ–¥–æ–±–Ω—ã–µ —ç—Ç–æ–º—É:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

–ö–∞–∫ –∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö, –≤—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–±—É—á–∏–º –∏ –∑–∞–≥—Ä—É–∑–∏–º –Ω–∞ Hub, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–¥, –∏ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è [–∑–¥–µ—Å—å](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö[[preparing-the-data]]

–ß—Ç–æ–±—ã –¥–æ–æ–±—É—á–∏—Ç—å –∏–ª–∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –Ω—É–ª—è, –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏. –ö–∞–∫ —É–∂–µ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [–¥–∞—Ç–∞—Å–µ—Ç KDE4](https://huggingface.co/datasets/kde4) , –Ω–æ –≤—ã –º–æ–∂–µ—Ç–µ –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–≤–æ–∏—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –¥–≤—É—Ö —è–∑—ã–∫–∞—Ö, —Å –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ [–ì–ª–∞–≤–µ 5](../chapter5/1) –µ—Å–ª–∏ –≤–∞–º –Ω—É–∂–Ω–æ –≤—Å–ø–æ–º–Ω–∏—Ç—å, –∫–∞–∫ –∑–∞–≥—Ä—É–∂–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ `Dataset`.

### –î–∞—Ç–∞—Å–µ—Ç KDE4[[the-kde4-dataset]]

–ö–∞–∫ –æ–±—ã—á–Ω–æ, –º—ã –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥—Ä—É–≥–æ–π –ø–∞—Ä–æ–π —è–∑—ã–∫–æ–≤, –≤—ã –º–æ–∂–µ—Ç–µ —É–∫–∞–∑–∞—Ç—å –∏—Ö –ø–æ –∫–æ–¥–∞–º. –í—Å–µ–≥–æ –¥–ª—è —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ—Å—Ç—É–ø–Ω–æ 92 —è–∑—ã–∫–∞; –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –∏—Ö –≤—Å–µ, —Ä–∞–∑–≤–µ—Ä–Ω—É–≤ —è–∑—ã–∫–æ–≤—ã–µ —Ç–µ–≥–∏ –≤ –µ–≥–æ [–∫–∞—Ä—Ç–æ—á–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞](https://huggingface.co/datasets/kde4).

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

–£ –Ω–∞—Å –µ—Å—Ç—å 210 173 –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –Ω–æ –≤ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä. –ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 5](../chapter5/1), —É `Dataset` –µ—Å—Ç—å –º–µ—Ç–æ–¥ `train_test_split()`, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –Ω–∞–º –ø–æ–º–æ—á—å. –ú—ã –∑–∞–¥–∞–¥–∏–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

–ú—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –∫–ª—é—á `"test"` –≤ `"validation"` —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par d√©faut, d√©velopper les fils de discussion'}
```

–ú—ã –ø–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–≤—É–º—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –Ω–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–π –ø–∞—Ä–µ —è–∑—ã–∫–æ–≤. –û–¥–Ω–∞ –∏–∑ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –Ω–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –≤—Å–µ –æ–Ω–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π —è–∑—ã–∫. –û–¥–Ω–∞–∫–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–µ –∏–Ω–∂–µ–Ω–µ—Ä—ã –ø—Ä–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ –æ—Å—Ç–∞–≤–ª—è—é—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫ —Å–ª–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–æ "threads" –≤–ø–æ–ª–Ω–µ –º–æ–∂–µ—Ç –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å—Å—è –≤–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–µ; –Ω–æ –≤ –¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –æ–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –∫–∞–∫ –±–æ–ª–µ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ "fils de discussion". –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –Ω–∞–º–∏ –º–æ–¥–µ–ª—å, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –±–æ–ª—å—à–µ–º –∫–æ—Ä–ø—É—Å–µ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏—Ö –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤—ã–±–∏—Ä–∞–µ—Ç –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç - –æ—Å—Ç–∞–≤–∏—Ç—å —Å–ª–æ–≤–æ –∫–∞–∫ –µ—Å—Ç—å:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par d√©faut pour les threads √©largis'}]
```

–î—Ä—É–≥–æ–π –ø—Ä–∏–º–µ—Ä —Ç–∞–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ —Å–ª–æ–≤–∞ "plugin", –∫–æ—Ç–æ—Ä–æ–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–º —Å–ª–æ–≤–æ–º, –Ω–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –Ω–æ—Å–∏—Ç–µ–ª–µ–π —è–∑—ã–∫–∞ –ø–æ–π–º—É—Ç –µ–≥–æ –∏ –Ω–µ —Å—Ç–∞–Ω—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å.
–í –¥–∞—Ç–∞—Å–µ—Ç–µ KDE4 —ç—Ç–æ —Å–ª–æ–≤–æ –±—ã–ª–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π –∫–∞–∫ –±–æ–ª–µ–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ "module d'extension":

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

–û–¥–Ω–∞–∫–æ –Ω–∞—à–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –∏ –∑–Ω–∞–∫–æ–º–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

–ë—É–¥–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, —Å–º–æ–∂–µ—Ç –ª–∏ –Ω–∞—à–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–ª–æ–≤–∏—Ç—å —ç—Ç–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Å–ø–æ–π–ª–µ—Ä: —Å–º–æ–∂–µ—Ç).

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ï—â–µ –æ–¥–Ω–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ —Å–ª–æ–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ, - "email". –ù–∞–π–¥–∏—Ç–µ –≤ –æ–±—É—á–∞—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–µ—Ä–≤—ã–π –æ–±—Ä–∞–∑–µ—Ü, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ. –ö–∞–∫ –æ–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è? –ö–∞–∫ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–æ –∂–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ?

</Tip>

### –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö[[processing-the-data]]

<Youtube id="XAR8jnZZuUs"/>

–í—ã —É–∂–µ –¥–æ–ª–∂–Ω—ã –∑–Ω–∞—Ç—å, –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è: –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –Ω–∞–±–æ—Ä—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –ø–æ–Ω—è—Ç—å –∏—Ö —Å–º—ã—Å–ª. –î–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ —Ü–µ–ª–µ–≤—ã—Ö. –ù–∞—à–∞ –ø–µ—Ä–≤–∞—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `tokenizer`. –ö–∞–∫ –æ—Ç–º–µ—á–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å Marian English to French. –ï—Å–ª–∏ –≤—ã –±—É–¥–µ—Ç–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∫–æ–¥ —Å –¥—Ä—É–≥–æ–π –ø–∞—Ä–æ–π —è–∑—ã–∫–æ–≤, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏. –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç—ã—Å—è—á–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –∑–∞–º–µ–Ω–∏—Ç—å `model_checkpoint` –Ω–∞ –ª—é–±—É—é –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –∏–∑ [Hub](https://huggingface.co/models) –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

<Tip>

üí° –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–Ω–æ–≥–æ—è–∑—ã–∫–æ–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —Ç–∞–∫–æ–π –∫–∞–∫ mBART, mBART-50 –∏–ª–∏ M2M100, –≤–∞–º –Ω—É–∂–Ω–æ –∑–∞–¥–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –∫–æ–¥—ã –≤–∞—à–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ, –∑–∞–¥–∞–≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º `tokenizer.src_lang` –∏ `tokenizer.tgt_lang`.

</Tip>

–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–≤–æ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞. –ù—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–± –æ–¥–Ω–æ–º: –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ (–∑–¥–µ—Å—å - —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º). –í—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ, –ø–µ—Ä–µ–¥–∞–≤ —Ü–µ–ª–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç `text_targets` –º–µ—Ç–æ–¥–∞ `__call__` —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

–ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –¥–∞–≤–∞–π—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞ –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, –≤ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤—Ö–æ–¥–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–º, —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–æ–ª–µ `labels`. –ï—Å–ª–∏ –≤—ã –∑–∞–±—É–¥–µ—Ç–µ —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –º–µ—Ç–æ–∫, –æ–Ω–∏ –±—É–¥—É—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤—Ö–æ–¥–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º, —á—Ç–æ –≤ —Å–ª—É—á–∞–µ —Å –º–æ–¥–µ–ª—å—é Marian –Ω–µ –ø—Ä–∏–≤–µ–¥–µ—Ç –Ω–∏ –∫ —á–µ–º—É —Ö–æ—Ä–æ—à–µ–º—É:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['‚ñÅPar', '‚ñÅd√©', 'f', 'aut', ',', '‚ñÅd√©', 've', 'lop', 'per', '‚ñÅles', '‚ñÅfil', 's', '‚ñÅde', '‚ñÅdiscussion', '</s>']
['‚ñÅPar', '‚ñÅd√©faut', ',', '‚ñÅd√©velopper', '‚ñÅles', '‚ñÅfils', '‚ñÅde', '‚ñÅdiscussion', '</s>']
```

–ö–∞–∫ –º—ã –≤–∏–¥–∏–º, –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–ª—É—á–∞–µ—Ç—Å—è –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –ø–æ—Å–∫–æ–ª—å–∫—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –∑–Ω–∞–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞ (–∫—Ä–æ–º–µ —Ç–µ—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–∫–∂–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä "discussion").

–ü–æ—Å–∫–æ–ª—å–∫—É `inputs` - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –Ω–∞—à–∏–º–∏ –æ–±—ã—á–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏ (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Ç. –¥.), –ø–æ—Å–ª–µ–¥–Ω–∏–º —à–∞–≥–æ–º –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—É—é –º—ã –±—É–¥–µ–º –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º:

```python
max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è –Ω–∞—à–∏—Ö –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤. –ü–æ—Å–∫–æ–ª—å–∫—É —Ç–µ–∫—Å—Ç—ã, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ, –¥–æ–≤–æ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–µ, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º 128.

<Tip>

üí° –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–æ–¥–µ–ª—å T5 (—Ç–æ—á–Ω–µ–µ, –æ–¥–Ω—É –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ `t5-xxx`), –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–∂–∏–¥–∞—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –∏–º–µ—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É, –Ω–∞–ø—Ä–∏–º–µ—Ä `translate: English to French:`.

</Tip>

<Tip warning={true}>

‚ö†Ô∏è –ú—ã –Ω–µ –æ–±—Ä–∞—â–∞–µ–º –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å –Ω–µ –±—É–¥–µ—Ç —ç—Ç–æ–≥–æ –æ–∂–∏–¥–∞—Ç—å. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º–µ—Ç–∫–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–∫–µ–Ω–∞–º –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è, –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–¥–∞–Ω—ã –∫–∞–∫ `-100`, —á—Ç–æ–±—ã –æ–Ω–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ç–µ—Ä—å. –≠—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –Ω–∞—à–∏–º –∫–æ–ª–ª–∞—Ç–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–∂–µ, —Ç–∞–∫ –∫–∞–∫ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ (dynamic padding), –Ω–æ –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ (padding) –∑–¥–µ—Å—å, –≤—ã –¥–æ–ª–∂–Ω—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –º–µ—Ç–∫–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—É –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è, –≤ `-100`.

</Tip>

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ –≤—Å–µ–º —á–∞—Å—Ç—è–º –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –º—ã –≥–æ—Ç–æ–≤—ã –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞—à—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å!

{#if fw === 'pt'}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é API `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π `Trainer`, –±—É–¥–µ—Ç —Ç–∞–∫–∏–º –∂–µ, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ, —Å –æ–¥–Ω–∏–º –ª–∏—à—å –Ω–µ–±–æ–ª—å—à–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º: –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer), –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥–∫–ª–∞—Å—Å–æ–º `Trainer`, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ—Ü–µ–Ω–∫–æ–π, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ `generate()` –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã—Ö–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–æ–≤. –ú—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —ç—Ç–æ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ, –∫–æ–≥–¥–∞ –±—É–¥–µ–º –≥–æ–≤–æ—Ä–∏—Ç—å –æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫.

–ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, –Ω–∞–º –Ω—É–∂–Ω–∞ —Ä–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å. –ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –æ–±—ã—á–Ω—ã–º API `AutoModel`:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å Keras[[fine-tuning-the-model-with-keras]]

–ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ, –Ω–∞–º –Ω—É–∂–Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å. –ú—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –æ–±—ã—á–Ω—ã–º API `AutoModel`:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

üí° –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ `Helsinki-NLP/opus-mt-en-fr` –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞ PyTorch, –ø–æ—ç—Ç–æ–º—É
–≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ—à–∏–±–∫—É, –µ—Å–ª–∏ –ø–æ–ø—ã—Ç–∞–µ—Ç–µ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞—Ä–≥—É–º–µ–Ω—Ç–∞
`from_pt=True` –≤ –º–µ—Ç–æ–¥–µ `from_pretrained()`. –ö–æ–≥–¥–∞ –≤—ã —É–∫–∞–∑—ã–≤–∞–µ—Ç–µ
`from_pt=True`, –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç
–≤–µ—Å–∞ –∏–∑ PyTorch –¥–ª—è –≤–∞—Å. –ö–∞–∫ –≤–∏–¥–∏—Ç–µ, –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É
—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏ –≤ ü§ó Transformers!

</Tip>

{/if}

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤ —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —É–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞, –ø–æ—ç—Ç–æ–º—É –Ω–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–æ–≤—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Å–∞—Ö.

### –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö[[data-collation]]

–î–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –±–∞—Ç—á–∞ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º (padding). –ú—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `DataCollatorWithPadding`, –∫–∞–∫ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1), –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –±—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã —Ç–æ–ª—å–∫–æ –≤—Ö–æ–¥—ã (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤—Ö–æ–¥–æ–≤, –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤). –ù–∞—à–∏ –º–µ—Ç–∫–∏ —Ç–∞–∫–∂–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã, –≤—Å—Ç—Ä–µ—á–∞—é—â–µ–π—Å—è –≤ –º–µ—Ç–∫–∞—Ö. –ò, –∫–∞–∫ —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å, –¥–æ–±–æ–≤–ª—è–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫, –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å `-100`, –∞ –Ω–µ –¥–æ–±–∞–≤–æ—á–Ω—ã–π —Ç–æ–∫–µ–Ω —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ–±—ã —ç—Ç–∏ –¥–æ–±–∞–≤–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ç–µ—Ä—å.

–í—Å–µ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). –ö–∞–∫ –∏ `DataCollatorWithPadding`, –æ–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç `tokenizer`, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ `model`. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∂–µ –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –≤—Ö–æ–¥–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–µ–∫–æ–¥–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Å–¥–≤–∏–Ω—É—Ç—ã–µ –≤–µ—Ä—Å–∏–∏ –º–µ—Ç–æ–∫ —Å–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º –≤ –Ω–∞—á–∞–ª–µ. –ü–æ—Å–∫–æ–ª—å–∫—É –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä —ç—Ç–æ—Ç —Å–¥–≤–∏–≥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É, `DataCollatorForSeq2Seq` –¥–æ–ª–∂–µ–Ω –∑–Ω–∞—Ç—å –æ–±—ä–µ–∫—Ç `model`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

–ß—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö, –º—ã –ø—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º –µ–≥–æ –Ω–∞ —Å–ø–∏—Å–∫–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –Ω–∞—à–µ–≥–æ —Ç–æ–∫–∏–Ω–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

–ú—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –Ω–∞—à–∏ –º–µ—Ç–∫–∏ –±—ã–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –±–∞—Ç—á–∞, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `-100`:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

–¢–∞–∫–∂–µ –º—ã –º–æ–∂–µ–º –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤—Ö–æ–¥–æ–≤ –¥–µ–∫–æ–¥–µ—Ä–∞ –∏ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω–∏ —è–≤–ª—è—é—Ç—Å—è —Å–¥–≤–∏–Ω—É—Ç—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –º–µ—Ç–æ–∫:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

–í–æ—Ç –º–µ—Ç–∫–∏ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

–ú—ã –ø–µ—Ä–µ–¥–∞–¥–∏–º —ç—Ç–æ—Ç `data_collator` –≤ `Seq2SeqTrainer`. –î–∞–ª–µ–µ –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –º–µ—Ç—Ä–∏–∫—É.

{:else}

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç `data_collator` –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∞—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ `tf.data.Dataset`, –≥–æ—Ç–æ–≤—ã–π –∫ –æ–±—É—á–µ–Ω–∏—é:

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### –ú–µ—Ç—Ä–∏–∫–∏[[metrics]]

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

–°–≤–æ–π—Å—Ç–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ `Seq2SeqTrainer` –¥–æ–±–∞–≤–ª—è–µ—Ç –∫ —Å–≤–æ–µ–º—É —Å—É–ø–µ—Ä–∫–ª–∞—Å—Å—É `Trainer`, - —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `generate()` –≤–æ –≤—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏ –∏–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `decoder_input_ids` —Å –º–∞—Å–∫–æ–π –≤–Ω–∏–º–∞–Ω–∏—è, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—â–µ–π, —á—Ç–æ –æ–Ω–∞ –Ω–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –í–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º—ã –Ω–µ —Å–º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –Ω–µ –±—É–¥–µ—Ç –º–µ—Ç–æ–∫, –ø–æ—ç—Ç–æ–º—É –±—ã–ª–æ –±—ã –Ω–µ–ø–ª–æ—Ö–æ –æ—Ü–µ–Ω–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å —Å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π.

–ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 1](../chapter1/6), –¥–µ–∫–æ–¥–µ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è —Ç–æ–∫–µ–Ω—ã –æ–¥–∏–Ω –∑–∞ –¥—Ä—É–≥–∏–º - —Ç–æ, —á—Ç–æ –∑–∞ –∫—É–ª–∏—Å–∞–º–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ ü§ó Transformers –º–µ—Ç–æ–¥–æ–º `generate()`. –¢—Ä–µ–Ω–µ—Ä `Seq2SeqTrainer` –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏, –µ—Å–ª–∏ –º—ã —É—Å—Ç–∞–Ω–æ–≤–∏–º `predict_with_generate=True`.

{/if}

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞, —è–≤–ª—è–µ—Ç—Å—è [BLEU score](https://en.wikipedia.org/wiki/BLEU), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –≤ [—Å—Ç–∞—Ç—å–µ 2002 –≥–æ–¥–∞](https://aclanthology.org/P02-1040.pdf) –ö–∏—à–æ—Ä–æ–º –ü–∞–ø–∏–Ω–µ–Ω–∏ –∏ –¥—Ä. BLEU score –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –±–ª–∏–∑–∫–∏ –ø–µ—Ä–µ–≤–æ–¥—ã –∫ —Å–≤–æ–∏–º –º–µ—Ç–∫–∞–º. –û–Ω –Ω–µ –∏–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–±–æ—Ä—á–∏–≤–æ—Å—Ç—å –∏–ª–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –≤—Å–µ —Å–ª–æ–≤–∞ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Ç–∞–∫–∂–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Ü–µ–ª–µ–≤—ã—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–∞–≤–∏–ª–∞, –Ω–∞–∫–∞–∑—ã–≤–∞—é—â–∏–µ –ø–æ–≤—Ç–æ—Ä—ã –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —Å–ª–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –≤ —Ü–µ–ª–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö (—á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–≤–æ–¥–∏–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–∏–ø–∞ `"the the the the the the the"), –∏ –≤—ã–≤–æ–¥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ—Ä–æ—á–µ, —á–µ–º —Ü–µ–ª–µ–≤—ã–µ (—á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–≤–æ–¥–∏–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–∏–ø–∞ `"the"`).

–û–¥–∏–Ω –∏–∑ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ BLEU –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç —É–∂–µ –ø—Ä–æ—à–µ–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–º–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã. –ü–æ—ç—Ç–æ–º—É —Å–µ–≥–æ–¥–Ω—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞—â–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ [SacreBLEU](https://github.com/mjpost/sacrebleu), –∫–æ—Ç–æ—Ä–∞—è —É—Å—Ç—Ä–∞–Ω—è–µ—Ç —ç—Ç–æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ (–∏ –¥—Ä—É–≥–∏–µ) –ø—É—Ç–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ —ç—Ç–∞–ø–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–µ—Ç—Ä–∏–∫—É, —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É SacreBLEU:

```py
!pip install sacrebleu
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ–µ —Å –ø–æ–º–æ—â—å—é `evaluate.load()`, –∫–∞–∫ –º—ã —ç—Ç–æ –¥–µ–ª–∞–ª–∏ –≤ [–ì–ª–∞–≤–µ 3](../chapter3/1):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

–≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö —Ü–µ–ª–µ–π, —Ç–∞–∫ –∫–∞–∫ —á–∞—Å—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–º –Ω–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω, –Ω–æ –≤ NLP –Ω–µ—Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç—ã, –¥–∞—é—â–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–æ–∫. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Å–æ–±–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∞ —Å—Å—ã–ª–∫–∏ - —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

–ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∏–º–µ—Ä:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

–≠—Ç–æ –¥–∞–µ—Ç –æ—Ü–µ–Ω–∫—É BLEU 46,75, —á—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ —Ö–æ—Ä–æ—à–æ - –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å Transformer –≤ —Å—Ç–∞—Ç—å–µ ["Attention Is All You Need" paper](https://arxiv.org/pdf/1706.03762.pdf) –¥–æ—Å—Ç–∏–≥–ª–∞ –æ—Ü–µ–Ω–∫–∏ BLEU 41,8 –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π! (–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ `counts` –∏ `bp`, –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ [SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74)). –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –µ—Å–ª–∏ –º—ã –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–∞ –ø–ª–æ—Ö–∏—Ö —Ç–∏–ø–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–º–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ), –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –ø–æ–ª—É—á–∞—é—Ç—Å—è –≤ –º–æ–¥–µ–ª—è—Ö –ø–µ—Ä–µ–≤–æ–¥–∞, –º—ã –ø–æ–ª—É—á–∏–º –¥–æ–≤–æ–ª—å–Ω–æ –ø–ª–æ—Ö–∏–µ –æ—Ü–µ–Ω–∫–∏ BLEU:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

–û—Ü–µ–Ω–∫–∞ –º–æ–∂–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –æ—Ç 0 –¥–æ 100, –ø—Ä–∏—á–µ–º —á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ.

{#if fw === 'tf'}

–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∞, –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –º–µ—Ç–æ–¥–æ–º `tokenizer.batch_decode()`. –ù–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –æ—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –∑–Ω–∞–∫–∏ `-100` –≤ –º–µ—Ç–∫–∞—Ö; —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–¥–µ–ª–∞–µ—Ç —Ç–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è –¥–æ–ø–æ–ª–Ω—è—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. –û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±–µ—Ä–µ—Ç –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –¥–∞—Ç–∞—Å–µ—Ç –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –Ω–∞ –Ω–µ–º –º–µ—Ç—Ä–∏–∫–∏. –ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—Ä—é–∫, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, - –∫–æ–º–ø–∏–ª—è—Ü–∏—é –Ω–∞—à–µ–≥–æ –∫–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é [XLA](https://www.tensorflow.org/xla), —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã TensorFlow. XLA –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫ –≥—Ä–∞—Ñ—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –ö–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ –±–ª–æ–≥–µ Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate), XLA –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–æ–≥–¥–∞ –Ω–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ —Ñ–æ—Ä–º—ã –Ω–µ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è. –ß—Ç–æ–±—ã —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å —ç—Ç–∏–º, –º—ã —Ä–∞–∑–¥–µ–ª–∏–º –Ω–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —á–∞—Å—Ç–∏, –∫—Ä–∞—Ç–Ω—ã–µ 128, –∏ —Å–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∫–æ–ª–ª–∞—Ç–æ—Ä–æ–º —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏–º –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä `@tf.function(jit_compile=True)` –∫ –Ω–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –æ–±–æ–∑–Ω–∞—á–∏—Ç –≤—Å—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é XLA.

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )


def compute_metrics():
    all_preds = []
    all_labels = []

    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∞, –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –º–µ—Ç–æ–¥–æ–º `tokenizer.batch_decode()`. –ù–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è `-100` –≤ –º–µ—Ç–∫–∞—Ö (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–¥–µ–ª–∞–µ—Ç —Ç–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è –¥–æ–ø–æ–ª–Ω—è—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # –í —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ª–æ–≥–∏—Ç—ã
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # –ó–∞–º–µ–Ω—è–µ–º -100 –≤ –º–µ—Ç–∫–∞—Ö, —Ç–∞–∫ –∫–∞–∫ –º—ã –Ω–µ –º–æ–∂–µ–º –∏—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # –ù–µ–º–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –≤—Å–µ –≥–æ—Ç–æ–≤–æ, –º—ã –≥–æ—Ç–æ–≤—ã –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å!


### –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏[[fine-tuning-the-model]]

–ü–µ—Ä–≤—ã–π —à–∞–≥ - –≤–æ–π—Ç–∏ –≤ Hugging Face, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Model Hub. –í –±–ª–æ–∫–Ω–æ—Ç–µ –µ—Å—Ç—å —É–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –≤ —ç—Ç–æ–º:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–ü–æ—è–≤–∏—Ç—Å—è –≤–∏–¥–∂–µ—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Hugging Face.

–ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–µ –≤ –±–ª–æ–∫–Ω–æ—Ç–µ, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

–ü—Ä–µ–∂–¥–µ —á–µ–º –Ω–∞—á–∞—Ç—å, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º—ã –ø–æ–ª—É—á–∏–º –æ—Ç –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –∫–∞–∫–æ–≥–æ-–ª–∏–±–æ –æ–±—É—á–µ–Ω–∏—è:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

–ö–∞–∫ —Ç–æ–ª—å–∫–æ —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ, –º—ã —Å–º–æ–∂–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `tf.keras.mixed_precision.set_global_policy("mixed_float16")` - —ç—Ç–æ —É–∫–∞–∂–µ—Ç Keras –æ–±—É—á–∞—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º float16, —á—Ç–æ –º–æ–∂–µ—Ç –¥–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é (Nvidia 20xx/V100 –∏–ª–∏ –Ω–æ–≤–µ–µ).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è - —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω–æ–µ –Ω–∞ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, –∑–∞—Ç–µ–º —É–º–Ω–æ–∂–µ–Ω–Ω–æ–µ
# –Ω–∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ tf_train_dataset –∑–¥–µ—Å—å - —ç—Ç–æ –±–∞—Ç—á tf.data.Dataset,
# –∞ –Ω–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç Hugging Face, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ len() —É–∂–µ —Ä–∞–≤–µ–Ω num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# –û–±—É—á–µ–Ω–∏–µ —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

–î–∞–ª–µ–µ –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π –≤—ã–∑–æ–≤ `PushToHubCallback` –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –≤ Hub –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –∫–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [—Ä–∞–∑–¥–µ–ª–µ 2](../chapter7/2), –∞ –∑–∞—Ç–µ–º –º—ã –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–≥–æ–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `hub_model_id` –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –∏–º—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å (–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é). –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é [`huggingface-course`](https://huggingface.co/huggingface-course), –º—ã –¥–æ–±–∞–≤–∏–ª–∏ `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` –≤ `Seq2SeqTrainingArguments`. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –≤–∞—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º–µ–Ω –∏ –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –≤–∞–º–∏ –≤—ã—Ö–æ–¥–Ω—ã–º –∫–∞—Ç–∞–ª–æ–≥–æ–º, –ø–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å —ç—Ç–æ –±—É–¥–µ—Ç `"sgugger/marian-finetuned-kde4-en-to-fr"` (—ç—Ç–æ –º–æ–¥–µ–ª—å, –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –º—ã —Å—Å—ã–ª–∞–ª–∏—Å—å –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞).

<Tip>

üí° –ï—Å–ª–∏ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ, —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–º –∫–ª–æ–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å push. –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–∞–∫, –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ—à–∏–±–∫—É –ø—Ä–∏ –≤—ã–∑–æ–≤–µ `model.fit()` –∏ –¥–æ–ª–∂–Ω—ã –±—É–¥–µ—Ç–µ –∑–∞–¥–∞—Ç—å –Ω–æ–≤–æ–µ –∏–º—è.

</Tip>

–ù–∞–∫–æ–Ω–µ—Ü, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç –Ω–∞—à–∏ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ Model Hub, —á—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å –∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –µ—é —Å –¥—Ä—É–∑—å—è–º–∏. –í—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ - –ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!

{:else}

–ö–æ–≥–¥–∞ —ç—Ç–æ —Å–¥–µ–ª–∞–Ω–æ, –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—à–∏ `Seq2SeqTrainingArguments`. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å `Trainer`, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–∫–ª–∞—Å—Å `TrainingArguments`, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –µ—â–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

–ü–æ–º–∏–º–æ –æ–±—ã—á–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ç–∞–∫–∏—Ö –∫–∞–∫ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–∞), –∑–¥–µ—Å—å –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–ª–∏—á–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç–µ–º, —á—Ç–æ –º—ã –≤–∏–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö:

- –ú—ã –Ω–µ –∑–∞–¥–∞–µ–º –Ω–∏–∫–∞–∫–∏—Ö —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫, —Ç–∞–∫ –∫–∞–∫ –æ—Ü–µ–Ω–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏; –º—ã –ø—Ä–æ—Å—Ç–æ –æ—Ü–µ–Ω–∏–º –Ω–∞—à—É –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –¥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ.
- –ú—ã —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ `fp16=True`, —á—Ç–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU.
- –ú—ã —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º `predict_with_generate=True`, –∫–∞–∫ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å –≤—ã—à–µ.
- –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `push_to_hub=True` –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ Hub –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–µ `hub_model_id` –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω–æ–µ –∏–º—è —Ä–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å (–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é). –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª—å –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é [`huggingface-course`](https://huggingface.co/huggingface-course), –º—ã –¥–æ–±–∞–≤–∏–ª–∏ `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` –≤ `Seq2SeqTrainingArguments`. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ä–æ–∑–∏—Ç–æ—Ä–∏–π –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –≤–∞—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏–º–µ–Ω –∏ –Ω–∞–∑—ã–≤–∞—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –≤–∞–º–∏ –≤—ã—Ö–æ–¥–Ω—ã–º –∫–∞—Ç–∞–ª–æ–≥–æ–º, –ø–æ—ç—Ç–æ–º—É –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –±—É–¥–µ—Ç `"sgugger/marian-finetuned-kde4-en-to-fr"` (—ç—Ç–æ –º–æ–¥–µ–ª—å, –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –º—ã —Å—Å—ã–ª–∞–ª–∏—Å—å –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞).

<Tip>

üí° –ï—Å–ª–∏ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ, —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–º –∫–ª–æ–Ω–æ–º —Ç–æ–≥–æ —Ä–æ–∑–∏—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å push. –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–∞–∫, –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ—à–∏–±–∫—É –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≤–∞—à–µ–≥–æ `Seq2SeqTrainer` –∏ –¥–æ–ª–∂–Ω—ã –±—É–¥–µ—Ç–µ –∑–∞–¥–∞—Ç—å –Ω–æ–≤–æ–µ –∏–º—è.

</Tip>


–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º –≤—Å–µ –≤ `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

–ü–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º—ã —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫—É—é –æ—Ü–µ–Ω–∫—É –ø–æ–ª—É—á–∏–ª–∞ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ —É—Ö—É–¥—à–∞–µ–º –ª–∏ –º—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ–æ–±—É—á–∏–≤ –µ–µ. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–æ–π –∫–æ–º–∞–Ω–¥—ã –∑–∞–π–º–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è, –ø–æ—ç—Ç–æ–º—É –≤–æ –≤—Ä–µ–º—è –µ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–æ–∂–Ω–æ –≤—ã–ø–∏—Ç—å –∫–æ—Ñ–µ:

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

–û—Ü–µ–Ω–∫–∞ BLEU –≤ 39 –±–∞–ª–ª–æ–≤ –Ω–µ —Ç–∞–∫ —É–∂ –ø–ª–æ—Ö–æ, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å —É–∂–µ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π.

–î–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Ç–∞–∫–∂–µ –∑–∞–π–º–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è:

```python
trainer.train()
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è (–∑–¥–µ—Å—å - –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É), –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ Hub –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—ã —Å–º–æ–∂–µ—Ç–µ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–æ–π –º–∞—à–∏–Ω–µ.

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º—ã —Å–Ω–æ–≤–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å - –Ω–∞–¥–µ–µ–º—Å—è, –º—ã —É–≤–∏–¥–∏–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ BLEU!

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

–≠—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ—á—Ç–∏ –Ω–∞ 14 –ø—É–Ω–∫—Ç–æ–≤, —á—Ç–æ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ.

–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ `push_to_hub()`, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏. –¢—Ä–µ–Ω–µ—Ä —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ—Ç –∫–∞—Ä—Ç—É –º–æ–¥–µ–ª–∏ —Å–æ –≤—Å–µ–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ–µ. –≠—Ç–∞ –∫–∞—Ä—Ç–∞ –º–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —Ö–∞–±—É –º–æ–¥–µ–ª–µ–π –≤—ã–±—Ä–∞—Ç—å –≤–∏–¥–∂–µ—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –û–±—ã—á–Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω—ã–π –≤–∏–¥–∂–µ—Ç –ø–æ –∫–ª–∞—Å—Å—É –º–æ–¥–µ–ª–∏, –Ω–æ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –≤—Å–µ—Ö –≤–∏–¥–æ–≤ –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏, –ø–æ—ç—Ç–æ–º—É –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞:

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

–≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL-–∞–¥—Ä–µ—Å —Ç–æ–ª—å–∫–æ —á—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ –∫–æ–º–º–∏—Ç–∞, –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ Model Hub, —á—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é –º–æ–¥–µ–ª—å –∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –µ—é —Å –¥—Ä—É–∑—å—è–º–∏. –í—ã —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ - –ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è, –º—ã –ø–æ–∫–∞–∂–µ–º –≤–∞–º, –∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–æ –∂–µ —Å–∞–º–æ–µ —Å –ø–æ–º–æ—â—å—é ü§ó Accelerate.

{/if}

{#if fw === 'pt'}

## –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è[[a-custom-training-loop]]

–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤—ã –º–æ–≥–ª–∏ –ª–µ–≥–∫–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω—É–∂–Ω—ã–µ –≤–∞–º —á–∞—Å—Ç–∏. –û–Ω –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —ç—Ç–æ –≤ [–ì–ª–∞–≤–µ 2](../chapter7/2) –∏ [–ì–ª–∞–≤–µ 3](../chapter3/4).

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ–≥–æ –∫ –æ–±—É—á–µ–Ω–∏—é[[preparing-everything-for-training]]

–í—ã —É–∂–µ –≤–∏–¥–µ–ª–∏ –≤—Å–µ —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑, –ø–æ—ç—Ç–æ–º—É –º—ã –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ –∫–æ–¥—É –¥–æ–≤–æ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ. –°–Ω–∞—á–∞–ª–∞ –º—ã —Å–æ–∑–¥–∞–¥–∏–º `DataLoader` –∏–∑ –Ω–∞—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–æ—Å–ª–µ —á–µ–≥–æ —É—Å—Ç–∞–Ω–æ–≤–∏–º –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Ñ–æ—Ä–º–∞—Ç `"torch"`, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã PyTorch:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

–ó–∞—Ç–µ–º –º—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º—ã –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏, –∞ –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

–¢–æ–≥–¥–∞ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

–ö–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ —ç—Ç–∏ –æ–±—ä–µ–∫—Ç—ã, –º—ã –º–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∏—Ö –≤ –º–µ—Ç–æ–¥ `accelerator.prepare()`. –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ TPU –≤ –±–ª–æ–∫–Ω–æ—Ç–µ Colab, –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤–µ—Å—å —ç—Ç–æ—Ç –∫–æ–¥ –≤ —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –¥–æ–ª–∂–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —è—á–µ–π–∫–∏, –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É—é—â–µ–π `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –Ω–∞—à `train_dataloader` –≤ `accelerator.prepare()`, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª–∏–Ω—É –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–º–µ–Ω–∏—Ç –¥–ª–∏–Ω—É `DataLoader`. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ª–∏–Ω–µ–π–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—à–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–æ 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

–ù–∞–∫–æ–Ω–µ—Ü, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –≤ Hub, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `Repository` –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–ø–∫–µ. –°–Ω–∞—á–∞–ª–∞ –≤–æ–π–¥–∏—Ç–µ –≤ Hub Hugging Face, –µ—Å–ª–∏ –≤—ã –µ—â–µ –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã. –ú—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º –∏–º—è —Ä–æ–∑–∏—Ç–æ—Ä–∏—è –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏—Å–≤–æ–∏—Ç—å –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–Ω–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –∑–∞–º–µ–Ω–∏—Ç—å `repo_name` –Ω–∞ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä; –æ–Ω–æ –ø—Ä–æ—Å—Ç–æ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–∞—à–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ –∏ –¥–µ–ª–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –∫–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ä–æ–∑–∏—Ç–æ—Ä–∏–π –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É. –ï—Å–ª–∏ –æ–Ω–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —ç—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫–ª–æ–Ω–æ–º —Ç–æ–≥–æ —Ä–æ–∑–∏—Ç–æ—Ä–∏—è, —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã —Ä–∞–±–æ—Ç–∞–µ–º:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤ `output_dir`, –≤—ã–∑–≤–∞–≤ –º–µ—Ç–æ–¥ `repo.push_to_hub()`. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏.

### –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è[[training-loop]]

–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –Ω–∞–ø–∏—Å–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è. –ß—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å –µ–≥–æ –æ—Ü–µ–Ω–æ—á–Ω—É—é —á–∞—Å—Ç—å, –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é `postprocess()`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ —Å–ø–∏—Å–∫–∏ —Å—Ç—Ä–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥–µ—Ç –æ–∂–∏–¥–∞—Ç—å –Ω–∞—à –æ–±—ä–µ–∫—Ç `metric`:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # –ó–∞–º–µ–Ω–∏–º -100 –≤ –º–µ—Ç–∫–∞—Ö, —Ç–∞–∫ –∫–∞–∫ –º—ã –Ω–µ –º–æ–∂–µ–º –∏—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # –ù–µ–º–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ —Ü–∏–∫–ª—ã –≤ [—Ä–∞–∑–¥–µ–ª–µ 2](../chapter7/2) –∏ [–≥–ª–∞–≤–µ 3](../chapter3/1), —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –æ—Ç–ª–∏—á–∏—è–º–∏ –≤ —á–∞—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏ - —Ç–∞–∫ —á—Ç–æ –¥–∞–≤–∞–π—Ç–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ —ç—Ç–æ–º!

–ü–µ—Ä–≤–æ–µ, —á—Ç–æ —Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —ç—Ç–æ —Ç–æ, —á—Ç–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ `generate()` –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, –Ω–æ —ç—Ç–æ –º–µ—Ç–æ–¥ –Ω–∞—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –∞ –Ω–µ –æ–±–µ—Ä–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏ ü§ó Accelerate, —Å–æ–∑–¥–∞–Ω–Ω–æ–π –≤ –º–µ—Ç–æ–¥–µ `prepare()`. –ü–æ—ç—Ç–æ–º—É –º—ã —Å–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–≤–µ—Ä–Ω–µ–º –º–æ–¥–µ–ª—å, –∞ –∑–∞—Ç–µ–º –≤—ã–∑–æ–≤–µ–º —ç—Ç–æ—Ç –º–µ—Ç–æ–¥.

–í—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ, –∫–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å [–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤](../chapter7/2), –¥–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º–æ–≥—É—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç—å –≤—Ö–æ–¥—ã –∏ –º–µ—Ç–∫–∏ –¥–æ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º, –ø–æ—ç—Ç–æ–º—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `accelerator.pad_across_processes()`, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –ø–æ —Ñ–æ—Ä–º–µ –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º –º–µ—Ç–æ–¥–∞ `gather()`. –ï—Å–ª–∏ –º—ã —ç—Ç–æ–≥–æ –Ω–µ —Å–¥–µ–ª–∞–µ–º, –æ—Ü–µ–Ω–∫–∞ –ª–∏–±–æ –≤—ã–¥–∞—Å—Ç –æ—à–∏–±–∫—É, –ª–∏–±–æ –∑–∞–≤–∏—Å–Ω–µ—Ç –Ω–∞–≤—Å–µ–≥–¥–∞.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # –û–±—É—á–µ–Ω–∏–µ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # –û—Ü–µ–Ω–∫–∞
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        ## –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç–∫–∏
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —É –≤–∞—Å –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—å—Å—è –º–æ–¥–µ–ª—å, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ—Ç–æ—Ä–æ–π –±—É–¥—É—Ç –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ –Ω–∞ –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é `Seq2SeqTrainer`. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–±—É—á–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –∫–æ–¥–∞, –Ω–∞ [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). –ê –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è, –≤—ã –º–æ–∂–µ—Ç–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏—Ö –Ω–∞–ø—Ä—è–º—É—é, –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–≤ –∫–æ–¥, –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–π –≤—ã—à–µ!

{/if}

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏[[using-the-fine-tuned-model]]

–ú—ã —É–∂–µ –ø–æ–∫–∞–∑–∞–ª–∏ –≤–∞–º, –∫–∞–∫ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º—ã –¥–æ–æ–±—É—á–∏–ª–∏ –Ω–∞ Model Hub, —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–∂–µ—Ç–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –ª–æ–∫–∞–ª—å–Ω–æ –≤ `pipeline`, –Ω–∞–º –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏:

```py
from transformers import pipeline

# –ó–∞–º–µ–Ω–∏—Ç–µ —ç—Ç–æ –Ω–∞ —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par d√©faut, d√©velopper les fils de discussion'}]
```

–ö–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å, –Ω–∞—à–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∞ —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –∫ –∫–æ—Ä–ø—É—Å—É, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –º—ã –µ–µ –¥–æ–æ–±—É—á–∏–ª–∏, –∏ –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ —Å–ª–æ–≤–æ "threads" –≤ –ø–æ–∫–æ–µ, –æ–Ω–∞ —Ç–µ–ø–µ—Ä—å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç. –¢–æ –∂–µ —Å–∞–º–æ–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∏ –∫ —Å–ª–æ–≤—É " plugin ":

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

–ï—â–µ –æ–¥–∏–Ω –æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä –¥–æ–º–µ–Ω–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏!

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ß—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ —Å–æ —Å–ª–æ–≤–æ–º "email", –∫–æ—Ç–æ—Ä—ã–π –≤—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Ä–∞–Ω–µ–µ?

</Tip>
