# –ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –æ—à–∏–±–∫–∏

<CourseFloatingBanner chapter={8}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb"},
]} />

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å, –∫–æ–≥–¥–∞ –≤—ã –ø—ã—Ç–∞–µ—Ç–µ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ —á—Ç–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Transformer. –≠—Ç–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç –≤–∞—Å –∫ [—Ä–∞–∑–¥–µ–ª—É 4](/course/chapters/ru/chapter8/4.mdx), –≥–¥–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –æ—Ç–ª–∞–¥–∏—Ç—å —Å–∞–º —ç—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è.
<Youtube id="DQ-CpJn6Rc4"/>

–î–ª—è —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Ç–∏–ø–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28), –∏ –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–¥ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ, –≤–∞–º —Å–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ —Å–≤–æ–π –∞–∫–∫–∞—É–Ω—Ç –Ω–∞ [Hugging Face Hub](https://huggingface.co). –î–ª—è —ç—Ç–æ–≥–æ —Å–Ω–∞—á–∞–ª–∞ –≤–æ–π–¥–∏—Ç–µ –≤ —Å–∏—Å—Ç–µ–º—É, –≤—ã–ø–æ–ª–Ω–∏–≤ –≤ Jupyter notebook –æ–¥–Ω–æ –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π:

```python
from huggingface_hub import notebook_login

notebook_login()
```

–∏–ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ –≤ –≤–∞—à–µ–º –ª—é–±–∏–º–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```bash
huggingface-cli login
```

–≠—Ç–æ –ø–æ–ø—Ä–æ—Å–∏—Ç –≤–∞—Å –≤–≤–µ—Å—Ç–∏ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø–∞—Ä–æ–ª—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç —Ç–æ–∫–µ–Ω –≤ *~/.cache/huggingface/*. –ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –≤—ã –≤–æ—à–ª–∏ –≤ —Å–∏—Å—Ç–µ–º—É, –≤—ã –º–æ–∂–µ—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —à–∞–±–ª–æ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–ª–µ–¥—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏:

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Clone the repo and extract the local path
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Create an empty repo on the Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Clone the empty repo
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copy files
    copy_tree(template_repo_dir, new_repo_dir)
    # Push to Hub
    repo.push_to_hub()
```

–¢–µ–ø–µ—Ä—å –ø—Ä–∏ –≤—ã–∑–æ–≤–µ `copy_repository_template()` –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è –∫–æ–ø–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —à–∞–±–ª–æ–Ω–æ–≤ –ø–æ–¥ –≤–∞—à–µ–π —É—á–µ—Ç–Ω–æ–π –∑–∞–ø–∏—Å—å—é.

## –û—Ç–ª–∞–¥–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–∑ ü§ó Transformers

–ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –Ω–∞—à–µ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ –≤ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π –º–∏—Ä –æ—Ç–ª–∞–¥–∫–∏ –º–æ–¥–µ–ª–µ–π Transformer, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–π: –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å –∫–æ–ª–ª–µ–≥–æ–π –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º –ø–æ –æ—Ç–≤–µ—Ç–∞–º –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º —Å–∞–π—Ç–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç—ã –æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Ç–æ–≤–∞—Ä–∞—Ö. –í–∞—à –∫–æ–ª–ª–µ–≥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤–∞–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è:

> –î–æ–±—Ä—ã–π –¥–µ–Ω—å! –Ø —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ–≤–µ–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ—Ö–Ω–∏–∫–∏ –∏–∑ [–≥–ª–∞–≤—ã 7](/course/chapters/ru/chapter7/7.mdx) –∫—É—Ä—Å–∞ Hugging Face –∏ –ø–æ–ª—É—á–∏–ª –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ SQuAD! –Ø –¥—É–º–∞—é, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–ø—Ä–∞–≤–Ω–æ–π —Ç–æ—á–∫–∏ –¥–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. ID –º–æ–¥–µ–ª–∏ –Ω–∞ —Ö–∞–±–µ - "lewtun/distillbert-base-uncased-finetuned-squad-d5716d28". –ù–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–µ :)

–∏ –ø–µ—Ä–≤–æ–µ, –æ —á–µ–º –≤—ã –¥—É–º–∞–µ—Ç–µ, —ç—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è `pipeline` –∏–∑ ü§ó Transformers:

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

–û –Ω–µ—Ç, –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫! –ï—Å–ª–∏ –≤—ã –Ω–æ–≤–∏—á–æ–∫ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –ø–æ–¥–æ–±–Ω—ã–µ –æ—à–∏–±–∫–∏ –º–æ–≥—É—Ç –ø–æ–∫–∞–∑–∞—Ç—å—Å—è –≤–∞–º –Ω–µ–º–Ω–æ–≥–æ –∑–∞–≥–∞–¥–æ—á–Ω—ã–º–∏ (—á—Ç–æ —Ç–∞–∫–æ–µ `OSError`?!). –û—à–∏–±–∫–∞, –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–∞—è –∑–¥–µ—Å—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏—à—å –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç—å—é –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª—å—à–µ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ, –Ω–∞–∑—ã–≤–∞–µ–º–æ–≥–æ _Python traceback_ (–æ–Ω –∂–µ —Ç—Ä–µ–π—Å–±–µ–∫). –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤—ã –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –≤ Google Colab, –≤—ã –¥–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–Ω–∏–º–∫–∞ —ç–∫—Ä–∞–Ω–∞:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png" alt="A Python traceback." width="100%"/>
</div>

–í —ç—Ç–∏—Ö –æ—Ç—á–µ—Ç–∞—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –º–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –≤–º–µ—Å—Ç–µ –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —á–∞—Å—Ç—è–º. –ü–µ—Ä–≤–æ–µ, —á—Ç–æ —Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —ç—Ç–æ —Ç–æ, —á—Ç–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç —á–∏—Ç–∞—Ç—å _—Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö_. –≠—Ç–æ –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å—Å—è —Å—Ç—Ä–∞–Ω–Ω—ã–º, –µ—Å–ª–∏ –≤—ã –ø—Ä–∏–≤—ã–∫–ª–∏ —á–∏—Ç–∞—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç —Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑, –Ω–æ —ç—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ —Ç—Ä–µ–π—Å–±–µ–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç `pipeline` –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. (–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ —Ç–æ–º, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç `pipeline` –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º, —á–∏—Ç–∞–π—Ç–µ –≤ [–ì–ª–∞–≤–µ 2](/course/chapters/ru/chapter2)).

<Tip>

üö® –í–∏–¥–∏—Ç–µ —Å–∏–Ω—é—é —Ä–∞–º–∫—É –≤–æ–∫—Ä—É–≥ "6 frames" –≤ —Ç—Ä–µ–π—Å–±–µ–∫–µ Google Colab? –≠—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Colab, –∫–æ—Ç–æ—Ä–∞—è —Å–∂–∏–º–∞–µ—Ç —Ç—Ä–µ–π—Å–±–µ–∫ –≤ "—Ä–∞–º–∫–∏". –ï—Å–ª–∏ –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –æ—à–∏–±–∫–∏, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã —Ä–∞–∑–≤–µ—Ä–Ω—É–ª–∏ –ø–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å–±–µ–∫, –Ω–∞–∂–∞–≤ –Ω–∞ —ç—Ç–∏ –¥–≤–µ –º–∞–ª–µ–Ω—å–∫–∏–µ —Å—Ç—Ä–µ–ª–∫–∏.

</Tip>

–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ —Ç—Ä–µ–π—Å–±–µ–∫–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –±—ã–ª–æ –≤—ã–∑–≤–∞–Ω–æ. –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Ç–∏–ø –∏—Å–∫–ª—é—á–µ–Ω–∏—è - `OSError`, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ—à–∏–±–∫—É, —Å–≤—è–∑–∞–Ω–Ω—É—é —Å —Å–∏—Å—Ç–µ–º–æ–π. –ï—Å–ª–∏ –º—ã –ø—Ä–æ—á–∏—Ç–∞–µ–º —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, —Ç–æ —É–≤–∏–¥–∏–º, —á—Ç–æ, –ø–æ—Ö–æ–∂–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å —Ñ–∞–π–ª–æ–º –º–æ–¥–µ–ª–∏ *config.json*, –∏ –Ω–∞–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –µ–µ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è:

```python out
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

<Tip>

üí° –ï—Å–ª–∏ –≤—ã —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—à–∏–±–∫–µ, –∫–æ—Ç–æ—Ä–æ–µ —Ç—Ä—É–¥–Ω–æ –ø–æ–Ω—è—Ç—å, –ø—Ä–æ—Å—Ç–æ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É –ø–æ–∏—Å–∫–∞ Google –∏–ª–∏ [Stack Overflow](https://stackoverflow.com/) (–¥–∞, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ!). –í–µ–ª–∏–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ –≤—ã –Ω–µ –ø–µ—Ä–≤—ã–π, –∫—Ç–æ —Å—Ç–æ–ª–∫–Ω—É–ª—Å—è —Å —ç—Ç–æ–π –æ—à–∏–±–∫–æ–π, –∏ —ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π —Å–ø–æ—Å–æ–± –Ω–∞–π—Ç–∏ —Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ –¥—Ä—É–≥–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É `OSError: Can't load config for` –Ω–∞ Stack Overflow –¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ [—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+), –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–ø—Ä–∞–≤–Ω–æ–π —Ç–æ—á–∫–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã.

</Tip>

–í –ø–µ—Ä–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–∞–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –Ω—É–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –≤—Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ –≤ —Å—Ç—Ä–æ–∫—É –ø–æ–∏—Å–∫–∞ Hub:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png" alt="The wrong model name." width="100%"/>
</div>

–•–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ, –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–∞—à–µ–≥–æ –∫–æ–ª–ª–µ–≥–∏ –Ω–µ—Ç –Ω–∞ Hub... –∞–≥–∞, –Ω–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –æ–ø–µ—á–∞—Ç–∫–∞! DistilBERT –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É "l" –≤ —Å–≤–æ–µ–º –Ω–∞–∑–≤–∞–Ω–∏–∏, —Ç–∞–∫ —á—Ç–æ –¥–∞–≤–∞–π—Ç–µ –∏—Å–ø—Ä–∞–≤–∏–º —ç—Ç–æ –∏ –ø–æ–∏—â–µ–º –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28":

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png" alt="The right model name." width="100%"/>
</div>

–•–æ—Ä–æ—à–æ, –∑–∞–≥—Ä—É–∑–∫–∞ —É–¥–∞–ª–∞—Å—å. –¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å–Ω–æ–≤–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª–∏:

```python
model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python out
"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

–ê—Ä–≥—Ö, –æ–ø—è—Ç—å –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å - –¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é –∂–∏–∑–Ω—å –∏–Ω–∂–µ–Ω–µ—Ä–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è! –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏—Å–ø—Ä–∞–≤–∏–ª–∏ ID –º–æ–¥–µ–ª–∏, –ø—Ä–æ–±–ª–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ —Å–∞–º–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏. –ë—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ ü§ó Hub - —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è `list_repo_files()` –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `huggingface_hub`:

```python
from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)
```

```python out
['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']
```
–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ - –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ—Ç —Ñ–∞–π–ª–∞ *config.json*! –ù–µ—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –Ω–∞—à `pipeline` –Ω–µ —Å–º–æ–≥ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å; –Ω–∞—à –∫–æ–ª–ª–µ–≥–∞, –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å, –∑–∞–±—ã–ª –¥–æ–±–∞–≤–∏—Ç—å —ç—Ç–æ—Ç —Ñ–∞–π–ª –≤ Hub –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –¥–æ—Ä–∞–±–æ—Ç–∞–ª –µ–≥–æ. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø—Ä–æ–±–ª–µ–º–∞ –∫–∞–∂–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π: –º—ã –º–æ–∂–µ–º –ø–æ–ø—Ä–æ—Å–∏—Ç—å –µ–≥–æ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª, –∏–ª–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –∏–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –º–æ–¥–µ–ª–∏ –≤–∏–¥–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased), –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø–æ–º–µ—Å—Ç–∏—Ç—å –µ–µ –≤ –Ω–∞—à —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π, —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, —Ä–µ—à–∏—Ç –ª–∏ —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º—É. –î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å. –ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–µ–º—ã, –∏–∑—É—á–µ–Ω–Ω—ã–µ –≤ [–≥–ª–∞–≤–µ 2](/course/chapters/ru/chapter2), –º—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∞ `AutoConfig`:

```python
from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)
```

<Tip warning={true}>

üö® –ü—Ä–∏–º–µ–Ω—è–µ–º—ã–π –Ω–∞–º–∏ –ø–æ–¥—Ö–æ–¥ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–¥–µ–∂–Ω—ã–º, –ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∞—à –∫–æ–ª–ª–µ–≥–∞ –º–æ–≥ –∏–∑–º–µ–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é `distilbert-base-uncased` –ø–µ—Ä–µ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –º–æ–¥–µ–ª–∏. –í —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏ –º—ã –±—ã —Ö–æ—Ç–µ–ª–∏ —Å–Ω–∞—á–∞–ª–∞ —É—Ç–æ—á–Ω–∏—Ç—å —É –Ω–µ–≥–æ, –Ω–æ –¥–ª—è —Ü–µ–ª–µ–π —ç—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –º—ã –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –æ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

</Tip>

–ó–∞—Ç–µ–º –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ —ç—Ç–æ –≤ –Ω–∞—à–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ `push_to_hub()`:

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

–¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—Ä–∞–±–æ—Ç–∞–ª–æ –ª–∏ —ç—Ç–æ, –∑–∞–≥—Ä—É–∑–∏–≤ –º–æ–¥–µ–ª—å –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–æ–º–º–∏—Ç–∞ –≤ –≤–µ—Ç–∫–µ `main`:

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)
```

```python out
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}
```

–£—Ö —Ç—ã, —Å—Ä–∞–±–æ—Ç–∞–ª–æ! –î–∞–≤–∞–π—Ç–µ –≤—Å–ø–æ–º–Ω–∏–º, —á–µ–º—É –≤—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ –Ω–∞—É—á–∏–ª–∏—Å—å:

- –°–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö –≤ Python –∏–∑–≤–µ—Å—Ç–Ω—ã –∫–∞–∫ _tracebacks_ –∏ —á–∏—Ç–∞—é—Ç—Å—è —Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö. –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã.
- –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø—Ä–æ–π–¥–∏—Ç–µ –ø—É—Ç—å –≤–≤–µ—Ä—Ö –ø–æ —Ç—Ä–µ–π—Å–±–µ–∫—É –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –≤ –∫–∞–∫–æ–º –º–µ—Å—Ç–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞.
- –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π –æ–± –æ—à–∏–±–∫–∞—Ö –Ω–µ –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –æ—Ç–ª–∞–¥–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∏—Å–∫–∞—Ç—å –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã.
- `huggingface_hub`
// ü§ó Hub?
–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –æ—Ç–ª–∞–¥–∫–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –Ω–∞ Hub.

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –≤—ã –∑–Ω–∞–µ—Ç–µ, –∫–∞–∫ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω, –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –ø—Ä–∏–º–µ—Ä –Ω–∞ –ø—Ä—è–º–æ–º –ø—Ä–æ—Ö–æ–¥–µ —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏.

## –û—Ç–ª–∞–¥–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏

–•–æ—Ç—è `pipeline` –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, –≥–¥–µ –≤–∞–º –Ω—É–∂–Ω–æ –±—ã—Å—Ç—Ä–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã, –∏–Ω–æ–≥–¥–∞ –≤–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–æ—Å—Ç—É–ø –∫ –ª–æ–≥–∏—Ç–∞–º –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∫–∞–∫–∞—è-—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞, –∫–æ—Ç–æ—Ä—É—é –≤—ã —Ö–æ—Ç–µ–ª–∏ –±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å). –ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–π—Ç–∏ –Ω–µ —Ç–∞–∫ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ, –¥–∞–≤–∞–π—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –≤–æ–∑—å–º–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –Ω–∞—à–µ–≥–æ `pipeline`:

```python
tokenizer = reader.tokenizer
model = reader.model
```

–î–∞–ª–µ–µ –Ω–∞–º –Ω—É–∂–µ–Ω –≤–æ–ø—Ä–æ—Å, –ø–æ—ç—Ç–æ–º—É –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –ª–∏ –Ω–∞—à–∏ –ª—é–±–∏–º—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏:

```python
question = "Which frameworks can I use?"
```

–ö–∞–∫ –º—ã –≤–∏–¥–µ–ª–∏ –≤ [–ì–ª–∞–≤–µ 7](/course/chapters/ru/chapter7), –æ–±—ã—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å, - —ç—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤, –∞ –∑–∞—Ç–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –æ—Ç–≤–µ—Ç–æ–≤:

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""
```

–û –±–æ–∂–µ, –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –≤ –Ω–∞—à–µ–º –∫–æ–¥–µ –µ—Å—Ç—å –æ—à–∏–±–∫–∞! –ù–æ –º—ã –Ω–µ –±–æ–∏–º—Å—è –Ω–µ–±–æ–ª—å—à–æ–π –æ—Ç–ª–∞–¥–∫–∏. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–ª–∞–¥—á–∏–∫ Python –≤ –±–ª–æ–∫–Ω–æ—Ç–µ:


<Youtube id="rSPyvPw0p9k"/>

–∏–ª–∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

<Youtube id="5PkZ4rbHL6c"/>

–ó–¥–µ—Å—å —á—Ç–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –≥–æ–≤–æ—Ä–∏—Ç –Ω–∞–º, —á—Ç–æ —É –æ–±—ä–µ–∫—Ç–∞ `'list'' –Ω–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ 'size'`, –∏ –º—ã –≤–∏–¥–∏–º —Å—Ç—Ä–µ–ª–∫—É `-->`, —É–∫–∞–∑—ã–≤–∞—é—â—É—é –Ω–∞ —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –≤–æ–∑–Ω–∏–∫–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞ –≤ `model(**inputs)`. –í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–ª–∞–¥–∏—Ç—å —ç—Ç–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–ª–∞–¥—á–∏–∫ Python, –Ω–æ –ø–æ–∫–∞ –º—ã –ø—Ä–æ—Å—Ç–æ —Ä–∞—Å–ø–µ—á–∞—Ç–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç `inputs`, —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å:

```python
inputs["input_ids"][:5]
```

```python out
[101, 2029, 7705, 2015, 2064]
```

–≠—Ç–æ, –∫–æ–Ω–µ—á–Ω–æ, –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –æ–±—ã—á–Ω—ã–π `list` –∏–∑ Python, –Ω–æ –¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏–º —Ç–∏–ø:

```python
type(inputs["input_ids"])
```

```python out
list
```

–î–∞, —ç—Ç–æ —Ç–æ—á–Ω–æ Python `list`. –¢–∞–∫ —á—Ç–æ –∂–µ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫? –í—Å–ø–æ–º–Ω–∏—Ç–µ –∏–∑ [–ì–ª–∞–≤—ã 2](/course/chapters/ru/chapter2), —á—Ç–æ –∫–ª–∞—Å—Å—ã `AutoModelForXxx` –≤ ü§ó Transformers —Ä–∞–±–æ—Ç–∞—é—Ç —Å _tensors_ (–ª–∏–±–æ –≤ PyTorch, –ª–∏–±–æ –≤ TensorFlow), –∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π —è–≤–ª—è–µ—Ç—Å—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–µ–Ω–∑–æ—Ä–∞ —Å –ø–æ–º–æ—â—å—é `Tensor.size()`, —Å–∫–∞–∂–µ–º, –≤ PyTorch. –î–∞–≤–∞–π—Ç–µ –µ—â–µ —Ä–∞–∑ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç—Ä–µ–π—Å–±–µ–∫, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫–∞—è —Å—Ç—Ä–æ–∫–∞ –≤—ã–∑–≤–∞–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ:

```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

–ü–æ—Ö–æ–∂–µ, —á—Ç–æ –Ω–∞—à –∫–æ–¥ –ø—ã—Ç–∞–ª—Å—è –≤—ã–∑–≤–∞—Ç—å `input_ids.size()`, –Ω–æ —ç—Ç–æ —è–≤–Ω–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è Python `list`, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–º. –ö–∞–∫ –º—ã –º–æ–∂–µ–º —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É? –ü–æ–∏—Å–∫ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –Ω–∞ Stack Overflow –¥–∞–µ—Ç –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö [—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f). –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –ø–µ—Ä–≤—ã–π –∏–∑ –Ω–∏—Ö –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –Ω–∞—à–µ–º—É, –∞ –æ—Ç–≤–µ—Ç –ø–æ–∫–∞–∑–∞–Ω –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–µ –Ω–∏–∂–µ:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png" alt="An answer from Stack Overflow." width="100%"/>
</div>

–í –æ—Ç–≤–µ—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å `return_tensors='pt'` –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É, —Ç–∞–∫ —á—Ç–æ –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —ç—Ç–æ –¥–ª—è –Ω–∞—Å:

```python out
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python out
"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""
```

–û—Ç–ª–∏—á–Ω–æ, —ç—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ! –≠—Ç–æ –æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–Ω—ã–º –º–æ–∂–µ—Ç –±—ã—Ç—å Stack Overflow: –æ–ø—Ä–µ–¥–µ–ª–∏–≤ —Å—Ö–æ–∂—É—é –ø—Ä–æ–±–ª–µ–º—É, –º—ã —Å–º–æ–≥–ª–∏ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ–ø—ã—Ç–æ–º –¥—Ä—É–≥–∏—Ö —á–ª–µ–Ω–æ–≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –û–¥–Ω–∞–∫–æ –ø–æ–¥–æ–±–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –≤—Å–µ–≥–¥–∞ –¥–∞–µ—Ç –Ω—É–∂–Ω—ã–π –æ—Ç–≤–µ—Ç, —á—Ç–æ –∂–µ –¥–µ–ª–∞—Ç—å –≤ —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö? –ö —Å—á–∞—Å—Ç—å—é, –Ω–∞ [Hugging Face forums](https://discuss.huggingface.co/) –µ—Å—Ç—å –≥–æ—Å—Ç–µ–ø—Ä–∏–∏–º–Ω–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –≤–∞–º –ø–æ–º–æ—á—å! –í —Å–ª–µ–¥—É—é—â–µ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —Å–æ—Å—Ç–∞–≤–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ñ–æ—Ä—É–º–µ, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –±—É–¥–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç.