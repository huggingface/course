# –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

–¢–µ–ø–µ—Ä—å –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –¥–æ—Å—Ç–∏—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤—ã –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ `Trainer`. –í —ç—Ç–æ–π –≥–ª–∞–≤–µ –º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏ —ç—Ç–∞–ø—ã –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ —Ä–∞–∑–¥–µ–ª–∞ 2. –ù–∏–∂–µ –∫–æ—Ä–æ—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ —Ç–æ–≥–æ, —á—Ç–æ –≤–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è: 

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é

–ü–µ—Ä–µ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤. –ü–µ—Ä–≤—ã–π: –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–¥–∞–ª–µ–µ - dataloaders), –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –±–∞—Ç—á–∞–º –¥–∞–Ω–Ω—ã—Ö. –ü–µ—Ä–µ–¥ —ç—Ç–∏–º –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∫ –Ω–∞—à–µ–º—É `tokenized_datasets`. –≠—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å: –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑ –∑–∞ –Ω–∞—Å —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–ª–∞–ª `Trainer`. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ: 


- –£–¥–∞–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `sentence1` –∏ `sentence2`).
- –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É `label` –≤ `labels` (–ø–æ—Ç–æ–º—É —á—Ç–æ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π `labels`). 
- –ó–∞–¥–∞—Ç—å —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ pytorch tensors –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–æ–≤. 

–ù–∞—à `tokenized_datasets` –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã—Ö –≤—ã—à–µ —à–∞–≥–æ–≤:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

–ú—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —É –Ω–∞—Å –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–æ–ª—å–∫–æ —Ç–µ –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–∂–∏–¥–∞–µ—Ç –Ω–∞—à–∞ –º–æ–¥–µ–ª—å:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤, –º—ã –º–æ–∂–µ—Ç –∑–∞–¥–∞—Ç—å dataloader:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

–î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –æ—à–∏–±–æ–∫ –≤ —Å–¥–µ–ª–∞–Ω–Ω–æ–º –Ω–∞–º–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ, –º—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–¥–∏–Ω –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–º–µ—Ä—ã, –≤–µ—Ä–æ—è—Ç–Ω–æ, –±—É–¥—É—Ç –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –¥–ª—è –≤ –≤–∞—à–µ–º —Å–ª—É—á–∞–µ, —Ç–∞–∫ –∫–∞–∫ –º—ã —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ `shuffle=True` –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∂–µ –º—ã –¥–æ–ø–æ–ª–Ω—è–µ–º (padding) –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞.

–¢–µ–ø–µ—Ä—å –º—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–∏–ª–∏ —ç—Ç–∞–ø –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ (–ø—Ä–∏—è—Ç–Ω—ã–π, –Ω–æ –Ω–µ—É–ª–æ–≤–∏–º—ã–π –º–æ–º–µ–Ω—Ç –¥–ª—è –ª—é–±–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é), –ø–µ—Ä–µ–π–¥–µ–º –∫ –º–æ–¥–µ–ª–∏. –ú—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –µ–µ —Ç–æ—á–Ω–æ —Ç–∞–∫, –∫–∞–∫ –¥–µ–ª–∞–ª–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø—Ä–∏–º–µ—Ä–µ: 

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

–ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–π–¥–µ—Ç –≥–ª–∞–¥–∫–æ, –≤—ã –ø–æ–¥–∞–¥–∏–º –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω –±–∞—Ç—á:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

–í—Å–µ –º–æ–¥–µ–ª–∏ ü§ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ `labels`, –∞ —Ç–∞–∫–∂–µ –ª–æ–≥–∏—Ç—ã (–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä 8 —Ö 2). 

–ú—ã –ø–æ—á—Ç–∏ –≥–æ—Ç–æ–≤—ã –∫ –Ω–∞–ø–∏—Å–∞–Ω–∏—é –æ–±—É—á–∞—é—â–µ–≥–æ —Ü–∏–∫–ª–∞! –ú—ã –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ —Ç–æ–ª—å–∫–æ –¥–≤–µ –≤–µ—â–∏: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (learning rate scheduler). –í–≤–∏–¥—É —Ç–æ–≥–æ, —á—Ç–æ –º—ã –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –≤—Ä—É—á–Ω—É—é —Ç–æ, —á—Ç–æ –¥–µ–ª–∞–ª –∑–∞ –Ω–∞—Å `Trainer`, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–∫–∏–µ –∂–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤ `Trainer` - `AdamW`, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ–π –∫–æ–ø–∏–µ–π Adam, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º —Ç—Ä—é–∫–∞ —Å —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤ (–¥–∞–ª–µ–µ - weight decay) (—Å–º. ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) –∑–∞ –∞–≤—Ç–æ—Ä—Å—Ç–≤–æ–º Ilya Loshchilov –∏ Frank Hutter). 

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

–ù–∞–∫–æ–Ω–µ—Ü, –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø—Ä–æ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –≤–µ—Å–æ–≤ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è (5e-5) –¥–æ 0. –ß—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–¥–∞—Ç—å –µ–≥–æ, –Ω–∞–º –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å —á–∏—Å–ª–æ —à–∞–≥–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–¥–∞–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ—Ö –∏ —á–∏—Å–ª–∞ –±–∞—Ç—á–µ–π (–¥–ª–∏–Ω—ã –Ω–∞—à–µ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö). –ß–∏—Å–ª–æ —ç–ø–æ—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ `Trainer` —Ä–∞–≤–Ω–æ 3, —Ç–∞–∫ –∂–µ –º—ã –∑–∞–¥–∞–¥–∏–º –µ–≥–æ –∏ —Å–µ–π—á–∞—Å: 

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### –û–±—É—á–∞—é—â–∏–π —Ü–∏–∫–ª–∞

–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–æ–º–µ–Ω—Ç: –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ —É –Ω–∞—Å –±—É–¥–µ—Ç —Ç–∞–∫–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å (–Ω–∞ CPU –ø—Ä–æ—Ü–µ—Å—Å –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ –≤–º–µ—Å—Ç–æ –ø–∞—Ä—ã –º–∏–Ω—É—Ç). –ß—Ç–æ–±—ã –¥–æ–±–∏—Ç—å—Å—è —ç—Ç–æ–≥–æ, –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `device` –∏ ¬´–ø—Ä–∏–∫—Ä–µ–ø–∏–º¬ª –∫ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–µ –Ω–∞—à—É –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

–¢–µ–ø–µ—Ä—å –º—ã –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏! –ß—Ç–æ–±—ã –∏–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å, –º—ã –¥–æ–±–∞–≤–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ. –≠—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–∏–ª–∏–æ—Ç–µ–∫–∏ tqdm:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

–í—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –≤—ã–≥–ª—è–¥–∏—Ç –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–º –Ω–∞ —Ç–æ, –∫–∞–∫ –æ–Ω –≤—ã–≥–ª—è–¥–µ–ª –≤ –Ω–∞—à–∏—Ö –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –ú—ã –Ω–µ —É–∫–∞–∑—ã–≤–∞–ª–∏ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–∞–º —á—Ç–æ-—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–ª–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –¥–æ–±–∞–≤–∏–º —Ü–∏–∫–ª –≤–∞–ª–∏–¥–∞—Ü–∏–∏. 

### –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Ü–∏–∫–ª

–†–∞–Ω–µ–µ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –º–µ—Ç—Ä–∏–∫—É, –∫–æ—Ç–æ—Ä—É—é –Ω–∞–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–ª–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Evaluate. –ú—ã —É–∂–µ –∑–Ω–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å –º–µ—Ç–æ–¥ `metric.compute()`, –æ–¥–Ω–∞–∫–æ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–≥—É—Ç –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –±–∞—Ç—á—É, –¥–ª—è —ç—Ç–æ–≥–æ –µ—Å—Ç—å –º–µ—Ç–æ–¥ `add_batch()`. –ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º, –º—ã —Å–º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–º–æ—â—å—é `metric.compute()`. –í–æ—Ç –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ —ç—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ —Ü–∏–∫–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

–ü–æ–≤—Ç–æ—Ä–∏–º: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—É—á–∏—Ç–µ –≤—ã, –º–æ–≥—É—Ç –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –∏–∑-–∑–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª–æ—è –º–æ–¥–µ–ª–∏ –∏ –∏–∑-–∑–∞ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞, –æ–¥–Ω–∞–∫–æ –∏—Ö –ø–æ—Ä—è–¥–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å.

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ò–∑–º–µ–Ω–∏—Ç–µ –æ–±—É—á–∞—é—â–∏–π —Ü–∏–∫–ª —Ç–∞–∫, —á—Ç–æ–±—ã –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ SST-2. 

</Tip>

### –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ —Ü–∏–∫–ª–∞ —Å –ø–æ–º–æ—â—å—é ü§ó Accelerate

<Youtube id="s7dy8QRgjJ0" />

–û–±—É—á–∞—é—â–∏–π —Ü–∏–∫–ª, –∑–∞–¥–∞–Ω–Ω—ã–π –≤—ã—à–µ, –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –æ–¥–Ω–æ–º GPU –∏–ª–∏ CPU. –û–¥–Ω–∞–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [ü§ó Accelerate](https://github.com/huggingface/accelerate) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ —Å–¥–µ–ª–∞—Ç—å —ç—Ç—É –ø—Ä–æ—Ü–µ–¥—É—Ä—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU –∏–ª–∏ TPU. –ù–∞—á–∏–∏–Ω–∞—è —Å –º–æ–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—à ¬´—Ä—É—á–Ω–æ–π¬ª –æ–±—É—á–∞—é—â–∏–π —Ü–∏–∫–ª –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫: 

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

–ê –≤–æ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –≤–Ω–µ—Å—Ç–∏, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å: 

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äì —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏. –í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç `Accelerator`, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. ü§ó Accelerate –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ, –ø–æ—ç—Ç–æ–º—É –≤—ã –º–æ–∂–µ—Ç–µ —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ ¬´–ø—Ä–∏–∫—Ä–µ–ø–ª—è—é—Ç¬ª
 –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ –∫ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–µ (–∏–ª–∏, –µ—Å–ª–∏ –≤–∞–º —Ç–∞–∫ —É–¥–æ–±–Ω–µ–µ, –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∏—Ö –Ω–∞ `accelerator.device` –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ `device`). 

–î–∞–ª–µ–µ –≥–ª–∞–≤–Ω–∞—è —á–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ —Å—Ç—Ä–æ–∫–µ, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ, –º–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ `accelerator.prepare()`. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ ¬´–æ–±–µ—Ä–Ω–µ—Ç¬ª –≤–∞—à–∏ –æ–±—ä–µ–∫—Ç—ã –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∏ —É–±–µ–¥–∏—Ç—Å—è, —á—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –û—Å—Ç–∞–≤—à–∏–µ—Å—è –∏–∑–º–µ–Ω–µ–Ω–∏—è ‚Äì —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –±–∞—Ç—á –Ω–∞ `device` (–ø–æ–≤—Ç–æ—Ä–∏–º: –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å —ç—Ç—É —Å—Ç—Ä–æ–∫—É, –∑–∞–º–µ–Ω–∏—Ç–µ `device` –Ω–∞ `accelerator.device`) –∏ –∑–∞–º–µ–Ω–∏—Ç–µ `loss.backward()` –Ω–∞ `accelerator.backward(loss)`.

<Tip>
‚ö†Ô∏è –ß—Ç–æ–±—ã –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–º –æ–±–ª–∞—á–Ω—ã–º–∏ TPU, –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ `padding="max_length"` –∏ `max_length` —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.
</Tip>

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç –∫–æ–¥, —ç—Ç–æ –ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ü§ó Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –≤ —Å–∫—Ä–∏–ø—Ç `train.py` —Å–¥–µ–ª–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –¥–ª—è –ª—é–±–æ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ–≥–æ –Ω–∞ –≤–∞—à–µ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ:  


```bash
accelerate config
```

—ç—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –≤–∞—à–∏ –æ—Ç–≤–µ—Ç—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –∫–æ–º–∞–Ω–¥—ã: 

which will prompt you to answer a few questions and dump your answers in a configuration file used by this command:

```
accelerate launch train.py
```

–∑–∞–ø—É—Å–∫–∞—é—â–µ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. 

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç –∫–æ–¥ –≤ Jupyter Notebook (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ —Å TPU –Ω–∞ Google Colab), –ø—Ä–æ—Å—Ç–æ –≤—Å—Ç–∞–≤—å—Ç–µ –∫–æ–¥ –≤ `training_function()` –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ–¥–Ω—é—é —è—á–µ–π–∫—É:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

–í—ã –º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ [ü§ó Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples).
