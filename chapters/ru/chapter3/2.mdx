<FrameworkSwitchCourse {fw} />

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
–ü—Ä–æ–¥–æ–ª–∂–∏–º —Å –ø—Ä–∏–º–µ—Ä–æ–º –∏–∑ [–ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤—ã](/course/ru/chapter2)
Continuing with the example from the [previous chapter](/course/ru/chapter2), –≤–æ—Ç –∫–∞–∫ –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ —Å –ø–æ–º–æ—â—å—é PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# –¢–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# –≠—Ç–∞ —á–∞—Å—Ç—å –Ω–æ–≤–∞—è
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Continuing with the example from the [previous chapter](/course/ru/chapter2), –≤–æ—Ç –∫–∞–∫ –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ —Å –ø–æ–º–æ—â—å—é TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# –¢–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# –≠—Ç–∞ —á–∞—Å—Ç—å –Ω–æ–≤–∞—è
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –ª–∏—à—å –Ω–∞ –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ–Ω–µ—á–Ω–æ, –Ω–µ –¥–∞—Å—Ç —Ö–æ—Ä–æ—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤–∞–º —Å–ª–µ–¥—É–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –±_–æ_–ª—å—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç.

–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ MRPC (Microsoft Research Paraphrase Corpus) dataset, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –≤ [—Å—Ç–∞—Ç—å–µ](https://www.aclweb.org/anthology/I05-5002.pdf) –∞–≤—Ç–æ—Ä–∞–º–∏ William B. Dolan –∏ Chris Brockett. –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 5801 –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∏–º –ª–µ–π–±–ª–æ–º: —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞—Ä–∞ –ø—Ä–µ–ª–æ–∂–µ–Ω–∏–π –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∞–º–∏ –∏–ª–∏ –Ω–µ—Ç (—Ç.–µ. –∏–¥–µ—Ç –ª–∏ —Ä–µ—á—å –≤ –æ–±–æ–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ). –ú—ã –≤—ã–±—Ä–∞–ª–∏ –∏–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —ç—Ç–æ–π –≥–ª–∞–≤—ã –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –Ω–µ–±–æ–ª—å—à–æ–π: —Å –Ω–∏–º –ª–µ–≥–∫–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.

### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏, —Ç–∞–º —Ç–∞–∫–∂–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –í—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –Ω–∏—Ö [—Ç—É—Ç](https://huggingface.co/datasets), –∞ —Ç–∞–∫–∂–µ –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–æ–ø—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã –∏–∑—É—á–∏—Ç–µ —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–¥–µ–ª (—Å–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é [–∑–¥–µ—Å—å](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). –ù–æ —Å–µ–π—á–∞—Å –≤–µ—Ä–Ω–µ–º—Å—è –∫ –¥–∞—Ç–∞—Å–µ—Ç—É MRPC! –≠—Ç–æ –æ–¥–∏–Ω –∏–∑ 10 –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏–∑ —Å–æ—Å—Ç–∞–≤–∞ [GLUE](https://gluebenchmark.com/), –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Datasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—É—é –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hub. –ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

–ö–∞–∫ –º–æ–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞ `DatasetDict`, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É. –ö–∞–∂–¥–∞—è –∏–∑ –Ω–∏—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ (`sentence1`, `sentence2`, `label`, –∏ `idx`) –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å —á–∏—Å–ª–æ–º —Å—Ç—Ä–æ–∫ (—á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–π –≤—ã–±–æ—Ä–∫–µ): 3668 –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏, 408 –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏ 1725 –≤ —Ç–µ—Å—Ç–æ–≤–æ–π .

–≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ *~/.cache/huggingface/dataset*). –í—Å–ø–æ–º–Ω–∏–º –∏–∑ –≥–ª–∞–≤—ã 2, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É –∏–∑–º–µ–Ω–∏–≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `HF_HOME`. 

–ú—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –≤ –æ–±—ä–µ–∫—Ç–µ `raw_datasets` –ø—É—Ç–µ–º –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–∞–∫ –≤ —Å–ª–æ–≤–∞—Ä–µ: 

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

–ú–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –ª–µ–π–±–ª—ã —É–∂–µ —è–≤–ª—è—é—Ç—Å—è —Ü–µ–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏ (integer), –∏—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ. –ß—Ç–æ–±—ã —Å–æ–ø–æ—Å–æ—Ç–∞–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ —Å –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –º–æ–∂–Ω–æ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π `features` —É `raw_train_dataset`: 

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è `label` —Ç–∏–ø–∞ `ClassLabel` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–º–µ–Ω–∞–º –≤ *names*. `0` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç `not_equivalent`, `1` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç `equivalent`. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ 15-–π —ç–ª–µ–º–µ–Ω—Ç –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –Ω–∞ 87-–π —ç–ª–µ–º–µ–Ω—Ç –≤–∞–¥–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏. –ö–∞–∫–∏–µ —É –Ω–∏—Ö –ª–µ–π–±–ª—ã?

</Tip>

### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

–ß—Ç–æ–±—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å. –ö–∞–∫ –≤—ã –≤–∏–¥–µ–ª–∏ –≤ [–ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤–µ](/course/ru/chapter2), —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞. –ú—ã –º–æ–∂–µ–º –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—É –æ–¥–Ω–æ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ç.–µ. –º–æ–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–ø–∞—Ä–Ω–æ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

–û–¥–Ω–∞–∫–æ –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –¥–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ —Ç–æ–≥–æ, —è–≤–ª—è—é—Ç—Å—è –ª–∏ —ç—Ç–∏ –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∞–º–∏ –∏–ª–∏ –Ω–µ—Ç. –ù–∞–º –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –ø–∞—Ä—É –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É. –ö —Å—á–∞—Å—Ç—å—é, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤–∑—è—Ç—å –ø–∞—Ä—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏—Ö —Ç–∞–∫, –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç –Ω–∞—à–∞ –º–æ–¥–µ–ª—å BERT:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

–ú—ã —É–∂–µ –æ–±—Å—É–∂–¥–∞–ª–∏ –∫–ª—é—á–∏ `input_ids` –∏ `attention_mask` –≤ [–≥–ª–∞–≤–µ 2](/course/ru/chapter2), –Ω–æ –Ω–µ —É–ø–æ–º–∏–Ω–∞–ª–∏ –æ `token_type_ids`. –í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª–∏ –∫–∞–∫–∞—è —á–∞—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –∞ –∫–∞–∫–∞—è –≤—Ç–æ—Ä—ã–º. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ 15-–π —ç–ª–µ–º–µ–Ω—Ç –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∫–∞–∫ –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏ –∫–∞–∫ –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏?

</Tip>

–ï—Å–ª–∏ –º—ã –¥–µ–∫–æ–¥–∏—Ä—É–µ–º ID –∏–∑ `input_ids` –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ª–æ–≤–∞: 

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

–º—ã –ø–æ–ª—É—á–∏–º

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

–í–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ: `[CLS] sentence1 [SEP] sentence2 [SEP]` –≤ —Å–ª—É—á–∞–µ –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ü–æ—Å–º–æ—Ç—Ä–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ `token_type_ids`

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

–ö–∞–∫ –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ—Ç–∏—Ç—å, —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö `[CLS] sentence1 [SEP]` –∏–º–µ—é—Ç —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞ `0`, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤—Ç–æ—Ä–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é `sentence2 [SEP]`, –∏–º–µ—é—Ç —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞ `1`.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –µ—Å–ª–∏ –≤—ã –≤—ã–±–µ—Ä–µ—Ç–µ –¥—Ä—É–≥–æ–π —á–µ–∫–ø–æ–∏–Ω—Ç, `token_type_ids` –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –±—É–¥—É—Ç –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –≤–∞—à–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω–∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è, –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–æ–¥–µ–ª—å DistilBERT). –û–Ω–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å, —á—Ç–æ —Å –Ω–∏–º–∏ –¥–µ–ª–∞—Ç—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –≤–∏–¥–µ–ª–∞ –∏—Ö –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è.

–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ BERT –±—ã–ª –æ–±—É—á–µ–Ω —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –∏ –ø–æ–º–∏–º–æ –∑–∞–¥–∞—á–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ –∫–æ—Ç–æ—Ä–æ–π –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –≤ [–≥–ª–∞–≤–µ 1](/course/ru/chapter1), –æ–Ω –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å –µ—â–µ –æ–¥–Ω—É –∑–∞–¥–∞—á—É: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (_next sentence prediction_). –°—É—Ç—å —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ - —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. 

–í —ç—Ç–æ–π –∑–∞–¥–∞—á–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–æ —Å–ª—É—á–∞–π–Ω–æ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏), –æ—Ç –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ç–µ–∫—É—â–µ–≥–æ. –ß—Ç–æ–±—ã –∑–∞–¥–∞—á–∞ –Ω–µ –±—ã–ª–∞ —Å–ª–∏—à–∫–æ–º —Ç—Ä–∏–≤–∏–∞–ª—å–Ω–æ–π, –ø–æ–ª–æ–≤–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –¥—Ä—É–≥—É—é –ø–æ–ª–æ–≤–∏–Ω—É –Ω–∞ –ø–∞—Ä–∞—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤–∑—è—Ç—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. 

–í –æ–±—â–µ–º —Å–ª—É—á–∞–µ –≤–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±–µ—Å–ø–æ–∫–æ–∏—Ç—å—Å—è –æ –Ω–∞–ª–∏—á–∏–∏ `token_type_ids` –≤ –≤–∞—à–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –ø–æ–∫–∞ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç –∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –∏ –¥–ª—è –º–æ–¥–µ–ª–∏ ‚Äì —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å, –∫–∞–∫ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. 

–¢–µ–ø–µ—Ä—å –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å—Ä–∞–∑—É –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∞ –∑–Ω–∞—á–∏—Ç –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è —Ü–µ–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: —Ç–∞–∫ –∂–µ –∫–∞–∫ –∏ –≤ [–ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤–µ](/course/ru/chapter2) –º–æ–∂–Ω–æ –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Å–ø–∏—Å–æ–∫ –≤—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –≠—Ç–æ —Ç–∞–∫–∂–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è (padding) –∏ —É—Å–µ—á–µ–Ω–∏—è –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã (truncation) - –æ–± —ç—Ç–æ–º –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –≤ [–≥–ª–∞–≤–µ 2](/course/chapter2). –ò—Ç–∞–∫, –æ–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Ç–∞–∫–æ–π: 

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

–≠—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –æ–¥–Ω–∞–∫–æ –µ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (—Å –∫–ª—é—á–∞–º–∏,  `input_ids`, `attention_mask`, –∏ `token_type_ids`, –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–ø–∏—Å–∫–∞ —Å–ø–∏—Å–∫–æ–≤). –≠—Ç–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ (RAM) –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–µ–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤–æ –≤—Ä–µ–º—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets —è–≤–ª—è—é—Ç—Å—è [Apache Arrow](https://arrow.apache.org/) —Ñ–∞–π–ª–∞–º–∏, —Ö—Ä–∞–Ω—è—â–∏–º–∏—Å—è –Ω–∞ –¥–∏—Å–∫–µ; –æ–Ω–∏ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Ç–æ–ª—å–∫–æ –≤ —Ç–æ—Ç –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –≤—ã –∏—Ö –±—É–¥–µ—Ç–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å). 

–ß—Ç–æ–±—ã —Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç–∞, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—ã—Å–æ–∫—É—é –≥–∏–±–∫–æ—Å—Ç—å –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∞–º –Ω—É–∂–Ω–æ —á—Ç–æ-—Ç–æ –±–æ–ª—å—à–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è. –ú–µ—Ç–æ–¥ `map()` —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫: –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É –¥–∞—Ç–∞—Å–µ—Ç–∞, –¥–∞–≤–∞–π—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –Ω–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–ª–≤–æ–∞—Ä—å (–ø–æ—Ö–æ–∂–∏–π –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞—à–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ `input_ids`, `attention_mask` –∏ `token_type_ids`. –ó–∞–º–µ—Ç—å—Ç–µ, —ç—Ç–æ —Ç–∞–∫–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –µ—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å `example` —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–π –∫–ª—é—á –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π), –ø–æ—Å–∫–æ–ª—å–∫—É `tokenizer` —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–∞–∫ –º—ã –∏ –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç `batched=True` –≤ –≤—ã–∑–æ–≤–µ `map()`, –∫–æ—Ç–æ—Ä–∞—è —É—Å–∫–æ—Ä–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. `tokenizer` –≤–Ω—É—Ç—Ä–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –Ω–∞ —è–∑—ã–∫–µ Rust –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [ü§ó Tokenizers](https://github.com/huggingface/tokenizers). –≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–º, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º—ã –ø–æ–¥–∞–¥–∏–º –±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –∑–∞ —Ä–∞–∑. 

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –≤ —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –æ—Å—Ç–∞–≤–∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç `padding` –ø—É—Å—Ç—ã–º, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ: –≥–æ—Ä–∞–∑–¥–æ –±—ã—Å—Ç—Ä–µ–µ –¥–µ–ª–∞—Ç—å —ç—Ç–æ –≤–æ –≤—Ä–µ–º—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–∞, –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –º—ã –±—É–¥–µ–º –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Ç–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞, –∞ –Ω–µ —Ü–µ–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –≤—Ä–µ–º—è –≤ —Å–ª—É—á–∞–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. 

–ù–∏–∂–µ –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫ —Ü–µ–ª–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ú—ã —É–∫–∞–∑—ã–≤–∞–µ–º `batched=True` –≤ –Ω–∞—à–µ–º –≤—ã–∑–æ–≤–µ `map` –∏ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —Å—Ä–∞–∑—É –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –∞ –Ω–µ –∫ –∫–∞–∂–¥–æ–º—É –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–µ–ª–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π. 

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Datasets –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É, –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—ã–µ –ø–æ–ª—è –≤ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

–í —Ñ—É–Ω–∫—Ü–∏–∏ `map()` –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥: –∑–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç `num_proc`. –ú—ã –µ–≥–æ –Ω–µ –ø—Ä–∏–º–µ–Ω—è–ª–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ –±–∏–±–ª–∏–æ–µ—Ç–µ–∫–∞ ü§ó Tokenizers —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –Ω–æ –µ—Å–ª–∏ –≤—ã –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –Ω–µ –∏–∑ ü§ó Tokenizers, —ç—Ç–æ –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å. 

–ù–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è `tokenize_function`  –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ `input_ids`, `attention_mask` –∏ `token_type_ids`, –æ–Ω–∏ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫–æ –≤—Å–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏—è–º–∏ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –º–æ–≥–ª–∏ –±—ã —Ç–∞–∫–∂–µ –∏–∑–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è, –µ—Å–ª–∏ –±—ã –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –≤–µ—Ä–Ω—É–ª–∞ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–ª—é—á–∞ –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –º—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ `map()`.

–ü–æ—Å–ª–µ–¥–Ω–µ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –¥–æ –¥–ª–∏–Ω—ã —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞, –∫–æ–≥–¥–∞ –º—ã —Å–æ–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–µ ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –º—ã –Ω–∞–∑—ã–≤–∞–µ–º *–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –ø—ç–¥–¥–∏–Ω–≥–æ–º* (*dynamic padding*).


### Dynamic padding

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
–§—É–Ω–∫—Ü–∏—è, –æ—Ç–≤–µ—á–∞—é—â–∞—è –∑–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞, –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è *collate function* (—Ñ—É–Ω–∫—Ü–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è). –≠—Ç–æ –∞—Ä–≥—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ `DataLoader`, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–∞—à–∏ –æ–±—Ä–∞–∑—Ü—ã –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ, –µ—Å–ª–∏ –≤–∞—à–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∏, –∫–æ—Ä—Ç–µ–∂–∏ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä–∏). –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —É –Ω–∞—Å –µ—Å—Ç—å, –Ω–µ –±—É–¥—É—Ç –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ú—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –Ω–µ —Å—Ç–∞–ª–∏ –¥–µ–ª–∞—Ç—å –ø—ç–¥–¥–∏–Ω–≥, —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω—è—Ç—å –µ–≥–æ —Ç–æ–ª—å–∫–æ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∫–∞–∂–¥–æ–º –ø–∞–∫–µ—Ç–µ –∏ –∏–∑–±–µ–≥–∞—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—Ç—Å—Ç—É–ø–æ–≤. –≠—Ç–æ –Ω–µ–º–Ω–æ–≥–æ —É—Å–∫–æ—Ä–∏—Ç –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ —É—á—Ç–∏—Ç–µ, —á—Ç–æ –µ—Å–ª–∏ –≤—ã —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç–µ—Å—å –Ω–∞ TPU, —ç—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã ‚Äî TPU –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω –ø—ç–¥–¥–∏–Ω–≥.

{:else}
–§—É–Ω–∫—Ü–∏—è, –æ—Ç–≤–µ—á–∞—é—â–∞—è –∑–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞, –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è *collate function* (—Ñ—É–Ω–∫—Ü–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–∞—à–∏ –æ–±—Ä–∞–∑—Ü—ã –≤ tf.Tensor –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ, –µ—Å–ª–∏ –≤–∞—à–∏ —ç–ª–µ–º–µ–Ω—Ç—ã ‚Äî —ç—Ç–æ —Å–ø–∏—Å–∫–∏, –∫–æ—Ä—Ç–µ–∂–∏ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä–∏). –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —É –Ω–∞—Å –µ—Å—Ç—å, –Ω–µ –±—É–¥—É—Ç –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ú—ã –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –æ—Ç–ª–æ–∂–∏–ª–∏ –æ—Ç—Å—Ç—É–ø—ã, —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω—è—Ç—å –∏—Ö —Ç–æ–ª—å–∫–æ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∫–∞–∂–¥–æ–º –ø–∞–∫–µ—Ç–µ –∏ –∏–∑–±–µ–≥–∞—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—Ç—Å—Ç—É–ø–æ–≤. –≠—Ç–æ –Ω–µ–º–Ω–æ–≥–æ —É—Å–∫–æ—Ä–∏—Ç –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ —É—á—Ç–∏—Ç–µ, —á—Ç–æ –µ—Å–ª–∏ –≤—ã —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç–µ—Å—å –Ω–∞ TPU, —ç—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã ‚Äî TPU –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ.

{/if}
–î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ, –º—ã –¥–æ–ª–∂–Ω—ã –∑–∞–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—ç–¥–¥–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤—ã–±–æ—Ä–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –±–∞—Ç—á. –ö —Å—á–∞—Å—Ç—å—é, –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–º —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å `DataCollatorWithPadding`. –ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–∫–∞–∑–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (—á—Ç–æ–±—ã –∑–Ω–∞—Ç—å, –∫–∞–∫–æ–π —Ç–æ–∫–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—ç–¥–¥–∏–Ω–≥–∞ –∏ —Å–ª–µ–≤–∞ –∏–ª–∏ —Å–ø—Ä–∞–≤–∞ –Ω—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥–∞–Ω–Ω—ã–µ), –∞ –¥–∞–ª—å—à–µ —Ñ—É–Ω–∫—Ü–∏—è —Å–¥–µ–ª–∞–µ—Ç –≤—Å–µ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ: 

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}
–ß—Ç–æ–±—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ, –¥–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ö–æ—Ç–∏–º –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –±–∞—Ç—á. –ú—ã —É–¥–∞–ª–∏–º –∫–æ–ª–æ–Ω–∫–∏ `idx`, `sentence1` –∏ `sentence2` —Ç.–∫. –æ–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Ç—Ä–æ–∫–∏ (–∞ –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –≤ —Ç–µ–Ω–∑–æ—Ä—ã) –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –≤ –±–∞—Ç—á–µ:


```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

–ù–µ—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ: –º—ã –ø–æ–ª—É—á–∏–ª–∏ –æ–±—ä–µ–∫—Ç—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –æ—Ç 32 –¥–æ 67. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—ç–¥–¥–∏–Ω–≥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç, —á—Ç–æ –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã –±—É–¥—É—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã, –¥–æ 67. 

No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. –ë–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ –≤—ã–±–æ—Ä–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤–æ –≤—Å–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –∏–ª–∏ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç –ø—Ä–∏–Ω—è—Ç—å –º–æ–¥–µ–ª—å. –î–∞–≤–∞–π—Ç–µ –¥–≤–∞–∂–¥—ã –ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –Ω–∞—à `data_collator` –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–æ–ø–æ–ª–Ω—è–µ—Ç –±–∞—Ç—á:
```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

–í—ã–≥–ª—è–¥–∏—Ç –Ω–µ–ø–ª–æ—Ö–æ! –¢–µ–ø–µ—Ä—å –º—ã –ø—Ä–∏—à–ª–∏ –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∫ –±–∞—Ç—á—É, —Å –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞—à–∞ –º–æ–¥–µ–ª—å. –ú–æ–∂–µ–º –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ fine-tuning!

{/if}

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ —ç—Ç–∞–ø –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –¥–ª—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö GLUE SST-2. –û–Ω –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∞ –Ω–µ –ø–∞—Ä, –Ω–æ –≤ –æ—Å—Ç–∞–ª—å–Ω–æ–º —ç—Ç–æ —Ç–æ –∂–µ —Å–∞–º–æ–µ, —á—Ç–æ –º—ã —Å–¥–µ–ª–∞–ª–∏. –î–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π –∏–∑ –∑–∞–¥–∞—á GLUE.

</Tip>

{#if fw === 'tf'}

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –∏—Ö –≤–º–µ—Å—Ç–µ. –ú—ã –º–æ–≥–ª–∏ –±—ã –≤—Ä—É—á–Ω—É—é –∑–∞–≥—Ä—É–∂–∞—Ç—å –ø–∞–∫–µ—Ç—ã –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ –Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é, –Ω–æ —ç—Ç–æ –Ω–µ—É–¥–æ–±–Ω–æ –∏ –Ω–µ –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –µ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥, –ø—Ä–µ–¥–ª–∞–≥–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã: `to_tf_dataset()`. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ–±–µ—Ä–Ω–µ—Ç `tf.data.Dataset` –≤–æ–∫—Ä—É–≥ –≤–∞—à–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è. `tf.data.Dataset` ‚Äî —ç—Ç–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç TensorFlow, –∫–æ—Ç–æ—Ä—ã–π Keras –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è `model.fit()`, –ø–æ—ç—Ç–æ–º—É —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç ü§ó –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç, –≥–æ—Ç–æ–≤—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —ç—Ç–æ –≤ –¥–µ–π—Å—Ç–≤–∏–∏ —Å –Ω–∞—à–∏–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

–≠—Ç–æ –≤—Å–µ! –ú—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç—ã –≤ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏, –≥–¥–µ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—ã–º –ø–æ—Å–ª–µ —Ä—É—Ç–∏–Ω–Ω–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏. 

{/if}
