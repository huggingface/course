<FrameworkSwitchCourse {fw} />

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
–ü—Ä–æ–¥–æ–ª–∂–∏–º —Å –ø—Ä–∏–º–µ—Ä–æ–º –∏–∑ [–ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤—ã](/course/chapter2)
Continuing with the example from the [previous chapter](/course/chapter2), –≤–æ—Ç –∫–∞–∫ –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ —Å –ø–æ–º–æ—â—å—é PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# –¢–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# –≠—Ç–∞ —á–∞—Å—Ç—å –Ω–æ–≤–∞—è
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Continuing with the example from the [previous chapter](/course/chapter2), –≤–æ—Ç –∫–∞–∫ –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ —Å –ø–æ–º–æ—â—å—é TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# –¢–∞–∫ –∂–µ, –∫–∞–∫ –∏ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# –≠—Ç–∞ —á–∞—Å—Ç—å –Ω–æ–≤–∞—è
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –ª–∏—à—å –Ω–∞ –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ–Ω–µ—á–Ω–æ, –Ω–µ –¥–∞—Å—Ç —Ö–æ—Ä–æ—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞. –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤–∞–º —Å–ª–µ–¥—É–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –±_–æ_–ª—å—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç.

–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ MRPC (Microsoft Research Paraphrase Corpus) dataset, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –≤ [—Å—Ç–∞—Ç—å–µ](https://www.aclweb.org/anthology/I05-5002.pdf) –∞–≤—Ç–æ—Ä–∞–º–∏ William B. Dolan –∏ Chris Brockett. –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 5801 –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∏–º –ª–µ–π–±–ª–æ–º: —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞—Ä–∞ –ø—Ä–µ–ª–æ–∂–µ–Ω–∏–π –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∞–º–∏ –∏–ª–∏ –Ω–µ—Ç (—Ç.–µ. –∏–¥–µ—Ç –ª–∏ —Ä–µ—á—å –≤ –æ–±–æ–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ). –ú—ã –≤—ã–±—Ä–∞–ª–∏ –∏–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —ç—Ç–æ–π –≥–ª–∞–≤—ã –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –Ω–µ–±–æ–ª—å—à–æ–π: —Å –Ω–∏–º –ª–µ–≥–∫–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.

### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏, —Ç–∞–º —Ç–∞–∫–∂–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –í—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –Ω–∏—Ö [—Ç—É—Ç](https://huggingface.co/datasets), –∞ —Ç–∞–∫–∂–µ –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–æ–ø—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã –∏–∑—É—á–∏—Ç–µ —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–¥–µ–ª (—Å–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é [–∑–¥–µ—Å—å](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). –ù–æ —Å–µ–π—á–∞—Å –≤–µ—Ä–Ω–µ–º—Å—è –∫ –¥–∞—Ç–∞—Å–µ—Ç—É MRPC! –≠—Ç–æ –æ–¥–∏–Ω –∏–∑ 10 –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏–∑ —Å–æ—Å—Ç–∞–≤–∞ [GLUE](https://gluebenchmark.com/), –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Datasets –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—É—é –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Hub. –ú—ã –º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

–ö–∞–∫ –º–æ–∂–Ω–æ –∑–∞–º–µ—Ç–∏—Ç—å, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞ `DatasetDict`, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É. –ö–∞–∂–¥–∞—è –∏–∑ –Ω–∏—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ (`sentence1`, `sentence2`, `label`, –∏ `idx`) –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å —á–∏—Å–ª–æ–º —Å—Ç—Ä–æ–∫ (—á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–π –≤—ã–±–æ—Ä–∫–µ): 3668 –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–±—É—á–∞—é—â–µ–π —á–∞—Å—Ç–∏, 408 –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏ 1725 –≤ —Ç–µ—Å—Ç–æ–≤–æ–π .

–≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ *~/.cache/huggingface/dataset*). –í—Å–ø–æ–º–Ω–∏–º –∏–∑ –≥–ª–∞–≤—ã 2, —á—Ç–æ –≤—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É –∏–∑–º–µ–Ω–∏–≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `HF_HOME`. 

–ú—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –≤ –æ–±—ä–µ–∫—Ç–µ `raw_datasets` –ø—É—Ç–µ–º –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–∞–∫ –≤ —Å–ª–æ–≤–∞—Ä–µ: 

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

–ú–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –ª–µ–π–±–ª—ã —É–∂–µ —è–≤–ª—è—é—Ç—Å—è —Ü–µ–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏ (integer), –∏—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ. –ß—Ç–æ–±—ã —Å–æ–ø–æ—Å–æ—Ç–∞–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ —Å –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –º–æ–∂–Ω–æ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π `features` —É `raw_train_dataset`: 

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è `label` —Ç–∏–ø–∞ `ClassLabel` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–º–µ–Ω–∞–º –≤ *names*. `0` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç `not_equivalent`, `1` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç `equivalent`. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ 15-–π —ç–ª–µ–º–µ–Ω—Ç –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –Ω–∞ 87-–π —ç–ª–µ–º–µ–Ω—Ç –≤–∞–¥–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏. –ö–∞–∫–∏–µ —É –Ω–∏—Ö –ª–µ–π–±–ª—ã?

</Tip>

### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

–ß—Ç–æ–±—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å. –ö–∞–∫ –≤—ã –≤–∏–¥–µ–ª–∏ –≤ [–ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤–µ](/course/chapter2), —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞. –ú—ã –º–æ–∂–µ–º –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä—É –æ–¥–Ω–æ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ç.–µ. –º–æ–∂–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–ø–∞—Ä–Ω–æ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º: 

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

–û–¥–Ω–∞–∫–æ –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞—Ç—å –¥–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ —Ç–æ–≥–æ, —è–≤–ª—è—é—Ç—Å—è –ª–∏ —ç—Ç–∏ –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∞–º–∏ –∏–ª–∏ –Ω–µ—Ç. –ù–∞–º –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –ø–∞—Ä—É –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É. –ö —Å—á–∞—Å—Ç—å—é, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤–∑—è—Ç—å –ø–∞—Ä—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏—Ö —Ç–∞–∫, –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç –Ω–∞—à–∞ –º–æ–¥–µ–ª—å BERT:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

–ú—ã —É–∂–µ –æ–±—Å—É–∂–¥–∞–ª–∏ –∫–ª—é—á–∏ `input_ids` –∏ `attention_mask` –≤ [–≥–ª–∞–≤–µ 2](/course/chapter2), –Ω–æ –Ω–µ —É–ø–æ–º–∏–Ω–∞–ª–∏ –æ `token_type_ids`. –í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª–∏ –∫–∞–∫–∞—è —á–∞—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –∞ –∫–∞–∫–∞—è –≤—Ç–æ—Ä—ã–º. 

<Tip>

‚úèÔ∏è **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ!** –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ 15-–π —ç–ª–µ–º–µ–Ω—Ç –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∫–∞–∫ –¥–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏ –∫–∞–∫ –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –í —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏?

</Tip>

–ï—Å–ª–∏ –º—ã –¥–µ–∫–æ–¥–∏—Ä—É–µ–º ID –∏–∑ `input_ids` –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ª–æ–≤–∞: 

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

–º—ã –ø–æ–ª—É—á–∏–º

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

–í–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ: `[CLS] sentence1 [SEP] sentence2 [SEP]` –≤ —Å–ª—É—á–∞–µ –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ü–æ—Å–º–æ—Ç—Ä–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ `token_type_ids`

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

–ö–∞–∫ –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ—Ç–∏—Ç—å, —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö `[CLS] sentence1 [SEP]` –∏–º–µ—é—Ç —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞ `0`, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –≤—Ç–æ—Ä–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é `sentence2 [SEP]`, –∏–º–µ—é—Ç —Ç–∏–ø —Ç–æ–∫–µ–Ω–∞ `1`.

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –µ—Å–ª–∏ –≤—ã –≤—ã–±–µ—Ä–µ—Ç–µ –¥—Ä—É–≥–æ–π —á–µ–∫–ø–æ–∏–Ω—Ç, `token_type_ids` –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –±—É–¥—É—Ç –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –≤–∞—à–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω–∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è, –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–æ–¥–µ–ª—å DistilBERT). –û–Ω–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å, —á—Ç–æ —Å –Ω–∏–º–∏ –¥–µ–ª–∞—Ç—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –≤–∏–¥–µ–ª–∞ –∏—Ö –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è.

–í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ BERT –±—ã–ª –æ–±—É—á–µ–Ω —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –∏ –ø–æ–º–∏–º–æ –∑–∞–¥–∞—á–∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ –∫–æ—Ç–æ—Ä–æ–π –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –≤ [–≥–ª–∞–≤–µ 1](/course/chapter1), –æ–Ω –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å –µ—â–µ –æ–¥–Ω—É –∑–∞–¥–∞—á—É: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (_next sentence prediction_). –°—É—Ç—å —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ - —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. 

–í —ç—Ç–æ–π –∑–∞–¥–∞—á–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Å–æ —Å–ª—É—á–∞–π–Ω–æ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏), –æ—Ç –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ç–µ–∫—É—â–µ–≥–æ. –ß—Ç–æ–±—ã –∑–∞–¥–∞—á–∞ –Ω–µ –±—ã–ª–∞ —Å–ª–∏—à–∫–æ–º —Ç—Ä–∏–≤–∏–∞–ª—å–Ω–æ–π, –ø–æ–ª–æ–≤–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –¥—Ä—É–≥—É—é –ø–æ–ª–æ–≤–∏–Ω—É –Ω–∞ –ø–∞—Ä–∞—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤–∑—è—Ç—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. 

–í –æ–±—â–µ–º —Å–ª—É—á–∞–µ –≤–∞–º –Ω–µ –Ω—É–∂–Ω–æ –±–µ—Å–ø–æ–∫–æ–∏—Ç—å—Å—è –æ –Ω–∞–ª–∏—á–∏–∏ `token_type_ids` –≤ –≤–∞—à–∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –ø–æ–∫–∞ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç –∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, –∏ –¥–ª—è –º–æ–¥–µ–ª–∏ ‚Äì —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å, –∫–∞–∫ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. 

–¢–µ–ø–µ—Ä—å –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å—Ä–∞–∑—É –ø–∞—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∞ –∑–Ω–∞—á–∏—Ç –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –¥–ª—è —Ü–µ–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: —Ç–∞–∫ –∂–µ –∫–∞–∫ –∏ –≤ [–ø—Ä–µ–¥—ã–¥—É—â–µ–π –≥–ª–∞–≤–µ](/course/chapter2) –º–æ–∂–Ω–æ –ø–æ–¥–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Å–ø–∏—Å–æ–∫ –≤—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –≠—Ç–æ —Ç–∞–∫–∂–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è (padding) –∏ —É—Å–µ—á–µ–Ω–∏—è –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã (truncation) - –æ–± —ç—Ç–æ–º –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –≤ [–≥–ª–∞–≤–µ 2](/course/chapter2). –ò—Ç–∞–∫, –æ–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Ç–∞–∫–æ–π: 

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

–≠—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –æ–¥–Ω–∞–∫–æ –µ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (—Å –∫–ª—é—á–∞–º–∏,  `input_ids`, `attention_mask`, –∏ `token_type_ids`, –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–ø–∏—Å–∫–∞ —Å–ø–∏—Å–∫–æ–≤). –≠—Ç–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ (RAM) –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–µ–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤–æ –≤—Ä–µ–º—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Datasets —è–≤–ª—è—é—Ç—Å—è [Apache Arrow](https://arrow.apache.org/) —Ñ–∞–π–ª–∞–º–∏, —Ö—Ä–∞–Ω—è—â–∏–º–∏—Å—è –Ω–∞ –¥–∏—Å–∫–µ; –æ–Ω–∏ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Ç–æ–ª—å–∫–æ –≤ —Ç–æ—Ç –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –≤—ã –∏—Ö –±—É–¥–µ—Ç–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å). 

–ß—Ç–æ–±—ã —Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç–∞, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—ã—Å–æ–∫—É—é –≥–∏–±–∫–æ—Å—Ç—å –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∞–º –Ω—É–∂–Ω–æ —á—Ç–æ-—Ç–æ –±–æ–ª—å—à–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è. –ú–µ—Ç–æ–¥ `map()` —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫: –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É –¥–∞—Ç–∞—Å–µ—Ç–∞, –¥–∞–≤–∞–π—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –Ω–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–ª–≤–æ–∞—Ä—å (–ø–æ—Ö–æ–∂–∏–π –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞—à–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ `input_ids`, `attention_mask` –∏ `token_type_ids`. –ó–∞–º–µ—Ç—å—Ç–µ, —ç—Ç–æ —Ç–∞–∫–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –µ—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å `example` —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–π –∫–ª—é—á –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π), –ø–æ—Å–∫–æ–ª—å–∫—É `tokenizer` —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–∞–∫ –º—ã –∏ –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω–µ–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç `batched=True` –≤ –≤—ã–∑–æ–≤–µ `map()`, –∫–æ—Ç–æ—Ä–∞—è —É—Å–∫–æ—Ä–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. `tokenizer` –≤–Ω—É—Ç—Ä–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –Ω–∞ —è–∑—ã–∫–µ Rust –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [ü§ó Tokenizers](https://github.com/huggingface/tokenizers). –≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–º, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º—ã –ø–æ–¥–∞–¥–∏–º –±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –∑–∞ —Ä–∞–∑. 

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –≤ —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –æ—Å—Ç–∞–≤–∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç `padding` –ø—É—Å—Ç—ã–º, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ: –≥–æ—Ä–∞–∑–¥–æ –±—ã—Å—Ç—Ä–µ–µ –¥–µ–ª–∞—Ç—å —ç—Ç–æ –≤–æ –≤—Ä–µ–º—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–∞, –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –º—ã –±—É–¥–µ–º –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Ç–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞, –∞ –Ω–µ —Ü–µ–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –≤—Ä–µ–º—è –≤ —Å–ª—É—á–∞–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. 

–ù–∏–∂–µ –ø—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫ —Ü–µ–ª–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ú—ã —É–∫–∞–∑—ã–≤–∞–µ–º `batched=True` –≤ –Ω–∞—à–µ–º –≤—ã–∑–æ–≤–µ `map` –∏ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —Å—Ä–∞–∑—É –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –∞ –Ω–µ –∫ –∫–∞–∂–¥–æ–º—É –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–µ–ª–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π. 

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ü§ó Datasets –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É, –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—ã–µ –ø–æ–ª—è –≤ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

–í —Ñ—É–Ω–∫—Ü–∏–∏ `map()` –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥: –∑–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç `num_proc`. –ú—ã –µ–≥–æ –Ω–µ –ø—Ä–∏–º–µ–Ω—è–ª–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ –±–∏–±–ª–∏–æ–µ—Ç–µ–∫–∞ ü§ó Tokenizers —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –Ω–æ –µ—Å–ª–∏ –≤—ã –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –Ω–µ –∏–∑ ü§ó Tokenizers, —ç—Ç–æ –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å. 

–ù–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è `tokenize_function`  –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ `input_ids`, `attention_mask` –∏ `token_type_ids`, –æ–Ω–∏ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫–æ –≤—Å–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏—è–º–∏ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –º–æ–≥–ª–∏ –±—ã —Ç–∞–∫–∂–µ –∏–∑–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è, –µ—Å–ª–∏ –±—ã –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –≤–µ—Ä–Ω—É–ª–∞ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–ª—é—á–∞ –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –º—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ `map()`.

–ü–æ—Å–ª–µ–¥–Ω–µ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –¥–æ –¥–ª–∏–Ω—ã —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞, –∫–æ–≥–¥–∞ –º—ã —Å–æ–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–µ ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –º—ã –Ω–∞–∑—ã–≤–∞–µ–º *–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º* (*dynamic padding*).

The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together ‚Äî a technique we refer to as *dynamic padding*.

### Dynamic padding

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
The function that is responsible for putting together samples inside a batch is called a *collate function*. It's an argument you can pass when you build a `DataLoader`, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won't be possible in our case since the inputs we have won't all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems ‚Äî TPUs prefer fixed shapes, even when that requires extra padding.

{:else}

The function that is responsible for putting together samples inside a batch is called a *collate function*. The default collator is a function that will just convert your samples to tf.Tensor and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won't be possible in our case since the inputs we have won't all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems ‚Äî TPUs prefer fixed shapes, even when that requires extra padding.

{/if}

To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ü§ó Transformers library provides us with such a function via `DataCollatorWithPadding`. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

To test this new toy, let's grab a few samples from our training set that we would like to batch together. Here, we remove the columns `idx`, `sentence1`, and `sentence2` as they won't be needed and contain strings (and we can't create tensors with strings) and have a look at the lengths of each entry in the batch:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Let's double-check that our `data_collator` is dynamically padding the batch properly:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Looking good! Now that we've gone from raw text to batches our model can deal with, we're ready to fine-tune it!

{/if}

<Tip>

‚úèÔ∏è **Try it out!** Replicate the preprocessing on the GLUE SST-2 dataset. It's a little bit different since it's composed of single sentences instead of pairs, but the rest of what we did should look the same. For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks.

</Tip>

{#if fw === 'tf'}

Now that we have our dataset and a data collator, we need to put them together. We could manually load batches and collate them, but that's a lot of work, and probably not very performant either. Instead, there's a simple method that offers a performant solution to this problem: `to_tf_dataset()`. This will wrap a `tf.data.Dataset` around your dataset, with an optional collation function. `tf.data.Dataset` is a native TensorFlow format that Keras can use for `model.fit()`, so this one method immediately converts a ü§ó Dataset to a format that's ready for training. Let's see it in action with our dataset!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

And that's it! We can take those datasets forward into the next lecture, where training will be pleasantly straightforward after all the hard work of data preprocessing.

{/if}
