<FrameworkSwitchCourse {fw} />

<!-- DISABLE-FRONTMATTER-SECTIONS -->

# అధ్యాయం ముగింపు క్విజ్[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={2}
    classNames="absolute z-10 right-0 top-0"
/>

### 1. Language modeling పైప్లైన్‌లో స్టెప్స్ సరైన క్రమం ఏది?

<Question
	choices={[
		{
			text: "మొదట మోడల్ text ను నేరుగా హ్యాండిల్ చేసి raw predictions ఇస్తుందని చెబుతోంది. తరువాత tokenizer ఆ predictions కి అర్థం వచ్చేలా మార్చి అవసరమైతే తిరిగి text గా మార్చుతుంది.",
			explain: "మోడల్ కి text అర్థం కాదు! ముందు tokenizer text ని tokenize చేసి IDs గా మార్చాలి, అప్పుడు మాత్రమే మోడల్ దానిని అర్థం చేసుకోగలదు."
		},
		{
			text: "మొదట tokenizer text ని హ్యాండిల్ చేసి IDs ఇస్తుంది. మోడల్ ఈ IDs ను హ్యాండిల్ చేసి prediction ఇస్తుంది, అది కొన్ని text అయి ఉండవచ్చు.",
			explain: "మోడల్ నుండి వచ్చే prediction వెంటనే text రూపంలో ఉండదు. ఆ prediction ని తిరిగి text గా మార్చడానికి tokenizer ని మళ్ళీ ఉపయోగించాలి!"
		},
		{
			text: "Tokenizer text ని హ్యాండిల్ చేసి IDs ఇస్తుంది. మోడల్ ఆ IDs ను హ్యాండిల్ చేసి prediction ఇస్తుంది. తరువాత tokenizer మళ్ళీ ఆ predictions ను text గా మార్చగలదు.",
			explain: "Tokenizer ని tokenizing కోసమే కాకుండా de-tokenizing కోసమూ ఉపయోగించవచ్చు.",
            correct: true
		}
	]}
/>

### 2. Base Transformer model output గా ఇచ్చే tensor కి ఎన్ని dimensions ఉంటాయి? అవి ఏవి?

<Question
	choices={[
		{
			text: "2: sequence length మరియు batch size",
			explain: "కాదు! మోడల్ output tensor కి మూడో dimension కూడా ఉంటుంది: hidden size."
		},
		{
			text: "2: sequence length మరియు hidden size",
			explain: "కాదు! అన్ని Transformer models batches ని హ్యాండిల్ చేస్తాయి—even ఒకే sequence ఉన్నప్పుడు కూడా అది batch size 1 గా తీసుకుంటాయి!"
		},
		{
			text: "3: sequence length, batch size, మరియు hidden size",
			explain: "చాలా బాగా చెప్పారు!",
            correct: true
		}
	]}
/>

### 3. క్రిందివాటిలో subword tokenization కి ఉదాహరణలు ఏవి?

<Question
	choices={[
		{
			text: "WordPiece",
			explain: "అవును, ఇది subword tokenization కి ఒక ఉదాహరణ!",
            correct: true
		},
		{
			text: "Character-based tokenization",
			explain: "Character-based tokenization అనేది subword tokenization రకం కాదు."
		},
		{
			text: "Whitespace మరియు punctuation మీద split చేయడం",
			explain: "అది ఒక word-based tokenization scheme!"
		},
		{
			text: "BPE",
			explain: "అవును, ఇది subword tokenization కి ఒక ఉదాహరణ!",
            correct: true
        },
		{
			text: "Unigram",
			explain: "అవును, ఇది subword tokenization కి ఒక ఉదాహరణ!",
            correct: true
        },
		{
			text: "ఇవేవీ కాదు",
			explain: "తప్పు!"
        }
	]}
/>

### 4. Model head అంటే ఏమిటి?

<Question
	choices={[
		{
			text: "Base Transformer network లో ఉండే, tensors ని సరైన layers కి పంపించే భాగం",
			explain: "అలాంటి component అసలు లేదు."
		},
		{
			text: "Self-attention mechanism కి మరో పేరు; ఇది sequence లోని ఇతర tokens ఆధారంగా ఒక token representation ని మార్చుతుంది",
			explain: "Self-attention layer లో attention “heads” ఉంటాయి; కానీ ఇవి adaptation heads కావు."
		},
		{
			text: "Transformer predictions ని task-specific output గా మార్చడానికి వేసే అదనపు component; ఇది సాధారణంగా ఒక్కటి లేదా కొన్ని layers తో ఉంటుంది",
			explain: "సరైంది. Adaptation heads (సరళంగా heads అని పిలుస్తారు) వేర్వేరు రూపాల్లో ఉంటాయి: language modeling heads, question answering heads, sequence classification heads... ",
			correct: true
		} 
	]}
/>

### 5. AutoModel అంటే ఏమిటి?

<Question
	choices={[
		{
			text: "మీ data మీద ఆటోమేటిక్‌గా train అయ్యే model",
			explain: "మీరు దీన్ని మా <a href='https://huggingface.co/autotrain'>AutoTrain</a> product తో గలత చేయుతున్నారా?"
		},
		{
			text: "ఇచ్చిన checkpoint ఆధారంగా సరైన architecture ని తిరిగి ఇవ్వగల object",
			explain: "అచ్చం అదే: <code>AutoModel</code> కి ఏ checkpoint నుండి initialize కావాలో తెలిస్తే, అది సరైన architecture ని ఎంచుకుని load చేస్తుంది.",
			correct: true
		},
		{
			text: "Inputs ఏ language లో ఉన్నాయో ఆటోమేటిక్‌గా గుర్తించి, దానికి సరిపోయే weights ని load చేసుకునే model",
			explain: "కొన్ని checkpoints మరియు models ఒకటి కంటే ఎక్కువ languages ను హ్యాండిల్ చేయగలుగుతాయి; కానీ language ఆధారంగా checkpoint ని ఆటోమేటిక్‌గా ఎంచుకునే built-in tools లేవు. మీ task కి సరైన checkpoint కోసం <a href='https://huggingface.co/models'>Model Hub</a> కి వెళ్లాలి!"
		} 
	]}
/>

### 6. వేర్వేరు పొడవుల sequences ని ఒకే batch లో కలిపేటప్పుడు, ఎలాంటి techniques గురించి తెలుసుకుని ఉండాలి?

<Question
	choices={[
		{
			text: "Truncating",
			explain: "అవును, truncation కూడా sequences ని సమాన పరిమాణంలోకి తెచ్చి rectangular shape చేయడానికి ఉపయోగపడే సరైన విధానం. కానీ ఇదే ఒక్కటీ కాదు కదా?",
			correct: true
		},
		{
			text: "Tensors ని తిరిగి ఇవ్వడం",
			explain: "ఇతర techniques వల్ల rectangular tensors వస్తాయి, కానీ “returning tensors” అనే విషయం batching కి ప్రత్యేకంగా సహాయం చేయదు."
		},
		{
			text: "Padding",
			explain: "అవును, padding కూడా sequences ని సమాన పరిమాణంలోకి తెచ్చి rectangular shape చేయడానికి సరైన విధానం. కాని ఇదే ఒక్కటే కాదు.",
			correct: true
		}, 
		{
			text: "Attention masking",
			explain: "ఖచ్చితంగా! వేర్వేరు పొడవుల sequences ను హ్యాండిల్ చేయడంలో attention masks చాలా ముఖ్యం. అయితే ఇదొక్కటి మాత్రమే కాదు.",
			correct: true
		} 
	]}
/>

### 7. Sequence classification model ఇచ్చే logits మీద SoftMax function వాడటంలో అసలు ప్రయోజనం ఏమిటి?

<Question
	choices={[
		{
			text: "Logits ను ‘soft’ చేసి మరింత reliable గా మారుస్తుంది.",
			explain: "కాదు, SoftMax function వల్ల results “reliable” అవ్వవు."
		},
		{
			text: "వాటికి lower మరియు upper bound అప్లై చేసి, values అర్థమయ్యేలా చేస్తుంది.",
			explain: "SoftMax తర్వాత values 0 మరియు 1 మధ్య bounded గా ఉంటాయి. అదే ఒక్క కారణం మాత్రం కాదు, మనం SoftMax వాడటానికి.",
            correct: true
		},
		{
			text: "Output మొత్తం sum 1 అవుతుంది, అందుకే దాన్ని probabilities లాగా interpret చేయవచ్చు.",
			explain: "అవును, ఇది సరైనది! ఇదొక్కటే కారణం కాదు, కానీ SoftMax వాడటానికి ఇది ప్రధాన కారణాల్లో ఒకటి.",
            correct: true
		}
	]}
/>

### 8. Tokenizer API లో ప్రధానంగా ఉపయోగించే method ఏది?

<Question
	choices={[
		{
			text: "<code>encode</code>, ఎందుకంటే అది text ను IDs గా మరియు IDs ను predictions గా మార్చగలదు",
			explain: "తప్పు! <code>encode</code> method tokenizers మీద ఉంది, కానీ models మీద లేదు."
		},
		{
			text: "Tokenizer object ని నేరుగా కాల్ చేయడం.",
			explain: "ఖచ్చితంగా! Tokenizer లోని <code>call</code> method చాల పవర్‌ఫుల్—it చాలా పనులు హ్యాండిల్ చేయగలదు. Model నుండి predictions తీసుకోడానికి కూడా ఇదే method వాడతాం.",
			correct: true
		},
		{
			text: "<code>pad</code>",
			explain: "తప్పు! Padding చాలా ఉపయోగకరమైనది, కానీ tokenizer API లో ఇది ఒక భాగం మాత్రమే."
		},
		{
			text: "<code>tokenize</code>",
			explain: "<code>tokenize</code> method బాగా ఉపయోగపడుతుంది, కానీ tokenizer API లో core method కాదు."
		}
	]}
/>

### 9. ఈ code sample లో result variable లో ఏముంది?

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```

<Question
	choices={[
		{
			text: "Strings లిస్ట్, ప్రతి string ఒక token గా ఉంటుంది",
			explain: "అవును! ఇప్పుడు దీన్ని IDs గా convert చేసి, ఒక model కి పంపవచ్చు!",
            correct: true
		},
		{
			text: "IDs లిస్ట్",
			explain: "కాదు; IDs కోసం <code>call</code> లేదా <code>convert_tokens_to_ids</code> methods వాడాలి!"
		},
		{
			text: "అన్ని tokens ని కలిపిన ఒకే string",
			explain: "అలా చేయడం suboptimal; అసలు లక్ష్యం string ని అనేక tokens గా విడదీయడమే."
		}
	]}
/>

### 10. ఈ code లో ఏదైనా తప్పు ఉందా?

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

<Question
	choices={[
		{
			text: "లేదు, ఇది సరి గానే కనిపిస్తోంది.",
			explain: "దురదృష్టవశాత్తు, వేరే checkpoint తో train చేసిన tokenizer ని వేరే checkpoint లో train అయిన model తో కలపడం చాలా అరుదుగా మంచి ఐడియా అవుతుంది. Model, ఈ tokenizer output ని అర్థం చేసుకునేలా train కాలేదు, కాబట్టి (run అయితే) outputs కి అర్థం ఉండకపోవచ్చు."
		},
		{
			text: "Tokenizer మరియు model ఎప్పుడూ ఒకే checkpoint నుండి ఉండాలి.",
            explain: "సరిగ్గా చెప్పారు!",
            correct: true
		},
		{
			text: "ప్రతి input ఒక batch కాబట్టి, padding మరియు truncation ఎప్పుడూ చేయడం మంచి పద్ధతి.",
            explain: "ప్రతి model input batch అయి ఉండాలి అన్నది నిజమే. కానీ ఇక్కడ ఒక్క sentence మాత్రమే ఉన్నప్పుడు truncate / pad చేయడం తప్పనిసరి కాదు; అవి ఎక్కువగా sentences లిస్ట్ ని batch చేయడానికి ఉపయోగించే techniques."
		}
	]}
/>
