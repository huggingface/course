<FrameworkSwitchCourse {fw} />

# ‡∞¨‡∞π‡±Å‡∞≥ ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞£[[handling-multiple-sequences]]

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section5_pt.ipynb"},
]} />

<Youtube id="M6adb1j2jPI"/>

‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞Ç‡∞≤‡±ã, ‡∞ö‡∞ø‡∞®‡±ç‡∞® ‡∞™‡±ä‡∞°‡∞µ‡±Å ‡∞â‡∞®‡±ç‡∞® ‡∞í‡∞ï‡±á ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞™‡±à inference ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç ‡∞≤‡∞æ‡∞Ç‡∞ü‡∞ø ‡∞Ö‡∞§‡±ç‡∞Ø‡∞Ç‡∞§ ‡∞∏‡∞∞‡∞≥‡∞Æ‡±à‡∞® ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞Ç ‡∞ö‡±Ç‡∞∂‡∞æ‡∞Ç. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞Ö‡∞ï‡±ç‡∞ï‡∞°‡±á ‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡±Å ‡∞µ‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø:

- ‡∞¨‡∞π‡±Å‡∞≥ ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞π‡±ç‡∞Ø‡∞æ‡∞Ç‡∞°‡∞ø‡∞≤‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç?
- *‡∞≠‡∞ø‡∞®‡±ç‡∞®‡∞Æ‡±à‡∞® ‡∞™‡±ä‡∞°‡∞µ‡±Å* ‡∞â‡∞®‡±ç‡∞® ‡∞¨‡∞π‡±Å‡∞≥ ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞π‡±ç‡∞Ø‡∞æ‡∞Ç‡∞°‡∞ø‡∞≤‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç?
- vocabulary indices (input IDs) ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞¨‡∞æ‡∞ó‡∞æ ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞∞‡∞ø‡∞™‡±ã‡∞§‡∞æ‡∞Ø‡∞æ?
- ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ‡∞ó‡∞æ ‡∞â‡∞Ç‡∞°‡∞ü‡∞Ç ‡∞ï‡±Ç‡∞°‡∞æ ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞æ?

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞à ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡±Å ‡∞è‡∞∞‡∞ï‡∞Æ‡±à‡∞® ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞≤‡∞®‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡±ã, ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ü§ó Transformers API ‡∞§‡±ã ‡∞é‡∞≤‡∞æ ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡±ã ‡∞ö‡±Ç‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç.

## ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å batch ‡∞∞‡±Ç‡∞™‡∞Ç‡∞≤‡±ã ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞Ü‡∞∂‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø[[models-expect-a-batch-of-inputs]]

‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø exercise ‡∞≤‡±ã, ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡±Å ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞≤ ‡∞ú‡∞æ‡∞¨‡∞ø‡∞§‡∞æ‡∞ó‡∞æ ‡∞é‡∞≤‡∞æ ‡∞Æ‡∞æ‡∞∞‡∞§‡∞æ‡∞Ø‡±ã ‡∞ö‡±Ç‡∞∂‡∞æ‡∞∞‡±Å.  
‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Ü ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞≤ ‡∞ú‡∞æ‡∞¨‡∞ø‡∞§‡∞æ‡∞®‡±Å ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡±Å‡∞¶‡∞æ‡∞Ç:

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
‡∞Ö‡∞Ø‡±ç‡∞Ø‡±ã! ‡∞á‡∞¶‡∞ø ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å fail ‡∞Ö‡∞Ø‡∞ø‡∞Ç‡∞¶‡∞ø? ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 2 ‡∞≤‡±ã pipeline ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞∏‡±ç‡∞ü‡±Ü‡∞™‡±ç‡∞™‡±Å‡∞≤‡∞®‡±á ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞ï‡±Ç‡∞°‡∞æ ‡∞™‡∞æ‡∞ü‡∞ø‡∞Ç‡∞ö‡∞æ‡∞Ç ‡∞ï‡∞¶‡∞æ.

‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø ‡∞è‡∞Æ‡∞ø‡∞ü‡∞Ç‡∞ü‡±á, ‡∞Æ‡∞®‡∞Ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞í‡∞ï‡±ç‡∞ï ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞™‡∞Ç‡∞™‡∞æ‡∞Ç, ‡∞ï‡∞æ‡∞®‡±Ä ü§ó Transformers ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å default ‡∞ó‡∞æ ‡∞¨‡∞π‡±Å‡∞≥ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å (multiple sentences) ‡∞Ü‡∞∂‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø.  
‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞Æ‡∞®‡∞Ç tokenizer `sequence` ‡∞Æ‡±Ä‡∞¶ ‡∞µ‡±Ü‡∞®‡±Å‡∞ï‡∞™‡∞ü‡±ç‡∞≤‡±ã ‡∞ö‡±á‡∞∏‡±á ‡∞™‡∞®‡∞ø ‡∞Æ‡∞®‡∞Æ‡±á ‡∞ö‡±á‡∞§‡∞ø‡∞§‡±ã ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞æ‡∞Ç. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞¨‡∞æ‡∞ó‡∞æ ‡∞ó‡∞Æ‡∞®‡∞ø‡∞∏‡±ç‡∞§‡±á, tokenizer input IDs ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞°‡∞Æ‡±á ‡∞ï‡∞æ‡∞ï‡±Å‡∞Ç‡∞°‡∞æ, ‡∞¶‡∞æ‡∞®‡∞ø‡∞™‡±à ‡∞Ö‡∞¶‡∞®‡∞Ç‡∞ó‡∞æ ‡∞í‡∞ï dimension ‡∞ï‡±Ç‡∞°‡∞æ ‡∞µ‡±á‡∞∏‡∞ø‡∞Ç‡∞¶‡∞ø:

```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡∞®‡∞Æ‡±á ‡∞Ü ‡∞ï‡±ä‡∞§‡±ç‡∞§ dimension ‡∞®‡±Å ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç:

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å input IDs ‡∞®‡∞ø, ‡∞Ö‡∞≤‡∞æ‡∞ó‡±á ‡∞≤‡∞≠‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® logits ‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞ø‡∞Ç‡∞ü‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç ‚Äî ‡∞Ö‡∞µ‡∞ø ‡∞à ‡∞µ‡∞ø‡∞ß‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø:

```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

*Batching* ‡∞Ö‡∞Ç‡∞ü‡±á, ‡∞í‡∞ï‡±á‡∞∏‡∞æ‡∞∞‡∞ø ‡∞¨‡∞π‡±Å‡∞≥ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å (multiple sentences) ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡∞°‡∞Ç.  
‡∞Æ‡±Ä ‡∞¶‡∞ó‡±ç‡∞ó‡∞∞ ‡∞í‡∞ï‡±ç‡∞ï sentence ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞â‡∞®‡±ç‡∞®‡∞æ, ‡∞Ü ‡∞í‡∞ï‡±ç‡∞ï ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞§‡±ã‡∞®‡±á ‡∞í‡∞ï batch ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```
batched_ids = [ids, ids]
```

‡∞á‡∞¶‡∞ø ‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞í‡∞ï‡±á ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø‡∞® ‡∞í‡∞ï batch!

<Tip>

‚úèÔ∏è **‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø!** ‡∞à `batched_ids` ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞ø ‡∞Æ‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞≤‡±ã‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡∞Ç‡∞°‡∞ø.  
‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® logits ‡∞Æ‡∞æ‡∞¶‡∞ø‡∞∞‡∞ø‡∞ó‡∞æ‡∞®‡±á (‡∞ï‡∞æ‡∞®‡±Ä ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å‡∞∏‡∞æ‡∞∞‡±ç‡∞≤‡±Å) ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡±ã ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø!

</Tip>

Batching ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞¨‡∞π‡±Å‡∞≥ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Ö‡∞¶‡∞ø ‡∞∏‡∞∞‡∞ø‡∞ó‡±ç‡∞ó‡∞æ ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞í‡∞ï ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞§‡±ã batch ‡∞§‡∞Ø‡∞æ‡∞∞‡±Å‡∞ö‡±á‡∞∏‡∞ø‡∞®‡∞ü‡±ç‡∞≤‡±á, ‡∞¨‡∞π‡±Å‡∞≥ ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞§‡±ã ‡∞ï‡±Ç‡∞°‡∞æ batch ‡∞§‡∞Ø‡∞æ‡∞∞‡±Å‡∞ö‡±á‡∞Ø‡∞°‡∞Ç ‡∞Ö‡∞Ç‡∞§‡±á ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç.

‡∞ï‡∞æ‡∞®‡±Ä ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞∞‡±Ü‡∞Ç‡∞°‡±ã ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø: ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å (‡∞≤‡±á‡∞¶‡∞æ ‡∞Ö‡∞Ç‡∞§‡∞ï‡∞Ç‡∞ü‡±á ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ) ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å batch ‡∞≤‡±ã ‡∞ï‡∞≤‡∞™‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±á, ‡∞Ö‡∞µ‡∞ø ‡∞í‡∞ï‡±ç‡∞ï‡±ã‡∞ü‡∞ø ‡∞í‡∞ï‡±ç‡∞ï‡±ã ‡∞™‡±ä‡∞°‡∞µ‡±Å (length) ‡∞≤‡±ã ‡∞â‡∞Ç‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å.  
‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞≤‡∞§‡±ã ‡∞Æ‡±Å‡∞Ç‡∞¶‡±á ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡∞ø ‡∞â‡∞Ç‡∞ü‡±á, ‡∞Ö‡∞µ‡∞ø rectangular shape ‡∞≤‡±ã ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡±ç‡∞∏‡∞ø‡∞Ç‡∞¶‡±á‡∞®‡∞®‡∞ø ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å; ‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø input IDs ‡∞ú‡∞æ‡∞¨‡∞ø‡∞§‡∞æ‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞≤‡±á‡∞Ç.  
‡∞à ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞®‡±Å ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø, ‡∞∏‡∞æ‡∞ß‡∞æ‡∞∞‡∞£‡∞Ç‡∞ó‡∞æ ‡∞Æ‡∞®‡∞Ç ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å *pad* ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç.

## ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å padding ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[padding-the-inputs]]

‡∞ï‡±ç‡∞∞‡∞ø‡∞Ç‡∞¶ ‡∞â‡∞®‡±ç‡∞® list of lists ‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞≤‡±á‡∞Ç:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø, ‡∞Æ‡∞®‡∞Ç *padding* ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ç, ‡∞§‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞≤‡±Å rectangular shape ‡∞≤‡±ã ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø.  
Padding ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ, ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Ç ‡∞í‡∞ï‡±á ‡∞™‡±ä‡∞°‡∞µ‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡±á‡∞≤‡∞æ, ‡∞§‡∞ï‡±ç‡∞ï‡±Å‡∞µ ‡∞™‡∞¶‡∞æ‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï ‡∞™‡∞¶‡∞Æ‡±à‡∞® *padding token* ‡∞®‡±Å ‡∞ö‡∞ø‡∞µ‡∞∞‡∞≤‡±ã ‡∞ú‡±ã‡∞°‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ç.  
‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±Å, 10 ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞í‡∞ï‡±ç‡∞ï‡±ã‡∞ü‡∞ø 10 ‡∞™‡∞¶‡∞æ‡∞≤‡∞§‡±ã ‡∞â‡∞Ç‡∞°‡∞ø, ‡∞Æ‡∞∞‡±ã ‡∞í‡∞ï ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞Ç 20 ‡∞™‡∞¶‡∞æ‡∞≤‡∞§‡±ã ‡∞â‡∞Ç‡∞¶‡∞®‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø; padding ‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Ç 20 ‡∞™‡∞¶‡∞æ‡∞≤‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø.  
‡∞Æ‡∞® ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞≤‡±ã, ‡∞ö‡∞ø‡∞µ‡∞∞‡∞ø‡∞ï‡∞ø ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç ‡∞á‡∞≤‡∞æ ‡∞ï‡∞®‡∞ø‡∞™‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø:

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

Padding token ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ID ‡∞®‡∞ø `tokenizer.pad_token_id` ‡∞≤‡±ã ‡∞ö‡±Ç‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å.  
‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡∞ø‡∞¶‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø, ‡∞Æ‡∞® ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞í‡∞ï‡±ç‡∞ï‡±ã‡∞∏‡∞æ‡∞∞‡∞ø ‡∞µ‡∞ø‡∞°‡∞ø‡∞µ‡∞ø‡∞°‡∞ø‡∞ó‡∞æ, ‡∞Ö‡∞≤‡∞æ‡∞ó‡±á batch ‡∞ó‡∞æ ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞≤‡±ã‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡±Å‡∞¶‡∞æ‡∞Ç:

```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```

‡∞á‡∞ï‡±ç‡∞ï‡∞° batched predictions ‡∞≤‡±ã logits ‡∞≤‡±ã ‡∞è‡∞¶‡±ã ‡∞§‡±á‡∞°‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø: ‡∞∞‡±Ü‡∞Ç‡∞°‡±ã row, ‡∞∞‡±Ü‡∞Ç‡∞°‡±ã ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞µ‡∞ø‡∞°‡∞ø‡∞ó‡∞æ ‡∞™‡∞Ç‡∞™‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® logits ‡∞Æ‡∞æ‡∞¶‡∞ø‡∞∞‡∞ø‡∞ó‡∞æ‡∞®‡±á ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø, ‡∞ï‡∞æ‡∞®‡±Ä ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø‡∞ó‡∞æ ‡∞µ‡±á‡∞∞‡±Å values ‡∞µ‡∞ö‡±ç‡∞ö‡∞æ‡∞Ø‡∞ø!

‡∞¶‡±Ä‡∞®‡∞ø‡∞ï‡∞ø ‡∞ï‡∞æ‡∞∞‡∞£‡∞Ç, Transformer ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞≤‡±ã ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞™‡∞æ‡∞§‡±ç‡∞∞ ‡∞™‡±ã‡∞∑‡∞ø‡∞Ç‡∞ö‡±á attention layers ‚Äî ‡∞á‡∞µ‡∞ø ‡∞™‡±ç‡∞∞‡∞§‡∞ø token ‡∞®‡±Å ‡∞¶‡∞æ‡∞®‡∞ø *context* ‡∞§‡±ã ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø ‡∞ö‡±Ç‡∞°‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø.  
‡∞á‡∞µ‡∞ø padding tokens ‡∞®‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞ó‡∞Æ‡∞®‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø, ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡∞Ç‡∞ü‡±á ‡∞Ö‡∞µ‡∞ø ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞Ö‡∞®‡±ç‡∞®‡∞ø tokens ‡∞™‡±à ‡∞¶‡±É‡∞∑‡±ç‡∞ü‡∞ø ‡∞™‡±Ü‡∞°‡∞§‡∞æ‡∞Ø‡∞ø.  
‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø, ‡∞µ‡±á‡∞∞‡±ç‡∞µ‡±á‡∞∞‡±Å ‡∞™‡±ä‡∞°‡∞µ‡±Å ‡∞â‡∞®‡±ç‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞µ‡∞ø‡∞°‡∞ø‡∞ó‡∞æ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡∞ø‡∞®‡∞æ, ‡∞Ö‡∞¶‡±á ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å padding ‡∞§‡±ã ‡∞ï‡±Ç‡∞°‡∞ø‡∞® batch ‡∞ó‡∞æ ‡∞™‡∞Ç‡∞™‡∞ø‡∞®‡∞æ, ‡∞í‡∞ï‡±á ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡±Å ‡∞∞‡∞æ‡∞µ‡∞æ‡∞≤‡∞Ç‡∞ü‡±á, attention layers‚Äå‡∞ï‡±Å padding tokens ‡∞®‡±Å *‡∞™‡∞ü‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡±Å‡∞ï‡±ã‡∞µ‡∞¶‡±ç‡∞¶‡±Å* ‡∞Ö‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞≤‡∞ø. ‡∞á‡∞¶‡∞ø *attention mask* ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç.

## Attention masks[[attention-masks]]

*Attention mask* ‡∞Ö‡∞®‡±á‡∞¶‡∞ø input IDs ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‚Äå‡∞ï‡∞ø ‡∞Ö‡∞ö‡±ç‡∞ö‡∞Ç ‡∞Ö‡∞¶‡±á ‡∞∞‡±Ç‡∞™‡∞Ç‡∞≤‡±ã ‡∞â‡∞Ç‡∞°‡±á ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç, ‡∞ï‡∞æ‡∞®‡±Ä 0‡∞≤‡±Ç 1‡∞≤‡∞§‡±ã ‡∞®‡∞ø‡∞Ç‡∞°‡∞ø‡∞®‡∞¶‡∞ø:  
1 ‡∞â‡∞®‡±ç‡∞® ‡∞∏‡±ç‡∞•‡∞æ‡∞®‡∞æ‡∞≤‡±ç‡∞≤‡±ã‡∞®‡∞ø tokens ‡∞™‡±à ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞¶‡±É‡∞∑‡±ç‡∞ü‡∞ø ‡∞™‡±Ü‡∞ü‡±ç‡∞ü‡∞æ‡∞≤‡∞ø, 0 ‡∞â‡∞®‡±ç‡∞® ‡∞∏‡±ç‡∞•‡∞æ‡∞®‡∞æ‡∞≤‡±ç‡∞≤‡±ã‡∞®‡∞ø tokens ‡∞®‡±Å ‡∞¶‡±É‡∞∑‡±ç‡∞ü‡∞ø ‡∞™‡±Ü‡∞ü‡±ç‡∞ü‡∞ï‡±Ç‡∞°‡∞¶‡±Å (‡∞Ö‡∞Ç‡∞ü‡±á, ‡∞Ü tokens ‡∞®‡±Å attention layers ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø‡∞ó‡∞æ *ignore* ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø).

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞®‡±ç‡∞® ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±á ‡∞í‡∞ï attention mask ‡∞ú‡±ã‡∞°‡∞ø‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å batch ‡∞≤‡±ã ‡∞∞‡±Ü‡∞Ç‡∞°‡±ã ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® logits, ‡∞Ü ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞µ‡∞ø‡∞°‡∞ø‡∞ó‡∞æ ‡∞™‡∞Ç‡∞™‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® logits‚Äå‡∞≤ ‡∞Æ‡∞æ‡∞¶‡∞ø‡∞∞‡∞ø‡∞ó‡∞æ‡∞®‡±á ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø.

‡∞ó‡∞Æ‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø: ‡∞∞‡±Ü‡∞Ç‡∞°‡±ã ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞≤‡±ã ‡∞ö‡∞ø‡∞µ‡∞∞‡∞ø ‡∞µ‡∞ø‡∞≤‡±Å‡∞µ padding ID ‡∞ï‡∞ø ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®‡∞¶‡∞ø; ‡∞¶‡∞æ‡∞®‡∞ø‡∞ï‡∞ø attention mask ‡∞≤‡±ã 0 ‡∞â‡∞Ç‡∞¶‡∞ø.

<Tip>

‚úèÔ∏è **‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø!** ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 2 ‡∞≤‡±ã ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤ (`"I've been waiting for a HuggingFace course my whole life."` ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å `"I hate this so much!"`) ‡∞Æ‡±Ä‡∞¶ ‡∞ü‡±ã‡∞ï‡∞®‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç‚Äå‡∞®‡±Å ‡∞ö‡±á‡∞§‡∞ø‡∞§‡±ã ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø.  
‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞µ‡∞ø‡∞°‡∞ø‡∞µ‡∞ø‡∞°‡∞ø‡∞ó‡∞æ ‡∞™‡∞Ç‡∞™‡∞ø, ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 2 ‡∞≤‡±ã ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® logits ‡∞Æ‡∞æ‡∞¶‡∞ø‡∞∞‡∞ø‡∞ó‡∞æ‡∞®‡±á ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡±ã ‡∞ö‡±Ç‡∞∏‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø.  
‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø padding token ‡∞§‡±ã ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø ‡∞í‡∞ï batch ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø, ‡∞∏‡∞∞‡±à‡∞® attention mask ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞≤‡±ã‡∞ï‡∞ø ‡∞™‡∞Ç‡∞™‡∞ø ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡±Å ‡∞í‡∞ï‡∞ü‡±á‡∞®‡∞æ ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø!

</Tip>

## ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡±Å[[longer-sequences]]

Transformer ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞§‡±ã ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å, ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ö‡∞Ç‡∞ó‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡±á ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤ ‡∞™‡±ä‡∞°‡∞µ‡±Å‡∞ï‡±Å ‡∞í‡∞ï ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡∞ø ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞ö‡∞æ‡∞≤‡∞æ ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å ‡∞ó‡∞∞‡∞ø‡∞∑‡±ç‡∞ü‡∞Ç‡∞ó‡∞æ 512 ‡∞≤‡±á‡∞¶‡∞æ 1024 tokens ‡∞µ‡∞∞‡∞ï‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞π‡±ç‡∞Ø‡∞æ‡∞Ç‡∞°‡∞ø‡∞≤‡±ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞µ‡±Å; ‡∞Ö‡∞Ç‡∞§‡∞ï‡∞Ç‡∞ü‡±á ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞™‡∞Ç‡∞™‡∞ø‡∞§‡±á, ‡∞Ö‡∞µ‡∞ø ‡∞ï‡±ç‡∞∞‡∞æ‡∞∑‡±ç ‡∞ï‡∞æ‡∞µ‡∞ö‡±ç‡∞ö‡±Å.  
‡∞à ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞ï‡±Å ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞æ‡∞∞‡∞æ‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø:

- ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞∏‡∞™‡±ã‡∞∞‡±ç‡∞ü‡±ç ‡∞ö‡±á‡∞∏‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç.  
- ‡∞Æ‡±Ä ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å truncation ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ö‡∞ø‡∞®‡±ç‡∞®‡∞µ‡∞ø‡∞ó‡∞æ ‡∞ï‡∞§‡±ç‡∞§‡∞ø‡∞∞‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç.

‡∞µ‡∞ø‡∞µ‡∞ø‡∞ß ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞ï‡±Å ‡∞µ‡∞ø‡∞µ‡∞ø‡∞ß ‡∞ó‡∞∞‡∞ø‡∞∑‡±ç‡∞ü ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞™‡±ä‡∞°‡∞µ‡±Å‡∞≤‡±Å ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø; ‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï‡∞Ç‡∞ó‡∞æ ‡∞∞‡±Ç‡∞™‡±ä‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡±ç‡∞°‡∞æ‡∞Ø‡∞ø.  
‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±Å, [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer), [LED](https://huggingface.co/docs/transformers/model_doc/led) ‡∞≤‡∞æ‡∞Ç‡∞ü‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å.  
‡∞Æ‡±Ä ‡∞™‡∞®‡∞ø ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±á, ‡∞á‡∞ü‡±Å‡∞µ‡∞Ç‡∞ü‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞™‡±à ‡∞í‡∞ï‡∞∏‡∞æ‡∞∞‡∞ø ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞Ç‡∞ö‡∞Æ‡∞®‡∞ø ‡∞∏‡±Ç‡∞ö‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡±Å.

‡∞≤‡±á‡∞¶‡∞Ç‡∞ü‡±á, ‡∞Æ‡±Ä ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å truncate ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç ‡∞Æ‡∞Ç‡∞ö‡∞ø‡∞¶‡∞ø; ‡∞¶‡∞æ‡∞®‡∞ø‡∞ï‡∞ø `max_sequence_length` ‡∞™‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç‚Äå‡∞®‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
sequence = sequence[:max_sequence_length]
```
