<FrameworkSwitchCourse {fw} />

# ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å[[the-models]]

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
]} />

<Youtube id="AhChOFRegn4"/>

‡∞à ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞Ç‡∞≤‡±ã, ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§ ‡∞≤‡±ã‡∞§‡±Å‡∞ó‡∞æ ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç. ‡∞Æ‡∞®‡∞Ç `AutoModel` ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç‚Äå‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡±Å, ‡∞á‡∞¶‡∞ø ‡∞è checkpoint ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞Ö‡∞Ø‡∞ø‡∞®‡∞æ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å instantiate ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞®‡±ç‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞™‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

## ‡∞í‡∞ï Transformer ‡∞®‡±Å ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç[[creating-a-transformer]]

`AutoModel` ‡∞®‡±Å instantiate ‡∞ö‡±á‡∞∏‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞è‡∞Æ‡∞ø ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡±ã ‡∞ö‡±Ç‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç:

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")
```

‡∞ü‡±ã‡∞ï‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞Æ‡∞æ‡∞¶‡∞ø‡∞∞‡∞ø‡∞ó‡∞æ‡∞®‡±á, `from_pretrained()` ‡∞™‡∞¶‡±ç‡∞ß‡∞§‡∞ø Hugging Face Hub ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞°‡±á‡∞ü‡∞æ‡∞®‡±Å ‡∞°‡±å‡∞®‡±ç‚Äå‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞∏‡∞ø cache ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞Æ‡±Å‡∞Ç‡∞¶‡±á ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞ø‡∞®‡∞ü‡±ç‡∞≤‡±Å‡∞ó‡∞æ, checkpoint ‡∞™‡±á‡∞∞‡±Å ‡∞í‡∞ï ‡∞®‡∞ø‡∞∞‡±ç‡∞¶‡∞ø‡∞∑‡±ç‡∞ü ‡∞Æ‡±ã‡∞°‡∞≤‡±ç architecture ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¶‡∞æ‡∞®‡∞ø weights ‡∞®‡±Å ‡∞∏‡±Ç‡∞ö‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞à ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞≤‡±ã, ‡∞á‡∞¶‡∞ø ‡∞í‡∞ï BERT ‡∞Ü‡∞ß‡∞æ‡∞∞‡∞ø‡∞§ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‚Äî 12 layers, 768 hidden size, 12 attention heads ‚Äî ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å *cased* ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞§‡±ã (‡∞Ö‡∞Ç‡∞ü‡±á uppercase/lowercase ‡∞§‡±á‡∞°‡∞æ ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞Ç).  
Hub ‡∞≤‡±ã ‡∞Ö‡∞®‡±á‡∞ï checkpoints ‡∞Ö‡∞Ç‡∞¶‡±Å‡∞¨‡∞æ‡∞ü‡±Å‡∞≤‡±ã ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø ‚Äî ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø [‡∞á‡∞ï‡±ç‡∞ï‡∞°](https://huggingface.co/models) ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å.

`AutoModel` ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¶‡∞æ‡∞®‡∞ø ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ "Auto" ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç‚Äå‡∞≤‡±Å, ‡∞á‡∞µ‡±ç‡∞µ‡∞¨‡∞°‡∞ø‡∞® checkpoint ‡∞ï‡∞ø ‡∞∏‡∞∞‡∞ø‡∞™‡±ã‡∞Ø‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç architecture ‡∞®‡±Å ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ ‡∞é‡∞Ç‡∞ö‡±Å‡∞ï‡±Å‡∞®‡∞ø ‡∞∏‡∞∞‡±à‡∞® ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç‚Äå‡∞®‡±Å instantiate ‡∞ö‡±á‡∞∏‡±á ‡∞∏‡∞∞‡∞≥‡∞Æ‡±à‡∞® wrappers ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á.  
‡∞Æ‡±Ä‡∞∞‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞®‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞∞‡∞ï‡∞Ç ‡∞Æ‡±Å‡∞Ç‡∞¶‡±á ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±á, architecture ‡∞®‡∞ø ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞ö‡∞ø‡∞Ç‡∞ö‡±á ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç‚Äå‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

## ‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[loading-and-saving]]

‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç, tokenizer ‡∞®‡±Å ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞∏‡∞ø‡∞®‡∞Ç‡∞§ ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç. ‡∞®‡∞ø‡∞ú‡∞æ‡∞®‡∞ø‡∞ï‡∞ø, ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞≤‡±ã ‡∞ï‡±Ç‡∞°‡∞æ ‡∞Ö‡∞¶‡±á `save_pretrained()` ‡∞™‡∞¶‡±ç‡∞ß‡∞§‡∞ø ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø, ‡∞á‡∞¶‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï weights ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å architecture configuration ‡∞®‡±Å ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø:

```py
model.save_pretrained("directory_on_my_computer")
```

‡∞¶‡±Ä‡∞Ç‡∞§‡±ã ‡∞Æ‡±Ä ‡∞ï‡∞Ç‡∞™‡±ç‡∞Ø‡±Ç‡∞ü‡∞∞‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞´‡±ã‡∞≤‡±ç‡∞°‡∞∞‡±ç‚Äå‡∞≤‡±ã ‡∞à ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞´‡±à‡∞≥‡±ç‡∞≤‡±Å ‡∞∏‡±á‡∞µ‡±ç ‡∞Ö‡∞µ‡±Å‡∞§‡∞æ‡∞Ø‡∞ø:

```
ls directory_on_my_computer

config.json model.safetensors
```

config.json ‡∞´‡±à‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞ö‡±á‡∞∏‡∞ø ‡∞ö‡±Ç‡∞∏‡±ç‡∞§‡±á, ‡∞Æ‡±ã‡∞°‡∞≤‡±ç architecture ‡∞®‡±Å ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Æ‡±à‡∞® attributes ‡∞Ö‡∞®‡±ç‡∞®‡±Ä ‡∞ï‡∞®‡∞ø‡∞™‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø. ‡∞á‡∞Ç‡∞¶‡±Å‡∞≤‡±ã checkpoint ‡∞é‡∞ï‡±ç‡∞ï‡∞°‡∞ø ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø, ‡∞Ö‡∞≤‡∞æ‡∞ó‡±á ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ö‡∞ø‡∞µ‡∞∞‡∞ø‡∞ó‡∞æ ‡∞à checkpoint ‡∞®‡±Å ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞∏‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ü§ó Transformers ‡∞µ‡±Ü‡∞∞‡±ç‡∞∑‡∞®‡±ç ‡∞µ‡∞Ç‡∞ü‡∞ø metadata ‡∞ï‡±Ç‡∞°‡∞æ ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø.

pytorch_model.safetensors ‡∞´‡±à‡∞≤‡±ç‚Äå‡∞®‡±Å state dictionary ‡∞Ö‡∞Ç‡∞ü‡∞æ‡∞∞‡±Å; ‡∞á‡∞Ç‡∞¶‡±Å‡∞≤‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞®‡±ç‡∞®‡∞ø weights ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø.
‡∞à ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞´‡±à‡∞≥‡±ç‡∞≤‡±Å ‡∞ï‡∞≤‡∞ø‡∞∏‡±á ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø: configuration ‡∞´‡±à‡∞≤‡±ç architecture ‡∞µ‡∞ø‡∞µ‡∞∞‡∞æ‡∞≤‡∞®‡±Å ‡∞á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø; weights ‡∞´‡±à‡∞≤‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï parameters ‡∞®‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø.

‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞Ç‡∞ü‡±á, `from_pretrained()` ‡∞™‡∞¶‡±ç‡∞ß‡∞§‡∞ø‡∞®‡∞ø ‡∞Æ‡∞∞‡±ã‡∞∏‡∞æ‡∞∞‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡±Å:

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("directory_on_my_computer")
```

ü§ó Transformers ‡∞≤‡±à‡∞¨‡±ç‡∞∞‡∞∞‡±Ä‡∞≤‡±ã‡∞®‡∞ø ‡∞Ö‡∞¶‡±ç‡∞≠‡±Å‡∞§‡∞Æ‡±à‡∞® ‡∞≤‡∞ï‡±ç‡∞∑‡∞£‡∞æ‡∞≤‡±ç‡∞≤‡±ã ‡∞í‡∞ï‡∞ü‡∞ø ‚Äî ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞®‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞ü‡±ã‡∞ï‡∞®‡±à‡∞ú‡∞∞‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ú‡∞Ç‡∞§‡±ã ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞™‡∞Ç‡∞ö‡±Å‡∞ï‡±Å‡∞®‡±á ‡∞∏‡∞æ‡∞Æ‡∞∞‡±ç‡∞•‡±ç‡∞Ø‡∞Ç. ‡∞¶‡±Ä‡∞®‡∞ø ‡∞ï‡±ã‡∞∏‡∞Ç, ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞ó‡∞æ ‡∞Æ‡±Ä‡∞∞‡±Å [Hugging Face](https://huggingface.co) ‡∞≤‡±ã ‡∞í‡∞ï ‡∞ñ‡∞æ‡∞§‡∞æ ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø.  
‡∞Æ‡±Ä‡∞∞‡±Å ‡∞í‡∞ï notebook ‡∞µ‡∞æ‡∞°‡±Å‡∞§‡±Å‡∞®‡±ç‡∞®‡∞ü‡±ç‡∞≤‡∞Ø‡∞ø‡∞§‡±á, ‡∞à ‡∞µ‡∞ø‡∞ß‡∞Ç‡∞ó‡∞æ ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞≤‡∞æ‡∞ó‡∞ø‡∞®‡±ç ‡∞ï‡∞æ‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```python
from huggingface_hub import notebook_login

notebook_login()
```

‡∞≤‡±á‡∞¶‡∞æ, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ü‡±Ü‡∞∞‡±ç‡∞Æ‡∞ø‡∞®‡∞≤‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞ü‡±á, ‡∞á‡∞≤‡∞æ ‡∞®‡∞°‡∞™‡∞Ç‡∞°‡∞ø:

```bash
huggingface-cli login
```

‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, `push_to_hub()` ‡∞™‡∞¶‡±ç‡∞ß‡∞§‡∞ø‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å Hub ‡∞ï‡±Å ‡∞™‡∞Ç‡∞™‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
model.push_to_hub("my-awesome-model")
```

‡∞á‡∞¶‡∞ø ‡∞Æ‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞´‡±à‡∞≥‡±ç‡∞≤‡∞®‡±Å Hub ‡∞≤‡±ã‡∞ï‡∞ø, ‡∞Æ‡±Ä namespace ‡∞ï‡∞ø‡∞Ç‡∞¶ ‡∞â‡∞®‡±ç‡∞® *my-awesome-model* ‡∞Ö‡∞®‡±á repository ‡∞≤‡±ã‡∞ï‡∞ø ‡∞Ö‡∞™‡±ç‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞Ö‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞é‡∞µ‡∞∞‡±à‡∞®‡∞æ ‡∞Æ‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å `from_pretrained()` ‡∞§‡±ã ‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞ö‡±ç‡∞ö‡±Å!

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("your-username/my-awesome-model")
```

Hub API ‡∞§‡±ã ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞á‡∞Ç‡∞ï‡∞æ ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞™‡∞®‡±Å‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å:
- ‡∞∏‡±ç‡∞•‡∞æ‡∞®‡∞ø‡∞ï repository ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å push ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç
- ‡∞Æ‡±ä‡∞§‡±ç‡∞§‡∞Ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞§‡∞ø‡∞∞‡∞ø‡∞ó‡∞ø ‡∞Ö‡∞™‡±ç‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ, ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï‡∞Æ‡±à‡∞® ‡∞´‡±à‡∞≥‡±ç‡∞≤‡∞®‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞Ö‡∞™‡±ç‡∞°‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç
- ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡±Å ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞∏‡∞æ‡∞Æ‡∞∞‡±ç‡∞•‡±ç‡∞Ø‡∞æ‡∞≤‡±Å, ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡±Å‡∞≤‡±Å, bias ‡∞≤‡±Å ‡∞Æ‡±ä‡∞¶‡∞≤‡±à‡∞® ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ‡∞µ‡∞ø‡∞µ‡∞∞‡∞ø‡∞Ç‡∞ö‡±á model cards ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç  

‡∞á‡∞µ‡∞®‡±ç‡∞®‡±Ä ‡∞é‡∞≤‡∞æ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡±ã ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø [‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±á‡∞∑‡∞®‡±ç](https://huggingface.co/docs/huggingface_hub/how-to-upstream) ‡∞®‡±Å ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø, ‡∞≤‡±á‡∞¶‡∞æ ‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§ ‡∞≤‡±ã‡∞§‡±à‡∞® ‡∞µ‡∞ø‡∞µ‡∞∞‡∞£ ‡∞ï‡±ã‡∞∏‡∞Ç advanced [Chapter 4](/course/chapter4) ‡∞®‡±Å ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.

## ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞é‡∞®‡±ç‡∞ï‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[encoding-text]]

Transformer ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞Ø‡±á‡∞ü‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å, ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞≤‡±Å‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞ø ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø. ‡∞à ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞Ç‡∞≤‡±ã, tokenizer ‡∞Æ‡±Ä ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞∏‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞®‡∞ø‡∞ú‡∞Ç‡∞ó‡∞æ ‡∞è‡∞Æ‡∞ø ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡±ã ‡∞ö‡±Ç‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç. [‡∞Ö‡∞ß‡±ç‡∞Ø‡∞æ‡∞Ø‡∞Ç 1](/course/chapter1)‡∞≤‡±ã ‡∞ö‡±Ç‡∞∏‡∞ø‡∞®‡∞ü‡±ç‡∞≤‡±Å‡∞ó‡∞æ, tokenizers ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å tokens ‡∞ó‡∞æ ‡∞µ‡∞ø‡∞°‡∞¶‡±Ä‡∞∏‡∞ø, ‡∞Ü tokens ‡∞®‡±Å ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞≤‡±Å‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡±Å‡∞§‡∞æ‡∞Ø‡∞ø. ‡∞à ‡∞Æ‡∞æ‡∞∞‡±ç‡∞™‡±Å‡∞®‡±Å ‡∞í‡∞ï ‡∞∏‡∞∞‡∞≥‡∞Æ‡±à‡∞® tokenizer ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

encoded_input = tokenizer("Hello, I'm a single sentence!")
print(encoded_input)
```

```python out
{'input_ids': [101, 8667, 117, 1000, 1045, 1005, 1049, 2235, 17662, 12172, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

‡∞Æ‡∞®‡∞ï‡±Å ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞® dictionary ‡∞≤‡±ã ‡∞à ‡∞ï‡±ç‡∞∞‡∞ø‡∞Ç‡∞¶‡∞ø fields ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø:
- `input_ids`: ‡∞Æ‡±Ä tokens ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞æ‡∞§‡±ç‡∞Æ‡∞ï ‡∞™‡±ç‡∞∞‡∞§‡∞ø‡∞®‡∞ø‡∞ß‡±Å‡∞≤‡±Å
- `token_type_ids`: ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡±ã sentence A ‡∞è‡∞¶‡∞ø, sentence B ‡∞è‡∞¶‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞Ø‡∞ú‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø (‡∞¶‡±Ä‡∞®‡±ç‡∞®‡∞ø ‡∞§‡∞∞‡±Å‡∞µ‡∞æ‡∞§‡∞ø ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞Ç‡∞≤‡±ã ‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§‡∞ó‡∞æ ‡∞ö‡±Ç‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å)
- `attention_mask`: ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞è tokens ‡∞™‡±à ‡∞¶‡±É‡∞∑‡±ç‡∞ü‡∞ø ‡∞™‡±Ü‡∞ü‡±ç‡∞ü‡∞æ‡∞≤‡∞ø, ‡∞è‡∞µ‡∞ø‡∞™‡±à ‡∞™‡±Ü‡∞ü‡±ç‡∞ü‡∞ï‡±Ç‡∞°‡∞¶‡±ã ‡∞∏‡±Ç‡∞ö‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø (‡∞ï‡∞æ‡∞∏‡±á‡∞™‡∞ü‡∞ø ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ ‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø ‡∞µ‡∞ø‡∞µ‡∞∞‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø)

‡∞à input IDs ‡∞®‡±Å ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä decode ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±á ‡∞Ö‡∞∏‡∞≤‡±Å ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞§‡∞ø‡∞∞‡∞ø‡∞ó‡∞ø ‡∞™‡±ä‡∞Ç‡∞¶‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
tokenizer.decode(encoded_input["input_ids"])
```

```python out
"[CLS] Hello, I'm a single sentence! [SEP]"
```

‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ó‡∞Æ‡∞®‡∞ø‡∞Ç‡∞ö‡∞ó‡∞≤‡∞∞‡±Å: tokenizer `[CLS]`, `[SEP]` ‡∞µ‡∞Ç‡∞ü‡∞ø ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï tokens ‡∞®‡±Å ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø ‚Äî ‡∞á‡∞µ‡∞ø ‡∞Ü ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡±Å ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Æ‡±à‡∞®‡∞µ‡∞ø. ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞ï‡±Å special tokens ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç ‡∞â‡∞Ç‡∞°‡∞µ‡±Å; ‡∞í‡∞ï ‡∞Æ‡±ã‡∞°‡∞≤‡±ç training ‡∞¶‡∞∂‡∞≤‡±ã ‡∞µ‡±Ä‡∞ü‡∞ø‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞â‡∞Ç‡∞ü‡±á, tokenizer ‡∞ï‡±Ç‡∞°‡∞æ ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡±ç‡∞∏‡∞ø ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø.

‡∞í‡∞ï‡±á‡∞∏‡∞æ‡∞∞‡∞ø ‡∞Ö‡∞®‡±á‡∞ï ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å encode ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å ‚Äî batching ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ (‡∞¶‡∞æ‡∞®‡∞ø ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞§‡±ç‡∞µ‡∞∞‡∞≤‡±ã ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡∞æ‡∞Æ‡±Å), ‡∞≤‡±á‡∞¶‡∞æ ‡∞∏‡∞æ‡∞¶‡∞æ‡∞∏‡±Ä‡∞¶‡∞æ‡∞ó‡∞æ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤ ‡∞ú‡∞æ‡∞¨‡∞ø‡∞§‡∞æ‡∞®‡±Å ‡∞™‡∞Ç‡∞™‡∞°‡∞Ç ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ:

```py
encoded_input = tokenizer("How are you?", "I'm fine, thank you!")
print(encoded_input)
```

```python out
{'input_ids': [[101, 1731, 1132, 1128, 136, 102], [101, 1045, 1005, 1049, 2503, 117, 5763, 1128, 136, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```

‡∞ó‡∞Æ‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø: ‡∞Ö‡∞®‡±á‡∞ï ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞™‡∞Ç‡∞™‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å, tokenizer ‡∞™‡±ç‡∞∞‡∞§‡∞ø dictionary key ‡∞ï‡∞ø, ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞µ‡±á‡∞∞‡±ç‡∞µ‡±á‡∞∞‡±Å ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞Æ‡∞®‡∞Ç tokenizer ‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ PyTorch tensors ‡∞®‡±Å ‡∞§‡∞ø‡∞∞‡∞ø‡∞ó‡∞ø ‡∞á‡∞µ‡±ç‡∞µ‡∞Æ‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞Ö‡∞°‡∞ó‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
encoded_input = tokenizer("How are you?", "I'm fine, thank you!", return_tensors="pt")
print(encoded_input)
```

```python out
{'input_ids': tensor([[  101,  1731,  1132,  1128,   136,   102],
         [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

‡∞ï‡∞æ‡∞®‡±Ä ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞í‡∞ï ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø ‡∞â‡∞Ç‡∞¶‡∞ø: ‡∞à ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞≤‡±Å ‡∞í‡∞ï‡±á ‡∞™‡±ä‡∞°‡∞µ‡±Å (length) ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞≤‡±á‡∞µ‡±Å! Arrays ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å tensors rectangular (‡∞∏‡∞Æ‡∞ö‡∞§‡±Å‡∞∞‡∞∏‡±ç‡∞∞‡∞æ‡∞ï‡∞æ‡∞∞) ‡∞ó‡∞æ ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø, ‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø ‡∞à lists ‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ PyTorch tensor (‡∞≤‡±á‡∞¶‡∞æ NumPy array) ‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞≤‡±á‡∞Ç.  
‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø tokenizer ‡∞≤‡±ã *padding* ‡∞Ö‡∞®‡±á ‡∞é‡∞Ç‡∞™‡∞ø‡∞ï ‡∞â‡∞Ç‡∞¶‡∞ø.

### ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[padding-inputs]]

‡∞Æ‡∞®‡∞Ç tokenizer ‡∞ï‡±Å padding ‡∞ö‡±á‡∞Ø‡∞Æ‡∞®‡∞ø ‡∞ö‡±Ü‡∞¨‡∞ø‡∞§‡±á, ‡∞Ö‡∞¶‡∞ø ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞í‡∞ï‡±á ‡∞™‡±ä‡∞°‡∞µ‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡±á‡∞≤‡∞æ ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‚Äî ‡∞Ö‡∞Ç‡∞ü‡±á, ‡∞Ö‡∞§‡∞ø ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞∞‡∞ø‡∞™‡∞°‡±á‡∞≤‡∞æ ‡∞ö‡∞ø‡∞®‡±ç‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤ ‡∞ö‡∞ø‡∞µ‡∞∞‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï‡∞Æ‡±à‡∞® padding token ‡∞®‡±Å ‡∞ú‡±ã‡∞°‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø:

```py
encoded_input = tokenizer(
    ["How are you?", "I'm fine, thank you!"], padding=True, return_tensors="pt"
)
print(encoded_input)
```

```python out
{'input_ids': tensor([[  101,  1731,  1132,  1128,   136,   102,     0,     0,     0,     0],
         [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡∞®‡∞ï‡±Å rectangular tensors ‡∞µ‡∞ö‡±ç‡∞ö‡∞æ‡∞Ø‡∞ø!  
‡∞ó‡∞Æ‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø: padding tokens ‡∞ï‡±Å input IDs ‡∞≤‡±ã 0 ‡∞Ö‡∞®‡±á ID ‡∞µ‡∞ö‡±ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø, ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞µ‡∞æ‡∞ü‡∞ø attention mask ‡∞µ‡∞ø‡∞≤‡±Å‡∞µ ‡∞ï‡±Ç‡∞°‡∞æ 0.  
‡∞¶‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞ï‡∞æ‡∞∞‡∞£‡∞Ç ‚Äî ‡∞à padding tokens ‡∞®‡±Å ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞µ‡∞ø‡∞∂‡±ç‡∞≤‡±á‡∞∑‡∞ø‡∞Ç‡∞ö‡∞ï‡±Ç‡∞°‡∞¶‡±Å; ‡∞á‡∞µ‡∞ø ‡∞®‡∞ø‡∞ú‡∞Æ‡±à‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞≠‡∞æ‡∞ó‡∞Ç ‡∞ï‡∞æ‡∞µ‡±Å.


### ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞ü‡±ç‡∞∞‡∞Ç‡∞ï‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[truncating-inputs]]

‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞≤‡±ç‡∞≤‡±ã tensors ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞™‡±Ü‡∞¶‡±ç‡∞¶‡∞µ‡±Å‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡∞ø, ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞Ø‡∞≤‡±á‡∞ï‡∞™‡±ã‡∞µ‡∞ö‡±ç‡∞ö‡±Å. ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±Å, BERT ‡∞™‡±ç‡∞∞‡±Ä‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø ‡∞ó‡∞∞‡∞ø‡∞∑‡±ç‡∞ü‡∞Ç‡∞ó‡∞æ **512 tokens** ‡∞µ‡∞∞‡∞ï‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á.  
‡∞Ö‡∞Ç‡∞ü‡±á, ‡∞Ü‡∞ï‡±Å ‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞™‡±ä‡∞°‡∞µ‡±Å ‡∞â‡∞®‡±ç‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞Ø‡∞≤‡±á‡∞°‡∞®‡∞ø ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç.

‡∞Ö‡∞≤‡∞æ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞π‡±ç‡∞Ø‡∞æ‡∞Ç‡∞°‡∞ø‡∞≤‡±ç ‡∞ö‡±á‡∞Ø‡∞≤‡±á‡∞®‡∞ø ‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞â‡∞Ç‡∞ü‡±á, ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø `truncation` ‡∞™‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç‚Äå ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ö‡∞ø‡∞®‡±ç‡∞®‡∞¶‡∞ø‡∞ó‡∞æ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø:


```py
encoded_input = tokenizer(
    "This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.",
    truncation=True,
)
print(encoded_input["input_ids"])
```

```python out
[101, 1188, 1110, 170, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1179, 5650, 119, 102]
```

Padding ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å truncation ‡∞®‡±Å ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡±á, ‡∞Æ‡±Ä tensors ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡±ç‡∞∏‡∞ø‡∞® ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞Æ‡±à‡∞® ‡∞™‡∞∞‡∞ø‡∞Æ‡∞æ‡∞£‡∞Ç‡∞≤‡±ã ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø:

```py
encoded_input = tokenizer(
    ["How are you?", "I'm fine, thank you!"],
    padding=True,
    truncation=True,
    max_length=5,
    return_tensors="pt",
)
print(encoded_input)
```

```python out
{'input_ids': tensor([[  101,  1731,  1132,  1128,   102],
         [  101,  1045,  1005,  1049,   102]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1],
         [1, 1, 1, 1, 1]])}
```

### ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï tokens ‡∞®‡∞ø ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç

Special tokens (‡∞≤‡±á‡∞¶‡∞æ ‡∞µ‡∞æ‡∞ü‡∞ø ‡∞≠‡∞æ‡∞µ‡∞Ç) BERT ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¶‡∞æ‡∞®‡∞ø ‡∞Ü‡∞ß‡∞æ‡∞∞‡∞ø‡∞§ ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞ï‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞Æ‡±à‡∞®‡∞µ‡∞ø.  
‡∞à tokens ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤ ‡∞™‡∞∞‡∞ø‡∞Æ‡∞ø‡∞§‡±Å‡∞≤‡∞®‡±Å ‡∞∏‡±Ç‡∞ö‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞™‡∞°‡∞§‡∞æ‡∞Ø‡∞ø ‚Äî ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±Å:

- ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞Ç: `[CLS]`
- ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤ ‡∞Æ‡∞ß‡±ç‡∞Ø ‡∞µ‡∞ø‡∞≠‡∞ú‡∞®: `[SEP]`

‡∞á‡∞¶‡∞ø‡∞ó‡±ã ‡∞í‡∞ï ‡∞∏‡∞æ‡∞ß‡∞æ‡∞∞‡∞£ ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£:

```py
encoded_input = tokenizer("How are you?")
print(encoded_input["input_ids"])
tokenizer.decode(encoded_input["input_ids"])
```

```python out
[101, 1731, 1132, 1128, 136, 102]
'[CLS] How are you? [SEP]'
```

‡∞à special tokens ‡∞®‡±Å tokenizer ‡∞∏‡±ç‡∞µ‡∞Ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞ï‡∞Ç‡∞ó‡∞æ ‡∞ú‡±ã‡∞°‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞ï‡±Å ‡∞á‡∞µ‡∞ø ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç ‡∞ï‡∞æ‡∞¶‡±Å;  
‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞™‡±ç‡∞∞‡±Ä‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞¶‡∞∂‡∞≤‡±ã special tokens ‡∞µ‡∞æ‡∞°‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á tokenizer ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ‡∞ú‡±ã‡∞°‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‚Äî ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡∞Ç‡∞ü‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø ‡∞Ü‡∞∂‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

### ‡∞á‡∞¶‡∞ø ‡∞Ö‡∞Ç‡∞§‡∞æ ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç?

‡∞á‡∞¶‡∞ø‡∞ó‡±ã ‡∞í‡∞ï ‡∞∏‡±ç‡∞™‡∞∑‡±ç‡∞ü‡∞Æ‡±à‡∞® ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£. ‡∞à encoded sequences ‡∞®‡±Å ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø:

```py
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
```

Tokenize ‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ ‡∞Æ‡∞®‡∞ï‡±Å ‡∞á‡∞¶‡∞ø ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø:

```python
encoded_sequences = [
    [
        101,
        1045,
        1005,
        2310,
        2042,
        3403,
        2005,
        1037,
        17662,
        12172,
        2607,
        2026,
        2878,
        2166,
        1012,
        102,
    ],
    [101, 1045, 5223, 2023, 2061, 2172, 999, 102],
]
```

‡∞á‡∞¶‡∞ø ‡∞í‡∞ï ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç ‡∞Ü‡∞´‡±ç ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞≤‡±Å ‚Äî ‡∞Ö‡∞Ç‡∞ü‡±á, ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞≤‡∞§‡±ã ‡∞ï‡±Ç‡∞°‡∞ø‡∞® ‡∞Ö‡∞®‡±á‡∞ï ‡∞≤‡∞ø‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞≤‡±Å.  
‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞≤‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Ç **‡∞∏‡∞Æ‡∞ö‡∞§‡±Å‡∞∞‡∞∏‡±ç‡∞∞‡∞æ‡∞ï‡∞æ‡∞∞ (rectangular)** ‡∞Ü‡∞ï‡∞æ‡∞∞‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞Ö‡∞Ç‡∞ó‡±Ä‡∞ï‡∞∞‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø ‚Äî ‡∞Æ‡±ç‡∞Ø‡∞æ‡∞ü‡±ç‡∞∞‡∞ø‡∞∏‡±Å‡∞≤‡∞ï‡±Å ‡∞Ü‡∞≤‡±ã‡∞ö‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.

‡∞à ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞≤‡±ã "array" ‡∞á‡∞™‡±ç‡∞™‡∞ü‡∞ø‡∞ï‡±á rectangular‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø, ‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø ‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø tensor ‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞°‡∞Ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç:

```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```

### ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞≤‡∞®‡±Å ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡±Å ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡±Å‡∞ó‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç[[using-the-tensors-as-inputs-to-the-model]]

‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞≤‡∞®‡±Å ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞§‡±ã ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç ‚Äî ‡∞Æ‡∞®‡∞Ç inputs ‡∞§‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞™‡∞ø‡∞≤‡±Å‡∞∏‡±ç‡∞§‡∞æ‡∞Ç:

```py
output = model(model_inputs)
```

‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ö‡∞®‡±á‡∞ï ‡∞∞‡∞ï‡∞æ‡∞≤ arguments ‡∞Ö‡∞Ç‡∞ó‡±Ä‡∞ï‡∞∞‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø, ‡∞ï‡∞æ‡∞®‡±Ä **input IDs ‡∞§‡∞™‡±ç‡∞™‡∞®‡∞ø‡∞∏‡∞∞‡∞ø**.  
‡∞á‡∞§‡∞∞ arguments ‡∞è‡∞Æ‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø, ‡∞é‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç ‡∞™‡∞°‡∞§‡∞æ‡∞Ø‡±ã ‡∞§‡∞∞‡±Å‡∞µ‡∞æ‡∞§‡∞ø ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞æ‡∞≤‡±ç‡∞≤‡±ã ‡∞µ‡∞ø‡∞µ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞ö‡±Ç‡∞∏‡±ç‡∞§‡∞æ‡∞Ç.  

‡∞ï‡∞æ‡∞®‡±Ä ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞ó‡∞æ, ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç‡∞≤‡±Å Transformer ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞ï‡±Å ‡∞Ö‡∞∞‡±ç‡∞•‡∞Æ‡∞Ø‡±ç‡∞Ø‡±á ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞§‡∞Ø‡∞æ‡∞∞‡±Å ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡±ã ‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§ ‡∞≤‡±ã‡∞§‡±Å‡∞ó‡∞æ ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞æ‡∞≤‡∞ø.
