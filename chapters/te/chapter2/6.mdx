<FrameworkSwitchCourse {fw} />

# ‡∞Ö‡∞®‡±ç‡∞®‡∞ø‡∞Ç‡∞ü‡∞ø‡∞®‡±Ä ‡∞ï‡∞≤‡∞ø‡∞™‡∞ø ‡∞ö‡±Ç‡∞°‡∞°‡∞Ç[[putting-it-all-together]]

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

‡∞ó‡∞§ ‡∞ï‡±ä‡∞¶‡±ç‡∞¶‡∞ø ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞æ‡∞≤‡±ç‡∞≤‡±ã, ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ ‡∞≠‡∞æ‡∞ó‡∞Ç ‡∞™‡∞®‡∞ø‡∞®‡∞ø ‡∞Æ‡∞® ‡∞ö‡±á‡∞§‡±Å‡∞≤‡∞§‡±ã ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞æ‡∞Ç.  
‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç‡∞≤‡±Å ‡∞é‡∞≤‡∞æ ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡±ã ‡∞ö‡±Ç‡∞∂‡∞æ‡∞Ç, tokenization, input IDs ‡∞Æ‡∞æ‡∞∞‡±ç‡∞™‡±Å, padding, truncation, attention masks ‡∞µ‡∞Ç‡∞ü‡∞ø ‡∞¶‡∞∂‡∞≤‡∞®‡±Å ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞Ç‡∞ö‡∞æ‡∞Æ‡±Å.

‡∞Ö‡∞Ø‡∞ø‡∞§‡±á, ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 2 ‡∞≤‡±ã ‡∞ö‡±Ç‡∞∏‡∞ø‡∞®‡∞ü‡±ç‡∞≤‡±Å‡∞ó‡∞æ, ü§ó Transformers API‡∞≤‡±ã‡∞®‡∞ø ‡∞í‡∞ï high-level ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞à ‡∞Ö‡∞®‡±ç‡∞®‡∞ø‡∞Ç‡∞ü‡∞ø‡∞®‡±Ä ‡∞Æ‡∞® ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å.  
‡∞Æ‡±Ä‡∞∞‡±Å `tokenizer` ‡∞®‡±Å ‡∞®‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞Ç ‡∞Æ‡±Ä‡∞¶ ‡∞ï‡∞æ‡∞≤‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±á, ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡±Å ‡∞™‡∞Ç‡∞™‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Æ‡±à‡∞® ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞≤‡∞≠‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

‡∞á‡∞ï‡±ç‡∞ï‡∞° `model_inputs` ‡∞µ‡±á‡∞∞‡∞ø‡∞Ø‡∞¨‡±Å‡∞≤‡±ç‚Äå‡∞≤‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞∏‡∞∞‡∞ø‡∞ó‡±ç‡∞ó‡∞æ ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Æ‡±à‡∞® ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø. DistilBERT ‡∞ï‡±ã‡∞∏‡∞Ç, ‡∞á‡∞Ç‡∞¶‡±Å‡∞≤‡±ã input IDs ‡∞Ö‡∞≤‡∞æ‡∞ó‡±á attention mask ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø.  
‡∞á‡∞§‡∞∞ ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡∞ï‡±Å ‡∞Ö‡∞¶‡∞®‡∞™‡±Å ‡∞á‡∞®‡±ç‚Äå‡∞™‡±Å‡∞ü‡±ç‚Äå‡∞≤‡±Å ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Æ‡±à‡∞§‡±á, tokenizer ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡±Ä ‡∞ï‡±Ç‡∞°‡∞æ ‡∞§‡∞Ø‡∞æ‡∞∞‡±Å ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

‡∞ï‡±ç‡∞∞‡∞ø‡∞Ç‡∞¶‡∞ø ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞≤‡±ç‡∞≤‡±ã ‡∞ö‡±Ç‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞ü‡±ç‡∞≤‡±Å‡∞ó‡∞æ, ‡∞à ‡∞µ‡∞ø‡∞ß‡∞æ‡∞®‡∞Ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞∂‡∞ï‡±ç‡∞§‡∞ø‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞®‡∞¶‡∞ø. ‡∞Æ‡±ä‡∞¶‡∞ü‡∞ó‡∞æ, ‡∞á‡∞¶‡∞ø ‡∞í‡∞ï‡±á ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞®‡±Å tokenize ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

‡∞Ö‡∞¶‡±á ‡∞µ‡∞ø‡∞ß‡∞Ç‡∞ó‡∞æ, API‡∞≤‡±ã ‡∞é‡∞≤‡∞æ‡∞Ç‡∞ü‡∞ø ‡∞Æ‡∞æ‡∞∞‡±ç‡∞™‡±Å ‡∞≤‡±á‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞¨‡∞π‡±Å‡∞≥ ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å ‡∞ï‡±Ç‡∞°‡∞æ tokenize ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

‡∞á‡∞¶‡∞ø padding‚Äå‡∞®‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞µ‡∞ø‡∞µ‡∞ø‡∞ß ‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Ø‡∞æ‡∞≤ ‡∞™‡±ç‡∞∞‡∞ï‡∞æ‡∞∞‡∞Ç ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞ó‡∞≤‡∞¶‡±Å:

```py
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

‡∞á‡∞¶‡∞ø ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡∞®‡±Å truncate ‡∞ï‡±Ç‡∞°‡∞æ ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞¶‡±Å:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` ‡∞Ü‡∞¨‡±ç‡∞ú‡±Ü‡∞ï‡±ç‡∞ü‡±ç ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï framework ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞≤‡±Å (TensorFlow, PyTorch, NumPy) ‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞™‡±Å ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç ‡∞ï‡±Ç‡∞°‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±Å, ‡∞ï‡±ç‡∞∞‡∞ø‡∞Ç‡∞¶‡∞ø ‡∞ï‡±ã‡∞°‡±ç‚Äå‡∞≤‡±ã `"pt"` ‡∞Ö‡∞Ç‡∞ü‡±á PyTorch ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞≤‡±Å, `"np"` ‡∞Ö‡∞Ç‡∞ü‡±á NumPy arrays:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï tokens[[special-tokens]]

‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞á‡∞ö‡±ç‡∞ö‡∞ø‡∞® input IDs ‡∞®‡±Å ‡∞™‡∞∞‡∞ø‡∞∂‡±Ä‡∞≤‡∞ø‡∞∏‡±ç‡∞§‡±á, ‡∞Ö‡∞µ‡∞ø ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞ó‡∞æ ‡∞ö‡±Ç‡∞∏‡∞ø‡∞® IDs ‡∞ï‡∞Ç‡∞ü‡±á ‡∞ï‡±ä‡∞¶‡±ç‡∞¶‡∞ø‡∞ó‡∞æ ‡∞≠‡∞ø‡∞®‡±ç‡∞®‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

‡∞Ü input IDs ‡∞®‡±Å decode ‡∞ö‡±á‡∞∏‡∞ø ‡∞ö‡±Ç‡∞∏‡±ç‡∞§‡±á ‡∞µ‡∞ø‡∞∑‡∞Ø‡∞Ç ‡∞∏‡±ç‡∞™‡∞∑‡±ç‡∞ü‡∞Æ‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞Ç‡∞≤‡±ã `[CLS]`, ‡∞ö‡∞ø‡∞µ‡∞∞‡∞≤‡±ã `[SEP]` ‡∞Ö‡∞®‡±á ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï ‡∞™‡∞¶‡∞æ‡∞≤‡∞®‡±Å ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø.  
‡∞¶‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞ï‡∞æ‡∞∞‡∞£‡∞Ç ‚Äî ‡∞Ü ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞™‡±ç‡∞∞‡±Ä‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∏‡∞Æ‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞à tokens ‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø, ‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø inference ‡∞∏‡∞Æ‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞ï‡±Ç‡∞°‡∞æ ‡∞Ö‡∞µ‡∞ø ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç ‡∞Ö‡∞µ‡±Å‡∞§‡∞æ‡∞Ø‡∞ø.

‡∞ó‡∞Æ‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø:  
‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å ‡∞á‡∞≤‡∞æ‡∞Ç‡∞ü‡∞ø ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï tokens ‡∞®‡±Å ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞µ‡±Å.  
‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å ‡∞µ‡±á‡∞∞‡±ç‡∞µ‡±á‡∞∞‡±Å special tokens ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø.  
‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø‡∞∏‡∞æ‡∞∞‡±ç‡∞≤‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞Ç‡∞≤‡±ã ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á, ‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø‡∞∏‡∞æ‡∞∞‡±ç‡∞≤‡±Å ‡∞ö‡∞ø‡∞µ‡∞∞‡∞≤‡±ã ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞ú‡±ã‡∞°‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø.  

‡∞è ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞è tokens ‡∞®‡±Å ‡∞ï‡±ã‡∞∞‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡±ã tokenizer ‡∞ï‡±Å ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø‡∞ó‡∞æ ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å ‚Äî ‡∞¶‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞Ö‡∞¶‡∞ø ‡∞Æ‡±Ä ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

## ‡∞Æ‡±Å‡∞ó‡∞ø‡∞Ç‡∞™‡±Å: Tokenizer ‡∞®‡±Å‡∞Ç‡∞°‡∞ø Model ‡∞µ‡∞∞‡∞ï‡±Å[[wrapping-up-from-tokenizer-to-model]]

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å `tokenizer` ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞™‡±à ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±á‡∞ü‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å follow ‡∞Ö‡∞Ø‡±ç‡∞Ø‡±á ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞¶‡∞∂ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å‡∞ï‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ç.  
‡∞ö‡∞ø‡∞µ‡∞∞‡∞ø‡∞∏‡∞æ‡∞∞‡∞ø, ‡∞á‡∞¶‡∞ø padding (‡∞¨‡∞π‡±Å‡∞≥ ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡±Å!), truncation (‡∞™‡±ä‡∞°‡∞µ‡±à‡∞® ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç‚Äå‡∞≤‡±Å!), ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å framework ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞≤‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡±ã ‡∞ö‡±Ç‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç:

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
