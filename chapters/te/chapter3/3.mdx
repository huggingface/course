<FrameworkSwitchCourse {fw} />

# Trainer API‡∞§‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç[[fine-tuning-a-model-with-the-trainer-api]]

<CourseFloatingBanner
  chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/te/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/te/chapter3/section3.ipynb"},
  ]}
/>

<Youtube id="nvBXf7s7vTI"/>

ü§ó Transformers ‡∞≤‡±à‡∞¨‡±ç‡∞∞‡∞∞‡±Ä‡∞≤‡±ã‡∞®‡∞ø `Trainer` ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç ‡∞Æ‡±Ä ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞™‡±à ‡∞™‡±ç‡∞∞‡±Ä-‡∞ü‡±ç‡∞∞‡±à‡∞®‡±ç‡∞°‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞™‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.  
‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞°‡±á‡∞ü‡∞æ ‡∞™‡±ç‡∞∞‡±Ä-‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞Ø‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, Trainer ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞ö‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞ï‡±á‡∞µ‡∞≤‡∞Ç ‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø ‡∞∏‡±ç‡∞ü‡±Ü‡∞™‡±ç‡∞∏‡±ç ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞Æ‡∞ø‡∞ó‡∞ø‡∞≤‡∞ø ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø.  

CPU‡∞≤‡±ã `Trainer.train()` ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞®‡±Ü‡∞Æ‡±ç‡∞Æ‡∞¶‡∞ø‡∞ó‡∞æ ‡∞∞‡∞®‡±ç ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. GPU ‡∞≤‡±á‡∞ï‡∞™‡±ã‡∞§‡±á [Google Colab](https://colab.research.google.com/)‡∞≤‡±ã ‡∞â‡∞ö‡∞ø‡∞§ GPU/TPU ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å.

> [!TIP]
> üìö **‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞∞‡∞ø‡∞∏‡±ã‡∞∞‡±ç‡∞∏‡±Ü‡∞∏‡±ç**: ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡±á ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å [ü§ó Transformers ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞ó‡±à‡∞°‡±ç](https://huggingface.co/docs/transformers/main/en/training) ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å [‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ï‡±Å‡∞ï‡±ç‚Äå‡∞¨‡±Å‡∞ï‡±ç](https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu) ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø.

‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞∞‡∞®‡±ç ‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞ï‡±ã‡∞°‡±ç ‡∞∞‡±Ä‡∞ï‡±ç‡∞Ø‡∞æ‡∞™‡±ç:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£[[training]]

Trainer ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞ö‡∞ø‡∞Ç‡∞ö‡±á ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å, `TrainingArguments` ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞ø. ‡∞á‡∞¶‡∞ø ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ & ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞π‡±à‡∞™‡∞∞‡±ç‚Äå‡∞™‡∞æ‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç‡∞∏‡±ç ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

`push_to_hub=True` ‡∞á‡∞∏‡±ç‡∞§‡±á ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞∏‡∞Æ‡∞Ø‡∞Ç‡∞≤‡±ã‡∞®‡±á ‡∞Æ‡±ã‡∞°‡∞≤‡±ç Hugging Face Hub‡∞ï‡∞ø ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ ‡∞Ö‡∞™‡±ç‚Äå‡∞≤‡±ã‡∞°‡±ç ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

> [!TIP]
> üöÄ **‡∞Ö‡∞°‡±ç‡∞µ‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞ï‡∞æ‡∞®‡±ç‡∞´‡∞ø‡∞ó‡∞∞‡±á‡∞∑‡∞®‡±ç**: ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞Ü‡∞™‡±ç‡∞∑‡∞®‡±ç‡∞∏‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç [TrainingArguments ‡∞°‡∞æ‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±á‡∞∑‡∞®‡±ç](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø.

‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞ö‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

> ‡∞µ‡∞æ‡∞∞‡±ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‚Äì ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡∞Ç‡∞ü‡±á BERT ‡∞™‡±ç‡∞∞‡±Ä-‡∞ü‡±ç‡∞∞‡±à‡∞®‡±ç‡∞°‡±ç ‡∞ï‡∞æ‡∞¶‡±Å sentence pair classification ‡∞ï‡±ã‡∞∏‡∞Ç. ‡∞™‡∞æ‡∞§ ‡∞π‡±Ü‡∞°‡±ç ‡∞§‡±Ä‡∞∏‡±á‡∞∏‡∞ø, ‡∞ï‡±ä‡∞§‡±ç‡∞§ sequence classification ‡∞π‡±Ü‡∞°‡±ç ‡∞ú‡±ã‡∞°‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞ø.

Trainer ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    processing_class=tokenizer,  # ‡∞ï‡±ä‡∞§‡±ç‡∞§ ‡∞´‡±Ä‡∞ö‡∞∞‡±ç ‚Äì ‡∞è ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞µ‡∞æ‡∞°‡∞æ‡∞≤‡±ã ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø
)
```

`processing_class` ‡∞á‡∞µ‡±ç‡∞µ‡∞°‡∞Ç ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ `data_collator` ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ `DataCollatorWithPadding` ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø:

```py
trainer.train()
```

**‡∞á‡∞¶‡∞ø ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø**, ‡∞ï‡∞æ‡∞®‡±Ä ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç ‡∞Æ‡±Ü‡∞ü‡±ç‡∞∞‡∞ø‡∞ï‡±ç‡∞∏‡±ç ‡∞∞‡∞æ‡∞µ‡±Å ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡∞Ç‡∞ü‡±á:

1. `eval_strategy` ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞≤‡±á‡∞¶‡±Å
2. `compute_metrics` ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡±Å

---

### ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç[[evaluation]]

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
# ‚Üí (408, 2) (408,)
```

```py
import numpy as np
preds = np.argmax(predictions.predictions, axis=-1)
```

```py
import evaluate
metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
# ‚Üí {'accuracy': 0.8578, 'f1': 0.8996}
```

‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø `compute_metrics` ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

---

### ‡∞ï‡±ä‡∞§‡±ç‡∞§ Trainer‡∞§‡±ã metrics‡∞§‡±ã ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£

```py
training_args = TrainingArguments("test-trainer", eval_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    processing_class=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞é‡∞™‡∞æ‡∞ï‡±ç ‡∞ö‡∞ø‡∞µ‡∞∞ validation loss + metrics ‡∞ï‡∞®‡∞ø‡∞™‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø!

---

### ‡∞Ö‡∞°‡±ç‡∞µ‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞´‡±Ä‡∞ö‡∞∞‡±ç‡∞∏‡±ç[[advanced-training-features]]

**‡∞Æ‡∞ø‡∞ï‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞™‡±ç‡∞∞‡±Ü‡∞∏‡∞ø‡∞∑‡∞®‡±ç (fp16)** ‚Äì ‡∞µ‡±á‡∞ó‡∞Ç + ‡∞Æ‡±Ü‡∞Æ‡∞∞‡±Ä ‡∞Ü‡∞¶‡∞æ:

```py
training_args = TrainingArguments(
    "test-trainer",
    eval_strategy="epoch",
    fp16=True,  # ‡∞Æ‡∞ø‡∞ï‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞™‡±ç‡∞∞‡±Ü‡∞∏‡∞ø‡∞∑‡∞®‡±ç ‡∞Ü‡∞®‡±ç
)
```

**‡∞ó‡±ç‡∞∞‡±á‡∞°‡∞ø‡∞Ø‡∞Ç‡∞ü‡±ç ‡∞Ö‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Å‡∞≤‡±á‡∞∑‡∞®‡±ç** ‚Äì ‡∞ö‡∞ø‡∞®‡±ç‡∞® GPU‡∞≤‡±ã ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ effective batch size:

```py
training_args = TrainingArguments(
    "test-trainer",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # effective batch size = 16
)
```

**‡∞≤‡±Ü‡∞∞‡±ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∞‡±á‡∞ü‡±ç ‡∞∑‡±Ü‡∞°‡±ç‡∞Ø‡±Ç‡∞≤‡∞∞‡±ç**:

```py
training_args = TrainingArguments(
    "test-trainer",
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
)
```

> [!TIP]
> üéØ **‡∞™‡∞∞‡±ç‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç**: ‡∞°‡∞ø‡∞∏‡±ç‡∞ü‡±ç‡∞∞‡∞ø‡∞¨‡±ç‡∞Ø‡±Ç‡∞ü‡±Ü‡∞°‡±ç ‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç, ‡∞Æ‡±Ü‡∞Æ‡∞∞‡±Ä ‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç [ü§ó Transformers performance guide](https://huggingface.co/docs/transformers/main/en/performance) ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø.

Trainer ‡∞Æ‡∞≤‡±ç‡∞ü‡±Ä-GPU/TPU‡∞≤‡±ã ‡∞î‡∞ü‡±ç-‡∞Ü‡∞´‡±ç-‡∞¶‡∞ø-‡∞¨‡∞æ‡∞ï‡±ç‡∞∏‡±ç ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‚Äì ‡∞ö‡∞æ‡∞™‡±ç‡∞ü‡∞∞‡±ç 10‡∞≤‡±ã ‡∞µ‡∞ø‡∞µ‡∞∞‡∞Ç‡∞ó‡∞æ.

---

## ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞ï‡±ç‡∞µ‡∞ø‡∞ú‡±ç[[section-quiz]]

### 1. Trainer‡∞≤‡±ã `processing_class` ‡∞™‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç ‡∞¶‡±á‡∞®‡∞ø‡∞ï‡∞ø?

<Question
choices={[
{text: "‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ü‡∞∞‡±ç‡∞ï‡∞ø‡∞ü‡±Ü‡∞ï‡±ç‡∞ö‡∞∞‡±ç ‡∞é‡∞Ç‡∞ö‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "Trainer‡∞ï‡∞ø ‡∞è ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡±ã ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø", explain: "‡∞∏‡∞∞‡±à‡∞®‡∞¶‡∞ø!", correct: true},
{text: "‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞∏‡±à‡∞ú‡±ç ‡∞®‡∞ø‡∞∞‡±ç‡∞£‡∞Ø‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç ‡∞´‡±ç‡∞∞‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±Ä ‡∞ï‡∞Ç‡∞ü‡±ç‡∞∞‡±ã‡∞≤‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"}
]}
/>

### 2. ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç ‡∞é‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞ú‡∞∞‡∞ó‡∞æ‡∞≤‡±ã ‡∞è ‡∞™‡∞æ‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç ‡∞®‡∞ø‡∞∞‡±ç‡∞£‡∞Ø‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{text: "eval_frequency", explain: "‡∞Ö‡∞≤‡∞æ‡∞Ç‡∞ü‡∞ø‡∞¶‡∞ø ‡∞≤‡±á‡∞¶‡±Å"},
{text: "eval_strategy", explain: "‡∞∏‡∞∞‡±à‡∞®‡∞¶‡∞ø! ('epoch'/'steps')", correct: true},
{text: "evaluation_steps", explain: "‡∞∏‡±ç‡∞ü‡±Ü‡∞™‡±ç‡∞∏‡±ç ‡∞é‡∞®‡±ç‡∞®‡∞ø ‡∞Ö‡∞®‡∞ø ‡∞ö‡±Ü‡∞¨‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‡∞ï‡∞æ‡∞®‡±Ä ‡∞é‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Ö‡∞®‡∞ø ‡∞ï‡∞æ‡∞¶‡±Å"},
{text: "do_eval", explain: "‡∞™‡∞æ‡∞§ ‡∞µ‡±Ü‡∞∞‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞â‡∞Ç‡∞°‡±á‡∞¶‡∞ø"}
]}
/>

### 3. `fp16=True` ‡∞è‡∞Æ‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{text: "16-‡∞¨‡∞ø‡∞ü‡±ç ‡∞á‡∞Ç‡∞ü‡∞ø‡∞ú‡∞∞‡±ç ‡∞™‡±ç‡∞∞‡±Ü‡∞∏‡∞ø‡∞∑‡∞®‡±ç", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞Æ‡∞ø‡∞ï‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞™‡±ç‡∞∞‡±Ü‡∞∏‡∞ø‡∞∑‡∞®‡±ç ‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‚Äì ‡∞µ‡±á‡∞ó‡∞Ç + ‡∞Æ‡±Ü‡∞Æ‡∞∞‡±Ä ‡∞Ü‡∞¶‡∞æ", explain: "‡∞∏‡∞∞‡±à‡∞®‡∞¶‡∞ø!", correct: true},
{text: "16 ‡∞é‡∞™‡∞æ‡∞ï‡±ç‡∞∏‡±ç ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞ü‡±ç‡∞∞‡±à‡∞®‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "16 GPU‡∞≤‡±Å ‡∞µ‡∞æ‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"}
]}
/>

### 4. `compute_metrics` ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞™‡∞æ‡∞§‡±ç‡∞∞ ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{text: "‡∞≤‡∞æ‡∞∏‡±ç ‡∞ï‡∞æ‡∞≤‡∞ø‡∞ï‡±ç‡∞Ø‡±Å‡∞≤‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞≤‡∞æ‡∞ú‡∞ø‡∞ü‡±ç‡∞∏‡±ç ‚Üí ‡∞™‡±ç‡∞∞‡∞ø‡∞°‡∞ø‡∞ï‡±ç‡∞∑‡∞®‡±ç‡∞∏‡±ç ‚Üí ‡∞Æ‡±Ü‡∞ü‡±ç‡∞∞‡∞ø‡∞ï‡±ç‡∞∏‡±ç (accuracy, F1)", explain: "‡∞∏‡∞∞‡±à‡∞®‡∞¶‡∞ø!", correct: true},
{text: "‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡∞∞‡±ç ‡∞é‡∞Ç‡∞ö‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞Ç", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞°‡±á‡∞ü‡∞æ ‡∞™‡±ç‡∞∞‡±Ä-‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç", explain: "‡∞≤‡±á‡∞¶‡±Å"}
]}
/>

### 5. `eval_dataset` ‡∞á‡∞µ‡±ç‡∞µ‡∞ï‡∞™‡±ã‡∞§‡±á ‡∞è‡∞Æ‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{text: "‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞´‡±Ü‡∞Ø‡∞ø‡∞≤‡±ç ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞°‡±á‡∞ü‡∞æ ‡∞®‡±Å‡∞Ç‡∞ö‡∞ø ‡∞Ü‡∞ü‡±ã ‡∞∏‡±ç‡∞™‡±ç‡∞≤‡∞ø‡∞ü‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç metrics ‡∞∞‡∞æ‡∞µ‡±Å, ‡∞ï‡∞æ‡∞®‡±Ä ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞∏‡∞∞‡±à‡∞®‡∞¶‡∞ø!", correct: true},
{text: "‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞°‡±á‡∞ü‡∞æ‡∞§‡±ã‡∞®‡±á ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"}
]}
/>

### 6. ‡∞ó‡±ç‡∞∞‡±á‡∞°‡∞ø‡∞Ø‡∞Ç‡∞ü‡±ç ‡∞Ö‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±Å‡∞≤‡±á‡∞∑‡∞®‡±ç ‡∞Ö‡∞Ç‡∞ü‡±á ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{text: "‡∞°‡∞ø‡∞∏‡±ç‡∞ï‡±ç‚Äå‡∞≤‡±ã gradients ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞¨‡∞π‡±Å‡∞≥ ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡∞≤‡±ã gradients accumulate ‚Üí ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ batch size simulation", explain: "‡∞∏‡∞∞‡±à‡∞®‡∞¶‡∞ø!", correct: true},
{text: "fp16‡∞§‡±ã ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø", explain: "‡∞≤‡±á‡∞¶‡±Å"},
{text: "‡∞ó‡±ç‡∞∞‡±á‡∞°‡∞ø‡∞Ø‡∞Ç‡∞ü‡±ç ‡∞ì‡∞µ‡∞∞‡±ç‚Äå‡∞´‡±ç‡∞≤‡±ã ‡∞®‡∞ø‡∞∞‡±ã‡∞ß‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç", explain: "gradient clipping"}
]}
/>

> [!TIP]
> üí° **‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø ‡∞™‡∞æ‡∞Ø‡∞ø‡∞Ç‡∞ü‡±ç‡∞≤‡±Å**:
>
> * `Trainer` API ‡∞Ö‡∞§‡∞ø ‡∞∏‡±Å‡∞≤‡∞≠‡∞Æ‡±à‡∞®, ‡∞∂‡∞ï‡±ç‡∞§‡∞ø‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞á‡∞Ç‡∞ü‡∞∞‡±ç‚Äå‡∞´‡±á‡∞∏‡±ç
> * `processing_class` ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞∏‡±ç‡∞™‡∞∑‡±ç‡∞ü‡∞Ç‡∞ó‡∞æ ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø
> * `TrainingArguments`‡∞≤‡±ã learning rate, batch size, fp16, gradient accumulation ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
> * `compute_metrics` ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ï‡∞æ‡∞µ‡∞≤‡∞∏‡∞ø‡∞® metrics ‡∞ö‡±Ç‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å
> * ‡∞Ü‡∞ß‡±Å‡∞®‡∞ø‡∞ï optimizations (fp16, gradient accumulation) ‡∞í‡∞ï‡±ç‡∞ï ‡∞≤‡±à‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞Ü‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å