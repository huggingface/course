<FrameworkSwitchCourse {fw} />

# ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞≤‡±Ç‡∞™‡±ç[[a-full-training]]

<CourseFloatingBanner
chapter={3}
classNames="absolute z-10 right-0 top-0"
notebooks={[
{label: "Google Colab", value: "[https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/te/chapter3/section4.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/te/chapter3/section4.ipynb)"},
{label: "Aws Studio", value: "[https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/te/chapter3/section4.ipynb](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/te/chapter3/section4.ipynb)"},
]}
/> <Youtube id="Dh9CL8fyG80"/>

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å `Trainer` ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡±ç ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ï‡±Å‡∞Ç‡∞°‡∞æ, ‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞ö‡±Ç‡∞™‡∞ø‡∞® ‡∞Ö‡∞¶‡±á ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡∞®‡±Å ‡∞∏‡∞æ‡∞ß‡∞ø‡∞Ç‡∞ö‡±á ‡∞µ‡∞ø‡∞ß‡∞Ç‡∞ó‡∞æ ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞≤‡±Ç‡∞™‡±ç‚Äå‡∞®‡±Å PyTorch‡∞≤‡±ã ‡∞Ö‡∞Æ‡∞≤‡±Å ‡∞ö‡±á‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç. ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞ö‡±Ü‡∞™‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ç ‚Äì ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç 2‡∞≤‡±ã ‡∞°‡±á‡∞ü‡∞æ ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡∞ø ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞®‡∞ø ‡∞Ö‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ç.

> [!TIP]
> üèóÔ∏è **‡∞Æ‡∞æ‡∞®‡±ç‡∞Ø‡±Å‡∞µ‡∞≤‡±ç ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£**: PyTorch‡∞≤‡±ã ‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞≤‡±Ç‡∞™‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¨‡±Ü‡∞∏‡±ç‡∞ü‡±ç ‡∞™‡±ç‡∞∞‡∞æ‡∞ï‡±ç‡∞ü‡±Ä‡∞∏‡±ç‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç [ü§ó Transformers training documentation](https://huggingface.co/docs/transformers/main/en/training#train-in-native-pytorch) ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å [custom training cookbook](https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu#model) ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø.

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£‡∞ï‡±Å ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[prepare-for-training]]

`Trainer` ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ ‡∞ö‡±á‡∞∏‡±á ‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø ‡∞¶‡∞∂‡∞≤‡∞®‡±Å ‡∞Æ‡∞®‡∞Ç ‡∞Æ‡∞æ‡∞®‡±ç‡∞Ø‡±Å‡∞µ‡∞≤‡±ç‚Äå‡∞ó‡∞æ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

tokenized_datasets["train"].column_names
```

```python out
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å DataLoader‡∞≤‡±Å ‡∞§‡∞Ø‡∞æ‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

‡∞í‡∞ï ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞ö‡±Ç‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

‡∞Æ‡±ã‡∞°‡∞≤‡±ç:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

‡∞ü‡±Ü‡∞∏‡±ç‡∞ü‡±ç:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

‡∞Ü‡∞™‡±ç‡∞ü‡∞ø‡∞Æ‡±à‡∞ú‡∞∞‡±ç:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
```

‡∞≤‡±Ü‡∞∞‡±ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞∞‡±á‡∞ü‡±ç ‡∞∑‡±Ü‡∞°‡±ç‡∞Ø‡±Ç‡∞≤‡∞∞‡±ç:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

GPU‡∞ï‡∞ø ‡∞§‡∞∞‡∞≤‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print(device)
```

```python out
device(type='cuda')
```

### ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞≤‡±Ç‡∞™‡±ç[[the-training-loop]]

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        # Gradient clipping (optional)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

> [!TIP]
> üí° **‡∞Ö‡∞°‡±ç‡∞µ‡∞æ‡∞®‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞ü‡∞ø‡∞™‡±ç‡∞∏‡±ç**:
>
> * ‡∞Æ‡∞ø‡∞ï‡±ç‡∞∏‡±ç‡∞°‡±ç ‡∞™‡±ç‡∞∞‡±Ü‡∞∏‡∞ø‡∞∑‡∞®‡±ç: `torch.cuda.amp.autocast()` ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å `GradScaler` ‡∞µ‡∞æ‡∞°‡∞Ç‡∞°‡∞ø
> * ‡∞ó‡±ç‡∞∞‡±á‡∞°‡∞ø‡∞Ø‡∞Ç‡∞ü‡±ç ‡∞Ö‡∞ï‡±ç‡∞Ø‡±Å‡∞Æ‡±ç‡∞Ø‡±Å‡∞≤‡±á‡∞∑‡∞®‡±ç: ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç ‡∞∏‡±à‡∞ú‡±ç ‡∞∏‡∞ø‡∞Æ‡±ç‡∞Ø‡±Å‡∞≤‡±á‡∞∑‡∞®‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞ó‡±ç‡∞∞‡±á‡∞°‡∞ø‡∞Ø‡∞Ç‡∞ü‡±ç‡∞∏‡±ç ‡∞®‡±Å accumulate ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
> * Checkpointing: ‡∞Æ‡∞ß‡±ç‡∞Ø‡∞≤‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø

### ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç ‡∞≤‡±Ç‡∞™‡±ç[[the-evaluation-loop]]

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()

for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

### ü§ó Accelerate‡∞§‡±ã ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞≤‡±Ç‡∞™‡±ç[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

```py
from accelerate import Accelerator
from transformers import get_scheduler
from torch.optim import AdamW
from tqdm.auto import tqdm

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss

        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

‡∞à ‡∞ï‡±ã‡∞°‡±ç‚Äå‡∞®‡±Å `train.py`‡∞≤‡±ã ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞∏‡∞ø ‡∞ï‡∞ø‡∞Ç‡∞¶‡∞ø ‡∞ï‡∞Æ‡∞æ‡∞Ç‡∞°‡±ç‚Äå‡∞≤‡∞§‡±ã ‡∞∞‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```bash
accelerate config
accelerate launch train.py
```

Colab‡∞≤‡±ã ‡∞ü‡±Ü‡∞∏‡±ç‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞Ç‡∞ü‡±á:

```py
from accelerate import notebook_launcher
notebook_launcher(training_function)
```

### ‡∞Æ‡∞∞‡∞ø‡∞®‡±ç‡∞®‡∞ø ‡∞¨‡±Ü‡∞∏‡±ç‡∞ü‡±ç ‡∞™‡±ç‡∞∞‡∞æ‡∞ï‡±ç‡∞ü‡±Ä‡∞∏‡±ç‡∞∏‡±ç[[next-steps-and-best-practices]]

* **Model Evaluation**: Accuracy‡∞§‡±ã ‡∞™‡∞æ‡∞ü‡±Å ‡∞á‡∞§‡∞∞ ‡∞Æ‡±Ü‡∞ü‡±ç‡∞∞‡∞ø‡∞ï‡±ç‡∞∏‡±ç‚Äå‡∞§‡±ã ‡∞ï‡±Ç‡∞°‡∞æ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞é‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
* **Hyperparameter Tuning**: Optuna ‡∞≤‡±á‡∞¶‡∞æ Ray Tune ‡∞µ‡∞Ç‡∞ü‡∞ø‡∞µ‡∞ø ‡∞µ‡∞æ‡∞°‡∞ø ‡∞π‡±à‡∞™‡∞∞‡±ç‚Äå‡∞™‡∞æ‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç‡∞∏‡±ç optimize ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
* **Model Monitoring**: Training metrics, learning curves, validation performance ‡∞ü‡±ç‡∞∞‡∞æ‡∞ï‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
* **Model Sharing**: Hugging Face Hub‡∞≤‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç share ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø
* **Efficiency**: Gradient checkpointing, LoRA, AdaLoRA, quantization techniques ‡∞µ‡∞æ‡∞°‡∞ø ‡∞™‡±Ü‡∞¶‡±ç‡∞¶ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç efficiency ‡∞™‡±Ü‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø

### ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞ï‡±ç‡∞µ‡∞ø‡∞ú‡±ç[[section-quiz]]

### 1. Adam ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å AdamW ‡∞Æ‡∞ß‡±ç‡∞Ø ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞§‡±á‡∞°‡∞æ ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{ text: "AdamW ‡∞µ‡±á‡∞∞‡±á learning rate schedule ‡∞µ‡∞æ‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Learning rate scheduling optimizer ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞µ‡±á‡∞∞‡±Å." },
{ text: "AdamW decoupled weight decay regularization ‡∞µ‡∞æ‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Correct! Weight decay gradient updates ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞µ‡±á‡∞∞‡±Å.", correct: true },
{ text: "AdamW transformer models‡∞ï‡∞ø ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞™‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞è ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ï‡∞ø ‡∞Ö‡∞Ø‡∞ø‡∞®‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å." },
{ text: "AdamW memory ‡∞§‡∞ï‡±ç‡∞ï‡±Å‡∞µ‡∞ó‡∞æ ‡∞µ‡∞æ‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Memory requirement ‡∞á‡∞∞‡±Å‡∞µ‡±Å‡∞∞‡∞ø‡∞ï‡±Ä ‡∞§‡±Å‡∞≤‡±ç‡∞Ø‡∞Ç." }
]}
/>

### 2. Training loop ‡∞≤‡±ã operations ‡∞∏‡∞∞‡±à‡∞® ‡∞ï‡±ç‡∞∞‡∞Æ‡∞Ç ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{ text: "Forward ‚Üí Backward ‚Üí Optimizer step ‚Üí Zero gradients", explain: "Zero gradients ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ forward ‡∞≤‡±ã‡∞™‡±Å ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø." },
{ text: "Forward ‚Üí Backward ‚Üí Optimizer step ‚Üí Scheduler step ‚Üí Zero gradients", explain: "Correct! ‡∞á‡∞¶‡∞ø proper order.", correct: true },
{ text: "Zero gradients ‚Üí Forward ‚Üí Optimizer step ‚Üí Backward", explain: "Backward forward ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ ‡∞µ‡±Å‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø." },
{ text: "Forward ‚Üí Zero gradients ‚Üí Backward ‚Üí Optimizer step", explain: "Zeroing gradients ‡∞§‡∞™‡±ç‡∞™‡±Å ‡∞∏‡±ç‡∞•‡∞æ‡∞®‡∞Ç‡∞≤‡±ã." }
]}
/>

### 3. ü§ó Accelerate library ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞Ç ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{ text: "Forward pass optimize ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Accelerate model architecture optimize ‡∞ö‡±á‡∞Ø‡∞¶‡±Å." },
{ text: "Hyperparameters select ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Hyperparameter optimization Accelerate ‡∞ö‡±á‡∞Ø‡∞¶‡±Å." },
{ text: "Multiple GPUs/TPUs distributed training enable ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Correct! Minimal code changes ‡∞§‡±ã distributed training.", correct: true },
{ text: "Models different frameworks‡∞ï‡∞ø convert ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Accelerate PyTorch ‡∞≤‡±ã ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø." }
]}
/>

### 4. Training loop ‡∞≤‡±ã batches device‡∞ï‡∞ø ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞§‡∞∞‡∞≤‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞∞‡±Å?

<Question
choices={[
{ text: "Training ‡∞µ‡±á‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø.", explain: "Speed effect ‡∞ï‡∞æ‡∞®‡±Ä main reason ‡∞ï‡∞æ‡∞¶‡±Å." },
{ text: "Model ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å data ‡∞í‡∞ï‡±á device ‡∞≤‡±ã ‡∞â‡∞Ç‡∞°‡∞æ‡∞≤‡∞ø.", explain: "Correct! PyTorch requires same device.", correct: true },
{ text: "Memory save ‡∞ï‡±ã‡∞∏‡∞Ç.", explain: "Device movement memory save ‡∞ï‡∞æ‡∞¶‡±Å." },
{ text: "DataLoader ‡∞ï‡±ã‡∞∏‡∞Ç required.", explain: "DataLoader specific device require ‡∞ï‡∞æ‡∞¶‡±Å." }
]}
/>

### 5. Evaluation ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å `model.eval()` ‡∞è‡∞Æ‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{ text: "Parameters freeze ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Freeze requires requires_grad=False." },
{ text: "Dropout ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å BatchNorm inference‡∞ï‡∞ø ‡∞Æ‡∞æ‡∞∞‡±Å‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Correct! eval mode disables dropout and uses running stats.", correct: true },
{ text: "Gradient computation enable ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Gradient evaluation torch.no_grad() ‡∞§‡±ã disabled." },
{ text: "Metrics automatically calculate ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Metrics calculate manually ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø." }
]}
/>

### 6. Evaluation ‡∞≤‡±ã `torch.no_grad()` ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞Ç ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{ text: "Model predictions block ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Predictions block ‡∞ï‡∞æ‡∞¶‡±Å." },
{ text: "Gradient tracking disable ‡∞ö‡±á‡∞∏‡∞ø memory, speed optimize ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Correct! Gradient computation disable ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", correct: true },
{ text: "Evaluation mode enable ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "Evaluation mode model.eval() ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ." },
{ text: "Results consistent across runs.", explain: "Reproducibility random seeds ‡∞§‡±ã achieve ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø." }
]}
/>

### 7. ü§ó Accelerate ‡∞µ‡∞æ‡∞°‡∞ø‡∞§‡±á training loop ‡∞≤‡±ã ‡∞è‡∞Æ‡∞ø ‡∞Æ‡∞æ‡∞∞‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{ text: "Entire loop rewrite ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø.", explain: "Minimal changes suffice." },
{ text: "Key objects accelerator.prepare() ‡∞§‡±ã wrap ‡∞ö‡±á‡∞∏‡∞ø accelerator.backward() ‡∞µ‡∞æ‡∞°‡∞æ‡∞≤‡∞ø.", explain: "Correct! Main changes.", correct: true },
{ text: "Number of GPUs specify ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø.", explain: "Accelerate hardware auto detect ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø." },
{ text: "Optimizer, scheduler ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞æ‡∞≤‡∞ø.", explain: "Same optimizers & schedulers ‡∞µ‡∞æ‡∞°‡∞µ‡∞ö‡±ç‡∞ö‡±Å." }
]}
/>

> [!TIP]
> üí° **Key Takeaways**:
>
> * ‡∞Æ‡∞æ‡∞®‡±ç‡∞Ø‡±Å‡∞µ‡∞≤‡±ç training loops ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø control ‡∞á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø; proper sequence: forward ‚Üí backward ‚Üí optimizer step ‚Üí scheduler step ‚Üí zero gradients
> * AdamW weight decay ‡∞§‡±ã transformer models‡∞ï‡∞ø recommended
> * Evaluation ‡∞≤‡±ã model.eval() ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å torch.no_grad() ‡∞§‡∞™‡±ç‡∞™‡∞®‡∞ø‡∞∏‡∞∞‡∞ø
> * ü§ó Accelerate distributed training minimal changes ‡∞§‡±ã
> * Device management (GPU/CPU tensors) PyTorch ‡∞≤‡±ã crucial
> * Mixed precision, gradient accumulation, gradient clipping efficiency ‡∞™‡±Ü‡∞Ç‡∞ö‡±Å‡∞§‡∞æ‡∞Ø‡∞ø
