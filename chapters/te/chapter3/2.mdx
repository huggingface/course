# ‡∞°‡±á‡∞ü‡∞æ ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç[[processing-the-data]]

<CourseFloatingBanner
chapter={3}
classNames="absolute z-10 right-0 top-0"
notebooks={[
{label: "Google Colab", value: "[https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/te/chapter3/section2.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/te/chapter3/section2.ipynb)"},
{label: "Aws Studio", value: "[https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/te/chapter3/section2.ipynb](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/te/chapter3/section2.ipynb)"},
]}
/>

‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞Ö‡∞ß‡±ç‡∞Ø‡∞æ‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞ö‡±Ç‡∞™‡∞ø‡∞® ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞®‡±á ‡∞ï‡±ä‡∞®‡∞∏‡∞æ‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡±Ç, ‡∞í‡∞ï‡±á ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞™‡±à ‡∞∏‡±Ä‡∞ï‡±ç‡∞µ‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡∞ø‡∞´‡±à‡∞Ø‡∞∞‡±ç‚Äå‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡±ã ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞ö‡±Ç‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç:

```python
import torch
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]

batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

‡∞∏‡∞π‡∞ú‡∞Ç‡∞ó‡∞æ‡∞®‡±á, ‡∞ï‡±á‡∞µ‡∞≤‡∞Ç ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞§‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞á‡∞∏‡±ç‡∞§‡±á ‡∞Æ‡∞Ç‡∞ö‡∞ø ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡±Å ‡∞∞‡∞æ‡∞µ‡±Å. ‡∞Æ‡∞Ç‡∞ö‡∞ø ‡∞´‡∞≤‡∞ø‡∞§‡∞æ‡∞≤‡±Å ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞Ç‡∞ü‡±á ‡∞™‡±Ü‡∞¶‡±ç‡∞¶ ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø.

‡∞à ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ó‡∞æ **MRPC** (Microsoft Research Paraphrase Corpus) ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡∞æ‡∞Ç. ‡∞à ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±ã 5,801 ‡∞ú‡∞§‡∞≤ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø ‚Äì ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞í‡∞ï‡±á ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞æ ‡∞≤‡±á‡∞¶‡∞æ ‡∞Ö‡∞®‡∞ø ‡∞≤‡±á‡∞¨‡±Å‡∞≤‡±ç ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞à ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç ‡∞ö‡∞ø‡∞®‡±ç‡∞®‡∞¶‡∞ø ‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡±ã‡∞ó‡∞æ‡∞≤‡±Å ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞∏‡±Å‡∞≤‡∞≠‡∞Ç.

### ‡∞π‡∞¨‡±ç ‡∞®‡±Å‡∞Ç‡∞ö‡∞ø ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±Å‡∞∞‡∞æ‡∞µ‡∞°‡∞Ç[[loading-a-dataset-from-the-hub]]

<Youtube id="_BZearw7f0w"/>

‡∞π‡∞¨‡±ç‚Äå‡∞≤‡±ã ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞ï‡∞æ‡∞¶‡±Å ‚Äì ‡∞µ‡±á‡∞≤‡∞æ‡∞¶‡∞ø ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø (‡∞ö‡∞æ‡∞≤‡∞æ ‡∞≠‡∞æ‡∞∑‡∞≤‡±ç‡∞≤‡±ã!). ‡∞µ‡∞æ‡∞ü‡∞ø‡∞®‡∞ø [‡∞á‡∞ï‡±ç‡∞ï‡∞°](https://huggingface.co/datasets) ‡∞ö‡±Ç‡∞°‡±ä‡∞ö‡±ç‡∞ö‡±Å. MRPC ‡∞Ö‡∞®‡±á‡∞¶‡∞ø **GLUE ‡∞¨‡±Ü‡∞Ç‡∞ö‡±ç‚Äå‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±ç**‡∞≤‡±ã ‡∞≠‡∞æ‡∞ó‡∞Ç. GLUE ‡∞Ö‡∞Ç‡∞ü‡±á 10 ‡∞∞‡∞ï‡∞æ‡∞≤ ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç ‡∞ï‡±ç‡∞≤‡∞æ‡∞∏‡∞ø‡∞´‡∞ø‡∞ï‡±á‡∞∑‡∞®‡±ç ‡∞ü‡∞æ‡∞∏‡±ç‡∞ï‡±ç‚Äå‡∞≤‡∞™‡±à ‡∞Æ‡±ã‡∞°‡∞≥‡±ç‡∞≤ ‡∞™‡∞®‡∞ø‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±ä‡∞≤‡∞ø‡∞ö‡±á ‡∞Ö‡∞ï‡∞°‡∞Æ‡∞ø‡∞ï‡±ç ‡∞¨‡±Ü‡∞Ç‡∞ö‡±ç‚Äå‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±ç.

ü§ó Datasets ‡∞≤‡±à‡∞¨‡±ç‡∞∞‡∞∞‡±Ä ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞í‡∞ï‡±ç‡∞ï ‡∞≤‡±à‡∞®‡±ç‚Äå‡∞≤‡±ã ‡∞°‡±å‡∞®‡±ç‚Äå‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞ö‡±ç‡∞ö‡±Å:

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

`DatasetDict`‡∞≤‡±ã ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£, ‡∞µ‡∞æ‡∞≤‡∞ø‡∞°‡±á‡∞∑‡∞®‡±ç, ‡∞ü‡±Ü‡∞∏‡±ç‡∞ü‡±ç ‡∞∏‡±Ü‡∞ü‡±ç‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø. ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞é‡∞≤‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç‚Äå‡∞≤‡±ã `sentence1`, `sentence2`, `label`, `idx` ‡∞ï‡∞æ‡∞≤‡∞Æ‡±ç‡∞∏‡±ç ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø.

```python
raw_datasets["train"][0]
```

```python out
{
  'idx': 0,
  'label': 1,
  'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
  'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'
}
```

‡∞≤‡±á‡∞¨‡±Å‡∞≤‡±ç‡∞∏‡±ç ‡∞á‡∞™‡±ç‡∞™‡∞ü‡∞ø‡∞ï‡±á ‡∞á‡∞Ç‡∞ü‡∞ø‡∞ú‡∞∞‡±ç‡∞≤‡±Å‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø (`0` = ‡∞∏‡∞Æ‡∞æ‡∞®‡∞Ç ‡∞ï‡∞æ‡∞¶‡±Å, `1` = ‡∞∏‡∞Æ‡∞æ‡∞®‡∞Ç).

> [!TIP]
> ‚úèÔ∏è **‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø!** ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±ã 15‡∞µ ‡∞é‡∞≤‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç, ‡∞µ‡∞æ‡∞≤‡∞ø‡∞°‡±á‡∞∑‡∞®‡±ç ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±ã 87‡∞µ ‡∞é‡∞≤‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø. ‡∞µ‡∞æ‡∞ü‡∞ø ‡∞≤‡±á‡∞¨‡±Å‡∞≤‡±ç‡∞∏‡±ç ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

### ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç ‡∞™‡±ç‡∞∞‡±Ä-‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç[[preprocessing-a-dataset]]

<Youtube id="0u3ioSwev3s"/>

‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±Å‡∞®‡±á ‡∞∏‡∞Ç‡∞ñ‡±ç‡∞Ø‡∞≤‡±Å‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞æ‡∞≤‡∞ø ‚Üí ‡∞¶‡±Ä‡∞®‡∞ø‡∞ï‡∞ø ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞µ‡∞æ‡∞°‡∞§‡∞æ‡∞Ç.

```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

`token_type_ids` ‚Üí ‡∞è ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç ‡∞Æ‡±ä‡∞¶‡∞ü‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞¶‡∞ø, ‡∞è‡∞¶‡∞ø ‡∞∞‡±Ü‡∞Ç‡∞°‡∞µ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞¶‡∞ø ‡∞Ö‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡±á ‡∞´‡±Ä‡∞≤‡±ç‡∞°‡±ç (BERT ‡∞¶‡±Ä‡∞®‡±ç‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡±Ä-‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç‚Äå‡∞≤‡±ã ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±Å‡∞Ç‡∞¶‡∞ø).

> [!TIP]
> ‚úèÔ∏è **‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø!** ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±ã 15‡∞µ ‡∞é‡∞≤‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞µ‡±á‡∞∞‡±ç‡∞µ‡±á‡∞∞‡±Å‡∞ó‡∞æ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞ú‡∞§‡∞ó‡∞æ ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡±ç ‡∞ö‡±á‡∞∏‡∞ø ‡∞ö‡±Ç‡∞°‡∞Ç‡∞°‡∞ø. ‡∞§‡±á‡∞°‡∞æ ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

### ‡∞Æ‡±ä‡∞§‡±ç‡∞§‡∞Ç ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞®‡±Å ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç

```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```

`batched=True` ‡∞µ‡∞æ‡∞°‡∞ø‡∞§‡±á ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞µ‡±á‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø (Rust‡∞≤‡±ã ‡∞∞‡∞æ‡∞∏‡∞ø‡∞® ‡∞´‡∞æ‡∞∏‡±ç‡∞ü‡±ç ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç ‡∞µ‡∞≤‡±ç‡∞≤).

‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞á‡∞Ç‡∞ï‡∞æ ‡∞ö‡±á‡∞Ø‡∞≤‡±á‡∞¶‡±Å ‚Äì ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡∞Ç‡∞ü‡±á ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞ó‡∞∞‡∞ø‡∞∑‡±ç‡∞ü ‡∞™‡±ä‡∞°‡∞µ‡±Å‡∞ï‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞Ç (**‡∞°‡±à‡∞®‡∞Æ‡∞ø‡∞ï‡±ç ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç**) ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ ‡∞∏‡∞Æ‡∞∞‡±ç‡∞•‡∞µ‡∞Ç‡∞§‡∞Ç.

##### ‡∞°‡±à‡∞®‡∞Æ‡∞ø‡∞ï‡±ç ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç[[dynamic-padding]]

<Youtube id="7q5NyFT8REg"/>

‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞í‡∞ï‡±á ‡∞™‡±ä‡∞°‡∞µ‡±Å‡∞ï‡±Å ‡∞§‡±Ü‡∞ö‡±ç‡∞ö‡±á ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞®‡±Å **collate function** ‡∞Ö‡∞Ç‡∞ü‡∞æ‡∞∞‡±Å. ‡∞π‡∞ó‡±ç‡∞ó‡∞ø‡∞Ç‡∞ó‡±ç ‡∞´‡±á‡∞∏‡±ç ‡∞¶‡±Ä‡∞®‡∞ø‡∞ï‡±ã‡∞∏‡∞Ç ‡∞∞‡±Ü‡∞°‡±Ä‡∞Æ‡±á‡∞°‡±ç ‡∞ü‡±Ç‡∞≤‡±ç ‡∞á‡∞ö‡±ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø:

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£‡∞ï‡±Å 8 ‡∞é‡∞≤‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç‡∞∏‡±ç ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±á:

```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
# ‚Üí [50, 59, 47, 67, 59, 50, 62, 32]
```

`data_collator` ‡∞µ‡±Ä‡∞ü‡∞ø‡∞®‡∞ø 67 ‡∞™‡±ä‡∞°‡∞µ‡±Å‡∞ï‡±Å ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø (‡∞Ü ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡±ã‡∞®‡±á ‡∞Ö‡∞§‡∞ø ‡∞™‡±Ü‡∞¶‡±ç‡∞¶‡∞¶‡∞ø). ‡∞Æ‡±ä‡∞§‡±ç‡∞§‡∞Ç ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±ã ‡∞Ö‡∞§‡∞ø ‡∞™‡±Ü‡∞¶‡±ç‡∞¶ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞¶‡±Å ‚Üí ‡∞∏‡∞Æ‡∞Ø‡∞Ç, ‡∞Æ‡±Ü‡∞Æ‡∞∞‡±Ä ‡∞Ü‡∞¶‡∞æ ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
# ‚Üí {'input_ids': torch.Size([8, 67]), 'attention_mask': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'labels': torch.Size([8])}
```

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡∞® ‡∞°‡±á‡∞ü‡∞æ ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø‡∞ó‡∞æ ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞ï‡±Å ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø ‚Äì ‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∞‡±Ü‡∞°‡±Ä!

> [!TIP]
> ‚úèÔ∏è **‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø!** GLUE‡∞≤‡±ã‡∞®‡∞ø SST-2 ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞™‡±à ‡∞á‡∞¶‡±á ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞∞‡∞ø‡∞™‡±Ä‡∞ü‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø (‡∞í‡∞ï‡±á ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞Ç ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø ‡∞ï‡∞æ‡∞¨‡∞ü‡±ç‡∞ü‡∞ø ‡∞ï‡±ä‡∞Ç‡∞ö‡±Ü‡∞Ç ‡∞°‡∞ø‡∞´‡∞∞‡±Ü‡∞Ç‡∞ü‡±ç). ‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§ ‡∞ï‡∞∑‡±ç‡∞ü‡∞Æ‡±à‡∞® ‡∞õ‡∞æ‡∞≤‡±Ü‡∞Ç‡∞ú‡±ç ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞Ç‡∞ü‡±á ‡∞è‡∞¶‡±à‡∞®‡∞æ GLUE ‡∞ü‡∞æ‡∞∏‡±ç‡∞ï‡±ç‚Äå‡∞ï‡∞ø ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±á ‡∞í‡∞ï‡±á ‡∞™‡±ç‡∞∞‡±Ä-‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç ‡∞´‡∞Ç‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞∞‡∞æ‡∞Ø‡∞Ç‡∞°‡∞ø.

## ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç ‡∞ï‡±ç‡∞µ‡∞ø‡∞ú‡±ç[[section-quiz]]

### 1. `Dataset.map()`‡∞≤‡±ã `batched=True` ‡∞µ‡∞æ‡∞°‡∞ø‡∞§‡±á ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞≤‡∞æ‡∞≠‡∞Ç ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?

<Question
choices={[
{text: "‡∞§‡∞ï‡±ç‡∞ï‡±Å‡∞µ ‡∞Æ‡±Ü‡∞Æ‡∞∞‡±Ä ‡∞µ‡∞æ‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞Ö‡∞¶‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞≤‡∞æ‡∞≠‡∞Æ‡±á ‡∞ï‡∞æ‡∞®‡±Ä ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞ï‡∞æ‡∞∞‡∞£‡∞Ç ‡∞ï‡∞æ‡∞¶‡±Å."},
{text: "‡∞í‡∞ï‡±á‡∞∏‡∞æ‡∞∞‡∞ø ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞é‡∞≤‡∞ø‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç‡∞∏‡±ç ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡±ç ‡∞Ö‡∞µ‡±Å‡∞§‡∞æ‡∞Ø‡∞ø ‚Üí ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞µ‡±á‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞∏‡∞∞‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç!", correct: true},
{text: "‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç‚Äå‡∞ó‡∞æ ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ï‡±ã‡∞∏‡∞Ç DataCollator ‡∞µ‡∞æ‡∞°‡∞§‡∞æ‡∞Ç."},
{text: "‡∞°‡±á‡∞ü‡∞æ‡∞®‡±Å PyTorch ‡∞ü‡±Ü‡∞®‡±ç‡∞∏‡∞∞‡±ç‡∞∏‡±ç‚Äå‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞∞‡±Å‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞Ö‡∞¶‡∞ø set_format()‡∞§‡±ã ‡∞ú‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø."}
]}
/>

### 2. ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞°‡±à‡∞®‡∞Æ‡∞ø‡∞ï‡±ç ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞µ‡∞æ‡∞°‡∞§‡∞æ‡∞Ç?

<Question
choices={[
{text: "‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞Ü‡∞∞‡±ç‡∞ï‡∞ø‡∞ü‡±Ü‡∞ï‡±ç‡∞ö‡∞∞‡±ç ‡∞¶‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞°‡∞ø‡∞Æ‡∞æ‡∞Ç‡∞°‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞≤‡±á‡∞¶‡±Å, ‡∞∞‡±Ü‡∞Ç‡∞°‡±Ç ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ø‡∞ø."},
{text: "‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞¨‡±ç‡∞Ø‡∞æ‡∞ö‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞ó‡∞∞‡∞ø‡∞∑‡±ç‡∞ü ‡∞™‡±ä‡∞°‡∞µ‡±Å‡∞ï‡±Å ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç ‚Üí ‡∞Ö‡∞®‡∞µ‡∞∏‡∞∞ ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç‡∞∏‡±ç ‡∞Æ‡±Ä‡∞¶ ‡∞ï‡∞Ç‡∞™‡±ç‡∞Ø‡±Ç‡∞ü‡±á‡∞∑‡∞®‡±ç ‡∞µ‡±É‡∞•‡∞æ ‡∞ï‡∞æ‡∞¶‡±Å.", explain: "‡∞∏‡∞∞‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç!", correct: true},
{text: "‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‡∞µ‡∞≤‡±ç‡∞≤ ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±ç‡∞∞‡∞≠‡∞æ‡∞µ‡∞ø‡∞§‡∞Ç ‡∞ï‡∞æ‡∞¶‡±Å."},
{text: "DataCollatorWithPadding ‡∞¶‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞¨‡∞≤‡∞µ‡∞Ç‡∞§‡∞Ç‡∞ó‡∞æ ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞Ö‡∞¶‡∞ø ‡∞é‡∞Ç‡∞™‡∞ø‡∞ï ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á."}
]}
/>

### 3. BERT ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã `token_type_ids` ‡∞è‡∞Æ‡∞ø ‡∞∏‡±Ç‡∞ö‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{text: "‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç ‡∞∏‡±ç‡∞•‡∞æ‡∞®‡∞Ç.", explain: "‡∞Ö‡∞¶‡∞ø ‡∞™‡±ä‡∞ú‡∞ø‡∞∑‡∞®‡±ç ‡∞é‡∞Ç‡∞¨‡±Ü‡∞°‡±ç‡∞°‡∞ø‡∞Ç‡∞ó‡±ç‡∞∏‡±ç."},
{text: "‡∞è ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç ‡∞Æ‡±ä‡∞¶‡∞ü‡∞ø ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞¶‡∞ø, ‡∞è‡∞¶‡∞ø ‡∞∞‡±Ü‡∞Ç‡∞°‡∞µ ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞®‡∞ø‡∞¶‡∞ø.", explain: "‡∞∏‡∞∞‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç!", correct: true},
{text: "‡∞è ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç‚Äå‡∞®‡±Å ‡∞ó‡∞Æ‡∞®‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞ø ‡∞Ö‡∞®‡±á ‡∞Æ‡∞æ‡∞∏‡±ç‡∞ï‡±ç.", explain: "‡∞Ö‡∞¶‡∞ø attention_mask."},
{text: "‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞µ‡±ã‡∞ï‡∞æ‡∞¨‡±ç ID.", explain: "‡∞Ö‡∞¶‡∞ø input_ids."}
]}
/>

### 4. `load_dataset('glue', 'mrpc')`‡∞≤‡±ã ‡∞∞‡±Ü‡∞Ç‡∞°‡∞µ ‡∞Ü‡∞∞‡±ç‡∞ó‡±ç‡∞Ø‡±Å‡∞Æ‡±Ü‡∞Ç‡∞ü‡±ç ‡∞è‡∞Æ‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø?

<Question
choices={[
{text: "‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç ‡∞µ‡±Ü‡∞∞‡±ç‡∞∑‡∞®‡±ç.", explain: "‡∞µ‡±á‡∞∞‡±á ‡∞™‡∞æ‡∞∞‡∞æ‡∞Æ‡±Ä‡∞ü‡∞∞‡±ç‚Äå‡∞§‡±ã ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç."},
{text: "GLUE ‡∞¨‡±Ü‡∞Ç‡∞ö‡±ç‚Äå‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï ‡∞ü‡∞æ‡∞∏‡±ç‡∞ï‡±ç (MRPC).", explain: "‡∞∏‡∞∞‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç!", correct: true},
{text: "train/validation/test ‡∞∏‡±ç‡∞™‡±ç‡∞≤‡∞ø‡∞ü‡±ç.", explain: "‡∞≤‡±ã‡∞°‡±ç ‡∞Ö‡∞Ø‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ ‡∞Ø‡∞æ‡∞ï‡±ç‡∞∏‡±Ü‡∞∏‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç."},
{text: "‡∞°‡±á‡∞ü‡∞æ ‡∞´‡∞æ‡∞∞‡±ç‡∞Æ‡∞æ‡∞ü‡±ç.", explain: "set_format()‡∞§‡±ã ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Ç."}
]}
/>

### 5. ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£‡∞ï‡±Å ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å `sentence1`, `sentence2` ‡∞ï‡∞æ‡∞≤‡∞Æ‡±ç‚Äå‡∞≤‡±Å ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞§‡±ä‡∞≤‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞Ç?

<Question
choices={[
{text: "‡∞Æ‡±Ü‡∞Æ‡∞∞‡±Ä ‡∞Ü‡∞¶‡∞æ ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞ï‡±ä‡∞Ç‡∞ö‡±Ü‡∞Ç ‡∞Ü‡∞¶‡∞æ ‡∞Ö‡∞µ‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‡∞ï‡∞æ‡∞®‡±Ä ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞ï‡∞æ‡∞∞‡∞£‡∞Ç ‡∞ï‡∞æ‡∞¶‡±Å."},
{text: "‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞∞‡∞æ ‡∞ü‡±Ü‡∞ï‡±ç‡∞∏‡±ç‡∞ü‡±ç ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±ã‡∞≤‡±á‡∞¶‡±Å ‚Üí ‡∞≤‡±á‡∞ï‡∞™‡±ã‡∞§‡±á ‡∞é‡∞∞‡±ç‡∞∞‡∞∞‡±ç ‡∞µ‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞∏‡∞∞‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç!", correct: true},
{text: "‡∞µ‡∞æ‡∞≤‡±ç‡∞Ø‡±Å‡∞Ø‡±á‡∞∑‡∞®‡±ç‚Äå‡∞ï‡∞ø ‡∞Ö‡∞µ‡∞∏‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å.", explain: "‡∞Ö‡∞¶‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞®‡∞ø‡∞ú‡∞Æ‡±á ‡∞ï‡∞æ‡∞®‡±Ä ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞ï‡∞æ‡∞∞‡∞£‡∞Ç ‡∞ï‡∞æ‡∞¶‡±Å."},
{text: "‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞µ‡±á‡∞ó‡∞Ç ‡∞¨‡∞æ‡∞ó‡∞æ ‡∞™‡±Ü‡∞∞‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.", explain: "‡∞µ‡±á‡∞ó‡∞Ç‡∞≤‡±ã ‡∞™‡±Ü‡∞¶‡±ç‡∞¶ ‡∞§‡±á‡∞°‡∞æ ‡∞â‡∞Ç‡∞°‡∞¶‡±Å."}
]}
/>

> [!TIP]
> **‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø ‡∞™‡∞æ‡∞Ø‡∞ø‡∞Ç‡∞ü‡±ç‡∞≤‡±Å**:
>
> * `batched=True` ‡∞§‡±ã `map()` ‚Üí ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞µ‡±á‡∞ó‡∞Æ‡±à‡∞® ‡∞™‡±ç‡∞∞‡±Ä-‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç
> * `DataCollatorWithPadding` ‡∞§‡±ã ‡∞°‡±à‡∞®‡∞Æ‡∞ø‡∞ï‡±ç ‡∞™‡±ç‡∞Ø‡∞æ‡∞°‡∞ø‡∞Ç‡∞ó‡±ç ‚Üí ‡∞é‡∞ï‡±ç‡∞ï‡±Å‡∞µ ‡∞∏‡∞Æ‡∞∞‡±ç‡∞•‡∞µ‡∞Ç‡∞§‡∞Ç
> * ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞è‡∞Æ‡∞ø ‡∞Ü‡∞∂‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡±ã ‡∞¶‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞§‡∞ó‡±ç‡∞ó‡∞ü‡±ç‡∞ü‡±Å ‡∞°‡±á‡∞ü‡∞æ ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø
> * ü§ó Datasets ‡∞≤‡±à‡∞¨‡±ç‡∞∞‡∞∞‡±Ä ‡∞∏‡±ç‡∞ï‡±á‡∞≤‡±ç‚Äå‡∞≤‡±ã ‡∞é‡∞´‡∞ø‡∞∑‡∞ø‡∞Ø‡±Ü‡∞Ç‡∞ü‡±ç ‡∞°‡±á‡∞ü‡∞æ ‡∞™‡±ç‡∞∞‡∞æ‡∞∏‡±Ü‡∞∏‡∞ø‡∞Ç‡∞ó‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø

‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞°‡±á‡∞ü‡∞æ ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞ø‡∞ó‡∞æ ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç! ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§ ‡∞∏‡±Ü‡∞ï‡±ç‡∞∑‡∞®‡±ç‚Äå‡∞≤‡±ã `Trainer` API‡∞§‡±ã ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞´‡±à‡∞®‡±ç-‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡±ç ‡∞ö‡±á‡∞¶‡±ç‡∞¶‡∞æ‡∞Ç!
