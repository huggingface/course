<!-- DISABLE-FRONTMATTER-SECTIONS -->

# End-of-chapter quiz

Bu bÃ¶lÃ¼mde Ã§ok fazla ÅŸey iÅŸledik! EÄŸer bÃ¼tÃ¼n detaylar oturmadÄ±ysa endiÅŸelenme; Ã¶nÃ¼mÃ¼zdeki bÃ¶lÃ¼mlerde bu konularÄ± daha derinlemesine iÅŸleyeceÄŸiz. 

Åimdilik, bu bÃ¶lÃ¼mde Ã¶ÄŸrendiklerini test edelim!


### 1. Hub'a girin ve `roberta-large-mnli` checkpointini arayÄ±n. Hangi task iÃ§in eÄŸitilmiÅŸ bir modeldir?


<Question
	choices={[
		{
			text: "Ã–zetleme",
			explain: "<a href=\"https://huggingface.co/roberta-large-mnli\">roberta-large-mnli sayfasÄ±nÄ±</a> tekrar kontrol et!"
		},
		{
			text: "Metin SÄ±nÄ±flandÄ±rma",
			explain: "Daha spesifik olarak iki cÃ¼mlenin mantÄ±ksal olarak baÄŸlÄ±lÄ±ÄŸÄ±nÄ± 3 label'da (karÅŸÄ± Ã§Ä±kma, nÃ¶tr, destekleme) sÄ±nÄ±flandÄ±rÄ±yor. â€” buna aynÄ± zamanda <em>doÄŸal dil Ã§Ä±karÄ±mÄ±</em> da denir.",
			correct: true
		},
		{
			text: "Metin Ã¼retme",
			explain: "<a href=\"https://huggingface.co/roberta-large-mnli\">roberta-large-mnli sayfasÄ±nÄ±</a> tekrar kontrol et!"
		}
	]}
/>

### 2. AÅŸaÄŸÄ±daki kod ne dÃ¶ndÃ¼rÃ¼r?

```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

<Question
	choices={[
		{
			text: "Bu cÃ¼mledeki sÄ±nÄ±flandÄ±rma puanlarÄ±nÄ± \"positive\" veya \"negative\" labellarÄ± ile dÃ¶ndÃ¼recek.",
			explain: "CevabÄ±nÄ±z yanlÄ±ÅŸ â€” bu <code>sentiment-analysis</code> pipeline'Ä± olurdu."
		},
		{
			text: "CÃ¼mleyi tamamlayan metni Ã¼retip dÃ¶ndÃ¼rÃ¼r.",
			explain: "CevabÄ±nÄ±z yanlÄ±ÅŸ â€” bu <code>text-generation</code> pipeline'Ä± olurdu.",
		},
		{
			text: "KiÅŸileri, kurum-kuruluÅŸlarÄ± veya mekanlarÄ± niteleyen kelimeleri dÃ¶ndÃ¼rÃ¼r.",
			explain: "DahasÄ±, <code>grouped_entities=True</code> yazarsanÄ±z aynÄ± varlÄ±ÄŸa ait kelimeleri gruplar. Ã–rnek: \"Hugging Face\".",
			correct: true
		}
	]}
/>

### 3. ...'yÄ± doldurun.

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

<Question
	choices={[
		{
			text: "This <mask> has been waiting for you.",
			explain: "CevabÄ±nÄ±z yanlÄ±ÅŸ. <code>bert-base-cased</code> model kartÄ±na bakÄ±n ve hatanÄ±zÄ± anlamaya Ã§alÄ±ÅŸÄ±n."
		},
		{
			text: "This [MASK] has been waiting for you.",
			explain: "DoÄŸru! Bu modelin mask token'Ä± [MASK].",
			correct: true
		},
		{
			text: "This man has been waiting for you.",
			explain: "CevabÄ±nÄ±z yanlÄ±ÅŸ. Bu pipeline masked word'leri (silinmiÅŸ kelimeleri) dolduruyor, bu yÃ¼zden cÃ¼mlenin bir yerinde mask token olmasÄ± gerekiyor."
		}
	]}
/>

### 4. Bu kod neden Ã§alÄ±ÅŸmaz?

```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

<Question
	choices={[
		{
			text: "Bu pipeline'Ä±n metni sÄ±nÄ±flandÄ±rmasÄ± iÃ§in labellarÄ±n verilmesi gerekiyor.",
			explain: "DoÄŸru â€” doÄŸru kodda <code>candidate_labels=[...]</code> yazÄ±lmasÄ± gerekiyor.",
			correct: true
		},
		{
			text: "Bu pipeline birden fazla cÃ¼mle gerektirir.",
			explain: "CevabÄ±nÄ±z yanlÄ±ÅŸ, bu pipeline birden fazla cÃ¼mle alabilir (diÄŸer hepsi gibi) fakat sorun bu deÄŸil."
		},
		{
			text: "ğŸ¤— Transformers kÃ¼tÃ¼phanesi saÃ§malÄ±yor (her zamanki gibi).",
			explain: "Bunu iÅŸaretlediyseniz size hiÃ§bir ÅŸey demiyoruz!"
		},
		{
			text: "This pipeline requires longer inputs; this one is too short.",
			explain: "CevabÄ±nÄ±z yanlÄ±ÅŸ. Sorun bu deÄŸil ama unutmayÄ±n ki Ã§ok uzun bir metin bu pipeline tarafÄ±ndan iÅŸlenirken kÄ±saltÄ±lÄ±r."
		}
	]}
/>

### 5. "transfer learning" ne anlama gelir?

<Question
	choices={[
		{
			text: "Daha Ã¶nceden eÄŸitilmiÅŸ bir modeli aynÄ± verisetiyle tekrar eÄŸiterek bilgisini yeni modele geÃ§irmek.",
			explain: "HayÄ±r eÄŸer bunu yaparsak aynÄ± modelden 2 tane elde ederdik."
		},
		{
			text: "Ä°kinci bir modeli ilk modelin weightleri ile baÅŸlatÄ±p ilk modelin bilgisini yeni modele geÃ§irmek.",
			explain: "DoÄŸru: ikinci modeli yeni bir iÅŸ iÃ§in eÄŸittiÄŸimizde birinci modelin bilgisini *transfers* (geÃ§irir).",
			correct: true
		},
		{
			text: "Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model.",
			explain: "The architecture is just the way the model is built; there is no knowledge shared or transferred in this case."
		}
	]}
/>

### 6. True or false? A language model usually does not need labels for its pretraining.


<Question
	choices={[
		{
			text: "True",
			explain: "The pretraining is usually <em>self-supervised</em>, which means the labels are created automatically from the inputs (like predicting the next word or filling in some masked words).",
			correct: true
		},
		{
			text: "False",
			explain: "This is not the correct answer."
		}
	]}
/>

### 7. "model," "architecture," (mimari) ve "weights." (aÄŸÄ±rlÄ±klar) kelimeleri arasÄ±ndaki fark nedir?

<Question
	choices={[
		{
			text: "EÄŸer model bir binaysa mimarisi bina planÄ± ve aÄŸÄ±rlÄ±klar iÃ§inde yaÅŸayan insanlar olurdu.",
			explain: "Bu metafora gÃ¶re, aÄŸÄ±rlÄ±klar, tuÄŸlalar ve binayÄ± yapmak iÃ§in kullanÄ±lan diÄŸer malzemeler olurdu."
		},
		{
			text: "Mimari bir model yapmak iÃ§in harita olsaydÄ± aÄŸÄ±rlÄ±klarÄ± haritanÄ±n Ã¼zerindeki ÅŸehirler olurdu.",
			explain: "Bu metafora gÃ¶re bir mimari iÃ§in birden fazla aÄŸÄ±rlÄ±k mÃ¼mkÃ¼n olmazdÄ±. (TÃ¼rkiye haritasÄ±nda sadece bir tane Ä°stanbul var.)"
		},
		{
			text: "Bir mimari bir modeli ve aÄŸÄ±rlÄ±klarÄ± oluÅŸturmak iÃ§in kullanÄ±lan matematiksel fonksiyonlarÄ±n bir dizisidir ve aÄŸÄ±rlÄ±klar bu fonksiyonlarÄ±n parametreleridir.",
			explain: "AynÄ± matematiksel fonksiyonlar (mimari) farklÄ± parametreler (aÄŸÄ±rlÄ±klar) kullanÄ±larak farklÄ± modeller oluÅŸturmak iÃ§in kullanÄ±labilir.",
			correct: true
		}
	]}
/>


### 8. Hangi model tÃ¼rÃ¼nÃ¼ promptlarÄ± metin Ã¼reterek tamamlamak iÃ§in kullanÄ±rsÄ±nÄ±z?

<Question
	choices={[
		{
			text: "Encoder (kodlayÄ±cÄ±) model",
			explain: "Encoder modeller bÃ¼tÃ¼n cÃ¼mleyi temsil eden bir gÃ¶sterim oluÅŸturur ve bu gÃ¶sterim sÄ±nÄ±flandÄ±rma gibi iÅŸler iÃ§in daha uygundur."
		},
		{
			text: "Decoder (kod Ã§Ã¶zÃ¼cÃ¼) model",
			explain: "Decoder modeller bu iÅŸ iÃ§in mÃ¼kemmeldir.",
			correct: true
		},
		{
			text: "Sequence-to-sequence model",
			explain: "Sequence-to-sequence models are better suited for tasks where you want to generate sentences in relation to the input sentences, not a given prompt. Sequences-to-sequence modeller input cÃ¼mlelerle iliÅŸkili cÃ¼mleler Ã¼retmek iÃ§in daha uygundur, verilen bir prompt iÃ§in deÄŸil."
		}
	]}
/>

### 9. Ã–zetleme iÃ§in hangi model tÃ¼rÃ¼nÃ¼ kullanÄ±rsÄ±nÄ±z?

<Question
	choices={[
		{
			text: "Encoder model",
			explain: "Encoder modeller bÃ¼tÃ¼n cÃ¼mleyi temsil eden bir gÃ¶sterim oluÅŸturur ve bu gÃ¶sterim sÄ±nÄ±flandÄ±rma gibi iÅŸler iÃ§in daha uygundur."
		},
		{
			text: "Decoder model",
			explain: "Decoder modeller Ã§Ä±ktÄ± metinleri (Ã¶zetler gibi) Ã¼retmek iÃ§in iyidir, fakat bÃ¼tÃ¼n metni Ã¶zetlemek iÃ§in gerekli olan baÄŸlamÄ± kullanma yeteneÄŸine sahip deÄŸillerdir."
		},
		{
			text: "Sequence-to-sequence model",
			explain: "Sequence-to-sequence modeller Ã¶zetleme iÅŸi iÃ§in mÃ¼kemmeldir.",
			correct: true
		}
	]}
/>

### 10. Hangi model tÃ¼rÃ¼nÃ¼ metin inputlarÄ±nÄ± belirli labellarÄ±na gÃ¶re sÄ±nÄ±flandÄ±rmak iÃ§in kullanÄ±rsÄ±nÄ±z?

<Question
	choices={[
		{
			text: "Encoder model",
			explain: "Encoder modeller bÃ¼tÃ¼n cÃ¼mleyi temsil eden bir gÃ¶sterim oluÅŸturur ve bu gÃ¶sterim sÄ±nÄ±flandÄ±rma gibi iÅŸler iÃ§in mÃ¼kemmeldir.",
			correct: true
		},
		{
			text: "Decoder model",
			explain: "Decoder models are good for generating output texts, not extracting a label out of a sentence. Decoder modeller Ã§Ä±ktÄ± metinleri Ã¼retmek iÃ§in iyidir falakat bir cÃ¼mleden label Ã§Ä±karamazlar."
		},
		{
			text: "Sequence-to-sequence model",
			explain: "Sequence-to-sequence modeller input cÃ¼mlelerle iliÅŸkili cÃ¼mleler Ã¼retmek iÃ§in daha uygundur, label Ã§Ä±karamazlar.",
		}
	]}
/>

### 11. Modelinizde bir Ã¶nyargÄ± gÃ¶zlemlediÄŸinizde bunun kaynaÄŸÄ± ne olabilir?

<Question
	choices={[
		{
			text: "Fine-tune yaptÄ±ÄŸÄ±nÄ±zda ilk modelin Ã¶nyargÄ±sÄ± fine-tuned modele geÃ§miÅŸ olabilir.",
			explain: "Transfer Learning yaparken, kullanÄ±lan Ã¶nceden eÄŸitilmiÅŸ modelin Ã¶nyargÄ±sÄ± fine-tuned modelde de silinmez.",
			correct: true
		},
		{
			text: "Modelin eÄŸitildiÄŸi veriseti Ã¶nyargÄ±lÄ± olabilir.",
			explain: "En basit Ã¶nyargÄ± kaynaÄŸÄ± bu fakat tek deÄŸil.",
			correct: true
		},
		{
			text: "The metric the model was optimizing for is biased. Modelin optimize ettiÄŸi metrik Ã¶nyargÄ±lÄ± olabilir.",
			explain: "GÃ¶rmesi daha zor bir Ã¶nyargÄ± kaynaÄŸÄ± modelin eÄŸitildiÄŸi metriktir. Modeliniz seÃ§tiÄŸiniz metriÄŸe kÃ¶rÃ¼ kÃ¶rÃ¼ne odaklanÄ±r."",
			correct: true
		}
	]}
/>
