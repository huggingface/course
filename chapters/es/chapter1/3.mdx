# Transformadores, ¬øqu√© pueden hacer?

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/es/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/es/chapter1/section3.ipynb"},
]} />

En esta secci√≥n, veremos qu√© pueden hacer los Transformadores y usaremos nuestra primera herramienta de la librer√≠a ü§ó Transformers: la funci√≥n `pipeline()`.

<Tip>

üëÄ Ves el bot√≥n <em>Open in Colab</em> en la parte superior derecha? Haz clic en √©l para abrir un cuaderno de Google Colab con todos los ejemplos de c√≥digo de esta secci√≥n. Este bot√≥n aparecer√° en cualquier secci√≥n que tenga ejemplos de c√≥digo.

Si quieres ejecutar los ejemplos localmente, te recomendamos revisar la <a href="/course/chapter0">configuraci√≥n</a>.

</Tip>

## ¬°Los Transformadores est√°n en todas partes!

Los Transformadores se usan para resolver todo tipo de tareas de PLN, como las mencionadas en la secci√≥n anterior. Aqu√≠ te mostramos algunas de las compa√±√≠as y organizaciones que usan Hugging Face y Transformadores, que tambi√©n contribuyen de vuelta a la comunidad al compartir sus modelos:

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="Companies using Hugging Face" width="100%">

La [librer√≠a ü§ó Transformers](https://github.com/huggingface/transformers) provee la funcionalidad de crear y usar estos modelos compartidos. El [Hub de Modelos](https://huggingface.co/models) contiene miles de modelos preentrenados que cualquiera puede descargar y usar. ¬°T√∫ tambi√©n puedes subir tus propios modelos al Hub!

<Tip>

‚ö†Ô∏è El Hub de Hugging Face no se limita a Transformadores. ¬°Cualquiera puede compartir los tipos de modelos o conjuntos de datos que quiera! ¬°<a href="https://huggingface.co/join">Crea una cuenta de huggingface.co</a> para beneficiarte de todas las funciones disponibles!

</Tip>

Antes de ver c√≥mo funcionan internamente los Transformadores, veamos un par de ejemplos sobre c√≥mo pueden ser usados para resolver tareas de PLN. 

## Trabajando con pipelines

<Youtube id="tiZFewofSLM" />

El objeto m√°s b√°sico en la librer√≠a ü§ó Transformers es la funci√≥n `pipeline()`. Esta funci√≥n conecta un modelo con los pasos necesarios para su preprocesamiento y posprocesamiento, permiti√©ndonos introducir de manera directa cualquier texto y obtener una respuesta inteligible:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

¬°Incluso podemos pasar varias oraciones!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Por defecto, este pipeline selecciona un modelo particular preentrenado que ha sido ajustado para el an√°lisis de sentimientos en Ingl√©s. El modelo se descarga y se almacena en el cach√© cuando creas el objeto `classifier`. Si vuelves a ejecutar el comando, se usar√° el modelo almacenado en cach√© y no habr√° necesidad de descargarlo de nuevo.

Hay tres pasos principales que ocurren cuando pasas un texto a un pipeline:

1. El texto es preprocesado en un formato que el modelo puede entender.
2. La entrada preprocesada se pasa al modelo.
3. Las predicciones del modelo son posprocesadas, de tal manera que las puedas entender.

Algunos de los [pipelines disponibles](https://huggingface.co/transformers/main_classes/pipelines.html) son:

- `feature-extraction` (obtener la representaci√≥n vectorial de un texto)
- `fill-mask`
- `ner` (reconocimiento de entidades nombradas)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classification`

¬°Veamos algunas de ellas!

## Clasificaci√≥n *zero-shot*

Empezaremos abordando una tarea m√°s compleja, en la que necesitamos clasificar textos que no han sido etiquetados. Este es un escenario com√∫n en proyectos de la vida real porque anotar texto usualmente requiere mucho tiempo y dominio del tema. Para este caso de uso, el pipeline `zero-shot-classification` es muy poderoso: permite que especifiques qu√© etiquetas usar para la clasificaci√≥n, para que no dependas de las etiquetas del modelo preentrenado. Ya viste c√≥mo el modelo puede clasificar una oraci√≥n como positiva o negativa usando esas dos etiquetas ‚Äî pero tambi√©n puede clasificar el texto usando cualquier otro conjunto de etiquetas que definas.

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

Este pipeline se llama _zero-shot_ porque no necesitas ajustar el modelo con tus datos para usarlo. ¬°Puede devolver directamente puntajes de probabilidad para cualquier lista de de etiquetas que escojas!

<Tip>

‚úèÔ∏è **¬°Pru√©balo!** Juega con tus propias secuencias y etiquetas, y observa c√≥mo se comporta el modelo.

</Tip>


## Generaci√≥n de texto

Ahora veamos c√≥mo usar un pipeline para generar texto. La idea es que proporciones una indicaci√≥n (*prompt*) y el modelo la va a completar autom√°ticamente al generar el texto restante. Esto es parecido a la funci√≥n de texto predictivo que est√° presente en muchos tel√©fonos. La generaci√≥n de texto involucra aleat√≥riedad, por lo que es normal que no obtengas el mismo resultado que se muestra abajo.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows ‚Äî data flows of various types, as seen by the '
                    'HTTP'}]
```

Puedes controlar cu√°ntas secuencias diferentes se generan con el argumento `num_return_sequences` y la longitud total del texto de salida con el argumento `max_length`.

<Tip>

‚úèÔ∏è **¬°Pru√©balo!** Usa los argumentos `num_return_sequences` y `max_length` para generar dos oraciones de 15 palabras cada una.

</Tip>


## Usa cualquier modelo del Hub en un pipeline

Los ejemplos anteriores usaban el modelo por defecto para cada tarea, pero tambi√©n puedes escoger un modelo particular del Hub y usarlo en un pipeline para una tarea espec√≠fica - por ejemplo, la generaci√≥n de texto. Ve al [Hub de Modelos](https://huggingface.co/models) y haz clic en la etiqueta correspondiente en la parte izquierda para mostrar √∫nicamente los modelos soportados para esa tarea. Deber√≠as ver una p√°gina [como esta](https://huggingface.co/models?pipeline_tag=text-generation).

¬°Intentemos con el modelo [`distilgpt2`](https://huggingface.co/distilgpt2)! Puedes cargarlo en el mismo pipeline de la siguiente manera:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```

```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

Puedes refinar tu b√∫squeda de un modelo haciendo clic en las etiquetas de idioma y escoger uno que genere textos en otro idioma. El Hub de Modelos tambi√©n contiene puntos de control (*checkpoints*) para modelos que soportan m√∫ltiples lenguajes.

Una vez has seleccionado un modelo haciendo clic en √©l, ver√°s que hay un widget que te permite probarlo directamente en l√≠nea. De esta manera puedes probar r√°pidamente las capacidades del modelo antes de descargarlo.

<Tip>

‚úèÔ∏è **¬°Pru√©balo!** Usa los filtros para encontrar un modelo de generaci√≥n de texto para un idioma diferente. ¬°Si√©ntete libre de jugar con el widget y √∫salo en un pipeline!

</Tip>


### La API de Inferencia

Todos los modelos pueden ser probados directamente en tu navegador usando la API de Inferencia, que est√° disponible en el [sitio web](https://huggingface.co/) de Hugging Face. Puedes jugar con el modelo directamente en esta p√°gina al pasar tu texto personalizado como entrada y ver c√≥mo lo procesa.

La API de Inferencia que hace funcionar al widget tambi√©n est√° disponible como un producto pago, algo √∫til si lo necesitas para tus flujos de trabajo. Dir√≠gete a la [p√°gina de precios](https://huggingface.co/pricing) para m√°s detalles.

## Llenado de ocultos (*Mask filling*)

El siguiente pipeline con el que vas a trabajar es `fill-mask`. La idea de esta tarea es llenar los espacios en blanco de un texto dado:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

El argumento `top_k` controla el n√∫mero de posibilidades que se van a mostrar. Nota que en este caso el modelo llena la palabra especial `<mask>`, que se denomina com√∫nmente como *mask token*. Otros modelos pueden tener diferentes tokens, por lo que es una buena idea verificar la palabra especial adecuada cuando est√©s explorando diferentes modelos. Una manera de confirmar es revisar la palabra usada en el widget.

<Tip>

‚úèÔ∏è **¬°Pru√©balo!** Busca el modelo `bert-base-cased` en el Hub e identifica su *mask token* en el widget de la API de Inferencia. ¬øQu√© predice este modelo para la oraci√≥n que est√° en el ejemplo de `pipeline` anterior?

</Tip>

## Reconocimiento de entidades nombradas

El reconocimiento de entidades nombradas (REN) es una tarea en la que el modelo tiene que encontrar cu√°les partes del texto introducido corresponden a entidades como personas, ubicaciones u organizaciones. Veamos un ejemplo:

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

En este caso el modelo identific√≥ correctamente que Sylvain es una persona (PER), Hugging Face una organizaci√≥n (ORG) y Brooklyn una ubicaci√≥n (LOC).

Pasamos la opci√≥n `grouped_entities=True` en la funci√≥n de creaci√≥n del pipeline para decirle que agrupe las partes de la oraci√≥n que corresponden a la misma entidad: Aqu√≠ el modelo agrup√≥ correctamente "Hugging" y "Face" como una sola organizaci√≥n, a pesar de que su nombre est√° compuesto de varias palabras. De hecho, como veremos en el siguente cap√≠tulo, el preprocesamiento puede incluso dividir palabras en partes m√°s peque√±as. Por ejemplo, 'Sylvain' se separa en cuatro piezas: `S`, `##yl`, `##va` y`##in`. En el paso de prosprocesamiento, el pipeline reagrupa de manera exitosa dichas piezas.

<Tip>

‚úèÔ∏è **¬°Pru√©balo!** Busca en el Model Hub un modelo capaz de hacer etiquetado *part-of-speech* (que se abrevia usualmente como POS) en Ingl√©s. ¬øQu√© predice este modelo para la oraci√≥n en el ejemplo de arriba?

</Tip>

## Responder preguntas

El pipeline `question-answering` responde preguntas usando informaci√≥n de un contexto dado:

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

Observa que este pipeline funciona extrayendo informaci√≥n del contexto ofrecido; m√°s no genera la respuesta.

## Resumir (*Summarization*)

Resumir es la tarea de reducir un texto en uno m√°s corto, conservando todos (o la mayor parte de) los aspectos importantes mencionados. Aqu√≠ va un ejemplo: 

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```

```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

Similar a la generaci√≥n de textos, puedes especificar los argumentos `max-length` o `min_length` para definir la longitud del resultado.


## Traducci√≥n

Para la traducci√≥n, puedes usar el modelo por defecto si indicas una pareja de idiomas en el nombre de la tarea (como `"translation_en_to_fr"`), pero la forma m√°s sencilla es escoger el modelo que quieres usar en el [Hub de Modelos](https://huggingface.co/models). Aqu√≠ intentaremos traducir de Franc√©s a Ingl√©s:

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

Al igual que los pipelines de generaci√≥n de textos y resumen, puedes especificar una longitud m√°xima (`max_length`) o m√≠nima (`min_length`) para el resultado.

<Tip>

‚úèÔ∏è **¬°Pru√©balo!** Busca modelos de traducci√≥n en otros idiomas e intenta traducir la oraci√≥n anterior en varios de ellos.

</Tip>

Los pipelines vistos hasta el momento son principalmente para fines demostrativos. Fueron programados para tareas espec√≠ficas y no pueden desarrollar variaciones de ellas. En el siguiente cap√≠tulo, aprender√°s qu√© est√° detr√°s de una funci√≥n `pipeline()` y c√≥mo personalizar su comportamiento.