# Resumen

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

En este capítulo viste cómo abordar diferentes tareas de PLN usando la función de alto nivel `pipeline()` de 🤗 Transformers. También viste como buscar modelos en el Hub, así como usar la API de Inferencia para probar los modelos directamente en tu navegador.

Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o sólo el codificador o decodificador, dependiendo de qué tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:

| Modelo                    | Ejemplos                                   | Tareas                                                                                              |
|---------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------------|
| Codificador               | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificación de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas |
| Decodificador             | CTRL, GPT, GPT-2, Transformer XL           | Generación de texto                                                                                 |
| Codificador-decodificador | BART, T5, Marian, mBART                    | Resumen, traducción, respuesta generativa a preguntas                                               |
