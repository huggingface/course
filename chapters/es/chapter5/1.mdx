# IntroducciÃ³n

<CourseFloatingBanner
    chapter={5}
    classNames="absolute z-10 right-0 top-0"
/>

En el [CapÃ­tulo 3](/course/chapter3) tuviste tu primer acercamento a la librerÃ­a ğŸ¤— Datasets y viste que existÃ­an 3 pasos principales para ajustar un modelo:

1. Cargar un conjunto de datos del Hub de Hugging Face.
2. Preprocesar los datos con `Dataset.map()`.
3. Cargar y calcular mÃ©tricas.

Â¡Esto es apenas el principio de lo que ğŸ¤— Datasets puede hacer! En este capÃ­tulo vamos a estudiar a profundidad esta librerÃ­a y responderemos las siguientes preguntas:

* Â¿QuÃ© hacer cuando tu dataset no estÃ¡ en el Hub?
* Â¿CÃ³mo puedes subdividir tu dataset? (Â¿Y quÃ© hacer si _realmente_ necesitas usar Pandas?)
* Â¿QuÃ© hacer cuando tu dataset es enorme y consume toda la RAM de tu computador?
* Â¿QuÃ© es la proyecciÃ³n en memoria (_memory mapping_) y Apache Arrow?
* Â¿CÃ³mo puedes crear tu propio dataset y subirlo al Hub?

Las tÃ©cnicas que aprenderÃ¡s aquÃ­ te van a preparar para las tareas de _tokenizaciÃ³n_ avanzada y ajuste que verÃ¡s en el [CapÃ­tulo 6](/course/chapter6) y el [CapÃ­tulo 7](/course/chapter7). Â¡AsÃ­ que ve por un cafÃ© y arranquemos!