# IntroducciÃ³n

<CourseFloatingBanner
    chapter={5}
    classNames="absolute z-10 right-0 top-0"
/>

En el [CapÃ­tulo 3](/course/chapter3) tuviste tu primer acercamento a la librerÃ­a ğŸ¤— Datasets y viste que existÃ­an 3 pasos principales para ajustar un modelo:

1. Cargar un conjunto de datos del Hib de Hugging Face.
2. Preprocesar los datos con `Dataset.map()`.
3. Cargar y calcular mÃ©tricas.

Â¡Y esto es apenas el principio de los que ğŸ¤— Datasets puede hacer! En este capÃ­tulo vamos a estudiar a profundidad la librerÃ­a. En el camino, responderemos las siguientes preguntas:

* Â¿QuÃ© hacer cuando tu dataset no estÃ¡ en el Hub?
* Â¿CÃ³mo puedes subdividir tu dataset? (Â¿Y quÃ© hacer si _realmente_ necesitas usar Pandas?)
* Â¿QuÃ© hacer cuando tu dataset es enorme y consume toda la RAM de tu computador?
* Â¿QuÃ© rayos es "memory mapping" y Apache Arrow?
* Â¿CÃ³mo puedes crear tu propio dataset y subirlo al Hub?

Las tÃ©cnicas que aprenderÃ¡s aquÃ­ te van a preparar para las tareas de _tokenizaciÃ³n_ avanzada y ajuste (_fine-tuning_) en el [CapÃ­tulo 6](/course/chapter6) y el [CapÃ­tulo 7](/course/chapter7). Â¡AsÃ­ que ve por un cafÃ© y arranquemos!