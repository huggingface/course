# Es momento de subdividir

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section3.ipynb"},
]} />

La mayor parte del tiempo tus datos no estar√°n perfectamente listos para entrenar modelos. En esta secci√≥n vamos a explorar distintas funciones que tiene ü§ó Datasets para limpiar tus conjuntos de datos.

<Youtube id="tqfSFcPMgOI"/>

## Subdivdiendo nuestros datos

De manera similar a Pandas, ü§ó Datasets incluye varias funciones para manipular el contenido de los objetos `Dataset` y `DatasetDict`. Ya vimos el m√©todo `Dataset.map()` en el [Cap√≠tulo 3](/course/chapter3) y en esta secci√≥n vamos a explorar otras funciones que tenemos a nuestra disposici√≥n.

Para este ejemplo, vamos a usar el [Dataset de rese√±as de medicamentos](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) alojado en el [Repositorio de Machine Learning de UC Irvine](https://archive.ics.uci.edu/ml/index.php), que contiene la evaluaci√≥n de varios medicamentos por parte de pacientes, junto con la condici√≥n por la que los estaban tratando y una calificaci√≥n en una escala de 10 estrellas sobre su satisfacci√≥n.

Primero, tenemos que descargar y extraer los datos, que se puede hacer con los comandos `wget` y `unzip`:

```py
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

Dado que TSV es una variaci√≥n de CSV en la que se usan tabulaciones en vez de comas como separadores, podemos cargar estos archivos usando el script de carga `csv` y especificando el argumento `delimiter` en la funci√≥n `load_dataset` de la siguiente manera:

```py
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t es el caracter para tabulaciones en Python
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

Una buena pr√°ctica al hacer cualquier tipo de an√°lisis de datos es tomar una muestra aleatoria del dataset para tener una vista r√°pida del tipo de datos con los que est√°s trabajando. En ü§ó Datasets, podemos crear una muestra aleatoria al encadenar las funciones `Dataset.shuffle()` y `Dataset.select()`:

```py
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# Mirar los primeros ejemplos
drug_sample[:3]
```

```python out
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

Puedes ver que hemos fijado la semilla en `Dataset.shuffle()` por motivos de reproducibilidad. `Dataset.select()` espera un interable de √≠ndices, as√≠ que incluimos `range(1000)` para tomar los primeros 1.000 ejemplos del conjunto de datos aleatorizado. Ya podemos ver algunos detalles para esta muestra:

* La columna `Unnamed: 0` se ve sospechosamente como un ID anonimizado para cada paciente.
* La columna `condition` incluye una mezcla de niveles en may√∫scula y min√∫scula.
* Las rese√±as tienen longitud variable y contienen una mezcla de separadores de l√≠nea de Python (`\r\n`), as√≠ como caracteres de HTML como `&\#039;`.

Veamos c√≥mo podemos usar ü§ó Datasets para lidiar con cada uno de estos asuntos. Para probar la hip√≥tesis de que la columna `Unnamed: 0` es un ID de los pacientes, podemos usar la funci√≥n `Dataset.unique()` para verificar que el n√∫mero de los ID corresponda con el n√∫mero de filas de cada conjunto:

```py
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

Esto parece confirmar nuestra hip√≥tesis, as√≠ que limpiemos el dataset un poco al cambiar el nombre de la columna `Unnamed: 0` a algo m√°s legible. Podemos usar la funci√≥n `DatasetDict.rename_column()` para renombrar la columna en ambos conjuntos en una sola operaci√≥n:

```py
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<Tip>

‚úèÔ∏è **¬°Int√©ntalo!** Usa la funci√≥n `Dataset.unique()` para encontrar el n√∫mero de medicamentos y condiciones √∫nicas en los conjuntos de entrenamiento y de prueba.

</Tip>

Ahora normalicemos todas las etiquetas de `condition` usando `Dataset.map()`. Tal como lo hicimos con la tokenizaci√≥n en el [Cap√≠tulo 3](/course/chapter3), podemos definir una funci√≥n simple que pueda ser aplicada en todas las filas de cada conjunto en el `drug_dataset`:

```py
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}


drug_dataset.map(lowercase_condition)
```

```python out
AttributeError: 'NoneType' object has no attribute 'lower'
```

¬°Tenemos un problema en nuestra funci√≥n de mapeo! Del error podemos inferir que algunas de las entradas de la columna `condici√≥n` son `None`, que no puede transformarse en min√∫scula al no ser un string. Filtremos estas filas usando `Dataset.filter()`, que funciona de una forma similar `Dataset.map()` y recibe como argumento una funci√≥n que toma un ejemplo particular del dataset. En vez de escribir una funci√≥n expl√≠cita como:

```py
def filter_nones(x):
    return x["condition"] is not None
```

y luego ejecutar `drug_dataset.filter(filter_nones)`, podemos hacerlo en una l√≠nea usando una _funci√≥n lambda_. En Python, las funciones lambda son funciones peque√±as que puedes definir sin nombrarlas expl√≠citamente. Estas toman la forma general:

```
lambda <arguments> : <expression>
```

en la que `lambda` es una de las [palabras especiales](https://docs.python.org/3/reference/lexical_analysis.html#keywords) de Python, `<arguments>` es una lista o conjunto de valores separados con coma que definen los argumentos de la funci√≥n y `<expression>` representa las operaciones que quieres ejecutar. Por ejemplo, podemos definir una funci√≥n lambda simple que eleve un n√∫mero al cuadrado de la siguiente manera:

```
lambda x : x * x
```

Para aplicar esta funci√≥n a un _input_, tenemos que envolverla a ella y al _input_ en par√©ntesis:

```py
(lambda x: x * x)(3)
```

```python out
9
```

De manera similar, podemos definir funciones lambda con m√∫ltiples argumentos separ√°ndolos con comas. Por ejemplo, podemos calcular el √°rea de un tri√°ngulo as√≠:

```py
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python out
16.0
```

Las funciones lambda son √∫tiles cuando quieres definir funciones peque√±as de un √∫nico uso (para m√°s informaci√≥n sobre ellas, te recomendamos leer este excelente [tutorial de Real Python](https://realpython.com/python-lambda/) escrito por Andre Burgaud). En el contexto de ü§ó Datasets, podemos usar las funciones lambda para definir operaciones simples de mapeo y filtrado, as√≠ que usemos este truco para eliminar las entradas `None` de nuestro dataset:

```py
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

Ahora que eliminamos los `None`, podemos normalizar nuestra columna `condition`:

```py
drug_dataset = drug_dataset.map(lowercase_condition)
# Revisar que se pasaron a min√∫scula
drug_dataset["train"]["condition"][:3]
```

```python out
['left ventricular dysfunction', 'adhd', 'birth control']
```

¬°Funcion√≥! Como ya limpiamos las etiquetas, veamos c√≥mo podemos limpiar las rese√±as.

## Creando nuevas columnas

Cuando est√°s lidiando con rese√±as de clientes, es una buena pr√°ctica revisar el n√∫mero de palabras de cada rese√±a. Una rese√±a puede ser una √∫nica palabra como "¬°Genial!" o un ensayo completo con miles de palabras y, seg√∫n el caso de uso, tendr√°s que abordar estos extremos de forma diferente. Para calcular el n√∫mero de palabras en cada rese√±a, usaremos una heur√≠stica aproximada basada en dividir cada texto por los espacios en blanco.

Definamos una funci√≥n simple que cuente el n√∫mero de palabras en cada rese√±a:

```py
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

Contrario a la funci√≥n `lowercase_condition()`, `compute_review_length()` devuelve un diccionario cuya llave no corresponde a uno de los nombres de las columnas en el conjunto de datos. En este caso, cuando se pasa `compute_review_length()` a `Dataset.map()`,  la funci√≥n se aplicar√° a todas las filas en el dataset para crear una nueva columna `review_length()`:

```py
drug_dataset = drug_dataset.map(compute_review_length)
# Inspeccionar el primer ejemplo de entrenamiento
drug_dataset["train"][0]
```

```python out
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

Tal como lo esper√°bamos, podemos ver que se a√±adi√≥ la columna `review_length` al conjunto de entrenamiento. Podemos ordenar esta columna nueva con `Dataset.sort()` para ver c√≥mo son los valores extremos:

```py
drug_dataset["train"].sort("review_length")[:3]
```

```python out
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

Como lo discutimos anteriormente, algunas rese√±as incluyen una sola palabra, que si bien puede ser √∫til para el an√°lisis de sentimientos, no ser√≠a tan informativa si quisieramos predecir la condici√≥n.

<Tip>

üôã Una forma alternativa de a√±adir nuevas columnas al dataset es a trav√©s de la funci√≥n `Dataset.add_column()`. Esta te permite incluir la columna como una lista de Python o un array de NumPy y puede ser √∫til en situaciones en las que `Dataset.map()` no se ajusta a tu caso de uso.

</Tip>

Usemos la funci√≥n `Dataset.filter()` para quitar las rese√±as que contienen menos de 30 palabras. Similar a lo que hicimos con la columna `condition`, podemos filtrar las rese√±as cortas al incluir una condici√≥n de que su longitud est√© por encima de este umbral:

```py
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python out
{'train': 138514, 'test': 46108}
```

Como puedes ver, esto ha eliminado alrededor del 15% de las rese√±as de nuestros conjuntos originales de entrenamiento y prueba.

<Tip>

‚úèÔ∏è **¬°Int√©ntalo!** Usa la funci√≥n `Dataset.sort()` para inspeccionar las rese√±as con el mayor n√∫mero de palabras. Revisa la [documentaci√≥n](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) para ver cu√°l argumento necesitas para ordenar las rese√±as de mayor a menor.

</Tip>

Por √∫ltimo, tenemos que lidiar con la presencia de c√≥digos de caracteres HTML en las rese√±as. Podemos usar el m√≥dulo `html` de Python para transformar estos c√≥digos as√≠:

```py
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python out
"I'm a transformer called BERT"
```

Usaremos `Dataset.map()` para transformar todos los caracteres HTML en el corpus:

```python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

Como puedes ver, el m√©todo `Dataset.map()` es muy √∫til para procesar datos y esta es apenas la punta del iceberg de lo que puede hacer.

## Los superpoderes del m√©todo `map()`

El m√©todo `Dataset.map()` recibe un argumento `matched` que, al definirse como `True`, env√≠a un lote de ejemplos a la funci√≥n de mapeo a la vez (el tama√±o del lote se puede configurar, pero tiene un valor por defecto de 1.000). Por ejemplo, la funci√≥n anterior de mapeo que transform√≥ todos los HTML se demor√≥ un poco en su ejecuci√≥n (puedes leer el tiempo en las barras de progreso). Podemos reducir el tiempo al procesar varios elementos a la vez usando un _list comprehension_.

Cuando especificas `batched=True`, la funci√≥n recibe un diccionario con los campos del dataset, pero cada valor es ahora una _lista de valores_ y no un valor individual. La salida de `Dataset.map()` deber√≠a ser igual: un diccionario con los campos que queremos actualizar o a√±adir a nuestro dataset y una lista de valores. Por ejemplo, aqu√≠ puedes ver otra forma de transformar todos los caracteres HTML usando `batched=True`:

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

Si est√°s ejecutando este c√≥digo en un cuaderno, ver√°s que este comando se ejecuta mucho m√°s r√°pido que el anterior. Y no es porque los caracteres HTML de las rese√±as ya se hubieran procesado; si vuelves a ejecutar la instrucci√≥n de la secci√≥n anterior (sin `batched=True`), se tomar√° el mismo tiempo de ejecuci√≥n que antes. Esto es porque las _list comprehensions_ suelen ser m√°s r√°pidas que ejecutar el mismo c√≥digo en un ciclo `for` y porque tambi√©n ganamos rendimiento al acceder a muchos elementos a la vez en vez de uno por uno.

Usar `Dataset.map()` con `batched=True` ser√° fundamental para desbloquear la velocidad de los tokenizadores "r√°pidos" que nos vamos a encontrar en el [Cap√≠tulo 6](/course/chapter6), que pueden tokenizar velozmente grandes listas de textos. Por ejemplo, para tokenizar todas las rese√±as de medicamentos con un tokenizador r√°pido, podr√≠amos usar una funci√≥n como la siguiente:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

Como viste en el [Cap√≠tulo 3](/course/chapter3), podemos pasar uno o varios ejemplos al tokenizador, as√≠ que podemos usar esta funci√≥n con o sin `batched=True`. Aprovechemos esta oportunidad para comparar el desempe√±o de las distintas opciones. En un cuaderno, puedes medir el tiempo de ejecuci√≥n de una instrucci√≥n de una l√≠nea a√±adiendo `%time` antes de la l√≠nea de c√≥digo de tu inter√©s:

```python no-format
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

Tambi√©n puedes medir el tiempo de una celda completa a√±adiendo `%%time` al inicio de la celda. En el hardware en el que lo ejecutamos, nos arroj√≥ 10.8s para esta instrucci√≥n (es el n√∫mero que aparece despu√©s de "Wall time").

<Tip>

‚úèÔ∏è **¬°Int√©ntalo!** Ejecuta la misma instrucci√≥n con y sin `batched=True` y luego usa un tokenizador "lento" (a√±ade `use_fast=False` en el m√©todo `AutoTokenizer.from_pretrained()`) para ver cu√°nto tiempo se toman en tu computador.

</Tip>

Estos son los resultados que obtuvimos con y sin la ejecuci√≥n por lotes, con un tokenizador r√°pido y lento:

Opciones         | Tokenizador r√°pido | Tokenizador lento
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

Esto significa que usar un tokenizador r√°pido con la opci√≥n `batched=True` es 30 veces m√°s r√°pido que su contraparte lenta sin usar lotes. ¬°Realmente impresionante! Esta es la raz√≥n principal por la que los tokenizadores r√°pidos son la opci√≥n por defecto al usar `AutoTokenizer` (y por qu√© se denominan "r√°pidos"). Estos logran tal rapidez gracias a que el c√≥digo de los tokenizadores corre en Rust, que es un lenguaje que facilita la ejecuci√≥n del c√≥digo en paralelo.

La paralelizaci√≥n tambi√©n es la raz√≥n para el incremento de 6x en la velocidad del tokenizador al ejecutarse por lotes: No puedes ejecutar una √∫nica operac√≥n de tokenizaci√≥n en paralelo, pero cuando quieres tokenizar muchos textos al mismo tiempo puedes dividir la ejecuci√≥n en diferentes procesos, cada uno responsable de sus propios textos.

`Dataset.map()` tambi√©n tiene algunas capacidades de paralelizaci√≥n. Dado que no funcionan con Rust, no van a hacer que un tokenizador lento alcance el rendimiento de uno r√°pido, pero a√∫n as√≠ pueden ser √∫tiles (especialmente si est√°s usando un tokenizador que no tiene una versi√≥n r√°pida). Para habilitar el multiprocesamiento, usa el argumento `num_proc` y especifica el n√∫mero de procesos para usar en `Dataset.map()`:

```py
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)


def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)


tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

Tambi√©n puedes medir el tiempo para determinar el n√∫mero de procesos que vas a usar. En nuestro caso, usar 8 procesos produjo la mayor ganancia de velocidad. Aqu√≠ est√°n algunos de los n√∫meros que obtuvimos con y sin multiprocesamiento:

Opciones         | Tokenizador r√°pido | Rokenizador lento
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s
`batched=True`, `num_proc=8`  | 6.52s          | 41.3s
`batched=False`, `num_proc=8` | 9.49s          | 45.2s

Estos son resultados mucho m√°s razonables para el tokenizador lento, aunque el desempe√±o del r√°pido tambi√©n mejor√≥ sustancialmente. Sin embargo, este no siempre ser√° el caso: para valores de `num_proc` diferentes a 8, nuestras pruebas mostraron que era m√°s r√°pido usar `batched=true` sin esta opci√≥n. En general, no recomendamos usar el multiprocesamiento de Python para tokenizadores r√°pidos con `batched=True`.

<Tip>

Usar `num_proc` para acelerar tu procesamiento suele ser una buena idea, siempre y cuando la funci√≥n que uses no est√© usando multiples procesos por si misma.

</Tip>

Que toda esta funcionalidad est√° incluida en un m√©todo es algo impresionante en si mismo, ¬°pero hay m√°s!. Con `Dataset.map()` y `batched=True` puedes cambiar el n√∫mero de elementos en tu dataset. Esto es s√∫per √∫til en situaciones en las que quieres crear varias caracter√≠sticas de entrenamiento de un ejemplo, algo que haremos en el preprocesamiento para varias de las tareas de PLN que abordaremos en el [Cap√≠tulo 7](/course/chapter7).

<Tip>

üí° Un _ejemplo_ en Machine Learning se suele definir como el conjunto de _features_ que le damos al modelo. En algunos contextos estos features ser√°n el conjuto de columnas en un `Dataset`, mientras que en otros se pueden extraer m√∫tiples features de un solo ejemplo que pertenecen a una columna ‚Äìcomo aqu√≠ y en tareas de responder preguntas-.

</Tip>

¬°Veamos c√≥mo funciona! En este ejemplo vamos a tokenizar nuestros ejemplos y limitarlos a una longitud m√°xima de 128, pero le pediremos al tokenizador que devuelva *todos* los fragmentos de texto en vez de unicamente el primero. Esto se puede lograr con el argumento `return_overflowing_tokens=True`:

```py
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

Prob√©moslo en un ejemplo puntual antes de usar `Dataset.map()` en todo el dataset:

```py
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python out
[128, 49]
```

El primer ejemplo en el conjunto de entrenamiento se convirti√≥ en dos features porque fue tokenizado en un n√∫mero superior de tokens al que especificamos: el primero de longitud 128 y el segundo de longitud 49. ¬°Vamos a aplicarlo a todo el dataset!

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python out
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

¬øPor qu√© no funcion√≥? El mensaje de error nos da una pista: hay un desajuste en las longitudes de una de las columnas, siendo una de logitud 1.463 y otra de longitud 1.000. Si has revisado la [documentaci√≥n de `Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map), te habr√°s dado cuenta que estamos mapeando el n√∫mero de muestras que le pasamos a la funci√≥n: en este caso los 1.000 ejemplos nos devuelven 1.463 features, arrojando un error.

El problema es que estamos tratando de mezclar dos datasets de tama√±os diferentes: las columnas de `drug_dataset` tendr√°n un cierto n√∫mero de ejemplos (los 1.000 en el error), pero el `tokenized_dataset` que estamos construyendo tendr√° m√°s (los 1.463 en el mensaje de error). Esto no funciona para un `Dataset`, as√≠ que tenemos que eliminar las columnas del anterior dataset o volverlas del mismo tama√±o del nuevo. Podemos hacer la primera operaci√≥n con el argumento `remove_columns`:

```py
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

Ahora funciona sin errores. Podemos revisar que nuestro dataset nuevo tiene m√°s elementos que el original al comparar sus longitudes:

```py
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python out
(206772, 138514)
```

Tambi√©n mencionamos que podemos trabajar con el problema de longitudes que no coinciden al convertir las columnas viejas en el mismo tama√±o de las nuevas. Para eso, vamos a necesitar el campo `overflow_to_sample_mapping` que devuelve el tokenizer cuando definimos `return_overflowing_tokens=True`. Esto devuelve un mapeo del √≠ndice de un nuevo feature al √≠ndice de la muestra de la que se origin√≥. Usando lo anterior, podemos asociar cada llave presente en el dataset original con una lista de valores del tama√±o correcto al repetir los valores de cada ejemplo tantas veces como genere nuevos features:

```py
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # Extraer el mapeo entre los √≠ndices nuevos y viejos
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

De esta forma, podemos ver que funciona con `Dataset.map()` sin necesidad de eliminar las columnas viejas.

```py
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

Como resultado, tenemos el mismo n√∫mero de features de entrenamiento que antes, pero conservando todos los campos anteriores. Quiz√°s prefieras usar esta opci√≥n si necesitas conservarlos para algunas tareas de post-procesamiento despu√©s de aplicar tu modelo.

Ya has visto como usar ü§ó Datasets para preprocesar un dataset de varias formas. Si bien las funciones de procesamiento de ü§ó Datasets van a suplir la mayor parte de tus necesidades de entrenamiento de modelos, hay ocasiones en las que puedes necesitar Pandas para tener acceso a herramientas m√°s poderosas, como `DataFrame.groupby()` o alg√∫n API de alto nivel para visualizaci√≥n. Afortunadamente, ü§ó Datasets est√° dise√±ado para ser interoperable con librer√≠as como Pandas, NumPy, PyTorch, TensoFlow y JAX. Veamos c√≥mo funciona.

## De `Dataset`s a `DataFrame`s y viceversa

<Youtube id="tfcY1067A5Q"/>

Para habilitar la conversi√≥n entre varias librer√≠as de terceros, ü§ó Datasets provee la funci√≥n `Dataset.set_format()`. Esta funci√≥n s√≥lo cambia el _formato de salida_ del dataset, de tal manera que puedas cambiar a otro formato sin cambiar el _formato de datos subyacente_, que es Apache Arrow. Este cambio de formato se hace _in place_. Para verlo en acci√≥n, convirtamos el dataset a Pandas: 

```py
drug_dataset.set_format("pandas")
```

Ahora, cuando accedemos a los elementos del dataset obtenemos un `pandas.DataFrame` en vez de un diccionario:

```py
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv..."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects..."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control..."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

Creemos un `pandas.DataFrame` para el conjunto de entrenamiento entero al seleccionar los elementos de `drug_dataset["train"]`:

```py
train_df = drug_dataset["train"][:]
```

<Tip>

üö® Internamente, `Dataset.set_format()` cambia el formato de devoluci√≥n del m√©todo _dunder_ `__getitem()__`. Esto significa que cuando queremos crear un objeto nuevo como `train_df` de un `Dataset` en formato `"pandas"`, tenemos que seleccionar el dataset completo para obtener un `pandas.DataFrame`. Puedes verificar por ti mismo que el tipo de `drug_dataset["train"]` es `Dataset` sin importar el formato de salida.

</Tip>

De aqu√≠ en adelante podemos usar toda la funcionalidad de pandas cuando queramos. Por ejemplo, podemos hacer un encadenamiento sofisticado para calcular la distribuci√≥n de clase entre las entradas de `condition`:

```py
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>

Y una vez hemos concluido el an√°lisis con Pandas, tenemos la posibilidad de crear un nuevo objeto `Dataset` usando la funci√≥n `Dataset.from_pandas()` de la siguiente manera:

```py
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python out
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<Tip>

‚úèÔ∏è **¬°Int√©ntalo!** Calcula la calificaci√≥n promedio por medicamento y guarda el resultado en un nuevo `Dataset`.

</Tip>

Con esto terminamos nuestro tour de las m√∫ltiples t√©cnicas de preprocesamiento disponibles en ü§ó Datasets. Para concluir, creemos un set de validaci√≥n para preparar el conjunto de datos y entrenar el clasificador. Antes de hacerlo, vamos a reiniciar el formato de salida de `drug_dataset` de `"pandas"` a `"arrow"`:

```python
drug_dataset.reset_format()
```

## Creando un conjunto de validaci√≥n

Si bien tenemos un conjunto de prueba que podr√≠amos usar para la evaluaci√≥n, es una buena pr√°ctica dejar el conjunto de prueba intacto y crear un conjunto de validaci√≥n aparte durante el desarrollo. Una vez est√©s satisfecho con el desempe√±o de tus modelos en el conjunto de validaci√≥n, puedes hacer un √∫ltimo chequeo con el conjunto de prueba. Este proceso ayuda a reducir el riesgo de sobreajustar al conjunto de prueba y desplegar un modelo que falle en datos reales.

ü§ó Datasets provee la funci√≥n `Dataset.train_test_split()` que est√° basada en la famosa funcionalidad de `scikit-learn`. Us√©mosla para separar nuestro conjunto de entrenamiento en dos partes `train` y `validation` (definiendo el argumento `seed` por motivos de reproducibilidad):

```py
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# Renombrar el conjunto "test" a "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# A√±adir el conjunto "test" al `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

S√∫per, ya preparamos un dataset que est√° listo para entrenar modelos. En la [secci√≥n 5](/course/chapter5/5) veremos c√≥mo subir datasets al Hub de Hugging Face, pero por ahora terminemos el an√°lisis estudiando algunas formas de guardarlos en tu m√°quina local.

## Saving a dataset

<Youtube id="blF9uxYcKHo"/>

A pesar de que ü§ó Datasets va a guardar en cach√© todo dataset que descargues, as√≠ como las operaciones que se ejecutan en √©l, hay ocasiones en las que querr√°s guardar un dataset en memoria (e.g., en caso que el cach√© se elimine). Como se muestra en la siguiente tabla, ü§ó Datasets tiene 3 funciones para guardar tu dataset en distintos formatos:


| Formato |        Funci√≥n        |
| :---------: | :--------------------: |
|    Arrow    | `Dataset.save_to_disk()` |
|     CSV     |    `Dataset.to_csv()`    |
|    JSON     |   `Dataset.to_json()`    |

Por ejemplo, guardemos el dataset limpio en formato Arrow:

```py
drug_dataset_clean.save_to_disk("drug-reviews")
```

Esto crear√° una carpeta con la siguiente estructura:

```
drug-reviews/
‚îú‚îÄ‚îÄ dataset_dict.json
‚îú‚îÄ‚îÄ test
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îú‚îÄ‚îÄ train
‚îÇ   ‚îú‚îÄ‚îÄ dataset.arrow
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json
‚îÇ   ‚îú‚îÄ‚îÄ indices.arrow
‚îÇ   ‚îî‚îÄ‚îÄ state.json
‚îî‚îÄ‚îÄ validation
    ‚îú‚îÄ‚îÄ dataset.arrow
    ‚îú‚îÄ‚îÄ dataset_info.json
    ‚îú‚îÄ‚îÄ indices.arrow
    ‚îî‚îÄ‚îÄ state.json
```

en las que podemos ver que cada parte del dataset est√° asociada con una tabla *dataset.arrow* y algunos metadatos en *dataset_info.json* y *state.json*. Puedes pensar en el formato Arrow como una tabla sofisticada de columnas y filas que est√° optimizada para construir aplicaciones de alto rendimiento que procesan y transportan datasets grandes.

Una vez el dataset est√° guardado, podemos cargarlo usando la funci√≥n `load_from_disk()` as√≠:

```py
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python out
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

Para los formatos CSV y JSON, tenemos que guardar cada parte en un archivo separado. Una forma de hacerlo es iterando sobre las llaves y valores del objeto `DatasetDict`:

```py
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

Esto guarda cada parte en formato [JSON Lines](https://jsonlines.org), donde cada fila del dataset est√° almacenada como una √∫nica l√≠nea de JSON. As√≠ se ve el primer ejemplo:

```py
!head -n 1 drug-reviews-train.jsonl
```

```python out
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

Podemos usar las t√©cnicas de la [secci√≥n 2](/course/chapter5/2) para cargar los archivos JSON de la siguiente manera:

```py
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

Esto es todo lo que vamos a ver en nuestra revisi√≥n del manejo de datos con ü§ó Datasets. Ahora que tenemos un dataset limpio para entrenar un modelo, aqu√≠ van algunas ideas que podr√≠as intentar:

1. Usa las t√©cnicas del [Cap√≠tulo 3](/course/chapter3) para entrenar un clasificador que pueda predecir la condici√≥n del paciente con base en las rese√±as de los medicamentos.
2. Usa el pipeline de `summarization` del [Cap√≠tulo 1](/course/chapter1) para generar res√∫menes de las rese√±as.

En la siguiente secci√≥n veremos c√≥mo ü§ó Datasets te puede ayudar a trabajar con datasets enormes ¬°sin explotar tu computador!
