# Â¿Y si mi dataset no estÃ¡ en el Hub?

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

Ya sabes cÃ³mo usar el [Hub de Hugging Face](https://huggingface.co/datasets) para descargar datasets, pero usualmente vas a tener que trabajar con datos que estÃ¡n guardados en tu computador o en un servidor remoto. En esta secciÃ³n te mostraremos cÃ³mo usar ğŸ¤— Datasets para cargar conjuntos de datos que no estÃ¡n disponibles en el Hub de Hugging Face.

<Youtube id="HyQgpJTkRdE"/>

## Trabajando con datos locales y remotos

ğŸ¤— Datasets contiene scripts para cargar datasets locales y remotos que soportan formatos comunes de datos como:

|    Formato de datos     | Script de carga |                         Ejemplo                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV y TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Archivos de texto     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON y JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

Como ves en la tabla, para cada formato de datos solo tenemos que especificar el tipo de script de carga en la funciÃ³n `load_dataset()`, asÃ­ como el argumento `data_files` que contiene la ruta a uno o mÃ¡s archivos. Comencemos por cargar un dataset desde archivos locales y luego veremos cÃ³mo hacer lo propio para archivos remotos.

## Cargando un dataset local

Para este ejemplo, vamos a usar el [dataset SQuAD-it], que es un dataset de gran escala para responder preguntas en italiano.

Los conjuntos de entrenamiento y de prueba estÃ¡n alojados en GitHub, asÃ­ que podemos descargarlos fÃ¡cilmente con el comando `wget`:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Esto va a descargar dos archivos comprimidos llamados *SQuAD_it-train.json.gz* y *SQuAD_it-test.json.gz*, que podemos descomprimir con el comando  `gzip` de Linux:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

De este modo, podemos ver que los archivos comprimidos son reemplazados por los archuvos en formato JSON _SQuAD_it-train.json_ y _SQuAD_it-test.json_.

> [!TIP]
> âœ Si te preguntas por quÃ© hay un carÃ¡cter de signo de admiraciÃ³n (`!`) en los comandos de shell, esto es porque los estamos ejecutando desde un cuaderno de Jupyter. Si quieres descargar y descomprimir el archivo directamente desde la terminal, elimina el signo de admiraciÃ³n.

Para cargar un archivo JSON con la funciÃ³n `load_dataset()`, necesitamos saber si estamos trabajando con un archivo JSON ordinario (parecido a un diccionario anidado) o con JSON Lines (JSON separado por lÃ­neas). Como muchos de los datasets de respuesta a preguntas que te vas a encontrar, SQuAD-it usa el formato anidado, en el que el texto estÃ¡ almacenado en un campo `data`. Esto significa que podemos cargar el dataset especificando el argumento `field` de la siguiente manera: 

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Por defecto, cuando cargas archivos locales se crea un objeto `DatasetDict` con un conjunto de entrenamiento â€“`train`â€“. Podemos verlo al inspeccionar el objeto `squad_it_dataset`:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Esto nos muestra el nÃºmero de filas y los nombres de las columnas asociadas al conjunto de entrenamiento. Podemos ver uno de los ejemplos al poner un Ã­ndice en el conjunto de entrenamiento asÃ­:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si Ã¨ verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Â¡Genial, ya cargamos nuestro primer dataset local! Sin embargo, esto funcionÃ³ Ãºnicamente para el conjunto de entrenamiento. Realmente, queremos incluir tanto el conjunto `train` como el conjunto `test` en un Ãºnico objeto `DatasetDict` para poder aplicar las funciones `Dataset.map()` en ambos conjuntos al mismo tiempo. Para hacerlo, podemos incluir un diccionario en el argumento `datafiles` que mapea cada nombre de conjunto a su archivo asociado:


```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Esto es exactamente lo que querÃ­amos. Ahora podemos aplicar varias tÃ©cnicas de preprocesamiento para limpiar los datos, _tokenizar_ las reseÃ±as, entre otras tareas.

> [!TIP]
> El argumento `data_files` de la funciÃ³n `load_dataset()` es muy flexible. Puede ser una Ãºnica ruta de archivo, una lista de rutas o un diccionario que mapee los nombres de los conjuntos a las rutas de archivo. TambiÃ©n puedes buscar archivos que cumplan con cierto patrÃ³n especÃ­fico de acuerdo con las reglas usadas por el shell de Unix (e.g., puedes buscar todos los archivos JSON en una carpeta al definir `datafiles="*.json"`). Revisa la [documentaciÃ³n](https://huggingface.co/docs/datasets/loading#local-and-remote-files) para mÃ¡s detalles.

Los scripts de carga en ğŸ¤— Datasets tambiÃ©n pueden descomprimir los archivos de entrada automÃ¡ticamente, asÃ­ que podemos saltarnos el uso de `gzip` especificando el argumento `data_files` directamente a la ruta de los archivos comprimidos.

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Esto puede ser Ãºtil si no quieres descomprimir manualmente muchos archivos GZIP. La descompresiÃ³n automÃ¡tica tambiÃ©n aplica para otros formatos de archivo comunes como TAR y ZIP, asÃ­ que solo necesitas dirigir el argumento `data_files` a los archivos comprimidos y Â¡listo!.

Ahora que sabes cÃ³mo cargar archivos locales en tu computador portÃ¡til o de escritorio, veamos cÃ³mo cargar archivos remotos.

## Cargando un dataset remoto

Si estÃ¡s trabajando como cientÃ­fico de datos o desarrollador en una compaÃ±Ã­a, hay una alta probabilidad de que los datasets que quieres analizar estÃ©n almacenados en un servidor remoto. Afortunadamente, Â¡la carga de archivos remotos es tan fÃ¡cil como cargar archivos locales! En vez de incluir una ruta de archivo, dirigimos el argumento `data_files` de la funciÃ³n `load_datasets()` a una o mÃ¡s URL en las que estÃ©n almacenados los archivos. Por ejemplo, para el dataset SQuAD-it alojado en GitHub, podemos apuntar `data_files` a las URL de _SQuAD_it-*.json.gz_ asÃ­:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Esto devuelve el mismo objeto `DatasetDict` que obtuvimos antes, pero nos ahorra el paso de descargar y descomprimir manualmente los archivos _SQuAD_it-*.json.gz_. Con esto concluimos nuestra exploraciÃ³n de las diferentes maneras de cargar datasets que no estÃ¡n alojados en el Hub de Hugging Face. Ahora que tenemos un dataset para experimentar, Â¡pongÃ¡monos manos a la obra con diferentes tÃ©cnicas de procesamiento de datos!

> [!TIP]
> âœï¸ **Â¡IntÃ©ntalo!** Escoge otro dataset alojado en GitHub o en el [Repositorio de Machine Learning de UCI](https://archive.ics.uci.edu/ml/index.php) e intenta cargarlo local y remotamente usando las tÃ©cnicas descritas con anterioridad. Para puntos extra, intenta cargar un dataset que estÃ© guardado en un formato CSV o de texto (revisa la [documentaciÃ³n](https://huggingface.co/docs/datasets/loading#local-and-remote-files) pata tener mÃ¡s informaciÃ³n sobre estos formatos).
