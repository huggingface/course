# ¬øY si mi dataset no est√° en el Hub?

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

Ya sabes c√≥mo usar el [Hub de Hugging Face](https://huggingface.co/datasets) para descargar datasets, pero usualmente vas a tener que trabajar con datos que est√°n guardados en tu computador o en un servidor remoto. En esta secci√≥n te mostraremos c√≥mo usar ü§ó Datasets para cargar conjuntos de datos que no est√°n disponibles en el Hub de Hugging Face.

<Youtube id="HyQgpJTkRdE"/>

## Trabajando con datos locales y remotos

ü§ó Datasets contiene scripts para cargar datasets locales y remotos que soportan formatos comunes de datos como:

|    Formato de datos     | Script de carga |                         Ejemplo                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV y TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Archivos de texto     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON y JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

Como ves en la tabla, para cada formato de datos solo tenemos que especificar el tipo de script de carga en la funci√≥n `load_dataset()`, as√≠ como el argumento `data_files` que contiene la ruta a uno o m√°s archivos. Comencemos por cargar un dataset desde archivos locales y luego veremos c√≥mo hacer lo propio para archivos remotos.

## Cargando un dataset local

Para este ejemplo, vamos a usar el [dataset SQuAD-it], que es un dataset de gran escala para responder preguntas en italiano.

Los conjuntos de entrenamiento y de prueba est√°n alojados en GitHub, as√≠ que podemos descargarlos f√°cilmente con el comando `wget`:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Esto va a descargar dos archivos comprimidos llamados *SQuAD_it-train.json.gz* y *SQuAD_it-test.json.gz*, que podemos descomprimir con el comando  `gzip` de Linux:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

De este modo, podemos ver que los archivos comprimidos son reemplazados por los archuvos en formato JSON _SQuAD_it-train.json_ y _SQuAD_it-test.json_.

<Tip>

‚úé Si te preguntas por qu√© hay un caracter de signo de admiraci√≥n (`!`) en los comandos de shell, esto es porque los estamos ejecutando desde un cuaderno de Jupyter. Si quieres descargar y descomprimir el archivo directamente desde la terminal, elimina el signo de admiraci√≥n.

</Tip>

Para cargar un archivo JSON con la funci√≥n `load_dataset()`, necesitamos saber si estamos trabajando con un archivo JSON ordinario (parecido a un diccionario anidado) o con JSON Lines (JSON separado por l√≠neas). Como muchos de los datasets de respuesta a preguntas que te vas a encontrar, SQuAD-it usa el formato anidado, en el que el texto est√° almacenado en un campo `data`. Esto significa que podemos cargar el dataset especificando el argumento `field` de la siguiente manera: 

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Por defecto, cuando cargas archivos locales se crea un objeto `DatasetDict` con un conjunto de entrenamiento ‚Äì`train`‚Äì. Podemos verlo al inspeccionar el objeto `squad_it_dataset`:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Esto nos muestra el n√∫mero de filas y los nombres de las columnas asociadas al conjunto de entrenamiento. Podemos ver uno de los ejemplos al poner un √≠ndice en el conjunto de entrenamiento as√≠:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si √® verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

¬°Genial, ya cargamos nuestro primer dataset local! Sin embargo, esto funcion√≥ √∫nicamente para el conjunto de entrenamiento. Realmente, queremos incluir tanto el conjunto `train` como el conjunto `test` en un √∫nico objeto `DatasetDict` para poder aplicar las funciones `Dataset.map()` en ambos conjuntos al mismo tiempo. Para hacerlo, podemos incluir un diccionario en el argumento `datafiles` que mapea cada nombre de conjunto a su archivo asociado:


```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Esto es exactamente lo que quer√≠amos. Ahora podemos aplicar varias t√©cnicas de preprocesamiento para limpiar los datos, _tokenizar_ las rese√±as, entre otras tareas.

<Tip>

El argumento `data_files` de la funci√≥n `load_dataset()` es muy flexible. Puede ser una √∫nica ruta de archivo, una lista de rutas o un diccionario que mapee los nombres de los conjuntos a las rutas de archivo. Tambi√©n puedes buscar archivos que cumplan con cierto patr√≥n espec√≠fico de acuerdo con las reglas usadas por el shell de Unix (e.g., puedes buscar todos los archivos JSON en una carpeta al definir `datafiles="*.json"`). Revisa la [documentaci√≥n](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) para m√°s detalles.

</Tip>

Los scripts de carga en ü§ó Datasets tambi√©n pueden descomprimir los archivos de entrada autom√°ticamente, as√≠ que podemos saltarnos el uso de `gzip` especificando el argumento `data_files` directamente a la ruta de los archivos comprimidos.

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Esto puede ser √∫til si no quieres descomprimir manualmente muchos archivos GZIP. La descompresi√≥n autom√°tica tambi√©n aplica para otros formatos de archivo comunes como TAR y ZIP, as√≠ que solo necesitas dirigir el argumento `data_files` a los archivos comprimidos y ¬°listo!.

Ahora que sabes c√≥mo cargar archivos locales en tu computador port√°til o de escritorio, veamos c√≥mo cargar archivos remotos.

## Cargando un dataset remoto

Si est√°s trabajando como cient√≠fico de datos o desarrollador en una compa√±√≠a, hay una alta probabilidad de que los datasets que quieres analizar est√©n almacenados en un servidor remoto. Afortunadamente, ¬°la carga de archivos remotos es tan f√°cil como cargar archivos locales! En vez de incluir una ruta de archivo, dirigimos el argumento `data_files` de la funci√≥n `load_datasets()` a una o m√°s URL en las que est√©n almacenados los archivos. Por ejemplo, para el dataset SQuAD-it alojado en GitHub, podemos apuntar `data_files` a las URL de _SQuAD_it-*.json.gz_ as√≠:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Esto devuelve el mismo objeto `DatasetDict` que obtuvimos antes, pero nos ahorra el paso de descargar y descomprimir manualmente los archivos _SQuAD_it-*.json.gz_. Con esto concluimos nuestra exploraci√≥n de las diferentes maneras de cargar datasets que no est√°n alojados en el Hub de Hugging Face. Ahora que tenemos un dataset para experimentar, ¬°pong√°monos manos a la obra con diferentes t√©cnicas de procesamiento de datos!

<Tip>

‚úèÔ∏è **¬°Int√©ntalo!** Escoge otro dataset alojado en GitHub o en el [Repositorio de Machine Learning de UCI](https://archive.ics.uci.edu/ml/index.php) e intenta cargarlo local y remotamente usando las t√©cnicas descritas con anterioridad. Para puntos extra, intenta cargar un dataset que est√© guardado en un formato CSV o de texto (revisa la [documentaci√≥n](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) pata tener m√°s informaci√≥n sobre estos formatos).

</Tip>
