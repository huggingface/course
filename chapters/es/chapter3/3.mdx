<FrameworkSwitchCourse {fw} />

# Ajuste de un modelo con la API Trainer

<CourseFloatingBanner
  chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb",
    },
  ]}
/>

<Youtube id="nvBXf7s7vTI" />

 Transformers incluye una clase `Trainer` para ayudarte a ajustar cualquiera de los modelos preentrenados proporcionados en tu dataset. Una vez que hayas hecho todo el trabajo de preprocesamiento de datos de la 煤ltima secci贸n, s贸lo te quedan unos pocos pasos para definir el `Trainer`. La parte m谩s dif铆cil ser谩 preparar el entorno para ejecutar `Trainer.train()`, ya que se ejecutar谩 muy lentamente en una CPU. Si no tienes una GPU preparada, puedes acceder a GPUs o TPUs gratuitas en [Google Colab](https://colab.research.google.com/).

Los siguientes ejemplos de c贸digo suponen que ya has ejecutado los ejemplos de la secci贸n anterior. Aqu铆 tienes un breve resumen de lo que necesitas:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Entrenamiento

El primer paso antes de que podamos definir nuestro `Trainer` es definir una clase `TrainingArguments` que contendr谩 todos los hiperpar谩metros que el `Trainer` utilizar谩 para el entrenamiento y la evaluaci贸n del modelo. El 煤nico argumento que tienes que proporcionar es el directorio donde se guardar谩n tanto el modelo entrenado como los puntos de control (checkpoints). Para los dem谩s par谩metros puedes dejar los valores por defecto, deber铆an funcionar bastante bien para un ajuste b谩sico.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

> [!TIP]
>  Si quieres subir autom谩ticamente tu modelo al Hub durante el entrenamiento, incluye `push_to_hub=True` en `TrainingArguments`. Aprenderemos m谩s sobre esto en el [Cap铆tulo 4](/course/chapter4/3).

El segundo paso es definir nuestro modelo. Como en el [cap铆tulo anterior](/course/chapter2), utilizaremos la clase `AutoModelForSequenceClassification`, con dos etiquetas:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Observar谩s que, a diferencia del [Cap铆tulo 2](/course/chapter2), aparece una advertencia despu茅s de instanciar este modelo preentrenado. Esto se debe a que BERT no ha sido preentrenado para la clasificaci贸n de pares de frases, por lo que la cabeza del modelo preentrenado se ha eliminado y en su lugar se ha a帽adido una nueva cabeza adecuada para la clasificaci贸n de secuencias. Las advertencias indican que algunos pesos no se han utilizado (los correspondientes a la cabeza de preentrenamiento eliminada) y que otros se han inicializado aleatoriamente (los correspondientes a la nueva cabeza). La advertencia concluye anim谩ndote a entrenar el modelo, que es exactamente lo que vamos a hacer ahora.

Una vez que tenemos nuestro modelo, podemos definir un `Trainer` pas谩ndole todos los objetos construidos hasta ahora: el `model`, los `training_args`, los datasets de entrenamiento y validaci贸n, nuestro `data_collator`, y nuestro `tokenizer`:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Ten en cuenta que cuando pasas el `tokenizer` como hicimos aqu铆, el `data_collator` por defecto utilizado por el `Trainer` ser谩 un `DataCollatorWithPadding` como definimos anteriormente, por lo que puedes omitir la l铆nea `data_collator=data_collator`. De todas formas, era importante mostrarte esta parte del proceso en la secci贸n 2.

Para ajustar el modelo en nuestro dataset, s贸lo tenemos que llamar al m茅todo `train()` de nuestro `Trainer`:

```py
trainer.train()
```

Esto iniciar谩 el ajuste (que deber铆a tardar un par de minutos en una GPU) e informar谩 de la training loss cada 500 pasos. Sin embargo, no te dir谩 lo bien (o mal) que est谩 rindiendo tu modelo. Esto se debe a que:

1. No le hemos dicho al `Trainer` que eval煤e el modelo durante el entrenamiento especificando un valor para `evaluation_strategy`: `steps` (evaluar cada `eval_steps`) o `epoch` (evaluar al final de cada 茅poca).
2. No hemos proporcionado al `Trainer` una funci贸n `compute_metrics()` para calcular una m茅trica durante dicha evaluaci贸n (de lo contrario, la evaluaci贸n s贸lo habr铆a impreso la p茅rdida, que no es un n煤mero muy intuitivo).

### Evaluaci贸n

Veamos c贸mo podemos construir una buena funci贸n `compute_metrics()` para utilizarla la pr贸xima vez que entrenemos. La funci贸n debe tomar un objeto `EvalPrediction` (que es una tupla nombrada con un campo `predictions` y un campo `label_ids`) y devolver谩 un diccionario que asigna cadenas a flotantes (las cadenas son los nombres de las m茅tricas devueltas, y los flotantes sus valores). Para obtener algunas predicciones de nuestro modelo, podemos utilizar el comando `Trainer.predict()`:

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

La salida del m茅todo `predict()` es otra tupla con tres campos: `predictions`, `label_ids`, y `metrics`. El campo `metrics` s贸lo contendr谩 la p茅rdida en el dataset proporcionado, as铆 como algunas m茅tricas de tiempo (cu谩nto se tard贸 en predecir, en total y de media). Una vez que completemos nuestra funci贸n `compute_metrics()` y la pasemos al `Trainer`, ese campo tambi茅n contendr谩 las m茅tricas devueltas por `compute_metrics()`.

Como puedes ver, `predictions` es una matriz bidimensional con forma 408 x 2 (408 es el n煤mero de elementos del dataset que hemos utilizado). Esos son los logits de cada elemento del dataset que proporcionamos a `predict()` (como viste en el [cap铆tulo anterior](/curso/cap铆tulo2), todos los modelos Transformer devuelven logits). Para convertirlos en predicciones que podamos comparar con nuestras etiquetas, necesitamos tomar el 铆ndice con el valor m谩ximo en el segundo eje:

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Ahora podemos comparar esas predicciones `preds` con las etiquetas. Para construir nuestra funci贸n `compute_metric()`, nos basaremos en las m茅tricas de la librer铆a  [Evaluate](https://github.com/huggingface/evaluate/). Podemos cargar las m茅tricas asociadas al dataset MRPC tan f谩cilmente como cargamos el dataset, esta vez con la funci贸n `evaluate.load()`. El objeto devuelto tiene un m茅todo `compute()` que podemos utilizar para calcular de la m茅trica:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Los resultados exactos que obtengas pueden variar, ya que la inicializaci贸n aleatoria de la cabeza del modelo podr铆a cambiar las m茅tricas obtenidas. Aqu铆, podemos ver que nuestro modelo tiene una precisi贸n del 85,78% en el conjunto de validaci贸n y una puntuaci贸n F1 de 89,97. Estas son las dos m茅tricas utilizadas para evaluar los resultados en el dataset MRPC para la prueba GLUE. La tabla del [paper de BERT](https://arxiv.org/pdf/1810.04805.pdf) recoge una puntuaci贸n F1 de 88,9 para el modelo base. Se trataba del modelo "uncased" (el texto se reescribe en min煤sculas antes de la tokenizaci贸n), mientras que nosotros hemos utilizado el modelo "cased" (el texto se tokeniza sin reescribir), lo que explica el mejor resultado.

Junt谩ndolo todo obtenemos nuestra funci贸n `compute_metrics()`:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Y para ver c贸mo se utiliza para informar de las m茅tricas al final de cada 茅poca, as铆 es como definimos un nuevo `Trainer` con nuestra funci贸n `compute_metrics()`:

```py
training_args = TrainingArguments("test-trainer", eval_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Ten en cuenta que hemos creado un nuevo `TrainingArguments` con su `evaluation_strategy` configurado como `"epoch"` y un nuevo modelo. De lo contrario s贸lo estar铆amos continuando el entrenamiento del modelo que ya hab铆amos entrenado. Para lanzar una nueva ejecuci贸n de entrenamiento, ejecutamos:

```py
trainer.train()
```

Esta vez, nos informar谩 de la p茅rdida de validaci贸n y las m茅tricas al final de cada 茅poca, adem谩s de la p茅rdida de entrenamiento. De nuevo, la puntuaci贸n exacta de precisi贸n/F1 que alcances puede ser un poco diferente de la que encontramos nosotros, debido a la inicializaci贸n aleatoria del modelo, pero deber铆a estar en el mismo rango.

El `Trainer` funciona en m煤ltiples GPUs o TPUs y proporciona muchas opciones, como el entrenamiento de precisi贸n mixta (usa `fp16 = True` en tus argumentos de entrenamiento). Repasaremos todo lo que ofrece en el cap铆tulo 10.

Con esto concluye la introducci贸n al ajuste utilizando la API de `Trainer`. En el [Cap铆tulo 7](/course/chapter7) se dar谩 un ejemplo de c贸mo hacer esto para las tareas m谩s comunes de PLN, pero ahora veamos c贸mo hacer lo mismo en PyTorch puro.

> [!TIP]
> 锔 **隆Int茅ntalo!** Ajusta un modelo sobre el dataset GLUE SST-2 utilizando el procesamiento de datos que has implementado en la secci贸n 2.
