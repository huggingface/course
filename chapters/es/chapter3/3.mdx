<FrameworkSwitchCourse {fw} />

# Ajuste de un modelo con la API Trainer

<CourseFloatingBanner
  chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb",
    },
  ]}
/>

<Youtube id="nvBXf7s7vTI" />

ü§ó Transformers incluye una clase `Trainer` para ayudarte a ajustar cualquiera de los modelos preentrenados proporcionados en tu dataset. Una vez que hayas hecho todo el trabajo de preprocesamiento de datos de la √∫ltima secci√≥n, s√≥lo te quedan unos pocos pasos para definir el `Trainer`. La parte m√°s dif√≠cil ser√° preparar el entorno para ejecutar `Trainer.train()`, ya que se ejecutar√° muy lentamente en una CPU. Si no tienes una GPU preparada, puedes acceder a GPUs o TPUs gratuitas en [Google Colab](https://colab.research.google.com/).

Los siguientes ejemplos de c√≥digo suponen que ya has ejecutado los ejemplos de la secci√≥n anterior. Aqu√≠ tienes un breve resumen de lo que necesitas:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Entrenamiento

El primer paso antes de que podamos definir nuestro `Trainer` es definir una clase `TrainingArguments` que contendr√° todos los hiperpar√°metros que el `Trainer` utilizar√° para el entrenamiento y la evaluaci√≥n del modelo. El √∫nico argumento que tienes que proporcionar es el directorio donde se guardar√°n tanto el modelo entrenado como los puntos de control (checkpoints). Para los dem√°s par√°metros puedes dejar los valores por defecto, deber√≠an funcionar bastante bien para un ajuste b√°sico.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<Tip>

üí° Si quieres subir autom√°ticamente tu modelo al Hub durante el entrenamiento, incluye `push_to_hub=True` en `TrainingArguments`. Aprenderemos m√°s sobre esto en el [Cap√≠tulo 4](/course/chapter4/3).

</Tip>

El segundo paso es definir nuestro modelo. Como en el [cap√≠tulo anterior](/course/chapter2), utilizaremos la clase `AutoModelForSequenceClassification`, con dos etiquetas:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Observar√°s que, a diferencia del [Cap√≠tulo 2](/course/chapter2), aparece una advertencia despu√©s de instanciar este modelo preentrenado. Esto se debe a que BERT no ha sido preentrenado para la clasificaci√≥n de pares de frases, por lo que la cabeza del modelo preentrenado se ha eliminado y en su lugar se ha a√±adido una nueva cabeza adecuada para la clasificaci√≥n de secuencias. Las advertencias indican que algunos pesos no se han utilizado (los correspondientes a la cabeza de preentrenamiento eliminada) y que otros se han inicializado aleatoriamente (los correspondientes a la nueva cabeza). La advertencia concluye anim√°ndote a entrenar el modelo, que es exactamente lo que vamos a hacer ahora.

Una vez que tenemos nuestro modelo, podemos definir un `Trainer` pas√°ndole todos los objetos construidos hasta ahora: el `model`, los `training_args`, los datasets de entrenamiento y validaci√≥n, nuestro `data_collator`, y nuestro `tokenizer`:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Ten en cuenta que cuando pasas el `tokenizer` como hicimos aqu√≠, el `data_collator` por defecto utilizado por el `Trainer` ser√° un `DataCollatorWithPadding` como definimos anteriormente, por lo que puedes omitir la l√≠nea `data_collator=data_collator`. De todas formas, era importante mostrarte esta parte del proceso en la secci√≥n 2.

Para ajustar el modelo en nuestro dataset, s√≥lo tenemos que llamar al m√©todo `train()` de nuestro `Trainer`:

```py
trainer.train()
```

Esto iniciar√° el ajuste (que deber√≠a tardar un par de minutos en una GPU) e informar√° de la training loss cada 500 pasos. Sin embargo, no te dir√° lo bien (o mal) que est√° rindiendo tu modelo. Esto se debe a que:

1. No le hemos dicho al `Trainer` que eval√∫e el modelo durante el entrenamiento especificando un valor para `evaluation_strategy`: `steps` (evaluar cada `eval_steps`) o `epoch` (evaluar al final de cada √©poca).
2. No hemos proporcionado al `Trainer` una funci√≥n `compute_metrics()` para calcular una m√©trica durante dicha evaluaci√≥n (de lo contrario, la evaluaci√≥n s√≥lo habr√≠a impreso la p√©rdida, que no es un n√∫mero muy intuitivo).

### Evaluaci√≥n

Veamos c√≥mo podemos construir una buena funci√≥n `compute_metrics()` para utilizarla la pr√≥xima vez que entrenemos. La funci√≥n debe tomar un objeto `EvalPrediction` (que es una tupla nombrada con un campo `predictions` y un campo `label_ids`) y devolver√° un diccionario que asigna cadenas a flotantes (las cadenas son los nombres de las m√©tricas devueltas, y los flotantes sus valores). Para obtener algunas predicciones de nuestro modelo, podemos utilizar el comando `Trainer.predict()`:

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

La salida del m√©todo `predict()` es otra tupla con tres campos: `predictions`, `label_ids`, y `metrics`. El campo `metrics` s√≥lo contendr√° la p√©rdida en el dataset proporcionado, as√≠ como algunas m√©tricas de tiempo (cu√°nto se tard√≥ en predecir, en total y de media). Una vez que completemos nuestra funci√≥n `compute_metrics()` y la pasemos al `Trainer`, ese campo tambi√©n contendr√° las m√©tricas devueltas por `compute_metrics()`.

Como puedes ver, `predictions` es una matriz bidimensional con forma 408 x 2 (408 es el n√∫mero de elementos del dataset que hemos utilizado). Esos son los logits de cada elemento del dataset que proporcionamos a `predict()` (como viste en el [cap√≠tulo anterior](/curso/cap√≠tulo2), todos los modelos Transformer devuelven logits). Para convertirlos en predicciones que podamos comparar con nuestras etiquetas, necesitamos tomar el √≠ndice con el valor m√°ximo en el segundo eje:

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Ahora podemos comparar esas predicciones `preds` con las etiquetas. Para construir nuestra funci√≥n `compute_metric()`, nos basaremos en las m√©tricas de la librer√≠a ü§ó [Evaluate](https://github.com/huggingface/evaluate/). Podemos cargar las m√©tricas asociadas al dataset MRPC tan f√°cilmente como cargamos el dataset, esta vez con la funci√≥n `evaluate.load()`. El objeto devuelto tiene un m√©todo `compute()` que podemos utilizar para calcular de la m√©trica:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Los resultados exactos que obtengas pueden variar, ya que la inicializaci√≥n aleatoria de la cabeza del modelo podr√≠a cambiar las m√©tricas obtenidas. Aqu√≠, podemos ver que nuestro modelo tiene una precisi√≥n del 85,78% en el conjunto de validaci√≥n y una puntuaci√≥n F1 de 89,97. Estas son las dos m√©tricas utilizadas para evaluar los resultados en el dataset MRPC para la prueba GLUE. La tabla del [paper de BERT](https://arxiv.org/pdf/1810.04805.pdf) recoge una puntuaci√≥n F1 de 88,9 para el modelo base. Se trataba del modelo "uncased" (el texto se reescribe en min√∫sculas antes de la tokenizaci√≥n), mientras que nosotros hemos utilizado el modelo "cased" (el texto se tokeniza sin reescribir), lo que explica el mejor resultado.

Junt√°ndolo todo obtenemos nuestra funci√≥n `compute_metrics()`:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Y para ver c√≥mo se utiliza para informar de las m√©tricas al final de cada √©poca, as√≠ es como definimos un nuevo `Trainer` con nuestra funci√≥n `compute_metrics()`:

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Ten en cuenta que hemos creado un nuevo `TrainingArguments` con su `evaluation_strategy` configurado como `"epoch"` y un nuevo modelo. De lo contrario s√≥lo estar√≠amos continuando el entrenamiento del modelo que ya hab√≠amos entrenado. Para lanzar una nueva ejecuci√≥n de entrenamiento, ejecutamos:

```py
trainer.train()
```

Esta vez, nos informar√° de la p√©rdida de validaci√≥n y las m√©tricas al final de cada √©poca, adem√°s de la p√©rdida de entrenamiento. De nuevo, la puntuaci√≥n exacta de precisi√≥n/F1 que alcances puede ser un poco diferente de la que encontramos nosotros, debido a la inicializaci√≥n aleatoria del modelo, pero deber√≠a estar en el mismo rango.

El `Trainer` funciona en m√∫ltiples GPUs o TPUs y proporciona muchas opciones, como el entrenamiento de precisi√≥n mixta (usa `fp16 = True` en tus argumentos de entrenamiento). Repasaremos todo lo que ofrece en el cap√≠tulo 10.

Con esto concluye la introducci√≥n al ajuste utilizando la API de `Trainer`. En el [Cap√≠tulo 7](/course/chapter7) se dar√° un ejemplo de c√≥mo hacer esto para las tareas m√°s comunes de PLN, pero ahora veamos c√≥mo hacer lo mismo en PyTorch puro.

<Tip>

‚úèÔ∏è **¬°Int√©ntalo!** Ajusta un modelo sobre el dataset GLUE SST-2 utilizando el procesamiento de datos que has implementado en la secci√≥n 2.

</Tip>
