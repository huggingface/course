<FrameworkSwitchCourse {fw} />

# Los poderes especiales de los Tokenizadores R√°pidos (Fast tokenizers)[[fast-tokenizers-special-powers]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3_tf.ipynb"},
]} />

{/if}





En esta secci√≥n miraremos m√°s de cerca las capacidades de los tokenizadores en ü§ó Transformers. Hasta ahora s√≥lo los hemos utilizado para tokenizar las entradas o decodificar los IDs en texto, pero los tokenizadores -- especialmente los que est√°n respaldados en la librer√≠a ü§ó Tokenizers -- pueden hacer mucho m√°s. Para ilustrar estas caracter√≠sticas adicionales, exploraremos c√≥mo reproducir los resultados de los pipelines de `clasificaci√≥n de tokens` (al que llamamos ner) y `question-answering` que nos encontramos en el [Cap√≠tulo 1](/course/chapter1).

<Youtube id="g8quOxoqhHQ"/>

En la siguiente discusi√≥n, a menudo haremos la diferencia entre un tokenizador "lento" y uno "r√°pido". Los tokenizadores lentos son aquellos escritos en Python dentro de la librer√≠a Transformers, mientras que las versiones provistas por la librer√≠a ü§ó Tokenizers, son los que est√°n escritos en Rust. Si recuerdas la tabla del [Cap√≠tulo 5](/course/chapter5/3) en la que se reportaron cuanto tom√≥ a un tokenizador r√°pido y uno lento  tokenizar el Drug Review Dataset, ya deber√≠as tener una idea de por qu√© los llamamos r√°pidos y lentos:


|               | Tokenizador R√°pido | Tokenizador Lento
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

‚ö†Ô∏è Al tokenizar una s√≥la oraci√≥n, no siempre ver√°s una diferencia de velocidad entre la versi√≥n lenta y la r√°pida del mismo tokenizador. De hecho, las versi√≥n r√°pida podr√≠a incluso ser m√°s lenta! Es s√≥lo cuando se tokenizan montones de textos en paralelos al mismo tiempo que ser√°s capaz de ver claramente la diferencia.

</Tip>

## Codificaci√≥n en Lotes (Batch Encoding)[[batch-encoding]]

<Youtube id="3umI3tm27Vw"/>

La salida de un tokenizador no siempre un simple diccionario; lo que se obtiene en realidad es un objeto especial `BatchEncoding`. Es una subclase de un diccionario (raz√≥n por la cual pudimos indexar el resultado sin ning√∫n problema anteriormente), pero con m√©todos adicionales que son mayormente usados por los tokenizadores r√°pidos (_Fast Tokenizers_).

Adem√°s de sus capacidad en paralelizaci√≥n, la funcionalidad clave de un tokenizador r√°pido es que siempre llevan registro de la porci√≥n de texto de la cual los tokens finales provienen -- una caracter√≠stica llamada *offset mapping*. Esto permite la capacidad de mapear cada palabra con el token generado o mapear cada caracter del texto original con el token respectivo y viceversa. 

Echemos un vistazo a un ejemplo:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```
Como se mencion√≥ previamente, obtenemos un objeto de tipo `BatchEncoding` como salida del tokenizador:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

Dado que la clase  `AutoTokenizer` escoge un tokenizador r√°pido por defecto, podemos usar los m√©todos adicionales que este objeto `BatchEncoding` provee. Tenemos dos manera de chequear si el tokenizador es r√°pido o lento. Podemos chequear el atributo `is_fast` del tokenizador:

```python
tokenizer.is_fast
```

```python out
True
```
o chequear el mismo atributo de nuestro `encoding`:

```python
encoding.is_fast
```

```python out
True
```

Veamos lo que un tokenizador r√°pido nos permite hacer. Primero podemos acceder a los tokens sin tener que convertir los IDs a tokens:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

En este caso el token con √≠ndice 5 is `##yl`, el cual es parte de la palabra "Sylvain" en la oraci√≥n original. Podemos tambi√©n utilizar el m√©todo `word_ids()` para obtener el √≠ndice de la palabra de la que cada token proviene:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

Podemos ver que los tokens especiales del tokenizador `[CLS]` y `[SEP]` est√°n mapeados a `None`, y que cada token est√° mapeado a la palabra de la cual se origina. Esto es especialmente √∫til para determinar si el token est√° al inicio de la palabra o si dos tokens est√°n en la misma palabra. POdr√≠amos confiar en el prefijo `[CLS]` and `[SEP]` para eso, pero eso s√≥lo funciona para tokenizadores tipo BERT; este m√©todo funciona para cualquier tipo de tokenizador mientras sea de tipo r√°pido. En el pr√≥ximo cap√≠tulo, veremos como podemos usar esta capacidad para aplicar etiquetas para cada palabra de manera apropiada en tareas como Reconocimiento de Entidades (Named Entity Recognition NER), y etiquetado de partes de discurso (part-of-speech POS tagging). Tambi√©n podemos usarlo para enmascarar todos los tokens que provienen de la misma palabra en masked language modeling (una t√©cnica llamada _whole word masking_).

<Tip>

La noci√≥n de qu√© es una palabra es complicada. Por ejemplo "I'll" (la contracci√≥n de "I will" en ingl√©s) ¬øcuenta como una o dos palabras? De hecho depende del tokenizador y la operaci√≥n de pretokenizaci√≥n que aplica. Algunos tokenizadores s√≥lo separan en espacios, por lo que considerar√°n esto como una s√≥la palabra. Otros utilizan puntuaci√≥n por sobre los espacios, por lo que lo considerar√°n como dos palabras. 

‚úèÔ∏è **Int√©ntalo!** Crea un tokenizador a partir de los checkpoints `bert-base-cased` y `roberta-base` y tokeniza con ellos "81s". ¬øQu√© observas? Cu√°l son los IDs de la palabra?

</Tip>

De manera similar est√° el m√©todo `sentence_ids()` que podemos utilizar para mapear un token a la oraci√≥n de la cu√°l proviene (aunque en este caso el `token_type_ids` retornado por el tokenizador puede darnos la misma informaci√≥n).

Finalmente, podemos mapear cualquier palabra o token a los caracteres originales del texto, y viceversa, utilizando los m√©todos `word_to_chars()` o `token_to_chars()` y los m√©todos `char_to_word()` o `char_to_token()`. Por ejemplo el m√©todo `word_ids()` nos dijo que `##yl` es parte de la palabra con √≠ndice 3, pero qu√© palabra es en la oraci√≥n? Podemos averiguarlo as√≠:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

Como mencionamos previamente, todo esto funciona gracias al hecho de que los tokenizadores r√°pidos llevan registro de la porci√≥n de texto del que cada token proviene en una lista de *offsets*. Para ilustrar sus usos, a continuaci√≥n mostraremos como replicar los resultados del pipeline de `clasificaci√≥n de tokens` de manera manual.

<Tip>

‚úèÔ∏è **Int√©ntalo!** Crea tu propio texto de ejemplo y ve si puedes entender qu√© tokens est√°n asociados con el ID de palabra, y tambi√©n c√≥mo extraer los caracteres para una palabra. Como bonus, intenta usar dos oraciones como entrada/input y ve si los IDs de oraciones te hacen sentido.

</Tip>

## Dentro del Pipeline de `clasificaci√≥n de tokens`[[inside-the-token-classification-pipeline]]

En el [Cap√≠tulo 1](/course/chapter1) tuvimos nuestra primera probada aplicando NER -- donde la tarea es identificar qu√© partes del texto corresponden a entidades como personas, locaciones, u organizaciones -- con la funci√≥n `pipeline()` de la librer√≠a ü§ó Transformers. Luego en el [Cap√≠tulo 2](/course/chapter2), vimos como un pipeline agrupa las tres etapas necesarias para obtener predicciones desde un texto crudo: tokenizaci√≥n, pasar los inputs a trav√©s del modelo, y post-procesamiento. Las primeras dos etapas en el pipeline de `clasificaci√≥n de tokens` son las mismas que en otros pipelines, pero el post-procesamiento es un poco m√°s complejo -- √ó+/¬°veamos c√≥mo!


{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### Obteniendo los resultados base con el pipeline[[getting-the-base-results-with-the-pipeline]]

Primero, agarremos un pipeline de clasificaci√≥n de tokens para poder tener resultados que podemos comparar manualmente. El usado por defecto es [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); el que realiza NER en oraciones:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

El modelo indentific√≥ apropiadamente cada token generado por "Sylvain" como una persona, cada token generado por "Hugging Face" como una organizaci√≥n y el token "Brooklyn" como una locaci√≥n. Podemos pedirle tambi√©n al pipeline que agrupe los tokens que corresponden a la misma identidad:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

La estrategia de agregaci√≥n (`aggregation_strategy`) elegida cambiar√° los puntajes calculados para cada entidad agrupada. Con `"simple"` el puntaje es la media los puntajes de cada token en la entidad dada: por ejemplo, el puntaje de "Sylvain" es la media de los puntajes que vimos en el ejemplo previo para los tokens `S`, `##yl`, `##va`, y `##in`. Otras estrategias disponibles son:

- `"first"`, donde el puntaje de cada entidad es el puntaje del primer token de la entidad (para el caso de "Sylvain" ser√≠a 0.9923828, el puntaje del token `S`)
- `"max"`, donde el puntaje de cada entidad es el puntaje m√°ximo de los tokens en esa entidad (para el caso de "Hugging Face" ser√≠a 0.98879766, el puntaje de "Face")
- `"average"`, donde el puntaje de cada entidad es el promedio de los puntajes de las palabras que componen la entidad (para el caso de "Sylvain" no habr√≠a diferencia con la estrategia "simple", pero "Hugging Face" tendr√≠a un puntaje de 0.9819, el promedio de los puntajes para "Hugging", 0.975, y "Face", 0.98879)

Ahora veamos como obtener estos resultados sin utilizar la funci√≥n `pipeline()`!

### De los inputs a las predicciones[[from-inputs-to-predictions]]

{#if fw === 'pt'}

Primero necesitamos tokenizar nuestro input y pasarlo a trav√©s del modelo. Esto es exactamente lo que se hace en el [Cap√≠tulo 2](/course/chapter2); instanciamos el tokenizador y el modelo usando las clases `AutoXxx` y luego los usamos en nuestro ejemplo:


```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

Dado que estamos usando ac√° `AutoModelForTokenClassification`, obtenemos un conjunto de logits para cada token en la secuencia de entrada:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

Primero necesitamos tokenizar nuestro input y pasarlo por nuestro modelo. Esto es exactamente lo que se hace en el [Cap√≠tulo 2](/course/chapter2); instanciamos el tokenizador y el modelo usando las clases `TFAutoXxx` y luego los usamos en nuestro ejemplo:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```
Dado que estamos usando ac√° `TFAutoModelForTokenClassification`, obtenemos un conjunto de logits para cada token en la secuencia de entrada:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

Tenemos un lote de 1 secuencia con 19 tokens y el modelo tiene 9 etiquetas diferentes, por lo que la salida del modelo tiene dimensiones 1 x 19 x 9. Al igual que el pipeline de clasificaci√≥n de texto, usamos la funci√≥n softmax para convertir esos logits en probabilidades, y tomamos el argmax para obtener las predicciones (notar que podemos tomar el argmax de los logits directamente porque el softmax no cambia el orden):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

El atributo `model.config.id2label` contiene el mapeo de los √≠ndices con las etiquetas para que podemos hacer sentido de las predicciones:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

Como vimos antes, hay 9 etiquetas: `0` es la etiqueta para los tokens que no tienen ning√∫na entidad (proviene del ingl√©s "outside"), y luego tenemos dos etiquetas para cada tipo de entidad (miscel√°neo, persona, organizaci√≥n, y locaci√≥n). La etiqueta `B-XXX` indica que el token is el inicio de la entidad `XXX` y la etiqueta `I-XXX` indica que el token est√° dentro de la entidad `XXX`. For ejemplo, en el ejemplo actual esperar√≠amos que nuestro modelo clasificar√° el token `S` como `B-PER` (inicio de la entidad persona), y los tokens `##yl`, `##va` y `##in` como `I-PER` (dentro de la entidad persona).

Podr√≠as pensar que el modelo est√° equivocado en este caso ya que entreg√≥ la etiqueta `I-PER` a los 4 tokens, pero eso no es completamente cierto. En realidad hay 4 formatos par esas etiquetas `B-` y `I-`: *I0B1* y *I0B2*. El formato I0B2 (abajo en rosado), es el que presentamos, mientras que en el formato I0B1 (en azul), las etiquetas de comenzando con `B-` son s√≥lo utilizadas para separar dos entidades adyacentes del mismo tipo. Al modelo que estamos usando se le hizo fine-tune en un conjunto de datos utilizando ese formato, lo cual explica por qu√© asigna la etiqueta `I-PER` al token `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

Con este mapa, estamos listos para reproducir (de manera casi completa) los resultados del primer pipeline -- basta con tomar los puntajes y etiquetas de cada token que no fue clasificado como `0`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

Esto es muy similar a lo que ten√≠amos antes, con una excepci√≥n: el pipeline tambi√©n nos di√≥ informaci√≥n acerca del `inicio` y el `final` de cada entidad en la oraci√≥n original. Aqu√≠ es donde nuestro mapeo de offsets entrar√°n en juego. Para obtener los offsets, s√≥lo tenemos que fijar la opci√≥n `return_offsets_mapping=True` cuando apliquemos el tokenizador a nuestros inputs:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

Cada tupla es la porci√≥n de texto correspondiente a cada token, donde `(0, 0)` est√° reservado para los tokens especiales. Vimos antes que el token con √≠ndice 5 is `##yl`, el cual tiene como offsets `(12, 14)`, Si tomamos los trozos correspondientes en nuestro ejemplo:


```py
example[12:14]
```

obtenemos la porci√≥n apropiada sin los `##`:


```python out
yl
```

Usando esto, ahora podemos completar los resultados previos:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Esto es lo mismo que obtuvimos en el primer pipeline!

### Agrupando Entidades[[grouping-entities]]

Usar los offsets para determinar las llaves de inicio y fin para cada entidad is √∫til, pero esa informaci√≥n no es estrictamente necesaria. Cuando queremos agrupar las entidades, sin embargo, los offsets nos ahorar√°n un mont√≥n de c√≥digo engorroso. Por ejemplo, si queremos agrupar los tokens `Hu`, `##gging`, y `Face`, podemos hacer reglas especiales que digan que los dos primeros se tienen que unir eliminando los `##`, y `Face` deber√≠a a√±adirse con el espacio ya que no comienza con `##` -- pero eso s√≥lo funcionar√≠a para este tipo particular de tokenizador. Tendr√≠amos que escribir otro grupo de reglas para un tokenizador tipo SentencePiece (trozo de oraci√≥n) tipo Byte-Pair-Encoding (codificaci√≥n por par de bytes) (los que se discutir√°n m√°s adelante en este cap√≠tulo).

Con estos offsets, todo ese c√≥digo hecho a medida no se necesita: basta tomar la porci√≥n del texto original que comienza con el primer token y termina con el √∫ltimo token. En el caso de los tokens `Hu`, `##gging`, and `Face`, deber√≠amos empezar en el character 33 (el inicio de `Hu`) y termianr antes del caracter 45 (al final de `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

Para escribir el c√≥digo encargado del post-procesamiento de las prediciones que agrupan entidades, agruparemos la entidades que son consecutivas y etiquetadas con `I-XXX`, excepto la primera, la cual puedes estar etiquetada como `B-XXX` o `I-XXX` (por lo que, dejamos de agrupar una entidad cuando nos encontramos un `0`, un nuevo tipo de entidad, o un `B-XXX` que nos dice que una entidad del mismo tipo est√° empezando):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Toma todos los tokens etiquetados con la etiqueta I
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # El puntaje es la media de todos los puntajes de los tokens en la entidad agrupada
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

Y obtenemos los mismos resultados de nuestro segundo pipeline!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Otro ejemplo de una tarea donde estos offsets son extremadamente √∫tiles es question answering. Sumergirnos en ese pipeline, lo cual haremos en la siguiente secci√≥n, tambi√©n nos permitir√° echar un vistazo a una √∫ltima caracter√≠stica de los tokenizadores en la librer√≠a ü§ó Transformers: lidiar con tokens desbordados (overflowing tokens) cuando truncamos una entrada/input a un largo dado. 
