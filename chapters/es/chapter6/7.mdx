# Tokenizaci√≥n Unigram[[unigram-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb"},
]} />

El algoritmo de Unigram es a menudo utilizado en SetencePiece, el cual es el algoritmo de tokenizaci√≥n usado por modelos como AlBERT, T5, mBART, Big Bird y XLNet.

<Youtube id="TGZfZVuF9Yc"/>

<Tip>

üí° Esta secci√≥n cubre Unigram en profundidad, yendo tan lejos como para mostrar una implementaci√≥n completa. Puedes saltarte hasta el final si s√≥lo quieres una descripci√≥n general del algoritmo de tokenizaci√≥n. 

</Tip>

## Algoritmo de Entrenamiento[[training-algorithm]]

Comparado con BPE y WordPiece, Unigram funciona en la otra direcci√≥n: comienza desde un gran vocabulario y remueve tokens hasta que alcanza el tama√±o deseado del vocabulario.. Hay varias opciones para construir el vocabulario base: podemos tomar los substrings m√°s comunes en palabras pre-tokenizadas, por ejemplo, o aplicar BPE en el corpus inicial con un tama√±o de vocabulario grande. 

En cada paso del entrenamiento, el algoritmo de Unigram calcula la p√©rdida (`loss`)sobre el corpus dado el vocabulario actual. Entonces para cada s√≠mbolo en el vocabulario, el algoritmo calcula cu√°nto incremetar√≠a el la p√©rdida (`loss`) total si el s√≠mbolo se remueve, y busca por los s√≠mbolos que lo incrementar√≠an lo menos posible. Esos s√≠mbolos tienen un efecto m√°s bajo en la p√©rdida sobre el corpus, por lo que en un sentido son "menos necesarios" y son los mejores candidatos para ser removidos. 

Esto es una operaci√≥n bastante costosa, por lo que no removemos un s√≥lo s√≠mbolo asociato con el incremento en la p√©rdida (`loss`) m√°s baja, sino que \\(p\\) (\\(p\\) es un par√°metro que puedes controlar, usualmente 10 o 20) porciento de los s√≠mbolos asociados con el incremento m√°s bajo de la p√©rdida. Este proceso es repetido hasta que el vocabulario ha alcanzado el tama√±o deseado. 

Nota que nunca removemos los caracteres base, para asegurarnos que cada palabra pueda ser tokenizada. 

Hora, esto es todav√≠a un poco vago: la parte principal del algoritmo es calcular una p√©rdida (`loss`) sobre el corpus, y ver como cambia cuando removemos algunos tokens desde el vocabulario, pero no hemos explicado como hacer esto a√∫n. Este paso se basa en el algoritmo de tokenizaci√≥n de un modelo Unigram, por lo que profundizaremos en esto a continuaci√≥n.

Usaremos el corpus de los ejemplos previos:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

y para este ejemplo, tomaremos todos los substrings strictos para el vocabulario inicial. 

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Algoritmo de Tokenizaci√≥n[[tokenization-algorithm]]

Un modelo Unigram es un tipo de modelo de lenguaje que considera cada token como independiente de los tokens antes que √©l. Es el modelo de lenguaje m√°s simple, en el sentido de que la probabilidad de que el token X dado el contexto previo es s√≥lo la probabilidad del token X. Por lo que, si usamos un modelo de Lenguaje Unigram para generar texto, siempre predecir√≠amos el token m√°s com√∫n.

La probabilidad de un token dado es su frecuencia (el n√∫mero de veces en el cual lo encontramos) en el corpus original, dividido por la suma de todas las frecuencias de todos los tokens en el vocabulario (para asegurarnos que las probabilidad sumen 1). Por ejemplo, `"ug"` est√° presente en `"hug"`, `"pug"`, y `"hugs"`, por lo que tiene una frecuencia de 20 en nuestro corpus. 

Ac√° est√°n las frecuencias de todas las posibles subpalabras en el vocabulario:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

Por lo que, la suma de todas las frecuencias es 210, y la probabilidad de la subpalabra `"ug"` es por lo tanto 20/210.

<Tip>

‚úèÔ∏è **Ahora es tu turno!** Escribe el c√≥digo para calcular las frecuencias de arriba y chequea que los resultados mostrados son correctos, como tambi√©n la suma total.

</Tip>

Ahora, para tokenizar una palabra dada, miramos todas las posibles segmentaciones en tokens y calculamos la probabilidad de cada uno de acuerdo al modelo Unigram. Dado que todos los tokens se consideran como independientes, esta probabilidad es s√≥lo el producto de la probabilidad de cada token. Por ejemplo, la tokenizaci√≥n `["p", "u", "g"]` de `"pug"` tiene como probabilidad: 

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

Comparativamente, la tokenizaci√≥n `["pu", "g"]` tiene como probabilidad:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

por lo que es un poco m√°s probable. En general, las tokenizaciones con el menor n√∫mero de tokens posibles tendr√°n la probabilidad m√°s alta (debido a la divisi√≥n por 210 repetida para cada token), lo cual corresponde a lo que queremos intuitivamente: separar una palabra en el menor n√∫mero de tokens posibles. 

La tokenizaci√≥n de una palabra con el modelo Unigram es entonces la tokenizaci√≥n con la probabilidad m√°s alta. Ac√° est√°n las probabilidades para el ejemplo de `"pug"` que obtendr√≠amos para cada posible segmentaci√≥n:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

Por lo que, `"pug"` ser√≠a tokenizado como `["p", "ug"]` o `["pu", "g"]`, dependiendo de cual de esas segmentaciones e encuentre primero (notar que en un corpus grande, casos equivalentes como este ser√°n raros).

En este caso, fue f√°cil encontrar todas las posibles segmentaciones y calcular sus probabilidades, pero en general, va a ser un poco m√°s dif√≠cil. Hay un algoritmo cl√°sico usado para esto, llamado el *Algoritmo de Viterbi* (*Viterbi algorithm*). Esencialmente, podemos construir un grafo para detectar las posibles segmentaciones de una palabra dada diciendo que existe una rama que va desde el caracter _a_ hasta el caracter _b_ si la subpalabra de _a_ hasta _b_ est√° en el vocabulario, y se atribuye a esa rama la probabilidad de la subpalabra. 

Para encontrar el camino en dicho grafo que va a tener el mejor puntaje el Algoritmo de Viterbi determina, por cada posici√≥n en la palabra, la segmentacion con el mejor puntaje que termina en esa posici√≥n. Dado que vamos desde el inicio al final, el mejor puntaje puede ser encontrado iterando a trav√©s de todas las subpalabras que terminan en la posici√≥n actual y luego usando el mejor puntaje de tokenizaci√≥n desde la posici√≥n en que esta palabra comienza. Luego s√≥lo tenemos que desenrollar el camino tomado para llegar al final. 

Echemos un vistazo a un ejemplo usando nuestro vocabulario y la palabra `"unhug"`. Para cada posici√≥n, las subpalabras con el mejor puntaje terminando ah√≠ son las siguientes:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

Por lo tanto, `"unhug"` se tokenizar√≠a como `["un", "hug"]`.

<Tip>

‚úèÔ∏è **Ahora es tu turno!** Determina la tokenizaci√≥n de la palabra `"huggun"`, y su puntaje


</Tip>

## De vuelta al entrenamiento[[back-to-training]]

Ahora que hemos visto c√≥mo funciona la tokenizaci√≥n, podemos ir un poco m√°s profundo en la p√©rdida (`loss`) usada durante el entrenamiento. En cualquier etapa, esta p√©rdida (`loss`) es calculada tokenizando cualquier palabra en el corpus, usando el vocabulario actual y el modelo Unigram determinado por las frecuencias de cada token en el corpus (como se vi√≥ antes).

Cada palabra en el corpus tiene un puntaje, y la p√©rdida (`loss`) es la log verosimilitud negativa (negative log likelihood) de estos puntajes -- es decir, la suma por todas las palabras en el corpus de todos los `-log(P(word))`.

Volvamos a nuestro ejemplo con el siguiente corpus:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

La tokenizaci√≥n de cada palabra con sus respectivos puntajes es:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

Por lo que la loss es:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Ahora necesitamos calcular c√≥mo remover cada token afecta a la p√©rdida (`loss`). Esto es bastante tedioso, por lo que lo haremos s√≥lo para dos tokens av√° y nos ahorraremos el proceso entero para cuando tengamos c√≥digo que nos ayude. En este (muy) particular caso, ten√≠amos dos tokenizaciones equivalentes de todas las palabras: como vimos antes, por ejemplo, `"pug"` podr√≠a ser tokenizado como `["p", "ug"]` con el mismo puntaje. Por lo tanto, removiendo el token `"pu"` del vocabulario nos dar√° la misma p√©rdida.

Por otro lado, remover, `"hug"` har√° nuestra p√©rdida peor, porque la tokenizaci√≥n de `"hug"` y `"hugs"` se convertir√° en:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Estos cambios causar√°n que la p√©rdida aumenta en:

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

Por lo tanto, el token `"pu"` ser√° probablemente removido del vocabulario, pero `"hug"`.

## Implementando Unigram[[implementing-unigram]]

Ahora, implementemos todo lo que hemos visto hasta ahora en c√≥digo. Al igual que BPE y WordPiece, esta es una implementaci√≥n no tan eficiente del algoritmo Unigram (de hecho, todo lo contrario), pero deber√≠a ayudar a entenderla un poco mejor. 

Usaremos el mismo corpus que antes como nuestro ejemplo:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Esta vez, usaremos `xlnet-base-cased` como nuestro modelo:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

Al igual que BPE y WordPiece, comenzamos contando el n√∫mero de ocurrencias para cada palabra en el corpus:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Luego, necesitamos inicializar nuestro vocabulario a algo m√°s grande que el tama√±o de vocabulario que querremos al final. Tenemos que incluir, todos los caracteres b√°sicos (de otra manera no seremos capaces de tokenizar cada palabra), pero para los substrings m√°s grandes mantendremos s√≥los los m√°s comunes, de manera que los ordenemos por frecuencia:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Loop through the subwords of length at least 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sort subwords by frequency
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('‚ñÅt', 7), ('is', 5), ('er', 5), ('‚ñÅa', 5), ('‚ñÅto', 4), ('to', 4), ('en', 4), ('‚ñÅT', 3), ('‚ñÅTh', 3), ('‚ñÅThi', 3)]
```

Agrupamos los caracteres con las mejores subpalabras para llegar a un vocabulario inicial de 300:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>

üí° SentencePiece usa un algoritmo m√°s eficiente llamado Enhanced Suffix Array (ESA) para crear el vocabulario inicial. 

</Tip>

A continuaci√≥n, calculamos la suma de todas las frecuencias, para convertir las frecuencias en probabilidades. Para nuestro modelo, almacenaremos los logaritmos de las probabilidades, porque es numericamente m√°s estable sumar logaritmos que multiplicar n√∫meros peque√±os, y esto simplificar√° el c√°lculo de la p√©rdida (`loss`) del modelo:

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Ahora, la funci√≥n es la que tokeniza palabras usando el algoritmo de Viterbi. Como vimos antes, el algoritmo calcula la mejor segmentaci√≥n de cada substring de la palabra, la cual almacenar√° en una variable llamada `best_segmentations`. Almacenaremos un diccionario por posici√≥n en la palabra (desde 0 hasta su largo total), con dos claves: el √≠ndice de inicio del √∫ltimo token en la mejor segmentaci√≥n, y el puntaje de la mejor segmentaci√≥n. Con el √≠ndice del inicio del √∫ltimo token, seremos capaces de recuperar la segmentaci√≥n total una vez que la lista est√© completamente poblada. 

Poblar la lista se hace con dos ciclos: el ciclo principal recorre cada posici√≥n de inicio, y el segundo loop, prueba todos los substrings comenaando en esa posici√≥n. Si el substring est√° en el vocabulario, tenemos una nueva segmentaci√≥n de la palabra hasta esa posici√≥n final, la cual comparamos con lo que est√° en `best_segmentations`.

Una vez que el ciclo principal se termina, empezamos desde el final y saltamos de una posici√≥n de inicio hasta la siguiente, guardando los tokens a medida que avanzamos, hasta alcanzar el inicio de la palabra:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # This should be properly filled by the previous steps of the loop
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # If we have found a better segmentation ending at end_idx, we update
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # We did not find a tokenization of the word -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

Ya podemos probar nuestro modelo inicial en algunas palabras:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Ahora es f√°cil calcular la p√©rdida (`loss`) del modelo en el corpus!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Podemos chequear que funciona en el modelo que tenemos:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

Calcular los puntajes para cada token no es tan dif√≠cil tampoco; s√≥lo tenemos que calcular la p√©rdida para los modelos obtenidos al eliminar cada token:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # We always keep tokens of length 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

Podemos probarlo en token dado: 

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

Dado que `"ll"` se usa en la tokenizaci√≥n de `"Hopefully"`, y removerlo nos har√° probablemente usar el token `"l"` dos veces, esperamos que tendr√° una p√©rdida positiva. `"his"` es s√≥lo usado dentro de la palabra `"This"`, lo cu√°l es tokenizado como s√≠ mismo, por lo que esperamos que tenga p√©rdida cero. Ac√° est√°n los resultados:

```python out
6.376412403623874
0.0
```

<Tip>

üí° Este acercamiento es muy ineficiente, por lo que SentencePiece usa una aproximaci√≥n de la p√©rdida del modelo sin el token X: en vez de comenzar desde cero, s√≥lo reemplaza el token X por su segmentaci√≥n en el vocabulario que queda. De esta manera, todos los puntajes se pueden calcular de una s√≥la vez al mismo tiempo que la p√©rdida del modelo.

</Tip>

Con todo esto en su lugar, lo √∫ltimo que necesitamos hacer es agregar los tokens especiales usados por el modelo al vocabulario, e iterar hasta haber podado suficientes tokens de nuestro vocabulario hasta alcanzar el tama√±o deseado:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Remove percent_to_remove tokens with the lowest scores.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Luego, para tokenizar algo de texto, s√≥lo necesitamos aplicar la pre-tokenizaci√≥n y luego usar nuestra funci√≥n `encode_word()`:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅHugging', '‚ñÅFace', '‚ñÅ', 'c', 'ou', 'r', 's', 'e', '.']
```

Eso es todo para Unigram! Ojal√° a esta altura te sientas como un experto en todos los aspectos de los tokenizadores. En la siguiente secci√≥n, ahondaremos en las unidades b√°sicas de la librer√≠a ü§ó Tokenizers, y te mostraremos c√≥mo puedes usarlo para construir tu propio tokenizador.