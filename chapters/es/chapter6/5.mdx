#  Tokenizaci칩n por Codificaci칩n Byte-Pair[[byte-pair-encoding-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb"},
]} />

La codificaci칩n por pares de byte (Byte-Pair Encoding (BPE)) fue inicialmente desarrollado como un algoritmo para comprimir textos, y luego fue usado por OpenAI para la tokenizaci칩n al momento de pre-entrenar el modelo GPT. Es usado por un mont칩n de modelos Transformers, incluyendo GPT, GPT-2, RoBERTa, BART, y DeBERTa.

<Youtube id="HEikzVL-lZU"/>

<Tip>

游눠 Esta secci칩n cubre BPE en produndidad, yendo tan lejos como para mostrar una implementaci칩n completa. Puedes saltarte hasta el final si s칩lo quieres una descripci칩n general del algoritmo de tokenizaci칩n. 

</Tip>

## Algoritmo de Entrenamiento[[training-algorithm]]

El entrenamiento de BPE comienza calculando el conjunto de palabras 칰nicas usada en el corpus (despu칠s de completar las etapas de normalizaci칩n y pre-tokenizaci칩n), para luego contruir el vocabulario tomando todos los s칤mbolos usados para escribir esas palabras. Como un ejemplo muy simple, digamos que nuestros corpus usa estas cinco palabras:


```
"hug", "pug", "pun", "bun", "hugs"
```

El vocabulario vase entonces ser치 `["b", "g", "h", "n", "p", "s", "u"]`. Para casos reales, el vocabulario base contendr치 todos los caracteres ASCII, al menos, y probablemente algunos caracteres Unicode tambi칠n. Si un ejemplo que est치s tokenizando usa un caracter que no est치 en el corpus de entrenamiento, ese caracter ser치 convertido al token "desconocido". Esa es una raz칩n por la cual muchos modelos de NLP son muy malos analizando contenido con emojis.

<Tip>

Los tokenizadores de GPT-2 y RoBERTa (que son bastante similares) tienen una manera bien inteligente de lidiar con esto: ellos no miran a las palabras como si estuvieran escritas con caracteres Unicode, sino con bytes. De esa manera el vocabulario base tiene un tama침o peque침o (256), pero cada caracter que te puedas imaginar estar치 incluido y no terminar치 convertido en el token "desconocido". Este truco se llama *byte-level BPE*.

</Tip>

Luego de obtener el vocabulario base, agregamos nuevos tokens hasta que el tama침o deseado del vocabulario se alcance por medio de aprender *fusiones* (merges), las cuales son reglas para fusionar dos elementos del vocabulario existente en uno nuevo. Por lo que al inicio de estas fusiones crearemos tokens con dos caracteres, y luego, a medida que el entrenamiento avance, subpalabras m치s largas.

En cualquier etapa durante el entrenamiento del tokenizador, el algoritmo BPE buscar치 pos los pares m치s frecuentes de los tokens existentes (por "par", ac치 nos referimos a dos tokens consecutivos en una palabra). El par m치s frecuente es el que ser치 fusionado, y enjuagamos y repetimos para la siguiente etapa. 

Volviedo a nuestro ejemplo previo, asumamos que las palabras ten칤an las siguientes frecuencias:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

lo que significa que `"hug"` estuvo presente 10 veces en el corpus, `"pug"` 5 veces, `"pun"` 12 veces, `"bun"` 4 veces, and `"hugs"` 5 veces. Empezamos el entrenamiento separando cada palabra en caracteres (los que formaron nuestro vocabulario inicial) para que podamos ver cada palabra como una lista de tokens:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

Luego miramos los pares. El par `("h", "u")` est치 presente en las palabras `"hug"` y `"hugs"`, 15 veces en el total del corpus. No es el par m치s frecuente: ese honor le corresponde a `("u", "g")`, el cual est치 presente en `"hug"`, `"pug"`, y `"hugs"`, para un gran total de 20 veces en el vocabulario. 

Por lo tanto, la primera regla de fusi칩n aprendida por el tokenizador es `("u", "g") -> "ug"`, lo que significa que `"ug"` ser치 agregado al vocabulario, y el par deber칤a ser fusionado en todas las palabras del corpus. Al final de esta etapa, el vocabulario se ve as칤:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

Ahora tenemos algunos pares que resultan en un token m치s largo de dos caracteres: por ejemplo el par `("h", "ug")` (presente 15 veces en el corpus). Sin embargo, el par m치s frecuente en este punto is `("u", "n")`, presente 16 veces en el corpus, por lo que la segunda regla de fusi칩n aprendida es `("u", "n") -> "un"`. Agregando esto y fusionando todas las ocurrencias existentes nos lleva a:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

Ahora el par m치s frecuente es `("h", "ug")`, por lo que aprendemos que la regla de fusi칩n es `("h", "ug") -> "hug"`, lo cual nos da tuestro primer token de tres letras. Luego de la fusi칩n el corpus se ve as칤:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

Y continuamos as칤 hasta que alcancemos el tama침o deseado del vocabulario.

<Tip>

九勇 **Ahora es tu turno!** Cu치l crees que ser치 la siguiente regla de fusi칩n?

</Tip>

## Algoritmo de Tokenizaci칩n[[tokenization-algorithm]]

La tokenizaci칩n sigue el proceso de entrenamiento de cerca, en el sentido que nuevos inputs son tokenizados aplicando los siguientes pasos:

1. Normalizaci칩n
2. Pre-tokenizaci칩n
3. Separar las palabras en caracteres individuales
4. Aplicar las reglas de fusi칩n aprendidas en orden en dichas separaciones.

Tomemos el ejemplo que usamos durante el entrenamiento, con las tres reglas de fusi칩n aprendidas:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```
La palabra `"bug"` ser치 tokenizada como `["b", "ug"]`. En cambio, `"mug"`, ser치 tokenizado como `["[UNK]", "ug"]` dado que la letra `"m"` no fue parte del vocabulario base. De la misma manera, la palabra `"thug"` ser치 tokenizada como `["[UNK]", "hug"]`: la letra `"t"` no est치 en el vocabulario base, y aplicando las reglas de fusi칩n resulta primero la fusi칩n de `"u"` y `"g"` y luego de `"hu"` and `"g"`.

<Tip>

九勇 **Ahora es tu turno!** 쮺칩mo crees ser치 tokenizada la palabra `"unhug"`?

</Tip>

## Implementando BPE[[implementing-bpe]]

Ahora echemos un vistazo a una implementaci칩n el algoritmo BPE. Esta no ser치 una versi칩n optimizada que puedes usar en corpus grande; s칩lo queremos mostrar el c칩digo para que puedas entender el algoritmo un poquito mejor. 

Primero necesitamos un corpus, as칤 que creemos uno simple con algunas oraciones:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

A continuaci칩n, necesitamos pre-tokenizar el corpus en palabras. Dado que estamos replicando un tokenizador BPE (como GPT-2), usaremos el tokenizdor `gpt2` para la pre-tokenizaci칩n:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Luego calculamos las frecuencias de cada palabra en el corpues mientras hacemos la pre-tokenizaci칩n:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, '말s': 2, '맚he': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, '맊hapter': 1,
    '마bout': 1, '맚okenization': 1, '맙ection': 1, '맙hows': 1, '맙everal': 1, '맚okenizer': 1, '마lgorithms': 1,
    'Hopefully': 1, ',': 1, '맟ou': 1, '망ill': 1, '막e': 1, '마ble': 1, '맚o': 1, '맛nderstand': 1, '맏ow': 1,
    '맚hey': 1, '마re': 1, '맚rained': 1, '마nd': 1, '많enerate': 1, '맚okens': 1})
```

El siguiente paso es calcualar el vocabulario base, formado por todos los caracteres usados en el corpus:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', '']
```

Tambi칠n agregamos el token especial usado por el modelo al inicio de ese vocabulario. En el caso de GPT-2, el 칰nico token especial es `"<|endoftext|>"`:

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

Ahora necesitamos separar cada palabra en caracteres individuales, para poder comenzar el entrenamiento:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

Ahora estamos listos para el entrenamiento, escribamos una funci칩n que calcule la frecuencia de cada par. Necesitaremos usar esto en cada paso del entrenamiento:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

Ahora miremos una parte de ese diccionario despu칠s de las separaciones iniciales:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('', 'i'): 2
('', 't'): 7
('t', 'h'): 3
```

Ahora, encontrar el par m치s frecuenta s칩lo toma un r치pido ciclo:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('', 't') 7
```

Por lo que la primera fusi칩n a aprender es `('', 't') -> '맚'`, y luego agregamos `'맚'` al vocabulario:

```python
merges = {("", "t"): "맚"}
vocab.append("맚")
```

Para continuar, necesitamos aplicar la fusi칩n en nuestro diccionario de divisiones (`splits` dictionary). Escribamos otra funci칩n para esto:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

Y podemos echar un vistazo al resultado de nuestra primera fusi칩n:

```py
splits = merge_pair("", "t", splits)
print(splits["맚rained"])
```

```python out
['맚', 'r', 'a', 'i', 'n', 'e', 'd']
```

Ahora tenemos todo lo que necesitamos para iterar hasta que aprendamos todas las fusiones que queramos. Apuntemos a un tama침o de vocabulario de 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

Como resultado, hemos aprendido 19 reglas de fusi칩n (el vocabulario inicial ten칤a un tama침o de 31 -- 30 caracteres del alfabeto, m치s el token especial):

```py
print(merges)
```

```python out
{('', 't'): '맚', ('i', 's'): 'is', ('e', 'r'): 'er', ('', 'a'): '마', ('맚', 'o'): '맚o', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('맚o', 'k'): '맚ok',
 ('맚ok', 'en'): '맚oken', ('n', 'd'): 'nd', ('', 'is'): '말s', ('맚', 'h'): '맚h', ('맚h', 'e'): '맚he',
 ('i', 'n'): 'in', ('마', 'b'): '마b', ('맚oken', 'i'): '맚okeni'}
```

And the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', '', '맚', 'is', 'er', '마', '맚o', 'en', 'Th', 'This', 'ou', 'se',
 '맚ok', '맚oken', 'nd', '말s', '맚h', '맚he', 'in', '마b', '맚okeni']
```

<Tip>

游눠 Usar `train_new_from_iterator()` en el mismo corpus no resultar치 en exactament el mismo vocabulario. Esto es porque cuando hay una elecci칩n del par m치s frecuente, seleccionamos el primero encontrado, mientras que la librer칤a 游뱅 Tokenizers selecciona el primero basado en sus IDs internos. 

</Tip>

Para tokenizar un nuevo texto lo pre-tokenizamos, lo separamos, luego aplicamos todas las reglas de fusi칩n aprendidas:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

Podemos intentar esto con cualquier texto compuesto de de caracteres del alfabeto:

```py
tokenize("This is not a token.")
```

```python out
['This', '말s', '', 'n', 'o', 't', '마', '맚oken', '.']
```

<Tip warning={true}>

丘멆잺 Nuestra implementaci칩n arrojar치 un error si hay un caracter desconocido dado que no hicimos nada para manejarlos. GPT-2 en realidad no tiene un token desconocido (es imposible obtener un caracter desconocido cuando se usa byte-level BPE), pero esto podr칤a ocurrir ac치 porque no inclu칤mos todos los posibles bytes en el vocabulario inicial. Este aspectode BPE va m치s all치 del alcance de est치 secci칩n, por lo que dejaremos los detalles fuera. 

</Tip>

Eso es todo para el algoritmo BPE! A continuaci칩n echaremos un vistazo a WordPiece.