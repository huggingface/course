# Entrenar un nuevo tokenizador a partir de uno existente[[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

Si un modelo de lenguaje no est√° disponible en el lenguaje en el que est√°s interesado, o si el corpus es muy diferente del lenguaje original en el que el modelo de lenguaje fue entrenado, es muy probable que quieras reentrenar el modelo desde cero utilizando un tokenizador adaptado a tus datos. Eso requerir√° entrenar un tokenizador nuevo en tu conjunto de datos. Pero, ¬øQu√© significa eso exactamente? Cuando revisamos los tokenizadores por primera vez en el [Cap√≠tulo 2](/course/chapter2), vimos que la mayor√≠a de los modelos basados en Transformers usan un algoritmo de _tokenizaci√≥n basado en subpalabras_. Para identificar qu√© subpalabras son de inter√©s y ocurren m√°s frecuentemente en el corpus deseado, el tokenizador necesita mirar de manera profunda todo el texto en el corpus -- un proceso al que llamamos *entrenamiento*. Las reglas exactas que gobiernan este entrenamiento dependen en el tipo de tokenizador usado, y revisaremos los 3 algoritmos principales m√°s tarde en el cap√≠tulo.


<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

‚ö†Ô∏è ¬°Entrenar un tokenizador no es lo mismo que entrenar un modelo! Entrenar un modelo utiliza `stochastic gradient descent` para minimizar la p√©rdida (`loss`) en cada lote (`batch`). Es un proceso aleatorio por naturaleza (lo que signifiva que hay que fijar semillas para poder obterner los mismos resultados cuando se realiza el mismo entrenamiento dos veces). Entrenar un tokenizador es un proceso estad√≠stico que intenta identificar cuales son las mejores subpalabras para un corpus dado, y las reglas exactas para elegir estas subpalabras dependen del algoritmo de tokenizaci√≥n. Es un proceso deterministico, lo que significa que siempre se obtienen los mismos resultados al entrenar el mismo algoritmo en el mismo corpus. 

</Tip>

## Ensamblando un Corpus[[assembling-a-corpus]]

Hay una API muy simple en ü§ó Transformers que se puede usar para entrenar un nuevo tokenizador con las mismas caracter√≠sticas que uno existente: `AutoTokenizer.train_new_from_iterator()`. Para verlo en acci√≥n, digamos que queremos entrenar GPT-2 desde cero, pero en lenguaje distinto al Ingl√©s. Nuestra primera tarea ser√° reunir muchos datos en ese lenguaje en un corpus de entrenamiento. Para proveer ejemplos que todos ser√°n capaces de entender no usaremos un lenguaje como el Ruso o el Chino, sino uno versi√≥n del ingl√©s m√°s especializado: C√≥digo en Python.

La librer√≠a [ü§ó Datasets](https://github.com/huggingface/datasets) nos puede ayudar a ensamblar un corpus de c√≥digo fuente en Python. Usaremos la t√≠pica funci√≥n `load_dataset()` para descargar y cachear el conjunto de datos [CodeSearchNet](https://huggingface.co/datasets/code_search_net). Este conjunto de datos fue creado para el [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) y contiene millones de funciones de librer√≠as open source en GitHub en varios lenguajes de programaci√≥n. Aqu√≠ cargaremos la parte del conjunto de datos que est√° en Python:

```py
from datasets import load_dataset

# Esto puede tomar varios minutos para cargarse, as√≠ que ¬°Agarra un t√© o un caf√© mientras esperas!
raw_datasets = load_dataset("code_search_net", "python")
```
Podemos echar un vistazo a la porci√≥n de entrenamiento para ver a qu√© columnas tenemos acceso:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

Podemos ver que el conjunto de datos separa los docstrings del c√≥digo y sugiere una tokenizaci√≥n de ambos. Ac√°, s√≥lo utilizaremos la columna `whole_func_string` para entrenar nuestro tokenizador. Podemos mirar un ejemplo de estas funciones utilizando alg√∫n √≠ndice en la porci√≥n de "train".

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```
lo cual deber√≠a imprimir lo siguiente:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

Lo primero que necesitamos hacer es transformar el dataset en un _iterador_ de listas de textos -- por ejemplo, una lista de listas de textos. utilizar listas de textos permitir√° que nuestro tokenizador vaya m√°s r√°pido (entrenar en batches de textos en vez de procesar textos de manera individual uno por uno), y deber√≠a ser un iterador si queremos evitar tener cargar todo en memoria de una sola vez. Si tu corpus es gigante, querr√°s tomar ventaja del hecho que ü§ó Datasets no carga todo en RAM sino que almacena los elementos del conjunto de datos en disco.
Hacer lo siguiente deber√≠a crear una lista de listas de 1000 textos cada una, pero cargando todo en memoria:

```py
# Don't uncomment the following line unless your dataset is small!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

Al usar un generador de Python, podemos evitar que Python cargue todo en memoria hasta que sea realmente necesario. Para crear dicho generador, solo necesitas reemplazar los corchetes con par√©ntesis:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```
Esta l√≠nea de c√≥digo no trae ning√∫n elemento del conjunto de datos; s√≥lo crea un objeto que se puede usar en Python con un ciclo `for`. Los textos s√≥lo ser√°n cargados cuando los necesites (es decir, cuando est√°s un paso del ciclo `for` que los requiera), y s√≥lo 1000 textos a la vez ser√°n cargados. De eso forma no agotar√°s toda tu memoria incluso si procesas un conjunto de datos gigante. 

El problema con un objeto generador es que s√≥lo se puede usar una vez. Entonces en vea que el siguiente c√≥digo nos entregue una lista de los primeros 10 d√≠gitos dos veces:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```
Nos lo entrega una vez, y luego una lista vac√≠a:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

Es por eso que definimos una funci√≥n que retorne un generador:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```
Tambi√©n puedes definir un generador dentro de un ciclo `for`utilizando el comando `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

el cual producir√° el mismo generador anterior, pero tambi√©n permitiendo usar l√≥gicas m√°s complejas de las que se puede hacer en un `list comprehension`.

## Entrenar un nuevo Tokenizador[[training-a-new-tokenizer]]

Ahora que tenemos nuestro corpus en la forma de un iterador de lotes de textos, estamos listos para entrenar un nuevo tokenizador. Para hacer esto, primero tenemos que cargar el tokenizador que queremos utilizar con nuestro modelo (en este caso, GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```
Aunque vamos a entrenar un nuevo tokenizador, es una buena idea hacer esto para evitar comenzar de cero completamente. De esta manera, no tendremos que especificar nada acerca del algoritmo de tokenizaci√≥n o de los tokens especiales que queremos usar; nuestro tokenizador ser√° exactamente el mismo que GPT-2, y lo √∫nico que cambiar√° ser√° el vocabulario, el cu√°l ser√° determinado por el entrenamiento en nuestro corpus. 

Primero, echemos un vistazo a c√≥mo este tokenizador tratar√° una funci√≥n de ejemplo:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'n', 'umbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two',
 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`', '."', '""', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

Este tokenizador tiene algunos s√≠mbolos especiales como `ƒ†` y `ƒä`, lo cual denota espacios y nuevas l√≠neas (saltos de l√≠neas) respectivamente. Como podemos ver, esto no es muy eficiente: el tokenizador retorna tokens individuales para cada espacio, cuando deber√≠a agrupar los niveles de indentaci√≥n (dado que tener grupos de cuatro u ocho espacios va a ser muy com√∫n en el uso de c√≥digo). Adem√°s separa el nombre de la funci√≥n de manera un poco extra√±a al no estar acostumbrado a ver palabras separadas con el caracter `_`.

Entrenemos nuestro nuevo tokenizador y veamos si resuelve nuestros problemas. Para esto usaremos el m√©todo `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```
Este comando puede tomar tiempo si tu corpus es muy largo, pero para este conjunto de datos de 1.6 GB de textos es muy r√°pido (1 minuto 16 segundos en un AMD Ryzen 9 3900X CPU con 12 n√∫cleos).

Nota que `AutoTokenizer.train_new_from_iterator()` s√≥lo funciona si el tokenizador que est√°s usando es un tokenizador r√°pido (_fast tokenizer_). C√≥mo ver√°s en la siguiente secci√≥n, la librer√≠a ü§ó Transformers contiene 2 tipos de tokenizadores: algunos est√°n escritos puramente en Python y otros (los r√°pidos) est√°n respaldados por la librer√≠a ü§ó Tokenizers, los cuales est√°n escritos en lenguaje de programaci√≥n [Rust](https://www.rust-lang.org). Python es el lenguaje mayormente usado en ciencia de datos y aplicaciones de deep learning, pero cuando algo necesita ser paralelizado para ser r√°pido, tiene que ser escrito en otro lenguaje. Por ejemplo, las multiplicaciones matriciales que est√°n en el coraz√≥n de los c√≥mputos de un modelo est√°n escritos en CUDA, una librer√≠a optimizada en C para GPUs. del computation are written in CUDA, an optimized C library for GPUs.

Entrenar un nuevo tokenizador en Python puro ser√≠a insoportablemente lento, raz√≥n pr la cual desarrollamos la librer√≠a ü§ó Tokenizers. Notar que de la misma manera que no tuviste que aprender el lenguaje CUDA para ser capaz de ejecutar tu modelo en un barch de inputs en una GPU, no necesitar√°s aprender Rust para usar los tokenizadores r√°pidos (_fast tokenizers_). La librer√≠a ü§ó Tokenizers provee bindings en Python para muchos m√©todos que internamente llaman trozos de c√≥digo en Rust; por ejemplo, para paralelizar el entrenamiento de un nuevo tokenizador o, como vimos en el [Cap√≠tulo 3](/course/chapter3), la tokenizaci√≥n de un batch de inputs. 

La mayor√≠a de los modelos Transformers tienen un tokenizador r√°pido (_Fast Tokenizer_) disponible (hay algunas excepciones que se pueden revisar [ac√°](https://huggingface.co/transformers/#supported-frameworks)), y la API `AutoTokenizer` siempre seleccionar un tokenizador r√°pido para ti en caso de estar disponible. En la siguiente secci√≥n echaremos un vistazo a algunas de las caracter√≠sticas especiales que tienen los tokenizadores r√°pidos, los cuales ser√°n realmente √∫tiles para tareas como clasificaci√≥n de tokens y question answering. Antes de sumergirnos en eso, probemos nuestro tokenizador reci√©n entrenado en nuestro ejemplo previo:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†"""', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`',
 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`."""', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b']
```

Ac√° nuevamente vemos los s√≠mbolos especiales `ƒ†` y `ƒä` que denotan espacios y nuevas l√≠neas (saltos de l√≠neas), pero tambi√©n podemos ver que nuestro tokenizador aprendi√≥ algunos tokens que son altamente espec√≠ficos para el corpus de funciones en Python: por ejemplo, est√° el token `ƒäƒ†ƒ†ƒ†` que representa una indentaci√≥n y un token `ƒ†"""` que representan la triple comilla para comenzar un docstring. El tokenizador tambi√©n divide correctamente los nombres de funciones usando `_`. Esta es una representaci√≥n m√°s compacta ya que utilizar un tokenizador com√∫n y corriente en ingl√©s en el mismo ejemplo nos dara una oraci√≥n m√°s larga:



```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```
Echemos un vistazo al siguiente ejemplo:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ƒ†Linear', 'Layer', '():', 'ƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'init', '__(', 'self', ',', 'ƒ†input', '_', 'size', ',',
 'ƒ†output', '_', 'size', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'weight', 'ƒ†=', 'ƒ†torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'ƒ†output', '_', 'size', ')', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'bias', 'ƒ†=', 'ƒ†torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ƒäƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'call', '__(', 'self', ',', 'ƒ†x', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',
 'ƒ†return', 'ƒ†x', 'ƒ†@', 'ƒ†self', '.', 'weights', 'ƒ†+', 'ƒ†self', '.', 'bias', 'ƒäƒ†ƒ†ƒ†ƒ†']
```

En adici√≥n al token correspondiente a la indentaci√≥n, tambi√©n podemos ver un token para la doble indentaci√≥n: `ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†`. Palabras espaciales del lenguaje Python como `class`, `init`, `call`, `self`, and `return` son tokenizadas como un s√≥lo token y podemos ver que adem√°s de dividir en `_` y `.`, el tokenizador correctamente divide incluso en nombres que usan camel-case: `LinearLayer` es tokenizado como `["ƒ†Linear", "Layer"]`.

## Guardar el Tokenizador[[saving-the-tokenizer]]


Para asegurarnos que podemos usar el tokenizador m√°s tarde, necesitamos guardar nuestro nuevo tokenizador. Al igual que los modelos, esto se hace con el m√©todo `save_pretrained()`. 

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

Esto crear√° una nueva carpeta llamada *code-search-net-tokenizer*, la cual contendr√° todos los archivos que el tokenizador necesita para ser cargado. Si quieres compartir el tokenizador con tus colegas y amigos, puedes subirlo al Hub logeando en tu cuenta. Si est√°s trabajando en notebooks, hay una funci√≥n conveniente para ayudarte a hacer esto:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Esto mostrar√° un widget donde puedes ingresar tus credenciales de Hugging Face. En caso de no estar usando un notebook, puedes escribir la siguiente l√≠nea en tu terminal:

```bash
huggingface-cli login
```

Una vez logueado puedes enviar tu tokenizador al Hub ejecutando el siguiente comando::

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```
Esto crear√° un nuevo repositorio en tu namespace con el nombre `code-search-net-tokenizer`, conteniendo el archivo del tokenizador. Luego puedes cargar tu tokenizador desde donde quieras utilizando m√©todo `from_pretrained()`.

```py
# Replace "huggingface-course" below with your actual namespace to use your own tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```
Ya est√°s listo para entrenar un modelo de lenguaje desde cero y hacer fine-tuning en la tarea que desees. Llegaremos a eso en el [Cap√≠tulo 7](/course/chapter7), pero primero en el resto del cap√≠tulo miraremos m√°s de cerca los tokenizadores r√°pidos (_Fast Tokenizers_) y explorar en detalle lo que pasa en realidad pasa cuando llamamos al m√©todo `train_new_from_iterator()`.
