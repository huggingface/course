# Entrenar un nuevo tokenizador a partir de uno existente[[training-a-new-tokenizer-from-an-old-one]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb"},
]} />

Si un modelo de lenguaje no est치 disponible en el lenguaje en el que est치s interesado, o si el corpus es muy diferente del lenguaje original en el que el modelo de lenguaje fue entrenado, es muy probable que quieras reentrenar el modelo desde cero utilizando un tokenizador adaptado a tus datos. Eso requerir치 entrenar un tokenizador nuevo en tu conjunto de datos. Pero, 쯈u칠 significa eso exactamente? Cuando revisamos los tokenizadores por primera vez en el [Cap칤tulo 2](/course/chapter2), vimos que la mayor칤a de los modelos basados en Transformers usan un algoritmo de _tokenizaci칩n basado en subpalabras_. Para identificar qu칠 subpalabras son de inter칠s y ocurren m치s frecuentemente en el corpus deseado, el tokenizador necesita mirar de manera profunda todo el texto en el corpus -- un proceso al que llamamos *entrenamiento*. Las reglas exactas que gobiernan este entrenamiento dependen en el tipo de tokenizador usado, y revisaremos los 3 algoritmos principales m치s tarde en el cap칤tulo.


<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

丘멆잺 춰Entrenar un tokenizador no es lo mismo que entrenar un modelo! Entrenar un modelo utiliza `stochastic gradient descent` para minimizar la p칠rdida (`loss`) en cada lote (`batch`). Es un proceso aleatorio por naturaleza (lo que signifiva que hay que fijar semillas para poder obterner los mismos resultados cuando se realiza el mismo entrenamiento dos veces). Entrenar un tokenizador es un proceso estad칤stico que intenta identificar cuales son las mejores subpalabras para un corpus dado, y las reglas exactas para elegir estas subpalabras dependen del algoritmo de tokenizaci칩n. Es un proceso deterministico, lo que significa que siempre se obtienen los mismos resultados al entrenar el mismo algoritmo en el mismo corpus. 

</Tip>

## Ensamblando un Corpus[[assembling-a-corpus]]

Hay una API muy simple en 游뱅 Transformers que se puede usar para entrenar un nuevo tokenizador con las mismas caracter칤sticas que uno existente: `AutoTokenizer.train_new_from_iterator()`. Para verlo en acci칩n, digamos que queremos entrenar GPT-2 desde cero, pero en lenguaje distinto al Ingl칠s. Nuestra primera tarea ser치 reunir muchos datos en ese lenguaje en un corpus de entrenamiento. Para proveer ejemplos que todos ser치n capaces de entender no usaremos un lenguaje como el Ruso o el Chino, sino uno versi칩n del ingl칠s m치s especializado: C칩digo en Python.

La librer칤a [游뱅 Datasets](https://github.com/huggingface/datasets) nos puede ayudar a ensamblar un corpus de c칩digo fuente en Python. Usaremos la t칤pica funci칩n `load_dataset()` para descargar y cachear el conjunto de datos [CodeSearchNet](https://huggingface.co/datasets/code_search_net). Este conjunto de datos fue creado para el [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) y contiene millones de funciones de librer칤as open source en GitHub en varios lenguajes de programaci칩n. Aqu칤 cargaremos la parte del conjunto de datos que est치 en Python:

```py
from datasets import load_dataset

# Esto puede tomar varios minutos para cargarse, as칤 que 춰Agarra un t칠 o un caf칠 mientras esperas!
raw_datasets = load_dataset("code_search_net", "python")
```
Podemos echar un vistazo a la porci칩n de entrenamiento para ver a qu칠 columnas tenemos acceso:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

Podemos ver que el conjunto de datos separa los docstrings del c칩digo y sugiere una tokenizaci칩n de ambos. Ac치, s칩lo utilizaremos la columna `whole_func_string` para entrenar nuestro tokenizador. Podemos mirar un ejemplo de estas funciones utilizando alg칰n 칤ndice en la porci칩n de "train".

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```
lo cual deber칤a imprimir lo siguiente:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

Lo primero que necesitamos hacer es transformar el dataset en un _iterador_ de listas de textos -- por ejemplo, una lista de listas de textos. utilizar listas de textos permitir치 que nuestro tokenizador vaya m치s r치pido (entrenar en batches de textos en vez de procesar textos de manera individual uno por uno), y deber칤a ser un iterador si queremos evitar tener cargar todo en memoria de una sola vez. Si tu corpus es gigante, querr치s tomar ventaja del hecho que 游뱅 Datasets no carga todo en RAM sino que almacena los elementos del conjunto de datos en disco.
Hacer lo siguiente deber칤a crear una lista de listas de 1000 textos cada una, pero cargando todo en memoria:

```py
# Don't uncomment the following line unless your dataset is small!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

Al usar un generador de Python, podemos evitar que Python cargue todo en memoria hasta que sea realmente necesario. Para crear dicho generador, solo necesitas reemplazar los corchetes con par칠ntesis:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```
Esta l칤nea de c칩digo no trae ning칰n elemento del conjunto de datos; s칩lo crea un objeto que se puede usar en Python con un ciclo `for`. Los textos s칩lo ser치n cargados cuando los necesites (es decir, cuando est치s un paso del ciclo `for` que los requiera), y s칩lo 1000 textos a la vez ser치n cargados. De eso forma no agotar치s toda tu memoria incluso si procesas un conjunto de datos gigante. 

El problema con un objeto generador es que s칩lo se puede usar una vez. Entonces en vea que el siguiente c칩digo nos entregue una lista de los primeros 10 d칤gitos dos veces:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```
Nos lo entrega una vez, y luego una lista vac칤a:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

Es por eso que definimos una funci칩n que retorne un generador:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```
Tambi칠n puedes definir un generador dentro de un ciclo `for`utilizando el comando `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

el cual producir치 el mismo generador anterior, pero tambi칠n permitiendo usar l칩gicas m치s complejas de las que se puede hacer en un `list comprehension`.

## Entrenar un nuevo Tokenizador[[training-a-new-tokenizer]]

Ahora que tenemos nuestro corpus en la forma de un iterador de lotes de textos, estamos listos para entrenar un nuevo tokenizador. Para hacer esto, primero tenemos que cargar el tokenizador que queremos utilizar con nuestro modelo (en este caso, GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```
Aunque vamos a entrenar un nuevo tokenizador, es una buena idea hacer esto para evitar comenzar de cero completamente. De esta manera, no tendremos que especificar nada acerca del algoritmo de tokenizaci칩n o de los tokens especiales que queremos usar; nuestro tokenizador ser치 exactamente el mismo que GPT-2, y lo 칰nico que cambiar치 ser치 el vocabulario, el cu치l ser치 determinado por el entrenamiento en nuestro corpus. 

Primero, echemos un vistazo a c칩mo este tokenizador tratar치 una funci칩n de ejemplo:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', '마dd', '_', 'n', 'umbers', '(', 'a', ',', '막', '):', '캙', '', '', '', '"""', 'Add', '맚he', '맚wo',
 '맕umbers', '`', 'a', '`', '마nd', '`', 'b', '`', '."', '""', '캙', '', '', '', 'return', '마', '+', '막']
```

Este tokenizador tiene algunos s칤mbolos especiales como `` y `캙`, lo cual denota espacios y nuevas l칤neas (saltos de l칤neas) respectivamente. Como podemos ver, esto no es muy eficiente: el tokenizador retorna tokens individuales para cada espacio, cuando deber칤a agrupar los niveles de indentaci칩n (dado que tener grupos de cuatro u ocho espacios va a ser muy com칰n en el uso de c칩digo). Adem치s separa el nombre de la funci칩n de manera un poco extra침a al no estar acostumbrado a ver palabras separadas con el caracter `_`.

Entrenemos nuestro nuevo tokenizador y veamos si resuelve nuestros problemas. Para esto usaremos el m칠todo `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```
Este comando puede tomar tiempo si tu corpus es muy largo, pero para este conjunto de datos de 1.6 GB de textos es muy r치pido (1 minuto 16 segundos en un AMD Ryzen 9 3900X CPU con 12 n칰cleos).

Nota que `AutoTokenizer.train_new_from_iterator()` s칩lo funciona si el tokenizador que est치s usando es un tokenizador r치pido (_fast tokenizer_). C칩mo ver치s en la siguiente secci칩n, la librer칤a 游뱅 Transformers contiene 2 tipos de tokenizadores: algunos est치n escritos puramente en Python y otros (los r치pidos) est치n respaldados por la librer칤a 游뱅 Tokenizers, los cuales est치n escritos en lenguaje de programaci칩n [Rust](https://www.rust-lang.org). Python es el lenguaje mayormente usado en ciencia de datos y aplicaciones de deep learning, pero cuando algo necesita ser paralelizado para ser r치pido, tiene que ser escrito en otro lenguaje. Por ejemplo, las multiplicaciones matriciales que est치n en el coraz칩n de los c칩mputos de un modelo est치n escritos en CUDA, una librer칤a optimizada en C para GPUs. del computation are written in CUDA, an optimized C library for GPUs.

Entrenar un nuevo tokenizador en Python puro ser칤a insoportablemente lento, raz칩n pr la cual desarrollamos la librer칤a 游뱅 Tokenizers. Notar que de la misma manera que no tuviste que aprender el lenguaje CUDA para ser capaz de ejecutar tu modelo en un barch de inputs en una GPU, no necesitar치s aprender Rust para usar los tokenizadores r치pidos (_fast tokenizers_). La librer칤a 游뱅 Tokenizers provee bindings en Python para muchos m칠todos que internamente llaman trozos de c칩digo en Rust; por ejemplo, para paralelizar el entrenamiento de un nuevo tokenizador o, como vimos en el [Cap칤tulo 3](/course/chapter3), la tokenizaci칩n de un batch de inputs. 

La mayor칤a de los modelos Transformers tienen un tokenizador r치pido (_Fast Tokenizer_) disponible (hay algunas excepciones que se pueden revisar [ac치](https://huggingface.co/transformers/#supported-frameworks)), y la API `AutoTokenizer` siempre seleccionar un tokenizador r치pido para ti en caso de estar disponible. En la siguiente secci칩n echaremos un vistazo a algunas de las caracter칤sticas especiales que tienen los tokenizadores r치pidos, los cuales ser치n realmente 칰tiles para tareas como clasificaci칩n de tokens y question answering. Antes de sumergirnos en eso, probemos nuestro tokenizador reci칠n entrenado en nuestro ejemplo previo:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', '마dd', '_', 'numbers', '(', 'a', ',', '막', '):', '캙먟먟', '"""', 'Add', '맚he', '맚wo', '맕umbers', '`',
 'a', '`', '마nd', '`', 'b', '`."""', '캙먟먟', 'return', '마', '+', '막']
```

Ac치 nuevamente vemos los s칤mbolos especiales `` y `캙` que denotan espacios y nuevas l칤neas (saltos de l칤neas), pero tambi칠n podemos ver que nuestro tokenizador aprendi칩 algunos tokens que son altamente espec칤ficos para el corpus de funciones en Python: por ejemplo, est치 el token `캙먟먟` que representa una indentaci칩n y un token `"""` que representan la triple comilla para comenzar un docstring. El tokenizador tambi칠n divide correctamente los nombres de funciones usando `_`. Esta es una representaci칩n m치s compacta ya que utilizar un tokenizador com칰n y corriente en ingl칠s en el mismo ejemplo nos dara una oraci칩n m치s larga:



```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```
Echemos un vistazo al siguiente ejemplo:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'Linear', 'Layer', '():', '캙먟먟', '맋ef', '__', 'init', '__(', 'self', ',', '말nput', '_', 'size', ',',
 '맖utput', '_', 'size', '):', '캙먟먟먟먟먟먟', '맙elf', '.', 'weight', '=', '맚orch', '.', 'randn', '(', 'input', '_',
 'size', ',', '맖utput', '_', 'size', ')', '캙먟먟먟먟먟먟', '맙elf', '.', 'bias', '=', '맚orch', '.', 'zeros', '(',
 'output', '_', 'size', ')', '캙캙먟먟', '맋ef', '__', 'call', '__(', 'self', ',', '맞', '):', '캙먟먟먟먟먟먟',
 'return', '맞', '@', '맙elf', '.', 'weights', '+', '맙elf', '.', 'bias', '캙먟먟먟']
```

En adici칩n al token correspondiente a la indentaci칩n, tambi칠n podemos ver un token para la doble indentaci칩n: `캙먟먟먟먟먟먟`. Palabras espaciales del lenguaje Python como `class`, `init`, `call`, `self`, and `return` son tokenizadas como un s칩lo token y podemos ver que adem치s de dividir en `_` y `.`, el tokenizador correctamente divide incluso en nombres que usan camel-case: `LinearLayer` es tokenizado como `["Linear", "Layer"]`.

## Guardar el Tokenizador[[saving-the-tokenizer]]


Para asegurarnos que podemos usar el tokenizador m치s tarde, necesitamos guardar nuestro nuevo tokenizador. Al igual que los modelos, esto se hace con el m칠todo `save_pretrained()`. 

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

Esto crear치 una nueva carpeta llamada *code-search-net-tokenizer*, la cual contendr치 todos los archivos que el tokenizador necesita para ser cargado. Si quieres compartir el tokenizador con tus colegas y amigos, puedes subirlo al Hub logeando en tu cuenta. Si est치s trabajando en notebooks, hay una funci칩n conveniente para ayudarte a hacer esto:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Esto mostrar치 un widget donde puedes ingresar tus credenciales de Hugging Face. En caso de no estar usando un notebook, puedes escribir la siguiente l칤nea en tu terminal:

```bash
huggingface-cli login
```

Una vez logueado puedes enviar tu tokenizador al Hub ejecutando el siguiente comando::

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```
Esto crear치 un nuevo repositorio en tu namespace con el nombre `code-search-net-tokenizer`, conteniendo el archivo del tokenizador. Luego puedes cargar tu tokenizador desde donde quieras utilizando m칠todo `from_pretrained()`.

```py
# Replace "huggingface-course" below with your actual namespace to use your own tokenizer
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```
Ya est치s listo para entrenar un modelo de lenguaje desde cero y hacer fine-tuning en la tarea que desees. Llegaremos a eso en el [Cap칤tulo 7](/course/chapter7), pero primero en el resto del cap칤tulo miraremos m치s de cerca los tokenizadores r치pidos (_Fast Tokenizers_) y explorar en detalle lo que pasa en realidad pasa cuando llamamos al m칠todo `train_new_from_iterator()`.
