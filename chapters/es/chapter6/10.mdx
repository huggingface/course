<!-- DISABLE-FRONTMATTER-SECTIONS -->

# Quiz de Final de Cap칤tulo[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

Probemos lo que aprendimos en este cap칤tulo!

### 1. Cuando deber칤a entrenar un nuevo tokenizador?

<Question
	choices={[
		{
			text: "Cuando tu conjunto de datos es similar al usado por un modelo pre-entrenado existente, y t칰 quieres pre-entrenar un nuevo modelo.",
			explain: "En este caso, para ahorrar tiempo y recursos computacionales, una mejor opci칩n ser칤a usar el mismo tokenizador que el modelo pre-entrenado y hacerle fine-tuning."
		},
		{
			text: "Cuando tu conjunto de datos es similar al usado por un modelo pre-entrenado existente, y quieres hacerle fine-tuning a un nuevo modelo usando este modelo pre-entrenado.",
			explain: "Para hacer fine-tuning a un modelo a partir de un modelo pre-entrenado, deber칤as siempre usar el mismo tokenizador."
		},
		{
			text: "Cuando tu dataset es diferente del que se utiliz칩 en el modelo pre-entrenado existente, y quieres pre-entrenar un nuevo modelo.",
			explain: "Correcto! En este caso no hay ning칰na ventaje en usar el mismo tokenizador.",
            correct: true
		},
        {
			text: "Cuando tu dataset es diferente del que se utiliz칩 en el modelo pre-entrenado existente, pero quieres hacer fine-tuning a un nuevo modelo usando el modelo pre-entrenado.",
			explain: "Para hacer fine-tuning a un modelo a partir de un modelo pre-entrenado, siempre deber칤as usar el mismo tokenizador."
		}
	]}
/>

### 2. Cu치l es la ventaja de usar un generador de listas de textos comparado con una lista de listas de textos al usar `train_new_from_iterator()`?

<Question
	choices={[
		{
			text: "Ese es el 칰nico tipo que el m칠todo <code>train_new_from_iterator()</code> acepta.",
			explain: "Una lista de listas de textos es un tipo particular de generador de listas de textos, por lo que el m칠todo aceptar치 esto tambi칠n. Intenta de nuevo!"
		},
		{
			text: "Evitar치s cargar todo el conjunto de datos en memoria de una s칩la vez.",
			explain: "Correcto! Cada lote de textos ser치 liberado de la memoria al ir iterando, y la ganancia ser치 especialmente visible si usas la librer칤a 游뱅 Datasets para almacenar tus textos.",
			correct: true
		},
		{
			text: "Esto permite que la librer칤a 游뱅 Tokenizers library use multiprocesamiento.",
			explain: "No, usar치 multiprocesamiento en ambos casos."
		},
        {
			text: "El tokenizador que entrenar치s generar치 mejores textos.",
			explain: "El tokenizador no genera texto -- est치s confundi칠ndolo con un modelo de lenguaje?"
		}
	]}
/>

### 3. Cu치les son las ventajas de utilizar un tokenizador "r치pido"?

<Question
	choices={[
		{
			text: "Puede procesar las entradas/inputs m치s r치pido que un tokenizador lento cuando empaquetas muchas entradas/inputs en lotes.",
			explain: "Correcto! Gracias al paralelismo implementado en Rust, ser치 m치s r치pido en lotes de entradas/inputs. En qu칠 otro beneficio puedes pensar?",
			correct: true
		},
		{
			text: "Los tokenizadores r치pidos siempre tokenizan m치s r치pidos que sus contrapartes lentas.",
			explain: "Un tokenizador r치pido puede ser en realidad m치s lento cuando s칩lo le d치s uno o pocos textos, dado que no puede usar paralelismo."
		},
		{
			text: "Puede aplicar relleno (padding) y truncamiento.",
			explain: "Verdadero, pero los tokenizadores lentos tambi칠n hacen eso."
		},
        {
			text: "Tiene algunas caracter칤sticas adicionales que permiten mapear tokens a la porci칩n de texto que los cre칩.",
			explain: "De hecho -- esos son los offset mappings. Aunque esa no es la 칰nica ventaja.",
			correct: true
		}
	]}
/>

### 4. Como hace el pipeline `token-classification` para manejar entidades que se extienden a varios tokens?

<Question
	choices={[
		{
			text: "Las entidades con las mismas etiquetas son fusionadas en una s칩lo entidad.",
			explain: "Eso es sobresimplificar las cosas un poco. Intenta de nuevo!"
		},
		{
			text: "Hay una etiqueta para el inicio de una entidad y una etiqueta para la continuaci칩n de una entidad.",
			explain: "Correcto!",
			correct: true
		},
		{
			text: "En una palabra dada, mientas el primer token tenga una etiquera de entidad, la palabra completa es considerada etiquetada con dicha entidad.",
			explain: "Esa es una estrategia para manipular entidades. Qu칠 otras respuestan aplican ac치?",
			correct: true
		},
        {
			text: "Cuando un token tiene la etiqueta de una entidad dada, cualquier otro token consecutivo con la misma etiqueta ser치 considerada parte de la misma entidad, a menos que sea etiquetada como el inicio de una nueva entidad.",
			explain: "Esa es la manera m치s com칰n de agrupar entidades -- aunque, no es la 칰nica respuesta correcta.",
			correct: true
		}
	]}
/>

### 5. C칩mo hace el pipeline de `question-answering` para manejar contextos largos?

<Question
	choices={[
		{
			text: "En realidad no lo hace, ya que trunca los contextos largos al largo m치ximo aceptado por el modelo.",
			explain: "Hay un truco que puedes usar para manejar contextos largos. Te acuerdas cu치l es?"
		},
		{
			text: "Separa el contexto en varias partes y promedia los resultados obtenidos.",
			explain: "No, no har칤a sentido promedias los resultados, ya que algunas partes del contexto no incluir치n la respuesta."
		},
		{
			text: "Separa el contexto en varias partes (con traslape) y encuentra el puntaje m치ximo para una respuesta en cada parte.",
			explain: "Esa es la respuesta correcta!",
			correct: true
		},
        {
			text: "Separa el contexto en varias partes (sin traslape, por eficiencia) y encuentra el puntaje m치ximo para una respuesta en cada parte.",
			explain: "No, incluye algo de traslape entre las partes para evitar la situaci칩n donde la respuesta estar칤a separada entre las partes."
		}
	]}
/>

### 6. Qu칠 es la normalizaci칩n?

<Question
	choices={[
		{
			text: "Es cualquier limpieza que el tokenizador realiza en los extos en las etapas iniciales.",
			explain: "Eso es correcto -- por ejemplo, podr칤a incolucrar la remoci칩n de acentos o espacios en blancos, o transformar a min칰sculas las entradas/inputs.",
			correct: true
		},
		{
			text: "Es una t칠cnica de aumento de datos que involucra hacer el texto m치s normal removiendo palabras raras.",
			explain: "Eso es incorrecto! Intenta de nuevo."
		},
		{
			text: "Es el paso final de post-procesamiento donde el tokenizador agrega los tokens especiales.",
			explain: "Esa etapa se llama simplemente post-procesamiento."
		},
        {
			text: "Es cuando los embeddings se llevan a media 0 y desviaci칩n est치ndar 1, restando la media y dividiendo la desviaci칩n est치ndar.",
			explain: "Ese proceso es com칰nmente llamado normalizaci칩n cuando se aplica a valores de p칤xel en visi칩n computacional, pero no es lo que significa normalizacion en el contexto de NLP."
		}
	]}
/>

### 7. Qu칠 es la pre-tokenizaci칩n para un tokenizador de subpalabra?

<Question
	choices={[
		{
			text: "Es un paso antes de la tokenizaci칩n, donde se aplica aumento de datos (como enmascaramiento aleatorio (random masking)).",
			explain: "No, ese paso es parte del pre-procesamiento."
		},
		{
			text: "Es el paso antes de la tokenizaci칩n, donde las operaciones de limpieza deseada son aplicados al texto.",
			explain: "No, ese es el paso de normalizaci칩n."
		},
		{
			text: "Es el paso antes que el modelo de tokenizaci칩n sea aplicado para separar la entrada/input en palabras.",
			explain: "Esa es la respuesta correcta!",
			correct: true
		},
        {
			text: "Es el paso antes de que el tokenizador se aplique para separar el input en tokens.",
			explain: "No, separar en tokens es parte del trabajo del modelo de tokenizaci칩n."
		}
	]}
/>

### 8. Selecciona las afirmaciones que aplican para el modelo de tokenizaci칩n BPE.

<Question
	choices={[
		{
			text: "BPE es un algoritmo de tokenizaci칩n por subpalabras que comienza con un vocabulario peque침o y aprende reglas de fusi칩n.",
			explain: "Ese es de hecho el caso!",
			correct: true
		},
		{
			text: "BPE es un algoritmo de tokenizaci칩n que comienza con un vocabulario grande y de manera progresiva remueve tokens de 칠l.",
			explain: "No, ese es el m칠todo que siguien un algoritmo de tokenizaci칩n diferente."
		},
		{
			text: "El tokenizador BPE aprende reglas de fusi칩n fusionando el parte de tokens que es el m치s frecuente.",
			explain: "Eso es correcto!",
			correct: true
		},
		{
			text: "Un tokenizador BPE aprende una regla de fusi칩n fusionando el par de tokens que maximiza un puntaje que privilegia pares frecuentes con partes individuales menos frecuentes.",
			explain: "No, esa es la estrategia aplicada por otro algoritmo de tokenizaci칩n."
		},
		{
			text: "BPE tokeniza palabras en subpalabras separ치ndolas en caracteres y luego aplicando reglas de fusi칩n.",
			explain: "Eso es correcto!",
			correct: true
		},
		{
			text: "BPE tokeniza palabras en subpalabras encontrando la subpalabra m치s larga partiendo desde el inicio que est치 en el vocabulario, luego repite el proceso para el resto del texto.",
			explain: "No, esa es la manera en que otro algoritmo de tokenizaci칩n hace las cosas."
		},
	]}
/>

### 9. Selecciona las afirmaciones que aplican para el modelo de tokenizacion WordPiece.

<Question
	choices={[
		{
			text: "WordPiece es un algoritmo de tokenizaci칩n de subpalabras que comienza con un vocabulario peque침o y aprende reglas de fusi칩n.",
			explain: "Este es el caso de hecho!",
			correct: true
		},
		{
			text: "WordPiece un algoritmo de tokenizaci칩n de subpalabras que comienza con un vocabulario grande y de manera progresiva remueve tokens de 칠l.",
			explain: "No, ese el m칠todo tomado por un algoritmo de tokenizaci칩n diferente."
		},
		{
			text: "Los tokenizadores WordPiece aprenden reglas de fusi칩n fusionando el par de tokens m치s frecuentes.",
			explain: "No, esa es la estrategia aplicada por otro algoritmo de tokenizaci칩n."
		},
		{
			text: "Un tokenizador WordPiece aprende una regla de fusi칩n fusionando el par de tokens que maximiza un puntaje que privilegia pares frecuentes con partes individuales menos frecuentes.",
			explain: "Eso es correcto!",
			correct: true
		},
		{
			text: "WordPiece tokeniza palabras en subpalabras encontrando la segmentaci칩n en tokens m치s probable, de acuerdo al modelo.",
			explain: "No, as칤 es como otro algoritmo de tokenizaci칩n funciona."
		},
		{
			text: "WordPiece tokeniza palabras en subpalabras encontrando la subpalabra m치as larga partiendo desde el inicio que est치 en el vocabularoi, luego repite el proceso para el resto del texto.",
			explain: "S칤, as칤 es como WordPiece procede para la codificaci칩n.",
			correct: true
		},
	]}
/>

### 10. Selecciona las afirmaciones que aplican para el modelo de tokenizaci칩n Unigram.

<Question
	choices={[
		{
			text: "Unigram es un algoritmo de tokenizaci칩n que comienza con un vocabulario peque침o y aprende reglas de fusi칩n.",
			explain: "No, ese el m칠todo tomado por otro algoritmo de tokenizaci칩n."
		},
		{
			text: "Unigram es un algoritmo de tokenizaci칩n que comienza con un vocabulario grande y progresivamente remueve tokens de 칠l.",
			explain: "Eso es correcto!",
			correct: true
		},
		{
			text: "Unigram adapta su vocabulario minimizando una p칠rdia calculada sobre el corpus completo.",
			explain: "Eso est치 correcto!",
			correct: true
		},
		{
			text: "Unigram adapta su vocabularo manteneiendo las subpalabras m치s frecuentes.",
			explain: "No, esto es incorrecto."
		},
		{
			text: "Unigram tokeniza palabras en subpalabras encontrando la segmentaci칩n en tokens m치s probable, de acuerdo al modelo.",
			explain: "Eso es correcto!",
			correct: true
		},
		{
			text: "Unigram tokeniza palabras en subpalabras separandolas en caracteres, luego aplicando caracteres, luego aplicando reglas de fusi칩n.",
			explain: "No, as칤 es como funciona otro algoritmo de tokenizaci칩n."
		},
	]}
/>
