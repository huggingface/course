# Introducci√≥n[[introduction]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

En el [Cap√≠tulo 3](/course/chapter3), revisamos como hacer fine-tuning a un modelo para una tarea dada. Cuando hacemos eso, usamos el mismo tokenizador con el que el modelo fue entrenado -- pero, ¬øQu√© hacemos cuando queremos entrenar un modelo desde cero? En estos casos, usar un tokenizador que fue entrenado en un corpus con otro dominio u otro lenguaje t√≠picamente no es lo m√°s √≥ptimo. Por ejemplo un tokenizador que es entrenado en un corpus en Ingl√©s tendr√° un desempe√±o pobre en un corpus de textos en Japon√©s porque el uso de los espacios y de la puntuaci√≥n es muy diferente entre los dos lenguajes.


En este cap√≠tulo, aprender√°s como entrenar un tokenizador completamente nuevo en un corpus, para que luego pueda ser usado para pre-entrenar un modelo de lenguaje. Todo esto ser√° hecho con la ayuda de la librer√≠a [ü§ó Tokenizers](https://github.com/huggingface/tokenizers), la cual provee tokenizadores r√°pidos (_fast tokenizers_) en la librer√≠a [ü§ó Transformers](https://github.com/huggingface/transformers). Miraremos de cerca todas las caracter√≠sticas que la provee la librer√≠a, y explorar c√≥mo los tokenizadores r√°pidos (fast tokenizers) difieren de las versiones "lentas".

Los temas a cubrir incluyen:

* C√≥mo entrenar un tokenizador nuevo similar a los usados por un checkpoint dado en un nuevo corpus de texto.
* Las caracter√≠sticas especiales de los tokenizador r√°pidos ("fast tokenizers").
* Las diferencias entre los tres principales algoritmos de tokenizaci√≥n usados en PLN hoy.
* Como construir un tokenizador desde cero con la librer√≠a ü§ó Tokenizers y entrenarlo en datos. 

Las t√©cnicas presentadas en este cap√≠tulo te preparar√°n para la secci√≥n en el [Cap√≠tulo 7](/course/chapter7/6) donde estudiaremos c√≥mo crear un modelo de lenguaje para C√≥digo Fuente en Python. Comenzaremos en primer lugar revisando qu√© significa "entrenar" un tokenizador.