# Normalizaci贸n y pre-tokenizaci贸n[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

Antes de sumergirnos m谩s profundamente en los tres algoritmos m谩s comunes de tokenizaci贸n usados con los modelos transformers (Byte-Pair Encoding [BPE], WordPiece, and Unigram), primero miraremos el preprocesamiento que cada tokenizador aplica al texto. Ac谩 una descripci贸n general de los pasos en el pipeline de tokenizaci贸n:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

Antes de dividir un texto en subtokens (de acuerdo a su modelo), el tokenizador realiza dos pasos: _normalizaci贸n_ y _pre-tokenizaci贸n_.

## Normalizaci贸n[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

El paso de normalizaci贸n involucra una limpieza general, como la remoci贸n de espacios en blanco innecesario, transformar a min煤sculas, y/o remoci贸n de acentos. Si est谩s familiarizado con [Normalizaci贸n Unicode](http://www.unicode.org/reports/tr15/) (como NFC o NFKC), esto es algo que el tokenizador tambi茅n puede aplicar.

Los tokenizadores de la librer铆a  Transformers tienen un atributo llamado `backend_tokenizer` que provee acceso al tokenizador subyacente de la librer铆a  Tokenizers:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

El atributo `normalizer` del objeto `tokenizer` tiene un m茅todo `normalize_str()` que puede puedes usar para ver c贸mo la normalizaci贸n se realiza:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("H茅ll貌 h么w are 眉?"))
```

```python out
'hello how are u?'
```

En este ejemplo, dado que elegimos el punto de control (checkpoint) `bert-base-uncased`, la normalizaci贸n aplic贸 transformaci贸n a min煤sculas y remoci贸n de acentos. 

<Tip>

锔 **Int茅ntalo!** Carga un tokenizador desde el punto de control (checkpoint)`bert-base-cased` y p谩sale el mismo ejemplo. Cu谩les son las principales diferencias que puedes ver entre las versiones cased y uncased de los tokenizadores?

</Tip>

## Pre-tokenizaci贸n[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

Como veremos en las siguientes secciones, un tokenizador no puede ser entrenado en un texto tal como viene as铆 nada m谩s. En vez de eso, primero necesitamos separar los textos en entidades m谩s peque帽as, como palabras. Ah铆 es donde el paso de pre-tokenizaci贸n entra en juego. Como vimos en el [Cap铆tulo 2](/course/chapter2), un tokenizador basado en palabras (word-based) puede dividir el texto en palabras separando en espacios en blanco y puntuaci贸n. Esas palabras ser谩n las fronteras de los subtokens que el tokenizador aprende durante su entrenamiento. 

Para ver qu茅 tan r谩pido un tokenizador r谩pido (fast tokenizer) realiza la pre-tokenizaci贸n, podemos usar el m茅todo `pre_tokenize_str()` del atributo `pre_tokenizer` del objeto `tokenizer`:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

Notar como el tokenizador ya lleva registro de los offsets, el cual nos entrega el mapeo de offsets que usamos en la secci贸n anterior. Ac谩 el tokenizador ignora los dos espacios y los reemplaza con uno s贸lo, pero el offset salta entre `are` y `you` para tomar eso en cuenta. 

Dado que estamos usando un tokenizador BERT, la pre-tokenizaci贸n involucra separar en espacios en blanco y puntuaci贸n. Otros tokenizadores pueden tener distintas reglas para esta etapa. Por ejemplo, si usamor el tokenizador de GPT-2:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

dividir谩 en espacios en blanco y puntuaci贸n tambi茅n, pero mantendr谩 los espacios y los reemplazar谩 con el s铆mbolo ``, permitiendo recobrar los espacios originales en el caso de decodificar los tokens:

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (6, 10)), ('are', (10, 14)), ('', (14, 15)), ('you', (15, 19)),
 ('?', (19, 20))]
```

Tambi茅n notar que a diferencia del tokenizador BERT, este tokenizador no ignora los espacios dobles. 

Para el 煤ltimo ejemplo, tenemos que mirar el tokenizador T5, el cu谩l est谩 basado en el algoritmo SentencePiece:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello,', (0, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you?', (16, 20))]
```

Al igual que el tokenizador GPT-2, este mantiene los espacios y los reemplaza con un token espec铆fico (`_`), pero el tokenizador T5 s贸lo divide en espacios en blanco, no en puntuaci贸n. Tambi茅n notar que agrego un espacio por defecto al inicio de la oraci贸n (antes de `Hello`) e ignor贸 el doble espacio entre `are` y `you`.

Ahora que hemos visto un poco de c贸mo los diferentes tokenizadores procesan texto, podemos empezar a explorar los algoritmos subyacentes propiamente tal. Comenzaremos con una mirada r谩pida al ampliamente aplicable SentencePiece; luego, a lo largo de las 3 secciones siguientes examinaremos c贸mo los tres principales algoritmos usados para el trabajo de tokenizaci贸n por subpalabra (subword tokenization).

## SentencePiece[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) es un algoritmo para el preprocesamiento de texto que puedes usar con cualquiera de los modelos que veremos en las siguientes tres secciones. ste considere el texto como una secuencia de caract茅res Unicode, y reemplaza los especios con un caracter especial, `_`. Usado en conjunto con el algoritmo Unigram (ver [Secci贸n 7](/course/chapter7/7)), ni siquiera requiere un paso de pre-tokenizaci贸n, lo cual es muy 煤til para lenguajes donde el caracter de espacio no es usado (como el Chino o el Japon茅s).

La otra caracter铆stica principal de SentencePiece es la *tokenizaci贸n reversible* (tokenizaci贸n reversible): dado que no hay tratamiento especial de los espacios, decodificar los tokens se hace simplemente concatenandolos y reemplazando los `_`s con espacios -- esto resulta en el texto normalizado. Como vimos antes, el tokenizador BERT remueve los espacios repetidos, por lo que su tokenizaci贸n no es reversible. 

## Descripci贸n General del Algoritmo[[algorithm-overview]]

En las siguientes secciones, profundizaremos en los tres principales algoritmos de tokenizaci贸n por subpalabra (subword tokenization): BPE (usado por GPT-2 y otros), WordPiece (usado por ejemplo por BERT), y Unigram (usado por T5 y otros). Antes de comenzar, aqu铆 una r谩pida descripci贸n general de c贸mo funciona cada uno de ellos. No dudes en regresar a esta tabla luego de leer cada una de las siguientes secciones si no te hace sentido a煤n. 


Model | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Entrenamiento | Comienza a partir de un peque帽o vocabulario y aprende reglas para fusionar tokens |  Comienza a partir de un peque帽o vocabulario y aprende reglas para fusionar tokens | Comienza de un gran vocabulario y aprende reglas para remover tokens
Etapa de Entrenamiento | Fusiona los tokens correspondiente a los pares m谩s comunes | Fusiona los tokens correspondientes al par con el mejor puntaje basado en la frecuencia del par, privilegiando  pares donde cada token individual es menos frecuente | Remueve todos los tokens en el vocabulario que minimizar谩n la funci贸n de p茅rdida (loss) calculado en el corpus completo.
Aprende | Reglas de fusi贸n y un vocabulario | S贸lo un vocabulario | Un vocabulario con puntaje para cada token
Codificaci贸n | Separa una palabra en caracteres y aplica las fusiones aprendidas durante el entrenamiento | Encuentra la subpalabra m谩s larga comenzando del inicio que est谩 en el vocabulario, luego hace lo mismo para el resto de las palabras | Encuentra la separaci贸n en tokens m谩s probable, usando los puntajes aprendidos durante el entrenamiento

Ahora profundicemos en BPE!