<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/en/chapter12/grpo_finetune.ipynb"},
]} />

# ExerciÈ›iu Practic: AjusteazÄƒ fin un model cu GRPO

Acum cÄƒ ai vÄƒzut teoria, sÄƒ o punem Ã®n practicÄƒ! Ãn acest exerciÈ›iu, vei ajusta fin un model cu GRPO.

<Tip>

Acest exerciÈ›iu a fost scris de expertul Ã®n ajustarea finÄƒ LLM [@mlabonne](https://huggingface.co/mlabonne).

</Tip>

## InstaleazÄƒ dependenÈ›ele

Ãn primul rÃ¢nd, sÄƒ instalÄƒm dependenÈ›ele pentru acest exerciÈ›iu.

```bash
!pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off
!pip install -qqq flash-attn --no-build-isolation --progress-bar off
```

Acum vom importa bibliotecile necesare.

```python
import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, GRPOTrainer
```

## ImportÄƒ È™i logheazÄƒ Ã®n Weights & Biases

Weights & Biases este un instrument pentru Ã®nregistrarea È™i monitorizarea experimentelor tale. Ãl vom folosi pentru a Ã®nregistra procesul nostru de ajustare finÄƒ.

```python
import wandb

wandb.login()
```

PoÈ›i face acest exerciÈ›iu fÄƒrÄƒ sÄƒ te loghezi Ã®n Weights & Biases, dar este recomandat sÄƒ faci asta pentru a-È›i urmÄƒri experimentele È™i pentru a interpreta rezultatele.

## ÃncarcÄƒ setul de date

Acum, sÄƒ Ã®ncÄƒrcÄƒm setul de date. Ãn acest caz, vom folosi setul de date [`mlabonne/smoltldr`](https://huggingface.co/datasets/mlabonne/smoltldr), care conÈ›ine o listÄƒ de poveÈ™ti scurte.

```python
dataset = load_dataset("mlabonne/smoltldr")
print(dataset)
```

## ÃncarcÄƒ modelul

Acum, sÄƒ Ã®ncÄƒrcÄƒm modelul.

Pentru acest exerciÈ›iu, vom folosi modelul [`SmolLM2-135M`](https://huggingface.co/HuggingFaceTB/SmolLM2-135M).

Acesta este un model mic de 135M parametri care ruleazÄƒ pe hardware limitat. Aceasta face modelul ideal pentru Ã®nvÄƒÈ›are, dar nu este cel mai puternic model de acolo. DacÄƒ ai acces la hardware mai puternic, poÈ›i Ã®ncerca sÄƒ ajustezi fin un model mai mare precum [`SmolLM2-1.7B`](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B).

```python
model_id = "HuggingFaceTB/SmolLM-135M-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

## ÃncarcÄƒ LoRA

Acum, sÄƒ Ã®ncÄƒrcÄƒm configuraÈ›ia LoRA. Vom profita de LoRA pentru a reduce numÄƒrul de parametri antrenabili, È™i prin urmare amprenta de memorie de care avem nevoie pentru a ajusta fin modelul.

DacÄƒ nu eÈ™ti familiar cu LoRA, poÈ›i citi mai multe despre aceasta Ã®n [Capitolul 11](https://huggingface.co/learn/course/en/chapter11/3).

```python
# ÃncarcÄƒ LoRA
lora_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=16,
    lora_alpha=32,
    target_modules="all-linear",
)
model = get_peft_model(model, lora_config)
print(model.print_trainable_parameters())
```

```sh
Parametri antrenabili totali: 135M
```

## DefineÈ™te funcÈ›ia de recompensÄƒ

DupÄƒ cum s-a menÈ›ionat Ã®n secÈ›iunea anterioarÄƒ, GRPO poate folosi orice funcÈ›ie de recompensÄƒ pentru a Ã®mbunÄƒtÄƒÈ›i modelul. Ãn acest caz, vom folosi o funcÈ›ie de recompensÄƒ simplÄƒ care Ã®ncurajeazÄƒ modelul sÄƒ genereze text lung de 50 de token-uri.

```python
# FuncÈ›ia de recompensÄƒ
ideal_length = 50


def reward_len(completions, **kwargs):
    return [-abs(ideal_length - len(completion)) for completion in completions]
```

## DefineÈ™te argumentele de antrenament

Acum, sÄƒ definim argumentele de antrenament. Vom folosi clasa `GRPOConfig` pentru a defini argumentele de antrenament Ã®ntr-un stil tipic `transformers`.

DacÄƒ aceasta este prima datÄƒ cÃ¢nd defineÈ™ti argumente de antrenament, poÈ›i verifica clasa [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainingarguments) pentru mai multe informaÈ›ii, sau [Capitolul 2](https://huggingface.co/learn/course/en/chapter2/1) pentru o introducere detaliatÄƒ.

```python
# Argumentele de antrenament
training_args = GRPOConfig(
    output_dir="GRPO",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    max_prompt_length=512,
    max_completion_length=96,
    num_generations=8,
    optim="adamw_8bit",
    num_train_epochs=1,
    bf16=True,
    report_to=["wandb"],
    remove_unused_columns=False,
    logging_steps=1,
)
```

Acum, putem iniÈ›ializa antrenorul cu modelul, setul de date È™i argumentele de antrenament È™i sÄƒ Ã®ncepem antrenamentul.

```python
# Antrenorul
trainer = GRPOTrainer(
    model=model,
    reward_funcs=[reward_len],
    args=training_args,
    train_dataset=dataset["train"],
)

# AntreneazÄƒ modelul
wandb.init(project="GRPO")
trainer.train()
```

Antrenamentul dureazÄƒ aproximativ 1 orÄƒ pe un singur GPU A10G care este disponibil pe Google Colab sau prin Hugging Face Spaces.

## Ãmpinge modelul pe Hub Ã®n timpul antrenamentului

DacÄƒ setÄƒm argumentul `push_to_hub` la `True` È™i argumentul `model_id` la un nume de model valid, modelul va fi Ã®mpins pe Hugging Face Hub Ã®n timp ce antrenÄƒm. Aceasta este utilÄƒ dacÄƒ vrei sÄƒ Ã®ncepi sÄƒ testezi modelul imediat!

## InterpreteazÄƒ rezultatele antrenamentului

`GRPOTrainer` Ã®nregistreazÄƒ recompensa din funcÈ›ia ta de recompensÄƒ, pierderea, È™i o gamÄƒ de alte metrici.

Ne vom concentra pe recompensa din funcÈ›ia de recompensÄƒ È™i pierderea.

DupÄƒ cum poÈ›i vedea, recompensa din funcÈ›ia de recompensÄƒ se apropie de 0 pe mÄƒsurÄƒ ce modelul Ã®nvaÈ›Äƒ. Acesta este un semn bun cÄƒ modelul Ã®nvaÈ›Äƒ sÄƒ genereze text de lungimea corectÄƒ.

![Rezultatul oferit din funcÈ›ia de recompensÄƒ](https://huggingface.co/reasoning-course/images/resolve/main/grpo/13.png)

Ai putea observa cÄƒ pierderea Ã®ncepe de la zero È™i apoi creÈ™te Ã®n timpul antrenamentului, ceea ce poate pÄƒrea contraontuitiv. Acest comportament este aÈ™teptat Ã®n GRPO È™i este direct legat de formularea matematicÄƒ a algoritmului. Pierderea Ã®n GRPO este proporÈ›ionalÄƒ cu divergenÈ›a KL (capsula relativÄƒ la politica originalÄƒ). Pe mÄƒsurÄƒ ce antrenamentul progreseazÄƒ, modelul Ã®nvaÈ›Äƒ sÄƒ genereze text care se potriveÈ™te mai bine cu funcÈ›ia de recompensÄƒ, fÄƒcÃ¢ndu-l sÄƒ devieze mai mult de la politica sa iniÈ›ialÄƒ. AceastÄƒ divergenÈ›Äƒ crescÃ¢ndÄƒ se reflectÄƒ Ã®n valoarea pierderii Ã®n creÈ™tere, care de fapt indicÄƒ cÄƒ modelul se adapteazÄƒ cu succes pentru a optimiza funcÈ›ia de recompensÄƒ.

![Pierderea](https://huggingface.co/reasoning-course/images/resolve/main/grpo/14.png)

## SalveazÄƒ È™i publicÄƒ modelul

SÄƒ partajÄƒm modelul cu comunitatea!

```python
merged_model = trainer.model.merge_and_unload()
merged_model.push_to_hub(
    "SmolGRPO-135M", private=False, tags=["GRPO", "Reasoning-Course"]
)
```

## GenereazÄƒ text

ğŸ‰ Ai ajustat fin cu succes un model cu GRPO! Acum, sÄƒ generÄƒm puÈ›in text cu modelul.

Ãn primul rÃ¢nd, vom defini un document foarte lung!

```python
prompt = """
# Un document lung despre pisicÄƒ

Pisica (Felis catus), numitÄƒ È™i pisica domesticÄƒ sau pisica de casÄƒ, este un mamifer carnivor 
mic domesticit. Este singura specie domesticitÄƒ din familia Felidae. Progresele Ã®n arheologie 
È™i geneticÄƒ au arÄƒtat cÄƒ domesticirea pisicii a avut loc Ã®n Orientul Apropiat Ã®n jurul anului 
7500 Ã®.Hr. Este È›inutÄƒ Ã®n mod obiÈ™nuit ca animal de companie È™i pisicÄƒ de fermÄƒ, dar se aflÄƒ 
È™i Ã®n libertate ca pisicÄƒ sÄƒlbaticÄƒ evitÃ¢nd contactul uman. Este preÈ›uitÄƒ de oameni pentru 
companie È™i capacitatea sa de a ucide dÄƒunÄƒtori. Ghearele sale retractabile sunt adaptate 
pentru uciderea speciilor de pradÄƒ mici precum È™oarecii È™i È™obolanii. Are un corp puternic 
È™i flexibil, reflexe rapide È™i dinÈ›i ascuÈ›iÈ›i, iar vederea nocturnÄƒ È™i simÈ›ul mirosului sunt 
bine dezvoltate. Este o specie socialÄƒ, dar un vÃ¢nÄƒtor solitar È™i un prÄƒdÄƒtor crepuscular. 
Comunicarea pisicii include vocalizÄƒriâ€”incluzÃ¢nd mieunat, tors, triluri, È™uierat, mormÄƒit 
È™i grunÈ›itâ€”precum È™i limbajul corpului. Poate auzi sunete prea slabe sau prea Ã®nalte Ã®n 
frecvenÈ›Äƒ pentru urechile umane, cum ar fi cele fÄƒcute de mamiferele mici. SecretÄƒ È™i 
percep feromoni.
"""
```

messages = [
    {"role": "user", "content": prompt},
]

Acum, putem genera text cu modelul.

```python
# GenereazÄƒ text
from transformers import pipeline

generator = pipeline("text-generation", model="SmolGRPO-135M")

## Sau foloseÈ™te modelul È™i tokenizer-ul pe care le-am definit mai devreme
# generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

generate_kwargs = {
    "max_new_tokens": 256,
    "do_sample": True,
    "temperature": 0.5,
    "min_p": 0.1,
}

generated_text = generator(messages, generate_kwargs=generate_kwargs)

print(generated_text)
```

# Concluzie

Ãn acest capitol, am vÄƒzut cum sÄƒ ajustÄƒm fin un model cu GRPO. Am vÄƒzut de asemenea cum sÄƒ interpretÄƒm rezultatele antrenamentului È™i sÄƒ generÄƒm text cu modelul.
