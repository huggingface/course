# ÃnÈ›elegerea AvansatÄƒ a OptimizÄƒrii Relative a Politicii de Grup (GRPO) Ã®n DeepSeekMath

<Tip>

AceastÄƒ secÈ›iune se scufundÄƒ Ã®n detaliile tehnice È™i matematice ale GRPO. A fost scrisÄƒ de [Shirin Yamani](https://github.com/shirinyamani).

</Tip>

SÄƒ ne aprofundÄƒm Ã®nÈ›elegerea GRPO astfel Ã®ncÃ¢t sÄƒ putem Ã®mbunÄƒtÄƒÈ›i procesul de antrenare al modelului nostru.

GRPO evalueazÄƒ direct rÄƒspunsurile generate de model comparÃ¢ndu-le Ã®n cadrul grupurilor de generare pentru a optimiza modelul de politicÄƒ, Ã®n loc sÄƒ antreneze un model de valoare separat (Critic). AceastÄƒ abordare duce la o reducere semnificativÄƒ a costului computaÈ›ional!

GRPO poate fi aplicat la orice sarcinÄƒ verificabilÄƒ unde corectitudinea rÄƒspunsului poate fi determinatÄƒ. De exemplu, Ã®n raÈ›ionamentul matematic, corectitudinea rÄƒspunsului poate fi uÈ™or verificatÄƒ prin compararea acestuia cu adevÄƒrul de bazÄƒ.

Ãnainte de a ne scufunda Ã®n detaliile tehnice, sÄƒ vizualizÄƒm cum funcÈ›ioneazÄƒ GRPO la un nivel Ã®nalt:

![deep](./img/2.jpg)

Acum cÄƒ avem o prezentare vizualÄƒ, sÄƒ descompunem cum funcÈ›ioneazÄƒ GRPO pas cu pas.

## Algoritmul GRPO

InovaÈ›ia principalÄƒ a GRPO este abordarea sa de evaluare È™i Ã®nvÄƒÈ›are din multiple rÄƒspunsuri generate simultan. Ãn loc sÄƒ se bazeze pe un model de recompensÄƒ separat, comparÄƒ rezultatele din acelaÈ™i grup pentru a determina care ar trebui sÄƒ fie Ã®ntÄƒrite.

SÄƒ parcurgem fiecare pas al algoritmului Ã®n detaliu:

### Pasul 1: EÈ™antionarea Grupului

Primul pas este sÄƒ genereze multiple rÄƒspunsuri posibile pentru fiecare Ã®ntrebare. Aceasta creeazÄƒ un set divers de rezultate care pot fi comparate Ã®ntre ele.

Pentru fiecare Ã®ntrebare $q$, modelul va genera $G$ rezultate (dimensiunea grupului) din politica antrenatÄƒ: ${o_1, o_2, o_3, \dots, o_G}\pi_{\theta_{\text{old}}}$, $G=8$ unde fiecare $o_i$ reprezintÄƒ o completare din model.

#### Exemplu:

Pentru a face acest lucru concret, sÄƒ ne uitÄƒm la o problemÄƒ aritmeticÄƒ simplÄƒ:

- **Ãntrebarea** $q$ : $\text{CalculeazÄƒ}\space2 + 2 \times 6$
- **Rezultatele** $(G = 8)$: $\{o_1:14 \text{ (corect)}, o_2:16 \text{ (greÈ™it)}, o_3:10 \text{ (greÈ™it)}, \ldots, o_8:14 \text{ (corect)}\}$

ObservÄƒ cum unele dintre rÄƒspunsurile generate sunt corecte (14) Ã®n timp ce altele sunt greÈ™ite (16 sau 10). AceastÄƒ diversitate este crucialÄƒ pentru urmÄƒtorul pas.

### Pasul 2: Calculul Avantajului

OdatÄƒ ce avem multiple rÄƒspunsuri, avem nevoie de o modalitate de a determina care sunt mai bune decÃ¢t altele. Aici intervine calculul avantajului.

#### DistribuÈ›ia Recompenselor:

Ãn primul rÃ¢nd, atribuim un scor de recompensÄƒ fiecÄƒrui rÄƒspuns generat. Ãn acest exemplu, vom folosi un model de recompensÄƒ, dar dupÄƒ cum am Ã®nvÄƒÈ›at Ã®n secÈ›iunea anterioarÄƒ, putem folosi orice funcÈ›ie care returneazÄƒ recompense.

Atribuie un scor RM fiecÄƒruia dintre rÄƒspunsurile generate bazat pe corectitudine $r_i$ *(de exemplu, 1 pentru rÄƒspuns corect, 0 pentru rÄƒspuns greÈ™it)* apoi pentru fiecare dintre $r_i$ calculeazÄƒ urmÄƒtoarea valoare de Avantaj

#### Formula Valorii Avantajului:

Ideea cheie a GRPO este cÄƒ nu avem nevoie de mÄƒsuri absolute ale calitÄƒÈ›ii - putem compara rezultatele Ã®n cadrul aceluiaÈ™i grup. Aceasta se face folosind standardizarea:

$$A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}$$

#### Exemplu:

ContinuÃ¢nd cu exemplul nostru aritmetic pentru acelaÈ™i exemplu de mai sus, imagineazÄƒ-È›i cÄƒ avem 8 rÄƒspunsuri, 4 dintre care sunt corecte È™i restul greÈ™ite, prin urmare:
- Media Grupului: $mean(r_i) = 0.5$
- Std: $std(r_i) = 0.53$
- Valoarea Avantajului:
	- RÄƒspuns corect: $A_i = \frac{1 - 0.5}{0.53}= 0.94$
	- RÄƒspuns greÈ™it: $A_i = \frac{0 - 0.5}{0.53}= -0.94$

#### Interpretarea:  

Acum cÄƒ am calculat valorile avantajului, sÄƒ Ã®nÈ›elegem ce Ã®nseamnÄƒ:

AceastÄƒ standardizare (adicÄƒ ponderarea $A_i$) permite modelului sÄƒ evalueze performanÈ›a relativÄƒ a fiecÄƒrui rÄƒspuns, ghidÃ¢nd procesul de optimizare sÄƒ favorizeze rÄƒspunsurile care sunt mai bune decÃ¢t media (recompensÄƒ mare) È™i sÄƒ descurajeze pe cele care sunt mai rele.  De exemplu, dacÄƒ $A_i > 0$, atunci $o_i$ este un rÄƒspuns mai bun decÃ¢t nivelul mediu din grupul sÄƒu; È™i dacÄƒ $A_i < 0$, atunci $o_i$ atunci calitatea rÄƒspunsului este mai micÄƒ decÃ¢t media (adicÄƒ calitate/performanÈ›Äƒ slabÄƒ).

Pentru exemplul de mai sus, dacÄƒ $A_i = 0.94 \text{(rezultat corect)}$ atunci Ã®n timpul paÈ™ilor de optimizare probabilitatea sa de generare va fi crescutÄƒ.

Cu valorile avantajului noastre calculate, suntem acum gata sÄƒ actualizÄƒm politica.

### Pasul 3: Actualizarea Politicii

Pasul final este sÄƒ folosim aceste valori ale avantajului pentru a actualiza modelul nostru astfel Ã®ncÃ¢t sÄƒ devinÄƒ mai probabil sÄƒ genereze rÄƒspunsuri bune Ã®n viitor.

FuncÈ›ia È›intÄƒ pentru actualizarea politicii este:

$$J_{GRPO}(\theta) = \left[\frac{1}{G} \sum_{i=1}^{G} \min \left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip}\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right)\right]- \beta D_{KL}(\pi_{\theta} || \pi_{ref})$$

AceastÄƒ formulÄƒ ar putea pÄƒrea intimidantÄƒ la Ã®nceput, dar este construitÄƒ din mai multe componente care fiecare servesc un scop important. SÄƒ le descompunem una cÃ¢te una.

## Componentele Cheie ale FuncÈ›iei ÈšintÄƒ

FuncÈ›ia de actualizare GRPO combinÄƒ mai multe tehnici pentru a asigura o Ã®nvÄƒÈ›are stabilÄƒ È™i eficientÄƒ. SÄƒ examinÄƒm fiecare componentÄƒ:

### 1. Raportul de Probabilitate

Raportul de probabilitate este definit ca:

$\left(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}\right)$ 

Intuitiv, formula comparÄƒ cÃ¢t de mult diferÄƒ probabilitatea de rÄƒspuns a modelului nou de probabilitatea de rÄƒspuns a modelului vechi Ã®n timp ce Ã®ncorporeazÄƒ o preferinÈ›Äƒ pentru rÄƒspunsuri care Ã®mbunÄƒtÄƒÈ›esc rezultatul aÈ™teptat.

#### Interpretarea:
- DacÄƒ $\text{raport} > 1$, modelul nou atribuie o probabilitate mai mare rÄƒspunsului $o_i$â€‹ decÃ¢t modelul vechi.
- DacÄƒ $\text{raport} < 1$, modelul nou atribuie o probabilitate mai micÄƒ lui $o_i$â€‹ 

Acest raport ne permite sÄƒ controlÄƒm cÃ¢t de mult se schimbÄƒ modelul la fiecare pas, ceea ce ne duce la urmÄƒtoarea componentÄƒ.

### 2. FuncÈ›ia de TÄƒiere

FuncÈ›ia de tÄƒiere este definitÄƒ ca:

$\text{clip}\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1 - \epsilon, 1 + \epsilon\right)$ 

LimiteazÄƒ raportul discutat mai sus sÄƒ fie Ã®n intervalul $[1 - \epsilon, 1 + \epsilon]$ pentru a evita/controla schimbÄƒri drastice sau actualizÄƒri nebuneÈ™ti È™i sÄƒ nu pÄƒÈ™eascÄƒ prea departe de politica veche. Cu alte cuvinte, limiteazÄƒ cÃ¢t de mult poate creÈ™te raportul de probabilitate pentru a ajuta la menÈ›inerea stabilitÄƒÈ›ii prin evitarea actualizÄƒrilor care Ã®mping modelul nou prea departe de cel vechi.

#### Exemplu $\space \text{sÄƒ presupunem}(\epsilon = 0.2)$
SÄƒ ne uitÄƒm la douÄƒ scenarii diferite pentru a Ã®nÈ›elege mai bine aceastÄƒ funcÈ›ie de tÄƒiere:

- **Cazul 1**: dacÄƒ noua politicÄƒ are o probabilitate de 0.9 pentru un rÄƒspuns specific È™i vechea politicÄƒ are o probabilitate de 0.5, Ã®nseamnÄƒ cÄƒ acest rÄƒspuns este Ã®ntÄƒrit de noua politicÄƒ sÄƒ aibÄƒ o probabilitate mai mare, dar Ã®ntr-o limitÄƒ controlatÄƒ care este tÄƒierea pentru a-È™i strÃ¢nge mÃ¢inile sÄƒ nu devinÄƒ drasticÄƒ 
	- $\text{Raport}: \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} = \frac{0.9}{0.5} = 1.8  â†’ \text{Clip}\space1.2$ (limita superioarÄƒ 1.2) 
- **Cazul 2**: DacÄƒ noua politicÄƒ nu este Ã®n favoarea unui rÄƒspuns (probabilitate mai micÄƒ de exemplu 0.2), Ã®nsemnÃ¢nd dacÄƒ rÄƒspunsul nu este benefic creÈ™terea ar putea fi incorectÄƒ, È™i modelul ar fi penalizat.
	- $\text{Raport}: \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} = \frac{0.2}{0.5} = 0.4  â†’\text{Clip}\space0.8$ (limita inferioarÄƒ 0.8)
#### Interpretarea:
- Formula Ã®ncurajeazÄƒ modelul nou sÄƒ favorizeze rÄƒspunsuri pe care modelul vechi le-a subponderat **dacÄƒ ele Ã®mbunÄƒtÄƒÈ›esc rezultatul**.
- DacÄƒ modelul vechi deja favoriza un rÄƒspuns cu o probabilitate mare, modelul nou poate totuÈ™i sÄƒ-l Ã®ntÄƒreascÄƒ **dar doar Ã®ntr-o limitÄƒ controlatÄƒ $[1 - \epsilon, 1 + \epsilon]$, $\text{(de exemplu, }\epsilon = 0.2, \space \text{deci} \space [0.8-1.2])$**.
- DacÄƒ modelul vechi a supraestimat un rÄƒspuns care performeazÄƒ prost, modelul nou este **descurajat** sÄƒ menÈ›inÄƒ acea probabilitate mare.
- Prin urmare, intuitiv, prin Ã®ncorporarea raportului de probabilitate, funcÈ›ia obiectiv asigurÄƒ cÄƒ actualizÄƒrile politicii sunt proporÈ›ionale cu avantajul $A_i$ Ã®n timp ce sunt moderate pentru a preveni schimbÄƒri drastice.

Ãn timp ce funcÈ›ia de tÄƒiere ajutÄƒ la prevenirea schimbÄƒrilor drastice, avem nevoie de Ã®ncÄƒ o mÄƒsurÄƒ de protecÈ›ie pentru a ne asigura cÄƒ modelul nostru nu deviazÄƒ prea departe de comportamentul sÄƒu original.

### 3. DivergenÈ›a KL

Termenul de divergenÈ›Äƒ KL este:

$\beta D_{KL}(\pi_{\theta} || \pi_{ref})$

Ãn termenul de divergenÈ›Äƒ KL, $\pi_{ref}$ este practic rezultatul modelului pre-actualizare, `per_token_logps` È™i $\pi_{\theta}$ este rezultatul modelului nou, `new_per_token_logps`. Teoretic, divergenÈ›a KL este minimizatÄƒ pentru a preveni modelul sÄƒ devieze prea departe de comportamentul sÄƒu original Ã®n timpul optimizÄƒrii. Aceasta ajutÄƒ sÄƒ gÄƒseascÄƒ un echilibru Ã®ntre Ã®mbunÄƒtÄƒÈ›irea performanÈ›ei bazatÄƒ pe semnalul de recompensÄƒ È™i menÈ›inerea coerenÈ›ei. Ãn acest context, minimizarea divergenÈ›ei KL reduce riscul ca modelul sÄƒ genereze text fÄƒrÄƒ sens sau, Ã®n cazul raÈ›ionamentului matematic, sÄƒ producÄƒ rÄƒspunsuri extrem de incorecte.

#### Interpretarea
- O penalitate de divergenÈ›Äƒ KL pÄƒstreazÄƒ rezultatele modelului aproape de distribuÈ›ia sa originalÄƒ, prevenind schimbÄƒri extreme.
- Ãn loc sÄƒ derive cÄƒtre rezultate complet iraÈ›ionale, modelul È™i-ar rafina Ã®nÈ›elegerea Ã®n timp ce Ã®ncÄƒ permite unele explorÄƒri

#### DefiniÈ›ia MatematicÄƒ
Pentru cei interesaÈ›i de detaliile matematice, sÄƒ ne uitÄƒm la definiÈ›ia formalÄƒ:

SÄƒ ne amintim cÄƒ distanÈ›a KL este definitÄƒ dupÄƒ cum urmeazÄƒ:
$$D_{KL}(P || Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}$$
Ãn RLHF, cele douÄƒ distribuÈ›ii de interes sunt adesea distribuÈ›ia versiunii noului model, P(x), È™i o distribuÈ›ie a politicii de referinÈ›Äƒ, Q(x).

#### Rolul Parametrului $\beta$

Coeficientul $\beta$ controleazÄƒ cÃ¢t de puternic impunem constrÃ¢ngerea divergenÈ›ei KL:

-  **$\beta$ Mai Mare (Penalitate KL Mai PuternicÄƒ)**
    - Mai multÄƒ constrÃ¢ngere asupra actualizÄƒrilor politicii. Modelul rÄƒmÃ¢ne aproape de distribuÈ›ia sa de referinÈ›Äƒ.
    - Poate Ã®ncetini adaptarea: Modelul ar putea avea dificultÄƒÈ›i sÄƒ exploreze rÄƒspunsuri mai bune.
- **$\beta$ Mai Mic (Penalitate KL Mai SlabÄƒ)**
    - Mai multÄƒ libertate sÄƒ actualizeze politica: Modelul poate devia mai mult de la referinÈ›Äƒ.
    - Adaptare mai rapidÄƒ dar risc de instabilitate: Modelul ar putea Ã®nvÄƒÈ›a comportamente de hack-uire a recompenselor.
	- Risc de supra-optimizare: DacÄƒ modelul de recompensÄƒ este defectuos, politica ar putea genera rezultate fÄƒrÄƒ sens.
- **Originala** [DeepSeekMath](https://arxiv.org/abs/2402.03300) lucrare a setat acest $\beta= 0.04$

Acum cÄƒ Ã®nÈ›elegem componentele GRPO, sÄƒ vedem cum funcÈ›ioneazÄƒ Ã®mpreunÄƒ Ã®ntr-un exemplu complet.

## Exemplu Lucrat cu GRPO

Pentru a ne solidifica Ã®nÈ›elegerea GRPO, sÄƒ parcurgem un exemplu complet de la Ã®nceput la sfÃ¢rÈ™it.

### Problema Exemplu

$$\text{Ã: CalculeazÄƒ}\space2 + 2 \times 6$$

### Pasul 1: EÈ™antionarea Grupului

Ãn primul rÃ¢nd, generÄƒm multiple rÄƒspunsuri din modelul nostru:

GenereazÄƒ $(G = 8)$ rÄƒspunsuri, $4$ dintre care sunt rÄƒspunsul corect ($14, \text{recompensÄƒ=} 1$) È™i $4$ incorecte $\text{(recompensÄƒ= 0)}$, Prin urmare:

$${o_1:14(corect), o_2:10 (greÈ™it), o_3:16 (greÈ™it), ... o_G:14(corect)}$$

### Pasul 2: Calculul Avantajului

Apoi, calculÄƒm valorile avantajului pentru a determina care rÄƒspunsuri sunt mai bune decÃ¢t media:

- Media Grupului: 
$$mean(r_i) = 0.5$$
- Std: $$std(r_i) = 0.53$$
- Valoarea Avantajului:
	- RÄƒspuns corect: $A_i = \frac{1 - 0.5}{0.53}= 0.94$
	- RÄƒspuns greÈ™it: $A_i = \frac{0 - 0.5}{0.53}= -0.94$

### Pasul 3: Actualizarea Politicii

Ãn final, actualizÄƒm modelul nostru pentru a Ã®ntÄƒri rÄƒspunsurile corecte:

- PresupunÃ¢nd cÄƒ probabilitatea vechii politici ($\pi_{\theta_{old}}$) pentru un rezultat corect $o_1$ este $0.5$ È™i noua politicÄƒ o creÈ™te la $0.7$ atunci:
$$\text{Raport}: \frac{0.7}{0.5} = 1.4  â†’\text{dupÄƒ Clip}\space1.2 \space (\epsilon = 0.2)$$
- Apoi cÃ¢nd funcÈ›ia È›intÄƒ este re-ponderatÄƒ, modelul tinde sÄƒ Ã®ntÄƒreascÄƒ generarea rezultatului corect, È™i $\text{DivergenÈ›a KL}$ limiteazÄƒ deviaÈ›ia de la politica de referinÈ›Äƒ.

Cu Ã®nÈ›elegerea teoreticÄƒ la locul ei, sÄƒ vedem cum poate fi implementat GRPO Ã®n cod.

## Exemplu de Implementare

SÄƒ punem totul Ã®mpreunÄƒ Ã®ntr-un exemplu practic. UrmÄƒtorul cod demonstreazÄƒ cum sÄƒ implementezi GRPO Ã®n PyTorch.

### 1. ÃncÄƒrcarea Modelului È™i Generarea RÄƒspunsurilor

Ãn primul rÃ¢nd, trebuie sÄƒ Ã®ncÄƒrcÄƒm un model È™i sÄƒ generÄƒm multiple rÄƒspunsuri pentru o Ã®ntrebare datÄƒ:

```python
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

# ÃncarcÄƒ modelul È™i tokenizer-ul
model_name = "Qwen/Qwen2-Math-1.5B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model.eval()

# MutÄƒ modelul pe GPU dacÄƒ este disponibil
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Prompt de intrare
prompt = "RezolvÄƒ y = 2x + 1 pentru x = 2, y = "  # RÄƒspuns corect: 5
inputs = tokenizer(prompt, return_tensors="pt", padding=True)
input_ids = inputs["input_ids"].to(device)  # Forma: (1, prompt_len)
attention_mask = inputs["attention_mask"].to(device)

# Pasul 1: GenereazÄƒ 8 rÄƒspunsuri (B = 2 grupuri, G = 4 rÄƒspunsuri per grup)
batch_size, num_generations = 2, 4
outputs = model.generate(
    input_ids=input_ids,  # Forma: (1, prompt_len)
    attention_mask=attention_mask,
    max_new_tokens=1,  # seq_len = 1 (un singur token per rÄƒspuns)
    num_return_sequences=batch_size * num_generations,  # 8 rÄƒspunsuri Ã®n total
    do_sample=True,
    top_k=10,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id,
    return_dict_in_generate=True,
    output_scores=True,
)
```

aceastÄƒ Generare iniÈ›ialÄƒ (Ãnainte de Orice PaÈ™i) va produce ceva de genul:

```text
Rezultatul 1: 5.0
Rezultatul 2: 6.0
Rezultatul 3: 7.0
Rezultatul 4: 5.0
Rezultatul 5: 10.0
Rezultatul 6: 2.0
Rezultatul 7: 5.0
Rezultatul 8: 5.0
```

### 2. Calcularea Recompenselor

Acum, trebuie sÄƒ determinÄƒm care rÄƒspunsuri sunt corecte È™i sÄƒ atribuim recompense Ã®n consecinÈ›Äƒ:

Cu GRPO, cu acelaÈ™i prompt de eÈ™antion, generÄƒm multiple completÄƒri. Deci, de exemplu, pentru prompt-urile noastre de `"RezolvÄƒ y = 2x + 1 pentru x = 2, y = "` È™i `RezolvÄƒ y = 2x + 1 pentru x = 4, y = "` avem douÄƒ grupuri de rezultate generate pentru prompt-ul dat unul este sÄƒ zicem 
- `[5, 6, 7, 5]` È™i celÄƒlalt este 
- `[10, 2, 9, 9]` Ã®n timp ce rÄƒspunsul corect este 5 È™i 9. 

ObservÄƒ cÄƒ Ã®n practicÄƒ aceste scoruri de recompensÄƒ sunt obÈ›inute printr-o funcÈ›ie de recompensÄƒ bazatÄƒ pe reguli care atribuie recompense bazate pe corectitudinea rÄƒspunsului sau un model mai complex bazat pe reÈ›ele neuronale care poate fi antrenat sÄƒ atribuie recompense bazate pe corectitudinea rÄƒspunsului sau un amestec din ambele. Dar pentru simplitate sÄƒ spunem cÄƒ recompensa noastrÄƒ per rÄƒspuns este 1 dacÄƒ rÄƒspunsul este corect È™i 0 dacÄƒ este greÈ™it, prin urmare:  
```python
reward_1 = [1, 0, 0, 1]
reward_2 = [0, 0, 1, 1]
```
apoi obÈ›inem media È™i std-ul grupului de recompense;

```python
# Forma: (B * G,) = (8,) pentru cÄƒ avem 2 grupuri de 4 generÄƒri pe care le aplatizÄƒm
rewards = torch.tensor([1, 0, 0, 1, 0, 0, 1, 1], dtype=torch.float32)
num_generations = 4

# Recompense grupate: Forma (B, G) = 2, 4)
rewards_grouped = rewards.view(-1, num_generations)

# Media per grup: Forma (B,) = (2,)
mean_grouped_rewards = rewards_grouped.mean(dim=1)

# Std per grup: Forma (B,) = (2,)
std_grouped_rewards = rewards_grouped.std(dim=1)

# DifuzeazÄƒ pentru a se potrivi cu recompensele È™i normalizeazÄƒ: Forma (B * G,) = (8,)
# de ce avem nevoie sÄƒ difuzÄƒm? pentru cÄƒ trebuie sÄƒ calculÄƒm valorile avantajului pentru fiecare rÄƒspuns din grup
mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(num_generations, dim=0)
std_grouped_rewards = std_grouped_rewards.repeat_interleave(num_generations, dim=0)
```
aceasta va produce:
```text
Recompense Grupate: tensor([[1., 0., 0., 1.],
                        [0., 0., 1., 1.]])
Media per grup: tensor([0.5000, 0.5000])
Std per grup: tensor([0.5774, 0.5774])
Media DifuzatÄƒ: tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000])
Std Difuzat: tensor([0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774])
```
Acum putem calcula valorile avantajului pentru fiecare rÄƒspuns:
```python
# Avantaje: Forma (B * G,) = (8,)
advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-8)
```
aceasta va produce:
```text
Avantaje: tensor([ 0.8659, -0.8660, -0.8660,  0.8659, -0.8660, -0.8660,  0.8659,  0.8659])
```
care vine din formula Avantajului de mai sus, deci:
```text
Pentru reward_1 = [1, 0, 0, 1]:
1 - 0.5 / 0.5774 â‰ˆ 0.8659
0 - 0.5 / 0.5774 â‰ˆ -0.8660
Pentru reward_2 = [0, 0, 1, 1]: AcelaÈ™i model.
```
cu toate acestea, forma aici este `(B*G,) = (8,)` dar Ã®n practicÄƒ, avem nevoie sÄƒ avem forma de `(B, G) = (2, 4)` pentru a se potrivi cu forma logits, nu? Prin urmare, trebuie sÄƒ unsqueeze tensor-ul avantajelor pentru a avea forma de `(B*G, 1) = (8, 1)` pentru a se potrivi cu forma logits.
```python
# Forma (B * G, 1) = (8, 1) pentru a se potrivi cu forma logits
advantages = advantages.unsqueeze(1)
```
care va produce:
```text
Avantaje: tensor([[ 0.8659],
                    [-0.8660],
                    [-0.8660],
                    [ 0.8659],
                    [-0.8660],
                    [-0.8660],
                    [ 0.8659],
                    [ 0.8659]])
```
acum suntem bine, sÄƒ trecem la urmÄƒtorul pas de actualizare a modelului de politicÄƒ bazat pe valorile avantajului.

### 3. Actualizarea Politicii
Ãn final, folosim valorile avantajului pentru a actualiza modelul nostru:

```python
# CalculeazÄƒ raportul de probabilitate Ã®ntre politicile noi È™i vechi
ratio = torch.exp(
    new_per_token_logps - per_token_logps
)  # Forma: (B*G, seq_len) seq_len este lungimea rezultatului adicÄƒ numÄƒrul de token-uri generate deci aici pentru simplitate sÄƒ presupunem cÄƒ este 1 # (8, 1)
```

ObservÄƒ cÄƒ `per_token_logps` poate fi obÈ›inut prin trecerea rezultatelor generate la model È™i obÈ›inerea logits-urilor È™i apoi aplicarea funcÈ›iei softmax pentru a obÈ›ine probabilitÄƒÈ›ile `F.softmax(logits, dim=-1)`.

```python
# FuncÈ›ia de TÄƒiere
eps = self.cliprange  # de exemplu 0.2
pg_losses1 = -advantages * ratio  # Forma: (B*G, seq_len)  #(8, 1)
pg_losses2 = -advantages * torch.clamp(
    ratio, 1.0 - eps, 1.0 + eps
)  # Forma: (B*G, seq_len) #(8, 1)
pg_loss_max = torch.max(pg_losses1, pg_losses2)  # Forma: (B*G, seq_len) #(8, 1)


# Acum CombinÄƒ cu penalitatea KL # Forma: (B*G, seq_len) #(8, 1)
per_token_loss = pg_loss_max + self.beta * per_token_kl
```

`per_token_kl` poate fi de asemenea calculat dupÄƒ cum urmeazÄƒ:

```python
# Forma: (B*G, seq_len) #(8, 1)
per_token_kl = F.kl_div(
    F.log_softmax(new_per_token_logps, dim=-1),
    F.softmax(per_token_logps, dim=-1),
    reduction="none",
).sum(dim=-1, keepdim=True)
```

Exemplul complet poate fi gÄƒsit [aici](./basic_example.py). GRPO este de asemenea implementat de excelenta echipÄƒ TRL, poÈ›i verifica implementarea [TRL/GRPO_trainer](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py) pentru mai multe detalii.

## Rezumat È™i PaÈ™i UrmÄƒtori

FelicitÄƒri! Acum ai Ã®nvÄƒÈ›at despre Optimizarea RelativÄƒ a Politicii de Grup (GRPO). Pentru a recapitula ce am acoperit:

1. GRPO comparÄƒ multiple rezultate Ã®n cadrul unui grup pentru a determina care sunt mai bune decÃ¢t altele, fÄƒrÄƒ a necesita un model de valoare separat.
2. Calculul avantajului standardizeazÄƒ recompensele pentru a identifica care rÄƒspunsuri sunt peste sau sub medie.
3. Actualizarea politicii foloseÈ™te o funcÈ›ie obiectiv tÄƒiatÄƒ cu o penalitate de divergenÈ›Äƒ KL pentru a asigura Ã®nvÄƒÈ›area stabilÄƒ.

AceastÄƒ abordare este deosebit de puternicÄƒ pentru sarcinile de raÈ›ionament matematic, unde corectitudinea poate fi verificatÄƒ obiectiv. Metoda GRPO permite antrenare mai eficientÄƒ comparativ cu abordÄƒrile RLHF tradiÈ›ionale care necesitÄƒ un model critic separat.

Pe mÄƒsurÄƒ ce continui sÄƒ explorezi GRPO, considerÄƒ sÄƒ experimentezi cu diferite dimensiuni de grup, funcÈ›ii de recompensÄƒ È™i coeficienÈ›i de penalitate KL pentru a vedea cum afecteazÄƒ performanÈ›a modelului tÄƒu.

Antrenare fericitÄƒ! ğŸš€

## ReferinÈ›e

1. [Cartea RLHF de Nathan Lambert](https://github.com/natolambert/rlhf-book)
2. [Raportul Tehnic DeepSeek-V3](https://huggingface.co/papers/2412.19437)
3. [DeepSeekMath](https://huggingface.co/papers/2402.03300) 