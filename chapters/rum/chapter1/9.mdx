# Sumar[[sumar]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

Ãn acest capitol, aÈ›i Ã®nvÄƒÈ›at cum sÄƒ abordaÈ›i diferite sarcini NLP utilizÃ¢nd funcÈ›ia `pipeline()` de nivel Ã®nalt din ğŸ¤— Transformers. De asemenea, aÈ›i descoperit cum sÄƒ cÄƒutaÈ›i È™i sÄƒ utilizaÈ›i modele Ã®n Hub, precum È™i cum sÄƒ testaÈ›i acestee modelee direct Ã®n browser folosind API-ul de inferenÈ›Äƒ.

Am discutat despre modul Ã®n care funcÈ›ioneazÄƒ modelele Transformer dintr-o perspectivÄƒ generalÄƒ È™i despre importanÈ›a transfer learning-ului È™i fine-tuning-ului. Un aspect cheie este faptul cÄƒ puteÈ›i utiliza Ã®ntreaga arhitecturÄƒ sau doar encoder-ul sau decoder-ul, Ã®n funcÈ›ie de tipul de sarcinÄƒ pe care doriÈ›i sÄƒ o rezolvaÈ›i. UrmÄƒtorul tabel rezumÄƒ acest aspect:

| Model           | Exemple                                    | Task-uri                                                                           |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificarea propoziÈ›iilor, recunoaÈ™terea entitÄƒÈ›ilor denumite, Extractive QA    |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Generarea de text                                                                |
| Encoder-decoder | BART, T5, Marian, mBART                    | Rezumare, traducere, rÄƒspunsuri generative la Ã®ntrebÄƒri                          |
