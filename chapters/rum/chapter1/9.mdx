# Sumar[[sumar]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

Ãn acest capitol, aÈ›i vÄƒzut cum sÄƒ abordaÈ›i diferite sarcini NLP utilizÃ¢nd funcÈ›ia de nivel Ã®nalt `pipeline()` din ğŸ¤— Transformers. De asemenea, aÈ›i vÄƒzut cum sÄƒ cÄƒutaÈ›i È™i sÄƒ utilizaÈ›i modele Ã®n Hub, precum È™i cum sÄƒ utilizaÈ›i API-ul de inferenÈ›Äƒ pentru a testa modelele direct Ã®n browser.

Am discutat despre modul Ã®n care funcÈ›ioneazÄƒ modelele Transformer la un nivel general È™i am vorbit despre importanÈ›a Ã®nvÄƒÈ›Äƒrii prin transfer È™i a reglÄƒrii fine. Un aspect cheie este faptul cÄƒ puteÈ›i utiliza Ã®ntreaga arhitecturÄƒ sau doar codificatorul sau decodificatorul, Ã®n funcÈ›ie de tipul de sarcinÄƒ pe care doriÈ›i sÄƒ o rezolvaÈ›i. UrmÄƒtorul tabel rezumÄƒ acest aspect:

| Model           | Exemple                                    | Task-uri                                                                           |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificarea propoziÈ›iilor, recunoaÈ™terea entitÄƒÈ›ilor denumite, Extractive QA    |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Generarea de text                                                                |
| Encoder-decoder | BART, T5, Marian, mBART                    | Rezumare, traducere, rÄƒspunsuri generative la Ã®ntrebÄƒri                          |
