# Sumar[[sumar]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

În acest capitol, ați învățat cum să abordați diferite sarcini NLP utilizând funcția `pipeline()` de nivel înalt din 🤗 Transformers. De asemenea, ați descoperit cum să căutați și să utilizați modele în Hub, precum și cum să testați acestee modelee direct în browser folosind API-ul de inferență.

Am discutat despre modul în care funcționează modelele Transformer dintr-o perspectivă generală și despre importanța transfer learning-ului și fine-tuning-ului. Un aspect cheie este faptul că puteți utiliza întreaga arhitectură sau doar encoder-ul sau decoder-ul, în funcție de tipul de sarcină pe care doriți să o rezolvați. Următorul tabel rezumă acest aspect:

| Model           | Exemple                                    | Task-uri                                                                           |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Clasificarea propozițiilor, recunoașterea entităților denumite, Extractive QA    |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Generarea de text                                                                |
| Encoder-decoder | BART, T5, Marian, mBART                    | Rezumare, traducere, răspunsuri generative la întrebări                          |
