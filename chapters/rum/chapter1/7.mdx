# Modele Sequence-to-sequence[modele-sequence-to-sequence]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

Modelele Encoder-Decoder (denumite și *modele sequence-to-sequence*) utilizează ambele părți ale arhitecturii Transformer. În fiecare etapă, layerele de atenție ale codificatorului pot accesa toate cuvintele din propoziția inițială, în timp ce layerele de atenție ale decodificatorului pot accesa doar cuvintele poziționate înaintea unui anumit cuvânt din intrare.

Preinstruirea acestor modele se poate face folosind obiectivele modelelor de codificare sau de decodificare, dar de obicei implică ceva un pic mai complex. De exemplu, [T5](https://huggingface.co/t5-base) este preinstruit prin înlocuirea unor intervale aleatorii de text (care pot conține mai multe cuvinte) cu un singur cuvânt special mascat, iar obiectivul este apoi de a prezice textul pe care îl înlocuiește acest cuvânt mascat.

Modelele Sequence-to-sequence sunt cele mai potrivite pentru sarcinile care se învârt în jurul generării de noi propoziții în funcție de o intrare dată, cum ar fi rezumarea, traducerea sau răspunsul generativ la întrebări.

Printre reprezentanții acestei familii de modele se numără:
- [BART](https://huggingface.co/transformers/model_doc/bart)
- [mBART](https://huggingface.co/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/transformers/model_doc/marian)
- [T5](https://huggingface.co/transformers/model_doc/t5)
