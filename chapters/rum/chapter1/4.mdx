# Cum funcÈ›ioneazÄƒ Transformers?[[cum-funcÈ›ioneazÄƒ-transformers]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

Ãn aceastÄƒ secÈ›iune, vom analiza arhitectura modelelor Transformer.

## Un pic despre istorie modelelor Transformer[[un-pic-despre-istoria-modelelor-transformer]]

IatÄƒ cÃ¢teva puncte de referinÈ›Äƒ Ã®n (scurta) istorie a modelelor Transformer:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

[Arhitectura Transformer] (https://arxiv.org/abs/1706.03762) a fost introdusÄƒ Ã®n iunie 2017. Accentul cercetÄƒrii iniÈ›iale a fost pus pe sarcinile de traducere. Aceasta a fost urmatÄƒ de introducerea mai multor modele influente, inclusiv:

- **Iunie 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), primul model Transformer preinstruit, utilizat pentru reglarea precisÄƒ a diferitelor sarcini NLP È™i a obÈ›inut rezultate de top

- **Octombrie 2018**: [BERT](https://arxiv.org/abs/1810.04805), un alt model mare preinstruit, conceput pentru a produce rezumate mai bune ale propoziÈ›iilor (mai multe despre el Ã®n capitolul urmÄƒtor!)

- **Februarie 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), o versiune Ã®mbunÄƒtÄƒÈ›itÄƒ (È™i mai mare) a GPT care nu a fost lansatÄƒ public imediat din motive etice

- **Octombrie 2019**: [DistilBERT](https://arxiv.org/abs/1910.01108), o versiune rafinatÄƒ a BERT care este cu 60% mai rapidÄƒ, cu 40% mai uÈ™oarÄƒ Ã®n memorie È™i care pÄƒstreazÄƒ Ã®ncÄƒ 97% din performanÈ›a BERT

- **Octombrie 2019**: [BART](https://arxiv.org/abs/1910.13461) È™i [T5](https://arxiv.org/abs/1910.10683), douÄƒ modele mari preinstruite folosind aceeaÈ™i arhitecturÄƒ ca modelul original Transformer (primul care face acest lucru)

- **Mai 2020**, [GPT-3](https://arxiv.org/abs/2005.14165), o versiune È™i mai mare a GPT-2, care este capabilÄƒ sÄƒ se descurce bine Ã®ntr-o varietate de sarcini, fÄƒrÄƒ a fi nevoie de o reglare finÄƒ (numitÄƒ _zero-shot learning_)

AceastÄƒ listÄƒ este departe de a fi completÄƒ È™i este menitÄƒ doar sÄƒ evidenÈ›ieze cÃ¢teva dintre diferitele tipuri de modele Transformer. Ãn linii mari, acestea pot fi grupate Ã®n trei categorii:

- Tip GPT (numite È™i modele de Transformer autoregresive)
- Tip BERT (numite È™i modele de Transfomer _auto-encoding_) 
- Tip BART/T5 (denumite È™i modele de Transformer _sequence-to-sequence_)

Vom analiza aceste familii Ã®n detaliu mai tÃ¢rziu.

##Transformers sunt modele de limbaj[transformers-sunt-modele-de-limbaj]]

Toate modelele Transformer menÈ›ionate mai sus (GPT, BERT, BART, T5 etc.) au fost antrenate ca *modele lingvistice*. Aceasta Ã®nseamnÄƒ cÄƒ au fost antrenate pe volume mari de text brut Ã®ntr-un mod autosupravegheat. ÃnvÄƒÈ›area autosupravegheatÄƒ este un tip de formare Ã®n care obiectivul este calculat automat din datele de intrare ale modelului. Aceasta Ã®nseamnÄƒ cÄƒ nu este nevoie de oameni pentru a eticheta datele!

Acest tip de model dezvoltÄƒ o Ã®nÈ›elegere statisticÄƒ a limbii pe care a fost antrenat, dar nu este foarte util pentru sarcini practice specifice. Din acest motiv, modelul general preinstruit trece apoi printr-un proces numit *transfer learning*. Ãn timpul acestui proces, modelul este ajustat Ã®ntr-un mod supravegheat - adicÄƒ folosind etichete notate de oameni - pentru o anumitÄƒ sarcinÄƒ.

Un exemplu de sarcinÄƒ este prezicerea urmÄƒtorului cuvÃ¢nt dintr-o propoziÈ›ie dupÄƒ citirea a *n* cuvinte anterioare. Aceasta se numeÈ™te *modelare cauzalÄƒ a limbajului* deoarece rezultatul depinde de intrÄƒrile trecute È™i prezente, dar nu È™i de cele viitoare.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

Un alt exemplu este *modelarea limbajului mascat*, Ã®n care modelul prezice un cuvÃ¢nt mascat din propoziÈ›ie.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## Modelele Transformer sunt mari[[modelele-transformer-sunt-mari]]

Cu excepÈ›ia cÃ¢torva cazuri excepÈ›ionale (cum ar fi DistilBERT), strategia generalÄƒ pentru a obÈ›ine performanÈ›e mai bune constÄƒ Ã®n creÈ™terea dimensiunilor modelelor, precum È™i a cantitÄƒÈ›ii de date pe care acestea sunt preinstruite.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

Din pÄƒcate, antrenarea unui model, Ã®n special a unuia mare, necesitÄƒ o cantitate mare de date. Acest lucru devine foarte costisitor Ã®n termeni de timp È™i resurse de calcul. Aceasta se reflectÄƒ chiar È™i Ã®n impactul asupra mediului, dupÄƒ cum se poate vedea Ã®n graficul urmÄƒtor.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>

<Youtube id="ftWlj4FBHTg"/>

Iar acest lucru aratÄƒ un proiect pentru un model (foarte mare) condus de o echipÄƒ care Ã®ncearcÄƒ Ã®n mod conÈ™tient sÄƒ reducÄƒ impactul de mediu al preinstruirii. Amprenta lÄƒsatÄƒ de efectuarea multor teste pentru a obÈ›ine cei mai buni hiperparametri ar fi È™i mai mare.

ImaginaÈ›i-vÄƒ dacÄƒ de fiecare datÄƒ cÃ¢nd o echipÄƒ de cercetare, o organizaÈ›ie studenÈ›eascÄƒ sau o companie ar dori sÄƒ antreneze un model, ar face-o de la zero. Acest lucru ar conduce la costuri globale uriaÈ™e È™i inutile!

Acesta este motivul pentru care partajarea modelelor lingvistice este esenÈ›ialÄƒ: partajarea ponderilor antrenate È™i construirea pe baza ponderilor deja antrenate reduce costul global de calcul È™i amprenta de carbon a comunitÄƒÈ›ii.

Apropo, puteÈ›i evalua amprenta de carbon a formÄƒrii modelelor dvs. prin intermediul mai multor instrumente. De exemplu [ML CO2 Impact](https://mlco2.github.io/impact/) sau [Code Carbon]( https://codecarbon.io/) care este integrat Ã®n ğŸ¤— Transformers. Pentru a afla mai multe despre acest lucru, puteÈ›i citi aceastÄƒ [postare pe blog](https://huggingface.co/blog/carbon-emissions-on-the-hub) care vÄƒ va arÄƒta cum sÄƒ generaÈ›i un fiÈ™ier `emissions.csv` cu o estimare a amprentei formÄƒrii dvs., precum È™i [documentaÈ›ia](https://huggingface.co/docs/hub/model-cards-co2) din ğŸ¤— Transformers care abordeazÄƒ acest subiect.


## Transfer Learning[[transfer-learning]]

<Youtube id="BqqfQnyjmgg" />

*Preinstruirea* este acÈ›iunea de formare a unui model de la zero: ponderile sunt iniÈ›ializate aleatoriu, iar formarea Ã®ncepe fÄƒrÄƒ nicio cunoaÈ™tere prealabilÄƒ.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

AceastÄƒ preinstruire se face de obicei pe cantitÄƒÈ›i foarte mari de date. Prin urmare, este nevoie de un corpus foarte mare de date, iar formarea poate dura pÃ¢nÄƒ la cÃ¢teva sÄƒptÄƒmÃ¢ni.

*Reglarea-finÄƒ*, pe de altÄƒ parte, este instruirea efectuatÄƒ **dupÄƒ** ce un model a fost preinstruit. Pentru a efectua reglarea finÄƒ, obÈ›ineÈ›i mai Ã®ntÃ¢i un model lingvistic preinstruit, apoi efectuaÈ›i o instruire suplimentarÄƒ cu un set de date specific sarcinii dumneavoastrÄƒ. StaÈ›i - de ce sÄƒ nu instruiÈ›i de la Ã®nceput modelul pentru cazul dvs. final de utilizare (**scratch**)? ExistÄƒ cÃ¢teva motive:

* Modelul preinstruit a fost deja instruit pe un set de date care are unele similitudini cu setul de date pentru reglarea finÄƒ. Procesul de reglare finÄƒ este astfel capabil sÄƒ profite de cunoÈ™tinÈ›ele dobÃ¢ndite de modelul iniÈ›ial Ã®n timpul preantrenÄƒrii (de exemplu, Ã®n cazul problemelor de NLP, modelul preantrenat va avea un anumit tip de Ã®nÈ›elegere statisticÄƒ a limbajului pe care Ã®l utilizaÈ›i pentru sarcina dumneavoastrÄƒ). 
* Deoarece modelul preinstruit a fost deja instruit pe o mulÈ›ime de date, reglarea finÄƒ necesitÄƒ mult mai puÈ›ine date pentru a obÈ›ine rezultate decente.
* Din acelaÈ™i motiv, timpul È™i resursele necesare pentru a obÈ›ine rezultate bune sunt mult mai reduse.

De exemplu, se poate valorifica un model preformat antrenat pe limba englezÄƒ È™i apoi ajustat pe un corpus arXiv, rezultÃ¢nd un model bazat pe È™tiinÈ›Äƒ/cercetare. Reglarea finÄƒ va necesita doar o cantitate limitatÄƒ de date: cunoÈ™tinÈ›ele dobÃ¢ndite de modelul pre-format sunt â€transferateâ€, de unde È™i termenul de *Ã®nvÄƒÈ›are prin transfer*.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

Prin urmare, reglarea finÄƒ a unui model are costuri mai mici de timp, date, financiare È™i de mediu. De asemenea, este mai rapid È™i mai uÈ™or sÄƒ iteraÈ›i pe diferite scheme de reglaj fin, deoarece formarea este mai puÈ›in constrÃ¢ngÄƒtoare decÃ¢t o preformare completÄƒ.

Acest proces va obÈ›ine, de asemenea, rezultate mai bune decÃ¢t formarea de la zero (cu excepÈ›ia cazului Ã®n care dispuneÈ›i de o mulÈ›ime de date), motiv pentru care ar trebui sÄƒ Ã®ncercaÈ›i Ã®ntotdeauna sÄƒ valorificaÈ›i un model preformat - unul cÃ¢t mai apropiat posibil de sarcina pe care o aveÈ›i la Ã®ndemÃ¢nÄƒ - È™i sÄƒ Ã®l reglaÈ›i fin.

## Arhitectura generalÄƒ[[arhitectura-generalÄƒ]]

Ãn aceastÄƒ secÈ›iune, vom analiza arhitectura generalÄƒ a modelului Transformer. Nu vÄƒ faceÈ›i griji dacÄƒ nu Ã®nÈ›elegeÈ›i anumite concepte; mai tÃ¢rziu existÄƒ secÈ›iuni detaliate care acoperÄƒ fiecare dintre componente.

<Youtube id="H39Z_720T5s" />

## Introducere[[introducere]]

Modelul este compus Ã®n principal din douÄƒ blocuri:

* **Codificator (stÃ¢nga)**: Codificatorul primeÈ™te o intrare È™i construieÈ™te o reprezentare a acesteia (caracteristicile sale). Aceasta Ã®nseamnÄƒ cÄƒ modelul este optimizat pentru a dobÃ¢ndi cunoÈ™tinÈ›e din datele de intrare.
* **Decodificator (dreapta)**: Decodificatorul utilizeazÄƒ reprezentarea codificatorului (caracteristicile) Ã®mpreunÄƒ cu alte intrÄƒri pentru a genera o secvenÈ›Äƒ È›intÄƒ. Aceasta Ã®nseamnÄƒ cÄƒ modelul este optimizat pentru a genera ieÈ™iri.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

Fiecare dintre aceste pÄƒrÈ›i poate fi utilizatÄƒ independent, Ã®n funcÈ›ie de sarcinÄƒ: 

* **Modeluri exclusiv de codare**: Bune pentru sarcini care necesitÄƒ Ã®nÈ›elegerea datelor de intrare, cum ar fi clasificarea propoziÈ›iilor È™i recunoaÈ™terea entitÄƒÈ›ilor numite.
* **Modele exclusiv decodificatoare**: Bune pentru sarcini generative, cum ar fi generarea de text.
* **Modele encoder-decoder** sau **modele sequence-to-sequence**: Bune pentru sarcinile generative care necesitÄƒ o intrare, cum ar fi traducerea sau rezumarea.

Vom analiza aceste arhitecturi Ã®n mod independent Ã®n secÈ›iunile urmÄƒtoare.

## Attention layers(straturi de atenÈ›ie)[[attention-layers]]

O caracteristicÄƒ cheie a modelelor Transformer este faptul cÄƒ acestea sunt construite cu straturi speciale numite *attention layers*. De fapt, titlul lucrÄƒrii de prezentare a arhitecturii Transformer a fost [â€Attention Is All You Needâ€] (https://arxiv.org/abs/1706.03762)! Vom explora detaliile attention layers-urilor mai tÃ¢rziu Ã®n curs; pentru moment, tot ce trebuie sÄƒ È™tiÈ›i este cÄƒ acest strat va spune modelului sÄƒ acorde o atenÈ›ie specificÄƒ anumitor cuvinte din propoziÈ›ia pe care i-aÈ›i transmis-o (È™i sÄƒ le ignore mai mult sau mai puÈ›in pe celelalte) atunci cÃ¢nd se ocupÄƒ de reprezentarea fiecÄƒrui cuvÃ¢nt.

Pentru a pune acest lucru Ã®n context, luaÈ›i Ã®n considerare sarcina de a traduce un text din englezÄƒ Ã®n francezÄƒ. AvÃ¢nd Ã®n vedere intrarea â€VÄƒ place acest cursâ€, un model de traducere va trebui sÄƒ se ocupe È™i de cuvÃ¢ntul adiacent â€Youâ€ pentru a obÈ›ine traducerea corectÄƒ pentru cuvÃ¢ntul â€likeâ€, deoarece Ã®n francezÄƒ verbul â€likeâ€ se conjugÄƒ diferit Ã®n funcÈ›ie de subiect. Cu toate acestea, restul propoziÈ›iei nu este util pentru traducerea acestui cuvÃ¢nt. Ãn aceeaÈ™i ordine de idei, la traducerea cuvÃ¢ntului â€thisâ€, modelul va trebui, de asemenea, sÄƒ acorde atenÈ›ie cuvÃ¢ntului â€courseâ€, deoarece â€thisâ€ se traduce diferit Ã®n funcÈ›ie de faptul dacÄƒ substantivul asociat este masculin sau feminin. Din nou, celelalte cuvinte din propoziÈ›ie nu vor conta pentru traducerea cuvÃ¢ntului â€courseâ€. Ãn cazul unor propoziÈ›ii mai complexe (È™i al unor reguli gramaticale mai complexe), modelul ar trebui sÄƒ acorde o atenÈ›ie deosebitÄƒ cuvintelor care ar putea apÄƒrea mai departe Ã®n propoziÈ›ie pentru a traduce corect fiecare cuvÃ¢nt.

AcelaÈ™i concept se aplicÄƒ oricÄƒrei sarcini asociate cu limbajul natural: un cuvÃ¢nt Ã®n sine are o semnificaÈ›ie, dar aceastÄƒ semnificaÈ›ie este profund afectatÄƒ de context, care poate fi orice alt cuvÃ¢nt (sau cuvinte) Ã®nainte sau dupÄƒ cuvÃ¢ntul studiat.

Acum, cÄƒ aveÈ›i o idee despre ce sunt attention layers, sÄƒ analizÄƒm mai Ã®ndeaproape arhitectura Transformer.

## Arhitectura originalÄƒ[[arhitectura-originalÄƒ]

Arhitectura Transformer a fost conceputÄƒ iniÈ›ial pentru traducere. Ãn timpul formÄƒrii, codificatorul primeÈ™te intrÄƒri (propoziÈ›ii) Ã®ntr-o anumitÄƒ limbÄƒ, Ã®n timp ce decodificatorul primeÈ™te aceleaÈ™i propoziÈ›ii Ã®n limba È›intÄƒ doritÄƒ. Ãn codificator, attention layets pot utiliza toate cuvintele dintr-o propoziÈ›ie (deoarece, dupÄƒ cum tocmai am vÄƒzut, traducerea unui anumit cuvÃ¢nt poate depinde de ceea ce se aflÄƒ Ã®nainte È™i dupÄƒ el Ã®n propoziÈ›ie). Cu toate acestea, decodificatorul funcÈ›ioneazÄƒ secvenÈ›ial È™i poate acorda atenÈ›ie numai cuvintelor din propoziÈ›ie pe care le-a tradus deja (deci, numai cuvintelor anterioare cuvÃ¢ntului generat Ã®n prezent). De exemplu, atunci cÃ¢nd am prezis primele trei cuvinte din È›inta tradusÄƒ, le dÄƒm decodificatorului, care utilizeazÄƒ apoi toate intrÄƒrile codificatorului pentru a Ã®ncerca sÄƒ prezicÄƒ al patrulea cuvÃ¢nt.

Pentru a accelera lucrurile Ã®n timpul formÄƒrii (atunci cÃ¢nd modelul are acces la propoziÈ›iile È›intÄƒ), decodorul primeÈ™te Ã®ntreaga È›intÄƒ, dar nu i se permite sÄƒ utilizeze cuvintele viitoare (dacÄƒ ar avea acces la cuvÃ¢ntul din poziÈ›ia 2 atunci cÃ¢nd Ã®ncearcÄƒ sÄƒ prezicÄƒ cuvÃ¢ntul din poziÈ›ia 2, problema nu ar fi foarte dificilÄƒ!) De exemplu, atunci cÃ¢nd Ã®ncearcÄƒ sÄƒ prezicÄƒ al patrulea cuvÃ¢nt, stratul de atenÈ›ie va avea acces doar la cuvintele de la poziÈ›iile 1 È™i 3.

Arhitectura originalÄƒ a Transformer arÄƒta astfel, cu codificatorul Ã®n stÃ¢nga È™i decodificatorul Ã®n dreapta:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

ReÈ›ineÈ›i cÄƒ primul attention layer dintr-un bloc decodor acordÄƒ atenÈ›ie tuturor intrÄƒrilor (trecute) cÄƒtre decodor, dar al doilea attention layer utilizeazÄƒ ieÈ™irea codorului. Astfel, acesta poate accesa Ã®ntreaga propoziÈ›ie de intrare pentru a prezice cel mai bine cuvÃ¢ntul curent. Acest lucru este foarte util deoarece diferite limbi pot avea reguli gramaticale care pun cuvintele Ã®n ordine diferitÄƒ sau un context furnizat mai tÃ¢rziu Ã®n propoziÈ›ie poate fi util pentru a determina cea mai bunÄƒ traducere a unui cuvÃ¢nt dat.

*attention mask* poate fi, de asemenea, utilizatÄƒ Ã®n codificator/decodificator pentru a Ã®mpiedica modelul sÄƒ acorde atenÈ›ie anumitor cuvinte speciale - de exemplu, cuvÃ¢ntul special de umpluturÄƒ utilizat pentru a face ca toate intrÄƒrile sÄƒ aibÄƒ aceeaÈ™i lungime atunci cÃ¢nd se grupeazÄƒ propoziÈ›iile.

##  Architectura vs. punctele de control[[architectura-vs-punctele-de-control]]

Pe mÄƒsurÄƒ ce analizÄƒm modelele Transformer Ã®n acest curs, veÈ›i vedea menÈ›iuni despre *arhitecturi* È™i *puncte de control*, precum È™i despre *modele*. ToÈ›i aceÈ™ti termeni au semnificaÈ›ii uÈ™or diferite: 

* **ArhitecturÄƒ**: Acesta este scheletul modelului - definiÈ›ia fiecÄƒrui strat È™i a fiecÄƒrei operaÈ›iuni care are loc Ã®n cadrul modelului. 
* **Puncte de control**: Acestea sunt ponderile care vor fi Ã®ncÄƒrcate Ã®ntr-o anumitÄƒ arhitecturÄƒ.
* **Model**: Acesta este un termen generic care nu este la fel de precis ca â€arhitecturÄƒâ€ sau â€punct de controlâ€: poate Ã®nsemna ambele. Acest curs va specifica *arhitecturÄƒ* sau *punct de control* atunci cÃ¢nd este necesar pentru a reduce ambiguitatea.

De exemplu, BERT este o arhitecturÄƒ, Ã®n timp ce `bert-base-cased`, un set de ponderi antrenate de echipa Google pentru prima versiune a BERT, este un punct de control. Cu toate acestea, se poate spune â€modelul BERTâ€ È™i â€modelul `bert-base-cased`â€.