# Modele Encoder[[modele-encoder]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="MUqNwgPjJvQ" />

Modelele Encoder utilizează doar encoderul unui model Transformer. La fiecare etapă, layer-urile de atenție pot accesa toate cuvintele din propoziția inițială. Aceste modele sunt adesea caracterizate ca având o atenție „bidirecțională” și sunt adesea numite *modele de autocodificare*.

Preinstruirea acestor modele se bazează, de obicei, pe alterarea unei propoziții date (de exemplu, prin mascarea unor cuvinte aleatorii) și pe sarcina modelului de a găsi sau reconstrui propoziția inițială.

Modelele Encoder sunt cele mai potrivite pentru sarcinile care necesită înțelegerea întregii propoziții, cum ar fi clasificarea propozițiilor, recunoașterea entităților numite (și, mai general, clasificarea cuvintelor) și Extractive QA.

Printre reprezentanții acestei familii de modele se numără:

- [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)
- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)
