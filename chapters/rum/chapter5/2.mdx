# Ce fac dacÄƒ dataset-ul meu nu este pe Hub?[[what-if-my-dataset-isnt-on-the-hub]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[  
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ro/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ro/chapter5/section2.ipynb"},  
]} />

Ai Ã®nvÄƒÈ›at sÄƒ foloseÈ™ti [Hugging Face Hub](https://huggingface.co/datasets) pentru a descÄƒrca dataseturi, dar vei gÄƒsi adesea cÄƒ lucraÈ›i cu date care sunt stocate fie pe laptopul dumneavoastrÄƒ, fie pe un server. Ãn aceastÄƒ secÈ›iune vÄƒ vom arÄƒta cum poate fi utilizat ğŸ¤— Datasets pentru a Ã®ncÄƒrca dataseturi care nu sunt disponibile pe Hugging Face Hub.
<Youtube id="HyQgpJTkRdE"/>

## LucrÃ¢nd cu dataseturi locale È™i remote[[working-with-local-and-remote-datasets]]

ğŸ¤— Datasets oferÄƒ loading scripts pentru a gestiona Ã®ncÄƒrcarea dataseturilor locale È™i remote. SuportÄƒ mai multe data formats , cum ar fi:

|    Data format     | Loading script |                         Example                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Text files     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

As shown in the table, for each data format we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files` argument that specifies the path to one or more files. Let's start by loading a dataset from local files; later we'll see how to do the same with remote files.

## Loading a local dataset[[loading-a-local-dataset]]

For this example we'll use the [SQuAD-it dataset](https://github.com/crux82/squad-it/), which is a large-scale dataset for question answering in Italian.

The training and test splits are hosted on GitHub, so we can download them with a simple `wget` command:

Ãn tabelele de mai sus, pentru fiecare format de date trebuie doar sÄƒ specificÄƒm tipul scriptului de Ã®ncÄƒrcare din funcÈ›ia `load_dataset()`, alÄƒturi de un argument `data_files` care specificÄƒ calea cÄƒtre un sau mai multe fiÈ™iere. Ãncepem cu Ã®ncÄƒrcarea datasetului din fiÈ™ierele locale; apoi vom vedea cum sÄƒ faceÈ›i acelaÈ™i lucru cu fiÈ™ierele remote.

## ÃncÄƒrcarea unui dataset local[[loading-a-local-dataset]]

Ãn acest exemplu, vom folosi [datasetul SQuAD-it](https://github.com/crux82/squad-it/), care este un large-scale dataset pentru Ã®ntrebÄƒri È™i rÄƒspunsuri Ã®n italianÄƒ.

Spliturile de training È™i test sunt disponibile pe GitHub, deci putem descarca cu o comanda `wget`:
```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

This will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json.gz*, which we can decompress with the Linux `gzip` command:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```



Putem observa cÄƒ fiÈ™ierele comprimate au fost Ã®nlocuite cu _SQuAD_it-train.json_ È™i _SQuAD_it-test.json_, iar datele sunt stocate Ã®n formatul JSON.

<Tip>
âœ DacÄƒ vÄƒ Ã®ntrebaÈ›i de ce existÄƒ un caracter `!` Ã®n comenziile shell din exemplu, acest lucru se Ã®ntÃ¢mplÄƒ pentru cÄƒ le-am rulat Ã®n cadrul unui jupyter notebook. Simplu sÄƒ È™tergeÈ›i prefixul dacÄƒ doriÈ›i sÄƒ descÄƒrcaÈ›i È™i sÄƒ faceÈ›i unzip datasetului Ã®ntr-un terminal.
</Tip>

SÄƒ Ã®ncarcÄƒm acum un fiÈ™ier JSON cu funcÈ›ia `load_dataset()`. Ne trebuie doar sÄƒ È™tim dacÄƒ ne confruntÄƒm cu un JSON obiÈ™nuit (similar cu un nested dictionary) sau JSON Lines (lines-separated JSON). Cum multe dataset-uri pentru Ã®ntrebÄƒri È™i rÄƒspunsuri, SQuAD-it foloseÈ™te formatul nested, Ã®n care toate textele sunt stocate Ã®ntr-un cÃ¢mp numit `data`. Acest lucru ne permite sÄƒ Ã®ncÄƒrcÄƒm datasetul specificÃ¢nd argumentul `field` astfel:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this by inspecting the `squad_it_dataset` object:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Acest lucru ne aratÄƒ numÄƒrul de rÃ¢nduri È™i numele coloanelor asociate cu setul de antrenare. Putem vizualiza un exemplu prin a indexa Ã®n `train` split astfel:
```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si Ã¨ verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Excelent, am reuÈ™it sÄƒ Ã®ncarcÄƒm datasetul nostru local! Dar, deÈ™i acest lucru a funcÈ›ionat pentru setul de antrenare, ceea ce dorim cu adevÄƒrat este sÄƒ includem ambele splituri, `train` È™i `test`, Ã®ntr-un singur obiect `DatasetDict` astfel Ã®ncÃ¢t sÄƒ putem aplica funcÈ›iile `Dataset.map()` asupra ambelor splituri Ã®n acelaÈ™i timp. Pentru a face acest lucru, putem furniza un dicÈ›ionar pentru argumentul `data_files` care va face maps fiecÄƒrui spilt name cu fiÈ™ierul asociat acelui split:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Astfel obÈ›inem exact ceea ce am dori. Acum putem aplica diverse tehnici de preprocesare pentru a curÄƒÈ›a datele, sÄƒ facem tokenize recenziilor È™i aÈ™a mai departe.

<Tip>

Argumentul `data_files` al funcÈ›iei `load_dataset()` este destul de flexibil È™i poate fi fie un singur file paths, o listÄƒ de file paths, sau un dicÈ›ionar care face maps split nameurilor cu file pathurile. De asemenea putem sÄƒ folosim glob files care se potrivest unui model specificat conform regulilor folosite de Unix shell (de exemplu putem sÄƒ facem glob tuturor fiÈ™ierelor JSON Ã®ntr-un folder ca un singur split setÃ¢nd `data_files="*.json"`). Pentru mai multe detalii consultaÈ›i [documentaÈ›ia](https://huggingface.co/docs/datasets/loading#local-and-remote-files) ğŸ¤— Datasets.

</Tip>

Scripturile de Ã®ncÄƒrcare din ğŸ¤— Datasets suportÄƒ automat decomprimarea fiÈ™ierelor de intrare, astfel putem sÄƒ evitÄƒm folosirea `gzip` arÄƒtÃ¢nd argumentul `data_files` direct cÄƒtre fiÈ™ierele comprimate:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Acest lucru poate fi util dacÄƒ nu dorim sÄƒ manualizÄƒm dezcomprimarea mai multe fiÈ™iere GZIP. Automatic decompression se aplicÄƒ È™i altor formate precum ZIP È™i TAR, astfel putem sÄƒ indicÄƒm `data_files` cÄƒtre fiÈ™ierele comprimate È™i suntem gata!

Acum cÄƒ È™tim cum sÄƒ Ã®ncÄƒrcÄƒm fiÈ™ierele locale pe laptop sau desktop, sÄƒ vedem cum sÄƒ Ã®ncarcÄƒm fiÈ™iere remote.

## ÃncÄƒrcarea unui dataset remote[[loading-a-remote-dataset]]

DacÄƒ lucraÈ›i ca Data Scientist sau programatori Ã®ntr-o companie, existÄƒ È™anse mari cÄƒ dataseturile pe care doriÈ›i sÄƒ le analizaÈ›i sunt stocate pe un anumit server. Din fericire, Ã®ncÄƒrcarea fiÈ™ierelor remote este la fel de simplÄƒ ca Ã®ncÄƒrcarea celor locale! Ãn schimb putem sÄƒ oferim pathul la fiÈ™iere locale, noi oferim argumentului `data_files` de la metoda `load_dataset()` cÄƒtre una sau mai multe adrese URL unde sunt stocate fiÈ™ierele remote. De exemplu pentru datasetul SQuAD-it gÄƒzduit pe GitHub, putem sÄƒ oferitum argumentului `data_files`, URL-urile _SQuAD_it-*.json.gz_ astfel:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Astfel obÈ›inem acelaÈ™i obiect `DatasetDict` ca mai sus, dar economisim timp, deoarece nu descÄƒrcÄƒm È™i decomprimÄƒm fiÈ™ierele _SQuAD_it-*.json.gz_. Asta Ã®ncheie aventura noastrÄƒ Ã®n diversele modalitÄƒÈ›i de Ã®ncarcare a dataseturilor care nu sunt gÄƒzduite pe Hugging Face Hub. Acum cÄƒ am avem un dataset la dispoziÈ›ie, hai sÄƒ Ã®l prelucrÄƒm!

<Tip>

âœï¸ **ÃncearcÄƒ!** AlegeÈ›i alt dataset gÄƒzduit pe GitHub sau [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) È™i Ã®ncercaÈ›i sÄƒ Ã®l Ã®ncÄƒrcaÈ›i atÃ¢t local cÃ¢t È™i remote folosind tehniciile introduse mai sus. Pentru puncte bonus, Ã®ncercaÈ›i sÄƒ Ã®ncÄƒrcaÈ›i un dataset care este stocat Ã®n format CSV sau text (consultaÈ›i [documentaÈ›ia](https://huggingface.co/docs/datasets/loading#local-and-remote-files) pentru detalii suplimentare despre aceste formate).

</Tip>




