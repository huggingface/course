<FrameworkSwitchCourse {fw} />

<!-- DISABLE-FRONTMATTER-SECTIONS -->

# Quiz de sf칙r탳it de capitol[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={7}
    classNames="absolute z-10 right-0 top-0"
/>

S캒 test캒m ce a탵i 칥nv캒탵at 칥n acest capitol!

### 1. Care dintre urm캒toarele sarcini pot fi 칥ncadrate ca o problem캒 de clasificare a tokenilor?

<Question
	choices={[
		{
			text: "G캒sirea componentelor gramaticale ale unei propozi탵ii.",
			explain: "Corect! Apoi putem eticheta fiecare cuv칙nt ca substantiv, verb etc.",
			correct: true
		},
		{
			text: "Verificarea dac캒 o propozi탵ie este corect캒 gramatical sau nu.",
			explain: "Nu, aceasta este o problem캒 de clasificare a secven탵elor."
		},
		{
			text: "G캒sirea persoanelor men탵ionate 칥ntr-o propozi탵ie.",
			explain: "Corect! Putem eticheta fiecare cuv칙nt ca persoan캒 sau nu ca o persoan캒.",
            correct: true
		},
        {
			text: "G캒sirea grupul de cuvinte dintr-o propozi탵ie care r캒spunde la o 칥ntrebare.",
			explain: "Nu, aceasta ar fi o problem캒 de r캒spundere a unei 칥ntreb캒ri."
		}
	]}
/>

### 2. Ce parte a preproces캒rii pentru clasificarea tokenilor difer캒 de celelalte pipelineuri de preprocesare?

<Question
	choices={[
		{
			text: "Nu este nevoie s캒 face탵i nimic; textele sunt deja tokenizate.",
			explain: "Textele sunt 칥ntr-adev캒r date ca cuvinte separate, dar tot trebuie s캒 aplic캒m modelul de tokenizare a sub-cuvintelor."
		},
		{
			text: "Textele sunt date sub form캒 de cuvinte, astfel 칥nc칙t trebuie s캒 aplic캒m doar tokenizarea sub-cuvintelor.",
			explain: "Corect! Acest lucru este diferit de preprocesarea obi탳nuit캒, 칥n care trebuie s캒 aplic캒m 칥ntregul proces de tokenizare. V캒 pute탵i g칙ndi la o alt캒 diferen탵캒?",
			correct: true
		},
		{
			text: "Folosim <code>-100</code> pentru a eticheta tokenii speciali.",
			explain: "Acest lucru nu este specific clasific캒rii tokenilor - folosim 칥ntotdeauna <code>-100</code> ca label pentru tokenii pe care dorim s캒 칥i ignor캒m 칥n pierdere."
		},
		{
			text: "Trebuie s캒 ne asigur캒m c캒 labelurile sunt trunchiate sau padded la aceea탳i dimensiune ca 탳i inputurile, atunci c칙nd aplic캒m trunchierea/paddingul.",
			explain: "칉ntr-adev캒r! Totu탳i, aceasta nu este singura diferen탵캒.",
			correct: true
		}
	]}
/>

### 3. Ce problem캒 apare atunci c칙nd tokeniz캒m cuvintele 칥ntr-o problem캒 de clasificare a tokenilor 탳i dorim s캒 etichet캒m tokenii?

<Question
	choices={[
		{
			text: "Tokenizerul adaug캒 tokeni speciali 탳i nu avem labeluri pentru ele.",
			explain: "Le etichet캒m pe acestea cu <code>-100</code>, astfel 칥nc칙t acestea s캒 fie ignorate 칥n pierdere."
		},
		{
			text: "Fiecare cuv칙nt poate produce mai mul탵i tokeni, astfel 칥nc칙t ajungem s캒 avem mai multe tokeni dec칙t labeluri.",
			explain: "Aceasta este problema principal캒, iar noi trebuie s캒 aliniem labelurile originale cu tokenii.",
			correct: true
		},
		{
			text: "Tokenii ad캒uga탵i nu au etichete, deci nu exist캒 nicio problem캒.",
			explain: "Incorect; avem nevoie de at칙tea etichete c칙탵i tokeni avem, altfel modelele noastre vor da erori."
		}
	]}
/>

### 4. Ce 칥nseamn캒 "domain adaptation"??

<Question
	choices={[
		{
			text: "Este atunci c칙nd rul캒m un model pe un dataset 탳i ob탵inem predic탵iile pentru fiecare sample din acel dataset.",
			explain: "Nu, aceasta este doar o inferen탵캒."
		},
		{
			text: "Este atunci c칙nd antren캒m un model pe un dataset.",
			explain: "Nu, acesta este antrenarea unui model; nu exist캒 nicio adaptare aici."
		},
		{
			text: "Este atunci c칙nd facem fine-tune unui model preantrenat pe un nou dataset, iar acesta ofer캒 predic탵ii care sunt mai bine adaptate la datasetul respectiv",
			explain: "Corect! Modelul 탳i-a adaptat cuno탳tin탵ele la noul dataset.",
            correct: true
		},
        {
			text: "Este atunci c칙nd ad캒ug캒m sampleuri neclasificate la un dataset pentru a face modelul nostru mai robust.",
			explain: "Cu siguran탵캒 este ceva ce ar trebui s캒 face탵i dac캒 v캒 reantrena탵i modelul 칥n mod regulat, dar nu este o adaptare la domeniu."
		}
	]}
/>

### 5. Ce sunt labelurile 칥ntr-o problem캒 de modelare a limbajului mascat?

<Question
	choices={[
		{
			text: "Unii dintre tokenii din propozi탵ia de intrare sunt mascate aleatoriu, iar labelurile sunt tokenii de input originali.",
			explain: "Corect!",
            correct: true
		},
		{
			text: "Unii dintre tokenii din propozi탵ia de intrare sunt mascate aleatoriu, iar labelurile sunt tokenii de input originali, deplasate spre st칙nga.",
			explain: "Nu, deplasarea labelurilor spre st칙nga corespunde prezicerii urm캒torului cuv칙nt, ceea ce este modelarea cauzal캒 a limbajului."
		},
		{
			text: "Unele dintre tokenii din propozi탵ia de intrare sunt mascate aleatoriu, iar labelul este dac캒 propozi탵ia este pozitiv캒 sau negativ캒.",
			explain: "Aceasta este o problem캒 de clasificare a secven탵elor cu o anumit캒 cre탳tere a datelor, nu de modelare a limbajului mascat."
		},
        {
			text: "Unii dintre tokenii din cele dou캒 propozi탵ii de intrare sunt mascate aleatoriu, iar labelul este dac캒 cele dou캒 propozi탵ii sunt similare sau nu.",
			explain: "Aceasta este o problem캒 de clasificare a secven탵elor cu o anumit캒 cre탳tere a datelor, nu de modelare a limbajului mascat."
		}
	]}
/>

### 6. Care dintre aceste sarcini poate fi v캒zut캒 ca o problem캒 de sequence-to-sequence?

<Question
	choices={[
		{
			text: "Scrierea de recenzii scurte ale documentelor lungi",
			explain: "Da, aceasta este o problem캒 de sumarizare. 칉ncerca탵i un alt r캒spuns!",
            correct: true
		},
		{
			text: "R캒spunderea 칥ntreb캒rilor despre un document",
			explain: "Acest lucru poate fi 칥ncadrat ca o problem캒 de sequence-to-sequence. Totu탳i, acesta nu este singurul r캒spuns corect.",
            correct: true
		},
		{
			text: "Traducerea unui text din chinez캒 칥n englez캒",
			explain: "Aceasta este cu siguran탵캒 o problem캒 de sequence-to-sequence. Mai pute탵i g캒si alta?",
            correct: true
		},
        {
			text: "Corectarea mesajelor trimise de nepotul/prietenul meu pentru ca acestea s캒 fie 칥n limba englez캒, scrise gramatical",
			explain: "Aceasta este un fel de problem캒 de traducere, deci cu siguran탵캒 o sarcin캒 de la sequence-to-sequence. Totu탳i, acesta nu este singurul r캒spuns corect!",
			correct: true
		}
	]}
/>

### 7. Care este modalitatea corect캒 de preprocesare a datelor pentru o problem캒 de sequence-to-sequence?

<Question
	choices={[
		{
			text: "Inputurile 탳i targeturile trebuie trimise 칥mpreun캒 la tokenizer cu <code>inputs=...</code> 탳i <code>targets=...</code>.",
			explain: "Acesta ar putea fi un API pe care 칥l vom ad캒uga 칥n viitor, dar nu este disponibil acum."
		},
		{
			text: "Inputurile 탳i targeturile trebuie preprocesate, 칥n dou캒 apeluri separate c캒tre tokenizer.",
			explain: "Acest lucru este adev캒rat, dar incomplet. Trebuie s캒 face탵i ceva pentru a v캒 asigura c캒 tokenizerul le proceseaz캒 pe ambele 칥n mod corespunz캒tor."
		},
		{
			text: "Ca de obicei, trebuie doar s캒 tokeniz캒m inputurile.",
			explain: "Nu 칥ntr-o problem캒 de clasificare a secven탵elor; targeturile sunt de asemenea texte pe care trebuie s캒 le convertim 칥n numere!"
		},
        {
			text: "Inputurile trebuie s캒 fie trimise c캒tre tokenizer, la fel 탳i targeturile, dar 칥n cadrul unui manager de context special.",
			explain: "Corect, tokenizerul trebuie s캒 fie pus 칥n target mode de c캒tre acel context manager.",
			correct: true
		}
	]}
/>

{#if fw === 'pt'}

### 8. De ce exist캒 o subclas캒 specific캒 a `Trainer` pentru problemele sequence-to-sequence?

<Question
	choices={[
		{
			text: "Deoarece problemele de sequence-to-sequence utilizeaz캒 o pierdere personalizat캒, pentru a ignora labelurile setate la <code>-100</code>",
			explain: "Aceasta nu este deloc o pierdere personalizat캒, ci modul 칥n care pierderea este 칥ntotdeauna calculat캒."
		},
		{
			text: "Deoarece problemele de sequence-to-sequence la secven탵캒 necesit캒 o bucl캒 de evaluare special캒",
			explain: "Acest lucru este corect. Predic탵iile modelelor sequence-to-sequence sunt de obicei rulate cu metoda <code>generate()</code>.",
			correct: true
		},
		{
			text: "Deoarece targeturile sunt texte 칥n probleme sequence-to-sequence",
			explain: "<code>Trainer</code>-ului nu prea 칥i pas캒 de asta, deoarece acestea au fost preprocesate 칥nainte."
		},
        {
			text: "Deoarece folosim dou캒 modele 칥n problemele sequence-to-sequence",
			explain: "칉ntr-un fel, folosim dou캒 modele, un codificator 탳i un decodificator, dar acestea sunt grupate 칥ntr-un singur model."
		}
	]}
/>

{:else}

### 9. De ce este adesea inutil s캒 se specifice o pierdere atunci c칙nd se apeleaz캒 `compile()` pe un model Transformer?

<Question
	choices={[
		{
			text: "Deoarece modelele Transformer sunt antrenate cu unsupervised learning",
			explain: "Nu chiar - chiar 탳i unsupervised learning are nevoie de o func탵ie de pierdere!"
		},
		{
			text: "Deoarece, 칥n mod prestabilit, se utilizeaz캒 valoarea pierderilor interne ale modelului",
			explain: "Corect!",
			correct: true
		},
		{
			text: "Deoarece calcul캒m metricele dup캒 antrenare",
			explain: "Facem adesea acest lucru, dar nu explic캒 de unde ob탵inem valoarea pierderii pe care o optimiz캒m 칥n formare."
		},
        {
			text: "Deoarece pierderea este specificat캒 칥n `model.fit()`",
			explain: "Nu, func탵ia de pierdere este 칥ntotdeauna fixat캒 odat캒 ce rula탵i `model.compile()`, 탳i nu poate fi modificat캒 칥n `model.fit()`."
		}
	]}
/>

{/if}

### 10. C칙nd ar trebui s캒 preantrena탵i un model nou?

<Question
	choices={[
		{
			text: "Atunci c칙nd nu exist캒 un model preantrenat disponibil pentru limba dorit캒.",
			explain: "Corect.",
			correct: true
		},
		{
			text: "Atunci c칙nd ave탵i o mul탵ime de date disponibile, chiar dac캒 exist캒 un model preantrenat care ar putea func탵iona pe ele",
			explain: "칉n acest caz, ar trebui, probabil, s캒 utiliza탵i modelul preantrenat 탳i s캒 칥i face탵i fine-tune pe datele voastre, pentru a evita costurile uria탳e de calcul."
		},
		{
			text: "Atunci c칙nd ave탵i 칥ndoieli cu privire la biasul modelului preantrenat pe care 칥l utiliza탵i",
			explain: "Este adev캒rat, dar trebuie s캒 v캒 asigura탵i c캒 datele pe care le ve탵i utiliza pentru antrenare sunt cu adev캒rat mai bune.",
			correct: true
		},
        {
			text: "Atunci c칙nd modelele preantrenate disponibile nu sunt suficient de bune",
			explain: "Sunte탵i sigur c캒 a탵i f캒cut debbuging 칥n mod corespunz캒tor antren캒rii?"
		}
	]}
/>

### 11. De ce este u탳or s캒 preantren캒m un model lingvistic pe o mul탵ime de texte?

<Question
	choices={[
		{
			text: "Deoarece exist캒 o mul탵ime de texte disponibile pe internet",
			explain: "De탳i este adev캒rat, acest lucru nu r캒spunde cu adev캒rat la 칥ntrebare. 칉ncerca탵i din nou!"
		},
		{
			text: "Deoarece obiectivul de preantrenare nu necesit캒 ca oamenii s캒 eticheteze datele",
			explain: "Acest lucru este corect, modelarea limbajului este o problem캒 autosupravegheat캒.",
			correct: true
		},
		{
			text: "Deoarece biblioteca 游뱅 Transformers necesit캒 doar c칙teva linii de cod pentru a 칥ncepe antrenarea",
			explain: "De탳i adev캒rat, acest lucru nu r캒spunde cu adev캒rat la 칥ntrebarea pus캒. 칉ncerca탵i un alt r캒spuns!"
		}
	]}
/>

### 12. Care sunt principalele provoc캒ri la preprocesarea datelor pentru o sarcin캒 de r캒spundere a 칥ntreb캒rilor?

<Question
	choices={[
		{
			text: "Trebuie s캒 tokeniza탵i inputurile.",
			explain: "Este corect, dar este cu adev캒rat o provocare principal캒?"
		},
		{
			text: "Trebuie s캒 ave탵i de-a face cu contexte foarte lungi, care ofer캒 mai multe caracteristici de antrenare care pot sau nu s캒 con탵in캒 r캒spunsul.",
			explain: "Aceasta este cu siguran탵캒 una dintre provoc캒ri.",
			correct: true
		},
		{
			text: "Trebuie s캒 tokeniza탵i r캒spunsurile la 칥ntreb캒ri, precum 탳i inputurile.",
			explain: "Nu, cu excep탵ia cazului 칥n care v캒 칥ncadra탵i problema de r캒spundere a 칥ntreb캒rilor ca o sarcin캒 de sequence-to-sequence."
		},
       {
			text: "Din intervalul r캒spunsului din text, trebuie s캒 g캒si탵i tokenul de 칥nceput 탳i de sf칙r탳it 칥n inputul tokenizat.",
			explain: "Aceasta este una dintre p캒r탵ile dificile!",
			correct: true
		}
	]}
/>

### 13. Cum se face de obicei post-procesarea 칥n r캒spunderea la 칥ntreb캒ri?

<Question
	choices={[
		{
			text: "Modelul v캒 ofer캒 pozi탵iile de 칥nceput 탳i de sf칙r탳it ale r캒spunsului, iar voi trebuie doar s캒 decodifica탵i intervalul corespunz캒tor de tokeni.",
			explain: "Acesta ar putea fi un mod de a face acest lucru, dar este un pic prea simplist."
		},
		{
			text: "Modelul v캒 ofer캒 pozi탵iile de 칥nceput 탳i de sf칙r탳it ale r캒spunsului pentru fiecare caracteristic캒 creat캒 de un exemplu, iar voi trebuie doar s캒 decodifica탵i intervalul corespunz캒tor de tokeni 칥n cel care are cel mai bun scor.",
			explain: "Acest lucru este apropiat de post-procesarea pe care am studiat-o, dar nu este complet corect."
		},
		{
			text: "Modelul v캒 ofer캒 pozi탵iile de 칥nceput 탳i de sf칙r탳it ale r캒spunsului pentru fiecare caracteristic캒 creat캒 de un exemplu, iar voi trebuie doar s캒 le potrivi탵i cu intervalul din context pentru cel care are cel mai bun scor.",
			explain: "Asta e corect!",
			correct: true
		},
        {
			text: "Modelul genereaz캒 un r캒spuns, iar tu trebuie doar s캒 칥l decodifici.",
			explain: "Nu, cu excep탵ia cazului 칥n care v캒 칥ncadra탵i problema de r캒spundere a 칥ntreb캒rii ca o sarcin캒 de sequence-to-sequence."
		}
	]}
/>
