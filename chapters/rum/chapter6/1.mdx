# Introducere[[introducere]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

Ãn [Capitolul 3](/course/chapter3), am examinat cum sÄƒ facem fine-tune unui model pentru o anumitÄƒ sarcinÄƒ. CÃ¢nd facem acest lucru, utilizÄƒm acelaÈ™i tokenizer cu care modelul a fost antrenat - dar ce facem cÃ¢nd dorim sÄƒ antrenÄƒm un model de la zero? Ãn aÈ™a cazuri, utilizarea unui tokenizer care a fost antrenat pe un corpus dintr-un alt domeniu sau limbÄƒ este, de obicei, suboptimal. De exemplu, un tokenizer antrenat pe un corpus Ã®n limba englezÄƒ va funcÈ›iona rÄƒu pe un corpus de texte Ã®n limba japonezÄƒ, deoarece utilizarea spaÈ›iilor È™i a punctuaÈ›iei este foarte diferitÄƒ Ã®n cele douÄƒ limbi.

Ãn acest capitol, veÈ›i Ã®nvÄƒÈ›a cum sÄƒ antrenaÈ›i un tokenizer complet nou pe un corpus de texte, astfel Ã®ncÃ¢t sÄƒ poatÄƒ fi utilizat pentru a antrena un model de limbaj. Acest lucru va fi realizat cu ajutorul bibliotecii [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers), care oferÄƒ tokenizerii "rapizi" din biblioteca [ğŸ¤— Transformers](https://github.com/huggingface/transformers). Vom examina Ã®ndeaproape caracteristicile pe care aceastÄƒ bibliotecÄƒ le oferÄƒ È™i vom explora cum tokenizatorii rapizi diferÄƒ de versiunile "lente".

Subiectele pe care le vom acoperi includ:

* Cum sÄƒ antrenaÈ›i un tokenizer nou, similar celui utilizat de un anumit checkpoint pe un corpus nou de texte
* Caracteristicile speciale ale tokenizerilor rapizi
* DiferenÈ›ele dintre cei trei algoritmi principali de subword tokenization utilizate Ã®n NLP Ã®n prezent
* Cum sÄƒ construiÈ›i un tokenizer de la zero cu biblioteca ğŸ¤— Tokenizers È™i sÄƒ Ã®l antrenaÈ›i pe anumite date

Tehnicile prezentate Ã®n acest capitol vÄƒ vor pregÄƒti pentru secÈ›iunea din [Capitolul 7](/course/chapter7/6), unde vom examina crearea unui model de limbaj pentru codul sursÄƒ Python. SÄƒ Ã®ncepem prin a explora ce Ã®nseamnÄƒ sÄƒ "antrenaÈ›i" un tokenizer Ã®n primul rÃ¢nd.
