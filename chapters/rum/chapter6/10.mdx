<!-- DISABLE-FRONTMATTER-SECTIONS -->

# Quiz de sf칙r탳it de capitol[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

Hai s캒 test캒m ceea ce ai 칥nv캒탵at 칥n acest capitol!

### 1. C칙nd ar trebui s캒 antrenezi un nou tokenizer?

<Question
	choices={[
		{
			text: "C칙nd datasetul t캒u este similar cu cel utilizat de un model preantrenat 탳i dori탵i s캒 preantrena탵i un model nou",
			explain: "칉n acest caz, pentru a economisi timp 탳i resurse, o alegere mai bun캒 ar fi s캒 folose탳ti acela탳i tokenizer ca 탳i modelul preantrenat 탳i s캒 face탵i fine-tune acelui model."
		},
		{
			text: "Atunci c칙nd datasetul t캒u este similar cu cel utilizat de un model preantrenat existent 탳i dori탵i s캒 face탵i fine-tune unui nou model utiliz칙nd acest model preantrenat",
			explain: "Pentru a face fine-tune unui model dintr-un model preantrenat, trebuie s캒 utiliza탵i 칥ntotdeauna acela탳i tokenizer."
		},
		{
			text: "Atunci c칙nd datasetul t캒u este diferit de cel utilizat de un model preantrenat existent 탳i dori탵i s캒 preantrena탵i un nou model",
			explain: "Corect! 칉n acest caz nu exist캒 niciun avantaj 칥n a utiliza acela탳i tokenizer.",
            correct: true
		},
        {
			text: "Atunci datasetul t캒u este diferit de cel utilizat de un model preantrenat existent, dar dori탵i s캒 face탵i fine-tune unui model nou utiliz칙nd acest model preantrenat",
			explain: "Pentru a face fine-tune unui model dintr-un model preantrenat, trebuie s캒 utiliza탵i 칥ntotdeauna acela탳i tokenizer."
		}
	]}
/>

### 2. Care este avantajul utiliz캒rii unui generator de liste de texte 칥n compara탵ie cu o list캒 de liste de texte atunci c칙nd utiliza탵i `train_new_from_iterator()`?

<Question
	choices={[
		{
			text: "Acesta este singurul tip pe care metoda <code>train_new_from_iterator()</code> 칥l accept캒.",
			explain: "O list캒 de liste de texte este un tip special de generator de liste de texte, astfel 칥nc칙t metoda o va accepta 탳i pe aceasta. 칉ncerca탵i din nou!"
		},
		{
			text: "Ve탵i evita 칥nc캒rcarea 칥ntregului dataset 칥n memorie.",
			explain: "Corect! Fiecare batch de texte va fi eliberat din memorie atunci c칙nd itera탵i, iar c칙탳tigul va fi vizibil mai ales dac캒 utiliza탵i 游뱅 Datasets pentru a stoca textele.",
			correct: true
		},
		{
			text: "Acest lucru va permite bibliotecii 游뱅 Tokenizers s캒 utilizeze multiprocessing.",
			explain: "Nu, oricum va folosi multiprocessing."
		},
        {
			text: "Tokenizerul pe care 칥l vei antrena va genera texte mai bune.",
			explain: "Tokenizerul nu genereaz캒 text - 칥l confunda탵i cu un model lingvistic?"
		}
	]}
/>

### 3. Care sunt avantajele utiliz캒rii unui tokenizer "rapid"?

<Question
	choices={[
		{
			text: "Acesta poate procesa inputuri mai rapid dec칙t un tokenizator lent atunci c칙nd faci batch mai multor inputuri 칥mpreun캒.",
			explain: "Corect! Datorit캒 paralelismului implementat 칥n Rust, acesta va fi mai rapid pe batch-uri de inputuri. La ce alt beneficiu te po탵i g칙ndi?",
			correct: true
		},
		{
			text: "Tokenizerele rapide tokenizeaz캒 칥ntotdeauna mai rapid dec칙t omologii lor len탵i.",
			explain: "Un tokenizer rapid poate fi de fapt mai lent atunci c칙nd 칥i dai doar unul sau foarte pu탵ine texte, deoarece nu poate utiliza paralelismul."
		},
		{
			text: "Poate aplica padding 탳i truncation.",
			explain: "Adev캒rat, dar 탳i tokenizerele lente fac asta."
		},
        {
			text: "Acesta are unele caracteristici suplimentare care v캒 permit s캒 asocia탵i tokenii cu intervalul de text care le-a creat.",
			explain: "칉ntr-adev캒r - acestea se numesc offset mappings. Totu탳i, acesta nu este singurul avantaj.",
			correct: true
		}
	]}
/>

### 4. Cum trateaz캒 pipelineul `token-classification` entit캒탵ile care se 칥ntind pe mai mul탵i tokeni?

<Question
	choices={[
		{
			text: "Entit캒탵ile cu acela탳i label sunt merged 칥ntr-o singur캒 entitate.",
			explain: "Explica탵ia aceasta e prea simpl캒. 칉ncearc캒 din nou!"
		},
		{
			text: "Exist캒 un label pentru 칥nceputul unei entit캒탵i 탳i un label pentru continuarea unei entit캒탵i.",
			explain: "Corect!",
			correct: true
		},
		{
			text: "칉ntr-un cuv칙nt dat, at칙ta timp c칙t primul token are labelul entit캒탵ii, 칥ntregul cuv칙nt este considerat etichetat cu entitatea respectiv캒.",
			explain: "Aceasta este o strategie de gestionare a entit캒탵ilor. Ce alte r캒spunsuri se aplic캒 aici?",
			correct: true
		},
        {
			text: "Atunci c칙nd un token are labelul unei entit캒탵i date, orice alt token urm캒tor cu aceea탳i label este considerat parte a aceleia탳i entit캒탵i, cu excep탵ia cazului 칥n care este etichetat ca fiind 칥nceputul unei noi entit캒탵i.",
			explain: "Acesta este cel mai comun mod de a grupa entit캒탵ile 칥mpreun캒 - de탳i nu este singurul r캒spuns corect.",
			correct: true
		}
	]}
/>

### 5. Cum gestioneaz캒 pipelineul `question-answering` contextele lungi?

<Question
	choices={[
		{
			text: "De fapt, nu o face, deoarece trunchiaz캒 contextul lung la lungimea maxim캒 acceptat캒 de model.",
			explain: "Exist캒 un truc pe care 칥l po탵i folosi pentru a gestiona contextele lungi. 칉탵i aminte탳ti care este acesta?"
		},
		{
			text: "Acesta 칥mparte contextul 칥n mai multe p캒r탵i 탳i calculeaz캒 media rezultatelor ob탵inute.",
			explain: "Nu, nu ar avea sens s캒 se fac캒 o medie a rezultatelor, deoarece unele p캒r탵i ale contextului nu vor include r캒spunsul."
		},
		{
			text: "Acesta 칥mparte contextul 칥n mai multe p캒r탵i (cu suprapuneri) 탳i g캒se탳te scorul maxim pentru un r캒spuns 칥n fiecare parte.",
			explain: "Acesta este r캒spunsul corect!",
			correct: true
		},
        {
			text: "Acesta 칥mparte contextul 칥n mai multe p캒r탵i (f캒r캒 suprapunere, pentru eficien탵캒) 탳i g캒se탳te scorul maxim pentru un r캒spuns 칥n fiecare parte.",
			explain: "Nu, acesta include o anumit캒 suprapunere 칥ntre p캒r탵i pentru a evita situa탵ia 칥n care r캒spunsul ar fi 칥mp캒r탵it 칥n dou캒 p캒r탵i."
		}
	]}
/>

### 6. Ce este normalizarea?

<Question
	choices={[
		{
			text: "Este vorba de orice cur캒탵are pe care tokenizerul o efectueaz캒 asupra textelor 칥n etapele ini탵iale.",
			explain: "Corect - de exemplu, ar putea implica eliminarea accentelor sau a spa탵iilor, sau scrierea cu mijuscule a inputurilor.",
			correct: true
		},
		{
			text: "Este o tehnic캒 de augmentare a datelor care const캒 칥n a face textul mai normal prin eliminarea cuvintelor rare.",
			explain: "Este incorect! 칉ncearc캒 din nou."
		},
		{
			text: "Acesta este ultimul pas de post-procesare 칥n care tokenizerul adaug캒 tokenii speciali.",
			explain: "Aceast캒 etap캒 se nume탳te pur 탳i simplu post-procesare."
		},
        {
			text: "Este atunci c칙nd embeddingurile sunt realizate cu media 0 탳i standard deviation 1, prin sc캒derea mediei 탳i 칥mp캒r탵irea la std.",
			explain: "Acest proces se nume탳te de obicei normalizare atunci c칙nd este aplicat valorilor pixelilor 칥n computer vision, dar nu este ceea ce 칥nseamn캒 normalizare 칥n NLP."
		}
	]}
/>

### 7. Ce este pre-tokenizarea pentru un subword tokenizer?

<Question
	choices={[
		{
			text: "Acesta este pasul dinaintea tokeniz캒rii, 칥n care se aplic캒 augmentarea datelor (cum ar fi mascarea aleatorie).",
			explain: "Nu, acest pas face parte din preprocesare."
		},
		{
			text: "Este pasul de dinaintea tokeniz캒rii, 칥n care opera탵iile de cur캒탵are dorite sunt aplicate textului.",
			explain: "Nu, acesta este pasul de normalizare."
		},
		{
			text: "Acesta este pasul dinaintea aplic캒rii modelul de tokenizer, pentru a 칥mp캒r탵i inputul 칥n cuvinte.",
			explain: "Acesta este r캒spunsul corect!",
			correct: true
		},
        {
			text: "Acesta este pasul dinaintea aplic캒rii modelul de  tokenizer, pentru a 칥mp캒r탵i inputul 칥n tokenuri.",
			explain: "Nu, 칥mp캒r탵irea 칥n tokenuri este sarcina modelul de tokenizer."
		}
	]}
/>

### 8. Selecta탵i propozi탵iile care se aplic캒 modelului de tokenizare BPE.

<Question
	choices={[
		{
			text: "BPE este un algoritm de tokenizare a subcuvintelor care 칥ncepe cu un vocabular mic 탳i 칥nva탵캒 reguli de merge.",
			explain: "Acesta este 칥ntr-adev캒r cazul!",
			correct: true
		},
		{
			text: "BPE este un algoritm de tokenizare a subcuvintelor care 칥ncepe cu un vocabular mare 탳i elimin캒 progresiv tokenii din acesta.",
			explain: "Nu, aceasta este abordarea adoptat캒 de un alt algoritm de tokenizare."
		},
		{
			text: "Tokenizerele BPE 칥nva탵캒 regulile de merge prin mergeul perechii de tokeni care este cea mai frecvent캒.",
			explain: "Acest lucru este corect!",
			correct: true
		},
		{
			text: "Un tokenizer BPE 칥nva탵캒 o regul캒 de merge prin mergeul perechii de tokeni care maximizeaz캒 un scor care privilegiaz캒 perechile frecvente cu p캒r탵i individuale mai pu탵in frecvente.",
			explain: "Nu, aceasta este strategia aplicat캒 de un alt algoritm de tokenizare."
		},
		{
			text: "BPE tokenizeaz캒 cuvintele 칥n subcuvinte prin divizarea lor 칥n caractere 탳i apoi prin aplicarea regulilor de merge.",
			explain: "Acest lucru este corect!",
			correct: true
		},
		{
			text: "BPE tokenizeaz캒 cuvintele 칥n subcuvinte prin g캒sirea celui mai lung subcuv칙nt de la 칥nceput care se afl캒 칥n vocabular, apoi repet캒 procesul pentru restul textului.",
			explain: "Nu, acesta este un alt mod de a face lucrurile al algoritmului de tokenizare."
		},
	]}
/>

### 9. Selecta탵i propozi탵iile care se aplic캒 modelului de tokenizare WordPiece.

<Question
	choices={[
		{
			text: "WordPiece este un algoritm de tokenizare a subcuvintelor care 칥ncepe cu un vocabular mic 탳i 칥nva탵캒 reguli de merge.",
			explain: "Acesta este 칥ntr-adev캒r cazul!",
			correct: true
		},
		{
			text: "WordPiece este un algoritm de tokenizare a subcuvintelor care 칥ncepe cu un vocabular mare 탳i elimin캒 progresiv tokenii din acesta.",
			explain: "Nu, aceasta este abordarea adoptat캒 de un alt algoritm de tokenizare."
		},
		{
			text: "Tokenizerele WordPiece 칥nva탵캒 regulile de merge prin mergeul perechii de tokeni care este cea mai frecvent캒.",
			explain: "Nu, aceasta este abordarea adoptat캒 de un alt algoritm de tokenizare!",
		},
		{
			text: "Un tokenizer WordPiece 칥nva탵캒 o regul캒 de merge prin mergeul perechii de tokeni care maximizeaz캒 un scor care privilegiaz캒 perechile frecvente cu p캒r탵i individuale mai pu탵in frecvente.",
			explain: "Acest lucru este corect!",
			correct: true
		},
		{
			text: "WordPiece tokenizeaz캒 cuvintele 칥n subcuvinte prin g캒sirea celei mai probabile segment캒ri 칥n tokeni, 칥n conformitate cu modelul.",
			explain: "Nu, acesta este modul 칥n care func탵ioneaz캒 un alt algoritm de tokenizare."
		},
		{
			text: "WordPiece tokenizeaz캒 cuvintele 칥n subcuvinte prin g캒sirea celui mai lung subcuv칙nt de la 칥nceput care se afl캒 칥n vocabular, apoi repet캒 procesul pentru restul textului.",
			explain: "Da, acesta este modul 칥n care WordPiece procedeaz캒 pentru encoding.",
			correct: true
		},
	]}
/>

### 10. Selecta탵i propozi탵iile care se aplic캒 modelului de tokenizare Unigram.

<Question
	choices={[
		{
			text: "Unigram este un algoritm de tokenizare a subcuvintelor care 칥ncepe cu un vocabular mic 탳i 칥nva탵캒 reguli de merge.",
			explain: "Nu, aceasta este abordarea adoptat캒 de un alt algoritm de tokenizare."
		},
		{
			text: "Unigram este un algoritm de tokenizare a subcuvintelor care 칥ncepe cu un vocabular mare 탳i elimin캒 progresiv tokenurile din el.",
			explain: "Acest lucru este corect!",
			correct: true
		},
		{
			text: "Unigram 칥탳i adapteaz캒 vocabularul prin minimizarea unei pierderi calculate pe 칥ntregul corpus.",
			explain: "Acest lucru este corect!",
			correct: true
		},
		{
			text: "Unigram 칥탳i adapteaz캒 vocabularul prin p캒strarea celor mai frecvente subcuvinte.",
			explain: "Nu, acest lucru este incorect."
		},
		{
			text: "Unigram tokenizeaz캒 cuvintele 칥n subcuvinte prin g캒sirea celei mai probabile segment캒ri 칥n tokenuri, conform modelului.",
			explain: "Acest lucru este corect!",
			correct: true
		},
		{
			text: "Unigram tokenizeaz캒 cuvintele 칥n subcuvinte prin divizarea lor 칥n caractere, apoi aplic캒 regulile de merge.",
			explain: "Nu, acesta este modul 칥n care func탵ioneaz캒 un alt algoritm de tokenizare."
		},
	]}
/>
