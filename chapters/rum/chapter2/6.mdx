<FrameworkSwitchCourse {fw} />

# SÄƒ punem totul cap la cap[[sÄƒ-punem-totul-cap-la-cap]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},
]} />

{/if}

Ãn ultimele secÈ›iuni, ne-am strÄƒduit sÄƒ facem cea mai mare parte a muncii manual. Am explorat modul Ã®n care funcÈ›ioneazÄƒ tokenizatoarele È™i am analizat tokenizarea, conversia Ã®n ID-uri de intrare, padding, trunchiere È™i mÄƒÈ™ti de atenÈ›ie.

Cu toate acestea, dupÄƒ cum am vÄƒzut Ã®n secÈ›iunea 2, API-ul ğŸ¤— Transformers poate gestiona toate acestea pentru noi cu o funcÈ›ie de nivel Ã®nalt Ã®n care ne vom adÃ¢nci aici. Atunci cÃ¢nd apelaÈ›i `tokenizer` direct pe propoziÈ›ie, primiÈ›i Ã®napoi intrÄƒri care sunt gata sÄƒ treacÄƒ prin modelul dvs:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

Aici, variabila `model_inputs` conÈ›ine tot ceea ce este necesar pentru ca un model sÄƒ funcÈ›ioneze bine. Pentru DistilBERT, aceasta include ID-urile de intrare, precum È™i masca de atenÈ›ie. Alte modele care acceptÄƒ intrÄƒri suplimentare le vor avea, de asemenea, la ieÈ™ire prin obiectul `tokenizer`.

DupÄƒ cum vom vedea Ã®n cÃ¢teva exemple de mai jos, aceastÄƒ metodÄƒ este foarte puternicÄƒ. Ãn primul rÃ¢nd, poate tokeniza o singurÄƒ secvenÈ›Äƒ:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

De asemenea, gestioneazÄƒ mai multe secvenÈ›e simultan, fÄƒrÄƒ nicio modificare a API-ului:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

Poate aplica padding Ã®n funcÈ›ie de mai multe obiective:

```py
# Va umple secvenÈ›ele pÃ¢nÄƒ la lungimea maximÄƒ a secvenÈ›ei
model_inputs = tokenizer(sequences, padding="longest")

# Va umple secvenÈ›ele pÃ¢nÄƒ la lungimea maximÄƒ a modelului
# (512 pentru BERT sau DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Va umple secvenÈ›ele pÃ¢nÄƒ la lungimea maximÄƒ specificatÄƒ
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

De asemenea, poate trunchia secvenÈ›ele:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Va trunchia secvenÈ›ele care sunt mai lungi decÃ¢t lungimea maximÄƒ a modelului
# (512 pentru BERT sau DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Va trunchia secvenÈ›ele care sunt mai lungi decÃ¢t lungimea maximÄƒ specificatÄƒ
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

Obiectul `tokenizer` poate gestiona conversia Ã®n tensori specifici framework-ului, care pot fi apoi trimiÈ™i direct la model. De exemplu, Ã®n urmÄƒtorul exemplu de cod, solicitÄƒm tokenizatorului sÄƒ returneze tensori din diferite framework-uri - `â€ptâ€` returneazÄƒ tensori PyTorch, `â€tfâ€` returneazÄƒ tensori TensorFlow, iar `â€npâ€` returneazÄƒ matrici NumPy:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# ReturneazÄƒ tensori PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# ReturneazÄƒ tensori TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# ReturneazÄƒ array-uri NumPy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## Token-uri speciale[[token-uri-speciale]]

DacÄƒ aruncÄƒm o privire la ID-urile de intrare returnate de tokenizer, vom vedea cÄƒ sunt puÈ›in diferite de cele pe care le-am avut mai devreme:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

Un token ID a fost adÄƒugat la Ã®nceput, iar unul la sfÃ¢rÈ™it. SÄƒ decodificÄƒm cele douÄƒ secvenÈ›e de ID-uri de mai sus pentru a vedea despre ce este vorba:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

Tokenizatorul a adÄƒugat cuvÃ¢ntul special `[CLS]` la Ã®nceput È™i cuvÃ¢ntul special `[SEP]` la sfÃ¢rÈ™it. Acest lucru se datoreazÄƒ faptului cÄƒ modelul a fost preantrenat cu aceste cuvinte, deci pentru a obÈ›ine aceleaÈ™i rezultate pentru inferenÈ›Äƒ trebuie sÄƒ le adÄƒugÄƒm È™i pe acestea. ReÈ›ineÈ›i cÄƒ unele modele nu adaugÄƒ cuvinte speciale sau adaugÄƒ cuvinte diferite; de asemenea, modelele pot adÄƒuga aceste cuvinte speciale doar la Ã®nceput sau doar la sfÃ¢rÈ™it. Ãn orice caz, tokenizatorul È™tie care sunt cele aÈ™teptate È™i se va ocupa de acest lucru pentru dumneavoastrÄƒ.

## Ãncheiere: De la tokenizer la model[[Ã®ncheiere-de-la-tokenizator-la-model]]

Acum cÄƒ am vÄƒzut toÈ›i paÈ™ii individuali pe care Ã®i utilizeazÄƒ obiectul `tokenizer` atunci cÃ¢nd este aplicat pe texte, sÄƒ vedem o ultimÄƒ datÄƒ cum poate gestiona secvenÈ›e multiple (padding!), secvenÈ›e foarte lungi (trunchiere!) È™i mai multe tipuri de tensori cu API-ul sÄƒu principal:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
