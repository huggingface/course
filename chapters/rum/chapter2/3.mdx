<FrameworkSwitchCourse {fw} />

# Modele[[modele]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
 
칉n aceast캒 sec탵iune vom analiza mai detaliat crearea 탳i utilizarea unui model. Vom folosi clasa `AutoModel`, care este util캒 atunci c칙nd dori탵i s캒 instan탵ia탵i orice model dintr-un checkpoint.

Clasa `AutoModel` 탳i toate celelalte clase 칥nrudite sunt, de fapt, simple wrappere ale gamei largi de modele disponibile 칥n bibliotec캒. Este un wrapper inteligent, deoarece poate ghici automat arhitectura modelului corespunz캒tor pentru checkpoint-ul dumneavoastr캒 탳i apoi instan탵iaz캒 un model cu aceast캒 arhitectur캒.

{:else}
 
칉n aceast캒 sec탵iune vom analiza mai detaliat crearea 탳i utilizarea unui model. Vom utiliza clasa `TFAutoModel`, care este util캒 atunci c칙nd dori탵i s캒 instan탵a탵i un model dintr-un checkpoint.

Clasa `TFAutoModel` 탳i toate clasele 칥nrudite cu aceasta sunt de fapt simple wrappere ale gamei largi de modele disponibile 칥n bibliotec캒. Este un wrapper inteligent, deoarece poate ghici automat arhitectura modelului corespunz캒tor pentru checkpoint-ul dumneavoastr캒 탳i apoi instan탵iaz캒 un model cu aceast캒 arhitectur캒.

{/if}

Cu toate acestea, dac캒 탳ti탵i ce tip de model dori탵i s캒 utiliza탵i, pute탵i folosi direct clasa care define탳te arhitectura acestuia. S캒 arunc캒m o privire la modul 칥n care func탵ioneaz캒 acest lucru cu un model BERT.

## Crearea unui Transformer[[crearea-unui-transformer]]

Primul lucru pe care va trebui s캒 칥l facem pentru a ini탵ializa un model BERT este s캒 칥nc캒rc캒m un obiect de configurare:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Construirea configura탵iei
config = BertConfig()

# Construirea modelului pornind de la configura탵ie
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Construirea configura탵iei
config = BertConfig()

#  Construirea modelului pornind de la configura탵ie
model = TFBertModel(config)
```
{/if}

Configura탵ia con탵ine multe atribute care sunt utilizate pentru a construi modelul:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

De탳i nu a탵i v캒zut 칥nc캒 ce fac toate aceste atribute, ar trebui s캒 recunoa탳te탵i unele dintre ele: atributul `hidden_size` define탳te dimensiunea vectorului `hidden_states`, iar `num_hidden_layers` define탳te num캒rul de straturi pe care le are modelul Transformer.

### Diferite metode de 칥nc캒rcare[[diferite-metode-de-칥nc캒rcare]]

Crearea unui model din configura탵ia implicit캒 칥l ini탵ializeaz캒 cu valori aleatorii:


{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Modelul este ini탵ializat aleatoriu!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Modelul este ini탵ializat aleatoriu!
```
{/if}

Modelul poate fi utilizat 칥n aceast캒 stare, dar va produce rezultate neclare; mai 칥nt칙i trebuie s캒 fie antrenat. Am putea antrena modelul de la zero pentru sarcina 칥n cauz캒, dar, dup캒 cum a탵i v캒zut 칥n [Capitolul 1](/course/chapter1), acest lucru ar necesita mult timp 탳i o mul탵ime de date 탳i ar avea un impact semnificativ asupra mediului. Pentru a evita efortul inutil, este esen탵ial s캒 pute탵i partaja 탳i reutiliza modelele care au fost deja antrenate.

칉nc캒rcarea unui model Transformer care este deja instruit este simpl캒 - putem face acest lucru utiliz칙nd metoda `from_pretrained()`, care este analog캒 metodei `from_pretrained()` 칥n PyTorch:

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

Dup캒 cum a탵i v캒zut mai devreme, am putea 칥nlocui `BertModel` cu clasa `AutoModel` echivalent캒. Vom face acest lucru 칥n continuare, deoarece acest lucru produce un cod independent de checkpoint-uri; dac캒 codul dumneavoastr캒 func탵ioneaz캒 pentru un checkpoint, ar trebui s캒 func탵ioneze f캒r캒 probleme pentru altul. Acest lucru este valabil chiar dac캒 arhitectura este diferit캒, at칙t timp c칙t checkpoint-ul a fost instruit pentru o sarcin캒 similar캒 (de exemplu, o sarcin캒 de analiz캒 a sentimentelor).

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

Dup캒 cum a탵i v캒zut mai devreme, am putea 칥nlocui `BertModel` cu clasa `AutoModel` echivalent캒. Vom face acest lucru 칥n continuare, deoarece acest lucru produce un cod independent de checkpoint-uri; dac캒 codul dumneavoastr캒 func탵ioneaz캒 pentru un checkpoint, ar trebui s캒 func탵ioneze f캒r캒 probleme pentru altul. Acest lucru este valabil chiar dac캒 arhitectura este diferit캒, at칙t timp c칙t checkpoint-ul a fost instruit pentru o sarcin캒 similar캒 (de exemplu, o sarcin캒 de analiz캒 a sentimentelor).

{/if}

칉n exemplul de cod de mai sus nu am utilizat `BertConfig` 탳i, 칥n loc de acesta, am 칥nc캒rcat un model preantrenat prin intermediul identificatorului `bert-base-cased`. Acesta este un checkpoint al modelului care a fost antrenat chiar de autorii BERT; pute탵i g캒si mai multe detalii despre acesta 칥n [model card](https://huggingface.co/bert-base-cased).

Acest model este acum ini탵ializat cu toate weight-urile din checkpoint. Acesta poate fi utilizat direct pentru inferen탵캒 pe sarcinile pe care a fost antrenat 탳i poate fi, de asemenea, ajustat pe o sarcin캒 nou캒. Prin antrenarea cu weight-uri preantrenate 칥n prealabil, putem ob탵ine rapid rezultate bune.

Greut캒탵ile au fost desc캒rcate 탳i stocate 칥n memorie cache (astfel 칥nc칙t apelurile viitoare la metoda `from_pretrained()` nu le vor desc캒rca din nou) 칥n folderul cache, care are ca valoare implicit캒 *~/.cache/huggingface/transformers*. Pute탵i personaliza folderul cache prin setarea variabilei de mediu `HF_HOME`.

Identificatorul utilizat pentru 칥nc캒rcarea modelului poate fi identificatorul oric캒rui model de pe Model Hub, at칙ta timp c칙t acesta este compatibil cu arhitectura BERT. Lista complet캒 a checkpoint-urilor BERT disponibile poate fi g캒sit캒 [aici](https://huggingface.co/models?filter=bert).


### Metode de salvare[[metode-de-salvare]]

Salvarea unui model este la fel de u탳oar캒 ca 칥nc캒rcarea unuia - folosim metoda `save_pretrained()`, care este analog캒 metodei `from_pretrained()`:

```py
model.save_pretrained("directory_on_my_computer")
```

Se salveaz캒 dou캒 fi탳iere pe disc:

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.safetensors
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

Dac캒 arunca탵i o privire la fi탳ierul *config.json*, ve탵i recunoa탳te atributele necesare pentru construirea arhitecturii modelului. Acest fi탳ier con탵ine, de asemenea, unele metadate, cum ar fi locul de origine al checkpoint-ului 탳i ce versiune 游뱅 Transformers a탵i folosit c칙nd a탵i salvat ultima dat캒 checkpoint-ul.

{#if fw === 'pt'}
 
Fi탳ierul *pytorch_model.safetensors* este cunoscut sub numele de *state dictionary*; acesta con탵ine toate weight-urile modelului dumneavoastr캒. Cele dou캒 fi탳iere merg m칙n캒 칥n m칙n캒; configura탵ia este necesar캒 pentru a cunoa탳te arhitectura modelului, 칥n timp ce weight-urile modelului sunt parametrii modelului.

{:else}
 
Fi탳ierul *tf_model.h5* este cunoscut sub numele de *state dictionary*; acesta con탵ine toate weight-urile modelului dumneavoastr캒. Cele dou캒 fi탳iere merg m칙n캒 칥n m칙n캒; configura탵ia este necesar캒 pentru a cunoa탳te arhitectura modelului, 칥n timp ce weight-urile modelului sunt parametrii modelului.

{/if}

## Utilizarea unui model Transformer pentru inferen탵캒[[utilizarea-unui-model-transformer-pentru-inferen탵캒]]


Acum c캒 탳ti탵i cum s캒 칥nc캒rca탵i 탳i s캒 salva탵i un model, s캒 칥ncerc캒m s캒 칥l folosim pentru a face c칙teva predic탵ii. Modelele Transformer pot procesa doar numere - numere pe care le genereaz캒 tokenizatorul. Dar 칥nainte de a discuta despre tokenizeri, s캒 explor캒m ce intr캒ri accept캒 modelul.

Tokenizerii se pot ocupa de transformarea intr캒rilor 칥n tensori ai framework-ului corespunz캒tor, dar pentru a v캒 ajuta s캒 칥n탵elege탵i ce se 칥nt칙mpl캒, vom arunca o privire rapid캒 la ceea ce trebuie f캒cut 칥nainte de trimiterea intr캒rilor c캒tre model.

S캒 spunem c캒 avem c칙teva secven탵e:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

Tokenizatorul le converte탳te 칥n indici de vocabular care sunt denumi탵i 칥n mod obi탳nuit *input IDs*. Fiecare secven탵캒 este acum o list캒 de numere! Rezultatul este:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

Aceasta este o list캒 de secven탵e codificate: o list캒 de liste. Tensorii accept캒 numai forme dreptunghiulare (g칙ndi탵i-v캒 la matrici). Acest "array" are deja o form캒 dreptunghiular캒, astfel 칥nc칙t convertirea sa 칥ntr-un tensor este u탳oar캒:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### Utilizarea tensorilor ca intr캒ri 칥n model[[utilizarea-tensorilor-ca-intr캒ri-칥n-model]]

Utilizarea tensorilor cu ajutorul modelului este extrem de simpl캒 - trebuie doar s캒 apel캒m modelul cu intr캒rile:

```py
output = model(model_inputs)
```

De탳i modelul accept캒 o mul탵ime de argumente diferite, doar ID-urile de intrare sunt necesare. Vom explica mai t칙rziu ce fac celelalte argumente 탳i c칙nd sunt necesare, 
dar mai 칥nt칙i trebuie s캒 arunc캒m o privire mai atent캒 la tokenizerii care construiesc intr캒rile pe care un model Transformer le poate 칥n탵elege.