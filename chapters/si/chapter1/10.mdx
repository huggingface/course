<!-- DISABLE-FRONTMATTER-SECTIONS -->

# ‡∂¥‡∂ª‡∑í‡∂†‡∑ä‡∂°‡∑ö‡∂Ø‡∂∫ ‡∂Ö‡∑Ä‡∑É‡∑è‡∂±‡∂∫‡∑ö ‡∂¥‡∑ä‚Äç‡∂ª‡∑Å‡∑ä‡∂±‡∑è‡∑Ä‡∂Ω‡∑í‡∂∫[[end-of-chapter-quiz]]

<CourseFloatingBanner chapter={1} classNames="absolute z-10 right-0 top-0" />

‡∂∏‡∑ô‡∂∏ ‡∂¥‡∂ª‡∑í‡∂†‡∑ä‡∂°‡∑ö‡∂Ø‡∂∫ ‡∂∂‡∑ú‡∑Ñ‡∑ù ‡∂ö‡∂ª‡∑î‡∂´‡∑î ‡∂Ü‡∑Ä‡∂ª‡∂´‡∂∫ ‡∂ö‡∑Ö‡∑ö‡∂∫! ‡∂î‡∂∂ ‡∑É‡∑í‡∂∫‡∂Ω‡∑î ‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª ‡∂≠‡∑ö‡∂ª‡∑î‡∂∏‡∑ä ‡∂±‡∑ú‡∂ú‡∂≠‡∑ä‡∂≠‡∑ö ‡∂±‡∂∏‡∑ä ‡∂ö‡∂ª‡∂Ø‡∂ª ‡∂±‡∑ú‡∑Ä‡∂±‡∑ä‡∂±; ‡∂∏‡∑ì‡∑Ö‡∂ü ‡∂¥‡∂ª‡∑í‡∂†‡∑ä‡∂°‡∑ö‡∂Ø ‡∂î‡∂∂‡∂ß ‡∂Ö‡∂∑‡∑ä‚Äç‡∂∫‡∂±‡∑ä‡∂≠‡∂ª‡∂∫ ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∑è ‡∂ö‡∂ª‡∂± ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂≠‡∑ö‡∂ª‡∑î‡∂∏‡∑ä ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂ã‡∂¥‡∂ö‡∑è‡∂ª‡∑ì ‡∑Ä‡∂±‡∑î ‡∂á‡∂≠.

‡∂ö‡∑ô‡∑É‡∑ö ‡∑Ä‡∑ô‡∂≠‡∂≠‡∑ä, ‡∂¥‡∑Ö‡∂∏‡∑î‡∑Ä, ‡∂î‡∂∂ ‡∂∏‡∑ô‡∂∏ ‡∂¥‡∂ª‡∑í‡∂†‡∑ä‡∂°‡∑ö‡∂Ø‡∂∫‡∑ô‡∂±‡∑ä ‡∂â‡∂ú‡∑ô‡∂± ‡∂ú‡∂≠‡∑ä ‡∂Ø‡∑ö ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∑è ‡∂ö‡∂ª ‡∂∂‡∂Ω‡∂∏‡∑î!

### 1. Hub ‡∂ë‡∂ö ‡∂ú‡∑Ä‡∑ö‡∑Ç‡∂´‡∂∫ ‡∂ö‡∂ª `roberta-large-mnli` ‡∑É‡∑ú‡∂∫‡∂±‡∑ä‡∂±. ‡∂ë‡∂∫ ‡∂â‡∂ß‡∑î ‡∂ö‡∂ª‡∂± ‡∂ö‡∑è‡∂ª‡∑ä‡∂∫‡∂∫ ‡∂ö‡∑î‡∂∏‡∂ö‡∑ä‡∂Ø?

<Question
  choices={[
    {
      text: '‡∑É‡∑è‡∂ª‡∑è‡∂Ç‡∑Å‡∂ú‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏',
      explain:
        '<a href="https://huggingface.co/roberta-large-mnli">roberta-large-mnli ‡∂¥‡∑í‡∂ß‡∑î‡∑Ä</a> ‡∂∏‡∂≠ ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂∂‡∂Ω‡∂±‡∑ä‡∂±.',
    },
    {
      text: '‡∂¥‡∑è‡∂® ‡∑Ä‡∂ª‡∑ä‡∂ú‡∑ì‡∂ö‡∂ª‡∂´‡∂∫',
      explain:
        '‡∑Ä‡∂©‡∑è‡∂≠‡∑ä ‡∂±‡∑í‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∑Ä, ‡∂ë‡∂∫ ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫ ‡∂Ø‡∑ô‡∂ö‡∂ö‡∑ä ‡∂Ω‡∑ö‡∂∂‡∂Ω‡∑ä ‡∂≠‡∑î‡∂±‡∂ö‡∑ä ‡∑Ñ‡∂ª‡∑Ñ‡∑è ‡∂≠‡∑è‡∂ª‡∑ä‡∂ö‡∑í‡∂ö‡∑Ä ‡∑É‡∂∏‡∑ä‡∂∂‡∂±‡∑ä‡∂∞ ‡∂ö‡∂ª ‡∂á‡∂≠‡∑ä‡∂±‡∂∏‡∑ä (‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∑Ä‡∑í‡∂ª‡∑ù‡∂∞‡∂≠‡∑è, ‡∂∏‡∂∞‡∑ä‚Äç‡∂∫‡∑É‡∑ä‡∂Æ, ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∑Ä‡∑í‡∂ª‡∑ù‡∂∞‡∂≠‡∑è) - ‡∂ë‡∂∫ <em>‡∑É‡∑ä‡∑Ä‡∑è‡∂∑‡∑è‡∑Ä‡∑í‡∂ö ‡∂∑‡∑è‡∑Ç‡∑è ‡∂Ö‡∂±‡∑î‡∂∏‡∑è‡∂±‡∂∫</em> ‡∂Ω‡∑ô‡∑É‡∂Ø ‡∑Ñ‡∑ê‡∂≥‡∑í‡∂±‡∑ä‡∑Ä‡∑ö.',
      correct: true,
    },
    {
      text: '‡∂¥‡∑è‡∂® ‡∂ã‡∂≠‡∑ä‡∂¥‡∑è‡∂Ø‡∂±‡∂∫',
      explain:
        '<a href="https://huggingface.co/roberta-large-mnli">roberta-large-mnli ‡∂¥‡∑í‡∂ß‡∑î‡∑Ä</a> ‡∂∏‡∂≠ ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂∂‡∂Ω‡∂±‡∑ä‡∂±.',
    },
  ]}
/>

### 2. ‡∂¥‡∑Ñ‡∂≠ ‡∂ö‡∑ö‡∂≠‡∂∫ ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±‡∑ö ‡∂ö‡∑î‡∂∏‡∂ö‡∑ä‡∂Ø?

```py
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

<Question
  choices={[
    {
      text: '‡∂ë‡∂∫ "positive" ‡∑Ñ‡∑ù "negative" ‡∂Ω‡∑ö‡∂∂‡∂Ω ‡∑É‡∂∏‡∂ü ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫ ‡∑É‡∂≥‡∑Ñ‡∑è ‡∑Ä‡∂ª‡∑ä‡∂ú‡∑ì‡∂ö‡∂ª‡∂´ ‡∂Ω‡∂ö‡∑î‡∂´‡∑î ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂±‡∑î ‡∂á‡∂≠.',
      explain: '‡∂∏‡∑ô‡∂∫ ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í - ‡∂∏‡∑ô‡∂∫ <code>‡∑Ñ‡∑ê‡∂ü‡∑ì‡∂∏‡∑ä-‡∑Ä‡∑í‡∑Å‡∑ä‡∂Ω‡∑ö‡∑Ç‡∂´‡∂∫‡∂ö‡∑ä</code> pipeline ‡∑Ä‡∑ö.',
    },
    {
      text: '‡∂ë‡∂∫ ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫‡∂∫ ‡∑É‡∂∏‡∑ä‡∂¥‡∑ñ‡∂ª‡∑ä‡∂´ ‡∂ö‡∂ª‡∂∏‡∑í‡∂±‡∑ä ‡∂ã‡∂≠‡∑ä‡∂¥‡∑è‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö ‡∂¥‡∑è‡∂®‡∂∫‡∂ö‡∑ä ‡∂Ü‡∂¥‡∑É‡∑î ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂±‡∑î ‡∂á‡∂≠',
      explain: '‡∂∏‡∑ô‡∂∫ ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í - ‡∂ë‡∂∫ <code>‡∂¥‡∑è‡∂® ‡∂ã‡∂≠‡∑ä‡∂¥‡∑è‡∂Ø‡∂±‡∂∫</code> pipeline ‡∑Ä‡∑ö .',
    },
    {
      text: '‡∂ë‡∂∫ ‡∂¥‡∑î‡∂Ø‡∑ä‡∂ú‡∂Ω‡∂∫‡∂±‡∑ä, ‡∑É‡∂Ç‡∑Ä‡∑í‡∂∞‡∑è‡∂± ‡∑Ñ‡∑ù ‡∑É‡∑ä‡∂Æ‡∑è‡∂± ‡∂±‡∑í‡∂∫‡∑ù‡∂¢‡∂±‡∂∫ ‡∂ö‡∂ª‡∂± ‡∑Ä‡∂†‡∂± ‡∂Ü‡∂¥‡∑É‡∑î ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂±‡∑î ‡∂á‡∂≠.',
      explain:
        '‡∂≠‡∑Ä‡∂Ø, <code>grouped_entities=True</code> ‡∑É‡∂∏‡∂ü‡∑í‡∂±‡∑ä, ‡∂ë‡∂∫ "Hugging Face" ‡∑Ä‡∑ê‡∂±‡∑í ‡∂ë‡∂ö‡∂∏ ‡∂Ü‡∂∫‡∂≠‡∂±‡∂∫‡∂ß ‡∂Ö‡∂∫‡∂≠‡∑ä ‡∑Ä‡∂†‡∂± ‡∑É‡∂∏‡∑ñ‡∑Ñ‡∂ú‡∂≠ ‡∂ö‡∂ª‡∂∫‡∑í.',
      correct: true,
    },
  ]}
/>

### 3. ‡∂∏‡∑ô‡∂∏ ‡∂ö‡∑ö‡∂≠ ‡∑É‡∑è‡∂∏‡∑ä‡∂¥‡∂Ω‡∂∫‡∑ö ... ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∑É‡∑ä‡∂Æ‡∑è‡∂¥‡∂±‡∂∫ ‡∂ö‡∑Ö ‡∂∫‡∑î‡∂≠‡∑ä‡∂≠‡∑ö ‡∂ö‡∑î‡∂∏‡∂ö‡∑ä‡∂Ø?

```py
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

<Question
  choices={[
    {
      text: '‡∂∏‡∑ô‡∂∏ &#60;mask> ‡∂î‡∂∂ ‡∂ë‡∂±‡∂≠‡∑î‡∂ª‡∑î ‡∂∂‡∂Ω‡∑è ‡∑É‡∑í‡∂ß‡∑ì.',
      explain:
        '‡∂∏‡∑ô‡∂∫ ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í. <code>bert-base-cased</code> ‡∂∏‡∑è‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∑è‡∂©‡∑ä‡∂¥‡∂≠ ‡∂¥‡∂ª‡∑ì‡∂ö‡∑ä‡∑Ç‡∑è ‡∂ö‡∂ª ‡∂î‡∂∂‡∑ö ‡∑Ä‡∑ê‡∂ª‡∑ê‡∂Ø‡∑ä‡∂Ø ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂ã‡∂≠‡∑ä‡∑É‡∑è‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.',
    },
    {
      text: '‡∂∏‡∑ô‡∂∏ [MASK] ‡∂î‡∂∂ ‡∂ë‡∂±‡∂≠‡∑î‡∂ª‡∑î ‡∂∂‡∂Ω‡∑è ‡∑É‡∑í‡∂ß‡∑ì.',
      explain: '‡∂±‡∑í‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í! ‡∂∏‡∑ô‡∂∏ ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∑ö ‡∂Ü‡∑Ä‡∂ª‡∂´ ‡∂ß‡∑ù‡∂ö‡∂±‡∂∫ [MASK].',
      correct: true,
    },
    {
      text: '‡∂∏‡∑ö ‡∂¥‡∑î‡∂Ø‡∑ä‡∂ú‡∂Ω‡∂∫‡∑è ‡∂î‡∂∂ ‡∂ë‡∂±‡∂≠‡∑î‡∂ª‡∑î ‡∂∂‡∂Ω‡∑è ‡∑É‡∑í‡∂ß‡∑í‡∂∫‡∑ö‡∂∫.',
      explain:
        '‡∂∏‡∑ô‡∂∫ ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í. ‡∂∏‡∑ô‡∂∏ pipeline‡∂∫ ‡∂Ü‡∑Ä‡∂ª‡∂´ ‡∑Ä‡∂†‡∂± ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂¥‡∑î‡∂ª‡∑Ä‡∂∫‡∑í, ‡∂ë‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä ‡∂ë‡∂∫‡∂ß ‡∂ö‡∑ú‡∂≠‡∑ê‡∂±‡∂ö ‡∑Ñ‡∑ù ‡∑Ä‡∑ô‡∑É‡∑ä ‡∂∏‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∂ß‡∑ù‡∂ö‡∂±‡∂∫‡∂ö‡∑ä ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∑Ä‡∑ö.',
    },
  ]}
/>

### 4. ‡∂∏‡∑ô‡∂∏ ‡∂ö‡∑ö‡∂≠‡∂∫ ‡∂Ö‡∑É‡∑è‡∂ª‡∑ä‡∂Æ‡∂ö ‡∑Ä‡∂±‡∑ä‡∂±‡∑ö ‡∂á‡∂∫‡∑í?

```py
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

<Question
  choices={[
    {
      text: '‡∂∏‡∑ô‡∂∏ pipeline‡∂∫‡∂ß ‡∂∏‡∑ô‡∂∏ ‡∂¥‡∑è‡∂®‡∂∫ ‡∑Ä‡∂ª‡∑ä‡∂ú ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂Ω‡∑ö‡∂∂‡∂Ω ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ì‡∂∏ ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∑Ä‡∑ö.',
      explain:
        '‡∂±‡∑í‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í - ‡∂±‡∑í‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í ‡∂ö‡∑ö‡∂≠‡∂∫‡∂ß <code>candidate_labels=[...]</code> ‡∂á‡∂≠‡∑î‡∑Ö‡∂≠‡∑ä ‡∑Ä‡∑í‡∂∫ ‡∂∫‡∑î‡∂≠‡∑î‡∂∫.',
      correct: true,
    },
    {
      text: '‡∂∏‡∑ô‡∂∏ pipeline‡∂∫‡∂ß ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫‡∂∫‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´‡∂ö‡∑ä ‡∂±‡∑ú‡∑Ä ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫ ‡∂ö‡∑í‡∑Ñ‡∑í‡∂¥‡∂∫‡∂ö‡∑ä ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∑Ä‡∑ö.',
      explain:
        '‡∂∏‡∑ô‡∂∫ ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í, ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂±‡∑í‡∑É‡∑í ‡∂Ω‡∑ô‡∑É ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∑è ‡∂ö‡∑Ö ‡∑Ä‡∑í‡∂ß, ‡∂∏‡∑ô‡∂∏ pipeline‡∂∫‡∂ß, ‡∑É‡∑ê‡∂ö‡∑É‡∑ì‡∂∏‡∂ß ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫ ‡∂Ω‡∑ê‡∂∫‡∑í‡∑É‡∑ä‡∂≠‡∑î‡∑Ä‡∂ö‡∑ä ‡∂ú‡∂≠ ‡∑Ñ‡∑ê‡∂ö (‡∂Ö‡∂±‡∑ô‡∂ö‡∑î‡∂≠‡∑ä ‡∑É‡∑í‡∂∫‡∂Ω‡∑î‡∂∏ pipeline ‡∂∏‡∑ô‡∂±‡∑ä).',
    },
    {
      text: 'ü§ó ‡∂¥‡∂ª‡∑í‡∑Ä‡∂ª‡∑ä‡∂≠‡∂ö library‡∂∫ ‡∑É‡∑î‡∂¥‡∑î‡∂ª‡∑î‡∂Ø‡∑î ‡∂¥‡∂ª‡∑í‡∂Ø‡∑í ‡∂ö‡∑ê‡∂©‡∑ì ‡∂á‡∂≠.',
      explain: '‡∂Ö‡∂¥‡∑í ‡∂∏‡∑ô‡∂∏ ‡∂¥‡∑í‡∑Ö‡∑í‡∂≠‡∑î‡∂ª ‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂Ø ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂Ø‡∑ê‡∂ö‡∑ä‡∑Ä‡∑ì‡∂∏‡∂ö‡∑ä ‡∂±‡∑ú‡∂ö‡∂ª‡∂∏‡∑î!',
    },
    {
      text: '‡∂∏‡∑ô‡∂∏ pipeline‡∂∫‡∂ß ‡∂Ø‡∑í‡∂ú‡∑î ‡∂Ü‡∂Ø‡∑è‡∂± ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫‡∂∫‡∑í; ‡∂∏‡∑ô‡∂∫ ‡∂ö‡∑ô‡∂ß‡∑í ‡∑Ä‡∑ê‡∂©‡∑í‡∂∫‡∑í.',
      explain:
        '‡∂∏‡∑ô‡∂∫ ‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∂∫‡∑í. ‡∂∏‡∑ô‡∂∏ pipeline‡∂∫‡∑ô‡∂±‡∑ä ‡∑É‡∑ê‡∂ö‡∑É‡∑ñ ‡∑Ä‡∑í‡∂ß ‡∂â‡∂≠‡∑è ‡∂Ø‡∑í‡∂ú‡∑î ‡∂¥‡∑è‡∂®‡∂∫‡∂ö‡∑ä ‡∂â‡∑Ä‡∂≠‡∑ä ‡∑Ä‡∂± ‡∂∂‡∑Ä ‡∑É‡∂Ω‡∂ö‡∂±‡∑ä‡∂±.',
    },
  ]}
/>

### 5. "‡∑Ñ‡∑î‡∑Ä‡∂∏‡∑è‡∂ª‡∑î ‡∂â‡∂ú‡∑ô‡∂±‡∑ì‡∂∏" ‡∂∫‡∂±‡∑ä‡∂±‡∑ô‡∂±‡∑ä ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂ö‡∑î‡∂∏‡∂ö‡∑ä‡∂Ø?

<Question
  choices={[
    {
      text: '‡∂¥‡∑ô‡∂ª ‡∂¥‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∂ö‡∑Ö ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∂ö ‡∂Ø‡∑ê‡∂±‡∑î‡∂∏ ‡∂ë‡∂∏ ‡∂Ø‡∂≠‡∑ä‡∂≠ ‡∂ö‡∂ß‡∑ä‡∂ß‡∂Ω‡∂∫ ‡∂∏‡∂≠‡∂∏ ‡∂¥‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂±‡∑Ä ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∂ö‡∂ß ‡∂∏‡∑è‡∂ª‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏.',
      explain: '‡∂±‡∑ê‡∂≠, ‡∂ë‡∂∫ ‡∂ë‡∂ö‡∂∏ ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∑ö ‡∂Ö‡∂±‡∑î‡∑Ä‡∑è‡∂Ø ‡∂Ø‡∑ô‡∂ö‡∂ö‡∑ä ‡∑Ä‡∂±‡∑î ‡∂á‡∂≠.',
    },
    {
      text: '‡∂¥‡∑Ö‡∂∏‡∑î ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∑ö ‡∂∂‡∂ª ‡∑É‡∂∏‡∂ü ‡∂Ø‡∑ô‡∑Ä‡∂± ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫ ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂¥‡∑ô‡∂ª ‡∂¥‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∂ö‡∑Ö ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∂ö ‡∂Ø‡∑ê‡∂±‡∑î‡∂∏ ‡∂±‡∑Ä ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∂ö‡∂ß ‡∂∏‡∑è‡∂ª‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏.',
      explain:
        '‡∂±‡∑í‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í: ‡∂Ø‡∑ô‡∑Ä‡∂± ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫ ‡∂±‡∑Ä ‡∂ö‡∑è‡∂ª‡∑ä‡∂∫‡∂∫‡∂ö‡∑ä ‡∂∏‡∂≠ ‡∂¥‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∂ö‡∂ª‡∂± ‡∑Ä‡∑í‡∂ß, ‡∂ë‡∂∫ ‡∂¥‡∑Ö‡∂∏‡∑î ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∑ö ‡∂Ø‡∑ê‡∂±‡∑î‡∂∏ *‡∑Ñ‡∑î‡∑Ä‡∂∏‡∑è‡∂ª‡∑î ‡∂ö‡∂ª‡∂∫‡∑í*.',
      correct: true,
    },
    {
      text: 'Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model.',
      explain:
        'The architecture is just the way the model is built; there is no knowledge shared or transferred in this case.',
    },
  ]}
/>

### 6. True or false? A language model usually does not need labels for its pretraining.

<Question
  choices={[
    {
      text: 'True',
      explain:
        'The pretraining is usually <em>self-supervised</em>, which means the labels are created automatically from the inputs (like predicting the next word or filling in some masked words).',
      correct: true,
    },
    {
      text: 'False',
      explain: 'This is not the correct answer.',
    },
  ]}
/>

### 7. Select the sentence that best describes the terms "model", "architecture", and "weights".

<Question
  choices={[
    {
      text: 'If a model is a building, its architecture is the blueprint and the weights are the people living inside.',
      explain:
        'Following this metaphor, the weights would be the bricks and other materials used to construct the building.',
    },
    {
      text: 'An architecture is a map to build a model and its weights are the cities represented on the map.',
      explain:
        'The problem with this metaphor is that a map usually represents one existing reality (there is only one city in France named Paris). For a given architecture, multiple weights are possible.',
    },
    {
      text: 'An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.',
      explain:
        'The same set of mathematical functions (architecture) can be used to build different models by using different parameters (weights).',
      correct: true,
    },
  ]}
/>

### 8. Which of these types of models would you use for completing prompts with generated text?

<Question
  choices={[
    {
      text: 'An encoder model',
      explain:
        'An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.',
    },
    {
      text: 'A decoder model',
      explain:
        'Decoder models are perfectly suited for text generation from a prompt.',
      correct: true,
    },
    {
      text: 'A sequence-to-sequence model',
      explain:
        'Sequence-to-sequence models are better suited for tasks where you want to generate sentences in relation to the input sentences, not a given prompt.',
    },
  ]}
/>

### 9. Which of those types of models would you use for summarizing texts?

<Question
  choices={[
    {
      text: 'An encoder model',
      explain:
        'An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.',
    },
    {
      text: 'A decoder model',
      explain:
        "Decoder models are good for generating output text (like summaries), but they don't have the ability to exploit a context like the whole text to summarize.",
    },
    {
      text: 'A sequence-to-sequence model',
      explain:
        'Sequence-to-sequence models are perfectly suited for a summarization task.',
      correct: true,
    },
  ]}
/>

### 10. Which of these types of models would you use for classifying text inputs according to certain labels?

<Question
  choices={[
    {
      text: 'An encoder model',
      explain:
        'An encoder model generates a representation of the whole sentence which is perfectly suited for a task like classification.',
      correct: true,
    },
    {
      text: 'A decoder model',
      explain:
        'Decoder models are good for generating output texts, not extracting a label out of a sentence.',
    },
    {
      text: 'A sequence-to-sequence model',
      explain:
        'Sequence-to-sequence models are better suited for tasks where you want to generate text based on an input sentence, not a label.',
    },
  ]}
/>

### 11. What possible source can the bias observed in a model have?

<Question
  choices={[
    {
      text: 'The model is a fine-tuned version of a pretrained model and it picked up its bias from it.',
      explain:
        'When applying Transfer Learning, the bias in the pretrained model used perspires in the fine-tuned model.',
      correct: true,
    },
    {
      text: 'The data the model was trained on is biased.',
      explain: 'This is the most obvious source of bias, but not the only one.',
      correct: true,
    },
    {
      text: 'The metric the model was optimizing for is biased.',
      explain:
        'A less obvious source of bias is the way the model is trained. Your model will blindly optimize for whatever metric you chose, without any second thoughts.',
      correct: true,
    },
  ]}
/>
