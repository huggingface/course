# පරිවර්තක ක්‍රියාකරන්නේ කෙසේද ?[[how-do-transformers-work]]

<CourseFloatingBanner chapter={1} classNames="absolute z-10 right-0 top-0" />

මෙම කොටසේදී, අපි පරිවර්තක ආකෘතිවල නිර්මිතය පිළිබඳ ඉහළ මට්ටමේ සොයා බැලීමක් කරමු.

## පරිවර්තක වල ඉතිහාසයෙන් බිදක්[[a-bit-of-transformer-history]]

පරිවර්තක ආකෘති වල (කෙටි) ඉතිහාසයේ යොමු කරුණු කිහිපයක්:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" alt="A brief chronology of Transformers models.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg" alt="A brief chronology of Transformers models.">
</div>

[පරිවර්තක නිර්මිතය](https://arxiv.org/abs/1706.03762) 2017 ජුනි මාසයේදී හඳුන්වා දෙන ලදී. මුල් පර්යේෂණයේ අවධානය යොමු වූයේ පරිවර්තන කාර්යයන් කෙරෙහි ය. වැදගත් ආකෘති කිහිපයක් හඳුන්වා දීමෙන් අනතුරුව මෙය සිදු විය. එනම්:

- **2018 ජූනි**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), පළමු පූර්ව පරිවර්තක ආකෘතිය, විවිධ NLP කාර්යයන් සියුම් ලෙස සකස් කිරීම සඳහා භාවිතා කර අති නවීන ප්‍රතිඵල ලබා ගන්නා ලදී

- **2018 ඔක්තෝබර්**: [BERT](https://arxiv.org/abs/1810.04805), තවත් විශාල පෙර පුහුණු ආකෘතියක්, මෙය වඩා හොඳ වාක්‍ය සාරාංශ නිෂ්පාදනය කිරීමට නිර්මාණය කර ඇත (ඊළඟ පරිච්ඡේදයේ මේ ගැන වැඩි විස්තර දැක්වේ!)

- **2019 පෙබරවාරි**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), සදාචාරාත්මක ගැටළු හේතුවෙන්, එක්වරම මහජනයාට නිකුත් නොකළ GPT හි වැඩිදියුණු කළ (සහ විශාල) අනුවාදයකි

- **2019 ඔක්තෝබර්**: [DistilBERT](https://arxiv.org/abs/1910.01108), BERT හි ආසවනය කළ අනුවාදයක් 60% වේගවත්, 40% සැහැල්ලු මතකයක් සහ තවමත් BERT හි කාර්ය සාධනයෙන් 97%ක් රඳවා තබා ගනී

- **2019 ඔක්තෝබර්**: [BART](https://arxiv.org/abs/1910.13461) සහ [T5](https://arxiv.org/abs/1910.10683), මුල් පරිවර්තක ආකෘතියට සමාන නිර්මිතය භාවිතා කරන විශාල පෙර පුහුණු මාදිලි දෙකක් (එසේ කළ පළමු ආකෘතිය)

- **2020 මැයි**, [GPT-3](https://arxiv.org/abs/2005.14165), GPT-2 හි ඊටත් වඩා විශාල අනුවාදයක් සියුම්-සුසර කිරීමේ අවශ්‍යතාවයකින් තොරව විවිධ කාර්යයන් හොඳින් ඉටු කිරීමට හැකි වේ (_zero-shot learning_ ලෙස හැඳින්වේ)

මෙම ලැයිස්තුව විස්තීර්ණ නොවන අතර, විවිධ ආකාරයේ පරිවර්තක ආකෘති කිහිපයක් ඉස්මතු කිරීමට අදහස් කෙරේ. පුළුල් ලෙස, ඒවා කාණ්ඩ තුනකට බෙදිය හැකිය:

- GPT වැනි (_auto-regressive_ පරිවර්තක ආකෘති ලෙසද හැඳින්වේ)
- BERT වැනි (_auto-encoding_ පරිවර්තක ආකෘති ලෙසද හැඳින්වේ)
- BART/T5 වැනි (_sequence-to-sequence_ පරිවර්තක ආකෘති ලෙසද හැඳින්වේ)

අපි පසුව මෙම ආකාර වඩාත් ගැඹුරින් අධ්‍යනය කරමු

## පරිවර්තක යනු භාෂා ආකෘති වේ[[transformers-are-language-models]]

ඉහත සඳහන් සියලුම පරිවර්තක ආකෘති (GPT, BERT, BART, T5, ආදිය) _භාෂා ආකෘති_ ලෙස පුහුණු කර ඇත. මෙයින් අදහස් කරන්නේ ඔවුන් ස්වයං-අධීක්‍ෂණය ආකාරයෙන්, විශාල අමු පාඨ පිළිබඳව පුහුණු කර ඇති බවයි. ස්වයං-අධීක්ෂණ ඉගෙනීම යනු ආකෘතියේ ආදාන වලින් අරමුණ ස්වයංක්‍රීයව ගණනය කෙරෙන පුහුණු වර්ගයකි. එනම් දත්ත ලේබල් කිරීමට මිනිසුන් අවශ්‍ය නොවන බවයි!

මෙම මාදිලියේ ආකෘතිය එය පුහුණු කර ඇති භාෂාව පිළිබඳ සංඛ්‍යානමය අවබෝධයක් වර්ධනය කරයි, නමුත් එය විශේෂිත ප්‍රායෝගික කාර්යයන් සඳහා එතරම් ප්‍රයෝජනවත් නොවේ. මේ නිසා, සාමාන්‍ය පෙර පුහුණු ආකෘතිය පසුව _හුවමාරු ඉගෙනීම_ නම් ක්‍රියාවලියක් හරහා ගමන් කරයි. මෙම ක්‍රියාවලිය අතරතුර, දී ඇති කාර්යයක් මත, ආකෘතිය අධීක්‍ෂණය කරන ලද ආකාරයෙන් -- එනම්, මානව-අනුසටහන් කළ ලේබල් භාවිතයෙන් - සියුම්ව සකස් කර ඇත.
කාර්යයක උදාහරණයක් වන්නේ _n_ පෙර වචන කියවා ඇති වාක්‍යයක ඊළඟ වචනය පුරෝකථනය කිරීමයි. මෙය _කාරක භාෂා ආකෘති නිර්මාණය_ ලෙස හඳුන්වනු ලබන්නේ ප්‍රතිදානය අතීත සහ වර්තමාන යෙදවුම් මත රඳා පවතින නමුත් අනාගත ඒවා මත නොවන බැවිනි.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg" alt="Example of causal language modeling in which the next word from a sentence is predicted.">
</div>

තවත් උදාහරණයක් නම් _ආවරණ භාෂා ආකෘති නිර්මාණය_, එහි ආකෘතිය වාක්‍යයේ ආවරණ වචනයක් පුරෝකථනය කරයි.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg" alt="Example of masked language modeling in which a masked word from a sentence is predicted.">
</div>

## පරිවර්තක විශාල ආකෘති වේ[[transformers-are-big-models]]

(DistilBERT වැනි) බාහිරස්ථයෝ කිහිපයක් හැරුණු විට, වඩා හොඳ කාර්ය සාධනයක් ලබා ගැනීමේ සාමාන්‍ය උපාය මාර්ගය වන්නේ ආකෘතිවල ප්‍රමාණයන් මෙන්ම ඒවා පෙර පුහුණු කර ඇති දත්ත ප්‍රමාණය වැඩි කිරීමෙනි.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png" alt="Number of parameters of recent Transformers models" width="90%">
</div>

අවාසනාවකට, ආකෘතියක් පුහුණු කිරීමට, විශේෂයෙන් විශාල ආකෘතියක්, විශාල දත්ත ප්‍රමාණයක් අවශ්‍යය වේ. කාලය සහ සම්පත් ගණනය කිරීම සම්බන්ධයෙන් මෙය ඉතා මිල අධික වේ. එය පහත ප්‍රස්ථාරයෙන් දැකිය හැකි පරිදි පාරිසරික බලපෑමට පවා පරිවර්තනය වේ.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg" alt="The carbon footprint of a large language model.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg" alt="The carbon footprint of a large language model.">
</div>

<Youtube id="ftWlj4FBHTg" />

තවද මෙය පූර්ව පුහුණුවේ පාරිසරික බලපෑම අවම කිරීමට දැනුවත්ව උත්සාහ කරන කණ්ඩායමක් විසින් මෙහෙයවනු ලබන (ඉතා විශාල) ආකෘතියක් සඳහා වන ව්‍යාපෘතියක් පෙන්වයි. හොඳම අධි පරාමිති ලබා ගැනීම සඳහා බොහෝ අත්හදා බැලීම් ක්‍රියාත්මක කිරීමේ මාර්ගය ඊටත් වඩා වැඩි වනු ඇත.

සිතන්න, පර්යේෂණ කණ්ඩායමකට, ශිෂ්‍ය සංවිධානයකට හෝ සමාගමකට ආකෘතියක් පුහුණු කිරීමට අවශ්‍ය වූ සෑම අවස්ථාවකම එය මුල සිටම එය කළේ නම්, මෙය විශාල, අනවශ්‍ය ගෝලීය පිරිවැයකට තුඩු දෙනු ඇත!

භාෂා ආකෘති බෙදාගැනීම අතිශයින් වැදගත් වන්නේ එබැවිනි: පුහුණු කළ භාර බෙදා ගැනීම සහ දැනටමත් පුහුණු කර ඇති භාර මත ගොඩනැගීම ප්‍රජාවේ සමස්ත ගණනය කිරීමේ පිරිවැය සහ කාබන් පියසටහන අඩු කරයි.

මේ වන විට, ඔබට මෙවලම් කිහිපයක් හරහා ඔබේ ආකෘති පුහුණු කිරීමේ කාබන් පියසටහන් ඇගයීමට ලක් කළ හැකිය. උදාහරණයක් ලෙස [ML CO2 Impact](https://mlco2.github.io/impact/) හෝ [Code Carbon](https://codecarbon.io/) එය 🤗 පරිවර්තක තුළ ඒකාබද්ධ කර ඇත. මේ ගැන වැඩි විස්තර දැනගැනීම සඳහා, ඔබට මෙම [බ්ලොග් සටහන](https://huggingface.co/blog/carbon-emissions-on-the-hub) කියවිය හැකි අතර එමඟින් ඔබට `emissions.csv` ගොනුවක් උත්පාදනය කරන්නේ කෙසේදැයි පෙන්වයි. ඔබේ පුහුණුවේ අඩි සටහනේ ඇස්තමේන්තුවක්, මෙන්ම මෙම මාතෘකාව ආමන්ත්‍රණය කරන 🤗 පරිවර්තක වල [ලේඛන](https://huggingface.co/docs/hub/model-cards-co2).

## හුවමාරු ඉගෙනීම[[transfer-learning]]

<Youtube id="BqqfQnyjmgg" />

_පූර්ව පුහුණුව_ යනු මුල සිටම ආකෘතියක් පුහුණු කිරීමේ ක්‍රියාවයි: භාර අහඹු ලෙස ආරම්භ කර ඇති අතර පුහුණුව කිසිදු පූර්ව දැනුමකින් තොරව ආරම්භ වේ.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" alt="The pretraining of a language model is costly in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg" alt="The pretraining of a language model is costly in both time and money.">
</div>

මෙම පූර්ව පුහුණුව සාමාන්‍යයෙන් ඉතා විශාල දත්ත ප්‍රමාණයක් මත සිදු කෙරේ. එබැවින්, එයට ඉතා විශාල දත්ත සමුදායක් අවශ්‍යය වන අතර, පුහුණුව සඳහා සති කිහිපයක් ගත විය හැක.

_සියුම් සීරුමාරුව_, අනෙක් අතට, ආකෘතියක් පූර්ව පුහුණු කිරීමෙන් **පසු** කරන පුහුණුවයි. සියුම් සීරුමාරුව කිරීම සිදු කිරීම සඳහා, ඔබ මුලින්ම පූර්ව පුහුණු භාෂා ආකෘතියක් ලබා ගනී, පසුව ඔබේ කාර්යයට විශේෂිත වූ දත්ත කට්ටලයක් සමඟ අමතර පුහුණුවක් කරන්න. රැඳී සිටින්න -- අවසාන කාර්යය සඳහා සරලව පුහුණු නොකරන්නේ මන්ද? හේතු කිහිපයක් තිබේ:

- පෙර පුහුණු කරන ලද ආකෘතිය දැනටමත් සියුම් සීරුමාරු දත්ත කට්ටලය සමඟ යම් සමානකම් ඇති දත්ත කට්ටලයක් මත පුහුණු කර ඇත. සියුම් සීරුමාරු කිරීමේ ක්‍රියාවලියට පෙර පුහුණුවේදී ආරම්භක ආකෘතියෙන් ලබාගත් දැනුමෙන් ප්‍රයෝජන ගැනීමට හැකි වේ (උදාහරණයක් ලෙස, NLP ගැටළු සමඟ, පූර්ව පුහුණු ආකෘතියට ඔබ ඔබේ කාර්යය සඳහා භාවිතා කරන භාෂාව පිළිබඳ යම් ආකාරයක සංඛ්‍යානමය අවබෝධයක් ඇත).
- පූර්ව පුහුණු ආකෘතිය දැනටමත් බොහෝ දත්ත මත පුහුණු කර ඇති බැවින්, සියුම් සීරුමාරු කිරීම සඳහා හොඳ ප්‍රතිඵල ලබා ගැනීම සඳහා අඩු දත්ත ප්‍රමාණයක් අවශ්‍යය වේ.
- එම හේතුව නිසා, හොඳ ප්‍රතිඵල ලබා ගැනීමට අවශ්‍ය කාලය හා සම්පත් ප්‍රමාණය බෙහෙවින් අඩු ය.

උදාහරණයක් ලෙස, කෙනෙකුට ඉංග්‍රීසි භාෂාව මත පුහුණු කරන ලද පූර්ව පුහුණු ආකෘතියක් භාවිතා කළ හැකි අතර පසුව එය arXiv corpus මත සියුම් ලෙස සකස් කර, විද්‍යාව/පර්යේෂණ පදනම් වූ ආකෘතියක් ඇති කරයි. සියුම් සීරුමාරු කිරීම සඳහා සීමිත දත්ත ප්‍රමාණයක් පමණක් අවශ්‍ය වනු ඇත: පූර්ව පුහුණු ආකෘතිය ලබාගෙන ඇති දැනුම "මාරු කර ඇත," එබැවින් _හුවමාරු ඉගෙනීම_ යන යෙදුම භාවිත වේ.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg" alt="The fine-tuning of a language model is cheaper than pretraining in both time and money.">
</div>

එබැවින් ආකෘතියක් මනාව සකස් කිරීම සඳහා කාලය, දත්ත, මූල්‍ය සහ පාරිසරික පිරිවැය අඩු වේ. පුහුණුව සම්පූර්ණ පූර්ව පුහුණුවකට වඩා අඩු සංරෝධනයක් බැවින්, විවිධ සියුම් සීරුමාරු යෝජනා ක්‍රම නැවත නැවත කිරීම ඉක්මන් සහ පහසු වේ.

මෙම ක්‍රියාවලිය මුල සිටම පුහුණුවට වඩා හොඳ ප්‍රතිඵල අත්කර ගනු ඇත (ඔබට දත්ත විශාල ප්‍රමාණයක් නොමැති නම්), එබැවින් ඔබ සැමවිටම පෙර පුහුණු කළ ආකෘතියක් -- ඔබ අත ඇති කාර්යයට හැකි තරම් සමීප එකක් වීම -- සහ හොඳයි. - එය සීරුමාරු කරන්න.

## සාමාන්‍යය නිර්මිතය[[general-architecture]]

මෙම කොටසේදී, අපි පරිවර්තක ආකෘතියේ සාමාන්‍ය නිර්මිතය හරහා යන්නෙමු. ඔබට සමහර සංකල්ප තේරුම් නොයන්නේ නම් කරදර නොවන්න; එක් එක් සංරචක ආවරණය වන පරිදි පසුව සවිස්තරාත්මක කොටස් ඇත.

<Youtube id="H39Z_720T5s" />

## හැඳින්වීම[[introduction]]

ආකෘතිය මූලික වශයෙන් කොටස් දෙකකින් සමන්විත වේ:

- **කේතකය (වමේ)**: කේතකයට ආදානයක් ලැබෙන අතර එහි නිරූපණයක් (එහි විශේෂාංග) ගොඩනඟයි. මෙයින් අදහස් කරන්නේ ආදානයෙන් අවබෝධය ලබා ගැනීමට ආකෘතිය ප්‍රශස්ත කර ඇති බවයි.
- **විකේතකය (දකුණ)**: ඉලක්ක අනුපිළිවෙලක් උත්පාදනය කිරීම සඳහා විකේතකය අනෙකුත් යෙදවුම් සමඟ සංකේතකයේ නිරූපණය (විශේෂාංග) භාවිතා කරයි. මෙයින් අදහස් කරන්නේ නිමැවුම් උත්පාදනය කිරීම සඳහා ආකෘතිය ප්‍රශස්ත කර ඇති බවයි.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg" alt="Architecture of a Transformers models">
</div>

මෙම සෑම කොටසක්ම කාර්යය මත පදනම්ව ස්වාධීනව භාවිතා කළ හැකිය:

- **කේතකය-පමණි ආකෘති**: වාක්‍ය වර්ගීකරණය සහ නම් කරන ලද භූතාර්ථ හඳුනාගැනීම වැනි ආදානය පිළිබඳ අවබෝධය අවශ්‍ය කාර්යයන් සඳහා ගැලපෙයි.
- **විකේතකය-පමණි ආකෘති**: පාඨ උත්පාදනය වැනි උත්පාදක කාර්යයන් සඳහා ගැලපෙයි.
- **කේතක-විකේතක ආකෘති** හෝ **අනුක්‍රමිකයක සිට අනුක්‍රමිකයකට ආකෘති**: පරිවර්තනය හෝ සාරාංශ කිරීම වැනි ආදානයක් අවශ්‍ය වන උත්පාදක කාර්යයන් සඳහා ගැලපෙයි.

අපි පසු කොටස් වලින් වෙනවෙනම එම නිර්මිත අධ්‍යයනය කරමු.

## අවධාන ස්ථර[[attention-layers]]

පරිවර්තක ආකෘතිවල ප්‍රධාන ලක්ෂණයක් වන්නේ ඒවා _අවධාන ස්ථර_ ලෙස හැඳින්වෙන විශේෂ ස්ථර වලින් සාදා තිබීමයි. ඇත්ත වශයෙන්ම, පරිවර්තක නිර්මිතය හඳුන්වා දෙන පත්‍රිකාවේ මාතෘකාව වූයේ ["ඔබට අවශ්‍ය සියල්ල අවධානයයි"](https://arxiv.org/abs/1706.03762)! අපි පසුව පාඨමාලාවේදී අවධානය යොමු කරන ස්ථර පිළිබඳ විස්තර ගවේෂණය කරන්නෙමු; දැනට, ඔබ දැනගත යුතු එකම දෙය නම්, මෙම ස්තරය එක් එක් වචනයේ නිරූපණය සමඟ කටයුතු කිරීමේදී ඔබ සම්මත කරන ලද වාක්‍යයේ (සහ අඩු හෝ වැඩි වශයෙන් අනෙක් ඒවා නොසලකා හරින්න) නිශ්චිත අවධානයක් යොමු කරන ලෙස ආකෘතියට පවසනු ඇත.

මෙය සන්දර්භයට ගෙන ඒම සඳහා, ඉංග්‍රීසි සිට ෆ්‍රංශ දක්වා පාඨ පරිවර්තනය කිරීමේ කාර්යය සලකා බලන්න. "ඔබ මෙම පාඨමාලාවට කැමතියි" යන ආදානය ලබා දීමෙන්, පරිවර්තන ආකෘතියක් "කැමතියි" යන වචනය සඳහා නිසි පරිවර්තනය ලබා ගැනීම සඳහා යාබදව ඇති "ඔබ" යන වචනයටද සහභාගී වීමට අවශ්‍ය වනු ඇත, මන්ද ප්‍රංශ භාෂාවෙන් "කැමතියි" යන ක්‍රියා පදය එකිනෙකට වෙනස් ලෙස සංයුක්ත වේ. විෂයය. කෙසේ වෙතත්, වාක්‍යයේ ඉතිරි කොටස එම වචනයේ පරිවර්තනය සඳහා ප්‍රයෝජනවත් නොවේ. ඒ ආකාරයෙන්ම, "මෙය" පරිවර්තනය කිරීමේදී ආකෘතිය "පාඨමාලාව" යන වචනය කෙරෙහි ද අවධානය යොමු කළ යුතුය, මන්ද "මෙය" ආශ්‍රිත නාම පදය පුරුෂ හෝ ස්ත්‍රී ද යන්න මත පදනම්ව වෙනස් ලෙස පරිවර්තනය වේ. නැවතත්, "මෙය" පරිවර්තනය සඳහා වාක්‍යයේ අනෙක් වචන වැදගත් නොවේ. වඩාත් සංකීර්ණ වාක්‍ය (සහ වඩාත් සංකීර්ණ ව්‍යාකරණ රීති) සමඟින්, එක් එක් වචනය නිවැරදිව පරිවර්ථනය කිරීම සඳහා වාක්‍යයේ ඈතින් දිස්විය හැකි වචන කෙරෙහි ආකෘතිය විශේෂ අවධානයක් යොමු කළ යුතුය.

ස්වාභාවික භාෂාව හා සම්බන්ධ ඕනෑම කාර්යයකට එකම සංකල්පය අදාළ වේ: වචනයකට එහිම අර්ථයක් ඇත, නමුත් එම අර්ථයට සන්දර්භය ගැඹුරින් බලපායි, එය අධ්‍යයනය කරන වචනයට පෙර හෝ පසුව වෙනත් ඕනෑම වචනයක් (හෝ වචන) මත විය හැකිය.

දැන් ඔබට අවධානය යොමු කරන ස්ථර මොනවාද යන්න පිළිබඳ අදහසක් ඇති බැවින්, අපි පරිවර්තක නිර්මිත දෙස සමීපව බලමු.

## මුල් නිර්මිතය[[the-original-architecture]]

පරිවර්තක නිර්මිතය මුලින් නිර්මාණය කර ඇත්තේ පරිවර්තනය සඳහා ය. පුහුණුව අතරතුර, කේතකය යම් භාෂාවකින් යෙදවුම් (වාක්‍ය) ලබා ගන්නා අතර විකේතකයට අපේක්ෂිත ඉලක්ක භාෂාවෙන් එම වාක්‍ය ලැබේ. කේතකය තුළ, අවධානය ස්ථරවලට වාක්‍යයක ඇති සියලුම වචන භාවිතා කළ හැකිය (අපි දැන් දුටු පරිදි, දී ඇති වචනයක පරිවර්තනය වාක්‍යයේ පසුව සහ ඊට පෙර වාක්‍ය මත රඳා පවතී). කෙසේ වෙතත්, විකේතකය අනුක්‍රමිකව ක්‍රියා කරන අතර එය දැනටමත් පරිවර්තනය කර ඇති වාක්‍යයේ ඇති වචන කෙරෙහි පමණක් අවධානය යොමු කරයි (එනම්, දැනට ජනනය වන වචනයට පෙර වචන පමණි). උදාහරණයක් ලෙස, අපි පරිවර්තනය කරන ලද ඉලක්කයේ පළමු වචන තුන පුරෝකථනය කළ විට, අපි ඒවා විකේතකය වෙත ලබා දෙන අතර එය හතරවන වචනය පුරෝකථනය කිරීමට උත්සාහ කිරීමේදී කේතකයේ සියලුම ආදාන භාවිතා කරයි.

පුහුණුව අතරතුර කාර්යය වේගවත් කිරීම සඳහා (ආකෘතියට ඉලක්කගත වාක්‍ය සඳහා ප්‍රවේශය ඇති විට), විකේතකය මුළු ඉලක්කයම පෝෂණය කරයි, නමුත් අනාගත වචන භාවිතා කිරීමට එයට ඉඩ නොදේ (එය 2 වන ස්ථානයේ වචනය, පුරෝකථනය කිරීමට උත්සාහ කරන විට 2 ස්ථානයේ ඇති වචනයට ප්‍රවේශය තිබේ නම්, මෙම ගැටලුව ඉතා අපහසු නොවනු ඇත!). නිදසුනක් වශයෙන්, සිව්වන වචනය පුරෝකථනය කිරීමට උත්සාහ කරන විට, අවධානය ස්තරයට ප්‍රවේශය ඇත්තේ 1 සිට 3 දක්වා ස්ථාන වල වචනවලට පමණි.

මුල් පරිවර්තක නිර්මිතය, කේතකය වම් පසින් සහ විකේතකය දකුණින් දිස් විය:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg" alt="Architecture of a Transformers models">
</div>

විකේතක කුට්ටියක පළමු අවධානය ස්තරය විකේතකය වෙත සියලුම (අතීත) ආදාන අවධානය යොමු කරවයි, නමුත් දෙවන අවධානය ස්තරය සංකේතකයේ ප්‍රතිදානය භාවිතා කරයි. එමගින් වත්මන් වචනය වඩාත් හොඳින් පුරෝකථනය කිරීම සඳහා සම්පූර්ණ ආදාන වාක්‍යයට ප්‍රවේශ විය හැක. විවිධ භාෂාවලට වචන විවිධ අනුපිළිවෙලවල් වලට ඇතුළත් කරන ව්‍යාකරණ රීති තිබිය හැකි බැවින් මෙය ඉතා ප්‍රයෝජනවත් වේ, නැතහොත් වාක්‍යයේ පසුව සපයන ලද යම් සන්දර්භයක් දී ඇති වචනයක හොඳම පරිවර්තනය තීරණය කිරීමට උපකාරී වේ.

_අවධානය ආවරණ_ ආදර්ශය සමහර විශේෂ වචන වෙත අවධානය යොමු කිරීම වැලැක්වීමට සංකේතකය/විකේතකය තුළ ද භාවිතා කළ හැක -- උදාහරණයක් ලෙස, වාක්‍ය එකට එකතු කිරීමේදී සියලුම යෙදවුම් එකම දිගක් කිරීමට භාවිතා කරන විශේෂ පිරවුම් වචනය.

## නිර්මිතයට එදිරිව මුරපොල[[architecture-vs-checkpoints]]

අපි මෙම පාඨමාලාවේ පරිවර්තක ආකෘතිවලට එබී බලන විට, ඔබට _නිර්මිතය_ සහ _මුරපොල_ මෙන්ම _ආකෘති_ ගැන සඳහන් වනු ඇත. මෙම පද සියල්ලටම තරමක් වෙනස් අර්ථයන් ඇත:

- **නිර්මිතය**: මෙය ආකෘතියේ ඇටසැකිල්ලයි -- එක් එක් ස්ථරයේ නිර්වචනය සහ ආකෘතිය තුළ සිදු වන එක් එක් මෙහෙයුම වේ.
- **මුරපොල**: දී ඇති නිර්මිතය තුළ පටවනු ලබන භාර මේවාය.
- **ආකෘති**: මෙය "නිර්මිතය" හෝ "මුරපොල" තරම් නිවැරදි නොවන ආවරණ පදයකි: එයට දෙකම අදහස් කළ හැකිය. මෙම පාඨමාලාව තුල අපැහැදිලි බව අඩු කිරීමට, වුවමනා විට පමණක් _නිර්මිතය_ හෝ _මුරපොල_ සඳහන් කරනු ඇත.

උදාහරණයක් ලෙස, BERT යනු නිර්මිතයක් වන අතර BERT හි පළමු නිකුතුව සඳහා Google කණ්ඩායම විසින් පුහුණු කරන ලද භාර කට්ටලයක් වන `bert-base-cased' යනු මුරපොලකි. කෙසේ වෙතත්, කෙනෙකුට "BERT ආකෘතිය" සහ "Bert-base-cased" ආකෘතිය ලෙසද පැවසිය හැකිය."
