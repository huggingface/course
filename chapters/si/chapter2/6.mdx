<FrameworkSwitchCourse {fw} />

# ‡∑É‡∑í‡∂∫‡∂Ω‡∑ä‡∂Ω ‡∂ë‡∂ö‡∑ä‡∑Ä ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏[[putting-it-all-together]]

{#if fw === 'pt'}

<CourseFloatingBanner
  chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: 'Google Colab',
      value:
        'https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb',
    },
    {
      label: 'Aws Studio',
      value:
        'https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb',
    },
  ]}
/>

{:else}

<CourseFloatingBanner
  chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: 'Google Colab',
      value:
        'https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb',
    },
    {
      label: 'Aws Studio',
      value:
        'https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb',
    },
  ]}
/>

{/if}

‡∂¥‡∑Ñ‡∑î‡∂ú‡∑í‡∂∫ ‡∂ö‡∑ú‡∂ß‡∑É‡∑ä ‡∂ö‡∑ì‡∂¥‡∂∫ ‡∂≠‡∑î‡∑Ö ‡∂Ö‡∂¥‡∑í ‡∂Ö‡∂¥‡∑ö ‡∂ã‡∂¥‡∂ª‡∑í‡∂∏‡∂∫‡∑ô‡∂±‡∑ä ‡∂ã‡∂≠‡∑ä‡∑É‡∑è‡∑Ñ ‡∂ö‡∑Ö‡∑ö ‡∂∂‡∑ú‡∑Ñ‡∑ù ‡∑Ä‡∑ê‡∂© ‡∂Ö‡∂≠‡∑í‡∂±‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß‡∂∫‡∑í. ‡∂Ö‡∂¥‡∑í ‡∂ß‡∑ù‡∂ö‡∂±‡∂ö‡∑è‡∂ª‡∂ö ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∑è ‡∂ö‡∂ª‡∂± ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂ú‡∑Ä‡∑ö‡∑Ç‡∂´‡∂∫ ‡∂ö‡∂ª ‡∂á‡∂≠‡∑í ‡∂Ö‡∂≠‡∂ª ‡∂ß‡∑ù‡∂ö‡∂±‡∑ì‡∂ö‡∂ª‡∂´‡∂∫, ‡∂Ü‡∂Ø‡∑è‡∂± ‡∑Ñ‡∑ê‡∂≥‡∑î‡∂±‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä ‡∂∂‡∑Ä‡∂ß ‡∂¥‡∂ª‡∑í‡∑Ä‡∂ª‡∑ä‡∂≠‡∂±‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏, ‡∂ã‡∂¥‡∂∞‡∑è‡∂±, ‡∂ö‡∂¥‡∑ä‡∂¥‡∑è‡∂Ø‡∑î‡∑Ä ‡∑É‡∑Ñ ‡∂Ö‡∑Ä‡∂∞‡∑è‡∂± ‡∂Ü‡∑Ä‡∂ª‡∂´ ‡∂Ø‡∑ô‡∑É ‡∂∂‡∑ê‡∂Ω‡∑î‡∑Ä‡∑ô‡∂∏‡∑î.

‡∂ö‡∑ô‡∑É‡∑ö ‡∑Ä‡∑ô‡∂≠‡∂≠‡∑ä, ‡∂Ö‡∂¥‡∑í 2 ‡∂ö‡∑ú‡∂ß‡∑É‡∑ö ‡∂Ø‡∑î‡∂ß‡∑î ‡∂¥‡∂ª‡∑í‡∂Ø‡∑í, ü§ó ‡∂¥‡∂ª‡∑í‡∑Ä‡∂ª‡∑ä‡∂≠‡∂ö API ‡∂ß ‡∂â‡∑Ñ‡∑Ö ‡∂∏‡∂ß‡∑ä‡∂ß‡∂∏‡∑ö ‡∂ö‡∑è‡∂ª‡∑ä‡∂∫‡∂∫‡∂ö‡∑ä ‡∑É‡∂∏‡∂ü‡∑í‡∂±‡∑ä ‡∂Ö‡∂¥ ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä ‡∂∏‡∑ö ‡∑É‡∑í‡∂∫‡∂Ω‡∑ä‡∂Ω ‡∑Ñ‡∑ê‡∑É‡∑í‡∂ª‡∑Ä‡∑í‡∂∫ ‡∑Ñ‡∑ê‡∂ö. ‡∂í ‡∂ú‡∑ê‡∂± ‡∂∏‡∑ô‡∑Ñ‡∑í‡∂Ø‡∑ì ‡∑É‡∂Ω‡∂ö‡∑è ‡∂∂‡∂Ω‡∂∏‡∑î. ‡∂î‡∂∂ ‡∂î‡∂∂‡∑ö `tokenizer` ‡∑Ä‡∑è‡∂ö‡∑ä‚Äç‡∂∫‡∂∫‡∂ß ‡∂ö‡∑ô‡∂Ω‡∑í‡∂±‡∑ä‡∂∏ ‡∂∫‡∑ô‡∂Ø‡∑î ‡∑Ä‡∑í‡∂ß, ‡∂î‡∂∂‡∂ß ‡∂î‡∂∂‡∑ö ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫ ‡∑Ñ‡∂ª‡∑Ñ‡∑è ‡∂∫‡∑è‡∂∏‡∂ß ‡∑É‡∑î‡∂Ø‡∑è‡∂±‡∂∏‡∑ä ‡∑Ä‡∑ñ ‡∂Ü‡∂Ø‡∑è‡∂± ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂Ω‡∑ê‡∂∂‡∑ö:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

‡∂∏‡∑ô‡∑Ñ‡∑í‡∂Ø‡∑ì, `model_inputs` ‡∑Ä‡∑í‡∂†‡∂Ω‡∑ä‚Äç‡∂∫‡∂∫‡∑ö ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∂ö‡∑ä ‡∑Ñ‡∑ú‡∂≥‡∑í‡∂±‡∑ä ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫‡∑è‡∂≠‡∑ä‡∂∏‡∂ö ‡∑Ä‡∑ì‡∂∏‡∂ß ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∑É‡∑í‡∂∫‡∂Ω‡∑ä‡∂Ω ‡∂Ö‡∂©‡∂Ç‡∂ú‡∑î ‡∑Ä‡∑ö. DistilBERT ‡∑É‡∂≥‡∑Ñ‡∑è, ‡∂ë‡∂∫‡∂ß ‡∂Ü‡∂Ø‡∑è‡∂± ‡∑Ñ‡∑ê‡∂≥‡∑î‡∂±‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä ‡∂∏‡∑ô‡∂±‡∑ä‡∂∏ ‡∂Ö‡∑Ä‡∂∞‡∑è‡∂± ‡∂Ü‡∑Ä‡∂ª‡∂´‡∂∫‡∂Ø ‡∂á‡∂≠‡∑î‡∑Ö‡∂≠‡∑ä ‡∑Ä‡∑ö. ‡∂Ö‡∂∏‡∂≠‡∂ª ‡∂Ü‡∂Ø‡∑è‡∂± ‡∂¥‡∑í‡∑Ö‡∑í‡∂ú‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂±‡∑ô‡∂ö‡∑î‡∂≠‡∑ä ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫‡∑Ä‡∂Ω ‡∂Ø ‡∂ë‡∂∏ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂Ø‡∑è‡∂±‡∂∫ `tokenizer` ‡∑Ä‡∑É‡∑ä‡∂≠‡∑î‡∑Ä ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∂á‡∂≠.

‡∂¥‡∑Ñ‡∂≠ ‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´ ‡∂ö‡∑í‡∑Ñ‡∑í‡∂¥‡∂∫‡∂ö‡∑í‡∂±‡∑ä ‡∂Ö‡∂¥‡∑í ‡∂Ø‡∂ö‡∑í‡∂± ‡∂¥‡∂ª‡∑í‡∂Ø‡∑í, ‡∂∏‡∑ô‡∂∏ ‡∂ö‡∑ä‚Äç‡∂ª‡∂∏‡∂∫ ‡∂â‡∂≠‡∑è ‡∂∂‡∂Ω‡∑Ä‡∂≠‡∑ä ‡∑Ä‡∑ö. ‡∂¥‡∑Ö‡∂∏‡∑î‡∑Ä, ‡∂ë‡∂∫ ‡∂≠‡∂±‡∑í ‡∂Ö‡∂±‡∑î‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω‡∂ö‡∑ä ‡∂ß‡∑ù‡∂ö‡∂±‡∑í‡∂ö‡∂ª‡∂´‡∂∫ ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í‡∂∫:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

‡∂ë‡∂∫ API ‡∑Ñ‡∑í ‡∂ö‡∑í‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑ô‡∂±‡∑É‡∂ö‡∑ä ‡∂±‡∑ú‡∂∏‡∑ê‡∂≠‡∑í‡∑Ä ‡∑Ä‡∂ª‡∂ö‡∂ß ‡∂∂‡∑Ñ‡∑î ‡∂Ö‡∂±‡∑î‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω‡∑Ä‡∂Ω‡∑ä ‡∑Ñ‡∑É‡∑î‡∂ª‡∑î‡∑Ä‡∂∫‡∑í:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

‡∂ë‡∂∫ ‡∂Ö‡∂ª‡∂∏‡∑î‡∂´‡∑î ‡∂ö‡∑í‡∑Ñ‡∑í‡∂¥‡∂∫‡∂ö‡∑ä ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂ã‡∂¥‡∂∞‡∑è‡∂± ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í‡∂∫:

```py
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

‡∂ë‡∂∫‡∂ß ‡∂Ö‡∂±‡∑î‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω‡∑Ä‡∂Ω‡∑ä ‡∂ö‡∂¥‡∑ä‡∂¥‡∑è‡∂Ø‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∂ß ‡∂Ø ‡∑Ñ‡∑ê‡∂ö‡∑í‡∂∫:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer` ‡∑Ä‡∑É‡∑ä‡∂≠‡∑î‡∑Ä‡∂ß ‡∂±‡∑í‡∑Å‡∑ä‡∂†‡∑í‡∂≠ ‡∂ª‡∑è‡∂∏‡∑î ‡∂Ü‡∂≠‡∑è‡∂±‡∂ö ‡∑Ä‡∑ô‡∂≠ ‡∂¥‡∂ª‡∑í‡∑Ä‡∂ª‡∑ä‡∂≠‡∂±‡∂∫ ‡∑Ñ‡∑ê‡∑É‡∑í‡∂ª‡∑Ä‡∑í‡∂∫ ‡∑Ñ‡∑ê‡∂ö, ‡∂¥‡∑É‡∑î‡∑Ä ‡∂ë‡∂∫ ‡∑É‡∑ò‡∂¢‡∑î‡∑Ä‡∂∏ ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫ ‡∑Ä‡∑ô‡∂≠ ‡∂∫‡∑ê‡∑Ä‡∑í‡∂∫ ‡∑Ñ‡∑ê‡∂ö. ‡∂ã‡∂Ø‡∑è‡∑Ñ‡∂ª‡∂´‡∂∫‡∂ö‡∑ä ‡∂Ω‡∑ô‡∑É, ‡∂¥‡∑Ñ‡∂≠ ‡∂ö‡∑ö‡∂≠ ‡∂±‡∑í‡∂∫‡∑ê‡∂Ø‡∑í‡∂∫‡∑ö ‡∂Ö‡∂¥‡∑í ‡∑Ä‡∑í‡∑Ä‡∑í‡∂∞ ‡∂ª‡∑è‡∂∏‡∑î ‡∑Ä‡∂Ω‡∑í‡∂±‡∑ä ‡∂Ü‡∂≠‡∑è‡∂±‡∂ö ‡∂Ü‡∂¥‡∑É‡∑î ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ì‡∂∏‡∂ß ‡∂ß‡∑ù‡∂ö‡∂±‡∂ö‡∑è‡∂ª‡∂ö ‡∂¥‡∑ú‡∑Ö‡∂π‡∑Ä‡∂∏‡∑î - `"pt"` PyTorch ‡∂Ü‡∂≠‡∑è‡∂±‡∂ö ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂∫‡∑í, `"tf"` TensorFlow ‡∂Ü‡∂≠‡∑è‡∂±‡∂ö ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂∫‡∑í, ‡∑É‡∑Ñ `"np"` NumPy ‡∂Ö‡∂ª‡∑è ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂∫‡∑í:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∂ß‡∑ù‡∂ö‡∂±[[special-tokens]]

‡∂Ö‡∂¥‡∑í ‡∂ß‡∑ù‡∂ö‡∂±‡∂ö‡∑è‡∂ª‡∂ö ‡∂∏‡∂ü‡∑í‡∂±‡∑ä ‡∂Ü‡∂¥‡∑É‡∑î ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ô‡∂± ‡∂Ü‡∂Ø‡∑è‡∂± ‡∑Ñ‡∑ê‡∂≥‡∑î‡∂±‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä ‡∂Ø‡∑ô‡∑É ‡∂∂‡∑ê‡∂Ω‡∑î‡∑Ä‡∑Ñ‡∑ú‡∂≠‡∑ä, ‡∂í‡∑Ä‡∑è ‡∂Ö‡∂¥ ‡∂ö‡∂Ω‡∑í‡∂±‡∑ä ‡∂≠‡∑í‡∂∂‡∑ñ ‡∂í‡∑Ä‡∑è‡∂ß ‡∑Ä‡∂©‡∑è ‡∂ö‡∑î‡∂©‡∑è ‡∑Ä‡∑ô‡∂±‡∑É‡∂ö‡∑ä ‡∂á‡∂≠‡∑í ‡∂∂‡∑Ä ‡∂Ö‡∂¥‡∂ß ‡∂¥‡∑ô‡∂±‡∑ô‡∂±‡∑î ‡∂á‡∂≠:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑‡∂∫‡∑ö‡∂Ø‡∑ì ‡∂ë‡∂ö‡∑ä ‡∑É‡∂Ç‡∂ö‡∑ö‡∂≠ ‡∑Ñ‡∑ê‡∂≥‡∑î‡∂±‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∂ö‡∑ä ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª‡∂± ‡∂Ω‡∂Ø ‡∂Ö‡∂≠‡∂ª ‡∂Ö‡∑Ä‡∑É‡∑è‡∂±‡∂∫‡∑ö ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∂ª‡∂± ‡∂Ω‡∂Ø‡∑ì. ‡∂∏‡∑ô‡∂∫ ‡∂ö‡∑î‡∂∏‡∂ö‡∑ä ‡∂Ø‡∑ê‡∂∫‡∑í ‡∂∂‡∑ê‡∂Ω‡∑ì‡∂∏‡∂ß ‡∂â‡∑Ñ‡∂≠ ‡∑Ñ‡∑ê‡∂≥‡∑î‡∂±‡∑î‡∂∏‡∑ä‡∂¥‡∂≠‡∑ä ‡∂Ö‡∂±‡∑î‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω ‡∂Ø‡∑ô‡∂ö ‡∑Ä‡∑í‡∂ö‡∑ö‡∂≠‡∂±‡∂∫ ‡∂ö‡∂ª‡∂∏‡∑î:

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

‡∂ß‡∑ù‡∂ö‡∂±‡∂ö‡∑è‡∂ª‡∂ö ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑‡∂∫‡∑ö‡∂Ø‡∑ì ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∑Ä‡∂†‡∂±‡∂∫ `[CLS]` ‡∑É‡∑Ñ ‡∂Ö‡∑Ä‡∑É‡∑è‡∂±‡∂∫‡∑ö `[SEP]` ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∑Ä‡∂†‡∂±‡∂∫ ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑Ö‡∑ö‡∂∫. ‡∂∏‡∂ö‡∑ä‡∂±‡∑í‡∑É‡∑è‡∂Ø ‡∂∫‡∂≠‡∑ä, ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫ ‡∂í‡∑Ä‡∑è ‡∑É‡∂∏‡∂ü ‡∂¥‡∑ñ‡∂ª‡∑ä‡∑Ä ‡∂¥‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∂ö‡∂ª ‡∂á‡∂≠‡∑í ‡∂±‡∑í‡∑É‡∑è, ‡∂Ö‡∂±‡∑î‡∂∏‡∑è‡∂± ‡∑É‡∂≥‡∑Ñ‡∑è ‡∑É‡∂∏‡∑è‡∂± ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂µ‡∂Ω ‡∂Ω‡∂∂‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥ ‡∂í‡∑Ä‡∑è ‡∂Ø ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∑Ö ‡∂∫‡∑î‡∂≠‡∑î‡∂∫. ‡∑É‡∂∏‡∑Ñ‡∂ª ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑ì‡∂±‡∑ä ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∑Ä‡∂†‡∂± ‡∂ë‡∂ö‡∂≠‡∑î ‡∂±‡∑ú‡∂ö‡∂ª‡∂± ‡∂∂‡∑Ä ‡∑É‡∂Ω‡∂ö‡∂±‡∑ä‡∂±, ‡∑Ñ‡∑ù ‡∑Ä‡∑ô‡∂±‡∑É‡∑ä ‡∂í‡∑Ä‡∑è ‡∂ë‡∂ö‡∂≠‡∑î ‡∂±‡∑ú‡∂ö‡∂ª‡∂∫‡∑í; ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í ‡∑Ä‡∂Ω‡∂ß ‡∂∏‡∑ô‡∂∏ ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∑Ä‡∂†‡∂± ‡∂ë‡∂ö‡∂≠‡∑î ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑ä‡∂ö‡∑ö ‡∂∏‡∑î‡∂Ω‡∂Ø‡∑ì ‡∑Ñ‡∑ù ‡∂Ö‡∑Ä‡∑É‡∑è‡∂±‡∂∫‡∑ö ‡∂¥‡∂∏‡∂´‡∑í. ‡∂ï‡∂±‡∑ë‡∂∏ ‡∂Ö‡∑Ä‡∑É‡∑ä‡∂Æ‡∑è‡∑Ä‡∂ö, ‡∂ß‡∑ù‡∂ö‡∂±‡∂ö‡∑è‡∂ª‡∂ö ‡∂∂‡∂Ω‡∑è‡∂¥‡∑ú‡∂ª‡∑ú‡∂≠‡∑ä‡∂≠‡∑î ‡∑Ä‡∂±‡∑ä‡∂±‡∑ö ‡∂ö‡∑î‡∂∏‡∂± ‡∂í‡∑Ä‡∑è‡∂Ø‡∑ê‡∂∫‡∑í ‡∂Ø‡∂±‡∑ä‡∂±‡∑è ‡∂Ö‡∂≠‡∂ª ‡∂î‡∂∂ ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä ‡∂∏‡∑ô‡∂∫ ‡∑É‡∑í‡∂Ø‡∑î ‡∂ö‡∂ª‡∂±‡∑î ‡∂á‡∂≠.

## ‡∂Ö‡∑Ä‡∑É‡∂±‡∑ä ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏: ‡∂ß‡∑ù‡∂ö‡∂±‡∂ö‡∑è‡∂ª‡∂ö ‡∑É‡∑í‡∂ß ‡∂Ü‡∂ö‡∑ò‡∂≠‡∑í‡∂∫ ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è[[wrapping-up-from-tokenizer-to-model]]

‡∂Ø‡∑ê‡∂±‡∑ä ‡∂Ö‡∂¥‡∑í `tokenizer` ‡∑Ä‡∑É‡∑ä‡∂≠‡∑î‡∑Ä ‡∂¥‡∑è‡∂® ‡∂∏‡∂≠ ‡∂∫‡∑ú‡∂Ø‡∂± ‡∑Ä‡∑í‡∂ß ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∑è ‡∂ö‡∂ª‡∂± ‡∑É‡∑í‡∂∫‡∂Ω‡∑î‡∂∏ ‡∂¥‡∑í‡∂∫‡∑Ä‡∂ª‡∂∫‡∂±‡∑ä ‡∂Ø‡∑ê‡∂ö ‡∂á‡∂≠‡∑í ‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä, ‡∂ë‡∂∫ ‡∂∂‡∑Ñ‡∑î ‡∂Ö‡∂±‡∑î‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω‡∑Ä‡∂Ω‡∑ä (‡∂ã‡∂¥‡∂∞‡∑è‡∂±!), ‡∂â‡∂≠‡∑è ‡∂Ø‡∑í‡∂ú‡∑î ‡∂Ö‡∂±‡∑î‡∂¥‡∑í‡∑Ö‡∑í‡∑Ä‡∑ô‡∂Ω‡∑Ä‡∂Ω‡∑ä (‡∂ö‡∂¥‡∑ä‡∂¥‡∑è‡∂Ø‡∑î ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏!) ‡∑É‡∑Ñ ‡∂ë‡∑Ñ‡∑í ‡∂¥‡∑ä‡∂ª‡∂∞‡∑è‡∂± API ‡∑É‡∂∏‡∂ü ‡∑Ä‡∑í‡∑Ä‡∑í‡∂∞ ‡∑Ä‡∂ª‡∑ä‡∂ú‡∑Ä‡∂Ω ‡∂Ü‡∂≠‡∑è‡∂±‡∂ö ‡∑Ñ‡∑ê‡∑É‡∑í‡∂ª‡∑Ä‡∑í‡∂∫ ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂Ö‡∂¥‡∑í ‡∂Ö‡∑Ä‡∑É‡∂±‡∑ä ‡∑Ä‡∂ª‡∂ß ‡∂∂‡∂Ω‡∂∏‡∑î:

{#if fw === 'pt'}

```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

{:else}

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```

{/if}
