<FrameworkSwitchCourse {fw} />

# ටෝකනකාරක[[tokenizers]]

{#if fw === 'pt'}

<CourseFloatingBanner
  <CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb"},
]} />

{/if}

<Youtube id="VFp38yj8h3A" />

ටෝකනකාරක යනු NLP pipeline යේ මූලික අංගයන්ගෙන් එකකි. ඔවුන් එක් අරමුණක් ඉටු කරයි: ආකෘතිය මඟින් සැකසිය හැකි දත්ත බවට පාඨ පරිවර්තනය කිරීම. ආකෘති වලට සංඛ්‍යා පමණක් සැකසිය හැක, එබැවින් ටෝකනකාරක වලට අපගේ පාඨ ආදාන සංඛ්‍යාත්මක දත්ත බවට පරිවර්තනය කිරීමට අවශ්‍ය වේ. මෙම කොටසේදී, අපි ටෝකනීකරණ pipeline යේ සිදුවන්නේ කුමක්ද යන්න හරියටම ගවේෂණය කරන්නෙමු.

NLP කාර්ය වලදී, සාමාන්‍යයෙන් අමු පාඨ සකසන ලද දත්ත වේ. මෙන්න එවැනි පාඨයකට උදාහරණයක්:

```
Jim Henson was a puppeteer
```

කෙසේ වෙතත්, ආකෘති වලට සංඛ්‍යා පමණක් සැකසිය හැක, එබැවින් අපි අමු පාඨ සංඛ්‍යා බවට පරිවර්තනය කිරීමට ක්‍රමයක් සොයා ගත යුතුය. ටෝකනකාරක කරන්නේ එයයි, මේ සඳහා බොහෝ ක්‍රම තිබේ. ඉලක්කය වන්නේ වඩාත්ම අර්ථවත් නිරූපණය සොයා ගැනීමයි - එනම්, ආකෘතියට වඩාත්ම අර්ථවත් කරන එක - සහ, හැකි නම්, කුඩාම නිරූපණය.

ටෝකනීකරණ ඇල්ගොරිතම පිළිබඳ උදාහරණ කිහිපයක් බලමු, සහ ටෝකනීකරණය පිළිබඳ ඔබට ඇති සමහර ප්‍රශ්නවලට පිළිතුරු දීමට උත්සාහ කරමු.

## වචන-පදනම් වූ[[word-based]]

<Youtube id="nhJxYji1aho" />

මතකයට එන පළමු ටෝකනකාරක වර්ගය _වචන-පදනම් වූ_ වේ. රීති කිහිපයක් සමඟ පමණක් සැකසීම සහ භාවිතා කිරීම සාමාන්‍යයෙන් ඉතා පහසු වන අතර එය බොහෝ විට යහපත් ප්‍රතිඵල ලබා දෙයි. උදාහරණයක් ලෙස, පහත රූපයේ, ඉලක්කය වන්නේ අමු පාඨ වචනවලට බෙදීම සහ ඒ සෑම එකක් සඳහාම සංඛ්‍යාත්මක නිරූපණයක් සොයා ගැනීමයි:

<div class="flex justify-center">
  <img
    class="block dark:hidden"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg"
    alt="An example of word-based tokenization."
  />
  <img
    class="hidden dark:block"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg"
    alt="An example of word-based tokenization."
  />
</div>

පාඨ බෙදීමට විවිධ ක්‍රම තිබේ. උදාහරණයක් ලෙස, අපට Python හි 'split()' කාර්ය යෙදීමෙන් පාඨ වචන වලට සංකේත කිරීමට සුදු අවකාශය භාවිතා කළ හැක:

```py
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

```python out
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

විරාම ලකුණු සඳහා අමතර නීති ඇති වචන ටෝකනකාරකවල වෙනස්කම් ද ඇත. මේ ආකාරයේ ටෝකනකාරක සමඟින්, අපට ඉතා විශාල "වචන මාලාවක්" සමඟ අවසන් කළ හැකිය, එහිදී වචන මාලාවක් නිර්වචනය කරනු ලබන්නේ අපගේ සංග්‍රහයෙහි ඇති මුළු ස්වාධීන ටෝකන ගණනෙනි.

සෑම වචනයකටම 0 සිට ආරම්භ වී වචන මාලාවේ ප්‍රමාණය දක්වා හැඳුනුම්පතක් පවරනු ලැබේ. ආකෘතිය එක් එක් වචනය හඳුනා ගැනීමට මෙම හැඳුනුම්පත භාවිතා කරයි.

අපට වචනයක් පදනම් වූ ටෝකනකාරක සමඟ භාෂාවක් සම්පූර්ණයෙන්ම ආවරණය කිරීමට අවශ්‍ය නම්, අපට භාෂාවේ සෑම වචනයක් සඳහාම හඳුනාගැනීමක් තිබීම අවශ්‍ය වේ, එමඟින් විශාල ටෝකන ප්‍රමාණයක් ජනනය වේ. උදාහරණයක් ලෙස, ඉංග්‍රීසි භාෂාවේ වචන 500,000 කට වඩා ඇත, එබැවින් සෑම වචනයකින්ම ආදාන හැඳුනුම්පතකට සිතියමක් තැනීමට අපට එම හැඳුනුම්පත් ගණන නිරීක්ෂණය කිරීමට අවශ්‍ය වේ. තවද, "බල්ලා" වැනි වචන "බල්ලන්" වැනි වචනවලට වඩා වෙනස් ලෙස නිරූපණය වන අතර, "බල්ලා" සහ "බල්ලන්" සමාන බව දැන ගැනීමට ආකෘතියට මුලදී ක්‍රමයක් නොමැත: එය වචන දෙක එකිනෙකට සම්බන්ධ නොවන ලෙස හඳුනා ගනී. "ධාවනය" සහ "ධාවනය වන" වැනි වෙනත් සමාන වචන සඳහාද මෙය අදාළ වේ, ආකෘතිය මුලින් සමාන බවක් නොපෙනේ.

අවසාන වශයෙන්, අපගේ වචන මාලාවේ නොමැති වචන නිරූපණය කිරීමට අභිරුචි ටෝකනයක් අවශ්‍ය වේ. මෙය "නොදන්නා" ටෝකනය ලෙස හැඳින්වේ, බොහෝ විට "[UNK]" හෝ "&lt;unk&gt;" ලෙස නියෝජනය වේ. ටෝකනකාරක මෙම ටෝකන බොහොමයක් නිපදවන බව ඔබ දුටුවහොත් එය සාමාන්‍යයෙන් නරක ලකුණකි, මන්ද එයට වචනයක සංවේදී නිරූපණයක් ලබා ගැනීමට නොහැකි වූ අතර ඔබට මඟදී තොරතුරු අහිමි වේ. වචන මාලාව සැකසීමේදී ඉලක්කය වන්නේ ටෝකනකාරක විසින් නොදන්නා ටෝකනයට හැකි තරම් වචන කිහිපයක් ටෝකනීකරණය කරන ආකාරයට එය කිරීමයි.

නොදන්නා ටෝකන ප්‍රමාණය අඩු කිරීමට එක් ක්‍රමයක් නම් _අක්ෂර මත පදනම් වූ_ ටෝකනකාරක භාවිතා කරමින් එක් මට්ටමක් ගැඹුරට යාමයි.

## අක්ෂර මත පදනම් වූ[[character-based]]

<Youtube id="ssLq_EK2jLE" />

අක්ෂර පාදක ටෝකනකාරක, වචන වලට වඩා අකුරු වලට පාඨ බෙදයි. මෙහි මූලික ප්‍රතිලාභ දෙකක් ඇත:

- වචන මාලාව බෙහෙවින් කුඩා ය.
- සෑම වචනයක්ම අක්ෂර වලින් ගොඩනගා ගත හැකි බැවින්, වචන මාලාවෙන් පිටත (නොදන්නා) ටෝකන ඉතා අඩුය.

නමුත් මෙහිද අවකාශ සහ විරාම ලකුණු සම්බන්ධයෙන් ප්‍රශ්න කිහිපයක් පැන නගී:

<div class="flex justify-center">
  <img
    class="block dark:hidden"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg"
    alt="An example of character-based tokenization."
  />
  <img
    class="hidden dark:block"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg"
    alt="An example of character-based tokenization."
  />
</div>

මෙම ප්‍රවේශය ද පරිපූර්ණ නොවේ. නිරූපණය දැන් වචන වලට වඩා අක්ෂර මත පදනම් වී ඇති බැවින්, එය අවබෝධාත්මකව, අඩු අර්ථවත් බව කෙනෙකුට තර්ක කළ හැකිය: සෑම අකුරක්ම බොහෝ දේ අදහස් කරන්නේ නැත, නමුත් වචන සම්බන්ධයෙන් එය එසේ වේ. කෙසේ වෙතත්, මෙය නැවතත් භාෂාව අනුව වෙනස් වේ; උදාහරණයක් ලෙස, චීන භාෂාවෙන්, සෑම අක්ෂරයක්ම ලතින් භාෂාවක අක්ෂරයකට වඩා වැඩි තොරතුරු රැගෙන යයි.

සලකා බැලිය යුතු තවත් කරුණක් නම්, අපගේ ආකෘතිය මඟින් සැකසීමට ඉතා විශාල ටෝකන ප්‍රමාණයක් අපට ලැබෙනු ඇත: වචනයක් වචන පදනම් වූ ටෝකනකාරකයක් සහිත තනි ටෝකනයක් පමණක් වන අතර, අක්ෂර බවට පරිවර්තනය කරන විට එය පහසුවෙන් ටෝකන 10ක් හෝ වැඩි ගණනක් බවට පත් විය හැක.

අන්ත දෙකෙන්ම හොඳම දේ ලබා ගැනීමට, අපට ප්‍රවේශයන් දෙක ඒකාබද්ධ කරන තුන්වන තාක්ෂණය භාවිතා කළ හැකිය: _උපපද ටෝකනීකරණය_.

## උපපද ටෝකනීකරණය[[subword-tokenization]]

<Youtube id="zHvTiHr506c" />

උපපද ටෝකනීකරණ ඇල්ගොරිතම නිතර භාවිතා කරන වචන කුඩා උපපදවලට බෙදිය යුතු නැත, නමුත් දුර්ලභ වචන අර්ථවත් යටිපද බවට වියෝජනය කළ යුතුය යන මූලධර්මය මත රඳා පවතී.

නිදසුනක් ලෙස, "annoyingly" යන්න දුර්ලභ වචනයක් ලෙස සැලකිය හැකි අතර එය "annoying" සහ "ly" ලෙස වියෝජනය කළ හැක. මේ දෙකම ස්වාධීන උප පද ලෙස නිතර දිස් වීමට ඉඩ ඇති අතර, ඒ අතරම "annoyingly" යන්නෙහි තේරුම " annoying" සහ "ly" යන සංයුක්ත අර්ථයෙන් තබා ඇත.

උප පද ටෝකනීකරණ ඇල්ගොරිතමයක් "අපි ටෝකනීකරණය කරමු!" අනුපිළිවෙල සංකේතනය කරන ආකාරය පෙන්වන උදාහරණයක් මෙන්න:

<div class="flex justify-center">
  <img
    class="block dark:hidden"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg"
    alt="A subword tokenization algorithm."
  />
  <img
    class="hidden dark:block"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg"
    alt="A subword tokenization algorithm."
  />
</div>

මෙම යටිපද බොහෝ අර්ථකථන අර්ථයක් ලබා දෙයි: නිදසුනක් ලෙස, ඉහත උදාහරණයේ "ටෝකනීකරණය" "ටෝකනය" සහ "කරණය" ලෙස බෙදා ඇත, අවකාශය-කාර්යක්ෂම වන අතරම අර්ථකථන අර්ථයක් ඇති ටෝකන දෙකක් (ටෝකන දෙකක් පමණක් අවශ්‍ය වේ. දිගු වචනයක් නියෝජනය කිරීමට). මෙය අපට කුඩා වචන මාලාවක් සමඟ සාපේක්ෂව හොඳ ආවරණයක් ලබා ගැනීමට ඉඩ සලසයි, සහ නොදන්නා ටෝකන වලට සමීප නොවේ.

මෙම ප්‍රවේශය විශේෂයෙන් ප්‍රයෝජනවත් වන්නේ තුර්කි වැනි එකතු කිරීමේ භාෂා වල ය, එහිදී ඔබට යටිපද එකතු කිරීමෙන් අත්තනෝමතික ලෙස දිගු සංකීර්ණ වචන (පාහේ) සෑදිය හැක.

### සහ තවත්![[and-more]]

පුදුමයට කරුණක් නම්, එහි තවත් බොහෝ ශිල්පීය ක්‍රම තිබේ. කිහිපයක් නම්:

- GPT-2 හි භාවිතා වන පරිදි Byte-level BPE
- BERT හි භාවිතා වන පරිදි WordPiece
- SentencePiece හෝ Unigram, බහුභාෂා ආකෘති කිහිපයක භාවිතා වන පරිදි

API සමඟ ආරම්භ කිරීමට ටෝකනකාරක ක්‍රියා කරන ආකාරය පිළිබඳව ඔබට දැන් ප්‍රමාණවත් දැනුමක් තිබිය යුතුය.

## පුරණය සහ සුරැකීම[[loading-and-saving]]

ටෝකනකාරක පුරණය සහ සුරැකීම ආකෘති සමඟ මෙන්ම සරල ය. ඇත්ත වශයෙන්ම, එය එකම ක්‍රම දෙක මත පදනම් වේ: `from_pretrained()` සහ `save_pretrained()`. මෙම ක්‍රම මගින් ටෝකනකාරක විසින් භාවිතා කරන ඇල්ගොරිතම (ආකෘතියේ _නිර්මිත_ වැනි තරමක්) මෙන්ම එහි වචන මාලාව (ආකෘතියේ _භාර_ වැනි තරමක්) පූරණය හෝ සුරැකෙනු ඇත.

BERT හා සමාන මුරපොලකින් පුහුණු කරන ලද BERT ටෝකනයිසර් පූරණය කිරීම, අපි `BertTokenizer` පන්තිය භාවිතා කරනවා හැර, ආකෘතිය පූරණය කරන ආකාරයටම සිදු කරයි:

```py
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

{#if fw === 'pt'}
`AutoModel` හා සමානව, `AutoTokenizer` පන්තිය මුරපොල නම මත පදනම්ව library යේ නිසි ටෝකනකාරක පන්තිය අල්ලා ගනු ඇති අතර, ඕනෑම මුරපොලක් සමඟ කෙලින්ම භාවිතා කළ හැක:

{:else}
`TFAutoModel` හා සමානව, `AutoTokenizer` පන්තිය මුරපොල නම මත පදනම්ව library යේ නිසි ටෝකනකාරක පන්තිය අල්ලා ගනු ඇති අතර, ඕනෑම මුරපොලක් සමඟ කෙලින්ම භාවිතා කළ හැක:

{/if}

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

පෙර කොටසේ පෙන්වා ඇති පරිදි අපට දැන් ටෝකනකාරක භාවිතා කළ හැකිය:

```python
tokenizer("Using a Transformer network is simple")
```

```python out
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ටෝකනකාරක සුරැකීම ආකෘතියක් සුරැකීමට සමාන වේ:

```py
tokenizer.save_pretrained("directory_on_my_computer")
```

අපි [පරිච්ඡේදය 3](/course/chapter3) හි `token_type_ids` ගැන වැඩි විස්තර කතා කරමු, අපි මඳ වේලාවකට පසුව `attention_mask` යතුර පැහැදිලි කරන්නෙමු. පළමුව, අපි බලමු `input_ids` ජනනය වන්නේ කෙසේදැයි. මෙය සිදු කිරීම සඳහා, අපි ටෝකනකාරක අතරමැදි ක්‍රම දෙස බැලිය යුතුය.

## කේතනය[[encoding]]

<Youtube id="Yffk5aydLzg" />

පාඨ අංකවලට පරිවර්තනය කිරීම _කේතනය_ ලෙස හැඳින්වේ. කේතනය කිරීම පියවර දෙකක ක්‍රියාවලියකින් සිදු කෙරේ: ටෝකනීකරණය, පසුව ආදාන හැඳුනුම්පත් බවට පරිවර්තනය කිරීම.

අප දැක ඇති පරිදි, පළමු පියවර වන්නේ සාමාන්‍යයෙන් _ටෝකන_ ලෙස හඳුන්වන වචන (හෝ වචනවල කොටස්, විරාම ලකුණු, ආදිය) පෙළට බෙදීමයි. එම ක්‍රියාවලිය පාලනය කළ හැකි බහුවිධ නීති ඇත, එම නිසා අපි ආකෘතියේ නම භාවිතයෙන් ටෝකනකාරක ක්ෂණිකව ක්‍රියාත්මක කිරීමට අවශ්‍ය වේ, අපි ආකෘතිය පෙර පුහුණු කළ විට භාවිතා කළ නීතිම භාවිතා කරන බවට වග බලා ගන්න.

දෙවන පියවර වන්නේ එම ටෝකන සංඛ්‍යා බවට පරිවර්තනය කිරීමයි, එවිට අපට ඒවායින් ආතානකයක් සාදා ඒවා ආකෘතියට පෝෂණය කළ හැකිය. මෙය සිදු කිරීම සඳහා, ටෝකනකාරක සතුව _පදමාලාවක්_ ඇත, එය අපි එය `from_pretrained()` ක්‍රමය සමඟ ක්ෂණිකව ලබා ගන්නා විට බාගන්නා කොටසයි. නැවතත්, අපි ආකෘතිය පෙර පුහුණු වූ විට භාවිතා කළ වචන මාලාවම භාවිතා කළ යුතුය.

පියවර දෙක පිළිබඳ වඩා හොඳ අවබෝධයක් ලබා ගැනීම සඳහා, අපි ඒවා වෙන වෙනම ගවේෂණය කරන්නෙමු. එම පියවරවල අතරමැදි ප්‍රතිඵල ඔබට පෙන්වීමට අපි ටෝකනකාරක pipeline යේ කොටස් වෙන වෙනම සිදු කරන ක්‍රම කිහිපයක් භාවිතා කරන බව සලකන්න, නමුත් ප්‍රායෝගිකව, ඔබ ඔබේ යෙදවුම් මත කෙලින්ම ටෝකනකාරක ක්‍රියාකරවන්න (2 කොටසේ පෙන්වා ඇති පරිදි).

### ටෝකනීකරණය[[tokenization]]

ටෝකනීකරණය කිරීමේ ක්‍රියාවලිය සිදු කරනු ලබන්නේ ටෝකනීකරණයේ `tokenize()` ක්‍රමය මගිනි:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

මෙම ක්‍රමයේ ප්‍රතිදානය යනු තන්තු හෝ ටෝකන ලැයිස්තුවකි:

```python out
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

මෙම ටෝකනකාරක යනු උපපද ටෝකනකාරක ය: එය එහි වචන මාලාවෙන් නිරූපණය කළ හැකි ටෝකන ලබා ගන්නා තෙක් වචන බෙදයි. ටෝකන දෙකකට බෙදා ඇති `transformer` සමඟ මෙහි තත්ත්වය එයයි: `transform` සහ `##er`.

### ටෝකන සිට ආදාන හැඳුනුම්පත් දක්වා[[from-tokens-to-input-ids]]

ආදාන හැඳුනුම්පත් බවට පරිවර්තනය `convert_tokens_to_ids()` ටෝකනකාරක ක්‍රමය මඟින් හසුරුවනු ලැබේ:

```py
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

```python out
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

මෙම ප්‍රතිදානයන්, සුදුසු රාමු ආතානක බවට පරිවර්තනය කළ පසු, මෙම පරිච්ඡේදයේ කලින් දැක ඇති පරිදි ආකෘතියකට යෙදවුම් ලෙස භාවිතා කළ හැක.

<Tip>

✏️ **උත්සාහ කරන්න!** අපි 2 කොටසේ භාවිතා කළ ආදාන වාක්‍යවල අවසාන පියවර දෙක (ටෝකනීකරණය සහ ආදාන හැඳුනුම්පත් බවට පරිවර්තනය කිරීම) අනුවර්තනය කරන්න ("I've been waiting for a HuggingFace course my whole life." සහ "I hate this so much!"). අප කලින් ලබා ගත් ආදාන හැඳුනුම්පත් ඔබටත් ලැබෙනවාදැයි පරීක්ෂා කරන්න!

</Tip>

## විකේතනය[[decoding]]

_විකේතනය_ අනෙක් ආකාරයට ක්‍රියාකරයි: වචන මාලාවේ දර්ශක වලින්, අපට තන්තුවක් ලබා ගැනීමට අවශ්‍යයි. මෙය පහත පරිදි 'decode()' ක්‍රමය සමඟ කළ හැක:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

`decode` ක්‍රමය මඟින් දර්ශක නැවත ටෝකන බවට පරිවර්තනය කරනවා පමණක් නොව, කියවිය හැකි වාක්‍යයක් නිපදවීම සඳහා එම වචනවලම කොටසක් වූ ටෝකන එකතු කරන බව සලකන්න. අපි නව පෙළ පුරෝකථනය කරන ආකෘති භාවිතා කරන විට මෙම හැසිරීම අතිශයින් ප්‍රයෝජනවත් වනු ඇත (එක්කෝ පෙළගැස්මකින් උත්පාදනය කරන ලද පෙළ, හෝ පරිවර්තනය හෝ සාරාංශ කිරීම වැනි අනුපිළිවෙලින් අනුක්‍රමික ගැටළු සඳහා).

මේ වන විට ඔබ ටෝකනකාරකයකට හැසිරවිය හැකි පරමාණුක ක්‍රියාවන් තේරුම් ගත යුතුය: ටෝකනීකරණය, හැඳුනුම්පත් බවට පරිවර්තනය කිරීම සහ හැඳුනුම්පත් නැවත තන්තුවකට පරිවර්තනය කිරීම. කෙසේ වෙතත්, අපි අයිස් කුට්ටියේ කෙළවර සීරීමට ලක් කර ඇත. ඊළඟ කොටසේදී, අපි අපගේ ප්‍රවේශය එහි සීමාවන් වෙත ගෙන ඒවා ජය ගන්නේ කෙසේදැයි බලමු.
