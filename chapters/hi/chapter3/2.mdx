<FrameworkSwitchCourse {fw} />

# рдбреЗрдЯрд╛ рд╕рдВрд╕рд╛рдзрд┐рдд рдХрд░рдирд╛

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/hi/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/hi/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/hi/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/hi/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
[рдкрд┐рдЫрд▓реЗ рдЕрдзреНрдпрд╛рдп](/course/chapter2) рдХреЗ рдЙрджрд╛рд╣рд░рдг рдХреЛ рдЬрд╛рд░реА рд░рдЦрддреЗ рд╣реБрдП, рдпрд╣рд╛рдВ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рд╣реИ рдХрд┐ рд╣рдо PyTorch рдореЗрдВ рдПрдХ рдмреИрдЪ рдкрд░ рдЕрдиреБрдХреНрд░рдо рд╡рд░реНрдЧреАрдХрд╛рд░рдХ рдХреЛ рдХреИрд╕реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдХрд░реЗрдВрдЧреЗ:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
[рдкрд┐рдЫрд▓реЗ рдЕрдзреНрдпрд╛рдп](/course/chapter2) рдХреЗ рдЙрджрд╛рд╣рд░рдг рдХреЛ рдЬрд╛рд░реА рд░рдЦрддреЗ рд╣реБрдП, рдпрд╣рд╛рдВ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рд╣реИ рдХрд┐ рд╣рдо TensorFlow рдореЗрдВ рдПрдХ рдмреИрдЪ рдкрд░ рдЕрдиреБрдХреНрд░рдо рд╡рд░реНрдЧреАрдХрд╛рд░рдХ рдХреЛ рдХреИрд╕реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдХрд░реЗрдВрдЧреЗ:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

рдмреЗрд╢рдХ, рдХреЗрд╡рд▓ рджреЛ рд╡рд╛рдХреНрдпреЛрдВ рдкрд░ рдореЙрдбрд▓ рдХреЛ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдХрд░рдиреЗ рд╕реЗ рдмрд╣реБрдд рдЕрдЪреНрдЫреЗ рдкрд░рд┐рдгрд╛рдо рдирд╣реАрдВ рдорд┐рд▓реЗрдВрдЧреЗред рдмреЗрд╣рддрд░ рдкрд░рд┐рдгрд╛рдо рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рдЖрдкрдХреЛ рдПрдХ рдмрдбрд╝рд╛ рдбреЗрдЯрд╛рд╕реЗрдЯ рддреИрдпрд╛рд░ рдХрд░рдирд╛ рд╣реЛрдЧрд╛ред

рдЗрд╕ рдЦрдВрдб рдореЗрдВ рд╣рдо рдПрдХ рдЙрджрд╛рд╣рд░рдг рдХреЗ рд░реВрдк рдореЗрдВ MRPC (Microsoft Research Paraphrase Corpus) рдбреЗрдЯрд╛рд╕реЗрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВрдЧреЗ, рдЬрд┐рд╕реЗ рд╡рд┐рд▓рд┐рдпрдо рдмреА. рдбреЛрд▓рди рдФрд░ рдХреНрд░рд┐рд╕ рдмреНрд░реЛрдХреЗрдЯ рджреНрд╡рд╛рд░рд╛ рдПрдХ [рдкреЗрдкрд░](https://www.aclweb.org/anthology/I05-5002.pdf) рдореЗрдВ рдкреЗрд╢ рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ред рдбреЗрдЯрд╛рд╕реЗрдЯ рдореЗрдВ 5,801 рд╡рд╛рдХреНрдпреЛрдВ рдХреЗ рдЬреЛрдбрд╝реЗ рд╣реИрдВ, рд╕рд╛рде рдореЗ рдПрдХ рд▓реЗрдмрд▓ рдЬреЛ рджрд░реНрд╢рд╛рддрд╛ рд╣реИ рдХрд┐ рд╡реЗ рдкреИрд░рд╛рдлреНрд░реЗрдЬ рд╣реИрдВ рдпрд╛ рдирд╣реАрдВ (рдпрд╛рдиреА, рдХреНрдпрд╛ рджреЛрдиреЛрдВ рд╡рд╛рдХреНрдпреЛрдВ рдХрд╛ рдорддрд▓рдм рдПрдХ рд╣реА рд╣реИ)ред рд╣рдордиреЗ рдЗрд╕реЗ рдЗрд╕ рдЕрдзреНрдпрд╛рдп рдХреЗ рд▓рд┐рдП рдЪреБрдирд╛ рд╣реИ рдХреНрдпреЛрдВрдХрд┐ рдпрд╣ рдПрдХ рдЫреЛрдЯрд╛ рдбреЗрдЯрд╛рд╕реЗрдЯ рд╣реИ, рдЗрд╕рд▓рд┐рдП рдЗрд╕ рдкрд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХреЗ рд╕рд╛рде рдкреНрд░рдпреЛрдЧ рдХрд░рдирд╛ рдЖрд╕рд╛рди рд╣реИред

### рд╣рдм рд╕реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рд▓реЛрдб рдХрд░рдирд╛

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

рд╣рдм рдореЗрдВ рдХреЗрд╡рд▓ рдореЙрдбрд▓ рд╣реА рдирд╣реАрдВ рд╣реИрдВ; рдЗрд╕рдореЗрдВ рдХрдИ рдЕрд▓рдЧ-рдЕрд▓рдЧ рднрд╛рд╖рд╛рдУрдВ рдореЗрдВ рдХрдИ рдбреЗрдЯрд╛рд╕реЗрдЯ рднреА рд╣реИрдВред рдЖрдк [рдпрд╣рд╛рдВ](https://huggingface.co/datasets) рдбреЗрдЯрд╛рд╕реЗрдЯ рдмреНрд░рд╛рдЙрдЬрд╝ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рдФрд░ рд╣рдо рдЕрдиреБрд╢рдВрд╕рд╛ рдХрд░рддреЗ рд╣реИрдВ рдХрд┐ рдЖрдк рдЗрд╕ рдЕрдиреБрднрд╛рдЧ рдХреЛ рдкрдврд╝рдиреЗ рдХреЗ рдмрд╛рдж рдПрдХ рдирдП рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рд▓реЛрдб рдФрд░ рд╕рдВрд╕рд╛рдзрд┐рдд рдХрд░рдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВ ([рдпрд╣рд╛рдВ](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub) рд╕рд╛рдорд╛рдиреНрдп рджрд╕реНрддрд╛рд╡реЗрдЬ рджреЗрдЦреЗрдВ)ред рд▓реЗрдХрд┐рди рдЕрднреА рдХреЗ рд▓рд┐рдП, рдЖрдЗрдП MRPC рдбреЗрдЯрд╛рд╕реЗрдЯ рдкрд░ рдзреНрдпрд╛рди рджреЗрдВ! рдпрд╣ [GLUE рдмреЗрдВрдЪрдорд╛рд░реНрдХ](https://gluebenchmark.com/) рдХреА рд░рдЪрдирд╛ рдХрд░рдиреЗ рд╡рд╛рд▓реЗ 10 рдбреЗрдЯрд╛рд╕реЗрдЯ рдореЗрдВ рд╕реЗ рдПрдХ рд╣реИ, рдЬреЛ рдПрдХ рдЕрдХрд╛рджрдорд┐рдХ рдмреЗрдВрдЪрдорд╛рд░реНрдХ рд╣реИ рдЬрд┐рд╕рдХрд╛ рдЙрдкрдпреЛрдЧ 10 рдЕрд▓рдЧ-рдЕрд▓рдЧ рдкрд╛рда рд╡рд░реНрдЧреАрдХрд░рдг рдХрд╛рд░реНрдпреЛрдВ рдореЗрдВ ML рдореЙрдбрд▓ рдХреЗ рдкреНрд░рджрд░реНрд╢рди рдХреЛ рдорд╛рдкрдиреЗ рдХреЗ рд▓рд┐рдП рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред

ЁЯдЧ рдбреЗрдЯрд╛рд╕реЗрдЯ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдПрдХ рдмрд╣реБрдд рд╣реА рд╕рд░рд▓ рдХрдорд╛рдВрдб рдкреНрд░рджрд╛рди рдХрд░рддреА рд╣реИ рд╣рдм рдкрд░ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдбрд╛рдЙрдирд▓реЛрдб рдФрд░ рдХреИрд╢ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдПред рд╣рдо MRPC рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдЗрд╕ рддрд░рд╣ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

рдЬреИрд╕рд╛ рдХрд┐ рдЖрдк рджреЗрдЦ рд╕рдХрддреЗ рд╣реИрдВ, рд╣рдореЗрдВ рдПрдХ `DatasetDict` рд╡рд╕реНрддреБ рдорд┐рд▓рд╛ рдЬрд┐рд╕рдореЗрдВ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╕реЗрдЯ, рд╕рддреНрдпрд╛рдкрди рд╕реЗрдЯ рдФрд░ рдкрд░реАрдХреНрд╖рдг рд╕реЗрдЯ рд╣реИред рдЙрдирдореЗрдВ рд╕реЗ рдкреНрд░рддреНрдпреЗрдХ рдореЗрдВ рдХрдИ рдХреЙрд▓рдо (`sentence1`, `sentence2`, `label`, рдФрд░ `idx`) рдФрд░ рдПрдХ рдЪрд░ рдкрдВрдХреНрддрд┐рдпреЛрдВ рдХреА рд╕рдВрдЦреНрдпрд╛, рдЬреЛ рдкреНрд░рддреНрдпреЗрдХ рд╕реЗрдЯ рдореЗрдВ рддрддреНрд╡реЛрдВ рдХреА рд╕рдВрдЦреНрдпрд╛ рд╣реИ (рддреЛ, рд╡рд╛рдХреНрдпреЛрдВ рдХреЗ 3,668 рдЬреЛрдбрд╝реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╕реЗрдЯ рдореЗрдВ, 408 рд╕рддреНрдпрд╛рдкрди рд╕реЗрдЯ рдореЗрдВ, рдФрд░ рдкрд░реАрдХреНрд╖рдг рд╕реЗрдЯ рдореЗрдВ 1,725 рд╣реИ)ред

рдпрд╣ рдХрдорд╛рдВрдб рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдбрд╛рдЙрдирд▓реЛрдб рдФрд░ рдХреИрд╢ рдХрд░рддрд╛ рд╣реИрдВ, рдЬреЛ рдбрд┐рдлрд╝реЙрд▓реНрдЯ рд░реВрдк рд╕реЗ рдЗрд╕ рдЬрдЧрд╣ рдореЗ *~/.cache/huggingface/dataset* рдЬрд╛рддрд╛ рд╣реИрдВред рдЕрдзреНрдпрд╛рдп 2 рд╕реЗ рдпрд╛рдж рдХрд░реЗрдВ рдХрд┐ рдЖрдк `HF_HOME` рдкрд░реНрдпрд╛рд╡рд░рдг рдЪрд░ рд╕реЗрдЯ рдХрд░рдХреЗ рдЕрдкрдиреЗ рдХреИрд╢реЗ рдлрд╝реЛрд▓реНрдбрд░ рдХреЛ рдЕрдиреБрдХреВрд▓рд┐рдд рдХрд░ рдЬрдЧрд╣ рдмрджрд▓ рд╕рдХрддреЗ рд╣реИрдВред

рд╣рдо рдЕрдкрдиреЗ `raw_datasets` рд╡рд╕реНрддреБ рдореЗрдВ рд╡рд╛рдХреНрдпреЛрдВ рдХреА рдкреНрд░рддреНрдпреЗрдХ рдЬреЛрдбрд╝реА рдХреЛ рдЕрдиреБрдХреНрд░рдордгрд┐рдд рдХрд░рдХреЗ рдЕрднрд┐рдЧрдо рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рдЬреИрд╕реЗ рдХрд┐рд╕реА рд╢рдмреНрджрдХреЛрд╢ рдХреЗ рд╕рд╛рде:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

рд╣рдо рджреЗрдЦ рд╕рдХрддреЗ рд╣реИрдВ рдХрд┐ рд▓реЗрдмрд▓ рдкрд╣рд▓реЗ рд╕реЗ рд╣реА рдкреВрд░реНрдгрд╛рдВрдХ рд╣реИрдВ, рдЗрд╕рд▓рд┐рдП рд╣рдореЗрдВ рд╡рд╣рд╛рдВ рдХреЛрдИ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдирд╣реАрдВ рдХрд░рдирд╛ рд╣реЛрдЧрд╛ред рдпрд╣ рдЬрд╛рдирдиреЗ рдХреЗ рд▓рд┐рдП рдХрд┐ рдХреМрди рд╕рд╛ рдкреВрд░реНрдгрд╛рдВрдХ рдХрд┐рд╕ рд▓реЗрдмрд▓ рд╕реЗ рдореЗрд▓ рдЦрд╛рддрд╛ рд╣реИ, рд╣рдо рдЕрдкрдиреЗ `raw_train_dataset` рдХреА `features` рдХрд╛ рдирд┐рд░реАрдХреНрд╖рдг рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред рдпрд╣ рд╣рдореЗрдВ рдкреНрд░рддреНрдпреЗрдХ рдХреЙрд▓рдо рдХрд╛ рдкреНрд░рдХрд╛рд░ рдмрддрд╛рдПрдЧрд╛:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

рдкрд░рджреЗ рдХреЗ рдкреАрдЫреЗ, `label` рдкреНрд░рдХрд╛рд░ `ClassLabel` рдХрд╛ рд╣реИ, рдФрд░ рдкреВрд░реНрдгрд╛рдВрдХ рдХрд╛ рд▓реЗрдмрд▓ рдирд╛рдо рд╕реЗ рдорд╛рдирдЪрд┐рддреНрд░рдг *names* рдлрд╝реЛрд▓реНрдбрд░ рдореЗрдВ рд╕рдВрдЧреНрд░рд╣рд┐рдд рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред `0` рдореЗрд▓ рдЦрд╛рддрд╛ рд╣реИ `not_equivalent` рд╕реЗ, рдФрд░ `1` рдореЗрд▓ рдЦрд╛рддрд╛ рд╣реИ `equivalent` рд╕реЗред

<Tip>

тЬПя╕П **рдХреЛрд╢рд┐рд╢ рдХрд░рдХреЗ рджреЗрдЦреЗ!** рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╕реЗрдЯ рдХреЗ рддрддреНрд╡ 15 рдФрд░ рд╕рддреНрдпрд╛рдкрди рд╕реЗрдЯ рдХреЗ рддрддреНрд╡ 87 рдХреЛ рджреЗрдЦреЗрдВред рдЙрдирдХреЗ рд▓реЗрдмрд▓ рдХреНрдпрд╛ рд╣реИрдВ?

</Tip>

### рдбреЗрдЯрд╛рд╕реЗрдЯ рдХрд╛ рдкреВрд░реНрд╡рдкреНрд░рдХреНрд░рдордг рдХрд░рдирд╛

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдкреВрд░реНрд╡ рд╕рдВрд╕рд╛рдзрд┐рдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рд╣рдореЗрдВ рдЯреЗрдХреНрд╕реНрдЯ рдХреЛ рдЙрди рдирдВрдмрд░реЛрдВ рдореЗрдВ рдмрджрд▓рдиреЗ рдХреА рдЬрд░реВрд░рдд рд╣реИ, рдЬрд┐рдиреНрд╣реЗрдВ рдореЙрдбрд▓ рд╕рдордЭ рд╕рдХрддрд╛ рд╣реИред рдЬреИрд╕рд╛ рдХрд┐ рдЖрдкрдиреЗ [рдкрд┐рдЫрд▓реЗ рдЕрдзреНрдпрд╛рдп](/course/chapter2) рдореЗрдВ рджреЗрдЦрд╛, рдпрд╣ рдПрдХ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдХреЗ рд╕рд╛рде рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред рд╣рдо рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдореЗ рдПрдХ рд╡рд╛рдХреНрдп рдпрд╛ рд╡рд╛рдХреНрдпреЛрдВ рдХреА рдПрдХ рд╕реВрдЪреА рдбрд╛рд▓ рд╕рдХрддреЗ рд╣реИрдВ, рддрд╛рдХрд┐ рд╣рдо рд╕реАрдзреЗ рд╕рднреА рдкрд╣рд▓реЗ рд╡рд╛рдХреНрдпреЛрдВ рдФрд░ рд╕рднреА рджреВрд╕рд░реЗ рд╡рд╛рдХреНрдпреЛрдВ рдХреА рдкреНрд░рддреНрдпреЗрдХ рдЬреЛрдбрд╝реА рдХреЛ рдЯреЛрдХрдирдирд╛рдЗрдЬ рдХрд░ рд╕рдХреЗ рдЗрд╕ рддрд░рд╣ рд╕реЗ :

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

рд╣рд╛рд▓рд╛рдБрдХрд┐, рд╣рдо рдХреЗрд╡рд▓ рджреЛ рдЕрдиреБрдХреНрд░рдореЛрдВ рдХреЛ рдореЙрдбрд▓ рдореЗрдВ рдкрд╛рд░рд┐рдд рдирд╣реАрдВ рдХрд░ рд╕рдХрддреЗ рдФрд░ рдкреНрд░рд┐рдбрд┐рдХреНрд╢рди рдХрд░ рд╕рдХрддреЗ рдХрд┐ рджреЛ рд╡рд╛рдХреНрдп рдкреИрд░рд╛рдлреНрд░реЗрд╢ рд╣реИрдВ рдпрд╛ рдирд╣реАрдВред рд╣рдореЗрдВ рджреЛ рдЕрдиреБрдХреНрд░рдореЛрдВ рдХреЛ рдПрдХ рдЬреЛрдбрд╝реА рдХреЗ рд░реВрдк рдореЗрдВ рд╕рдВрднрд╛рд▓рдиреЗ рдХреА рдЬрд░реВрд░рдд рд╣реИ, рдФрд░ рдЙрдкрдпреБрдХреНрдд рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рд▓рд╛рдЧреВ рдХрд░рдирд╛ рд╣реИред рд╕реМрднрд╛рдЧреНрдп рд╕реЗ, рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдЕрдиреБрдХреНрд░рдореЛрдВ рдХреА рдЬреЛрдбрд╝реА рднреА рд▓реЗ рд╕рдХрддрд╛ рд╣реИ рдФрд░ рдЗрд╕реЗ рд╣рдорд╛рд░реЗ BERT рдореЙрдбрд▓ рдХреА рдЕрдкреЗрдХреНрд╖рд╛ рдХреЗ рдЕрдиреБрд╕рд╛рд░ рддреИрдпрд╛рд░ рдХрд░ рд╕рдХрддрд╛ рд╣реИ: 

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

рд╣рдордиреЗ [рдЕрдзреНрдпрд╛рдп 2](/course/chapter2) рдореЗрдВ `input_ids` рдФрд░ `attention_mask` рдХреБрдВрдЬрд┐рдпреЛрдВ рдкрд░ рдЪрд░реНрдЪрд╛ рдХреА, рд▓реЗрдХрд┐рди рд╣рдордиреЗ `token_type_ids` рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдмрд╛рдд рдирд╣реАрдВ рдХреАред рдЗрд╕ рдЙрджрд╛рд╣рд░рдг рдореЗрдВ, рдпрд╣ рдХреБрдВрдЬреА рдореЙрдбрд▓ рдХреЛ рдмрддрд╛рддрд╛ рд╣реИ рдХрд┐ рдЗрдирдкреБрдЯ рдХрд╛ рдХреМрди рд╕рд╛ рд╣рд┐рд╕реНрд╕рд╛ рдкрд╣рд▓рд╛ рд╡рд╛рдХреНрдп рд╣реИ рдФрд░ рдХреМрди рд╕рд╛ рджреВрд╕рд░рд╛ рд╡рд╛рдХреНрдп рд╣реИред

<Tip>

тЬПя╕П **рдХреЛрд╢рд┐рд╢ рдХрд░рдХреЗ рджреЗрдЦреЗ!** рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╕реЗрдЯ рдХреЗ рддрддреНрд╡ 15 рдХреЛ рд▓реЗрдВ рдФрд░ рдЯреЛрдХрдирдирд╛рдЗрдЬ рдХрд░реЗрдВ рджреЛ рд╡рд╛рдХреНрдпреЛрдВ рдХреЛ рдЕрд▓рдЧ-рдЕрд▓рдЧ рдФрд░ рдПрдХ рдЬреЛрдбрд╝реА рдХреЗ рд░реВрдк рдореЗрдВред рджреЛрдиреЛрдВ рдкрд░рд┐рдгрд╛рдореЛрдВ рдореЗрдВ рдХреНрдпрд╛ рдЕрдВрддрд░ рд╣реИ?

</Tip>

рдпрджрд┐ рд╣рдо `input_ids` рдХреЗ рдЕрдВрджрд░ IDs рдХреЛ рд╢рдмреНрджреЛрдВ рдореЗрдВ рд╡рд╛рдкрд╕ рд╡реНрдпрд╛рдЦреНрдпрд╛ рдХрд░рддреЗ рд╣реИрдВ:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

рд╣рдореЗрдВ рдорд┐рд▓реЗрдЧрд╛:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

рддреЛ рд╣рдо рджреЗрдЦ рд╕рдХрддреЗ рд╣реИрдВ рдХрд┐ рдореЙрдбрд▓ рдЕрдкреЗрдХреНрд╖рд╛ рдХрд░рддрд╛ рд╣реИ рдХрд┐ рдЗрдирдкреБрдЯ рдХрд╛ рдлреЙрд░реНрдо `[CLS] sentence1 [SEP] sentence2 [SEP]` рдХрд╛ рд╣реЛрдЧрд╛ рдЬрдм рджреЛ рд╡рд╛рдХреНрдп рд╣реЛрдВред рдЗрд╕реЗ `token_type_ids` рдХреЗ рд╕рд╛рде рд╕рдВрд░реЗрдЦрд┐рдд рдХрд░рдиреЗ рд╕реЗ рд╣рдореЗрдВ рдпрд╣ рдорд┐рд▓рддрд╛ рд╣реИ:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

рдЬреИрд╕рд╛ рдХрд┐ рдЖрдк рджреЗрдЦ рд╕рдХрддреЗ рд╣реИрдВ, рдЗрдирдкреБрдЯ рдХреЗ рдЬреЛ рд╣рд┐рд╕реНрд╕реЗ `[CLS] sentence1 [SEP]` рдХреЗ рдЕрдиреБрд░реВрдк рд╣реИ, рдЙрди рд╕рднреА рдХреЗ рдкрд╛рд╕ рдЯреЛрдХрди рдЯрд╛рдЗрдк рдЖрдИрдбреА рд╣реИ `0`, рдЬрдмрдХрд┐ рдЕрдиреНрдп рд╣рд┐рд╕реНрд╕реЗ, рдЬреЛ `sentence2 [SEP]` рдХреЗ рдЕрдиреБрд░реВрдк рд╣реИ, рд╕рднреА рдХреЗ рдкрд╛рд╕ рдПрдХ рдЯреЛрдХрди рдЯрд╛рдЗрдк рдЖрдИрдбреА рд╣реИ `1`ред

рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рдпрджрд┐ рдЖрдк рдПрдХ рдЕрд▓рдЧ рдЪреЗрдХрдкреЙрдЗрдВрдЯ рдХрд╛ рдЪрдпрди рдХрд░рддреЗ рд╣реИрдВ, рддреЛ рдЬрд░реВрд░реА рдирд╣реАрдВ рдХрд┐ рдЖрдкрдХреЗ рдЯреЛрдХрдирдирд╛рдЗрдЬ рдЗрдирдкреБрдЯ рдореЗрдВ `token_type_ids` рд╣реЛрдВ (рдЙрджрд╛рд╣рд░рдг рдХреЗ рд▓рд┐рдП, рдпрджрд┐ рдЖрдк DistilBERT рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреЗ рд╣реИрдВ рддреЛ рд╡реЗ рд╡рд╛рдкрд╕ рдирд╣реАрдВ рдЖрддреЗ рд╣реИрдВ)ред рдЙрдиреНрд╣реЗрдВ рдХреЗрд╡рд▓ рддрднреА рд▓реМрдЯрд╛рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ рдЬрдм рдореЙрдбрд▓ рдХреЛ рдкрддрд╛ рдЪрд▓ рдЬрд╛рдПрдЧрд╛ рдХрд┐ рдЙрдирдХреЗ рд╕рд╛рде рдХреНрдпрд╛ рдХрд░рдирд╛ рд╣реИ, рдХреНрдпреЛрдВрдХрд┐ рдЗрд╕рдиреЗ рдЙрдиреНрд╣реЗрдВ рдЕрдкрдиреЗ рдкреВрд░реНрд╡ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХреЗ рджреМрд░рд╛рди рджреЗрдЦрд╛ рд╣реИред 

рдпрд╣рд╛рдВ, BERT рдХреЛ рдЯреЛрдХрди рдЯрд╛рдЗрдк рдЖрдИрдбреА рдХреЗ рд╕рд╛рде рдкреВрд░реНрд╡ рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ, рдФрд░ рдирдХрд╛рдмрдкреЛрд╢ рднрд╛рд╖рд╛ рдореЙрдбрд▓рд┐рдВрдЧ рдХрд╛ рдЙрджреНрджреЗрд╢реНрдп рдЬрд┐рд╕рдХреА рд╣рдордиреЗ [рдЕрдзреНрдпрд╛рдп 1](/course/chapter1) рдореЗрдВ рдмрд╛рдд рдХреА рдереА рдХреЗ рд╢реАрд░реНрд╖ рдкрд░, рдЗрд╕рдХрд╛ рдПрдХ рдЕрддрд┐рд░рд┐рдХреНрдд рдЙрджреНрджреЗрд╢реНрдп рд╣реИ рдЬрд┐рд╕реЗ _рдЕрдЧрд▓реЗ рд╡рд╛рдХреНрдп рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди_ рдХрд╣рд╛ рдЬрд╛рддрд╛ рд╣реИред рдЗрд╕ рдХрд╛рд░реНрдп рдХрд╛ рд▓рдХреНрд╖реНрдп рд╡рд╛рдХреНрдпреЛрдВ рдХреЗ рдЬреЛрдбрд╝реЗ рдХреЗ рдмреАрдЪ рд╕рдВрдмрдВрдз рдХреЛ рдореЙрдбрд▓ рдХрд░рдирд╛ рд╣реИред

рдЕрдЧрд▓реЗ рд╡рд╛рдХреНрдп рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди рдХреЗ рд╕рд╛рде, рдореЙрдбрд▓ рдХреЛ рд╡рд╛рдХреНрдпреЛрдВ рдХреЗ рдЬреЛрдбрд╝реЗ (рдмреЗрддрд░рддреАрдм рдврдВрдЧ рд╕реЗ рдирдХрд╛рдмрдкреЛрд╢ рдЯреЛрдХрди рдХреЗ рд╕рд╛рде) рдкреНрд░рджрд╛рди рдХрд┐рдП рдЬрд╛рддреЗ рд╣реИрдВ рдФрд░ рдкреВрдЫрд╛ рдЬрд╛рддрд╛ рд╣реИ рдХрд┐ рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди рд▓рдЧрд╛рдУ рдХрд┐ рдХреНрдпрд╛ рджреВрд╕рд░рд╛ рд╡рд╛рдХреНрдп рдкрд╣рд▓реЗ рдХрд╛ рдЕрдиреБрд╕рд░рдг рдХрд░рддрд╛ рд╣реИред рдХрд╛рд░реНрдп рдХреЛ рдЧреИрд░-рддреБрдЪреНрдЫ рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП, рдЖрдзреЗ рд╕рдордп рдореЗрдВ рд╡рд╛рдХреНрдп рдПрдХ-рджреВрд╕рд░реЗ рдХрд╛ рдЕрдиреБрд╕рд░рдг рдХрд░рддреЗ рд╣реИрдВ рдореВрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рдореЗрдВ, рдФрд░ рджреВрд╕рд░реЗ рдЖрдзреЗ рд╕рдордп рдореЗрдВ рджреЛ рд╡рд╛рдХреНрдп рджреЛ рдЕрд▓рдЧ-рдЕрд▓рдЧ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝реЛрдВ рд╕реЗ рдЖрддреЗ рд╣реИрдВред 

рд╕рд╛рдорд╛рдиреНрдп рддреМрд░ рдкрд░, рдЖрдкрдХреЛ рдЗрд╕ рдмрд╛рд░реЗ рдореЗрдВ рдЪрд┐рдВрддрд╛ рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд╣реАрдВ рд╣реИ рдХрд┐ рдЖрдкрдХреЗ рдЯреЛрдХрдирдирд╛рдЗреЫрдб рдЗрдирдкреБрдЯ рдореЗрдВ `token_type_ids` рд╣реИрдВ рдпрд╛ рдирд╣реАрдВ: рдЬрдм рддрдХ рдЖрдк рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдФрд░ рдореЙрдбрд▓ рдХреЗ рд▓рд┐рдП рдПрдХ рд╣реА рдЪреЗрдХрдкреЙрдЗрдВрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреЗ рд╣реИрдВ, рддрдм рддрдХ рд╕рдм рдХреБрдЫ рдареАрдХ рд░рд╣реЗрдЧрд╛ рдХреНрдпреЛрдВрдХрд┐ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдЬрд╛рдирддрд╛ рд╣реИ рдХрд┐ рдЙрд╕рдХреЗ рдореЙрдбрд▓ рдХреЛ рдХреНрдпрд╛ рдкреНрд░рджрд╛рди рдХрд░рдирд╛ рд╣реИред

рдЕрдм рдЬрдм рд╣рдордиреЗ рджреЗрдЦрд╛ рдХрд┐ рдХреИрд╕реЗ рд╣рдорд╛рд░рд╛ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рд╡рд╛рдХреНрдпреЛрдВ рдХреА рдПрдХ рдЬреЛрдбрд╝реА рд╕реЗ рдирд┐рдкрдЯрддрд╛ рд╣реИ, рд╣рдо рдЗрд╕рдХрд╛ рдЙрдкрдпреЛрдЧ рдЕрдкрдиреЗ рдкреВрд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ: [рдкрд┐рдЫрд▓реЗ рдЕрдзреНрдпрд╛рдп](/course/chapter2) рдХреА рддрд░рд╣, рд╣рдо рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдХреЛ рдкрд╣рд▓реЗ рд╡рд╛рдХреНрдпреЛрдВ рдХреА рд╕реВрдЪреА, рдлрд┐рд░ рджреВрд╕рд░реЗ рд╡рд╛рдХреНрдпреЛрдВ рдХреА рд╕реВрдЪреА рджреЗрдХрд░ рд╡рд╛рдХреНрдпреЛрдВ рдХреЗ рдЬреЛрдбрд╝реЗ рдХреА рд╕реВрдЪреА рдЦрд┐рд▓рд╛ рд╕рдХрддреЗ рд╣реИред рдпрд╣ рдкреИрдбрд┐рдВрдЧ рдФрд░ рдЯреНрд░рдВрдХреЗрд╢рди рд╡рд┐рдХрд▓реНрдкреЛрдВ рдХреЗ рд╕рд╛рде рднреА  рд╕рдВрдЧрдд рд╣реИ рдЬрд┐рд╕реЗ рд╣рдордиреЗ [рдЕрдзреНрдпрд╛рдп 2](/course/chapter2) рдореЗрдВ рджреЗрдЦрд╛ рдерд╛ред рдЗрд╕рд▓рд┐рдП, рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдХрд░рдиреЗ рдХрд╛ рдПрдХ рддрд░реАрдХрд╛ рд╣реИ:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

рдпрд╣ рдЕрдЪреНрдЫреА рддрд░рд╣ рд╕реЗ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ, рд▓реЗрдХрд┐рди рдЗрд╕рдореЗрдВ рдПрдХ рд╢рдмреНрджрдХреЛрд╢ (рд╕рд╛рде рдореЗрдВ рд╣рдорд╛рд░реА рдХреБрдВрдЬреА, `input_ids`, `attention_mask`, рдФрд░ `token_type_ids`, рдФрд░ рдорд╛рди рдЬреЛ рд╕реВрдЪрд┐рдпреЛрдВ рдХреА рд╕реВрдЪрд┐рдпрд╛рдВ рд╣реИрдВ) рдХреЗ рд▓реМрдЯрдиреЗ рдХрд╛ рдиреБрдХрд╕рд╛рди рд╣реИред рдпрд╣ рдХреЗрд╡рд▓ рддрднреА рдХрд╛рдо рдХрд░реЗрдЧрд╛ рдЬрдм рдЖрдкрдХреЗ рдкрд╛рд╕ рдкрд░реНрдпрд╛рдкреНрдд RAM рд╣реЛ рдЕрдкрдиреЗ рдкреВрд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдЯреЛрдХрдирдирд╛рдЗрдЬреЗрд╢рди рдХреЗ рджреМрд░рд╛рди рд╕реНрдЯреЛрд░ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП (рдЬрдмрдХрд┐ ЁЯдЧ рдбреЗрдЯрд╛рд╕реЗрдЯ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдХреЗ рдбреЗрдЯрд╛рд╕реЗрдЯ [рдЕрдкрд╛рдЪреЗ рдПрд░реЛ](https://arrow.apache.org/) рдлрд╛рдЗрд▓реЗрдВ рд╣реИрдВ рдЬреЛ рдбрд┐рд╕реНрдХ рдкрд░ рд╕рдВрдЧреНрд░рд╣реАрдд рд╣реИ, рддреЛ рдЖрдк рдХреЗрд╡рд▓ рдЙрди рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рд░рдЦрддреЗ рд╣реИрдВ рдЬрд┐рдиреНрд╣реЗрдВ рдЖрдк рдореЗрдореЛрд░реА рдореЗ рд▓реЛрдб рдХрд░рдирд╛ рдЪрд╛рд╣рддреЗрдВ рд╣реИ)ред

рдбреЗрдЯрд╛ рдХреЛ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рд░реВрдк рдореЗрдВ рд░рдЦрдиреЗ рдХреЗ рд▓рд┐рдП, рд╣рдо [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) рдкрджреНрдзрддрд┐ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВрдЧреЗред рдЕрдЧрд░ рд╣рдореЗрдВ рд╕рд┐рд░реНрдл рдЯреЛрдХрдирдирд╛рдЗрдЬреЗрд╢рди рдХреА рддреБрд▓рдирд╛ рдореЗрдВ рдЕрдзрд┐рдХ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИ, рддреЛ рдпрд╣ рд╣рдореЗрдВ рдХреБрдЫ рдЕрдзрд┐рдХ рд▓рдЪреАрд▓реЗрдкрди рдХреА рднреА рдЕрдиреБрдорддрд┐ рджреЗрддрд╛ рд╣реИред  `map()` рд╡рд┐рдзрд┐ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рдкреНрд░рддреНрдпреЗрдХ рддрддреНрд╡ рдкрд░ рдПрдХ рдлрд╝рдВрдХреНрд╢рди рд▓рд╛рдЧреВ рдХрд░рдХреЗ рдХрд╛рдо рдХрд░рддреА рд╣реИ, рддреЛ рдЪрд▓рд┐рдП рдПрдХ рдлрд╝рдВрдХреНрд╢рди рдХреЛ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдХрд░рддреЗ рд╣реИрдВ рдЬреЛ рд╣рдорд╛рд░реЗ рдЗрдирдкреБрдЯ рдХреЛ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝ рдХрд░реЗрдЧрд╛ :

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

рдпрд╣ рдлрд╝рдВрдХреНрд╢рди рдПрдХ рд╢рдмреНрджрдХреЛрд╢ рд▓реЗрддрд╛ рд╣реИ (рдЬреИрд╕реЗ рд╣рдорд╛рд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рдЖрдЗрдЯрдо) рдФрд░ `input_ids`, `attention_mask`, рдФрд░ `token_type_ids` рдХреБрдВрдЬрд┐рдпреЛрдВ рдХреЗ рд╕рд╛рде рдПрдХ рдирдпрд╛ рд╢рдмреНрджрдХреЛрд╢ рджреЗрддрд╛ рд╣реИред рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рдпрд╣ рддрдм рднреА рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ рдЬрдм `example` рд╢рдмреНрджрдХреЛрд╢ рдореЗрдВ рдХрдИ рд╕реИрдореНрдкрд▓реНрд╕ рд╣реЛрдВ (рдкреНрд░рддреНрдпреЗрдХ рдХреБрдВрдЬреА рд╡рд╛рдХреНрдпреЛрдВ рдХреА рд╕реВрдЪреА рдХреЗ рд░реВрдк рдореЗрдВ) рдХреНрдпреЛрдВрдХрд┐ `рдЯреЛрдХрдирд╛рдЗрдЬрд╝рд░` рд╡рд╛рдХреНрдпреЛрдВ рдХреЗ рдЬреЛрдбрд╝реЗ рдХреА рд╕реВрдЪреА рдкрд░ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ, рдЬреИрд╕рд╛ рдХрд┐ рдкрд╣рд▓реЗ рджреЗрдЦрд╛ рдЧрдпрд╛ рдерд╛ред рдпрд╣ рд╣рдореЗрдВ рд╣рдорд╛рд░реЗ `map()` рдХреЗ рдХреЙрд▓ рдореЗрдВ `batched=True` рд╡рд┐рдХрд▓реНрдк рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдиреЗ рдХреА рдЕрдиреБрдорддрд┐ рджреЗрдЧрд╛, рдЬреЛ рдЯреЛрдХрдирд╛рдЗрдЬреЗрд╢рди рдХреЛ рдмрд╣реБрдд рддреЗрдЬ рдХрд░реЗрдЧрд╛ред `рдЯреЛрдХрдирд╛рдЗрдЬрд╝рд░` рдХреЛ [ЁЯдЧ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░рд╕](https://github.com/huggingface/tokenizers) рд▓рд╛рдЗрдмреНрд░реЗрд░реА рд╕реЗ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдЬреЛ Rust рдореЗрдВ рд▓рд┐рдЦрд╛ рд╣реИ рджреНрд╡рд╛рд░рд╛ рд╕рдорд░реНрдерд┐рдд рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред рдпрд╣ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рдмрд╣реБрдд рддреЗрдЬрд╝ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ, рд▓реЗрдХрд┐рди рдХреЗрд╡рд▓ рддрднреА рдЬрдм рд╣рдо рдЗрд╕реЗ рдПрдХ рд╕рд╛рде рдвреЗрд░ рд╕рд╛рд░реЗ рдЗрдирдкреБрдЯ рджреЗрдВред

рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рд╣рдордиреЗ рдЕрднреА рдХреЗ рд▓рд┐рдП рдЕрдкрдиреЗ рдЯреЛрдХрдирдирд╛рдЗрдЬреЗрд╢рди рдлрд╝рдВрдХреНрд╢рди рдореЗрдВ `рдкреИрдбрд┐рдВрдЧ` рдЖрд░реНрдЧреВрдореЗрдиреНрдЯ рдХреЛ рдЫреЛрдбрд╝ рджрд┐рдпрд╛ рд╣реИред рдРрд╕рд╛ рдЗрд╕рд▓рд┐рдП рд╣реИ рдХреНрдпреЛрдВрдХрд┐ рд╕рднреА рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рдЕрдзрд┐рдХрддрдо рд▓рдВрдмрд╛рдИ рддрдХ рдкреИрдбрд┐рдВрдЧ рдХрд░рдирд╛ рдХреБрд╢рд▓ рдирд╣реАрдВ рд╣реИ: рдЬрдм рд╣рдо рдмреИрдЪ рдмрдирд╛ рд░рд╣реЗ рд╣реЛрдВ рддреЛ рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рдкреИрдб рдХрд░рдирд╛ рдмреЗрд╣рддрд░ рд╣реЛрддрд╛ рд╣реИ, рдХреНрдпреЛрдВрдХрд┐ рддрдм рд╣рдореЗрдВ рдХреЗрд╡рд▓ рдЙрд╕ рдмреИрдЪ рдореЗрдВ рдЕрдзрд┐рдХрддрдо рд▓рдВрдмрд╛рдИ рддрдХ рдкреИрдб рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИ, рдФрд░ рди рдХрд┐ рдкреБрд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдореЗ рдЕрдзрд┐рдХрддрдо рд▓рдВрдмрд╛рдИ рддрдХред рдпрд╣ рдмрд╣реБрдд рд╕рдордп рдФрд░ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рд╢рдХреНрддрд┐ рдХреЛ рдмрдЪрд╛ рд╕рдХрддрд╛ рд╣реИ рдЬрдм рдЗрдирдкреБрдЯ рдореЗрдВ рдмрд╣реБрдд рдкрд░рд┐рд╡рд░реНрддрдирд╢реАрд▓ рд▓рдВрдмрд╛рдИ рд╣реЛрддреА рд╣реИ! 

рдпрд╣рд╛рдВ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рд╣реИ рдХрд┐ рд╣рдо рдЕрдкрдиреЗ рд╕рднреА рдбреЗрдЯрд╛рд╕реЗрдЯ рдкрд░ рдПрдХ рдмрд╛рд░ рдореЗрдВ рдЯреЛрдХрдирдирд╛рдЗрдЬреЗрд╢рди рдлрд╝рдВрдХреНрд╢рди рдХреИрд╕реЗ рд▓рд╛рдЧреВ рдХрд░рддреЗ рд╣реИрдВред рд╣рдо `batched=True` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░ рд░рд╣реЗ рд╣реИ `map` рдХреЛ рдХреЙрд▓ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рдЗрд╕рд▓рд┐рдП рдлрд╝рдВрдХреНрд╢рди рд╣рдорд╛рд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рдХрдИ рддрддреНрд╡реЛрдВ рдкрд░ рдПрдХ рд╕рд╛рде рд▓рд╛рдЧреВ рд╣реЛрддрд╛ рд╣реИ, рди рдХрд┐ рдкреНрд░рддреНрдпреЗрдХ рддрддреНрд╡ рдкрд░ рдЕрд▓рдЧ рд╕реЗред рдпрд╣ рддреЗрдЬреА рд╕реЗ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдХреА рдЕрдиреБрдорддрд┐ рджреЗрддрд╛ рд╣реИред

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ЁЯдЧ рдбреЗрдЯрд╛рд╕реЗрдЯ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдЗрд╕ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдХреЛ рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП , рдбреЗрдЯрд╛рд╕реЗрдЯ рдореЗрдВ рдирдП рдлрд╝реАрд▓реНрдб рдЬреЛрдбрд╝рддреЗ рд╣реИ, рдЬреЛ рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдлрд╝рдВрдХреНрд╢рди рджреНрд╡рд╛рд░рд╛ рд▓реМрдЯрд╛рдП рдЧрдП рд╢рдмреНрджрдХреЛрд╢ рдореЗрдВ рдкреНрд░рддреНрдпреЗрдХ рдХреБрдВрдЬреА рдХреЗ рд▓рд┐рдП рдПрдХ рд╣реЛрддрд╛ рд╣реИ:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

рдЖрдк рдмрд╣реБрдкреНрд░рдХреНрд░рдордг рдХрд╛ рднреА рдЙрдкрдпреЛрдЧ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдмрд╕ рдЕрдкрдиреЗ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдлрд╝рдВрдХреНрд╢рди рдХреЛ `map()` рдХреЗ рд╕рд╛рде рд▓рд╛рдЧреВ рдХрд░рддреЗ рд╕рдордп `num_proc` рддрд░реНрдХ рдХреЛ рдкрд╛рд╕ рдХрд░рдирд╛ рд╣реИред  рд╣рдордиреЗ рдпрд╣рд╛рдВ рдРрд╕рд╛ рдирд╣реАрдВ рдХрд┐рдпрд╛ рдХреНрдпреЛрдВрдХрд┐ ЁЯдЧ рдЯреЛрдХрдирд╛рдЗрдЬрд╝рд░рд╕ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рдкрд╣рд▓реЗ рд╕реЗ рд╣реА рд╣рдорд╛рд░реЗ рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рддреЗрдЬрд╝реА рд╕реЗ рдЯреЛрдХрдирд╛рдЗрдЬрд╝ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдХрдИ рдереНрд░реЗрдбреНрд╕ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреА рд╣реИ, рд▓реЗрдХрд┐рди рдпрджрд┐ рдЖрдк рдЗрд╕ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рджреНрд╡рд╛рд░рд╛ рд╕рдорд░реНрдерд┐рдд рддреЗрдЬрд╝ рдЯреЛрдХрдирд╛рдЗрдЬрд╝рд░ рдХрд╛ рдЙрдкрдпреЛрдЧ рдирд╣реАрдВ рдХрд░ рд░рд╣реЗ рд╣реИрдВ, рддреЛ рдпрд╣ рдЖрдкрдХреЗ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдХреЛ рдЧрддрд┐ рджреЗ рд╕рдХрддрд╛ рд╣реИред

рд╣рдорд╛рд░рд╛ `tokenize_function` рдПрдХ рд╢рдмреНрджрдХреЛрд╢ `input_ids`, `attention_mask`, рдФрд░ `token_type_ids` рдХреБрдВрдЬрд┐рдпреЛрдВ рдХреЗ рд╕рд╛рде рджреЗрддрд╛ рд╣реИ, рдЗрд╕рд▓рд┐рдП рдЙрди рддреАрдиреЛ рдХреНрд╖реЗрддреНрд░реЛрдВ рдХреЛ рд╣рдорд╛рд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рд╕рднреА рд╡рд┐рднрд╛рдЬрдиреЛрдВ рдореЗрдВ рдЬреЛрдбрд╝ рджрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИред рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рд╣рдо рдореМрдЬреВрджрд╛ рдлрд╝реАрд▓реНрдбрд╕ рдХреЛ рднреА рдмрджрд▓ рд╕рдХрддреЗ рдереЗ рдпрджрд┐ рд╣рдорд╛рд░реЗ рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдлрд╝рдВрдХреНрд╢рди рдиреЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдореЗрдВ рдореМрдЬреВрджрд╛ рдХреБрдВрдЬреА рдХреЗ рд▓рд┐рдП рдПрдХ рдирдпрд╛ рдорд╛рди рд▓реМрдЯрд╛рдпрд╛ рд╣реЛрддрд╛, рдЬрд┐рд╕ рдкрд░ рд╣рдордиреЗ `map()` рд▓рд╛рдЧреВ рдХрд┐рдпрд╛ред

рдЖрдЦрд┐рд░реА рдЪреАрдЬ рдЬреЛ рд╣рдореЗрдВ рдХрд░рдиреЗ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрдЧреА рд╡рд╣ рд╣реИ рд╕рднреА рдЙрджрд╛рд╣рд░рдгреЛрдВ рдХреЛ рд╕рдмрд╕реЗ рд▓рдВрдмреЗ рддрддреНрд╡ рдХреА рд▓рдВрдмрд╛рдИ рддрдХ рдкреИрдб рдХрд░рдирд╛ рдЬрдм рд╣рдо рддрддреНрд╡реЛрдВ рдХреЛ рдПрдХ рд╕рд╛рде рдмреИрдЪ рдХрд░рддреЗ рд╣реИрдВ тАФ рдпрд╣ рдПрдХ рддрдХрдиреАрдХ рд╣реИ рдЬрд┐рд╕реЗ рд╣рдо *рдбрд╛рдпрдирд╛рдорд┐рдХ рдкреИрдбрд┐рдВрдЧ* рдХреЗ рд░реВрдк рдореЗрдВ рд╕рдВрджрд░реНрднрд┐рдд рдХрд░рддреЗ рд╣реИрдВред

### рдбрд╛рдпрдирд╛рдорд┐рдХ рдкреИрдбрд┐рдВрдЧ

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
рдЬреЛ рдлрд╝рдВрдХреНрд╢рди рдмреИрдЪ рдХреЗ рдЕрдВрджрд░ рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рдПрдХ рд╕рд╛рде рд░рдЦрдиреЗ рдХреЗ рд▓рд┐рдП рдЬрд┐рдореНрдореЗрджрд╛рд░ рд╣реЛ рдЙрд╕реЗ *collate function* рдХрд╣рд╛ рдЬрд╛рддрд╛ рд╣реИред рдпрд╣ рдПрдХ рдЖрд░реНрдЧреВрдореЗрдиреНрдЯ рд╣реИ рдЬрд┐рд╕реЗ рдЖрдк рдПрдХ `DataLoader` рдмрдирд╛рддреЗ рд╕рдордп рдкрд╛рд░рд┐рдд рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рд╡рд░рдирд╛ рдПрдХ рдРрд╕рд╛ рдлрд╝рдВрдХреНрд╢рди рд╣реИ рдЬреЛ рдЖрдкрдХреЗ рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рдХреЗрд╡рд▓ PyTorch рдЯреЗрдВрд╕рд░ рдореЗрдВ рдмрджрд▓ рджреЗрдЧрд╛ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдЬреЛрдбрд╝ рджреЗрдЧрд╛ (рдкреБрдирд░рд╛рд╡рд░реНрддреА рдпрджрд┐ рдЖрдкрдХреЗ рддрддреНрд╡ рд╕реВрдЪрд┐рдпрд╛рдВ, рдЯреБрдкрд▓реНрд╕ рдпрд╛ рд╢рдмреНрджрдХреЛрд╢ рд╣реИрдВ)ред рд╣рдорд╛рд░реЗ рдорд╛рдорд▓реЗ рдореЗрдВ рдпрд╣ рд╕рдВрднрд╡ рдирд╣реАрдВ рд╣реЛрдЧрд╛ рдХреНрдпреЛрдВрдХрд┐ рд╣рдорд╛рд░реЗ рдкрд╛рд╕ рдЬреЛ рдЗрдирдкреБрдЯ рд╣реИрдВ рд╡реЗ рд╕рднреА рдПрдХ рд╣реА рдЖрдХрд╛рд░ рдХреЗ рдирд╣реАрдВ рд╣реЛрдВрдЧреЗред рд╣рдордиреЗ рдЬрд╛рдирдмреВрдЭрдХрд░ рдкреИрдбрд┐рдВрдЧ рдХреЛ рд╕реНрдердЧрд┐рдд рдХрд░ рджрд┐рдпрд╛ рд╣реИ, рдХреЗрд╡рд▓ рдЗрд╕реЗ рдкреНрд░рддреНрдпреЗрдХ рдмреИрдЪ рдкрд░ рдЖрд╡рд╢реНрдпрдХ рд░реВрдк рд╕реЗ рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдФрд░ рдмрд╣реБрдд рдЕрдзрд┐рдХ рдкреИрдбрд┐рдВрдЧ рдХреЗ рд╕рд╛рде рдЕрдзрд┐рдХ рд▓рдВрдмреЗ рдЗрдирдкреБрдЯ рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдПред рдпрд╣ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХреЛ рдХрд╛рдлреА рддреЗрдЬ рдХрд░ рджреЗрдЧрд╛, рд▓реЗрдХрд┐рди рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рдпрджрд┐ рдЖрдк TPU рдкрд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХрд░ рд░рд╣реЗ рд╣реИрдВ рддреЛ рдпрд╣ рд╕рдорд╕реНрдпрд╛ рдкреИрджрд╛ рдХрд░ рд╕рдХрддрд╛ рд╣реИ тАФ TPUs рдирд┐рд╢реНрдЪрд┐рдд рдЖрдХрд╛рд░ рдкрд╕рдВрдж рдХрд░рддреЗ рд╣реИрдВ, рддрдм рднреА рдЬрдм рдЗрд╕рдХреЗ рд▓рд┐рдП рдЕрддрд┐рд░рд┐рдХреНрдд рдкреИрдбрд┐рдВрдЧ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИред

{:else}

рдЬреЛ рдлрд╝рдВрдХреНрд╢рди рдмреИрдЪ рдХреЗ рдЕрдВрджрд░ рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рдПрдХ рд╕рд╛рде рд░рдЦрдиреЗ рдХреЗ рд▓рд┐рдП рдЬрд┐рдореНрдореЗрджрд╛рд░ рд╣реЛ рдЙрд╕реЗ *collate function* рдХрд╣рд╛ рдЬрд╛рддрд╛ рд╣реИред рдбрд┐рдлрд╝реЙрд▓реНрдЯ рдХреЛрд▓реЗрдЯрд░ рдПрдХ рдРрд╕рд╛ рдлрд╝рдВрдХреНрд╢рди рд╣реИ рдЬреЛ рдЖрдкрдХреЗ рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ tf.Tensor рдореЗрдВ рдмрджрд▓ рджреЗрдЧрд╛ рдФрд░ рдЙрдиреНрд╣реЗрдВ рдЬреЛрдбрд╝ рджреЗрдЧрд╛ (рдкреБрдирд░рд╛рд╡рд░реНрддреА рдпрджрд┐ рдЖрдкрдХреЗ рддрддреНрд╡ рд╕реВрдЪрд┐рдпрд╛рдВ, рдЯреБрдкрд▓реНрд╕ рдпрд╛ рд╢рдмреНрджрдХреЛрд╢ рд╣реИрдВ)ред рд╣рдорд╛рд░реЗ рдорд╛рдорд▓реЗ рдореЗрдВ рдпрд╣ рд╕рдВрднрд╡ рдирд╣реАрдВ рд╣реЛрдЧрд╛ рдХреНрдпреЛрдВрдХрд┐ рд╣рдорд╛рд░реЗ рдкрд╛рд╕ рдЬреЛ рдЗрдирдкреБрдЯ рд╣реИрдВ рд╡реЗ рд╕рднреА рдПрдХ рд╣реА рдЖрдХрд╛рд░ рдХреЗ рдирд╣реАрдВ рд╣реЛрдВрдЧреЗред рд╣рдордиреЗ рдЬрд╛рдирдмреВрдЭрдХрд░ рдкреИрдбрд┐рдВрдЧ рдХреЛ рд╕реНрдердЧрд┐рдд рдХрд░ рджрд┐рдпрд╛ рд╣реИ, рдХреЗрд╡рд▓ рдЗрд╕реЗ рдкреНрд░рддреНрдпреЗрдХ рдмреИрдЪ рдкрд░ рдЖрд╡рд╢реНрдпрдХ рд░реВрдк рд╕реЗ рд▓рд╛рдЧреВ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдФрд░ рдмрд╣реБрдд рдЕрдзрд┐рдХ рдкреИрдбрд┐рдВрдЧ рдХреЗ рд╕рд╛рде рдЕрдзрд┐рдХ рд▓рдВрдмреЗ рдЗрдирдкреБрдЯ рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдПред рдпрд╣ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХреЛ рдХрд╛рдлреА рддреЗрдЬ рдХрд░ рджреЗрдЧрд╛, рд▓реЗрдХрд┐рди рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рдпрджрд┐ рдЖрдк TPU рдкрд░ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХрд░ рд░рд╣реЗ рд╣реИрдВ рддреЛ рдпрд╣ рд╕рдорд╕реНрдпрд╛ рдкреИрджрд╛ рдХрд░ рд╕рдХрддрд╛ рд╣реИ тАФ TPUs рдирд┐рд╢реНрдЪрд┐рдд рдЖрдХрд╛рд░ рдкрд╕рдВрдж рдХрд░рддреЗ рд╣реИрдВ, рддрдм рднреА рдЬрдм рдЗрд╕рдХреЗ рд▓рд┐рдП рдЕрддрд┐рд░рд┐рдХреНрдд рдкреИрдбрд┐рдВрдЧ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИред

{/if}

рд╡реНрдпрд╡рд╣рд╛рд░ рдореЗрдВ рдРрд╕рд╛ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рд╣рдореЗрдВ рдПрдХ рдХреЛрд▓реЗрдЯ рдлрд╝рдВрдХреНрд╢рди рдХреЛ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдХрд░рдирд╛ рд╣реЛрдЧрд╛ рдЬреЛ рдЙрд╕ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рдЖрдЗрдЯрдо рдкрд░ рд╕рд╣реА рдорд╛рддреНрд░рд╛ рдореЗрдВ рдкреИрдбрд┐рдВрдЧ рд▓рд╛рдЧреВ рдХрд░реЗрдЧрд╛ рдЬрд┐рд╕реЗ рд╣рдо рдПрдХ рд╕рд╛рде рдмреИрдЪ рдмрдирд╛рдирд╛ рд╣реИрдВред рд╕реМрднрд╛рдЧреНрдп рд╕реЗ, ЁЯдЧ рдЯреНрд░рд╛рдиреНрд╕рдлрд╝реЙрд░реНрдорд░реНрд╕ рд▓рд╛рдЗрдмреНрд░реЗрд░реА рд╣рдореЗрдВ `DataCollatorWithPadding` рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдРрд╕рд╛ рдлрд╝рдВрдХреНрд╢рди рдкреНрд░рджрд╛рди рдХрд░рддреА рд╣реИред рдЬрдм рдЖрдк рдЗрд╕реЗ рдЗрдиреНрд╕реНрдЯреИрдиреНрд╢реАрдРрдЯ рдХрд░рддреЗ рд╣реИрдВ рддреЛ рдпрд╣ рдПрдХ рдЯреЛрдХрдирдирд╛рдЗрдЬрд╝рд░ рд▓реЗрддрд╛ рд╣реИ (рдпрд╣ рдЬрд╛рдирдиреЗ рдХреЗ рд▓рд┐рдП рдХрд┐ рдХрд┐рд╕ рдкреИрдбрд┐рдВрдЧ рдЯреЛрдХрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдирд╛ рд╣реИ, рдФрд░ рдХреНрдпрд╛ рдореЙрдбрд▓ рдХреЛ рдкреИрдбрд┐рдВрдЧ рдХреЗ рдмрд╛рдИрдВ рдУрд░ рдпрд╛ рдЗрдирдкреБрдЯ рдХреЗ рджрд╛рдИрдВ рдУрд░ рдЪрд╛рд╣рд┐рдП) рдФрд░ рд╡рд╣ рд╕рдм рдХреБрдЫ рдХрд░реЗрдЧрд╛ рдЬреЛ рдЖрдкрдХреЛ рдЪрд╛рд╣рд┐рдП:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

рдЗрд╕ рдирдП рдЦрд┐рд▓реМрдиреЗ рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рдЖрдЗрдП рд╣рдорд╛рд░реЗ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╕реЗрдЯ рд╕реЗ рдХреБрдЫ рд╕реИрдореНрдкрд▓реНрд╕ рд▓реЗрдВ рдЬрд┐рдиреНрд╣реЗрдВ рд╣рдо рдПрдХ рд╕рд╛рде рдмреИрдЪ рдмрдирд╛рдирд╛ рдЪрд╛рд╣реЗрдВрдЧреЗред рдпрд╣рд╛рдВ, рд╣рдо рдХреЙрд▓рдо `idx`, `sentence1`, рдФрд░ `sentence2` рдХреЛ рд╣рдЯрд╛ рджреЗрддреЗ рд╣реИрдВ рдХреНрдпреЛрдВрдХрд┐ рдЙрдирдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд╣реАрдВ рд╣реЛрдЧреА рдФрд░ рдЗрд╕рдореЗрдВ рд╕реНрдЯреНрд░рд┐рдВрдЧреНрд╕ рд╣реЛрдВрдЧреЗ (рдФрд░ рд╣рдо рд╕реНрдЯреНрд░рд┐рдВрдЧреНрд╕ рдХреЗ рд╕рд╛рде рдЯреЗрдВрд╕рд░ рдирд╣реАрдВ рдмрдирд╛ рд╕рдХрддреЗ) рдФрд░ рдЖрдЗрдпреЗ рдмреИрдЪ рдореЗрдВ рдкреНрд░рддреНрдпреЗрдХ рдкреНрд░рд╡рд┐рд╖реНрдЯрд┐ рдХреА рд▓рдВрдмрд╛рдИ рдкрд░ рдПрдХ рдирдЬрд╝рд░ рдбрд╛рд▓реЗ:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

рдХреЛрдИ рдЖрд╢реНрдЪрд░реНрдп рдирд╣реАрдВ, рд╣рдореЗрдВ 32 рд╕реЗ 67 рддрдХ рдХреА рдЕрд▓рдЧ-рдЕрд▓рдЧ рд▓рдВрдмрд╛рдИ рдХреЗ рд╕реИрдореНрдкрд▓реНрд╕ рдорд┐рд▓рддреЗ рд╣реИрдВред рдбрд╛рдпрдиреЗрдорд┐рдХ рдкреИрдбрд┐рдВрдЧ рдХрд╛ рдорддрд▓рдм рд╣реИ рдХрд┐ рдЗрд╕ рдмреИрдЪ рдХреЗ рд╕рднреА рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ 67 рдХреА рд▓рдВрдмрд╛рдИ рддрдХ рдкреИрдб рдХрд┐рдпрд╛ рдЬрд╛рдирд╛ рдЪрд╛рд╣рд┐рдП, рдЬреЛ рдХреА рд╕рдмрд╕реЗ рдЕрдзрд┐рдХрддрдо рд▓рдВрдмрд╛рдИ рд╣реИ рдмреИрдЪ рдХреЗ рдЕрдВрджрд░ рдХреАред рдбрд╛рдпрдиреЗрдорд┐рдХ рдкреИрдбрд┐рдВрдЧ рдХреЗ рдмрд┐рдирд╛, рд╕рднреА рд╕реИрдореНрдкрд▓реНрд╕ рдХреЛ рдкреВрд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдореЗрдВ рдЕрдзрд┐рдХрддрдо рд▓рдВрдмрд╛рдИ рддрдХ рдпрд╛ рдореЙрдбрд▓ рджреНрд╡рд╛рд░рд╛ рд╕реНрд╡реАрдХрд╛рд░ рдХреА рдЬрд╛ рд╕рдХрдиреЗ рд╡рд╛рд▓реА рдЕрдзрд┐рдХрддрдо рд▓рдВрдмрд╛рдИ рддрдХ рдкреИрдб рдХрд░рдирд╛ рд╣реЛрдЧрд╛ред рдЖрдЗрдП рджреЛрдмрд╛рд░рд╛ рдЬрд╛рдВрдЪреЗрдВ рдХрд┐ рд╣рдорд╛рд░рд╛  `data_collator` рдмреИрдЪ рдХреЛ рдбрд╛рдпрдиреЗрдорд┐рдХрд▓реА рдкреИрдбрд┐рдВрдЧ рдХрд░ рд░рд╣рд╛ рд╣реИ:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

рджреЗрдЦрдиреЗ рдореЗрдВ рд╕рд╣реА рд╣реИ! рдЕрдм рдЬрдмрдХрд┐ рд╣рдо рджреЗрдЦ рдЪреБрдХреЗ рд╣реИ рдХреА рд╣рдорд╛рд░рд╛ рдореЙрдбрд▓ рдХрдЪреНрдЪреЗ рдЯреЗрдХреНрд╕реНрдЯ рд╕реЗ рдмреИрдЪрд╕ рддрдХ рдирд┐рдкрдЯ рд╕рдХрддрд╛ рд╣реИ, рддреЛ рдЕрдм рд╣рдо рдЗрд╕реЗ рдлрд╝рд╛рдЗрди рдЯреНрдпреВрди рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рддреИрдпрд╛рд░ рд╣реИрдВ!

{/if}

<Tip>

тЬПя╕П **рдХреЛрд╢рд┐рд╢ рдХрд░рдХреЗ рджреЗрдЦреЗ!** рдХреЛрд╢рд┐рд╢ рдХрд░рдХреЗ рджреЗрдЦреЗ! GLUE SST-2 рдбреЗрдЯрд╛рд╕реЗрдЯ рдкрд░ рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдХреЛ рджреЛрд╣рд░рд╛рдПрдВред рдпрд╣ рдереЛрдбрд╝рд╛ рдЕрд▓рдЧ рд╣реИ рдХреНрдпреЛрдВрдХрд┐ рдпрд╣ рдЬреЛрдбрд╝реЗ рдХреЗ рдмрдЬрд╛рдп рдПрдХрд▓ рд╡рд╛рдХреНрдпреЛрдВ рд╕реЗ рдмрдирд╛ рд╣реИ, рд▓реЗрдХрд┐рди рдмрд╛рдХреА рдЬреЛ рд╣рдордиреЗ рдХрд┐рдпрд╛ рд╡реЛ рд╡реИрд╕рд╛ рд╣реА рджрд┐рдЦрдирд╛ рдЪрд╛рд╣рд┐рдПред рдПрдХ рдХрдард┐рди рдЪреБрдиреМрддреА рдХреЗ рд▓рд┐рдП, рдПрдХ рдкреНрд░реАрдкреНрд░реЛрд╕реЗрд╕рд┐рдВрдЧ рдлрд╝рдВрдХреНрд╢рди рд▓рд┐рдЦрдиреЗ рдХрд╛ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВ рдЬреЛ рдХрд┐рд╕реА рднреА GLUE рдХрд╛рд░реНрдпреЛрдВ рдкрд░ рдХрд╛рдо рдХрд░рддрд╛ рд╣реЛред

</Tip>

{#if fw === 'tf'}

рдЕрдм рдЬрдм рд╣рдорд╛рд░реЗ рдкрд╛рд╕ рд╣рдорд╛рд░реЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдФрд░ рдбреЗрдЯрд╛ рдХреЛрд▓реЗрдЯрд░ рд╣реИрдВ, рддреЛ рд╣рдореЗрдВ рдЙрдиреНрд╣реЗрдВ рдПрдХ рд╕рд╛рде рд░рдЦрдирд╛ рд╣реИред рд╣рдо рдореИрдиреНрдпреБрдЕрд▓ рд░реВрдк рд╕реЗ рдмреИрдЪрд╕ рдХреЛ рд▓реЛрдб рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ рдФрд░ рдЙрдирдХрд╛ рдорд┐рд▓рд╛рди рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рд▓реЗрдХрд┐рди рдпрд╣ рдмрд╣реБрдд рдХрд╛рдо рд╣реИ, рдФрд░ рд╢рд╛рдпрдж рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рдкреНрд░рджрд░реНрд╢рди рдХрд░рдиреЗ рд╡рд╛рд▓рд╛ рднреА рдирд╣реАрдВ рд╣реИред рдЗрд╕рдХреЗ рдмрдЬрд╛рдп, рдПрдХ рд╕рд░рд▓ рд╡рд┐рдзрд┐ рд╣реИ рдЬреЛ рдЗрд╕ рд╕рдорд╕реНрдпрд╛ рдХрд╛ рдПрдХ рдирд┐рд╖реНрдкрд╛рджрдХ рд╕рдорд╛рдзрд╛рди рдкреНрд░рджрд╛рди рдХрд░рддреА рд╣реИ: `to_tf_dataset()`ред рдпрд╣ рдПрдХ рд╡реИрдХрд▓реНрдкрд┐рдХ рдХреЛрд▓реЗрд╢рди рдлрд╝рдВрдХреНрд╢рди рдХреЗ рд╕рд╛рде, рдЖрдкрдХреЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рдЪрд╛рд░реЛрдВ рдУрд░ рдПрдХ `tf.data.Dataset` рд▓рдкреЗрдЯ рджреЗрдЧрд╛ред `tf.data.Dataset` рдПрдХ рджреЗрд╢реА TensorFlow рдкреНрд░рд╛рд░реВрдк рд╣реИ рдЬрд┐рд╕реЗ Keras рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ `model.fit()` рдХреЗ рд▓рд┐рдП, рдЗрд╕рд▓рд┐рдП рдпрд╣ рдПрдХ рд╡рд┐рдзрд┐ рддреБрд░рдВрдд ЁЯдЧ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдПрдХ рдкреНрд░рд╛рд░реВрдк рдореЗрдВ рдкрд░рд┐рд╡рд░реНрддрд┐рдд рдХрд░ рджреЗрддреА рд╣реИ рдЬреЛ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХреЗ рд▓рд┐рдП рддреИрдпрд╛рд░ рд╣реИред рдЖрдЗрдП рдЗрд╕реЗ рдЕрдкрдиреЗ рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рд╕рд╛рде рдХреНрд░рд┐рдпрд╛ рдореЗрдВ рджреЗрдЦреЗрдВ!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

рдФрд░ рдмрд╕! рдбреЗрдЯрд╛ рдкреВрд░реНрд╡ рдкреНрд░рд╕рдВрд╕реНрдХрд░рдг рдХреА рдХрдбрд╝реА рдореЗрд╣рдирдд рдХреЗ рдмрд╛рдж рд╣рдо рдЙрди рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЛ рдЕрдЧрд▓реЗ рд╡реНрдпрд╛рдЦреНрдпрд╛рди рдореЗрдВ рдЖрдЧреЗ рд▓реЗ рдЬрд╛ рд╕рдХрддреЗ рд╣реИрдВ, рдЬрд╣рд╛рдВ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рд╕реБрдЦрдж рд░реВрдк рд╕реЗ рд╕реАрдзрд╛ рд╣реЛрдЧрд╛ред

{/if}
