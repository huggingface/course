<FrameworkSwitchCourse {fw} />

# CÃ¡c mÃ´ hÃ¬nh

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section3_pt.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section3_pt.ipynb",
    },
  ]}
/>

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section3_tf.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section3_tf.ipynb",
    },
  ]}
/>

{/if}

{#if fw === 'pt'}

<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
Trong pháº§n nÃ y, chÃºng ta sáº½ xem xÃ©t ká»¹ hÆ¡n vá» viá»‡c táº¡o vÃ  sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh. ChÃºng tÃ´i sáº½ sá»­ dá»¥ng `AutoModel`, ráº¥t tiá»‡n lá»£i khi báº¡n muá»‘n khá»Ÿi táº¡o báº¥t ká»³ mÃ´ hÃ¬nh nÃ o tá»« má»™t checkpoint.

`AutoModel` vÃ  táº¥t cáº£ cÃ¡c lá»›p há» hÃ ng cá»§a nÃ³ thá»±c ra lÃ  cÃ¡c hÃ m Ä‘Ã³ng gÃ³i Ä‘Æ¡n giáº£n trÃªn nhiá»u loáº¡i mÃ´ hÃ¬nh cÃ³ sáºµn trong thÆ° viá»‡n. ÄÃ³ lÃ  má»™t hÃ m Ä‘Ã³ng gÃ³i thÃ´ng minh vÃ¬ nÃ³ cÃ³ thá»ƒ tá»± Ä‘á»™ng Ä‘oÃ¡n kiáº¿n trÃºc mÃ´ hÃ¬nh thÃ­ch há»£p cho checkpoint cá»§a báº¡n vÃ  sau Ä‘Ã³ khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh vá»›i kiáº¿n trÃºc nÃ y.

{:else}
Trong pháº§n nÃ y, chÃºng ta sáº½ xem xÃ©t ká»¹ hÆ¡n vá» viá»‡c táº¡o vÃ  sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh. ChÃºng tÃ´i sáº½ sá»­ dá»¥ng `TFAutoModel`, ráº¥t tiá»‡n lá»£i khi báº¡n muá»‘n khá»Ÿi táº¡o báº¥t ká»³ mÃ´ hÃ¬nh nÃ o tá»« má»™t checkpoint.

`TFAutoModel` vÃ  táº¥t cáº£ cÃ¡c lá»›p há» hÃ ng cá»§a nÃ³ thá»±c ra lÃ  cÃ¡c hÃ m Ä‘Ã³ng gÃ³i Ä‘Æ¡n giáº£n trÃªn nhiá»u loáº¡i mÃ´ hÃ¬nh cÃ³ sáºµn trong thÆ° viá»‡n. ÄÃ³ lÃ  má»™t hÃ m Ä‘Ã³ng gÃ³i thÃ´ng minh vÃ¬ nÃ³ cÃ³ thá»ƒ tá»± Ä‘á»™ng Ä‘oÃ¡n kiáº¿n trÃºc mÃ´ hÃ¬nh thÃ­ch há»£p cho checkpoint cá»§a báº¡n vÃ  sau Ä‘Ã³ khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh vá»›i kiáº¿n trÃºc nÃ y.

{/if}

Tuy nhiÃªn, náº¿u báº¡n biáº¿t loáº¡i mÃ´ hÃ¬nh báº¡n muá»‘n sá»­ dá»¥ng, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng trá»±c tiáº¿p lá»›p Ä‘á»‹nh nghÄ©a kiáº¿n trÃºc cá»§a nÃ³. ChÃºng ta hÃ£y xem cÃ¡ch nÃ y hoáº¡t Ä‘á»™ng vá»›i mÃ´ hÃ¬nh BERT.

## Táº¡o ra má»™t Transformer

Äiá»u Ä‘áº§u tiÃªn ta cáº§n lÃ m Ä‘á»ƒ khá»Ÿi táº¡o mÃ´ hÃ¬nh BERT lÃ  táº£i má»™t cáº¥u hÃ¬nh:

{#if fw === 'pt'}

```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```

{:else}

```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```

{/if}

Cáº¥u hÃ¬nh chá»©a nhiá»u thuá»™c tÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

Máº·c dÃ¹ báº¡n chÆ°a tháº¥y táº¥t cáº£ cÃ¡c thuá»™c tÃ­nh nÃ y cÃ³ tÃ¡c dá»¥ng gÃ¬, nhÆ°ng báº¡n nÃªn nháº­n ra má»™t sá»‘ trong sá»‘ chÃºng: thuá»™c tÃ­nh `hidden_size` xÃ¡c Ä‘á»‹nh kÃ­ch thÆ°á»›c cá»§a vectÆ¡ `hidden_states` vÃ  `num_hidden_layers` xÃ¡c Ä‘á»‹nh sá»‘ lá»›p mÃ  mÃ´ hÃ¬nh Transformer cÃ³.

### CÃ¡c phÆ°Æ¡ng phÃ¡p táº£i khÃ¡c nhau

Viá»‡c táº¡o mÃ´ hÃ¬nh tá»« cáº¥u hÃ¬nh máº·c Ä‘á»‹nh sáº½ khá»Ÿi táº¡o mÃ´ hÃ¬nh Ä‘Ã³ vá»›i cÃ¡c giÃ¡ trá»‹ ngáº«u nhiÃªn:

{#if fw === 'pt'}

```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```

{:else}

```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```

{/if}
MÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ tráº¡ng thÃ¡i nÃ y, nhÆ°ng nÃ³ sáº½ tráº£ ra vÃ´ nghÄ©a; nÃ³ cáº§n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. ChÃºng ta cÃ³ thá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« Ä‘áº§u tÃ¡c vá»¥ nÃ y, nhÆ°ng nhÆ° báº¡n Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 1](/course/chapter1), Ä‘iá»u nÃ y sáº½ Ä‘Ã²i há»i má»™t thá»i gian dÃ i vÃ  nhiá»u dá»¯ liá»‡u, vÃ  nÃ³ sáº½ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ tá»›i mÃ´i trÆ°á»ng. Äá»ƒ trÃ¡nh ná»— lá»±c khÃ´ng cáº§n thiáº¿t vÃ  trÃ¹ng láº·p, kháº£ nÄƒng chia sáº» vÃ  sá»­ dá»¥ng láº¡i cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trá»Ÿ nÃªn báº¯t buá»™c.

Viá»‡c táº£i má»™t mÃ´ hÃ¬nh Transformer Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c ráº¥t Ä‘Æ¡n giáº£n - chÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n viá»‡c nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `from_pretrained()`:

{#if fw === 'pt'}

```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

NhÆ° báº¡n Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ thay tháº¿ `BertModel` báº±ng `AutoModel` tÆ°Æ¡ng Ä‘Æ°Æ¡ng. ChÃºng ta sáº½ lÃ m Ä‘iá»u nÃ y tá»« bÃ¢y giá» vÃ¬ Ä‘iá»u nÃ y táº¡o ra cÃ¡c Ä‘oáº¡n mÃ£ checkpoint báº¥t kháº£ tri; náº¿u mÃ£ cá»§a báº¡n hoáº¡t Ä‘á»™ng cho má»™t checkpoint, nÃ³ sáº½ hoáº¡t Ä‘á»™ng liá»n máº¡ch vá»›i má»™t checkpoint khÃ¡c. Äiá»u nÃ y Ã¡p dá»¥ng ngay cáº£ khi kiáº¿n trÃºc khÃ¡c nhau, miá»…n lÃ  checkpoint Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n cho má»™t tÃ¡c vá»¥ tÆ°Æ¡ng tá»± (vÃ­ dá»¥: má»™t tÃ¡c vá»¥ phÃ¢n tÃ­ch cáº£m xÃºc).

{:else}

```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

NhÆ° báº¡n Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ thay tháº¿ `BertModel` báº±ng `TFAutoModel` tÆ°Æ¡ng Ä‘Æ°Æ¡ng. ChÃºng ta sáº½ lÃ m Ä‘iá»u nÃ y tá»« bÃ¢y giá» vÃ¬ Ä‘iá»u nÃ y táº¡o ra cÃ¡c Ä‘oáº¡n mÃ£ checkpoint báº¥t kháº£ tri; náº¿u mÃ£ cá»§a báº¡n hoáº¡t Ä‘á»™ng cho má»™t checkpoint, nÃ³ sáº½ hoáº¡t Ä‘á»™ng liá»n máº¡ch vá»›i má»™t checkpoint khÃ¡c. Äiá»u nÃ y Ã¡p dá»¥ng ngay cáº£ khi kiáº¿n trÃºc khÃ¡c nhau, miá»…n lÃ  checkpoint Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n cho má»™t tÃ¡c vá»¥ tÆ°Æ¡ng tá»± (vÃ­ dá»¥: má»™t tÃ¡c vá»¥ phÃ¢n tÃ­ch cáº£m xÃºc).

{/if}

Trong Ä‘oáº¡n mÃ£ á»Ÿ trÃªn, ta khÃ´ng sá»­ dá»¥ng `BertConfig` vÃ  thay vÃ o Ä‘Ã³, táº£i má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c thÃ´ng qua mÃ£ Ä‘á»‹nh danh `bert-base-cased`. ÄÃ¢y lÃ  má»™t checkpoint mÃ´ hÃ¬nh do chÃ­nh cÃ¡c tÃ¡c giáº£ cá»§a BERT huáº¥n luyá»‡n; báº¡n cÃ³ thá»ƒ tÃ¬m thÃªm thÃ´ng tin chi tiáº¿t vá» nÃ³ trong [tháº» mÃ´ hÃ¬nh](https://huggingface.co/bert-base-cased).

MÃ´ hÃ¬nh nÃ y hiá»‡n Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i táº¥t cáº£ cÃ¡c trá»ng sá»‘ cá»§a checkpoint. NÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trá»±c tiáº¿p Ä‘á»ƒ luáº­n suy vá» cÃ¡c tÃ¡c vá»¥ mÃ  nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n, vÃ  nÃ³ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh trÃªn má»™t tÃ¡c vá»¥ má»›i. Báº±ng cÃ¡ch huáº¥n luyá»‡n vá»›i trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c chá»© khÃ´ng pháº£i tá»« Ä‘áº§u, chÃºng ta cÃ³ thá»ƒ nhanh chÃ³ng Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t.

CÃ¡c trá»ng sá»‘ Ä‘Ã£ Ä‘Æ°á»£c táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› cache (vÃ¬ váº­y cÃ¡c lá»‡nh gá»i tá»›i phÆ°Æ¡ng thá»©c `from_pretrained()` trong tÆ°Æ¡ng lai sáº½ khÃ´ng táº£i xuá»‘ng láº¡i chÃºng) trong thÆ° má»¥c bá»™ nhá»› cache, máº·c Ä‘á»‹nh lÃ  _~/.cache/huggingface/transformers_. Báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh thÆ° má»¥c bá»™ nhá»› cache cá»§a mÃ¬nh báº±ng cÃ¡ch Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng `HF_HOME`.

Sá»‘ Ä‘á»‹nh danh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº£i mÃ´ hÃ¬nh cÃ³ thá»ƒ lÃ  sá»‘ Ä‘á»‹nh danh cá»§a báº¥t ká»³ mÃ´ hÃ¬nh nÃ o trÃªn Model Hub, miá»…n lÃ  nÃ³ tÆ°Æ¡ng thÃ­ch vá»›i kiáº¿n â€‹â€‹trÃºc BERT. ToÃ n bá»™ danh sÃ¡ch cÃ¡c checkpoint BERT hiá»‡n cÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y [táº¡i Ä‘Ã¢y](https://huggingface.co/models?filter=bert).

### PhÆ°Æ¡ng phÃ¡p lÆ°u trá»¯ checkpoint

LÆ°u má»™t mÃ´ hÃ¬nh cÅ©ng dá»… dÃ ng nhÆ° táº£i má»™t mÃ´ hÃ¬nh - chÃºng ta sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `save_pretrained()`, tÆ°Æ¡ng tá»± vá»›i phÆ°Æ¡ng thá»©c `from_pretrained()`:

```py
model.save_pretrained("directory_on_my_computer")
```

Thao tÃ¡c nÃ y sáº½ lÆ°u hai tá»‡p vÃ o Ä‘Ä©a cá»§a báº¡n:

{#if fw === 'pt'}

```
ls directory_on_my_computer

config.json pytorch_model.bin
```

{:else}

```
ls directory_on_my_computer

config.json tf_model.h5
```

{/if}

Náº¿u báº¡n xem tá»‡p _config.json_, báº¡n sáº½ nháº­n ra cÃ¡c thuá»™c tÃ­nh cáº§n thiáº¿t Ä‘á»ƒ xÃ¢y dá»±ng kiáº¿n trÃºc mÃ´ hÃ¬nh. Tá»‡p nÃ y cÅ©ng chá»©a má»™t sá»‘ siÃªu dá»¯ liá»‡u, cháº³ng háº¡n nhÆ° Ä‘iá»ƒm báº¯t nguá»“n cá»§a checkpoint vÃ  phiÃªn báº£n ğŸ¤— Transformers báº¡n Ä‘ang sá»­ dá»¥ng khi báº¡n lÆ°u checkpoint láº§n cuá»‘i.

{#if fw === 'pt'}

Tá»‡p _pytorch_model.bin_ Ä‘Æ°á»£c gá»i lÃ  _state dictionary_ (_tá»« Ä‘iá»ƒn tráº¡ng thÃ¡i_); nÃ³ chá»©a táº¥t cáº£ cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh cá»§a báº¡n. Hai táº­p tin Ä‘i Ä‘Ã´i vá»›i nhau; cáº¥u hÃ¬nh lÃ  cáº§n thiáº¿t Ä‘á»ƒ biáº¿t kiáº¿n trÃºc mÃ´ hÃ¬nh cá»§a báº¡n, trong khi trá»ng sá»‘ mÃ´ hÃ¬nh lÃ  thÃ´ng sá»‘ cá»§a mÃ´ hÃ¬nh cá»§a báº¡n.

{:else}

Tá»‡p _tf_model.h5_ Ä‘Æ°á»£c gá»i lÃ  _state dictionary_ (_tá»« Ä‘iá»ƒn tráº¡ng thÃ¡i_); nÃ³ chá»©a táº¥t cáº£ cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh cá»§a báº¡n. Hai táº­p tin Ä‘i Ä‘Ã´i vá»›i nhau; cáº¥u hÃ¬nh lÃ  cáº§n thiáº¿t Ä‘á»ƒ biáº¿t kiáº¿n trÃºc mÃ´ hÃ¬nh cá»§a báº¡n, trong khi trá»ng sá»‘ mÃ´ hÃ¬nh lÃ  thÃ´ng sá»‘ cá»§a mÃ´ hÃ¬nh cá»§a báº¡n.

{/if}

## Sá»­ dá»¥ng mÃ´ hÃ¬nh Transformer Ä‘á»ƒ luáº­n suy

Giá» báº¡n Ä‘Ã£ biáº¿t cÃ¡ch táº£i vÃ  lÆ°u má»™t mÃ´ hÃ¬nh, hÃ£y thá»­ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ Ä‘Æ°a ra má»™t sá»‘ dá»± Ä‘oÃ¡n. CÃ¡c mÃ´ hÃ¬nh Transfomer chá»‰ cÃ³ thá»ƒ xá»­ lÃ½ sá»‘ - cÃ¡c sá»‘ mÃ  tokenizer táº¡o ra. NhÆ°ng trÆ°á»›c khi chÃºng ta tháº£o luáº­n vá» tokenizer, chÃºng ta hÃ£y khÃ¡m phÃ¡ nhá»¯ng yáº¿u tá»‘ Ä‘áº§u vÃ o mÃ  mÃ´ hÃ¬nh cháº¥p nháº­n.

Tokenizer cÃ³ thá»ƒ Ä‘áº£m nháº­n viá»‡c truyá»n cÃ¡c Ä‘áº§u vÃ o Ä‘áº¿n cÃ¡c tensor cá»§a khung thÃ­ch há»£p, nhÆ°ng Ä‘á»ƒ giÃºp báº¡n hiá»ƒu nhá»¯ng gÃ¬ Ä‘ang xáº£y ra, chÃºng ta sáº½ xem xÃ©t nhanh nhá»¯ng gÃ¬ pháº£i thá»±c hiá»‡n trÆ°á»›c khi gá»­i Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh.

Giáº£ sá»­ chÃºng ta cÃ³ má»™t vÃ i chuá»—i nhÆ° sau:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

Tokenizer chuyá»ƒn Ä‘á»•i cÃ¡c chá»‰ sá»‘ nÃ y thÃ nh cÃ¡c chá»‰ má»¥c tá»« vá»±ng thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  _ID Ä‘áº§u vÃ o_. Má»—i chuá»—i giá» lÃ  má»™t danh sÃ¡ch cÃ¡c sá»‘! Káº¿t quáº£ Ä‘áº§u ra lÃ :

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

ÄÃ¢y lÃ  danh sÃ¡ch cÃ¡c chuá»—i Ä‘Æ°á»£c mÃ£ hÃ³a: danh sÃ¡ch cÃ¡c danh sÃ¡ch. Tensor chá»‰ cháº¥p nháº­n dáº¡ng hÃ¬nh chá»¯ nháº­t (hÃ£y nghÄ© tá»›i ma tráº­n). "Máº£ng" nÃ y Ä‘Ã£ cÃ³ dáº¡ng hÃ¬nh chá»¯ nháº­t, vÃ¬ váº­y viá»‡c chuyá»ƒn Ä‘á»•i nÃ³ thÃ nh má»™t tensor ráº¥t dá»… dÃ ng:

{#if fw === 'pt'}

```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```

{:else}

```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```

{/if}

### Sá»­ dá»¥ng tensor lÃ m Ä‘áº§u vÃ o mÃ´ hÃ¬nh

Viá»‡c sá»­ dá»¥ng cÃ¡c tensors vá»›i mÃ´ hÃ¬nh cá»±c ká»³ Ä‘Æ¡n giáº£n - chÃºng ta chá»‰ cáº§n gá»i mÃ´ hÃ¬nh vá»›i cÃ¡c Ä‘áº§u vÃ o:

```py
output = model(model_inputs)
```

Máº·c dÃ¹ mÃ´ hÃ¬nh cháº¥p nháº­n ráº¥t nhiá»u tham sá»‘ khÃ¡c nhau, nhÆ°ng chá»‰ cÃ¡c ID Ä‘áº§u vÃ o lÃ  cáº§n thiáº¿t. ChÃºng tÃ´i sáº½ giáº£i thÃ­ch nhá»¯ng gÃ¬ cÃ¡c tham sá»‘ khÃ¡c lÃ m vÃ  khi nÃ o chÃºng Ä‘Æ°á»£c yÃªu cáº§u sau, nhÆ°ng trÆ°á»›c tiÃªn, chÃºng ta cáº§n xem xÃ©t ká»¹ hÆ¡n cÃ¡c bá»™ tokenizer táº¡o ra cÃ¡c Ä‘áº§u vÃ o mÃ  má»™t mÃ´ hÃ¬nh Transformer cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.
