<FrameworkSwitchCourse {fw} />

# Káº¿t há»£p láº¡i

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section6_tf.ipynb"},
]} />

{/if}

Trong vÃ i pháº§n trÆ°á»›c, chÃºng ta Ä‘Ã£ cá»‘ gáº¯ng háº¿t sá»©c Ä‘á»ƒ lÃ m háº§u háº¿t cÃ¡c tÃ¡c vá»¥ báº±ng tay. ChÃºng ta Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a cÃ¡c cÃ´ng cá»¥ tokenize vÃ  xem xÃ©t quÃ¡ trÃ¬nh tokenize, chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u sang ID Ä‘áº§u vÃ o, Ä‘á»‡m, cáº¯t bá»›t vÃ  cÃ¡c lá»›p che attention.

Tuy nhiÃªn, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong pháº§n 2, API ğŸ¤— Transformers cÃ³ thá»ƒ xá»­ lÃ½ táº¥t cáº£ nhá»¯ng Ä‘iá»u nÃ y cho chÃºng ta báº±ng má»™t chá»©c nÄƒng cáº¥p cao mÃ  chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o Ä‘Ã¢y. Khi báº¡n gá»i trá»±c tiáº¿p `tokenizer` trÃªn cÃ¢u, báº¡n sáº½ nháº­n láº¡i Ä‘Æ°á»£c cÃ¡c thÃ´ng tin Ä‘áº§u vÃ o sáºµn sÃ ng chuyá»ƒn qua mÃ´ hÃ¬nh cá»§a báº¡n:

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

á» Ä‘Ã¢y, biáº¿n `model_inputs` chá»©a má»i thá»© cáº§n thiáº¿t Ä‘á»ƒ má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t. Äá»‘i vá»›i DistilBERT, Ä‘iá»u Ä‘Ã³ bao gá»“m cÃ¡c ID Ä‘áº§u vÃ o cÅ©ng nhÆ° lá»›p che attention. CÃ¡c mÃ´ hÃ¬nh khÃ¡c cháº¥p nháº­n Ä‘áº§u vÃ o bá»• sung cÅ©ng sáº½ cÃ³ Ä‘áº§u ra Ä‘Ã³ tá»« Ä‘á»‘i tÆ°á»£ng `tokenizer`.

NhÆ° chÃºng ta sáº½ tháº¥y trong má»™t sá»‘ vÃ­ dá»¥ bÃªn dÆ°á»›i, phÆ°Æ¡ng phÃ¡p nÃ y ráº¥t máº¡nh máº½. Äáº§u tiÃªn, nÃ³ cÃ³ thá»ƒ mÃ£ hÃ³a má»™t chuá»—i duy nháº¥t:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

NÃ³ cÅ©ng xá»­ lÃ½ nhiá»u chuá»—i cÃ¹ng má»™t lÃºc mÃ  khÃ´ng cáº§n thay Ä‘á»•i trong API:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

NÃ³ cÃ³ thá»ƒ Ä‘á»‡m thÃªm tuá»³ theo má»™t sá»‘ má»¥c tiÃªu nhÆ° sau:

```py
# Sáº½ Ä‘á»‡m thÃªm vÃ o chuá»—i sao cho Ä‘á»™ dÃ i báº±ng Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a chuá»—i
model_inputs = tokenizer(sequences, padding="longest")

# Sáº½ Ä‘á»‡m thÃªm vÃ o chuá»—i sao cho Ä‘á»™ dÃ i báº±ng Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh
# (512 cho BERT hoáº·c DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Sáº½ Ä‘á»‡m thÃªm vÃ o chuá»—i sao cho Ä‘á»™ dÃ i báº±ng Ä‘á»™ dÃ i tá»‘i Ä‘a Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

NÃ³ cÅ©ng cÃ³ thá»ƒ cáº¯t bá»›t cÃ¡c chuá»—i:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Sáº½ cáº¯t bá»›t chuá»—i cho báº±ng Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh
# (512 cho BERT hoáº·c DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Sáº½ cáº¯t bá»›t chuá»—i cÃ³ Ä‘á»™ dÃ i dÃ i hÆ¡n Ä‘á»™ dÃ i tá»‘i Ä‘a Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

Äá»‘i tÆ°á»£ng `tokenizer` cÃ³ thá»ƒ xá»­ lÃ½ viá»‡c chuyá»ƒn Ä‘á»•i sang cÃ¡c tensor cá»¥ thá»ƒ, sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c gá»­i trá»±c tiáº¿p Ä‘áº¿n mÃ´ hÃ¬nh. VÃ­ dá»¥: trong Ä‘oáº¡n mÃ£ sau, chÃºng tÃ´i Ä‘ang nháº¯c tokenizer tráº£ vá» tensors tá»« cÃ¡c khung khÃ¡c nhau - `"pt"` tráº£ vá» tensors PyTorch, `"tf"` tráº£ vá» tensors TensorFlow vÃ  `"np"` tráº£ vá» máº£ng NumPy:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Tráº£ vá» tensor PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Tráº£ vá» tensor TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Tráº£ vá» máº£ng NumPy
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## CÃ¡c token Ä‘áº·c biá»‡t

Náº¿u chÃºng ta xem xÃ©t cÃ¡c ID Ä‘áº§u vÃ o Ä‘Æ°á»£c tráº£ vá» bá»Ÿi tokenizer, chÃºng ta sáº½ tháº¥y chÃºng hÆ¡i khÃ¡c má»™t chÃºt so vá»›i nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ cÃ³ trÆ°á»›c Ä‘Ã³:

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

Má»™t token ID Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o vá»‹ trÃ­ Ä‘áº§u vÃ  cuá»‘i. HÃ£y giáº£i mÃ£ hai chuá»—i ID á»Ÿ trÃªn Ä‘á»ƒ xem nÃ³ lÃ  gÃ¬:


```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

Tokenizer Ä‘Ã£ thÃªm tá»« Ä‘áº·c biá»‡t `[CLS]` vÃ o Ä‘áº§u vÃ  tá»« Ä‘áº·c biá»‡t `[SEP]` á»Ÿ cuá»‘i. Äiá»u nÃ y lÃ  do mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i chÃºng, vÃ¬ váº­y Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± Ä‘á»ƒ luáº­n suy, chÃºng ta cÅ©ng cáº§n thÃªm chÃºng vÃ o. LÆ°u Ã½ ráº±ng má»™t sá»‘ mÃ´ hÃ¬nh khÃ´ng thÃªm cÃ¡c tá»« Ä‘áº·c biá»‡t hoáº·c thÃªm cÃ¡c tá»« khÃ¡c; mÃ´ hÃ¬nh cÅ©ng cÃ³ thá»ƒ chá»‰ thÃªm nhá»¯ng tá»« Ä‘áº·c biá»‡t nÃ y vÃ o Ä‘áº§u hoáº·c chá»‰ á»Ÿ cuá»‘i. Trong má»i trÆ°á»ng há»£p, tokenizer biáº¿t cÃ¡i nÃ o Ä‘Æ°á»£c mong Ä‘á»£i vÃ  sáº½ giáº£i quyáº¿t viá»‡c nÃ y cho báº¡n.

## Tá»•ng káº¿t: Tá»« tokenizer Ä‘áº¿n mÃ´ hÃ¬nh

Giá» chÃºng ta Ä‘Ã£ tháº¥y táº¥t cáº£ cÃ¡c bÆ°á»›c riÃªng láº» mÃ  `tokenizer` sá»­ dá»¥ng khi Ã¡p dá»¥ng lÃªn vÄƒn báº£n, chÃºng ta hÃ£y xem láº§n cuá»‘i cÃ¡ch nÃ³ cÃ³ thá»ƒ xá»­ lÃ½ nhiá»u chuá»—i (Ä‘á»‡m thÃªm!), chuá»—i ráº¥t dÃ i (cáº¯t ngáº¯n!) VÃ  nhiá»u kiá»ƒu tensor vá»›i API chÃ­nh cá»§a nÃ³:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
