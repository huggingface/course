<FrameworkSwitchCourse {fw} />

# Xá»­ lÃ½ Ä‘a chuá»—i

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

Trong pháº§n trÆ°á»›c, chÃºng ta Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng Ä‘Æ¡n giáº£n nháº¥t: thá»±c hiá»‡n luáº­n suy trÃªn má»™t dÃ£y Ä‘Æ¡n cÃ³ Ä‘á»™ dÃ i nhá». Tuy nhiÃªn, má»™t sá»‘ cÃ¢u há»i Ä‘Æ°á»£c Ä‘á» cáº­p nhÆ°:

- LÃ m tháº¿ nÃ o Ä‘á»ƒ chÃºng ta xá»­ lÃ½ nhiá»u chuá»—i?
- LÃ m tháº¿ nÃ o Ä‘á»ƒ chÃºng ta xá»­ lÃ½ nhiá»u chuá»—i *cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau*?
- CÃ¡c chá»‰ sá»‘ tá»« vá»±ng cÃ³ pháº£i lÃ  Ä‘áº§u vÃ o duy nháº¥t cho phÃ©p má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t khÃ´ng?
- Náº¿u nhÆ° má»™t chuá»—i quÃ¡ dÃ i thÃ¬ sao?

HÃ£y xem nhá»¯ng cÃ¢u há»i nÃ y Ä‘áº·t ra nhá»¯ng loáº¡i váº¥n Ä‘á» nÃ o vÃ  cÃ¡ch chÃºng tÃ´i cÃ³ thá»ƒ giáº£i quyáº¿t chÃºng báº±ng cÃ¡ch sá»­ dá»¥ng API ğŸ¤— Transformers.

## MÃ´ hÃ¬nh kÃ¬ vá»ng má»™t lÃ´ cÃ¡c Ä‘áº§u vÃ o

Trong bÃ i táº­p trÆ°á»›c, báº¡n Ä‘Ã£ tháº¥y cÃ¡ch cÃ¡c chuá»—i Ä‘Æ°á»£c chuyá»ƒn thÃ nh danh sÃ¡ch cÃ¡c sá»‘. HÃ£y chuyá»ƒn Ä‘á»•i danh sÃ¡ch cÃ¡c sá»‘ nÃ y thÃ nh má»™t tensor vÃ  gá»­i nÃ³ Ä‘áº¿n mÃ´ hÃ¬nh:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

Ã”i khÃ´ng! Táº¡i sao Ä‘oáº¡n mÃ£ láº¡i khÃ´ng thÃ nh cÃ´ng? ChÃºng ta Ä‘Ã£ lÃ m theo cÃ¡c bÆ°á»›c tá»« pipeline trong pháº§n 2.

Váº¥n Ä‘á» á»Ÿ Ä‘Ã¢y Ä‘Ã³ lÃ  chÃºng ta Ä‘Ã£ gá»­i má»™t chuá»—i Ä‘Æ¡n cho mÃ´ hÃ¬nh, trong khi mÃ´ hÃ¬nh ğŸ¤— Transformers mong Ä‘á»£i nhiá»u cÃ¢u theo máº·c Ä‘á»‹nh. á» Ä‘Ã¢y, chÃºng ta Ä‘Ã£ cá»‘ gáº¯ng thá»±c hiá»‡n má»i thá»© mÃ  tokenizer Ä‘Ã£ lÃ m á»Ÿ phÃ­a sau khi Ã¡p dá»¥ng nÃ³ vÃ o má»™t `chuá»—i`, nhÆ°ng náº¿u báº¡n nhÃ¬n ká»¹, báº¡n sáº½ tháº¥y ráº±ng nÃ³ khÃ´ng chá»‰ chuyá»ƒn Ä‘á»•i danh sÃ¡ch ID Ä‘áº§u vÃ o thÃ nh má»™t tensor, nÃ³  cÃ²n thÃªm má»™t chiá»u lÃªn trÃªn:

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

HÃ£y cÅ©ng thá»­ láº¡i vÃ  thÃªm má»™t chiá»u má»›i:

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

Ta in ra cÃ¡c ID Ä‘áº§u vÃ o cÅ©ng nhÆ° káº¿t quáº£ logit nhÆ° sau:

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

*Batching* hay *LÃ´* lÃ  hÃ nh Ä‘á»™ng gá»­i nhiá»u cÃ¢u qua mÃ´ hÃ¬nh, táº¥t cáº£ cÃ¹ng má»™t lÃºc. Náº¿u báº¡n chá»‰ cÃ³ má»™t cÃ¢u, báº¡n chá»‰ cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t lÃ´ vá»›i má»™t chuá»—i duy nháº¥t:

```
batched_ids = [ids, ids]
```

ÄÃ¢y lÃ  má»™t lÃ´ chá»©a hai chuá»—i giá»‘ng nhau!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Chuyá»ƒn Ä‘á»•i danh sÃ¡ch `batch_ids` nÃ y thÃ nh má»™t tensor vÃ  chuyá»ƒn nÃ³ qua mÃ´ hÃ¬nh cá»§a báº¡n. Kiá»ƒm tra Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng báº¡n cÃ³ Ä‘Æ°á»£c logit giá»‘ng nhÆ° trÆ°á»›c Ä‘Ã¢y (nhÆ°ng hai láº§n)!

</Tip>

Viá»‡c phÃ¢n phá»‘i lÃ´ cho phÃ©p mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng khi báº¡n Ä‘Æ°a vÃ o nhiá»u cÃ¢u. Viá»‡c sá»­ dá»¥ng nhiá»u chuá»—i cÅ©ng Ä‘Æ¡n giáº£n nhÆ° xÃ¢y dá»±ng má»™t lÃ´ vá»›i má»™t chuá»—i duy nháº¥t. Tuy nhiÃªn, cÃ³ má»™t váº¥n Ä‘á» thá»© hai. Khi báº¡n cá»‘ gáº¯ng ghÃ©p hai (hoáº·c nhiá»u) cÃ¢u láº¡i vá»›i nhau, chÃºng cÃ³ thá»ƒ cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau. Náº¿u báº¡n Ä‘Ã£ tá»«ng lÃ m viá»‡c vá»›i tensor trÆ°á»›c Ä‘Ã¢y, báº¡n biáº¿t ráº±ng chÃºng cáº§n cÃ³ dáº¡ng hÃ¬nh chá»¯ nháº­t, vÃ¬ váº­y báº¡n sáº½ khÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i trá»±c tiáº¿p danh sÃ¡ch ID Ä‘áº§u vÃ o thÃ nh tensor. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i thÆ°á»ng *Ä‘á»‡m* cÃ¡c Ä‘áº§u vÃ o.

## ÄÃªm thÃªm vÃ o Ä‘áº§u vÃ o

Danh sÃ¡ch cÃ¡c danh sÃ¡ch dÆ°á»›i Ä‘Ã¢y khÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i thÃ nh má»™t tensor:

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng ta sáº½ sá»­ dá»¥ng *Ä‘á»‡m* Ä‘á»ƒ lÃ m cho cÃ¡c tensor cá»§a chÃºng ta cÃ³ hÃ¬nh chá»¯ nháº­t. Äá»‡m Ä‘áº£m báº£o táº¥t cáº£ cÃ¡c cÃ¢u cá»§a chÃºng ta cÃ³ cÃ¹ng Ä‘á»™ dÃ i báº±ng cÃ¡ch thÃªm má»™t tá»« Ä‘áº·c biá»‡t Ä‘Æ°á»£c gá»i lÃ  *padding token* hay *token Ä‘Æ°á»£c Ä‘á»‡m thÃªm* vÃ o cÃ¡c cÃ¢u cÃ³ Ã­t giÃ¡ trá»‹ hÆ¡n. VÃ­ dá»¥: náº¿u báº¡n cÃ³ 10 cÃ¢u 10 tá»« vÃ  1 cÃ¢u 20 tá»«, pháº§n Ä‘á»‡m sáº½ Ä‘áº£m báº£o táº¥t cáº£ cÃ¡c cÃ¢u cÃ³ 20 tá»«. Trong vÃ­ dá»¥ cá»§a chÃºng tÃ´i, tensor káº¿t quáº£ trÃ´ng giá»‘ng nhÆ° sau:

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

ID cá»§a token Ä‘á»‡m cÃ³ thá»ƒ tÃ¬m tháº¥y á»Ÿ `tokenizer.pad_token_id`. HÃ£y sá»­ dá»¥ng nÃ³ vÃ  gá»­i hai cÃ¢u cá»§a chÃºng ta thÃ´ng qua mÃ´ hÃ¬nh riÃªng láº» vÃ  theo lÃ´ vá»›i nhau:

{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

CÃ³ Ä‘iá»u gÃ¬ Ä‘Ã³ khÃ´ng á»•n vá»›i cÃ¡c logit trong cÃ¡c dá»± Ä‘oÃ¡n theo lÃ´ cá»§a chÃºng ta: hÃ ng thá»© hai pháº£i giá»‘ng vá»›i logit cho cÃ¢u thá»© hai, nhÆ°ng chÃºng ta cÃ³ cÃ¡c giÃ¡ trá»‹ hoÃ n toÃ n khÃ¡c nhau!

Äiá»u nÃ y lÃ  do tÃ­nh nÄƒng chÃ­nh cá»§a cÃ¡c mÃ´ hÃ¬nh Transformer lÃ  cÃ¡c lá»›p attention Ä‘Ã£ *ngá»¯ cáº£nh hÃ³a* má»—i token. ChÃºng sáº½ tÃ­nh Ä‘áº¿n cÃ¡c padding token vÃ¬ chÃºng tham gia vÃ o táº¥t cáº£ cÃ¡c token cá»§a má»™t chuá»—i. Äá»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± khi chuyá»ƒn cÃ¡c cÃ¢u riÃªng láº» cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau qua mÃ´ hÃ¬nh hoáº·c khi chuyá»ƒn má»™t lÃ´ vá»›i cÃ¡c cÃ¢u vÃ  pháº§n Ä‘á»‡m giá»‘ng nhau Ä‘Æ°á»£c Ã¡p dá»¥ng, chÃºng ta cáº§n yÃªu cáº§u cÃ¡c lá»›p attention Ä‘Ã³ bá» qua cÃ¡c tháº» Ä‘á»‡m. Äiá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng attention mask.

## Attention masks

*Attention masks* lÃ  cÃ¡c tensor cÃ³ hÃ¬nh dáº¡ng chÃ­nh xÃ¡c nhÆ° tensor ID Ä‘áº§u vÃ o, Ä‘Æ°á»£c láº¥p Ä‘áº§y bá»Ÿi 0 vÃ  1: 1 cho biáº¿t cÃ¡c tokenn tÆ°Æ¡ng á»©ng nÃªn Ä‘Æ°á»£c tham gia vÃ  cÃ¡c sá»‘ 0 cho biáº¿t cÃ¡c token tÆ°Æ¡ng á»©ng khÃ´ng Ä‘Æ°á»£c tham gia (tá»©c lÃ  chÃºng pháº£i bá»‹ bá» qua bá»Ÿi cÃ¡c lá»›p attention cá»§a mÃ´ hÃ¬nh).

HÃ£y hoÃ n thÃ nh vÃ­ dá»¥ trÆ°á»›c vá»›i má»™t attention mask:

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

BÃ¢y giá» chÃºng ta nháº­n Ä‘Æ°á»£c cÃ¡c logit tÆ°Æ¡ng tá»± cho cÃ¢u thá»© hai trong lÃ´.

LÆ°u Ã½ cÃ¡ch giÃ¡ trá»‹ cuá»‘i cÃ¹ng cá»§a chuá»—i thá»© hai lÃ  ID Ä‘á»‡m, lÃ  giÃ¡ trá»‹ 0 trong attention mask.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Ãp dá»¥ng thá»§ cÃ´ng tokenize cho hai cÃ¢u Ä‘Æ°á»£c sá»­ dá»¥ng trong pháº§n 2 ("I've been waiting for a HuggingFace course my whole life." vÃ  "I hate this so much!"). ÄÆ°a chÃºng vÃ o mÃ´ hÃ¬nh vÃ  kiá»ƒm tra xem báº¡n cÃ³ nháº­n Ä‘Æ°á»£c cÃ¡c logit giá»‘ng nhÆ° trong pháº§n 2 khÃ´ng. BÃ¢y giá», gá»™p chÃºng láº¡i vá»›i nhau báº±ng cÃ¡ch sá»­ dá»¥ng token Ä‘á»‡m, sau Ä‘Ã³ táº¡o attention mask thÃ­ch há»£p. Kiá»ƒm tra xem báº¡n cÃ³ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± khi Ä‘Æ°a qua mÃ´ hÃ¬nh khÃ´ng!

</Tip>

## Nhá»¯ng chuá»—i dÃ i hÆ¡n

Vá»›i cÃ¡c mÃ´ hÃ¬nh Transformer, cÃ³ má»™t giá»›i háº¡n vá» Ä‘á»™ dÃ i cá»§a cÃ¡c chuá»—i mÃ  chÃºng tÃ´i cÃ³ thá»ƒ vÆ°á»£t qua cÃ¡c mÃ´ hÃ¬nh. Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh xá»­ lÃ½ chuá»—i lÃªn Ä‘áº¿n 512 hoáº·c 1024 token vÃ  sáº½ bá»‹ lá»—i khi Ä‘Æ°á»£c yÃªu cáº§u xá»­ lÃ½ chuá»—i dÃ i hÆ¡n. CÃ³ hai giáº£i phÃ¡p cho váº¥n Ä‘á» nÃ y:

- Sá»­ dá»¥ng mÃ´ hÃ¬nh cÃ³ Ä‘á»™ dÃ i chuá»—i Ä‘Æ°á»£c há»— trá»£ dÃ i hÆ¡n.
- Cáº¯t ngáº¯n chuá»—i cá»§a báº¡n.

CÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ dÃ i chuá»—i Ä‘Æ°á»£c há»— trá»£ khÃ¡c nhau vÃ  má»™t sá»‘ mÃ´ hÃ¬nh chuyÃªn xá»­ lÃ½ cÃ¡c trÃ¬nh tá»± ráº¥t dÃ i. [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) lÃ  má»™t vÃ­ dá»¥ vÃ  má»™t vÃ­ dá»¥ khÃ¡c lÃ  [LED](https://huggingface.co/transformers/model_doc/led.html). Náº¿u báº¡n Ä‘ang thá»±c hiá»‡n má»™t cÃ´ng viá»‡c Ä‘Ã²i há»i trÃ¬nh tá»± ráº¥t dÃ i, chÃºng tÃ´i khuyÃªn báº¡n nÃªn xem cÃ¡c mÃ´ hÃ¬nh Ä‘Ã³.

Náº¿u khÃ´ng, chÃºng tÃ´i khuyÃªn báº¡n nÃªn cáº¯t bá»›t cÃ¡c chuá»—i cá»§a mÃ¬nh báº±ng cÃ¡ch chá»‰ Ä‘á»‹nh tham sá»‘ `max_sequence_length`:

```py
sequence = sequence[:max_sequence_length]
```
