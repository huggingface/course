<FrameworkSwitchCourse {fw} />

# Gá»¡ lá»—i quy trÃ¬nh huáº¥n luyá»‡n

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter8/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter8/section4_tf.ipynb"},
]} />

Báº¡n Ä‘Ã£ viáº¿t má»™t ká»‹ch báº£n tuyá»‡t Ä‘áº¹p Ä‘á»ƒ huáº¥n luyá»‡n hoáº·c tinh chá»‰nh má»™t mÃ´ hÃ¬nh trong má»™t tÃ¡c vá»¥ nháº¥t Ä‘á»‹nh, tuÃ¢n thá»§ má»™t cÃ¡ch nghiÃªm tÃºc lá»i khuyÃªn tá»« [ChÆ°Æ¡ng 7](/course/chapter7). NhÆ°ng khi báº¡n khá»Ÿi cháº¡y lá»‡nh `model.fit()`, má»™t Ä‘iá»u kinh khá»§ng xáº£y ra: báº¡n gáº·p lá»—i ğŸ˜±! Hoáº·c tá»‡ hÆ¡n, má»i thá»© dÆ°á»ng nhÆ° á»•n vÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n cháº¡y mÃ  khÃ´ng cÃ³ lá»—i, nhÆ°ng mÃ´ hÃ¬nh káº¿t quáº£ lÃ  tá»“i tá»‡. Trong pháº§n nÃ y, chÃºng tÃ´i sáº½ chá»‰ cho báº¡n nhá»¯ng gÃ¬ báº¡n cÃ³ thá»ƒ lÃ m Ä‘á»ƒ gá»¡ lá»—i cÃ¡c loáº¡i váº¥n Ä‘á» nÃ y.

<Youtube id="N9kO52itd0Q"/>

Váº¥n Ä‘á» khi báº¡n gáº·p lá»—i trong  `model.fit()`  cÃ³ thá»ƒ Ä‘áº¿n tá»« nhiá»u nguá»“n, vÃ¬ viá»‡c huáº¥n luyá»‡n thÆ°á»ng táº­p há»£p ráº¥t nhiá»u thá»© láº¡i vá»›i nhau. Váº¥n Ä‘á» cÃ³ thá»ƒ lÃ  má»™t cÃ¡i gÃ¬ Ä‘Ã³ sai trong bá»™ dá»¯ liá»‡u cá»§a báº¡n hoáº·c má»™t sá»‘ váº¥n Ä‘á» khi cá»‘ gáº¯ng káº¿t há»£p hÃ ng loáº¡t cÃ¡c pháº§n tá»­ cá»§a bá»™ dá»¯ liá»‡u vá»›i nhau. Sau Ä‘Ã³, nÃ³ láº¥y má»™t loáº¡t dá»¯ liá»‡u vÃ  Ä‘Æ°a nÃ³ vÃ o mÃ´ hÃ¬nh, vÃ¬ váº­y váº¥n Ä‘á» cÃ³ thá»ƒ náº±m á»Ÿ mÃ£ mÃ´ hÃ¬nh. Sau Ä‘Ã³, nÃ³ tÃ­nh toÃ¡n cÃ¡c Ä‘á»™ dá»‘c vÃ  thá»±c hiá»‡n bÆ°á»›c tá»‘i Æ°u hÃ³a, vÃ¬ váº­y váº¥n Ä‘á» cÅ©ng cÃ³ thá»ƒ náº±m trong trÃ¬nh tá»‘i Æ°u hÃ³a cá»§a báº¡n. VÃ  ngay cáº£ khi má»i thá»© diá»…n ra tá»‘t Ä‘áº¹p cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n, váº«n cÃ³ thá»ƒ xáº£y ra sá»± cá»‘ trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ náº¿u cÃ³ váº¥n Ä‘á» vá»›i chá»‰ sá»‘ cá»§a báº¡n.

CÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ gá»¡ lá»—i phÃ¡t sinh trong `model.fit()` lÃ  Ä‘i qua toÃ n pipeline nÃ y theo cÃ¡ch thá»§ cÃ´ng Ä‘á»ƒ xem má»i thá»© diá»…n ra nhÆ° tháº¿ nÃ o. Sau Ä‘Ã³, lá»—i thÆ°á»ng ráº¥t dá»… giáº£i quyáº¿t.

Äá»ƒ chá»©ng minh Ä‘iá»u nÃ y, chÃºng ta sáº½ sá»­ dá»¥ng táº­p lá»‡nh (cá»‘ gáº¯ng) tinh chá»‰nh mÃ´ hÃ¬nh DistilBERT trÃªn [táº­p dá»¯ liá»‡u MNLI](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)
```

Náº¿u báº¡n cá»‘ gáº¯ng thá»±c thi nÃ³, báº¡n cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c má»™t sá»‘ `VisibleDeprecationWarning` khi thá»±c hiá»‡n chuyá»ƒn Ä‘á»•i táº­p dá»¯ liá»‡u - Ä‘Ã¢y lÃ  má»™t váº¥n Ä‘á» UX Ä‘Ã£ biáº¿t mÃ  chÃºng ta gáº·p pháº£i, vÃ¬ váº­y vui lÃ²ng bá» qua nÃ³. Náº¿u báº¡n Ä‘ang Ä‘á»c khÃ³a há»c sau Ä‘Ã³, cháº³ng háº¡n nhÆ° thÃ¡ng 11 nÄƒm 2021 vÃ  nÃ³ váº«n Ä‘ang diá»…n ra, thÃ¬ hÃ£y gá»­i nhá»¯ng dÃ²ng tweet giáº­n dá»¯ táº¡i @carrigmat cho Ä‘áº¿n khi anh áº¥y sá»­a nÃ³.

Tuy nhiÃªn, má»™t váº¥n Ä‘á» nghiÃªm trá»ng hÆ¡n lÃ  chÃºng ta nháº­n Ä‘Æ°á»£c má»™t lá»—i trá»n váº¹n. VÃ  nÃ³ thá»±c sá»± ráº¥t dÃ i:

```python out
ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']
```

Äiá»u Ä‘Ã³ nghÄ©a lÃ  gÃ¬? ChÃºng ta Ä‘Ã£ cá»‘ gáº¯ng huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u cá»§a mÃ¬nh, nhÆ°ng chÃºng ta khÃ´ng cÃ³ gradient? Äiá»u nÃ y khÃ¡ bá»‘i rá»‘i; lÃ m tháº¿ nÃ o Ä‘á»ƒ chÃºng ta tháº­m chÃ­ báº¯t Ä‘áº§u gá»¡ lá»—i má»™t cÃ¡i gÃ¬ Ä‘Ã³ nhÆ° váº­y? Khi lá»—i báº¡n gáº·p pháº£i khÃ´ng gá»£i Ã½ ngay váº¥n Ä‘á» náº±m á»Ÿ Ä‘Ã¢u, giáº£i phÃ¡p tá»‘t nháº¥t thÆ°á»ng lÃ  thá»±c hiá»‡n má»i thá»© theo trÃ¬nh tá»±, Ä‘áº£m báº£o á»Ÿ má»—i giai Ä‘oáº¡n má»i thá»© Ä‘á»u á»•n. VÃ  táº¥t nhiÃªn, nÆ¡i báº¯t Ä‘áº§u luÃ´n lÃ ...

### Kiá»ƒm tra dá»¯ liá»‡u cá»§a báº¡n

Äiá»u nÃ y khÃ´ng cáº§n pháº£i nÃ³i, nhÆ°ng náº¿u dá»¯ liá»‡u cá»§a báº¡n bá»‹ há»ng, Keras sáº½ khÃ´ng thá»ƒ sá»­a nÃ³ cho báº¡n. VÃ¬ váº­y, Ä‘iá»u Ä‘áº§u tiÃªn, báº¡n cáº§n pháº£i xem xÃ©t nhá»¯ng gÃ¬ bÃªn trong bá»™ huáº¥n luyá»‡n cá»§a báº¡n.

Máº·c dÃ¹ ráº¥t háº¥p dáº«n khi nhÃ¬n vÃ o bÃªn trong `raw_datasets` vÃ  `tokenized_datasets`, chÃºng tÃ´i thá»±c sá»± khuyÃªn báº¡n nÃªn truy cáº­p dá»¯ liá»‡u ngay táº¡i Ä‘iá»ƒm mÃ  nÃ³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o mÃ´ hÃ¬nh. Äiá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  Ä‘á»c káº¿t quáº£ Ä‘áº§u ra tá»« `tf.data.Dataset` mÃ  báº¡n Ä‘Ã£ táº¡o báº±ng hÃ m `to_tf_dataset()`! VÃ¬ váº­y, lÃ m tháº¿ nÃ o Ä‘á»ƒ chÃºng ta lÃ m Ä‘iá»u Ä‘Ã³? CÃ¡c Ä‘á»‘i tÆ°á»£ng `tf.data.Dataset` cung cáº¥p cho chÃºng ta toÃ n bá»™ cÃ¡c lÃ´ cÃ¹ng má»™t lÃºc vÃ  khÃ´ng há»— trá»£ láº­p chá»‰ má»¥c, vÃ¬ váº­y chÃºng ta khÃ´ng thá»ƒ chá»‰ yÃªu cáº§u `train_dataset[0]`. Tuy nhiÃªn, chÃºng ta cÃ³ thá»ƒ yÃªu cáº§u nÃ³ má»™t cÃ¡ch lá»‹ch sá»± cho má»™t lÃ´:

```py
for batch in train_dataset:
    break
```

`break` káº¿t thÃºc vÃ²ng láº·p sau má»™t láº§n láº·p, vÃ¬ váº­y, Ä‘iá»u nÃ y láº¥y lÃ´ Ä‘áº§u tiÃªn ra khá»i `train_dataset` vÃ  lÆ°u nÃ³ dÆ°á»›i dáº¡ng `batch`. BÃ¢y giá», chÃºng ta hÃ£y xem nhá»¯ng gÃ¬ bÃªn trong:

```python out
{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}
```

Äiá»u nÃ y cÃ³ váº» Ä‘Ãºng, pháº£i khÃ´ng? ChÃºng ta Ä‘ang chuyá»ƒn cÃ¡c nhÃ£n `labels`, `attention_mask`, vÃ  `input_ids` cho mÃ´ hÃ¬nh, Ä‘Ã¢y sáº½ lÃ  má»i thá»© nÃ³ cáº§n Ä‘á»ƒ tÃ­nh toÃ¡n káº¿t quáº£ Ä‘áº§u ra vÃ  tÃ­nh toÃ¡n máº¥t mÃ¡t. Váº­y táº¡i sao chÃºng ta khÃ´ng cÃ³ má»™t gradient? NhÃ¬n ká»¹ hÆ¡n: chÃºng ta Ä‘ang chuyá»ƒn má»™t tá»« Ä‘iá»ƒn duy nháº¥t lÃ m Ä‘áº§u vÃ o, nhÆ°ng má»™t lÃ´ huáº¥n luyá»‡n thÆ°á»ng lÃ  má»™t tensor Ä‘áº§u vÃ o hoáº·c tá»« Ä‘iá»ƒn, cá»™ng vá»›i má»™t tensor nhÃ£n. NhÃ£n cá»§a chÃºng ta lÃ  má»™t chÃ¬a khÃ³a trong tá»« Ä‘iá»ƒn Ä‘áº§u vÃ o cá»§a mÃ¬nh.

ÄÃ¢y cÃ³ phaÌ‰i lÃ  váº¥n Ä‘Ãª? KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng váº­y! NhÆ°ng Ä‘Ã³ lÃ  má»™t trong nhá»¯ng váº¥n Ä‘á» phá»• biáº¿n nháº¥t mÃ  báº¡n sáº½ gáº·p pháº£i khi huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Transformer vá»›i TensorFlow. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng ta Ä‘á»u cÃ³ thá»ƒ tÃ­nh toÃ¡n máº¥t mÃ¡t trong ná»™i bá»™, nhÆ°ng Ä‘á»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, cÃ¡c nhÃ£n cáº§n Ä‘Æ°á»£c chuyá»ƒn vÃ o tá»« Ä‘iá»ƒn Ä‘áº§u vÃ o. ÄÃ¢y lÃ  máº¥t mÃ¡t Ä‘Æ°á»£c sá»­ dá»¥ng khi chÃºng ta khÃ´ng chá»‰ Ä‘á»‹nh giÃ¡ trá»‹ tá»•n tháº¥t cho `compile()`. Máº·t khÃ¡c, Keras thÆ°á»ng mong Ä‘á»£i cÃ¡c nhÃ£n Ä‘Æ°á»£c chuyá»ƒn riÃªng khá»i tá»« Ä‘iá»ƒn Ä‘áº§u vÃ o vÃ  cÃ¡c tÃ­nh toÃ¡n tá»•n tháº¥t thÆ°á»ng sáº½ tháº¥t báº¡i náº¿u báº¡n khÃ´ng lÃ m Ä‘iá»u Ä‘Ã³.

Váº¥n Ä‘á» giá» Ä‘Ã£ trá»Ÿ nÃªn rÃµ rÃ ng hÆ¡n: chÃºng ta Ä‘Ã£ thÃ´ng qua tham sá»‘ `loss`, cÃ³ nghÄ©a lÃ  chÃºng ta Ä‘ang yÃªu cáº§u Keras tÃ­nh toÃ¡n khoáº£n máº¥t mÃ¡t cá»§a mÃ¬nh, nhÆ°ng chÃºng ta Ä‘Ã£ chuyá»ƒn nhÃ£n cá»§a mÃ¬nh lÃ m Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh, khÃ´ng pháº£i lÃ  nhÃ£n á»Ÿ nÆ¡i Keras mong Ä‘á»£i chÃºng! ChÃºng ta cáº§n chá»n cÃ¡i nÃ y hay cÃ¡i kia: hoáº·c chÃºng ta sá»­ dá»¥ng tá»•n tháº¥t bÃªn trong cá»§a mÃ´ hÃ¬nh vÃ  giá»¯ cÃ¡c nhÃ£n á»Ÿ vá»‹ trÃ­ cá»§a chÃºng, hoáº·c chÃºng ta tiáº¿p tá»¥c sá»­ dá»¥ng tá»•n tháº¥t Keras, nhÆ°ng chÃºng ta chuyá»ƒn cÃ¡c nhÃ£n Ä‘áº¿n nÆ¡i mÃ  Keras mong Ä‘á»£i chÃºng. Äá»ƒ Ä‘Æ¡n giáº£n, chÃºng ta hÃ£y thá»±c hiá»‡n cÃ¡ch tiáº¿p cáº­n Ä‘áº§u tiÃªn. Thay Ä‘á»•i lá»‡nh gá»i thÃ nh `compile()` Ä‘á»ƒ Ä‘á»c:

```py
model.compile(optimizer="adam")
```

BÃ¢y giá» chÃºng ta sáº½ sá»­ dá»¥ng máº¥t mÃ¡t bÃªn trong cá»§a mÃ´ hÃ¬nh vÃ  váº¥n Ä‘á» nÃ y sáº½ Ä‘Æ°á»£c giáº£i quyáº¿t!

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** LÃ  má»™t thá»­ thÃ¡ch khÃ´ng báº¯t buá»™c sau khi chÃºng ta Ä‘Ã£ giáº£i quyáº¿t xong cÃ¡c váº¥n Ä‘á» khÃ¡c, báº¡n cÃ³ thá»ƒ thá»­ quay láº¡i bÆ°á»›c nÃ y vÃ  lÃ m cho mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng vá»›i máº¥t mÃ¡t do Keras tÃ­nh toÃ¡n ban Ä‘áº§u thay vÃ¬ máº¥t mÃ¡t ná»™i bá»™. Báº¡n sáº½ cáº§n pháº£i thÃªm `"labels"` vÃ o `label_cols` cá»§a `to_tf_dataset()` Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c nhÃ£n Ä‘Æ°á»£c xuáº¥t chÃ­nh xÃ¡c, Ä‘iá»u nÃ y sáº½ giÃºp báº¡n cÃ³ Ä‘Æ°á»£c Ä‘á»™ dá»‘c - nhÆ°ng cÃ³ má»™t váº¥n Ä‘á» ná»¯a vá»›i sá»± máº¥t mÃ¡t mÃ  chÃºng ta Ä‘Ã£ chá»‰ Ä‘á»‹nh. Viá»‡c huáº¥nl uyá»‡n váº«n sáº½ diá»…n ra vá»›i váº¥n Ä‘á» nÃ y, nhÆ°ng viá»‡c há»c sáº½ ráº¥t cháº­m vÃ  sáº½ kháº£ nÄƒng máº¥t mÃ¡t huáº¥n luyá»‡n cao. Báº¡n cÃ³ thá»ƒ tÃ¬m ra nÃ³ lÃ  gÃ¬ khÃ´ng?

Má»™t gá»£i Ã½ mÃ£ hoÃ¡ ROT13, náº¿u báº¡n báº¿ táº¯c: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf `ybtvgf`. Jung ner ybtvgf?

VÃ  má»™t gá»£i Ã½ thá»© hai: Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?

</Tip>

BÃ¢y giá», chÃºng ta hÃ£y thá»­ huáº¥n luyá»‡n. BÃ¢y giá» chÃºng ta sáº½ nháº­n Ä‘Æ°á»£c gradient, vÃ¬ váº­y hy vá»ng (nháº¡c Ä‘Ã¡ng ngáº¡i phÃ¡t á»Ÿ Ä‘Ã¢y) chÃºng ta cÃ³ thá»ƒ gá»i `model.fit()` vÃ  má»i thá»© sáº½ hoáº¡t Ä‘á»™ng tá»‘t!

```python out
  246/24543 [..............................] - ETA: 15:52 - loss: nan
```

Ã”i khÃ´ng.

`nan` khÃ´ng pháº£i lÃ  má»™t giÃ¡ trá»‹ máº¥t mÃ¡t Ä‘Ã¡ng khÃ­ch lá»‡. Tuy nhiÃªn, chÃºng ta Ä‘Ã£ kiá»ƒm tra dá»¯ liá»‡u cá»§a mÃ¬nh vÃ  nÃ³ trÃ´ng khÃ¡ á»•n. Náº¿u Ä‘Ã³ khÃ´ng pháº£i lÃ  váº¥n Ä‘á», chÃºng ta cÃ³ thá»ƒ Ä‘i Ä‘Ã¢u tiáº¿p theo? BÆ°á»›c tiáº¿p theo rÃµ rÃ ng lÃ  ...

### Kiá»ƒm tra mÃ´ hÃ¬nh cá»§a báº¡n

`model.fit()` lÃ  má»™t hÃ m tiá»‡n lá»£i thá»±c sá»± tuyá»‡t vá»i trong Keras, nhÆ°ng nÃ³ lÃ m Ä‘Æ°á»£c ráº¥t nhiá»u thá»© cho báº¡n vÃ  Ä‘iá»u Ä‘Ã³ cÃ³ thá»ƒ khiáº¿n viá»‡c tÃ¬m chÃ­nh xÃ¡c vá»‹ trÃ­ Ä‘Ã£ xáº£y ra sá»± cá»‘ trá»Ÿ nÃªn khÃ³ khÄƒn hÆ¡n. Náº¿u báº¡n Ä‘ang gá»¡ lá»—i mÃ´ hÃ¬nh cá»§a mÃ¬nh, má»™t chiáº¿n lÆ°á»£c thá»±c sá»± cÃ³ thá»ƒ há»¯u Ã­ch lÃ  chá»‰ chuyá»ƒn má»™t lÃ´ duy nháº¥t cho mÃ´ hÃ¬nh vÃ  xem xÃ©t chi tiáº¿t káº¿t quáº£ Ä‘áº§u ra cá»§a má»™t lÃ´ Ä‘Ã³. Má»™t máº¹o thá»±c sá»± há»¯u Ã­ch khÃ¡c náº¿u mÃ´ hÃ¬nh Ä‘ang gáº·p lá»—i lÃ  `compile()` mÃ´ hÃ¬nh vá»›i `run_eagerly=True`. Äiá»u nÃ y sáº½ lÃ m cho nÃ³ cháº­m hÆ¡n ráº¥t nhiá»u, nhÆ°ng nÃ³ sáº½ lÃ m cho cÃ¡c thÃ´ng bÃ¡o lá»—i dá»… hiá»ƒu hÆ¡n nhiá»u, bá»Ÿi vÃ¬ chÃºng sáº½ chá»‰ ra chÃ­nh xÃ¡c vá»‹ trÃ­ xáº£y ra sá»± cá»‘ trong mÃ£ mÃ´ hÃ¬nh cá»§a báº¡n.

Tuy nhiÃªn, hiá»‡n táº¡i, chÃºng ta chÆ°a cáº§n Ä‘áº¿n `run_eagerly`. HÃ£y cháº¡y `batch` mÃ  chÃºng ta Ä‘Ã£ cÃ³ trÆ°á»›c Ä‘Ã³ thÃ´ng qua mÃ´ hÃ¬nh vÃ  xem káº¿t quáº£ Ä‘áº§u ra trÃ´ng nhÆ° tháº¿ nÃ o:

```py
model(batch)
```

```python out
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)
```

ChÃ , Ä‘iá»u nÃ y tháº­t khÃ³. Má»i thá»© Ä‘á»u lÃ  `nan`! NhÆ°ng tháº­t láº¡ pháº£i khÃ´ng? LÃ m tháº¿ nÃ o mÃ  táº¥t cáº£ nháº­t kÃ½ cá»§a chÃºng ta sáº½ trá»Ÿ thÃ nh `nan`? `nan` cÃ³ nghÄ©a lÃ  "khÃ´ng pháº£i lÃ  sá»‘". GiÃ¡ trá»‹ `nan` thÆ°á»ng xáº£y ra khi báº¡n thá»±c hiá»‡n má»™t thao tÃ¡c bá»‹ cáº¥m, cháº³ng háº¡n nhÆ° chia cho sá»‘ khÃ´ng. NhÆ°ng má»™t Ä‘iá»u ráº¥t quan trá»ng cáº§n biáº¿t vá» `nan` trong há»c mÃ¡y lÃ  giÃ¡ trá»‹ nÃ y cÃ³ xu hÆ°á»›ng *lan truyá»n*. Náº¿u báº¡n nhÃ¢n má»™t sá»‘ vá»›i `nan`, káº¿t quáº£ cÅ©ng lÃ  `nan`. VÃ  náº¿u báº¡n nháº­n Ä‘Æ°á»£c má»™t `nan` á»Ÿ báº¥t ká»³ Ä‘Ã¢u trong Ä‘áº§u ra, sá»± máº¥t mÃ¡t hoáº·c Ä‘á»™ dá»‘c cá»§a báº¡n, thÃ¬ nÃ³ sáº½ nhanh chÃ³ng lan rá»™ng ra toÃ n bá»™ mÃ´ hÃ¬nh cá»§a báº¡n - bá»Ÿi vÃ¬ khi giÃ¡ trá»‹ `nan` Ä‘Ã³ Ä‘Æ°á»£c truyá»n trá»Ÿ láº¡i qua máº¡ng cá»§a báº¡n, báº¡n sáº½ nháº­n Ä‘Æ°á»£c `nan` gradient vÃ  khi cáº­p nháº­t trá»ng sá»‘ Ä‘Æ°á»£c tÃ­nh toÃ¡n vá»›i nhá»¯ng gradient Ä‘Ã³, báº¡n sáº½ nháº­n Ä‘Æ°á»£c trá»ng sá»‘ `nan` vÃ  nhá»¯ng trá»ng sá»‘ Ä‘Ã³ sáº½ tÃ­nh toÃ¡n nhiá»u káº¿t quáº£ Ä‘áº§u ra `nan` hÆ¡n ná»¯a! Cháº³ng bao lÃ¢u ná»¯a, toÃ n bá»™ máº¡ng lÆ°á»›i sáº½ chá»‰ lÃ  má»™t khá»‘i lá»›n gá»“m cÃ¡c giÃ¡ trá»‹ `nan`. Má»™t khi Ä‘iá»u Ä‘Ã³ xáº£y ra, tháº­t khÃ³ Ä‘á»ƒ xem váº¥n Ä‘á» báº¯t Ä‘áº§u tá»« Ä‘Ã¢u. LÃ m tháº¿ nÃ o chÃºng ta cÃ³ thá»ƒ cÃ´ láº­p nÆ¡i mÃ  `nan` len lá»i Ä‘áº§u tiÃªn?

CÃ¢u tráº£ lá»i lÃ  hÃ£y thá»­ *khá»Ÿi Ä‘á»™ng láº¡i* mÃ´ hÃ¬nh. Khi chÃºng ta báº¯t Ä‘áº§u huáº¥n luyá»‡n, chÃºng ta cÃ³ má»™t `nan` á»Ÿ Ä‘Ã¢u Ä‘Ã³ vÃ  nÃ³ nhanh chÃ³ng Ä‘Æ°á»£c truyá»n bÃ¡ qua toÃ n bá»™ mÃ´ hÃ¬nh. VÃ¬ váº­y, hÃ£y táº£i mÃ´ hÃ¬nh tá»« má»™t checkpoint vÃ  khÃ´ng thá»±c hiá»‡n báº¥t ká»³ cáº­p nháº­t trá»ng sá»‘ nÃ o vÃ  xem nÆ¡i chÃºng ta nháº­n Ä‘Æ°á»£c giÃ¡ trá»‹ `nan`:

```py
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)
```

When we run that, we get:

```py out
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)
```

*BÃ¢y giá»* chÃºng ta Ä‘ang Ä‘áº¿n má»™t nÆ¡i nÃ o Ä‘Ã³! KhÃ´ng cÃ³ giÃ¡ trá»‹ `nan` nÃ o trong nháº­t kÃ½ cá»§a mÃ¬nh, Ä‘iá»u nÃ y khiáº¿n báº¡n yÃªn tÃ¢m. NhÆ°ng chÃºng ta tháº¥y cÃ³ má»™t vÃ i giÃ¡ trá»‹ `nan` bá»‹ máº¥t! CÃ³ Ä‘iá»u gÃ¬ Ä‘Ã³ Ä‘áº·c biá»‡t vá» cÃ¡c máº«u Ä‘Ã³ gÃ¢y ra váº¥n Ä‘á» nÃ y khÃ´ng? HÃ£y xem chÃºng lÃ  nhá»¯ng cÃ¡i nÃ o (lÆ°u Ã½ ráº±ng náº¿u báº¡n tá»± cháº¡y mÃ£ nÃ y, báº¡n cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c cÃ¡c chá»‰ sá»‘ khÃ¡c nhau vÃ¬ táº­p dá»¯ liá»‡u Ä‘Ã£ bá»‹ xÃ¡o trá»™n):

```python
import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices
```

```python out
array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])
```

HÃ£y xem cÃ¡c máº«u mÃ  táº¡o ra chá»‰ sá»‘ nÃ y:

```python
input_ids = batch["input_ids"].numpy()
input_ids[indices]
```

```python out
array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])
```

ChÃ , cÃ³ ráº¥t nhiá»u thá»© á»Ÿ Ä‘Ã¢y, nhÆ°ng khÃ´ng cÃ³ gÃ¬ ná»•i báº­t lÃ  báº¥t thÆ°á»ng. HÃ£y xem cÃ¡c nhÃ£n:

```python out
labels = batch['labels'].numpy()
labels[indices]
```

```python out
array([2, 2, 2, 2, 2, 2, 2, 2, 2])
```

Ah! CÃ¡c máº«u `nan` Ä‘á»u cÃ³ cÃ¹ng má»™t nhÃ£n, vÃ  Ä‘Ã³ lÃ  nhÃ£n 2. ÄÃ¢y lÃ  má»™t gá»£i Ã½ ráº¥t máº¡nh máº½. Thá»±c táº¿ lÃ  chÃºng ta chá»‰ bá»‹ máº¥t `nan` khi nhÃ£n cá»§a chÃºng ta lÃ  2 cho tháº¥y ráº±ng Ä‘Ã¢y lÃ  thá»i Ä‘iá»ƒm ráº¥t tá»‘t Ä‘á»ƒ kiá»ƒm tra sá»‘ lÆ°á»£ng nhÃ£n trong mÃ´ hÃ¬nh cá»§a mÃ¬nh:

```python
model.config.num_labels
```

```python out
2
```

BÃ¢y giá» chÃºng ta tháº¥y váº¥n Ä‘á»: mÃ´ hÃ¬nh cho ráº±ng chá»‰ cÃ³ hai lá»›p, nhÆ°ng cÃ¡c nhÃ£n tÄƒng lÃªn 2, cÃ³ nghÄ©a lÃ  thá»±c táº¿ cÃ³ ba lá»›p (vÃ¬ 0 cÅ©ng lÃ  má»™t lá»›p). ÄÃ¢y lÃ  cÃ¡ch chÃºng ta cÃ³ má»™t `nan` - báº±ng cÃ¡ch cá»‘ gáº¯ng tÃ­nh toÃ¡n máº¥t mÃ¡t cho má»™t lá»›p khÃ´ng tá»“n táº¡i! HÃ£y thá»­ thay Ä‘á»•i Ä‘iá»u Ä‘Ã³ vÃ  láº¯p láº¡i mÃ´ hÃ¬nh:

```
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)
```

```python out
  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032
```

ChÃºng ta Ä‘ang huáº¥n luyá»‡n! KhÃ´ng cÃ²n `nan` ná»¯a, vÃ  máº¥t mÃ¡t cá»§a chÃºng ta Ä‘ang giáº£m dáº§n ... Ä‘áº¡i loáº¡i váº­y. Náº¿u báº¡n quan sÃ¡t nÃ³ má»™t lÃºc, báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u hÆ¡i máº¥t kiÃªn nháº«n, bá»Ÿi vÃ¬ giÃ¡ trá»‹ tá»•n tháº¥t váº«n á»Ÿ má»©c cao. ChÃºng ta hÃ£y dá»«ng huáº¥n luyá»‡n á»Ÿ Ä‘Ã¢y vÃ  thá»­ nghÄ© xem Ä‘iá»u gÃ¬ cÃ³ thá»ƒ gÃ¢y ra váº¥n Ä‘á» nÃ y. Táº¡i thá»i Ä‘iá»ƒm nÃ y, chÃºng ta khÃ¡ cháº¯c cháº¯n ráº±ng cáº£ dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh Ä‘á»u á»•n, nhÆ°ng mÃ´ hÃ¬nh cá»§a chÃºng ta khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t. CÃ²n láº¡i gÃ¬ ná»¯a? Äáº¿n lÃºc Ä‘á»ƒ...

### Kiá»ƒm tra siÃªu tham sá»‘ cá»§a báº¡n

Náº¿u báº¡n nhÃ¬n láº¡i Ä‘oáº¡n mÃ£ á»Ÿ trÃªn, báº¡n cÃ³ thá»ƒ khÃ´ng nhÃ¬n tháº¥y báº¥t ká»³ siÃªu tham sá»‘ nÃ o, cÃ³ láº½ ngoáº¡i trá»« `batch_size`, vÃ  Ä‘iá»u Ä‘Ã³ dÆ°á»ng nhÆ° khÃ´ng pháº£i lÃ  thá»§ pháº¡m. Tuy nhiÃªn, Ä‘á»«ng Ä‘á»ƒ bá»‹ lá»«a; luÃ´n cÃ³ siÃªu tham sá»‘ vÃ  náº¿u báº¡n khÃ´ng thá»ƒ nhÃ¬n tháº¥y chÃºng, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  báº¡n khÃ´ng biáº¿t chÃºng Ä‘Æ°á»£c Ä‘áº·t thÃ nh gÃ¬. Äáº·c biá»‡t, hÃ£y nhá»› má»™t Ä‘iá»u quan trá»ng vá» Keras: náº¿u báº¡n Ä‘áº·t hÃ m máº¥t mÃ¡t, trÃ¬nh tá»‘i Æ°u hÃ³a hoáº·c kÃ­ch hoáº¡t báº±ng má»™t chuá»—i, _táº¥t cáº£ cÃ¡c Ä‘á»‘i sá»‘ cá»§a nÃ³ sáº½ Ä‘Æ°á»£c Ä‘áº·t thÃ nh giÃ¡ trá»‹ máº·c Ä‘á»‹nh cá»§a chÃºng_. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  máº·c dÃ¹ viá»‡c sá»­ dá»¥ng chuá»—i kÃ½ tá»± ráº¥t tiá»‡n lá»£i, nhÆ°ng báº¡n nÃªn háº¿t sá»©c cáº©n tháº­n khi lÃ m nhÆ° váº­y, vÃ¬ nÃ³ cÃ³ thá»ƒ dá»… dÃ ng che giáº¥u nhá»¯ng thá»© quan trá»ng vá»›i báº¡n. (Báº¥t ká»³ ai Ä‘ang thá»­ thÃ¡ch thá»©c tÃ¹y chá»n á»Ÿ trÃªn nÃªn lÆ°u Ã½ cáº©n tháº­n vá» thá»±c táº¿ nÃ y.)

Trong trÆ°á»ng há»£p nÃ y, chÃºng ta Ä‘Ã£ Ä‘áº·t tham sá»‘ báº±ng chuá»—i á»Ÿ Ä‘Ã¢u? Ban Ä‘áº§u, chÃºng ta Ä‘Ã£ Ä‘áº·t giÃ¡ trá»‹ máº¥t mÃ¡t báº±ng má»™t chuá»—i, nhÆ°ng chÃºng ta khÃ´ng lÃ m Ä‘iá»u Ä‘Ã³ ná»¯a. Tuy nhiÃªn, chÃºng ta Ä‘ang thiáº¿t láº­p trÃ¬nh tá»‘i Æ°u hÃ³a báº±ng má»™t chuá»—i. Äiá»u Ä‘Ã³ cÃ³ thá»ƒ Ä‘ang che giáº¥u báº¥t cá»© Ä‘iá»u gÃ¬ vá»›i mÃ¬nh? HÃ£y xem [cÃ¡c tham sá»‘ cá»§a nÃ³](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).

CÃ³ gÃ¬ ná»•i báº­t á»Ÿ Ä‘Ã¢y khÃ´ng? ÄÃºng váº­y - tá»‘c Ä‘á»™ há»c! Khi chÃºng ta chá»‰ sá»­ dá»¥ng chuá»—i `'adam'`, chÃºng ta sáº½ nháº­n Ä‘Æ°á»£c tá»‘c Ä‘á»™ há»c máº·c Ä‘á»‹nh, lÃ  0.001 hoáº·c 1e-3. ÄÃ¢y lÃ  má»©c quÃ¡ cao Ä‘á»‘i vá»›i má»™t mÃ´ hÃ¬nh Transformer! NÃ³i chung, chÃºng tÃ´i khuyÃªn báº¡n nÃªn thá»­ tá»‘c Ä‘á»™ há»c tá»« 1e-5 Ä‘áº¿n 1e-4 cho cÃ¡c mÃ´ hÃ¬nh cá»§a báº¡n; Ä‘Ã³ lÃ  má»™t nÆ¡i nÃ o Ä‘Ã³ nhá» hÆ¡n tá»« 10X Ä‘áº¿n 100X so vá»›i giÃ¡ trá»‹ mÃ  ta thá»±c sá»± Ä‘ang sá»­ dá»¥ng á»Ÿ Ä‘Ã¢y. Äiá»u Ä‘Ã³ nghe cÃ³ váº» nhÆ° nÃ³ cÃ³ thá»ƒ lÃ  má»™t váº¥n Ä‘á» lá»›n, vÃ¬ váº­y hÃ£y thá»­ giáº£m bá»›t nÃ³. Äá»ƒ lÃ m Ä‘iá»u Ä‘Ã³, chÃºng ta cáº§n nháº­p vÃ o Ä‘á»‘i tÆ°á»£ng `optimizer`. Trong khi chÃºng ta Ä‘ang á»Ÿ Ä‘Ã³, hÃ£y báº¯t Ä‘áº§u láº¡i mÃ´ hÃ¬nh tá»« checkpoint, trong trÆ°á»ng há»£p huáº¥n luyá»‡n vá»›i tá»‘c Ä‘á»™ há»c cao lÃ m há»ng trá»ng sá»‘ cá»§a nÃ³:

```python
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))
```

<Tip>

ğŸ’¡ Báº¡n cÅ©ng cÃ³ thá»ƒ nháº­p hÃ m `create_optimizer()` tá»« ğŸ¤— Transformers, hÃ m nÃ y sáº½ cho báº¡n má»™t trÃ¬nh tá»‘i Æ°u AdamW vá»›i vá»›i Ä‘á»™ phÃ¢n rÃ£ trá»ng sá»‘ chÃ­nh xÃ¡c cÅ©ng nhÆ° khá»Ÿi Ä‘á»™ng vÃ  phÃ¢n rÃ£ tá»‘c Ä‘á»™ há»c. TrÃ¬nh nÃ y thÆ°á»ng sáº½ táº¡o ra káº¿t quáº£ tá»‘t hÆ¡n má»™t chÃºt so vá»›i káº¿t quáº£ báº¡n nháº­n Ä‘Æ°á»£c vá»›i trÃ¬nh tá»‘i Æ°u hÃ³a Adam máº·c Ä‘á»‹nh.

</Tip>

BÃ¢y giá», chÃºng tÃ´i cÃ³ thá»ƒ thá»­ Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh vá»›i tá»‘c Ä‘á»™ há»c má»›i, Ä‘Æ°á»£c cáº£i thiá»‡n:

```python
model.fit(train_dataset)
```

```python out
319/24543 [..............................] - ETA: 16:07 - loss: 0.9718
```

BÃ¢y giá» máº¥t mÃ¡t cá»§a chÃºng ta thá»±c sá»± Ä‘i Ä‘Ã¢u Ä‘Ã³! Viá»‡c huáº¥n luyá»‡n cuá»‘i cÃ¹ng cÃ³ váº» nhÆ° nÃ³ Ä‘Ã£ hoáº¡t Ä‘á»™ng. CÃ³ má»™t bÃ i há»c á»Ÿ Ä‘Ã¢y: khi mÃ´ hÃ¬nh cá»§a báº¡n Ä‘ang cháº¡y nhÆ°ng má»©c hao há»¥t khÃ´ng giáº£m vÃ  báº¡n cháº¯c cháº¯n ráº±ng dá»¯ liá»‡u cá»§a mÃ¬nh váº«n á»•n, báº¡n nÃªn kiá»ƒm tra cÃ¡c siÃªu tham sá»‘ nhÆ° tá»‘c Ä‘á»™ há»c vÃ  giáº£m trá»ng lÆ°á»£ng. Äáº·t má»™t trong hai giÃ¡ trá»‹ Ä‘Ã³ quÃ¡ cao ráº¥t cÃ³ thá»ƒ khiáº¿n quÃ¡ trÃ¬nh huáº¥n luyá»‡n bá»‹ "Ä‘Ã¬nh trá»‡" vá»›i giÃ¡ trá»‹ tá»•n tháº¥t cao.

## CÃ¡c váº¥n Ä‘á» tiá»m áº©n khÃ¡c

ChÃºng ta Ä‘Ã£ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c váº¥n Ä‘á» trong táº­p lá»‡nh á»Ÿ trÃªn, nhÆ°ng cÃ³ má»™t sá»‘ lá»—i phá»• biáº¿n khÃ¡c mÃ  báº¡n cÃ³ thá»ƒ gáº·p pháº£i. ChÃºng ta hÃ£y nhÃ¬n vÃ o má»™t danh sÃ¡ch (khÃ´ng Ä‘áº§y Ä‘á»§ cho láº¯m).

### Xá»­ lÃ½ lá»—i háº¿t bá»™ nhá»›

Dáº¥u hiá»‡u cho biáº¿t sáº¯p háº¿t bá»™ nhá»› lÃ  má»™t lá»—i nhÆ° "OOM when allocating tensor"  - OOM lÃ  viáº¿t táº¯t cá»§a "háº¿t bá»™ nhá»›." ÄÃ¢y lÃ  má»™t nguy cÆ¡ ráº¥t phá»• biáº¿n khi xá»­ lÃ½ cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. Náº¿u báº¡n gáº·p pháº£i Ä‘iá»u nÃ y, má»™t chiáº¿n lÆ°á»£c tá»‘t lÃ  giáº£m má»™t ná»­a kÃ­ch thÆ°á»›c lÃ´ cá»§a báº¡n vÃ  thá»­ láº¡i. Tuy nhiÃªn, hÃ£y nhá»› ráº±ng má»™t sá»‘ mÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c *ráº¥t* lá»›n. VÃ­ dá»¥: GPT-2 kÃ­ch thÆ°á»›c Ä‘áº§y Ä‘á»§ cÃ³ thÃ´ng sá»‘ 1.5B, cÃ³ nghÄ©a lÃ  báº¡n sáº½ cáº§n 6GB bá»™ nhá»› chá»‰ Ä‘á»ƒ lÆ°u mÃ´ hÃ¬nh vÃ  6GB khÃ¡c cho Ä‘á»™ dá»‘c cá»§a nÃ³! Huáº¥n luyá»‡n mÃ´ hÃ¬nh GPT-2 Ä‘áº§y Ä‘á»§ thÆ°á»ng sáº½ yÃªu cáº§u hÆ¡n 20GB VRAM báº¥t ká»ƒ báº¡n sá»­ dá»¥ng kÃ­ch thÆ°á»›c lÃ´ nÃ o, Ä‘iá»u mÃ  chá»‰ má»™t sá»‘ GPU cÃ³. CÃ¡c mÃ´ hÃ¬nh nháº¹ hÆ¡n nhÆ° `distilbert-base-cased`  dá»… cháº¡y hÆ¡n nhiá»u vÃ  huáº¥n luyá»‡n cÅ©ng nhanh hÆ¡n nhiá»u.

<Tip>

Trong pháº§n tiáº¿p theo cá»§a khÃ³a há»c, chÃºng ta sáº½ xem xÃ©t cÃ¡c ká»¹ thuáº­t nÃ¢ng cao hÆ¡n cÃ³ thá»ƒ giÃºp báº¡n giáº£m dung lÆ°á»£ng bá»™ nhá»› vÃ  cho phÃ©p báº¡n tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh lá»›n nháº¥t.

</Tip>

### TensorFlow Ä‘Ã³i rá»“i Ä‘Ã³i rá»“iğŸ¦›

Má»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t cá»§a TensorFlow mÃ  báº¡n nÃªn biáº¿t lÃ  nÃ³ phÃ¢n bá»• *táº¥t cáº£* bá»™ nhá»› GPU cá»§a báº¡n cho chÃ­nh nÃ³ ngay khi báº¡n táº£i má»™t mÃ´ hÃ¬nh hoáº·c thá»±c hiá»‡n báº¥t ká»³ huáº¥n luyá»‡n nÃ o vÃ  sau Ä‘Ã³ nÃ³ sáº½ phÃ¢n chia bá»™ nhá»› Ä‘Ã³ theo yÃªu cáº§u. Äiá»u nÃ y khÃ¡c vá»›i hÃ nh vi cá»§a cÃ¡c khung khÃ¡c, nhÆ° PyTorch, phÃ¢n bá»• bá»™ nhá»› theo yÃªu cáº§u vá»›i CUDA thay vÃ¬ thá»±c hiá»‡n nÃ³ trong ná»™i bá»™. Má»™t Æ°u Ä‘iá»ƒm cá»§a phÆ°Æ¡ng phÃ¡p TensorFlow lÃ  nÃ³ thÆ°á»ng cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¡c lá»—i há»¯u Ã­ch khi báº¡n háº¿t bá»™ nhá»› vÃ  nÃ³ cÃ³ thá»ƒ phá»¥c há»“i tá»« tráº¡ng thÃ¡i Ä‘Ã³ mÃ  khÃ´ng lÃ m há»ng toÃ n bá»™ nhÃ¢n CUDA. NhÆ°ng cÅ©ng cÃ³ má»™t nhÆ°á»£c Ä‘iá»ƒm quan trá»ng: náº¿u báº¡n cháº¡y hai tiáº¿n trÃ¬nh TensorFlow cÃ¹ng má»™t lÃºc, thÃ¬ **báº¡n sáº½ cÃ³ má»™t khoáº£ng thá»i gian tá»“i tá»‡**.

Náº¿u báº¡n Ä‘ang cháº¡y trÃªn Colab, báº¡n khÃ´ng cáº§n pháº£i lo láº¯ng vá» Ä‘iá»u nÃ y, nhÆ°ng náº¿u báº¡n Ä‘ang cháº¡y cá»¥c bá»™ thÃ¬ Ä‘Ã¢y cháº¯c cháº¯n lÃ  Ä‘iá»u báº¡n nÃªn cáº©n tháº­n. Äáº·c biá»‡t, hÃ£y lÆ°u Ã½ ráº±ng viá»‡c Ä‘Ã³ng má»™t tab sá»• ghi chÃ©p khÃ´ng nháº¥t thiáº¿t pháº£i Ä‘Ã³ng notebook Ä‘Ã³ láº¡i! Báº¡n cÃ³ thá»ƒ cáº§n chá»n notebok Ä‘ang cháº¡y (notebook cÃ³ biá»ƒu tÆ°á»£ng mÃ u xanh lÃ¡ cÃ¢y) vÃ  táº¯t chÃºng theo cÃ¡ch thá»§ cÃ´ng trong danh sÃ¡ch thÆ° má»¥c. Báº¥t ká»³ notebook Ä‘ang cháº¡y nÃ o Ä‘ang sá»­ dá»¥ng TensorFlow váº«n cÃ³ thá»ƒ Ä‘ang giá»¯ má»™t loáº¡t bá»™ nhá»› GPU cá»§a báº¡n vÃ  Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  báº¥t ká»³ notebook má»›i nÃ o báº¡n báº¯t Ä‘áº§u Ä‘á»u cÃ³ thá»ƒ gáº·p pháº£i má»™t sá»‘ váº¥n Ä‘á» ráº¥t ká»³ quáº·c.

Náº¿u báº¡n báº¯t Ä‘áº§u gáº·p lá»—i vá» CUDA, BLAS hoáº·c cuBLAS trong mÃ£ hoáº¡t Ä‘á»™ng trÆ°á»›c Ä‘Ã³, Ä‘Ã¢y ráº¥t thÆ°á»ng lÃ  thá»§ pháº¡m. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng má»™t lá»‡nh nhÆ° `nvidia-smi` Ä‘á»ƒ kiá»ƒm tra - khi báº¡n táº¯t hoáº·c khá»Ÿi Ä‘á»™ng láº¡i notebook hiá»‡n táº¡i cá»§a mÃ¬nh, bá»™ nhá»› cá»§a báº¡n cÃ²n trá»‘ng hay váº«n cÃ²n sá»­ dá»¥ng Ä‘Æ°á»£c? Náº¿u nÃ³ váº«n cÃ²n Ä‘Æ°á»£c sá»­ dá»¥ng, má»™t cÃ¡i gÃ¬ Ä‘Ã³ khÃ¡c Ä‘ang giá»¯ nÃ³!

### Kiá»ƒm tra láº¡i dá»¯ liá»‡u cá»§a báº¡n (má»™t láº§n ná»¯a!)

MÃ´ hÃ¬nh cá»§a báº¡n sáº½ chá»‰ há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³ náº¿u nÃ³ thá»±c sá»± cÃ³ thá»ƒ há»c Ä‘Æ°á»£c báº¥t cá»© Ä‘iá»u gÃ¬ tá»« dá»¯ liá»‡u cá»§a báº¡n. Náº¿u cÃ³ lá»—i lÃ m há»ng dá»¯ liá»‡u hoáº·c cÃ¡c nhÃ£n Ä‘Æ°á»£c gÃ¡n ngáº«u nhiÃªn, ráº¥t cÃ³ thá»ƒ báº¡n sáº½ khÃ´ng huáº¥n luyá»‡n Ä‘Æ°á»£c mÃ´ hÃ¬nh nÃ o vá» táº­p dá»¯ liá»‡u cá»§a mÃ¬nh. Má»™t cÃ´ng cá»¥ há»¯u Ã­ch á»Ÿ Ä‘Ã¢y lÃ  `tokenizer.decode()`. Thao tÃ¡c nÃ y sáº½ biáº¿n `input_ids` trá»Ÿ láº¡i thÃ nh chuá»—i, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ xem dá»¯ liá»‡u vÃ  xem liá»‡u dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a báº¡n cÃ³ Ä‘ang dáº¡y nhá»¯ng gÃ¬ báº¡n muá»‘n nÃ³ dáº¡y hay khÃ´ng. VÃ­ dá»¥: sau khi báº¡n nháº­n Ä‘Æ°á»£c má»™t `batch` tá»« `tf.data.Dataset` nhÆ° chÃºng ta Ä‘Ã£ lÃ m á»Ÿ trÃªn, báº¡n cÃ³ thá»ƒ giáº£i mÃ£ pháº§n tá»­ Ä‘áº§u tiÃªn nhÆ° sau:

```py
input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])
```

Then you can compare it with the first label, like so:

```py
labels = batch["labels"].numpy()
label = labels[0]
```

Khi báº¡n cÃ³ thá»ƒ xem dá»¯ liá»‡u cá»§a mÃ¬nh nhÆ° tháº¿ nÃ y, báº¡n cÃ³ thá»ƒ tá»± há»i báº£n thÃ¢n nhá»¯ng cÃ¢u há»i sau:

- Dá»¯ liá»‡u Ä‘Æ°á»£c giáº£i mÃ£ cÃ³ dá»… hiá»ƒu khÃ´ng?
- Báº¡n cÃ³ Ä‘á»“ng Ã½ vá»›i cÃ¡c nhÃ£n?
- CÃ³ má»™t nhÃ£n nÃ o phá»• biáº¿n hÆ¡n nhá»¯ng nhÃ£n khÃ¡c khÃ´ng?
- Máº¥t mÃ¡t/Chá»‰ sá»‘ sáº½ lÃ  bao nhiÃªu náº¿u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n má»™t cÃ¢u tráº£ lá»i ngáº«u nhiÃªn/luÃ´n lÃ  má»™t cÃ¢u tráº£ lá»i giá»‘ng nhau?

Sau khi xem xÃ©t dá»¯ liá»‡u cá»§a báº¡n, hÃ£y xem qua má»™t sá»‘ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh - náº¿u mÃ´ hÃ¬nh cá»§a báº¡n xuáº¥t ra cÃ¡c token, hÃ£y thá»­ giáº£i mÃ£ chÃºng! Náº¿u mÃ´ hÃ¬nh luÃ´n dá»± Ä‘oÃ¡n cÃ¹ng má»™t Ä‘iá»u thÃ¬ Ä‘Ã³ cÃ³ thá»ƒ lÃ  do táº­p dá»¯ liá»‡u cá»§a báº¡n thiÃªn vá» má»™t loáº¡i (Ä‘á»‘i vá»›i cÃ¡c váº¥n Ä‘á» phÃ¢n loáº¡i), vÃ¬ váº­y cÃ¡c ká»¹ thuáº­t nhÆ° láº¥y máº«u quÃ¡ má»©c cÃ¡c lá»›p hiáº¿m cÃ³ thá»ƒ há»¯u Ã­ch. NgoÃ i ra, Ä‘iá»u nÃ y cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c gÃ¢y ra bá»Ÿi cÃ¡c váº¥n Ä‘á» huáº¥n luyá»‡n nhÆ° cÃ i Ä‘áº·t siÃªu tham sá»‘ tá»‡.

Náº¿u pháº§n máº¥t mÃ¡t/ cÃ¡c chá»‰ sá»‘ báº¡n nháº­n Ä‘Æ°á»£c trÃªn mÃ´ hÃ¬nh ban Ä‘áº§u cá»§a mÃ¬nh trÆ°á»›c khi huáº¥n luyá»‡n ráº¥t khÃ¡c vá»›i cÃ¡i báº¡n mong Ä‘á»£i cho cÃ¡c dá»± Ä‘oÃ¡n ngáº«u nhiÃªn, hÃ£y kiá»ƒm tra ká»¹ cÃ¡ch tÃ­nh toÃ¡n máº¥t mÃ¡t hoáº·c chá»‰ sá»‘ cá»§a báº¡n, vÃ¬ cÃ³ thá»ƒ cÃ³ má»™t lá»—i á»Ÿ Ä‘Ã³. Náº¿u báº¡n Ä‘ang sá»­ dá»¥ng má»™t sá»‘ khoáº£ng máº¥t mÃ¡t mÃ  báº¡n thÃªm vÃ o cuá»‘i, hÃ£y Ä‘áº£m báº£o ráº±ng chÃºng cÃ³ cÃ¹ng quy mÃ´.

Khi báº¡n cháº¯c cháº¯n dá»¯ liá»‡u cá»§a mÃ¬nh lÃ  hoÃ n háº£o, báº¡n cÃ³ thá»ƒ xem liá»‡u mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng huáº¥n luyá»‡n vá» nÃ³ hay khÃ´ng báº±ng má»™t bÃ i kiá»ƒm tra Ä‘Æ¡n giáº£n.

### Há»c kÄ© mÃ´ hÃ¬nh cá»§a báº¡n trong má»™t lÃ´

Viá»‡c há»c quÃ¡ nhiá»u thÆ°á»ng lÃ  Ä‘iá»u chÃºng ta cá»‘ gáº¯ng trÃ¡nh khi huáº¥n luyá»‡n, vÃ¬ nÃ³ cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh khÃ´ng há»c cÃ¡ch nháº­n ra cÃ¡c Ä‘áº·c Ä‘iá»ƒm chung ta muá»‘n mÃ  thay vÃ o Ä‘Ã³ chá»‰ lÃ  ghi nhá»› cÃ¡c máº«u huáº¥n luyá»‡n. Tuy nhiÃªn, cá»‘ gáº¯ng huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a báº¡n láº·p Ä‘i láº·p láº¡i lÃ  má»™t bÃ i kiá»ƒm tra tá»‘t Ä‘á»ƒ kiá»ƒm tra xem váº¥n Ä‘á» nhÆ° báº¡n Ä‘Ã£ Ä‘á»‹nh hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng mÃ´ hÃ¬nh mÃ  báº¡n Ä‘ang cá»‘ gáº¯ng huáº¥n luyá»‡n hay khÃ´ng. NÃ³ cÅ©ng sáº½ giÃºp báº¡n xem liá»‡u tá»‘c Ä‘á»™ há»c ban Ä‘áº§u cá»§a báº¡n cÃ³ quÃ¡ cao hay khÃ´ng.

Thá»±c hiá»‡n Ä‘iá»u nÃ y khi báº¡n Ä‘Ã£ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c `model` cá»§a mÃ¬nh thá»±c sá»± dá»… dÃ ng; chá»‰ cáº§n láº¥y má»™t loáº¡t dá»¯ liá»‡u huáº¥n luyá»‡n, sau Ä‘Ã³ coi `batch` Ä‘Ã³ lÃ  toÃ n bá»™ táº­p dá»¯ liá»‡u cá»§a báº¡n, Ä‘Æ°a nÃ³ vÃ o mÃ´ hÃ¬nh vá»›i má»™t lÆ°á»£ng epoch lá»›n:

```py
for batch in train_dataset:
    break

# Äáº£m báº£o ráº±ng báº¡n Ä‘Ã£ cháº¡y model.compile() vÃ  Ä‘áº·t trÃ¬nh tá»‘i Æ°u hÃ³a cá»§a mÃ¬nh,
# vÃ  máº¥t mÃ¡t/chá»‰ sá»‘ cá»§a báº¡n náº¿u báº¡n Ä‘ang sá»­ dá»¥ng chÃºng

model.fit(batch, epochs=20)
```

<Tip>

ğŸ’¡ Náº¿u dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a báº¡n khÃ´ng cÃ¢n báº±ng, hÃ£y Ä‘áº£m báº£o táº¡o má»™t loáº¡t dá»¯ liá»‡u huáº¥n luyá»‡n cÃ³ chá»©a táº¥t cáº£ cÃ¡c nhÃ£n.

</Tip>

MÃ´ hÃ¬nh pháº£i cÃ³ káº¿t quáº£ gáº§n nhÆ° hoÃ n háº£o trÃªn `batch`, vá»›i má»©c máº¥t mÃ¡t giáº£m nhanh vá» 0 (hoáº·c giÃ¡ trá»‹ tá»‘i thiá»ƒu cho khoáº£n máº¥t mÃ¡t báº¡n Ä‘ang sá»­ dá»¥ng).

Náº¿u báº¡n khÃ´ng quáº£n lÃ½ Ä‘á»ƒ mÃ´ hÃ¬nh cá»§a mÃ¬nh cÃ³ Ä‘Æ°á»£c káº¿t quáº£ hoÃ n háº£o nhÆ° tháº¿ nÃ y, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  cÃ³ Ä‘iá»u gÃ¬ Ä‘Ã³ khÃ´ng á»•n trong cÃ¡ch báº¡n Ä‘á»‹nh khung váº¥n Ä‘á» hoáº·c dá»¯ liá»‡u cá»§a mÃ¬nh, vÃ¬ váº­y báº¡n nÃªn kháº¯c phá»¥c Ä‘iá»u Ä‘Ã³. Chá»‰ khi báº¡n vÆ°á»£t qua Ä‘Æ°á»£c bÃ i kiá»ƒm tra overfit, báº¡n má»›i cÃ³ thá»ƒ cháº¯c cháº¯n ráº±ng mÃ´ hÃ¬nh cá»§a mÃ¬nh thá»±c sá»± cÃ³ thá»ƒ há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³.

<Tip warning={true}>

âš ï¸ Báº¡n sáº½ pháº£i táº¡o láº¡i mÃ´ hÃ¬nh cá»§a mÃ¬nh vÃ  biÃªn dá»‹ch láº¡i sau bÃ i kiá»ƒm tra overfitt nÃ y, vÃ¬ mÃ´ hÃ¬nh thu Ä‘Æ°á»£c cÃ³ thá»ƒ sáº½ khÃ´ng thá»ƒ khÃ´i phá»¥c vÃ  há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³ há»¯u Ã­ch trÃªn táº­p dá»¯ liá»‡u Ä‘áº§y Ä‘á»§ cá»§a báº¡n.

</Tip>

### KhÃ´ng Ä‘iá»u chá»‰nh báº¥t cá»© thá»© gÃ¬ cho Ä‘áº¿n khi báº¡n cÃ³ mÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘áº§u tiÃªn

Äiá»u chá»‰nh siÃªu tham sá»‘ luÃ´n Ä‘Æ°á»£c nháº¥n máº¡nh lÃ  pháº§n khÃ³ nháº¥t cá»§a há»c mÃ¡y, nhÆ°ng nÃ³ chá»‰ lÃ  bÆ°á»›c cuá»‘i cÃ¹ng giÃºp báº¡n hiá»ƒu Ä‘Æ°á»£c má»™t chÃºt vá» chá»‰ sá»‘ nÃ y. *CÃ¡c giÃ¡ trá»‹ ráº¥t khÃ´ng tá»‘t* cho cÃ¡c siÃªu tham sá»‘ cá»§a báº¡n, cháº³ng háº¡n nhÆ° sá»­ dá»¥ng tá»‘c Ä‘á»™ há»c Adam máº·c Ä‘á»‹nh lÃ  1e-3 vá»›i mÃ´ hÃ¬nh Transformer, táº¥t nhiÃªn sáº½ khiáº¿n viá»‡c há»c tiáº¿n hÃ nh ráº¥t cháº­m hoáº·c hoÃ n toÃ n bá»‹ Ä‘Ã¬nh trá»‡, nhÆ°ng háº§u háº¿t thá»i gian lÃ  cÃ¡c siÃªu tham sá»‘ "há»£p lÃ½", nhÆ° tá»‘c Ä‘á»™ há»c tá»« 1e-5 Ä‘áº¿n 5e-5, sáº½ hoáº¡t Ä‘á»™ng tá»‘t Ä‘á»ƒ mang láº¡i cho báº¡n káº¿t quáº£ tá»‘t. VÃ¬ váº­y Ä‘á»«ng khá»Ÿi cháº¡y tÃ¬m kiáº¿m siÃªu tham sá»‘ tá»‘n thá»i gian vÃ  tá»‘n kÃ©m cho Ä‘áº¿n khi báº¡n cÃ³ thá»© gÃ¬ Ä‘Ã³ vÆ°á»£t qua mÃ´ hÃ¬nh cÆ¡ sá»Ÿ mÃ  báº¡n cÃ³ trÃªn táº­p dá»¯ liá»‡u cá»§a mÃ¬nh.

Khi báº¡n Ä‘Ã£ cÃ³ má»™t mÃ´ hÃ¬nh Ä‘á»§ tá»‘t, báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u Ä‘iá»u chá»‰nh má»™t chÃºt. Äá»«ng thá»­ khá»Ÿi cháº¡y má»™t nghÃ¬n láº§n cháº¡y vá»›i cÃ¡c siÃªu tham sá»‘ khÃ¡c nhau, nhÆ°ng hÃ£y so sÃ¡nh má»™t vÃ i láº§n cháº¡y vá»›i cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau cho má»™t siÃªu thÃ´ng sá»‘ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ã½ tÆ°á»Ÿng vá» giÃ¡ trá»‹ nÃ o cÃ³ tÃ¡c Ä‘á»™ng lá»›n nháº¥t.

Náº¿u báº¡n Ä‘ang Ä‘iá»u chá»‰nh chÃ­nh mÃ´ hÃ¬nh, hÃ£y giá»¯ nÃ³ Ä‘Æ¡n giáº£n vÃ  Ä‘á»«ng thá»­ báº¥t cá»© Ä‘iá»u gÃ¬ mÃ  báº¡n khÃ´ng thá»ƒ biá»‡n minh má»™t cÃ¡ch há»£p lÃ½. LuÃ´n Ä‘áº£m báº£o ráº±ng báº¡n quay láº¡i kiá»ƒm tra overfit Ä‘á»ƒ xÃ¡c minh ráº±ng thay Ä‘á»•i cá»§a báº¡n khÃ´ng gÃ¢y ra báº¥t ká»³ háº­u quáº£ ngoÃ i Ã½ muá»‘n nÃ o.

### YÃªu cáº§u giÃºp Ä‘á»¡

Hy vá»ng ráº±ng báº¡n sáº½ tÃ¬m tháº¥y má»™t sá»‘ lá»i khuyÃªn trong pháº§n nÃ y Ä‘á»ƒ giÃºp báº¡n giáº£i quyáº¿t váº¥n Ä‘á» cá»§a mÃ¬nh, nhÆ°ng náº¿u khÃ´ng pháº£i váº­y, hÃ£y nhá»› ráº±ng báº¡n luÃ´n cÃ³ thá»ƒ há»i cá»™ng Ä‘á»“ng trÃªn [diá»…n Ä‘Ã n](https://discuss.huggingface.co/).

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ tÃ i liá»‡u bá»• sung cÃ³ thá»ƒ há»¯u Ã­ch:

- ["Reproducibility as a vehicle for engineering best practices"](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) bá»Ÿi Joel Grus
- ["Checklist for debugging neural networks"](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) bá»Ÿi Cecelia Shao
- ["How to unit test machine learning code"](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) bá»Ÿi Chase Roberts
- ["A Recipe for Training Neural Networks"](http://karpathy.github.io/2019/04/25/recipe/) bá»Ÿi Andrej Karpathy

Táº¥t nhiÃªn, khÃ´ng pháº£i má»i váº¥n Ä‘á» báº¡n gáº·p pháº£i khi huáº¥n luyá»‡n máº¡ng tháº§n kinh Ä‘á»u lÃ  lá»—i cá»§a chÃ­nh báº¡n! Náº¿u báº¡n gáº·p Ä‘iá»u gÃ¬ Ä‘Ã³ trong thÆ° viá»‡n ğŸ¤— Transformers hoáº·c ğŸ¤— Datasets cÃ³ váº» khÃ´ng á»•n, cÃ³ thá»ƒ báº¡n Ä‘Ã£ gáº·p lá»—i. Báº¡n cháº¯c cháº¯n nÃªn cho chÃºng tÃ´i biáº¿t táº¥t cáº£ vá» Ä‘iá»u Ä‘Ã³ vÃ  trong pháº§n tiáº¿p theo, chÃºng tÃ´i sáº½ giáº£i thÃ­ch chÃ­nh xÃ¡c cÃ¡ch thá»±c hiá»‡n Ä‘iá»u Ä‘Ã³.
