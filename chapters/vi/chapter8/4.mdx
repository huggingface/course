<FrameworkSwitchCourse {fw} />

# Gá»¡ lá»—i quy trÃ¬nh huáº¥n luyá»‡n

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter8/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter8/section4.ipynb"},
]} />

Báº¡n Ä‘Ã£ viáº¿t má»™t ká»‹ch báº£n tuyá»‡t Ä‘áº¹p Ä‘á»ƒ huáº¥n luyá»‡n hoáº·c tinh chá»‰nh má»™t mÃ´ hÃ¬nh trong má»™t tÃ¡c vá»¥ nháº¥t Ä‘á»‹nh, tuÃ¢n thá»§ má»™t cÃ¡ch nghiÃªm tÃºc lá»i khuyÃªn tá»« [ChÆ°Æ¡ng 7](/course/chapter7). NhÆ°ng khi báº¡n khá»Ÿi cháº¡y lá»‡nh `trainr.train()`, má»™t Ä‘iá»u kinh khá»§ng xáº£y ra: báº¡n gáº·p lá»—i ğŸ˜±! Hoáº·c tá»‡ hÆ¡n, má»i thá»© dÆ°á»ng nhÆ° á»•n vÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n cháº¡y mÃ  khÃ´ng cÃ³ lá»—i, nhÆ°ng mÃ´ hÃ¬nh káº¿t quáº£ lÃ  tá»“i tá»‡. Trong pháº§n nÃ y, chÃºng tÃ´i sáº½ chá»‰ cho báº¡n nhá»¯ng gÃ¬ báº¡n cÃ³ thá»ƒ lÃ m Ä‘á»ƒ gá»¡ lá»—i cÃ¡c loáº¡i váº¥n Ä‘á» nÃ y.

## Gá»¡ lá»—i quy trÃ¬nh huáº¥n luyá»‡n

<Youtube id="L-WSwUWde1U"/>

Váº¥n Ä‘á» khi báº¡n gáº·p lá»—i trong `trainr.train()` cÃ³ thá»ƒ Ä‘áº¿n tá»« nhiá»u nguá»“n, vÃ¬ `Trainer` thÆ°á»ng táº­p há»£p ráº¥t nhiá»u thá»© láº¡i vá»›i nhau. NÃ³ chuyá»ƒn Ä‘á»•i bá»™ dá»¯ liá»‡u thÃ nh cÃ¡c dataloader, do Ä‘Ã³, váº¥n Ä‘á» cÃ³ thá»ƒ lÃ  má»™t cÃ¡i gÃ¬ Ä‘Ã³ sai trong bá»™ dá»¯ liá»‡u cá»§a báº¡n hoáº·c má»™t sá»‘ váº¥n Ä‘á» khi cá»‘ gáº¯ng káº¿t há»£p hÃ ng loáº¡t cÃ¡c pháº§n tá»­ cá»§a bá»™ dá»¯ liá»‡u vá»›i nhau. Sau Ä‘Ã³, nÃ³ láº¥y má»™t loáº¡t dá»¯ liá»‡u vÃ  Ä‘Æ°a nÃ³ vÃ o mÃ´ hÃ¬nh, vÃ¬ váº­y váº¥n Ä‘á» cÃ³ thá»ƒ náº±m á»Ÿ mÃ£ mÃ´ hÃ¬nh. Sau Ä‘Ã³, nÃ³ tÃ­nh toÃ¡n cÃ¡c Ä‘á»™ dá»‘c vÃ  thá»±c hiá»‡n bÆ°á»›c tá»‘i Æ°u hÃ³a, vÃ¬ váº­y váº¥n Ä‘á» cÅ©ng cÃ³ thá»ƒ náº±m trong trÃ¬nh tá»‘i Æ°u hÃ³a cá»§a báº¡n. VÃ  ngay cáº£ khi má»i thá»© diá»…n ra tá»‘t Ä‘áº¹p cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n, váº«n cÃ³ thá»ƒ xáº£y ra sá»± cá»‘ trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ náº¿u cÃ³ váº¥n Ä‘á» vá»›i chá»‰ sá»‘ cá»§a báº¡n.

CÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ gá»¡ lá»—i phÃ¡t sinh trong `trainr.train()` lÃ  Ä‘i qua toÃ n pipeline nÃ y theo cÃ¡ch thá»§ cÃ´ng Ä‘á»ƒ xem má»i thá»© diá»…n ra nhÆ° tháº¿ nÃ o. Sau Ä‘Ã³, lá»—i thÆ°á»ng ráº¥t dá»… giáº£i quyáº¿t.

Äá»ƒ chá»©ng minh Ä‘iá»u nÃ y, chÃºng ta sáº½ sá»­ dá»¥ng táº­p lá»‡nh (cá»‘ gáº¯ng) tinh chá»‰nh mÃ´ hÃ¬nh DistilBERT trÃªn [táº­p dá»¯ liá»‡u MNLI](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

Náº¿u báº¡n cá»‘ gáº¯ng thá»±c thi nÃ³, báº¡n sáº½ gáº·p pháº£i má»™t lá»—i khÃ¡ khÃ³ hiá»ƒu:

```python out
'ValueError: You have to specify either input_ids or inputs_embeds'
```

### Kiá»ƒm tra dá»¯ liá»‡u cá»§a báº¡n

Äiá»u nÃ y khÃ´ng cáº§n pháº£i nÃ³i, nhÆ°ng náº¿u dá»¯ liá»‡u cá»§a báº¡n bá»‹ há»ng, `Trainer` sáº½ khÃ´ng thá»ƒ táº¡o ra cÃ¡c lÃ´ chá»© Ä‘á»«ng nÃ³i Ä‘áº¿n viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a báº¡n. VÃ¬ váº­y, Ä‘iá»u Ä‘áº§u tiÃªn, báº¡n cáº§n pháº£i xem xÃ©t nhá»¯ng gÃ¬ bÃªn trong bá»™ huáº¥n luyá»‡n cá»§a báº¡n.

Äá»ƒ trÃ¡nh máº¥t vÃ´ sá»‘ giá» Ä‘á»ƒ cá»‘ gáº¯ng sá»­a má»™t cÃ¡i gÃ¬ Ä‘Ã³ khÃ´ng pháº£i lÃ  nguá»“n gá»‘c cá»§a lá»—i, chÃºng tÃ´i khuyÃªn báº¡n nÃªn sá»­ dá»¥ng `trainr.train_dataset` Ä‘á»ƒ kiá»ƒm tra. VÃ¬ váº­y, hÃ£y lÃ m Ä‘iá»u Ä‘Ã³ á»Ÿ Ä‘Ã¢y:

```py
trainer.train_dataset[0]
```

```python out
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```

Báº¡n cÃ³ nháº­n tháº¥y Ä‘iá»u gÃ¬ Ä‘Ã³ sai khÃ´ng? Äiá»u nÃ y, cÃ¹ng vá»›i thÃ´ng bÃ¡o lá»—i vá» viá»‡c thiáº¿u `input_ids`, sáº½ khiáº¿n báº¡n nháº­n ra Ä‘Ã³ lÃ  cÃ¡c vÄƒn báº£n chá»© khÃ´ng pháº£i sá»‘ mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c. á» Ä‘Ã¢y, lá»—i ban Ä‘áº§u ráº¥t dá»… gÃ¢y hiá»ƒu nháº§m bá»Ÿi vÃ¬ `Trainer` tá»± Ä‘á»™ng loáº¡i bá» cÃ¡c cá»™t khÃ´ng khá»›p vá»›i Ä‘áº·c trÆ°ng cá»§a mÃ´ hÃ¬nh (nghÄ©a lÃ  cÃ¡c tham sá»‘ mÃ  mÃ´ hÃ¬nh mong Ä‘á»£i). Äiá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  á»Ÿ Ä‘Ã¢y, má»i thá»© ngoáº¡i trá»« nhÃ£n Ä‘á»u bá»‹ loáº¡i bá». Do Ä‘Ã³, khÃ´ng cÃ³ váº¥n Ä‘á» gÃ¬ vá»›i viá»‡c táº¡o cÃ¡c lÃ´ vÃ  sau Ä‘Ã³ gá»­i chÃºng Ä‘áº¿n mÃ´ hÃ¬nh, Ä‘iá»u nÃ y do Ä‘Ã³ phÃ n nÃ n ráº±ng nÃ³ khÃ´ng nháº­n Ä‘Æ°á»£c Ä‘áº§u vÃ o thÃ­ch há»£p.

Táº¡i sao dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½? ChÃºng ta Ä‘Ã£ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `Dataset.map()` trÃªn cÃ¡c táº­p dá»¯ liá»‡u Ä‘á»ƒ Ã¡p dá»¥ng tokenizer trÃªn má»—i máº«u. NhÆ°ng náº¿u báº¡n xem ká»¹ mÃ£, báº¡n sáº½ tháº¥y ráº±ng chÃºng ta Ä‘Ã£ máº¯c sai láº§m khi chuyá»ƒn cÃ¡c bá»™ huáº¥n luyá»‡n vÃ  kiá»ƒm Ä‘á»‹nh cho `Trainer`. Thay vÃ¬ sá»­ dá»¥ng `tokenized_datasets` á»Ÿ Ä‘Ã¢y, chÃºng ta Ä‘Ã£ sá»­ dá»¥ng `raw_datasets` ğŸ¤¦. VÃ¬ váº­y, hÃ£y cÃ¹ng sá»­a chá»¯a Ä‘iá»u nÃ y!

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

MÃ£ má»›i nÃ y bÃ¢y giá» sáº½ Ä‘Æ°a ra má»™t lá»—i khÃ¡c (cÃ³ tiáº¿n triá»ƒn!):

```python out
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```

NhÃ¬n vÃ o dáº¥u truy váº¿t, chÃºng ta cÃ³ thá»ƒ tháº¥y lá»—i xáº£y ra trong bÆ°á»›c Ä‘á»‘i chiáº¿u dá»¯ liá»‡u:

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

VÃ¬ váº­y, chÃºng ta nÃªn chuyá»ƒn sang Ä‘iá»u Ä‘Ã³. Tuy nhiÃªn, trÆ°á»›c khi thá»±c hiá»‡n, chÃºng ta hÃ£y hoÃ n thÃ nh viá»‡c kiá»ƒm tra dá»¯ liá»‡u cá»§a mÃ¬nh, Ä‘á»ƒ cháº¯c cháº¯n ráº±ng nÃ³ chÃ­nh xÃ¡c 100%.

Má»™t Ä‘iá»u báº¡n luÃ´n nÃªn lÃ m khi gá»¡ lá»—i má»™t phiÃªn huáº¥n luyá»‡n lÃ  xem xÃ©t cÃ¡c Ä‘áº§u vÃ o Ä‘Æ°á»£c giáº£i mÃ£ cá»§a mÃ´ hÃ¬nh cá»§a báº¡n. ChÃºng ta khÃ´ng thá»ƒ hiá»ƒu Ä‘Æ°á»£c nhá»¯ng con sá»‘ mÃ  chÃºng ta cung cáº¥p trá»±c tiáº¿p cho nÃ³, vÃ¬ váº­y chÃºng ta nÃªn xem nhá»¯ng con sá»‘ Ä‘Ã³ Ä‘áº¡i diá»‡n cho Ä‘iá»u gÃ¬. VÃ­ dá»¥: trong thá»‹ giÃ¡c mÃ¡y tÃ­nh, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  nhÃ¬n vÃ o hÃ¬nh áº£nh Ä‘Æ°á»£c giáº£i mÃ£ cá»§a cÃ¡c pixel báº¡n chuyá»ƒn qua, trong lá»i nÃ³i, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  nghe cÃ¡c máº«u Ã¢m thanh Ä‘Æ°á»£c giáº£i mÃ£ vÃ  Ä‘á»‘i vá»›i vÃ­ dá»¥ NLP cá»§a chÃºng ta á»Ÿ Ä‘Ã¢y, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  sá»­ dá»¥ng trÃ¬nh tokenizer Ä‘á»ƒ giáº£i mÃ£ Ä‘áº§u vÃ o:

```py
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python out
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

VÃ¬ váº­y, Ä‘iá»u Ä‘Ã³ cÃ³ váº» chÃ­nh xÃ¡c. Báº¡n nÃªn lÃ m Ä‘iá»u nÃ y cho táº¥t cáº£ cÃ¡c phÃ­m trong Ä‘áº§u vÃ o:

```py
trainer.train_dataset[0].keys()
```

```python out
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

LÆ°u Ã½ ráº±ng cÃ¡c khÃ³a khÃ´ng tÆ°Æ¡ng á»©ng vá»›i Ä‘áº§u vÃ o Ä‘Æ°á»£c mÃ´ hÃ¬nh cháº¥p nháº­n sáº½ tá»± Ä‘á»™ng bá»‹ loáº¡i bá», vÃ¬ váº­y á»Ÿ Ä‘Ã¢y chÃºng tÃ´i sáº½ chá»‰ giá»¯ láº¡i `input_ids`, `attention_mask`, vÃ  `label` (sáº½ Ä‘Æ°á»£c Ä‘á»•i tÃªn thÃ nh `labels`). Äá»ƒ kiá»ƒm tra ká»¹ mÃ´ hÃ¬nh, báº¡n cÃ³ thá»ƒ in loáº¡i mÃ´ hÃ¬nh cá»§a mÃ¬nh, sau Ä‘Ã³ kiá»ƒm tra tÃ i liá»‡u cá»§a nÃ³:

```py
type(trainer.model)
```

```python out
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

VÃ¬ váº­y, trong trÆ°á»ng há»£p cá»§a mÃ¬nh, chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra cÃ¡c tham sá»‘ Ä‘Æ°á»£c cháº¥p nháº­n trÃªn [trang nÃ y](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification). `Trainer` cÅ©ng sáº½ ghi láº¡i cÃ¡c cá»™t mÃ  nÃ³ Ä‘ang loáº¡i bá».

ChÃºng ta Ä‘Ã£ kiá»ƒm tra xem cÃ¡c ID Ä‘áº§u vÃ o cÃ³ chÃ­nh xÃ¡c hay khÃ´ng báº±ng cÃ¡ch giáº£i mÃ£ chÃºng. Tiáº¿p theo lÃ  `attention_mask`:

```py
trainer.train_dataset[0]["attention_mask"]
```

```python out
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

VÃ¬ chÃºng ta khÃ´ng Ã¡p dá»¥ng Ä‘á»‡m trong quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ cá»§a mÃ¬nh, Ä‘iá»u nÃ y cÃ³ váº» hoÃ n toÃ n tá»± nhiÃªn. Äá»ƒ Ä‘áº£m báº£o khÃ´ng cÃ³ váº¥n Ä‘á» gÃ¬ vá»›i attention mask Ä‘Ã³, hÃ£y kiá»ƒm tra xem nÃ³ cÃ³ cÃ¹ng Ä‘á»™ dÃ i vá»›i ID Ä‘áº§u vÃ o cá»§a chÃºng ta khÃ´ng:

```py
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python out
True
```

Tá»‘t Ä‘áº¥y! Cuá»‘i cÃ¹ng, hÃ£y kiá»ƒm tra nhÃ£n cá»§a mÃ¬nh:

```py
trainer.train_dataset[0]["label"]
```

```python out
1
```

Giá»‘ng nhÆ° cÃ¡c ID Ä‘áº§u vÃ o, Ä‘Ã¢y lÃ  má»™t con sá»‘ khÃ´ng thá»±c sá»± cÃ³ Ã½ nghÄ©a. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã¢y, Ã¡nh xáº¡ giá»¯a cÃ¡c sá»‘ nguyÃªn vÃ  tÃªn nhÃ£n Ä‘Æ°á»£c lÆ°u trá»¯ bÃªn trong thuá»™c tÃ­nh `names` cá»§a *Ä‘áº·c trÆ°ng* tÆ°Æ¡ng á»©ng cá»§a táº­p dá»¯ liá»‡u:

```py
trainer.train_dataset.features["label"].names
```

```python out
['entailment', 'neutral', 'contradiction']
```

VÃ¬ váº­y, `1` cÃ³ nghÄ©a lÃ  `neutral`, cÃ³ nghÄ©a lÃ  hai cÃ¢u chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ trÃªn khÃ´ng mÃ¢u thuáº«n vá»›i nhau vÃ  cÃ¢u Ä‘áº§u tiÃªn khÃ´ng bao hÃ m cÃ¢u thá»© hai. Äiá»u Ä‘Ã³ cÃ³ váº» Ä‘Ãºng!

ChÃºng ta khÃ´ng cÃ³ token ID á»Ÿ Ä‘Ã¢y, vÃ¬ DistilBERT khÃ´ng mong Ä‘á»£i chÃºng; náº¿u báº¡n cÃ³ má»™t sá»‘ trong mÃ´ hÃ¬nh cá»§a mÃ¬nh, báº¡n cÅ©ng nÃªn Ä‘áº£m báº£o ráº±ng chÃºng khá»›p Ä‘Ãºng vá»›i vá»‹ trÃ­ cá»§a cÃ¢u Ä‘áº§u tiÃªn vÃ  cÃ¢u thá»© hai trong Ä‘áº§u vÃ o.

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** Kiá»ƒm tra xem má»i thá»© cÃ³ chÃ­nh xÃ¡c khÃ´ng vá»›i pháº§n tá»­ thá»© hai cá»§a táº­p dá»¯ liá»‡u huáº¥n luyá»‡n.

</Tip>

ChÃºng ta chá»‰ thá»±c hiá»‡n kiá»ƒm tra táº­p huáº¥n luyá»‡n á»Ÿ Ä‘Ã¢y, nhÆ°ng táº¥t nhiÃªn báº¡n nÃªn kiá»ƒm tra ká»¹ cÃ¡c táº­p kiá»ƒm Ä‘á»‹nh vÃ  kiá»ƒm tra theo cÃ¹ng má»™t cÃ¡ch.

BÃ¢y giá» chÃºng ta biáº¿t bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh trÃ´ng á»•n, Ä‘Ã£ Ä‘áº¿n lÃºc kiá»ƒm tra bÆ°á»›c tiáº¿p theo cá»§a quy trÃ¬nh huáº¥n luyá»‡n.

### Tá»« bá»™ dá»¯ liá»‡u thÃ nh dataloader

Äiá»u tiáº¿p theo cÃ³ thá»ƒ xáº£y ra sai sÃ³t trong quy trÃ¬nh huáº¥n luyá»‡n lÃ  khi `Trainer` cá»‘ gáº¯ng táº¡o cÃ¡c lÃ´ tá»« táº­p huáº¥n luyá»‡n hoáº·c kiá»ƒm Ä‘á»‹nh. Khi báº¡n cháº¯c cháº¯n ráº±ng táº­p dá»¯ liá»‡u cá»§a `Trainer` lÃ  chÃ­nh xÃ¡c, báº¡n cÃ³ thá»ƒ thá»­ táº¡o má»™t loáº¡t theo cÃ¡ch thá»§ cÃ´ng báº±ng cÃ¡ch thá»±c hiá»‡n nhÆ° sau (thay tháº¿ `train` báº±ng `eval` cho dataloader kiá»ƒm Ä‘á»‹nh):

```py
for batch in trainer.get_train_dataloader():
    break
```

MÃ£ nÃ y táº¡o ra dataloader huáº¥n luyá»‡n, sau Ä‘Ã³ láº·p qua nÃ³, dá»«ng láº¡i á»Ÿ láº§n láº·p Ä‘áº§u tiÃªn. Náº¿u mÃ£ thá»±c thi mÃ  khÃ´ng cÃ³ lá»—i, báº¡n cÃ³ lÃ´ huáº¥n luyá»‡n Ä‘áº§u tiÃªn mÃ  báº¡n cÃ³ thá»ƒ kiá»ƒm tra vÃ  náº¿u mÃ£ lá»—i xáº£y ra, báº¡n biáº¿t cháº¯c cháº¯n váº¥n Ä‘á» náº±m trong dataloader, nhÆ° trÆ°á»ng há»£p á»Ÿ Ä‘Ã¢y:

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

Viá»‡c kiá»ƒm tra khung cuá»‘i cÃ¹ng cá»§a quÃ¡ trÃ¬nh truy xuáº¥t sáº½ Ä‘á»§ Ä‘á»ƒ cung cáº¥p cho báº¡n manh má»‘i, nhÆ°ng hÃ£y tÃ¬m hiá»ƒu ká»¹ hÆ¡n má»™t chÃºt. Háº§u háº¿t cÃ¡c váº¥n Ä‘á» trong quÃ¡ trÃ¬nh táº¡o lÃ´ Ä‘á»u phÃ¡t sinh do viá»‡c Ä‘á»‘i chiáº¿u cÃ¡c vÃ­ dá»¥ thÃ nh má»™t lÃ´ duy nháº¥t, vÃ¬ váº­y, Ä‘iá»u Ä‘áº§u tiÃªn cáº§n kiá»ƒm tra khi nghi ngá» lÃ  `collate_fn` mÃ  ` DataLoader` cá»§a báº¡n Ä‘ang sá»­ dá»¥ng:

```py
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python out
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

VÃ¬ váº­y, Ä‘Ã¢y lÃ  `default_data_collator`, nhÆ°ng Ä‘Ã³ khÃ´ng pháº£i lÃ  nhá»¯ng gÃ¬ chÃºng ta muá»‘n trong trÆ°á»ng há»£p nÃ y. ChÃºng ta muá»‘n Ä‘Æ°a cÃ¡c vÃ­ dá»¥ cá»§a mÃ¬nh vÃ o cÃ¢u dÃ i nháº¥t trong lÃ´, Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi trÃ¬nh Ä‘á»‘i chiáº¿u `DataCollatorWithPadding`. VÃ  trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c cho lÃ  Ä‘Æ°á»£c sá»­ dá»¥ng theo máº·c Ä‘á»‹nh bá»Ÿi `Trainer`, váº­y táº¡i sao nÃ³ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ Ä‘Ã¢y?

CÃ¢u tráº£ lá»i lÃ  vÃ¬ chÃºng ta Ä‘Ã£ khÃ´ng chuyá»ƒn `tokenizer` cho `Trainer`, vÃ¬ váº­y nÃ³ khÃ´ng thá»ƒ táº¡o `DataCollatorWithPadding` mÃ  chÃºng ta muá»‘n. Trong thá»±c táº¿, báº¡n Ä‘á»«ng bao giá» ngáº§n ngáº¡i chuyá»ƒn má»™t cÃ¡ch rÃµ rÃ ng bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u mÃ  báº¡n muá»‘n sá»­ dá»¥ng, Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng báº¡n trÃ¡nh Ä‘Æ°á»£c nhá»¯ng loáº¡i lá»—i nÃ y. HÃ£y Ä‘iá»u chá»‰nh mÃ£ cá»§a chÃºng ta Ä‘á»ƒ thá»±c hiá»‡n chÃ­nh xÃ¡c Ä‘iá»u Ä‘Ã³:

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

Tin tá»‘t? ChÃºng ta khÃ´ng gáº·p lá»—i nhÆ° trÆ°á»›c ná»¯a, Ä‘Ã³ cháº¯c cháº¯n lÃ  sá»± tiáº¿n bá»™. CÃ¡c tin xáº¥u? Thay vÃ o Ä‘Ã³, chÃºng ta nháº­n Ä‘Æ°á»£c má»™t lá»—i CUDA khÃ©t tiáº¿ng:

```python out
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```

Äiá»u nÃ y tháº­t tá»‡ vÃ¬ lá»—i CUDA nÃ³i chung ráº¥t khÃ³ gá»¡ lá»—i. ChÃºng ta sáº½ xem trong má»™t phÃºt ná»¯a cÃ¡ch giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, nhÆ°ng trÆ°á»›c tiÃªn hÃ£y káº¿t thÃºc phÃ¢n tÃ­ch cá»§a chÃºng ta vá» táº¡o lÃ´.

Náº¿u báº¡n cháº¯c cháº¯n trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u cá»§a mÃ¬nh lÃ  Ä‘Ãºng, báº¡n nÃªn thá»­ Ã¡p dá»¥ng nÃ³ trÃªn má»™t vÃ i máº«u cá»§a táº­p dá»¯ liá»‡u cá»§a mÃ¬nh:

```py
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```

MÃ£ nÃ y sáº½ khÃ´ng thÃ nh cÃ´ng vÃ¬ `train_dataset` chá»©a cÃ¡c cá»™t chuá»—i mÃ  `Trainer` thÆ°á»ng loáº¡i bá». Báº¡n cÃ³ thá»ƒ xÃ³a chÃºng theo cÃ¡ch thá»§ cÃ´ng hoáº·c náº¿u báº¡n muá»‘n sao chÃ©p chÃ­nh xÃ¡c nhá»¯ng gÃ¬ mÃ  `Trainer` Ä‘ang lÃ m á»Ÿ háº­u trÆ°á»ng, báº¡n cÃ³ thá»ƒ gá»i phÆ°Æ¡ng thá»©c riÃªng  `Trainer._remove_unused_columns()` Ä‘á»ƒ thá»±c hiá»‡n Ä‘iá»u Ä‘Ã³:

```py
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```

Sau Ä‘Ã³, báº¡n sáº½ cÃ³ thá»ƒ gá»¡ lá»—i theo cÃ¡ch thá»§ cÃ´ng nhá»¯ng gÃ¬ xáº£y ra bÃªn trong bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u náº¿u lá»—i váº«n tiáº¿p diá»…n.

BÃ¢y giá» chÃºng ta Ä‘Ã£ gá»¡ lá»—i quy trÃ¬nh táº¡o lÃ´, Ä‘Ã£ Ä‘áº¿n lÃºc chuyá»ƒn qua mÃ´ hÃ¬nh!

### Xem qua mÃ´ hÃ¬nh

Báº¡n sáº½ cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c má»™t lÃ´ báº±ng cÃ¡ch thá»±c hiá»‡n lá»‡nh sau:

```py
for batch in trainer.get_train_dataloader():
    break
```

Náº¿u báº¡n Ä‘ang cháº¡y mÃ£ nÃ y trong notebook, báº¡n cÃ³ thá»ƒ gáº·p lá»—i CUDA tÆ°Æ¡ng tá»± nhÆ° lá»—i Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, trong trÆ°á»ng há»£p Ä‘Ã³, báº¡n cáº§n khá»Ÿi Ä‘á»™ng láº¡i notebook cá»§a mÃ¬nh vÃ  thá»±c hiá»‡n láº¡i Ä‘oáº¡n mÃ£ cuá»‘i cÃ¹ng mÃ  khÃ´ng cÃ³ dÃ²ng `trainer.train()`. ÄÃ³ lÃ  Ä‘iá»u khÃ³ chá»‹u thá»© hai vá» lá»—i CUDA: chÃºng phÃ¡ vá»¡ kernel cá»§a báº¡n má»™t cÃ¡ch khÃ´ng thá»ƒ kháº¯c phá»¥c Ä‘Æ°á»£c. Äiá»u khÃ³ chá»‹u nháº¥t vá» chÃºng lÃ  thá»±c táº¿ lÃ  chÃºng ráº¥t khÃ³ Ä‘á»ƒ gá»¡ lá»—i.

Táº¡i sao váº­y? NÃ³ liÃªn quan Ä‘áº¿n cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a GPU. ChÃºng cá»±c ká»³ hiá»‡u quáº£ trong viá»‡c thá»±c hiá»‡n song song nhiá»u thao tÃ¡c, nhÆ°ng háº¡n cháº¿ lÃ  khi má»™t trong cÃ¡c lá»‡nh Ä‘Ã³ dáº«n Ä‘áº¿n lá»—i, báº¡n sáº½ khÃ´ng biáº¿t ngay láº­p tá»©c. Chá»‰ khi chÆ°Æ¡ng trÃ¬nh gá»i Ä‘á»“ng bá»™ hÃ³a nhiá»u quy trÃ¬nh trÃªn GPU thÃ¬ nÃ³ má»›i nháº­n ra cÃ³ gÃ¬ Ä‘Ã³ khÃ´ng á»•n, vÃ¬ váº­y lá»—i thá»±c sá»± Ä‘Æ°á»£c phÃ¡t sinh á»Ÿ má»™t nÆ¡i khÃ´ng liÃªn quan gÃ¬ Ä‘áº¿n nhá»¯ng gÃ¬ Ä‘Ã£ táº¡o ra nÃ³. VÃ­ dá»¥, náº¿u chÃºng ta xem láº¡i láº§n truy xuáº¥t trÆ°á»›c cá»§a mÃ¬nh, lá»—i Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t sinh trong quÃ¡ trÃ¬nh truyá»n ngÆ°á»£c, nhÆ°ng chÃºng ta sáº½ tháº¥y trong má»™t phÃºt ráº±ng nÃ³ thá»±c sá»± báº¯t nguá»“n tá»« má»™t cÃ¡i gÃ¬ Ä‘Ã³ trong truyá»n tháº³ng.

Váº­y lÃ m cÃ¡ch nÃ o Ä‘á»ƒ gá»¡ nhá»¯ng lá»—i Ä‘Ã³? CÃ¢u tráº£ lá»i ráº¥t dá»… dÃ ng: chÃºng tÃ´i khÃ´ng. Trá»« khi lá»—i CUDA cá»§a báº¡n lÃ  lá»—i háº¿t bá»™ nhá»› (cÃ³ nghÄ©a lÃ  khÃ´ng cÃ³ Ä‘á»§ bá»™ nhá»› trong GPU cá»§a báº¡n), báº¡n nÃªn quay láº¡i CPU Ä‘á»ƒ gá»¡ lá»—i.

Äá»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y trong trÆ°á»ng há»£p cá»§a mÃ¬nh, chÃºng ta chá»‰ cáº§n Ä‘áº·t mÃ´ hÃ¬nh trá»Ÿ láº¡i CPU vÃ  gá»i nÃ³ vÃ o lÃ´ cá»§a mÃ¬nh - lÃ´ Ä‘Æ°á»£c tráº£ vá» bá»Ÿi `DataLoader` váº«n chÆ°a Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n GPU:

```python
outputs = trainer.model.cpu()(**batch)
```

```python out
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```

VÃ¬ váº­y, bá»©c tranh ngÃ y cÃ ng rÃµ rÃ ng. Thay vÃ¬ gáº·p lá»—i CUDA, bÃ¢y giá» chÃºng ta cÃ³ `IndexError` trong tÃ­nh toÃ¡n máº¥t mÃ¡t (vÃ¬ váº­y khÃ´ng liÃªn quan gÃ¬ Ä‘áº¿n lan truyá»n ngÆ°á»£c, nhÆ° ta Ä‘Ã£ nÃ³i trÆ°á»›c Ä‘Ã³). ChÃ­nh xÃ¡c hÆ¡n, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng nhÃ£n 2 táº¡o ra lá»—i, vÃ¬ váº­y Ä‘Ã¢y lÃ  thá»i Ä‘iá»ƒm ráº¥t tá»‘t Ä‘á»ƒ kiá»ƒm tra sá»‘ lÆ°á»£ng nhÃ£n cá»§a mÃ´ hÃ¬nh cá»§a ta:

```python
trainer.model.config.num_labels
```

```python out
2
```

Vá»›i hai nhÃ£n, chá»‰ cÃ³ 0 vÃ  1 Ä‘Æ°á»£c phÃ©p lÃ m nhÃ£n, nhÆ°ng theo thÃ´ng bÃ¡o lá»—i, chÃºng tÃ´i nháº­n Ä‘Æ°á»£c 2. Nháº­n Ä‘Æ°á»£c 2 thá»±c ra lÃ  bÃ¬nh thÆ°á»ng: náº¿u chÃºng ta nhá»› tÃªn nhÃ£n mÃ  chÃºng ta Ä‘Ã£ trÃ­ch xuáº¥t trÆ°á»›c Ä‘Ã³, cÃ³ ba, vÃ¬ váº­y chÃºng ta cÃ³ chá»‰ sá»‘ 0 , 1 vÃ  2 trong táº­p dá»¯ liá»‡u cá»§a mÃ¬nh. Váº¥n Ä‘á» lÃ  chÃºng ta Ä‘Ã£ khÃ´ng nÃ³i Ä‘iá»u Ä‘Ã³ vá»›i mÃ´ hÃ¬nh cá»§a mÃ¬nh, mÃ´ hÃ¬nh nÃ y láº½ ra pháº£i Ä‘Æ°á»£c táº¡o vá»›i ba nhÃ£n. VÃ¬ váº­y, chÃºng ta hÃ£y kháº¯c phá»¥c Ä‘iá»u Ä‘Ã³!

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

ChÃºng ta chÆ°a bao gá»“m dÃ²ng `trainer.train()`, Ä‘á»ƒ dÃ nh thá»i gian kiá»ƒm tra xem má»i thá»© cÃ³ á»•n khÃ´ng. Náº¿u chÃºng ta yÃªu cáº§u má»™t lÃ´ vÃ  chuyá»ƒn nÃ³ vÃ o mÃ´ hÃ¬nh cá»§a mÃ¬nh, nÃ³ hiá»‡n hoáº¡t Ä‘á»™ng mÃ  khÃ´ng cÃ³ lá»—i!

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

BÆ°á»›c tiáº¿p theo lÃ  quay láº¡i GPU vÃ  kiá»ƒm tra xem má»i thá»© váº«n hoáº¡t Ä‘á»™ng khÃ´ng:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

Náº¿u báº¡n váº«n gáº·p lá»—i, hÃ£y Ä‘áº£m báº£o ráº±ng báº¡n khá»Ÿi Ä‘á»™ng láº¡i notebook cá»§a mÃ¬nh vÃ  chá»‰ thá»±c thi phiÃªn báº£n cuá»‘i cÃ¹ng cá»§a táº­p lá»‡nh.

### Thá»±c hiá»‡n má»™t bÆ°á»›c tá»‘i Æ°u hÃ³a

BÃ¢y giá» ta biáº¿t ráº±ng chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng cÃ¡c lÃ´ thá»±c sá»± Ä‘i qua mÃ´ hÃ¬nh, chÃºng ta Ä‘Ã£ sáºµn sÃ ng cho bÆ°á»›c tiáº¿p theo cá»§a quy trÃ¬nh huáº¥n luyá»‡n: tÃ­nh toÃ¡n Ä‘á»™ dá»‘c vÃ  thá»±c hiá»‡n bÆ°á»›c tá»‘i Æ°u hÃ³a.

Pháº§n Ä‘áº§u tiÃªn chá»‰ lÃ  váº¥n Ä‘á» gá»i phÆ°Æ¡ng thá»©c `backward()` khi tÃ­nh máº¥t mÃ¡t:

```py
loss = outputs.loss
loss.backward()
```

Ráº¥t hiáº¿m khi gáº·p lá»—i á»Ÿ giai Ä‘oáº¡n nÃ y, nhÆ°ng náº¿u báº¡n gáº·p lá»—i, hÃ£y Ä‘áº£m báº£o quay láº¡i CPU Ä‘á»ƒ nháº­n Ä‘Æ°á»£c thÃ´ng bÃ¡o lá»—i há»¯u Ã­ch.

To perform the optimization step, we just need to create the `optimizer` and call its `step()` method:

```py
trainer.create_optimizer()
trainer.optimizer.step()
```

Má»™t láº§n ná»¯a, náº¿u báº¡n Ä‘ang sá»­ dá»¥ng trÃ¬nh tá»‘i Æ°u hÃ³a máº·c Ä‘á»‹nh trong `Trainer`, báº¡n sáº½ khÃ´ng gáº·p lá»—i á»Ÿ giai Ä‘oáº¡n nÃ y, nhÆ°ng náº¿u báº¡n cÃ³ trÃ¬nh tá»‘i Æ°u hÃ³a tÃ¹y chá»‰nh, cÃ³ thá»ƒ cÃ³ má»™t sá»‘ váº¥n Ä‘á» cáº§n gá»¡ lá»—i á»Ÿ Ä‘Ã¢y. Äá»«ng quÃªn quay láº¡i CPU náº¿u báº¡n gáº·p lá»—i CUDA láº¡ á»Ÿ giai Ä‘oáº¡n nÃ y. NÃ³i vá» lá»—i CUDA, trÆ°á»›c Ä‘Ã³ chÃºng ta Ä‘Ã£ Ä‘á» cáº­p Ä‘áº¿n má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t. BÃ¢y giá» chÃºng ta hÃ£y xem xÃ©t Ä‘iá»u Ä‘Ã³.

### Xá»­ lÃ½ lá»—i háº¿t bá»™ nhá»› CUDA

Báº¥t cá»© khi nÃ o báº¡n nháº­n Ä‘Æ°á»£c thÃ´ng bÃ¡o lá»—i báº¯t Ä‘áº§u báº±ng `RuntimeError: CUDA out of memory`, Ä‘iá»u nÃ y cho biáº¿t báº¡n Ä‘Ã£ háº¿t bá»™ nhá»› GPU. Äiá»u nÃ y khÃ´ng Ä‘Æ°á»£c liÃªn káº¿t trá»±c tiáº¿p vá»›i mÃ£ cá»§a báº¡n vÃ  nÃ³ cÃ³ thá»ƒ xáº£y ra vá»›i má»™t táº­p lá»‡nh cháº¡y hoÃ n toÃ n tá»‘t. Lá»—i nÃ y cÃ³ nghÄ©a lÃ  báº¡n Ä‘Ã£ cá»‘ gáº¯ng Ä‘Æ°a quÃ¡ nhiá»u thá»© vÃ o bá»™ nhá»› trong cá»§a GPU vÃ  dáº«n Ä‘áº¿n lá»—i. Giá»‘ng nhÆ° vá»›i cÃ¡c lá»—i CUDA khÃ¡c, báº¡n sáº½ cáº§n khá»Ÿi Ä‘á»™ng láº¡i kernel cá»§a mÃ¬nh Ä‘á»ƒ á»Ÿ vá»‹ trÃ­ mÃ  báº¡n cÃ³ thá»ƒ cháº¡y láº¡i quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a mÃ¬nh.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, báº¡n chá»‰ cáº§n sá»­ dá»¥ng Ã­t dung lÆ°á»£ng GPU hÆ¡n - Ä‘iá»u mÃ  nÃ³i thÃ¬ dá»… hÆ¡n lÃ m. TrÆ°á»›c tiÃªn, hÃ£y Ä‘áº£m báº£o ráº±ng báº¡n khÃ´ng cÃ³ hai mÃ´ hÃ¬nh GPU trÃªn cÃ¹ng má»™t lÃºc (táº¥t nhiÃªn lÃ  trá»« khi Ä‘Ã³ lÃ  yÃªu cáº§u cho váº¥n Ä‘á» cá»§a báº¡n). Sau Ä‘Ã³, báº¡n cÃ³ thá»ƒ nÃªn giáº£m kÃ­ch thÆ°á»›c lÃ´ cá»§a mÃ¬nh, vÃ¬ nÃ³ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n kÃ­ch thÆ°á»›c cá»§a táº¥t cáº£ cÃ¡c Ä‘áº§u ra trung gian cá»§a mÃ´ hÃ¬nh vÃ  Ä‘á»™ dá»‘c cá»§a chÃºng. Náº¿u sá»± cá»‘ váº«n tiáº¿p diá»…n, hÃ£y xem xÃ©t sá»­ dá»¥ng phiÃªn báº£n mÃ´ hÃ¬nh nhá» hÆ¡n cá»§a báº¡n.

<Tip>

Trong pháº§n tiáº¿p theo cá»§a khÃ³a há»c, chÃºng ta sáº½ xem xÃ©t cÃ¡c ká»¹ thuáº­t nÃ¢ng cao hÆ¡n cÃ³ thá»ƒ giÃºp báº¡n giáº£m dung lÆ°á»£ng bá»™ nhá»› vÃ  cho phÃ©p báº¡n tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh lá»›n nháº¥t.

</Tip>

### ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh

BÃ¢y giá» chÃºng tÃ´i Ä‘Ã£ giáº£i quyáº¿t táº¥t cáº£ cÃ¡c váº¥n Ä‘á» vá»›i mÃ£ cá»§a mÃ¬nh, má»i thá»© Ä‘á»u hoÃ n háº£o vÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n sáº½ diá»…n ra suÃ´n sáº», pháº£i khÃ´ng? KhÃ´ng quÃ¡ nhanh! Náº¿u báº¡n cháº¡y lá»‡nh `trainer.train()`, lÃºc Ä‘áº§u má»i thá»© sáº½ á»•n, nhÆ°ng sau má»™t thá»i gian, báº¡n sáº½ nháº­n Ä‘Æ°á»£c nhá»¯ng Ä‘iá»u sau:

```py
# QuÃ¡ trÃ¬nh nÃ y sáº½ máº¥t nhiá»u thá»i gian vÃ  xáº£y ra lá»—i, vÃ¬ váº­y báº¡n khÃ´ng nÃªn cháº¡y Ã´ nÃ y
trainer.train()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

Báº¡n sáº½ nháº­n ra lá»—i nÃ y xuáº¥t hiá»‡n trong giai Ä‘oáº¡n kiá»ƒm Ä‘á»‹nh, vÃ¬ váº­y Ä‘Ã¢y lÃ  Ä‘iá»u cuá»‘i cÃ¹ng chÃºng tÃ´i sáº½ cáº§n gá»¡ lá»—i.

Báº¡n cÃ³ thá»ƒ cháº¡y vÃ²ng láº·p kiá»ƒm Ä‘á»‹nh cá»§a `Trainer` má»™t cÃ¡ch Ä‘á»™c láº­p Ä‘á»ƒ hÃ¬nh thÃ nh khÃ³a huáº¥n luyá»‡n nhÆ° sau:

```py
trainer.evaluate()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

<Tip>

ğŸ’¡ Báº¡n pháº£i luÃ´n Ä‘áº£m báº£o ráº±ng mÃ¬nh cÃ³ thá»ƒ cháº¡y `trainr.evaluate()` trÆ°á»›c khi khá»Ÿi cháº¡y `trainer.train()`, Ä‘á»ƒ trÃ¡nh lÃ£ng phÃ­ nhiá»u tÃ i nguyÃªn mÃ¡y tÃ­nh trÆ°á»›c khi gáº·p lá»—i.

</Tip>

TrÆ°á»›c khi cá»‘ gáº¯ng gá»¡ lá»—i má»™t váº¥n Ä‘á» trong vÃ²ng kiá»ƒm Ä‘á»‹nh, trÆ°á»›c tiÃªn báº¡n nÃªn Ä‘áº£m báº£o ráº±ng báº¡n Ä‘Ã£ xem xÃ©t dá»¯ liá»‡u, cÃ³ thá»ƒ táº¡o má»™t lÃ´ Ä‘Ãºng cÃ¡ch vÃ  cÃ³ thá»ƒ cháº¡y mÃ´ hÃ¬nh cá»§a báº¡n trÃªn Ä‘Ã³. ChÃºng ta Ä‘Ã£ hoÃ n thÃ nh táº¥t cáº£ cÃ¡c bÆ°á»›c Ä‘Ã³, vÃ¬ váº­y mÃ£ sau cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c thi mÃ  khÃ´ng cÃ³ lá»—i:

```py
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

Lá»—i xuáº¥t hiá»‡n sau Ä‘Ã³, vÃ o cuá»‘i giai Ä‘oáº¡n Ä‘Ã¡nh giÃ¡ vÃ  náº¿u chÃºng ta xem láº¡i báº£n ghi láº¡i, chÃºng ta tháº¥y Ä‘iá»u nÃ y:

```python trace
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

Äiá»u nÃ y cho chÃºng tÃ´i biáº¿t ráº±ng lá»—i báº¯t nguá»“n tá»« mÃ´-Ä‘un `datasets/metric.py` - vÃ¬ váº­y Ä‘Ã¢y lÃ  sá»± cá»‘ vá»›i hÃ m `compute_metrics()` cá»§a mÃ¬nh. NÃ³ cáº§n má»™t bá»™ dá»¯ liá»‡u vá»›i cÃ¡c logits vÃ  cÃ¡c nhÃ£n dÆ°á»›i dáº¡ng máº£ng NumPy, vÃ¬ váº­y chÃºng ta hÃ£y thá»­ cung cáº¥p cho nÃ³ ráº±ng:

```py
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

ChÃºng ta nháº­n Ä‘Æ°á»£c cÃ¹ng má»™t lá»—i, vÃ¬ váº­y váº¥n Ä‘á» cháº¯c cháº¯n náº±m á»Ÿ hÃ m Ä‘Ã³. Náº¿u chÃºng ta nhÃ¬n láº¡i mÃ£ cá»§a nÃ³, chÃºng ta tháº¥y nÃ³ chá»‰ chuyá»ƒn tiáº¿p cÃ¡c  `predictions` vÃ  `labels`  Ä‘áº¿n `metric.compute()`. Váº­y cÃ³ váº¥n Ä‘á» gÃ¬ vá»›i phÆ°Æ¡ng phÃ¡p Ä‘Ã³ khÃ´ng? KhÃ´ng háº³n váº­y. ChÃºng ta hÃ£y xem nhanh cÃ¡c hÃ¬nh dáº¡ng:

```py
predictions.shape, labels.shape
```

```python out
((8, 3), (8,))
```

CÃ¡c dá»± Ä‘oÃ¡n cá»§a chÃºng tÃ´i váº«n lÃ  logit, khÃ´ng pháº£i dá»± Ä‘oÃ¡n thá»±c táº¿, Ä‘Ã³ lÃ  lÃ½ do táº¡i sao sá»‘ liá»‡u tráº£ vá» lá»—i (hÆ¡i tá»‘i nghÄ©a) nÃ y. Viá»‡c sá»­a chá»¯a khÃ¡ dá»… dÃ ng; chÃºng ta chá»‰ cáº§n thÃªm má»™t argmax trong hÃ m `compute_metrics()`:

```py
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))
```

```python out
{'accuracy': 0.625}
```

BÃ¢y giá» lá»—i cá»§a chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c sá»­a chá»¯a! ÄÃ¢y lÃ  láº§n cuá»‘i cÃ¹ng, vÃ¬ váº­y ká»‹ch báº£n cá»§a chÃºng ta bÃ¢y giá» sáº½ Ä‘Ã o táº¡o má»™t mÃ´ hÃ¬nh Ä‘Ãºng cÃ¡ch.

Äá»ƒ tham kháº£o, Ä‘Ã¢y lÃ  táº­p lá»‡nh hoÃ n toÃ n cá»‘ Ä‘á»‹nh:

```py
import numpy as np
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

Trong trÆ°á»ng há»£p nÃ y, khÃ´ng cÃ²n váº¥n Ä‘á» gÃ¬ ná»¯a vÃ  táº­p lá»‡nh cá»§a chÃºng ta sáº½ tinh chá»‰nh má»™t mÃ´ hÃ¬nh sáº½ cho káº¿t quáº£ há»£p lÃ½. NhÆ°ng chÃºng ta cÃ³ thá»ƒ lÃ m gÃ¬ khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra mÃ  khÃ´ng cÃ³ báº¥t ká»³ lá»—i nÃ o, vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t chÃºt nÃ o? ÄÃ³ lÃ  pháº§n khÃ³ nháº¥t cá»§a há»c mÃ¡y vÃ  chÃºng ta sáº½ chá»‰ cho báº¡n má»™t vÃ i ká»¹ thuáº­t cÃ³ thá»ƒ há»¯u Ã­ch.

<Tip>

ğŸ’¡ Náº¿u báº¡n Ä‘ang sá»­ dá»¥ng vÃ²ng láº·p huáº¥n luyá»‡n thá»§ cÃ´ng, cÃ¡c bÆ°á»›c tÆ°Æ¡ng tá»± sáº½ Ã¡p dá»¥ng Ä‘á»ƒ gá»¡ lá»—i quy trÃ¬nh huáº¥n luyá»‡n cá»§a báº¡n, nhÆ°ng viá»‡c tÃ¡ch chÃºng ra sáº½ dá»… dÃ ng hÆ¡n. Tuy nhiÃªn, hÃ£y Ä‘áº£m báº£o ráº±ng báº¡n khÃ´ng quÃªn `model.eval()` hoáº·c `model.train()` á»Ÿ Ä‘Ãºng nÆ¡i, hoáº·c `zero_grad()` á»Ÿ má»—i bÆ°á»›c!

</Tip>

## Debugging silent errors during training

What can we do to debug a training that completes without error but doesn't get good results? We'll give you some pointers here, but be aware that this kind of debugging is the hardest part of machine learning, and there is no magical answer.

### Kiá»ƒm tra láº¡i dá»¯ liá»‡u cá»§a báº¡n (má»™t láº§n ná»¯a!)

MÃ´ hÃ¬nh cá»§a báº¡n sáº½ chá»‰ há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³ náº¿u nÃ³ thá»±c sá»± cÃ³ thá»ƒ há»c Ä‘Æ°á»£c báº¥t cá»© Ä‘iá»u gÃ¬ tá»« dá»¯ liá»‡u cá»§a báº¡n. Náº¿u cÃ³ lá»—i lÃ m há»ng dá»¯ liá»‡u hoáº·c cÃ¡c nhÃ£n Ä‘Æ°á»£c gÃ¡n ngáº«u nhiÃªn, ráº¥t cÃ³ thá»ƒ báº¡n sáº½ khÃ´ng huáº¥n luyá»‡n Ä‘Æ°á»£c mÃ´ hÃ¬nh nÃ o vá» táº­p dá»¯ liá»‡u cá»§a mÃ¬nh. VÃ¬ váº­y, hÃ£y luÃ´n báº¯t Ä‘áº§u báº±ng cÃ¡ch kiá»ƒm tra ká»¹ cÃ¡c Ä‘áº§u vÃ o vÃ  nhÃ£n Ä‘Ã£ Ä‘Æ°á»£c giáº£i mÃ£ cá»§a báº¡n vÃ  tá»± há»i báº£n thÃ¢n nhá»¯ng cÃ¢u há»i sau:

- Dá»¯ liá»‡u Ä‘Æ°á»£c giáº£i mÃ£ cÃ³ dá»… hiá»ƒu khÃ´ng?
- Báº¡n cÃ³ Ä‘á»“ng Ã½ vá»›i cÃ¡c nhÃ£n?
- CÃ³ má»™t nhÃ£n nÃ o phá»• biáº¿n hÆ¡n nhá»¯ng nhÃ£n khÃ¡c khÃ´ng?
- Máº¥t mÃ¡t/Chá»‰ sá»‘ sáº½ lÃ  bao nhiÃªu náº¿u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n má»™t cÃ¢u tráº£ lá»i ngáº«u nhiÃªn/luÃ´n lÃ  má»™t cÃ¢u tráº£ lá»i giá»‘ng nhau?

<Tip warning={true}>

âš ï¸ Náº¿u báº¡n Ä‘ang thá»±c hiá»‡n huáº¥n luyá»‡n phÃ¢n tÃ¡n, hÃ£y in cÃ¡c máº«u táº­p dá»¯ liá»‡u cá»§a báº¡n trong má»—i quy trÃ¬nh vÃ  kiá»ƒm tra ba láº§n Ä‘á»ƒ Ä‘áº£m báº£o báº¡n nháº­n Ä‘Æ°á»£c Ä‘iá»u tÆ°Æ¡ng tá»±. Má»™t lá»—i phá»• biáº¿n lÃ  cÃ³ má»™t sá»‘ nguá»“n ngáº«u nhiÃªn trong quÃ¡ trÃ¬nh táº¡o dá»¯ liá»‡u khiáº¿n má»—i quy trÃ¬nh cÃ³ má»™t phiÃªn báº£n khÃ¡c nhau cá»§a táº­p dá»¯ liá»‡u.

</Tip>

Sau khi xem xÃ©t dá»¯ liá»‡u cá»§a báº¡n, hÃ£y xem qua má»™t sá»‘ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  giáº£i mÃ£ chÃºng. Náº¿u mÃ´ hÃ¬nh luÃ´n dá»± Ä‘oÃ¡n cÃ¹ng má»™t Ä‘iá»u, cÃ³ thá»ƒ lÃ  do táº­p dá»¯ liá»‡u cá»§a báº¡n thiÃªn vá» má»™t loáº¡i (Ä‘á»‘i vá»›i cÃ¡c váº¥n Ä‘á» phÃ¢n loáº¡i); cÃ¡c ká»¹ thuáº­t nhÆ° láº¥y máº«u quÃ¡ má»©c cÃ¡c lá»›p hiáº¿m cÃ³ thá»ƒ há»¯u Ã­ch.

Náº¿u máº¥t mÃ¡t/chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ báº¡n nháº­n Ä‘Æ°á»£c trÃªn mÃ´ hÃ¬nh ban Ä‘áº§u cá»§a mÃ¬nh ráº¥t khÃ¡c vá»›i cÃ¡i báº¡n mong Ä‘á»£i cho cÃ¡c dá»± Ä‘oÃ¡n ngáº«u nhiÃªn, hÃ£y kiá»ƒm tra ká»¹ cÃ¡ch tÃ­nh toÃ¡n tá»•n tháº¥t hoáº·c sá»‘ liá»‡u cá»§a báº¡n, vÃ¬ cÃ³ thá»ƒ cÃ³ má»™t lá»—i á»Ÿ Ä‘Ã³. Náº¿u báº¡n Ä‘ang sá»­ dá»¥ng má»™t sá»‘ máº¥t mÃ¡t mÃ  báº¡n thÃªm vÃ o cuá»‘i, hÃ£y Ä‘áº£m báº£o ráº±ng chÃºng cÃ³ cÃ¹ng quy mÃ´.

Khi báº¡n cháº¯c cháº¯n dá»¯ liá»‡u cá»§a mÃ¬nh lÃ  hoÃ n háº£o, báº¡n cÃ³ thá»ƒ xem liá»‡u mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng huáº¥n luyá»‡n vá» nÃ³ hay khÃ´ng báº±ng má»™t bÃ i kiá»ƒm tra Ä‘Æ¡n giáº£n.

### Há»c kÄ© mÃ´ hÃ¬nh cá»§a báº¡n trong má»™t lÃ´

Viá»‡c há»c quÃ¡ nhiá»u thÆ°á»ng lÃ  Ä‘iá»u chÃºng ta cá»‘ gáº¯ng trÃ¡nh khi huáº¥n luyá»‡n, vÃ¬ nÃ³ cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh khÃ´ng há»c cÃ¡ch nháº­n ra cÃ¡c Ä‘áº·c Ä‘iá»ƒm chung ta muá»‘n mÃ  thay vÃ o Ä‘Ã³ chá»‰ lÃ  ghi nhá»› cÃ¡c máº«u huáº¥n luyá»‡n. Tuy nhiÃªn, cá»‘ gáº¯ng huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a báº¡n láº·p Ä‘i láº·p láº¡i lÃ  má»™t bÃ i kiá»ƒm tra tá»‘t Ä‘á»ƒ kiá»ƒm tra xem váº¥n Ä‘á» nhÆ° báº¡n Ä‘Ã£ Ä‘á»‹nh hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng mÃ´ hÃ¬nh mÃ  báº¡n Ä‘ang cá»‘ gáº¯ng huáº¥n luyá»‡n hay khÃ´ng. NÃ³ cÅ©ng sáº½ giÃºp báº¡n xem liá»‡u tá»‘c Ä‘á»™ há»c ban Ä‘áº§u cá»§a báº¡n cÃ³ quÃ¡ cao hay khÃ´ng.

Thá»±c hiá»‡n Ä‘iá»u nÃ y khi báº¡n Ä‘Ã£ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c `Trainer` cá»§a mÃ¬nh thá»±c sá»± dá»… dÃ ng; chá»‰ cáº§n láº¥y má»™t loáº¡t dá»¯ liá»‡u huáº¥n luyá»‡n, sau Ä‘Ã³ cháº¡y má»™t vÃ²ng huáº¥n luyá»‡n thá»§ cÃ´ng nhá» chá»‰ sá»­ dá»¥ng lÃ´ Ä‘Ã³ cho má»™t cÃ¡i gÃ¬ Ä‘Ã³ giá»‘ng nhÆ° 20 bÆ°á»›c:

```py
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

<Tip>

ğŸ’¡ Náº¿u dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a báº¡n khÃ´ng cÃ¢n báº±ng, hÃ£y Ä‘áº£m báº£o táº¡o má»™t loáº¡t dá»¯ liá»‡u huáº¥n luyá»‡n cÃ³ chá»©a táº¥t cáº£ cÃ¡c nhÃ£n.

</Tip>

MÃ´ hÃ¬nh pháº£i cÃ³ káº¿t quáº£ tráº£ vá» gáº§n nhÆ° hoÃ n háº£o trÃªn cÃ¹ng má»™t `lÃ´`. HÃ£y tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ trÃªn cÃ¡c dá»± Ä‘oÃ¡n káº¿t quáº£:

```py
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python out
{'accuracy': 1.0}
```

ChÃ­nh xÃ¡c 100%, Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh vá» viá»‡c overfitt(cÃ³ nghÄ©a lÃ  náº¿u báº¡n thá»­ mÃ´ hÃ¬nh cá»§a mÃ¬nh trÃªn báº¥t ká»³ cÃ¢u nÃ o khÃ¡c, ráº¥t cÃ³ thá»ƒ nÃ³ sáº½ Ä‘Æ°a ra cÃ¢u tráº£ lá»i sai)!

Náº¿u báº¡n khÃ´ng quáº£n lÃ½ Ä‘á»ƒ mÃ´ hÃ¬nh cá»§a mÃ¬nh cÃ³ Ä‘Æ°á»£c káº¿t quáº£ hoÃ n háº£o nhÆ° tháº¿ nÃ y, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  cÃ³ Ä‘iá»u gÃ¬ Ä‘Ã³ khÃ´ng á»•n trong cÃ¡ch báº¡n Ä‘á»‹nh khung váº¥n Ä‘á» hoáº·c dá»¯ liá»‡u cá»§a mÃ¬nh, vÃ¬ váº­y báº¡n nÃªn kháº¯c phá»¥c Ä‘iá»u Ä‘Ã³. Chá»‰ khi báº¡n vÆ°á»£t qua Ä‘Æ°á»£c bÃ i kiá»ƒm tra overfit, báº¡n má»›i cÃ³ thá»ƒ cháº¯c cháº¯n ráº±ng mÃ´ hÃ¬nh cá»§a mÃ¬nh thá»±c sá»± cÃ³ thá»ƒ há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³.

<Tip warning={true}>

âš ï¸ Báº¡n sáº½ pháº£i táº¡o láº¡i mÃ´ hÃ¬nh vÃ  `Trainer`cá»§a mÃ¬nh sau bÃ i kiá»ƒm tra overfitt nÃ y, vÃ¬ mÃ´ hÃ¬nh thu Ä‘Æ°á»£c cÃ³ thá»ƒ sáº½ khÃ´ng thá»ƒ khÃ´i phá»¥c vÃ  há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³ há»¯u Ã­ch trÃªn táº­p dá»¯ liá»‡u Ä‘áº§y Ä‘á»§ cá»§a báº¡n.

</Tip>

### KhÃ´ng Ä‘iá»u chá»‰nh báº¥t cá»© thá»© gÃ¬ cho Ä‘áº¿n khi báº¡n cÃ³ mÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘áº§u tiÃªn

Äiá»u chá»‰nh siÃªu tham sá»‘ luÃ´n Ä‘Æ°á»£c nháº¥n máº¡nh lÃ  pháº§n khÃ³ nháº¥t cá»§a há»c mÃ¡y, nhÆ°ng nÃ³ chá»‰ lÃ  bÆ°á»›c cuá»‘i cÃ¹ng giÃºp báº¡n hiá»ƒu Ä‘Æ°á»£c má»™t chÃºt vá» chá»‰ sá»‘ nÃ y. Háº§u háº¿t thá»i gian, cÃ¡c siÃªu tham sá»‘ máº·c Ä‘á»‹nh cá»§a `Trainer` sáº½ hoáº¡t Ä‘á»™ng tá»‘t Ä‘á»ƒ cung cáº¥p cho báº¡n káº¿t quáº£ tá»‘t, vÃ¬ váº­y Ä‘á»«ng khá»Ÿi cháº¡y tÃ¬m kiáº¿m siÃªu tham sá»‘ tá»‘n thá»i gian vÃ  tá»‘n kÃ©m cho Ä‘áº¿n khi báº¡n cÃ³ thá»© gÃ¬ Ä‘Ã³ vÆ°á»£t qua mÃ´ hÃ¬nh cÆ¡ sá»Ÿ mÃ  báº¡n cÃ³ trÃªn táº­p dá»¯ liá»‡u cá»§a mÃ¬nh.

Khi báº¡n Ä‘Ã£ cÃ³ má»™t mÃ´ hÃ¬nh Ä‘á»§ tá»‘t, báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u Ä‘iá»u chá»‰nh má»™t chÃºt. Äá»«ng thá»­ khá»Ÿi cháº¡y má»™t nghÃ¬n láº§n cháº¡y vá»›i cÃ¡c siÃªu tham sá»‘ khÃ¡c nhau, nhÆ°ng hÃ£y so sÃ¡nh má»™t vÃ i láº§n cháº¡y vá»›i cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau cho má»™t siÃªu thÃ´ng sá»‘ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ã½ tÆ°á»Ÿng vá» giÃ¡ trá»‹ nÃ o cÃ³ tÃ¡c Ä‘á»™ng lá»›n nháº¥t.

Náº¿u báº¡n Ä‘ang Ä‘iá»u chá»‰nh chÃ­nh mÃ´ hÃ¬nh, hÃ£y giá»¯ nÃ³ Ä‘Æ¡n giáº£n vÃ  Ä‘á»«ng thá»­ báº¥t cá»© Ä‘iá»u gÃ¬ mÃ  báº¡n khÃ´ng thá»ƒ biá»‡n minh má»™t cÃ¡ch há»£p lÃ½. LuÃ´n Ä‘áº£m báº£o ráº±ng báº¡n quay láº¡i kiá»ƒm tra overfit Ä‘á»ƒ xÃ¡c minh ráº±ng thay Ä‘á»•i cá»§a báº¡n khÃ´ng gÃ¢y ra báº¥t ká»³ háº­u quáº£ ngoÃ i Ã½ muá»‘n nÃ o.

### YÃªu cáº§u giÃºp Ä‘á»¡

Hy vá»ng ráº±ng báº¡n sáº½ tÃ¬m tháº¥y má»™t sá»‘ lá»i khuyÃªn trong pháº§n nÃ y Ä‘á»ƒ giÃºp báº¡n giáº£i quyáº¿t váº¥n Ä‘á» cá»§a mÃ¬nh, nhÆ°ng náº¿u khÃ´ng pháº£i váº­y, hÃ£y nhá»› ráº±ng báº¡n luÃ´n cÃ³ thá»ƒ há»i cá»™ng Ä‘á»“ng trÃªn [diá»…n Ä‘Ã n](https://discuss.huggingface.co/).

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ tÃ i liá»‡u bá»• sung cÃ³ thá»ƒ há»¯u Ã­ch:

- ["Reproducibility as a vehicle for engineering best practices"](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) bá»Ÿi Joel Grus
- ["Checklist for debugging neural networks"](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) bá»Ÿi Cecelia Shao
- ["How to unit test machine learning code"](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) bá»Ÿi Chase Roberts
- ["A Recipe for Training Neural Networks"](http://karpathy.github.io/2019/04/25/recipe/) bá»Ÿi Andrej Karpathy

Táº¥t nhiÃªn, khÃ´ng pháº£i má»i váº¥n Ä‘á» báº¡n gáº·p pháº£i khi huáº¥n luyá»‡n máº¡ng tháº§n kinh Ä‘á»u lÃ  lá»—i cá»§a chÃ­nh báº¡n! Náº¿u báº¡n gáº·p Ä‘iá»u gÃ¬ Ä‘Ã³ trong thÆ° viá»‡n ğŸ¤— Transformers hoáº·c ğŸ¤— Datasets cÃ³ váº» khÃ´ng á»•n, cÃ³ thá»ƒ báº¡n Ä‘Ã£ gáº·p lá»—i. Báº¡n cháº¯c cháº¯n nÃªn cho chÃºng tÃ´i biáº¿t táº¥t cáº£ vá» Ä‘iá»u Ä‘Ã³ vÃ  trong pháº§n tiáº¿p theo, chÃºng tÃ´i sáº½ giáº£i thÃ­ch chÃ­nh xÃ¡c cÃ¡ch thá»±c hiá»‡n Ä‘iá»u Ä‘Ã³.
