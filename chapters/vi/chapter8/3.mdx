# YÃªu cáº§u trá»£ giÃºp trÃªn diá»…n Ä‘Ã n

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter8/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter8/section3.ipynb"},
]} />

<Youtube id="S2EEG3JIt2A"/>

[Diá»…n Ä‘Ã n Hugging Face](https://discuss.huggingface.co) lÃ  nÆ¡i tuyá»‡t vá»i Ä‘á»ƒ nháº­n Ä‘Æ°á»£c sá»± giÃºp Ä‘á»¡ tá»« cÃ¡c nhÃ³m nguá»“n má»Ÿ vÃ  cá»™ng Ä‘á»“ng Hugging Face. Trang chá»§ luÃ´n trÃ´ng nhÆ° sau:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forums.png" alt="The Hugging Face forums." width="100%"/>
</div>

á» bÃªn tay trÃ¡i, báº¡n cÃ³ thá»ƒ tháº¥y táº¥t cáº£ cÃ¡c danh má»¥c mÃ  cÃ¡c chá»§ Ä‘á» khÃ¡c nhau Ä‘Æ°á»£c nhÃ³m láº¡i, trong khi bÃªn tay pháº£i hiá»ƒn thá»‹ cÃ¡c chá»§ Ä‘á» gáº§n Ä‘Ã¢y nháº¥t. Chá»§ Ä‘á» lÃ  má»™t bÃ i Ä‘Äƒng cÃ³ chá»©a tiÃªu Ä‘á», danh má»¥c vÃ  mÃ´ táº£; nÃ³ khÃ¡ giá»‘ng vá»›i Ä‘á»‹nh dáº¡ng váº¥n Ä‘á» GitHub mÃ  chÃºng ta Ä‘Ã£ tháº¥y khi táº¡o táº­p dá»¯ liá»‡u cá»§a riÃªng mÃ¬nh trong [ChÆ°Æ¡ng 5](/course/chapter5). NhÆ° tÃªn cho tháº¥y, danh má»¥c [Beginners](https://discuss.huggingface.co/c/beginners/5) chá»§ yáº¿u dÃ nh cho nhá»¯ng ngÆ°á»i má»›i báº¯t Ä‘áº§u vá»›i há»‡ sinh thÃ¡i vÃ  thÆ° viá»‡n Hugging Face. Má»i cÃ¢u há»i trÃªn báº¥t ká»³ thÆ° viá»‡n nÃ o Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh á»Ÿ Ä‘Ã³, cÃ³ thá»ƒ lÃ  Ä‘á»ƒ gá»¡ lá»—i má»™t sá»‘ mÃ£ hoáº·c Ä‘á»ƒ yÃªu cáº§u trá»£ giÃºp vá» cÃ¡ch thá»±c hiá»‡n Ä‘iá»u gÃ¬ Ä‘Ã³. (Äiá»u Ä‘Ã³ nÃ³i ráº±ng, náº¿u cÃ¢u há»i cá»§a báº¡n liÃªn quan Ä‘áº¿n má»™t thÆ° viá»‡n cá»¥ thá»ƒ, báº¡n cÃ³ thá»ƒ nÃªn chuyá»ƒn Ä‘áº¿n danh má»¥c thÆ° viá»‡n tÆ°Æ¡ng á»©ng trÃªn diá»…n Ä‘Ã n.)

TÆ°Æ¡ng tá»±, danh má»¥c [Intermediate](https://discuss.huggingface.co/c/intermediate/6)vÃ  [Research](https://discuss.huggingface.co/c/research/7) dÃ nh cho cÃ¡c cÃ¢u há»i nÃ¢ng cao hÆ¡n , vÃ­ dá»¥ vá» thÆ° viá»‡n hoáº·c má»™t sá»‘ nghiÃªn cá»©u NLP má»›i thÃº vá»‹ mÃ  báº¡n muá»‘n tháº£o luáº­n.

VÃ  Ä‘Æ°Æ¡ng nhiÃªn, chÃºng ta cÅ©ng nÃªn Ä‘á» cáº­p Ä‘áº¿n danh má»¥c [Course](https://discuss.huggingface.co/c/course/20), nÆ¡i báº¡n cÃ³ thá»ƒ Ä‘áº·t báº¥t ká»³ cÃ¢u há»i nÃ o liÃªn quan Ä‘áº¿n khÃ³a há»c Hugging Face!

Khi báº¡n Ä‘Ã£ chá»n má»™t danh má»¥c, báº¡n sáº½ sáºµn sÃ ng viáº¿t chá»§ Ä‘á» Ä‘áº§u tiÃªn cá»§a mÃ¬nh. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y má»™t sá»‘ [hÆ°á»›ng dáº«n](https://discuss.huggingface.co/t/how-to-request-support/3128) trong diá»…n Ä‘Ã n vá» cÃ¡ch thá»±c hiá»‡n viá»‡c nÃ y vÃ  trong pháº§n nÃ y chÃºng ta sáº½ xem xÃ©t má»™t sá»‘ tÃ­nh nÄƒng táº¡o nÃªn má»™t chá»§ Ä‘á» hay.

## Viáº¿t má»™t bÃ i Ä‘Äƒng tá»‘t trÃªn diá»…n Ä‘Ã n

NhÆ° má»™t vÃ­ dá»¥, giáº£ sá»­ ráº±ng chÃºng ta Ä‘ang cá»‘ gáº¯ng táº¡o cÃ¡c biá»ƒu diá»…n tá»« cÃ¡c bÃ i viáº¿t trÃªn Wikipedia Ä‘á»ƒ táº¡o má»™t cÃ´ng cá»¥ tÃ¬m kiáº¿m tÃ¹y chá»‰nh. NhÆ° thÆ°á»ng lá»‡, chÃºng ta táº£i tokenizer vÃ  mÃ´ hÃ¬nh nhÆ° sau:

```python
from transformers import AutoTokenizer, AutoModel

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModel.from_pretrained(model_checkpoint)
```

Giá» giáº£ sá»­ ta Ä‘ang cá»‘ nhÃºng toÃ n bá»™ pháº§n nÃ y cá»§a [Wikipedia](https://en.wikipedia.org/wiki/Transformers) lÃªn Transformers:

```python
text = """
Generation One is a retroactive term for the Transformers characters that
appeared between 1984 and 1993. The Transformers began with the 1980s Japanese
toy lines Micro Change and Diaclone. They presented robots able to transform
into everyday vehicles, electronic items or weapons. Hasbro bought the Micro
Change and Diaclone toys, and partnered with Takara. Marvel Comics was hired by
Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall
story, and gave the task of creating the characthers to writer Dennis O'Neil.
Unhappy with O'Neil's work (although O'Neil created the name "Optimus Prime"),
Shooter chose Bob Budiansky to create the characters.

The Transformers mecha were largely designed by ShÅji Kawamori, the creator of
the Japanese mecha anime franchise Macross (which was adapted into the Robotech
franchise in North America). Kawamori came up with the idea of transforming
mechs while working on the Diaclone and Macross franchises in the early 1980s
(such as the VF-1 Valkyrie in Macross and Robotech), with his Diaclone mechs
later providing the basis for Transformers.

The primary concept of Generation One is that the heroic Optimus Prime, the
villainous Megatron, and their finest soldiers crash land on pre-historic Earth
in the Ark and the Nemesis before awakening in 1985, Cybertron hurtling through
the Neutral zone as an effect of the war. The Marvel comic was originally part
of the main Marvel Universe, with appearances from Spider-Man and Nick Fury,
plus some cameos, as well as a visit to the Savage Land.

The Transformers TV series began around the same time. Produced by Sunbow
Productions and Marvel Productions, later Hasbro Productions, from the start it
contradicted Budiansky's backstories. The TV series shows the Autobots looking
for new energy sources, and crash landing as the Decepticons attack. Marvel
interpreted the Autobots as destroying a rogue asteroid approaching Cybertron.
Shockwave is loyal to Megatron in the TV series, keeping Cybertron in a
stalemate during his absence, but in the comic book he attempts to take command
of the Decepticons. The TV series would also differ wildly from the origins
Budiansky had created for the Dinobots, the Decepticon turned Autobot Jetfire
(known as Skyfire on TV), the Constructicons (who combine to form
Devastator),[19][20] and Omega Supreme. The Marvel comic establishes early on
that Prime wields the Creation Matrix, which gives life to machines. In the
second season, the two-part episode The Key to Vector Sigma introduced the
ancient Vector Sigma computer, which served the same original purpose as the
Creation Matrix (giving life to Transformers), and its guardian Alpha Trion.
"""

inputs = tokenizer(text, return_tensors="pt")
logits = model(**inputs).logits
```

```python output
IndexError: index out of range in self
```

Ráº¥t tiáº¿c, chÃºng ta Ä‘Ã£ gáº·p sá»± cá»‘ - vÃ  thÃ´ng bÃ¡o lá»—i khÃ³ hiá»ƒu hÆ¡n nhiá»u so vá»›i nhá»¯ng thÃ´ng bÃ¡o chÃºng ta tháº¥y trong [pháº§n 2](/course/chapter8/section2)! ChÃºng ta khÃ´ng thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c toÃ n bá»™ quÃ¡ trÃ¬nh truy váº¿t, vÃ¬ váº­y chÃºng ta quyáº¿t Ä‘á»‹nh chuyá»ƒn sang diá»…n Ä‘Ã n Hugging Face Ä‘á»ƒ Ä‘Æ°á»£c trá»£ giÃºp. LÃ m tháº¿ nÃ o chÃºng ta cÃ³ thá»ƒ táº¡o ra chá»§ Ä‘á»?

Äá»ƒ báº¯t Ä‘áº§u, chÃºng ta cáº§n nháº¥p vÃ o nÃºt "New Topic" hay "Chá»§ Ä‘á» má»›i" á»Ÿ gÃ³c trÃªn bÃªn pháº£i (lÆ°u Ã½ ráº±ng Ä‘á»ƒ táº¡o chá»§ Ä‘á», chÃºng ta cáº§n Ä‘Äƒng nháº­p):

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forums-new-topic.png" alt="Creating a new forum topic." width="100%"/>
</div>

Thao tÃ¡c nÃ y sáº½ hiá»ƒn thá»‹ má»™t giao diá»‡n viáº¿t, nÆ¡i chÃºng ta cÃ³ thá»ƒ nháº­p tiÃªu Ä‘á» cá»§a chá»§ Ä‘á» cá»§a mÃ¬nh, chá»n má»™t danh má»¥c vÃ  soáº¡n tháº£o ná»™i dung:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic01.png" alt="The interface for creating a forum topic." width="100%"/>
</div>

VÃ¬ lá»—i dÆ°á»ng nhÆ° chá»‰ xáº£y ra vá»›i ğŸ¤— Transformers, nÃªn chÃºng ta sáº½ chá»n lá»—i nÃ y cho danh má»¥c. Ná»— lá»±c Ä‘áº§u tiÃªn cá»§a chÃºng ta trong viá»‡c giáº£i thÃ­ch váº¥n Ä‘á» cÃ³ thá»ƒ trÃ´ng giá»‘ng nhÆ° sau:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic02.png" alt="Drafting the content for a new forum topic." width="100%"/>
</div>

Máº·c dÃ¹ chá»§ Ä‘á» nÃ y chá»©a thÃ´ng bÃ¡o lá»—i mÃ  chÃºng tÃ´i cáº§n trá»£ giÃºp, nhÆ°ng cÃ³ má»™t sá»‘ váº¥n Ä‘á» vá»›i cÃ¡ch viáº¿t:

1. TiÃªu Ä‘á» khÃ´ng mang tÃ­nh mÃ´ táº£ cao, vÃ¬ váº­y báº¥t ká»³ ai duyá»‡t diá»…n Ä‘Ã n sáº½ khÃ´ng thá»ƒ biáº¿t chá»§ Ä‘á» lÃ  gÃ¬ náº¿u khÃ´ng Ä‘á»c pháº§n ná»™i dung.
2. Pháº§n thÃ¢n khÃ´ng cung cáº¥p Ä‘á»§ thÃ´ng tin vá» _nÆ¡i_ báº¯t nguá»“n lá»—i vÃ  _cÃ¡ch_ Ä‘á»ƒ táº¡o láº¡i lá»—i Ä‘Ã³.
3. Chá»§ Ä‘á» gáº¯n tháº» trá»±c tiáº¿p má»™t vÃ i ngÆ°á»i vá»›i giá»ng Ä‘iá»‡u hÆ¡i kháº¯t khe.

CÃ¡c chá»§ Ä‘á» nhÆ° tháº¿ nÃ y khÃ´ng cÃ³ kháº£ nÄƒng nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i nhanh (náº¿u há» nháº­n Ä‘Æ°á»£c má»™t cÃ¢u tráº£ lá»i nÃ o Ä‘Ã³), vÃ¬ váº­y hÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ cáº£i thiá»‡n nÃ³. ChÃºng ta sáº½ báº¯t Ä‘áº§u vá»›i váº¥n Ä‘á» Ä‘áº§u tiÃªn lÃ  chá»n má»™t tiÃªu Ä‘á» hay.

### Choosing a descriptive title

Náº¿u báº¡n Ä‘ang cá»‘ gáº¯ng nháº­n trá»£ giÃºp vá» má»™t lá»—i trong mÃ£ cá»§a mÃ¬nh, má»™t nguyÃªn táº¯c chung lÃ  Ä‘Æ°a Ä‘á»§ thÃ´ng tin vÃ o tiÃªu Ä‘á» Ä‘á»ƒ ngÆ°á»i khÃ¡c cÃ³ thá»ƒ nhanh chÃ³ng xÃ¡c Ä‘á»‹nh xem há» cÃ³ nghÄ© ráº±ng há» cÃ³ thá»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n hay khÃ´ng. Trong vÃ­ dá»¥ Ä‘ang cháº¡y cá»§a mÃ¬nh, chÃºng ta biáº¿t tÃªn cá»§a ngoáº¡i lá»‡ Ä‘ang Ä‘Æ°á»£c nÃªu ra vÃ  cÃ³ má»™t sá»‘ gá»£i Ã½ ráº±ng nÃ³ Ä‘Æ°á»£c kÃ­ch hoáº¡t trong pháº§n truyá»n tháº³ng cá»§a mÃ´ hÃ¬nh, nÆ¡i chÃºng tÃ´i gá»i lÃ  `model(**inputs)`. Äá»ƒ thÃ´ng bÃ¡o Ä‘iá»u nÃ y, má»™t tiÃªu Ä‘á» cÃ³ thá»ƒ cÃ³ lÃ :

> Source of IndexError in the AutoModel forward pass?

hay 

> Nguá»“n cá»§a IndexError trong tháº» chuyá»ƒn tiáº¿p AutoModel?

TiÃªu Ä‘á» nÃ y cho ngÆ°á»i Ä‘á»c biáº¿t báº¡n nghÄ© ráº±ng lá»—i Ä‘áº¿n tá»« _Ä‘Ã¢u_ vÃ  náº¿u há» Ä‘Ã£ gáº·p pháº£i `IndexError` trÆ°á»›c Ä‘Ã³, thÃ¬ ráº¥t cÃ³ thá»ƒ há» sáº½ biáº¿t cÃ¡ch gá»¡ lá»—i nÃ³. Táº¥t nhiÃªn, tiÃªu Ä‘á» cÃ³ thá»ƒ lÃ  báº¥t ká»³ thá»© gÃ¬ báº¡n muá»‘n vÃ  cÃ¡c biáº¿n thá»ƒ khÃ¡c nhÆ°:

> Why does my model produce an IndexError?

hay 

> Táº¡i sao mÃ´ hÃ¬nh cá»§a tÃ´i táº¡o ra má»™t IndexError?

cÅ©ng cÃ³ thá»ƒ á»•n. BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ má»™t tiÃªu Ä‘á» mÃ´ táº£, hÃ£y xem cÃ¡ch cáº£i thiá»‡n ná»™i dá»¥ng pháº§n thÃ¢n bÃ i.

### Äá»‹nh dáº¡ng cÃ¡c Ä‘oáº¡n mÃ£ cá»§a báº¡n

Äá»c mÃ£ nguá»“n Ä‘Ã£ Ä‘á»§ khÃ³ trong IDE, nhÆ°ng cÃ²n khÃ³ hÆ¡n khi mÃ£ Ä‘Æ°á»£c sao chÃ©p vÃ  dÃ¡n dÆ°á»›i dáº¡ng vÄƒn báº£n thuáº§n tÃºy! May máº¯n thay, cÃ¡c diá»…n Ä‘Ã n vá» Hugging Face há»— trá»£ viá»‡c sá»­ dá»¥ng Markdown, vÃ¬ váº­y báº¡n nÃªn luÃ´n Ä‘áº·t cÃ¡c khá»‘i mÃ£ cá»§a mÃ¬nh báº±ng ba dáº¥u gáº¡ch ngÆ°á»£c (```) Ä‘á»ƒ dá»… Ä‘á»c hÆ¡n. HÃ£y lÃ m Ä‘iá»u nÃ y Ä‘á»ƒ sá»­a chá»¯a thÃ´ng bÃ¡o lá»—i - vÃ  trong khi chÃºng ta xá»­ lÃ½ nÃ³, hÃ£y lÃ m cho pháº§n ná»™i dung lá»‹ch sá»± hÆ¡n má»™t chÃºt so vá»›i phiÃªn báº£n gá»‘c cá»§a mÃ¬nh:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic03.png" alt="Our revised forum topic, with proper code formatting." width="100%"/>
</div>

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y trong áº£nh chá»¥p mÃ n hÃ¬nh, viá»‡c bao bá»c cÃ¡c khá»‘i mÃ£ trong dáº¥u gáº¡ch ngÆ°á»£c sáº½ chuyá»ƒn vÄƒn báº£n thÃ´ thÃ nh mÃ£ Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng, hoÃ n chá»‰nh vá»›i kiá»ƒu mÃ u! CÅ©ng lÆ°u Ã½ ráº±ng cÃ¡c dáº¥u gáº¡ch ngÆ°á»£c Ä‘Æ¡n láº» cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘á»‹nh dáº¡ng cÃ¡c biáº¿n ná»™i tuyáº¿n, giá»‘ng nhÆ° chÃºng tÃ´i Ä‘Ã£ lÃ m cho `distilbert-base-unsased`. Chá»§ Ä‘á» nÃ y cÃ³ váº» tá»‘t hÆ¡n nhiá»u vÃ  vá»›i má»™t chÃºt may máº¯n, chÃºng ta cÃ³ thá»ƒ tÃ¬m tháº¥y ai Ä‘Ã³ trong cá»™ng Ä‘á»“ng cÃ³ thá»ƒ Ä‘oÃ¡n Ä‘Æ°á»£c lá»—i lÃ  gÃ¬. Tuy nhiÃªn, thay vÃ¬ dá»±a vÃ o may máº¯n, chÃºng ta hÃ£y lÃ m cho cuá»™c sá»‘ng dá»… dÃ ng hÆ¡n báº±ng cÃ¡ch Ä‘Æ°a vÃ o chi tiáº¿t Ä‘áº§y Ä‘á»§ cÃ¡c truy váº¿t cá»§a nÃ³!

### Bao gá»“m toÃ n bá»™ truy váº¿t

VÃ¬ dÃ²ng cuá»‘i cÃ¹ng cá»§a báº£n truy váº¿t thÆ°á»ng Ä‘á»§ Ä‘á»ƒ gá»¡ lá»—i Ä‘oáº¡n mÃ£ cá»§a riÃªng báº¡n, nÃªn báº¡n cÃ³ thá»ƒ chá»‰ cung cáº¥p dÃ²ng Ä‘Ã³ trong chá»§ Ä‘á» cá»§a mÃ¬nh Ä‘á»ƒ "tiáº¿t kiá»‡m dung lÆ°á»£ng". Máº·c dÃ¹ cÃ³ chá»§ Ã½ tá»‘t, Ä‘iá»u nÃ y thá»±c sá»± khiáº¿n ngÆ°á»i khÃ¡c cÃ³ thá»ƒ _khÃ³_ gá»¡ lá»—i váº¥n Ä‘á» _hÆ¡n_ vÃ¬ thÃ´ng tin cao hÆ¡n trong báº£n truy xuáº¥t cÃ³ thá»ƒ thá»±c sá»± há»¯u Ã­ch. VÃ¬ váº­y, má»™t phÆ°Æ¡ng phÃ¡p hay lÃ  sao chÃ©p vÃ  dÃ¡n _toÃ n bá»™_ dáº¥u váº¿t, Ä‘á»“ng thá»i Ä‘áº£m báº£o ráº±ng nÃ³ Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng Ä‘á»™c Ä‘Ã¡o. VÃ¬ nhá»¯ng láº§n truy xuáº¥t nÃ y cÃ³ thá»ƒ máº¥t nhiá»u thá»i gian, má»™t sá»‘ ngÆ°á»i thÃ­ch hiá»ƒn thá»‹ chÃºng sau khi há» Ä‘Ã£ giáº£i thÃ­ch mÃ£ nguá»“n. LÃ m thÃ´i nÃ o. BÃ¢y giá», chá»§ Ä‘á» diá»…n Ä‘Ã n cá»§a chÃºng ta trÃ´ng giá»‘ng nhÆ° sau:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic04.png" alt="Our example forum topic, with the complete traceback." width="100%"/>
</div>

Äiá»u nÃ y cÃ³ nhiá»u thÃ´ng tin hÆ¡n vÃ  má»™t ngÆ°á»i Ä‘á»c cáº©n tháº­n cÃ³ thá»ƒ chá»‰ ra ráº±ng váº¥n Ä‘á» dÆ°á»ng nhÆ° lÃ  do chuyá»ƒn má»™t Ä‘áº§u vÃ o dÃ i vÃ¬ dÃ²ng nÃ y trong báº£n truy xuáº¥t:

> Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512).

Tuy nhiÃªn, chÃºng ta cÃ³ thá»ƒ khiáº¿n má»i thá»© trá»Ÿ nÃªn dá»… dÃ ng hÆ¡n vá»›i há» báº±ng cÃ¡ch cung cáº¥p mÃ£ thá»±c Ä‘Ã£ gÃ¢y ra lá»—i. HÃ£y lÃ m Ä‘iá»u Ä‘Ã³ ngay bÃ¢y giá».

### Cung cáº¥p má»™t vÃ­ dá»¥ cÃ³ thá»ƒ tÃ¡i táº¡o

Náº¿u báº¡n Ä‘Ã£ tá»«ng cá»‘ gáº¯ng gá»¡ lá»—i Ä‘oáº¡n mÃ£ cá»§a ngÆ°á»i khÃ¡c, trÆ°á»›c tiÃªn cÃ³ thá»ƒ báº¡n Ä‘Ã£ cá»‘ gáº¯ng táº¡o láº¡i sá»± cá»‘ mÃ  há» Ä‘Ã£ bÃ¡o cÃ¡o Ä‘á»ƒ báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u lÃ m viá»‡c theo cÃ¡ch cá»§a mÃ¬nh thÃ´ng qua truy xuáº¥t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh lá»—i. NÃ³ khÃ´ng khÃ¡c gÃ¬ khi nÃ³i Ä‘áº¿n (hoáº·c cung cáº¥p) há»— trá»£ trÃªn cÃ¡c diá»…n Ä‘Ã n, vÃ¬ váº­y sáº½ thá»±c sá»± há»¯u Ã­ch náº¿u báº¡n cÃ³ thá»ƒ cung cáº¥p má»™t vÃ­ dá»¥ nhá» mÃ´ táº£ láº¡i lá»—i. Má»™t ná»­a thá»i gian, chá»‰ cáº§n Ä‘i qua bÃ i táº­p nÃ y sáº½ giÃºp báº¡n nháº­n ra Ä‘iá»u gÃ¬ Ä‘ang xáº£y ra. Trong má»i trÆ°á»ng há»£p, pháº§n cÃ²n thiáº¿u trong vÃ­ dá»¥ cá»§a chÃºng ta lÃ  hiá»ƒn thá»‹ _cÃ¡c Ä‘áº§u vÃ o_ mÃ  ta Ä‘Ã£ cung cáº¥p cho mÃ´ hÃ¬nh. LÃ m Ä‘iá»u Ä‘Ã³ cho chÃºng ta má»™t cÃ¡i gÃ¬ Ä‘Ã³ giá»‘ng nhÆ° vÃ­ dá»¥ Ä‘Ã£ hoÃ n thÃ nh sau:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic05.png" alt="The final version of our forum topic." width="100%"/>
</div>

Chá»§ Ä‘á» nÃ y hiá»‡n chá»©a khÃ¡ nhiá»u thÃ´ng tin vÃ  nÃ³ Ä‘Æ°á»£c viáº¿t theo cÃ¡ch cÃ³ nhiá»u kháº£ nÄƒng thu hÃºt sá»± chÃº Ã½ cá»§a cá»™ng Ä‘á»“ng vÃ  nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i há»¯u Ã­ch. Vá»›i nhá»¯ng hÆ°á»›ng dáº«n cÆ¡ báº£n nÃ y, giá» Ä‘Ã¢y báº¡n cÃ³ thá»ƒ táº¡o cÃ¡c chá»§ Ä‘á» tuyá»‡t vá»i Ä‘á»ƒ tÃ¬m cÃ¢u tráº£ lá»i cho cÃ¡c cÃ¢u há»i vá» ğŸ¤— Transformers cá»§a mÃ¬nh!
