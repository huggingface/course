<FrameworkSwitchCourse {fw} />

# Tinh chá»‰nh má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section3_tf.ipynb"},
]} />

{/if}

Äá»‘i vá»›i nhiá»u á»©ng dá»¥ng NLP liÃªn quan Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh Transformer, báº¡n cÃ³ thá»ƒ chá»‰ cáº§n láº¥y má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tá»« Hugging Face Hub vÃ  tinh chá»‰nh trá»±c tiáº¿p trÃªn dá»¯ liá»‡u cá»§a báº¡n cho tÃ¡c vá»¥ hiá»‡n táº¡i. Vá»›i Ä‘iá»u kiá»‡n lÃ  ngá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c khÃ´ng quÃ¡ khÃ¡c biá»‡t vá»›i ngá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tinh chá»‰nh, viá»‡c há»c chuyá»ƒn tiáº¿p thÆ°á»ng sáº½ mang láº¡i káº¿t quáº£ tá»‘t.

Tuy nhiÃªn, cÃ³ má»™t vÃ i trÆ°á»ng há»£p mÃ  trÆ°á»›c tiÃªn báº¡n sáº½ muá»‘n tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ trÃªn dá»¯ liá»‡u cá»§a mÃ¬nh, trÆ°á»›c khi huáº¥n luyá»‡n Ä‘áº§u tÃ¡c vá»¥ cá»¥ thá»ƒ. VÃ­ dá»¥: náº¿u táº­p dá»¯ liá»‡u cá»§a báº¡n chá»©a cÃ¡c há»£p Ä‘á»“ng phÃ¡p lÃ½ hoáº·c cÃ¡c bÃ i bÃ¡o khoa há»c, thÃ¬ mÃ´ hÃ¬nh thuáº§n Transformer nhÆ° BERT thÆ°á»ng sáº½ coi cÃ¡c tá»« chuyÃªn mÃ´n trong kho dá»¯ liá»‡u cá»§a báº¡n lÃ  token hiáº¿m vÃ  hiá»‡u suáº¥t káº¿t quáº£ cÃ³ thá»ƒ kÃ©m hÆ¡n. Báº±ng cÃ¡ch tinh chá»‰nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ trÃªn dá»¯ liá»‡u chuyÃªn mÃ´n, báº¡n cÃ³ thá»ƒ tÄƒng hiá»‡u suáº¥t cá»§a nhiá»u tÃ¡c vá»¥ xuÃ´i dÃ²ng, cÃ³ nghÄ©a lÃ  báº¡n thÆ°á»ng chá»‰ pháº£i thá»±c hiá»‡n bÆ°á»›c nÃ y má»™t láº§n!

QuÃ¡ trÃ¬nh tinh chá»‰nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn dá»¯ liá»‡u trong máº£ng nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  _domain adapt_ hay _thÃ­ch á»©ng chuyÃªn mÃ´n_. NÃ³ Ä‘Æ°á»£c phá»• biáº¿n vÃ o nÄƒm 2018 bá»Ÿi [ULMFiT](https://arxiv.org/abs/1801.06146), lÃ  má»™t trong nhá»¯ng kiáº¿n â€‹â€‹trÃºc máº¡ng tháº§n kinh Ä‘áº§u tiÃªn (dá»±a trÃªn LSTM) Ä‘á»ƒ lÃ m cho viá»‡c há»c chuyá»ƒn tiáº¿p thá»±c sá»± hiá»‡u quáº£ cho NLP. Má»™t vÃ­ dá»¥ vá» thÃ­ch á»©ng chuyÃªn mÃ´n vá»›i ULMFiT Ä‘Æ°á»£c hiá»ƒn thá»‹ trong hÃ¬nh dÆ°á»›i Ä‘Ã¢y; trong pháº§n nÃ y, chÃºng ta sáº½ lÃ m Ä‘iá»u tÆ°Æ¡ng tá»±, nhÆ°ng vá»›i Transformer thay vÃ¬ LSTM!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg" alt="ULMFiT."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg" alt="ULMFiT."/>
</div>

Äáº¿n cuá»‘i pháº§n nÃ y, báº¡n sáº½ cÃ³ má»™t [mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) trÃªn Hub cÃ³ thá»ƒ tá»± Ä‘á»™ng hoÃ n thiá»‡n cÃ¢u nhÆ° dÆ°á»›i Ä‘Ã¢y:

<iframe src="https://course-demos-distilbert-base-uncased-finetune-7400b54.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

CÃ¹ng Ä‘i sÃ¢u vÃ o thÃ´i!

<Youtube id="mqElG5QJWUg"/>

<Tip>

ğŸ™‹ Náº¿u cÃ¡c thuáº­t ngá»¯ "mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i" vÃ  "mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c" nghe cÃ³ váº» xa láº¡ vá»›i báº¡n, hÃ£y xem [ChÆ°Æ¡ng 1](/course/chapter1), nÆ¡i chÃºng tÃ´i giáº£i thÃ­ch táº¥t cáº£ cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi nÃ y, kÃ¨m theo video!

</Tip>

## Chá»n má»™t mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i

Äá»ƒ báº¯t Ä‘áº§u, hÃ£y chá»n má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c phÃ¹ há»£p Ä‘á»ƒ táº¡o mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong áº£nh chá»¥p mÃ n hÃ¬nh dÆ°á»›i Ä‘Ã¢y, báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y danh sÃ¡ch cÃ¡c á»©ng cá»­ viÃªn báº±ng cÃ¡ch Ã¡p dá»¥ng bá»™ lá»c "Fill-Mask" trÃªn [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads):

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png" alt="Hub models." width="80%"/>
</div>

Máº·c dÃ¹ dÃ²ng mÃ´ hÃ¬nh BERT vÃ  RoBERTa Ä‘Æ°á»£c táº£i xuá»‘ng nhiá»u nháº¥t, chÃºng ta sáº½ sá»­ dá»¥ng mÃ´ hÃ¬nh cÃ³ tÃªn [DistilBERT](https://huggingface.co/distilbert-base-uncased)
cÃ³ thá»ƒ huáº¥n luyá»‡n nhanh hÆ¡n nhiá»u mÃ  Ã­t hoáº·c khÃ´ng bá»‹ máº¥t hiá»‡u suáº¥t. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng má»™t ká»¹ thuáº­t Ä‘áº·c biá»‡t cÃ³ tÃªn lÃ  [_knowledge distillation_](https://en.wikipedia.org/wiki/Knowledge_distillation), trong Ä‘Ã³ má»™t "mÃ´ hÃ¬nh giÃ¡o viÃªn" lá»›n nhÆ° BERT Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hÆ°á»›ng dáº«n huáº¥n luyá»‡n "mÃ´ hÃ¬nh sinh viÃªn" cÃ³ Ã­t tham sá»‘ hÆ¡n nhiá»u. Pháº§n giáº£i thÃ­ch chi tiáº¿t vá» quÃ¡ trÃ¬nh cháº¯t lá»c kiáº¿n â€‹â€‹thá»©c sáº½ Ä‘Æ°a chÃºng ta Ä‘i quÃ¡ xa trong pháº§n nÃ y, nhÆ°ng náº¿u báº¡n quan tÃ¢m, báº¡n cÃ³ thá»ƒ Ä‘á»c táº¥t cáº£ vá» nÃ³ trong [_Natural Language Processing with Transformers_](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) (thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  sÃ¡ch giÃ¡o khoa vá» Transformer).

{#if fw === 'pt'}

HÃ£y tiáº¿p tá»¥c vÃ  táº£i xuá»‘ng DistilBERT báº±ng cÃ¡ch sá»­ dá»¥ng lá»›p `AutoModelForMaskedLM`:

```python
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ChÃºng ta cÃ³ thá»ƒ xem mÃ´ hÃ¬nh nÃ y cÃ³ bao nhiÃªu tham sá»‘ báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `num_parameters()`:

```python
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")
```

```python out
'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'
```

{:else}

HÃ£y tiáº¿p tá»¥c vÃ  táº£i xuá»‘ng DistilBERT báº±ng cÃ¡ch sá»­ dá»¥ng lá»›p `AutoModelForMaskedLM`:

```python
from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ChÃºng ta cÃ³ thá»ƒ xem mÃ´ hÃ¬nh nÃ y cÃ³ bao nhiÃªu tham sá»‘ báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `summary()`:

```python
model(model.dummy_inputs)  # XÃ¢y dá»±ng mÃ´ hÃ¬nh
model.summary()
```

```python out
Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________
```

{/if}

Vá»›i khoáº£ng 67 triá»‡u tham sá»‘, DistilBERT nhá» hÆ¡n khoáº£ng hai láº§n so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ BERT, gáº§n nhÆ° Ä‘Æ°á»£c hiá»ƒu lÃ  tÄƒng tá»‘c gáº¥p hai láº§n khi huáº¥n luyá»‡n - tháº­t tuyá»‡t! BÃ¢y giá» chÃºng ta hÃ£y xem nhá»¯ng loáº¡i token nÃ o mÃ´ hÃ¬nh nÃ y dá»± Ä‘oÃ¡n lÃ  cÃ³ nhiá»u kháº£ nÄƒng hoÃ n thÃ nh má»™t máº«u vÄƒn báº£n nhá»:

```python
text = "This is a great [MASK]."
```

LÃ  con ngÆ°á»i, chÃºng ta cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng ra nhiá»u kháº£ nÄƒng Ä‘á»‘i vá»›i token `[MASK]`, vÃ­ dá»¥ "day", "ride", hoáº·c "painting". Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, cÃ¡c dá»± Ä‘oÃ¡n phá»¥ thuá»™c vÃ o kho ngá»¯ liá»‡u mÃ´ hÃ¬nh Ä‘Ã³ huáº¥n luyá»‡n, vÃ¬ nÃ³ há»c cÃ¡ch chá»n cÃ¡c máº«u thá»‘ng kÃª cÃ³ trong dá»¯ liá»‡u. Giá»‘ng BERT, DistilBERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn bá»™ dá»¯ liá»‡u [English Wikipedia](https://huggingface.co/datasets/wikipedia) vÃ  [BookCorpus](https://huggingface.co/datasets/bookcorpus), nÃªn ta kÃ¬ vá»ng cÃ¡c dá»± Ä‘oÃ¡n cho `[MASK]` sáº½ pháº£n Ã¡nh cÃ¡c máº£ng nÃ y. Äá»ƒ dá»± Ä‘oÃ¡n ta cáº§n trÃ¬nh tokenizer cá»§a DistilBERT táº¡o ra cÃ¡c Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh, vÃ¬ váº­y hÃ£y táº£i tá»« Hub:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Vá»›i má»™t tokenizer vÃ  má»™t mÃ´ hÃ¬nh, ta cÃ³ thá»ƒ truyá»n cÃ¡c Ä‘oáº¡n vÄƒn vÃ­ dá»¥ tá»›i mÃ´ hÃ¬nh, trÃ­ch xuáº¥t logits, vÃ  xin ra 5 á»©ng cá»­ viÃªn: 

{#if fw === 'pt'}

```python
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# TÃ¬m vá»‹ trÃ­ [MASK] vÃ  trÃ­ch xuáº¥t logit
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Chá»n á»©ng viÃªn cho [MASK] vá»›i logit cao nháº¥t
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")
```

{:else}

```python
import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# TÃ¬m vá»‹ trÃ­ [MASK] vÃ  trÃ­ch xuáº¥t logit
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# PChá»n á»©ng viÃªn cho [MASK] vá»›i logit cao nháº¥t
# ChÃºng ta phá»§ Ä‘á»‹nh máº£ng trÆ°á»›c argsort Ä‘á»ƒ láº¥y logits lá»›n nháº¥t chá»© khÃ´ng pháº£i nhá» nháº¥t
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")
```

{/if}

```python out
'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y tá»« káº¿t quáº£ Ä‘áº§u ra ráº±ng cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh Ä‘á» cáº­p Ä‘áº¿n cÃ¡c thuáº­t ngá»¯ hÃ ng ngÃ y, Ä‘iá»u nÃ y cÃ³ láº½ khÃ´ng cÃ³ gÃ¬ Ä‘Ã¡ng ngáº¡c nhiÃªn khi dá»±a trÃªn ná»n táº£ng cá»§a Wikipedia tiáº¿ng Anh. HÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ thay Ä‘á»•i máº£ng nÃ y thÃ nh má»™t thá»© gÃ¬ Ä‘Ã³ thÃ­ch há»£p hÆ¡n má»™t chÃºt - cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim phÃ¢n cá»±c cao!

## Bá»™ dá»¯ liá»‡u

Äá»ƒ giá»›i thiá»‡u viá»‡c thÃ­ch á»©ng chuyÃªn mÃ´n, chÃºng ta sáº½ sá»­ dá»¥ng bá»™ dá»¯ liá»‡u ná»•i tiáº¿ng [Large Movie Review Dataset](https://huggingface.co/datasets/imdb)(hay viáº¿t táº¯t lÃ  IMDb), lÃ  táº­p há»£p cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh phÃ¢n tÃ­ch cáº£m xÃºc. Báº±ng cÃ¡ch tinh chá»‰nh DistilBERT trÃªn kho ngá»¯ liá»‡u nÃ y, chÃºng ta hy vá»ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ sáº½ Ä‘iá»u chá»‰nh vá»‘n tá»« vá»±ng cá»§a nÃ³ tá»« dá»¯ liá»‡u thá»±c táº¿ cá»§a Wikipedia mÃ  nÃ³ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c yáº¿u tá»‘ chá»§ quan hÆ¡n cá»§a cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim. ChÃºng ta cÃ³ thá»ƒ láº¥y dá»¯ liá»‡u tá»« Hugging Face Hub báº±ng hÃ m `load_dataset()` tá»« ğŸ¤— Datasets:

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng má»—i pháº§n `huáº¥n luyá»‡n` vÃ  `kiá»ƒm thá»­` bao gá»“m 25,000 Ä‘Ã¡nh giÃ¡, trong khi pháº§n khÃ´ng Ä‘Æ°á»£c gáº¯n nhÃ£n Ä‘Æ°á»£c gá»i lÃ  `phi giÃ¡m sÃ¡t` chá»©a 50,000 Ä‘Ã¡nh giÃ¡. ChÃºng ta hÃ£y xem má»™t vÃ i máº«u Ä‘á»ƒ cÃ³ Ã½ tÆ°á»Ÿng vá» loáº¡i vÄƒn báº£n mÃ  ta Ä‘ang xá»­ lÃ½. NhÆ° chÃºng ta Ä‘Ã£ thá»±c hiá»‡n trong cÃ¡c chÆ°Æ¡ng trÆ°á»›c cá»§a khÃ³a há»c, chÃºng ta sáº½ xÃ¢u chuá»—i cÃ¡c hÃ m `Dataset.shuffle()` vÃ  `Dataset.select()` Ä‘á»ƒ táº¡o má»™t máº«u ngáº«u nhiÃªn:

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")
```

```python out

'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clichÃ©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'
```

ÄÃºng, Ä‘Ã¢y cháº¯c cháº¯n lÃ  nhá»¯ng bÃ i Ä‘Ã¡nh giÃ¡ phim, vÃ  náº¿u báº¡n Ä‘á»§ lá»›n, báº¡n tháº­m chÃ­ cÃ³ thá»ƒ hiá»ƒu nháº­n xÃ©t trong bÃ i Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng vá» viá»‡c sá»Ÿ há»¯u phiÃªn báº£n VHS ğŸ˜œ! Máº·c dÃ¹ chÃºng ta sáº½ khÃ´ng cáº§n nhÃ£n Ä‘á»ƒ cho mÃ´ hÃ¬nh ngÃ´n ngá»¯, nhÆ°ng chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng `0` biá»ƒu thá»‹ má»™t Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c, trong khi `1` tÆ°Æ¡ng á»©ng vá»›i má»™t Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Táº¡o ra cÃ¡c máº«u ngáº«u nhiá»n tá»« pháº§n `phi giÃ¡m sÃ¡t` vÃ  kiá»ƒm Ä‘á»‹nh xem nhÃ£n cá»§a chÃºng lÃ  `0` hay `1`. Khi Ä‘ang á»Ÿ Ä‘Ã³, báº¡n cÅ©ng cÃ³ thá»ƒ kiá»ƒm tra xem cÃ¡c nhÃ£n trong pháº§n `huáº¥n luyá»‡n` vÃ  `kiá»ƒm thá»­` cÃ³ thá»±c sá»­ lÃ  `0` hoáº·c `1` khÃ´ng -- Ä‘Ã¢y lÃ  má»™t pháº§n kiá»ƒm tra há»¯u Ã­ch mÃ nhá»¯ng nhÃ  NLP nÃªn thá»±c hiá»‡n Ä‘áº§u dá»± Ã¡n!. 

</Tip>

BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ má»™t cÃ¡i nhÃ¬n nhanh vá» dá»¯ liá»‡u, hÃ£y Ä‘i sÃ¢u vÃ o viá»‡c chuáº©n bá»‹ nÃ³ cho viá»‡c láº­p mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i. NhÆ° chÃºng ta sáº½ tháº¥y, cÃ³ má»™t sá»‘ bÆ°á»›c bá»• sung mÃ  ngÆ°á»i ta cáº§n thá»±c hiá»‡n so vá»›i cÃ¡c tÃ¡c vá»¥ phÃ¢n loáº¡i chuá»—i mÃ  chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 3](/course/chapter3). Äi thÃ´i!

## Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

<Youtube id="8PmhEIXhBvI"/>

Äá»‘i vá»›i cáº£ mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± Ä‘á»™ng há»“i quy vÃ  bá»‹ áº©n Ä‘i, má»™t bÆ°á»›c tiá»n xá»­ lÃ½ phá»• biáº¿n lÃ  ná»‘i táº¥t cáº£ cÃ¡c máº«u vÃ  sau Ä‘Ã³ chia toÃ n bá»™ ngá»¯ liá»‡u thÃ nh cÃ¡c pháº§n cÃ³ kÃ­ch thÆ°á»›c báº±ng nhau. Äiá»u nÃ y hoÃ n toÃ n khÃ¡c vá»›i cÃ¡ch tiáº¿p cáº­n thÃ´ng thÆ°á»ng, khi chÃºng ta chá»‰ cáº§n tokenize cÃ¡c máº«u riÃªng láº». Táº¡i sao láº¡i ná»‘i má»i thá»© láº¡i vá»›i nhau? LÃ½ do lÃ  cÃ¡c máº«u riÃªng láº» cÃ³ thá»ƒ bá»‹ cáº¯t ngáº¯n náº¿u chÃºng quÃ¡ dÃ i vÃ  Ä‘iá»u Ä‘Ã³ sáº½ dáº«n Ä‘áº¿n viá»‡c máº¥t thÃ´ng tin cÃ³ thá»ƒ há»¯u Ã­ch cho tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯!

VÃ¬ váº­y, Ä‘á»ƒ báº¯t Ä‘áº§u, trÆ°á»›c tiÃªn chÃºng ta sáº½ tokenize kho tÃ i liá»‡u cá»§a mÃ¬nh nhÆ° bÃ¬nh thÆ°á»ng, nhÆ°ng _khÃ´ng_ Ä‘áº·t tÃ¹y chá»n `truncation=True` trong trÃ¬nh tokenize cá»§a chÃºng ta. ChÃºng ta cÅ©ng sáº½ láº¥y cÃ¡c ID tá»« náº¿u chÃºng cÃ³ sáºµn ((chÃºng sáº½ cÃ³ sáºµn náº¿u ta Ä‘ang sá»­ dá»¥ng cÃ´ng cá»¥ tokenize nhanh, nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong [ChÆ°Æ¡ng 6](/course/chap6/3)), vÃ¬ ta sáº½ cáº§n chÃºng sau nÃ y Ä‘á»ƒ thá»±c hiá»‡n che toÃ n bá»™ tá»«. ChÃºng ta sáº½ gÃ³i nÃ³ trong má»™t hÃ m Ä‘Æ¡n giáº£n vÃ  trong khi thá»±c hiá»‡n, ta sáº½ xÃ³a cÃ¡c cá»™t `text` vÃ  `label` vÃ¬ khÃ´ng cáº§n chÃºng ná»¯a:

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# DÃ¹ng batched=True Ä‘á»ƒ kÃ­ch hoáº¡t Ä‘a luá»“ng nhanh!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})
```

VÃ¬ DistilBERT lÃ  má»™t mÃ´ hÃ¬nh giá»‘ng nhÆ° BERT, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c tokenize bao gá»“m `input_ids` vÃ  `attention_mask` ta Ä‘Ã£ tháº¥y trong cÃ¡c chÆ°Æ¡ng khÃ¡c, cÅ©ng nhÆ° `word_ids` mÃ  ta Ä‘Ã£ thÃªm vÃ o.

Gá» chÃºng ta Ä‘Ã£ tokenize cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim cá»§a mÃ¬nh, bÆ°á»›c tiáº¿p theo lÃ  nhÃ³m táº¥t cáº£ chÃºng láº¡i vá»›i nhau vÃ  chia káº¿t quáº£ thÃ nh nhiá»u pháº§n. NhÆ°ng nhá»¯ng khá»‘i nÃ y pháº£i lá»›n Ä‘áº¿n má»©c nÃ o? Äiá»u nÃ y cuá»‘i cÃ¹ng sáº½ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi dung lÆ°á»£ng bá»™ nhá»› GPU mÃ  báº¡n cÃ³ sáºµn, nhÆ°ng Ä‘iá»ƒm khá»Ÿi Ä‘áº§u tá»‘t lÃ  xem kÃ­ch thÆ°á»›c ngá»¯ cáº£nh tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh lÃ  bao nhiÃªu. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c suy ra báº±ng cÃ¡ch kiá»ƒm tra thuá»™c tÃ­nh `model_max_length` cá»§a tokenizer:

```python
tokenizer.model_max_length
```

```python out
512
```

GiÃ¡ trá»‹ nÃ y cÃ³ nguá»“n gá»‘c tá»« tá»‡p *tokenizer_config.json* Ä‘Æ°á»£c liÃªn káº¿t vá»›i má»™t checkpoint; trong trÆ°á»ng há»£p nÃ y, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng kÃ­ch thÆ°á»›c ngá»¯ cáº£nh lÃ  512 token, giá»‘ng nhÆ° vá»›i BERT.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Má»™t sá»‘ mÃ´ hÃ¬nh Transformer, nhÆ°[BigBird](https://huggingface.co/google/bigbird-roberta-base) vÃ  [Longformer](hf.co/allenai/longformer-base-4096),cÃ³ Ä‘á»™ dÃ i ngá»¯ cáº£nh dÃ i hÆ¡n nhiá»u so vá»›i BERT vÃ  cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘á»i Ä‘áº§u khÃ¡c. Khá»Ÿi táº¡o tokenizer cho má»™t trong nhá»¯ng checkpoint vÃ  xÃ¡c minh ráº±ng `model_max_length` tÆ°Æ¡ng á»©ng vá»›i nhá»¯ng gÃ¬ Ä‘Æ°á»£c trÃ­ch dáº«n trÃªn tháº» mÃ´ hÃ¬nh cá»§a nÃ³.

</Tip>

VÃ¬ váº­y, Ä‘á»ƒ cháº¡y cÃ¡c thá»­ nghiá»‡m trÃªn GPU nhÆ° nhá»¯ng GPU Ä‘Æ°á»£c tÃ¬m tháº¥y trÃªn Google Colab, chÃºng ta sáº½ chá»n thá»© gÃ¬ Ä‘Ã³ nhá» hÆ¡n má»™t chÃºt cÃ³ thá»ƒ vá»«a vá»›i bá»™ nhá»›:

```python
chunk_size = 128
```

<Tip warning={true}>

LÆ°u Ã½ ráº±ng viá»‡c sá»­ dá»¥ng kÃ­ch thÆ°á»›c phÃ¢n Ä‘oáº¡n nhá» cÃ³ thá»ƒ gÃ¢y báº¥t lá»£i trong cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿, vÃ¬ váº­y báº¡n nÃªn sá»­ dá»¥ng kÃ­ch thÆ°á»›c tÆ°Æ¡ng á»©ng vá»›i trÆ°á»ng há»£p sá»­ dá»¥ng mÃ  báº¡n sáº½ Ã¡p dá»¥ng mÃ´ hÃ¬nh cá»§a mÃ¬nh.

</Tip>

BÃ¢y giá» Ä‘áº¿n pháº§n thÃº vá»‹. Äá»ƒ cho biáº¿t cÃ¡ch ná»‘i hoáº¡t Ä‘á»™ng, hÃ£y láº¥y má»™t vÃ i bÃ i Ä‘Ã¡nh giÃ¡ tá»« bá»™ huáº¥n luyá»‡n Ä‘Æ°á»£c tokenize vÃ  in ra sá»‘ lÆ°á»£ng token cho má»—i bÃ i Ä‘Ã¡nh giÃ¡:

```python
# Táº¡o ra má»™t danh sÃ¡ch cÃ¡c danh sÃ¡ch cho tá»«ng Ä‘áº·c trÆ°ng
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python out
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

We can then concatenate all these examples with a simple dictionary comprehension, as follows:

```python
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")
```

```python out
'>>> Concatenated reviews length: 951'
```

Tuyá»‡t vá»i, tá»•ng Ä‘á»™ dÃ i Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm tra - vÃ¬ váº­y bÃ¢y giá» hÃ£y chia cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c ná»‘i thÃ nh cÃ¡c pháº§n cÃ³ kÃ­ch thÆ°á»›c Ä‘Æ°á»£c cung cáº¥p bá»Ÿi `block_size`. Äá»ƒ lÃ m nhÆ° váº­y, chÃºng ta láº·p qua cÃ¡c Ä‘áº·c trÆ°ng trong `concatenated_examples` vÃ  sá»­ dá»¥ng kháº£ nÄƒng hiá»ƒu danh sÃ¡ch Ä‘á»ƒ táº¡o cÃ¡c pháº§n cá»§a tá»«ng Ä‘áº·c trÆ°ng. Káº¿t quáº£ lÃ  má»™t tá»« Ä‘iá»ƒn cÃ¡c khá»‘i cho tá»«ng Ä‘áº·c trÆ°ng:

```python
chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")
```

```python out
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y trong vÃ­ dá»¥ nÃ y, Ä‘oáº¡n cuá»‘i thÆ°á»ng sáº½ nhá» hÆ¡n kÃ­ch thÆ°á»›c Ä‘oáº¡n tá»‘i Ä‘a. CÃ³ hai chiáº¿n lÆ°á»£c chÃ­nh Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y:

* Bá» Ä‘oáº¡n cuá»‘i cÃ¹ng náº¿u nÃ³ nhá» hÆ¡n `chunk_size`.
* Äá»‡m Ä‘oáº¡n cuá»‘i cÃ¹ng cho Ä‘áº¿n khi Ä‘á»™ dÃ i cá»§a nÃ³ báº±ng `chunk_size`.

ChÃºng tÃ´i sáº½ thá»±c hiá»‡n cÃ¡ch tiáº¿p cáº­n Ä‘áº§u tiÃªn á»Ÿ Ä‘Ã¢y, vÃ¬ váº­y hÃ£y gÃ³i táº¥t cáº£ logic á»Ÿ trÃªn trong má»™t hÃ m duy nháº¥t mÃ  chÃºng tÃ´i cÃ³ thá»ƒ Ã¡p dá»¥ng cho táº­p dá»¯ liá»‡u Ä‘Æ°á»£c tokenize cá»§a mÃ¬nh:


```python
def group_texts(examples):
    # Ná»‘i táº¥t cáº£ cÃ¡c vÄƒn báº£n
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # TÃ­nh Ä‘á»™ dÃ i cá»§a cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c ná»‘i
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # ChÃºng tÃ´i bá» Ä‘oáº¡n cuá»‘i cÃ¹ng náº¿u nÃ³ nhá» hÆ¡n chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Chia pháº§n theo max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # Táº¡o cá»™t nhÃ£n má»›i
    result["labels"] = result["input_ids"].copy()
    return result
```

LÆ°u Ã½ ráº±ng trong bÆ°á»›c cuá»‘i cÃ¹ng cá»§a `group_texts()`, chÃºng ta táº¡o má»™t cá»™t má»›i `labels` lÃ  báº£n sao cá»§a cá»™t `input_ids`. NhÆ° chÃºng ta sáº½ tháº¥y ngay sau Ä‘Ã¢y, Ä‘Ã³ lÃ  bá»Ÿi vÃ¬ trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i, má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n cÃ¡c token Ä‘Æ°á»£c che ngáº«u nhiÃªn trong lÃ´ Ä‘áº§u vÃ o vÃ  báº±ng cÃ¡ch táº¡o cá»™t `labels`, chÃºng ta cung cáº¥p sá»± tháº­t cÆ¡ báº£n cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘á»ƒ há»c há»i.

BÃ¢y giá», hÃ£y Ã¡p dá»¥ng `group_texts()` cho cÃ¡c táº­p dá»¯ liá»‡u Ä‘Æ°á»£c tokenize cá»§a mÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m `Dataset.map()` Ä‘Ã¡ng tin cáº­y:

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})
```

Báº¡n cÃ³ thá»ƒ tháº¥y ráº±ng viá»‡c nhÃ³m vÃ  sau Ä‘Ã³ phÃ¢n chia cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Ã£ táº¡o ra nhiá»u máº«u hÆ¡n so vá»›i 25,000 máº«u ban Ä‘áº§u cá»§a chÃºng ta cho pháº§n tÃ¡ch `huáº¥n luyá»‡n` vÃ  `kiá»ƒm thá»­`. ÄÃ³ lÃ  bá»Ÿi vÃ¬ chÃºng ta hiá»‡n cÃ³ cÃ¡c máº«u liÃªn quan Ä‘áº¿n _token liÃªn tá»¥c_ tráº£i dÃ i trÃªn nhiá»u máº«u tá»« kho tÃ i liá»‡u gá»‘c. Báº¡n cÃ³ thá»ƒ tháº¥y Ä‘iá»u nÃ y má»™t cÃ¡ch rÃµ rÃ ng báº±ng cÃ¡ch tÃ¬m kiáº¿m cÃ¡c token Ä‘áº·c biá»‡t `[SEP]` vÃ  `[CLS]` trong má»™t trong cÃ¡c pháº§n:

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

Trong vÃ­ dá»¥ nÃ y, báº¡n cÃ³ thá»ƒ tháº¥y hai bÃ i Ä‘Ã¡nh giÃ¡ phim trÃ¹ng nhau, má»™t bÃ i vá» phim cáº¥p ba vÃ  bÃ i cÃ²n láº¡i vá» tÃ¬nh tráº¡ng vÃ´ gia cÆ°. HÃ£y cÅ©ng xem cÃ¡c nhÃ£n trÃ´ng nhÆ° tháº¿ nÃ o cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i:

```python out
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

NhÆ° mong Ä‘á»£i tá»« hÃ m `group_texts()` cá»§a chÃºng ta á»Ÿ trÃªn, hÃ m nÃ y trÃ´ng giá»‘ng há»‡t vá»›i `input_ids` Ä‘Ã£ Ä‘Æ°á»£c giáº£i mÃ£ - nhÆ°ng sau Ä‘Ã³ lÃ m tháº¿ nÃ o Ä‘á»ƒ mÃ´ hÃ¬nh cá»§a chÃºng ta cÃ³ thá»ƒ há»c Ä‘Æ°á»£c báº¥t cá»© Ä‘iá»u gÃ¬? ChÃºng ta Ä‘ang thiáº¿u má»™t bÆ°á»›c quan trá»ng: chÃ¨n token `[MASK]` á»Ÿ cÃ¡c vá»‹ trÃ­ ngáº«u nhiÃªn trong Ä‘áº§u vÃ o! HÃ£y xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y má»™t cÃ¡ch nhanh chÃ³ng trong quÃ¡ trÃ¬nh tinh chá»‰nh báº±ng cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘áº·c biá»‡t.

## Tinh chá»‰nh DistilBERT vá»›i API `Trainer`

Tinh chá»‰nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i gáº§n giá»‘ng nhÆ° tinh chá»‰nh mÃ´ hÃ¬nh phÃ¢n loáº¡i chuá»—i, giá»‘ng nhÆ° chÃºng ta Ä‘Ã£ lÃ m trong [ChÆ°Æ¡ng 3](/course/chapter3). Sá»± khÃ¡c biá»‡t duy nháº¥t lÃ  chÃºng ta cáº§n má»™t trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘áº·c biá»‡t cÃ³ thá»ƒ che giáº¥u ngáº«u nhiÃªn má»™t sá»‘ token trong má»—i lÃ´ vÄƒn báº£n. May máº¯n thay, ğŸ¤— Transformers Ä‘Æ°á»£c chuáº©n bá»‹ vá»›i má»™t `DataCollatorForLanguageModeling` dÃ nh riÃªng cho tÃ¡c vá»¥ nÃ y. ChÃºng ta chá»‰ cáº§n chuyá»ƒn nÃ³ vÃ o tokenizer vÃ  tham sá»‘ `mlm_probability` Ä‘á»ƒ chá»‰ Ä‘á»‹nh pháº§n nÃ o trong sá»‘ cÃ¡c token cáº§n che. ChÃºng tÃ´i sáº½ chá»n 15%, lÃ  sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng cho BERT vÃ  má»™t lá»±a chá»n phá»• biáº¿n trong cÃ¡c tÃ i liá»‡u:

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

Äá»ƒ xem cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a viá»‡c che ngáº«u nhiÃªn, hÃ£y cung cáº¥p má»™t vÃ i máº«u cho trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u. VÃ¬ nÃ³ mong Ä‘á»£i má»™t danh sÃ¡ch cÃ¡c `dict`, trong Ä‘Ã³ má»—i `dict` Ä‘áº¡i diá»‡n cho má»™t Ä‘oáº¡n vÄƒn báº£n liá»n ká», Ä‘áº§u tiÃªn chÃºng ta láº·p táº­p dá»¯ liá»‡u trÆ°á»›c khi cung cáº¥p lÃ´ cho bá»™ Ä‘á»‘i chiáº¿u. ChÃºng ta xÃ³a khÃ³a `"word_ids"` cho trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u nÃ y vÃ¬ nÃ³ khÃ´ng cáº§n chÃºng:

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george å®‡in stated )å…¬ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

Tá»‘t, nÃ³ Ä‘Ã£ hoáº¡t Ä‘á»™ng! ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng `[MASK]` Ä‘Ã£ Ä‘Æ°á»£c chÃ¨n ngáº«u nhiÃªn táº¡i cÃ¡c vá»‹ trÃ­ khÃ¡c nhau trong vÄƒn báº£n. ÄÃ¢y sáº½ lÃ  nhá»¯ng token mÃ  mÃ´ hÃ¬nh sáº½ pháº£i dá»± Ä‘oÃ¡n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n - vÃ  cÃ¡i hay cá»§a cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u lÃ  nÃ³ sáº½ ngáº«u nhiÃªn chÃ¨n `[MASK]` vá»›i má»i lÃ´!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Cháº¡y Ä‘oáº¡n mÃ£ trÃªn vÃ i láº§n Ä‘á»ƒ xem viá»‡c che ngáº«u nhiÃªn diá»…n ra ngay trÆ°á»›c máº¯t báº¡n! Äá»“ng thá»i  thá»­ thay tháº¿ phÆ°Æ¡ng thá»©c `tokenizer.decode()` báº±ng `tokenizer.convert_ids_to_tokens()` Ä‘á»ƒ tháº¥y ráº±ng Ä‘Ã´i khi má»™t token tá»« má»™t tá»« nháº¥t Ä‘á»‹nh bá»‹ che, chá»© khÃ´ng pháº£i nhá»¯ng cÃ¡i khÃ¡c.

</Tip>

{#if fw === 'pt'}

Má»™t tÃ¡c dá»¥ng phá»¥ cá»§a viá»‡c che ngáº«u nhiÃªn lÃ  cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ cá»§a chÃºng ta sáº½ khÃ´ng xÃ¡c Ä‘á»‹nh khi sá»­ dá»¥ng `Trainer`, vÃ¬ chÃºng ta sá»­ dá»¥ng cÃ¹ng má»™t cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u cho cÃ¡c táº­p huáº¥n luyá»‡n vÃ  kiá»ƒm thá»§. ChÃºng ta sáº½ tháº¥y á»Ÿ pháº§n sau, khi chÃºng ta xem xÃ©t viá»‡c tinh chá»‰nh vá»›i ğŸ¤— Accelerate, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng tÃ­nh linh hoáº¡t cá»§a vÃ²ng Ä‘Ã¡nh giÃ¡ tÃ¹y chá»‰nh nhÆ° tháº¿ nÃ o Ä‘á»ƒ Ä‘Ã³ng bÄƒng tÃ­nh ngáº«u nhiÃªn.

{/if}

Khi huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i, má»™t ká»¹ thuáº­t cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  ghÃ©p cÃ¡c tá»« láº¡i vá»›i nhau, khÃ´ng chá»‰ cÃ¡c token riÃªng láº». CÃ¡ch tiáº¿p cáº­n nÃ y Ä‘Æ°á»£c gá»i lÃ  _whole word masking_ hay _che toÃ n bá»™ tá»«_. Náº¿u chÃºng ta muá»‘n che toÃ n bá»™ tá»«, chÃºng ta sáº½ cáº§n pháº£i tá»± xÃ¢y dá»±ng má»™t bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u. Bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u chá»‰ lÃ  má»™t chá»©c nÄƒng láº¥y danh sÃ¡ch cÃ¡c máº«u vÃ  chuyá»ƒn Ä‘á»•i chÃºng thÃ nh má»™t lÃ´, vÃ¬ váº­y hÃ£y lÃ m Ä‘iá»u nÃ y ngay bÃ¢y giá»! ChÃºng ta sáº½ sá»­ dá»¥ng cÃ¡c ID tá»« Ä‘Ã£ tÃ­nh toÃ¡n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ táº¡o báº£n Ä‘á»“ giá»¯a cÃ¡c chá»‰ sá»‘ tá»« vÃ  cÃ¡c mÃ£ thÃ´ng bÃ¡o tÆ°Æ¡ng á»©ng, sau Ä‘Ã³ quyáº¿t Ä‘á»‹nh ngáº«u nhiÃªn nhá»¯ng tá»« nÃ o cáº§n che vÃ  che cÃ¡c Ä‘áº§u vÃ o. LÆ°u Ã½ ráº±ng táº¥t cáº£ cÃ¡c nhÃ£n Ä‘á»u lÃ  `-100` ngoáº¡i trá»« cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng vá»›i cÃ¡c tá»« bá»‹ che.

{#if fw === 'pt'}

```py
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Táº¡o ra Ã¡nh xáº¡ giá»¯a cÃ¡c tá»« vÃ  chá»‰ má»¥c token tÆ°Æ¡ng á»©ng
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Che ngáº«u nhiá»n tá»«
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return default_data_collator(features)
```

{:else}

```py
import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Táº¡o ra Ã¡nh xáº¡ giá»¯a cÃ¡c tá»« vÃ  chá»‰ má»¥c token tÆ°Æ¡ng á»©ng
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Che ngáº«u nhiá»n tá»«
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return tf_default_data_collator(features)
```

{/if}

Tiáº¿p theo, ta cso thá»ƒ thá»­ trÃªn má»™t vÃ i máº«u nhÆ° trÃªn:

```py
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python out
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Cháº¡y Ä‘oáº¡n mÃ£ trÃªn vÃ i láº§n Ä‘á»ƒ xem viá»‡c che ngáº«u nhiÃªn diá»…n ra ngay trÆ°á»›c máº¯t báº¡n! Äá»“ng thá»i  thá»­ thay tháº¿ phÆ°Æ¡ng thá»©c `tokenizer.decode()` báº±ng `tokenizer.convert_ids_to_tokens()` Ä‘á»ƒ tháº¥y ráº±ng Ä‘Ã´i khi má»™t token tá»« má»™t tá»« nháº¥t Ä‘á»‹nh bá»‹ che, chá»© khÃ´ng pháº£i nhá»¯ng cÃ¡i khÃ¡c.

</Tip>

Giá» chÃºng ta cÃ³ hai trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u, pháº§n cÃ²n láº¡i cá»§a cÃ¡c bÆ°á»›c tinh chá»‰nh lÃ  tiÃªu chuáº©n. QuÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ³ thá»ƒ máº¥t má»™t khoáº£ng thá»i gian trÃªn Google Colab náº¿u báº¡n khÃ´ng Ä‘á»§ may máº¯n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c GPU P100 tháº§n thoáº¡i ğŸ˜­, vÃ¬ váº­y, trÆ°á»›c tiÃªn chÃºng ta sáº½ giáº£m kÃ­ch thÆ°á»›c cá»§a táº­p huáº¥n luyá»‡n xuá»‘ng cÃ²n vÃ i nghÃ¬n máº«u. Äá»«ng lo láº¯ng, chÃºng ta sáº½ váº«n nháº­n Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ¡ tá»‘t! Má»™t cÃ¡ch nhanh chÃ³ng Ä‘á»ƒ giáº£m máº«u má»™t táº­p dá»¯ liá»‡u trong ğŸ¤— Datasets lÃ  thÃ´ng qua hÃ m `Dataset.train_test_split()` mÃ  chÃºng ta Ä‘Ã£ tháº¥y trong [Chapter 5](/course/chapter5):

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})
```

Äiá»u nÃ y Ä‘Ã£ tá»± Ä‘á»™ng táº¡o cÃ¡c pháº§n tÃ¡ch `huáº¥n luyá»‡n` vÃ  `kiá»ƒm thá»­` má»›i, vá»›i kÃ­ch thÆ°á»›c táº­p huáº¥n luyá»‡n Ä‘Æ°á»£c Ä‘áº·t thÃ nh 10,000 máº«u vÃ  xÃ¡c thá»±c Ä‘Æ°á»£c Ä‘áº·t thÃ nh 10% - vui lÃ²ng tÄƒng Ä‘iá»u nÃ y náº¿u báº¡n cÃ³ GPU máº¡nh! Äiá»u tiáº¿p theo chÃºng ta cáº§n lÃ m lÃ  Ä‘Äƒng nháº­p vÃ o Hugging Face Hub. Náº¿u báº¡n Ä‘ang cháº¡y mÃ£ nÃ y trong notebook, báº¡n cÃ³ thá»ƒ lÃ m nhÆ° váº­y vá»›i chá»©c nÄƒng tiá»‡n Ã­ch sau:

```python
from huggingface_hub import notebook_login

notebook_login()
```

sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p cá»§a mÃ¬nh. NgoÃ i ra, báº¡n cÃ³ thá»ƒ cháº¡y:

```
huggingface-cli login
```

trong thiáº¿t bá»‹ Ä‘áº§u cuá»‘i yÃªu thÃ­ch cá»§a báº¡n vÃ  Ä‘Äƒng nháº­p á»Ÿ Ä‘Ã³.

{#if fw === 'tf'}

Khi Ä‘Ã£ Ä‘Äƒng nháº­p, chÃºng ta cÃ³ thá»ƒ táº¡o táº­p dá»¯ liá»‡u `tf.data` cá»§a mÃ¬nh. ChÃºng tÃ´i sáº½ chá»‰ sá»­ dá»¥ng trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u tiÃªu chuáº©n á»Ÿ Ä‘Ã¢y, nhÆ°ng báº¡n cÅ©ng cÃ³ thá»ƒ thá»­ trÃ¬nh Ä‘á»‘i chiáº¿u che toÃ n bá»™ tá»« vÃ  so sÃ¡nh káº¿t quáº£ nhÆ° má»™t bÃ i táº­p:

```python
tf_train_dataset = downsampled_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = downsampled_dataset["test"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

Tiáº¿p theo, chÃºng ta thiáº¿t láº­p cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n vÃ  biÃªn dá»‹ch mÃ´ hÃ¬nh. ChÃºng ta sá»­ dá»¥ng hÃ m `create_optimizer()` tá»« thÆ° viá»‡n ğŸ¤— Transformers, cung cáº¥p cho chÃºng ta trÃ¬nh tá»‘i Æ°u hÃ³a `AdamW` vá»›i phÃ¢n rÃ£ tá»‘c Ä‘á»™ há»c tuyáº¿n tÃ­nh. ChÃºng ta cÅ©ng sá»­ dá»¥ng hÃ m máº¥t mÃ¡t cÃ³ sáºµn cá»§a mÃ´ hÃ¬nh, lÃ  máº·c Ä‘á»‹nh khi khÃ´ng cÃ³ tá»•n tháº¥t nÃ o Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh lÃ m tham sá»‘ cho `compile()` vÃ  Ä‘áº·t Ä‘á»™ chÃ­nh xÃ¡c huáº¥n luyá»‡n thÃ nh `"mixed_float16"`. LÆ°u Ã½ ráº±ng náº¿u báº¡n Ä‘ang sá»­ dá»¥ng GPU Colab hoáº·c GPU khÃ¡c khÃ´ng cÃ³ há»— trá»£ float16 tÄƒng tá»‘c, báº¡n cÃ³ thá»ƒ nÃªn Ä‘á»•i dÃ²ng Ä‘Ã³ thÃ nh chÃº thÃ­ch.

NgoÃ i ra, chÃºng ta thiáº¿t láº­p má»™t `PushToHubCallback` sáº½ lÆ°u mÃ´ hÃ¬nh vÃ o Hub sau má»—i epoch. Báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n báº±ng tham sá»‘ `hub_model_id` (cá»¥ thá»ƒ lÃ  báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y Ä‘áº¿n má»™t tá»• chá»©c). VÃ­ dá»¥: Ä‘á»ƒ Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course), chÃºng tÃ´i Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/distilbert-finetuned-imdb"`. Theo máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ¬ váº­y trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, nÃ³ sáº½ lÃ  `"lewtun/distilbert-finetuned-imdb"`.

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Huáº¥n luyá»‡n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)
```

BÃ¢y giá» chÃºng ta Ä‘Ã£ sáºµn sÃ ng cháº¡y `model.fit()` - nhÆ°ng trÆ°á»›c khi lÃ m nhÆ° váº­y, hÃ£y xem xÃ©t ngáº¯n gá»n _perplexity_, lÃ  má»™t chá»‰ sá»‘ phá»• biáº¿n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯.

{:else}

Khi Ä‘Ã£ Ä‘Äƒng nháº­p, chÃºng ta cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh cÃ¡c tham sá»‘ cho `Trainer`:

```python
from transformers import TrainingArguments

batch_size = 64
# In ra sá»± máº¥t mÃ¡t khi huáº¥n luyá»‡n á»Ÿ má»—i epoch
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)
```

á» Ä‘Ã¢y, chÃºng ta Ä‘Ã£ Ä‘iá»u chá»‰nh má»™t sá»‘ tÃ¹y chá»n máº·c Ä‘á»‹nh, bao gá»“m `log_steps` Ä‘á»ƒ Ä‘áº£m báº£o theo dÃµi sá»± máº¥t mÃ¡t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n theo tá»«ng epoch. ChÃºng ta cÅ©ng Ä‘Ã£ sá»­ dá»¥ng `fp16=True` Ä‘á»ƒ cho phÃ©p huáº¥n luyá»‡n chÃ­nh xÃ¡c há»—n há»£p, giÃºp tÄƒng tá»‘c Ä‘á»™. Theo máº·c Ä‘á»‹nh, `Trainer` sáº½ loáº¡i bá» báº¥t ká»³ cá»™t nÃ o khÃ´ng pháº£i lÃ  má»™t pháº§n cá»§a phÆ°Æ¡ng thá»©c `forward()` cá»§a mÃ´ hÃ¬nh. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  náº¿u báº¡n Ä‘ang sá»­ dá»¥ng cÃ´ng cá»¥ che toÃ n bá»™ tá»«, báº¡n cÅ©ng cáº§n Ä‘áº·t `remove_unused_columns=False` Ä‘á»ƒ Ä‘áº£m báº£o chÃºng ta khÃ´ng máº¥t cá»™t `word_ids` trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

LÆ°u Ã½ ráº±ng báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n báº±ng tham sá»‘ `hub_model_id` (cá»¥ thá»ƒ lÃ  báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y Ä‘áº¿n má»™t tá»• chá»©c). VÃ­ dá»¥: khi chÃºng ta Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course), chÃºng ta Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` vÃ o `TrainingArguments`. Theo máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ¬ váº­y trong trÆ°á»ng há»£p cá»§a chÃºng ta, nÃ³ sáº½ lÃ `"lewtun/distilbert-finetuned-imdb"`.

BÃ¢y giá» chÃºng ta cÃ³ táº¥t cáº£ cÃ¡c thÃ nh pháº§n Ä‘á»ƒ táº¡o ra `Trainer`. á» Ä‘Ã¢y chÃºng ta chá»‰ sá»­ dá»¥ng `data_collator` tiÃªu chuáº©n, nhÆ°ng báº¡n cÃ³ thá»ƒ thá»­ toÃ n bá»™ cÃ´ng cá»¥ che toÃ n bá»™ tá»« vÃ  so sÃ¡nh káº¿t quáº£ nhÆ° má»™t bÃ i táº­p:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Giá» chÃºng ta Ä‘Ã£ sáºµn sÃ ng cháº¡y `trainer.train()` - nhÆ°ng trÆ°á»›c khi lÃ m nhÆ° váº­y, chÃºng ta hÃ£y xem xÃ©t ngáº¯n gá»n _perplexity_, lÃ  má»™t chá»‰ sá»‘ phá»• biáº¿n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯.

{/if}

### Perplexity cho mÃ´ hÃ¬nh ngÃ´n ngá»¯

<Youtube id="NURcDHhYe98"/>

KhÃ´ng giá»‘ng nhÆ° cÃ¡c tÃ¡c vá»¥ khÃ¡c nhÆ° phÃ¢n loáº¡i vÄƒn báº£n hoáº·c há»i Ä‘Ã¡p mÃ  chÃºng ta Ä‘Æ°á»£c cung cáº¥p má»™t kho ngá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n Ä‘á»ƒ huáº¥n luyá»‡n, vá»›i mÃ´ hÃ¬nh ngÃ´n ngá»¯, ta khÃ´ng cÃ³ báº¥t ká»³ nhÃ£n rÃµ rÃ ng nÃ o. Váº­y lÃ m cÃ¡ch nÃ o Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘iá»u gÃ¬ táº¡o nÃªn má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘t? Giá»‘ng nhÆ° tÃ­nh nÄƒng tá»± Ä‘á»™ng sá»­a lá»—i trong Ä‘iá»‡n thoáº¡i cá»§a báº¡n, má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘t lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ Ä‘á»‹nh xÃ¡c suáº¥t cao cho cÃ¡c cÃ¢u Ä‘Ãºng ngá»¯ phÃ¡p vÃ  xÃ¡c suáº¥t tháº¥p cho cÃ¡c cÃ¢u vÃ´ nghÄ©a. Äá»ƒ giÃºp báº¡n biáº¿t rÃµ hÆ¡n vá» hÃ¬nh thá»©c nÃ y, báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y toÃ n bá»™ táº­p há»£p "tá»± Ä‘á»™ng sá»­a lá»—i" trá»±c tuyáº¿n, trong Ä‘Ã³ mÃ´ hÃ¬nh trong Ä‘iá»‡n thoáº¡i Ä‘Ã£ táº¡o ra má»™t sá»‘ hoÃ n thÃ nh khÃ¡ hÃ i hÆ°á»›c (vÃ  thÆ°á»ng khÃ´ng phÃ¹ há»£p)!

{#if fw === 'pt'}

Giáº£ sá»­ bá»™ kiá»ƒm thá»­ cá»§a chÃºng ta bao gá»“m háº§u háº¿t cÃ¡c cÃ¢u Ä‘Ãºng ngá»¯ phÃ¡p, thÃ¬ má»™t cÃ¡ch Ä‘á»ƒ Ä‘o lÆ°á»ng cháº¥t lÆ°á»£ng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lÃ  tÃ­nh toÃ¡n xÃ¡c suáº¥t nÃ³ gÃ¡n cho tá»« tiáº¿p theo trong táº¥t cáº£ cÃ¡c cÃ¢u cá»§a bá»™ kiá»ƒm thá»­. Kháº£ nÄƒng xáº£y ra cao chá»‰ ra ráº±ng mÃ´ hÃ¬nh khÃ´ng bá»‹ "ngáº¡c nhiÃªn" hoáº·c "bá»‘i rá»‘i" bá»Ÿi cÃ¡c máº«u khÃ´ng nhÃ¬n tháº¥y vÃ  cho tháº¥y nÃ³ Ä‘Ã£ há»c Ä‘Æ°á»£c cÃ¡c máº«u ngá»¯ phÃ¡p cÆ¡ báº£n trong ngÃ´n ngá»¯. CÃ³ nhiá»u Ä‘á»‹nh nghÄ©a toÃ¡n há»c khÃ¡c nhau vá» perplexity, nhÆ°ng chÃºng ta sáº½ sá»­ dá»¥ng Ä‘á»‹nh nghÄ©a lÃ  hÃ m mÅ© cá»§a máº¥t mÃ¡t entropy chÃ©o. Do Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n perplexity cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a mÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m `Trainer.evaluate()` Ä‘á»ƒ tÃ­nh toÃ¡n máº¥t mÃ¡t entropy chÃ©o trÃªn táº­p kiá»ƒm thá»­ vÃ  sau Ä‘Ã³ láº¥y theo cáº¥p sá»‘ nhÃ¢n cá»§a káº¿t quáº£:


```python
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}


Giáº£ sá»­ bá»™ kiá»ƒm thá»­ cá»§a chÃºng ta bao gá»“m háº§u háº¿t cÃ¡c cÃ¢u Ä‘Ãºng ngá»¯ phÃ¡p, thÃ¬ má»™t cÃ¡ch Ä‘á»ƒ Ä‘o lÆ°á»ng cháº¥t lÆ°á»£ng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lÃ  tÃ­nh toÃ¡n xÃ¡c suáº¥t nÃ³ gÃ¡n cho tá»« tiáº¿p theo trong táº¥t cáº£ cÃ¡c cÃ¢u cá»§a bá»™ kiá»ƒm thá»­. Kháº£ nÄƒng xáº£y ra cao chá»‰ ra ráº±ng mÃ´ hÃ¬nh khÃ´ng bá»‹ "ngáº¡c nhiÃªn" hoáº·c "bá»‘i rá»‘i" bá»Ÿi cÃ¡c máº«u khÃ´ng nhÃ¬n tháº¥y vÃ  cho tháº¥y nÃ³ Ä‘Ã£ há»c Ä‘Æ°á»£c cÃ¡c máº«u ngá»¯ phÃ¡p cÆ¡ báº£n trong ngÃ´n ngá»¯. CÃ³ nhiá»u Ä‘á»‹nh nghÄ©a toÃ¡n há»c khÃ¡c nhau vá» perplexity, nhÆ°ng chÃºng ta sáº½ sá»­ dá»¥ng Ä‘á»‹nh nghÄ©a lÃ  hÃ m mÅ© cá»§a máº¥t mÃ¡t entropy chÃ©o. Do Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n perplexity cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a mÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m `Trainer.evaluate()` Ä‘á»ƒ tÃ­nh toÃ¡n máº¥t mÃ¡t entropy chÃ©o trÃªn táº­p kiá»ƒm thá»­ vÃ  sau Ä‘Ã³ láº¥y theo cáº¥p sá»‘ nhÃ¢n cá»§a káº¿t quáº£:

```python
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 21.75
```

Perplexity tháº¥p hÆ¡n cÃ³ nghÄ©a lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘t hÆ¡n vÃ  chÃºng ta cÃ³ thá»ƒ tháº¥y á»Ÿ Ä‘Ã¢y ráº±ng mÃ´ hÃ¬nh báº¯t Ä‘áº§u cá»§a chÃºng ta cÃ³ má»™t giÃ¡ trá»‹ hÆ¡i lá»›n. HÃ£y xem liá»‡u chÃºng ta cÃ³ thá»ƒ háº¡ tháº¥p nÃ³ báº±ng cÃ¡ch tinh chá»‰nh khÃ´ng! Äá»ƒ lÃ m Ä‘iá»u Ä‘Ã³, trÆ°á»›c tiÃªn chÃºng ta cháº¡y vÃ²ng láº·p huáº¥n luyá»‡n:

{#if fw === 'pt'}

```python
trainer.train()
```

{:else}

```python
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

vÃ  sau Ä‘Ã³ tÃ­nh káº¿t quáº£ perplexity trÃªn táº­p kiá»ƒm thá»­:

{#if fw === 'pt'}

```python
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

```python
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 11.32
```

Tá»‘t -- nÃ³ giáº£m perplexity, cho tháº¥y mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c Ä‘iá»u gÃ¬ Ä‘Ã³ vá» máº£ng Ä‘Ã¡nh giÃ¡ phim!

{#if fw === 'pt'}

Sau khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n káº¿t thÃºc, chÃºng ta cÃ³ thá»ƒ Ä‘áº©y tháº» mÃ´ hÃ¬nh cÃ³ thÃ´ng tin huáº¥n luyá»‡n vÃ o Hub (cÃ¡c checkpoint Ä‘Æ°á»£c lÆ°u trong quÃ¡ trÃ¬nh tá»± huáº¥n luyá»‡n):

Once training is finished, we can push the model card with the training information to the Hub (the checkpoints are saved during training itself):

```python
trainer.push_to_hub()
```

{/if}

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** Cháº¡y bÆ°á»›chuáº¥n luyá»‡n trÃªn sau khi thay Ä‘á»•i trÃ¬nh thu tháº­p dá»¯ liá»‡u thÃ nh che toÃ n bá»™ tá»«. Báº¡n cÃ³ nháº­n Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n khÃ´ng?

</Tip>

{#if fw === 'pt'}

Trong trÆ°á»ng há»£p cá»§a mÃ¬nh, chÃºng ta khÃ´ng cáº§n lÃ m gÃ¬ Ä‘áº·c biá»‡t vá»›i vÃ²ng huáº¥n luyá»‡n, nhÆ°ng má»™t sá»‘ trÆ°á»ng há»£p báº¡n sáº½ cáº§n pháº£i triá»ƒn khai má»™t sá»‘ logic tuá»³ chá»‰nh. Vá»›i nhá»¯ng á»©ng dá»¥ng nÃ y, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng ğŸ¤— Accelerate -- hÃ£y cÅ©ng xem xem!

## Tinh chá»‰nh DistilBERT vá»›i ğŸ¤— Accelerate

NhÆ° chÃºng ta Ä‘Ã£ tháº¥y vá»›i `Trainer`, viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ bá»‹ áº£n Ä‘i ráº¥t giá»‘ng vá»›i vÃ­ dá»¥ phÃ¢n loáº¡i vÄƒn báº£n tá»« [Chapter 3](/course/chapter3). TrÃªn thá»±c táº¿, sá»± tinh táº¿ duy nháº¥t lÃ  viá»‡c sá»­ dá»¥ng má»™t cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘áº·c biá»‡t vÃ  chÃºng ta Ä‘Ã£ Ä‘á» cáº­p Ä‘áº¿n Ä‘iá»u Ä‘Ã³ trÆ°á»›c Ä‘Ã³ trong pháº§n nÃ y!

Tuy nhiÃªn, chÃºng ta tháº¥y ráº±ng `DataCollatorForLanguageModeling` cÅ©ng Ã¡p dá»¥ng tÃ­nh nÄƒng che ngáº«u nhiÃªn vá»›i má»—i láº§n Ä‘Ã¡nh giÃ¡, vÃ¬ váº­y chÃºng ta sáº½ tháº¥y má»™t sá»‘ biáº¿n Ä‘á»™ng vá» perplexity vá»›i má»—i láº§n cháº¡y huáº¥n luyá»‡n. Má»™t cÃ¡ch Ä‘á»ƒ loáº¡i bá» tÃ­nh ngáº«u nhiÃªn nÃ y lÃ  Ã¡p dá»¥ng  che _chá»‰ má»™t láº§n_ trÃªn toÃ n bá»™ táº­p kiá»ƒm thá»­, sau Ä‘Ã³ sá»­ dá»¥ng trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u máº·c Ä‘á»‹nh trong ğŸ¤— Transformers Ä‘á»ƒ thu tháº­p cÃ¡c lÃ´ trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡. Äá»ƒ xem cÃ¡ch nÃ y hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o, hÃ£y triá»ƒn khai má»™t chá»©c nÄƒng Ä‘Æ¡n giáº£n Ã¡p dá»¥ng che trÃªn má»™t lÃ´, tÆ°Æ¡ng tá»± nhÆ° láº§n Ä‘áº§u cá»§a chÃºng ta vá»›i `DataCollatorForLanguageModeling`:

```python
def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # Táº¡o ra má»™t cá»™t "masked" má»›i cho má»—i cá»™t trong bá»™ dá»¯ liá»‡u
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}
```

Tiáº¿p theo, chÃºng ta sáº½ Ã¡p dá»¥ng chá»©c nÄƒng nÃ y cho táº­p kiá»ƒm thá»­ cá»§a mÃ¬nh vÃ  bá» cÃ¡c cá»™t khÃ´ng che Ä‘á»ƒ cÃ³ thá»ƒ thay tháº¿ chÃºng báº±ng nhá»¯ng cá»™t bá»‹ che. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng che toÃ n bá»™ tá»« báº±ng cÃ¡ch thay tháº¿ `data_collator` á»Ÿ trÃªn báº±ng cÃ¡i thÃ­ch há»£p, trong trÆ°á»ng há»£p Ä‘Ã³, báº¡n nÃªn xÃ³a dÃ²ng Ä‘áº§u tiÃªn táº¡i Ä‘Ã¢y:

```py
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ thiáº¿t láº­p bá»™ lÆ°u dá»¯ liá»‡u nhÆ° bÃ¬nh thÆ°á»ng, nhÆ°ng ta sáº½ sá»­ dá»¥ng `default_data_collator` tá»« ğŸ¤— Transformers cho táº­p kiá»ƒm Ä‘á»‹nh:

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)
```

Tá»« Ä‘Ã¢y, chÃºng ta lÃ m theo cÃ¡c bÆ°á»›c tiÃªu chuáº©n vá»›i ğŸ¤— Accelerate. YÃªu cáº§u Ä‘áº§u tiÃªn cá»§a cÃ´ng viá»‡c lÃ  táº£i má»™t phiÃªn báº£n má»›i cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c:

```
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

Sau Ä‘Ã³, chÃºng ta cáº§n chá»‰ Ä‘á»‹nh trÃ¬nh tá»‘i Æ°u hÃ³a; chÃºng ta sáº½ sá»­ dá»¥ng tiÃªu chuáº©n `AdamW`:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Vá»›i nhá»¯ng Ä‘á»‘i tÆ°á»£ng nÃ y, bÃ¢y giá» chÃºng ta cÃ³ thá»ƒ chuáº©n bá»‹ má»i thá»© cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n vá»›i Ä‘á»‘i tÆ°á»£ng `Accelerator`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

BÃ¢y giá» mÃ´ hÃ¬nh, trÃ¬nh tá»‘i Æ°u hÃ³a vÃ  bá»™ ghi dá»¯ liá»‡u cá»§a chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh cáº¥u hÃ¬nh, chÃºng ta cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh bá»™ láº­p lá»‹ch tá»‘c Ä‘á»™ há»c nhÆ° sau:

```python
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Chá»‰ cÃ³ má»™t Ä‘iá»u cuá»‘i cÃ¹ng cáº§n lÃ m trÆ°á»›c khi huáº¥n luyá»‡n: táº¡o má»™t kho lÆ°u trá»¯ mÃ´ hÃ¬nh trÃªn Hugging Face Hub! TrÆ°á»›c tiÃªn, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng thÆ° viá»‡n ğŸ¤— Hub Ä‘á»ƒ táº¡o tÃªn Ä‘áº§y Ä‘á»§ cho repo cá»§a mÃ¬nh:

```python
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'
```

sau Ä‘Ã³ táº¡o vÃ  sao chÃ©p kho lÆ°u trá»¯ báº±ng cÃ¡ch sá»­ dá»¥ng lá»›p `Repository` tá»« ğŸ¤— Hub:

```python
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)
```

Sau khi thá»±c hiá»‡n xong, viá»‡c viáº¿t ra toÃ n bá»™ vÃ²ng láº·p huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ chá»‰ lÃ  má»™t váº¥n Ä‘á» Ä‘Æ¡n giáº£n:

```python
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Huáº¥n luyá»‡n
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ÄÃ¡nh giÃ¡
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # LÆ°u vÃ  táº£i
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409
```

Tuyá»‡t vá»i, chÃºng tÃ´i Ä‘Ã£ cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ phá»©c táº¡p theo tá»«ng epoch vÃ  Ä‘áº£m báº£o ráº±ng nhiá»u láº§n cháº¡y huáº¥n luyá»‡n cÃ³ thá»ƒ tÃ¡i táº¡o!

{/if}

## Sá»­ dá»¥ng mÃ´ hÃ¬nh tinh chá»‰nh cá»§a mÃ¬nh

áº¡n cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c vá»›i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh cá»§a mÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng tiá»‡n Ã­ch cá»§a nÃ³ trÃªn Hub hoáº·c cá»¥c bá»™ vá»›i `pipeline` tá»« ğŸ¤— Transformers. HÃ£y sá»­ dá»¥ng cÃ¡i sau Ä‘á»ƒ táº£i xuá»‘ng mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i báº±ng cÃ¡ch sá»­ dá»¥ng pipeline `fill-mask`:

```python
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ cung cáº¥p vÄƒn báº£n máº«u "This is a great [MASK]" vÃ  xem 5 dá»± Ä‘oÃ¡n Ä‘áº§u lÃ  gÃ¬:

```python
preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")
```

```python out
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

Gá»n gÃ ng - mÃ´ hÃ¬nh cá»§a chÃºng ta rÃµ rÃ ng Ä‘Ã£ Ä‘iá»u chá»‰nh trá»ng sá»‘ cá»§a nÃ³ Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c tá»« liÃªn quan nhiá»u hÆ¡n Ä‘áº¿n phim!

<Youtube id="0Oxphw4Q9fo"/>

Äiá»u nÃ y káº¿t thÃºc thá»­ nghiá»‡m Ä‘áº§u tiÃªn cá»§a chÃºng ta vá»›i viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯. Trong [pháº§n 6](/course/chapter7/section6), báº¡n sáº½ há»c cÃ¡ch huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tá»± Ä‘á»™ng há»“i quy nhÆ° GPT-2 tá»« Ä‘áº§u; hÃ£y Ä‘áº¿n Ä‘Ã³ náº¿u báº¡n muá»‘n xem cÃ¡ch báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n trÆ°á»›c mÃ´ hÃ¬nh Transformer cá»§a riÃªng mÃ¬nh!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Äá»ƒ Ä‘á»‹nh lÆ°á»£ng lá»£i Ã­ch cá»§a viá»‡c thÃ­ch á»©ng chuyÃªn mÃ´n, hÃ£y tinh chá»‰nh bá»™ phÃ¢n loáº¡i trÃªn cÃ¡c nhÃ£n IMDb cho cáº£ cÃ¡c checkpoint DistilBERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  tinh chá»‰nh. Náº¿u báº¡n cáº§n bá»“i dÆ°á»¡ng vá» phÃ¢n loáº¡i vÄƒn báº£n, hÃ£y xem [ChÆ°Æ¡ng 3](/course/chapter3).

</Tip>
