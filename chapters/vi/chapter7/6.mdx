<FrameworkSwitchCourse {fw} />

# THuáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£ tá»« Ä‘áº§u

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_pt.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_pt.ipynb",
    },
  ]}
/>

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_tf.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section6_tf.ipynb",
    },
  ]}
/>

{/if}

Cho Ä‘áº¿n thá»i Ä‘iá»ƒm hiá»‡n táº¡i, chÃºng ta chá»§ yáº¿u sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  tinh chá»‰nh chÃºng cho cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng má»›i báº±ng cÃ¡ch sá»­ dá»¥ng láº¡i cÃ¡c trá»ng sá»‘ tá»« huáº¥n luyá»‡n trÆ°á»›c. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 1](/course/chapter1), Ä‘iá»u nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  _transfer learning_ hay _há»c chuyá»ƒn giao_, vÃ  Ä‘Ã³ lÃ  má»™t chiáº¿n lÆ°á»£c ráº¥t thÃ nh cÃ´ng Ä‘á»ƒ Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh Transformer cho háº§u háº¿t cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng trong tháº¿ giá»›i thá»±c nÆ¡i dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n lÃ  thÆ°a thá»›t. Trong chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ thá»±c hiá»‡n má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c vÃ  huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh hoÃ n toÃ n má»›i tá»« Ä‘áº§u. ÄÃ¢y lÃ  má»™t cÃ¡ch tiáº¿p cáº­n tá»‘t Ä‘á»ƒ thá»±c hiá»‡n náº¿u báº¡n cÃ³ nhiá»u dá»¯ liá»‡u vÃ  nÃ³ ráº¥t khÃ¡c vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c mÃ´ hÃ¬nh cÃ³ sáºµn. Tuy nhiÃªn, nÃ³ cÅ©ng Ä‘Ã²i há»i nhiá»u tÃ i nguyÃªn mÃ¡y tÃ­nh hÆ¡n Ä‘Ã¡ng ká»ƒ Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ hÆ¡n lÃ  chá»‰ Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh hiá»‡n cÃ³. CÃ¡c vÃ­ dá»¥ cÃ³ thá»ƒ cÃ³ Ã½ nghÄ©a khi huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh má»›i bao gá»“m cÃ¡c táº­p dá»¯ liá»‡u bao gá»“m cÃ¡c ná»‘t nháº¡c, trÃ¬nh tá»± phÃ¢n tá»­ nhÆ° DNA hoáº·c ngÃ´n ngá»¯ láº­p trÃ¬nh. CÃ´ng cá»¥ thá»© hai gáº§n Ä‘Ã¢y Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c sá»©c hÃºt nhá» cÃ¡c cÃ´ng cá»¥ nhÆ° TabNine vÃ  GitHub's Copilot, Ä‘Æ°á»£c há»— trá»£ bá»Ÿi mÃ´ hÃ¬nh Codex cá»§a OpenAI, cÃ³ thá»ƒ táº¡o ra cÃ¡c chuá»—i mÃ£ dÃ i. TÃ¡c vá»¥ táº¡o vÄƒn báº£n nÃ y Ä‘Æ°á»£c giáº£i quyáº¿t tá»‘t nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± Ä‘á»™ng há»“i quy hoáº·c nhÃ¢n quáº£ nhÆ° GPT-2.

Trong pháº§n nÃ y, chÃºng ta sáº½ xÃ¢y dá»±ng má»™t phiÃªn báº£n thu nhá» cá»§a mÃ´ hÃ¬nh táº¡o mÃ£: chÃºng ta sáº½ táº­p trung vÃ o cÃ¡c hoÃ n thÃ nh má»™t dÃ²ng thay vÃ¬ cÃ¡c hÃ m hoáº·c lá»›p Ä‘áº§y Ä‘á»§, sá»­ dá»¥ng má»™t táº­p há»£p con mÃ£ Python. Khi lÃ m viá»‡c vá»›i dá»¯ liá»‡u báº±ng Python, báº¡n thÆ°á»ng xuyÃªn tiáº¿p xÃºc vá»›i bá»™ khoa há»c dá»¯ liá»‡u Python, bao gá»“m cÃ¡c thÆ° viá»‡n `matplotlib`, `seaborn`, `pandas` vÃ  `scikit-learn`. Khi sá»­ dá»¥ng chÃºng, thÃ´ng thÆ°á»ng cáº§n pháº£i tra cá»©u cÃ¡c lá»‡nh cá»¥ thá»ƒ, vÃ¬ váº­y sáº½ ráº¥t tuyá»‡t náº¿u chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh Ä‘á»ƒ hoÃ n thÃ nh cÃ¡c lá»‡nh gá»i nÃ y cho chÃºng ta.

<Youtube id="Vpjb1lu0MDk" />

Trong [ChÆ°Æ¡ng 6](/course/chapter6), chÃºng ta Ä‘Ã£ táº¡o má»™t trÃ¬nh tokenize hiá»‡u quáº£ Ä‘á»ƒ xá»­ lÃ½ mÃ£ nguá»“n Python, nhÆ°ng nhá»¯ng gÃ¬ chÃºng ta váº«n cáº§n lÃ  má»™t táº­p dá»¯ liá»‡u quy mÃ´ lá»›n Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c má»™t mÃ´ hÃ¬nh. á» Ä‘Ã¢y, chÃºng ta sáº½ Ã¡p dá»¥ng tokenizer cho má»™t kho lÆ°u trá»¯ mÃ£ Python cÃ³ nguá»“n gá»‘c tá»« kho lÆ°u trá»¯ GitHub. Sau Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng API `Trainer` vÃ  ğŸ¤— Accelerate Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. ChÃºng ta hÃ£y Ä‘i Ä‘áº¿n Ä‘Ã³!

<iframe
  src="https://course-demos-codeparrot-ds.hf.space"
  frameBorder="0"
  height="300"
  title="Gradio app"
  class="block dark:hidden container p-0 flex-grow space-iframe"
  allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"
  sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"
></iframe>

ÄÃ¢y thá»±c sá»± cÃ¡ch mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  táº£i lÃªn Hub báº±ng cÃ¡ch sá»­ dá»¥ng mÃ£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong pháº§n nÃ y. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y nÃ³ [táº¡i Ä‘Ã¢y](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). LÆ°u Ã½ ráº±ng vÃ¬ cÃ³ má»™t sá»‘ ngáº«u nhiÃªn xáº£y ra trong quÃ¡ trÃ¬nh táº¡o vÄƒn báº£n, báº¡n cÃ³ thá»ƒ sáº½ nháº­n Ä‘Æ°á»£c má»™t káº¿t quáº£ hÆ¡i khÃ¡c.

## Thu tháº­p dá»¯ liá»‡u

MÃ£ Python cÃ³ sáºµn ráº¥t nhiá»u tá»« cÃ¡c kho mÃ£ nhÆ° GitHub, mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ táº¡o táº­p dá»¯ liá»‡u báº±ng cÃ¡ch Ä‘Ã o má»i kho lÆ°u trá»¯ Python. ÄÃ¢y lÃ  phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c thá»±c hiá»‡n trong [sÃ¡ch giÃ¡o khoa vá» Transformers](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/) Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c má»™t mÃ´ hÃ¬nh GPT-2 lá»›n. Sá»­ dá»¥ng káº¿t xuáº¥t GitHub khoáº£ng 180 GB chá»©a khoáº£ng 20 triá»‡u tá»‡p Python cÃ³ tÃªn lÃ  `codeparrot`, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ xÃ¢y dá»±ng má»™t táº­p dá»¯ liá»‡u mÃ  sau Ä‘Ã³ há» chia sáº» trÃªn [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot) .

Tuy nhiÃªn, viá»‡c huáº¥n luyá»‡n trÃªn toÃ n bá»™ ngá»¯ liá»‡u nÃ y tá»‘n nhiá»u thá»i gian vÃ  tÃ­nh toÃ¡n, vÃ  chÃºng ta chá»‰ cáº§n táº­p con cá»§a táº­p dá»¯ liá»‡u liÃªn quan Ä‘áº¿n ngÄƒn xáº¿p khoa há»c dá»¯ liá»‡u Python. VÃ¬ váº­y, hÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch lá»c táº­p dá»¯ liá»‡u `codeparrot` cho táº¥t cáº£ cÃ¡c tá»‡p bao gá»“m báº¥t ká»³ thÆ° viá»‡n nÃ o trong ngÄƒn xáº¿p nÃ y. Do kÃ­ch thÆ°á»›c cá»§a táº­p dá»¯ liá»‡u, chÃºng ta muá»‘n trÃ¡nh táº£i nÃ³ xuá»‘ng; thay vÃ o Ä‘Ã³, ta sáº½ sá»­ dá»¥ng tÃ­nh nÄƒng phÃ¡t trá»±c tuyáº¿n Ä‘á»ƒ lá»c nÃ³ má»™t cÃ¡ch nhanh chÃ³ng. Äá»ƒ giÃºp chÃºng ta lá»c cÃ¡c máº«u mÃ£ báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, ta sáº½ sá»­ dá»¥ng hÃ m sau:

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

HÃ£y kiá»ƒm tra nÃ³ trÃªn hai vÃ­ dá»¥:

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘iá»u nÃ y Ä‘á»ƒ táº¡o má»™t hÃ m sáº½ truyá»n trá»±c tuyáº¿n táº­p dá»¯ liá»‡u vÃ  lá»c cÃ¡c pháº§n tá»­ ta muá»‘n:

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset


def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ chá»‰ cáº§n Ã¡p dá»¥ng chá»©c nÄƒng nÃ y cho táº­p dá»¯ liá»‡u phÃ¡t trá»±c tuyáº¿n:

```py
# Ã” nÃ y sáº½ máº¥t ráº¥t nhiá»u thá»i gian Ä‘á»ƒ thá»±c thi, vÃ¬ váº­y báº¡n nÃªn bá» qua vÃ  chuyá»ƒn Ä‘áº¿n
# cÃ¡i tiáº¿p theo!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

Äiá»u nÃ y Ä‘á»ƒ láº¡i cho chÃºng ta khoáº£ng 3% táº­p dá»¯ liá»‡u ban Ä‘áº§u, váº«n cÃ²n khÃ¡ lá»›n - táº­p dá»¯ liá»‡u káº¿t quáº£ lÃ  6GB vÃ  bao gá»“m 600,000 táº­p lá»‡nh Python!

Viá»‡c lá»c toÃ n bá»™ táº­p dá»¯ liá»‡u cÃ³ thá»ƒ máº¥t 2-3 giá» tÃ¹y thuá»™c vÃ o mÃ¡y vÃ  bÄƒng thÃ´ng cá»§a báº¡n. Náº¿u báº¡n khÃ´ng muá»‘n tá»± mÃ¬nh tráº£i qua quÃ¡ trÃ¬nh kÃ©o dÃ i nÃ y, chÃºng ta cung cáº¥p táº­p dá»¯ liá»‡u Ä‘Ã£ lá»c trÃªn Hub Ä‘á»ƒ báº¡n táº£i xuá»‘ng:

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

Viá»‡c huáº¥n luyá»‡n trÆ°á»›c mÃ´ hÃ¬nh ngÃ´n ngá»¯ sáº½ máº¥t má»™t lÃºc. ChÃºng tÃ´i khuyÃªn báº¡n trÆ°á»›c tiÃªn nÃªn cháº¡y vÃ²ng láº·p huáº¥n luyá»‡n trÃªn má»™t máº«u dá»¯ liá»‡u báº±ng cÃ¡ch bá» chÃº thÃ­ch hai dÃ²ng má»™t pháº§n á»Ÿ trÃªn vÃ  Ä‘áº£m báº£o ráº±ng quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t thÃ nh cÃ´ng vÃ  cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u trá»¯. KhÃ´ng cÃ³ gÃ¬ khÃ³ chá»‹u hÆ¡n lÃ  má»™t láº§n cháº¡y huáº¥n luyá»‡n khÃ´ng thÃ nh cÃ´ng á»Ÿ bÆ°á»›c cuá»‘i cÃ¹ng vÃ¬ báº¡n quÃªn táº¡o má»™t thÆ° má»¥c hoáº·c vÃ¬ cÃ³ lá»—i Ä‘Ã¡nh mÃ¡y á»Ÿ cuá»‘i vÃ²ng láº·p huáº¥n luyá»‡n!

</Tip>

HÃ£y xem má»™t vÃ­ dá»¥ tá»« táº­p dá»¯ liá»‡u. ChÃºng ta sáº½ chá»‰ hiá»ƒn thá»‹ 200 kÃ½ tá»± Ä‘áº§u tiÃªn cá»§a má»—i trÆ°á»ng:

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng trÆ°á»ng `content` chá»©a mÃ£ mÃ  chÃºng ta muá»‘n mÃ´ hÃ¬nh cá»§a mÃ¬nh huáº¥n luyá»‡n. BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ má»™t táº­p dá»¯ liá»‡u, chÃºng ta cáº§n chuáº©n bá»‹ cÃ¡c vÄƒn báº£n Ä‘á»ƒ chÃºng cÃ³ Ä‘á»‹nh dáº¡ng phÃ¹ há»£p Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c.

## Chuáº©n bá»‹ táº­p dá»¯ liá»‡u

<Youtube id="ma1TrR7gE7I" />

BÆ°á»›c Ä‘áº§u tiÃªn sáº½ lÃ  tokenize dá»¯ liá»‡u Ä‘á»ƒ chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ huáº¥n luyá»‡n. VÃ¬ má»¥c tiÃªu cá»§a chÃºng ta chá»§ yáº¿u lÃ  tá»± Ä‘á»™ng hoÃ n thÃ nh cÃ¡c lá»‡nh gá»i hÃ m ngáº¯n, chÃºng ta cÃ³ thá»ƒ giá»¯ kÃ­ch thÆ°á»›c ngá»¯ cáº£nh tÆ°Æ¡ng Ä‘á»‘i nhá». Äiá»u nÃ y cÃ³ lá»£i Ã­ch lÃ  chÃºng ta cÃ³ thá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh nhanh hÆ¡n nhiá»u vÃ  nÃ³ cáº§n Ã­t bá»™ nhá»› hÆ¡n Ä‘Ã¡ng ká»ƒ. Náº¿u Ä‘iá»u quan trá»ng lÃ  á»©ng dá»¥ng cá»§a báº¡n pháº£i cÃ³ nhiá»u ngá»¯ cáº£nh hÆ¡n (vÃ­ dá»¥: náº¿u báº¡n muá»‘n mÃ´ hÃ¬nh viáº¿t cÃ¡c bÃ i kiá»ƒm tra Ä‘Æ¡n vá»‹ dá»±a trÃªn tá»‡p cÃ³ Ä‘á»‹nh nghÄ©a hÃ m), hÃ£y Ä‘áº£m báº£o báº¡n tÄƒng con sá»‘ Ä‘Ã³, nhÆ°ng cÅ©ng lÆ°u Ã½ ráº±ng Ä‘iá»u nÃ y Ä‘i kÃ¨m vá»›i bá»™ nhá»› GPU lá»›n hÆ¡n. Hiá»‡n táº¡i, hÃ£y sá»­a kÃ­ch thÆ°á»›c ngá»¯ cáº£nh á»Ÿ 128 token, trÃ¡i ngÆ°á»£c vá»›i 1,024 hoáº·c 2,048 Ä‘Æ°á»£c sá»­ dá»¥ng trong GPT-2 hoáº·c GPT-3, tÆ°Æ¡ng á»©ng.

Háº§u háº¿t cÃ¡c tÃ i liá»‡u chá»©a nhiá»u hÆ¡n 128 token, vÃ¬ váº­y chá»‰ cáº§n cáº¯t bá»›t Ä‘áº§u vÃ o Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a sáº½ loáº¡i bá» má»™t pháº§n lá»›n táº­p dá»¯ liá»‡u cá»§a mÃ¬nh. Thay vÃ o Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng tÃ¹y chá»n `return_overflowing_tokens` Ä‘á»ƒ token toÃ n bá»™ Ä‘áº§u vÃ o vÃ  chia nÃ³ thÃ nh nhiá»u pháº§n, nhÆ° chÃºng ta Ä‘Ã£ lÃ m trong [ChÆ°Æ¡ng 6](/course/chapter6/4). ChÃºng ta cÅ©ng sáº½ sá»­ dá»¥ng tÃ¹y chá»n `return_length` Ä‘á»ƒ tá»± Ä‘á»™ng tráº£ vá» Ä‘á»™ dÃ i cá»§a má»—i Ä‘oáº¡n Ä‘Æ°á»£c táº¡o. ThÆ°á»ng thÃ¬ pháº§n cuá»‘i cÃ¹ng sáº½ nhá» hÆ¡n kÃ­ch thÆ°á»›c ngá»¯ cáº£nh vÃ  chÃºng ta sáº½ loáº¡i bá» nhá»¯ng pháº§n nÃ y Ä‘á»ƒ trÃ¡nh cÃ¡c váº¥n Ä‘á» vá» pháº§n Ä‘á»‡m; chÃºng ta khÃ´ng thá»±c sá»± cáº§n chÃºng vÃ¬ dÃ¹ sao chÃºng ta cÅ©ng cÃ³ nhiá»u dá»¯ liá»‡u.

<div class="flex justify-center">
  <img
    class="block dark:hidden"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg"
    alt="Chunking a large texts in several pieces."
  />
  <img
    class="hidden dark:block"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg"
    alt="Chunking a large texts in several pieces."
  />
</div>

HÃ£y xem chÃ­nh xÃ¡c cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a Ä‘iá»u nÃ y báº±ng cÃ¡ch xem hai vÃ­ dá»¥ Ä‘áº§u tiÃªn:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng chÃºng ta nháº­n Ä‘Æ°á»£c tá»•ng cá»™ng 34 phÃ¢n Ä‘oáº¡n tá»« hai vÃ­ dá»¥ Ä‘Ã³. NhÃ¬n vÃ o Ä‘á»™ dÃ i phÃ¢n Ä‘oáº¡n, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c Ä‘oáº¡n á»Ÿ cuá»‘i cáº£ hai tÃ i liá»‡u cÃ³ Ã­t hÆ¡n 128 token (tÆ°Æ¡ng á»©ng lÃ  117 vÃ  41). ChÃºng chá»‰ Ä‘áº¡i diá»‡n cho má»™t pháº§n nhá» trong tá»•ng sá»‘ cÃ¡c khá»‘i mÃ  chÃºng ta cÃ³, vÃ¬ váº­y chÃºng ta cÃ³ thá»ƒ vá»©t chÃºng Ä‘i má»™t cÃ¡ch an toÃ n. Vá»›i trÆ°á»ng `overflow_to_sample_mapping`, chÃºng ta cÅ©ng cÃ³ thá»ƒ táº¡o láº¡i cÃ¡c pháº§n thuá»™c vá» máº«u Ä‘áº§u vÃ o nÃ o.

Vá»›i thao tÃ¡c nÃ y, chÃºng ta Ä‘ang sá»­ dá»¥ng má»™t tÃ­nh nÄƒng tiá»‡n dá»¥ng cá»§a hÃ m `Dataset.map()` trong ğŸ¤— Datasets, Ä‘Ã³ lÃ  nÃ³ khÃ´ng yÃªu cáº§u Ã¡nh xáº¡ 1-1; nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [pháº§n 3](/course/chapter7/3), chÃºng ta cÃ³ thá»ƒ táº¡o cÃ¡c lÃ´ cÃ³ nhiá»u pháº§n tá»­ hÆ¡n hoáº·c Ã­t hÆ¡n lÃ´ Ä‘áº§u vÃ o. Äiá»u nÃ y ráº¥t há»¯u Ã­ch khi thá»±c hiá»‡n cÃ¡c hoáº¡t Ä‘á»™ng nhÆ° tÄƒng dá»¯ liá»‡u hoáº·c lá»c dá»¯ liá»‡u lÃ m thay Ä‘á»•i sá»‘ lÆ°á»£ng pháº§n tá»­. Trong trÆ°á»ng há»£p cá»§a chÃºng ta, khi tokenize má»—i pháº§n tá»­ thÃ nh cÃ¡c pháº§n cÃ³ kÃ­ch thÆ°á»›c ngá»¯ cáº£nh Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh, chÃºng ta táº¡o nhiá»u máº«u tá»« má»—i tÃ i liá»‡u. ChÃºng ta chá»‰ cáº§n Ä‘áº£m báº£o xÃ³a cÃ¡c cá»™t hiá»‡n cÃ³, vÃ¬ chÃºng cÃ³ kÃ­ch thÆ°á»›c xung Ä‘á»™t. Náº¿u chÃºng ta muá»‘n giá»¯ chÃºng, chÃºng ta cÃ³ thá»ƒ láº·p láº¡i chÃºng má»™t cÃ¡ch thÃ­ch há»£p vÃ  tráº£ láº¡i chÃºng trong lá»‡nh gá»i `Dataset.map()`:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

Hiá»‡n chÃºng ta cÃ³ 16,7 triá»‡u vÃ­ dá»¥ vá»›i 128 token má»—i vÃ­ dá»¥, tÆ°Æ¡ng á»©ng vá»›i tá»•ng cá»™ng khoáº£ng 2,1 tá»· token. Äá»ƒ tham kháº£o, cÃ¡c mÃ´ hÃ¬nh GPT-3 vÃ  Codex cá»§a OpenAI Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 300 vÃ  100 tá»· token tÆ°Æ¡ng á»©ng, trong Ä‘Ã³ cÃ¡c mÃ´ hÃ¬nh Codex Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« cÃ¡c checkpoint GPT-3. Má»¥c tiÃªu cá»§a chÃºng ta trong pháº§n nÃ y khÃ´ng pháº£i lÃ  cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh nÃ y, cÃ³ thá»ƒ táº¡o ra cÃ¡c vÄƒn báº£n dÃ i, máº¡ch láº¡c, mÃ  lÃ  táº¡o ra má»™t phiÃªn báº£n thu nhá» cung cáº¥p chá»©c nÄƒng tá»± Ä‘á»™ng hoÃ n thÃ nh nhanh chÃ³ng cho cÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u.

BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ táº­p dá»¯ liá»‡u sáºµn sÃ ng, hÃ£y thiáº¿t láº­p mÃ´ hÃ¬nh!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Loáº¡i bá» táº¥t cáº£ cÃ¡c pháº§n nhá» hÆ¡n kÃ­ch thÆ°á»›c ngá»¯ cáº£nh khÃ´ng pháº£i lÃ  váº¥n Ä‘á» lá»›n á»Ÿ Ä‘Ã¢y vÃ¬ chÃºng ta Ä‘ang sá»­ dá»¥ng cÃ¡c cá»­a sá»• ngá»¯ cáº£nh nhá». Khi báº¡n tÄƒng kÃ­ch thÆ°á»›c ngá»¯ cáº£nh (hoáº·c náº¿u báº¡n cÃ³ má»™t kho tÃ i liá»‡u ngáº¯n), pháº§n nhá» cÃ¡c pháº§n bá»‹ vá»©t bá» cÅ©ng sáº½ tÄƒng lÃªn. Má»™t cÃ¡ch hiá»‡u quáº£ hÆ¡n Ä‘á»ƒ chuáº©n bá»‹ dá»¯ liá»‡u lÃ  káº¿t há»£p táº¥t cáº£ cÃ¡c máº«u Ä‘Æ°á»£c tokenize trong má»™t lÃ´ vá»›i token `eos_token_id` á»Ÿ giá»¯a, vÃ  sau Ä‘Ã³ thá»±c hiá»‡n phÃ¢n Ä‘oáº¡n trÃªn cÃ¡c chuá»—i Ä‘Æ°á»£c ná»‘i. NhÆ° má»™t bÃ i táº­p, hÃ£y sá»­a Ä‘á»•i hÃ m `tokenize()` Ä‘á»ƒ sá»­ dá»¥ng cÃ¡ch tiáº¿p cáº­n Ä‘Ã³. LÆ°u Ã½ ráº±ng báº¡n sáº½ muá»‘n Ä‘áº·t `truncation=False` vÃ  xÃ³a cÃ¡c tham sá»‘ khÃ¡c khá»i tokenizer Ä‘á»ƒ nháº­n Ä‘Æ°á»£c chuá»—i Ä‘áº§y Ä‘á»§ cá»§a token ID.

</Tip>

## Khá»Ÿi táº¡o mÃ´ hÃ¬nh má»›i

BÆ°á»›c Ä‘áº§u tiÃªn cá»§a chÃºng ta lÃ  khá»Ÿi cháº¡y má»›i mÃ´ hÃ¬nh GPT-2. ChÃºng ta sáº½ sá»­ dá»¥ng cÃ¹ng má»™t cáº¥u hÃ¬nh cho mÃ´ hÃ¬nh cá»§a mÃ¬nh nhÆ° cho mÃ´ hÃ¬nh GPT-2 nhá», vÃ¬ váº­y chÃºng ta táº£i cáº¥u hÃ¬nh Ä‘á»‹nh sáºµn, Ä‘áº£m báº£o ráº±ng kÃ­ch thÆ°á»›c tokenizer khá»›p vá»›i kÃ­ch thÆ°á»›c tá»« vá»±ng cá»§a mÃ´ hÃ¬nh vÃ  chuyá»ƒn `bos` vÃ  `eos` (báº¯t Ä‘áº§u vÃ  cuá»‘i chuá»—i) token ID:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Vá»›i cáº¥u hÃ¬nh Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ táº£i má»™t mÃ´ hÃ¬nh má»›i. LÆ°u Ã½ ráº±ng Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn chÃºng ta khÃ´ng sá»­ dá»¥ng hÃ m `from_pretrained()`, vÃ¬ chÃºng ta thá»±c sá»± Ä‘ang khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh cá»§a chÃ­nh chÃºng ta:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

Vá»›i cáº¥u hÃ¬nh Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ táº£i má»™t mÃ´ hÃ¬nh má»›i. LÆ°u Ã½ ráº±ng Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn chÃºng ta khÃ´ng sá»­ dá»¥ng hÃ m `from_pretrained()`, vÃ¬ chÃºng ta thá»±c sá»± Ä‘ang khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh cá»§a chÃ­nh chÃºng ta:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # XÃ¢y mÃ´ hÃ¬nh
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

MÃ´ hÃ¬nh cá»§a chÃºng ta cÃ³ 124 triá»‡u thÃ´ng sá»‘ mÃ  ta sáº½ pháº£i Ä‘iá»u chá»‰nh. TrÆ°á»›c khi cÃ³ thá»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n, chÃºng ta cáº§n thiáº¿t láº­p má»™t bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u sáº½ Ä‘áº£m nháº­n viá»‡c táº¡o cÃ¡c lÃ´. ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng trÃ¬nh cáº¯t ghÃ©p `DataCollatorForLanguageModeling`, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ (nhÆ° tÃªn gá»i gá»£i Ã½ má»™t cÃ¡ch tinh táº¿). BÃªn cáº¡nh viá»‡c xáº¿p chá»“ng vÃ  Ä‘á»‡m cÃ¡c lÃ´, nÃ³ cÅ©ng Ä‘áº£m nháº­n viá»‡c táº¡o cÃ¡c nhÃ£n cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ - trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£, cÃ¡c Ä‘áº§u vÃ o cÅ©ng Ä‘Ã³ng vai trÃ² lÃ  nhÃ£n (chá»‰ Ä‘Æ°á»£c dá»‹ch chuyá»ƒn bá»Ÿi má»™t pháº§n tá»­) vÃ  trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u nÃ y táº¡o chÃºng nhanh chÃ³ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, vÃ¬ váº­y chÃºng tÃ´i ta khÃ´ng cáº§n sao chÃ©p `input_ids`.

LÆ°u Ã½ ráº±ng `DataCollatorForLanguageModeling` há»— trá»£ cáº£ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ bá»‹ áº©n Ä‘i (MLM) vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£ (CLM). Theo máº·c Ä‘á»‹nh, nÃ³ chuáº©n bá»‹ dá»¯ liá»‡u cho MLM, nhÆ°ng chÃºng ta cÃ³ thá»ƒ chuyá»ƒn sang CLM báº±ng cÃ¡ch Ä‘áº·t Ä‘á»‘i sá»‘ `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

HÃ£y xem má»™t vÃ­ dá»¥:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c vÃ­ dá»¥ Ä‘Ã£ Ä‘Æ°á»£c xáº¿p chá»“ng lÃªn nhau vÃ  táº¥t cáº£ cÃ¡c tensor cÃ³ cÃ¹ng hÃ¬nh dáº¡ng.

{#if fw === 'tf'}

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `to_tf_dataset()` Ä‘á»ƒ chuyá»ƒn Ä‘á»•i táº­p dá»¯ liá»‡u cá»§a mÃ¬nh thÃ nh táº­p dá»¯ liá»‡u TensorFlow báº±ng cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘Ã£ táº¡o á»Ÿ trÃªn:

```python
tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

âš ï¸ Viá»‡c dá»‹ch chuyá»ƒn cÃ¡c Ä‘áº§u vÃ o vÃ  nhÃ£n Ä‘á»ƒ cÄƒn chá»‰nh chÃºng xáº£y ra bÃªn trong mÃ´ hÃ¬nh, do Ä‘Ã³, bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u chá»‰ cáº§n sao chÃ©p cÃ¡c Ä‘áº§u vÃ o Ä‘á»ƒ táº¡o nhÃ£n.

</Tip>

BÃ¢y giá» chÃºng ta cÃ³ má»i thá»© Ä‘á»ƒ thá»±c sá»± huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh - Ä‘Ã³ khÃ´ng pháº£i lÃ  quÃ¡ nhiá»u cÃ´ng viá»‡c! TrÆ°á»›c khi báº¯t Ä‘áº§u luyá»‡n táº­p, chÃºng ta nÃªn Ä‘Äƒng nháº­p vÃ o Hugging Face. Náº¿u báº¡n Ä‘ang lÃ m viá»‡c trong notebook, báº¡n cÃ³ thá»ƒ lÃ m nhÆ° váº­y vá»›i hÃ m tiá»‡n Ã­ch sau:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Thao tÃ¡c nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p Hugging Face cá»§a mÃ¬nh.

Náº¿u báº¡n khÃ´ng lÃ m viá»‡c trong notebook, chá»‰ cáº§n nháº­p dÃ²ng sau vÃ o thiáº¿t bá»‹ Ä‘áº§u cuá»‘i cá»§a báº¡n:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Táº¥t cáº£ nhá»¯ng gÃ¬ cÃ²n láº¡i cáº§n lÃ m lÃ  cáº¥u hÃ¬nh cÃ¡c tham sá»‘ huáº¥n luyá»‡n vÃ  kÃ­ch hoáº¡t `Trainer`. ChÃºng ta sáº½ sá»­ dá»¥ng lá»‹ch trÃ¬nh tá»‘c Ä‘á»™ há»c cosine vá»›i má»™t sá»‘ láº§n khá»Ÿi Ä‘á»™ng vÃ  kÃ­ch thÆ°á»›c lÃ´ hiá»‡u quáº£ lÃ  256 (`per_device_train_batch_size` \* `gradient_accumulation_steps`). TÃ­ch lÅ©y gradient Ä‘Æ°á»£c sá»­ dá»¥ng khi má»™t loáº¡t lÃ´ duy nháº¥t khÃ´ng vá»«a vá»›i bá»™ nhá»› vÃ  dáº§n dáº§n tÃ­ch lÅ©y gradient thÃ´ng qua má»™t sá»‘ láº§n truyá»n xuÃ´i/ngÆ°á»£c. ChÃºng ta sáº½ tháº¥y Ä‘iá»u nÃ y hoáº¡t Ä‘á»™ng khi chÃºng ta táº¡o vÃ²ng huáº¥n luyá»‡n vá»›i ğŸ¤— Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ khá»Ÿi Ä‘á»™ng `Trainer` vÃ  Ä‘á»£i quÃ¡ trÃ¬nh huáº¥n luyá»‡n káº¿t thÃºc. TÃ¹y thuá»™c vÃ o viá»‡c báº¡n cháº¡y nÃ³ trÃªn toÃ n bá»™ hay má»™t táº­p há»£p con cá»§a bá»™ huáº¥n luyá»‡n, tÆ°Æ¡ng á»©ng sáº½ máº¥t 20 hoáº·c 2 giá», vÃ¬ váº­y hÃ£y láº¥y má»™t Ã­t cÃ  phÃª vÃ  má»™t cuá»‘n sÃ¡ch hay Ä‘á»ƒ Ä‘á»c!

```py
trainer.train()
```

Sau khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t, chÃºng ta cÃ³ thá»ƒ Ä‘áº©y mÃ´ hÃ¬nh vÃ  trÃ¬nh tokenizer vÃ o Hub:

```py
trainer.push_to_hub()
```

{:else}

Táº¥t cáº£ nhá»¯ng gÃ¬ cÃ²n láº¡i cáº§n lÃ m lÃ  Ä‘á»‹nh cáº¥u hÃ¬nh cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n vÃ  gá»i `compile()`vÃ  `fit()`. ChÃºng ta sáº½ sá»­ dá»¥ng lá»‹ch trÃ¬nh tá»‘c Ä‘á»™ há»c vá»›i má»™t sá»‘ khá»Ÿi Ä‘á»™ng Ä‘á»ƒ cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Huáº¥n luyá»‡n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ gá»i `model.fit()` vÃ  Ä‘á»£i quÃ¡ trÃ¬nh huáº¥n luyá»‡n káº¿t thÃºc. TÃ¹y thuá»™c vÃ o viá»‡c báº¡n cháº¡y nÃ³ trÃªn toÃ n bá»™ hay má»™t táº­p há»£p con cá»§a bá»™ huáº¥n luyá»‡n, tÆ°Æ¡ng á»©ng sáº½ máº¥t 20 hoáº·c 2 giá», vÃ¬ váº­y hÃ£y láº¥y má»™t Ã­t cÃ  phÃª vÃ  má»™t cuá»‘n sÃ¡ch hay Ä‘á»ƒ Ä‘á»c! Sau khi huáº¥n luyá»‡n xong, chÃºng ta cÃ³ thá»ƒ Ä‘áº©y mÃ´ hÃ¬nh vÃ  trÃ¬nh tokenize vÃ o Hub:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Chá»‰ máº¥t khoáº£ng 30 dÃ²ng mÃ£ ngoÃ i `TrainingArguments` Ä‘á»ƒ tá»« vÄƒn báº£n thÃ´ Ä‘áº¿n huáº¥n luyá»‡n GPT-2. HÃ£y dÃ¹ng thá»­ vá»›i táº­p dá»¯ liá»‡u cá»§a riÃªng báº¡n vÃ  xem liá»‡u báº¡n cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hay khÃ´ng!

</Tip>

<Tip>

{#if fw === 'pt'}

ğŸ’¡ Náº¿u báº¡n cÃ³ quyá»n truy cáº­p vÃ o má»™t mÃ¡y cÃ³ nhiá»u GPU, hÃ£y thá»­ cháº¡y mÃ£ á»Ÿ Ä‘Ã³. `Trainer` tá»± Ä‘á»™ng quáº£n lÃ½ nhiá»u mÃ¡y vÃ  Ä‘iá»u nÃ y cÃ³ thá»ƒ tÄƒng tá»‘c quÃ¡ trÃ¬nh huáº¥n luyá»‡n lÃªn ráº¥t nhiá»u.

{:else}

ğŸ’¡ Náº¿u báº¡n cÃ³ quyá»n truy cáº­p vÃ o má»™t mÃ¡y cÃ³ nhiá»u GPU, báº¡n cÃ³ thá»ƒ thá»­ sá»­ dá»¥ng ngá»¯ cáº£nh `MirroredStrategy` Ä‘á»ƒ tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Báº¡n sáº½ cáº§n táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `tf.distribute.MirroredStrategy` vÃ  Ä‘áº£m báº£o ráº±ng cÃ¡c lá»‡nh `to_tf_dataset` cÅ©ng nhÆ° táº¡o mÃ´ hÃ¬nh vÃ  lá»‡nh gá»i Ä‘áº¿n `fit()` Ä‘á»u Ä‘Æ°á»£c cháº¡y trong ngá»¯ cáº£nh `scope()` cá»§a nÃ³. Báº¡n cÃ³ thá»ƒ xem tÃ i liá»‡u vá» Ä‘iá»u nÃ y [táº¡i Ä‘Ã¢y](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## Táº¡o mÃ£ vá»›i má»™t pipeline

BÃ¢y giá» lÃ  thá»i Ä‘iá»ƒm cá»§a sá»± tháº­t: chÃºng ta hÃ£y xem mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n thá»±c sá»± hoáº¡t Ä‘á»™ng tá»‘t nhÆ° tháº¿ nÃ o! ChÃºng ta cÃ³ thá»ƒ tháº¥y trong nháº­t kÃ½ ráº±ng máº¥t mÃ¡t Ä‘Ã£ giáº£m Ä‘á»u Ä‘áº·n, nhÆ°ng Ä‘á»ƒ Ä‘Æ°a mÃ´ hÃ¬nh vÃ o thá»­ nghiá»‡m, chÃºng ta hÃ£y xem nÃ³ hoáº¡t Ä‘á»™ng tá»‘t nhÆ° tháº¿ nÃ o trÃªn má»™t sá»‘ lá»i nháº¯c. Äá»ƒ lÃ m Ä‘iá»u Ä‘Ã³, chÃºng ta sáº½ bao bá»c mÃ´ hÃ¬nh trong má»™t `pipeline` táº¡o vÄƒn báº£n vÃ  chÃºng ta sáº½ Ä‘Æ°a nÃ³ vÃ o GPU cho cÃ¡c tháº¿ há»‡ nhanh náº¿u cÃ³ sáºµn:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

HÃ£y báº¯t Ä‘áº§u vá»›i tÃ¡c vá»¥ Ä‘Æ¡n giáº£n lÃ  táº¡o má»™t biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

Káº¿t quáº£ cÃ³ váº» chÃ­nh xÃ¡c. NÃ³ cÅ©ng hoáº¡t Ä‘á»™ng vá»›i `pandas`? HÃ£y xem liá»‡u chÃºng ta cÃ³ thá»ƒ táº¡o má»™t `DataFrame` tá»« hai máº£ng khÃ´ng:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

Tháº­t tuyá»‡t, Ä‘Ã³ lÃ  cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c - máº·c dÃ¹ sau Ä‘Ã³ nÃ³ láº¡i chÃ¨n thÃªm cá»™t `x`. VÃ¬ sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c táº¡o cÃ³ giá»›i háº¡n, vÃ²ng láº·p `for` sau Ä‘Ã¢y sáº½ bá»‹ cáº¯t. HÃ£y xem liá»‡u chÃºng ta cÃ³ thá»ƒ lÃ m Ä‘iá»u gÃ¬ Ä‘Ã³ phá»©c táº¡p hÆ¡n má»™t chÃºt vÃ  Ä‘á»ƒ mÃ´ hÃ¬nh giÃºp chÃºng ta sá»­ dá»¥ng hoáº¡t Ä‘á»™ng `groupby`:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

KhÃ´ng tá»‡; Ä‘Ã³ lÃ  cÃ¡ch lÃ m Ä‘Ãºng. Cuá»‘i cÃ¹ng, hÃ£y xem liá»‡u chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ cho `scikit-learn` vÃ  thiáº¿t láº­p mÃ´ hÃ¬nh Random Forest hay khÃ´ng:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

NhÃ¬n vÃ o má»™t vÃ i vÃ­ dá»¥ nÃ y, cÃ³ váº» nhÆ° mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c má»™t sá»‘ cÃº phÃ¡p cá»§a bá»™ khoa há»c dá»¯ liá»‡u Python. Táº¥t nhiÃªn, chÃºng ta sáº½ cáº§n pháº£i Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh ká»¹ lÆ°á»¡ng hÆ¡n trÆ°á»›c khi triá»ƒn khai nÃ³ trong tháº¿ giá»›i thá»±c, nhÆ°ng Ä‘Ã¢y váº«n lÃ  má»™t nguyÃªn máº«u áº¥n tÆ°á»£ng.

{:else}

NhÃ¬n vÃ o má»™t vÃ i vÃ­ dá»¥ nÃ y, cÃ³ váº» nhÆ° mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c má»™t sá»‘ cÃº phÃ¡p cá»§a bá»™ khoa há»c dá»¯ liá»‡u Python (táº¥t nhiÃªn, chÃºng tÃ´i sáº½ cáº§n Ä‘Ã¡nh giÃ¡ ká»¹ lÆ°á»¡ng hÆ¡n trÆ°á»›c khi triá»ƒn khai mÃ´ hÃ¬nh trong tháº¿ giá»›i thá»±c). Tuy nhiÃªn, Ä‘Ã´i khi nÃ³ Ä‘Ã²i há»i pháº£i tÃ¹y chá»‰nh nhiá»u hÆ¡n viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº§n thiáº¿t cho má»™t trÆ°á»ng há»£p sá»­ dá»¥ng nháº¥t Ä‘á»‹nh. VÃ­ dá»¥: Ä‘iá»u gÃ¬ sáº½ xáº£y ra náº¿u chÃºng ta muá»‘n cáº­p nháº­t Ä‘á»™ng kÃ­ch thÆ°á»›c lÃ´ hoáº·c cÃ³ má»™t vÃ²ng láº·p huáº¥n luyá»‡n cÃ³ Ä‘iá»u kiá»‡n Ä‘á»ƒ bá» qua cÃ¡c vÃ­ dá»¥ xáº¥u má»™t cÃ¡ch nhanh chÃ³ng? Má»™t tÃ¹y chá»n sáº½ lÃ  phÃ¢n lá»›p `Trainer` vÃ  thÃªm cÃ¡c thay Ä‘á»•i cáº§n thiáº¿t, nhÆ°ng Ä‘Ã´i khi viá»‡c viáº¿t vÃ²ng láº·p huáº¥n luyá»‡n tá»« Ä‘áº§u sáº½ Ä‘Æ¡n giáº£n hÆ¡n. ÄÃ³ lÃ  lÃºc ğŸ¤— Accelerate xuáº¥t hiá»‡n.

{/if}

{#if fw === 'pt'}

## Huáº¥n luyá»‡n vá»›i ğŸ¤— Accelerate

ChÃºng ta Ä‘Ã£ tháº¥y cÃ¡ch huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh vá»›i `Trainer`, cÃ³ thá»ƒ cho phÃ©p má»™t sá»‘ tÃ¹y chá»‰nh. Tuy nhiÃªn, Ä‘Ã´i khi chÃºng ta muá»‘n toÃ n quyá»n kiá»ƒm soÃ¡t vÃ²ng láº·p huáº¥n luyá»‡n hoáº·c chÃºng ta muá»‘n thá»±c hiá»‡n má»™t sá»‘ thay Ä‘á»•i ká»³ láº¡. Trong trÆ°á»ng há»£p nÃ y ğŸ¤— Accelerate lÃ  má»™t lá»±a chá»n tuyá»‡t vá»i vÃ  trong pháº§n nÃ y, chÃºng ta sáº½ xem xÃ©t cÃ¡c bÆ°á»›c sá»­ dá»¥ng nÃ³ Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh. Äá»ƒ lÃ m cho má»i thá»© thÃº vá»‹ hÆ¡n, chÃºng ta cÅ©ng sáº½ thÃªm má»™t sá»‘ Ä‘iá»u chá»‰nh vÃ o vÃ²ng láº·p huáº¥n luyá»‡n.

<Youtube id="Hm8_PgVTFuc" />

VÃ¬ chÃºng ta chá»§ yáº¿u quan tÃ¢m Ä‘áº¿n tÃ­nh nÄƒng tá»± Ä‘á»™ng hoÃ n thÃ nh há»£p lÃ½ cho cÃ¡c thÆ° viá»‡n khoa há»c dá»¯ liá»‡u, nÃªn viá»‡c Ä‘Æ°a ra nhiá»u trá»ng sá»‘ hÆ¡n cho cÃ¡c máº«u huáº¥n luyá»‡n sá»­ dá»¥ng nhiá»u hÆ¡n cÃ¡c thÆ° viá»‡n nÃ y. ChÃºng ta cÃ³ thá»ƒ dá»… dÃ ng xÃ¡c Ä‘á»‹nh nhá»¯ng vÃ­ dá»¥ nÃ y thÃ´ng qua viá»‡c sá»­ dá»¥ng cÃ¡c tá»« khÃ³a nhÆ° `plt`, `pd`, `sk`, `fit` vÃ  `predict`, lÃ  nhá»¯ng tÃªn nháº­p thÆ°á»ng gáº·p nháº¥t cho `matplotlib.pyplot`, `pandas`, vÃ  `sklearn` cÅ©ng nhÆ° cÃ¡c hÃ nh vi sau Ä‘Ã³. Náº¿u chÃºng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t token duy nháº¥t, chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng kiá»ƒm tra xem chÃºng cÃ³ xuáº¥t hiá»‡n trong chuá»—i Ä‘áº§u vÃ o hay khÃ´ng. CÃ¡c token cÃ³ thá»ƒ cÃ³ tiá»n tá»‘ khoáº£ng tráº¯ng, vÃ¬ váº­y chÃºng ta cÅ©ng sáº½ kiá»ƒm tra cÃ¡c phiÃªn báº£n Ä‘Ã³ trong tá»« vá»±ng bá»™ tokenizer. Äá»ƒ xÃ¡c minh ráº±ng nÃ³ hoáº¡t Ä‘á»™ng, chÃºng ta sáº½ thÃªm má»™t token kiá»ƒm thá»­ sáº½ Ä‘Æ°á»£c chia thÃ nh nhiá»u token:

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

Tuyá»‡t vá»i, Ä‘iá»u Ä‘Ã³ cÃ³ váº» hoáº¡t Ä‘á»™ng tá»‘t! BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ viáº¿t má»™t hÃ m máº¥t mÃ¡t tÃ¹y chá»‰nh láº¥y chuá»—i Ä‘áº§u vÃ o, logits vÃ  token khÃ³a mÃ  chÃºng ta vá»«a chá»n lÃ m Ä‘áº§u vÃ o. TrÆ°á»›c tiÃªn, chÃºng ta cáº§n cÄƒn chá»‰nh logits vÃ  Ä‘áº§u vÃ o: chuá»—i Ä‘áº§u vÃ o Ä‘Æ°á»£c dá»‹ch chuyá»ƒn má»™t Ä‘Æ¡n vá»‹ sang bÃªn pháº£i táº¡o thÃ nh cÃ¡c nhÃ£n, vÃ¬ token tiáº¿p theo lÃ  nhÃ£n cho token hiá»‡n táº¡i. ChÃºng ta cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng cÃ¡ch báº¯t Ä‘áº§u cÃ¡c nhÃ£n tá»« token thá»© hai cá»§a chuá»—i Ä‘áº§u vÃ o, vÃ¬ dÃ¹ sao thÃ¬ mÃ´ hÃ¬nh cÅ©ng khÃ´ng Ä‘Æ°a ra dá»± Ä‘oÃ¡n cho token Ä‘áº§u tiÃªn. Sau Ä‘Ã³, chÃºng ta cáº¯t logit cuá»‘i cÃ¹ng, vÃ¬ chÃºng ta khÃ´ng cÃ³ nhÃ£n cho token theo trÃ¬nh tá»± Ä‘áº§u vÃ o Ä‘áº§y Ä‘á»§. Nhá» Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n sá»± máº¥t mÃ¡t trÃªn má»—i máº«u vÃ  Ä‘áº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a táº¥t cáº£ cÃ¡c tá»« khÃ³a trong má»—i máº«u. Cuá»‘i cÃ¹ng, chÃºng ta tÃ­nh giÃ¡ trá»‹ trung bÃ¬nh cÃ³ trá»ng sá»‘ trÃªn táº¥t cáº£ cÃ¡c máº«u báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c láº§n xuáº¥t hiá»‡n dÆ°á»›i dáº¡ng trá»ng sá»‘. VÃ¬ chÃºng ta khÃ´ng muá»‘n loáº¡i bá» táº¥t cáº£ cÃ¡c máº«u khÃ´ng cÃ³ tá»« khÃ³a, chÃºng ta thÃªm 1 vÃ o cÃ¡c trá»ng sá»‘:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Dá»‹ch chuyá»ƒn Ä‘á»ƒ token < n dá»± Ä‘oÃ¡n n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # TÃ­nh Ä‘á»™ máº¥t mÃ¡t tá»«ng token
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Thay Ä‘á»•i kÃ­ch thÆ°á»›c vÃ  máº¥t mÃ¡t trung bÃ¬nh trÃªn má»—i máº«u
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # TÃ­nh toÃ¡n vÃ  chia tá»· trá»ng
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # TÃ­nh giÃ¡ trá»‹ trung bÃ¬nh cÃ³ trá»ng sá»‘
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

TrÆ°á»›c khi cÃ³ thá»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n vá»›i hÃ m máº¥t mÃ¡t má»›i tuyá»‡t vá»i nÃ y, chÃºng ta cáº§n chuáº©n bá»‹ má»™t sá»‘ thá»©:

- ChÃºng ta cáº§n bá»™ ghi dá»¯ liá»‡u Ä‘á»ƒ táº£i dá»¯ liá»‡u theo lÃ´.
- ChÃºng ta cáº§n thiáº¿t láº­p cÃ¡c thÃ´ng sá»‘ phÃ¢n rÃ£ trá»ng sá»‘.
- Theo thá»i gian, chÃºng ta muá»‘n Ä‘Ã¡nh giÃ¡, vÃ¬ váº­y sáº½ há»£p lÃ½ khi bao mÃ£ Ä‘Ã¡nh giÃ¡ trong má»™t hÃ m.

HÃ£y báº¯t Ä‘áº§u vá»›i bá»™ dá»¯ liá»‡u. ChÃºng ta chá»‰ cáº§n Ä‘áº·t Ä‘á»‹nh dáº¡ng cá»§a táº­p dá»¯ liá»‡u thÃ nh `"torch"`, vÃ  sau Ä‘Ã³ cÃ³ thá»ƒ chuyá»ƒn nÃ³ Ä‘áº¿n PyTorch `DataLoader` vá»›i kÃ­ch thÆ°á»›c lÃ´ thÃ­ch há»£p:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

Tiáº¿p theo, chÃºng ta nhÃ³m cÃ¡c tham sá»‘ Ä‘á»ƒ trÃ¬nh tá»‘i Æ°u hÃ³a biáº¿t nhá»¯ng thÃ´ng sá»‘ nÃ o sáº½ bá»‹ giáº£m trá»ng sá»‘ bá»• sung. ThÃ´ng thÆ°á»ng, táº¥t cáº£ cÃ¡c Ä‘iá»u khoáº£n thiÃªn vá»‹ vÃ  trá»ng sá»‘ LayerNorm Ä‘á»u Ä‘Æ°á»£c miá»…n trá»«; Ä‘Ã¢y lÃ  cÃ¡ch chÃºng ta cÃ³ thá»ƒ lÃ m Ä‘iá»u nÃ y:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

VÃ¬ chÃºng ta muá»‘n Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh thÆ°á»ng xuyÃªn trÃªn bá»™ xÃ¡c nháº­n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng ta hÃ£y viáº¿t má»™t hÃ m cho Ä‘iá»u Ä‘Ã³. NÃ³ chá»‰ cháº¡y qua bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ vÃ  táº­p há»£p táº¥t cáº£ cÃ¡c máº¥t mÃ¡t qua cÃ¡c quy trÃ¬nh:

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

Vá»›i hÃ m `evaluate()`, chÃºng ta cÃ³ thá»ƒ bÃ¡o cÃ¡o máº¥t mÃ¡t vÃ  [perplexity](/course/chapter7/3) theo khoáº£ng thá»i gian Ä‘á»u Ä‘áº·n. Tiáº¿p theo, chÃºng ta xÃ¡c Ä‘á»‹nh láº¡i mÃ´ hÃ¬nh cá»§a mÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o chÃºng ta huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u:

```py
model = GPT2LMHeadModel(config)
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh trÃ¬nh tá»‘i Æ°u hÃ³a cá»§a mÃ¬nh, sá»­ dá»¥ng hÃ m tá»« trÆ°á»›c Ä‘á»ƒ phÃ¢n chia cÃ¡c tham sá»‘ cho phÃ¢n rÃ£ trá»ng sá»‘:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

BÃ¢y giá», hÃ£y chuáº©n bá»‹ mÃ´ hÃ¬nh, trÃ¬nh tá»‘i Æ°u hÃ³a vÃ  bá»™ ghi dá»¯ liá»‡u Ä‘á»ƒ chÃºng ta cÃ³ thá»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ Náº¿u báº¡n Ä‘ang huáº¥n luyá»‡n trÃªn TPU, báº¡n sáº½ cáº§n chuyá»ƒn táº¥t cáº£ mÃ£ báº¯t Ä‘áº§u tá»« Ã´ á»Ÿ trÃªn vÃ o má»™t hÃ m huáº¥n luyá»‡n chuyÃªn dá»¥ng. Xem [Chapter 3](/course/chapter3) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

</Tip>

BÃ¢y giá», chÃºng ta Ä‘Ã£ gá»­i `train_dataloader` cá»§a mÃ¬nh tá»›i `accelerator.prepare()`, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»™ dÃ i cá»§a nÃ³ Ä‘á»ƒ tÃ­nh sá»‘ bÆ°á»›c huáº¥n luyá»‡n. HÃ£y nhá»› ráº±ng chÃºng ta pháº£i luÃ´n lÃ m Ä‘iá»u nÃ y sau khi chuáº©n bá»‹ dataloader, vÃ¬ phÆ°Æ¡ng thá»©c Ä‘Ã³ sáº½ thay Ä‘á»•i Ä‘á»™ dÃ i cá»§a nÃ³. ChÃºng ta sá»­ dá»¥ng má»™t lá»‹ch trÃ¬nh tuyáº¿n tÃ­nh cá»• Ä‘iá»ƒn tá»« tá»‘c Ä‘á»™ há»c Ä‘áº¿n 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

Cuá»‘i cÃ¹ng, Ä‘á»ƒ Ä‘áº©y mÃ´ hÃ¬nh lÃªn Hub, chÃºng ta sáº½ cáº§n táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `Repository` trong má»™t thÆ° má»¥c Ä‘ang lÃ m viá»‡c. TrÆ°á»›c tiÃªn, hÃ£y Ä‘Äƒng nháº­p vÃ o Hugging Face Hub, náº¿u báº¡n chÆ°a Ä‘Äƒng nháº­p. ChÃºng ta sáº½ xÃ¡c Ä‘á»‹nh tÃªn kho lÆ°u trá»¯ tá»« ID mÃ´ hÃ¬nh mÃ  ta muá»‘n cung cáº¥p cho mÃ´ hÃ¬nh cá»§a mÃ¬nh (vui lÃ²ng thay tháº¿ `repo_name` báº±ng sá»± lá»±a chá»n cá»§a riÃªng báº¡n; nÃ³ chá»‰ cáº§n chá»©a tÃªn ngÆ°á»i dÃ¹ng cá»§a báº¡n, Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ hÃ m `get_full_repo_name()` thá»±c hiá»‡n ):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ sao chÃ©p kho lÆ°u trá»¯ Ä‘Ã³ trong má»™t thÆ° má»¥c cá»¥c bá»™. Náº¿u nÃ³ Ä‘Ã£ tá»“n táº¡i, thÆ° má»¥c cá»¥c bá»™ nÃ y pháº£i lÃ  báº£n sao hiá»‡n cÃ³ cá»§a kho lÆ°u trá»¯ mÃ  chÃºng ta Ä‘ang lÃ m viá»‡c:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ táº£i lÃªn báº¥t cá»© thá»© gÃ¬ chÃºng ta lÆ°u trong `output_dir` báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `repo.push_to_hub()`. Äiá»u nÃ y sáº½ giÃºp chÃºng ta táº£i lÃªn cÃ¡c mÃ´ hÃ¬nh trung gian á»Ÿ cuá»‘i má»—i epoch.

TrÆ°á»›c khi huáº¥n luyá»‡n, hÃ£y cháº¡y thá»­ nhanh Ä‘á»ƒ xem chá»©c nÄƒng Ä‘Ã¡nh giÃ¡ cÃ³ hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng khÃ´ng:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

ÄÃ³ lÃ  nhá»¯ng giÃ¡ trá»‹ ráº¥t cao vá» má»©c máº¥t mÃ¡t vÃ  perplexity, nhÆ°ng Ä‘iá»u Ä‘Ã³ khÃ´ng Ä‘Ã¡ng ngáº¡c nhiÃªn vÃ¬ chÃºng ta chÆ°a huáº¥n luyá»‡n mÃ´ hÃ¬nh. CÃ¹ng vá»›i Ä‘Ã³, chÃºng ta Ä‘Ã£ chuáº©n bá»‹ má»i thá»© Ä‘á»ƒ viáº¿t pháº§n cá»‘t lÃµi cá»§a ká»‹ch báº£n huáº¥n luyá»‡n: vÃ²ng láº·p huáº¥n luyá»‡n. Trong vÃ²ng láº·p huáº¥n luyá»‡n, chÃºng ta láº·p qua dataloader vÃ  truyá»n cÃ¡c lÃ´ vÃ o mÃ´ hÃ¬nh. Vá»›i nháº­t kÃ½, sau Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ hÃ m máº¥t mÃ¡t tÃ¹y chá»‰nh cá»§a mÃ¬nh. ChÃºng ta chia tá»· lá»‡ máº¥t mÃ¡t theo sá»‘ bÆ°á»›c tÃ­ch lÅ©y gradient Ä‘á»ƒ khÃ´ng táº¡o ra máº¥t mÃ¡t lá»›n hÆ¡n khi tá»•ng há»£p nhiá»u bÆ°á»›c hÆ¡n. TrÆ°á»›c khi tá»‘i Æ°u hÃ³a, chÃºng ta cÅ©ng cáº¯t cÃ¡c gradient Ä‘á»ƒ há»™i tá»¥ tá»‘t hÆ¡n. Cuá»‘i cÃ¹ng, cá»© sau vÃ i bÆ°á»›c, chÃºng ta Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p há»£p Ä‘Ã¡nh giÃ¡ vá»›i hÃ m `eval()` má»›i cá»§a mÃ¬nh:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

Váº­y lÃ  xong - bÃ¢y giá» báº¡n cÃ³ vÃ²ng huáº¥n luyá»‡n tÃ¹y chá»‰nh cá»§a riÃªng mÃ¬nh cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£ cháº³ng háº¡n nhÆ° GPT-2 mÃ  báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh thÃªm theo nhu cáº§u cá»§a mÃ¬nh.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Hoáº·c táº¡o hÃ m máº¥t tÃ¹y chá»‰nh cá»§a riÃªng báº¡n phÃ¹ há»£p vá»›i trÆ°á»ng há»£p sá»­ dá»¥ng cá»§a báº¡n hoáº·c thÃªm má»™t bÆ°á»›c tÃ¹y chá»‰nh khÃ¡c vÃ o vÃ²ng láº·p huáº¥n luyá»‡n.

</Tip>

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Khi cháº¡y cÃ¡c thá»­ nghiá»‡m huáº¥n luyá»‡n dÃ i, báº¡n nÃªn ghi láº¡i cÃ¡c chá»‰ sá»‘ quan trá»ng báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ nhÆ° TensorBoard hoáº·c Weights & Biases. ThÃªm ghi nháº­t kÃ½ thÃ­ch há»£p vÃ o vÃ²ng láº·p huáº¥n luyá»‡n Ä‘á»ƒ báº¡n luÃ´n cÃ³ thá»ƒ kiá»ƒm tra quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra nhÆ° tháº¿ nÃ o.

</Tip>

{/if}
