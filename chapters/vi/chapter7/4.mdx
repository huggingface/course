<FrameworkSwitchCourse {fw} />

# Dá»‹ch mÃ¡y

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section4_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section4_tf.ipynb"},
]} />

{/if}

BÃ¢y giá» chÃºng ta hÃ£y Ä‘i sÃ¢u vÃ o dá»‹ch mÃ¡y. ÄÃ¢y lÃ  má»™t [tÃ¡c vá»¥ chuá»—i sang chuá»—i](/course/chapter1/7), cÃ³ nghÄ©a lÃ  Ä‘Ã¢y lÃ  má»™t váº¥n Ä‘á» cÃ³ thá»ƒ Ä‘Æ°á»£c hÃ¬nh thÃ nh nhÆ° Ä‘i tá»« má»™t chuá»—i nÃ y sang chuá»—i khÃ¡c. Theo nghÄ©a Ä‘Ã³, váº¥n Ä‘á» khÃ¡ giá»‘ng vá»›i [tÃ³m táº¯t](/course/chapter7/6) vÃ  báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh nhá»¯ng gÃ¬ chÃºng ta sáº½ tháº¥y á»Ÿ Ä‘Ã¢y thÃ nh cÃ¡c váº¥n Ä‘á» chuá»—i sang chuá»—i khÃ¡c nhÆ°:

- **Chuyá»ƒn vÄƒn phong**: Táº¡o mÃ´ hÃ¬nh *dá»‹ch* vÄƒn báº£n Ä‘Æ°á»£c viáº¿t theo má»™t phong cÃ¡ch nháº¥t Ä‘á»‹nh sang má»™t phong cÃ¡ch khÃ¡c (vÃ­ dá»¥: tá»« trang trá»ng sang thÃ´ng thÆ°á»ng hoáº·c tiáº¿ng Anh Shakespearean sang tiáº¿ng Anh hiá»‡n Ä‘áº¡i)
- **Há»i Ä‘Ã¡p chung**: Táº¡o má»™t mÃ´ hÃ¬nh táº¡o cÃ¢u tráº£ lá»i cho cÃ¡c cÃ¢u há»i, dá»±a trÃªn ngá»¯ cáº£nh

<Youtube id="1JvfrvZgi6c"/>

Náº¿u báº¡n cÃ³ má»™t kho vÄƒn báº£n Ä‘á»§ lá»›n vá»›i hai (hoáº·c nhiá»u) ngÃ´n ngá»¯, báº¡n cÃ³ thá»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh dá»‹ch má»›i tá»« Ä‘áº§u giá»‘ng nhÆ° chÃºng ta sáº½ lÃ m trong pháº§n [láº­p mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÃ¢n quáº£](/course/chapter7/6). Tuy nhiÃªn, sáº½ nhanh hÆ¡n náº¿u tinh chá»‰nh mÃ´ hÃ¬nh dá»‹ch hiá»‡n cÃ³, cÃ³ thá»ƒ lÃ  mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ nhÆ° mT5 hoáº·c mBART mÃ  báº¡n muá»‘n tinh chá»‰nh cho phÃ¹ há»£p vá»›i má»™t cáº·p ngÃ´n ngá»¯ cá»¥ thá»ƒ hoáº·c tháº­m chÃ­ lÃ  má»™t mÃ´ hÃ¬nh chuyÃªn dá»¥ng Ä‘á»ƒ dá»‹ch tá»« ngÃ´n ngá»¯ nÃ y sang ngÃ´n ngá»¯ khÃ¡c mÃ  báº¡n muá»‘n tinh chá»‰nh Ä‘á»ƒ phÃ¹ há»£p vá»›i kho dá»¯ liá»‡u cá»¥ thá»ƒ cá»§a mÃ¬nh.

Trong pháº§n nÃ y, chÃºng ta sáº½ tinh chá»‰nh mÃ´ hÃ¬nh Marian Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ dá»‹ch tá»« tiáº¿ng Anh sang tiáº¿ng PhÃ¡p (vÃ¬ ráº¥t nhiá»u nhÃ¢n viÃªn cá»§a Hugging Face nÃ³i cáº£ hai ngÃ´n ngá»¯ Ä‘Ã³) trÃªn [táº­p dá»¯ liá»‡u KDE4](https://huggingface.co/datasets/kde4 ), lÃ  táº­p dá»¯ liá»‡u cÃ¡c tá»‡p Ä‘Æ°á»£c báº£n Ä‘á»‹a hÃ³a cho [á»©ng dá»¥ng KDE](https://apps.kde.org/). MÃ´ hÃ¬nh chÃºng ta sáº½ sá»­ dá»¥ng Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn má»™t kho dá»¯ liá»‡u lá»›n gá»“m cÃ¡c vÄƒn báº£n tiáº¿ng PhÃ¡p vÃ  tiáº¿ng Anh Ä‘Æ°á»£c láº¥y tá»« [Táº­p dá»¯ liá»‡u Opus](https://opus.nlpl.eu/), thá»±c sá»± chá»©a táº­p dá»¯ liá»‡u KDE4. NhÆ°ng ngay cáº£ khi mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c mÃ  chÃºng ta sá»­ dá»¥ng Ä‘Ã£ nhÃ¬n tháº¥y dá»¯ liá»‡u Ä‘Ã³ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c cá»§a nÃ³, chÃºng ta sáº½ tháº¥y ráº±ng ta cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c phiÃªn báº£n tá»‘t hÆ¡n cá»§a nÃ³ sau khi tinh chá»‰nh.

Sau khi hoÃ n thÃ nh, chÃºng ta sáº½ cÃ³ má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n nhÆ° sau:

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

NhÆ° trong cÃ¡c pháº§n trÆ°á»›c, báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y mÃ´ hÃ¬nh thá»±c táº¿ mÃ  chÃºng ta sáº½ huáº¥n luyá»‡n vÃ  táº£i lÃªn Hub báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘oáº¡n mÃ£ bÃªn dÆ°á»›i vÃ  kiá»ƒm tra ká»¹ cÃ¡c dá»± Ä‘oÃ¡n cá»§a nÃ³ [táº¡i Ä‘Ã¢y](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.).

## Chuáº©n bá»‹ dá»¯ liá»‡u

Äá»ƒ tinh chá»‰nh hoáº·c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh dá»‹ch tá»« Ä‘áº§u, chÃºng ta sáº½ cáº§n má»™t táº­p dá»¯ liá»‡u phÃ¹ há»£p vá»›i tÃ¡c vá»¥. NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã¢y, chÃºng ta sáº½ sá»­ dá»¥ng [táº­p dá»¯ liá»‡u KDE4](https://huggingface.co/datasets/kde4) trong pháº§n nÃ y, nhÆ°ng báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh Ä‘oáº¡n mÃ£ Ä‘á»ƒ sá»­ dá»¥ng dá»¯ liá»‡u cá»§a riÃªng mÃ¬nh khÃ¡ dá»… dÃ ng, miá»…n lÃ  báº¡n cÃ³ cÃ¡c cáº·p cá»§a cÃ¡c cÃ¢u báº±ng hai ngÃ´n ngá»¯ mÃ  báº¡n muá»‘n dá»‹ch tá»« vÃ  tá»›i. Tham kháº£o láº¡i [ChÆ°Æ¡ng 5](/course/chapter5) náº¿u báº¡n cáº§n lá»i nháº¯c vá» cÃ¡ch táº£i dá»¯ liá»‡u tÃ¹y chá»‰nh cá»§a mÃ¬nh trong `Dataset`.

### Bá»™ dá»¯ liá»‡u KDE4

NhÆ° thÆ°á»ng lá»‡, chÃºng ta táº£i xuá»‘ng táº­p dá»¯ liá»‡u cá»§a mÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

Náº¿u báº¡n muá»‘n lÃ m viá»‡c vá»›i má»™t cáº·p ngÃ´n ngá»¯ khÃ¡c, báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh chÃºng báº±ng Ä‘oáº¡n mÃ£ cá»§a chÃºng. CÃ³ tá»•ng sá»‘ 92 ngÃ´n ngá»¯ cÃ³ sáºµn cho bá»™ dá»¯ liá»‡u nÃ y; báº¡n cÃ³ thá»ƒ tháº¥y táº¥t cáº£ chÃºng báº±ng cÃ¡ch má»Ÿ rá»™ng cÃ¡c tháº» ngÃ´n ngá»¯ trÃªn [tháº» dá»¯ liá»‡u](https://huggingface.co/datasets/kde4) cá»§a nÃ³.

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

HÃ£y xem táº­p dá»¯ liá»‡u:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

ChÃºng ta cÃ³ 210,173 cáº·p cÃ¢u, nhÆ°ng chá»‰ trong má»™t láº§n tÃ¡ch, vÃ¬ váº­y chÃºng ta sáº½ cáº§n táº¡o bá»™ kiá»ƒm Ä‘á»‹nh cá»§a riÃªng mÃ¬nh. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 5](/course/chapter5), `Dataset` cÃ³ phÆ°Æ¡ng thá»©c `train_test_split()` cÃ³ thá»ƒ giÃºp chÃºng ta. ChÃºng ta sáº½ cung cáº¥p má»™t háº¡t giá»‘ng (seed) cho kháº£ nÄƒng tÃ¡i táº¡o:

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

Báº¡n cÃ³ thá»ƒ Ä‘á»•i `"test"` thÃ nh `"validation"` nhÆ° sau:

```py
split_datasets["validation"] = split_datasets.pop("test")
```

BÃ¢y giá» chÃºng ta hÃ£y xem xÃ©t má»™t pháº§n tá»­ cá»§a táº­p dá»¯ liá»‡u:

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

ChÃºng ta nháº­n Ä‘Æ°á»£c má»™t tá»« Ä‘iá»ƒn cÃ³ hai cÃ¢u báº±ng cáº·p ngÃ´n ngá»¯ mÃ  ta yÃªu cáº§u. Má»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t cá»§a bá»™ dá»¯ liá»‡u Ä‘áº§y Ä‘á»§ cÃ¡c thuáº­t ngá»¯ khoa há»c mÃ¡y tÃ­nh ká»¹ thuáº­t nÃ y lÃ  chÃºng Ä‘á»u Ä‘Æ°á»£c dá»‹ch hoÃ n toÃ n báº±ng tiáº¿ng PhÃ¡p. Tuy nhiÃªn, cÃ¡c ká»¹ sÆ° PhÃ¡p thÆ°á»ng lÆ°á»i biáº¿ng vÃ  Ä‘á»ƒ láº¡i háº§u háº¿t cÃ¡c tá»« chuyÃªn ngÃ nh khoa há»c mÃ¡y tÃ­nh báº±ng tiáº¿ng Anh khi há» nÃ³i chuyá»‡n. VÃ­ dá»¥, á»Ÿ Ä‘Ã¢y, tá»« "threads" cÃ³ thá»ƒ xuáº¥t hiá»‡n trong má»™t cÃ¢u tiáº¿ng PhÃ¡p, Ä‘áº·c biá»‡t lÃ  trong má»™t cuá»™c trÃ² chuyá»‡n ká»¹ thuáº­t; nhÆ°ng trong táº­p dá»¯ liá»‡u nÃ y, nÃ³ Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch thÃ nh Ä‘Ãºng hÆ¡n lÃ  "fils de discussion". MÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c mÃ  chÃºng ta sá»­ dá»¥ng, Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn má»™t kho ngá»¯ liá»‡u lá»›n hÆ¡n cá»§a cÃ¡c cÃ¢u tiáº¿ng PhÃ¡p vÃ  tiáº¿ng Anh, cÃ³ tÃ¹y chá»n dá»… dÃ ng hÆ¡n lÃ  Ä‘á»ƒ nguyÃªn tá»« nhÆ° sau:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

Má»™t vÃ­ dá»¥ khÃ¡c vá» hÃ nh vi nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c nhÃ¬n tháº¥y vá»›i tá»« "plugin", Ä‘Ã¢y khÃ´ng pháº£i lÃ  má»™t tá»« chÃ­nh thá»©c trong tiáº¿ng PhÃ¡p nhÆ°ng háº§u háº¿t ngÆ°á»i báº£n ngá»¯ sáº½ hiá»ƒu vÃ  khÃ´ng báº­n tÃ¢m Ä‘áº¿n viá»‡c dá»‹ch.
Trong táº­p dá»¯ liá»‡u KDE4, tá»« nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng tiáº¿ng PhÃ¡p má»™t cÃ¡ch chÃ­nh thá»‘ng hÆ¡n thÃ nh "module d'extension":

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

Tuy nhiÃªn, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a chÃºng ta gáº¯n vá»›i tá»« tiáº¿ng Anh nhá» gá»n vÃ  quen thuá»™c:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

Sáº½ ráº¥t thÃº vá»‹ khi xem liá»‡u mÃ´ hÃ¬nh tinh chá»‰nh cá»§a mÃ¬nh cÃ³ tiáº¿p thu nhá»¯ng Ä‘áº·c Ä‘iá»ƒm Ä‘Ã³ cá»§a táº­p dá»¯ liá»‡u hay khÃ´ng (cáº£nh bÃ¡o spoiler: nÃ³ sáº½ xáº£y ra).

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** Má»™t tá»« tiáº¿ng Anh khÃ¡c thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong tiáº¿ng PhÃ¡p lÃ  "email". TÃ¬m máº«u Ä‘áº§u tiÃªn trong táº­p dá»¯ liá»‡u huáº¥n luyá»‡n sá»­ dá»¥ng tá»« nÃ y. NÃ³ Ä‘Æ°á»£c dá»‹ch nhÆ° tháº¿ nÃ o? LÃ m tháº¿ nÃ o Ä‘á»ƒ mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c dá»‹ch cÃ¹ng má»™t cÃ¢u tiáº¿ng Anh?

</Tip>

### Chuáº©n bá»‹ dá»¯ liá»‡u

<Youtube id="XAR8jnZZuUs"/>

BÃ¢y giá» báº¡n nÃªn biáº¿t Ä‘iá»u nÃ y: táº¥t cáº£ cÃ¡c vÄƒn báº£n cáº§n Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh táº­p há»£p cÃ¡c token ID Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c chÃºng. Äá»‘i vá»›i tÃ¡c vá»¥ nÃ y, chÃºng ta sáº½ cáº§n tokenize cáº£ Ä‘áº§u vÃ o vÃ  nhÃ£n. TÃ¡c vá»¥ Ä‘áº§u tiÃªn cá»§a chÃºng ta lÃ  táº¡o Ä‘á»‘i tÆ°á»£ng `tokenizer`. NhÆ° Ä‘Ã£ lÆ°u Ã½ trÆ°á»›c Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c tá»« tiáº¿ng Anh sang tiáº¿ng PhÃ¡p cá»§a Marian. Náº¿u báº¡n Ä‘ang thá»­ Ä‘oáº¡n mÃ£ nÃ y vá»›i má»™t cáº·p ngÃ´n ngá»¯ khÃ¡c, hÃ£y Ä‘áº£m báº£o Ä‘iá»u chá»‰nh checkpoint cá»§a mÃ´ hÃ¬nh. Tá»• chá»©c [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) cung cáº¥p hÆ¡n má»™t nghÃ¬n mÃ´ hÃ¬nh báº±ng nhiá»u ngÃ´n ngá»¯.

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="tf")
```

You can also replace the `model_checkpoint` with any other model you prefer from the [Hub](https://huggingface.co/models), or a local folder where you've saved a pretrained model and a tokenizer.

<Tip>

ğŸ’¡ Náº¿u báº¡n Ä‘ang sá»­ dá»¥ng trÃ¬nh tokenize Ä‘a ngÃ´n ngá»¯ nhÆ° mBART, mBART-50 hoáº·c M2M100, báº¡n sáº½ cáº§n Ä‘áº·t mÃ£ ngÃ´n ngá»¯ cá»§a Ä‘áº§u vÃ o vÃ  nhÃ£n cá»§a mÃ¬nh trong trÃ¬nh tokenize báº±ng cÃ¡ch Ä‘áº·t `tokenizer.src_lang` vÃ  `tokenizer.tgt_lang` á»Ÿ bÃªn pháº£i cÃ¡c giÃ¡ trá»‹.

</Tip>

Viá»‡c chuáº©n bá»‹ dá»¯ liá»‡u cá»§a chÃºng ta khÃ¡ Ä‘Æ¡n giáº£n. Chá»‰ cÃ³ má»™t Ä‘iá»u cáº§n nhá»›: báº¡n xá»­ lÃ½ cÃ¡c Ä‘áº§u vÃ o nhÆ° bÃ¬nh thÆ°á»ng, nhÆ°ng Ä‘á»‘i vá»›i cÃ¡c nhÃ£n, báº¡n cáº§n pháº£i bá»c tokenizer bÃªn trong trÃ¬nh quáº£n lÃ½ ngá»¯ cáº£nh `as_target_tokenizer()`.

TrÃ¬nh quáº£n lÃ½ ngá»¯ cáº£nh trong Python Ä‘Æ°á»£c giá»›i thiá»‡u vá»›i cÃ¢u lá»‡nh `with` vÃ  ráº¥t há»¯u Ã­ch khi báº¡n cÃ³ hai hoáº¡t Ä‘á»™ng liÃªn quan Ä‘á»ƒ thá»±c thi nhÆ° má»™t cáº·p. VÃ­ dá»¥ phá»• biáº¿n nháº¥t vá» Ä‘iá»u nÃ y lÃ  khi báº¡n viáº¿t hoáº·c Ä‘á»c má»™t tá»‡p, thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n bÃªn trong má»™t lá»‡nh nhÆ°:

```
with open(file_path) as f:
    content = f.read()
```

á» Ä‘Ã¢y, hai hoáº¡t Ä‘á»™ng liÃªn quan Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° má»™t cáº·p lÃ  cÃ¡c hÃ nh Ä‘á»™ng má»Ÿ vÃ  Ä‘Ã³ng tá»‡p. Äá»‘i tÆ°á»£ng tÆ°Æ¡ng á»©ng vá»›i tá»‡p Ä‘Ã£ má»Ÿ `f` chá»‰ tá»“n táº¡i bÃªn trong khá»‘i Ä‘Æ°á»£c thá»¥t lá» dÆ°á»›i dáº¥u `with`; sá»± má»Ÿ Ä‘áº§u xáº£y ra trÆ°á»›c khá»‘i Ä‘Ã³ vÃ  Ä‘Ã³ng á»Ÿ cuá»‘i khá»‘i.

Trong trÆ°á»ng há»£p nÃ y, trÃ¬nh quáº£n lÃ½ ngá»¯ cáº£nh `as_target_tokenizer()` sáº½ Ä‘áº·t tokenizer á»Ÿ ngÃ´n ngá»¯ Ä‘áº§u ra (á»Ÿ Ä‘Ã¢y, tiáº¿ng PhÃ¡p) trÆ°á»›c khi khá»‘i Ä‘Æ°á»£c thá»¥t lá» Ä‘Æ°á»£c thá»±c thi, sau Ä‘Ã³ Ä‘áº·t nÃ³ trá»Ÿ láº¡i báº±ng ngÃ´n ngá»¯ Ä‘áº§u vÃ o (á»Ÿ Ä‘Ã¢y, tiáº¿ng Anh).

VÃ¬ váº­y, viá»‡c xá»­ lÃ½ trÆ°á»›c má»™t máº«u trÃ´ng nhÆ° tháº¿ nÃ y:

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence)
with tokenizer.as_target_tokenizer():
    targets = tokenizer(fr_sentence)
```

Náº¿u chÃºng ta quÃªn tokenize cÃ¡c nhÃ£n bÃªn trong trÃ¬nh quáº£n lÃ½ ngá»¯ cáº£nh, chÃºng sáº½ Ä‘Æ°á»£c tokenize bá»Ÿi trÃ¬nh tokenize Ä‘áº§u vÃ o, trong trÆ°á»ng há»£p mÃ´ hÃ¬nh Marian sáº½ khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t chÃºt nÃ o:

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(targets["input_ids"]))
```

```python out
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, viá»‡c sá»­ dá»¥ng trÃ¬nh tokenize tiáº¿ng Anh Ä‘á»ƒ xá»­ lÃ½ trÆ°á»›c má»™t cÃ¢u tiáº¿ng PhÃ¡p dáº«n Ä‘áº¿n nhiá»u token hÆ¡n, vÃ¬ trÃ¬nh tokenize khÃ´ng biáº¿t báº¥t ká»³ tá»« tiáº¿ng PhÃ¡p nÃ o (ngoáº¡i trá»« nhá»¯ng tá»« cÅ©ng xuáº¥t hiá»‡n trong tiáº¿ng Anh, nhÆ° "discussion").

Cáº£ `inputs` vÃ  `targets` Ä‘á»u lÃ  tá»« Ä‘iá»ƒn vá»›i cÃ¡c khÃ³a thÃ´ng thÆ°á»ng cá»§a chÃºng ta (ID Ä‘áº§u vÃ o, attention mask, v.v.), vÃ¬ váº­y bÆ°á»›c cuá»‘i cÃ¹ng lÃ  Ä‘áº·t `"labels"` bÃªn trong cÃ¡c Ä‘áº§u vÃ o. ChÃºng ta thá»±c hiá»‡n Ä‘iá»u nÃ y trong chá»©c nÄƒng tiá»n xá»­ lÃ½ mÃ  ta sáº½ Ã¡p dá»¥ng trÃªn cÃ¡c táº­p dá»¯ liá»‡u:

```python
max_input_length = 128
max_target_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Thiáº¿t láº­p tokenizer cho nhÃ£n
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

Note that we set similar maximum lengths for our inputs and outputs. Since the texts we're dealing with seem pretty short, we use 128.

<Tip>

ğŸ’¡ Náº¿u báº¡n Ä‘ang sá»­ dá»¥ng mÃ´ hÃ¬nh T5 (cá»¥ thá»ƒ hÆ¡n lÃ  má»™t trong cÃ¡c checkpoint `t5-xxx`), mÃ´ hÃ¬nh sáº½ mong Ä‘á»£i cÃ¡c Ä‘áº§u vÃ o vÄƒn báº£n cÃ³ tiá»n tá»‘ cho biáº¿t tÃ¡c vá»¥ Ä‘ang thá»±c hiá»‡n, cháº³ng háº¡n nhÆ° `translate: English to French:`.

</Tip>

<Tip warning={true}>

âš ï¸ ChÃºng ta khÃ´ng chÃº Ã½ Ä‘áº¿n attention mask cá»§a cÃ¡c nhÃ£n, vÃ¬ mÃ´ hÃ¬nh sáº½ khÃ´ng mong Ä‘á»£i Ä‘iá»u Ä‘Ã³. Thay vÃ o Ä‘Ã³, cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng vá»›i token Ä‘á»‡m pháº£i Ä‘Æ°á»£c Ä‘áº·t thÃ nh `-100` Ä‘á»ƒ chÃºng bá»‹ bá» qua trong tÃ­nh toÃ¡n máº¥t mÃ¡t. Äiá»u nÃ y sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u cá»§a chÃºng ta sau nÃ y vÃ¬ chÃºng ta Ä‘ang Ã¡p dá»¥ng Ä‘á»‡m Ä‘á»™ng, nhÆ°ng náº¿u báº¡n sá»­ dá»¥ng Ä‘á»‡m á»Ÿ Ä‘Ã¢y, báº¡n nÃªn Ä‘iá»u chá»‰nh chá»©c nÄƒng tiá»n xá»­ lÃ½ Ä‘á»ƒ Ä‘áº·t táº¥t cáº£ cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng vá»›i token Ä‘á»‡m thÃ nh `-100`.

</Tip>

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng tiá»n xá»­ lÃ½ Ä‘Ã³ trong má»™t láº§n trÃªn táº¥t cáº£ cÃ¡c pháº§n cá»§a táº­p dá»¯ liá»‡u cá»§a mÃ¬nh:

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

BÃ¢y giá» dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½, chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh tiá»n xá»­ lÃ½ cá»§a mÃ¬nh!

{#if fw === 'pt'}

## Tinh chá»‰nh mÃ´ hÃ¬nh vá»›i API `Trainer`

Äoáº¡n mÃ£ thá»±c sá»­ dá»¥ng `Trainer` sáº½ giá»‘ng nhÆ° trÆ°á»›c Ä‘Ã¢y, chá»‰ vá»›i má»™t thay Ä‘á»•i nhá»: chÃºng ta sá»­ dá»¥ng [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) táº¡i Ä‘Ã¢y, lÃ  má»™t lá»›p con cá»§a `Trainer` sáº½ cho phÃ©p chÃºng ta xá»­ lÃ½ tá»‘t viá»‡c Ä‘Ã¡nh giÃ¡, sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `generate()` Ä‘á»ƒ dá»± Ä‘oÃ¡n káº¿t quáº£ Ä‘áº§u ra tá»« cÃ¡c Ä‘áº§u vÃ o. ChÃºng ta sáº½ Ä‘i sÃ¢u vÃ o váº¥n Ä‘á» Ä‘Ã³ chi tiáº¿t hÆ¡n khi ta nÃ³i vá» tÃ­nh toÃ¡n sá»‘ liá»‡u.

Äiá»u Ä‘áº§u tiÃªn, chÃºng ta cáº§n má»™t mÃ´ hÃ¬nh thá»±c táº¿ Ä‘á»ƒ tinh chá»‰nh. ChÃºng ta sáº½ sá»­ dá»¥ng API `AutoModel`:

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## Tinh chá»‰nh mÃ´ hÃ¬nh vá»›i Keras

Äiá»u Ä‘áº§u tiÃªn, chÃºng ta cáº§n má»™t mÃ´ hÃ¬nh thá»±c táº¿ Ä‘á»ƒ tinh chá»‰nh. ChÃºng ta sáº½ sá»­ dá»¥ng API `AutoModel`:

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

ğŸ’¡ Checkpoint `Helsinki-NLP/opus-mt-en-fr` chá»‰ cÃ³ trá»ng sá»‘ PyTorch, nÃªn báº¡n sáº½ nháº­n Ä‘Æ°á»£c lá»—i náº¿u báº¡n cá»‘ táº£i mÃ´ hÃ¬nh mÃ  khÃ´ng sá»­ dá»¥ng tham sá»‘ `from_pt=True`, thÆ° viá»‡n sáº½ tá»± Ä‘á»™ng táº£i vÃ  chuyá»ƒn cÃ¡c trá»ng sá»‘ Pytorch cho báº¡n. NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, ráº¥t Ä‘Æ¡n giáº£n Ä‘á»ƒ chuyá»ƒn giá»¯a cÃ¡c khung trong ğŸ¤— Transformers!

</Tip>

{/if}

LÆ°u Ã½ ráº±ng láº§n nÃ y chÃºng ta Ä‘ang sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá» tÃ¡c vá»¥ dá»‹ch vÃ  thá»±c sá»± cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng, vÃ¬ váº­y khÃ´ng cÃ³ cáº£nh bÃ¡o nÃ o vá» viá»‡c thiáº¿u cÃ¡c trá»ng sá»‘ hoáº·c nhá»¯ng trá»ng sá»‘ má»›i Ä‘Æ°á»£c khá»Ÿi táº¡o.

### Äá»‘i chiáº¿u dá»¯ liá»‡u

ChÃºng ta sáº½ cáº§n má»™t cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½ pháº§n Ä‘á»‡m cho phÃ¢n phá»‘i Ä‘á»™ng. ChÃºng ta khÃ´ng thá»ƒ chá»‰ sá»­ dá»¥ng má»™t `DataCollatorWithPadding` nhÆ° [ChÆ°Æ¡ng 3](/course/ chapter3) trong trÆ°á»ng há»£p nÃ y, bá»Ÿi vÃ¬ Ä‘iá»u Ä‘Ã³ chá»‰ Ä‘á»‡m cÃ¡c Ä‘áº§u vÃ o (ID Ä‘áº§u vÃ o, attention mask, vÃ  loáº¡i token ID). CÃ¡c nhÃ£n cá»§a chÃºng ta cÅ©ng pháº£i Ä‘Æ°á»£c Ä‘á»‡m theo chiá»u dÃ i tá»‘i Ä‘a cÃ³ trong nhÃ£n. VÃ , nhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã¢y, giÃ¡ trá»‹ Ä‘á»‡m Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘á»‡m cÃ¡c nhÃ£n pháº£i lÃ  `-100` chá»© khÃ´ng pháº£i token Ä‘á»‡m cá»§a trÃ¬nh tokenize, Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c giÃ¡ trá»‹ Ä‘á»‡m Ä‘Ã³ bá»‹ bá» qua trong tÃ­nh toÃ¡n máº¥t mÃ¡t.

Táº¥t cáº£ Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). Giá»‘ng nhÆ° `DataCollatorWithPadding`, nÃ³ sá»­ dá»¥ng `tokenizer` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xá»­ lÃ½ trÆ°á»›c cÃ¡c Ä‘áº§u vÃ o, nhÆ°ng nÃ³ cÅ©ng láº¥y `model`. Äiá»u nÃ y lÃ  do trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u nÃ y cÅ©ng sáº½ chá»‹u trÃ¡ch nhiá»‡m chuáº©n bá»‹ cÃ¡c ID Ä‘áº§u vÃ o cá»§a bá»™ giáº£i mÃ£, lÃ  cÃ¡c phiÃªn báº£n Ä‘Æ°á»£c dá»‹ch chuyá»ƒn cá»§a cÃ¡c nhÃ£n vá»›i má»™t token Ä‘áº·c biá»‡t á»Ÿ Ä‘áº§u. VÃ¬ sá»± thay Ä‘á»•i nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n hÆ¡i khÃ¡c Ä‘á»‘i vá»›i cÃ¡c kiáº¿n â€‹â€‹trÃºc khÃ¡c nhau, nÃªn `DataCollatorForSeq2Seq` cáº§n biáº¿t Ä‘á»‘i tÆ°á»£ng `model`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

Äá»ƒ kiá»ƒm tra Ä‘iá»u nÃ y trÃªn má»™t sá»‘ máº«u, chÃºng ta chá»‰ cáº§n gá»i nÃ³ trong danh sÃ¡ch cÃ¡c vÃ­ dá»¥ tá»« bá»™ huáº¥n luyá»‡n Ä‘Æ°á»£c tokenize cá»§a mÃ¬nh:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

ChÃºng tÃ´i cÃ³ thá»ƒ kiá»ƒm tra nhÃ£n Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‡m Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a lÃ´ hay chÆ°a, báº±ng cÃ¡ch sá»­ dá»¥ng `-100`:

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

VÃ  chÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ xem xÃ©t cÃ¡c ID Ä‘áº§u vÃ o cá»§a bá»™ giáº£i mÃ£, Ä‘á»ƒ biáº¿t ráº±ng chÃºng lÃ  cÃ¡c phiÃªn báº£n Ä‘Æ°á»£c thay Ä‘á»•i cá»§a nhÃ£n:

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c nhÃ£n cho cÃ¡c pháº§n tá»­ Ä‘áº§u tiÃªn vÃ  thá»© hai trong táº­p dá»¯ liá»‡u cá»§a mÃ¬nh:

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

ChÃºng ta sáº½ truyá»n `data_collator` vÃ o `Seq2SeqTrainer`. Tiáº¿p theo, chÃºng ta hÃ£y xem xÃ©t chá»‰ sá»‘.

{:else}

Ta cÃ³ thá»ƒ sá»­ dá»¥ng `data_collator` Ä‘á»ƒ chuyá»ƒn má»—i pháº§n dá»¯ liá»‡u thÃ nh `tf.data.Dataset`, sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n:

```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}

### ThÆ°á»›c Ä‘o

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

TÃ­nh nÄƒng mÃ  `Seq2SeqTrainer` thÃªm vÃ o lá»›p cha `Trainer` cá»§a nÃ³ lÃ  kháº£ nÄƒng sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `generate()` trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ hoáº·c dá»± Ä‘oÃ¡n. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, mÃ´ hÃ¬nh sáº½ sá»­ dá»¥ng `decoder_input_ids` vá»›i attention mask Ä‘áº£m báº£o nÃ³ khÃ´ng sá»­ dá»¥ng cÃ¡c token sau token mÃ  nÃ³ Ä‘ang cá»‘ gáº¯ng dá»± Ä‘oÃ¡n, Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n. Trong quÃ¡ trÃ¬nh luáº­n suy, chÃºng ta sáº½ khÃ´ng thá»ƒ sá»­ dá»¥ng nhá»¯ng thá»© Ä‘Ã³ vÃ¬ chÃºng ta sáº½ khÃ´ng cÃ³ nhÃ£n, vÃ¬ váº­y, báº¡n nÃªn Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a mÃ¬nh vá»›i cÃ¹ng má»™t thiáº¿t láº­p.

NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 1](/course/chapter1/6), bá»™ giáº£i mÃ£ thá»±c hiá»‡n luáº­n suy báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»«ng token - má»™t thá»© Ä‘Æ°á»£c triá»ƒn khai phÃ­a sau trong ğŸ¤— Transformers báº±ng phÆ°Æ¡ng thá»©c `generate()`. `Seq2SeqTrainer` sáº½ cho phÃ©p chÃºng ta sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Ä‘Ã³ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ náº¿u chÃºng ta Ä‘áº·t `predict_with_generate=True`.

{/if}

Chá»‰ sá»‘ truyá»n thá»‘ng Ä‘Æ°á»£c sá»­ dá»¥ng cho bÃ i toÃ¡n dá»‹ch lÃ  [Ä‘iá»ƒm BLEU](https://en.wikipedia.org/wiki/BLEU), Ä‘Æ°á»£c giá»›i thiá»‡u trong [má»™t bÃ i bÃ¡o nÄƒm 2002](https://aclanthology.org/P02-1040.pdf) bá»Ÿi Kishore Papineni vÃ  cá»™ng sá»±. Äiá»ƒm BLEU Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ gáº§n gÅ©i cá»§a báº£n dá»‹ch vá»›i nhÃ£n cá»§a chÃºng. NÃ³ khÃ´ng Ä‘o lÆ°á»ng má»©c Ä‘á»™ dá»… hiá»ƒu hoáº·c tÃ­nh Ä‘Ãºng ngá»¯ phÃ¡p cá»§a cÃ¡c Ä‘áº§u ra Ä‘Æ°á»£c táº¡o ra cá»§a mÃ´ hÃ¬nh, nhÆ°ng sá»­ dá»¥ng cÃ¡c quy táº¯c thá»‘ng kÃª Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c tá»« trong cÃ¡c Ä‘áº§u ra Ä‘Æ°á»£c táº¡o cÅ©ng xuáº¥t hiá»‡n trong cÃ¡c nhÃ£n. NgoÃ i ra, cÃ³ cÃ¡c quy táº¯c pháº¡t viá»‡c láº·p láº¡i cÃ¡c tá»« giá»‘ng nhau náº¿u chÃºng khÃ´ng Ä‘Æ°á»£c láº·p láº¡i trong cÃ¡c nhÃ£n (Ä‘á»ƒ trÃ¡nh mÃ´ hÃ¬nh xuáº¥t ra cÃ¡c cÃ¢u nhÆ° `"the the the"`) vÃ  xuáº¥t ra cÃ¡c cÃ¢u ngáº¯n hÆ¡n cÃ¡c cÃ¢u trong nhÃ£n (Ä‘á»ƒ trÃ¡nh mÃ´ hÃ¬nh xuáº¥t ra cÃ¡c cÃ¢u nhÆ° `"the"`).

Má»™t Ä‘iá»ƒm yáº¿u cá»§a BLEU lÃ  nÃ³ mong Ä‘á»£i vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c tokenize, Ä‘iá»u nÃ y gÃ¢y khÃ³ khÄƒn cho viá»‡c so sÃ¡nh Ä‘iá»ƒm giá»¯a cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng cÃ¡c bá»™ tokenize khÃ¡c nhau. VÃ¬ váº­y, thay vÃ o Ä‘Ã³, chá»‰ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n nháº¥t cho cÃ¡c mÃ´ hÃ¬nh dá»‹ch Ä‘iá»ƒm chuáº©n ngÃ y nay lÃ  [SacreBLEU](https://github.com/mjpost/sacrebleu), giáº£i quyáº¿t Ä‘iá»ƒm yáº¿u nÃ y (vÃ  cÃ¡c chá»‰ sá»‘ khÃ¡c) báº±ng cÃ¡ch chuáº©n hÃ³a bÆ°á»›c tokenize. Äá»ƒ sá»­ dá»¥ng chá»‰ sá»‘ nÃ y, trÆ°á»›c tiÃªn chÃºng ta cáº§n cÃ i Ä‘áº·t thÆ° viá»‡n SacreBLEU:

```py
!pip install sacrebleu
```

ChÃºng ta cÃ³ thá»ƒ táº£i nÃ³ vá»›i `evaluate.load()` nhÆ° chÃºng ta Ä‘Ã£ lÃ m trong [ChÆ°Æ¡ng 3](/course/chapter3):

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

Chá»‰ sá»‘ nÃ y sáº½ láº¥y vÄƒn báº£n lÃ m Ä‘áº§u vÃ o vÃ  nhÃ£n. NÃ³ Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cháº¥p nháº­n má»™t sá»‘ nhÃ£n cÃ³ thá»ƒ cháº¥p nháº­n Ä‘Æ°á»£c, vÃ¬ thÆ°á»ng cÃ³ nhiá»u báº£n dá»‹ch cÃ³ thá»ƒ cháº¥p nháº­n Ä‘Æ°á»£c cá»§a cÃ¹ng má»™t cÃ¢u - táº­p dá»¯ liá»‡u ta Ä‘ang sá»­ dá»¥ng chá»‰ cung cáº¥p má»™t nhÃ£n, nhÆ°ng khÃ´ng hiáº¿m trong NLP Ä‘á»ƒ tÃ¬m táº­p dá»¯ liá»‡u cung cáº¥p má»™t sá»‘ cÃ¢u dÆ°á»›i dáº¡ng nhÃ£n. VÃ¬ váº­y, cÃ¡c dá»± Ä‘oÃ¡n pháº£i lÃ  má»™t danh sÃ¡ch cÃ¡c cÃ¢u, nhÆ°ng cÃ¡c tham chiáº¿u pháº£i lÃ  má»™t danh sÃ¡ch cÃ¡c danh sÃ¡ch cÃ¡c cÃ¢u.

HÃ£y thá»­ má»™t máº«u:

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

Ta nháº­n Ä‘Æ°á»£c Ä‘iá»ƒm BLEU lÃ  46.75, khÃ¡ tá»‘t - Ä‘á»ƒ tham kháº£o, mÃ´ hÃ¬nh Transformer ban Ä‘áº§u trong bÃ i bÃ¡o ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf) Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm BLEU lÃ  41.8 cho má»™t tÃ¡c vá»¥ dá»‹ch tÆ°Æ¡ng tá»± giá»¯a tiáº¿ng Anh vÃ  tiáº¿ng PhÃ¡p! (Äá»ƒ biáº¿t thÃªm thÃ´ng tin vá» cÃ¡c chá»‰ sá»‘ riÃªng láº», nhÆ° `counts` vÃ  `bp`, hÃ£y xem [kho lÆ°u trá»¯ SacreBLEU](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74 ).) Máº·t khÃ¡c, náº¿u chÃºng ta thá»­ vá»›i hai loáº¡i dá»± Ä‘oÃ¡n khÃ´ng tá»‘t (nhiá»u láº§n láº·p láº¡i hoáº·c quÃ¡ ngáº¯n) thÆ°á»ng xuáº¥t hiá»‡n trong cÃ¡c mÃ´ hÃ¬nh dá»‹ch, chÃºng ta sáº½ nháº­n Ä‘Æ°á»£c Ä‘iá»ƒm BLEU khÃ¡ tá»‡:

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

Äiá»ƒm sá»‘ cÃ³ thá»ƒ tÄƒng tá»« 0 Ä‘áº¿n 100 vÃ  cÃ ng cao thÃ¬ cÃ ng tá»‘t.

{#if fw === 'tf'}

Äá»ƒ chuyá»ƒn tá»« káº¿t quáº£ Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh thÃ nh vÄƒn báº£n mÃ  chá»‰ sá»‘ cÃ³ thá»ƒ sá»­ dá»¥ng, chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `tokenizer.batch_decode()`. ChÃºng ta chá»‰ cáº§n xÃ³a táº¥t cáº£ cÃ¡c `-100` trong cÃ¡c nhÃ£n; tokenizer sáº½ tá»± Ä‘á»™ng lÃ m Ä‘iá»u tÆ°Æ¡ng tá»± Ä‘á»‘i vá»›i token Ä‘á»‡m. HÃ£y xÃ¡c Ä‘á»‹nh má»™t hÃ m sá»­ dá»¥ng mÃ´ hÃ¬nh vÃ  táº­p dá»¯ liá»‡u cá»§a chÃºng ta vÃ  tÃ­nh toÃ¡n cÃ¡c sá»‘ liá»‡u trÃªn Ä‘Ã³. VÃ¬ viá»‡c táº¡o chuá»—i dÃ i cÃ³ thá»ƒ cháº­m, chÃºng ta láº¥y máº«u thay tháº¿ bá»™ kiá»ƒm Ä‘á»‹nh Ä‘á»ƒ Ä‘áº£m báº£o Ä‘iá»u nÃ y khÃ´ng cháº¡y mÃ£i mÃ£i:

```py
import numpy as np


def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch in tf_generate_dataset:
        predictions = model.generate(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = batch["labels"].numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

To get from the model outputs to texts the metric can use, we will use the `tokenizer.batch_decode()` method. We just have to clean up all the `-100`s in the labels (the tokenizer will automatically do the same for the padding token):

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # Trong trÆ°á»ng há»£p mÃ´ hÃ¬nh tráº£ vá» nhiá»u hÆ¡n logit dá»± Ä‘oÃ¡n
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Thay cÃ¡c gÃ­a trá»‹ -100 trong nhÃ£n vÃ¬ ta khÃ´ng giáº£i mÃ£ chÃºng
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Thá»±c má»™t má»™t xá»‘ háº­u xá»§ lÃ½ Ä‘Æ¡n giáº£n
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

BÃ¢y giá» Ä‘iá»u nÃ y Ä‘Ã£ hoÃ n táº¥t, chÃºng ta Ä‘Ã£ sáºµn sÃ ng tinh chá»‰nh mÃ´ hÃ¬nh cá»§a mÃ¬nh!

### Tinh chá»‰nh mÃ´ hÃ¬nh

BÆ°á»›c Ä‘áº§u tiÃªn lÃ  Ä‘Äƒng nháº­p vÃ o Hugging Face Ä‘á»ƒ báº¡n cÃ³ thá»ƒ táº£i káº¿t quáº£ cá»§a mÃ¬nh lÃªn Model Hub. CÃ³ má»™t chá»©c nÄƒng tiá»‡n lá»£i Ä‘á»ƒ giÃºp báº¡n lÃ m Ä‘iá»u nÃ y trong notebook:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Thao tÃ¡c nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p Hugging Face cá»§a mÃ¬nh.

Náº¿u báº¡n khÃ´ng lÃ m viá»‡c trong notebook, chá»‰ cáº§n nháº­p dÃ²ng sau vÃ o terminal cá»§a báº¡n:

```bash
huggingface-cli login
```

{#if fw === 'tf'}

TrÆ°á»›c khi báº¯t Ä‘áº§u, hÃ£y xem loáº¡i káº¿t quáº£ nÃ o chÃºng tÃ´i nháº­n Ä‘Æ°á»£c tá»« mÃ´ hÃ¬nh cá»§a mÃ¬nh mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n:

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

Khi Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n, chÃºng ta cÃ³ thá»ƒ chuáº©n bá»‹ má»i thá»© cáº§n Ä‘á»ƒ biÃªn dá»‹ch vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh. LÆ°u Ã½ viá»‡c sá»­ dá»¥ng `tf.keras.mixed_precision.set_global_policy("mixed_float16")` - Ä‘iá»u nÃ y sáº½ yÃªu cáº§u Keras huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng float16, Ä‘iá»u cÃ³ thá»ƒ giÃºp tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ trÃªn cÃ¡c GPU há»— trá»£ nÃ³ (Nvidia 20xx/V100 hoáº·c má»›i hÆ¡n).

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# Sá»‘ bÆ°á»›c huáº¥n luyá»‡n lÃ  sá»‘ lÆ°á»£ng máº«u trong táº­p dá»¯ liá»‡u, chia cho kÃ­ch thÆ°á»›c lÃ´ sau Ä‘Ã³ nhÃ¢n
# vá»›i tá»•ng sá»‘ epoch. LÆ°u Ã½ ráº±ng tf_train_dataset á»Ÿ Ä‘Ã¢y lÃ  tf.data.Dataset theo lÃ´,
# khÃ´ng pháº£i lÃ  Hugging Face Dataset ban Ä‘áº§u, vÃ¬ váº­y len() cá»§a nÃ³ vá»‘n lÃ  num_samples // batch_size.

num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Huáº¥n luyá»‡n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Tiáº¿p theo, chÃºng ta xÃ¡c Ä‘á»‹nh má»™t `PushToHubCallback` Ä‘á»ƒ táº£i mÃ´ hÃ¬nh lÃªn Hub trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [pháº§n 2]((/course/chapter7/2)), vÃ  sau Ä‘Ã³ chÃºng ta chá»‰ cáº§n Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh vá»›i lá»‡nh gá»i láº¡i Ä‘Ã³:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

LÆ°u Ã½ ráº±ng báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y lÃªn báº±ng tham sá»‘ `hub_model_id` (cá»¥ thá»ƒ lÃ  báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y lÃªn má»™t tá»• chá»©c). VÃ­ dá»¥: khi chÃºng tÃ´i Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course), chÃºng ta Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` thÃ nh `Seq2SeqTrainingArguments`. Theo máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ¬ váº­y á»Ÿ Ä‘Ã¢y nÃ³ sáº½ lÃ  `"sgugger/marian-finetuned-kde4-en-to-fr"` (lÃ  mÃ´ hÃ¬nh mÃ  chÃºng tÃ´i Ä‘Ã£ liÃªn káº¿t vá»›i á»Ÿ Ä‘áº§u pháº§n nÃ y).

<Tip>

ğŸ’¡ Náº¿u thÆ° má»¥c Ä‘áº§u ra báº¡n Ä‘ang sá»­ dá»¥ng Ä‘Ã£ tá»“n táº¡i, nÃ³ cáº§n pháº£i lÃ  báº£n sao cá»¥c bá»™ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n. Náº¿u khÃ´ng, báº¡n sáº½ gáº·p lá»—i khi gá»i `model.fit()` vÃ  sáº½ cáº§n Ä‘áº·t tÃªn má»›i.

</Tip>

Cuá»‘i cÃ¹ng, hÃ£y xem cÃ¡c chá»‰ sá»‘ cá»§a chÃºng ta trÃ´ng nhÆ° tháº¿ nÃ o khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘Ã£ káº¿t thÃºc:

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

á» giai Ä‘oáº¡n nÃ y, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tiá»‡n Ã­ch luáº­n suy trÃªn Model Hub Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh cá»§a mÃ¬nh vÃ  chia sáº» vá»›i báº¡n bÃ¨. Báº¡n Ä‘Ã£ tinh chá»‰nh thÃ nh cÃ´ng má»™t mÃ´ hÃ¬nh trong tÃ¡c vá»¥ dá»‹ch - xin chÃºc má»«ng!

{:else}

Khi Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n, chÃºng ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh `Seq2SeqTrainingArguments`. Giá»‘ng nhÆ° Ä‘á»‘i vá»›i `Trainer`, chÃºng ta sá»­ dá»¥ng má»™t lá»›p con cá»§a `TrainingArguments` chá»©a thÃªm má»™t sá»‘ trÆ°á»ng:

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

NgoÃ i cÃ¡c siÃªu tham sá»‘ thÃ´ng thÆ°á»ng (nhÆ° tá»‘c Ä‘á»™ há»c, sá»‘ epoch, kÃ­ch thÆ°á»›c lÃ´ vÃ  má»™t sá»‘ phÃ¢n rÃ£ trá»ng sá»‘), Ä‘Ã¢y lÃ  má»™t sá»‘ thay Ä‘á»•i so vá»›i nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ tháº¥y trong cÃ¡c pháº§n trÆ°á»›c:

- ChÃºng ta khÃ´ng Ä‘áº·t báº¥t ká»³ Ä‘Ã¡nh giÃ¡ thÆ°á»ng xuyÃªn nÃ o, vÃ¬ quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ sáº½ máº¥t má»™t khoáº£ng thá»i gian; chÃºng ta sáº½ chá»‰ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a mÃ¬nh má»™t láº§n trÆ°á»›c khi huáº¥n luyá»‡n vÃ  sau Ä‘Ã³.
- ChÃºng ta Ä‘áº·t `fp16=True`, giÃºp tÄƒng tá»‘c quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÃªn cÃ¡c GPU hiá»‡n Ä‘áº¡i.
- ChÃºng ta Ä‘áº·t `predict_with_generate=True`, nhÆ° Ä‘Ã£ tháº£o luáº­n á»Ÿ trÃªn.
- ChÃºng ta sá»­ dá»¥ng `push_to_hub=True` Ä‘á»ƒ táº£i mÃ´ hÃ¬nh lÃªn Hub vÃ o cuá»‘i má»—i epoch.

LÆ°u Ã½ ráº±ng báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn Ä‘áº§y Ä‘á»§ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n báº±ng tham sá»‘ `hub_model_id` (Ä‘áº·c biá»‡t, báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y Ä‘áº¿n má»™t tá»• chá»©c). VÃ­ dá»¥: khi chÃºng ta Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course), chÃºng ta Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/marian-finetuned-kde4-en-to-fr"` thÃ nh `Seq2SeqTrainingArguments`. Theo máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ¬ váº­y trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, nÃ³ sáº½ lÃ  `"sgugger/marian-finetuned-kde4-en-to-fr"` (lÃ  mÃ´ hÃ¬nh chÃºng tÃ´i liÃªn káº¿t Ä‘áº¿n á»Ÿ Ä‘áº§u pháº§n nÃ y).

<Tip>

ğŸ’¡ Náº¿u thÆ° má»¥c Ä‘áº§u ra báº¡n Ä‘ang sá»­ dá»¥ng Ä‘Ã£ tá»“n táº¡i, nÃ³ cáº§n pháº£i lÃ  báº£n sao cá»¥c bá»™ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n. Náº¿u khÃ´ng, báº¡n sáº½ gáº·p lá»—i khi xÃ¡c Ä‘á»‹nh `Seq2SeqTrainer` cá»§a mÃ¬nh vÃ  sáº½ cáº§n Ä‘áº·t tÃªn má»›i.

</Tip>

Cuá»‘i cÃ¹ng, ta chá»‰ cáº§n truyá»n má»i thá»© cho `Seq2SeqTrainer`:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

TrÆ°á»›c khi huáº¥n luyá»‡n, trÆ°á»›c tiÃªn chÃºng ta sáº½ xem xÃ©t Ä‘iá»ƒm mÃ  mÃ´ hÃ¬nh cá»§a chÃºng ta nháº­n Ä‘Æ°á»£c, Ä‘á»ƒ kiá»ƒm tra ká»¹ xem chÃºng ta cÃ³ Ä‘ang khÃ´ng lÃ m má»i thá»© tá»“i tá»‡ hÆ¡n vá»›i viá»‡c tinh chá»‰nh cá»§a chÃºng ta hay khÃ´ng. Lá»‡nh nÃ y sáº½ máº¥t má»™t chÃºt thá»i gian, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ uá»‘ng má»™t ly cÃ  phÃª trong khi nÃ³ thá»±c thi:

```python
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

Äiá»ƒm BLEU lÃ  39 khÃ´ng quÃ¡ tá»‡, Ä‘iá»u nÃ y pháº£n Ã¡nh thá»±c táº¿ lÃ  mÃ´ hÃ¬nh cá»§a chÃºng ta Ä‘Ã£ ráº¥t giá»i trong viá»‡c dá»‹ch cÃ¡c cÃ¢u tiáº¿ng Anh sang tiáº¿ng PhÃ¡p.

Tiáº¿p theo lÃ  huáº¥n luyá»‡n, cÅ©ng sáº½ máº¥t má»™t chÃºt thá»i gian:

```python
trainer.train()
```

LÆ°u Ã½ ráº±ng trong khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra, má»—i khi mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u (á»Ÿ Ä‘Ã¢y, má»—i epoch), nÃ³ sáº½ Ä‘Æ°á»£c táº£i lÃªn Hub á»Ÿ cháº¿ Ä‘á»™ ná»n. Báº±ng cÃ¡ch nÃ y, báº¡n sáº½ cÃ³ thá»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n cá»§a mÃ¬nh trÃªn má»™t mÃ¡y khÃ¡c náº¿u cáº§n.

Sau khi huáº¥n luyá»‡n xong, chÃºng ta Ä‘Ã¡nh giÃ¡ láº¡i mÃ´ hÃ¬nh cá»§a mÃ¬nh - hy vá»ng chÃºng ta sáº½ tháº¥y má»™t sá»‘ cáº£i thiá»‡n trong Ä‘iá»ƒm BLEU!

```py
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

ÄÃ³ lÃ  má»™t cáº£i tiáº¿n hÆ¡n gáº§n 14 Ä‘iá»ƒm, tháº­t tuyá»‡t vá»i.

Cuá»‘i cÃ¹ng, chÃºng ta sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `push_to_hub()` Ä‘á»ƒ Ä‘áº£m báº£o táº£i lÃªn phiÃªn báº£n má»›i nháº¥t cá»§a mÃ´ hÃ¬nh. `Trainer` cÅ©ng soáº¡n tháº£o má»™t tháº» mÃ´ hÃ¬nh vá»›i táº¥t cáº£ cÃ¡c káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ  táº£i nÃ³ lÃªn. Tháº» mÃ´ hÃ¬nh nÃ y chá»©a siÃªu dá»¯ liá»‡u giÃºp Model Hub chá»n tiá»‡n Ã­ch con cho báº£n trÃ¬nh diá»…n luáº­n suy. ThÃ´ng thÆ°á»ng, khÃ´ng cáº§n pháº£i nÃ³i báº¥t cá»© Ä‘iá»u gÃ¬ vÃ¬ nÃ³ cÃ³ thá»ƒ suy ra tiá»‡n Ã­ch con phÃ¹ há»£p tá»« lá»›p mÃ´ hÃ¬nh, nhÆ°ng trong trÆ°á»ng há»£p nÃ y, cÃ¹ng má»™t lá»›p mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cho táº¥t cáº£ cÃ¡c loáº¡i váº¥n Ä‘á» chuá»—i sang chuá»—i, vÃ¬ váº­y chÃºng ta chá»‰ Ä‘á»‹nh Ä‘Ã³ lÃ  má»™t mÃ´ hÃ¬nh dá»‹ch:

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```
Lá»‡nh nÃ y tráº£ vá» URL cá»§a cam káº¿t mÃ  nÃ³ vá»«a thá»±c hiá»‡n, náº¿u báº¡n muá»‘n kiá»ƒm tra nÃ³:

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

á» giai Ä‘oáº¡n nÃ y, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tiá»‡n Ã­ch luáº­n suy trÃªn Model Hub Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh cá»§a mÃ¬nh vÃ  chia sáº» vá»›i báº¡n bÃ¨. Báº¡n Ä‘Ã£ tinh chá»‰nh thÃ nh cÃ´ng má»™t mÃ´ hÃ¬nh trong tÃ¡c vá»¥ dá»‹ch - xin chÃºc má»«ng!

Náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n má»™t chÃºt vá» vÃ²ng láº·p huáº¥n luyá»‡n, bÃ¢y giá» chÃºng tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n cÃ¡ch thá»±c hiá»‡n Ä‘iá»u tÆ°Æ¡ng tá»± báº±ng cÃ¡ch sá»­ dá»¥ng ğŸ¤— Accelerate.

{/if}

{#if fw === 'pt'}

## Má»™t vÃ²ng huáº¥n luyá»‡n tÃ¹y chá»‰nh

BÃ¢y giá» chÃºng ta hÃ£y xem toÃ n bá»™ vÃ²ng láº·p huáº¥n luyá»‡n, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ dá»… dÃ ng tÃ¹y chá»‰nh cÃ¡c pháº§n báº¡n cáº§n. NÃ³ sáº½ trÃ´ng ráº¥t giá»‘ng nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ lÃ m trong [pháº§n 2](/course/chapter7/2) vÃ  [ChÆ°Æ¡ng 3](/course/chapter3/4).

### Chuáº©n bá»‹ má»i thá»© cho qua trÃ¬nh huáº¥n luyá»‡n

Báº¡n Ä‘Ã£ tháº¥y táº¥t cáº£ Ä‘iá»u nÃ y má»™t vÃ i láº§n rá»“i, vÃ¬ váº­y chÃºng ta sáº½ xem qua Ä‘oáº¡n mÃ£ khÃ¡ nhanh. Äáº§u tiÃªn, chÃºng ta sáº½ xÃ¢y dá»±ng cÃ¡c `DataLoader` tá»« cÃ¡c táº­p dá»¯ liá»‡u cá»§a mÃ¬nh, sau khi Ä‘áº·t cÃ¡c táº­p dá»¯ liá»‡u thÃ nh Ä‘á»‹nh dáº¡ng` "torch" `Ä‘á»ƒ chÃºng ta nháº­n Ä‘Æ°á»£c cÃ¡c tensor PyTorch:

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

Tiáº¿p theo, chÃºng ta khÃ´i phá»¥c mÃ´ hÃ¬nh cá»§a mÃ¬nh, Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng chÃºng ta khÃ´ng tiáº¿p tá»¥c tinh chá»‰nh tá»« trÆ°á»›c mÃ  báº¯t Ä‘áº§u láº¡i tá»« mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c:

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

Then we will need an optimizer:

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Khi chÃºng ta cÃ³ táº¥t cáº£ cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ gá»­i chÃºng Ä‘áº¿n phÆ°Æ¡ng thá»©c `accelerator.prepare()`. HÃ£y nhá»› ráº±ng náº¿u báº¡n muá»‘n huáº¥n luyá»‡n vá» TPU trong notebook Colab, báº¡n sáº½ cáº§n chuyá»ƒn táº¥t cáº£ mÃ£ nÃ y vÃ o má»™t hÃ m huáº¥n luyá»‡n vÃ  Ä‘iá»u Ä‘Ã³ sáº½ khÃ´ng thá»±c thi báº¥t ká»³ Ã´ nÃ o khá»Ÿi táº¡o má»™t `Accelerator`.

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

BÃ¢y giá», chÃºng ta Ä‘Ã£ gá»­i `train_dataloader` cá»§a mÃ¬nh tá»›i `accelerator.prepare()`, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»™ dÃ i cá»§a nÃ³ Ä‘á»ƒ tÃ­nh sá»‘ bÆ°á»›c huáº¥n luyá»‡n. HÃ£y nhá»› ráº±ng chÃºng ta pháº£i luÃ´n lÃ m Ä‘iá»u nÃ y sau khi chuáº©n bá»‹ dataloader, vÃ¬ phÆ°Æ¡ng thá»©c Ä‘Ã³ sáº½ thay Ä‘á»•i Ä‘á»™ dÃ i cá»§a `DataLoader`. ChÃºng ta sá»­ dá»¥ng má»™t lá»‹ch trÃ¬nh tuyáº¿n tÃ­nh cá»• Ä‘iá»ƒn tá»« tá»‘c Ä‘á»™ há»c Ä‘áº¿n 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Cuá»‘i cÃ¹ng, Ä‘á»ƒ Ä‘áº©y mÃ´ hÃ¬nh cá»§a mÃ¬nh lÃªn Hub, chÃºng ta sáº½ cáº§n táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `Repository` trong má»™t thÆ° má»¥c Ä‘ang lÃ m viá»‡c. Äáº§u tiÃªn hÃ£y Ä‘Äƒng nháº­p vÃ o Hugging Face Hub, náº¿u báº¡n chÆ°a Ä‘Äƒng nháº­p. ChÃºng ta sáº½ xÃ¡c Ä‘á»‹nh tÃªn kho lÆ°u trá»¯ tá»« ID mÃ´ hÃ¬nh mÃ  chÃºng ta muá»‘n cung cáº¥p cho mÃ´ hÃ¬nh cá»§a mÃ¬nh (vui lÃ²ng thay tháº¿ `repo_name` báº±ng sá»± lá»±a chá»n cá»§a riÃªng báº¡n; nÃ³ chá»‰ cáº§n chá»©a tÃªn ngÆ°á»i dÃ¹ng cá»§a báº¡n, Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ hÃ m `get_full_repo_name()` thá»±c hiá»‡n):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ sao chÃ©p kho lÆ°u trá»¯ Ä‘Ã³ trong má»™t thÆ° má»¥c cá»¥c bá»™. Náº¿u nÃ³ Ä‘Ã£ tá»“n táº¡i, thÆ° má»¥c cá»¥c bá»™ nÃ y pháº£i lÃ  báº£n sao cá»§a kho lÆ°u trá»¯ mÃ  chÃºng ta Ä‘ang lÃ m viá»‡c:

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ táº£i lÃªn báº¥t cá»© thá»© gÃ¬ chÃºng ta lÆ°u trong `output_dir` báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `repo.push_to_hub()`. Äiá»u nÃ y sáº½ giÃºp chÃºng ta táº£i lÃªn cÃ¡c mÃ´ hÃ¬nh trung gian á»Ÿ cuá»‘i má»—i epoch.

### VÃ²ng láº·p huáº¥n luyá»‡n

BÃ¢y giá» chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ viáº¿t vÃ²ng láº·p huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ¡n giáº£n hÃ³a pháº§n Ä‘Ã¡nh giÃ¡ cá»§a nÃ³, chÃºng ta Ä‘á»‹nh nghÄ©a hÃ m `postprocess()` nÃ y láº¥y cÃ¡c dá»± Ä‘oÃ¡n vÃ  nhÃ£n vÃ  chuyá»ƒn Ä‘á»•i chÃºng thÃ nh danh sÃ¡ch cÃ¡c chuá»—i mÃ  Ä‘á»‘i tÆ°á»£ng `metric` kÃ¬ vá»ng:

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Thay -100 trong nhÃ£n vÃ¬ ta khÃ´ng tháº¿ giáº£i mÃ£ chÃºng.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Thá»±c hiá»‡n má»™t sá»‘ háº­u xá»­ lÃ½ Ä‘Æ¡n giáº£n
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

VÃ²ng láº·p huáº¥n luyá»‡n trÃ´ng ráº¥t giá»‘ng vá»›i cÃ¡c vÃ²ng láº·p trong [pháº§n 2](/course/chapter7/2) vÃ  [ChÆ°Æ¡ng 3](/course/chapter3), vá»›i má»™t vÃ i Ä‘iá»ƒm khÃ¡c biá»‡t trong pháº§n Ä‘Ã¡nh giÃ¡ - vÃ¬ váº­y hÃ£y táº­p trung vÃ o Ä‘iá»u Ä‘Ã³!

Äiá»u Ä‘áº§u tiÃªn cáº§n lÆ°u Ã½ lÃ  chÃºng ta sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `generate()` Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c dá»± Ä‘oÃ¡n, nhÆ°ng Ä‘Ã¢y lÃ  má»™t phÆ°Æ¡ng thá»©c trÃªn mÃ´ hÃ¬nh cÆ¡ sá»Ÿ, khÃ´ng pháº£i mÃ´ hÃ¬nh Ä‘Æ°á»£c bao bá»c ğŸ¤— Accelerate Ä‘Æ°á»£c táº¡o trong phÆ°Æ¡ng thá»©c `prepare()` . ÄÃ³ lÃ  lÃ½ do táº¡i sao chÃºng ta má»Ÿ mÃ´ hÃ¬nh trÆ°á»›c, sau Ä‘Ã³ gá»i phÆ°Æ¡ng thá»©c nÃ y.

Äiá»u thá»© hai lÃ , giá»‘ng nhÆ° vá»›i [phÃ¢n loáº¡i token](/course/chapter7/2), hai quy trÃ¬nh cÃ³ thá»ƒ Ä‘Ã£ Ä‘Ãªm cÃ¡c Ä‘áº§u vÃ o vÃ  nhÃ£n thÃ nh cÃ¡c hÃ¬nh dáº¡ng khÃ¡c nhau, vÃ¬ váº­y chÃºng tÃ´i sá»­ dá»¥ng `accelerator.pad_across_processes()` Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n vÃ  nhÃ£n cÃ¹ng má»™t hÃ¬nh dáº¡ng trÆ°á»›c khi gá»i phÆ°Æ¡ng thá»©c `gather()` . Náº¿u chÃºng ta khÃ´ng lÃ m Ä‘iá»u nÃ y, Ä‘Ã¡nh giÃ¡ sáº½ bá»‹ lá»—i hoáº·c bá»‹ treo vÄ©nh viá»…n.

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Huáº¥n luyá»‡n
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ÄÃ¡nh giÃ¡
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Cáº§n Ä‘á»‡m dá»± Ä‘oÃ¡n vÃ  nhÃ£n Ä‘á»ƒ dá»… gom láº¡i
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # LÆ°u vÃ  táº£i
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

Khi Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n, báº¡n sáº½ cÃ³ má»™t mÃ´ hÃ¬nh cÃ³ káº¿t quáº£ khÃ¡ giá»‘ng vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i `Seq2SeqTrainer`. Báº¡n cÃ³ thá»ƒ kiá»ƒm tra Ä‘oáº¡n mÃ£ mÃ  chÃºng ta Ä‘Ã£ huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng mÃ£ nÃ y táº¡i [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). VÃ  náº¿u báº¡n muá»‘n kiá»ƒm tra báº¥t ká»³ tinh chá»‰nh nÃ o Ä‘á»‘i vá»›i vÃ²ng láº·p huáº¥n luyá»‡n, báº¡n cÃ³ thá»ƒ trá»±c tiáº¿p thá»±c hiá»‡n chÃºng báº±ng cÃ¡ch chá»‰nh sá»­a Ä‘oáº¡n mÃ£ Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ trÃªn!

{/if}

## Sá»­ dá»¥ng mÃ´ hÃ¬nh tinh chá»‰nh

ChÃºng tÃ´i Ä‘Ã£ chá»‰ cho báº¡n cÃ¡ch báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh mÃ  ta Ä‘Ã£ tinh chá»‰nh trÃªn Model Hub báº±ng tiá»‡n Ã­ch luáº­n suy. Äá»ƒ sá»­ dá»¥ng nÃ³ cá»¥c bá»™ trong má»™t `pipeline`, chÃºng ta chá»‰ cáº§n chá»‰ Ä‘á»‹nh mÃ£ Ä‘á»‹nh danh mÃ´ hÃ¬nh thÃ­ch há»£p:

```py
from transformers import pipeline

# Thay nÃ³ vá»›i checkpoint cá»§a báº¡n
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

ÄÃºng nhÆ° mong Ä‘á»£i, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a chÃºng ta Ä‘Ã£ Ä‘iá»u chá»‰nh kiáº¿n thá»©c cá»§a nÃ³ cho phÃ¹ há»£p vá»›i kho ngá»¯ liá»‡u mÃ  chÃºng ta Ä‘Ã£ tinh chá»‰nh vÃ  thay vÃ¬ Ä‘á»ƒ nguyÃªn tá»« "thread" trong tiáº¿ng Anh, giá» Ä‘Ã¢y nÃ³ Ä‘Ã£ dá»‹ch nÃ³ sang phiÃªn báº£n chÃ­nh thá»©c tiáº¿ng PhÃ¡p. Äá»‘i vá»›i "plugin" cÅ©ng váº­y:

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

Má»™t vÃ­ dá»¥ tuyá»‡t vá»i khÃ¡c vá» thÃ­ch á»©ng chuyá»‡n mÃ´n!

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** MÃ´ hÃ¬nh tráº£ vá» cÃ¡i gÃ¬ vá»›i tá»« "email" báº¡n xÃ¡c Ä‘á»‹nh trÆ°á»›c Ä‘Ã³?

</Tip>
