<FrameworkSwitchCourse {fw} />

# PhÃ¢n loáº¡i token

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section2_pt.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section2_pt.ipynb",
    },
  ]}
/>

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section2_tf.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section2_tf.ipynb",
    },
  ]}
/>

{/if}

á»¨ng dá»¥ng Ä‘áº§u tiÃªn chÃºng ta sáº½ cÃ¹ng khÃ¡m phÃ¡ lÃ  phÃ¢n loáº¡i token. TÃ¡c vá»¥ chung nÃ y bao gá»“m báº¥t ká»³ váº¥n Ä‘á» nÃ o cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng dÆ°á»›i dáº¡ng "gÃ¡n nhÃ£n cho má»—i token trong má»™t cÃ¢u", cháº³ng háº¡n nhÆ°:

- **Nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (NER)**: TÃ¬m cÃ¡c thá»±c thá»ƒ (cháº³ng háº¡n nhÆ° ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm hoáº·c tá»• chá»©c) trong má»™t cÃ¢u. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng nhÆ° lÃ  gÃ¡n nhÃ£n cho má»—i token báº±ng cÃ¡ch cÃ³ má»™t nhÃ£n cho má»—i thá»±c thá»ƒ vÃ  má»™t nhÃ£n cho "khÃ´ng cÃ³ thá»±c thá»ƒ".
- **GÃ¡n nhÃ£n tá»« loáº¡i (POS)**: ÄÃ¡nh dáº¥u má»—i tá»« trong cÃ¢u tÆ°Æ¡ng á»©ng vá»›i má»™t tá»« loáº¡i cá»¥ thá»ƒ cá»§a vÄƒn báº£n (cháº³ng háº¡n nhÆ° danh tá»«, Ä‘á»™ng tá»«, tÃ­nh tá»«, v.v.).
- **PhÃ¢n khÃºc**: TÃ¬m cÃ¡c token thuá»™c cÃ¹ng má»™t thá»±c thá»ƒ. TÃ¡c vá»¥ nÃ y (cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p vá»›i POS hoáº·c NER) cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng dÆ°á»›i dáº¡ng gÃ¡n má»™t nhÃ£n (thÆ°á»ng lÃ  `B-`) cho báº¥t ká»³ token nÃ o á»Ÿ Ä‘áº§u má»™t Ä‘oáº¡n, má»™t nhÃ£n khÃ¡c (thÆ°á»ng lÃ  `I-`) cho cÃ¡c token Ä‘Ã³ náº±m bÃªn trong má»™t Ä‘oáº¡n vÃ  má»™t nhÃ£n thá»© ba (thÆ°á»ng lÃ  `O`) token khÃ´ng thuá»™c báº¥t ká»³ Ä‘oáº¡n nÃ o.

<Youtube id="wVHdVlPScxA" />

Táº¥t nhiÃªn, cÃ³ nhiá»u loáº¡i váº¥n Ä‘á» phÃ¢n loáº¡i token khÃ¡c; Ä‘Ã³ chá»‰ lÃ  má»™t vÃ i vÃ­ dá»¥ tiÃªu biá»ƒu. Trong pháº§n nÃ y, chÃºng ta sáº½ tinh chá»‰nh má»™t mÃ´ hÃ¬nh (BERT) trÃªn má»™t tÃ¡c vá»¥ NER, sau Ä‘Ã³ sáº½ cÃ³ thá»ƒ tÃ­nh toÃ¡n cÃ¡c dá»± Ä‘oÃ¡n nhÆ° sau:

<iframe
  src="https://course-demos-bert-finetuned-ner.hf.space"
  frameBorder="0"
  height="350"
  title="Gradio app"
  class="block dark:hidden container p-0 flex-grow space-iframe"
  allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"
  sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"
></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
  <img
    class="block dark:hidden lg:w-3/5"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png"
    alt="One-hot encoded labels for question answering."
  />
  <img
    class="hidden dark:block lg:w-3/5"
    src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png"
    alt="One-hot encoded labels for question answering."
  />
</a>

Báº¡n cÃ³ thá»ƒ tÃ¬m mÃ´ hÃ¬nh ta sáº½ huáº¥n luyá»‡n vÃ  táº£i lÃªn Hub vÃ  kiá»ƒm tra láº¡i cÃ¡c dá»± Ä‘oÃ¡n [táº¡i Ä‘Ã¢y](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).

## Chuáº©n bá»‹ dá»¯ liá»‡u

Äáº§u tiÃªn, ta cáº§n bá»™ dá»¯ liá»‡u chuáº©n bá»‹ cho phÃ¢n loáº¡i token. Trong chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ sá»­ dá»¥ng bá»™ dá»¯ liá»‡u [CoNLL-2003](https://huggingface.co/datasets/conll2003), bao gá»“m cÃ¡c cÃ¢u chuyá»‡n tin tá»©c tá»« Reuters.

<Tip>

ğŸ’¡ Miá»…n lÃ  táº­p dá»¯ liá»‡u cá»§a báº¡n bao gá»“m cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c chia thÃ nh cÃ¡c tá»« vá»›i nhÃ£n tÆ°Æ¡ng á»©ng cá»§a chÃºng, báº¡n sáº½ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ Ä‘Ã¢y vá»›i táº­p dá»¯ liá»‡u cá»§a riÃªng báº¡n. Tham kháº£o láº¡i [Chapter 5](/course/chapter5) náº¿u báº¡n cáº§n cáº­p nháº­t vá» cÃ¡ch táº£i dá»¯ liá»‡u tÃ¹y chá»‰nh cá»§a riÃªng báº¡n trong `Dataset`.

</Tip>

### Táº­p dá»¯ liá»‡u CoNLL-2003

Äá»ƒ táº£i bá»™ dá»¯ liá»‡u CoNLL-2003, ta cáº§n sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `load_dataset()` tá»« thÆ° viá»‡n ğŸ¤— Datasets:

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

Ta sáº½ táº£i vÃ  lÆ°u bá»™ dá»¯ liá»‡u vÃ o cache, nhÆ° ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 3](/course/chapter3) cho bá»™ dá»¯ liá»‡u GLUE MRPC. Viá»‡c kiá»ƒm tra Ä‘á»‘i tÆ°á»£ng nÃ y cho chÃºng ta tháº¥y cÃ¡c cá»™t hiá»‡n cÃ³ vÃ  sá»± phÃ¢n chia giá»¯a cÃ¡c táº­p huáº¥n luyá»‡n, kiá»ƒm Ä‘á»‹nh vÃ  kiá»ƒm thá»­:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

Äáº·c biá»‡t, chÃºng ta cÃ³ thá»ƒ tháº¥y táº­p dá»¯ liá»‡u chá»©a cÃ¡c nhÃ£n cho ba tÃ¡c vá»¥ mÃ  chÃºng ta Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³: NER, POS vÃ  chunking. Má»™t sá»± khÃ¡c biá»‡t lá»›n so vá»›i cÃ¡c bá»™ dá»¯ liá»‡u khÃ¡c lÃ  cÃ¡c vÄƒn báº£n Ä‘áº§u vÃ o khÃ´ng Ä‘Æ°á»£c trÃ¬nh bÃ y dÆ°á»›i dáº¡ng cÃ¢u hoáº·c tÃ i liá»‡u, mÃ  lÃ  danh sÃ¡ch cÃ¡c tá»« (cá»™t cuá»‘i cÃ¹ng Ä‘Æ°á»£c gá»i lÃ  `tokens`, nhÆ°ng nÃ³ chá»©a cÃ¡c tá»« theo nghÄ©a Ä‘Ã¢y lÃ  cÃ¡c Ä‘áº§u vÃ o Ä‘Æ°á»£c tokenize trÆ°á»›c váº«n cáº§n Ä‘á»ƒ Ä‘i qua trÃ¬nh tokenize Ä‘á»ƒ tokenize tá»« phá»¥).

HÃ£y xem pháº§n tá»­ Ä‘áº§u tiÃªn cá»§a táº­p huáº¥n luyá»‡n:

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

VÃ¬ ta muá»‘n thá»±c hiá»‡n nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn, chÃºng ta sáº½ nhÃ¬n vÃ o cÃ¡c tháº» NER:

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

ÄÃ³ lÃ  nhá»¯ng nhÃ£n dÆ°á»›i dáº¡ng sá»‘ nguyÃªn sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n, nhÆ°ng chÃºng khÃ´ng nháº¥t thiáº¿t há»¯u Ã­ch khi chÃºng ta muá»‘n kiá»ƒm tra dá»¯ liá»‡u. Giá»‘ng nhÆ° phÃ¢n loáº¡i vÄƒn báº£n, chÃºng ta cÃ³ thá»ƒ truy cáº­p sá»± tÆ°Æ¡ng á»©ng giá»¯a cÃ¡c sá»‘ nguyÃªn Ä‘Ã³ vÃ  tÃªn nhÃ£n báº±ng cÃ¡ch xem thuá»™c tÃ­nh `features` cá»§a táº­p dá»¯ liá»‡u:

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

VÃ¬ váº­y, cá»™t nÃ y chá»©a cÃ¡c pháº§n tá»­ lÃ  chuá»—i cá»§a `ClassLabel`. Loáº¡i pháº§n tá»­ cá»§a chuá»—i náº±m trong thuá»™c tÃ­nh `feature` cá»§a `ner_feature` nÃ y, vÃ  chÃºng ta cÃ³ thá»ƒ truy cáº­p danh sÃ¡ch tÃªn báº±ng cÃ¡ch xem thuá»™c tÃ­nh `names` cá»§a `feature` Ä‘Ã³:

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

ChÃºng ta Ä‘Ã£ tháº¥y cÃ¡c nhÃ£n khi Ä‘Ã o sÃ¢u vÃ o pipeline `token-classification` trong [ChÆ°Æ¡ng 6](/course/chapter6/3), nhÆ°ng Ä‘á»ƒ cáº­p nháº­t nhanh:

- `O` nghÄ©a lÃ  tá»« khÃ´ng thuá»™c báº¥t kÃ¬ thá»±c thá»ƒ nÃ o.
- `B-PER`/`I-PER` nghÄ©a lÃ  tá»« tÆ°Æ¡ng á»©ng pháº§n báº¯t Ä‘áº§u/ náº±m bÃªn trong cá»§a thá»±c thá»ƒ _person_ hay _con ngÆ°á»i_.
- `B-ORG`/`I-ORG` nghÄ©a lÃ  tá»« tÆ°Æ¡ng á»©ng pháº§n báº¯t Ä‘áº§u/ náº±m bÃªn trong cá»§a thá»±c thá»ƒ _organization_ hay _tá»• chá»©c_.
- `B-LOC`/`I-LOC` nghÄ©a lÃ  tá»« tÆ°Æ¡ng á»©ng pháº§n báº¯t Ä‘áº§u/ náº±m bÃªn trong cá»§a thá»±c thá»ƒ _location_ hay _Ä‘á»‹a Ä‘iá»ƒm_.
- `B-MISC`/`I-MISC` nghÄ©a lÃ  tá»« tÆ°Æ¡ng á»©ng pháº§n báº¯t Ä‘áº§u/ náº±m bÃªn trong cá»§a thá»±c thá»ƒ _miscellaneous_ hay _lá»™n xá»™n_.

Giá» khi giáº£i mÃ£ cÃ¡c nhÃ£n, ta tháº¥y chÃºng cho ta:

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

VÃ  Ä‘á»‘i vá»›i má»™t vÃ­ dá»¥ trá»™n nhÃ£n `B-` vÃ  `I-`, Ä‘Ã¢y lÃ  nhá»¯ng gÃ¬ mÃ  cÃ¹ng má»™t Ä‘oáº¡n mÃ£ cung cáº¥p cho chÃºng ta vá» pháº§n tá»­ cá»§a táº­p huáº¥n luyá»‡n á»Ÿ chá»‰ má»¥c 4:

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, cÃ¡c thá»±c thá»ƒ bao gá»“m hai tá»«, nhÆ° "European Union" vÃ  "Werner Zwingmann", Ä‘Æ°á»£c gÃ¡n nhÃ£n `B-` cho tá»« Ä‘áº§u tiÃªn vÃ  nhÃ£n `I-` cho tá»« thá»© hai.

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** In hai cÃ¢u giá»‘ng nhau báº±ng nhÃ£n POS hoáº·c phÃ¢n khÃºc cá»§a chÃºng.

</Tip>

### Xá»­ lÃ½ dá»¯ liá»‡u

<Youtube id="iY2AZYdZAr0" />

NhÆ° thÆ°á»ng lá»‡, cÃ¡c vÄƒn báº£n cá»§a chÃºng ta cáº§n Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i sang token ID trÆ°á»›c khi mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c chÃºng. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 6](/course/chapter6/), má»™t sá»± khÃ¡c biá»‡t lá»›n trong trÆ°á»ng há»£p tÃ¡c vá»¥ phÃ¢n loáº¡i token lÃ  chÃºng ta cÃ³ cÃ¡c Ä‘áº§u vÃ o Ä‘Æ°á»£c tokenize trÆ°á»›c. May máº¯n thay, API tokenizer cÃ³ thá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã³ khÃ¡ dá»… dÃ ng; chÃºng ta chá»‰ cáº§n bÃ¡o `tokenizer` báº±ng má»™t lÃ¡ cá» Ä‘áº·c biá»‡t.

Äá»ƒ báº¯t Ä‘áº§u, hÃ£y táº¡o Ä‘á»‘i tÆ°á»£ng `tokenizer` cá»§a chÃºng ta. NhÆ° chÃºng tÃ´i Ä‘Ã£ nÃ³i trÆ°á»›c Ä‘Ã¢y, chÃºng ta sáº½ sá»­ dá»¥ng mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c BERT, vÃ¬ váº­y chÃºng ta sáº½ báº¯t Ä‘áº§u báº±ng cÃ¡ch táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› Ä‘á»‡m cá»§a tokenizer liÃªn quan:

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Báº¡n cÃ³ thá»ƒ thay tháº¿ `model_checkpoint` báº±ng báº¥t ká»³ mÃ´ hÃ¬nh nÃ o khÃ¡c mÃ  báº¡n thÃ­ch tá»« [Hub](https://huggingface.co/models) hoáº·c báº±ng má»™t thÆ° má»¥c cá»¥c bá»™ trong Ä‘Ã³ báº¡n Ä‘Ã£ lÆ°u má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  má»™t trÃ¬nh tokenize. Háº¡n cháº¿ duy nháº¥t lÃ  tokenizer cáº§n Ä‘Æ°á»£c há»— trá»£ bá»Ÿi thÆ° viá»‡n ğŸ¤— Tokenizers, vÃ¬ váº­y sáº½ cÃ³ phiÃªn báº£n "nhanh". Báº¡n cÃ³ thá»ƒ xem táº¥t cáº£ cÃ¡c kiáº¿n trÃºc Ä‘i kÃ¨m vá»›i phiÃªn báº£n nhanh trong [báº£ng lá»›n nÃ y](https://huggingface.co/transformers/#supported-frameworks) vÃ  Ä‘á»ƒ kiá»ƒm tra xem Ä‘á»‘i tÆ°á»£ng `tokenizer` mÃ  báº¡n Ä‘ang sá»­ dá»¥ng cÃ³ thá»±c sá»± lÃ  Ä‘Æ°á»£c há»— trá»£ bá»Ÿi ğŸ¤— Tokenizers, báº¡n cÃ³ thá»ƒ xem thuá»™c tÃ­nh `is_fast` cá»§a nÃ³:

```py
tokenizer.is_fast
```

```python out
True
```

Äá»ƒ tokenize dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Ã£ tiá»n tokenize, ta cÃ³ thá»ƒ sá»­ dá»¥ng `tokenizer` nhÆ° thÆ°á»ng lá»‡ vÃ  chá»‰ thÃªm `is_split_into_words=True`:

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, trÃ¬nh tokenizer Ä‘Ã£ thÃªm cÃ¡c token Ä‘áº·c biá»‡t Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi mÃ´ hÃ¬nh (`[CLS]` á»Ÿ Ä‘áº§u vÃ  `[SEP]` á»Ÿ cuá»‘i) vÃ  Ä‘á»ƒ nguyÃªn háº§u háº¿t cÃ¡c tá»«. Tuy nhiÃªn, tá»« `lamb` Ä‘Ã£ Ä‘Æ°á»£c tokenize thÃ nh hai tá»« phá»¥, `la` vÃ  `##mb`. Äiá»u nÃ y dáº«n Ä‘áº¿n sá»± khÃ´ng khá»›p giá»¯a Ä‘áº§u vÃ o vÃ  cÃ¡c nhÃ£n: danh sÃ¡ch nhÃ£n chá»‰ cÃ³ 9 pháº§n tá»­, trong khi Ä‘áº§u vÃ o cá»§a chÃºng ta hiá»‡n cÃ³ 12 token. Viá»‡c tÃ­nh toÃ¡n cÃ¡c token Ä‘áº·c biá»‡t ráº¥t dá»… dÃ ng (chÃºng ta biáº¿t chÃºng náº±m á»Ÿ Ä‘áº§u vÃ  cuá»‘i), nhÆ°ng chÃºng ta cÅ©ng cáº§n Ä‘áº£m báº£o ráº±ng chÃºng ta sáº¯p xáº¿p táº¥t cáº£ cÃ¡c nhÃ£n vá»›i cÃ¡c tá»« thÃ­ch há»£p.

May máº¯n thay, bá»Ÿi vÃ¬ chÃºng ta Ä‘ang sá»­ dá»¥ng má»™t tokenizer nhanh, chÃºng ta cÃ³ quyá»n truy cáº­p vÃ o sá»©c máº¡nh siÃªu cÆ°á»ng ğŸ¤— Tokenizers, cÃ³ nghÄ©a lÃ  chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng Ã¡nh xáº¡ tá»«ng token vá»›i tá»« tÆ°Æ¡ng á»©ng cá»§a nÃ³ (nhÆ° Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 6](/course/chapter6/3)):

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

Vá»›i má»™t chÃºt cÃ´ng viá»‡c, sau Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ má»Ÿ rá»™ng danh sÃ¡ch nhÃ£n cá»§a mÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c token. Quy táº¯c Ä‘áº§u tiÃªn chÃºng ta sáº½ Ã¡p dá»¥ng lÃ  cÃ¡c token Ä‘áº·c biá»‡t cÃ³ nhÃ£n lÃ  `-100`. Äiá»u nÃ y lÃ  do theo máº·c Ä‘á»‹nh `-100` lÃ  chá»‰ sá»‘ bá»‹ bá» qua trong hÃ m máº¥t mÃ¡t mÃ  chÃºng ta sáº½ sá»­ dá»¥ng (entropy chÃ©o). Sau Ä‘Ã³, má»—i token cÃ³ cÃ¹ng nhÃ£n vá»›i token báº¯t Ä‘áº§u tá»« bÃªn trong nÃ³, vÃ¬ chÃºng lÃ  má»™t pháº§n cá»§a cÃ¹ng má»™t thá»±c thá»ƒ. Äá»‘i vá»›i cÃ¡c token bÃªn trong má»™t tá»« nhÆ°ng khÃ´ng á»Ÿ Ä‘áº§u, chÃºng ta thay tháº¿ `B-` báº±ng `I-` (vÃ¬ token khÃ´ng báº¯t Ä‘áº§u thá»±c thá»ƒ):

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Báº¯t Ä‘áº§u má»™t tá»« má»›i!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Token Ä‘áº·c biá»‡t
            new_labels.append(-100)
        else:
            # Tá»« giá»‘ng vá»›i token trÆ°á»›c Ä‘Ã³
            label = labels[word_id]
            # Náº¿u nhÃ£n lÃ  B-XXX, ta Ä‘á»•i sang I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

HÃ£y cÃ¹ng thá»­ vá»›i cÃ¢u Ä‘áº§u tiÃªn:

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, hÃ m Ä‘Ã£ thÃªm `-100` cho hai token Ä‘áº·c biá»‡t á»Ÿ Ä‘áº§u vÃ  cuá»‘i, vÃ  dáº¥u `0` má»›i cho tá»« cá»§a chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c chia thÃ nh hai token.

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** Má»™t sá»‘ nhÃ  nghiÃªn cá»©u chá»‰ thÃ­ch gÃ¡n má»™t nhÃ£n cho má»—i tá»« vÃ  gÃ¡n `-100` cho cÃ¡c token con khÃ¡c trong má»™t tá»« nháº¥t Ä‘á»‹nh. Äiá»u nÃ y lÃ  Ä‘á»ƒ trÃ¡nh cÃ¡c tá»« dÃ i Ä‘Æ°á»£c chia thÃ nh nhiá»u token phá»¥ gÃ³p pháº§n lá»›n vÃ o hÃ m máº¥t mÃ¡t. Thay Ä‘á»•i chá»©c nÄƒng trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ cÄƒn chá»‰nh nhÃ£n vá»›i ID Ä‘áº§u vÃ o báº±ng cÃ¡ch tuÃ¢n theo quy táº¯c nÃ y.

</Tip>

Äá»ƒ xá»­ lÃ½ trÆ°á»›c toÃ n bá»™ táº­p dá»¯ liá»‡u cá»§a mÃ¬nh, chÃºng ta cáº§n tokenize táº¥t cáº£ cÃ¡c Ä‘áº§u vÃ o vÃ  Ã¡p dá»¥ng `align_labels_with_tokens()` trÃªn táº¥t cáº£ cÃ¡c nhÃ£n. Äá»ƒ táº­n dá»¥ng tá»‘c Ä‘á»™ cá»§a trÃ¬nh tokenize nhanh cá»§a mÃ¬nh, tá»‘t nháº¥t báº¡n nÃªn tokenize nhiá»u vÄƒn báº£n cÃ¹ng má»™t lÃºc, vÃ¬ váº­y chÃºng ta sáº½ viáº¿t má»™t hÃ m xá»­ lÃ½ danh sÃ¡ch cÃ¡c vÃ­ dá»¥ vÃ  sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `Dataset.map()` vá»›i tÃ¹y chá»n `batched=True`. Äiá»u duy nháº¥t khÃ¡c vá»›i vÃ­ dá»¥ trÆ°á»›c lÃ  hÃ m `word_ids()` cáº§n láº¥y chá»‰ má»¥c cá»§a máº«u mÃ  chÃºng ta muá»‘n cÃ¡c ID tá»« khi cÃ¡c Ä‘áº§u vÃ o cho tokenizer lÃ  danh sÃ¡ch vÄƒn báº£n (hoáº·c trong trÆ°á»ng há»£p cá»§a chÃºng ta lÃ  danh sÃ¡ch danh sÃ¡ch cÃ¡c tá»«), vÃ¬ váº­y chÃºng ta cÅ©ng thÃªm vÃ o Ä‘Ã³:

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

LÆ°u Ã½ ráº±ng chÃºng ta chÆ°a Ä‘á»‡m vÃ o cá»§a mÃ¬nh; chÃºng ta sáº½ lÃ m Ä‘iá»u Ä‘Ã³ sau, khi táº¡o cÃ¡c lÃ´ báº±ng trÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u.

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng táº¥t cáº£ tiá»n xá»­ lÃ½ Ä‘Ã³ trong má»™t láº§n vÃ o cÃ¡c pháº§n khÃ¡c cá»§a táº­p dá»¯ liá»‡u cá»§a mÃ¬nh:

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

ChÃºng ta Ä‘Ã£ hoÃ n thÃ nh pháº§n khÃ³ nháº¥t! BÃ¢y giá», dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½, quÃ¡ trÃ¬nh huáº¥n luyá»‡n thá»±c táº¿ sáº½ giá»‘ng nhÆ° nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ lÃ m trong [ChÆ°Æ¡ng 3](/course/chapter3).

{#if fw === 'pt'}

## Tinh chá»‰nh mÃ´ hÃ¬nh trong API `Trainer`

MÃ£ thá»±c sá»­ dá»¥ng `Trainer` sáº½ giá»‘ng nhÆ° trÆ°á»›c Ä‘Ã¢y; nhá»¯ng thay Ä‘á»•i duy nháº¥t lÃ  cÃ¡ch dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘á»‘i chiáº¿u thÃ nh má»™t lÃ´ vÃ  chá»©c nÄƒng tÃ­nh toÃ¡n sá»‘ liá»‡u.

{:else}

## Tinh chá»‰nh mÃ´ hÃ¬nh vá»›i Keras

MÃ£ thá»±c sá»­ dá»¥ng Keras sáº½ giá»‘ng nhÆ° trÆ°á»›c Ä‘Ã¢y; nhá»¯ng thay Ä‘á»•i duy nháº¥t lÃ  cÃ¡ch dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘á»‘i chiáº¿u thÃ nh má»™t lÃ´ vÃ  chá»©c nÄƒng tÃ­nh toÃ¡n sá»‘ liá»‡u.

{/if}

### Äá»‘i chiáº¿u dá»¯ liá»‡u

ChÃºng ta khÃ´ng thá»ƒ chá»‰ sá»­ dá»¥ng má»™t `DataCollatorWithPadding` nhÆ° trong [ChÆ°Æ¡ng 3](/course/chapter3) vÃ¬ nÃ³ chá»‰ Ä‘á»‡m cÃ¡c Ä‘áº§u vÃ o (ID Ä‘áº§u vÃ o, attention mask vÃ  loáº¡i token ID). á» Ä‘Ã¢y, cÃ¡c nhÃ£n cá»§a chÃºng ta nÃªn Ä‘Æ°á»£c Ä‘á»‡m theo cÃ¹ng má»™t cÃ¡ch giá»‘ng nhÆ° cÃ¡c Ä‘áº§u vÃ o Ä‘á»ƒ chÃºng giá»¯ nguyÃªn kÃ­ch thÆ°á»›c, sá»­ dá»¥ng `-100` lÃ m giÃ¡ trá»‹ Ä‘á»ƒ cÃ¡c dá»± Ä‘oÃ¡n tÆ°Æ¡ng á»©ng bá»‹ bá» qua trong tÃ­nh toÃ¡n tá»•n tháº¥t.

Táº¥t cáº£ Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification). Giá»‘ng nhÆ° `DataCollatorWithPadding`, nÃ³ sá»­ dá»¥ng `tokenizer` Ä‘á»ƒ xá»­ lÃ½ trÆ°á»›c cÃ¡c Ä‘áº§u vÃ o:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

Äá»ƒ kiá»ƒm tra nÃ³ trÃªn vÃ i máº«u, ta cÃ³ thá»ƒ gá»i nÃ³ trÃªn danh sÃ¡ch cÃ¡c máº«u tá»« táº­p huáº¥n luyá»‡n Ä‘Ã£ Ä‘Æ°á»£c tokenize:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

HÃ£y so sÃ¡nh Ä‘iá»u nÃ y vá»›i cÃ¡c nhÃ£n cho pháº§n tá»­ Ä‘áº§u tiÃªn vÃ  thá»© hai trong táº­p dá»¯ liá»‡u cá»§a mÃ¬nh:

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, táº­p há»£p nhÃ£n thá»© hai Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‡m báº±ng Ä‘á»™ dÃ i cá»§a táº­p Ä‘áº§u tiÃªn báº±ng cÃ¡ch sá»­ dá»¥ng `-100`.

{:else}

TrÃ¬nh Ä‘á»‘i chiáº¿u dá»¯ liá»‡u cá»§a chÃºng ta Ä‘Ã£ sáºµn sÃ ng hoáº¡t Ä‘á»™ng! BÃ¢y giá» hÃ£y sá»­ dá»¥ng nÃ³ Ä‘á»ƒ táº¡o má»™t `tf.data.Dataset` vá»›i phÆ°Æ¡ng thá»©c `to_tf_dataset()`.

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

Äiá»ƒm dá»«ng tiáº¿p theo: chÃ­nh lÃ  mÃ´ hÃ¬nh.

{/if}

{#if fw === 'tf'}

### Äá»‹nh nghÄ©a mÃ´ hÃ¬nh

VÃ¬ chÃºng tÃ´i Ä‘ang giáº£i quyáº¿t váº¥n Ä‘á» phÃ¢n loáº¡i token, chÃºng ta sáº½ sá»­ dá»¥ng lá»›p `TFAutoModelForTokenClassification`. Äiá»u chÃ­nh cáº§n nhá»› khi xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh nÃ y lÃ  truyá»n má»™t sá»‘ thÃ´ng tin vá» sá»‘ lÆ°á»£ng nhÃ£n mÃ  chÃºng ta cÃ³. CÃ¡ch dá»… nháº¥t Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y lÃ  truyá»n vÃ o tham sá»‘ `num_labels`, nhÆ°ng náº¿u chÃºng ta muá»‘n má»™t tiá»‡n Ã­ch luáº­n suy Ä‘áº¹p hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° tiá»‡n Ã­ch chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ Ä‘áº§u pháº§n nÃ y, tá»‘t hÆ¡n nÃªn Ä‘áº·t cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng chÃ­nh xÃ¡c thay tháº¿.

ChÃºng pháº£i Ä‘Æ°á»£c Ä‘áº·t bá»Ÿi hai tá»« Ä‘iá»ƒn, `id2label` vÃ  `label2id`, chá»©a Ã¡nh xáº¡ tá»« ID Ä‘áº¿n nhÃ£n vÃ  ngÆ°á»£c láº¡i:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ chuyá»ƒn chÃºng Ä‘áº¿n phÆ°Æ¡ng thá»©c `TFAutoModelForTokenClassification.from_pretrained()` vÃ  chÃºng sáº½ Ä‘Æ°á»£c Ä‘áº·t trong cáº¥u hÃ¬nh cá»§a mÃ´ hÃ¬nh, sau Ä‘Ã³ Ä‘Æ°á»£c lÆ°u vÃ  táº£i lÃªn Hub má»™t cÃ¡ch chÃ­nh xÃ¡c:

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Giá»‘ng nhÆ° khi chÃºng tÃ´i Ä‘á»‹nh nghÄ©a `TFAutoModelForSequenceClassification` cá»§a mÃ¬nh trong [ChÆ°Æ¡ng 3](/course/chapter3), viá»‡c táº¡o mÃ´ hÃ¬nh Ä‘Æ°a ra cáº£nh bÃ¡o ráº±ng má»™t sá»‘ trá»ng sá»‘ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng (nhá»¯ng trá»ng sá»‘ tá»« Ä‘áº§u huáº¥n luyá»‡n trÆ°á»›c) vÃ  má»™t sá»‘ trá»ng sá»‘ khÃ¡c Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn (nhá»¯ng trá»ng sá»‘ tá»« Ä‘áº§u phÃ¢n loáº¡i token má»›i) vÃ  mÃ´ hÃ¬nh nÃ y nÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n. ChÃºng ta sáº½ lÃ m Ä‘iá»u Ä‘Ã³ sau má»™t phÃºt, nhÆ°ng trÆ°á»›c tiÃªn hÃ£y kiá»ƒm tra ká»¹ xem mÃ´ hÃ¬nh cá»§a chÃºng ta cÃ³ Ä‘Ãºng sá»‘ lÆ°á»£ng nhÃ£n hay khÃ´ng:

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ Náº¿u báº¡n cÃ³ má»™t mÃ´ hÃ¬nh cÃ³ sá»‘ nhÃ£n sai, báº¡n sáº½ gáº·p lá»—i khÃ³ hiá»ƒu khi gá»i `model.fit()` sau nÃ y. Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y khÃ³ chá»‹u khi gá»¡ lá»—i, vÃ¬ váº­y hÃ£y Ä‘áº£m báº£o báº¡n thá»±c hiá»‡n kiá»ƒm tra nÃ y Ä‘á»ƒ xÃ¡c nháº­n ráº±ng báº¡n cÃ³ sá»‘ lÆ°á»£ng nhÃ£n dá»± kiáº¿n.

</Tip>

### Tinh chá»‰nh má»™t mÃ´ hÃ¬nh

BÃ¢y giá» chÃºng tÃ´i Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh! Tuy nhiÃªn, chÃºng ta chá»‰ cáº§n lÃ m thÃªm má»™t chÃºt cÃ´ng viá»‡c trÆ°á»›c tiÃªn: chÃºng ta nÃªn Ä‘Äƒng nháº­p vÃ o Hugging Face vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n cá»§a mÃ¬nh. Náº¿u báº¡n Ä‘ang lÃ m viá»‡c trÃªn notebook, cÃ³ má»™t chá»©c nÄƒng tiá»‡n lá»£i Ä‘á»ƒ giÃºp báº¡n lÃ m Ä‘iá»u nÃ y:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Thao tÃ¡c nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p  Hugging Face cá»§a mÃ¬nh.

Náº¿u báº¡n khÃ´ng lÃ m viá»‡c trong notebook, chá»‰ cáº§n nháº­p dÃ²ng sau vÃ o thiáº¿t bá»‹ Ä‘áº§u cuá»‘i cá»§a báº¡n:

```bash
huggingface-cli login
```

Sau khi Ä‘Äƒng nháº­p, chÃºng ta cÃ³ thá»ƒ chuáº©n bá»‹ má»i thá»© cáº§n thiáº¿t Ä‘á»ƒ biÃªn dá»‹ch mÃ´ hÃ¬nh cá»§a mÃ¬nh. ğŸ¤— Transformers cung cáº¥p má»™t hÃ m `create_optimizer()` thuáº­n tiá»‡n sáº½ cung cáº¥p cho báº¡n trÃ¬nh tá»‘i Æ°u hÃ³a `AdamW` vá»›i cÃ¡c cÃ i Ä‘áº·t thÃ­ch há»£p cho giáº£m trá»ng lÆ°á»£ng vÃ  giáº£m tá»‘c Ä‘á»™ há»c táº­p, cáº£ hai Ä‘á»u sáº½ cáº£i thiá»‡n hiá»‡u suáº¥t mÃ´ hÃ¬nh cá»§a báº¡n so vá»›i trÃ¬nh tá»‘i Æ°u hÃ³a `Adam` tÃ­ch há»£p sáºµn:

```python
from transformers import create_optimizer
import tensorflow as tf

# Huáº¥n luyá»‡n trong mixed-precision float16
# BÃ¬nh luáº­n dÃ²ng nÃ y náº¿u báº¡n Ä‘ang sá»­ dá»¥ng GPU nÃªn khÃ´ng Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« Ä‘iá»u nÃ y
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# Sá»‘ bÆ°á»›c huáº¥n luyá»‡n lÃ  sá»‘ lÆ°á»£ng máº«u trong táº­p dá»¯ liá»‡u, chia cho kÃ­ch thÆ°á»›c lÃ´ sau Ä‘Ã³ nhÃ¢n
# vá»›i sá»‘ epoch. LÆ°u Ã½ ráº±ng tf_train_dataset á»Ÿ Ä‘Ã¢y lÃ  lÃ´ tf.data.Dataset,
# khÃ´ng pháº£i Hugging Face Dataset gá»‘c, nÃªn len() cá»§a nÃ³ vá»‘n lÃ  num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

CÅ©ng lÆ°u Ã½ ráº±ng chÃºng ta khÃ´ng cung cáº¥p tham sá»‘ `loss` cho `compile()`. Äiá»u nÃ y lÃ  do cÃ¡c mÃ´ hÃ¬nh thá»±c sá»± cÃ³ thá»ƒ tÃ­nh toÃ¡n máº¥t mÃ¡t bÃªn trong - náº¿u báº¡n biÃªn dá»‹ch mÃ  khÃ´ng máº¥t mÃ¡t vÃ  cung cáº¥p cÃ¡c nhÃ£n cá»§a báº¡n trong tá»« Ä‘iá»ƒn Ä‘áº§u vÃ o (nhÆ° chÃºng ta lÃ m trong bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh), thÃ¬ mÃ´ hÃ¬nh sáº½ huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng máº¥t mÃ¡t ná»™i bá»™ Ä‘Ã³, Ä‘iá»u nÃ y sáº½ thÃ­ch há»£p cho tÃ¡c vá»¥ vÃ  loáº¡i mÃ´ hÃ¬nh báº¡n Ä‘Ã£ chá»n.

Tiáº¿p theo, chÃºng ta xÃ¡c Ä‘á»‹nh má»™t `PushToHubCallback` Ä‘á»ƒ táº£i mÃ´ hÃ¬nh lÃªn Hub trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  phÃ¹ há»£p vá»›i mÃ´ hÃ¬nh vá»›i lá»‡nh gá»i láº¡i Ä‘Ã³:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

Báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn Ä‘áº§y Ä‘á»§ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n báº±ng tham sá»‘ `hub_model_id` (Ä‘áº·c biá»‡t, báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y Ä‘áº¿n má»™t tá»• chá»©c). VÃ­ dá»¥: khi chÃºng ta Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course), chÃºng ta Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/bert-finetuned-ner"`. Theo máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ­ dá»¥: `"cool_huggingface_user/bert-finetuned-ner"`.

<Tip>

ğŸ’¡ Náº¿u thÆ° má»¥c Ä‘áº§u ra báº¡n Ä‘ang sá»­ dá»¥ng Ä‘Ã£ tá»“n táº¡i, nÃ³ cáº§n pháº£i lÃ  báº£n sao cá»¥c bá»™ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n. Náº¿u khÃ´ng, báº¡n sáº½ gáº·p lá»—i khi gá»i `model.fit()` vÃ  sáº½ cáº§n Ä‘áº·t tÃªn má»›i.

</Tip>

LÆ°u Ã½ ráº±ng trong khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra, má»—i khi mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u (á»Ÿ Ä‘Ã¢y, má»—i epoch), nÃ³ sáº½ Ä‘Æ°á»£c táº£i lÃªn Hub á»Ÿ cháº¿ Ä‘á»™ ná»n. Báº±ng cÃ¡ch nÃ y, báº¡n sáº½ cÃ³ thá»ƒ tiáº¿p tá»¥c quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a mÃ¬nh trÃªn má»™t mÃ¡y khÃ¡c náº¿u cáº§n.

á» giai Ä‘oáº¡n nÃ y, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tiá»‡n Ã­ch luáº­n suy trÃªn Model Hub Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh cá»§a mÃ¬nh vÃ  chia sáº» vá»›i báº¡n bÃ¨. Báº¡n Ä‘Ã£ tinh chá»‰nh thÃ nh cÃ´ng má»™t mÃ´ hÃ¬nh trong tÃ¡c vá»¥ phÃ¢n loáº¡i token - xin chÃºc má»«ng! NhÆ°ng mÃ´ hÃ¬nh cá»§a chÃºng ta thá»±c sá»± tá»‘t Ä‘áº¿n má»©c nÃ o? ChÃºng ta nÃªn cÃ³ má»™t sá»‘ chá»‰ sá»‘ hay thÆ°á»›c Ä‘o Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.

{/if}

### ThÆ°á»›c Ä‘o

{#if fw === 'pt'}

Äá»ƒ `Trainer` tÃ­nh toÃ¡n má»™t thÆ°á»›c Ä‘o cho má»—i epoch, ta sáº½ cáº§n Ä‘á»‹nh nghÄ©a hÃ m `compute_metrics()` nháº­n má»™t array cÃ¡c dá»± Ä‘oÃ¡n vÃ  nhÃ£n, vÃ  tráº£ vá» má»™t tá»« Ä‘iá»ƒn vá»›i tÃªn cÃ¡c thÆ°á»›c Ä‘o vÃ  giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng.

Khung truyá»n thá»‘ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ phÃ¢n loáº¡i token lÃ  [_seqeval_](https://github.com/chakki-works/seqeval). Äá»ƒ sá»­ dá»¥ng thÆ°á»›c Ä‘o nÃ y, ta sáº½ cáº§n cÃ i Ä‘áº·t thÆ° viá»‡n _seqeval_:

```py
!pip install seqeval
```

ChÃºng ta cÃ³ thá»ƒ táº£i nÃ³ qua hÃ m `evaluate.load()` nhÆ° Ä‘Ã£ lÃ m á»Ÿ [ChÆ°Æ¡ng 3 3](/course/chapter3):

{:else}

Khung truyá»n thá»‘ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ phÃ¢n loáº¡i token lÃ  [_seqeval_](https://github.com/chakki-works/seqeval). Äá»ƒ sá»­ dá»¥ng thÆ°á»›c Ä‘o nÃ y, ta sáº½ cáº§n cÃ i Ä‘áº·t thÆ° viá»‡n _seqeval_:

```py
!pip install seqeval
```

ChÃºng ta cÃ³ thá»ƒ táº£i nÃ³ qua hÃ m `evaluate.load()` nhÆ° Ä‘Ã£ lÃ m á»Ÿ [ChÆ°Æ¡ng 3 3](/course/chapter3):

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

ThÆ°á»›c Ä‘o nÃ y khÃ´ng giá»‘ng nhÆ° cÃ¡c thÆ°á»›c Ä‘o Ä‘á»™ chÃ­nh xÃ¡c thÃ´ng thÆ°Æ¡ng: nÃ³ sáº½ nháº­n má»™t danh sÃ¡ch cÃ¡c nhÃ£n nhÆ° lÃ  chuá»—i vÄƒn báº£n, khÃ´ng pháº£i sá»‘ nguyÃªn, nÃªn ta sáº½ cáº§n giáº£i mÃ£ toÃ n bá»™ nhá»¯ng dá»± Ä‘oÃ¡n vÃ  nhÃ£n trÆ°á»›c khi truyá»n chÃºng vÃ o thÆ°á»›c Ä‘o. HÃ£y cÃ¹ng xem nÃ³ hoáº¡t Ä‘á»™ng ra sao. Äáº§u tiÃªn, ta sáº½ láº¥y nhá»¯ng nhÃ£n tá»« máº«u huáº¥n luyá»‡n Ä‘áº§u tiÃªn:

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

Ta cÃ³ thá»ƒ táº¡o ra nhá»¯ng dá»± Ä‘oÃ¡n giáº£ cho chÃºng báº±ng cÃ¡ch thay Ä‘á»•i giÃ¡ trá»‹ á»Ÿ chá»‰ má»¥c 2:

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

LÆ°u Ã½ ráº±ng thÆ°á»›c Ä‘o nháº­n danh sÃ¡ch cÃ¡c dá»± Ä‘oÃ¡n (khÃ´ng chá»‰ má»™t) vÃ  danh sÃ¡ch cÃ¡c nhÃ£n. ÄÃ¢y lÃ  Ä‘áº§u ra:

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

Äiá»u nÃ y Ä‘ang gá»­i láº¡i ráº¥t nhiá»u thÃ´ng tin! ChÃºng ta nháº­n Ä‘Æ°á»£c precision, recall, vÃ  Ä‘iá»ƒm F1 cho tá»«ng thá»±c thá»ƒ riÃªng biá»‡t, cÅ©ng nhÆ° tá»•ng thá»ƒ. Äá»‘i vá»›i tÃ­nh toÃ¡n sá»‘ liá»‡u cá»§a mÃ¬nh, chÃºng ta sáº½ chá»‰ giá»¯ láº¡i Ä‘iá»ƒm tá»•ng thá»ƒ, nhÆ°ng hÃ£y tinh chá»‰nh chá»©c nÄƒng `compute_metrics()` Ä‘á»ƒ tráº£ vá» táº¥t cáº£ cÃ¡c sá»‘ liá»‡u báº¡n muá»‘n bÃ¡o cÃ¡o.

HÃ m `compute_metrics()` nÃ y trÆ°á»›c tiÃªn láº¥y argmax cá»§a logits Ä‘á»ƒ chuyá»ƒn chÃºng thÃ nh cÃ¡c dá»± Ä‘oÃ¡n (nhÆ° thÆ°á»ng lá»‡, logits vÃ  xÃ¡c suáº¥t theo cÃ¹ng má»™t thá»© tá»±, vÃ¬ váº­y chÃºng ta khÃ´ng cáº§n Ã¡p dá»¥ng softmax). Sau Ä‘Ã³, chÃºng ta pháº£i chuyá»ƒn Ä‘á»•i cáº£ nhÃ£n vÃ  dá»± Ä‘oÃ¡n tá»« sá»‘ nguyÃªn sang chuá»—i. ChÃºng ta xÃ³a táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cÃ³ nhÃ£n lÃ  `-100`, sau Ä‘Ã³ chuyá»ƒn káº¿t quáº£ Ä‘áº¿n phÆ°Æ¡ng thá»©c `metric.compute()`:

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # XoÃ¡ nhá»¯ng chá»‰ má»¥c bá»‹ ngÃ³ lÆ¡ (token Ä‘áº·c biá»‡t) vÃ  chuyá»ƒn chÃºng thÃ nh nhÃ£n
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

BÃ¢y giá» Ä‘iá»u nÃ y Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n, chÃºng ta gáº§n nhÆ° Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh `Trainer` cá»§a mÃ¬nh. ChÃºng ta chá»‰ cáº§n má»™t `model`  Ä‘á»ƒ tinh chá»‰nh!

{:else}

Äiá»u nÃ y Ä‘ang gá»­i láº¡i ráº¥t nhiá»u thÃ´ng tin! ChÃºng ta nháº­n Ä‘Æ°á»£c precision, recall, vÃ  Ä‘iá»ƒm F1 cho tá»«ng thá»±c thá»ƒ riÃªng biá»‡t, cÅ©ng nhÆ° tá»•ng thá»ƒ. HÃ£y cÅ©ng xem chuyá»‡n gÃ¬ xáº£y ra nÃªu sta thá»­ sá»­ dá»¥ng giÃ¡ trá»‹ dá»± Ä‘oÃ¡n thá»±c cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ tÃ­nh ra Ä‘iá»ƒm sá»‘ thá»±c.

TensorFlow khÃ´ng giá»‘ng nhÆ° viá»‡c ná»‘i cÃ¡c dá»± Ä‘oÃ¡n cá»§a chÃºng ta láº¡i vá»›i nhau, bá»Ÿi vÃ¬ chÃºng cÃ³ Ä‘á»™ dÃ i chuá»—i thay Ä‘á»•i. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng ta khÃ´ng thá»ƒ chá»‰ sá»­ dá»¥ng `model.predict()` --  hÆ°ng Ä‘iá»u Ä‘Ã³ sáº½ khÃ´ng ngÄƒn cáº£n chÃºng ta. ChÃºng ta sáº½ nháº­n Ä‘Æ°á»£c má»™t sá»‘ dá»± Ä‘oÃ¡n táº¡i má»™t thá»i Ä‘iá»ƒm vÃ  ná»‘i chÃºng thÃ nh má»™t danh sÃ¡ch dÃ i lá»›n khi tiáº¿p tá»¥c, bá» cÃ¡c token `-100` tÆ°Æ¡ng á»©ng bá»‹ áº©n Ä‘i hoáº·c Ä‘á»‡m, sau Ä‘Ã³ tÃ­nh toÃ¡n cÃ¡c sá»‘ liá»‡u trÃªn danh sÃ¡ch á»Ÿ cuá»‘i:

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```

```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

MÃ´ hÃ¬nh cá»§a báº¡n Ä‘Ã£ lÃ m nhÆ° tháº¿ nÃ o, so vá»›i mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i? Náº¿u báº¡n cÃ³ nhá»¯ng con sá»‘ tÆ°Æ¡ng tá»±, khÃ³a Ä‘Ã o táº¡o cá»§a báº¡n Ä‘Ã£ thÃ nh cÃ´ng!

{/if}

{#if fw === 'pt'}

### Äá»‹nh nghÄ©a mÃ´ hÃ¬nh

VÃ¬ chÃºng ta Ä‘ang giáº£i quyáº¿t váº¥n Ä‘á» phÃ¢n loáº¡i token, chÃºng ta sáº½ sá»­ dá»¥ng lá»›p `AutoModelForTokenClassification`. Äiá»u chÃ­nh cáº§n nhá»› khi xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh nÃ y lÃ  truyá»n má»™t sá»‘ thÃ´ng tin vá» sá»‘ lÆ°á»£ng nhÃ£n mÃ  chÃºng ta cÃ³. CÃ¡ch dá»… nháº¥t Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y lÃ  truyá»n vÃ o tham sá»‘ `num_labels`, nhÆ°ng náº¿u chÃºng ta muá»‘n má»™t tiá»‡n Ã­ch luáº­n suy hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° tiá»‡n Ã­ch chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ Ä‘áº§u pháº§n nÃ y, tá»‘t hÆ¡n nÃªn Ä‘áº·t cÃ¡c nhÃ£n tÆ°Æ¡ng á»©ng chÃ­nh xÃ¡c thay tháº¿.

ChÃºng pháº£i Ä‘Æ°á»£c Ä‘áº·t bá»Ÿi hai tá»« Ä‘iá»ƒn, `id2label` vÃ  `label2id`, chá»©a cÃ¡c Ã¡nh xáº¡ tá»« ID Ä‘áº¿n nhÃ£n vÃ  ngÆ°á»£c láº¡i:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

Giá» ta cÃ³ thá»ƒ truyá»n chÃºng vÃ o phÆ°Æ¡ng thá»©c `AutoModelForTokenClassification.from_pretrained()`, vÃ  chÃºng sáº½ Ä‘Æ°á»£c thiáº¿t láº­p trong cáº¥u hÃ¬nh mÃ´ hÃ¬nh vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c lÆ°u váº£ táº£i lÃªn Hub:

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Giá»‘ng nhÆ° khi chÃºng tÃ´i Ä‘á»‹nh nghÄ©a `AutoModelForSequenceClassification` cá»§a mÃ¬nh trong [ChÆ°Æ¡ng 3](/course/chapter3), viá»‡c táº¡o mÃ´ hÃ¬nh Ä‘Æ°a ra cáº£nh bÃ¡o ráº±ng má»™t sá»‘ trá»ng sá»‘ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng (nhá»¯ng trá»ng sá»‘ tá»« Ä‘áº§u huáº¥n luyá»‡n trÆ°á»›c) vÃ  má»™t sá»‘ trá»ng sá»‘ khÃ¡c Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn (nhá»¯ng trá»ng sá»‘ tá»« Ä‘áº§u phÃ¢n loáº¡i token má»›i) vÃ  mÃ´ hÃ¬nh nÃ y nÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n. ChÃºng ta sáº½ lÃ m Ä‘iá»u Ä‘Ã³ sau má»™t phÃºt, nhÆ°ng trÆ°á»›c tiÃªn hÃ£y kiá»ƒm tra ká»¹ xem mÃ´ hÃ¬nh cá»§a chÃºng ta cÃ³ Ä‘Ãºng sá»‘ lÆ°á»£ng nhÃ£n hay khÃ´ng:

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ Náº¿u báº¡n cÃ³ mÃ´ hÃ¬nh vá»›i sá»‘ lÆ°á»£ng nhÃ£n sai, báº¡n sáº½ nháº­n má»™t lá»—i khÃ³ hiá»ƒu khi gá»i hÃ m `Trainer.train()` sau Ä‘Ã³ (giá»‘ng nhÆ° "CUDA error: device-side assert triggered"). ÄÃ¢y lÃ  nguyÃªn nhÃ¢n sá»‘ má»™t gÃ¢y ra lá»—i do ngÆ°á»i dÃ¹ng bÃ¡o cÃ¡o vá» nhá»¯ng lá»—i nhÆ° váº­y, vÃ¬ váº­y hÃ£y Ä‘áº£m báº£o báº¡n thá»±c hiá»‡n kiá»ƒm tra nÃ y Ä‘á»ƒ xÃ¡c nháº­n ráº±ng báº¡n cÃ³ sá»‘ lÆ°á»£ng nhÃ£n dá»± kiáº¿n.

</Tip>

### Tinh chá»‰nh mÃ´ hÃ¬nh

Giá» ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh! ChÃºng ta chá»‰ cáº§n lÃ m hai Ä‘iá»u trÆ°á»›c khi Ä‘á»‹nh nghÄ©a `Trainer`:  Ä‘Äƒng nháº­p vÃ o Hugging Face vÃ  Ä‘á»‹nh nghÄ©a cÃ¡c tham sá»‘ huáº¥n luyá»‡n. Náº¿u báº¡n Ä‘ang lÃ m viá»‡c vá»›i notebook, cÃ³ má»™t hÃ m thuáº­n tiá»‡n cÃ³ thá»ƒ giÃºp báº¡n: 

```python
from huggingface_hub import notebook_login

notebook_login()
```

Thao tÃ¡c nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p Hugging Facecá»§a mÃ¬nh.

Náº¿u báº¡n khÃ´ng lÃ m viá»‡c trong sá»• ghi chÃ©p, chá»‰ cáº§n nháº­p dÃ²ng sau vÃ o thiáº¿t bá»‹ Ä‘áº§u cuá»‘i cá»§a báº¡n:

```bash
huggingface-cli login
```

Once this is done, we can define our `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

Báº¡n Ä‘Ã£ tá»«ng tháº¥y háº§u háº¿t nhá»¯ng Ä‘iá»u Ä‘Ã³ trÆ°á»›c Ä‘Ã¢y: chÃºng ta Ä‘áº·t má»™t sá»‘ siÃªu tham sá»‘ (nhÆ° tá»‘c Ä‘á»™ há»c, sá»‘ epoch cáº§n luyá»‡n táº­p vÃ  giáº£m trá»ng lÆ°á»£ng) vÃ  chÃºng ta chá»‰ Ä‘á»‹nh `push_to_hub=True` Ä‘á»ƒ chá»‰ ra ráº±ng chÃºng ta muá»‘n lÆ°u mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ nÃ³ vÃ o cuá»‘i má»—i epoch vÃ  ráº±ng chÃºng ta muá»‘n táº£i káº¿t quáº£ cá»§a mÃ¬nh lÃªn Model Hub. LÆ°u Ã½ ráº±ng báº¡n cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh tÃªn cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n báº±ng tham sá»‘ `hub_model_id` (cá»¥ thá»ƒ lÃ  báº¡n sáº½ pháº£i sá»­ dá»¥ng tham sá»‘ nÃ y Ä‘á»ƒ Ä‘áº©y Ä‘áº¿n má»™t tá»• chá»©c). VÃ­ dá»¥: khi Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c [`huggingface-course`](https://huggingface.co/huggingface-course) chÃºng ta Ä‘Ã£ thÃªm `hub_model_id="huggingface-course/bert-finetuned-ner"` vÃ o `TrainingArguments`. Theo máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ¬ váº­y trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, nÃ³ sáº½ lÃ  `"sgugger/bert-finetuned-ner"`.

<Tip>

ğŸ’¡ Náº¿u thÆ° má»¥c Ä‘áº§u ra báº¡n Ä‘ang sá»­ dá»¥ng Ä‘Ã£ tá»“n táº¡i, nÃ³ cáº§n pháº£i lÃ  báº£n sao cá»¥c bá»™ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n. Náº¿u khÃ´ng, báº¡n sáº½ gáº·p lá»—i khi xÃ¡c Ä‘á»‹nh `Trainer` cá»§a mÃ¬nh vÃ  sáº½ cáº§n Ä‘áº·t má»™t tÃªn má»›i.

</Tip>

Cuá»‘i cÃ¹ng, chÃºng ta chá»‰ cáº§n truyá»n má»i thá»© cho  `Trainer` vÃ  báº¯t Ä‘áº§u huáº¥n luyá»‡n:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

LÆ°u Ã½ ráº±ng trong khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra, má»—i khi mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u (á»Ÿ Ä‘Ã¢y, má»—i epoch), nÃ³ sáº½ Ä‘Æ°á»£c táº£i lÃªn Hub á»Ÿ cháº¿ Ä‘á»™ ná»n. Báº±ng cÃ¡ch nÃ y, báº¡n sáº½ cÃ³ thá»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n cá»§a mÃ¬nh trÃªn má»™t mÃ¡y khÃ¡c náº¿u cáº§n.

Sau khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t, chÃºng ta sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `push_to_hub()` Ä‘á»ƒ Ä‘áº£m báº£o chÃºng ta táº£i lÃªn phiÃªn báº£n má»›i nháº¥t cá»§a mÃ´ hÃ¬nh:

```py
trainer.push_to_hub(commit_message="Training complete")
```

CÃ¢u lá»‡nh nÃ y tráº£ vá» URL cá»§a cam kháº¿t nÃ³ vá»«a lÃ m, náº¿u báº¡n muá»‘n kiá»ƒm tra nÃ³:

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

`Trainer` cÅ©ng soáº¡n tháº£o má»™t tháº» mÃ´ hÃ¬nh vá»›i táº¥t cáº£ cÃ¡c káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ  táº£i nÃ³ lÃªn. á» giai Ä‘oáº¡n nÃ y, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tiá»‡n Ã­ch luáº­n suy trÃªn Model Hub Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh cá»§a mÃ¬nh vÃ  chia sáº» vá»›i báº¡n bÃ¨. Báº¡n Ä‘Ã£ tinh chá»‰nh thÃ nh cÃ´ng má»™t mÃ´ hÃ¬nh trong tÃ¡c vá»¥ phÃ¢n loáº¡i token - xin chÃºc má»«ng!

Náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n má»™t chÃºt vá» vÃ²ng huáº¥n luyá»‡n, bÃ¢y giá» chÃºng tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n cÃ¡ch thá»±c hiá»‡n Ä‘iá»u tÆ°Æ¡ng tá»± báº±ng cÃ¡ch sá»­ dá»¥ng ğŸ¤— Accelerate.

## Má»™t vÃ²ng huáº¥n luyá»‡n tuá»³ chá»‰nh

BÃ¢y giá» chÃºng ta hÃ£y xem toÃ n bá»™ vÃ²ng láº·p huáº¥n luyá»‡n, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ dá»… dÃ ng tÃ¹y chá»‰nh cÃ¡c pháº§n báº¡n cáº§n. NÃ³ sáº½ trÃ´ng ráº¥t giá»‘ng nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ lÃ m trong [ChÆ°Æ¡ng 3](/course/chapter3/4), vá»›i má»™t vÃ i thay Ä‘á»•i cho pháº§n Ä‘Ã¡nh giÃ¡.

### Chuáº©n bá»‹ má»i thá»© Ä‘á»ƒ huáº¥n luyá»‡n

Äáº§u tiÃªn, chÃºng ta cáº§n xÃ¢y dá»±ng cÃ¡c `DataLoader` tá»« cÃ¡c táº­p dá»¯ liá»‡u cá»§a mÃ¬nh. ChÃºng ta sáº½ sá»­ dá»¥ng láº¡i `data_collator` cá»§a mÃ¬nh dÆ°á»›i dáº¡ng `collate_fn` vÃ  xÃ¡o trá»™n táº­p huáº¥n luyá»‡n, nhÆ°ng khÃ´ng pháº£i táº­p kiá»ƒm Ä‘á»‹nh:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

Tiáº¿p theo, chÃºng ta khÃ´i phá»¥c mÃ´ hÃ¬nh cá»§a mÃ¬nh, Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng chÃºng ta khÃ´ng tiáº¿p tá»¥c tinh chá»‰nh tá»« trÆ°á»›c mÃ  báº¯t Ä‘áº§u láº¡i tá»« mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c BERT:

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```
Sau Ä‘Ã³, chÃºng tÃ´i sáº½ cáº§n má»™t trÃ¬nh tá»‘i Æ°u hÃ³a. ChÃºng ta sáº½ sá»­ dá»¥ng `AdamW` cá»• Ä‘iá»ƒn, giá»‘ng nhÆ° `Adam`, nhÆ°ng vá»›i má»™t báº£n sá»­a lá»—i trong cÃ¡ch Ã¡p dá»¥ng weight decay (phÃ¢n rÃ£ trá»ng sá»‘):

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Once we have all those objects, we can send them to the `accelerator.prepare()` method:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ Náº¿u báº¡n huáº¥n luyá»‡n trÃªn TPU, báº¡n sáº½ cáº§n chuyá»ƒn táº¥t cáº£ cÃ¡c Ä‘oáº¡n mÃ£ á»Ÿ trÃªn thÃ nh má»™t hÃ m huáº¥n luyá»‡n. Xem [ChÆ°Æ¡ng 3](/course/chapter3) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

</Tip>

BÃ¢y giá», chÃºng ta Ä‘Ã£ gá»­i `train_dataloader` cá»§a mÃ¬nh tá»›i `speedrator.prepare()`, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»™ dÃ i cá»§a nÃ³ Ä‘á»ƒ tÃ­nh sá»‘ bÆ°á»›c huáº¥n luyá»‡n. HÃ£y nhá»› ráº±ng chÃºng ta pháº£i luÃ´n lÃ m Ä‘iá»u nÃ y sau khi chuáº©n bá»‹ dataloader, vÃ¬ phÆ°Æ¡ng thá»©c Ä‘Ã³ sáº½ thay Ä‘á»•i Ä‘á»™ dÃ i cá»§a nÃ³. ChÃºng ta sá»­ dá»¥ng má»™t lá»‹ch trÃ¬nh tuyáº¿n tÃ­nh cá»• Ä‘iá»ƒn tá»« tá»‘c Ä‘á»™ há»c Ä‘áº¿n 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Cuá»‘i cÃ¹ng, Ä‘á»ƒ Ä‘áº©y mÃ´ hÃ¬nh cá»§a chÃºng ta lÃªn Hub, chÃºng ta sáº½ cáº§n táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `Repository` trong má»™t thÆ° má»¥c Ä‘ang lÃ m viá»‡c. Äáº§u tiÃªn hÃ£y Ä‘Äƒng nháº­p vÃ o Hugging Face, náº¿u báº¡n chÆ°a Ä‘Äƒng nháº­p. ChÃºng ta sáº½ xÃ¡c Ä‘á»‹nh tÃªn kho lÆ°u trá»¯ tá»« ID mÃ´ hÃ¬nh mÃ  ta muá»‘n cung cáº¥p cho mÃ´ hÃ¬nh cá»§a mÃ¬nh (vui lÃ²ng thay tháº¿ `repo_name` báº±ng sá»± lá»±a chá»n cá»§a riÃªng báº¡n; nÃ³ chá»‰ cáº§n chá»©a tÃªn ngÆ°á»i dÃ¹ng cá»§a báº¡n, Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ hÃ m `get_full_repo_name ()` thá»±c hiá»‡n):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

Sau Ä‘Ã³, ta cÃ³ thá»ƒ sao chÃ©p kho lÆ°u trá»¯ Ä‘Ã³ trong má»™t thÆ° má»¥c cá»¥c bá»™. Náº¿u nÃ³ Ä‘Ã£ tá»“n táº¡i, thÆ° má»¥c cá»¥c bá»™ nÃ y pháº£i lÃ  báº£n sao hiá»‡n cÃ³ cá»§a kho lÆ°u trá»¯ mÃ  chÃºng ta Ä‘ang lÃ m viá»‡c:

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Giá» ta cÃ³ thá»ƒ táº£i má»i thá»© ta lÆ°u trong `output_dir` báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `repo.push_to_hub()`. Äiá»u nÃ y sáº½ giÃºp ta táº£i ngay láº­p tá»©c mÃ´ hÃ¬nh á»Ÿ cuá»‘i má»—i epoch.

### VÃ²ng láº·p huáº¥n luyá»‡n

BÃ¢y giá» chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ viáº¿t vÃ²ng láº·p  huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ¡n giáº£n hÃ³a pháº§n Ä‘Ã¡nh giÃ¡ cá»§a nÃ³, chÃºng ta Ä‘á»‹nh nghÄ©a hÃ m `postprocess()` láº¥y cÃ¡c dá»± Ä‘oÃ¡n vÃ  nhÃ£n vÃ  chuyá»ƒn Ä‘á»•i chÃºng thÃ nh danh sÃ¡ch cÃ¡c chuá»—i, giá»‘ng nhÆ° Ä‘á»‘i tÆ°á»£ng `metric` mong Ä‘á»£i:

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Loáº¡i bá» cÃ¡c chá»‰ má»¥c bá»‹ ngÃ³ lÆ¡ (cÃ¡c token Ä‘áº·c biá»‡t) vÃ  chuyá»ƒn chÃºng thÃ nh nhÃ£n
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ viáº¿t vÃ²ng láº·p huáº¥n luyá»‡n. Sau khi xÃ¡c Ä‘á»‹nh má»™t thanh tiáº¿n trÃ¬nh Ä‘á»ƒ theo dÃµi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra nhÆ° tháº¿ nÃ o, vÃ²ng láº·p cÃ³ ba pháº§n:

- Báº£n thÃ¢n quÃ¡ trÃ¬nh huáº¥n luyá»‡n, lÃ  vÃ²ng láº·p cá»• Ä‘iá»ƒn trÃªn `train_dataloader`, truyá»n tháº³ng qua mÃ´ hÃ¬nh, sau Ä‘Ã³ truyá»n ngÆ°á»£c vÃ  tá»‘i Æ°u hÃ³a.
- ÄÃ¡nh giÃ¡, trong Ä‘Ã³ cÃ³ má»™t Ä‘iá»ƒm má»›i sau khi nháº­n Ä‘Æ°á»£c káº¿t quáº£ Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh trÃªn má»™t lÃ´: vÃ¬ hai quy trÃ¬nh cÃ³ thá»ƒ Ä‘Ã£ Ä‘á»™n cÃ¡c Ä‘áº§u vÃ o vÃ  nhÃ£n thÃ nh cÃ¡c hÃ¬nh dáº¡ng khÃ¡c nhau, chÃºng ta cáº§n sá»­ dá»¥ng `accelerator.pad_across_processes()`Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n vÃ  dÃ¡n nhÃ£n cho cÃ¹ng má»™t hÃ¬nh dáº¡ng trÆ°á»›c khi gá»i phÆ°Æ¡ng thá»©c `collect()`. Náº¿u khÃ´ng lÃ m Ä‘iá»u nÃ y, Ä‘Ã¡nh giÃ¡ sáº½ bá»‹ lá»—i hoáº·c bá»‹ treo vÄ©nh viá»…n. Sau Ä‘Ã³, chÃºng ta gá»­i káº¿t quáº£ Ä‘áº¿n `metric.add_batch()` vÃ  gá»i `metric.compute()` khi vÃ²ng láº·p Ä‘Ã¡nh giÃ¡ káº¿t thÃºc.
- LÆ°u vÃ  táº£i lÃªn, nÆ¡i Ä‘áº§u tiÃªn chÃºng ta lÆ°u mÃ´ hÃ¬nh vÃ  trÃ¬nh tokenize, sau Ä‘Ã³ gá»i `repo.push_to_hub()`. LÆ°u Ã½ ráº±ng chÃºng ta sá»­ dá»¥ng Ä‘á»‘i sá»‘ `blocks=False` Ä‘á»ƒ yÃªu cáº§u thÆ° viá»‡n ğŸ¤— Hub Ä‘áº©y vÃ o má»™t quÃ¡ trÃ¬nh khÃ´ng Ä‘á»“ng bá»™. Báº±ng cÃ¡ch nÃ y, quÃ¡ trÃ¬nh huáº¥n luyá»‡n tiáº¿p tá»¥c diá»…n ra bÃ¬nh thÆ°á»ng vÃ  lá»‡nh (dÃ i) nÃ y Ä‘Æ°á»£c thá»±c thi á»Ÿ cháº¿ Ä‘á»™ ná»n.

ÄÃ¢y lÃ  mÃ£ hoÃ n chá»‰nh cho vÃ²ng láº·p huáº¥n luyá»‡n:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Huáº¥n luyá»‡n
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ÄÃ¡nh giÃ¡
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Cáº§n Ä‘á»‡m cÃ¡c dá»± Ä‘oÃ¡n vÃ  nhÃ£n Ä‘á»ƒ táº­p há»£p
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # LÆ°u vÃ  táº£i
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

Trong trÆ°á»ng há»£p Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn báº¡n tháº¥y má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u báº±ng ğŸ¤— Accelerate, hÃ£y dÃ nh má»™t chÃºt thá»i gian Ä‘á»ƒ kiá»ƒm tra ba dÃ²ng mÃ£ Ä‘i kÃ¨m vá»›i nÃ³:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

DÃ²ng Ä‘áº§u tiÃªn Ä‘Ã£ tá»± giáº£i thÃ­ch: nÃ³ cho táº¥t cáº£ cÃ¡c quÃ¡ trÃ¬nh chá» cho Ä‘áº¿n khi má»i ngÆ°á»i á»Ÿ giai Ä‘oáº¡n Ä‘Ã³ trÆ°á»›c khi tiáº¿p tá»¥c. Äiá»u nÃ y lÃ  Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng ta cÃ³ cÃ¹ng má»™t mÃ´ hÃ¬nh trong má»i quy trÃ¬nh trÆ°á»›c khi lÆ°u. Sau Ä‘Ã³, chÃºng ta láº¥y `unwrapped_model`, lÃ  mÃ´ hÃ¬nh cÆ¡ sá»Ÿ mÃ  ta Ä‘Ã£ xÃ¡c Ä‘á»‹nh. PhÆ°Æ¡ng thá»©c `accelerator.prepare()` thay Ä‘á»•i mÃ´ hÃ¬nh Ä‘á»ƒ hoáº¡t Ä‘á»™ng trong huáº¥n luyá»‡n phÃ¢n tÃ¡n, vÃ¬ váº­y nÃ³ sáº½ khÃ´ng cÃ³ phÆ°Æ¡ng thá»©c `save_pretrained()` ná»¯a; phÆ°Æ¡ng thá»©c `accelerator.unwrap_model()` hoÃ n tÃ¡c bÆ°á»›c Ä‘Ã³. Cuá»‘i cÃ¹ng, chÃºng ta gá»i lÃ  `save_pretrained()` nhÆ°ng yÃªu cáº§u phÆ°Æ¡ng thá»©c Ä‘Ã³ sá»­ dá»¥ng `accelerator.save()` thay vÃ¬ `torch.save()`.

Khi Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n, báº¡n sáº½ cÃ³ má»™t mÃ´ hÃ¬nh táº¡o ra káº¿t quáº£ khÃ¡ giá»‘ng vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i `Trainer`. Báº¡n cÃ³ thá»ƒ kiá»ƒm tra mÃ´ hÃ¬nh mÃ  chÃºng ta Ä‘Ã£ huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘oáº¡n mÃ£ nÃ y táº¡i [_huggingface-course/bert-finetuned-ner-accelerate_](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate). VÃ  náº¿u báº¡n muá»‘n kiá»ƒm tra báº¥t ká»³ tinh chá»‰nh nÃ o Ä‘á»‘i vá»›i vÃ²ng láº·p huáº¥n luyá»‡n, báº¡n cÃ³ thá»ƒ trá»±c tiáº¿p thá»±c hiá»‡n chÃºng báº±ng cÃ¡ch chá»‰nh sá»­a Ä‘oáº¡n mÃ£ Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ trÃªn!

{/if}

## Sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh

ChÃºng tÃ´i Ä‘Ã£ chá»‰ cho báº¡n cÃ¡ch báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh mÃ  chÃºng ta Ä‘Ã£ tinh chá»‰nh trÃªn Model Hub báº±ng tiá»‡n Ã­ch luáº­n suy. Äá»ƒ sá»­ dá»¥ng nÃ³ cá»¥c bá»™ trong má»™t `pipeline`, báº¡n chá»‰ cáº§n chá»‰ Ä‘á»‹nh mÃ£ Ä‘á»‹nh danh mÃ´ hÃ¬nh thÃ­ch há»£p:

```py
from transformers import pipeline

# Thay tháº¿ nÃ³ vá»›i checkpoint cá»§a ta
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Tuyá»‡t quÃ¡! MÃ´ hÃ¬nh cá»§a chÃºng ta Ä‘ang hoáº¡t Ä‘á»™ng tá»‘t nhÆ° mÃ´ hÃ¬nh máº·c Ä‘á»‹nh cho pipeline nÃ y!
