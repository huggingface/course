<FrameworkSwitchCourse {fw} />

# Há»i Ä‘Ã¡p

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section7_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section7_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter7/section7_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter7/section7_tf.ipynb"},
]} />

{/if}

ÄÃ£ Ä‘áº¿n lÃºc xem pháº§n há»i Ä‘Ã¡p! TÃ¡c vá»¥ nÃ y cÃ³ nhiá»u loáº¡i, nhÆ°ng tÃ¡c vá»¥ mÃ  chÃºng ta sáº½ táº­p trung vÃ o trong pháº§n nÃ y Ä‘Æ°á»£c gá»i lÃ  tráº£ lá»i cÃ¢u há»i *khai thÃ¡c*. Äiá»u nÃ y liÃªn quan Ä‘áº¿n viá»‡c Ä‘áº·t ra cÃ¡c cÃ¢u há»i vá» má»™t tÃ i liá»‡u vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c cÃ¢u tráº£ lá»i dÆ°á»›i dáº¡ng _cÃ¡c khoáº£ng cá»§a vÄƒn báº£n_ trong chÃ­nh tÃ i liá»‡u Ä‘Ã³.

<Youtube id="ajPx5LwJD-I"/>

ChÃºng ta sáº½ tinh chá»‰nh mÃ´ hÃ¬nh BERT trÃªn [bá»™ dá»¯ liá»‡u SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), bao gá»“m cÃ¡c cÃ¢u há»i do cá»™ng Ä‘á»“ng Ä‘áº·t ra trÃªn má»™t táº­p cÃ¡c bÃ i viáº¿t trÃªn Wikipedia. Äiá»u nÃ y sáº½ cung cáº¥p cho chÃºng ta má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ tÃ­nh toÃ¡n cÃ¡c dá»± Ä‘oÃ¡n nhÆ° tháº¿ nÃ y:

<iframe src="https://course-demos-bert-finetuned-squad.hf.space" frameBorder="0" height="450" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ÄÃ¢y thá»±c sá»± cÃ¡ch mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  táº£i lÃªn Hub báº±ng cÃ¡ch sá»­ dá»¥ng mÃ£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong pháº§n nÃ y. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y nÃ³ vÃ  kiá»ƒm tra cÃ¡c dá»± Ä‘oáº¡n [táº¡i Ä‘Ã¢y](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F).

<Tip>

ğŸ’¡ CÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a nhÆ° BERT cÃ³ xu hÆ°á»›ng tuyá»‡t vá»i trong viá»‡c trÃ­ch xuáº¥t cÃ¢u tráº£ lá»i cho cÃ¡c cÃ¢u há»i dáº¡ng thá»±c táº¿ nhÆ° "Ai Ä‘Ã£ phÃ¡t minh ra kiáº¿n trÃºc Transformer?" nhÆ°ng khÃ¡ kÃ©m khi tráº£ lá»i nhá»¯ng cÃ¢u há»i má»Ÿ nhÆ° "Táº¡i sao báº§u trá»i láº¡i cÃ³ mÃ u xanh?" Trong nhá»¯ng trÆ°á»ng há»£p khÃ³ khÄƒn hÆ¡n nÃ y, cÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a-giáº£i mÃ£ nhÆ° T5 vÃ  BART thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin theo cÃ¡ch khÃ¡ giá»‘ng vá»›i [tÃ³m táº¯t vÄƒn báº£n](/course/chapter7/5). Náº¿u báº¡n quan tÃ¢m Ä‘áº¿n kiá»ƒu tráº£ lá»i cÃ¢u há»i *chung chung* nÃ y, chÃºng tÃ´i khuyÃªn báº¡n nÃªn xem [demo](https://yjernite.github.io/lfqa.html) cá»§a chÃºng tÃ´i dá»±a trÃªn [bá»™ dá»¯ liá»‡u ELI5](https://huggingface.co/datasets/eli5).

</Tip>

## Chuáº©n bá»‹ dá»¯ liá»‡u

Táº­p dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u nháº¥t lÃ m tiÃªu chuáº©n há»c thuáº­t Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khai thÃ¡c lÃ  [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), vÃ¬ váº­y Ä‘Ã³ lÃ  táº­p chÃºng ta sáº½ sá»­ dá»¥ng á»Ÿ Ä‘Ã¢y. NgoÃ i ra cÃ²n cÃ³ má»™t Ä‘iá»ƒm chuáº©n khÃ³ hÆ¡n [SQuAD v2](https://huggingface.co/datasets/squad_v2), bao gá»“m cÃ¡c cÃ¢u há»i khÃ´ng cÃ³ cÃ¢u tráº£ lá»i. Miá»…n lÃ  táº­p dá»¯ liá»‡u cá»§a riÃªng báº¡n chá»©a má»™t cá»™t cho ngá»¯ cáº£nh, má»™t cá»™t cho cÃ¢u há»i vÃ  má»™t cá»™t cho cÃ¢u tráº£ lá»i, báº¡n sáº½ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c bÆ°á»›c bÃªn dÆ°á»›i.

### Bá»™ dá»¯ liá»‡u SQuAD

NhÆ° thÆ°á»ng lá»‡, chÃºng ta cÃ³ thá»ƒ táº£i xuá»‘ng vÃ  lÆ°u bá»™ dá»¯ liá»‡u vÃ o bá»™ nhá»› cache chá»‰ trong má»™t bÆ°á»›c nhá» vÃ o `load_dataset()`:

```py
from datasets import load_dataset

raw_datasets = load_dataset("squad")
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ xem xÃ©t Ä‘á»‘i tÆ°á»£ng nÃ y Ä‘á»ƒ tÃ¬m hiá»ƒu thÃªm vá» táº­p dá»¯ liá»‡u SQuAD:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})
```

CÃ³ váº» nhÆ° chÃºng ta cÃ³ má»i thá»© ta cáº§n vá»›i cÃ¡c trÆ°á»ng `context`, `question`, vÃ  `answers`, vÃ¬ váº­y hÃ£y in chÃºng cho pháº§n tá»­ Ä‘áº§u tiÃªn cá»§a táº­p huáº¥n luyá»‡n cá»§a mÃ¬nh:

```py
print("Context: ", raw_datasets["train"][0]["context"])
print("Question: ", raw_datasets["train"][0]["question"])
print("Answer: ", raw_datasets["train"][0]["answers"])
```

```python out
Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'
Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'
Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
```

CÃ¡c trÆ°á»ng `context` vÃ  `question` ráº¥t dá»… sá»­ dá»¥ng. TrÆ°á»ng `answers` phá»©c táº¡p hÆ¡n má»™t chÃºt vÃ¬ nÃ³ so sÃ¡nh má»™t tá»« Ä‘iá»ƒn vá»›i hai trÆ°á»ng Ä‘á»u lÃ  danh sÃ¡ch. ÄÃ¢y lÃ  Ä‘á»‹nh dáº¡ng sáº½ Ä‘Æ°á»£c mong Ä‘á»£i bá»Ÿi chá»‰ sá»‘ `squad` trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡; náº¿u báº¡n Ä‘ang sá»­ dá»¥ng dá»¯ liá»‡u cá»§a riÃªng mÃ¬nh, báº¡n khÃ´ng nháº¥t thiáº¿t pháº£i lo láº¯ng vá» viá»‡c Ä‘áº·t cÃ¡c cÃ¢u tráº£ lá»i á»Ÿ cÃ¹ng má»™t Ä‘á»‹nh dáº¡ng. TrÆ°á»ng `text` khÃ¡ rÃµ rÃ ng vÃ  trÆ°á»ng `answer_start` chá»©a chá»‰ má»¥c kÃ½ tá»± báº¯t Ä‘áº§u cá»§a má»—i cÃ¢u tráº£ lá»i trong ngá»¯ cáº£nh.

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chá»‰ cÃ³ má»™t cÃ¢u tráº£ lá»i kháº£ dÄ©. ChÃºng ta cÃ³ thá»ƒ kiá»ƒm tra ká»¹ Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `Dataset.filter()`:

```py
raw_datasets["train"].filter(lambda x: len(x["answers"]["text"]) != 1)
```

```python out
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
```

Tuy nhiÃªn, Ä‘á»ƒ Ä‘Ã¡nh giÃ¡, cÃ³ má»™t sá»‘ cÃ¢u tráº£ lá»i cÃ³ thá»ƒ cÃ³ cho má»—i máº«u, cÃ³ thá»ƒ giá»‘ng hoáº·c khÃ¡c nhau:

```py
print(raw_datasets["validation"][0]["answers"])
print(raw_datasets["validation"][2]["answers"])
```

```python out
{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}
{'text': ['Santa Clara, California', "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], 'answer_start': [403, 355, 355]}
```

ChÃºng ta sáº½ khÃ´ng Ä‘i sÃ¢u vÃ o táº­p lá»‡nh Ä‘Ã¡nh giÃ¡ vÃ¬ táº¥t cáº£ sáº½ Ä‘Æ°á»£c bao bá»c bá»Ÿi chá»‰ sá»‘ ğŸ¤— Datasets, nhÆ°ng phiÃªn báº£n ngáº¯n lÃ  má»™t sá»‘ cÃ¢u há»i cÃ³ má»™t sá»‘ cÃ¢u tráº£ lá»i cÃ³ thá»ƒ cÃ³ vÃ  táº­p lá»‡nh nÃ y sáº½ so sÃ¡nh má»™t cÃ¢u tráº£ lá»i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n cho táº¥t cáº£ cÃ¢u tráº£ lá»i cÃ³ thá»ƒ cháº¥p nháº­n Ä‘Æ°á»£c vÃ  dÃ nh Ä‘iá»ƒm cao nháº¥t. VÃ­ dá»¥: náº¿u chÃºng ta xem xÃ©t máº«u á»Ÿ chá»‰ má»¥c 2:

```py
print(raw_datasets["validation"][2]["context"])
print(raw_datasets["validation"][2]["question"])
```

```python out
'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.'
'Where did Super Bowl 50 take place?'
```

ta cÃ³ thá»ƒ tháº¥y cÃ¢u tráº£ lá»i cÃ³ thá»ƒ thá»±c ra lÃ  má»™t trong sá»‘ ba kháº£ nÄƒng ta tháº¥y trÆ°á»›c Ä‘Ã³.

### Xá»­ lÃ½ dá»¯ liá»‡u huáº¥n luyá»‡n

<Youtube id="qgaM0weJHpA"/>

HÃ£y báº¯t Ä‘áº§u vá»›i viá»‡c xá»­ lÃ½ trÆ°á»›c dá»¯ liá»‡u huáº¥n luyá»‡n. Pháº§n khÃ³ sáº½ lÃ  táº¡o nhÃ£n cho cÃ¢u tráº£ lá»i cá»§a cÃ¢u há»i, Ä‘Ã³ sáº½ lÃ  vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a cÃ¡c tháº» tÆ°Æ¡ng á»©ng vá»›i cÃ¢u tráº£ lá»i bÃªn trong ngá»¯ cáº£nh.

NhÆ°ng chÃºng ta Ä‘á»«ng vÆ°á»£t lÃªn chÃ­nh mÃ¬nh. Äáº§u tiÃªn, chÃºng ta cáº§n chuyá»ƒn Ä‘á»•i vÄƒn báº£n trong Ä‘áº§u vÃ o thÃ nh cÃ¡c ID mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c, sá»­ dá»¥ng tokenizer:

```py
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, chÃºng ta sáº½ tinh chá»‰nh mÃ´ hÃ¬nh BERT, nhÆ°ng báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng báº¥t ká»³ loáº¡i mÃ´ hÃ¬nh nÃ o khÃ¡c miá»…n lÃ  nÃ³ cÃ³ triá»ƒn khai trÃ¬nh tokenize nhanh. Báº¡n cÃ³ thá»ƒ xem táº¥t cáº£ cÃ¡c kiáº¿n trÃºc Ä‘i kÃ¨m vá»›i phiÃªn báº£n nhanh trong [báº£ng lá»›n nÃ y](https://huggingface.co/transformers/#supported-frameworks) vÃ  Ä‘á»ƒ kiá»ƒm tra xem Ä‘á»‘i tÆ°á»£ng `tokenizer` mÃ  báº¡n Ä‘ang sá»­ dá»¥ng cÃ³ thá»±c sá»± lÃ  Ä‘Æ°á»£c há»— trá»£ bá»Ÿi ğŸ¤— Tokenizers, báº¡n cÃ³ thá»ƒ xem thuá»™c tÃ­nh `is_fast` cá»§a nÃ³:
```py
tokenizer.is_fast
```

```python out
True
```

ChÃºng ta cÃ³ thá»ƒ truyá»n cÃ¢u há»i vÃ  ngá»¯ cáº£nh cho trÃ¬nh tokenizer cá»§a mÃ¬nh vÃ  nÃ³ sáº½ chÃ¨n Ä‘Ãºng cÃ¡c token Ä‘áº·c biá»‡t Ä‘á»ƒ táº¡o thÃ nh má»™t cÃ¢u nhÆ° sau:

```
[CLS] question [SEP] context [SEP]
```

HÃ£y cÃ¹ng kiá»ƒm tra nÃ³:

```py
context = raw_datasets["train"][0]["context"]
question = raw_datasets["train"][0]["question"]

inputs = tokenizer(question, context)
tokenizer.decode(inputs["input_ids"])
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '
'the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin '
'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '
'upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred '
'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '
'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '
'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '
'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

CÃ¡c nhÃ£n sau Ä‘Ã³ sáº½ lÃ  chá»‰ má»¥c cá»§a cÃ¡c token báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cÃ¢u tráº£ lá»i vÃ  mÃ´ hÃ¬nh sáº½ cÃ³ nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n má»™t logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cho má»—i token trong Ä‘áº§u vÃ o, vá»›i cÃ¡c nhÃ£n lÃ½ thuyáº¿t nhÆ° sau:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels-dark.svg" alt="One-hot encoded labels for question answering."/>
</div>

Trong trÆ°á»ng há»£p nÃ y, ngá»¯ cáº£nh khÃ´ng quÃ¡ dÃ i, nhÆ°ng má»™t sá»‘ máº«u trong táº­p dá»¯ liá»‡u cÃ³ ngá»¯ cáº£nh ráº¥t dÃ i sáº½ vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i tá»‘i Ä‘a mÃ  chÃºng tÃ´i Ä‘áº·t (trong trÆ°á»ng há»£p nÃ y lÃ  384). NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 6](/course/chapter6/4) khi chÃºng ta khÃ¡m phÃ¡ pháº§n bÃªn trong cá»§a pipeline `question-answering`, chÃºng ta sáº½ Ä‘á»‘i phÃ³ vá»›i cÃ¡c ngá»¯ cáº£nh dÃ i báº±ng cÃ¡ch táº¡o má»™t sá»‘ Ä‘áº·c trÆ°ng huáº¥n luyá»‡n tá»« má»™t máº«u táº­p dá»¯ liá»‡u cá»§a mÃ¬nh, vá»›i cá»­a sá»• trÆ°á»£t giá»¯a chÃºng.

Äá»ƒ xem cÃ¡ch nÃ y hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o báº±ng cÃ¡ch sá»­ dá»¥ng vÃ­ dá»¥ hiá»‡n táº¡i, chÃºng ta cÃ³ thá»ƒ giá»›i háº¡n Ä‘á»™ dÃ i á»Ÿ 100 vÃ  sá»­ dá»¥ng cá»­a sá»• trÆ°á»£t gá»“m 50 token. Xin nháº¯c láº¡i, chÃºng ta sá»­ dá»¥ng:

- `max_length` Ä‘á»ƒ Ä‘áº·t Ä‘á»™ dÃ i tá»‘i Ä‘a (á»Ÿ Ä‘Ã¢y lÃ  100)
- `truncation="only_second"` Ä‘á»ƒ cáº¯t ngáº¯n ngá»¯ cáº£nh (á»Ÿ vá»‹ trÃ­ thá»© hai) khi cÃ¢u há»i cÃ³ ngá»¯ cáº£nh quÃ¡ dÃ i
- `stride` Ä‘á»ƒ Ä‘áº·t sá»‘ lÆ°á»£ng token chá»“ng chÃ©o giá»¯a hai pháº§n liÃªn tiáº¿p (á»Ÿ Ä‘Ã¢y lÃ  50)
- `return_overflowing_tokens=True` Ä‘á»ƒ cho trÃ¬nh tokenizer biáº¿t chÃºng ta muá»‘n cÃ¡c token trÃ n

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, vÃ­ dá»¥ cá»§a chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c chia thÃ nh bá»‘n Ä‘áº§u vÃ o, má»—i Ä‘áº§u vÃ o chá»©a cÃ¢u há»i vÃ  má»™t sá»‘ pháº§n cá»§a ngá»¯ cáº£nh. LÆ°u Ã½ ráº±ng cÃ¢u tráº£ lá»i cho cÃ¢u há»i ("Bernadette Soubirous") chá»‰ xuáº¥t hiá»‡n trong Ä‘áº§u vÃ o thá»© ba vÃ  cuá»‘i cÃ¹ng, vÃ¬ váº­y báº±ng cÃ¡ch xá»­ lÃ½ cÃ¡c ngá»¯ cáº£nh dÃ i theo cÃ¡ch nÃ y, chÃºng ta sáº½ táº¡o má»™t sá»‘ máº«u huáº¥n luyá»‡n trong Ä‘Ã³ cÃ¢u tráº£ lá»i khÃ´ng Ä‘Æ°á»£c Ä‘Æ°a vÃ o ngá»¯ cáº£nh. Äá»‘i vá»›i nhá»¯ng vÃ­ dá»¥ Ä‘Ã³, nhÃ£n sáº½ lÃ  `start_position = end_position = 0` (vÃ¬ váº­y chÃºng tÃ´i dá»± Ä‘oÃ¡n token `[CLS]`). ChÃºng ta cÅ©ng sáº½ Ä‘áº·t cÃ¡c nhÃ£n Ä‘Ã³ trong trÆ°á»ng há»£p khÃ´ng may khi cÃ¢u tráº£ lá»i Ä‘Ã£ bá»‹ cáº¯t bá»›t Ä‘á»ƒ chÃºng ta chá»‰ cÃ³ pháº§n Ä‘áº§u (hoáº·c pháº§n cuá»‘i) cá»§a cÃ¢u tráº£ lá»i. Äá»‘i vá»›i cÃ¡c vÃ­ dá»¥ trong Ä‘Ã³ cÃ¢u tráº£ lá»i náº±m Ä‘áº§y Ä‘á»§ trong ngá»¯ cáº£nh, cÃ¡c nhÃ£n sáº½ lÃ  chá»‰ má»¥c cá»§a token nÆ¡i cÃ¢u tráº£ lá»i báº¯t Ä‘áº§u vÃ  chá»‰ má»¥c cá»§a token nÆ¡i cÃ¢u tráº£ lá»i káº¿t thÃºc.

Táº­p dá»¯ liá»‡u cung cáº¥p cho chÃºng ta kÃ½ tá»± báº¯t Ä‘áº§u cá»§a cÃ¢u tráº£ lá»i trong ngá»¯ cáº£nh vÃ  báº±ng cÃ¡ch thÃªm Ä‘á»™ dÃ i cá»§a cÃ¢u tráº£ lá»i, chÃºng ta cÃ³ thá»ƒ tÃ¬m tháº¥y kÃ½ tá»± káº¿t thÃºc trong ngá»¯ cáº£nh. Äá»ƒ Ã¡nh xáº¡ chÃºng vá»›i cÃ¡c chá»‰ sá»‘ token, chÃºng ta sáº½ cáº§n sá»­ dá»¥ng Ã¡nh xáº¡ offset mÃ  chÃºng ta Ä‘Ã£ nghiÃªn cá»©u trong [ChÆ°Æ¡ng 6](/course/chapter6/4). ChÃºng ta cÃ³ thá»ƒ yÃªu cáº§u tokenizer tráº£ láº¡i nhá»¯ng thá»© nÃ y báº±ng cÃ¡ch truyá»n theo `return_offsets_mapping=True`:

```py
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
inputs.keys()
```

```python out
dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, chÃºng ta láº¥y láº¡i cÃ¡c ID Ä‘áº§u vÃ o thÃ´ng thÆ°á»ng, token ID vÃ  attention mask, cÅ©ng nhÆ° Ã¡nh xáº¡ offset mÃ  chÃºng ta yÃªu cáº§u vÃ  má»™t khÃ³a bá»• sung, `overflow_to_sample_mapping`. GiÃ¡ trá»‹ tÆ°Æ¡ng á»©ng sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng cho chÃºng ta khi tokenize nhiá»u vÄƒn báº£n cÃ¹ng má»™t lÃºc (chÃºng ta nÃªn lÃ m Ä‘á»ƒ hÆ°á»Ÿng lá»£i tá»« thá»±c táº¿ lÃ  trÃ¬nh tokenizer Ä‘Æ°á»£c há»— trá»£ bá»Ÿi Rust). VÃ¬ má»™t máº«u cÃ³ thá»ƒ cung cáº¥p má»™t sá»‘ Ä‘á»‘i tÆ°á»£ng Ä‘á»‹a lÃ½, nÃªn nÃ³ Ã¡nh xáº¡ tá»«ng Ä‘á»‘i tÆ°á»£ng Ä‘á»‹a lÃ½ vá»›i vÃ­ dá»¥ mÃ  nÃ³ cÃ³ nguá»“n gá»‘c. Bá»Ÿi vÃ¬ á»Ÿ Ä‘Ã¢y chÃºng ta chá»‰ tokenize má»™t vÃ­ dá»¥, chÃºng ta nháº­n Ä‘Æ°á»£c danh sÃ¡ch cÃ¡c `0`:

```py
inputs["overflow_to_sample_mapping"]
```

```python out
[0, 0, 0, 0]
```

NhÆ°ng náº¿u chÃºng ta mÃ£ hÃ³a nhiá»u máº«u hÆ¡n, Ä‘iá»u nÃ y sáº½ trá»Ÿ nÃªn há»¯u Ã­ch hÆ¡n:

```py
inputs = tokenizer(
    raw_datasets["train"][2:6]["question"],
    raw_datasets["train"][2:6]["context"],
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)

print(f"The 4 examples gave {len(inputs['input_ids'])} features.")
print(f"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.")
```

```python out
'The 4 examples gave 19 features.'
'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'
```

NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, ba máº«u Ä‘áº§u tiÃªn (táº¡i chá»‰ sá»‘ 2, 3 vÃ  4 trong táº­p huáº¥n luyá»‡n) má»—i máº«u Ä‘Æ°a ra bá»‘n Ä‘áº·c trÆ°ng vÃ  máº«u cuá»‘i cÃ¹ng (táº¡i chá»‰ má»¥c 5 trong táº­p huáº¥n luyá»‡n) Ä‘Æ°a ra 7 Ä‘áº·c trÆ°ng.

ThÃ´ng tin nÃ y sáº½ há»¯u Ã­ch Ä‘á»ƒ Ã¡nh xáº¡ tá»«ng Ä‘á»‘i tÆ°á»£ng mÃ  chÃºng ta nháº­n Ä‘Æ°á»£c vá»›i nhÃ£n tÆ°Æ¡ng á»©ng cá»§a nÃ³. NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, cÃ¡c nhÃ£n Ä‘Ã³ lÃ :

- `(0, 0)` náº¿u cÃ¢u tráº£ lá»i khÃ´ng náº±m trong khoáº£ng tÆ°Æ¡ng á»©ng cá»§a ngá»¯ cáº£nh
- `(start_position, end_position)` náº¿u cÃ¢u tráº£ lá»i náº±m trong khoáº£ng tÆ°Æ¡ng á»©ng cá»§a ngá»¯ cáº£nh, vá»›i `start_position` lÃ  chá»‰ má»¥c cá»§a token (trong cÃ¡c ID Ä‘áº§u vÃ o) á»Ÿ Ä‘áº§u cÃ¢u tráº£ lá»i vÃ  `end_position` lÃ  chá»‰ má»¥c cá»§a token (trong cÃ¡c ID Ä‘áº§u vÃ o) nÆ¡i cÃ¢u tráº£ lá»i káº¿t thÃºc.

Äá»ƒ xÃ¡c Ä‘á»‹nh Ä‘Ã¢y lÃ  trÆ°á»ng há»£p nÃ o vÃ  náº¿u cÃ³ liÃªn quan, vá»‹ trÃ­ cá»§a cÃ¡c token, trÆ°á»›c tiÃªn chÃºng ta tÃ¬m cÃ¡c chá»‰ sá»‘ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc ngá»¯ cáº£nh trong cÃ¡c ID Ä‘áº§u vÃ o. ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c token ID Ä‘á»ƒ thá»±c hiá»‡n viá»‡c nÃ y, nhÆ°ng vÃ¬ chÃºng khÃ´ng nháº¥t thiáº¿t pháº£i tá»“n táº¡i cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh (vÃ­ dá»¥: DistilBERT khÃ´ng yÃªu cáº§u chÃºng), thay vÃ o Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `sequence_ids()` cá»§a `BatchEncoding` mÃ  tokenizer cá»§a ta tráº£ vá».

Khi ta cÃ³ cÃ¡c chá»‰ má»¥c token Ä‘Ã³, chÃºng ta xem xÃ©t cÃ¡c offset, lÃ  cÃ¡c bá»™ giÃ¡ trá»‹ cá»§a hai sá»‘ nguyÃªn Ä‘áº¡i diá»‡n cho khoáº£ng kÃ½ tá»± bÃªn trong ngá»¯ cáº£nh ban Ä‘áº§u. Do Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ phÃ¡t hiá»‡n xem Ä‘oáº¡n ngá»¯ cáº£nh trong Ä‘áº·c trÆ°ng nÃ y báº¯t Ä‘áº§u sau cÃ¢u tráº£ lá»i hay káº¿t thÃºc trÆ°á»›c khi cÃ¢u tráº£ lá»i báº¯t Ä‘áº§u (trong trÆ°á»ng há»£p Ä‘Ã³ nhÃ£n lÃ  `(0, 0)`). Náº¿u khÃ´ng pháº£i nhÆ° váº­y, chÃºng ta láº·p láº¡i Ä‘á»ƒ tÃ¬m mÃ£ token Ä‘áº§u tiÃªn vÃ  cuá»‘i cÃ¹ng cá»§a cÃ¢u tráº£ lá»i:

```py
answers = raw_datasets["train"][2:6]["answers"]
start_positions = []
end_positions = []

for i, offset in enumerate(inputs["offset_mapping"]):
    sample_idx = inputs["overflow_to_sample_mapping"][i]
    answer = answers[sample_idx]
    start_char = answer["answer_start"][0]
    end_char = answer["answer_start"][0] + len(answer["text"][0])
    sequence_ids = inputs.sequence_ids(i)

    # TÃ¬m Ä‘iá»ƒm báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a ngá»¯ cáº£nh
    while sequence_ids[idx] != 1:
        idx += 1
    context_start = idx
    while sequence_ids[idx] == 1:
        idx += 1
    context_end = idx - 1

    # Náº¿u cÃ¢u tráº£ lá»i khÃ´ng hoÃ n toÃ n náº±m trong ngá»¯ cáº£nh, nhÃ£n lÃ  (0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # Náº¿u khÃ´ng nÃ³ sáº½ lÃ  vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python out
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

HÃ£y cÃ¹ng xem má»™t vÃ i káº¿t quáº£ Ä‘á»ƒ xÃ¡c minh ráº±ng cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng ta lÃ  Ä‘Ãºng. Äá»‘i vá»›i Ä‘áº·c trÆ°ng Ä‘áº§u tiÃªn chÃºng ta tÃ¬m tháº¥y `(83, 85)` dÆ°á»›i dáº¡ng nhÃ£n, hÃ£y so sÃ¡nh cÃ¢u tráº£ lá»i lÃ½ thuyáº¿t vá»›i khoáº£ng token Ä‘Æ°á»£c giáº£i mÃ£ tá»« 83 Ä‘áº¿n 85 (bao gá»“m):

```py
idx = 0
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs["input_ids"][idx][start : end + 1])

print(f"Theoretical answer: {answer}, labels give: {labeled_answer}")
```

```python out
'Theoretical answer: the Main Building, labels give: the Main Building'
```

Káº¿t quáº£ khÃ¡ lÃ  khá»›p nhau! BÃ¢y giá» chÃºng ta hÃ£y kiá»ƒm tra chá»‰ má»¥c 4, nÆ¡i chÃºng ta Ä‘áº·t nhÃ£n thÃ nh `(0, 0)`, cÃ³ nghÄ©a lÃ  cÃ¢u tráº£ lá»i khÃ´ng náº±m trong pháº§n ngá»¯ cáº£nh cá»§a Ä‘áº·c trÆ°ng Ä‘Ã³:

```py
idx = 4
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

decoded_example = tokenizer.decode(inputs["input_ids"][idx])
print(f"Theoretical answer: {answer}, decoded example: {decoded_example}")
```

```python out
'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'
```

</Tip>

BÃ¢y giá» chÃºng ta Ä‘Ã£ tháº¥y tá»«ng bÆ°á»›c cÃ¡ch tiá»n xá»­ lÃ½ dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a mÃ¬nh, chÃºng ta cÃ³ thá»ƒ nhÃ³m nÃ³ trong má»™t hÃ m mÃ  ta sáº½ Ã¡p dá»¥ng trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n. ChÃºng ta sáº½ Ä‘á»‡m má»i Ä‘áº·c trÆ°ng Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a mÃ  ta Ä‘Ã£ Ä‘áº·t, vÃ¬ háº§u háº¿t cÃ¡c ngá»¯ cáº£nh sáº½ dÃ i (vÃ  cÃ¡c máº«u tÆ°Æ¡ng á»©ng sáº½ Ä‘Æ°á»£c chia thÃ nh nhiá»u Ä‘áº·c trÆ°ng), vÃ¬ váº­y khÃ´ng cÃ³ lá»£i Ã­ch thá»±c sá»± nÃ o khi Ã¡p dá»¥ng Ä‘á»‡m Ä‘á»™ng á»Ÿ Ä‘Ã¢y:

Tháº­t váº­y, chÃºng ta khÃ´ng tháº¥y cÃ¢u tráº£ lá»i bÃªn trong ngá»¯ cáº£nh.

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** Khi sá»­ dá»¥ng kiáº¿n trÃºc XLNet, pháº§n Ä‘á»‡m Ä‘Æ°á»£c Ã¡p dá»¥ng á»Ÿ bÃªn trÃ¡i vÃ  cÃ¢u há»i vÃ  ngá»¯ cáº£nh Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i. Äiá»u chá»‰nh táº¥t cáº£ mÃ£ chÃºng ta vá»«a tháº¥y vá»›i kiáº¿n trÃºc XLNet (vÃ  thÃªm `padding=True`). LÆ°u Ã½ ráº±ng token `[CLS]` cÃ³ thá»ƒ khÃ´ng á»Ÿ vá»‹ trÃ­ 0 khi Ã¡p dá»¥ng pháº§n Ä‘á»‡m.

</Tip>

BÃ¢y giá» chÃºng ta Ä‘Ã£ tháº¥y tá»«ng bÆ°á»›c cÃ¡ch tiá»n xá»­ lÃ½ dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a mÃ¬nh, chÃºng ta cÃ³ thá»ƒ nhÃ³m nÃ³ trong má»™t hÃ m mÃ  chÃºng ta sáº½ Ã¡p dá»¥ng trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n. ChÃºng ta sáº½ Ä‘á»‡m má»i Ä‘áº·c trÆ°ng Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a mÃ  chÃºng ta Ä‘Ã£ Ä‘áº·t, vÃ¬ háº§u háº¿t cÃ¡c ngá»¯ cáº£nh sáº½ dÃ i (vÃ  cÃ¡c máº«u tÆ°Æ¡ng á»©ng sáº½ Ä‘Æ°á»£c chia thÃ nh nhiá»u Ä‘áº·c trÆ°ng), vÃ¬ váº­y khÃ´ng cÃ³ lá»£i Ã­ch thá»±c sá»± nÃ o khi Ã¡p dá»¥ng Ä‘á»‡m Ä‘á»™ng á»Ÿ Ä‘Ã¢y:

```py
max_length = 384
stride = 128


def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # TÃ¬m Ä‘iá»ƒm báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a ngá»¯ cáº£nh
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # Náº¿u cÃ¢u tráº£ lá»i khÃ´ng hoÃ n toÃ n náº±m trong ngá»¯ cáº£nh, nhÃ£n lÃ  (0, 0)
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Náº¿u khÃ´ng nÃ³ sáº½ lÃ  vá»‹ trÃ­ token báº¯t Ä‘áº§u vÃ  káº¿t thÃºc
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs
```

LÆ°u Ã½ ráº±ng chÃºng ta Ä‘Ã£ xÃ¡c Ä‘á»‹nh hai háº±ng sá»‘ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ dÃ i tá»‘i Ä‘a Ä‘Æ°á»£c sá»­ dá»¥ng cÅ©ng nhÆ° Ä‘á»™ dÃ i cá»§a cá»­a sá»• trÆ°á»£t vÃ  ta Ä‘Ã£ thÃªm má»™t chÃºt dá»n dáº¹p trÆ°á»›c khi tokenize: má»™t sá»‘ cÃ¢u há»i trong táº­p dá»¯ liá»‡u SQuAD cÃ³ thÃªm khoáº£ng tráº¯ng á»Ÿ Ä‘áº§u vÃ  káº¿t thÃºc mÃ  khÃ´ng thÃªm báº¥t ká»³ thá»© gÃ¬ (vÃ  chiáº¿m dung lÆ°á»£ng khi Ä‘Æ°á»£c tokenize náº¿u báº¡n sá»­ dá»¥ng mÃ´ hÃ¬nh nhÆ° RoBERTa), vÃ¬ váº­y ta Ä‘Ã£ xÃ³a nhá»¯ng khoáº£ng tráº¯ng thá»«a Ä‘Ã³.

Äá»ƒ Ã¡p dá»¥ng hÃ m nÃ y cho toÃ n bá»™ táº­p huáº¥n luyá»‡n, chÃºng ta sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `Dataset.map()` vá»›i `batched=True`. Äiá»u nÃ y cáº§n thiáº¿t á»Ÿ Ä‘Ã¢y vÃ¬ ta Ä‘ang thay Ä‘á»•i Ä‘á»™ dÃ i cá»§a táº­p dá»¯ liá»‡u (vÃ¬ má»™t máº«u cÃ³ thá»ƒ cung cáº¥p má»™t sá»‘ Ä‘áº·c trÆ°ng huáº¥n luyá»‡n):

```py
train_dataset = raw_datasets["train"].map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
len(raw_datasets["train"]), len(train_dataset)
```

```python out
(87599, 88729)
```

NhÆ° ta cÃ³ thá»ƒ tháº¥y, quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ Ä‘Ã£ thÃªm khoáº£ng 1,000 Ä‘áº·c trÆ°ng. Bá»™ huáº¥n luyá»‡n hiá»‡n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ sá»­ dá»¥ng - hÃ£y cÃ¹ng tÃ¬m hiá»ƒu vá» quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ cá»§a bá»™ kiá»ƒm Ä‘á»‹nh!

### Xá»­ lÃ½ dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh

Viá»‡c xá»­ lÃ½ trÆ°á»›c dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh sáº½ dá»… dÃ ng hÆ¡n má»™t chÃºt vÃ¬ chÃºng ta khÃ´ng cáº§n táº¡o nhÃ£n (trá»« khi chÃºng ta muá»‘n tÃ­nh toÃ¡n máº¥t mÃ¡t kiá»ƒm Ä‘á»‹nh, nhÆ°ng con sá»‘ Ä‘Ã³ sáº½ khÃ´ng thá»±c sá»± giÃºp chÃºng ta hiá»ƒu mÃ´ hÃ¬nh tá»‘t nhÆ° tháº¿ nÃ o). Niá»m vui thá»±c sá»± sáº½ lÃ  diá»…n giáº£i cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh thÃ nh cÃ¡c khoáº£ng cá»§a bá»‘i cáº£nh ban Ä‘áº§u. Äá»‘i vá»›i Ä‘iá»u nÃ y, chÃºng ta sáº½ chá»‰ cáº§n lÆ°u trá»¯ cáº£ Ã¡nh xáº¡ offset vÃ  má»™t sá»‘ cÃ¡ch Ä‘á»ƒ khá»›p tá»«ng Ä‘á»‘i tÆ°á»£ng Ä‘Ã£ táº¡o vá»›i vÃ­ dá»¥ ban Ä‘áº§u mÃ  nÃ³ xuáº¥t phÃ¡t. VÃ¬ cÃ³ má»™t cá»™t ID trong táº­p dá»¯ liá»‡u gá»‘c, chÃºng ta sáº½ sá»­ dá»¥ng ID Ä‘Ã³.

Äiá»u duy nháº¥t chÃºng ta sáº½ thÃªm á»Ÿ Ä‘Ã¢y lÃ  má»™t chÃºt dá»n dáº¹p cÃ¡c Ã¡nh xáº¡ offset. ChÃºng sáº½ chá»©a cÃ¡c pháº§n bÃ¹ cho cÃ¢u há»i vÃ  ngá»¯ cáº£nh, nhÆ°ng khi chÃºng ta Ä‘ang á»Ÿ giai Ä‘oáº¡n háº­u xá»­ lÃ½, chÃºng ta sáº½ khÃ´ng cÃ³ cÃ¡ch nÃ o Ä‘á»ƒ biáº¿t pháº§n nÃ o cá»§a ID Ä‘áº§u vÃ o tÆ°Æ¡ng á»©ng vá»›i ngá»¯ cáº£nh vÃ  pháº§n nÃ o lÃ  cÃ¢u há»i (phÆ°Æ¡ng thá»©c `sequence_ids()` ta Ä‘Ã£ sá»­ dá»¥ng chá»‰ cÃ³ sáºµn cho Ä‘áº§u ra cá»§a tokenizer). VÃ¬ váº­y, chÃºng ta sáº½ Ä‘áº·t cÃ¡c offset tÆ°Æ¡ng á»©ng vá»›i cÃ¢u há»i thÃ nh `None`:

```py
def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs
```

ChÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng hÃ m nÃ y trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh nhÆ° trÆ°á»›c Ä‘Ã¢y:

```py
validation_dataset = raw_datasets["validation"].map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
len(raw_datasets["validation"]), len(validation_dataset)
```

```python out
(10570, 10822)
```

Trong trÆ°á»ng há»£p nÃ y, chÃºng ta chá»‰ thÃªm má»™t vÃ i trÄƒm máº«u, vÃ¬ váº­y cÃ³ váº» nhÆ° cÃ¡c ngá»¯ cáº£nh trong táº­p dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh ngáº¯n hÆ¡n má»™t chÃºt.

BÃ¢y giá» chÃºng ta Ä‘Ã£ tiá»n xá»­ lÃ½ táº¥t cáº£ dá»¯ liá»‡u, chÃºng ta cÃ³ thá»ƒ tham gia khÃ³a huáº¥n luyá»‡n.

{#if fw === 'pt'}

## Tinh chá»‰nh mÃ´ hÃ¬n vá»›i API `Trainer`

Äoáº¡n mÃ£ huáº¥n luyá»‡n cho máº«u nÃ y sáº½ trÃ´ng ráº¥t giá»‘ng trong cÃ¡c pháº§n trÆ°á»›c - Ä‘iá»u khÃ³ nháº¥t sáº½ lÃ  viáº¿t hÃ m `compute_metrics()`. VÃ¬ chÃºng ta Ä‘Ã£ Ä‘á»‡m táº¥t cáº£ cÃ¡c máº«u Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a mÃ  ta Ä‘áº·t, khÃ´ng cÃ³ cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘á»ƒ xÃ¡c Ä‘á»‹nh, vÃ¬ váº­y viá»‡c tÃ­nh toÃ¡n sá»‘ liá»‡u nÃ y thá»±c sá»± lÃ  Ä‘iá»u duy nháº¥t chÃºng ta pháº£i lo láº¯ng. Pháº§n khÃ³ khÄƒn sáº½ lÃ  háº­u xá»­ lÃ½ cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh thÃ nh cÃ¡c khoáº£ng vÄƒn báº£n trong cÃ¡c vÃ­ dá»¥ ban Ä‘áº§u; khi ta Ä‘Ã£ lÃ m Ä‘iá»u Ä‘Ã³, chá»‰ sá»‘ tá»« thÆ° viá»‡n ğŸ¤— Datasets sáº½ thá»±c hiá»‡n háº§u háº¿t cÃ´ng viá»‡c cho mÃ¬nh.

{:else}

## Tinh chá»‰nh mÃ´ hÃ¬n vá»›i Keras

Äoáº¡n mÃ£ huáº¥n luyá»‡n cho máº«u nÃ y sáº½ trÃ´ng ráº¥t giá»‘ng trong cÃ¡c pháº§n trÆ°á»›c, nhÆ°ng viá»‡c tÃ­nh toÃ¡n cÃ¡c sá»‘ liá»‡u sáº½ lÃ  má»™t thá»­ thÃ¡ch Ä‘á»™c Ä‘Ã¡o. VÃ¬ chÃºng ta Ä‘Ã£ Ä‘á»‡m táº¥t cáº£ cÃ¡c máº«u Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a mÃ  chÃºng ta Ä‘áº·t, khÃ´ng cÃ³ cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u Ä‘á»ƒ xÃ¡c Ä‘á»‹nh, vÃ¬ váº­y viá»‡c tÃ­nh toÃ¡n sá»‘ liá»‡u nÃ y thá»±c sá»± lÃ  Ä‘iá»u duy nháº¥t ta pháº£i lo láº¯ng. Pháº§n khÃ³ sáº½ lÃ  háº­u xá»­ lÃ½ cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh thÃ nh cÃ¡c khoáº£ng vÄƒn báº£n trong cÃ¡c vÃ­ dá»¥ ban Ä‘áº§u; khi chÃºng ta Ä‘Ã£ lÃ m Ä‘iá»u Ä‘Ã³, chá»‰ sá»‘ tá»« thÆ° viá»‡n ğŸ¤— Datasets sáº½ thá»±c hiá»‡n háº§u háº¿t cÃ´ng viá»‡c cho ta.

{/if}

### Háº­u xá»­ lÃ½

{#if fw === 'pt'}

<Youtube id="BNy08iIWVJM"/>

{:else}

<Youtube id="VN67ZpN33Ss"/>

{/if}

MÃ´ hÃ¬nh sáº½ tráº£ vá» cÃ¡c logit Ä‘áº§u ra cho cÃ¡c vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a cÃ¢u tráº£ lá»i trong ID Ä‘áº§u vÃ o, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong quÃ¡ trÃ¬nh khÃ¡m phÃ¡ pipeline [`question-answering`](/course/chapter6/3b). BÆ°á»›c tiá»n xá»­ lÃ½ sáº½ tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ lÃ m á»Ÿ Ä‘Ã³, vÃ¬ váº­y Ä‘Ã¢y lÃ  lá»i nháº¯c nhanh vá» cÃ¡c bÆ°á»›c chÃºng ta Ä‘Ã£ thá»±c hiá»‡n:

- ChÃºng ta Ä‘Ã£ che cÃ¡c logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc tÆ°Æ¡ng á»©ng vá»›i cÃ¡c token bÃªn ngoÃ i ngá»¯ cáº£nh.
- Sau Ä‘Ã³, chÃºng ta chuyá»ƒn Ä‘á»•i cÃ¡c logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc thÃ nh xÃ¡c suáº¥t báº±ng cÃ¡ch sá»­ dá»¥ng softmax.
- ChÃºng ta quy Ä‘iá»ƒm cho tá»«ng cáº·p `(start_token, end_token)` cÃ¡ch láº¥y tÃ­ch cá»§a hai xÃ¡c suáº¥t tÆ°Æ¡ng á»©ng.
- ChÃºng ta Ä‘Ã£ tÃ¬m kiáº¿m cáº·p cÃ³ Ä‘iá»ƒm tá»‘i Ä‘a mang láº¡i cÃ¢u tráº£ lá»i há»£p lá»‡ (vÃ­ dá»¥: `start_token` tháº¥p hÆ¡n `end_token`).

á» Ä‘Ã¢y, chÃºng ta sáº½ thay Ä‘á»•i quy trÃ¬nh nÃ y má»™t chÃºt vÃ¬ chÃºng ta khÃ´ng cáº§n tÃ­nh Ä‘iá»ƒm thá»±c táº¿ (chá»‰ lÃ  cÃ¢u tráº£ lá»i dá»± Ä‘oÃ¡n). Äiá»u nÃ y cÃ³ nghÄ©a lÃ  chÃºng ta cÃ³ thá»ƒ bá» qua bÆ°á»›c softmax. Äá»ƒ Ä‘i nhanh hÆ¡n, chÃºng ta cÅ©ng sáº½ khÃ´ng tÃ­nh Ä‘iá»ƒm táº¥t cáº£ cÃ¡c cáº·p `(start_token, end_token)` cÃ³ thá»ƒ, mÃ  chá»‰ nhá»¯ng cáº·p tÆ°Æ¡ng á»©ng vá»›i logit `n_best` cao nháº¥t (vá»›i `n_best = 20`). VÃ¬ chÃºng ta sáº½ bá» qua softmax, nhá»¯ng Ä‘iá»ƒm Ä‘Ã³ sáº½ lÃ  Ä‘iá»ƒm logit vÃ  sáº½ cÃ³ Ä‘Æ°á»£c báº±ng cÃ¡ch láº¥y tá»•ng cá»§a logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc (thay vÃ¬ nhÃ¢n, vÃ¬ quy táº¯c \(\log(ab) = \log(a) + \log(b)\\)).

Äá»ƒ chá»©ng minh táº¥t cáº£ nhá»¯ng Ä‘iá»u nÃ y, chÃºng ta sáº½ cáº§n má»™t sá»‘ loáº¡i dá»± Ä‘oÃ¡n. VÃ¬ chÃºng ta chÆ°a huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh, chÃºng ta sáº½ sá»­ dá»¥ng mÃ´ hÃ¬nh máº·c Ä‘á»‹nh cho pipeline QA Ä‘á»ƒ táº¡o ra má»™t sá»‘ dá»± Ä‘oÃ¡n trÃªn má»™t pháº§n nhá» cá»§a táº­p há»£p kiá»ƒm. ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng chá»©c nÄƒng xá»­ lÃ½ tÆ°Æ¡ng tá»± nhÆ° trÆ°á»›c Ä‘Ã¢y; bá»Ÿi vÃ¬ nÃ³ dá»±a vÃ o háº±ng sá»‘ toÃ n cá»¥c `tokenizer`, chÃºng ta chá»‰ cáº§n thay Ä‘á»•i Ä‘á»‘i tÆ°á»£ng Ä‘Ã³ thÃ nh tokenizer cá»§a mÃ´ hÃ¬nh mÃ  chÃºng ta muá»‘n sá»­ dá»¥ng táº¡m thá»i:

```python
small_eval_set = raw_datasets["validation"].select(range(100))
trained_checkpoint = "distilbert-base-cased-distilled-squad"

tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)
eval_set = small_eval_set.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
```

BÃ¢y giá», quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ Ä‘Ã£ hoÃ n táº¥t, chÃºng ta thay Ä‘á»•i tokenizer trá»Ÿ láº¡i cÃ¡i mÃ  chÃºng ta Ä‘Ã£ chá»n ban Ä‘áº§u:

```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Sau Ä‘Ã³, chÃºng ta loáº¡i bá» cÃ¡c cá»™t cá»§a `eval_set` mÃ  mÃ´ hÃ¬nh khÃ´ng mong Ä‘á»£i, xÃ¢y dá»±ng má»™t lÃ´ vá»›i táº¥t cáº£ bá»™ kiá»ƒm Ä‘á»‹nh nhá» Ä‘Ã³ vÃ  chuyá»ƒn nÃ³ qua mÃ´ hÃ¬nh. Náº¿u cÃ³ sáºµn GPU, chÃºng ta sá»­ dá»¥ng nÃ³ Ä‘á»ƒ cháº¡y nhanh hÆ¡n:

{#if fw === 'pt'}

```python
import torch
from transformers import AutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}
trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(
    device
)

with torch.no_grad():
    outputs = trained_model(**batch)
```

VÃ¬ `Trainer` sáº½ tráº£ cho ta cÃ¡c dá»± Ä‘oÃ¡n dÆ°á»›i dáº¡ng máº£ng NumPy, ta sáº½ láº¥y cÃ¡c logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc vÃ  chuyá»ƒn nÃ³ thÃ nh dáº¡ng:

```python
start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()
```

{:else}

```python
import tensorflow as tf
from transformers import TFAutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("numpy")

batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}
trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)

outputs = trained_model(**batch)
```

Äá»ƒ dá»… dÃ ng thá»­ nghiá»‡m, hÃ£y chuyá»ƒn Ä‘á»•i cÃ¡c káº¿t quáº£ Ä‘áº§u ra nÃ y thÃ nh máº£ng NumPy:

```python
start_logits = outputs.start_logits.numpy()
end_logits = outputs.end_logits.numpy()
```

{/if}

BÃ¢y giá», chÃºng ta cáº§n tÃ¬m cÃ¢u tráº£ lá»i dá»± Ä‘oÃ¡n cho tá»«ng vÃ­ dá»¥ trong `small_eval_set` cá»§a chÃºng ta. Má»™t vÃ­ dá»¥ cÃ³ thá»ƒ Ä‘Ã£ Ä‘Æ°á»£c chia thÃ nh nhiá»u Ä‘áº·c trÆ°ng trong `eval_set`, vÃ¬ váº­y bÆ°á»›c Ä‘áº§u tiÃªn lÃ  Ã¡nh xáº¡ tá»«ng máº«u trong `small_eval_set` vá»›i cÃ¡c Ä‘áº·c trÆ°ng tÆ°Æ¡ng á»©ng trong `eval_set`:

```python
import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(eval_set):
    example_to_features[feature["example_id"]].append(idx)
```

Vá»›i Ä‘iá»u nÃ y trong tay, chÃºng ta thá»±c sá»± cÃ³ thá»ƒ báº¯t Ä‘áº§u lÃ m viá»‡c báº±ng cÃ¡ch láº·p láº¡i táº¥t cáº£ cÃ¡c máº«u vÃ , Ä‘á»‘i vá»›i má»—i máº«u, thÃ´ng qua táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng liÃªn quan. NhÆ° chÃºng ta Ä‘Ã£ nÃ³i trÆ°á»›c Ä‘Ã¢y, chÃºng ta sáº½ xem xÃ©t Ä‘iá»ƒm logit cho cÃ¡c logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a `n_best`, ngoáº¡i trá»« cÃ¡c vá»‹ trÃ­ cung cáº¥p:

- Má»™t cÃ¢u tráº£ lá»i sáº½ khÃ´ng náº±m trong ngá»¯ cáº£nh
- Má»™t cÃ¢u tráº£ lá»i cÃ³ Ä‘á»™ dÃ i Ã¢m
- Má»™t cÃ¢u tráº£ lá»i quÃ¡ dÃ i (chÃºng ta giá»›i háº¡n kháº£ nÄƒng á»Ÿ má»©c `max_answer_length=30`)

Khi chÃºng ta cÃ³ táº¥t cáº£ cÃ¡c cÃ¢u tráº£ lá»i cÃ³ thá»ƒ Ä‘Æ°á»£c ghi cho má»™t máº«u, ta chá»‰ cáº§n chá»n má»™t cÃ¢u cÃ³ Ä‘iá»ƒm logit tá»‘t nháº¥t:

```python
import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in small_eval_set:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = eval_set["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # Bá» qua cÃ¡c cÃ¢u tráº£ lá»i khÃ´ng Ä‘áº§u Ä‘á»§ trong ngá»¯ cáº£nh
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # Bá» qua nhá»¯ng cÃ¢u tráº£ lá»i cÃ³ Ä‘á»™ dÃ i < 0 hoáº·c > max_answer_length.
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
```

Äá»‹nh dáº¡ng cuá»‘i cÃ¹ng cá»§a cÃ¡c cÃ¢u tráº£ lá»i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  Ä‘á»‹nh dáº¡ng sáº½ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n theo chá»‰ sá»‘ mÃ  chÃºng ta sáº½ sá»­ dá»¥ng. NhÆ° thÆ°á»ng lá»‡, chÃºng ta cÃ³ thá»ƒ táº£i nÃ³ vá»›i sá»± trá»£ giÃºp cá»§a thÆ° viá»‡n ğŸ¤— Evaluate:

```python
import evaluate

metric = evaluate.load("squad")
```

ThÆ°á»›c Ä‘o nÃ y mong Ä‘á»£i cÃ¡c cÃ¢u tráº£ lá»i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n á»Ÿ Ä‘á»‹nh dáº¡ng mÃ  chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ trÃªn (danh sÃ¡ch cÃ¡c tá»« Ä‘iá»ƒn cÃ³ má»™t khÃ³a cho ID cá»§a máº«u vÃ  má»™t khÃ³a cho vÄƒn báº£n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n) vÃ  cÃ¡c cÃ¢u tráº£ lá»i lÃ½ thuyáº¿t á»Ÿ Ä‘á»‹nh dáº¡ng bÃªn dÆ°á»›i (danh sÃ¡ch cÃ¡c tá»« Ä‘iá»ƒn cÃ³ má»™t khÃ³a cho ID cá»§a máº«u vÃ  má»™t khÃ³a cho cÃ¡c cÃ¢u tráº£ lá»i cÃ³ thá»ƒ cÃ³):


```python
theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]} for ex in small_eval_set
]
```

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra xem ta cÃ³ nháº­n Ä‘Æ°á»£c káº¿t quáº£ há»£p lÃ½ hay khÃ´ng báº±ng cÃ¡ch xem xÃ©t yáº¿u tá»‘ Ä‘áº§u tiÃªn cá»§a cáº£ hai danh sÃ¡ch:

```python
print(predicted_answers[0])
print(theoretical_answers[0])
```

```python out
{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}
```

KhÃ´ng tá»‡ láº¯m! BÃ¢y giá» chÃºng ta hÃ£y xem xÃ©t Ä‘iá»ƒm sá»‘ mÃ  sá»‘ liá»‡u mang láº¡i cho chÃºng ta:

```python
metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

Má»™t láº§n ná»¯a, Ä‘iá»u Ä‘Ã³ khÃ¡ tá»‘t theo [bÃ i bÃ¡o cá»§a nÃ³](https://arxiv.org/abs/1910.01108v2), DistilBERT Ä‘Æ°á»£c tinh chá»‰nh trÃªn SQuAD thu Ä‘Æ°á»£c 79.1 vÃ  86.9 trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u.

{#if fw === 'pt'}

BÃ¢y giá» chÃºng ta hÃ£y Ä‘áº·t má»i thá»© ta vá»«a lÃ m trong má»™t hÃ m `compute_metrics()` mÃ  ta sáº½ sá»­ dá»¥ng trong `Trainer`. ThÃ´ng thÆ°á»ng, hÃ m `compute_metrics()` Ä‘Ã³ chá»‰ nháº­n Ä‘Æ°á»£c má»™t tuple `eval_preds` vá»›i cÃ¡c logit vÃ  nhÃ£n. á» Ä‘Ã¢y chÃºng ta sáº½ cáº§n nhiá»u hÆ¡n má»™t chÃºt, vÃ¬ chÃºng ta pháº£i tÃ¬m trong táº­p dá»¯ liá»‡u cÃ¡c Ä‘áº·c trÆ°ng cho pháº§n bÃ¹ vÃ  trong táº­p dá»¯ liá»‡u cÃ¡c vÃ­ dá»¥ cho cÃ¡c ngá»¯ cáº£nh ban Ä‘áº§u, vÃ¬ váº­y chÃºng ta sáº½ khÃ´ng thá»ƒ sá»­ dá»¥ng chá»©c nÄƒng nÃ y Ä‘á»ƒ nháº­n káº¿t quáº£ Ä‘Ã¡nh giÃ¡ thÆ°á»ng xuyÃªn trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. ChÃºng ta sáº½ chá»‰ sá»­ dá»¥ng nÃ³ khi káº¿t thÃºc khÃ³a huáº¥nl luyá»‡n Ä‘á»ƒ kiá»ƒm tra káº¿t quáº£.

HÃ m `compute_metrics()` nhÃ³m cÃ¡c bÆ°á»›c giá»‘ng nhÆ° trÆ°á»›c; chÃºng ta chá»‰ thÃªm má»™t kiá»ƒm tra nhá» trong trÆ°á»ng há»£p ta khÃ´ng Ä‘Æ°a ra báº¥t ká»³ cÃ¢u tráº£ lá»i há»£p lá»‡ nÃ o (trong trÆ°á»ng há»£p Ä‘Ã³ ta dá»± Ä‘oÃ¡n má»™t chuá»—i trá»‘ng).

{:else}

BÃ¢y giá», hÃ£y Ä‘áº·t má»i thá»© ta vá»«a lÃ m vÃ o má»™t hÃ m `compute_metrics()` mÃ  ta sáº½ sá»­ dá»¥ng sau khi huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh. ChÃºng ta sáº½ cáº§n truyá»n nhiá»u hÆ¡n lÃ  nháº­t kÃ½ Ä‘áº§u ra, vÃ¬ ta pháº£i tÃ¬m trong táº­p dá»¯ liá»‡u cÃ¡c Ä‘áº·c trÆ°ng cho pháº§n offset vÃ  trong táº­p dá»¯ liá»‡u cÃ¡c máº«u cho cÃ¡c ngá»¯ cáº£nh ban Ä‘áº§u:

{/if}

```python
from tqdm.auto import tqdm


def compute_metrics(start_logits, end_logits, features, examples):
    example_to_features = collections.defaultdict(list)
    for idx, feature in enumerate(features):
        example_to_features[feature["example_id"]].append(idx)

    predicted_answers = []
    for example in tqdm(examples):
        example_id = example["id"]
        context = example["context"]
        answers = []

        # Láº·p qua táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng liÃªn quan tá»›i máº«u Ä‘Ã³
        for feature_index in example_to_features[example_id]:
            start_logit = start_logits[feature_index]
            end_logit = end_logits[feature_index]
            offsets = features[feature_index]["offset_mapping"]

            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Bá» qua cÃ¢u tráº£ lá»i khÃ´ng xuáº¥t hiá»‡n hoÃ n toÃ n trong ngá»¯ cáº£nh
                    if offsets[start_index] is None or offsets[end_index] is None:
                        continue
                    # Bá» qua nhá»¯ng cÃ¢u tráº£ lá»i vá»›i Ä‘á»™ dÃ i < 0 hoáº·c > max_answer_length
                    if (
                        end_index < start_index
                        or end_index - start_index + 1 > max_answer_length
                    ):
                        continue

                    answer = {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                    answers.append(answer)

        # Chá»n cÃ¢u tráº£ lá»i cÃ³ Ä‘iá»ƒm cao nháº¥t
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append(
                {"id": example_id, "prediction_text": best_answer["text"]}
            )
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    theoretical_answers = [{"id": ex["id"], "answers": ex["answers"]} for ex in examples]
    return metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

ChÃºng ta cÃ³ thá»ƒ kiá»ƒm tra nÃ³ hoáº¡t Ä‘á»™ng dá»±a trÃªn dá»± Ä‘oÃ¡n cá»§a mÃ¬nh:

```python
compute_metrics(start_logits, end_logits, eval_set, small_eval_set)
```

```python out
{'exact_match': 83.0, 'f1': 88.25}
```

TrÃ´ng khÃ¡ á»•n! BÃ¢y giá» chÃºng ta hÃ£y sá»­ dá»¥ng Ä‘iá»u nÃ y Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh cá»§a mÃ¬nh.

### Tinh chá»‰nh mÃ´ hÃ¬nh

{#if fw === 'pt'}

Giá» ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh. HÃ£y cÅ©ng táº¡o ra nÃ³ sá»­ dá»¥ng lá»›p `AutoModelForQuestionAnswering` nhÆ° trÆ°á»›c Ä‘Ã³:

```python
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{:else}

Giá» ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a mÃ¬nh. HÃ£y cÅ©ng táº¡o ra nÃ³ sá»­ dá»¥ng lá»›p `TFAutoModelForQuestionAnswering` nhÆ° trÆ°á»›c Ä‘Ã³:

```python
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

{/if}

NhÆ° thÆ°á»ng lá»‡, chÃºng ta nháº­n Ä‘Æ°á»£c cáº£nh bÃ¡o ráº±ng má»™t sá»‘ trá»ng sá»‘ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng (cÃ¡c trá»ng sá»‘ tá»« pháº§n Ä‘áº§u huáº¥n luyá»‡n trÆ°á»›c) vÃ  má»™t sá»‘ trá»ng sá»‘ khÃ¡c Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn (cÃ¡c trá»ng sá»‘ cho Ä‘áº§u tráº£ lá»i cÃ¢u há»i). BÃ¢y giá» báº¡n nÃªn quen vá»›i Ä‘iá»u nÃ y, nhÆ°ng Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh nÃ y chÆ°a sáºµn sÃ ng Ä‘á»ƒ sá»­ dá»¥ng vÃ  cáº§n Ä‘Æ°á»£c tinh chá»‰nh - Ä‘iá»u tá»‘t lÃ  chÃºng ta sáº¯p lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³!

Äá»ƒ cÃ³ thá»ƒ Ä‘áº©y mÃ´ hÃ¬nh cá»§a mÃ¬nh lÃªn Hub, chÃºng ta cáº§n Ä‘Äƒng nháº­p vÃ o Hugging Face. Náº¿u báº¡n Ä‘ang cháº¡y Ä‘oáº¡n mÃ£ nÃ y trong notebook, báº¡n cÃ³ thá»ƒ lÃ m nhÆ° váº­y vá»›i hÃ m tiá»‡n Ã­ch sau, hÃ m nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p cá»§a mÃ¬nh:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Náº¿u báº¡n khÃ´ng lÃ m viá»‡c trong notebook, chá»‰ cáº§n nháº­p dÃ²ng sau vÃ o thiáº¿t bá»‹ Ä‘áº§u cuá»‘i cá»§a báº¡n:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Khi Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n, chÃºng ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh `TrainingArguments` cá»§a mÃ¬nh. NhÆ° ta Ä‘Ã£ nÃ³i khi xÃ¡c Ä‘á»‹nh chá»©c nÄƒng cá»§a mÃ¬nh Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘, chÃºng ta sáº½ khÃ´ng thá»ƒ cÃ³ vÃ²ng láº·p Ä‘Ã¡nh giÃ¡ thÆ°á»ng xuyÃªn vÃ¬ Ä‘áº·c trÆ°ng cá»§a hÃ m `compute_metrics()`. ChÃºng ta cÃ³ thá»ƒ viáº¿t lá»›p con cá»§a riÃªng mÃ¬nh vá» `Trainer` Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y (má»™t cÃ¡ch tiáº¿p cáº­n báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y trong [bá»™ lá»‡nh máº«u cho há»i Ä‘Ã¡p](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)), nhÆ°ng hÆ¡i dÃ i cho pháº§n nÃ y. Thay vÃ o Ä‘Ã³, chÃºng ta sáº½ chá»‰ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh khi káº¿t thÃºc huáº¥n luyá»‡n táº¡i Ä‘Ã¢y vÃ  chá»‰ cho báº¡n cÃ¡ch thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡ thÆ°á»ng xuyÃªn trong "VÃ²ng huáº¥n luyá»‡n tÃ¹y chá»‰nh" bÃªn dÆ°á»›i.

ÄÃ¢y thá»±c sá»± lÃ  nÆ¡i API `Trainer` thá»ƒ hiá»‡n cÃ¡c giá»›i háº¡n cá»§a nÃ³ vÃ  lÃ  lÃºc thÆ° viá»‡n ğŸ¤— Accelerate tá»a sÃ¡ng: viá»‡c tÃ¹y chá»‰nh lá»›p cho má»™t trÆ°á»ng há»£p sá»­ dá»¥ng cá»¥ thá»ƒ cÃ³ thá»ƒ gÃ¢y khÃ³ khÄƒn, nhÆ°ng viá»‡c Ä‘iá»u chá»‰nh má»™t vÃ²ng huáº¥n luyá»‡n Ä‘Æ°á»£c tiáº¿p xÃºc hoÃ n toÃ n ráº¥t dá»… dÃ ng.

ChÃºng ta hÃ£y xem xÃ©t cÃ¡c `TrainingArguments` cá»§a mÃ¬nh:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-squad",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)
```

ChÃºng ta Ä‘Ã£ tháº¥y háº§u háº¿t nhá»¯ng Ä‘iá»u nÃ y trÆ°á»›c Ä‘Ã¢y: chÃºng ta Ä‘áº·t má»™t sá»‘ siÃªu tham sá»‘ (nhÆ° tá»‘c Ä‘á»™ há»c, sá»‘ epoch ta dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n vÃ  má»™t sá»‘ phÃ¢n rÃ£ trá»ng sá»‘) vÃ  cho biáº¿t ráº±ng chÃºng ta muá»‘n lÆ°u mÃ´ hÃ¬nh vÃ o cuá»‘i má»—i epoch, bá» qua Ä‘Ã¡nh giÃ¡ vÃ  táº£i káº¿t quáº£ cá»§a mÃ¬nh lÃªn Model Hub. ChÃºng ta cÅ©ng cho phÃ©p huáº¥n luyá»‡n chÃ­nh xÃ¡c há»—n há»£p vá»›i `fp16 = True`, vÃ¬ nÃ³ cÃ³ thá»ƒ tÄƒng tá»‘c huáº¥n luyá»‡n má»™t cÃ¡ch Ä‘á»™c Ä‘Ã¡o trÃªn GPU gáº§n Ä‘Ã¢y.

{:else}

BÃ¢y giá» Ä‘Ã£ xong, chÃºng ta cÃ³ thá»ƒ táº¡o TF Datasets cá»§a mÃ¬nh. ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cÃ´ng cá»¥ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u máº·c Ä‘á»‹nh Ä‘Æ¡n giáº£n láº§n nÃ y:

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

VÃ  giá» chÃºng ta táº¡o bá»™ dá»¯ liá»‡u nhÆ° bÃ¬nh thÆ°á»ng.

```python
tf_train_dataset = train_dataset.to_tf_dataset(
    columns=[
        "input_ids",
        "start_positions",
        "end_positions",
        "attention_mask",
        "token_type_ids",
    ],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)
tf_eval_dataset = validation_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

Tiáº¿p theo, chÃºng ta thiáº¿t láº­p cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n vÃ  biÃªn dá»‹ch mÃ´ hÃ¬nh cá»§a mÃ¬nh:

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# Sá»‘ bÆ°á»›c huáº¥n luyá»‡n lÃ  sá»‘ lÆ°á»£ng máº«u trong táº­p dá»¯ liá»‡u, chia cho kÃ­ch thÆ°á»›c lÃ´ sau Ä‘Ã³ nhÃ¢n
# vá»›i tá»•ng sá»‘ epoch. LÆ°u Ã½ ráº±ng tf_train_dataset á»Ÿ Ä‘Ã¢y lÃ  tf.data.Dataset theo lÃ´,
# khÃ´ng pháº£i lÃ  Hugging Face Dataset ban Ä‘áº§u, vÃ¬ váº­y len() cá»§a nÃ³ vá»‘n lÃ  num_samples // batch_size.

num_train_epochs = 3
num_train_steps = len(tf_train_dataset) * num_train_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Huáº¥n luyá»‡n trong mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

Cuá»‘i cÃ¹ng, ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n vá»›i `model.fit()`. Ta sá»­ dá»¥ng `PushToHubCallback` Ä‘á»ƒ táº£i mÃ´ hÃ¬nh lÃªn Hub sau má»—i epoch.

{/if}

Máº·c Ä‘á»‹nh, kho lÆ°u trá»¯ Ä‘Æ°á»£c sá»­ dá»¥ng sáº½ náº±m trong khÃ´ng gian tÃªn cá»§a báº¡n vÃ  Ä‘Æ°á»£c Ä‘áº·t tÃªn theo thÆ° má»¥c Ä‘áº§u ra mÃ  báº¡n Ä‘Ã£ Ä‘áº·t, vÃ¬ váº­y trong trÆ°á»ng há»£p cá»§a mÃ¬nh, nÃ³ sáº½ náº±m trong `"sgugger/bert-finetuned-squad"`. ChÃºng ta cÃ³ thá»ƒ ghi Ä‘Ã¨ Ä‘iá»u nÃ y báº±ng cÃ¡ch chuyá»ƒn má»™t `hub_model_id`; vÃ­ dá»¥: Ä‘á»ƒ Ä‘áº©y mÃ´ hÃ¬nh vÃ o tá»• chá»©c `huggingface_course`, chÃºng ta Ä‘Ã£ sá»­ dá»¥ng `hub_model_id="huggingface_course/bert-finetuned-squad"` (lÃ  mÃ´ hÃ¬nh mÃ  ta Ä‘Ã£ liÃªn káº¿t á»Ÿ Ä‘áº§u pháº§n nÃ y).

{#if fw === 'pt'}

<Tip>

ğŸ’¡ Náº¿u thÆ° má»¥c Ä‘áº§u ra báº¡n Ä‘ang sá»­ dá»¥ng tá»“n táº¡i, nÃ³ cáº§n pháº£i lÃ  báº£n sao cá»¥c bá»™ cá»§a kho lÆ°u trá»¯ mÃ  báº¡n muá»‘n Ä‘áº©y Ä‘áº¿n (vÃ¬ váº­y hÃ£y Ä‘áº·t tÃªn má»›i náº¿u báº¡n gáº·p lá»—i khi xÃ¡c Ä‘á»‹nh `Trainer` cá»§a mÃ¬nh).

</Tip>

Cuá»‘i cÃ¹ng, ta chá»‰ cáº§n truyá»n má»i thá»© vÃ o lá»›p `Trainer` vÃ  khá»Ÿi Ä‘á»™ng viá»‡c huáº¥n luyá»‡n:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

{:else}

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-squad", tokenizer=tokenizer)

# ChÃºng ta sáº½ thá»±c hiá»‡n kiá»ƒm Ä‘á»‹nh sau Ä‘Ã³, vÃ¬ váº­y khÃ´ng cÃ³ quÃ¡ trÃ¬nh huáº¥nlluyá»‡n giá»¯a quÃ¡ trÃ¬nh kiá»ƒm Ä‘á»‹nh
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)
```

{/if}

LÆ°u Ã½ ráº±ng trong khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra, má»—i khi mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u (á»Ÿ Ä‘Ã¢y, má»—i epoch), nÃ³ sáº½ Ä‘Æ°á»£c táº£i lÃªn Hub á»Ÿ cháº¿ Ä‘á»™ ná»n. Báº±ng cÃ¡ch nÃ y, báº¡n sáº½ cÃ³ thá»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n cá»§a mÃ¬nh trÃªn má»™t mÃ¡y khÃ¡c náº¿u cáº§n. ToÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¥t má»™t khoáº£ng thá»i gian (hÆ¡n má»™t giá» trÃªn Titan RTX), vÃ¬ váº­y báº¡n cÃ³ thá»ƒ uá»‘ng má»™t ly cÃ  phÃª hoáº·c Ä‘á»c láº¡i má»™t sá»‘ pháº§n cá»§a khÃ³a há»c mÃ  báº¡n tháº¥y khÃ³ khÄƒn hÆ¡n trong khi tiáº¿p tá»¥c. CÅ©ng lÆ°u Ã½ ráº±ng ngay sau khi epoch Ä‘áº§u tiÃªn káº¿t thÃºc, báº¡n sáº½ tháº¥y má»™t sá»‘ trá»ng sá»‘ Ä‘Æ°á»£c táº£i lÃªn Hub vÃ  báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u chÆ¡i vá»›i mÃ´ hÃ¬nh cá»§a mÃ¬nh trÃªn trang cá»§a nÃ³.

{#if fw === 'pt'}

Sau khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t, cuá»‘i cÃ¹ng ta cÅ©ng cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a mÃ¬nh (vÃ  cáº§u nguyá»‡n ráº±ng ta Ä‘Ã£ khÃ´ng dÃ nh táº¥t cáº£ thá»i gian tÃ­nh toÃ¡n vÃ o viá»‡c gÃ¬). PhÆ°Æ¡ng thá»©c `predict()` cá»§a `Trainer` sáº½ tráº£ vá» má»™t bá»™ giÃ¡ trá»‹ trong Ä‘Ã³ cÃ¡c pháº§n tá»­ Ä‘áº§u tiÃªn sáº½ lÃ  cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh (á»Ÿ Ä‘Ã¢y lÃ  má»™t cáº·p vá»›i cÃ¡c logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc). ChÃºng ta gá»­i chÃºng Ä‘áº¿n hÃ m `compute_metrics())` cá»§a mÃ¬nh:

```python
predictions, _, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets["validation"])
```

{:else}

Sau khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t, cuá»‘i cÃ¹ng ta cÅ©ng cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a mÃ¬nh (vÃ  cáº§u nguyá»‡n ráº±ng ta Ä‘Ã£ khÃ´ng dÃ nh táº¥t cáº£ thá»i gian tÃ­nh toÃ¡n vÃ o viá»‡c gÃ¬). PhÆ°Æ¡ng thá»©c `predict()` cá»§a `model` sáº½ Ä‘áº£m nháº­n viá»‡c nháº­n cÃ¡c dá»± Ä‘oÃ¡n vÃ  vÃ¬ ta Ä‘Ã£ thá»±c hiá»‡n táº¥t cáº£ cÃ¡c cÃ´ng viá»‡c khÃ³ khÄƒn trong viá»‡c xÃ¡c Ä‘á»‹nh má»™t hÃ m `compute_metrics()` trÆ°á»›c Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c káº¿t quáº£ cá»§a mÃ¬nh trong má»™t dÃ²ng duy nháº¥t:

```python
predictions = model.predict(tf_eval_dataset)
compute_metrics(
    predictions["start_logits"],
    predictions["end_logits"],
    validation_dataset,
    raw_datasets["validation"],
)
```

{/if}

```python out
{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}
```

Tuyá»‡t quÃ¡! Äá»ƒ so sÃ¡nh, Ä‘iá»ƒm cÆ¡ báº£n Ä‘Æ°á»£c bÃ¡o cÃ¡o trong bÃ i bÃ¡o BERT cho mÃ´ hÃ¬nh nÃ y lÃ  80.8 vÃ  88.5, vÃ¬ váº­y chÃºng ta Ä‘ang á»Ÿ Ä‘Ãºng vá»‹ trÃ­ cá»§a mÃ¬nh.

{#if fw === 'pt'}

Cuá»‘i cÃ¹ng, ta sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `push_to_hub()` Ä‘á»ƒ Ä‘áº£m báº£o ta sáº½ táº£i phiÃªn báº£n má»›i nháº¥t cá»§a mÃ´ hÃ¬nh:

```py
trainer.push_to_hub(commit_message="Training complete")
```

Äiá»u nÃ y tráº£ vá» URL cá»§a cam káº¿t mÃ  nÃ³ vá»«a thá»±c hiá»‡n, náº¿u báº¡n muá»‘n kiá»ƒm tra nÃ³:

```python out
'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68'
```

`Trainer` cÅ©ng soáº¡n tháº£o má»™t tháº» mÃ´ hÃ¬nh vá»›i táº¥t cáº£ cÃ¡c káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vÃ  táº£i nÃ³ lÃªn.

{/if}

á» giai Ä‘oáº¡n nÃ y, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tiá»‡n Ã­ch luáº­n suy trÃªn Model Hub Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh vÃ  chia sáº» mÃ´ hÃ¬nh Ä‘Ã³ vá»›i báº¡n bÃ¨, gia Ä‘Ã¬nh vÃ  váº­t nuÃ´i yÃªu thÃ­ch cá»§a báº¡n. Báº¡n Ä‘Ã£ tinh chá»‰nh thÃ nh cÃ´ng má»™t mÃ´ hÃ¬nh trong tÃ¡c vá»¥ há»i Ä‘Ã¡p - xin chÃºc má»«ng!

<Tip>

âœï¸ **Äáº¿n lÆ°á»£t báº¡n!** HÃ£y thá»­ má»™t kiáº¿n trÃºc mÃ´ hÃ¬nh khÃ¡c Ä‘á»ƒ xem liá»‡u nÃ³ cÃ³ hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n trong tÃ¡c vá»¥ nÃ y khÃ´ng!

</Tip>

{#if fw === 'pt'}

Náº¿u báº¡n muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n má»™t chÃºt vá» vÃ²ng huáº¥n luyá»‡n, bÃ¢y giá» chÃºng tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n cÃ¡ch thá»±c hiá»‡n Ä‘iá»u tÆ°Æ¡ng tá»± báº±ng cÃ¡ch sá»­ dá»¥ng ğŸ¤— Accelerate.

## Má»™t vÃ²ng láº·p huáº¥n luyá»‡n tuá»³ chá»‰nh

BÃ¢y giá» chÃºng ta hÃ£y xem toÃ n bá»™ vÃ²ng láº·p huáº¥n luyá»‡n, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ dá»… dÃ ng tÃ¹y chá»‰nh cÃ¡c pháº§n báº¡n cáº§n. NÃ³ sáº½ trÃ´ng ráº¥t giá»‘ng vá»›i vÃ²ng láº·p huáº¥n luyá»‡n trong [ChÆ°Æ¡ng 3](/course/chapter3/4), ngoáº¡i trá»« vÃ²ng láº·p Ä‘Ã¡nh giÃ¡. ChÃºng ta sáº½ cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh thÆ°á»ng xuyÃªn vÃ¬ ta khÃ´ng bá»‹ háº¡n cháº¿ bá»Ÿi lá»›p `Trainer` ná»¯a.

### Chuáº¥n bá»‹ má»i thá»© cho huáº¥n luyá»‡n

Äáº§u tiÃªn, chÃºng ta cáº§n xÃ¢y dá»±ng cÃ¡c `DataLoader` tá»« cÃ¡c táº­p dá»¯ liá»‡u cá»§a mÃ¬nh. ChÃºng ta Ä‘áº·t Ä‘á»‹nh dáº¡ng cá»§a cÃ¡c táº­p dá»¯ liá»‡u Ä‘Ã³ thÃ nh `"torch"` vÃ  xÃ³a cÃ¡c cá»™t trong táº­p xÃ¡c thá»±c khÃ´ng Ä‘Æ°á»£c mÃ´ hÃ¬nh sá»­ dá»¥ng. Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng `default_data_collator` Ä‘Æ°á»£c cung cáº¥p bá»Ÿi Transformers dÆ°á»›i dáº¡ng `collate_fn` vÃ  xÃ¡o trá»™n bá»™ huáº¥n luyá»‡n, nhÆ°ng khÃ´ng pháº£i bá»™ kiá»ƒm Ä‘á»‹nh:

```py
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_dataset.set_format("torch")
validation_set = validation_dataset.remove_columns(["example_id", "offset_mapping"])
validation_set.set_format("torch")

train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=default_data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    validation_set, collate_fn=default_data_collator, batch_size=8
)
```

Tiáº¿p theo, chÃºng ta khÃ´i phá»¥c mÃ´ hÃ¬nh cá»§a mÃ¬nh, Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng ta khÃ´ng tiáº¿p tá»¥c tinh chá»‰nh tá»« trÆ°á»›c mÃ  báº¯t Ä‘áº§u láº¡i tá»« mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c BERT:

```py
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

Sau Ä‘Ã³, chÃºng ta sáº½ cáº§n má»™t trÃ¬nh tá»‘i Æ°u hÃ³a. NhÆ° thÆ°á»ng lá»‡, ta sá»­ dá»¥ng `AdamW` cá»• Ä‘iá»ƒn, giá»‘ng nhÆ° Adam, nhÆ°ng vá»›i má»™t báº£n sá»­a lá»—i trong cÃ¡ch phÃ¢n rÃ£ trá»ng sá»‘ Ä‘Æ°á»£c Ã¡p dá»¥ng:

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Khi chÃºng ta cÃ³ táº¥t cáº£ cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ gá»­i chÃºng Ä‘áº¿n phÆ°Æ¡ng thá»©c `accelerator.prepare()`. HÃ£y nhá»› ráº±ng náº¿u báº¡n muá»‘n huáº¥n luyá»‡n vá» TPU trong notebook Colab, báº¡n sáº½ cáº§n chuyá»ƒn táº¥t cáº£ mÃ£ nÃ y vÃ o má»™t hÃ m huáº¥n luyá»‡n vÃ  Ä‘iá»u Ä‘Ã³ sáº½ khÃ´ng thá»±c thi báº¥t ká»³ Ã´ khá»Ÿi táº¡o má»™t `Accelerator` nÃ o. ChÃºng ta cÃ³ thá»ƒ buá»™c huáº¥n luyá»‡n Ä‘á»™ chÃ­nh xÃ¡c há»—n há»£p báº±ng cÃ¡ch chuyá»ƒn `fp16=True` vÃ o `Accelerator` (hoáº·c, náº¿u báº¡n Ä‘ang thá»±c thi mÃ£ dÆ°á»›i dáº¡ng táº­p lá»‡nh, chá»‰ cáº§n Ä‘áº£m báº£o Ä‘iá»n vÃ o ğŸ¤— Accelerate `config` má»™t cÃ¡ch thÃ­ch há»£p).

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

NhÆ° báº¡n Ä‘Ã£ biáº¿t tá»« cÃ¡c pháº§n trÆ°á»›c, chÃºng ta chá»‰ cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»™ dÃ i `train_dataloader` Ä‘á»ƒ tÃ­nh sá»‘ bÆ°á»›c huáº¥n luyá»‡n sau khi nÃ³ Ä‘Ã£ tráº£i qua phÆ°Æ¡ng thá»©c `accelerator.prepare()`. ChÃºng ta sá»­ dá»¥ng cÃ¹ng má»™t lá»‹ch trÃ¬nh tuyáº¿n tÃ­nh nhÆ° trong cÃ¡c pháº§n trÆ°á»›c:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Äá»ƒ Ä‘áº©y mÃ´ hÃ¬nh cá»§a mÃ¬nh lÃªn Hub, chÃºng ta sáº½ cáº§n táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `Repository` trong má»™t thÆ° má»¥c Ä‘ang lÃ m viá»‡c. Äáº§u tiÃªn hÃ£y Ä‘Äƒng nháº­p vÃ o Hugging Face Hub, náº¿u báº¡n chÆ°a Ä‘Äƒng nháº­p. ChÃºng ta sáº½ xÃ¡c Ä‘á»‹nh tÃªn kho lÆ°u trá»¯ tá»« ID mÃ´ hÃ¬nh mÃ  ta muá»‘n cung cáº¥p cho mÃ´ hÃ¬nh cá»§a mÃ¬nh (vui lÃ²ng thay tháº¿ `repo_name` báº±ng sá»± lá»±a chá»n cá»§a riÃªng báº¡n; nÃ³ chá»‰ cáº§n chá»©a tÃªn ngÆ°á»i dÃ¹ng cá»§a báº¡n, Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ hÃ m `get_full_repo_name()` thá»±c hiá»‡n ):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-squad-accelerate'
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ sao chÃ©p kho lÆ°u trá»¯ Ä‘Ã³ trong má»™t thÆ° má»¥c cá»¥c bá»™. Náº¿u nÃ³ Ä‘Ã£ tá»“n táº¡i, thÆ° má»¥c cá»¥c bá»™ nÃ y pháº£i lÃ  báº£n sao cá»§a kho lÆ°u trá»¯ mÃ  ta Ä‘ang lÃ m viá»‡c:

```py
output_dir = "bert-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Giá» ta cÃ³ thá»ƒ táº£i má»i thá»­ ta lÆ°u trong `output_dir` báº±ng cÃ¡ch gá»i phÆ°Æ¡ng thá»©c `repo.push_to_hub()`. NÃ³ sáº½ giÃºp ta táº£i cÃ¡c mÃ´ hÃ¬nh tá»©c thÃ¬ á»Ÿ cuá»‘i má»—i epoch.

## VÃ²ng láº·p huáº¥n luyá»‡n

BÃ¢y giá» chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ viáº¿t vÃ²ng láº·p huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§. Sau khi xÃ¡c Ä‘á»‹nh thanh tiáº¿n trÃ¬nh Ä‘á»ƒ theo dÃµi quÃ¡ trÃ¬nh huáº¥n luyá»‡n diá»…n ra nhÆ° tháº¿ nÃ o, vÃ²ng láº·p cÃ³ ba pháº§n:

- Báº£n thÃ¢n quÃ¡ trÃ¬nh huáº¥n luyá»‡n, lÃ  sá»± láº·p láº¡i cá»• Ä‘iá»ƒn trÃªn `train_dataloader`, truyá»n tháº³ng qua mÃ´ hÃ¬nh, sau Ä‘Ã³ truyá»n ngÆ°á»£c vÃ  tá»‘i Æ°u hÃ³a.
- BÆ°á»›c Ä‘Ã¡nh giÃ¡, trong Ä‘Ã³ ta thu tháº­p táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cho `start_logits` vÃ  `end_logits` trÆ°á»›c khi chuyá»ƒn Ä‘á»•i chÃºng thÃ nh máº£ng NumPy. Khi vÃ²ng láº·p Ä‘Ã¡nh giÃ¡ káº¿t thÃºc, chÃºng ta ná»‘i táº¥t cáº£ cÃ¡c káº¿t quáº£. LÆ°u Ã½ ráº±ng chÃºng ta cáº§n cáº¯t bá»›t vÃ¬ `Accelerator` cÃ³ thá»ƒ Ä‘Ã£ thÃªm má»™t vÃ i máº«u vÃ o cuá»‘i Ä‘á»ƒ Ä‘áº£m báº£o chÃºng ta cÃ³ cÃ¹ng sá»‘ lÆ°á»£ng máº«u trong má»—i quy trÃ¬nh.
- LÆ°u vÃ  táº£i lÃªn, nÆ¡i trÆ°á»›c tiÃªn chÃºng ta lÆ°u mÃ´ hÃ¬nh vÃ  trÃ¬nh mÃ£ hÃ³a, sau Ä‘Ã³ gá»i `repo.push_to_hub()`. NhÆ° chÃºng ta Ä‘Ã£ lÃ m trÆ°á»›c Ä‘Ã¢y, chÃºng ta sá»­ dá»¥ng Ä‘á»‘i sá»‘ `blocking=False` Ä‘á»ƒ yÃªu cáº§u thÆ° viá»‡n ğŸ¤— Hub Ä‘áº©y vÃ o má»™t quÃ¡ trÃ¬nh khÃ´ng Ä‘á»“ng bá»™. Báº±ng cÃ¡ch nÃ y, quÃ¡ trÃ¬nh huáº¥n luyá»‡n tiáº¿p tá»¥c diá»…n ra bÃ¬nh thÆ°á»ng vÃ  lá»‡nh (dÃ i) nÃ y Ä‘Æ°á»£c thá»±c thi á»Ÿ cháº¿ Ä‘á»™ ná»n.

ÄÃ¢y lÃ  mÃ£ hoÃ n chá»‰nh cho vÃ²ng láº·p huáº¥n luyá»‡n:

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Huáº¥n luyá»‡n
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # ÄÃ¡nh giÃ¡
    model.eval()
    start_logits = []
    end_logits = []
    accelerator.print("Evaluation!")
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())
        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())

    start_logits = np.concatenate(start_logits)
    end_logits = np.concatenate(end_logits)
    start_logits = start_logits[: len(validation_dataset)]
    end_logits = end_logits[: len(validation_dataset)]

    metrics = compute_metrics(
        start_logits, end_logits, validation_dataset, raw_datasets["validation"]
    )
    print(f"epoch {epoch}:", metrics)

    # LÆ°u vÃ  táº£i
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

Trong trÆ°á»ng há»£p Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn báº¡n tháº¥y má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u báº±ng ğŸ¤— Accelerate, hÃ£y dÃ nh má»™t chÃºt thá»i gian Ä‘á»ƒ kiá»ƒm tra ba dÃ²ng mÃ£ Ä‘i kÃ¨m vá»›i nÃ³:

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

DÃ²ng Ä‘áº§u tiÃªn Ä‘Ã£ tá»± giáº£i thÃ­ch: nÃ³ cho táº¥t cáº£ cÃ¡c quÃ¡ trÃ¬nh chá» cho Ä‘áº¿n khi má»i ngÆ°á»i á»Ÿ giai Ä‘oáº¡n Ä‘Ã³ trÆ°á»›c khi tiáº¿p tá»¥c. Äiá»u nÃ y lÃ  Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng chÃºng ta cÃ³ cÃ¹ng má»™t mÃ´ hÃ¬nh trong má»i quy trÃ¬nh trÆ°á»›c khi lÆ°u. Sau Ä‘Ã³, ta láº¥y `unwrapped_model`, lÃ  mÃ´ hÃ¬nh cÆ¡ sá»Ÿ mÃ  ta Ä‘Ã£ xÃ¡c Ä‘á»‹nh. PhÆ°Æ¡ng thá»©c `accelerator.prepare()` thay Ä‘á»•i mÃ´ hÃ¬nh Ä‘á»ƒ hoáº¡t Ä‘á»™ng trong huáº¥n luyá»‡n phÃ¢n tÃ¡n, vÃ¬ váº­y nÃ³ sáº½ khÃ´ng cÃ³ phÆ°Æ¡ng thá»©c `save_pretrained()` ná»¯a; phÆ°Æ¡ng thá»©c `accelerator.unwrap_model()` hoÃ n tÃ¡c bÆ°á»›c Ä‘Ã³. Cuá»‘i cÃ¹ng, chÃºng ta gá»i `save_pretrained()` nhÆ°ng yÃªu cáº§u phÆ°Æ¡ng thá»©c Ä‘Ã³ sá»­ dá»¥ng `accelerator.save()` thay vÃ¬ `torch.save()`.

Khi Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n, báº¡n sáº½ cÃ³ má»™t mÃ´ hÃ¬nh táº¡o ra káº¿t quáº£ khÃ¡ giá»‘ng vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i `Trainer`. Báº¡n cÃ³ thá»ƒ kiá»ƒm tra mÃ´ hÃ¬nh mÃ  ta Ä‘Ã£ huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng mÃ£ nÃ y táº¡i [*huggingface-course/bert-finetuned-squad-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate). VÃ  náº¿u báº¡n muá»‘n kiá»ƒm tra báº¥t ká»³ tinh chá»‰nh nÃ o Ä‘á»‘i vá»›i vÃ²ng láº·p huáº¥n luyá»‡n, báº¡n cÃ³ thá»ƒ trá»±c tiáº¿p thá»±c hiá»‡n chÃºng báº±ng cÃ¡ch chá»‰nh sá»­a Ä‘oáº¡n mÃ£ Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ trÃªn!

{/if}

## Sá»­ dá»¥ng mÃ´ hÃ¬nh tinh chá»‰nh

ChÃºng tÃ´i Ä‘Ã£ chá»‰ cho báº¡n cÃ¡ch báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh mÃ  chÃºng ta Ä‘Ã£ tinh chá»‰nh trÃªn Model Hub báº±ng tiá»‡n Ã­ch luáº­n suy. Äá»ƒ sá»­ dá»¥ng nÃ³ cá»¥c bá»™ trong má»™t `pipeline`, báº¡n chá»‰ cáº§n chá»‰ Ä‘á»‹nh mÃ£ Ä‘á»‹nh danh mÃ´ hÃ¬nh:

```py
from transformers import pipeline

# Thay tháº¿ nÃ³ vá»›i checkpoint cá»§a báº¡n
model_checkpoint = "huggingface-course/bert-finetuned-squad"
question_answerer = pipeline("question-answering", model=model_checkpoint)

context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.9979003071784973,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Tuyá»‡t quÃ¡! MÃ´ hÃ¬nh cá»§a chÃºng ta Ä‘ang hoáº¡t Ä‘á»™ng tá»‘t nhÆ° mÃ´ hÃ¬nh máº·c Ä‘á»‹nh cho pipeline nÃ y!
