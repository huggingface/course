<FrameworkSwitchCourse {fw} />

# Sá»©c máº¡nh Ä‘áº·c biá»‡t cá»§a tokenizer nhanh

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3_tf.ipynb"},
]} />

{/if}

Trong pháº§n nÃ y, chÃºng ta sáº½ xem xÃ©t ká»¹ hÆ¡n cÃ¡c kháº£ nÄƒng cá»§a tokenizer trong ğŸ¤— Transformers. Cho Ä‘áº¿n nay, chÃºng ta chá»‰ sá»­ dá»¥ng chÃºng Ä‘á»ƒ tokenize Ä‘áº§u vÃ o hoáº·c giáº£i mÃ£ ID trá»Ÿ láº¡i thÃ nh vÄƒn báº£n, nhÆ°ng cÃ¡c trÃ¬nh tokenize - Ä‘áº·c biá»‡t lÃ  nhá»¯ng trÃ¬nh tokenize Ä‘Æ°á»£c há»— trá»£ bá»Ÿi thÆ° viá»‡n ğŸ¤— Tokenizers - cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c nhiá»u hÆ¡n tháº¿. Äá»ƒ minh há»a cÃ¡c tÃ­nh nÄƒng bá»• sung nÃ y, chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡ch tÃ¡i táº¡o káº¿t quáº£ cá»§a `token-classification` (mÃ  chÃºng ta gá»i lÃ  `ner`) vÃ  `question-answering` chÃºng ta gáº·p pháº£i láº§n Ä‘áº§u tiÃªn trong [ChÆ°Æ¡ng 1](/course/chapter1).

<Youtube id="g8quOxoqhHQ"/>

Trong pháº§n tháº£o luáº­n káº¿ tiáº¿p, chÃºng ta sáº½ phÃ¢n biá»‡t giá»¯a cÃ¡c loáº¡i tokenizer "cháº­m" vÃ  "nhanh". PhiÃªn báº£n cháº­m lÃ  nhá»¯ng phiÃªn báº£n Ä‘Æ°á»£c viáº¿t báº±ng Python bÃªn trong thÆ° viá»‡n ğŸ¤— Transformers, trong khi phiÃªn báº£n nhanh lÃ  nhá»¯ng phiÃªn báº£n Ä‘Æ°á»£c cung cáº¥p bá»Ÿi ğŸ¤— Tokenizers, Ä‘Æ°á»£c viáº¿t báº±ng Rust. Náº¿u báº¡n nhá»› báº£ng tá»« [ChÆ°Æ¡ng 5](/course/chapter5/3) bÃ¡o cÃ¡o khoáº£ng thá»i gian tokenize nhanh vÃ  cháº­m cáº§n Ä‘á»ƒ tokenize Drug Review Dataset, báº¡n nÃªn biáº¿t lÃ½ do táº¡i sao chÃºng tÃ´i gá»i chÃºng lÃ  nhanh vÃ  cháº­m :

                |  Tokenizer nhanh | Tokenizer cháº­m
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

âš ï¸ Khi tokenize má»™t cÃ¢u, báº¡n khÃ´ng pháº£i lÃºc nÃ o cÅ©ng tháº¥y sá»± khÃ¡c biá»‡t vá» tá»‘c Ä‘á»™ giá»¯a cÃ¡c phiÃªn báº£n cháº­m vÃ  nhanh cá»§a cÃ¹ng má»™t trÃ¬nh tokenize. TrÃªn thá»±c táº¿, phiÃªn báº£n nhanh cÃ³ thá»ƒ cháº­m hÆ¡n! Chá»‰ khi tokenize nhiá»u vÄƒn báº£n song song cÃ¹ng má»™t lÃºc, báº¡n má»›i cÃ³ thá»ƒ tháº¥y rÃµ sá»± khÃ¡c biá»‡t.

</Tip>

## MÃ£ hoÃ¡ theo lÃ´

<Youtube id="3umI3tm27Vw"/>

Äáº§u ra cá»§a tokenizer khÃ´ng pháº£i lÃ  má»™t tá»« Ä‘iá»ƒn Python Ä‘Æ¡n giáº£n; nhá»¯ng gÃ¬ chÃºng ta nháº­n Ä‘Æ°á»£c thá»±c sá»± lÃ  má»™t Ä‘á»‘i tÆ°á»£ng `BatchEncoding` Ä‘áº·c biá»‡t. ÄÃ³ lÃ  má»™t lá»›p con cá»§a tá»« Ä‘iá»ƒn (Ä‘Ã³ lÃ  lÃ½ do táº¡i sao trÆ°á»›c Ä‘Ã¢y chÃºng ta cÃ³ thá»ƒ láº­p chá»‰ má»¥c vÃ o káº¿t quáº£ Ä‘Ã³ mÃ  khÃ´ng gáº·p báº¥t ká»³ váº¥n Ä‘á» gÃ¬), nhÆ°ng vá»›i cÃ¡c phÆ°Æ¡ng thá»©c bá»• sung háº§u háº¿t Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c trÃ¬nh tokenize nhanh.

BÃªn cáº¡nh kháº£ nÄƒng song song hÃ³a cá»§a chÃºng, chá»©c nÄƒng chÃ­nh cá»§a cÃ¡c trÃ¬nh tokenize nhanh lÃ  chÃºng luÃ´n theo dÃµi khoáº£ng vÄƒn báº£n ban Ä‘áº§u mÃ  ta tokenize - má»™t tÃ­nh nÄƒng Ä‘Æ°á»£c gá»i lÃ  *offset mapping* hay *Ã¡nh xáº¡ bÃ¹ trá»«*. Äiá»u nÃ y láº§n lÆ°á»£t má»Ÿ khÃ³a cÃ¡c tÃ­nh nÄƒng nhÆ° Ã¡nh xáº¡ tá»«ng tá»« vá»›i token mÃ  nÃ³ táº¡o ra hoáº·c Ã¡nh xáº¡ tá»«ng kÃ½ tá»± cá»§a vÄƒn báº£n gá»‘c vá»›i token bÃªn trong vÃ  ngÆ°á»£c láº¡i.

CÃ¹ng xem má»™t máº«u:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã¢y, chÃºng ta nháº­n Ä‘Æ°á»£c má»™t Ä‘á»‘i tÆ°á»£ng `BatchEncoding` trong Ä‘áº§u ra cá»§a trÃ¬nh tokenize:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

VÃ¬ lá»›p `AutoTokenizer` chá»n má»™t trÃ¬nh tokenizer nhanh theo máº·c Ä‘á»‹nh, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng thá»©c bá»• sung mÃ  Ä‘á»‘i tÆ°á»£ng `BatchEncoding` nÃ y cung cáº¥p. ChÃºng ta cÃ³ hai cÃ¡ch Ä‘á»ƒ kiá»ƒm tra xem trÃ¬nh tokenize lÃ  nhanh hay cháº­m. ChÃºng ta cÃ³ thá»ƒ kiá»ƒm tra báº±ng thuá»™c tÃ­nh `is_fast` cá»§a `tokenizer`:

```python
tokenizer.is_fast
```

```python out
True
```

hoáº·c kiá»ƒm tra cÃ¹ng thuá»™c tÃ­nh Ä‘Ã³ cá»§a `encoding`:

```python
encoding.is_fast
```

```python out
True
```

HÃ£y xem nhá»¯ng gÃ¬ má»™t tokenizer nhanh cho phÃ©p chÃºng ta lÃ m. Äáº§u tiÃªn, chÃºng tÃ´i cÃ³ thá»ƒ truy cáº­p token mÃ  khÃ´ng cáº§n pháº£i chuyá»ƒn Ä‘á»•i ID trá»Ÿ láº¡i token:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

Trong trÆ°á»ng há»£p nÃ y, token á»Ÿ chá»‰ má»¥c 5 lÃ  `##yl`, lÃ  má»™t pháº§n cá»§a tá»« "Sylvain" trong cÃ¢u gá»‘c. ChÃºng ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `word_ids()` Ä‘á»ƒ láº¥y chá»‰ má»¥c cá»§a tá»« mÃ  má»—i token Ä‘áº¿n tá»«:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ¡c token Ä‘áº·c biá»‡t cá»§a trÃ¬nh tokenize nhÆ° `[CLS]` vÃ  `[SEP]` Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh `None`, vÃ  sau Ä‘Ã³ má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ tá»›i tá»« mÃ  nÃ³ báº¯t nguá»“n. Äiá»u nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem má»™t token náº±m á»Ÿ Ä‘áº§u má»™t tá»« hay náº¿u hai token cÃ³ trong cÃ¹ng thuá»™c má»™t tá»«. ChÃºng ta cÃ³ thá»ƒ dá»±a vÃ o tiá»n tá»‘ `##` cho Ä‘iá»u Ä‘Ã³, nhÆ°ng nÃ³ chá»‰ hoáº¡t Ä‘á»™ng Ä‘á»‘i vá»›i cÃ¡c tokenize kiá»ƒu BERT; phÆ°Æ¡ng phÃ¡p nÃ y hoáº¡t Ä‘á»™ng vá»›i báº¥t ká»³ loáº¡i tokenizer nÃ o miá»…n nÃ³ lÃ  phiÃªn báº£n nhanh. Trong chÆ°Æ¡ng tiáº¿p theo, chÃºng ta sáº½ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng kháº£ nÄƒng nÃ y Ä‘á»ƒ Ã¡p dá»¥ng nhÃ£n chÃºng ta cÃ³ cho má»—i tá»« Ä‘Ãºng cÃ¡ch vá»›i cÃ¡c token trong cÃ¡c tÃ¡c vá»¥ nhÆ° nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (NER) vÃ   gÃ¡n nhÃ£n tá»« loáº¡i (POS). ChÃºng ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ che giáº¥u táº¥t cáº£ cÃ¡c token Ä‘áº¿n tá»« cÃ¹ng má»™t tá»« trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c che (má»™t ká»¹ thuáº­t Ä‘Æ°á»£c gá»i lÃ  _whole word masking_).

<Tip>

KhÃ¡i niá»‡m vá» má»™t tá»« ráº¥t lÃ  phá»©c táº¡p. VÃ­ dá»¥: "I'll" (tá»« rÃºt gá»n cá»§a "I will") cÃ³ Ä‘Æ°á»£c tÃ­nh lÃ  má»™t hay hai tá»«? NÃ³ thá»±c sá»± phá»¥ thuá»™c vÃ o trÃ¬nh tokenize vÃ  hoáº¡t Ä‘á»™ng tiá»n tokenize mÃ  nÃ³ Ã¡p dá»¥ng. Má»™t sá»‘ tokenizer chá»‰ tÃ¡ch ra trÃªn khoáº£ng tráº¯ng, vÃ¬ váº­y há» sáº½ coi Ä‘Ã¢y lÃ  má»™t tá»«. Nhá»¯ng ngÆ°á»i khÃ¡c sá»­ dá»¥ng dáº¥u cÃ¢u á»Ÿ Ä‘áº§u khoáº£ng tráº¯ng, vÃ¬ váº­y sáº½ coi nÃ³ lÃ  hai tá»«.

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Táº¡o tokenizer tá»« cÃ¡c checkpoints `bert-base-cased` vÃ ` roberta-base` vÃ  tokenize "81s" vá»›i chÃºng. Báº¡n quan sÃ¡t tháº¥y gÃ¬? ID tá»« lÃ  gÃ¬?

</Tip>

TÆ°Æ¡ng tá»±, cÃ³ má»™t phÆ°Æ¡ng thá»©c `question_ids()` mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ Ã¡nh xáº¡ token Ä‘áº¿n cÃ¢u mÃ  nÃ³ báº¯t nguá»“n (máº·c dÃ¹ trong trÆ°á»ng há»£p nÃ y, `token_type_ids` Ä‘Æ°á»£c tráº£ vá» bá»Ÿi tokenizer cÃ³ thá»ƒ cung cáº¥p cho chÃºng ta cÃ¹ng má»™t thÃ´ng tin).

Cuá»‘i cÃ¹ng, chÃºng ta cÃ³ thá»ƒ Ã¡nh xáº¡ báº¥t ká»³ tá»« hoáº·c token nÃ o vá»›i cÃ¡c kÃ½ tá»± trong vÄƒn báº£n gá»‘c vÃ  ngÆ°á»£c láº¡i, thÃ´ng qua cÃ¡c phÆ°Æ¡ng thá»©c `word_to_chars()` hoáº·c `token_to_chars()` vÃ  `char_to_word()` hoáº·c `char_to_token()`. VÃ­ dá»¥: phÆ°Æ¡ng thá»©c `word_ids()` cho chÃºng ta biáº¿t ráº±ng `##yl` lÃ  má»™t pháº§n cá»§a tá»« á»Ÿ chá»‰ má»¥c 3, nhÆ°ng tá»« Ä‘Ã³ náº±m trong cÃ¢u nÃ o? ChÃºng ta cÃ³ thá»ƒ tÃ¬m hiá»ƒu nhÆ° tháº¿ nÃ y:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, táº¥t cáº£ Ä‘iá»u nÃ y thá»±c táº¿ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi lÃ  trÃ¬nh tokenizer nhanh káº¿t há»£p khoáº£ng vÄƒn báº£n mÃ  má»—i token Ä‘áº¿n tá»« danh sÃ¡ch *offset* hay *offset*. Äá»ƒ minh há»a viá»‡c sá»­ dá»¥ng chÃºng, tiáº¿p theo, chÃºng tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n cÃ¡ch sao chÃ©p cÃ¡c káº¿t quáº£ cá»§a `token-classification` theo cÃ¡ch thá»§ cÃ´ng.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i** Táº¡o vÄƒn báº£n máº«u cá»§a riÃªng báº¡n vÃ  xem liá»‡u báº¡n cÃ³ thá»ƒ hiá»ƒu nhá»¯ng token nÃ o Ä‘Æ°á»£c liÃªn káº¿t vá»›i ID tá»«, cÅ©ng nhÆ° cÃ¡ch trÃ­ch xuáº¥t kÃ½ tá»± kÃ©o dÃ i cho má»™t tá»«. Äá»ƒ cÃ³ Ä‘iá»ƒm thÆ°á»Ÿng, hÃ£y thá»­ sá»­ dá»¥ng hai cÃ¢u lÃ m Ä‘áº§u vÃ o vÃ  xem liá»‡u ID cÃ¢u cÃ³ phÃ¹ há»£p vá»›i báº¡n khÃ´ng.

</Tip>

## BÃªn trong pipeline `token-classification`

Trong [ChÆ°Æ¡ng 1](/course/chapter1), chÃºng ta láº§n Ä‘áº§u Ä‘Æ°á»£c thá»­ Ã¡p dá»¥ng NER - tÃ¡c vá»¥ xÃ¡c Ä‘á»‹nh nhá»¯ng pháº§n nÃ o cá»§a vÄƒn báº£n tÆ°Æ¡ng á»©ng vá»›i cÃ¡c thá»±c thá»ƒ nhÆ° ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm hoáº·c tá»• chá»©c - vá»›i `pipeline()` cá»§a ğŸ¤— Transformers. Sau Ä‘Ã³, trong [ChÆ°Æ¡ng 2](/course/chapter2), chÃºng ta Ä‘Ã£ tháº¥y cÃ¡ch má»™t pipeline nhÃ³m ba giai Ä‘oáº¡n cáº§n thiáº¿t láº¡i vá»›i nhau Ä‘á»ƒ nháº­n cÃ¡c dá»± Ä‘oÃ¡n tá»« má»™t vÄƒn báº£n thÃ´: tokenize, chuyá»ƒn cÃ¡c Ä‘áº§u vÃ o qua mÃ´ hÃ¬nh vÃ  háº­u xá»­ lÃ½. Hai bÆ°á»›c Ä‘áº§u tiÃªn trong quy trÃ¬nh `token-classification` cÅ©ng giá»‘ng nhÆ° trong báº¥t ká»³ quy trÃ¬nh nÃ o khÃ¡c, nhÆ°ng quÃ¡ trÃ¬nh háº­u xá»­ lÃ½ phá»©c táº¡p hÆ¡n má»™t chÃºt - hÃ£y cÃ¹ng xem cÃ¡ch thá»±c hiá»‡n!

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### Nháº­n káº¿t quáº£ cÆ¡ sá»Ÿ vá»›i baseline

TrÆ°á»›c tiÃªn, hÃ£y láº¥y má»™t pipeline phÃ¢n loáº¡i token chÃºng ta cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c má»™t sá»‘ káº¿t quáº£ Ä‘á»ƒ so sÃ¡nh theo cÃ¡ch thá»§ cÃ´ng. MÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng theo máº·c Ä‘á»‹nh lÃ  [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); nÃ³ thá»±c hiá»‡n NER trÃªn cÃ¡c cÃ¢u:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

MÃ´ hÃ¬nh Ä‘Ã£ xÃ¡c Ä‘á»‹nh Ä‘Ãºng má»—i token do "Sylvain" táº¡o ra lÃ  má»™t ngÆ°á»i, má»—i token Ä‘Æ°á»£c táº¡o bá»Ÿi "Hugging Face" lÃ  má»™t tá»• chá»©c vÃ  token "Brooklyn" lÃ  má»™t Ä‘á»‹a Ä‘iá»ƒm. ChÃºng ta cÅ©ng cÃ³ thá»ƒ yÃªu cáº§u pipeline nhÃ³m cÃ¡c token tÆ°Æ¡ng á»©ng vá»›i cÃ¹ng má»™t thá»±c thá»ƒ láº¡i vá»›i nhau:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

`aggregation_strategy` Ä‘Æ°á»£c chá»n sáº½ thay Ä‘á»•i Ä‘iá»ƒm Ä‘Æ°á»£c tÃ­nh cho má»—i thá»±c thá»ƒ Ä‘Æ°á»£c nhÃ³m láº¡i, Vá»›i `"simple"`, Ä‘iá»ƒm chá»‰ lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a Ä‘iá»ƒm cá»§a má»—i token trong thá»±c thá»ƒ Ä‘Ã£ cho: vÃ­ dá»¥: Ä‘iá»ƒm cá»§a "Sylvain" lÃ  trung bÃ¬nh Ä‘iá»ƒm mÃ  chÃºng ta Ä‘Ã£ tháº¥y trong vÃ­ dá»¥ trÆ°á»›c cho cÃ¡c token `S` , `##yl`,`## va` vÃ  `##in`. CÃ¡c chiáº¿n lÆ°á»£c cÃ³ sáºµn khÃ¡c lÃ :

- `"first"`, trong Ä‘Ã³ Ä‘iá»ƒm cá»§a má»—i thá»±c thá»ƒ lÃ  Ä‘iá»ƒm cá»§a token Ä‘áº§u tiÃªn cá»§a thá»±c thá»ƒ Ä‘Ã³ (vÃ¬ váº­y Ä‘á»‘i vá»›i "Sylvain", nÃ³ sáº½ lÃ  0.993828, Ä‘iá»ƒm cá»§a token `S`)
- `"max"`, trong Ä‘Ã³ Ä‘iá»ƒm cá»§a má»—i thá»±c thá»ƒ lÃ  Ä‘iá»ƒm tá»‘i Ä‘a cá»§a cÃ¡c token trong thá»±c tháº¿ Ä‘Ã³ (vÃ¬ váº­y vá»›i "Hugging Face" sáº½ lÃ  0.98879766, Ä‘iá»ƒm cá»§a "Face")
- `"average"`, trong Ä‘Ã³ Ä‘iá»ƒm cá»§a má»—i thá»±c thá»ƒ lÃ  Ä‘iá»ƒm trung bÃ¬nh cá»§a cÃ¡c tá»« táº¡o nÃªn thá»±c thá»ƒ Ä‘Ã³ (vÃ­ dá»¥ vá»›i "Sylvain" thÃ¬ phÆ°Æ¡ng phÃ¡p nÃ y sáº½ khÃ´ng cÃ³ sá»± khÃ¡c biá»‡t so vá»›i phÆ°Æ¡ng phÃ¡p `"simple"`, nhÆ°ng vá»›i "Hugging Face", Ä‘iá»ƒm tráº£ vá» sáº½ lÃ  0.9819, Ä‘iá»ƒm trung bÃ¬nh cá»§a "Hugging", 0.975, vÃ  "Face", 0.98879)

Giá» chÃºng ta hÃ£y xem lÃ m tháº¿ nÃ o Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c nhá»¯ng káº¿t quáº£ nÃ y mÃ  khÃ´ng cáº§n sá»­ dá»¥ng hÃ m`pipeline()`!

### Tá»« Ä‘áº§u vÃ o tá»›i dá»± Ä‘oÃ¡n

{#if fw === 'pt'}

Äáº§u tiÃªn chÃºng ta cáº§n tokenize Ä‘áº§u vÃ o cá»§a chÃºng ta vÃ  truyá»n chÃºng vÃ o mÃ´ hÃ¬nh. ÄÃ¢y chÃ­nh xÃ¡c lÃ  nhá»¯ng gÃ¬ ta Ä‘Ã£ lÃ m á»Ÿ [ChÆ°Æ¡ng 2](/course/chapter2); ta khá»Ÿi táº¡o tokenizer vÃ  mÃ´ hÃ¬nh sá»­ dá»¥ng lá»›p `AutoXxx` vÃ  sau Ä‘Ã³ dÃ¹ng chÃºng vÃ o cÃ¡c máº«u cá»§a mÃ¬nh:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

VÃ¬ chÃºng ta sá»­ dá»¥ng `AutoModelForTokenClassification` á»Ÿ Ä‘Ã¢y,ta sáº½ nháº­n Ä‘Æ°á»£c táº­p há»£p cÃ¡c logits cho tá»«ng token cá»§a chuá»—i Ä‘áº§u vÃ o:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

Äáº§u tiÃªn chÃºng ta cáº§n tokenize Ä‘áº§u vÃ o cá»§a chÃºng ta vÃ  truyá»n chÃºng vÃ o mÃ´ hÃ¬nh. ÄÃ¢y chÃ­nh xÃ¡c lÃ  nhá»¯ng gÃ¬ ta Ä‘Ã£ lÃ m á»Ÿ [ChÆ°Æ¡ng 2](/course/chapter2); ta khá»Ÿi táº¡o tokenizer vÃ  mÃ´ hÃ¬nh sá»­ dá»¥ng lá»›p `TFAutoXxx` vÃ  sau Ä‘Ã³ dÃ¹ng chÃºng vÃ o cÃ¡c máº«u cá»§a mÃ¬nh:

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

VÃ¬ ta dÃ¹ng `TFAutoModelForTokenClassification` á»Ÿ Ä‘Ã¢y, ta sáº½ nháº­n Ä‘Æ°á»£c táº­p há»£p cÃ¡c logits cho tá»«ng token cá»§a chuá»—i Ä‘áº§u vÃ o:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

ChÃºng ta cÃ³ má»™t lÃ´ vá»›i 1 chuá»—i gá»“m 19 token vÃ  mÃ´ hÃ¬nh cÃ³ 9 nhÃ£n khÃ¡c nhau, vÃ¬ váº­y Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh cÃ³ hÃ¬nh dáº¡ng 1 x 19 x 9. Giá»‘ng nhÆ° Ä‘á»‘i vá»›i pipeline phÃ¢n loáº¡i vÄƒn báº£n, chÃºng ta sá»­ dá»¥ng hÃ m softmax Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c logits Ä‘Ã³ theo xÃ¡c suáº¥t, vÃ  chÃºng ta láº¥y argmax Ä‘á»ƒ nháº­n dá»± Ä‘oÃ¡n (lÆ°u Ã½ ráº±ng ta cÃ³ thá»ƒ láº¥y argmax trÃªn logits vÃ¬ softmax khÃ´ng thay Ä‘á»•i thá»© tá»±):

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

Thuá»™c tÃ­nh `model.config.id2label` chá»©a Ã¡nh xáº¡ cÃ¡c chá»‰ má»¥c tá»›i cÃ¡c nhÃ£n mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ hiá»ƒu cÃ¡c dá»± Ä‘oÃ¡n: 

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, cÃ³ 9 nhÃ£n: `O` lÃ  nhÃ£n cho cÃ¡c token khÃ´ng náº±m trong báº¥t ká»³ thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn nÃ o (nÃ³ lÃ  viáº¿t táº¯t cá»§a "outside" hay "bÃªn ngoÃ i") vÃ  sau Ä‘Ã³ chÃºng ta cÃ³ hai nhÃ£n cho má»—i loáº¡i thá»±c thá»ƒ (linh tinh, ngÆ°á»i , tá»• chá»©c vÃ  vá»‹ trÃ­). NhÃ£n `B-XXX` cho biáº¿t token náº±m á»Ÿ Ä‘áº§u thá»±c thá»ƒ `XXX` vÃ  nhÃ£n `I-XXX` cho biáº¿t token náº±m bÃªn trong thá»±c thá»ƒ `XXX`. VÃ­ dá»¥: trong máº«u hiá»‡n táº¡i, chÃºng ta kÃ¬ vá»ng mÃ´ hÃ¬nh phÃ¢n loáº¡i token `S` lÃ  `B-PER` (báº¯t Ä‘áº§u cá»§a má»™t thá»±c thá»ƒ ngÆ°á»i) vÃ  cÃ¡c token `##yl`,`##va` vÃ  `##in` lÃ  `I-PER` (bÃªn trong má»™t thá»±c thá»ƒ ngÆ°á»i).

Báº¡n cÃ³ thá»ƒ nghÄ© ráº±ng mÃ´ hÃ¬nh Ä‘Ã£ sai trong trÆ°á»ng há»£p nÃ y vÃ¬ nÃ³ Ä‘Ã£ gáº¯n nhÃ£n `I-PER` cho cáº£ bá»‘n token nÃ y, nhÆ°ng Ä‘iá»u Ä‘Ã³ khÃ´ng hoÃ n toÃ n Ä‘Ãºng. Thá»±c táº¿ cÃ³ hai Ä‘á»‹nh dáº¡ng cho cÃ¡c nhÃ£n `B-` vÃ  `I-` Ä‘Ã³ lÃ : *IOB1* vÃ  *IOB2*. Äá»‹nh dáº¡ng IOB2 (mÃ u há»“ng bÃªn dÆ°á»›i), lÃ  Ä‘á»‹nh dáº¡ng chÃºng ta Ä‘Ã£ giá»›i thiá»‡u trong khi á»Ÿ Ä‘á»‹nh dáº¡ng IOB1 (mÃ u xanh lam), cÃ¡c nhÃ£n báº¯t Ä‘áº§u báº±ng `B-` chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ¡ch hai thá»±c thá»ƒ liá»n ká» cÃ¹ng loáº¡i. MÃ´ hÃ¬nh chÃºng tÃ´i Ä‘ang sá»­ dá»¥ng Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh trÃªn táº­p dá»¯ liá»‡u báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng Ä‘Ã³, Ä‘Ã³ lÃ  lÃ½ do táº¡i sao nÃ³ gÃ¡n nhÃ£n `I-PER` cho mÃ£ thÃ´ng bÃ¡o `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

Vá»›i phÃ©p Ã¡nh xáº¡ nÃ y, chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á» tÃ¡i táº¡o láº¡i (gáº§n nhÆ° hoÃ n toÃ n) káº¿t quáº£ cá»§a pipeline Ä‘áº§u -- ta chá»‰ cáº§n láº¥y Ä‘iá»ƒm vÃ  nhÃ£n cá»§a má»—i token mÃ  khÃ´ng Ä‘Æ°á»£c phÃ¢n vÃ o `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

Äiá»u nÃ y ráº¥t giá»‘ng vá»›i nhá»¯ng gÃ¬ ta Ä‘Ã£ cÃ³ trÆ°á»›c Ä‘Ã¢y, ngoáº¡i trá»« má»™t ngoáº¡i lá»‡: pipeline cÅ©ng cung cáº¥p thÃ´ng tin vá» Ä‘iá»ƒm `start` hay `báº¯t Ä‘áº§u` vÃ  `end`  hay `káº¿t thÃºc` cá»§a má»—i thá»±c thá»ƒ trong cÃ¢u gá»‘c. ÄÃ¢y lÃ  lÃºc Ã¡nh xáº¡ bÃ¹ trá»« cá»§a chÃºng ta sáº½ phÃ¡t huy tÃ¡c dá»¥ng. Äá»ƒ cÃ³ Ä‘Æ°á»£c offset, chÃºng ta chá»‰ cáº§n Ä‘áº·t `return_offsets_mapping=True` khi chÃºng ta Ã¡p dá»¥ng tokenizer cho cÃ¡c Ä‘áº§u vÃ o cá»§a mÃ¬nh:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

Má»—i tuple lÃ  khoáº£ng vÄƒn báº£n tÆ°Æ¡ng á»©ng vá»›i má»—i token, trong Ä‘Ã³ `(0, 0)` Ä‘Æ°á»£c dÃ nh riÃªng cho cÃ¡c token Ä‘áº·c biá»‡t. TrÆ°á»›c Ä‘Ã¢y, chÃºng ta Ä‘Ã£ tháº¥y ráº±ng token á»Ÿ chá»‰ má»¥c 5 lÃ  `##yl`, cÃ³ `(12, 14)` lÃ  cÃ¡c pháº§n bÃ¹ á»Ÿ Ä‘Ã¢y. Náº¿u chÃºng ta láº¥y pháº§n tÆ°Æ¡ng á»©ng trong máº«u cá»§a mÃ¬nh:


```py
example[12:14]
```

ta nháº­n Ä‘Æ°á»£c khoáº£ng vÄƒn báº£n thÃ­ch há»£p mÃ  khÃ´ng cÃ³ `##`:

```python out
yl
```

Sá»­ dá»¥ng Ä‘iá»u nÃ y, bÃ¢y giá» chÃºng ta cÃ³ thá»ƒ hoÃ n thÃ nh cÃ¡c káº¿t quáº£ trÆ°á»›c Ä‘Ã³:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ÄÃ¢y giá»‘ng nhÆ° nhá»¯ng gÃ¬ chÃºng ta cÃ³ Ä‘Æ°á»£c tá»« pipeline Ä‘áº§u tiÃªn!

### NhÃ³m cÃ¡c thá»±c thá»ƒ

Sá»­ dá»¥ng cÃ¡c offset Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘iá»ƒm báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cho má»—i thá»±c thá»ƒ lÃ  ráº¥t tiá»‡n dá»¥ng, nhÆ°ng thÃ´ng tin Ä‘Ã³ khÃ´ng hoÃ n toÃ n cáº§n thiáº¿t. Tuy nhiÃªn, khi chÃºng ta muá»‘n nhÃ³m cÃ¡c thá»±c thá»ƒ láº¡i vá»›i nhau, viá»‡c offset sáº½ giÃºp chÃºng ta tiáº¿t kiá»‡m ráº¥t nhiá»u Ä‘oáº¡n mÃ£ lá»™n xá»™n. VÃ­ dá»¥: náº¿u chÃºng ta muá»‘n nhÃ³m cÃ¡c token `Hu`, `##gging` vÃ  `Face` láº¡i vá»›i nhau, chÃºng ta cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¡c quy táº¯c Ä‘áº·c biá»‡t nÃ³i ráº±ng hai token Ä‘áº§u tiÃªn pháº£i Ä‘Æ°á»£c Ä‘Ã­nh kÃ¨m trong khi xÃ³a dáº¥u `##` vÃ  `Face` nÃªn Ä‘Æ°á»£c thÃªm má»™t khoáº£ng tráº¯ng vÃ¬ nÃ³ khÃ´ng báº¯t Ä‘áº§u báº±ng `##` - nhÆ°ng Ä‘iá»u Ä‘Ã³ sáº½ chá»‰ hoáº¡t Ä‘á»™ng Ä‘á»‘i vá»›i loáº¡i tokenizer cá»¥ thá»ƒ nÃ y. ChÃºng ta sáº½ pháº£i viáº¿t má»™t bá»™ quy táº¯c khÃ¡c cho SentencePiece hoáº·c Byte-Pair-Encoding (sáº½ Ä‘Æ°á»£c tháº£o luáº­n á»Ÿ pháº§n sau cá»§a chÆ°Æ¡ng nÃ y).

Vá»›i offset, táº¥t cáº£ mÃ£ tÃ¹y chá»‰nh Ä‘Ã³ sáº½ biáº¿n máº¥t: chÃºng ta chá»‰ cÃ³ thá»ƒ láº¥y khoáº£ng trong vÄƒn báº£n gá»‘c báº¯t Ä‘áº§u báº±ng token Ä‘áº§u tiÃªn vÃ  káº¿t thÃºc báº±ng token cuá»‘i cÃ¹ng. VÃ¬ váº­y, trong trÆ°á»ng há»£p cÃ¡c mÃ£ thÃ´ng bÃ¡o `Hu`, `##gging` vÃ  `Face`, chÃºng ta nÃªn báº¯t Ä‘áº§u á»Ÿ kÃ½ tá»± 33 (Ä‘áº§u cá»§a `Hu`) vÃ  káº¿t thÃºc trÆ°á»›c kÃ½ tá»± 45 (cuá»‘i cá»§a `Face`) :

```py
example[33:45]
```

```python out
Hugging Face
```

Äá»ƒ viáº¿t Ä‘oáº¡n mÃ£ háº­u xá»­ lÃ½ cÃ¡c dá»± Ä‘oÃ¡n trong khi nhÃ³m cÃ¡c thá»±c thá»ƒ, ta sáº½ nhÃ³m cÃ¡c thá»±c thá»ƒ liÃªn tiáº¿p vÃ  cÃ³ nhÃ£n `I-XXX` vá»›i nhau trá»« khi nÃ³ lÃ  tá»« Ä‘áº§u tiÃªn, Ä‘Æ°á»£c gÃ¡n nhÃ£n `B-XXX` hoáº·c `I-XXX` (Ä‘á»ƒ ta cÃ³ thá»ƒ dá»«ng nhÃ³m má»™t thá»±c thá»ƒ khi nháº­n Ä‘Æ°á»£c `O`, má»™t kiá»ƒu thá»±c thá»ƒ má»›i, hoáº·c má»™t `B-XXX` cho ta biáº¿t thá»±c thá»ƒ cÃ³ kiá»ƒu giá»‘ng vá»›i Ä‘iá»ƒm báº¯t Ä‘áº§u):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # XoÃ¡ B- hoáº·c I-
        label = label[2:]
        start, _ = offsets[idx]

        # Láº¥y táº¥t cáº£ cÃ¡c tokens cÃ³ nhÃ£n I-
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # Äiá»ƒm lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a táº¥t cáº£ Ä‘iá»ƒm cá»§a cÃ¡c token trong thá»±c thá»ƒ Ä‘Æ°á»£c nhÃ³m Ä‘Ã³
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

VÃ  chÃºng ta nháº­n Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± nhÆ° vá»›i pipeline thá»© hai cá»§a mÃ¬nh!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

Má»™t vÃ­ dá»¥ khÃ¡c vá» tÃ¡c vá»¥ mÃ  nhá»¯ng offset nÃ y cá»±c ká»³ há»¯u Ã­ch lÃ  há»i Ä‘Ã¡p. ÄÃ o sÃ¢u vÃ o pipeline nÃ y, chÃºng ta sáº½ thá»±c hiá»‡n trong pháº§n tiáº¿p theo, cÅ©ng sáº½ cho phÃ©p chÃºng ta xem xÃ©t má»™t tÃ­nh nÄƒng cuá»‘i cÃ¹ng cá»§a cÃ¡c tokenizers trong thÆ° viá»‡n ğŸ¤— Transformers: xá»­ lÃ½ cÃ¡c token trÃ n khi chÃºng ta cáº¯t bá»›t má»™t Ä‘áº§u vÃ o Ä‘áº¿n má»™t Ä‘á»™ dÃ i nháº¥t Ä‘á»‹nh.
