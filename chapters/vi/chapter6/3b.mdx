<FrameworkSwitchCourse {fw} />

# Fast tokenizers in the QA pipeline

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_tf.ipynb"},
]} />

{/if}

Giá» chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o pipeline `question-answering` vÃ  xem cÃ¡ch táº­n dá»¥ng cÃ¡c offset Ä‘á»ƒ láº¥y cÃ¢u tráº£ lá»i cho cÃ¡c cÃ¢u há»i dá»±a theo tá»« ngá»¯ cáº£nh, giá»‘ng nhÆ° chÃºng ta Ä‘Ã£ lÃ m vá»›i cÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c nhÃ³m trong pháº§n trÆ°á»›c. Sau Ä‘Ã³, chÃºng ta sáº½ xem lÃ m tháº¿ nÃ o cÃ³ thá»ƒ Ä‘á»‘i phÃ³ vá»›i nhá»¯ng ngá»¯ cáº£nh ráº¥t dÃ i mÃ  cuá»‘i cÃ¹ng láº¡i bá»‹ cáº¯t bá»›t. Báº¡n cÃ³ thá»ƒ bá» qua pháº§n nÃ y náº¿u khÃ´ng quan tÃ¢m Ä‘áº¿n tÃ¡c vá»¥ há»i Ä‘Ã¡p.

{#if fw === 'pt'}

<Youtube id="_wxyB3j3mk4"/>

{:else}

<Youtube id="b3u8RzBCX9Y"/>

{/if}

## Sá»­ dá»¥ng pipeline `question-answering`

NhÆ° Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 1](/course/chapter1), ta cÃ³ thá»ƒ sá»­ dá»¥ng pipeline `question-answering` nhÆ° sau Ä‘á»ƒ nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i cho cÃ¢u há»i:

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

KhÃ´ng nhÆ° cÃ¡c pipeline khÃ¡c khÃ´ng thá»ƒ cáº¯t gá»n vÃ  chia vÄƒn báº£n dÃ i hÆ¡n Ä‘á»™ dÃ i tá»‘i Ä‘a cho phÃ©p cá»§a mÃ´ hÃ¬nh (dáº«n Ä‘áº¿n bá» lá»¡ nhá»¯ng thÃ´ng tin á»Ÿ pháº§n cuá»‘i vÄƒn báº£n), pipeline nÃ y cÃ³ thá»ƒ xá»­ lÃ½ tá»‘t vá»›i nhá»¯ng ngá»¯ cáº£nh dÃ i vÃ  sáº½ tráº£ vá» cÃ¢u tráº£ lá»i ká»ƒ cáº£ khi nÃ³ náº±m á»Ÿ cuá»‘i vÄƒn báº£n:

```py
long_context = """
ğŸ¤— Transformers: State of the Art NLP

ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

HÃ£y cÃ¹ng nhau xem nÃ³ lÃ m tháº¿ nÃ o!

## Sá»­ dá»¥ng mÃ´ hÃ¬nh cho tÃ¡c vá»¥ há»i Ä‘Ã¡p

NhÆ° nhá»¯ng pipeline khÃ¡c, ta sáº½ báº¯t Ä‘áº§u vá»›i viá»‡c tokenize Ä‘áº§u vÃ o vÃ  sau Ä‘Ã³ truyá»n chÃºng vÃ o trong mÃ´ hÃ¬nh. Máº·c Ä‘á»‹nh checkpoint Ä‘Æ°á»£c sá»­ dá»¥ng cho pipeline `question-answering` lÃ  [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) ( "squad" trong tÃªn báº¯t nguá»“n tá»« bá»™ dá»¯ liá»‡u mÃ  mÃ´ hÃ¬nh sá»­ dá»¥ng Ä‘á»ƒ tinh chá»‰nh; ta sáº½ nÃ³i sÃ¢u hÆ¡n vá» bá»™ dá»¯ liá»‡u SQuAD nÃ y á»Ÿ  [ChÆ°Æ¡ng 7](/course/chapter7/7)):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)
```

{/if}

LÆ°u Ã½ ráº±ng chÃºng ta tokenize cÃ¢u há»i vÃ  ngá»¯ cáº£nh nhÆ° má»™t cáº·p, vá»›i cÃ¢u há»i Ä‘á»©ng trÆ°á»›c.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

CÃ¡c mÃ´ hÃ¬nh há»i Ä‘Ã¡p hoáº¡t Ä‘á»™ng hÆ¡i khÃ¡c so vá»›i cÃ¡c mÃ´ hÃ¬nh mÃ  ta Ä‘Ã£ tháº¥y cho Ä‘áº¿n nay. Sá»­ dá»¥ng hÃ¬nh trÃªn lÃ m vÃ­ dá»¥, mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n chá»‰ má»¥c cá»§a token báº¯t Ä‘áº§u cÃ¢u tráº£ lá»i (á»Ÿ Ä‘Ã¢y lÃ  21) vÃ  chá»‰ má»¥c cá»§a token nÆ¡i cÃ¢u tráº£ lá»i káº¿t thÃºc (á»Ÿ Ä‘Ã¢y lÃ  24). ÄÃ¢y lÃ  lÃ½ do táº¡i sao cÃ¡c mÃ´ hÃ¬nh Ä‘Ã³ khÃ´ng tráº£ vá» má»™t tensor logit mÃ  lÃ  hai: má»™t cho cÃ¡c logit tÆ°Æ¡ng á»©ng vá»›i token báº¯t Ä‘áº§u cá»§a cÃ¢u tráº£ lá»i vÃ  má»™t cho cÃ¡c cÃ¡c logit tÆ°Æ¡ng á»©ng vá»›i token káº¿t thÃºc cá»§a cÃ¢u tráº£ lá»i. VÃ¬ trong trÆ°á»ng há»£p nÃ y, chÃºng ta chá»‰ cÃ³ má»™t Ä‘áº§u vÃ o chá»©a 66 token, ta nháº­n Ä‘Æ°á»£c:

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

{:else}

```python out
(1, 66) (1, 66)
```

{/if}

Äá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c logit Ä‘Ã³ thÃ nh xÃ¡c suáº¥t, chÃºng ta sáº½ Ã¡p dá»¥ng má»™t hÃ m softmax - nhÆ°ng trÆ°á»›c Ä‘Ã³, chÃºng ta cáº§n Ä‘áº£m báº£o ráº±ng chÃºng ta che dáº¥u cÃ¡c chá»‰ má»¥c khÃ´ng pháº£i lÃ  má»™t pháº§n cá»§a ngá»¯ cáº£nh. Äáº§u vÃ o cá»§a chÃºng tÃ´i lÃ  `[CLS] question [SEP] context [SEP]`, vÃ¬ váº­y chÃºng ta cáº§n che dáº¥u cÃ¡c token cá»§a cÃ¢u há»i cÅ©ng nhÆ° token `[SEP]`. Tuy nhiÃªn, chÃºng ta sáº½ giá»¯ token `[CLS]` vÃ¬ má»™t sá»‘ mÃ´ hÃ¬nh sá»­ dá»¥ng nÃ³ Ä‘á»ƒ chá»‰ ra ráº±ng cÃ¢u tráº£ lá»i khÃ´ng náº±m trong ngá»¯ cáº£nh.

VÃ¬ chÃºng ta sáº½ Ã¡p dá»¥ng softmax sau Ä‘Ã³, chÃºng ta chá»‰ cáº§n thay tháº¿ cÃ¡c logit muá»‘n che báº±ng má»™t sá»‘ Ã¢m lá»›n. á» Ä‘Ã¢y, chÃºng ta sá»­ dá»¥ng `-10000`:

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
# Che táº¥t cáº£ má»i thá»© trá»« token cá»§a ngá»¯ cáº£nh
mask = [i != 1 for i in sequence_ids]
# Hiá»ƒn thá»‹ token [CLS]
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Che táº¥t cáº£ má»i thá»© trá»« token cá»§a ngá»¯ cáº£nh
mask = [i != 1 for i in sequence_ids]
# Hiá»ƒn thá»‹ token [CLS]
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Giá» chÃºng ta Ä‘Ã£ che cÃ¡c logit tÆ°Æ¡ng á»©ng vá»›i cÃ¡c vá»‹ trÃ­ mÃ  chÃºng ta khÃ´ng muá»‘n dá»± Ä‘oÃ¡n, chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng softmax:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

á» giai Ä‘oáº¡n nÃ y, chÃºng ta cÃ³ thá»ƒ láº¥y argmax xÃ¡c suáº¥t báº¯t Ä‘áº§u vÃ  káº¿t thÃºc - nhÆ°ng chÃºng ta cÃ³ thá»ƒ káº¿t thÃºc vá»›i chá»‰ má»¥c báº¯t Ä‘áº§u lá»›n hÆ¡n káº¿t thÃºc, vÃ¬ váº­y chÃºng ta cáº§n thá»±c hiá»‡n thÃªm má»™t sá»‘ biá»‡n phÃ¡p phÃ²ng ngá»«a. ChÃºng ta sáº½ tÃ­nh toÃ¡n xÃ¡c suáº¥t cá»§a tá»«ng `start_index` vÃ  `end_index` cÃ³ thá»ƒ trong Ä‘Ã³ `start_index <= end_index`, sau Ä‘Ã³ láº¥y `(start_index, end_index)` vá»›i xÃ¡c suáº¥t cao nháº¥t.

Giáº£ sá»­ cÃ¡c sá»± kiá»‡n "CÃ¢u tráº£ lá»i báº¯t Ä‘áº§u á»Ÿ `start_index`" vÃ  "CÃ¢u tráº£ lá»i káº¿t thÃºc á»Ÿ `end_index`" lÃ  Ä‘á»™c láº­p, xÃ¡c suáº¥t Ä‘á»ƒ cÃ¢u tráº£ lá»i báº¯t Ä‘áº§u táº¡i `start_index` vÃ  káº¿t thÃºc táº¡i `end_index` lÃ :

$$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]$$ 

VÃ¬ váº­y, Ä‘á»ƒ tÃ­nh táº¥t cáº£ cÃ¡c Ä‘iá»ƒm, chÃºng ta chá»‰ cáº§n tÃ­nh tÃ­ch \\(\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]\\) vá»›i `start_index <= end_index`.

Äáº§u tiÃªn, hÃ£y tÃ­nh toÃ¡n táº¥t cáº£ cÃ¡c Ä‘áº§u ra cÃ³ thá»ƒ cÃ³:

```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

{#if fw === 'pt'}

Sau Ä‘Ã³, chÃºng tÃ´i sáº½ che cÃ¡c giÃ¡ trá»‹ trong Ä‘Ã³ `start_index > end_index` báº±ng cÃ¡ch Ä‘áº·t chÃºng thÃ nh `0` (cÃ¡c xÃ¡c suáº¥t khÃ¡c Ä‘á»u lÃ  sá»‘ dÆ°Æ¡ng). HÃ m `torch.triu()` tráº£ vá» pháº§n tam giÃ¡c phÃ­a trÃªn cá»§a tensor 2D Ä‘Æ°á»£c truyá»n dÆ°á»›i dáº¡ng tham sá»‘, vÃ¬ váº­y nÃ³ sáº½ thá»±c hiá»‡n viá»‡c che Ä‘Ã³ cho chÃºng ta:

```py
scores = torch.triu(scores)
```

{:else}

Sau Ä‘Ã³, chÃºng tÃ´i sáº½ che cÃ¡c giÃ¡ trá»‹ trong Ä‘Ã³ `start_index > end_index` báº±ng cÃ¡ch Ä‘áº·t chÃºng thÃ nh `0` (cÃ¡c xÃ¡c suáº¥t khÃ¡c Ä‘á»u lÃ  sá»‘ dÆ°Æ¡ng). HÃ m `np.triu()` tráº£ vá» pháº§n tam giÃ¡c phÃ­a trÃªn cá»§a tensor 2D Ä‘Æ°á»£c truyá»n dÆ°á»›i dáº¡ng tham sá»‘, vÃ¬ váº­y nÃ³ sáº½ thá»±c hiá»‡n viá»‡c che Ä‘Ã³ cho chÃºng ta:

```py
import numpy as np

scores = np.triu(scores)
```

{/if}

BÃ¢y giá» chÃºng ta chá»‰ cáº§n láº¥y chá»‰ má»¥c tá»‘i Ä‘a. VÃ¬ PyTorch sáº½ tráº£ vá» chá»‰ má»¥c trong tensor pháº³ng, chÃºng ta cáº§n sá»­ dá»¥ng phÃ©p chia lÃ m trÃ²n xuá»‘ng `//` vÃ  láº¥y dÆ° `%` Ä‘á»ƒ nháº­n Ä‘Æ°á»£c `start_index` vÃ  `end_index`:

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

ChÃºng ta chÆ°a xong Ä‘Ã¢u, nhÆ°ng Ã­t nháº¥t chÃºng ta Ä‘Ã£ cÃ³ Ä‘iá»ƒm chÃ­nh xÃ¡c cho cÃ¢u tráº£ lá»i (báº¡n cÃ³ thá»ƒ kiá»ƒm tra Ä‘iá»u nÃ y báº±ng cÃ¡ch so sÃ¡nh nÃ³ vá»›i káº¿t quáº£ Ä‘áº§u tiÃªn trong pháº§n trÆ°á»›c):

```python out
0.97773
```

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** TÃ­nh chá»‰ má»¥c báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cho nÄƒm cáº¥u tráº£ lá»i Ä‘áº§u tiá»‡n.

</Tip>

Ta cÃ³ `start_index` vÃ  `end_index` cá»§a cÃ¢u tráº£ lá»i theo token nÃªn ta chá»‰ cáº§n chuyá»ƒn Ä‘á»•i cÃ¡c chá»‰ má»¥c kÃ­ tá»± trong ngá»¯ cáº£nh. Äáº¥y lÃ  nÆ¡i offset sáº½ cá»±c kÃ¬ há»¯u Ã­ch. Ta cÃ³ thá»ƒ láº¥y vÃ  sá»­ dá»¥ng chÃºng nhÆ° cÃ¡ch ta lÃ m trong tÃ¡c vá»¥ phÃ¢n loáº¡i token:

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

BÃ¢y giá» chÃºng ta chá»‰ cáº§n Ä‘á»‹nh dáº¡ng má»i thá»© Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£:

```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

Tuyá»‡t quÃ¡! Káº¿t quáº£ Ä‘Ã³ giá»‘ng nhÆ° trong vÃ­ dá»¥ Ä‘áº§u tiÃªn cá»§a chÃºng ta!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Sá»­ dá»¥ng Ä‘iá»ƒm tá»‘t nháº¥t mÃ  báº¡n Ä‘Ã£ tÃ­nh toÃ¡n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ hiá»ƒn thá»‹ nÄƒm cÃ¢u tráº£ lá»i cÃ³ kháº£ nÄƒng nháº¥t. Äá»ƒ kiá»ƒm tra káº¿t quáº£ cá»§a báº¡n, hÃ£y quay láº¡i Ä‘Æ°á»ng dáº«n Ä‘áº§u tiÃªn vÃ  truyá»n vÃ o `top_k=5` khi gá»i nÃ³.

</Tip>

## Xá»­ lÃ½ cÃ¡c ngá»¯ cáº£nh dÃ i

Náº¿u chÃºng ta cá»‘ gáº¯ng tokenize cÃ¡c cÃ¢u há»i vÃ  ngá»¯ cáº£nh dÃ i ta tá»«ng láº¥y lÃ m vÃ­ dá»¥ trÆ°á»›c Ä‘Ã³, ta sáº½ nháº­n Ä‘Æ°á»£c sá»‘ token nhiá»u hÆ¡n Ä‘á»™ dÃ i tá»‘i da sá»­ dá»¥ng trong pipeline `question-answering` (Ä‘Ã³ lÃ  384):

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

VÃ¬ váº­y, chÃºng ta sáº½ cáº§n pháº£i cáº¯t bá»›t Ä‘áº§u vÃ o cá»§a mÃ¬nh á»Ÿ Ä‘á»™ dÃ i tá»‘i Ä‘a Ä‘Ã³. CÃ³ má»™t sá»‘ cÃ¡ch ta cÃ³ thá»ƒ lÃ m Ä‘iá»u nÃ y, nhÆ°ng chÃºng ta khÃ´ng muá»‘n cáº¯t ngáº¯n cÃ¢u há»i, chá»‰ cáº¯t bá» ngá»¯ cáº£nh. VÃ¬ ngá»¯ cáº£nh lÃ  cÃ¢u thá»© hai, chÃºng ta sáº½ sá»­ dá»¥ng chiáº¿n lÆ°á»£c cáº¯t ngáº¯n `"only_second"`. Váº¥n Ä‘á» náº£y sinh sau Ä‘Ã³ lÃ  cÃ¢u tráº£ lá»i cho cÃ¢u há»i cÃ³ thá»ƒ khÃ´ng náº±m trong ngá»¯ cáº£nh Ä‘Ã£ bá»‹ cáº¯t ngáº¯n. VÃ­ dá»¥: á»Ÿ Ä‘Ã¢y, chÃºng ta Ä‘Ã£ chá»n má»™t cÃ¢u há»i trong Ä‘Ã³ cÃ¢u tráº£ lá»i náº±m á»Ÿ cuá»‘i ngá»¯ cáº£nh vÃ  khi cáº¯t ngáº¯n cÃ¢u tráº£ lá»i Ä‘Ã³ thÃ¬ cÃ¢u tráº£ lá»i khÃ´ng cÃ²n:

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""
```

Äiá»u nÃ y cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh sáº½ gáº·p khÃ³ khÄƒn trong viá»‡c chá»n ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c. Äá»ƒ kháº¯c phá»¥c Ä‘iá»u nÃ y, pipeline há»i Ä‘Ã¡p cho phÃ©p chÃºng ta chia ngá»¯ cáº£nh thÃ nh cÃ¡c pháº§n nhá» hÆ¡n, chá»‰ Ä‘á»‹nh Ä‘á»™ dÃ i tá»‘i Ä‘a. Äá»ƒ Ä‘áº£m báº£o ráº±ng chÃºng ta khÃ´ng chia bá»‘i cáº£nh chÃ­nh xÃ¡c á»Ÿ vá»‹ trÃ­ sai Ä‘á»ƒ cÃ³ thá»ƒ tÃ¬m ra cÃ¢u tráº£ lá»i, nÃ³ cÅ©ng bao gá»“m má»™t sá»‘ pháº§n trÃ¹ng láº·p giá»¯a cÃ¡c pháº§n.

ChÃºng ta cÃ³ thá»ƒ yÃªu cáº§u tokenizer (nhanh hoáº·c cháº­m) thá»±c hiá»‡n viá»‡c nÃ y báº±ng cÃ¡ch thÃªm `return_overflowing_tokens=True` vÃ  ta cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh sá»± giao thoa mÃ  ta muá»‘n qua than sá»‘ `stride`. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥, sá»­ dá»¥ng má»™t cÃ¢u nhá» hÆ¡n:

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

CÃ³ thá»ƒ tháº¥y, cÃ¢u Ä‘Ã£ bá»‹ chia thÃ nh cÃ¡c Ä‘oáº¡n sao cho má»—i pháº§n trong `inputs["input_ids"]` cÃ³ nhiá»u nháº¥t 6 token (ta sáº½ cáº§n thÃªm Ä‘á»‡m Ä‘á»ƒ Ä‘áº£m báº£o chÃºng cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c) vÃ  sáº½ cÃ³ sá»­ giao thoa cá»§a 2 token giá»¯a cÃ¡c pháº§n.

HÃ£y cÃ¹ng nhÃ¬n kÄ© hÆ¡n vÃ o káº¿t quáº£ tokenize:

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

NhÆ° dá»± Ä‘oÃ¡n, ta nháº­n Ä‘Æ°á»£c ID Ä‘áº§u vÃ o vÃ  attention mask.á» Ä‘Ã¢y, `overflow_to_sample_mapping` lÃ  má»™t phÃ©p Ã¡nh xáº¡ cho ta biáº¿t cÃ¢u nÃ o trong káº¿t quáº£ liÃªn quan -- ta cÃ³ 7 káº¿t quáº£ dá»u tá»« cÃ¢u mÃ  ta truyá»n vÃ o tokenizer:

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

Äiá»u nÃ y há»¯u Ã­ch hÆ¡n khi ta tokenize nhiá»u cÃ¢u cÃ¹ng nhau, VÃ­ dá»¥:

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

tráº£ cho ta:

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

nghÄ©a lÃ  cÃ¢u Ä‘áº§u tiÃªn Ä‘Æ°á»£c chia thÃ nh 7 Ä‘oáº¡n nhÆ° pháº§n phÃ­a trÆ°á»›c, vÃ  4 Ä‘oáº¡n tiáº¿p theo Ä‘áº¿n tá»« cÃ¢u thá»© hai.

BÃ¢y giá» chÃºng ta hÃ£y cÃ¹ng quay trá»Ÿ láº¡i ngá»¯ cáº£nh dÃ i. Theo máº·c Ä‘á»‹nh, pipeline ``question-answering` sá»­ dá»¥ng Ä‘á»™ dÃ i tá»‘i Ä‘a lÃ  384, nhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³ vÃ  khoáº£ng cÃ¡ch 128, tÆ°Æ¡ng á»©ng vá»›i cÃ¡ch mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh (báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘Ã³ báº±ng cÃ¡ch truyá»n `max_seq_len` vÃ  `stride` khi gá»i pipeline). Do Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng cÃ¡c tham sá»‘ Ä‘Ã³ khi tokenize. ChÃºng ta cÅ©ng sáº½ thÃªm pháº§n Ä‘á»‡m (Ä‘á»ƒ cÃ³ cÃ¡c máº«u cÃ³ cÃ¹ng chiá»u dÃ i, vÃ¬ váº­y chÃºng ta cÃ³ thá»ƒ táº¡o ra cÃ¡c tensor) cÅ©ng nhÆ° yÃªu cáº§u cÃ¡c offset:

```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

CÃ¡c `inputs` sáº½ chá»©a cÃ¡c ID Ä‘áº§u vÃ o vÃ  cÃ¡c attention mask mÃ  mÃ´ hÃ¬nh kÃ¬ vá»ng, cÅ©ng nhÆ° offset vÃ  `overflow_to_sample_mapping` ta vá»«a trao Ä‘á»•i á»Ÿ trÃªn. VÃ¬ hai tham sá»‘ Ä‘Ã³ khÃ´ng pháº£i lÃ  tham sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi mÃ´ hÃ¬nh, chÃºng ta sáº½ Ä‘Æ°a chÃºng ra khá»i `inputs` (vÃ  khÃ´ng lÆ°u trá»¯ Ã¡nh xáº¡, vÃ¬ nÃ³ khÃ´ng há»¯u Ã­ch á»Ÿ Ä‘Ã¢y) trÆ°á»›c khi chuyá»ƒn Ä‘á»•i nÃ³ thÃ nh tensor:

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python out
(2, 384)
```

{/if}

Bá»‘i cáº£nh dÃ i cá»§a chÃºng ta Ä‘Æ°á»£c chia lÃ m hai, Ä‘á»“ng nghÄ©a sau khi nÃ³ Ä‘i qua mÃ´ hÃ¬nh, chÃºng ta sáº½ cÃ³ hai bá»™ logit báº¯t Ä‘áº§u vÃ  káº¿t thÃºc:

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python out
(2, 384) (2, 384)
```

{/if}

Giá»‘ng nhÆ° trÆ°á»›c Ä‘Ã¢y, Ä‘áº§u tiÃªn chÃºng ta che cÃ¡c token khÃ´ng pháº£i lÃ  má»™t pháº§n cá»§a ngá»¯ cáº£nh trÆ°á»›c khi sá»­ dá»¥ng softmax. ChÃºng ta cÅ©ng che táº¥t cáº£ cÃ¡c token Ä‘á»‡m (Ä‘Æ°á»£c gáº¯n mÃ¡c bá»Ÿi attention mask):

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
# Che táº¥t cáº£ má»i thá»© trá»« token cá»§a ngá»¯ cáº£nh
mask = [i != 1 for i in sequence_ids]
# Hiá»ƒn thá»‹ token [CLS]
mask[0] = False
# Che táº¥t cáº£ token [PAD]
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
# Che táº¥t cáº£ má»i thá»© trá»« token cá»§a ngá»¯ cáº£nh
mask = [i != 1 for i in sequence_ids]
# Hiá»ƒn thá»‹ token [CLS]
mask[0] = False
# Che táº¥t cáº£ token [PAD]
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng softmax Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c logit cá»§a chÃºng ta thÃ nh xÃ¡c suáº¥t:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()
```

{/if}

BÆ°á»›c tiáº¿p theo tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng gÃ¬ chÃºng ta Ä‘Ã£ lÃ m cho bá»‘i cáº£nh nhá», nhÆ°ng chÃºng ta láº·p láº¡i nÃ³ cho má»—i pháº§n trong hai pháº§n cá»§a mÃ¬nh. ChÃºng ta tÃ­nh Ä‘iá»ƒm cho táº¥t cáº£ cÃ¡c khoáº£ng cÃ¢u tráº£ lá»i cÃ³ thá»ƒ cÃ³, sau Ä‘Ã³ láº¥y pháº§n cÃ³ Ä‘iá»ƒm tá»‘t nháº¥t:

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

Hai á»©ng cá»­ viÃªn Ä‘Ã³ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c cÃ¢u tráº£ lá»i tá»‘t nháº¥t mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ tÃ¬m tháº¥y trong má»—i Ä‘oáº¡n. MÃ´ hÃ¬nh cháº¯c cháº¯n hÆ¡n ráº±ng cÃ¢u tráº£ lá»i Ä‘Ãºng náº±m á»Ÿ pháº§n thá»© hai (Ä‘Ã³ lÃ  má»™t dáº¥u hiá»‡u tá»‘t!). BÃ¢y giá» chÃºng ta chá»‰ cáº§n Ã¡nh xáº¡ khoáº£ng hai token Ä‘Ã³ vá»›i khoáº£ng cÃ¡c kÃ½ tá»± trong ngá»¯ cáº£nh (chÃºng ta chá»‰ cáº§n láº­p Ã¡nh xáº¡ cÃ¡i thá»© hai Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i, nhÆ°ng tháº­t thÃº vá»‹ khi xem mÃ´ hÃ¬nh Ä‘Ã£ chá»n nhá»¯ng gÃ¬ trong Ä‘oáº¡n Ä‘áº§u tiÃªn).

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** HÃ£y Ä‘iá»u chá»‰nh Ä‘oáº¡n mÃ£ trÃªn Ä‘á»ƒ tráº£ vá» Ä‘iá»ƒm vÃ  khoáº£ng cho nÄƒm cÃ¢u tráº£ lá»i cÃ³ nhiá»u kháº£ nÄƒng nháº¥t (tá»•ng cá»™ng, khÃ´ng pháº£i cho má»—i Ä‘oáº¡n).

</Tip>

`offsets` mÃ  chÃºng ta Ä‘Ã£ náº¯m Ä‘Æ°á»£c trÆ°á»›c Ä‘Ã³ thá»±c sá»± lÃ  má»™t danh sÃ¡ch cÃ¡c offset, vá»›i má»™t danh sÃ¡ch trÃªn má»—i Ä‘oáº¡n vÄƒn báº£n:

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python out
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

Náº¿u chÃºng ta bá» qua káº¿t quáº£ Ä‘áº§u tiÃªn, chÃºng ta sáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± nhÆ° pipeline cho ngá»¯ cáº£nh dÃ i nÃ y - yayy!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Sá»­ dá»¥ng Ä‘iá»ƒm tá»‘t nháº¥t báº¡n Ä‘Ã£ tÃ­nh toÃ¡n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ hiá»ƒn thá»‹ nÄƒm cÃ¢u tráº£ lá»i cÃ³ kháº£ nÄƒng xáº£y ra nháº¥t (cho toÃ n bá»™ ngá»¯ cáº£nh, khÃ´ng pháº£i tá»«ng Ä‘oáº¡n). Äá»ƒ kiá»ƒm tra káº¿t quáº£ cá»§a báº¡n, hÃ£y quay láº¡i pipeline Ä‘áº§u tiÃªn vÃ  truyá»n vÃ o `top_k=5` khi gá»i nÃ³.

</Tip>

Äiá»u nÃ y káº¿t thÃºc pháº§n Ä‘i sÃ¢u vÃ o cÃ¡c kháº£ nÄƒng cá»§a tokenizer. ChÃºng ta sáº½ Ä‘Æ°a táº¥t cáº£ nhá»¯ng Ä‘iá»u nÃ y vÃ o thá»±c táº¿ má»™t láº§n ná»¯a trong chÆ°Æ¡ng tiáº¿p theo, khi chÃºng tÃ´i hÆ°á»›ng dáº«n báº¡n cÃ¡ch tinh chá»‰nh má»™t mÃ´ hÃ¬nh vá» má»™t loáº¡t cÃ¡c tÃ¡c vá»¥ NLP phá»• biáº¿n.
