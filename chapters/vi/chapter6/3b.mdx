<FrameworkSwitchCourse {fw} />

# Fast tokenizers in the QA pipeline

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section3b_tf.ipynb"},
]} />

{/if}

Gi·ªù ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o pipeline `question-answering` v√† xem c√°ch t·∫≠n d·ª•ng c√°c offset ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi cho c√°c c√¢u h·ªèi d·ª±a theo t·ª´ ng·ªØ c·∫£nh, gi·ªëng nh∆∞ ch√∫ng ta ƒë√£ l√†m v·ªõi c√°c th·ª±c th·ªÉ ƒë∆∞·ª£c nh√≥m trong ph·∫ßn tr∆∞·ªõc. Sau ƒë√≥, ch√∫ng ta s·∫Ω xem l√†m th·∫ø n√†o c√≥ th·ªÉ ƒë·ªëi ph√≥ v·ªõi nh·ªØng ng·ªØ c·∫£nh r·∫•t d√†i m√† cu·ªëi c√πng l·∫°i b·ªã c·∫Øt b·ªõt. B·∫°n c√≥ th·ªÉ b·ªè qua ph·∫ßn n√†y n·∫øu kh√¥ng quan t√¢m ƒë·∫øn t√°c v·ª• h·ªèi ƒë√°p.

{#if fw === 'pt'}

<Youtube id="_wxyB3j3mk4"/>

{:else}

<Youtube id="b3u8RzBCX9Y"/>

{/if}

## S·ª≠ d·ª•ng pipeline `question-answering`

Nh∆∞ ƒë√£ th·∫•y trong [Ch∆∞∆°ng 1](/course/chapter1), ta c√≥ th·ªÉ s·ª≠ d·ª•ng pipeline `question-answering` nh∆∞ sau ƒë·ªÉ nh·∫≠n ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi:

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ü§ó Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

Kh√¥ng nh∆∞ c√°c pipeline kh√°c kh√¥ng th·ªÉ c·∫Øt g·ªçn v√† chia vƒÉn b·∫£n d√†i h∆°n ƒë·ªô d√†i t·ªëi ƒëa cho ph√©p c·ªßa m√¥ h√¨nh (d·∫´n ƒë·∫øn b·ªè l·ª° nh·ªØng th√¥ng tin ·ªü ph·∫ßn cu·ªëi vƒÉn b·∫£n), pipeline n√†y c√≥ th·ªÉ x·ª≠ l√Ω t·ªët v·ªõi nh·ªØng ng·ªØ c·∫£nh d√†i v√† s·∫Ω tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi k·ªÉ c·∫£ khi n√≥ n·∫±m ·ªü cu·ªëi vƒÉn b·∫£n:

```py
long_context = """
ü§ó Transformers: State of the Art NLP

ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

H√£y c√πng nhau xem n√≥ l√†m th·∫ø n√†o!

## S·ª≠ d·ª•ng m√¥ h√¨nh cho t√°c v·ª• h·ªèi ƒë√°p

Nh∆∞ nh·ªØng pipeline kh√°c, ta s·∫Ω b·∫Øt ƒë·∫ßu v·ªõi vi·ªác tokenize ƒë·∫ßu v√†o v√† sau ƒë√≥ truy·ªÅn ch√∫ng v√†o trong m√¥ h√¨nh. M·∫∑c ƒë·ªãnh checkpoint ƒë∆∞·ª£c s·ª≠ d·ª•ng cho pipeline `question-answering` l√† [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad) ( "squad" trong t√™n b·∫Øt ngu·ªìn t·ª´ b·ªô d·ªØ li·ªáu m√† m√¥ h√¨nh s·ª≠ d·ª•ng ƒë·ªÉ tinh ch·ªânh; ta s·∫Ω n√≥i s√¢u h∆°n v·ªÅ b·ªô d·ªØ li·ªáu SQuAD n√†y ·ªü  [Ch∆∞∆°ng 7](/course/chapter7/7)):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)
```

{/if}

L∆∞u √Ω r·∫±ng ch√∫ng ta tokenize c√¢u h·ªèi v√† ng·ªØ c·∫£nh nh∆∞ m·ªôt c·∫∑p, v·ªõi c√¢u h·ªèi ƒë·ª©ng tr∆∞·ªõc.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

C√°c m√¥ h√¨nh h·ªèi ƒë√°p ho·∫°t ƒë·ªông h∆°i kh√°c so v·ªõi c√°c m√¥ h√¨nh m√† ta ƒë√£ th·∫•y cho ƒë·∫øn nay. S·ª≠ d·ª•ng h√¨nh tr√™n l√†m v√≠ d·ª•, m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n ch·ªâ m·ª•c c·ªßa token b·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi (·ªü ƒë√¢y l√† 21) v√† ch·ªâ m·ª•c c·ªßa token n∆°i c√¢u tr·∫£ l·ªùi k·∫øt th√∫c (·ªü ƒë√¢y l√† 24). ƒê√¢y l√† l√Ω do t·∫°i sao c√°c m√¥ h√¨nh ƒë√≥ kh√¥ng tr·∫£ v·ªÅ m·ªôt tensor logit m√† l√† hai: m·ªôt cho c√°c logit t∆∞∆°ng ·ª©ng v·ªõi token b·∫Øt ƒë·∫ßu c·ªßa c√¢u tr·∫£ l·ªùi v√† m·ªôt cho c√°c c√°c logit t∆∞∆°ng ·ª©ng v·ªõi token k·∫øt th√∫c c·ªßa c√¢u tr·∫£ l·ªùi. V√¨ trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta ch·ªâ c√≥ m·ªôt ƒë·∫ßu v√†o ch·ª©a 66 token, ta nh·∫≠n ƒë∆∞·ª£c:

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

{:else}

```python out
(1, 66) (1, 66)
```

{/if}

ƒê·ªÉ chuy·ªÉn ƒë·ªïi c√°c logit ƒë√≥ th√†nh x√°c su·∫•t, ch√∫ng ta s·∫Ω √°p d·ª•ng m·ªôt h√†m softmax - nh∆∞ng tr∆∞·ªõc ƒë√≥, ch√∫ng ta c·∫ßn ƒë·∫£m b·∫£o r·∫±ng ch√∫ng ta che d·∫•u c√°c ch·ªâ m·ª•c kh√¥ng ph·∫£i l√† m·ªôt ph·∫ßn c·ªßa ng·ªØ c·∫£nh. ƒê·∫ßu v√†o c·ªßa ch√∫ng t√¥i l√† `[CLS] question [SEP] context [SEP]`, v√¨ v·∫≠y ch√∫ng ta c·∫ßn che d·∫•u c√°c token c·ªßa c√¢u h·ªèi c≈©ng nh∆∞ token `[SEP]`. Tuy nhi√™n, ch√∫ng ta s·∫Ω gi·ªØ token `[CLS]` v√¨ m·ªôt s·ªë m√¥ h√¨nh s·ª≠ d·ª•ng n√≥ ƒë·ªÉ ch·ªâ ra r·∫±ng c√¢u tr·∫£ l·ªùi kh√¥ng n·∫±m trong ng·ªØ c·∫£nh.

V√¨ ch√∫ng ta s·∫Ω √°p d·ª•ng softmax sau ƒë√≥, ch√∫ng ta ch·ªâ c·∫ßn thay th·∫ø c√°c logit mu·ªën che b·∫±ng m·ªôt s·ªë √¢m l·ªõn. ·ªû ƒë√¢y, ch√∫ng ta s·ª≠ d·ª•ng `-10000`:

{#if fw === 'pt'}

```py
import torch

sequence_ids = inputs.sequence_ids()
# Che t·∫•t c·∫£ m·ªçi th·ª© tr·ª´ token c·ªßa ng·ªØ c·∫£nh
mask = [i != 1 for i in sequence_ids]
# Hi·ªÉn th·ªã token [CLS]
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Che t·∫•t c·∫£ m·ªçi th·ª© tr·ª´ token c·ªßa ng·ªØ c·∫£nh
mask = [i != 1 for i in sequence_ids]
# Hi·ªÉn th·ªã token [CLS]
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Gi·ªù ch√∫ng ta ƒë√£ che c√°c logit t∆∞∆°ng ·ª©ng v·ªõi c√°c v·ªã tr√≠ m√† ch√∫ng ta kh√¥ng mu·ªën d·ª± ƒëo√°n, ch√∫ng ta c√≥ th·ªÉ √°p d·ª•ng softmax:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()
```

{/if}

·ªû giai ƒëo·∫°n n√†y, ch√∫ng ta c√≥ th·ªÉ l·∫•y argmax x√°c su·∫•t b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c - nh∆∞ng ch√∫ng ta c√≥ th·ªÉ k·∫øt th√∫c v·ªõi ch·ªâ m·ª•c b·∫Øt ƒë·∫ßu l·ªõn h∆°n k·∫øt th√∫c, v√¨ v·∫≠y ch√∫ng ta c·∫ßn th·ª±c hi·ªán th√™m m·ªôt s·ªë bi·ªán ph√°p ph√≤ng ng·ª´a. Ch√∫ng ta s·∫Ω t√≠nh to√°n x√°c su·∫•t c·ªßa t·ª´ng `start_index` v√† `end_index` c√≥ th·ªÉ trong ƒë√≥ `start_index <= end_index`, sau ƒë√≥ l·∫•y `(start_index, end_index)` v·ªõi x√°c su·∫•t cao nh·∫•t.

Gi·∫£ s·ª≠ c√°c s·ª± ki·ªán "C√¢u tr·∫£ l·ªùi b·∫Øt ƒë·∫ßu ·ªü `start_index`" v√† "C√¢u tr·∫£ l·ªùi k·∫øt th√∫c ·ªü `end_index`" l√† ƒë·ªôc l·∫≠p, x√°c su·∫•t ƒë·ªÉ c√¢u tr·∫£ l·ªùi b·∫Øt ƒë·∫ßu t·∫°i `start_index` v√† k·∫øt th√∫c t·∫°i `end_index` l√†:

$$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]$$ 

V√¨ v·∫≠y, ƒë·ªÉ t√≠nh t·∫•t c·∫£ c√°c ƒëi·ªÉm, ch√∫ng ta ch·ªâ c·∫ßn t√≠nh t√≠ch \\(\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]\\) v·ªõi `start_index <= end_index`.

ƒê·∫ßu ti√™n, h√£y t√≠nh to√°n t·∫•t c·∫£ c√°c ƒë·∫ßu ra c√≥ th·ªÉ c√≥:

```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

{#if fw === 'pt'}

Sau ƒë√≥, ch√∫ng t√¥i s·∫Ω che c√°c gi√° tr·ªã trong ƒë√≥ `start_index > end_index` b·∫±ng c√°ch ƒë·∫∑t ch√∫ng th√†nh `0` (c√°c x√°c su·∫•t kh√°c ƒë·ªÅu l√† s·ªë d∆∞∆°ng). H√†m `torch.triu()` tr·∫£ v·ªÅ ph·∫ßn tam gi√°c ph√≠a tr√™n c·ªßa tensor 2D ƒë∆∞·ª£c truy·ªÅn d∆∞·ªõi d·∫°ng tham s·ªë, v√¨ v·∫≠y n√≥ s·∫Ω th·ª±c hi·ªán vi·ªác che ƒë√≥ cho ch√∫ng ta:

```py
scores = torch.triu(scores)
```

{:else}

Sau ƒë√≥, ch√∫ng t√¥i s·∫Ω che c√°c gi√° tr·ªã trong ƒë√≥ `start_index > end_index` b·∫±ng c√°ch ƒë·∫∑t ch√∫ng th√†nh `0` (c√°c x√°c su·∫•t kh√°c ƒë·ªÅu l√† s·ªë d∆∞∆°ng). H√†m `np.triu()` tr·∫£ v·ªÅ ph·∫ßn tam gi√°c ph√≠a tr√™n c·ªßa tensor 2D ƒë∆∞·ª£c truy·ªÅn d∆∞·ªõi d·∫°ng tham s·ªë, v√¨ v·∫≠y n√≥ s·∫Ω th·ª±c hi·ªán vi·ªác che ƒë√≥ cho ch√∫ng ta:

```py
import numpy as np

scores = np.triu(scores)
```

{/if}

B√¢y gi·ªù ch√∫ng ta ch·ªâ c·∫ßn l·∫•y ch·ªâ m·ª•c t·ªëi ƒëa. V√¨ PyTorch s·∫Ω tr·∫£ v·ªÅ ch·ªâ m·ª•c trong tensor ph·∫≥ng, ch√∫ng ta c·∫ßn s·ª≠ d·ª•ng ph√©p chia l√†m tr√≤n xu·ªëng `//` v√† l·∫•y d∆∞ `%` ƒë·ªÉ nh·∫≠n ƒë∆∞·ª£c `start_index` v√† `end_index`:

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

Ch√∫ng ta ch∆∞a xong ƒë√¢u, nh∆∞ng √≠t nh·∫•t ch√∫ng ta ƒë√£ c√≥ ƒëi·ªÉm ch√≠nh x√°c cho c√¢u tr·∫£ l·ªùi (b·∫°n c√≥ th·ªÉ ki·ªÉm tra ƒëi·ªÅu n√†y b·∫±ng c√°ch so s√°nh n√≥ v·ªõi k·∫øt qu·∫£ ƒë·∫ßu ti√™n trong ph·∫ßn tr∆∞·ªõc):

```python out
0.97773
```

> [!TIP]
> ‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** T√≠nh ch·ªâ m·ª•c b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c cho nƒÉm c·∫•u tr·∫£ l·ªùi ƒë·∫ßu ti·ªán.

Ta c√≥ `start_index` v√† `end_index` c·ªßa c√¢u tr·∫£ l·ªùi theo token n√™n ta ch·ªâ c·∫ßn chuy·ªÉn ƒë·ªïi c√°c ch·ªâ m·ª•c k√≠ t·ª± trong ng·ªØ c·∫£nh. ƒê·∫•y l√† n∆°i offset s·∫Ω c·ª±c k√¨ h·ªØu √≠ch. Ta c√≥ th·ªÉ l·∫•y v√† s·ª≠ d·ª•ng ch√∫ng nh∆∞ c√°ch ta l√†m trong t√°c v·ª• ph√¢n lo·∫°i token:

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

B√¢y gi·ªù ch√∫ng ta ch·ªâ c·∫ßn ƒë·ªãnh d·∫°ng m·ªçi th·ª© ƒë·ªÉ c√≥ ƒë∆∞·ª£c k·∫øt qu·∫£:

```py
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}
```

Tuy·ªát qu√°! K·∫øt qu·∫£ ƒë√≥ gi·ªëng nh∆∞ trong v√≠ d·ª• ƒë·∫ßu ti√™n c·ªßa ch√∫ng ta!

> [!TIP]
> ‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** S·ª≠ d·ª•ng ƒëi·ªÉm t·ªët nh·∫•t m√† b·∫°n ƒë√£ t√≠nh to√°n tr∆∞·ªõc ƒë√≥ ƒë·ªÉ hi·ªÉn th·ªã nƒÉm c√¢u tr·∫£ l·ªùi c√≥ kh·∫£ nƒÉng nh·∫•t. ƒê·ªÉ ki·ªÉm tra k·∫øt qu·∫£ c·ªßa b·∫°n, h√£y quay l·∫°i ƒë∆∞·ªùng d·∫´n ƒë·∫ßu ti√™n v√† truy·ªÅn v√†o `top_k=5` khi g·ªçi n√≥.

## X·ª≠ l√Ω c√°c ng·ªØ c·∫£nh d√†i

N·∫øu ch√∫ng ta c·ªë g·∫Øng tokenize c√°c c√¢u h·ªèi v√† ng·ªØ c·∫£nh d√†i ta t·ª´ng l·∫•y l√†m v√≠ d·ª• tr∆∞·ªõc ƒë√≥, ta s·∫Ω nh·∫≠n ƒë∆∞·ª£c s·ªë token nhi·ªÅu h∆°n ƒë·ªô d√†i t·ªëi da s·ª≠ d·ª•ng trong pipeline `question-answering` (ƒë√≥ l√† 384):

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

V√¨ v·∫≠y, ch√∫ng ta s·∫Ω c·∫ßn ph·∫£i c·∫Øt b·ªõt ƒë·∫ßu v√†o c·ªßa m√¨nh ·ªü ƒë·ªô d√†i t·ªëi ƒëa ƒë√≥. C√≥ m·ªôt s·ªë c√°ch ta c√≥ th·ªÉ l√†m ƒëi·ªÅu n√†y, nh∆∞ng ch√∫ng ta kh√¥ng mu·ªën c·∫Øt ng·∫Øn c√¢u h·ªèi, ch·ªâ c·∫Øt b·ªè ng·ªØ c·∫£nh. V√¨ ng·ªØ c·∫£nh l√† c√¢u th·ª© hai, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c c·∫Øt ng·∫Øn `"only_second"`. V·∫•n ƒë·ªÅ n·∫£y sinh sau ƒë√≥ l√† c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi c√≥ th·ªÉ kh√¥ng n·∫±m trong ng·ªØ c·∫£nh ƒë√£ b·ªã c·∫Øt ng·∫Øn. V√≠ d·ª•: ·ªü ƒë√¢y, ch√∫ng ta ƒë√£ ch·ªçn m·ªôt c√¢u h·ªèi trong ƒë√≥ c√¢u tr·∫£ l·ªùi n·∫±m ·ªü cu·ªëi ng·ªØ c·∫£nh v√† khi c·∫Øt ng·∫Øn c√¢u tr·∫£ l·ªùi ƒë√≥ th√¨ c√¢u tr·∫£ l·ªùi kh√¥ng c√≤n:

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""
```

ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† m√¥ h√¨nh s·∫Ω g·∫∑p kh√≥ khƒÉn trong vi·ªác ch·ªçn ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c. ƒê·ªÉ kh·∫Øc ph·ª•c ƒëi·ªÅu n√†y, pipeline h·ªèi ƒë√°p cho ph√©p ch√∫ng ta chia ng·ªØ c·∫£nh th√†nh c√°c ph·∫ßn nh·ªè h∆°n, ch·ªâ ƒë·ªãnh ƒë·ªô d√†i t·ªëi ƒëa. ƒê·ªÉ ƒë·∫£m b·∫£o r·∫±ng ch√∫ng ta kh√¥ng chia b·ªëi c·∫£nh ch√≠nh x√°c ·ªü v·ªã tr√≠ sai ƒë·ªÉ c√≥ th·ªÉ t√¨m ra c√¢u tr·∫£ l·ªùi, n√≥ c≈©ng bao g·ªìm m·ªôt s·ªë ph·∫ßn tr√πng l·∫∑p gi·ªØa c√°c ph·∫ßn.

Ch√∫ng ta c√≥ th·ªÉ y√™u c·∫ßu tokenizer (nhanh ho·∫∑c ch·∫≠m) th·ª±c hi·ªán vi·ªác n√†y b·∫±ng c√°ch th√™m `return_overflowing_tokens=True` v√† ta c√≥ th·ªÉ ch·ªâ ƒë·ªãnh s·ª± giao thoa m√† ta mu·ªën qua than s·ªë `stride`. ƒê√¢y l√† m·ªôt v√≠ d·ª•, s·ª≠ d·ª•ng m·ªôt c√¢u nh·ªè h∆°n:

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

C√≥ th·ªÉ th·∫•y, c√¢u ƒë√£ b·ªã chia th√†nh c√°c ƒëo·∫°n sao cho m·ªói ph·∫ßn trong `inputs["input_ids"]` c√≥ nhi·ªÅu nh·∫•t 6 token (ta s·∫Ω c·∫ßn th√™m ƒë·ªám ƒë·ªÉ ƒë·∫£m b·∫£o ch√∫ng c√≥ c√πng k√≠ch th∆∞·ªõc) v√† s·∫Ω c√≥ s·ª≠ giao thoa c·ªßa 2 token gi·ªØa c√°c ph·∫ßn.

H√£y c√πng nh√¨n kƒ© h∆°n v√†o k·∫øt qu·∫£ tokenize:

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

Nh∆∞ d·ª± ƒëo√°n, ta nh·∫≠n ƒë∆∞·ª£c ID ƒë·∫ßu v√†o v√† attention mask.·ªû ƒë√¢y, `overflow_to_sample_mapping` l√† m·ªôt ph√©p √°nh x·∫° cho ta bi·∫øt c√¢u n√†o trong k·∫øt qu·∫£ li√™n quan -- ta c√≥ 7 k·∫øt qu·∫£ d·ªÅu t·ª´ c√¢u m√† ta truy·ªÅn v√†o tokenizer:

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

ƒêi·ªÅu n√†y h·ªØu √≠ch h∆°n khi ta tokenize nhi·ªÅu c√¢u c√πng nhau, V√≠ d·ª•:

```py
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

tr·∫£ cho ta:

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

nghƒ©a l√† c√¢u ƒë·∫ßu ti√™n ƒë∆∞·ª£c chia th√†nh 7 ƒëo·∫°n nh∆∞ ph·∫ßn ph√≠a tr∆∞·ªõc, v√† 4 ƒëo·∫°n ti·∫øp theo ƒë·∫øn t·ª´ c√¢u th·ª© hai.

B√¢y gi·ªù ch√∫ng ta h√£y c√πng quay tr·ªü l·∫°i ng·ªØ c·∫£nh d√†i. Theo m·∫∑c ƒë·ªãnh, pipeline ``question-answering` s·ª≠ d·ª•ng ƒë·ªô d√†i t·ªëi ƒëa l√† 384, nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥ v√† kho·∫£ng c√°ch 128, t∆∞∆°ng ·ª©ng v·ªõi c√°ch m√¥ h√¨nh ƒë∆∞·ª£c tinh ch·ªânh (b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√°c tham s·ªë ƒë√≥ b·∫±ng c√°ch truy·ªÅn `max_seq_len` v√† `stride` khi g·ªçi pipeline). Do ƒë√≥, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng c√°c tham s·ªë ƒë√≥ khi tokenize. Ch√∫ng ta c≈©ng s·∫Ω th√™m ph·∫ßn ƒë·ªám (ƒë·ªÉ c√≥ c√°c m·∫´u c√≥ c√πng chi·ªÅu d√†i, v√¨ v·∫≠y ch√∫ng ta c√≥ th·ªÉ t·∫°o ra c√°c tensor) c≈©ng nh∆∞ y√™u c·∫ßu c√°c offset:

```py
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

C√°c `inputs` s·∫Ω ch·ª©a c√°c ID ƒë·∫ßu v√†o v√† c√°c attention mask m√† m√¥ h√¨nh k√¨ v·ªçng, c≈©ng nh∆∞ offset v√† `overflow_to_sample_mapping` ta v·ª´a trao ƒë·ªïi ·ªü tr√™n. V√¨ hai tham s·ªë ƒë√≥ kh√¥ng ph·∫£i l√† tham s·ªë ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi m√¥ h√¨nh, ch√∫ng ta s·∫Ω ƒë∆∞a ch√∫ng ra kh·ªèi `inputs` (v√† kh√¥ng l∆∞u tr·ªØ √°nh x·∫°, v√¨ n√≥ kh√¥ng h·ªØu √≠ch ·ªü ƒë√¢y) tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi n√≥ th√†nh tensor:

{#if fw === 'pt'}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

{:else}

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)
```

```python out
(2, 384)
```

{/if}

B·ªëi c·∫£nh d√†i c·ªßa ch√∫ng ta ƒë∆∞·ª£c chia l√†m hai, ƒë·ªìng nghƒ©a sau khi n√≥ ƒëi qua m√¥ h√¨nh, ch√∫ng ta s·∫Ω c√≥ hai b·ªô logit b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c:

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

{:else}

```python out
(2, 384) (2, 384)
```

{/if}

Gi·ªëng nh∆∞ tr∆∞·ªõc ƒë√¢y, ƒë·∫ßu ti√™n ch√∫ng ta che c√°c token kh√¥ng ph·∫£i l√† m·ªôt ph·∫ßn c·ªßa ng·ªØ c·∫£nh tr∆∞·ªõc khi s·ª≠ d·ª•ng softmax. Ch√∫ng ta c≈©ng che t·∫•t c·∫£ c√°c token ƒë·ªám (ƒë∆∞·ª£c g·∫Øn m√°c b·ªüi attention mask):

{#if fw === 'pt'}

```py
sequence_ids = inputs.sequence_ids()
# Che t·∫•t c·∫£ m·ªçi th·ª© tr·ª´ token c·ªßa ng·ªØ c·∫£nh
mask = [i != 1 for i in sequence_ids]
# Hi·ªÉn th·ªã token [CLS]
mask[0] = False
# Che t·∫•t c·∫£ token [PAD]
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

{:else}

```py
sequence_ids = inputs.sequence_ids()
# Che t·∫•t c·∫£ m·ªçi th·ª© tr·ª´ token c·ªßa ng·ªØ c·∫£nh
mask = [i != 1 for i in sequence_ids]
# Hi·ªÉn th·ªã token [CLS]
mask[0] = False
# Che t·∫•t c·∫£ token [PAD]
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)
```

{/if}

Sau ƒë√≥, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng softmax ƒë·ªÉ chuy·ªÉn ƒë·ªïi c√°c logit c·ªßa ch√∫ng ta th√†nh x√°c su·∫•t:

{#if fw === 'pt'}

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

{:else}

```py
start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()
```

{/if}

B∆∞·ªõc ti·∫øp theo t∆∞∆°ng t·ª± nh∆∞ nh·ªØng g√¨ ch√∫ng ta ƒë√£ l√†m cho b·ªëi c·∫£nh nh·ªè, nh∆∞ng ch√∫ng ta l·∫∑p l·∫°i n√≥ cho m·ªói ph·∫ßn trong hai ph·∫ßn c·ªßa m√¨nh. Ch√∫ng ta t√≠nh ƒëi·ªÉm cho t·∫•t c·∫£ c√°c kho·∫£ng c√¢u tr·∫£ l·ªùi c√≥ th·ªÉ c√≥, sau ƒë√≥ l·∫•y ph·∫ßn c√≥ ƒëi·ªÉm t·ªët nh·∫•t:

{#if fw === 'pt'}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{:else}

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

{/if}

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

Hai ·ª©ng c·ª≠ vi√™n ƒë√≥ t∆∞∆°ng ·ª©ng v·ªõi c√°c c√¢u tr·∫£ l·ªùi t·ªët nh·∫•t m√† m√¥ h√¨nh c√≥ th·ªÉ t√¨m th·∫•y trong m·ªói ƒëo·∫°n. M√¥ h√¨nh ch·∫Øc ch·∫Øn h∆°n r·∫±ng c√¢u tr·∫£ l·ªùi ƒë√∫ng n·∫±m ·ªü ph·∫ßn th·ª© hai (ƒë√≥ l√† m·ªôt d·∫•u hi·ªáu t·ªët!). B√¢y gi·ªù ch√∫ng ta ch·ªâ c·∫ßn √°nh x·∫° kho·∫£ng hai token ƒë√≥ v·ªõi kho·∫£ng c√°c k√Ω t·ª± trong ng·ªØ c·∫£nh (ch√∫ng ta ch·ªâ c·∫ßn l·∫≠p √°nh x·∫° c√°i th·ª© hai ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi, nh∆∞ng th·∫≠t th√∫ v·ªã khi xem m√¥ h√¨nh ƒë√£ ch·ªçn nh·ªØng g√¨ trong ƒëo·∫°n ƒë·∫ßu ti√™n).

> [!TIP]
> ‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** H√£y ƒëi·ªÅu ch·ªânh ƒëo·∫°n m√£ tr√™n ƒë·ªÉ tr·∫£ v·ªÅ ƒëi·ªÉm v√† kho·∫£ng cho nƒÉm c√¢u tr·∫£ l·ªùi c√≥ nhi·ªÅu kh·∫£ nƒÉng nh·∫•t (t·ªïng c·ªông, kh√¥ng ph·∫£i cho m·ªói ƒëo·∫°n).

`offsets` m√† ch√∫ng ta ƒë√£ n·∫Øm ƒë∆∞·ª£c tr∆∞·ªõc ƒë√≥ th·ª±c s·ª± l√† m·ªôt danh s√°ch c√°c offset, v·ªõi m·ªôt danh s√°ch tr√™n m·ªói ƒëo·∫°n vƒÉn b·∫£n:

```py
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

```python out
{'answer': '\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

N·∫øu ch√∫ng ta b·ªè qua k·∫øt qu·∫£ ƒë·∫ßu ti√™n, ch√∫ng ta s·∫Ω nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£ t∆∞∆°ng t·ª± nh∆∞ pipeline cho ng·ªØ c·∫£nh d√†i n√†y - yayy!

> [!TIP]
> ‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** S·ª≠ d·ª•ng ƒëi·ªÉm t·ªët nh·∫•t b·∫°n ƒë√£ t√≠nh to√°n tr∆∞·ªõc ƒë√≥ ƒë·ªÉ hi·ªÉn th·ªã nƒÉm c√¢u tr·∫£ l·ªùi c√≥ kh·∫£ nƒÉng x·∫£y ra nh·∫•t (cho to√†n b·ªô ng·ªØ c·∫£nh, kh√¥ng ph·∫£i t·ª´ng ƒëo·∫°n). ƒê·ªÉ ki·ªÉm tra k·∫øt qu·∫£ c·ªßa b·∫°n, h√£y quay l·∫°i pipeline ƒë·∫ßu ti√™n v√† truy·ªÅn v√†o `top_k=5` khi g·ªçi n√≥.

ƒêi·ªÅu n√†y k·∫øt th√∫c ph·∫ßn ƒëi s√¢u v√†o c√°c kh·∫£ nƒÉng c·ªßa tokenizer. Ch√∫ng ta s·∫Ω ƒë∆∞a t·∫•t c·∫£ nh·ªØng ƒëi·ªÅu n√†y v√†o th·ª±c t·∫ø m·ªôt l·∫ßn n·ªØa trong ch∆∞∆°ng ti·∫øp theo, khi ch√∫ng t√¥i h∆∞·ªõng d·∫´n b·∫°n c√°ch tinh ch·ªânh m·ªôt m√¥ h√¨nh v·ªÅ m·ªôt lo·∫°t c√°c t√°c v·ª• NLP ph·ªï bi·∫øn.
