# Giới thiệu

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

Trong [Chương 3](/course/chapter3), chúng ta đã xem xét cách tinh chỉnh một mô hình trong một tác vụ nhất định. Khi làm điều đó, chúng ta sử dụng cùng một trình tokenizer mà mô hình đã được huấn luyện trước - nhưng chúng ra phải làm gì khi muốn huấn luyện một mô hình từ đầu? Trong những trường hợp này, việc sử dụng trình tokenizer đã được huấn luyện trước trên một kho ngữ liệu từ một lĩnh vực hoặc ngôn ngữ khác thường là không tối ưu. Ví dụ: một tokenizer được huấn luyện trên ngữ liệu tiếng Anh sẽ hoạt động kém trên ngữ liệu văn bản tiếng Nhật vì việc sử dụng dấu cách và dấu câu trong hai ngôn ngữ rất khác nhau.

Trong chương này, bạn sẽ học cách huấn luyện một trình tokenize hoàn toàn mới trên kho ngữ liệu văn bản, do đó, nó có thể được sử dụng để huấn luyện trước một mô hình ngôn ngữ. Tất cả điều này sẽ được thực hiện với sự trợ giúp của thư viện [🤗 Tokenizers](https://github.com/huggingface/tokenizers), nơi cung cấp các tokenizer "nhanh" trong thư viện [🤗 Transformers](https://github.com/huggingface/transformers). Chúng ta sẽ xem xét kỹ các tính năng mà thư viện này cung cấp và khám phá cách các bản tokenizer nhanh khác so với các phiên bản "chậm".

Các chủ đề chúng ta sẽ đề cập bao gồm:

- Cách huấn luyện một trình tokenize mới tương tự như một trình được sử dụng bởi một checkpoint nhất định trên một kho văn bản mới
- Các tính năng đặc biệt của tokenizer nhanh
- Sự khác biệt giữa ba thuật toán tokenize từ phụ được sử dụng trong NLP ngày nay
- Cách xây dựng một tokenizer từ đầu với thư viện 🤗 Tokenizer và huấn luyện nó trên một số dữ liệu

Các kỹ thuật được giới thiệu trong chương này sẽ giúp bạn chuẩn bị cho phần trong [Chương 7](/course/chapter7/6), nơi chúng ta xem xét việc tạo mô hình ngôn ngữ cho mã nguồn Python. Hãy bắt đầu bằng cách xem xét ý nghĩa của việc "huấn luyện" một tokenizer ngay từ đầu.
