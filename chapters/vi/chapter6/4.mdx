# Chuáº©n hoÃ¡ vÃ  tiá»n tokenize

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section4.ipynb"},
]} />

TrÆ°á»›c khi Ä‘i sÃ¢u hÆ¡n vÃ o ba thuáº­t toÃ¡n tokenize tá»« phá»¥ phá»• biáº¿n nháº¥t Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c mÃ´ hÃ¬nh Transformer (MÃ£ hÃ³a theo cáº·p [BPE], WordPiece vÃ  Unigram), trÆ°á»›c tiÃªn chÃºng ta sáº½ xem xÃ©t tiá»n xá»­ lÃ½ mÃ  má»—i trÃ¬nh tokenize Ã¡p dá»¥ng cho vÄƒn báº£n. DÆ°á»›i Ä‘Ã¢y lÃ  tá»•ng quan cáº¥p cao vá» cÃ¡c bÆ°á»›c trong pipeline tokenize:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

TrÆ°á»›c khi tÃ¡ch má»™t Ä‘oáº¡n vÄƒn báº£n thÃ nh cÃ¡c token phá»¥ (dá»±a theo mÃ´ hÃ¬nh)tokenizer sáº½ thá»±c hiá»‡n 2 bÆ°á»›c: _normalization_ (chuáº©n hoÃ¡) vÃ  _pre-tokenization_ (tiá»n tokenize).

## Chuáº©n hoÃ¡

<Youtube id="4IIC2jI9CaU"/>

BÆ°á»›c chuáº©n hÃ³a bao gá»“m má»™t sá»‘ thao tÃ¡c dá»n dáº¹p, cháº³ng háº¡n nhÆ° loáº¡i bá» khoáº£ng tráº¯ng khÃ´ng cáº§n thiáº¿t, viáº¿t thÆ°á»ng táº¥t cáº£ cÃ¡c chá»¯, vÃ /hoáº·c xÃ³a dáº¥u. Náº¿u báº¡n Ä‘Ã£ quen vá»›i [chuáº©n hÃ³a Unicode](http://www.unicode.org/reports/tr15/) (cháº³ng háº¡n nhÆ° NFC hoáº·c NFKC), thÃ¬ Ä‘Ã¢y cÅ©ng lÃ  Ä‘iá»u mÃ  tokenizer cÃ³ thá»ƒ Ã¡p dá»¥ng.

`tokenizer` cá»§a ğŸ¤— Transformers cÃ³ má»™t thuá»™c tÃ­nh gá»i lÃ  `backend_tokenizer` cung cáº¥p quyá»n truy cáº­p vÃ o tokenizer bÃªn dÆ°á»›i tá»« thÆ° viá»‡n ğŸ¤— Tokenizers:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

Thuá»™c tÃ­nh `normalizer` cá»§a Ä‘á»‘i tÆ°á»£ng `tokenizer` cÃ³ phÆ°Æ¡ng thá»©c `normalize_str()` mÃ  ta cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ tháº¥y cÃ¡ch bÆ°á»›c chuáº©n hoÃ¡ Ä‘Æ°á»£c thá»±c hiá»‡n:

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
'hello how are u?'
```

Trong vÃ­ dá»¥ nÃ y, vÃ¬ chÃºng ta chá»n checkpoint `bert-base-uncased`, bÆ°á»›c chuáº©n hoÃ¡ sáº½ thá»±c hiá»‡n viáº¿t thÆ°á»ng vÃ  loáº¡i bá» cÃ¡c dáº¥u.

<Tip>

âœï¸ **Try it out!** Táº£i tokenizer tá»« checkpoint `bert-base-cased` vÃ  truyá»n vÃ o cÃ¹ng má»™t vÃ­ dá»¥ vÃ o.Sá»± khÃ¡c biá»‡t chÃ­nh mÃ  báº¡n cÃ³ thá»ƒ tháº¥y giá»¯a cÃ¡c phiÃªn báº£n cÃ³ dáº¥u vÃ  khÃ´ng dáº¥u cá»§a tokenizer lÃ  gÃ¬?

</Tip>

## Pre-tokenization

<Youtube id="grlLV8AIXug"/>

NhÆ° chÃºng ta sáº½ tháº¥y trong cÃ¡c pháº§n tiáº¿p theo, má»™t tokenizer khÃ´ng thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn vÄƒn báº£n thÃ´. Thay vÃ o Ä‘Ã³, trÆ°á»›c tiÃªn chÃºng ta cáº§n chia cÃ¡c vÄƒn báº£n thÃ nh cÃ¡c thá»±c thá»ƒ nhá», nhÆ° cÃ¡c tá»«. ÄÃ³ lÃ  khi bÆ°á»›c pre-tokenization báº¯t Ä‘áº§u. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 2](/course/chapter2), trÃ¬nh tokenize dá»±a trÃªn tá»« cÃ³ thá»ƒ chá»‰ cáº§n tÃ¡ch má»™t vÄƒn báº£n thÃ´ thÃ nh cÃ¡c tá»« dá»±a trÃªn khoáº£ng tráº¯ng vÃ  dáº¥u cÃ¢u. Nhá»¯ng tá»« Ä‘Ã³ sáº½ lÃ  ranh giá»›i cá»§a cÃ¡c token con mÃ  tokenizer cÃ³ thá»ƒ há»c Ä‘Æ°á»£c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a nÃ³.

Äá»ƒ xem cÃ¡ch má»™t tokenizer nhanh thá»±c hiá»‡n pre-tokenization, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `pre_tokenize_str()` cá»§a thuá»™c tÃ­nh `pre_tokenizer` cá»§a Ä‘á»‘i tÆ°á»£ng `tokenizer`:

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

LÆ°u Ã½ cÃ¡ch tokenizer Ä‘Ã£ theo dÃµi cÃ¡c offset, Ä‘Ã³ lÃ  cÃ¡ch nÃ³ cÃ³ thá»ƒ cung cáº¥p cho chÃºng ta Ã¡nh xáº¡ offset mÃ  ta Ä‘Ã£ sá»­ dá»¥ng trong pháº§n trÆ°á»›c. á» Ä‘Ã¢y tokenizer bá» qua hai khoáº£ng tráº¯ng vÃ  thay tháº¿ chÃºng báº±ng chá»‰ má»™t, nhÆ°ng cÃ¡c offset xen giá»¯a `are` vÃ  `you` Ä‘á»ƒ giáº£i thÃ­ch Ä‘iá»u Ä‘Ã³.

VÃ¬ chÃºng ta Ä‘ang sá»­ dá»¥ng BERT tokenizer, pre-tokenization liÃªn quan Ä‘áº¿n viá»‡c phÃ¢n tÃ¡ch dá»±a trÃªn khoáº£ng tráº¯ng vÃ  dáº¥u cháº¥m cÃ¢u. CÃ¡c tokenizer khÃ¡c cÃ³ thá»ƒ cÃ³ cÃ¡c quy táº¯c khÃ¡c nhau cho bÆ°á»›c nÃ y. VÃ­ dá»¥: náº¿u sá»­ dá»¥ng GPT-2 tokenizer:

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

nÃ³ sáº½ tÃ¡ch dá»±a trÃªn dáº¥u cÃ¡ch vÃ  dáº¥u cÃ¢u, nhÆ°ng sáº½ giá»¯a dáº¥u cÃ¡ch vÃ  thay tháº¿ chÃºng bá»Ÿi kÃ­ hiá»‡u `Ä `, cho phÃ©p nÃ³ khÃ´i phá»¥c khÃ´ng gian ban Ä‘áº§u náº¿u chÃºng tÃ´i giáº£i mÃ£ cÃ¡c token:

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('Ä how', (6, 10)), ('Ä are', (10, 14)), ('Ä ', (14, 15)), ('Ä you', (15, 19)),
 ('?', (19, 20))]
```

Cáº§n lÆ°u Ã½ thÃªm ráº±ng khÃ´ng nhÆ° BERT tokenizer, tokenizer nÃ y bá» qua dáº¥u cÃ¡ch kÃ©p.

á» vÃ­ dá»¥ cuá»‘i, hÃ£y cÃ¹ng xem T5 tokenizer dá»±a trÃªn thuáº­t toÃ¡n SentencePiece:

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('â–Hello,', (0, 6)), ('â–how', (7, 10)), ('â–are', (11, 14)), ('â–you?', (16, 20))]
```

Giá»‘ng nhÆ° GPT-2 tokenizer, phÆ°Æ¡ng phÃ¡p nÃ y giá»¯ cÃ¡c dáº¥u cÃ¡ch vÃ  thay tháº¿ chÃºng bá»Ÿi má»™t tÃ­ tá»± Ä‘áº·c biá»‡t (`_`), nhÆ°ng T5 tokenizer chá»‰ tÃ¡ch dá»±a theo dáº¥u cÃ¡ch, khÃ´ng dá»±a theo dáº¥u cÃ¢u. Má»™t lÆ°u Ã½ ná»¯a Ä‘Ã³ lÃ  nÃ³ cÅ©ng máº·c Ä‘á»‹nh thÃªm dáº¥u cÃ¡ch á»Ÿ phÃ­a Ä‘áº§u cÃ¢u (trÆ°á»›c `Hello`) vÃ  bá» qua nhá»¯ng dáº¥u cÃ¡ch káº¹p á»Ÿ giá»¯a `are` vÃ  `you`.

BÃ¢y giá» chÃºng ta Ä‘Ã£ biáº¿t má»™t chÃºt vá» cÃ¡ch má»™t sá»‘ loáº¡i tokenizers khÃ¡c nhau Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n, chÃºng ta cÃ³ thá»ƒ báº¯t Ä‘áº§u tá»± khÃ¡m phÃ¡ cÃ¡c thuáº­t toÃ¡n cÆ¡ báº£n. ChÃºng ta sáº½ báº¯t Ä‘áº§u báº±ng má»™t cÃ¡i nhÃ¬n nhanh vá» SentencePiece Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i; sau Ä‘Ã³, trong ba pháº§n tiáº¿p theo, chÃºng ta sáº½ xem xÃ©t cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a ba thuáº­t toÃ¡n chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ£ hÃ³a tá»« phá»¥.

## SentencePiece

[SentencePiece](https://github.com/google/sentencepiece) lÃ  má»™t thuáº­t toÃ¡n tokenize Ä‘á»ƒ tiá»n xá»­ lÃ½ vÄƒn báº£n mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng vá»›i báº¥t ká»³ mÃ´ hÃ¬nh nÃ o chÃºng ta sáº½ tháº¥y trong ba pháº§n tiáº¿p theo. NÃ³ coi vÄƒn báº£n lÃ  má»™t chuá»—i cÃ¡c kÃ½ tá»± Unicode vÃ  thay tháº¿ dáº¥u cÃ¡ch báº±ng má»™t kÃ½ tá»± Ä‘áº·c biá»‡t, `â–`. ÄÆ°á»£c sá»­ dá»¥ng cÃ¹ng vá»›i thuáº­t toÃ¡n Unigram (xem [pháº§n 7](/course/chapter7/7)), nÃ³ tháº­m chÃ­ khÃ´ng yÃªu cáº§u bÆ°á»›c pre-tokenization, ráº¥t há»¯u Ã­ch cho cÃ¡c ngÃ´n ngá»¯ khÃ´ng sá»­ dá»¥ng dáº¥u cÃ¡ch (nhÆ° Trung Quá»‘c hoáº·c Nháº­t Báº£n).

TÃ­nh nÄƒng chÃ­nh khÃ¡c cá»§a SentencePiece lÃ  *reversible tokenization* hay *tokenize cÃ³ thá»ƒ Ä‘áº£o ngÆ°á»£c*: vÃ¬ khÃ´ng cÃ³ cÃ¡ch xá»­ lÃ½ Ä‘áº·c biá»‡t nÃ o cho dáº¥u cÃ¡ch, nÃªn viá»‡c giáº£i mÃ£ cÃ¡c token Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘Æ¡n giáº£n báº±ng cÃ¡ch ná»‘i chÃºng vÃ  thay tháº¿ cÃ¡c dáº¥u `_` báº±ng dáº¥u cÃ¡ch - Ä‘iá»u nÃ y giÃºp vÄƒn báº£n Ä‘Æ°á»£c chuáº©n hÃ³a. NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, BERT tokenizer loáº¡i bá» cÃ¡c dáº¥u cÃ¡ch láº·p láº¡i, vÃ¬ váº­y token cá»§a nÃ³ khÃ´ng thá»ƒ Ä‘áº£o ngÆ°á»£c.

## Tá»•ng quan thuáº­t toÃ¡n

Trong cÃ¡c pháº§n tiáº¿p theo, chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o ba thuáº­t toÃ¡n tokenize tá»« phá»¥ tiÃªu biá»ƒu: BPE (Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi GPT-2 vÃ  cÃ¡c thuáº­t toÃ¡n khÃ¡c), WordPiece (Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi BERT) vÃ  Unigram (Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi T5 vÃ  cÃ¡c thuáº­t toÃ¡n khÃ¡c). TrÆ°á»›c khi chÃºng ta báº¯t Ä‘áº§u, Ä‘Ã¢y lÃ  tá»•ng quan nhanh vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a tá»«ng loáº¡i. Äá»«ng ngáº§n ngáº¡i quay láº¡i báº£ng nÃ y sau khi Ä‘á»c tá»«ng pháº§n tiáº¿p theo náº¿u báº¡n chÆ°a hiá»ƒu háº¿t.

MÃ´ hÃ¬nh | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Huáº¥n luyá»‡n | Báº¯t Ä‘áº§u vá»›i má»™t bá»™ tá»« vá»±ng nhá» vÃ  há»c bá»™ quy táº¯c há»£p nháº¥t token |  Báº¯t Ä‘áº§u vá»›i má»™t bá»™ tá»« vá»±ng nhá» vÃ  há»c bá»™ quy táº¯c há»£p nháº¥t token | Báº¯t Ä‘áº§u vá»›i má»™t bá»™ tá»« vá»±ng lá»›n vÃ  há»c bá»™ quy táº¯c Ä‘á»ƒ loáº¡i bá» token
BÆ°á»›c huáº¥n luyá»‡n | Gá»™p cÃ¡c token liÃªn quan Ä‘áº¿n cáº·p phá»• biáº¿n nháº¥t | Gá»™p cÃ¡c token liÃªn quan Ä‘áº¿n cáº·p cÃ³ Ä‘iá»ƒm cao nháº¥t dá»±a trÃªn táº§n suáº¥t cá»§a cáº·p, with the best score based on the frequency of the pair,  Æ°u tiÃªn cÃ¡c cáº·p mÃ  má»—i token cÃ¡ nhÃ¢n táº§n suáº¥t tháº¥p hÆ¡n| Loáº¡i bá» táº¥t cáº£ cÃ¡c token trong bá»™ tá»« Ä‘iá»ƒn giáº£m thiá»ƒu tá»‘i Ä‘a Ä‘á»™ máº¥t mÃ¡t Ä‘Æ°á»£c tÃ­nh trÃªn toÃ n bá»™ kho ngá»¯ liá»‡u
Há»c | Gá»™p bá»™ quy táº¯c vÃ  bá»™ tá»« vá»±ng | Chá»‰ bá»™ tá»« vá»±ng  | Má»™t bá»™ tá»± vá»±ng vá»›i Ä‘iá»ƒm cho má»—i token
MÃ£ hoÃ¡ | Chia tá»« thÃ nh cÃ¡c kÃ­ tá»± vÃ  Ã¡p dá»¥ng bÆ°á»›c gá»™p tá»« quÃ¡ trÃ¬nh huáº¥n luyá»‡n | TÃ¬m ra chuá»—i tá»« phá»¥ dÃ i nháº¥t báº¯t Ä‘áº§u tá»« pháº§n báº¯t Ä‘áº§u cÃ³ trong bá»™ tá»« vá»±ng, sau Ä‘Ã³ lÃ m tÆ°Æ¡ng tá»± vá»›i cÃ¡c pháº§n cÃ²n láº¡i cá»§a tá»« | TÃ¬m tá»« cÃ³ kháº£ nÄƒng chia thÃ nh token cao nháº¥t sá»­ dá»¥ng Ä‘iá»ƒm cÃ³ Ä‘Æ°á»£c tá»« quÃ¡ trÃ¬nh huáº¥n luyá»‡n

Giá» chÃºng ta hÃ£y Ä‘i sÃ¢u vÃ o BPE thÃ´i!
