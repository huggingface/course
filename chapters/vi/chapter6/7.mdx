# Unigram tokenization

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section7.ipynb"},
]} />

Thuáº­t toÃ¡n Unigram thÆ°á»ng Ä‘Æ°á»£c sá»­ dung trong SentencePiece,  Ä‘Ã¢y lÃ  má»™t thuáº­t toÃ¡n tokenize cho cÃ¡c mÃ´ hÃ¬nh nhÆ° AlBERT, T5, mBART, Big Bird, vÃ  XLNet.

<Youtube id="TGZfZVuF9Yc"/>

<Tip>

ğŸ’¡ Pháº§n nÃ y sáº½ Ä‘i sÃ¢u vÃ o Unigram cÅ©ng nhÆ° toÃ n bá»™ cÃ¡ch triá»ƒn khai. Báº¡n cÃ³ thá»ƒ bá» qua pháº§n cuá»‘i náº¿u báº¡n chá»‰ quan tÃ¢m tá»•ng quan thuáº­t toÃ¡n tokenize.

</Tip>

## Thuáº­t toÃ¡n huáº¥n luyá»‡n

So vá»›i BPE vÃ  WordPiece, Unigram hoáº¡t Ä‘á»™ng theo hÆ°á»›ng khÃ¡c: nÃ³ báº¯t Ä‘áº§u tá»« má»™t tá»« vá»±ng lá»›n vÃ  loáº¡i bá» cÃ¡c token cho Ä‘áº¿n khi nÃ³ Ä‘áº¡t Ä‘áº¿n kÃ­ch thÆ°á»›c tá»« vá»±ng mong muá»‘n. CÃ³ má»™t sá»‘ tÃ¹y chá»n Ä‘á»ƒ sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng vá»‘n tá»« vá»±ng cÆ¡ báº£n: vÃ­ dá»¥: chÃºng ta cÃ³ thá»ƒ láº¥y cÃ¡c chuá»—i con phá»• biáº¿n nháº¥t trong cÃ¡c tá»« Ä‘Æ°á»£c tiá»n tokenize hoáº·c Ã¡p dá»¥ng BPE trÃªn kho ngá»¯ liá»‡u ban Ä‘áº§u cÃ³ kÃ­ch thÆ°á»›c tá»« vá»±ng lá»›n.

Táº¡i má»—i bÆ°á»›c cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n, thuáº­t toÃ¡n Unigram tÃ­nh toÃ¡n sá»± máº¥t mÃ¡t trÃªn kho ngá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p tá»« vá»±ng hiá»‡n táº¡i. Sau Ä‘Ã³, Ä‘á»‘i vá»›i má»—i kÃ½ hiá»‡u trong tá»« vá»±ng, thuáº­t toÃ¡n sáº½ tÃ­nh toÃ¡n má»©c Ä‘á»™ tá»•n tháº¥t tá»•ng thá»ƒ sáº½ tÄƒng lÃªn bao nhiÃªu náº¿u kÃ½ hiá»‡u bá»‹ xÃ³a vÃ  tÃ¬m kiáº¿m cÃ¡c kÃ½ hiá»‡u lÃ m tÄƒng nÃ³ Ã­t nháº¥t. Nhá»¯ng biá»ƒu tÆ°á»£ng Ä‘Ã³ cÃ³ áº£nh hÆ°á»Ÿng tháº¥p hÆ¡n Ä‘áº¿n sá»± máº¥t mÃ¡t tá»•ng thá»ƒ Ä‘á»‘i vá»›i kho dá»¯ liá»‡u, vÃ¬ váº­y theo má»™t nghÄ©a nÃ o Ä‘Ã³, chÃºng "Ã­t cáº§n thiáº¿t hÆ¡n" vÃ  lÃ  nhá»¯ng á»©ng cá»­ viÃªn tá»‘t nháº¥t Ä‘á»ƒ loáº¡i bá».

ÄÃ¢y lÃ  má»™t hoáº¡t Ä‘á»™ng ráº¥t tá»‘n kÃ©m, vÃ¬ váº­y chÃºng tÃ´i khÃ´ng chá»‰ loáº¡i bá» má»™t biá»ƒu tÆ°á»£ng liÃªn quan Ä‘áº¿n má»©c tÄƒng tá»•n tháº¥t tháº¥p nháº¥t, mÃ  \\(p\\) (\\(p\\) lÃ  má»™t siÃªu tham sá»‘ báº¡n cÃ³ thá»ƒ kiá»ƒm soÃ¡t, thÆ°á»ng lÃ  10 hoáº·c 20) pháº§n trÄƒm cÃ¡c kÃ½ hiá»‡u liÃªn quan Ä‘áº¿n má»©c tÄƒng tá»•n tháº¥t tháº¥p nháº¥t. QuÃ¡ trÃ¬nh nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c láº·p láº¡i cho Ä‘áº¿n khi tá»« vá»±ng Ä‘áº¡t Ä‘Æ°á»£c kÃ­ch thÆ°á»›c mong muá»‘n.

LÆ°u Ã½ ráº±ng chÃºng ta khÃ´ng bao giá» xÃ³a cÃ¡c kÃ½ tá»± cÆ¡ sá»Ÿ, Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng báº¥t ká»³ tá»« nÃ o cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c tokenize.

BÃ¢y giá», Ä‘iá»u nÃ y váº«n cÃ²n hÆ¡i mÆ¡ há»“: pháº§n chÃ­nh cá»§a thuáº­t toÃ¡n lÃ  tÃ­nh toÃ¡n sá»± máº¥t mÃ¡t trong kho ngá»¯ liá»‡u vÃ  xem nÃ³ thay Ä‘á»•i nhÆ° tháº¿ nÃ o khi chÃºng tÃ´i xÃ³a má»™t sá»‘ token khá»i tá»« vá»±ng, nhÆ°ng chÃºng ta chÆ°a giáº£i thÃ­ch cÃ¡ch thá»±c hiá»‡n Ä‘iá»u nÃ y. BÆ°á»›c nÃ y dá»±a trÃªn thuáº­t toÃ¡n tokenize cá»§a mÃ´ hÃ¬nh Unigram, vÃ¬ váº­y chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o pháº§n tiáº¿p theo.

ChÃºng ta sáº½ tÃ¡i sá»­ dá»¥ng kho ngá»¯ liá»‡u tá»« cÃ¡c vÃ­ dá»¥ trÆ°á»›c:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

and for this example, we will take all strict substrings for the initial vocabulary :

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Thuáº­t toÃ¡n tokenize

MÃ´ hÃ¬nh Unigram lÃ  má»™t loáº¡i mÃ´ hÃ¬nh ngÃ´n ngá»¯ coi má»—i token lÃ  Ä‘á»™c láº­p vá»›i cÃ¡c token trÆ°á»›c nÃ³. ÄÃ³ lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ¡n giáº£n nháº¥t, theo nghÄ©a xÃ¡c suáº¥t cá»§a token X trong bá»‘i cáº£nh trÆ°á»›c Ä‘Ã³ chá»‰ lÃ  xÃ¡c suáº¥t cá»§a token X. VÃ¬ váº­y, náº¿u chÃºng ta sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ Unigram Ä‘á»ƒ táº¡o vÄƒn báº£n, chÃºng ta sáº½ luÃ´n dá»± Ä‘oÃ¡n token phá»• biáº¿n nháº¥t.

XÃ¡c suáº¥t cá»§a má»™t token nháº¥t Ä‘á»‹nh lÃ  táº§n suáº¥t cá»§a nÃ³ (sá»‘ láº§n chÃºng ta tÃ¬m tháº¥y nÃ³) trong kho tÃ i liá»‡u gá»‘c, chia cho tá»•ng táº¥t cáº£ cÃ¡c táº§n sá»‘ cá»§a táº¥t cáº£ cÃ¡c token trong tá»« vá»±ng (Ä‘á»ƒ Ä‘áº£m báº£o xÃ¡c suáº¥t tá»•ng báº±ng 1). VÃ­ dá»¥: `"ug"` cÃ³ trong `"hug"`, `"pug"` vÃ  `"hugs"`, vÃ¬ váº­y nÃ³ cÃ³ táº§n suáº¥t lÃ  20 trong kho ngá»¯ liá»‡u cá»§a chÃºng tÃ´i.

DÆ°á»›i Ä‘Ã¢y lÃ  táº§n suáº¥t cá»§a táº¥t cáº£ cÃ¡c tá»« phá»¥ cÃ³ thá»ƒ cÃ³ trong tá»« vá»±ng:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

Váº­y nÃªn, tá»•ng cá»§a táº¥t cáº£ cÃ¡c táº§n suáº¥t lÃ  210, vÃ  xÃ¡c suáº¥t cá»§a tá»« phá»¥ `"ug"` lÃ  20/210.

<Tip>

âœï¸ **Giá» Ä‘áº¿n lÆ°á»£t báº¡n!** Viáº¿t Ä‘oáº¡n mÃ£ Ä‘á»ƒ tÃ­nh táº§n suáº¥t trÃªn vÃ  kiá»ƒm tra láº¡i káº¿t quáº£ hiá»ƒn thá»‹ cÅ©ng nhÆ° tá»•ng Ä‘Ã£ Ä‘Ãºng chÆ°a.

</Tip>

Giá», Ä‘á»ƒ tokenize má»™t tá»« cho trÆ°á»›c, chÃºng ta sáº½ nhÃ¬n vÃ o táº¥t cáº£ cÃ¡c pháº§n Ä‘oáº¡n thÃ nh token vÃ  tÃ­nh xÃ¡c suáº¥t cá»§a tá»«ng cÃ¡i theo mÃ´ hÃ¬nh Unigram. VÃ¬ táº¥t cáº£ token Ä‘Æ°á»£c cho lÃ  Ä‘á»™c láº­p, xÃ¡c suáº¥t nÃ y chá»‰ lÃ  tÃ­ch cá»§a xÃ¡c suáº¥t má»—i token. VÃ­ dá»¥, `["p", "u", "g"]` cá»§a `"pug"` cÃ³ xÃ¡c suáº¥t:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

TÆ°Æ¡ng tá»±, `["pu", "g"]` cÃ³ xÃ¡c suáº¥t:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

NÃ³i chung, viá»‡c tokenize cÃ³ Ã­t token nháº¥t cÃ³ thá»ƒ sáº½ cÃ³ xÃ¡c suáº¥t cao nháº¥t (vÃ¬ phÃ©p chia cho 210 láº·p láº¡i cho má»—i token), tÆ°Æ¡ng á»©ng vá»›i nhá»¯ng gÃ¬ chÃºng ta muá»‘n trá»±c quan: chia má»™t tá»« thÃ nh sá»‘ lÆ°á»£ng token nháº¥t cÃ³ thá»ƒ.

Tokenize cá»§a má»™t tá»« vá»›i mÃ´ hÃ¬nh Unigram sau Ä‘Ã³ lÃ  token cÃ³ xÃ¡c suáº¥t cao nháº¥t. Trong vÃ­ dá»¥ vá» `"pug"`, Ä‘Ã¢y lÃ  cÃ¡c xÃ¡c suáº¥t mÃ  chÃºng ta sáº½ nháº­n Ä‘Æ°á»£c cho má»—i phÃ¢n Ä‘oáº¡n cÃ³ thá»ƒ cÃ³:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

VÃ¬ váº­y, `"pug"` sáº½ Ä‘Æ°á»£c tokenize lÃ  `["p", "ug"]`  hoáº·c `["pu", "g"]`, tÃ¹y thuá»™c vÃ o phÃ¢n Ä‘oáº¡n nÃ o trong sá»‘ Ä‘Ã³ Ä‘Æ°á»£c gáº·p Ä‘áº§u tiÃªn (lÆ°u Ã½ ráº±ng trong má»™t phÃ¢n Ä‘oáº¡n lá»›n hÆ¡n ngá»¯ liá»‡u, nhá»¯ng trÆ°á»ng há»£p bÃ¬nh Ä‘áº³ng nhÆ° tháº¿ nÃ y sáº½ ráº¥t hiáº¿m).

Trong trÆ°á»ng há»£p nÃ y, tháº­t dá»… dÃ ng Ä‘á»ƒ tÃ¬m táº¥t cáº£ cÃ¡c phÃ¢n Ä‘oáº¡n cÃ³ thá»ƒ cÃ³ vÃ  tÃ­nh toÃ¡n xÃ¡c suáº¥t cá»§a chÃºng, nhÆ°ng nÃ³i chung sáº½ khÃ³ hÆ¡n má»™t chÃºt. CÃ³ má»™t thuáº­t toÃ¡n cá»• Ä‘iá»ƒn Ä‘Æ°á»£c sá»­ dá»¥ng cho viá»‡c nÃ y, Ä‘Æ°á»£c gá»i lÃ  thuáº­t toÃ¡n *Viterbi*. Vá» cÆ¡ báº£n, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t biá»ƒu Ä‘á»“ Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c phÃ¢n Ä‘oáº¡n cÃ³ thá»ƒ cÃ³ cá»§a má»™t tá»« nháº¥t Ä‘á»‹nh báº±ng cÃ¡ch nÃ³i ráº±ng cÃ³ má»™t nhÃ¡nh tá»« kÃ½ tá»± _a_ Ä‘áº¿n kÃ½ tá»± _b_ náº¿u tá»« phá»¥ tá»« _a_ Ä‘áº¿n _b_ náº±m trong tá»« vá»±ng vÃ  quy cho nhÃ¡nh Ä‘Ã³ xÃ¡c suáº¥t cá»§a tá»« phá»¥ .

Äá»ƒ tÃ¬m Ä‘Æ°á»ng dáº«n trong biá»ƒu Ä‘á»“ Ä‘Ã³ sáº½ cÃ³ Ä‘iá»ƒm tá»‘t nháº¥t, thuáº­t toÃ¡n Viterbi xÃ¡c Ä‘á»‹nh, Ä‘á»‘i vá»›i má»—i vá»‹ trÃ­ trong tá»«, phÃ¢n Ä‘oáº¡n cÃ³ Ä‘iá»ƒm tá»‘t nháº¥t káº¿t thÃºc táº¡i vá»‹ trÃ­ Ä‘Ã³. VÃ¬ chÃºng ta Ä‘i tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i, Ä‘iá»ƒm tá»‘t nháº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y báº±ng cÃ¡ch láº·p qua táº¥t cáº£ cÃ¡c tá»« phá»¥ káº¿t thÃºc á»Ÿ vá»‹ trÃ­ hiá»‡n táº¡i vÃ  sau Ä‘Ã³ sá»­ dá»¥ng Ä‘iá»ƒm token tá»‘t nháº¥t tá»« â€‹â€‹vá»‹ trÃ­ mÃ  tá»« phá»¥ nÃ y báº¯t Ä‘áº§u. Sau Ä‘Ã³, chÃºng ta chá»‰ cáº§n bá» qua con Ä‘Æ°á»ng Ä‘Ã£ thá»±c hiá»‡n Ä‘á»ƒ Ä‘áº¿n cuá»‘i.

HÃ£y xem má»™t vÃ­ dá»¥ sá»­ dá»¥ng tá»« vá»±ng cá»§a chÃºng ta vÃ  tá»« `"unhug"`. Äá»‘i vá»›i má»—i vá»‹ trÃ­, cÃ¡c tá»« phá»¥ cÃ³ Ä‘iá»ƒm sá»‘ tá»‘t nháº¥t káº¿t thÃºc á»Ÿ Ä‘Ã³ nhÆ° sau:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

Váº­y `"unhug"` cÃ³ thá»ƒ tokenize thÃ nh `["un", "hug"]`.

<Tip>

âœï¸ **Giá» Ä‘áº¿n lÆ°á»£t báº¡n!** XÃ¡c Ä‘á»‹nh token cá»§a tá»« `"huggun"`, vÃ  Ä‘iá»ƒm cáº£u chÃºng.

</Tip>

## Quay láº¡i huáº¥n luyá»‡n

BÃ¢y giá» chÃºng ta Ä‘Ã£ tháº¥y cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a tokenize, chÃºng ta cÃ³ thá»ƒ tÃ¬m hiá»ƒu sÃ¢u hÆ¡n má»™t chÃºt vá» sá»± máº¥t mÃ¡t Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. á» báº¥t ká»³ giai Ä‘oáº¡n nháº¥t Ä‘á»‹nh nÃ o, sá»± máº¥t mÃ¡t nÃ y Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng cÃ¡ch tokenize má»i tá»« trong kho ngá»¯ liá»‡u, sá»­ dá»¥ng tá»« vá»±ng hiá»‡n táº¡i vÃ  mÃ´ hÃ¬nh Unigram Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi táº§n sá»‘ cá»§a má»—i token trong kho ngá»¯ liá»‡u (nhÆ° Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã¢y).

Má»—i tá»« trong kho ngá»¯ liá»‡u Ä‘á»u cÃ³ má»™t Ä‘iá»ƒm vÃ  sá»± máº¥t mÃ¡t lÃ  kháº£ nÄƒng bá»‹ Ã¢m cá»§a nhá»¯ng Ä‘iá»ƒm sá»‘ Ä‘Ã³ - nghÄ©a lÃ  tá»•ng cho táº¥t cáº£ cÃ¡c tá»« trong kho ngá»¯ liá»‡u cá»§a táº¥t cáº£ cÃ¡c `-log(P(word))`.

CÃ¹ng xem vÃ­ dá»¥ cá»§a chÃºng ta vá»›i kho ngá»¯ liá»‡u dÆ°á»›i Ä‘Ã¢y:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

Káº¿t quáº£ tokenize má»—i tá»« vÃ  Ä‘iá»ƒm tÆ°Æ¡ng á»©ng cá»§a chÃºng nhÆ° sau:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

VÃ  sá»± máº¥t mÃ¡t báº±ng:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Giá» thÃ¬ ta cáº§n tÃ­nh xem viá»‡c loáº¡i bá» má»—i token sáº½ áº£nh hÆ°á»Ÿng tháº¿ nÃ o tá»›i sá»± máº¥t mÃ¡t. Äiá»u nÃ y khÃ¡ táº» nháº¡t, vÃ¬ váº­y chÃºng ta sáº½ chá»‰ lÃ m Ä‘iá»u Ä‘Ã³ cho hai token á»Ÿ Ä‘Ã¢y vÃ  lÆ°u toÃ n bá»™ quÃ¡ trÃ¬nh khi chÃºng ta cÃ³ Ä‘oáº¡n mÃ£ trá»£ giÃºp. Trong trÆ°á»ng há»£p (ráº¥t) cá»¥ thá»ƒ nÃ y, chÃºng tÃ´i cÃ³ hai token tÆ°Æ¡ng Ä‘Æ°Æ¡ng cá»§a táº¥t cáº£ cÃ¡c tá»«: nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, vÃ­ dá»¥: `"pug"` cÃ³ thá»ƒ Ä‘Æ°á»£c tokenize `["p","ug"]` vá»›i cÃ¹ng sá»‘ Ä‘iá»ƒm. Do Ä‘Ã³, loáº¡i bá» mÃ£ thÃ´ng bÃ¡o `"pu"` khá»i tá»« vá»±ng sáº½ gÃ¢y ra sá»± máº¥t mÃ¡t tÆ°Æ¡ng tá»±.

Máº·t khÃ¡c, viá»‡c loáº¡i bá» `"hug"` sáº½ lÃ m cho sá»± máº¥t mÃ¡t trá»Ÿ nÃªn tá»“i tá»‡ hÆ¡n, bá»Ÿi vÃ¬ token cá»§a `"hug"` vÃ  `"hugs"` sáº½ trá»Ÿ thÃ nh:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Nhá»¯ng thay Ä‘á»•i nÃ y sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n vÃ  lÃ m sá»± máº¥t mÃ¡t tÄƒng lÃªn

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

VÃ¬ váº­y, token `"pu"` cháº¯c cháº¯n sáº½ bá»‹ loáº¡i khá»i bá»™ tá»« vá»±ng, nhÆ°ng `"hug"` thÃ¬ khÃ´ng.

## Triá»ƒn khai Unigram

Giá» hÃ£y cÃ¹ng triá»ƒn khai má»i thá»© ta Ä‘Ã£ cÃ¹ng xem thÃ´ng qua Ä‘aonj mÃ£. Giá»‘ng nhÆ° BPE vÃ  WordPiece, Ä‘Ã¢y khÃ´ng pháº£i lÃ  má»™t cÃ¡ch triá»ƒn khai hiá»ƒu quáº£ cá»§a thuáº­t toÃ¡n (khÃ¡ ngÆ°á»£c láº¡i), nhÆ°ng nÃ³ sáº½ giÃºp báº¡n hiá»ƒu hÆ¡n vá» Unigram.

Ta sáº½ sá»­ dÃ¹ng cÃ¹ng bá»™ ngá»¯ liá»‡u Ä‘Ã£ sá»­ dá»¥ng nhÆ° má»™t vÃ­ dá»¥:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Láº§n nÃ y, ta sáº½ sá»­ dá»¥ng `xlnet-base-cased` nhÆ° mÃ´ hÃ¬nh:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

TÆ°Æ¡ng tá»± BPE vÃ  WordPiece, ta sáº½ báº¯t Ä‘áº§u Ä‘áº¿m táº§n suáº¥t xuáº¥t hiá»‡n cá»§a má»—i tá»« trong kho ngá»¯ liá»‡u:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Sau Ä‘Ã³, ta cáº§n khá»Ÿi táº¡o bá»™ tá»« vá»±ng vá»›i sá»‘ lÆ°á»£ng lá»›n hÆ¡n kÃ­ch thÆ°á»›c ta muá»‘n cuá»‘i cÃ¹ng. Ta pháº£i tá»•ng káº¿t táº¥t cáº£ cÃ¡c kÃ­ tá»± cÆ¡ báº£n (náº¿u khÃ´ng ta sáº½ khÃ´ng thá»ƒ tokenize táº¥t cáº£ cÃ¡c tá»«), nhÆ°ng vá»›i cÃ¡c chuá»—i con lá»›n hÆ¡n ta sáº½ giá»¯ pháº§n thÃ´ng dá»¥ng nháº¥t vÃ  sáº¯p xáº¿p chÃºng theo táº§n suáº¥t:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Láº·p qua cÃ¡c tá»« con cÃ³ Ä‘á»™ dÃ i >= 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sáº¯p xáº¿p cÃ¡c tá»« con theo táº§n suáº¥t
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

Ta nhÃ³m cÃ¡c kÃ­ tá»± cÃ³ cÃ¡c tá»« con tá»‘t nháº¥t vÃ o bá»™ tá»« vá»±ng ban Ä‘áº§u cÃ³ kÃ­ch thÆ°á»›c lÃ  300:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>

ğŸ’¡ SentencePiece sá»­ dá»¥ng má»™t thuáº­t toÃ¡n hiá»‡u quáº£ hÆ¡n gá»i lÃ  Enhanced Suffix Array (ESA) Ä‘á»ƒ táº¡o ra bá»™ tá»« vá»±ng ban Ä‘áº§u.

</Tip>

Tiáº¿p theo, chÃºng ta tÃ­nh tá»•ng táº§n suáº¥t Ä‘á»ƒ biáº¿n Ä‘á»•i cÃ¡c táº§n suáº¥t nÃ y thÃ nh xÃ¡c suáº¥t. Vá»›i mÃ´ hÃ¬nh, chÃºng ta sáº½ lÆ°u cÃ¡c log cá»§a xÃ¡c xuáº¥t, vÃ¬ nÃ³ á»•n Ä‘á»‹nh hÆ¡n vá» máº·t sá»‘ há»c khi cá»™ng logarit hÆ¡n lÃ  nhÃ¢n cÃ¡c sá»‘ nhá» vÃ  Ä‘iá»u nÃ y sáº½ Ä‘Æ¡n giáº£n hÃ³a viá»‡c tÃ­nh toÃ¡n máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh: 

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Giá» thÃ¬ cÃ¡c hÃ m chÃ­nh lÃ  hÃ m tokenize tá»« sá»­ dá»¥ng thuáº­t toÃ¡n Viterbi. NhÆ° Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã³, thuáº­t toÃ¡n tÃ­nh phÃ¢n Ä‘oáº¡n tá»‘t nháº¥t cá»§a má»—i chuá»—i con cá»§a tá»«, Ä‘Æ°á»£c lÆ°u dÆ°á»›i biáº¿n  `best_segmentations`. ChÃºng ta sáº½ lÆ°u má»—i vá»‹ trÃ­ má»™t tá»« Ä‘iá»ƒn trong tá»« (tá»« 0 cho tá»›i Ä‘á»™ dÃ i cá»§a nÃ³), vá»›i hai khoÃ¡: chá»‰ má»¥c Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a token cuá»‘i trong pháº§n Ä‘oáº¡n tá»‘t nháº¥t, vÃ  Ä‘iá»ƒm cá»§a phÃ¢n Ä‘oáº¡n tá»‘t nháº¥t. Vá»›i chá»‰ má»¥c cá»§a Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a token cuá»‘i trong pháº§n Ä‘oáº¡n tá»‘t nháº¥t, ta sáº½ cÃ³ thá»ƒ truy váº¥n toÃ n bá»™ phÃ¢n Ä‘oáº¡n má»™t khi danh sÃ¡ch Ä‘Æ°á»£c Ä‘iá»n Ä‘á»§.

Viá»‡c Ä‘iá»n danh sÃ¡ch Ä‘Æ°á»£c thá»±c hiá»‡n chá»‰ vá»›i hai vÃ²ng láº·p: vÃ²ng láº·p chÃ­nh Ä‘i qua tá»«ng vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  vÃ²ng láº·p thá»© hai thá»­ táº¥t cáº£ cÃ¡c chuá»—i con báº¯t Ä‘áº§u tá»« vá»‹ trÃ­ báº¯t Ä‘áº§u Ä‘Ã³. Náº¿u chuá»—i con cÃ³ trong tá»« vá»±ng, chÃºng ta cÃ³ má»™t phÃ¢n Ä‘oáº¡n má»›i cá»§a tá»« cho Ä‘áº¿n vá»‹ trÃ­ káº¿t thÃºc Ä‘Ã³, vÃ  so sÃ¡nh vá»›i nhá»¯ng gÃ¬ cÃ³ trong `best_segmentations`.

Khi vÃ²ng láº·p chÃ­nh káº¿t thÃºc, chÃºng ta chá»‰ báº¯t Ä‘áº§u tá»« cuá»‘i vÃ  nháº£y tá»« vá»‹ trÃ­ báº¯t Ä‘áº§u nÃ y sang vá»‹ trÃ­ tiáº¿p theo, ghi láº¡i cÃ¡c token khi chÃºng ta Ä‘i, cho Ä‘áº¿n khi chÃºng ta Ä‘áº¿n vá»‹ trÃ­ Ä‘áº§u cá»§a tá»«:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # NÃ³ nÃªn Ä‘Æ°á»£c láº¥p Ä‘áº§y bá»Ÿi cÃ¡c bÆ°á»›c phÃ­a trÆ°á»›c cá»§a vÃ²ng láº·p
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # Náº¿u chÃºng ta tÃ¬m tháº¥y má»™t phÃ¢n Ä‘oáº¡n káº¿t thÃºc tá»‘t hÆ¡n táº¡i end_idx, chÃºng ta cáº­p nháº­t
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # Ta Ä‘Ã£ khÃ´ng tÃ¬m tháº¥y tokenize cá»§a tá»« -> khÃ´ng xÃ¡c Ä‘á»‹nh
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

Ta cÃ³ thá»ƒ sáºµn sÃ ng thá»­ mÃ´ hÃ¬nh ban Ä‘áº§u lÃªn má»™t sá»‘ tá»«:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Giá» thÃ¬ tháº­t dá»… dÃ ng Ä‘á»ƒ tÃ­nh sá»± máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh trÃªn kho ngá»¯ liá»‡u!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Ta cÃ³ thá»ƒ kiá»ƒm tra cÃ¡ch nÃ³ hoáº¡t Ä‘á»™ng trÃªn mÃ´ hÃ¬nh ta cÃ³:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

Viá»‡c tÃ­nh Ä‘iá»ƒm cho má»—i token khÃ´ng quáº£ khÃ³; ta chá»‰ pháº£i tÃ­nh sá»± máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh khi xoÃ¡ má»—i token:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # Ta luÃ´n giá»¯ Ä‘á»™ dÃ i cÃ¡c token báº±ng 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

Ta cÃ³ thá»ƒ thá»­ vá»›i token cho trÆ°á»›c:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

VÃ¬ `"ll"` Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh tokenize  `"Hopefully"`, vÃ  loáº¡i bá» nÃ³ cháº¯c cháº¯n sáº½ lÃ m ta thay vÃ o Ä‘Ã³ sá»­ dá»¥ng `"l"` hai láº§n, ta kÃ¬ vá»ng nÃ³ sáº½ Ä‘em láº¡i sá»± máº¥t mÃ¡t dÆ°Æ¡ng. `"his"` chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng trong tá»« `"This"`, nÃ³ Ä‘Æ°á»£c tokenize thÃ nh chÃ­nh nÃ³, nÃªn ta kÃ¬ vá»ng nÃ³ sáº½ khÃ´ng cÃ³ máº¥t mÃ¡t. VÃ  Ä‘Ã¢y lÃ  káº¿t quáº£:

```python out
6.376412403623874
0.0
```

<Tip>

ğŸ’¡ PhÆ°Æ¡ng phÃ¡p nÃ y ráº¥t khÃ´ng hiá»‡u quáº£, nÃªn SentencePiece  sá»­ dá»¥ng má»™t xáº¥p xá»‰ cá»§a hÃ m máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh mÃ  khÃ´ng dÃ¹ng token X: thay vÃ¬ báº¯t Ä‘áº§u tá»« Ä‘áº§u, nÃ³ chá»‰ thay tháº¿ token X bá»Ÿi phÃ¢n Ä‘oáº¡n bÃªn trÃ¡i cá»§a nÃ³ trong bá»™ tá»« vá»±ng. Báº±ng cÃ¡ch nÃ y, táº¥t cáº£ Ä‘iá»ƒm cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh trong cÃ¹ng má»™t láº§n Ä‘á»“ng thá»i vá»›i sá»± máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh.

</Tip>

Vá»›i táº¥t cáº£ nhá»¯ng Ä‘iá»u trÃªn, Ä‘iá»u cuá»‘i cÃ¹ng ta cáº§n pháº£i lÃ m lÃ  thÃªm cÃ¡c token Ä‘áº·c biá»‡t cá»§a mÃ´ hÃ¬nh vÃ o bá»™ tá»« vá»±ng, sau Ä‘Ã³ láº·p cho Ä‘áº¿n khi chÃºng ta cáº¯t Ä‘á»§ sá»‘ token ta mong muá»‘n cho kÃ­ch cá»¡ bá»™ tá»« vá»±ng:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Loáº¡i token percent_to_remove vá»›i Ä‘iá»ƒm tháº¥p nháº¥t.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])
    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Sau Ä‘Ã³, Ä‘á»ƒ tokenize cÃ¡c Ä‘oáº¡n vÄƒn báº£n, ta chá»‰ cáº§n Ã¡p dá»¥ng pre-tokenization vÃ  sau Ä‘á» sá»­ dá»¥ng hÃ m `encode_word()`:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

VÃ  Ä‘Ã³ lÃ  Unigram! Hy vá»ng ráº±ng bÃ¢y giá» báº¡n cáº£m tháº¥y nhÆ° má»™t chuyÃªn gia trong táº¥t cáº£ má»i tokenizer. Trong pháº§n tiáº¿p theo, chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o cÃ¡c khá»‘i cá»§a thÆ° viá»‡n ğŸ¤— Tokenizers vÃ  chá»‰ cho báº¡n cÃ¡ch báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng chÃºng Ä‘á»ƒ táº¡o tokenizer cá»§a riÃªng mÃ¬nh.
