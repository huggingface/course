# Unigram tokenization

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section7.ipynb"},
]} />

Thu·∫≠t to√°n Unigram th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ dung trong SentencePiece,  ƒë√¢y l√† m·ªôt thu·∫≠t to√°n tokenize cho c√°c m√¥ h√¨nh nh∆∞ AlBERT, T5, mBART, Big Bird, v√† XLNet.

<Youtube id="TGZfZVuF9Yc"/>

> [!TIP]
> üí° Ph·∫ßn n√†y s·∫Ω ƒëi s√¢u v√†o Unigram c≈©ng nh∆∞ to√†n b·ªô c√°ch tri·ªÉn khai. B·∫°n c√≥ th·ªÉ b·ªè qua ph·∫ßn cu·ªëi n·∫øu b·∫°n ch·ªâ quan t√¢m t·ªïng quan thu·∫≠t to√°n tokenize.

## Thu·∫≠t to√°n hu·∫•n luy·ªán

So v·ªõi BPE v√† WordPiece, Unigram ho·∫°t ƒë·ªông theo h∆∞·ªõng kh√°c: n√≥ b·∫Øt ƒë·∫ßu t·ª´ m·ªôt t·ª´ v·ª±ng l·ªõn v√† lo·∫°i b·ªè c√°c token cho ƒë·∫øn khi n√≥ ƒë·∫°t ƒë·∫øn k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën. C√≥ m·ªôt s·ªë t√πy ch·ªçn ƒë·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ x√¢y d·ª±ng v·ªën t·ª´ v·ª±ng c∆° b·∫£n: v√≠ d·ª•: ch√∫ng ta c√≥ th·ªÉ l·∫•y c√°c chu·ªói con ph·ªï bi·∫øn nh·∫•t trong c√°c t·ª´ ƒë∆∞·ª£c ti·ªÅn tokenize ho·∫∑c √°p d·ª•ng BPE tr√™n kho ng·ªØ li·ªáu ban ƒë·∫ßu c√≥ k√≠ch th∆∞·ªõc t·ª´ v·ª±ng l·ªõn.

T·∫°i m·ªói b∆∞·ªõc c·ªßa qu√° tr√¨nh hu·∫•n luy·ªán, thu·∫≠t to√°n Unigram t√≠nh to√°n s·ª± m·∫•t m√°t tr√™n kho ng·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p t·ª´ v·ª±ng hi·ªán t·∫°i. Sau ƒë√≥, ƒë·ªëi v·ªõi m·ªói k√Ω hi·ªáu trong t·ª´ v·ª±ng, thu·∫≠t to√°n s·∫Ω t√≠nh to√°n m·ª©c ƒë·ªô t·ªïn th·∫•t t·ªïng th·ªÉ s·∫Ω tƒÉng l√™n bao nhi√™u n·∫øu k√Ω hi·ªáu b·ªã x√≥a v√† t√¨m ki·∫øm c√°c k√Ω hi·ªáu l√†m tƒÉng n√≥ √≠t nh·∫•t. Nh·ªØng bi·ªÉu t∆∞·ª£ng ƒë√≥ c√≥ ·∫£nh h∆∞·ªüng th·∫•p h∆°n ƒë·∫øn s·ª± m·∫•t m√°t t·ªïng th·ªÉ ƒë·ªëi v·ªõi kho d·ªØ li·ªáu, v√¨ v·∫≠y theo m·ªôt nghƒ©a n√†o ƒë√≥, ch√∫ng "√≠t c·∫ßn thi·∫øt h∆°n" v√† l√† nh·ªØng ·ª©ng c·ª≠ vi√™n t·ªët nh·∫•t ƒë·ªÉ lo·∫°i b·ªè.

ƒê√¢y l√† m·ªôt ho·∫°t ƒë·ªông r·∫•t t·ªën k√©m, v√¨ v·∫≠y ch√∫ng t√¥i kh√¥ng ch·ªâ lo·∫°i b·ªè m·ªôt bi·ªÉu t∆∞·ª£ng li√™n quan ƒë·∫øn m·ª©c tƒÉng t·ªïn th·∫•t th·∫•p nh·∫•t, m√† \\(p\\) (\\(p\\) l√† m·ªôt si√™u tham s·ªë b·∫°n c√≥ th·ªÉ ki·ªÉm so√°t, th∆∞·ªùng l√† 10 ho·∫∑c 20) ph·∫ßn trƒÉm c√°c k√Ω hi·ªáu li√™n quan ƒë·∫øn m·ª©c tƒÉng t·ªïn th·∫•t th·∫•p nh·∫•t. Qu√° tr√¨nh n√†y sau ƒë√≥ ƒë∆∞·ª£c l·∫∑p l·∫°i cho ƒë·∫øn khi t·ª´ v·ª±ng ƒë·∫°t ƒë∆∞·ª£c k√≠ch th∆∞·ªõc mong mu·ªën.

L∆∞u √Ω r·∫±ng ch√∫ng ta kh√¥ng bao gi·ªù x√≥a c√°c k√Ω t·ª± c∆° s·ªü, ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng b·∫•t k·ª≥ t·ª´ n√†o c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c tokenize.

B√¢y gi·ªù, ƒëi·ªÅu n√†y v·∫´n c√≤n h∆°i m∆° h·ªì: ph·∫ßn ch√≠nh c·ªßa thu·∫≠t to√°n l√† t√≠nh to√°n s·ª± m·∫•t m√°t trong kho ng·ªØ li·ªáu v√† xem n√≥ thay ƒë·ªïi nh∆∞ th·∫ø n√†o khi ch√∫ng t√¥i x√≥a m·ªôt s·ªë token kh·ªèi t·ª´ v·ª±ng, nh∆∞ng ch√∫ng ta ch∆∞a gi·∫£i th√≠ch c√°ch th·ª±c hi·ªán ƒëi·ªÅu n√†y. B∆∞·ªõc n√†y d·ª±a tr√™n thu·∫≠t to√°n tokenize c·ªßa m√¥ h√¨nh Unigram, v√¨ v·∫≠y ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o ph·∫ßn ti·∫øp theo.

Ch√∫ng ta s·∫Ω t√°i s·ª≠ d·ª•ng kho ng·ªØ li·ªáu t·ª´ c√°c v√≠ d·ª• tr∆∞·ªõc:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

and for this example, we will take all strict substrings for the initial vocabulary :

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## Thu·∫≠t to√°n tokenize

M√¥ h√¨nh Unigram l√† m·ªôt lo·∫°i m√¥ h√¨nh ng√¥n ng·ªØ coi m·ªói token l√† ƒë·ªôc l·∫≠p v·ªõi c√°c token tr∆∞·ªõc n√≥. ƒê√≥ l√† m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆°n gi·∫£n nh·∫•t, theo nghƒ©a x√°c su·∫•t c·ªßa token X trong b·ªëi c·∫£nh tr∆∞·ªõc ƒë√≥ ch·ªâ l√† x√°c su·∫•t c·ªßa token X. V√¨ v·∫≠y, n·∫øu ch√∫ng ta s·ª≠ d·ª•ng m√¥ h√¨nh ng√¥n ng·ªØ Unigram ƒë·ªÉ t·∫°o vƒÉn b·∫£n, ch√∫ng ta s·∫Ω lu√¥n d·ª± ƒëo√°n token ph·ªï bi·∫øn nh·∫•t.

X√°c su·∫•t c·ªßa m·ªôt token nh·∫•t ƒë·ªãnh l√† t·∫ßn su·∫•t c·ªßa n√≥ (s·ªë l·∫ßn ch√∫ng ta t√¨m th·∫•y n√≥) trong kho t√†i li·ªáu g·ªëc, chia cho t·ªïng t·∫•t c·∫£ c√°c t·∫ßn s·ªë c·ªßa t·∫•t c·∫£ c√°c token trong t·ª´ v·ª±ng (ƒë·ªÉ ƒë·∫£m b·∫£o x√°c su·∫•t t·ªïng b·∫±ng 1). V√≠ d·ª•: `"ug"` c√≥ trong `"hug"`, `"pug"` v√† `"hugs"`, v√¨ v·∫≠y n√≥ c√≥ t·∫ßn su·∫•t l√† 20 trong kho ng·ªØ li·ªáu c·ªßa ch√∫ng t√¥i.

D∆∞·ªõi ƒë√¢y l√† t·∫ßn su·∫•t c·ªßa t·∫•t c·∫£ c√°c t·ª´ ph·ª• c√≥ th·ªÉ c√≥ trong t·ª´ v·ª±ng:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

V·∫≠y n√™n, t·ªïng c·ªßa t·∫•t c·∫£ c√°c t·∫ßn su·∫•t l√† 210, v√† x√°c su·∫•t c·ªßa t·ª´ ph·ª• `"ug"` l√† 20/210.

> [!TIP]
> ‚úèÔ∏è **Gi·ªù ƒë·∫øn l∆∞·ª£t b·∫°n!** Vi·∫øt ƒëo·∫°n m√£ ƒë·ªÉ t√≠nh t·∫ßn su·∫•t tr√™n v√† ki·ªÉm tra l·∫°i k·∫øt qu·∫£ hi·ªÉn th·ªã c≈©ng nh∆∞ t·ªïng ƒë√£ ƒë√∫ng ch∆∞a.

Gi·ªù, ƒë·ªÉ tokenize m·ªôt t·ª´ cho tr∆∞·ªõc, ch√∫ng ta s·∫Ω nh√¨n v√†o t·∫•t c·∫£ c√°c ph·∫ßn ƒëo·∫°n th√†nh token v√† t√≠nh x√°c su·∫•t c·ªßa t·ª´ng c√°i theo m√¥ h√¨nh Unigram. V√¨ t·∫•t c·∫£ token ƒë∆∞·ª£c cho l√† ƒë·ªôc l·∫≠p, x√°c su·∫•t n√†y ch·ªâ l√† t√≠ch c·ªßa x√°c su·∫•t m·ªói token. V√≠ d·ª•, `["p", "u", "g"]` c·ªßa `"pug"` c√≥ x√°c su·∫•t:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

T∆∞∆°ng t·ª±, `["pu", "g"]` c√≥ x√°c su·∫•t:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

N√≥i chung, vi·ªác tokenize c√≥ √≠t token nh·∫•t c√≥ th·ªÉ s·∫Ω c√≥ x√°c su·∫•t cao nh·∫•t (v√¨ ph√©p chia cho 210 l·∫∑p l·∫°i cho m·ªói token), t∆∞∆°ng ·ª©ng v·ªõi nh·ªØng g√¨ ch√∫ng ta mu·ªën tr·ª±c quan: chia m·ªôt t·ª´ th√†nh s·ªë l∆∞·ª£ng token nh·∫•t c√≥ th·ªÉ.

Tokenize c·ªßa m·ªôt t·ª´ v·ªõi m√¥ h√¨nh Unigram sau ƒë√≥ l√† token c√≥ x√°c su·∫•t cao nh·∫•t. Trong v√≠ d·ª• v·ªÅ `"pug"`, ƒë√¢y l√† c√°c x√°c su·∫•t m√† ch√∫ng ta s·∫Ω nh·∫≠n ƒë∆∞·ª£c cho m·ªói ph√¢n ƒëo·∫°n c√≥ th·ªÉ c√≥:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

V√¨ v·∫≠y, `"pug"` s·∫Ω ƒë∆∞·ª£c tokenize l√† `["p", "ug"]`  ho·∫∑c `["pu", "g"]`, t√πy thu·ªôc v√†o ph√¢n ƒëo·∫°n n√†o trong s·ªë ƒë√≥ ƒë∆∞·ª£c g·∫∑p ƒë·∫ßu ti√™n (l∆∞u √Ω r·∫±ng trong m·ªôt ph√¢n ƒëo·∫°n l·ªõn h∆°n ng·ªØ li·ªáu, nh·ªØng tr∆∞·ªùng h·ª£p b√¨nh ƒë·∫≥ng nh∆∞ th·∫ø n√†y s·∫Ω r·∫•t hi·∫øm).

Trong tr∆∞·ªùng h·ª£p n√†y, th·∫≠t d·ªÖ d√†ng ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c ph√¢n ƒëo·∫°n c√≥ th·ªÉ c√≥ v√† t√≠nh to√°n x√°c su·∫•t c·ªßa ch√∫ng, nh∆∞ng n√≥i chung s·∫Ω kh√≥ h∆°n m·ªôt ch√∫t. C√≥ m·ªôt thu·∫≠t to√°n c·ªï ƒëi·ªÉn ƒë∆∞·ª£c s·ª≠ d·ª•ng cho vi·ªác n√†y, ƒë∆∞·ª£c g·ªçi l√† thu·∫≠t to√°n *Viterbi*. V·ªÅ c∆° b·∫£n, ch√∫ng ta c√≥ th·ªÉ x√¢y d·ª±ng m·ªôt bi·ªÉu ƒë·ªì ƒë·ªÉ ph√°t hi·ªán c√°c ph√¢n ƒëo·∫°n c√≥ th·ªÉ c√≥ c·ªßa m·ªôt t·ª´ nh·∫•t ƒë·ªãnh b·∫±ng c√°ch n√≥i r·∫±ng c√≥ m·ªôt nh√°nh t·ª´ k√Ω t·ª± _a_ ƒë·∫øn k√Ω t·ª± _b_ n·∫øu t·ª´ ph·ª• t·ª´ _a_ ƒë·∫øn _b_ n·∫±m trong t·ª´ v·ª±ng v√† quy cho nh√°nh ƒë√≥ x√°c su·∫•t c·ªßa t·ª´ ph·ª• .

ƒê·ªÉ t√¨m ƒë∆∞·ªùng d·∫´n trong bi·ªÉu ƒë·ªì ƒë√≥ s·∫Ω c√≥ ƒëi·ªÉm t·ªët nh·∫•t, thu·∫≠t to√°n Viterbi x√°c ƒë·ªãnh, ƒë·ªëi v·ªõi m·ªói v·ªã tr√≠ trong t·ª´, ph√¢n ƒëo·∫°n c√≥ ƒëi·ªÉm t·ªët nh·∫•t k·∫øt th√∫c t·∫°i v·ªã tr√≠ ƒë√≥. V√¨ ch√∫ng ta ƒëi t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi, ƒëi·ªÉm t·ªët nh·∫•t c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y b·∫±ng c√°ch l·∫∑p qua t·∫•t c·∫£ c√°c t·ª´ ph·ª• k·∫øt th√∫c ·ªü v·ªã tr√≠ hi·ªán t·∫°i v√† sau ƒë√≥ s·ª≠ d·ª•ng ƒëi·ªÉm token t·ªët nh·∫•t t·ª´ ‚Äã‚Äãv·ªã tr√≠ m√† t·ª´ ph·ª• n√†y b·∫Øt ƒë·∫ßu. Sau ƒë√≥, ch√∫ng ta ch·ªâ c·∫ßn b·ªè qua con ƒë∆∞·ªùng ƒë√£ th·ª±c hi·ªán ƒë·ªÉ ƒë·∫øn cu·ªëi.

H√£y xem m·ªôt v√≠ d·ª• s·ª≠ d·ª•ng t·ª´ v·ª±ng c·ªßa ch√∫ng ta v√† t·ª´ `"unhug"`. ƒê·ªëi v·ªõi m·ªói v·ªã tr√≠, c√°c t·ª´ ph·ª• c√≥ ƒëi·ªÉm s·ªë t·ªët nh·∫•t k·∫øt th√∫c ·ªü ƒë√≥ nh∆∞ sau:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

V·∫≠y `"unhug"` c√≥ th·ªÉ tokenize th√†nh `["un", "hug"]`.

> [!TIP]
> ‚úèÔ∏è **Gi·ªù ƒë·∫øn l∆∞·ª£t b·∫°n!** X√°c ƒë·ªãnh token c·ªßa t·ª´ `"huggun"`, v√† ƒëi·ªÉm c·∫£u ch√∫ng.

## Quay l·∫°i hu·∫•n luy·ªán

B√¢y gi·ªù ch√∫ng ta ƒë√£ th·∫•y c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa tokenize, ch√∫ng ta c√≥ th·ªÉ t√¨m hi·ªÉu s√¢u h∆°n m·ªôt ch√∫t v·ªÅ s·ª± m·∫•t m√°t ƒë∆∞·ª£c s·ª≠ d·ª•ng trong qu√° tr√¨nh hu·∫•n luy·ªán. ·ªû b·∫•t k·ª≥ giai ƒëo·∫°n nh·∫•t ƒë·ªãnh n√†o, s·ª± m·∫•t m√°t n√†y ƒë∆∞·ª£c t√≠nh to√°n b·∫±ng c√°ch tokenize m·ªçi t·ª´ trong kho ng·ªØ li·ªáu, s·ª≠ d·ª•ng t·ª´ v·ª±ng hi·ªán t·∫°i v√† m√¥ h√¨nh Unigram ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi t·∫ßn s·ªë c·ªßa m·ªói token trong kho ng·ªØ li·ªáu (nh∆∞ ƒë√£ th·∫•y tr∆∞·ªõc ƒë√¢y).

M·ªói t·ª´ trong kho ng·ªØ li·ªáu ƒë·ªÅu c√≥ m·ªôt ƒëi·ªÉm v√† s·ª± m·∫•t m√°t l√† kh·∫£ nƒÉng b·ªã √¢m c·ªßa nh·ªØng ƒëi·ªÉm s·ªë ƒë√≥ - nghƒ©a l√† t·ªïng cho t·∫•t c·∫£ c√°c t·ª´ trong kho ng·ªØ li·ªáu c·ªßa t·∫•t c·∫£ c√°c `-log(P(word))`.

C√πng xem v√≠ d·ª• c·ªßa ch√∫ng ta v·ªõi kho ng·ªØ li·ªáu d∆∞·ªõi ƒë√¢y:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

K·∫øt qu·∫£ tokenize m·ªói t·ª´ v√† ƒëi·ªÉm t∆∞∆°ng ·ª©ng c·ªßa ch√∫ng nh∆∞ sau:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

V√† s·ª± m·∫•t m√°t b·∫±ng:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

Gi·ªù th√¨ ta c·∫ßn t√≠nh xem vi·ªác lo·∫°i b·ªè m·ªói token s·∫Ω ·∫£nh h∆∞·ªüng th·∫ø n√†o t·ªõi s·ª± m·∫•t m√°t. ƒêi·ªÅu n√†y kh√° t·∫ª nh·∫°t, v√¨ v·∫≠y ch√∫ng ta s·∫Ω ch·ªâ l√†m ƒëi·ªÅu ƒë√≥ cho hai token ·ªü ƒë√¢y v√† l∆∞u to√†n b·ªô qu√° tr√¨nh khi ch√∫ng ta c√≥ ƒëo·∫°n m√£ tr·ª£ gi√∫p. Trong tr∆∞·ªùng h·ª£p (r·∫•t) c·ª• th·ªÉ n√†y, ch√∫ng t√¥i c√≥ hai token t∆∞∆°ng ƒë∆∞∆°ng c·ªßa t·∫•t c·∫£ c√°c t·ª´: nh∆∞ ch√∫ng ta ƒë√£ th·∫•y tr∆∞·ªõc ƒë√≥, v√≠ d·ª•: `"pug"` c√≥ th·ªÉ ƒë∆∞·ª£c tokenize `["p","ug"]` v·ªõi c√πng s·ªë ƒëi·ªÉm. Do ƒë√≥, lo·∫°i b·ªè m√£ th√¥ng b√°o `"pu"` kh·ªèi t·ª´ v·ª±ng s·∫Ω g√¢y ra s·ª± m·∫•t m√°t t∆∞∆°ng t·ª±.

M·∫∑t kh√°c, vi·ªác lo·∫°i b·ªè `"hug"` s·∫Ω l√†m cho s·ª± m·∫•t m√°t tr·ªü n√™n t·ªìi t·ªá h∆°n, b·ªüi v√¨ token c·ªßa `"hug"` v√† `"hugs"` s·∫Ω tr·ªü th√†nh:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

Nh·ªØng thay ƒë·ªïi n√†y s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn v√† l√†m s·ª± m·∫•t m√°t tƒÉng l√™n

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

V√¨ v·∫≠y, token `"pu"` ch·∫Øc ch·∫Øn s·∫Ω b·ªã lo·∫°i kh·ªèi b·ªô t·ª´ v·ª±ng, nh∆∞ng `"hug"` th√¨ kh√¥ng.

## Tri·ªÉn khai Unigram

Gi·ªù h√£y c√πng tri·ªÉn khai m·ªçi th·ª© ta ƒë√£ c√πng xem th√¥ng qua ƒëaonj m√£. Gi·ªëng nh∆∞ BPE v√† WordPiece, ƒë√¢y kh√¥ng ph·∫£i l√† m·ªôt c√°ch tri·ªÉn khai hi·ªÉu qu·∫£ c·ªßa thu·∫≠t to√°n (kh√° ng∆∞·ª£c l·∫°i), nh∆∞ng n√≥ s·∫Ω gi√∫p b·∫°n hi·ªÉu h∆°n v·ªÅ Unigram.

Ta s·∫Ω s·ª≠ d√πng c√πng b·ªô ng·ªØ li·ªáu ƒë√£ s·ª≠ d·ª•ng nh∆∞ m·ªôt v√≠ d·ª•:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

L·∫ßn n√†y, ta s·∫Ω s·ª≠ d·ª•ng `xlnet-base-cased` nh∆∞ m√¥ h√¨nh:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

T∆∞∆°ng t·ª± BPE v√† WordPiece, ta s·∫Ω b·∫Øt ƒë·∫ßu ƒë·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªói t·ª´ trong kho ng·ªØ li·ªáu:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

Sau ƒë√≥, ta c·∫ßn kh·ªüi t·∫°o b·ªô t·ª´ v·ª±ng v·ªõi s·ªë l∆∞·ª£ng l·ªõn h∆°n k√≠ch th∆∞·ªõc ta mu·ªën cu·ªëi c√πng. Ta ph·∫£i t·ªïng k·∫øt t·∫•t c·∫£ c√°c k√≠ t·ª± c∆° b·∫£n (n·∫øu kh√¥ng ta s·∫Ω kh√¥ng th·ªÉ tokenize t·∫•t c·∫£ c√°c t·ª´), nh∆∞ng v·ªõi c√°c chu·ªói con l·ªõn h∆°n ta s·∫Ω gi·ªØ ph·∫ßn th√¥ng d·ª•ng nh·∫•t v√† s·∫Øp x·∫øp ch√∫ng theo t·∫ßn su·∫•t:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # L·∫∑p qua c√°c t·ª´ con c√≥ ƒë·ªô d√†i >= 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# S·∫Øp x·∫øp c√°c t·ª´ con theo t·∫ßn su·∫•t
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('‚ñÅt', 7), ('is', 5), ('er', 5), ('‚ñÅa', 5), ('‚ñÅto', 4), ('to', 4), ('en', 4), ('‚ñÅT', 3), ('‚ñÅTh', 3), ('‚ñÅThi', 3)]
```

Ta nh√≥m c√°c k√≠ t·ª± c√≥ c√°c t·ª´ con t·ªët nh·∫•t v√†o b·ªô t·ª´ v·ª±ng ban ƒë·∫ßu c√≥ k√≠ch th∆∞·ªõc l√† 300:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

> [!TIP]
> üí° SentencePiece s·ª≠ d·ª•ng m·ªôt thu·∫≠t to√°n hi·ªáu qu·∫£ h∆°n g·ªçi l√† Enhanced Suffix Array (ESA) ƒë·ªÉ t·∫°o ra b·ªô t·ª´ v·ª±ng ban ƒë·∫ßu.

Ti·∫øp theo, ch√∫ng ta t√≠nh t·ªïng t·∫ßn su·∫•t ƒë·ªÉ bi·∫øn ƒë·ªïi c√°c t·∫ßn su·∫•t n√†y th√†nh x√°c su·∫•t. V·ªõi m√¥ h√¨nh, ch√∫ng ta s·∫Ω l∆∞u c√°c log c·ªßa x√°c xu·∫•t, v√¨ n√≥ ·ªïn ƒë·ªãnh h∆°n v·ªÅ m·∫∑t s·ªë h·ªçc khi c·ªông logarit h∆°n l√† nh√¢n c√°c s·ªë nh·ªè v√† ƒëi·ªÅu n√†y s·∫Ω ƒë∆°n gi·∫£n h√≥a vi·ªác t√≠nh to√°n m·∫•t m√°t c·ªßa m√¥ h√¨nh: 

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Gi·ªù th√¨ c√°c h√†m ch√≠nh l√† h√†m tokenize t·ª´ s·ª≠ d·ª•ng thu·∫≠t to√°n Viterbi. Nh∆∞ ƒë√£ th·∫•y tr∆∞·ªõc ƒë√≥, thu·∫≠t to√°n t√≠nh ph√¢n ƒëo·∫°n t·ªët nh·∫•t c·ªßa m·ªói chu·ªói con c·ªßa t·ª´, ƒë∆∞·ª£c l∆∞u d∆∞·ªõi bi·∫øn  `best_segmentations`. Ch√∫ng ta s·∫Ω l∆∞u m·ªói v·ªã tr√≠ m·ªôt t·ª´ ƒëi·ªÉn trong t·ª´ (t·ª´ 0 cho t·ªõi ƒë·ªô d√†i c·ªßa n√≥), v·ªõi hai kho√°: ch·ªâ m·ª•c ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa token cu·ªëi trong ph·∫ßn ƒëo·∫°n t·ªët nh·∫•t, v√† ƒëi·ªÉm c·ªßa ph√¢n ƒëo·∫°n t·ªët nh·∫•t. V·ªõi ch·ªâ m·ª•c c·ªßa ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ªßa token cu·ªëi trong ph·∫ßn ƒëo·∫°n t·ªët nh·∫•t, ta s·∫Ω c√≥ th·ªÉ truy v·∫•n to√†n b·ªô ph√¢n ƒëo·∫°n m·ªôt khi danh s√°ch ƒë∆∞·ª£c ƒëi·ªÅn ƒë·ªß.

Vi·ªác ƒëi·ªÅn danh s√°ch ƒë∆∞·ª£c th·ª±c hi·ªán ch·ªâ v·ªõi hai v√≤ng l·∫∑p: v√≤ng l·∫∑p ch√≠nh ƒëi qua t·ª´ng v·ªã tr√≠ b·∫Øt ƒë·∫ßu v√† v√≤ng l·∫∑p th·ª© hai th·ª≠ t·∫•t c·∫£ c√°c chu·ªói con b·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ b·∫Øt ƒë·∫ßu ƒë√≥. N·∫øu chu·ªói con c√≥ trong t·ª´ v·ª±ng, ch√∫ng ta c√≥ m·ªôt ph√¢n ƒëo·∫°n m·ªõi c·ªßa t·ª´ cho ƒë·∫øn v·ªã tr√≠ k·∫øt th√∫c ƒë√≥, v√† so s√°nh v·ªõi nh·ªØng g√¨ c√≥ trong `best_segmentations`.

Khi v√≤ng l·∫∑p ch√≠nh k·∫øt th√∫c, ch√∫ng ta ch·ªâ b·∫Øt ƒë·∫ßu t·ª´ cu·ªëi v√† nh·∫£y t·ª´ v·ªã tr√≠ b·∫Øt ƒë·∫ßu n√†y sang v·ªã tr√≠ ti·∫øp theo, ghi l·∫°i c√°c token khi ch√∫ng ta ƒëi, cho ƒë·∫øn khi ch√∫ng ta ƒë·∫øn v·ªã tr√≠ ƒë·∫ßu c·ªßa t·ª´:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # N√≥ n√™n ƒë∆∞·ª£c l·∫•p ƒë·∫ßy b·ªüi c√°c b∆∞·ªõc ph√≠a tr∆∞·ªõc c·ªßa v√≤ng l·∫∑p
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # N·∫øu ch√∫ng ta t√¨m th·∫•y m·ªôt ph√¢n ƒëo·∫°n k·∫øt th√∫c t·ªët h∆°n t·∫°i end_idx, ch√∫ng ta c·∫≠p nh·∫≠t
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # Ta ƒë√£ kh√¥ng t√¨m th·∫•y tokenize c·ªßa t·ª´ -> kh√¥ng x√°c ƒë·ªãnh
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

Ta c√≥ th·ªÉ s·∫µn s√†ng th·ª≠ m√¥ h√¨nh ban ƒë·∫ßu l√™n m·ªôt s·ªë t·ª´:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

Gi·ªù th√¨ th·∫≠t d·ªÖ d√†ng ƒë·ªÉ t√≠nh s·ª± m·∫•t m√°t c·ªßa m√¥ h√¨nh tr√™n kho ng·ªØ li·ªáu!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

Ta c√≥ th·ªÉ ki·ªÉm tra c√°ch n√≥ ho·∫°t ƒë·ªông tr√™n m√¥ h√¨nh ta c√≥:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

Vi·ªác t√≠nh ƒëi·ªÉm cho m·ªói token kh√¥ng qu·∫£ kh√≥; ta ch·ªâ ph·∫£i t√≠nh s·ª± m·∫•t m√°t c·ªßa m√¥ h√¨nh khi xo√° m·ªói token:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # Ta lu√¥n gi·ªØ ƒë·ªô d√†i c√°c token b·∫±ng 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

Ta c√≥ th·ªÉ th·ª≠ v·ªõi token cho tr∆∞·ªõc:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

V√¨ `"ll"` ƒë∆∞·ª£c s·ª≠ d·ª•ng trong qu√° tr√¨nh tokenize  `"Hopefully"`, v√† lo·∫°i b·ªè n√≥ ch·∫Øc ch·∫Øn s·∫Ω l√†m ta thay v√†o ƒë√≥ s·ª≠ d·ª•ng `"l"` hai l·∫ßn, ta k√¨ v·ªçng n√≥ s·∫Ω ƒëem l·∫°i s·ª± m·∫•t m√°t d∆∞∆°ng. `"his"` ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong t·ª´ `"This"`, n√≥ ƒë∆∞·ª£c tokenize th√†nh ch√≠nh n√≥, n√™n ta k√¨ v·ªçng n√≥ s·∫Ω kh√¥ng c√≥ m·∫•t m√°t. V√† ƒë√¢y l√† k·∫øt qu·∫£:

```python out
6.376412403623874
0.0
```

> [!TIP]
> üí° Ph∆∞∆°ng ph√°p n√†y r·∫•t kh√¥ng hi·ªáu qu·∫£, n√™n SentencePiece  s·ª≠ d·ª•ng m·ªôt x·∫•p x·ªâ c·ªßa h√†m m·∫•t m√°t c·ªßa m√¥ h√¨nh m√† kh√¥ng d√πng token X: thay v√¨ b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu, n√≥ ch·ªâ thay th·∫ø token X b·ªüi ph√¢n ƒëo·∫°n b√™n tr√°i c·ªßa n√≥ trong b·ªô t·ª´ v·ª±ng. B·∫±ng c√°ch n√†y, t·∫•t c·∫£ ƒëi·ªÉm c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh trong c√πng m·ªôt l·∫ßn ƒë·ªìng th·ªùi v·ªõi s·ª± m·∫•t m√°t c·ªßa m√¥ h√¨nh.

V·ªõi t·∫•t c·∫£ nh·ªØng ƒëi·ªÅu tr√™n, ƒëi·ªÅu cu·ªëi c√πng ta c·∫ßn ph·∫£i l√†m l√† th√™m c√°c token ƒë·∫∑c bi·ªát c·ªßa m√¥ h√¨nh v√†o b·ªô t·ª´ v·ª±ng, sau ƒë√≥ l·∫∑p cho ƒë·∫øn khi ch√∫ng ta c·∫Øt ƒë·ªß s·ªë token ta mong mu·ªën cho k√≠ch c·ª° b·ªô t·ª´ v·ª±ng:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Lo·∫°i token percent_to_remove v·ªõi ƒëi·ªÉm th·∫•p nh·∫•t.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])
    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Sau ƒë√≥, ƒë·ªÉ tokenize c√°c ƒëo·∫°n vƒÉn b·∫£n, ta ch·ªâ c·∫ßn √°p d·ª•ng pre-tokenization v√† sau ƒë·ªè s·ª≠ d·ª•ng h√†m `encode_word()`:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅHugging', '‚ñÅFace', '‚ñÅ', 'c', 'ou', 'r', 's', 'e', '.']
```

V√† ƒë√≥ l√† Unigram! Hy v·ªçng r·∫±ng b√¢y gi·ªù b·∫°n c·∫£m th·∫•y nh∆∞ m·ªôt chuy√™n gia trong t·∫•t c·∫£ m·ªçi tokenizer. Trong ph·∫ßn ti·∫øp theo, ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o c√°c kh·ªëi c·ªßa th∆∞ vi·ªán ü§ó Tokenizers v√† ch·ªâ cho b·∫°n c√°ch b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ t·∫°o tokenizer c·ªßa ri√™ng m√¨nh.
