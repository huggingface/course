# Huáº¥n luyá»‡n má»™t tokenizer má»›i tá»« cÃ¡i cÅ©

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section2.ipynb"},
]} />

Náº¿u mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng cÃ³ sáºµn ngÃ´n ngá»¯ báº¡n quan tÃ¢m hoáº·c náº¿u kho tÃ i liá»‡u cá»§a báº¡n ráº¥t khÃ¡c vá»›i kho mÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ cá»§a báº¡n Ä‘Ã£ huáº¥n luyá»‡n, báº¡n ráº¥t cÃ³ thá»ƒ sáº½ muá»‘n huáº¥n luyá»‡n láº¡i mÃ´ hÃ¬nh tá»« Ä‘áº§u báº±ng cÃ¡ch sá»­ dá»¥ng trÃ¬nh tokenize phÃ¹ há»£p vá»›i dá»¯ liá»‡u cá»§a báº¡n. Äiá»u Ä‘Ã³ sáº½ yÃªu cáº§u huáº¥n luyá»‡n má»™t trÃ¬nh tokenize má»›i trÃªn táº­p dá»¯ liá»‡u cá»§a báº¡n. NhÆ°ng chÃ­nh xÃ¡c thÃ¬ Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  gÃ¬? Khi chÃºng ta láº§n Ä‘áº§u xem xÃ©t cÃ¡c tokenizer trong [ChÆ°Æ¡ng 2](/course/chapter2), chÃºng ta tháº¥y ráº±ng háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Transformer sá»­ dá»¥ng thuáº­t toÃ¡n tokenize _tá»« phá»¥_. Äá»ƒ xÃ¡c Ä‘á»‹nh nhá»¯ng tá»« phá»¥ nÃ o Ä‘Æ°á»£c quan tÃ¢m vÃ  xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn nháº¥t trong kho ngá»¯ liá»‡u hiá»‡n cÃ³, trÃ¬nh tokenize cáº§n pháº£i xem xÃ©t ká»¹ táº¥t cáº£ cÃ¡c vÄƒn báº£n trong kho ngá»¯ liá»‡u - má»™t quÃ¡ trÃ¬nh mÃ  chÃºng ta gá»i lÃ  *huáº¥n luyá»‡n*. CÃ¡c quy táº¯c chi phá»‘i viá»‡c huáº¥n luyá»‡n nÃ y phá»¥ thuá»™c vÃ o loáº¡i tokenizer Ä‘Æ°á»£c sá»­ dá»¥ng vÃ  chÃºng ta sáº½ xem xÃ©t ba thuáº­t toÃ¡n chÃ­nh á»Ÿ pháº§n sau cá»§a chÆ°Æ¡ng nÃ y.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

âš ï¸ Huáº¥n luyá»‡n má»™t tokenizer khÃ´ng giá»‘ng nhÆ° huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh! Huáº¥n luyá»‡n mÃ´ hÃ¬nh sá»­ dá»¥ng giáº£m Ä‘á»™ dá»‘c ngáº«u nhiÃªn Ä‘á»ƒ lÃ m cho tá»•n tháº¥t nhá» hÆ¡n má»™t chÃºt cho má»—i Ä‘á»£t. NÃ³ Ä‘Æ°á»£c ngáº«u nhiÃªn hÃ³a bá»Ÿi tá»± nhiÃªn (cÃ³ nghÄ©a lÃ  báº¡n pháº£i Ä‘áº·t má»™t giÃ¡ trá»‹ seed Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± khi thá»±c hiá»‡n cÃ¹ng thá»±c hiá»‡n huáº¥n luyá»‡n hai láº§n). Huáº¥n luyá»‡n má»™t trÃ¬nh tokenize lÃ  má»™t quy trÃ¬nh thá»‘ng kÃª cá»‘ gáº¯ng xÃ¡c Ä‘á»‹nh nhá»¯ng tá»« phá»¥ nÃ o tá»‘t nháº¥t Ä‘á»ƒ chá»n cho má»™t kho dá»¯ liá»‡u nháº¥t Ä‘á»‹nh, vÃ  cÃ¡c quy táº¯c Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chá»n chÃºng dá»±a trÃªn thuáº­t toÃ¡n tokenize. NÃ³ mang tÃ­nh cá»‘ Ä‘á»‹nh, nghÄ©a lÃ  báº¡n luÃ´n nháº­n Ä‘Æ°á»£c cÃ¹ng má»™t káº¿t quáº£ khi huáº¥n luyá»‡n vá»›i cÃ¹ng má»™t thuáº­t toÃ¡n trÃªn cÃ¹ng má»™t kho tÃ i liá»‡u.

</Tip>

## Táº­p há»£p má»™t kho ngá»¯ liá»‡u

CÃ³ má»™t API ráº¥t Ä‘Æ¡n giáº£n trong ğŸ¤— Transformers mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n má»™t tokenizer má»›i cÃ³ cÃ¹ng Ä‘áº·c Ä‘iá»ƒm vá»›i cÃ¡i hiá»‡n cÃ³: `AutoTokenizer.train_new_from_iterator()`. Äá»ƒ tháº¥y Ä‘iá»u nÃ y trong thá»±c táº¿, giáº£ sá»­ chÃºng ta muá»‘n huáº¥n luyá»‡n GPT-2 tá»« Ä‘áº§u, nhÆ°ng báº±ng má»™t ngÃ´n ngá»¯ khÃ¡c ngoÃ i tiáº¿ng Anh. Nhiá»‡m vá»¥ Ä‘áº§u tiÃªn cá»§a chÃºng ta sáº½ lÃ  thu tháº­p nhiá»u dá»¯ liá»‡u báº±ng ngÃ´n ngá»¯ Ä‘Ã³ trong má»™t kho dá»¯ liá»‡u huáº¥n luyá»‡n. Äá»ƒ cung cáº¥p cÃ¡c máº«u mÃ  má»i ngÆ°á»i cÃ³ hiá»ƒu Ä‘Æ°á»£c, chÃºng ta sáº½ khÃ´ng sá»­ dá»¥ng ngÃ´n ngá»¯ nhÆ° tiáº¿ng Nga hoáº·c tiáº¿ng Trung á»Ÿ Ä‘Ã¢y, mÃ  lÃ  ngÃ´n ngá»¯ tiáº¿ng Anh chuyÃªn dá»¥ng: Ä‘oáº¡n mÃ£ Python.

ThÆ° viá»‡n [ğŸ¤— Datasets](https://github.com/huggingface/datasets) cÃ³ thá»ƒ giÃºp chÃºng ta táº­p há»£p má»™t kho dá»¯ liá»‡u mÃ£ nguá»“n Python. ChÃºng ta sáº½ sá»­ dá»¥ng hÃ m `load_dataset()` thÃ´ng thÆ°á»ng Ä‘á»ƒ táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› cache cá»§a táº­p dá»¯ liá»‡u [CodeSearchNet](https://huggingface.co/datasets/code_search_net). Táº­p dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c táº¡o cho [thá»­ thÃ¡ch CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) vÃ  chá»©a hÃ ng triá»‡u hÃ m tá»« cÃ¡c thÆ° viá»‡n mÃ£ nguá»“n má»Ÿ trÃªn GitHub báº±ng má»™t sá»‘ ngÃ´n ngá»¯ láº­p trÃ¬nh. á» Ä‘Ã¢y, chÃºng ta sáº½ táº£i pháº§n Python cá»§a táº­p dá»¯ liá»‡u nÃ y:

```py
from datasets import load_dataset

# QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t má»™t vÃ i phÃºt Ä‘á»ƒ táº£i, vÃ¬ váº­y hÃ£y láº¥y cÃ  phÃª hoáº·c trÃ  trong khi chá» Ä‘á»£i!
raw_datasets = load_dataset("code_search_net", "python")
```

ChÃºng ta cÃ³ thá»ƒ xem xÃ©t pháº§n tÃ¡ch huáº¥n luyá»‡n Ä‘á»ƒ xem ta cÃ³ quyá»n truy cáº­p vÃ o nhá»¯ng cá»™t nÃ o:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y táº­p dá»¯ liá»‡u tÃ¡ch chuá»—i tÃ i liá»‡u mÃ´ táº£ khá»i Ä‘oáº¡n mÃ£ vÃ  Ä‘á» xuáº¥t tokenize cáº£ hai. á» Ä‘Ã¢y, chÃºng ta sáº½ chá»‰ sá»­ dá»¥ng cá»™t `whole_func_string` Ä‘á»ƒ huáº¥n luyá»‡n trÃ¬nh tokenize. ChÃºng ta cÃ³ thá»ƒ xem xÃ©t máº«u má»™t trong nhá»¯ng hÃ m nÃ y báº±ng cÃ¡ch láº­p chá»‰ má»¥c vÃ o pháº§n `train`:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

nÃ³ nÃªn tráº£ vá» káº¿t quáº£ nhÆ° dÆ°á»›i Ä‘Ã¢y:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

Äiá»u Ä‘áº§u tiÃªn chÃºng ta cáº§n lÃ m lÃ  chuyá»ƒn Ä‘á»•i táº­p dá»¯ liá»‡u thÃ nh má»™t _iterator_ danh sÃ¡ch cÃ¡c vÄƒn báº£n - vÃ­ dá»¥, má»™t danh sÃ¡ch cÃ¡c vÄƒn báº£n. Viá»‡c sá»­ dá»¥ng danh sÃ¡ch vÄƒn báº£n sáº½ cho phÃ©p tokenizer hoáº¡t Ä‘á»™ng nhanh hÆ¡n (huáº¥n luyá»‡n hÃ ng loáº¡t vÄƒn báº£n thay vÃ¬ xá»­ lÃ½ tá»«ng vÄƒn báº£n riÃªng láº») vÃ  nÃ³ pháº£i lÃ  má»™t trÃ¬nh láº·p náº¿u chÃºng ta muá»‘n trÃ¡nh cÃ³ má»i thá»© trong bá»™ nhá»› cÃ¹ng má»™t lÃºc. Náº¿u kho dá»¯ liá»‡u cá»§a báº¡n lá»›n, báº¡n sáº½ muá»‘n táº­n dá»¥ng lá»£i tháº¿ thá»±c tiá»…n lÃ  ğŸ¤— Datasets khÃ´ng táº£i má»i thá»© vÃ o RAM mÃ  lÆ°u trá»¯ cÃ¡c pháº§n tá»­ cá»§a táº­p dá»¯ liá»‡u trÃªn Ä‘Ä©a.

LÃ m nhÆ° sau sáº½ táº¡o má»™t danh sÃ¡ch cÃ¡c danh sÃ¡ch vá»›i má»—i danh sÃ¡ch gá»“m 1,000 vÄƒn báº£n, nhÆ°ng sáº½ táº£i má»i thá»© vÃ o bá»™ nhá»›:

```py
# Äá»«ng bá» ghi chÃº dÃ²ng bÃªn dÆ°á»›i trá»« khi táº­p dá»¯ liá»‡u cá»§a báº¡n nhá»!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

Sá»­ dá»¥ng trÃ¬nh táº¡o Python, chÃºng ta cÃ³ thá»ƒ trÃ¡nh viá»‡c Python táº£i báº¥t ká»³ thá»© gÃ¬ vÃ o bá»™ nhá»› cho Ä‘áº¿n khi nÃ³ thá»±c sá»± cáº§n thiáº¿t. Äá»ƒ táº¡o má»™t trÃ¬nh táº¡o nhÆ° váº­y, báº¡n chá»‰ cáº§n thay dáº¥u ngoáº·c vuÃ´ng báº±ng dáº¥u ngoáº·c Ä‘Æ¡n:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

DÃ²ng mÃ£ nÃ y khÃ´ng tÃ¬m náº¡p báº¥t ká»³ pháº§n tá»­ nÃ o cá»§a táº­p dá»¯ liá»‡u; nÃ³ chá»‰ táº¡o má»™t Ä‘á»‘i tÆ°á»£ng mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng trong vÃ²ng láº·p Python `for`. CÃ¡c vÄƒn báº£n sáº½ chá»‰ Ä‘Æ°á»£c táº£i khi báº¡n cáº§n (nghÄ©a lÃ  khi báº¡n Ä‘ang á»Ÿ bÆ°á»›c cá»§a vÃ²ng láº·p `for` mÃ  yÃªu cáº§u chÃºng) vÃ  chá»‰ 1,000 vÄƒn báº£n sáº½ Ä‘Æ°á»£c táº£i má»—i láº§n. Báº±ng cÃ¡ch nÃ y, báº¡n sáº½ khÃ´ng sá»­ dá»¥ng háº¿t bá»™ nhá»› cá»§a mÃ¬nh ngay cáº£ khi báº¡n Ä‘ang xá»­ lÃ½ má»™t táº­p dá»¯ liá»‡u lá»›n.

Váº¥n Ä‘á» vá»›i má»™t Ä‘á»‘i tÆ°á»£ng táº¡o lÃ  nÃ³ chá»‰ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng má»™t láº§n. VÃ¬ váº­y, thay vÃ¬ Ä‘iá»u nÃ y cho ta danh sÃ¡ch 10 chá»¯ sá»‘ Ä‘áº§u tiÃªn hai láº§n:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

chÃºng ta cÃ³ thá»ƒ láº¥y chÃºng trong má»™t láº§n vÃ  sau Ä‘Ã³ danh sÃ¡ng sáº½ trá»‘ng:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

ÄÃ³ lÃ  lÃ­ do chÃºng ta Ä‘á»‹nh nghÄ©a má»™t hÃ m thay vÃ o Ä‘Ã³ tráº£ vá» má»™t trÃ¬nh táº¡o:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

Ta cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a trÃ¬nh táº¡o bÃªn trong vÃ²ng láº·p `for` sá»­ dá»¥ng `yield`:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

sáº½ táº¡o ra trÃ¬nh táº¡o hoÃ n toÃ n giá»‘ng nhÆ° trÆ°á»›c Ä‘Ã¢y, nhÆ°ng cho phÃ©p báº¡n sá»­ dá»¥ng logic phá»©c táº¡p hÆ¡n báº¡n cÃ³ thá»ƒ trong má»™t bao hÃ m.

## Huáº¥n luyá»‡n má»™t tokenizer má»›i

BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ kho vÄƒn báº£n cá»§a mÃ¬nh dÆ°á»›i dáº¡ng má»™t trÃ¬nh láº·p cÃ¡c loáº¡t vÄƒn báº£n, chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n má»™t trÃ¬nh tokenize má»›i. Äá»ƒ thá»±c hiá»‡n viá»‡c nÃ y, trÆ°á»›c tiÃªn chÃºng ta cáº§n táº£i tokenizer mÃ  chÃºng ta muá»‘n ghÃ©p ná»‘i vá»›i mÃ´ hÃ¬nh cá»§a mÃ¬nh (á»Ÿ Ä‘Ã¢y, GPT-2):

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Máº·c dÃ¹ chÃºng ta sáº½ huáº¥n luyá»‡n má»™t tokenizer, nhÆ°ng báº¡n nÃªn lÃ m Ä‘iá»u nÃ y Ä‘á»ƒ trÃ¡nh báº¯t Ä‘áº§u hoÃ n toÃ n tá»« Ä‘áº§u. Báº±ng cÃ¡ch nÃ y, chÃºng ta sáº½ khÃ´ng pháº£i chá»‰ Ä‘á»‹nh báº¥t ká»³ Ä‘iá»u gÃ¬ vá» thuáº­t toÃ¡n tokenize hoáº·c cÃ¡c token Ä‘áº·c biá»‡t mÃ  ta muá»‘n sá»­ dá»¥ng; tokenizer má»›i sáº½ giá»‘ng há»‡t nhÆ° GPT-2 vÃ  Ä‘iá»u duy nháº¥t sáº½ thay Ä‘á»•i lÃ  tá»« vá»±ng, sáº½ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÃªn kho ngá»¯ liá»‡u cá»§a chÃºng tÃ´i.

Äáº§u tiÃªn, chÃºng ta hÃ£y xem cÃ¡ch mÃ  tokenizer nÃ y sáº½ xá»­ lÃ½ má»™t hÃ m máº«u tháº¿ nÃ o:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two',
 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '."', '""', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

Tokenizer nÃ y cÃ³ má»™t sá»‘ kÃ½ hiá»‡u Ä‘áº·c biá»‡t, nhÆ° `Ä ` vÃ  `ÄŠ`, tÆ°Æ¡ng á»©ng biá»ƒu thá»‹ dáº¥u cÃ¡ch vÃ  dÃ²ng má»›i. NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, Ä‘iá»u nÃ y khÃ´ng quÃ¡ hiá»‡u quáº£: tokenizer tráº£ vá» cÃ¡c mÃ£ thÃ´ng bÃ¡o riÃªng láº» cho tá»«ng khoáº£ng tráº¯ng, khi nÃ³ cÃ³ thá»ƒ nhÃ³m cÃ¡c má»©c thá»¥t lá» láº¡i vá»›i nhau (vÃ¬ cÃ³ bá»™ bá»‘n hoáº·c tÃ¡m dáº¥u cÃ¡ch sáº½ ráº¥t phá»• biáº¿n trong mÃ£). NÃ³ cÅ©ng tÃ¡ch tÃªn hÃ m hÆ¡i ká»³ láº¡, nhÃ¬n khÃ´ng quen cÃ¡c tá»« cÃ³ kÃ½ tá»± `_`.

HÃ£y huáº¥n luyá»‡n má»™t tokenizer má»›i vÃ  xem liá»‡u nÃ³ cÃ³ giáº£i quyáº¿t Ä‘Æ°á»£c nhá»¯ng váº¥n Ä‘á» Ä‘Ã³ khÃ´ng. Äá»‘i vá»›i Ä‘iá»u nÃ y, chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `train_new_from_iterator()`:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

Lá»‡nh nÃ y cÃ³ thá»ƒ máº¥t má»™t chÃºt thá»i gian náº¿u kho dá»¯ liá»‡u cá»§a báº¡n ráº¥t lá»›n, nhÆ°ng Ä‘á»‘i vá»›i táº­p dá»¯ liá»‡u 1.6GB vÄƒn báº£n nÃ y, nÃ³ ráº¥t nhanh (1 phÃºt 16 giÃ¢y trÃªn CPU AMD Ryzen 9 3900X vá»›i 12 lÃµi).

LÆ°u Ã½ ráº±ng `AutoTokenizer.train_new_from_iterator()` chá»‰ hoáº¡t Ä‘á»™ng náº¿u tokenizer báº¡n Ä‘ang sá»­ dá»¥ng lÃ  tokenizer "nhanh". NhÆ° báº¡n sáº½ tháº¥y trong pháº§n tiáº¿p theo, thÆ° viá»‡n ğŸ¤— Transformers chá»©a hai loáº¡i tokenizers: má»™t sá»‘ Ä‘Æ°á»£c viáº¿t hoÃ n toÃ n báº±ng Python vÃ  nhá»¯ng loáº¡i khÃ¡c (loáº¡i nhanh) Ä‘Æ°á»£c há»— trá»£ bá»Ÿi thÆ° viá»‡n ğŸ¤— Tokenizers, Ä‘Æ°á»£c viáº¿t báº±ng ngÃ´n ngá»¯ láº­p trÃ¬nh [Rust](https://www.rust-lang.org). Python lÃ  ngÃ´n ngá»¯ thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng nháº¥t cho cÃ¡c á»©ng dá»¥ng khoa há»c dá»¯ liá»‡u vÃ  há»c sÃ¢u, nhÆ°ng khi báº¥t ká»³ thá»© gÃ¬ cáº§n Ä‘Æ°á»£c song song hÃ³a cho nhanh, nÃ³ pháº£i Ä‘Æ°á»£c viáº¿t báº±ng má»™t ngÃ´n ngá»¯ khÃ¡c. VÃ­ dá»¥, cÃ¡c phÃ©p nhÃ¢n ma tráº­n lÃ  cá»‘t lÃµi cá»§a tÃ­nh toÃ¡n mÃ´ hÃ¬nh Ä‘Æ°á»£c viáº¿t báº±ng CUDA, má»™t thÆ° viá»‡n C Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho GPU.

Viá»‡c huáº¥n luyá»‡n má»™t tokenizer hoÃ n toÃ n má»›i báº±ng Python thuáº§n tÃºy sáº½ ráº¥t cháº­m, Ä‘Ã³ lÃ  lÃ½ do táº¡i sao chÃºng tÃ´i Ä‘Ã£ phÃ¡t triá»ƒn thÆ° viá»‡n ğŸ¤— Tokenizer. LÆ°u Ã½ ráº±ng cÅ©ng giá»‘ng nhÆ° báº¡n khÃ´ng pháº£i há»c ngÃ´n ngá»¯ CUDA Ä‘á»ƒ cÃ³ thá»ƒ thá»±c thi mÃ´ hÃ¬nh cá»§a mÃ¬nh trÃªn má»™t loáº¡t Ä‘áº§u vÃ o trÃªn GPU, báº¡n sáº½ khÃ´ng cáº§n pháº£i há»c Rust Ä‘á»ƒ sá»­ dá»¥ng trÃ¬nh tokenizer nhanh. ThÆ° viá»‡n ğŸ¤— Tokenizers cung cáº¥p cÃ¡c liÃªn káº¿t Python cho nhiá»u phÆ°Æ¡ng thá»©c gá»i ná»™i bá»™ má»™t sá»‘ Ä‘oáº¡n mÃ£ trong Rust; vÃ­ dá»¥: Ä‘á»ƒ song song huáº¥n luyá»‡n trÃ¬nh tokenize má»›i cá»§a báº¡n hoáº·c, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 3](/course/chapter3), tokenize má»™t loáº¡t Ä‘áº§u vÃ o.

Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘á»u cÃ³ sáºµn cÃ´ng cá»¥ tokenize nhanh (cÃ³ má»™t sá»‘ ngoáº¡i lá»‡ mÃ  báº¡n cÃ³ thá»ƒ kiá»ƒm tra [táº¡i Ä‘Ã¢y](https://huggingface.co/transformers/#supported-frameworks)) vÃ  API `AutoTokenizer` luÃ´n chá»n tá»‘c tokenizer nhanh cho báº¡n náº¿u nÃ³ cÃ³ sáºµn. Trong pháº§n tiáº¿p theo, chÃºng ta sáº½ xem xÃ©t má»™t sá»‘ tÃ­nh nÄƒng Ä‘áº·c biá»‡t khÃ¡c mÃ  cÃ¡c tokenize nhanh cÃ³ mÃ  thá»±c sá»± há»¯u Ã­ch cho cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n loáº¡i token vÃ  há»i Ä‘Ã¡p. Tuy nhiÃªn, trÆ°á»›c khi Ä‘i sÃ¢u vÃ o váº¥n Ä‘á» Ä‘Ã³, chÃºng ta hÃ£y thá»­ tokenizer hoÃ n toÃ n má»›i cá»§a chÃºng ta trÃªn máº«u trÆ°á»›c:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `',
 'a', '`', 'Ä and', 'Ä `', 'b', '`."""', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

á» Ä‘Ã¢y chÃºng ta láº¡i tháº¥y cÃ¡c kÃ½ hiá»‡u Ä‘áº·c biá»‡t `Ä ` vÃ  `ÄŠ` biá»ƒu thá»‹ dáº¥u cÃ¡ch vÃ  dÃ²ng má»›i, nhÆ°ng chÃºng ta cÅ©ng cÃ³ thá»ƒ tháº¥y ráº±ng trÃ¬nh tokenize Ä‘Ã£ há»c Ä‘Æ°á»£c má»™t sá»‘ token ráº¥t cá»¥ thá»ƒ cho má»™t kho cÃ¡c hÃ m Python: vÃ­ dá»¥: cÃ³ má»™t token `ÄŠÄ Ä Ä ` Ä‘áº¡i diá»‡n cho má»™t thá»¥t lá» vÃ  token `Ä """` Ä‘áº¡i diá»‡n cho ba dáº¥u ngoáº·c kÃ©p báº¯t Ä‘áº§u má»™t chuá»—i tÃ i liá»‡u. Tokenizer cÅ©ng phÃ¢n chia chÃ­nh xÃ¡c tÃªn hÃ m trÃªn `_`. ÄÃ¢y lÃ  má»™t biá»…u diá»…n khÃ¡ nhá» gá»n; tÆ°Æ¡ng Ä‘á»‘i, sá»­ dá»¥ng tokenizer Ä‘Æ¡n giáº£n báº±ng tiáº¿ng Anh trÃªn cÃ¹ng má»™t máº«u sáº½ cho ta má»™t cÃ¢u dÃ i hÆ¡n:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

HÃ£y cÃ¹ng nhÃ¬n vÃ o vÃ­ dá»¥ sau:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',',
 'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',
 'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']
```

NgoÃ i token tÆ°Æ¡ng á»©ng vá»›i thá»¥t lá», á»Ÿ Ä‘Ã¢y chÃºng ta cÅ©ng cÃ³ thá»ƒ tháº¥y token cho thá»¥t lá» kÃ©p:`ÄŠÄ Ä Ä Ä Ä Ä Ä `. CÃ¡c tá»« Ä‘áº·c biá»‡t trong Python nhÆ° `class`, `init`, `call`, `self`, vÃ  `return`, má»—i tá»« Ä‘Æ°á»£c tokenize thÃ nh má»™t token vÃ  chÃºng ta cÃ³ thá»ƒ tháº¥y cÅ©ng nhÆ° tÃ¡ch `_`  vÃ  `.`,  tokenizer phÃ¢n chia chÃ­nh xÃ¡c cÃ¡c tÃªn: `LinearLayer` Ä‘Æ°á»£c tokenize lÃ  `["Ä Linear", "Layer"]`.

## LÆ°u tokenizer

Äá»ƒ Ä‘áº£m báº£o ráº±ng chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ sau nÃ y, chÃºng ta cáº§n pháº£i lÆ°u tokenizer má»›i cá»§a mÃ¬nh. Giá»‘ng nhÆ° Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh, Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i phÆ°Æ¡ng thá»©c `save_pretrained()`:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

Thao tÃ¡c nÃ y sáº½ táº¡o má»™t thÆ° má»¥c má»›i cÃ³ tÃªn *code-search-net-tokenizer*, sáº½ chá»©a táº¥t cáº£ cÃ¡c tá»‡p mÃ  tokenizer cáº§n Ä‘Æ°á»£c táº£i láº¡i. Náº¿u báº¡n muá»‘n chia sáº» tokenizer nÃ y vá»›i Ä‘á»“ng nghiá»‡p vÃ  báº¡n bÃ¨ cá»§a mÃ¬nh, báº¡n cÃ³ thá»ƒ táº£i nÃ³ lÃªn Hub báº±ng cÃ¡ch Ä‘Äƒng nháº­p vÃ o tÃ i khoáº£n cá»§a mÃ¬nh. Náº¿u báº¡n Ä‘ang lÃ m viá»‡c trÃªn notebook, cÃ³ má»™t hÃ m tiá»‡n Ã­ch giÃºp báº¡n lÃ m Ä‘iá»u nÃ y:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Thao tÃ¡c nÃ y sáº½ hiá»ƒn thá»‹ má»™t tiá»‡n Ã­ch mÃ  báº¡n cÃ³ thá»ƒ nháº­p thÃ´ng tin Ä‘Äƒng nháº­p Hugging Face cá»§a mÃ¬nh. Náº¿u báº¡n khÃ´ng lÃ m viá»‡c trong notebook, chá»‰ cáº§n nháº­p dÃ²ng sau vÃ o thiáº¿t bá»‹ Ä‘áº§u cuá»‘i cá»§a báº¡n:

```bash
huggingface-cli login
```

Khi báº¡n Ä‘Ã£ Ä‘Äƒng nháº­p, báº¡n cÃ³ thá»ƒ Ä‘áº©y tokenizer cá»§a mÃ¬nh báº±ng cÃ¡ch thá»±c hiá»‡n lá»‡nh sau:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

Thao tÃ¡c nÃ y sáº½ táº¡o má»™t kho lÆ°u trá»¯ má»›i trong khÃ´ng gian tÃªn cá»§a báº¡n vá»›i tÃªn `code-search-net-tokenizer`, chá»©a tá»‡p tokenizer. Sau Ä‘Ã³ báº¡n cÃ³ thá»ƒ táº£i tokenizer tá»« báº¥t kÃ¬ Ä‘Ã¢u vá»›i phÆ°Æ¡ng thá»©c `from_pretrained()`:

```py
# Thay "huggingface-course" dÆ°á»›i Ä‘áº¥y vá»›i tÃªn khÃ´ng gian thá»±c sá»± sá»­ dá»¥ng tokenizer riÃªng cá»§a báº¡n
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

Giá» báº¡n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»« Ä‘áº§u vÃ  viá»‡c tinh chá»‰nh nÃ³ trong táº§m tay cá»§a báº¡n! ChÃºng ta sáº½ tÃ¬m hiá»ƒu Ä‘iá»u Ä‘Ã³ trong [ChÆ°Æ¡ng 7](/course/chap7), nhÆ°ng trÆ°á»›c tiÃªn, trong pháº§n cÃ²n láº¡i cá»§a chÆ°Æ¡ng nÃ y, chÃºng ta sáº½ xem xÃ©t ká»¹ hÆ¡n vá» cÃ¡c trÃ¬nh tokenize nhanh vÃ  khÃ¡m phÃ¡ chi tiáº¿t nhá»¯ng gÃ¬ thá»±c sá»± xáº£y ra khi chÃºng ta gá»i phÆ°Æ¡ng thá»©c `train_new_from_iterator()`.
