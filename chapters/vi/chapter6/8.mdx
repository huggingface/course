# XÃ¢y dá»±ng tá»«ng khá»‘i tokenizer

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section8.ipynb"},
]} />

NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong cÃ¡c pháº§n trÆ°á»›c, tokenize bao gá»“m má»™t sá»‘ bÆ°á»›c:

- Chuáº©n hÃ³a (má»i thao tÃ¡c dá»n dáº¹p vÄƒn báº£n Ä‘Æ°á»£c cho lÃ  cáº§n thiáº¿t, cháº³ng háº¡n nhÆ° xÃ³a dáº¥u cÃ¡ch hoáº·c dáº¥u, chuáº©n hÃ³a Unicode, v.v.)
- Tiá»n tokenize (chia nhá» Ä‘áº§u vÃ o thÃ nh cÃ¡c tá»«)
- ÄÆ°a Ä‘áº§u vÃ o thÃ´ng qua mÃ´ hÃ¬nh (sá»­ dá»¥ng cÃ¡c tá»« Ä‘Æ°á»£c tiá»n tokenize Ä‘á»ƒ táº¡o ra má»™t chuá»—i token)
- Háº­u xá»­ lÃ½ (thÃªm token Ä‘áº·c biá»‡t cá»§a trÃ¬nh tokenize, táº¡o attention mask vÃ  ID token)

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

ThÆ° viá»‡n ğŸ¤— Tokenizers Ä‘Ã£ Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ cung cáº¥p nhiá»u sá»± lá»±a chá»n cho cÃ¡c bÆ°á»›c nÃ y, vÃ  ta cÃ³ thá»ƒ káº¿t há»£p vÃ  ná»‘i chÃºng vá»›i nhau. Trong pháº§n nÃ y, chÃºng ta sáº½ xem cÃ¡c cÃ³ thá»ƒ xÃ¢y má»™t tokenizer tá»« Ä‘áº§u, trÃ¡i ngÆ°á»£c vá»›i cÃ¡ch huáº¥n luyá»‡n má»™t tokenizer má»›i tá»« cÃ¡i cÅ© nhÆ° ta Ä‘Ã£ lÃ m á»Ÿ [pháº§n 2](/course/chapter6/2). ChÃºng ta sáº½ cÃ³ thá»ƒ xÃ¢y báº¥t kÃ¬ kiá»ƒu tokenizer nÃ o ta cÃ³ thá»ƒ nghÄ© ra!

<Youtube id="MR8tZm5ViWU"/>

ChÃ­nh xÃ¡c hÆ¡n, thÆ° viá»‡n Ä‘Æ°á»£c xÃ¢y dá»±ng táº­p trung vÃ o lá»›p `Tokenizer` vá»›i cÃ¡c khá»‘i Ä‘Æ°á»£c táº­p há»£p láº¡i trong cÃ¡c mÃ´-Ä‘un con:

- `normalizers` chá»©a táº¥t cáº£ cÃ¡c kiá»ƒu `Normalizer` báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng (hoÃ n thiá»‡n danh sÃ¡ch táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/api/normalizers)).
- `pre_tokenizers` chá»©a táº¥t cáº£ cÃ¡c kiá»ƒu `PreTokenizer` báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng (hoÃ n thiá»‡n danh sÃ¡ch táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)).
- `models` chá»©a táº¥t cáº£ cÃ¡c kiá»ƒu `Model` báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng, nhÆ° `BPE`, `WordPiece`, and `Unigram` (hoÃ n thiá»‡n danh sÃ¡ch táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/api/models)).
- `trainers` chá»©a táº¥t cáº£ cÃ¡c kiá»ƒu `Trainer` khÃ¡c nhau báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»§a báº¡n trÃªn kho ngá»¯ liá»‡u (má»™t cho má»—i loáº¡i mÃ´ hÃ¬nh; hoÃ n thiá»‡n danh sÃ¡ch táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/api/trainers)).
- `post_processors` chá»©a táº¥t cáº£ cÃ¡c kiá»ƒu `PostProcessor` báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng (hoÃ n thiá»‡n danh sÃ¡ch táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/api/post-processors)).
- `decoders` chá»©a táº¥t cáº£ cÃ¡c kiá»ƒu `Decoder` Ä‘a dáº¡ng báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ giáº£i mÃ£ Ä‘áº§u ra cá»§a tokenize (hoÃ n thiá»‡n danh sÃ¡ch táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/components#decoders)).

Báº¡n cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c toÃ n bá»™ danh sÃ¡ch cÃ¡c khá»‘i táº¡i [Ä‘Ã¢y](https://huggingface.co/docs/tokenizers/components).

## Thu tháº­p má»™t kho ngá»¯ liá»‡u

Äá»ƒ huáº¥n luyá»‡n tokenizer má»›i cá»§a mÃ¬nh, chÃºng ta sáº½ sá»­ dá»¥ng má»™t kho ngá»¯ liá»‡u nhá» chá»©a cÃ¡c Ä‘oáº¡n vÄƒn (cho nhanh). CÃ¡c bÆ°á»›c Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c kho ngá»¯ liá»‡u tÆ°Æ¡ng tá»± nhÆ° chÃºng ta Ä‘Ã£ lÃ m á»Ÿ [pháº§n Ä‘áº§u cá»§a chÆ°Æ¡ng nÃ y](/course/chapter6/2), nhÆ°ng láº§n nÃ y chÃºng ta sáº½ sá»­ dá»¥ng [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

HÃ m `get_training_corpus()` lÃ  má»™t hÃ m táº¡o cÃ³ thá»ƒ tráº£ vá» cÃ¡c lÃ´ vá»›i má»—i lÃ´ lÃ  1,000 Ä‘oáº¡n vÄƒn, cÃ¡i mÃ  ta sáº½ sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n  tokenizer.

ğŸ¤— Tokenizers cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n trá»±c tiáº¿p trÃªn cÃ¡c tá»‡p vÄƒn báº£n. ÄÃ¢y lÃ  cÃ¡ch chÃºng ta táº¡o ra má»™t tá»‡p vÄƒn báº£n bao gá»“m cÃ¡c Ä‘oáº¡n vÄƒn/Ä‘áº§u vÃ o tá»« WikiText-2 mÃ  ta cÃ³ thá»ƒ sá»­ dá»¥ng cá»¥c bá»™:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

Tiáº¿p theo chÃºng tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n cÃ¡ch tá»± xÃ¢y dá»±ng tá»«ng khá»‘i BERT, GPT-2, vÃ  XLNet tokenizer cá»§a riÃªng mÃ¬nh. Äiá»u nÃ y sáº½ cung cáº¥p cho chÃºng ta má»™t vÃ­ dá»¥ vá» tá»«ng thuáº­t toÃ¡n trong sá»‘ ba thuáº­t toÃ¡n tokenize chÃ­nh: WordPiece, BPE, vÃ  Unigram. HÃ£y cÅ©ng báº¯t Ä‘áº§u vá»›i BERT!

## XÃ¢y dá»±ng má»™t WordPiece tokenizer tá»« Ä‘áº§u

Äá»ƒ xÃ¢y dá»±ng má»™t tokenizer vá»›i thÆ° viá»‡n ğŸ¤— Tokenizers, chÃºng ta sáº½ báº¯t Ä‘áº§u vá»›i viá»‡c khá»Ÿi táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `Tokenizer` vá»›i `model`, sau Ä‘Ã³ thiáº¿t láº­p `normalizer`, `pre_tokenizer`, `post_processor`, vÃ  `decoder` tá»›i cÃ¡c giÃ¡ trá»‹ ta muá»‘n.

Vá»›i vÃ­ dá»¥ nÃ y, ta sáº½ táº¡o ra má»™t `Tokenizer` vá»›i má»™t mÃ´ hÃ¬nh WordPiece:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

ChÃºng ta pháº£i chá»‰ rÃµ `unk_token` Ä‘á»ƒ mÃ´ hÃ¬nh biáº¿t pháº£i tráº£ vá» gÃ¬ khi gáº·p cÃ¡c kÃ­ tá»± chÆ°a tá»«ng gáº·p trÆ°á»›c Ä‘Ã³. CÃ¡c tham sá»‘ khÃ¡c chÃºng ta cÃ³ thá»ƒ cÃ i Ä‘áº·t gá»“m `vocab` cá»§a mÃ´ hÃ¬nh (ta sáº½ huáº¥n luyá»‡n mÃ´ hÃ¬nh nÃªn khÃ´ng cáº§n thiáº¿t láº­p nÃ³) vÃ  `max_input_chars_per_word`, tÆ°Æ¡ng á»©ng Ä‘á»™ dÃ i tá»‘i Ä‘a cho má»™t tá»« (tá»« dÃ i hÆ¡n giÃ¡ trá»‹ nÃ y sáº½ bá»‹ chia nhá»)

BÆ°á»›c Ä‘áº§u tiÃªn Ä‘á»ƒ tokenize Ä‘Ã³ lÃ  chuáº©n hoÃ¡, vÃ¬ váº­y hÃ£y cÅ©ng báº¯t Ä‘áº§u vá»›i bÆ°á»›c nÃ y. VÃ¬ BERT Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng tÃ£i, ta cÃ³ thá»ƒ sá»­ dá»¥ng `BertNormalizer` vá»›i tuá»³ chá»n kinh Ä‘iá»ƒn Ä‘á»ƒ thiáº¿t láº­p cho BERT: `lowercase` vÃ  `strip_accents`, tá»± cÃ¡i tÃªn Ä‘Ã£ giáº£i thÃ­ch má»¥c Ä‘Ã­ch cá»§a chÃºng; `clean_text` Ä‘á»ƒ loáº¡i bá» táº¥t cáº£ cÃ¡c kÃ­ tá»± kiá»ƒm soÃ¡t vÃ  dáº¥u cÃ¡ch láº·p láº¡i thÃ nh má»™t; vÃ  `handle_chinese_chars` thÃªm dáº¥u cÃ¡ch giá»¯a cÃ¡c kÃ­ tá»± tiáº¿ng Trung. Äá»ƒ , which places spaces around Chinese characters. To tÃ¡i táº¡o tokenizer `bert-base-uncased`, ta cÃ³ thá»ƒ thiáº¿t láº­p chuáº©n hoÃ¡ sau:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ThÃ´ng thÆ°á»ng, khi xÃ¢y dá»±ng má»™t tokenizer, báº¡n khÃ´ng cáº§n pháº£i truy cáº­p vÃ o má»™t hÃ m chuáº©n hoÃ¡ thá»§ cÃ´ng vÃ¬ nÃ³ Ä‘Ã£ cÃ³ sáºµn trong thÆ° viá»‡n ğŸ¤— Tokenizers library -- tuy nhiÃªn, hÃ£y cÃ¹ng táº¡o ra chuáº©n hoÃ¡ BERT thá»§ cÃ´ng. ThÆ° viá»‡n cung cÃ¢p trÃ¬nh chuáº©n hoÃ¡ `Lowercase` vÃ  `StripAccents`, báº¡n hoÃ n toÃ n cÃ³ thá»ƒ káº¿t há»£p nhiá»u trÃ¬nh chuáº©n hoÃ¡ vá»›i nhau thÃ´ng qua `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

Ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng chuáº©n hoÃ¡ Unicode `NFD` Unicode normalizer, vÃ¬ náº¿u khÃ´ng chuáº©n hoÃ¡ `StripAccents` sáº½ khÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c nhá»¯ng kÃ­ tá»± cÃ³ dáº¥u vÃ  khÃ´ng thá»ƒ tÃ¡ch nÃ³ Ä‘Ãºng nhÆ° ta muá»‘n.

NhÆ° Ä‘Ã£ tháº¥y á»Ÿ trÃªn, ta cÃ³ thá»ƒ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `normalize_str()` cá»§a `normalizer` Ä‘á»ƒ kiá»ƒm tra tÃ¡c Ä‘á»™ng cá»§a nÃ³ lÃªn má»™t chuá»—i vÄƒn báº£n:

```python
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
hello how are u?
```

<Tip>

**ÄÃ o sÃ¢u hÆ¡n** Náº¿u báº¡n muá»‘n kiá»ƒm tra xem hai phiÃªn báº£n chuáº©n hoÃ¡ trÆ°á»›c Ä‘Ã³ trÃªn cÅ©ng má»™t chuá»—i chá»©a kÃ­ tá»± unicode `u"\u0085"`, báº¡n cháº¯c cháº¯n sáº½ nháº­n tháº¥y ráº±ng hai cÃ¡ch chuáº©n hoÃ¡ nÃ y khÃ´ng hoÃ n toÃ n giá»‘ng nhau.
Äá»ƒ trÃ¡nh phá»©c táº¡p hoÃ¡ phiÃªn báº£n vá»›i `normalizers.Sequence` quÃ¡ nhiá»u, chÃºng tÃ´i sáº½ khÃ´ng bao gá»“m cÃ¡c sá»± thay tháº¿ theo Regex mÃ  `BertNormalizer` yÃªu cáº§u khi tham sá»‘ `clean_text` Ä‘Æ°á»£c thiáº¿t láº­p lÃ  `True` - Ä‘Ã¢y cÅ©ng lÃ  giÃ¡ trá»‹ máº·c Ä‘á»‹nh. NhÆ°ng Ä‘á»«ng lo: cÃ³ kháº£ nÄƒng ta sáº½ nháº­n Ä‘Æ°á»£c káº¿t quáº£ chuáº©n hoÃ¡ giá»‘ng nhau mÃ  khÃ´ng cáº§n sá»­ dá»¥ng `BertNormalizer` thá»§ cÃ´ng báº±ng cÃ¡ch thÃªm hai `normalizers.Replace` vÃ o chuá»—i chuáº©n hoÃ¡.

</Tip>

Tiáº¿p theo lÃ  bÆ°á»›c pre-tokenization. Má»™t láº§n ná»¯a, ta cÃ³ `BertPreTokenizer` Ä‘Æ°á»£c xÃ¢y dá»±ng sáºµn Ä‘á»ƒ dÃ¹ng:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

Hoáº·c ta cÃ³ thá»ƒ xÃ¢y tá»« Ä‘áº§u:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

LÆ°u Ã½ ráº±ng `Whitespace` sáº½ tÃ¡ch theo dáº¥u cÃ¡ch vÃ  cÃ¡c kÃ­ tá»± khÃ´ng pháº£i chá»¯ cÃ¡i, sá»‘, hoáº·c dáº¥u gáº¡ch dÆ°á»›i, nÃªn vá» máº·t ká»¹ thuáº­t nÃ³ sáº½ tÃ¡ch theo dáº¥u cÃ¡ch vÃ  dáº¥u cÃ¢u:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

NÃªu sbanj chá»‰ muá»‘n tÃ¡ch theo dáº¥u cÃ¡ch, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng `WhitespaceSplit` thay tháº¿:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

Giá»‘ng nhÆ° chuáº©n hoÃ¡, báº£n cÃ³ thá»ƒ sá»­ dá»¥ng `Sequence` Ä‘á»ƒ káº¿t há»£p cÃ¡c tiá»n tokenizer vá»›i nhau:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

BÆ°á»›c tiáº¿p theo trong pipeline tokenize lÃ  Ä‘Æ°a Ä‘áº§u vÃ o qua mÃ´ hÃ¬nh. Ta Ä‘Ã£ chá»‰ Ä‘á»‹nh mÃ´ hÃ¬nh cá»§a mÃ¬nh khi khá»Ÿi táº¡o, nhÆ°ng ta váº«n cáº§n huáº¥n luyá»‡n nÃ³, Ä‘iá»u nÃ y cáº§n tá»›i `WordPieceTrainer`.  Váº¥n Ä‘á» chÃ­nh á»Ÿ Ä‘Ã¢y lÃ  khi khá»Ÿi táº¡o má»™t trÃ¬nh huáº¥n luyá»‡n trong ğŸ¤— Tokenizers thÃ¬ báº¡n cáº§n pháº£i truyá»n táº¥t cáº£ cÃ¡c token Ä‘áº·c biá»‡t báº¡n cÃ³ Ã½ Ä‘á»‹nh sá»­ dá»¥ng, náº¿u khÃ´ng nÃ³ sáº½ khÃ´ng thÃªm vÃ o bá»™ tá»« vá»±ng, vÃ¬ chÃºng khÃ´ng cÃ³ trong kho ngá»¯ liá»‡u huáº¥n luyá»‡n:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

CÅ©ng nhÆ° viá»‡c chá»‰ Ä‘á»‹nh `vocab_size` vÃ  `special_tokens`, ta cáº§n thiáº¿t láº­p `min_frequency` (sá»‘ láº§n má»™t token pháº£i xuáº¥t hiá»‡n Ä‘á»ƒ Ä‘Æ°á»£c thÃªm vÃ o bá»™ tá»« vá»±ng) hoáº·c thay Ä‘á»•i `continuing_subword_prefix` (náº¿u ta muá»‘n sá»­ dá»¥ng thá»© gÃ¬ khÃ¡c ngoÃ i `##`).

Äá»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh sá»­ dá»¥ng trÃ¬nh láº·p ta Ä‘á»‹nh nghÄ©a trÆ°á»›c Ä‘Ã³, ta chá»‰ cáº§n thá»±c hiá»‡n lá»‡nh nÃ y:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ChÃºng ta cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c tá»‡p vÄƒn báº£n Ä‘á»ƒ huáº¥n luyá»‡n tokenizer cá»§a mÃ¬nh nhÆ° sau (ta tÃ¡i khá»Ÿi táº¡o mÃ´ hÃ¬nh vá»›i má»™t `WordPiece` rá»—ng):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

Trong cáº£ hai trÆ°á»ng há»£p, ta cÃ³ thá»ƒ kiá»ƒm tra xem tokenizer trÃªn má»™t Ä‘oáº¡n vÄƒn báº£n báº±ng cÃ¡ch sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `encode()`:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

`encoding` thu Ä‘Æ°á»£c lÃ  má»™t `Encoding` gá»“m táº¥t cáº£ cÃ¡c Ä‘áº§u ra cáº§n thiáº¿t cá»§a má»™t tokenizer trong táº¥t cáº£ cÃ¡c thÃ´ng sá»‘ Ä‘a dáº¡ng cá»§a nÃ³: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, vÃ  `overflowing`.

BÆ°á»›c cuá»‘i cá»§a quy trÃ¬nh Ä‘Ã³ lÃ  háº­u xá»­ lÃ½. Ta cáº§n thÃªm token `[CLS]` token at the beginning and the `[SEP]` á»Ÿ cuá»‘i (hoáº·c sau má»—i cÃ¢u, náº¿u ta cÃ³ cáº·p cÃ¢u). ChÃºng ta sáº½ sá»­ dá»¥ng `TemplateProcessor` Ä‘á»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y, nhÆ°ng trÆ°á»›c háº¿t ta cáº§n biáº¿t cÃ¡c ID cá»§a token `[CLS]` vÃ  `[SEP]` trong bá»™ tá»« vá»±ng.

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

Äá»ƒ viáº¿t báº£n máº«u cho `TemplateProcessor`, chÃºng ta pháº£i chá»‰ Ä‘á»‹nh cÃ¡ch xá»­ lÃ½ má»™t cÃ¢u Ä‘Æ¡n vÃ  má»™t cáº·p cÃ¢u. Äá»‘i vá»›i cáº£ hai, chÃºng tÃ´i viáº¿t cÃ¡c token Ä‘áº·c biá»‡t muá»‘n sá»­ dá»¥ng; cÃ¢u Ä‘áº§u tiÃªn (hoáº·c cÃ¢u Ä‘Æ¡n) Ä‘Æ°á»£c biá»ƒu thá»‹ báº±ng `$A`, trong khi cÃ¢u thá»© hai (náº¿u token má»™t cáº·p) Ä‘Æ°á»£c biá»ƒu thá»‹ báº±ng `$B`. Äá»‘i vá»›i má»—i loáº¡i trong sá»‘ nÃ y (token vÃ  cÃ¢u Ä‘áº·c biá»‡t), chÃºng ta cÅ©ng chá»‰ Ä‘á»‹nh loáº¡i token ID tÆ°Æ¡ng á»©ng sau dáº¥u hai cháº¥m.

Do Ä‘Ã³, báº£n máº«u BERT cá»• Ä‘iá»ƒn Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

LÆ°u Ã½ ráº±ng chÃºng ta cáº§n truyá»n vÃ o táº¥t cáº£ cÃ¡c IDs cá»§a cÃ¡c kÃ­ tá»± Ä‘áº·c biá»‡t, nÃªn cÃ¡c tokenize cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i chÃºng thÃ nh cÃ¡c cáº·p ID.

Má»™t khi Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o, chÃºng ta cÃ³ thá»ƒ quay láº¡i vÃ­ dá»¥ trÆ°á»›c Ä‘Ã³ vÃ  sáº½ nháº­n Ä‘Æ°á»£c:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

VÃ  trÃªn má»™t cáº·p cÃ¢u, chÃºng ta cÃ³ thá»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ sau:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

ChÃºng ta Ä‘Ã£ gáº§n nhÆ° hoÃ n thÃ nh viá»‡c xÃ¢y dá»±ng tokenizer nÃ y tá»« Ä‘áº§u -- bÆ°á»›c cuá»‘i cÃ¹ng lÃ  thÃªm vÃ o má»™t trÃ¬nh giáº£i mÃ£:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

HÃ£y cÅ©ng kiá»ƒm thá»­ vá»›i `encoding`:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

Tuyá»‡t vá»i! Ta cÃ³ thá»ƒ lÆ°u tokenizer cá»§a mÃ¬nh vÃ o trong má»™t tá»‡p JSON nhÆ° dÆ°á»›i Ä‘Ã¢y:

```python
tokenizer.save("tokenizer.json")
```

Ta sau Ä‘Ã³ cÃ³ thá»ƒ táº£i láº¡i tá»‡p nÃ y trong Ä‘á»‘i tÆ°á»£ng `Tokenizer` vá»›i phÆ°Æ¡ng thá»©c `from_file()`:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

Äá»ƒ sá»­ dá»¥ng tokenizer nÃ y trong ğŸ¤— Transformers, chÃºng ta pháº£i bá»c nÃ³ trong `PreTrainedTokenizerFast`. ChÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng lá»›p chung hoáº·c, náº¿u tokenizer cá»§a chÃºng ta tÆ°Æ¡ng á»©ng vá»›i má»™t mÃ´ hÃ¬nh hiá»‡n cÃ³, hÃ£y sá»­ dá»¥ng lá»›p Ä‘Ã³ (á»Ÿ Ä‘Ã¢y lÃ  `BertTokenizerFast`). Náº¿u báº¡n Ã¡p dá»¥ng bÃ i há»c nÃ y Ä‘á»ƒ xÃ¢y dá»±ng má»™t tokenizer hoÃ n toÃ n má»›i, báº¡n sáº½ pháº£i sá»­ dá»¥ng tÃ¹y chá»n Ä‘áº§u tiÃªn.

Äá»ƒ bá»c tokenizer trong má»™t `PreTrainedTokenizerFast`, chÃºng ta cÃ³ thá»ƒ chuyá»ƒn tokenizer mÃ  chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng dÆ°á»›i dáº¡ng `tokenizer_object` hoáº·c truyá»n tá»‡p tokenizer mÃ  chÃºng ta Ä‘Ã£ lÆ°u dÆ°á»›i dáº¡ng `tokenizer_file`. Äiá»u quan trá»ng cáº§n nhá»› lÃ  chÃºng ta pháº£i Ä‘áº·t thá»§ cÃ´ng táº¥t cáº£ cÃ¡c token Ä‘áº·c biá»‡t, vÃ¬ lá»›p Ä‘Ã³ khÃ´ng thá»ƒ suy ra tá»« Ä‘á»‘i tÆ°á»£ng `tokenizer` token nÃ o lÃ  token bá»‹ che, `[CLS]`, v.v.:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # Báº¡n cÃ³ thá»ƒ táº£i tá»« tá»‡p tokenizer
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

Náº¿u báº¡n Ä‘ang sá»± dá»¥ng má»™t lá»›p tokenizer Ä‘áº·c biá»‡t (nhÆ° `BertTokenizerFast`), báº¡n chá»‰ cáº§n chá»‰ Ä‘á»‹nh má»™t token Ä‘áº·c biáº¿t khÃ¡c so vá»›i máº·c Ä‘á»‹nh (á»Ÿ Ä‘Ã¢y lÃ  khÃ´ng xÃ¡c Ä‘á»‹nh):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tokenizer nhÆ° báº¥t ká»³ tokenizer nÃ o khÃ¡c cá»§a ğŸ¤— Transformers. Báº¡n cÃ³ thá»ƒ lÆ°u nÃ³ vá»›i phÆ°Æ¡ng thá»©c `save_pretrained()`, hoáº·c láº¡i nÃ³ lÃªn Hub sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `push_to_hub()`.

Giá» chÃºng ta Ä‘Ã£ tháº¥y cÃ¡ch xÃ¢y dá»±ng bá»™ WordPiece tokenizer, hÃ£y lÃ m tÆ°Æ¡ng tá»± Ä‘á»‘i vá»›i BPE tokenizer. ChÃºng ta sáº½ tiáº¿n hÃ nh nhanh hÆ¡n má»™t chÃºt vÃ¬ báº¡n Ä‘Ã£ biáº¿t táº¥t cáº£ cÃ¡c bÆ°á»›c vÃ  chá»‰ lÃ m ná»•i báº­t nhá»¯ng Ä‘iá»ƒm khÃ¡c biá»‡t.

## XÃ¢y dá»±ng má»™t BPE tokenizer tá»« Ä‘áº§u

Giá» hÃ£y cÅ©ng nhau xÃ¢y dá»±ng GPT-2 tokenizer. Giá»‘ng nhÆ° BERT tokenizer, chÃºng ta báº¯t Ä‘áº§u báº±ng viá»‡c khá»Ÿi táº¡o `Tokenizer` vá»›i mÃ´ hÃ¬nh BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

CÅ©ng giá»‘ng nhÆ° BERT, chÃºng ta cÃ³ thá»ƒ khá»Ÿi táº¡o mÃ´ hÃ¬nh nÃ y vá»›i má»™t bá»™ tá»« vá»±ng náº¿u ta Ä‘Ã£ cÃ³ (ta sáº½ cáº§n truyá»n vÃ o `vocab` vÃ  `merges` trong trÆ°á»ng há»£p nÃ y), nhÆ°ng vÃ¬ ta sáº½ huáº¥n luyá»‡n tá»« Ä‘áº§u, chÃºng ta khÃ´ng cáº§n lÃ m váº­y. Ta cÅ©ng khÃ´ng cáº§n chá»‰ Ä‘á»‹nh `unk_token` vÃ¬ GPT-2 sá»­ dá»¥ng BPE cáº¥p byte, phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n Ä‘áº¿n nÃ³.

GPT-2 khÃ´ng sá»­ dá»¥ng má»™t trÃ¬nh chuáº©n hoÃ¡, nÃªn ta cÃ³ thá»ƒ bá» qua bÆ°á»›c nÃ y vÃ  Ä‘i trá»±c tiáº¿p vÃ o bÆ°á»›c pre-tokenization:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

Tuá»³ chá»n `ByteLevel` chÃºng ta thÃªm vÃ o á»Ÿ Ä‘Ã¢y khÃ´ng thÃªm dáº¥u cÃ¡ch vÃ o Ä‘áº§u cá»§a má»™t cÃ¢u (thÆ°á»ng nÃ³ lÃ  máº·c Ä‘á»‹nh). Ta cÃ³ thá»ƒ nhÃ¬n cÃ¡c pre-tokenization tá»« vÃ­ dá»¥ tÆ°Æ¡ng tá»± á»Ÿ trÃªn:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

Tiáº¿p theo lÃ  mÃ´ hÃ¬nh mÃ  ta cáº§n huáº¥n luyá»‡n. Vá»›i GPT-2, token Ä‘áº·c biá»‡t duy nháº¥t ta cáº§n lÃ  token káº¿t thÃºc vÄƒn báº£n: 

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

NhÆ° vá»›i `WordPieceTrainer`, cÅ©ng nhÆ° `vocab_size` vÃ  `special_tokens`, ta cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh `min_frequency` náº¿u muá»‘n, hoáº·c náº¿u ta cÃ³ háº­u tá»‘ káº¿t thÃºc tá»« (nhÆ° `</w>`), ta cÃ³ thá»ƒ thiáº¿t láº­p nÃ³ vá»›i `end_of_word_suffix`.

Tokenizer nÃ y cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c tá»‡p vÄƒn báº£n:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

HÃ£y cÅ©ng xem káº¿t quáº£ tokenize trÃªn má»™t vÄƒn báº£n máº«u:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

Ta Ã¡p dá»¥ng háº­u xá»­ lÃ½ cáº¥p byte cho GPT-2 tokenizer nhÆ° sau:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

Tuá»³ chá»n `trim_offsets = False` chá»‰ cho trÃ¬nh háº­u xá»­ lÃ½ biáº¿t ráº±ng ta cáº§n bá» má»‘t sá»‘ offset token báº¯t Ä‘áº§u vá»›i 'Ä ': theo cÃ¡ch nÃ y, Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a offset sáº½ trá» vÃ o vÃ¹ng khÃ´ng gian phÃ­a trÆ°á»›c cá»§a tá»«, khÃ´ng pháº£i kÃ­ tá»± Ä‘áº§u tiÃªn cá»§a tá»« (vÃ¬ khÃ´ng gian nÃ y vá» máº·t ká»¹ thuáº­t lÃ  má»™t pháº§n cá»§a tá»«). HÃ£y cÃ¹ng nhÃ¬n xem káº¿t quáº£ vá»›i chuá»—i vÄƒn báº£n ta vá»«a mÃ£ hoÃ¡ vá»›i `'Ä test'` lÃ  token á»Ÿ chá»‰ má»¥c 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

Cuá»‘i cÃ¹ng, ta thÃªm má»™t trÃ¬nh giáº£i mÃ£i cáº¥p byte:

```python
tokenizer.decoder = decoders.ByteLevel()
```

vÃ  ta kiá»ƒm tra láº¡i xem nÃ³ hoáº¡t Ä‘á»™ng Ä‘Ãºng chÆ°a:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

Tuyá»‡t vá»i! Giá» ta Ä‘Ã£ xong rá»“i, ta cÃ³ thá»ƒ lÆ°u tokenizer nhÆ° trÃªn, vÃ  bao nÃ³ láº¡i trong `PreTrainedTokenizerFast` hoáº·c  `GPT2TokenizerFast` náº¿u ta muá»‘n nÃ³ trong ğŸ¤— Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

or:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

NhÆ° má»™t vÃ­ dá»¥ cuá»‘i, chÃºng tÃ´i sáº½ chá»‰ báº¡n cÃ¡ch xÃ¢y dá»±ng má»™t Unigram tokenizer tá»« Ä‘áº§u.

## XÃ¢y dá»±ng má»™t Unigram tokenizer tá»« Ä‘áº§u

HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng má»™t XLNet tokenizer. CÅ©ng giá»‘ng nhÆ° cÃ¡c tokenizer trÆ°á»›c Ä‘Ã³, ta cÃ³ thá»ƒ báº¯t Ä‘áº§u khá»Ÿi táº¡o `Tokenizer` vá»›i má»™t mÃ´ hÃ¬nh Unigram:

```python
tokenizer = Tokenizer(models.Unigram())
```

Má»™t láº§n ná»¯a, chÃºng ta cÃ³ thá»ƒ khá»Ÿi táº¡o mÃ´ hÃ¬nh nÃ y vá»›i má»™t tá»« vá»±ng náº¿u cÃ³.

Vá»›i sá»± chuáº©n hoÃ¡ nÃ y, XLNet sá»­ dá»¥ng má»™t vÃ i phÆ°Æ¡ng phÃ¡p thay tháº¿ (Ä‘áº¿n tá»« SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

Äiá»u nÃ y thay tháº¿ <code>``</code> and <code>''</code> báº±ng <code>"</code> vÃ  thay tháº¿ báº¥t kÃ¬ chuá»—i nÃ o chá»©a hai hoáº·c nhiá»u hÆ¡n dáº¥u cÃ¡ch liá»n nhau thÃ nh má»™t dáº¥u duy nháº¥t, cÅ©ng nhÆ° loáº¡i bá» cÃ¡c dáº¥u cÃ³ trong vÄƒn báº£n Ä‘á»ƒ tokenize.

Tiá»n tokenizer Ä‘Æ°á»£c sá»­ dá»¥ng cho báº¥t ká»³ SentencePiece tokenizer lÃ  `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

Ta cÃ³ thá»ƒ nhÃ¬n vÃ o Ä‘áº§u ra quy trÃ¬nh tiá»n tokenize qua vÃ­ dá»¥ vÄƒn báº£n á»Ÿ dÆ°á»›i:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("â–Let's", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', (14, 29))]
```

Tiáº¿p theo lÃ  mÃ´ hÃ¬nh ta cáº§n huáº¥n luyá»‡n. XLNet cÃ³ má»™t sá»‘ token Ä‘áº·c biá»‡t:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

Má»™t tham sá»‘ vÃ´ cÃ¹ng quan trong mÃ  ta khÃ´ng thá»ƒ quÃªn cá»§a `UnigramTrainer` lÃ  `unk_token`. Ta cÃ³ thá»ƒ truyá»n vÃ o cÃ¡c tham sá»‘ cá»¥ thá»ƒ khÃ¡c tá»›i thuáº­t toÃ¡n Unigram, vÃ­ dá»¥ `shrinking_factor` cho cÃ¡c bÆ°á»›c mÃ  ta xoÃ¡ token (máº·c Ä‘á»‹nh lÃ  0.75) hoáº·c `max_piece_length` Ä‘á»ƒ chá»‰ Ä‘á»‹nh Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a má»™t token (máº·c Ä‘á»‹nh lÃ  16).

Tokenizer nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c tá»‡p vÄƒn báº£n:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

HÃ£y cÃ¹ng nhÃ¬n xem káº¿t quáº£ tokenize trÃªn táº­p máº«u:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']
```

Má»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t cá»§a XLNet Ä‘Ã³ lÃ  nÃ³ thÃªm token `<cls>` á»Ÿ cuá»‘i má»—i cÃ¢u, vá»›i kiá»ƒu ID lÃ  2 (Ä‘á»ƒ phÃ¢n biáº¿t vá»›i cÃ¡c token khÃ¡c). NÃ³ Ä‘á»‡m thÃªm vÃ o phÃ­a bÃªn tay trÃ¡i giá»‘ng nhÆ° káº¿t quáº£ á»Ÿ trÃªn. Ta cÃ³ thá»ƒ xá»­ lÃ½ táº¥t cáº£ cÃ¡c token Ä‘áº·c biá»‡t vÃ  cÃ¡c token kiá»ƒu ID vá»›i cÃ¹ng má»™t báº£n máº«u, nhÆ° BERT, nhÆ°ng Ä‘áº§u tiÃªn ta pháº£i láº¥y cÃ¡c ID cá»§a token `<cls>` vÃ  `<sep>`:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

Báº£n máº«u sáº½ trÃ´ng nhÆ° sau:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

VÃ  ta cÃ³ thá»ƒ kiá»ƒm tra xem nÃ³ hoáº¡t Ä‘á»™ng khÃ´ng báº±ng cÃ¡ch mÃ£ hoÃ¡ cáº·p cÃ¢u:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

Cuá»‘i cÃ¹ng, ta sáº½ thÃªm trÃ¬nh giáº£i mÃ£ `Metaspace`:

```python
tokenizer.decoder = decoders.Metaspace()
```

vÃ  ta Ä‘Ã£ xong vá»›i tokenizer nÃ y! Ta cÃ³ thá»ƒ lÆ°u tokenizer nhÆ° trÃªn, vÃ  bao nÃ³ láº¡i trong `PreTrainedTokenizerFast` hoáº·c  `XLNetTokenizerFast` náº¿u ta muá»‘n nÃ³ trong ğŸ¤— Transformers. Má»™t Ä‘iá»ƒm cáº§n lÆ°u Ã½ lÃ  khi sá»­ dá»¥ng `PreTrainedTokenizerFast` thÃ¬ trÃªn Ä‘áº§u cá»§a cÃ¡c token Ä‘áº·c biá»‡t ta cáº§n nÃ³i cho thÆ° viá»‡n ğŸ¤— Transformers viáº¿t ta cáº§n Ä‘á»‡m vÃ o phÃ­a bÃªn trÃ¡i:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

Hoáº·c má»™t cÃ¡ch khÃ¡c:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

BÃ¢y giá» báº¡n Ä‘Ã£ tháº¥y cÃ¡ch cÃ¡c khá»‘i khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c tokenizer hiá»‡n nay, báº¡n sáº½ cÃ³ thá»ƒ viáº¿t báº¥t ká»³ trÃ¬nh tokenize nÃ o mÃ  báº¡n muá»‘n vá»›i thÆ° viá»‡n ğŸ¤— Tokenizers vÃ  cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ trong ğŸ¤— Transformers.
