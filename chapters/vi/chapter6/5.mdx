# Byte-Pair Encoding tokenization

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section5.ipynb"},
]} />

MÃ£ hÃ³a theo cáº·p (BPE) tiá»n thÃ¢n Ä‘Æ°á»£c phÃ¡t triá»ƒn nhÆ° má»™t thuáº­t toÃ¡n Ä‘á»ƒ nÃ©n vÄƒn báº£n, sau Ä‘Ã³ Ä‘Æ°á»£c OpenAI sá»­ dá»¥ng Ä‘á»ƒ tokenize khi huáº¥n luyá»‡n trÆ°á»›c mÃ´ hÃ¬nh GPT. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi ráº¥t nhiá»u mÃ´ hÃ¬nh Transformer, bao gá»“m GPT, GPT-2, RoBERTa, BART vÃ  DeBERTa.

<Youtube id="HEikzVL-lZU"/>

<Tip>

ğŸ’¡ Pháº§n nÃ y trÃ¬nh bÃ y sÃ¢u hÆ¡n vá» BPE, Ä‘i xa hÆ¡n ná»¯a lÃ  trÃ¬nh bÃ y cÃ¡ch triá»ƒn khai Ä‘áº§y Ä‘á»§. Báº¡n cÃ³ thá»ƒ bá» qua pháº§n cuá»‘i náº¿u báº¡n chá»‰ muá»‘n cÃ³ má»™t cÃ¡i nhÃ¬n tá»•ng quan chung vá» thuáº­t toÃ¡n tokenize.

</Tip>

## Thuáº­t toÃ¡n huáº¥n luyá»‡n

Huáº¥n luyá»‡n BPE báº¯t Ä‘áº§u báº±ng cÃ¡ch tÃ­nh toÃ¡n táº­p há»£p cÃ¡c tá»« duy nháº¥t Ä‘Æ°á»£c sá»­ dá»¥ng trong kho ngá»¯ liá»‡u (sau khi hoÃ n thÃ nh cÃ¡c bÆ°á»›c chuáº©n hÃ³a vÃ  pre-tokenization), sau Ä‘Ã³ xÃ¢y dá»±ng tá»« vá»±ng báº±ng cÃ¡ch láº¥y táº¥t cáº£ cÃ¡c kÃ½ hiá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ viáº¿t nhá»¯ng tá»« Ä‘Ã³. VÃ­ dá»¥ ráº¥t Ä‘Æ¡n giáº£n, giáº£ sá»­ kho dá»¯ liá»‡u cá»§a chÃºng ta sá»­ dá»¥ng nÄƒm tá»« sau:

```
"hug", "pug", "pun", "bun", "hugs"
```

Tá»« vá»±ng cÆ¡ sá»Ÿ khi Ä‘Ã³ sáº½ lÃ  `["b", "g", "h", "n", "p", "s", "u"]`. Äá»‘i vá»›i cÃ¡c trÆ°á»ng há»£p trong thá»±c táº¿, tá»« vá»±ng cÆ¡ sá»Ÿ Ä‘Ã³ sáº½ chá»©a táº¥t cáº£ cÃ¡c kÃ½ tá»± ASCII, Ã­t nháº¥t vÃ  cÃ³ thá»ƒ lÃ  má»™t sá»‘ kÃ½ tá»± Unicode. Náº¿u má»™t máº«u báº¡n Ä‘ang tokenize sá»­ dá»¥ng má»™t kÃ½ tá»± khÃ´ng cÃ³ trong kho dá»¯ liá»‡u huáº¥n luyá»‡n, thÃ¬ kÃ½ tá»± Ä‘Ã³ sáº½ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh token khÃ´ng xÃ¡c Ä‘á»‹nh. ÄÃ³ lÃ  má»™t lÃ½ do táº¡i sao nhiá»u mÃ´ hÃ¬nh NLP ráº¥t kÃ©m trong viá»‡c phÃ¢n tÃ­ch ná»™i dung báº±ng biá»ƒu tÆ°á»£ng cáº£m xÃºc.

<Tip>

GPT-2 vÃ  RoBERTa tokenizer (khÃ¡ giá»‘ng nhau) cÃ³ má»™t cÃ¡ch thÃ´ng minh Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y: chÃºng khÃ´ng xem cÃ¡c tá»« Ä‘Æ°á»£c viáº¿t báº±ng cÃ¡c kÃ½ tá»± Unicode mÃ  lÃ  cÃ¡c byte. Báº±ng cÃ¡ch nÃ y, tá»« vá»±ng cÆ¡ sá»Ÿ cÃ³ kÃ­ch thÆ°á»›c nhá» (256), nhÆ°ng má»i kÃ½ tá»± báº¡n cÃ³ thá»ƒ nghÄ© Ä‘áº¿n sáº½ váº«n Ä‘Æ°á»£c bao gá»“m vÃ  khÃ´ng bá»‹ chuyá»ƒn Ä‘á»•i thÃ nh token khÃ´ng xÃ¡c Ä‘á»‹nh. Thá»§ thuáº­t nÃ y Ä‘Æ°á»£c gá»i lÃ  *BPE cáº¥p byte*.

</Tip>

Sau khi cÃ³ Ä‘Æ°á»£c bá»™ tá»« vá»±ng cÆ¡ báº£n nÃ y, chÃºng ta thÃªm cÃ¡c token má»›i cho Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c kÃ­ch thÆ°á»›c tá»« vá»±ng mong muá»‘n báº±ng cÃ¡ch há»c *há»£p nháº¥t*, Ä‘Ã¢y lÃ  cÃ¡c quy táº¯c Ä‘á»ƒ há»£p nháº¥t hai yáº¿u tá»‘ cá»§a tá»« vá»±ng hiá»‡n cÃ³ vá»›i nhau thÃ nh má»™t tá»« má»›i. VÃ¬ váº­y, lÃºc Ä‘áº§u sá»± há»£p nháº¥t nÃ y sáº½ táº¡o ra cÃ¡c token cÃ³ hai kÃ½ tá»± vÃ  sau Ä‘Ã³, khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n tiáº¿n triá»ƒn, cÃ¡c tá»« phá»¥ sáº½ dÃ i hÆ¡n.

Táº¡i báº¥t ká»³ bÆ°á»›c nÃ o trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n token, thuáº­t toÃ¡n BPE sáº½ tÃ¬m kiáº¿m cáº·p token hiá»‡n cÃ³ thÆ°á»ng xuyÃªn nháº¥t (theo "cáº·p", á»Ÿ Ä‘Ã¢y cÃ³ nghÄ©a lÃ  hai token liÃªn tiáº¿p trong má»™t tá»«). Cáº·p thÆ°á»ng xuyÃªn nháº¥t Ä‘Ã³ lÃ  cáº·p sáº½ Ä‘Æ°á»£c há»£p nháº¥t, vÃ  chÃºng ta xáº£ vÃ  láº·p láº¡i cho bÆ°á»›c tiáº¿p theo.

Quay trá»Ÿ láº¡i vÃ­ dá»¥ trÆ°á»›c, giáº£ sá»­ cÃ¡c tá»« cÃ³ táº§n sá»‘ nhÆ° sau:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

nghÄ©a lÃ  `"hug"` cÃ³ máº·t 10 láº§n trong kho ngá»¯ liá»‡u, `"pug"` 5 láº§n, `"pun"` 12 láº§n, `"bun"` 4 láº§n vÃ  `"hug"` 5 láº§n. ChÃºng ta báº¯t Ä‘áº§u huáº¥n luyá»‡n báº±ng cÃ¡ch tÃ¡ch tá»«ng tá»« thÃ nh cÃ¡c kÃ½ tá»± (nhá»¯ng kÃ½ tá»± hÃ¬nh thÃ nh tá»« vá»±ng ban Ä‘áº§u cá»§a chÃºng ta) Ä‘á»ƒ cÃ³ thá»ƒ xem má»—i tá»« nhÆ° má»™t danh sÃ¡ch cÃ¡c token:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```


Sau Ä‘Ã³, chÃºng ta xem xÃ©t cÃ¡c cáº·p. Cáº·p `("h", "u")` cÃ³ trong cÃ¡c tá»«  `"hug"` vÃ   `"hugs"`, vÃ¬ váº­y tá»•ng cá»™ng lÃ  15 láº§n trong ngá»¯ liá»‡u. Tuy nhiÃªn, Ä‘Ã¢y khÃ´ng pháº£i lÃ  cáº·p thÆ°á»ng xuyÃªn nháº¥t: vinh dá»± Ä‘Ã³ thuá»™c vá» `("u", "g")`, cÃ³ trong `"hug"`, `"pug"`, vÃ  `"hugs"`, vá»›i tá»•ng cá»™ng 20 láº§n xuáº¥t hiá»‡n trong bá»™ tá»« vá»±ng.

Do Ä‘Ã³, quy táº¯c há»£p nháº¥t Ä‘áº§u tiÃªn Ä‘Æ°á»£c há»c bá»Ÿi tokenizer lÃ  `("u", "g") -> "ug"`, cÃ³ nghÄ©a lÃ  `"ug"` sáº½ Ä‘Æ°á»£c thÃªm vÃ o tá»« vá»±ng vÃ  cáº·p nÃ y sáº½ Ä‘Æ°á»£c há»£p nháº¥t trong táº¥t cáº£ cÃ¡c tá»« cá»§a ngá»¯ liá»‡u. VÃ o cuá»‘i giai Ä‘oáº¡n nÃ y, tá»« vá»±ng vÃ  ngá»¯ liá»‡u sáº½ giá»‘ng nhÆ° sau:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

BÃ¢y giá» chÃºng ta cÃ³ má»™t sá»‘ cáº·p dáº«n Ä‘áº¿n má»™t token dÃ i hÆ¡n hai kÃ½ tá»±: vÃ­ dá»¥: cáº·p `("h", "ug")`, (hiá»‡n diá»‡n 15 láº§n trong kho ngá»¯ liá»‡u). Cáº·p thÆ°á»ng gáº·p nháº¥t á»Ÿ giai Ä‘oáº¡n nÃ y lÃ  `("u", "n")`, xuáº¥t hiá»‡n 16 láº§n trong kho ngá»¯ liá»‡u, vÃ¬ váº­y quy táº¯c há»£p nháº¥t thá»© hai Ä‘Ã£ há»c lÃ  `("u", "n") -> "un"`. ThÃªm nÃ³ vÃ o bá»™ tá»« vá»±ng vÃ  há»£p nháº¥t táº¥t cáº£ cÃ¡c láº§n xuáº¥t hiá»‡n hiá»‡n cÃ³ sáº½ dáº«n chÃºng ta Ä‘áº¿n:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

Giá» thÃ¬ cáº·p xuáº¥t hiá»‡n nhiá»u nháº¥t lÃ  `("h", "ug")`, nÃªn chÃºng ta há»£p nháº¥t `("h", "ug") -> "hug"`, tráº£ vá» cho chÃºng ta token gá»“n ba kÃ­ tá»± Ä‘áº§u tiÃªn. Sau sá»± há»£p nháº¥t nÃ y, kho ngá»¯ liá»‡u sáº½ nhÆ° sau:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

VÃ  chÃºng ta tiáº¿p tÃºc lÃ m váº­y cho Ä‘áº¿n khi chÃºng ta cháº¡m Ä‘áº¿n kÃ­ch thÆ°á»›c bá»™ tá»± Ä‘iá»ƒn ta mong muá»‘n.

<Tip>

âœï¸ **Giá» thÃ¬ Ä‘áº¿n lÆ°á»£t báº¡n!** Báº¡n nghÄ© bÆ°á»›c há»£p nháº¥t tiáº¿p theo sáº½ lÃ  gÃ¬?

</Tip>

## Thuáº­t toÃ¡n tokenize

Tokenize tuÃ¢n thá»§ cháº·t cháº½ quÃ¡ trÃ¬nh huáº¥n luyá»‡n, theo nghÄ©a lÃ  cÃ¡c Ä‘áº§u vÃ o má»›i Ä‘Æ°á»£c tokenize báº±ng cÃ¡ch Ã¡p dá»¥ng cÃ¡c bÆ°á»›c sau:

1. Chuáº©n hoÃ¡
2. Pre-tokenization
3. TÃ¡ch cÃ¡c tá»« thÃ nh cÃ¡c kÃ½ tá»± riÃªng láº»
4. Ãp dá»¥ng cÃ¡c quy táº¯c há»£p nháº¥t Ä‘Ã£ há»c theo thá»© tá»± trÃªn cÃ¡c pháº§n tÃ¡ch Ä‘Ã³

Láº¥y vÃ­ dá»¥ mÃ  ta Ä‘Ã£ sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, vá»›i ba quy táº¯c há»£p nháº¥t Ä‘Ã£ há»c:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

Tá»« `"bug"` sáº½ Ä‘Æ°á»£c tokenize thÃ nh `["b", "ug"]`. `"mug"`, tuy nhiÃªn, sáº½ tokenize thÃ nh `["[UNK]", "ug"]` vÃ¬ kÃ­ tá»± `"m"` khÃ´ng cÃ³ trong bá»™ tá»± vá»±ng gá»‘c. TÆ°Æ¡ng tá»±, tá»« `"thug"` sáº½ Ä‘Æ°á»£c tokenize thÃ nh  `["[UNK]", "hug"]`: kÃ­ tá»± `"t"` khÃ´ng cÃ³ trong bá»™ tá»± vá»±ng gá»‘c, vÃ  Ã¡p dá»¥ng quy táº¯c há»£p nháº¥t á»Ÿ `"u"` vÃ  `"g"` vÃ  sau Ä‘Ã³ `"hu"` vÃ  `"g"`.

<Tip>

âœï¸ **Giá» tá»›i lÆ°á»£t báº¡n!** Báº¡n nghÄ© ráº±ng `"unhug"` sáº½ Ä‘Æ°á»£c tokenize nhÆ° tháº¿ nÃ o?

</Tip>

## Triá»ƒn khai BPE

HÃ£y cÃ¹ng xem cÃ¡c thuáº­t toÃ¡n BPE Ä‘Æ°á»£c triá»ƒn khai. ÄÃ¢y khÃ´ng pháº£i lÃ  phiÃªn báº£n tá»‘i Æ°u mÃ  báº¡n cÃ³ thá»ƒ thá»±c sá»± sá»­ dá»¥ng cho má»™t kho ngá»¯ liá»‡u lá»›n; chÃºng tÃ´i chá»‰ muá»‘n cho báº¡n xem Ä‘oáº¡n mÃ£ Ä‘á»ƒ báº¡n cÃ³ thá»ƒ hiá»ƒu thuáº­t toÃ¡n nÃ y tá»‘t hÆ¡n.

Äáº§u tiÃªn chÃºng ta cáº§n má»™t kho ngá»¯ liá»‡u, váº­y nÃªn hay táº¡o ra má»™t báº£n Ä‘Æ¡n giáº£n vá»›i má»™t vÃ i cÃ¢u:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Tiáº¿p theo, ta cáº§n tiá»n tokenize kho ngá»¯ liá»‡u nÃ y thÃ nh cÃ¡c tá»«. VÃ¬ ta Ä‘ang sao chÃ©p má»™t báº£n BPE tokenizer (nhÆ° GPT-2), ta váº«n cÃ³ thá»ƒ sá»­ dá»¥ng `gpt2` tokenize cho bÆ°á»›c pre-tokenization:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

Sau Ä‘Ã³ ta tÃ­nh táº§n suáº¥t cá»§a tá»«ng tá»« trong kho ngá»¯ liá»‡u nhÆ° khi lÃ m vá»›i pre-tokenization:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

Tiáº¿p theo chÃºng ta sáº½ tÃ­nh bá»™ tá»« vá»±ng cÆ¡ sá»Ÿ tá»« cÃ¡c kÃ­ tá»± sá»­ dá»¥ng trong kho ngá»¯ liá»‡u:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

Ta cÅ©ng cÃ³ thá»ƒ thÃªm cÃ¡c token Ä‘áº·c biá»‡t tá»« mÃ´ hÃ¬nh á»Ÿ Ä‘áº§u cá»§a bá»™ tá»± vá»±ng. Trong trÆ°á»ng há»£p cá»§a GPT-2, token Ä‘áº·c biá»‡t duy nháº¥t Ä‘Ã³ lÃ  `"<|endoftext|>"`:

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

Ta giá» cáº§n pháº£i chia má»—i tá»« thÃ nh cÃ¡c kÃ­ tá»± riÃªng láº» Ä‘á»ƒ cÃ³ thá»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

Giá» ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n, hÃ£y cÃ¹ng viáº¿t má»™t hÃ m tÃ­nh táº§n suáº¥t má»—i cáº·p. Ta sáº½ cáº§n sá»­ dá»¥ng nÃ³ á»Ÿ bÆ°á»›c huáº¥n luyá»‡n:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

HÃ£y nhÃ¬n vÃ o má»™t pháº§n tá»« Ä‘iáº»n sau khi tÃ¡ch:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

Giá» thÃ¬, tÃ¬m xem cáº·p xuáº¥t hiá»‡n nhiá»u nháº¥t báº±ng má»™t vÃ²ng láº·p nhanh:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ä ', 't') 7
```

Váº­y phÃ©p há»£p nháº¥t Ä‘áº§u tiÃªn lÃ  `('Ä ', 't') -> 'Ä t'`, vÃ  ta thÃªm `'Ä t'` vÃ o bá»™ tá»« vá»±ng:

```python
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

Äá»ƒ tiáº¿p tá»¥c, ta cáº§n Ã¡p dá»¥ng sá»± há»£p nháº¥t á»Ÿ tá»« Ä‘iá»ƒn `splits`. HÃ£y cÃ¹ng viáº¿t má»™t hÃ m khÃ¡c cho nÃ³:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

Giá» ta cÃ³ thá»ƒ nhÃ¬n xem káº¿t quáº£ cá»§a láº§n há»£p nháº¥t Ä‘áº§u tiÃªn:

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```python out
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

Giá» thÃ¬ ta cÃ³ táº¥t cáº£ nhá»¯ng gÃ¬ mÃ¬nh cáº§n Ä‘á»ƒ láº·p cho Ä‘áº¿n khi ta há»c táº¥t cÃ¡c cÃ¡c há»£p nháº¥t mÃ  ta muá»‘n. HÃ£y cÅ©ng nháº¯m tá»›i bá»™ tá»± vá»±ng cÃ³ kÃ­ch cá»¡ lÃ  50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

Káº¿t quáº£ lÃ , chÃºng ta Ä‘Ã£ há»c 19 quy táº¯c há»£p nháº¥t (bá»™ tá»« Ä‘iá»ƒn gá»‘c cÃ³ kÃ­ch cá»¡ lÃ  31 tÆ°Æ¡ng á»©ng 30 kÃ­ tá»± trong báº£ng chá»¯ cÃ¡i cÃ¹ng má»™t token Ä‘áº·t biá»‡t):

```py
print(merges)
```

```python out
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

VÃ  bá»™ tá»± vá»±ng cáº¥u thÃ nh bá»Ÿi token Ä‘áº·c biáº¿t, cÃ¡c kÃ­ tá»± trong báº£ng chá»¯ cÃ¡i, vÃ  táº¥t cáº£ káº¿t quáº£ tá»« cÃ¡c quy táº¯c há»£p nháº¥t:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

<Tip>

ğŸ’¡ Sá»­ dá»¥ng `train_new_from_iterator()` trÃªn cÃ¹ng kho ngá»¯ liá»‡u sáº½ khÃ´ng mang vá» káº¿t quáº£ kho ngá»¯ liá»‡u y há»‡t. ÄÃ³ lÃ  bá»Ÿi khi cÃ³ sá»± lá»±a chá»n vá» cáº·p cÃ³ táº§n suáº¥t cao nháº¥t, ta Ä‘Ã£ chá»n cÃ¡i Ä‘áº§u tiÃªn xuáº¥t hiá»‡n, trong khi thÆ° viá»‡n ğŸ¤— Tokenizers chá»n cÃ¡i Ä‘áº§u tiÃªn dá»±a trÃªn ID bÃªn trong cá»§a nÃ³.

</Tip>

Äá»ƒ tokenize vÄƒn báº£n má»›i, chÃºng ta tiá»n tokenize nÃ³, tÃ¡ch ra, rá»“i Ã¡p dá»¥ng quy táº¯c há»£p nháº¥t Ä‘Æ°á»£c há»c:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```
t
Ta cÃ³ thá»ƒ thá»­ cÃ¡c nÃ y vá»›i báº¥t kÃ¬ Ä‘oáº¡n vÄƒn nÃ o khÃ¡c Ä‘Æ°á»£c táº¡o thÃ nh tá»« cÃ¡c kÃ­ tá»± trong báº£ng chá»¯ cÃ¡i:

```py
tokenize("This is not a token.")
```

```python out
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

<Tip warning={true}>

âš ï¸ CÃ¡c triá»ƒn khai cá»§a chÃºng ta sáº½ gáº·p lá»—i náº¿u cÃ³ nhá»¯ng kÃ­ tá»± vÃ´ danh vÃ¬ chÃºng ta Ä‘Ã£ khÃ´ng lÃ m gÃ¬ Ä‘á»ƒ xá»­ lÃ½ chÃºng. GPT-2 khÃ´ng thá»±c sá»± cÃ³ nhá»¯ng token vÃ´ danh (khÃ´ng thá»ƒ cÃ³ kÃ­ tá»± vÃ´ danh khi sá»­ dá»¥ng BPE cáº¥p byte), nhÆ°ng nÃ³ cÃ³ thá»ƒ xáº£y ra á»Ÿ Ä‘Ã¢y vÃ¬ ta khÃ´ng bao gá»“m táº¥t cáº£ cÃ¡c byte cÃ³ thá»ƒ cÃ³ trong bá»™ tá»« vá»±ng gá»‘c. KhÃ­a cáº¡nh nÃ y cá»§a BPE náº±m ngoÃ i pháº¡m vi pháº§n nÃ y, nÃªn chÃºng tÃ´i sáº½ khÃ´ng Ä‘i sau vÃ o chi tiáº¿t.

</Tip>

ÄÃ³ lÃ  nhá»¯ng gÃ¬ ta cáº§n biáº¿t vá» thuáº­t toÃ¡n BPE! Tiáº¿p theo, chÃºng ta sáº½ cÃ¹ng tÃ¬m hiá»ƒu vá» WordPiece.
