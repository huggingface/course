# WordPiece tokenization

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section6.ipynb"},
]} />

WordPiece lÃ  má»™t thuáº­t toÃ¡n tokenize Ä‘Æ°á»£c Google phÃ¡t triá»ƒn Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c BERT. NÃ³ Ä‘Ã£ Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng trong má»™t vÃ i mÃ´ hÃ¬nh Transformer dá»±a trÃªn BERT, nhÆ° DistilBERT, MobileBERT, Funnel Transformers, vÃ  MPNET. NÃ³ khÃ¡ tÆ°Æ¡ng tá»± vá»›i BPE vá» máº·t huáº¥n luyá»‡n, nhÆ°ng tokenize thá»±c sá»± Ä‘Æ°á»£c thá»±c hiá»‡n hoÃ n toÃ n khÃ¡c.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

ğŸ’¡ Pháº§n nÃ y sáº½ Ä‘i sÃ¢u vÃ o WordPiece, cÅ©ng nhÆ° cÃ¡c triá»ƒn khai Ä‘áº§y Ä‘á»§ cá»§a nÃ³. Báº¡n cÃ³ thá»ƒ bá» qua pháº§n cuá»‘i náº¿u báº¡n chá»‰ muá»‘n cÃ³ má»™t cÃ¡i nhÃ¬n tá»•ng quan vá» thuáº­t toÃ¡n tokenize nÃ y.

</Tip>

## Thuáº­t toÃ¡n huáº¥n luyá»‡n

<Tip warning={true}>

âš ï¸ Google khÃ´ng bao giá» cÃ³ nguá»“n má»Ÿ vá» cÃ¡ch triá»ƒn khai cÃ¡c thuáº­t toÃ¡n huáº¥n luyá»‡n cá»§a WordPiece,vÃ¬ váº­y nhá»¯ng gÃ¬ dÆ°á»›i Ä‘Ã¢y lÃ  phá»ng Ä‘oÃ¡n tá»‘t nháº¥t cá»§a chÃºng tÃ´i dá»±a trÃªn cÃ¡c tÃ i liá»‡u Ä‘Ã£ xuáº¥t báº£n. NÃ³ cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c 100%.

</Tip>

Giá»‘ng nhÆ° BPE, WordPiece báº¯t Ä‘áº§u tá»« má»™t tá»« vá»±ng nhá» bao gá»“m cÃ¡c token Ä‘áº·c biá»‡t Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi mÃ´ hÃ¬nh vÃ  báº£ng chá»¯ cÃ¡i Ä‘áº§u tiÃªn. VÃ¬ nÃ³ xÃ¡c Ä‘á»‹nh cÃ¡c tá»« phá»¥ báº±ng cÃ¡ch thÃªm tiá»n tá»‘ (nhÆ° `##` cho BERT), ban Ä‘áº§u má»—i tá»« Ä‘Æ°á»£c tÃ¡ch báº±ng cÃ¡ch thÃªm tiá»n tá»‘ Ä‘Ã³ vÃ o táº¥t cáº£ cÃ¡c kÃ½ tá»± bÃªn trong tá»«. VÃ¬ váº­y, vÃ­ dá»¥, `"word"` Ä‘Æ°á»£c chia nhÆ° tháº¿ nÃ y:

```
w ##o ##r ##d
```

VÃ¬ váº­y, báº£ng chá»¯ cÃ¡i chá»©a táº¥t cáº£ cÃ¡c kÃ­ tá»± xuáº¥t hiá»‡n á»Ÿ Ä‘áº§u cá»§a má»™t tá»« vÃ  cÃ¡c kÃ­ tá»± xuáº¥t hiá»‡n bÃªn trong cá»§a tá»« Ä‘Æ°á»£c thÃªm má»™t tiá»n tá»‘ cá»§a WordPiece phÃ­a trÆ°á»›c.

Sau Ä‘Ã³, má»™t láº§n ná»¯a, giá»‘ng nhÆ° BPE, WordPiece há»c cÃ¡c quy táº¯c há»£p nháº¥t. Sá»± khÃ¡c biá»‡t chÃ­nh lÃ  cÃ¡ch chá»n cáº·p Ä‘Æ°á»£c há»£p nháº¥t. Thay vÃ¬ chá»n cáº·p phá»• biáº¿n nháº¥t, WordPiece tÃ­nh Ä‘iá»ƒm cho tá»«ng cáº·p, sá»­ dá»¥ng cÃ´ng thá»©c sau:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

Báº±ng cÃ¡ch chia táº§n suáº¥t cá»§a cáº·p cho tÃ­ch táº§n suáº¥t cá»§a tá»«ng con cá»§a nÃ³, thuáº­t toÃ¡n Æ°u tiÃªn há»£p nháº¥t cÃ¡c cáº·p mÃ  cÃ¡c bá»™ pháº­n riÃªng láº» Ã­t thÆ°á»ng xuyÃªn hÆ¡n trong tá»« vá»±ng. VÃ­ dá»¥: nÃ³ sáº½ khÃ´ng nháº¥t thiáº¿t pháº£i há»£p nháº¥t `("un", "##able")` ngay cáº£ khi cáº·p Ä‘Ã³ xuáº¥t hiá»‡n ráº¥t thÆ°á»ng xuyÃªn trong tá»« vá»±ng, vÃ¬ hai cáº·p `"un"` vÃ  `"##able"` má»—i tá»« cÃ³ thá»ƒ sáº½ xuáº¥t hiá»‡n báº±ng nhiá»u tá»« khÃ¡c vÃ  cÃ³ táº§n suáº¥t cao. NgÆ°á»£c láº¡i, má»™t cáº·p nhÆ° `("hu", "##gging")` cÃ³ thá»ƒ sáº½ Ä‘Æ°á»£c há»£p nháº¥t nhanh hÆ¡n (giáº£ sá»­ tá»« "hugging" xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn trong tá»« vá»±ng) vÃ¬ `"hu"` vÃ  `"##gging"`cÃ³ kháº£ nÄƒng Ã­t xuáº¥t hiá»‡n hÆ¡n vá»›i tá»«ng cÃ¡ thá»ƒ.

HÃ£y cÃ¹ng nhÃ¬n vÃ o cÃ¹ng bá»™ tá»± vá»±ng chÃºng ta sá»­ dá»¥ng cho BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

NÃ³ sáº½ Ä‘Æ°á»£c chia ra nhÆ° sau:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

vÃ¬ váº­y tá»« vá»±ng ban Ä‘áº§u sáº½ lÃ  `["b", "h", "p", "##g", "##n", "##s", "##u"]` (náº¿u ta táº¡m quÃªn cÃ¡c token Ä‘áº·c biá»‡t). Cáº·p thÆ°á»ng gáº·p nháº¥t lÃ  `("##u", "##g")` (xuáº¥t hiá»‡n 20 láº§n), nhÆ°ng táº§n suáº¥t xuáº¥t hiá»‡n riÃªng cá»§a `"##u"` ráº¥t cao, vÃ¬ váº­y Ä‘iá»ƒm cá»§a nÃ³ khÃ´ng pháº£i lÃ  cao nháº¥t (Ä‘Ã³ lÃ  1 / 36). Táº¥t cáº£ cÃ¡c cáº·p cÃ³ `"##u"`thá»±c sá»± cÃ³ cÃ¹ng Ä‘iá»ƒm (1 / 36), vÃ¬ váº­y Ä‘iá»ƒm tá»‘t nháº¥t thuá»™c vá» cáº·p `("##g", "##s")` -- cáº·p duy nháº¥t khÃ´ng cÃ³ `"##u"` -- lÃ  1 / 20, vÃ  phÃ©p há»£p nháº¥t Ä‘áº§u tiÃªn Ä‘Ã£ há»c lÃ  `("##g", "##s") -> ("##gs")`.

LÆ°u Ã½ ráº±ng khi há»£p nháº¥t, chÃºng ta loáº¡i bá» `##` giá»¯a hai token, vÃ¬ váº­y chÃºng ta thÃªm `"##gs"` vÃ o tá»« vá»±ng vÃ  Ã¡p dá»¥ng há»£p nháº¥t trong cÃ¡c tá»« cá»§a ngá»¯ liá»‡u:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

Táº¡i thá»i Ä‘iá»ƒm nÃ y, `"##u"` náº±m trong táº¥t cáº£ cÃ¡c cáº·p cÃ³ thá»ƒ cÃ³, vÃ¬ váº­y táº¥t cáº£ chÃºng Ä‘á»u cÃ³ cÃ¹ng Ä‘iá»ƒm. Giáº£ sá»­ trong trÆ°á»ng há»£p nÃ y, cáº·p Ä‘áº§u tiÃªn Ä‘Æ°á»£c há»£p nháº¥t, vÃ¬ váº­y `("h", "##u") -> "hu"`. Äiá»u nÃ y Ä‘Æ°a chÃºng ta Ä‘áº¿n:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

Sau Ä‘Ã³, Ä‘iá»ƒm sá»‘ tá»‘t nháº¥t tiáº¿p theo Ä‘Æ°á»£c chia sáº» bá»Ÿi `("hu", "##g")` and `("hu", "##gs")` (vá»›i 1/15, so vá»›i 1/21 cá»§a cÃ¡c cáº·p khÃ¡c), vÃ¬ váº­y cáº·p Ä‘áº§u tiÃªn cÃ³ Ä‘iá»ƒm lá»›n nháº¥t Ä‘Æ°á»£c há»£p nháº¥t:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

vÃ  tiáº¿p tá»¥c nhÆ° váº­y cho Ä‘áº¿n khi chÃºng ta Ä‘áº¡t Ä‘Æ°á»£c kÃ­ch thÆ°á»›c bá»™ tá»« vá»±ng mong muá»‘n.

<Tip>

âœï¸ **Giá» Ä‘áº¿n lÆ°á»£t báº¡n!** Bá»™ quy luáº­t há»£p nháº¥t tiáº¿p theo lÃ  gÃ¬?

</Tip>

## Thuáº­t toÃ¡n tokenize

Tokenize cá»§a WordPiece khÃ¡c BPE á»Ÿ chá»— WordPiece chá»‰ lÆ°u tá»« vá»±ng cuá»‘i cÃ¹ng, khÃ´ng lÆ°u cÃ¡c quy táº¯c há»£p nháº¥t Ä‘Ã£ há»c. Báº¯t Ä‘áº§u tá»« tá»« cáº§n tokenize, WordPiece tÃ¬m tá»« con dÃ i nháº¥t cÃ³ trong tá»« vá»±ng, sau Ä‘Ã³ tÃ¡ch tá»« Ä‘Ã³ ra. VÃ­ dá»¥: náº¿u chÃºng ta sá»­ dá»¥ng tá»« vá»±ng Ä‘Ã£ há»c trong vÃ­ dá»¥ trÃªn, Ä‘á»‘i vá»›i tá»« `"hugs"` tá»« phá»¥ dÃ i nháº¥t báº¯t Ä‘áº§u tá»« Ä‘áº§u mÃ  bÃªn trong tá»« vá»±ng lÃ  `"hug"`, vÃ¬ váº­y chÃºng ta tÃ¡ch á»Ÿ Ä‘Ã³ vÃ  nháº­n Ä‘Æ°á»£c `["hug","##s"]`. Sau Ä‘Ã³, chÃºng ta tiáº¿p tá»¥c vá»›i `"##s"`, trong tá»« vá»±ng, vÃ¬ váº­y tokenize cá»§a `"hugs"` lÃ   `["hug","##s"]`.

Vá»›i BPE, chÃºng ta sáº½ Ã¡p dá»¥ng cÃ¡c phÃ©p há»£p nháº¥t Ä‘Ã£ há»c theo thá»© tá»± vÃ  token Ä‘iá»u nÃ y thÃ nh `["hu", "##gs"]`, do Ä‘Ã³, cÃ¡ch mÃ£ hoÃ¡ sáº½ khÃ¡c.

VÃ­ dá»¥ khÃ¡c, hÃ£y xem tá»« `"bugs"` sáº½ Ä‘Æ°á»£c tokenize nhÆ° tháº¿ nÃ o. `"b"`  lÃ  tá»« phá»¥ dÃ i nháº¥t báº¯t Ä‘áº§u tá»« Ä‘áº§u cá»§a tá»« cÃ³ trong tá»« vá»±ng, vÃ¬ váº­y chÃºng tÃ´i tÃ¡ch á»Ÿ Ä‘Ã³ vÃ  nháº­n Ä‘Æ°á»£c `["b", "##ugs"]`. Sau Ä‘Ã³, `"##u"` lÃ  tá»« con dÃ i nháº¥t báº¯t Ä‘áº§u á»Ÿ Ä‘áº§u `"##ugs"` cÃ³ trong tá»« vá»±ng, vÃ¬ váº­y chÃºng ta tÃ¡ch á»Ÿ Ä‘Ã³ vÃ  nháº­n Ä‘Æ°á»£c `["b", "##u, "##gs"]`. Cuá»‘i cÃ¹ng, `"##gs"` cÃ³ trong tá»« vá»±ng, vÃ¬ váº­y danh sÃ¡ch cuá»‘i cÃ¹ng nÃ y lÃ  mÃ£ hÃ³a cá»§a `"bugs"`.

Khi quÃ¡ trÃ¬nh tokenize Ä‘áº¿n giai Ä‘oáº¡n khÃ´ng thá»ƒ tÃ¬m tháº¥y má»™t tá»« khÃ³a phá»¥ trong tá»« vá»±ng, toÃ n bá»™ tá»« Ä‘Æ°á»£c tokenize thÃ nh khÃ´ng xÃ¡c Ä‘á»‹nh - vÃ¬ váº­y, vÃ­ dá»¥: `"mug"` sáº½ Ä‘Æ°á»£c tokenize lÃ   `["[UNK]"]`, cÅ©ng nhÆ° `"bum"` (ngay cáº£ khi chÃºng ta cÃ³ thá»ƒ báº¯t Ä‘áº§u báº±ng `"b"` vÃ  `"##u"`, `"##m"` khÃ´ng pháº£i thuá»™c bá»™ tá»« vá»±ng vÃ  káº¿t quáº£ tokenize sáº½ chá»‰ lÃ  `["[UNK]"]`, khÃ´ng pháº£i `["b", "##u", "[UNK]"]`). ÄÃ¢y lÃ  má»™t Ä‘iá»ƒm khÃ¡c biá»‡t so vá»›i BPE, chá»‰ phÃ¢n loáº¡i cÃ¡c kÃ½ tá»± riÃªng láº» khÃ´ng cÃ³ trong tá»« vá»±ng lÃ  khÃ´ng xÃ¡c Ä‘á»‹nh.

<Tip>

âœï¸ **Giá» Ä‘áº¿n lÆ°á»£t báº¡n!** `"pugs"` sáº½ Ä‘Æ°á»£c tokenize nhÆ° tháº¿ nÃ o?

</Tip>

## Triá»ƒn khai WordPiece

BÃ¢y giá» chÃºng ta hÃ£y xem xÃ©t viá»‡c triá»ƒn khai thuáº­t toÃ¡n WordPiece. Giá»‘ng nhÆ° vá»›i BPE, Ä‘Ã¢y chá»‰ lÃ  phÆ°Æ¡ng phÃ¡p sÆ° pháº¡m vÃ  báº¡n sáº½ khÃ´ng thá»ƒ sá»­ dá»¥ng nÃ³ trÃªn má»™t kho ngá»¯ liá»‡u lá»›n.

ChÃºng tÃ´i sáº½ sá»­ dá»¥ng cÃ¹ng má»™t kho dá»¯ liá»‡u nhÆ° trong vÃ­ dá»¥ BPE:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

Äáº§u tiÃªn, ta cáº§n tiá»n tokenize kho ngá»¯ liá»‡u thÃ nh cÃ¡c tá»«, VÃ¬ ta sao chÃ©p láº¡i WordPiece tokenizer (nhÆ° BERT), ta sáº½ sá»­ dá»¥ng `bert-base-cased` tokenizer cho pre-tokenization:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

sau Ä‘Ã³ ta sáº½ tÃ­nh táº§n suáº¥t cá»§a má»—i tá»« trong kho ngá»¯ liá»‡u nhÆ° cÃ¡ch ta lÃ m vá»›i pre-tokenization:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

NhÆ° chÃºng ta Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã¢y, báº£ng chá»¯ cÃ¡i lÃ  táº­p há»£p duy nháº¥t bao gá»“m táº¥t cáº£ cÃ¡c chá»¯ cÃ¡i Ä‘áº§u tiÃªn cá»§a tá»« vÃ  táº¥t cáº£ cÃ¡c chá»¯ cÃ¡i khÃ¡c xuáº¥t hiá»‡n trong cÃ¡c tá»« cÃ³ tiá»n tá»‘ lÃ  `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

Ta cÅ©ng thÃªm cÃ¡c kÃ­ tá»± Ä‘áº·c biá»‡t tá»« mÃ´ hÃ¬nh á»Ÿ Ä‘áº§u bá»™ tá»± vá»±ng. Trong trÆ°á»ng há»£p cá»§a BERT, ta cÃ³ danh sÃ¡ch `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

Tiáº¿p theo, chÃºng ta cáº§n tÃ¡ch tá»«ng tá»«, vá»›i táº¥t cáº£ cÃ¡c chá»¯ cÃ¡i khÃ´ng pháº£i lÃ  tiá»n tá»‘ Ä‘áº§u tiÃªn bá»Ÿi `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

BÃ¢y giá» chÃºng ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ luyá»‡n táº­p, hÃ£y viáº¿t má»™t hÃ m tÃ­nh Ä‘iá»ƒm cá»§a tá»«ng cáº·p. ChÃºng ta sáº½ cáº§n sá»­ dá»¥ng Ä‘iá»u nÃ y á»Ÿ má»—i bÆ°á»›c huáº¥n luyá»‡n:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

HÃ£y cÃ¹ng nhÃ¬n vÃ o má»™t pháº§n cá»§a bá»™ tá»« Ä‘iá»ƒn sau láº§n chia Ä‘áº§u:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

Giá» thÃ¬ tÃ¬m cáº·p cÃ³ Ä‘iá»ƒm cao nháº¥t chá»‰ cáº§n má»™t vÃ²ng láº¯p nhanh:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

Váº­y quy táº¯c há»£p nháº¥t Ä‘áº§u tiÃªn lÃ  `('a', '##b') -> 'ab'`, vÃ  ta thÃªm `'ab'` vÃ o bá»™ tá»« vá»±ng:

```python
vocab.append("ab")
```

Tiáº¿p theo, ta cáº§n Ã¡p dá»¥ng há»£p nháº¥t trong tá»« Ä‘iá»ƒn `splits`, HÃ£y cÃ¹ng viáº¿t má»™t hÃ m cho nÃ³:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

VÃ  ta cÃ³ thá»ƒ tháº¥y káº¿t quáº£ láº§n há»£p nháº¥t Ä‘áº§u tiá»‡n:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

Giá» thÃ¬ ta Ä‘Ã£ cÃ³ táº¥t cáº£ nhá»¯ng gÃ¬ ta cáº§n Ä‘á»ƒ láº·p cho Ä‘áº¿n khi há»c háº¿t táº¥t cáº£ cÃ¡c há»£p nháº¥t ta muá»‘n. HÃ£y cÅ©ng hÆ°á»›ng tá»›i bá»™ tá»« vá»±ng cÃ³ kÃ­ch thÆ°á»›c lÃ  70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

Giá» ta cÃ³ thá»ƒ nhÃ¬n vÃ o bá»™ tá»« Ä‘iá»ƒn Ä‘Æ°á»£c táº¡o ra:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

NhÆ° cÃ³ thá»ƒ tháº¥y, so vá»›i BPE, tokenizer nÃ y há»c cÃ¡c pháº§n cá»§a tá»« nhÆ° lÃ  token nhanh hÆ¡n má»™t chÃºt.

<Tip>

ğŸ’¡ Sá»­ dá»¥ng `train_new_from_iterator()` trÃªn cÃ¹ng kho ngá»¯ liá»‡u sáº½ khÃ´ng mang vá» káº¿t quáº£ kho ngá»¯ liá»‡u y há»‡t. ÄÃ³ lÃ  bá»Ÿi thÆ° viá»‡n ğŸ¤— Tokenizers khÃ´ng triá»ƒn khai WordPiece cho huáº¥n luyá»‡n (vÃ¬ chÃºng ta khÃ´ng hoÃ n toÃ n náº±m rÃµ bÃªn trong), vÃ  sá»­ dá»¥ng BPE thay vÃ o Ä‘Ã³.

</Tip>

Äá»ƒ tokenize nhá»¯ng Ä‘oáº¡n vÄƒn má»›i, ta tiá»n tokenize nÃ³, chia nhá» vÃ  Ã¡p dá»¥ng thuáº­t toÃ¡n tokenize cho tá»«ng tá»«. Váº­y Ä‘Ã³, chÃºng ta nhÃ¬n vÃ o cá»¥m tá»« con dÃ i nháº¥t báº¯t Ä‘áº§u tá»« Ä‘áº§u tá»« Ä‘áº§u tiÃªn vÃ  chia nhá» nÃ³, sau Ä‘Ã³ láº·p láº¡i quy trÃ¬nh vá»›i pháº§n thá»© hai, vÃ  tiáº¿p tá»¥c cho Ä‘áº¿n háº¿t tá»« vÃ  cÃ¡c tá»« tiáº¿p theo trong vÄƒn báº£n: 

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

HÃ£y cÅ©ng kiá»ƒm tra trÃªn má»™t tá»« cÃ³ tronng vÃ  khÃ´ng cÃ³ trong bá»™ tá»« vá»±ng:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

Giá» hÃ£y cÃ¹ng viáº¿t má»™t hÃ m tokenize vÄƒn báº£n:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

Ta cÃ³ thá»ƒ thá»­ trÃªn báº¥t kÃ¬ vÄƒn báº£n nÃ o:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

ÄÃ³ lÃ  nhá»¯ng gÃ¬ ta cáº§n biáº¿t vá» thuáº­t toÃ¡n WordPiece! Tiáº¿p theo, chÃºng ta sáº½ cÃ¹ng tÃ¬m hiá»ƒu vá» Unigram.
