# WordPiece tokenization

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter6/section6.ipynb"},
]} />

WordPiece l√† m·ªôt thu·∫≠t to√°n tokenize ƒë∆∞·ª£c Google ph√°t tri·ªÉn ƒë·ªÉ hu·∫•n luy·ªán tr∆∞·ªõc BERT. N√≥ ƒë√£ ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng trong m·ªôt v√†i m√¥ h√¨nh Transformer d·ª±a tr√™n BERT, nh∆∞ DistilBERT, MobileBERT, Funnel Transformers, v√† MPNET. N√≥ kh√° t∆∞∆°ng t·ª± v·ªõi BPE v·ªÅ m·∫∑t hu·∫•n luy·ªán, nh∆∞ng tokenize th·ª±c s·ª± ƒë∆∞·ª£c th·ª±c hi·ªán ho√†n to√†n kh√°c.

<Youtube id="qpv6ms_t_1A"/>

> [!TIP]
> üí° Ph·∫ßn n√†y s·∫Ω ƒëi s√¢u v√†o WordPiece, c≈©ng nh∆∞ c√°c tri·ªÉn khai ƒë·∫ßy ƒë·ªß c·ªßa n√≥. B·∫°n c√≥ th·ªÉ b·ªè qua ph·∫ßn cu·ªëi n·∫øu b·∫°n ch·ªâ mu·ªën c√≥ m·ªôt c√°i nh√¨n t·ªïng quan v·ªÅ thu·∫≠t to√°n tokenize n√†y.

## Thu·∫≠t to√°n hu·∫•n luy·ªán

> [!WARNING]
> ‚ö†Ô∏è Google kh√¥ng bao gi·ªù c√≥ ngu·ªìn m·ªü v·ªÅ c√°ch tri·ªÉn khai c√°c thu·∫≠t to√°n hu·∫•n luy·ªán c·ªßa WordPiece,v√¨ v·∫≠y nh·ªØng g√¨ d∆∞·ªõi ƒë√¢y l√† ph·ªèng ƒëo√°n t·ªët nh·∫•t c·ªßa ch√∫ng t√¥i d·ª±a tr√™n c√°c t√†i li·ªáu ƒë√£ xu·∫•t b·∫£n. N√≥ c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%.

Gi·ªëng nh∆∞ BPE, WordPiece b·∫Øt ƒë·∫ßu t·ª´ m·ªôt t·ª´ v·ª±ng nh·ªè bao g·ªìm c√°c token ƒë·∫∑c bi·ªát ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi m√¥ h√¨nh v√† b·∫£ng ch·ªØ c√°i ƒë·∫ßu ti√™n. V√¨ n√≥ x√°c ƒë·ªãnh c√°c t·ª´ ph·ª• b·∫±ng c√°ch th√™m ti·ªÅn t·ªë (nh∆∞ `##` cho BERT), ban ƒë·∫ßu m·ªói t·ª´ ƒë∆∞·ª£c t√°ch b·∫±ng c√°ch th√™m ti·ªÅn t·ªë ƒë√≥ v√†o t·∫•t c·∫£ c√°c k√Ω t·ª± b√™n trong t·ª´. V√¨ v·∫≠y, v√≠ d·ª•, `"word"` ƒë∆∞·ª£c chia nh∆∞ th·∫ø n√†y:

```
w ##o ##r ##d
```

V√¨ v·∫≠y, b·∫£ng ch·ªØ c√°i ch·ª©a t·∫•t c·∫£ c√°c k√≠ t·ª± xu·∫•t hi·ªán ·ªü ƒë·∫ßu c·ªßa m·ªôt t·ª´ v√† c√°c k√≠ t·ª± xu·∫•t hi·ªán b√™n trong c·ªßa t·ª´ ƒë∆∞·ª£c th√™m m·ªôt ti·ªÅn t·ªë c·ªßa WordPiece ph√≠a tr∆∞·ªõc.

Sau ƒë√≥, m·ªôt l·∫ßn n·ªØa, gi·ªëng nh∆∞ BPE, WordPiece h·ªçc c√°c quy t·∫Øc h·ª£p nh·∫•t. S·ª± kh√°c bi·ªát ch√≠nh l√† c√°ch ch·ªçn c·∫∑p ƒë∆∞·ª£c h·ª£p nh·∫•t. Thay v√¨ ch·ªçn c·∫∑p ph·ªï bi·∫øn nh·∫•t, WordPiece t√≠nh ƒëi·ªÉm cho t·ª´ng c·∫∑p, s·ª≠ d·ª•ng c√¥ng th·ª©c sau:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

B·∫±ng c√°ch chia t·∫ßn su·∫•t c·ªßa c·∫∑p cho t√≠ch t·∫ßn su·∫•t c·ªßa t·ª´ng con c·ªßa n√≥, thu·∫≠t to√°n ∆∞u ti√™n h·ª£p nh·∫•t c√°c c·∫∑p m√† c√°c b·ªô ph·∫≠n ri√™ng l·∫ª √≠t th∆∞·ªùng xuy√™n h∆°n trong t·ª´ v·ª±ng. V√≠ d·ª•: n√≥ s·∫Ω kh√¥ng nh·∫•t thi·∫øt ph·∫£i h·ª£p nh·∫•t `("un", "##able")` ngay c·∫£ khi c·∫∑p ƒë√≥ xu·∫•t hi·ªán r·∫•t th∆∞·ªùng xuy√™n trong t·ª´ v·ª±ng, v√¨ hai c·∫∑p `"un"` v√† `"##able"` m·ªói t·ª´ c√≥ th·ªÉ s·∫Ω xu·∫•t hi·ªán b·∫±ng nhi·ªÅu t·ª´ kh√°c v√† c√≥ t·∫ßn su·∫•t cao. Ng∆∞·ª£c l·∫°i, m·ªôt c·∫∑p nh∆∞ `("hu", "##gging")` c√≥ th·ªÉ s·∫Ω ƒë∆∞·ª£c h·ª£p nh·∫•t nhanh h∆°n (gi·∫£ s·ª≠ t·ª´ "hugging" xu·∫•t hi·ªán th∆∞·ªùng xuy√™n trong t·ª´ v·ª±ng) v√¨ `"hu"` v√† `"##gging"`c√≥ kh·∫£ nƒÉng √≠t xu·∫•t hi·ªán h∆°n v·ªõi t·ª´ng c√° th·ªÉ.

H√£y c√πng nh√¨n v√†o c√πng b·ªô t·ª± v·ª±ng ch√∫ng ta s·ª≠ d·ª•ng cho BPE:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

N√≥ s·∫Ω ƒë∆∞·ª£c chia ra nh∆∞ sau:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

v√¨ v·∫≠y t·ª´ v·ª±ng ban ƒë·∫ßu s·∫Ω l√† `["b", "h", "p", "##g", "##n", "##s", "##u"]` (n·∫øu ta t·∫°m qu√™n c√°c token ƒë·∫∑c bi·ªát). C·∫∑p th∆∞·ªùng g·∫∑p nh·∫•t l√† `("##u", "##g")` (xu·∫•t hi·ªán 20 l·∫ßn), nh∆∞ng t·∫ßn su·∫•t xu·∫•t hi·ªán ri√™ng c·ªßa `"##u"` r·∫•t cao, v√¨ v·∫≠y ƒëi·ªÉm c·ªßa n√≥ kh√¥ng ph·∫£i l√† cao nh·∫•t (ƒë√≥ l√† 1 / 36). T·∫•t c·∫£ c√°c c·∫∑p c√≥ `"##u"`th·ª±c s·ª± c√≥ c√πng ƒëi·ªÉm (1 / 36), v√¨ v·∫≠y ƒëi·ªÉm t·ªët nh·∫•t thu·ªôc v·ªÅ c·∫∑p `("##g", "##s")` -- c·∫∑p duy nh·∫•t kh√¥ng c√≥ `"##u"` -- l√† 1 / 20, v√† ph√©p h·ª£p nh·∫•t ƒë·∫ßu ti√™n ƒë√£ h·ªçc l√† `("##g", "##s") -> ("##gs")`.

L∆∞u √Ω r·∫±ng khi h·ª£p nh·∫•t, ch√∫ng ta lo·∫°i b·ªè `##` gi·ªØa hai token, v√¨ v·∫≠y ch√∫ng ta th√™m `"##gs"` v√†o t·ª´ v·ª±ng v√† √°p d·ª•ng h·ª£p nh·∫•t trong c√°c t·ª´ c·ªßa ng·ªØ li·ªáu:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

T·∫°i th·ªùi ƒëi·ªÉm n√†y, `"##u"` n·∫±m trong t·∫•t c·∫£ c√°c c·∫∑p c√≥ th·ªÉ c√≥, v√¨ v·∫≠y t·∫•t c·∫£ ch√∫ng ƒë·ªÅu c√≥ c√πng ƒëi·ªÉm. Gi·∫£ s·ª≠ trong tr∆∞·ªùng h·ª£p n√†y, c·∫∑p ƒë·∫ßu ti√™n ƒë∆∞·ª£c h·ª£p nh·∫•t, v√¨ v·∫≠y `("h", "##u") -> "hu"`. ƒêi·ªÅu n√†y ƒë∆∞a ch√∫ng ta ƒë·∫øn:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

Sau ƒë√≥, ƒëi·ªÉm s·ªë t·ªët nh·∫•t ti·∫øp theo ƒë∆∞·ª£c chia s·∫ª b·ªüi `("hu", "##g")` and `("hu", "##gs")` (v·ªõi 1/15, so v·ªõi 1/21 c·ªßa c√°c c·∫∑p kh√°c), v√¨ v·∫≠y c·∫∑p ƒë·∫ßu ti√™n c√≥ ƒëi·ªÉm l·ªõn nh·∫•t ƒë∆∞·ª£c h·ª£p nh·∫•t:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

v√† ti·∫øp t·ª•c nh∆∞ v·∫≠y cho ƒë·∫øn khi ch√∫ng ta ƒë·∫°t ƒë∆∞·ª£c k√≠ch th∆∞·ªõc b·ªô t·ª´ v·ª±ng mong mu·ªën.

> [!TIP]
> ‚úèÔ∏è **Gi·ªù ƒë·∫øn l∆∞·ª£t b·∫°n!** B·ªô quy lu·∫≠t h·ª£p nh·∫•t ti·∫øp theo l√† g√¨?

## Thu·∫≠t to√°n tokenize

Tokenize c·ªßa WordPiece kh√°c BPE ·ªü ch·ªó WordPiece ch·ªâ l∆∞u t·ª´ v·ª±ng cu·ªëi c√πng, kh√¥ng l∆∞u c√°c quy t·∫Øc h·ª£p nh·∫•t ƒë√£ h·ªçc. B·∫Øt ƒë·∫ßu t·ª´ t·ª´ c·∫ßn tokenize, WordPiece t√¨m t·ª´ con d√†i nh·∫•t c√≥ trong t·ª´ v·ª±ng, sau ƒë√≥ t√°ch t·ª´ ƒë√≥ ra. V√≠ d·ª•: n·∫øu ch√∫ng ta s·ª≠ d·ª•ng t·ª´ v·ª±ng ƒë√£ h·ªçc trong v√≠ d·ª• tr√™n, ƒë·ªëi v·ªõi t·ª´ `"hugs"` t·ª´ ph·ª• d√†i nh·∫•t b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu m√† b√™n trong t·ª´ v·ª±ng l√† `"hug"`, v√¨ v·∫≠y ch√∫ng ta t√°ch ·ªü ƒë√≥ v√† nh·∫≠n ƒë∆∞·ª£c `["hug","##s"]`. Sau ƒë√≥, ch√∫ng ta ti·∫øp t·ª•c v·ªõi `"##s"`, trong t·ª´ v·ª±ng, v√¨ v·∫≠y tokenize c·ªßa `"hugs"` l√†  `["hug","##s"]`.

V·ªõi BPE, ch√∫ng ta s·∫Ω √°p d·ª•ng c√°c ph√©p h·ª£p nh·∫•t ƒë√£ h·ªçc theo th·ª© t·ª± v√† token ƒëi·ªÅu n√†y th√†nh `["hu", "##gs"]`, do ƒë√≥, c√°ch m√£ ho√° s·∫Ω kh√°c.

V√≠ d·ª• kh√°c, h√£y xem t·ª´ `"bugs"` s·∫Ω ƒë∆∞·ª£c tokenize nh∆∞ th·∫ø n√†o. `"b"`  l√† t·ª´ ph·ª• d√†i nh·∫•t b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu c·ªßa t·ª´ c√≥ trong t·ª´ v·ª±ng, v√¨ v·∫≠y ch√∫ng t√¥i t√°ch ·ªü ƒë√≥ v√† nh·∫≠n ƒë∆∞·ª£c `["b", "##ugs"]`. Sau ƒë√≥, `"##u"` l√† t·ª´ con d√†i nh·∫•t b·∫Øt ƒë·∫ßu ·ªü ƒë·∫ßu `"##ugs"` c√≥ trong t·ª´ v·ª±ng, v√¨ v·∫≠y ch√∫ng ta t√°ch ·ªü ƒë√≥ v√† nh·∫≠n ƒë∆∞·ª£c `["b", "##u, "##gs"]`. Cu·ªëi c√πng, `"##gs"` c√≥ trong t·ª´ v·ª±ng, v√¨ v·∫≠y danh s√°ch cu·ªëi c√πng n√†y l√† m√£ h√≥a c·ªßa `"bugs"`.

Khi qu√° tr√¨nh tokenize ƒë·∫øn giai ƒëo·∫°n kh√¥ng th·ªÉ t√¨m th·∫•y m·ªôt t·ª´ kh√≥a ph·ª• trong t·ª´ v·ª±ng, to√†n b·ªô t·ª´ ƒë∆∞·ª£c tokenize th√†nh kh√¥ng x√°c ƒë·ªãnh - v√¨ v·∫≠y, v√≠ d·ª•: `"mug"` s·∫Ω ƒë∆∞·ª£c tokenize l√†  `["[UNK]"]`, c≈©ng nh∆∞ `"bum"` (ngay c·∫£ khi ch√∫ng ta c√≥ th·ªÉ b·∫Øt ƒë·∫ßu b·∫±ng `"b"` v√† `"##u"`, `"##m"` kh√¥ng ph·∫£i thu·ªôc b·ªô t·ª´ v·ª±ng v√† k·∫øt qu·∫£ tokenize s·∫Ω ch·ªâ l√† `["[UNK]"]`, kh√¥ng ph·∫£i `["b", "##u", "[UNK]"]`). ƒê√¢y l√† m·ªôt ƒëi·ªÉm kh√°c bi·ªát so v·ªõi BPE, ch·ªâ ph√¢n lo·∫°i c√°c k√Ω t·ª± ri√™ng l·∫ª kh√¥ng c√≥ trong t·ª´ v·ª±ng l√† kh√¥ng x√°c ƒë·ªãnh.

> [!TIP]
> ‚úèÔ∏è **Gi·ªù ƒë·∫øn l∆∞·ª£t b·∫°n!** `"pugs"` s·∫Ω ƒë∆∞·ª£c tokenize nh∆∞ th·∫ø n√†o?

## Tri·ªÉn khai WordPiece

B√¢y gi·ªù ch√∫ng ta h√£y xem x√©t vi·ªác tri·ªÉn khai thu·∫≠t to√°n WordPiece. Gi·ªëng nh∆∞ v·ªõi BPE, ƒë√¢y ch·ªâ l√† ph∆∞∆°ng ph√°p s∆∞ ph·∫°m v√† b·∫°n s·∫Ω kh√¥ng th·ªÉ s·ª≠ d·ª•ng n√≥ tr√™n m·ªôt kho ng·ªØ li·ªáu l·ªõn.

Ch√∫ng t√¥i s·∫Ω s·ª≠ d·ª•ng c√πng m·ªôt kho d·ªØ li·ªáu nh∆∞ trong v√≠ d·ª• BPE:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

ƒê·∫ßu ti√™n, ta c·∫ßn ti·ªÅn tokenize kho ng·ªØ li·ªáu th√†nh c√°c t·ª´, V√¨ ta sao ch√©p l·∫°i WordPiece tokenizer (nh∆∞ BERT), ta s·∫Ω s·ª≠ d·ª•ng `bert-base-cased` tokenizer cho pre-tokenization:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

sau ƒë√≥ ta s·∫Ω t√≠nh t·∫ßn su·∫•t c·ªßa m·ªói t·ª´ trong kho ng·ªØ li·ªáu nh∆∞ c√°ch ta l√†m v·ªõi pre-tokenization:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

Nh∆∞ ch√∫ng ta ƒë√£ th·∫•y tr∆∞·ªõc ƒë√¢y, b·∫£ng ch·ªØ c√°i l√† t·∫≠p h·ª£p duy nh·∫•t bao g·ªìm t·∫•t c·∫£ c√°c ch·ªØ c√°i ƒë·∫ßu ti√™n c·ªßa t·ª´ v√† t·∫•t c·∫£ c√°c ch·ªØ c√°i kh√°c xu·∫•t hi·ªán trong c√°c t·ª´ c√≥ ti·ªÅn t·ªë l√† `##`:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

Ta c≈©ng th√™m c√°c k√≠ t·ª± ƒë·∫∑c bi·ªát t·ª´ m√¥ h√¨nh ·ªü ƒë·∫ßu b·ªô t·ª± v·ª±ng. Trong tr∆∞·ªùng h·ª£p c·ªßa BERT, ta c√≥ danh s√°ch `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

Ti·∫øp theo, ch√∫ng ta c·∫ßn t√°ch t·ª´ng t·ª´, v·ªõi t·∫•t c·∫£ c√°c ch·ªØ c√°i kh√¥ng ph·∫£i l√† ti·ªÅn t·ªë ƒë·∫ßu ti√™n b·ªüi `##`:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

B√¢y gi·ªù ch√∫ng ta ƒë√£ s·∫µn s√†ng ƒë·ªÉ luy·ªán t·∫≠p, h√£y vi·∫øt m·ªôt h√†m t√≠nh ƒëi·ªÉm c·ªßa t·ª´ng c·∫∑p. Ch√∫ng ta s·∫Ω c·∫ßn s·ª≠ d·ª•ng ƒëi·ªÅu n√†y ·ªü m·ªói b∆∞·ªõc hu·∫•n luy·ªán:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

H√£y c√πng nh√¨n v√†o m·ªôt ph·∫ßn c·ªßa b·ªô t·ª´ ƒëi·ªÉn sau l·∫ßn chia ƒë·∫ßu:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

Gi·ªù th√¨ t√¨m c·∫∑p c√≥ ƒëi·ªÉm cao nh·∫•t ch·ªâ c·∫ßn m·ªôt v√≤ng l·∫Øp nhanh:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

V·∫≠y quy t·∫Øc h·ª£p nh·∫•t ƒë·∫ßu ti√™n l√† `('a', '##b') -> 'ab'`, v√† ta th√™m `'ab'` v√†o b·ªô t·ª´ v·ª±ng:

```python
vocab.append("ab")
```

Ti·∫øp theo, ta c·∫ßn √°p d·ª•ng h·ª£p nh·∫•t trong t·ª´ ƒëi·ªÉn `splits`, H√£y c√πng vi·∫øt m·ªôt h√†m cho n√≥:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

V√† ta c√≥ th·ªÉ th·∫•y k·∫øt qu·∫£ l·∫ßn h·ª£p nh·∫•t ƒë·∫ßu ti·ªán:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

Gi·ªù th√¨ ta ƒë√£ c√≥ t·∫•t c·∫£ nh·ªØng g√¨ ta c·∫ßn ƒë·ªÉ l·∫∑p cho ƒë·∫øn khi h·ªçc h·∫øt t·∫•t c·∫£ c√°c h·ª£p nh·∫•t ta mu·ªën. H√£y c≈©ng h∆∞·ªõng t·ªõi b·ªô t·ª´ v·ª±ng c√≥ k√≠ch th∆∞·ªõc l√† 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

Gi·ªù ta c√≥ th·ªÉ nh√¨n v√†o b·ªô t·ª´ ƒëi·ªÉn ƒë∆∞·ª£c t·∫°o ra:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

Nh∆∞ c√≥ th·ªÉ th·∫•y, so v·ªõi BPE, tokenizer n√†y h·ªçc c√°c ph·∫ßn c·ªßa t·ª´ nh∆∞ l√† token nhanh h∆°n m·ªôt ch√∫t.

> [!TIP]
> üí° S·ª≠ d·ª•ng `train_new_from_iterator()` tr√™n c√πng kho ng·ªØ li·ªáu s·∫Ω kh√¥ng mang v·ªÅ k·∫øt qu·∫£ kho ng·ªØ li·ªáu y h·ªát. ƒê√≥ l√† b·ªüi th∆∞ vi·ªán ü§ó Tokenizers kh√¥ng tri·ªÉn khai WordPiece cho hu·∫•n luy·ªán (v√¨ ch√∫ng ta kh√¥ng ho√†n to√†n n·∫±m r√µ b√™n trong), v√† s·ª≠ d·ª•ng BPE thay v√†o ƒë√≥.

ƒê·ªÉ tokenize nh·ªØng ƒëo·∫°n vƒÉn m·ªõi, ta ti·ªÅn tokenize n√≥, chia nh·ªè v√† √°p d·ª•ng thu·∫≠t to√°n tokenize cho t·ª´ng t·ª´. V·∫≠y ƒë√≥, ch√∫ng ta nh√¨n v√†o c·ª•m t·ª´ con d√†i nh·∫•t b·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu t·ª´ ƒë·∫ßu ti√™n v√† chia nh·ªè n√≥, sau ƒë√≥ l·∫∑p l·∫°i quy tr√¨nh v·ªõi ph·∫ßn th·ª© hai, v√† ti·∫øp t·ª•c cho ƒë·∫øn h·∫øt t·ª´ v√† c√°c t·ª´ ti·∫øp theo trong vƒÉn b·∫£n: 

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

H√£y c≈©ng ki·ªÉm tra tr√™n m·ªôt t·ª´ c√≥ tronng v√† kh√¥ng c√≥ trong b·ªô t·ª´ v·ª±ng:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

Gi·ªù h√£y c√πng vi·∫øt m·ªôt h√†m tokenize vƒÉn b·∫£n:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

Ta c√≥ th·ªÉ th·ª≠ tr√™n b·∫•t k√¨ vƒÉn b·∫£n n√†o:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

ƒê√≥ l√† nh·ªØng g√¨ ta c·∫ßn bi·∫øt v·ªÅ thu·∫≠t to√°n WordPiece! Ti·∫øp theo, ch√∫ng ta s·∫Ω c√πng t√¨m hi·ªÉu v·ªÅ Unigram.
