<FrameworkSwitchCourse {fw} />

# Tinh chá»‰nh má»™t mÃ´ hÃ¬nh vá»›i Keras

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter3/section3_tf.ipynb"},
]} />

Khi báº¡n Ä‘Ã£ hoÃ n thÃ nh táº¥t cáº£ cÃ´ng viá»‡c tiá»n xá»­ lÃ½ dá»¯ liá»‡u trong pháº§n trÆ°á»›c, báº¡n chá»‰ cÃ²n má»™t vÃ i bÆ°á»›c ná»¯a Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. Tuy nhiÃªn, lÆ°u Ã½ ráº±ng lá»‡nh `model.fit()` sáº½ cháº¡y ráº¥t cháº­m trÃªn CPU. Náº¿u báº¡n chÆ°a thiáº¿t láº­p GPU, báº¡n cÃ³ thá»ƒ cÃ³ quyá»n truy cáº­p vÃ o GPU hoáº·c TPU miá»…n phÃ­ trÃªn [Google Colab](https://colab.research.google.com/).

CÃ¡c Ä‘oáº¡n mÃ£ vÃ­ dá»¥ bÃªn dÆ°á»›i giáº£ sá»­ báº¡n Ä‘Ã£ thá»±c thi cÃ¡c vÃ­ dá»¥ trong pháº§n trÆ°á»›c. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t báº£n tÃ³m táº¯t ngáº¯n gá»n tÃ³m táº¯t láº¡i nhá»¯ng gÃ¬ báº¡n cáº§n:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### Huáº¥n luyá»‡n

CÃ¡c mÃ´ hÃ¬nh TensorFlow nháº­p tá»« ğŸ¤— Transformers vá»‘n lÃ  cÃ¡c mÃ´ hÃ¬nh Keras. ÄÃ¢y lÃ  pháº§n giá»›i thiá»‡u ngáº¯n vá» Keras.

<Youtube id="rnTGBy2ax1c"/>

Äiá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  má»™t khi chÃºng tÃ´i cÃ³ dá»¯ liá»‡u riÃªng mÃ¬nh, chÃºng ta chá»‰ cáº§n thao tÃ¡c Ã­t bÆ°á»›c ná»¯a thÃ´i Ä‘á»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n.

<Youtube id="AUozVp78dhk"/>

NhÆ° trong [chÆ°Æ¡ng trÆ°á»›c](/course/chapter2), chÃºng ta sáº½ sá»­ dá»¥ng lá»›p `TFAutoModelForSequenceClassification`, vá»›i hai nhÃ£n:

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Báº¡n sáº½ nháº­n tháº¥y ráº±ng khÃ´ng nhÆ° trong [ChÆ°Æ¡ng 2](/course/chapter2), báº¡n nháº­n Ä‘Æ°á»£c má»™t cáº£nh bÃ¡o sau khi khá»Ÿi táº¡o mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c nÃ y. ÄÃ¢y lÃ  do BERT chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá» phÃ¢n loáº¡i cÃ¡c cáº·p cÃ¢u, vÃ¬ váº­y pháº§n Ä‘áº§u cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã£ bá»‹ loáº¡i bá» vÃ  pháº§n Ä‘áº§u má»›i phÃ¹ há»£p Ä‘á»ƒ phÃ¢n loáº¡i chuá»—i Ä‘Ã£ Ä‘Æ°á»£c chÃ¨n vÃ o thay tháº¿. CÃ¡c cáº£nh bÃ¡o chá»‰ ra ráº±ng má»™t sá»‘ trá»ng sá»‘ Ä‘Ã£ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng (nhá»¯ng trá»ng sá»‘ tÆ°Æ¡ng á»©ng vá»›i Ä‘áº§u huáº¥n luyá»‡n trÆ°á»›c bá»‹ rá»¥ng) vÃ  má»™t sá»‘ trá»ng sá»‘ khÃ¡c khÃ¡c Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn (nhá»¯ng trá»ng sá»‘ dÃ nh cho Ä‘áº§u má»›i). NÃ³ káº¿t thÃºc báº±ng cÃ¡ch khuyáº¿n khÃ­ch báº¡n huáº¥n luyá»‡n mÃ´ hÃ¬nh, Ä‘Ã³ chÃ­nh xÃ¡c lÃ  nhá»¯ng gÃ¬ chÃºng ta sáº½ lÃ m bÃ¢y giá».

Äá»ƒ tinh chá»‰nh mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u cá»§a mÃ¬nh, chÃºng ta chá»‰ cáº§n `compile()` mÃ´ hÃ¬nh vÃ  sau Ä‘Ã³ chuyá»ƒn dá»¯ liá»‡u cá»§a ta Ä‘áº¿n phÆ°Æ¡ng thá»©c `fit()`. Thao tÃ¡c nÃ y sáº½ báº¯t Ä‘áº§u quÃ¡ trÃ¬nh tinh chá»‰nh (sáº½ máº¥t vÃ i phÃºt trÃªn GPU) vÃ  bÃ¡o cÃ¡o sá»± máº¥t mÃ¡t á»Ÿ táº­p huáº¥n luyá»‡n khi nÃ³ diá»…n ra, cá»™ng vá»›i máº¥t mÃ¡t á»Ÿ táº­p kiá»ƒm Ä‘á»‹nh á»Ÿ cuá»‘i má»—i epoch.

<Tip>

LÆ°u Ã½ ráº±ng ğŸ¤— cÃ¡c mÃ´ hÃ¬nh Transformers cÃ³ má»™t kháº£ nÄƒng Ä‘áº·c biá»‡t mÃ  háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Keras khÃ´ng cÃ³ - chÃºng cÃ³ thá»ƒ tá»± Ä‘á»™ng sá»­ dá»¥ng má»™t lÆ°á»£ng máº¥t mÃ¡t thÃ­ch há»£p mÃ  chÃºng tÃ­nh toÃ¡n bÃªn trong. ChÃºng sáº½ sá»­ dá»¥ng sá»± máº¥t mÃ¡t nÃ y theo máº·c Ä‘á»‹nh náº¿u báº¡n khÃ´ng Ä‘áº·t tham sá»‘ máº¥t mÃ¡t bÃªn trong `compile()`. LÆ°u Ã½ ráº±ng Ä‘á»ƒ sá»­ dá»¥ng hÃ m máº¥t mÃ¡t trong ná»™i bá»™, báº¡n sáº½ cáº§n truyá»n cÃ¡c nhÃ£n cá»§a mÃ¬nh nhÆ° má»™t pháº§n cá»§a Ä‘áº§u vÃ o, khÃ´ng pháº£i dÆ°á»›i dáº¡ng nhÃ£n riÃªng biá»‡t, Ä‘Ã¢y lÃ  cÃ¡ch thÃ´ng thÆ°á»ng Ä‘á»ƒ sá»­ dá»¥ng nhÃ£n vá»›i cÃ¡c mÃ´ hÃ¬nh Keras. Báº¡n sáº½ tháº¥y cÃ¡c vÃ­ dá»¥ vá» Ä‘iá»u nÃ y trong Pháº§n 2 cá»§a khÃ³a há»c, trong Ä‘Ã³ viá»‡c xÃ¡c Ä‘á»‹nh hÃ m máº¥t mÃ¡t chÃ­nh xÃ¡c cÃ³ thá»ƒ khÃ³ khÄƒn. Tuy nhiÃªn, Ä‘á»‘i vá»›i phÃ¢n loáº¡i chuá»—i, má»™t hÃ m máº¥t mÃ¡t Keras tiÃªu chuáº©n hoáº¡t Ä‘á»™ng khÃ¡ tá»‘t, vÃ¬ váº­y Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ chÃºng ta sáº½ sá»­ dá»¥ng á»Ÿ Ä‘Ã¢y.

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

LÆ°u Ã½ má»™t lá»—i ráº¥t phá»• biáº¿n á»Ÿ Ä‘Ã¢y - báº¡n *cÃ³ thá»ƒ* chá»‰ cáº§n truyá»n tÃªn cá»§a hÃ m máº¥t mÃ¡t dÆ°á»›i dáº¡ng chuá»—i cho Keras, nhÆ°ng theo máº·c Ä‘á»‹nh, Keras sáº½ cho ráº±ng báº¡n Ä‘Ã£ Ã¡p dá»¥ng softmax cho Ä‘áº§u ra cá»§a mÃ¬nh. Tuy nhiÃªn, nhiá»u mÃ´ hÃ¬nh xuáº¥t ra cÃ¡c giÃ¡ trá»‹ ngay trÆ°á»›c khi Ã¡p dá»¥ng softmax, cÃ²n Ä‘Æ°á»£c gá»i lÃ  *logit*. ChÃºng ta cáº§n nÃ³i vá»›i hÃ m máº¥t mÃ¡t ráº±ng Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ mÃ´ hÃ¬nh cá»§a chÃºng ta lÃ m vÃ  cÃ¡ch duy nháº¥t Ä‘á»ƒ lÃ m Ä‘iá»u Ä‘Ã³ lÃ  gá»i nÃ³ trá»±c tiáº¿p, thay vÃ¬ Ä‘áº·t tÃªn báº±ng má»™t chuá»—i.

</Tip>

### Cáº£i thiá»‡n hiá»‡u suáº¥t huáº¥n luyá»‡n

<Youtube id="cpzq6ESSM5c"/>

Náº¿u báº¡n thá»­ Ä‘oáº¡n mÃ£ trÃªn, nÃ³ cháº¯c cháº¯n cháº¡y, nhÆ°ng báº¡n sáº½ tháº¥y ráº±ng hÃ m máº¥t mÃ¡t chá»‰ giáº£m tá»« tá»« hoáº·c khÃ´ng thÆ°á»ng xuyÃªn. NguyÃªn nhÃ¢n chÃ­nh lÃ  do *learning rate* hay *tá»‘c Ä‘á»™ há»c*. Vá»›i hÃ m máº¥t mÃ¡t, khi ta truyá»n cho Keras tÃªn cá»§a trÃ¬nh tá»‘i Æ°u hÃ³a dÆ°á»›i dáº¡ng má»™t chuá»—i, Keras sáº½ khá»Ÿi táº¡o trÃ¬nh tá»‘i Æ°u hÃ³a Ä‘Ã³ vá»›i cÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh cho táº¥t cáº£ cÃ¡c tham sá»‘, bao gá»“m cáº£ tá»‘c Ä‘á»™ há»c. Tuy nhiÃªn, tá»« kinh nghiá»‡m lÃ¢u nÄƒm, chÃºng tÃ´i biáº¿t
ráº±ng cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« tá»‘c Ä‘á»™ há»c tháº¥p hÆ¡n nhiá»u so vá»›i tá»· lá»‡ máº·c Ä‘á»‹nh cho Adam, lÃ  1e-3, cÅ©ng Ä‘Æ°á»£c viáº¿t báº±ng 10 lÅ©y thá»«a cá»§a -3, hoáº·c 0,001. 5e-5 (0,00005), tháº¥p hÆ¡n khoáº£ng hai mÆ°Æ¡i láº§n, lÃ  má»™t Ä‘iá»ƒm khá»Ÿi Ä‘áº§u tá»‘t hÆ¡n nhiá»u.

NgoÃ i viá»‡c giáº£m tá»‘c Ä‘á»™ há»c, chÃºng tÃ´i cÃ³ má»™t máº¹o thá»© hai: Ta cÃ³ thá»ƒ tá»« tá»« giáº£m tá»‘c Ä‘á»™ há»c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Trong tÃ i liá»‡u, Ä‘Ã´i khi báº¡n sáº½ tháº¥y Ä‘iá»u nÃ y Ä‘Æ°á»£c gá»i lÃ  *phÃ¢n rÃ£* hoáº·c *á»§* tá»‘c Ä‘á»™ há»c. á» Keras, cÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y lÃ  sá»­ dá»¥ng *learning rate scheduler* hay *cÃ´ng cá»¥ láº­p lá»‹ch trÃ¬nh tá»‘c Ä‘á»™ há»c*. Má»™t cÃ¡i hay Ä‘á»ƒ sá»­ dá»¥ng lÃ  `PolynomialDecay` - vá»›i cÃ i Ä‘áº·t máº·c Ä‘á»‹nh, nÃ³ chá»‰ Ä‘Æ¡n giáº£n lÃ  giáº£m Ä‘á»™ tuyáº¿n tÃ­nh tá»‘c Ä‘á»™ há»c tá»« giÃ¡ trá»‹ ban Ä‘áº§u Ä‘áº¿n giÃ¡ trá»‹ cuá»‘i cÃ¹ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, Ä‘Ã³ chÃ­nh xÃ¡c lÃ  nhá»¯ng gÃ¬ ta muá»‘n. Tuy nhiÃªn, Ä‘á»ƒ sá»­ dá»¥ng bá»™ láº­p lá»‹ch má»™t cÃ¡ch chÃ­nh xÃ¡c, chÃºng ta cáº§n cho nÃ³ biáº¿t thá»i gian huáº¥n luyá»‡n sáº½ kÃ©o dÃ i. ChÃºng ta tÃ­nh giÃ¡ trá»‹ Ä‘Ã³ dÆ°á»›i dáº¡ng `num_train_steps` nhÆ° sau.

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# Sá»‘ bÆ°á»›c huáº¥n luyá»‡n lÃ  sá»‘ lÆ°á»£ng máº«u trong táº­p dá»¯ liá»‡u, chia cho kÃ­ch thÆ°á»›c lÃ´ sau Ä‘Ã³ nhÃ¢n
# vá»›i tá»•ng sá»‘ epoch. LÆ°u Ã½ ráº±ng tf_train_dataset á»Ÿ Ä‘Ã¢y lÃ  tf.data.Dataset theo lÃ´,
# khÃ´ng pháº£i lÃ  Hugging Face Dataset, vÃ¬ váº­y len() cá»§a nÃ³ Ä‘Ã£ lÃ  num_samples // batch_size.
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

ThÆ° viá»‡n ğŸ¤— Transformers cÅ©ng cÃ³ má»™t hÃ m `create_optimizer()` sáº½ táº¡o ra má»™t trÃ¬nh tá»‘i Æ°u hÃ³a `AdamW` vá»›i sá»± giáº£m tá»‘c Ä‘á»™ há»c. ÄÃ¢y lÃ  má»™t phÃ­m táº¯t thuáº­n tiá»‡n mÃ  báº¡n sáº½ tháº¥y chi tiáº¿t trong cÃ¡c pháº§n sau cá»§a khÃ³a há»c.

</Tip>

BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ trÃ¬nh tá»‘i Æ°u hÃ³a hoÃ n toÃ n má»›i vÃ  ta cÃ³ thá»ƒ thá»­ huáº¥n luyá»‡n vá»›i nÃ³. Äáº§u tiÃªn, hÃ£y táº£i láº¡i mÃ´ hÃ¬nh, Ä‘á»ƒ Ä‘áº·t láº¡i cÃ¡c thay Ä‘á»•i Ä‘á»‘i vá»›i trá»ng sá»‘ tá»« láº§n cháº¡y huáº¥n luyá»‡n mÃ  chÃºng ta vá»«a thá»±c hiá»‡n vÃ  sau Ä‘Ã³ ta cÃ³ thá»ƒ biÃªn dá»‹ch nÃ³ báº±ng trÃ¬nh tá»‘i Æ°u hÃ³a má»›i:

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

Giá» ta sáº½ fit láº¡i 1 láº§n ná»¯a:

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

ğŸ’¡ Náº¿u báº¡n muá»‘n tá»± Ä‘á»™ng táº£i mÃ´ hÃ¬nh cá»§a mÃ¬nh lÃªn Hub trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, báº¡n cÃ³ thá»ƒ truyá»n `PushToHubCallback` vÃ o trong phÆ°Æ¡ng thá»©c `model.fit()`. ChÃºng ta sáº½ tÃ¬m hiá»ƒu thÃªm vá» Ä‘iá»u nÃ y trong [ChÆ°Æ¡ng 4](/course/chapter4/3)

</Tip>

### CÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh

<Youtube id="nx10eh4CoOs"/>

Viá»‡c huáº¥n luyá»‡n vÃ  theo dÃµi sá»± máº¥t mÃ¡t giáº£m xuá»‘ng Ä‘á»u ráº¥t tá»‘t, nhÆ°ng náº¿u chÃºng ta muá»‘n thá»±c sá»± cÃ³ Ä‘Æ°á»£c káº¿t quáº£ Ä‘áº§u ra tá»« mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n, Ä‘á»ƒ tÃ­nh toÃ¡n má»™t sá»‘ chá»‰ sá»‘ hoáº·c sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Ã³ trong sáº£n xuáº¥t thÃ¬ sao? Äá»ƒ lÃ m Ä‘iá»u Ä‘Ã³, chÃºng ta chá»‰ cÃ³ thá»ƒ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `predict()`. Äiá»u nÃ y sáº½ tráº£ vá» *logit* tá»« Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh, má»™t cho má»—i lá»›p.

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

ChÃºng ta cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c logit nÃ y thÃ nh cÃ¡c dá»± Ä‘oÃ¡n lá»›p cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng `argmax` Ä‘á»ƒ tÃ¬m logit cao nháº¥t, tÆ°Æ¡ng á»©ng vá»›i lá»›p cÃ³ nhiá»u kháº£ nÄƒng nháº¥t:

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

BÃ¢y giá», hÃ£y sá»­ dá»¥ng cÃ¡c `preds` Ä‘Ã³ Ä‘á»ƒ tÃ­nh toÃ¡n má»™t sá»‘ chá»‰ sá»‘! ChÃºng ta cÃ³ thá»ƒ táº£i cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c liÃªn káº¿t vá»›i táº­p dá»¯ liá»‡u MRPC dá»… dÃ ng nhÆ° khi ta táº£i táº­p dá»¯ liá»‡u, láº§n nÃ y lÃ  vá»›i hÃ m `eval.load())`. Äá»‘i tÆ°á»£ng Ä‘Æ°á»£c tráº£ vá» cÃ³ phÆ°Æ¡ng thá»©c `compute()` mÃ  chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ thá»±c hiá»‡n phÃ©p tÃ­nh sá»‘ liá»‡u:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Káº¿t quáº£ chÃ­nh xÃ¡c báº¡n nháº­n Ä‘Æ°á»£c cÃ³ thá»ƒ khÃ¡c nhau, vÃ¬ viá»‡c khá»Ÿi táº¡o ngáº«u nhiÃªn pháº§n Ä‘áº§u mÃ´ hÃ¬nh cÃ³ thá»ƒ thay Ä‘á»•i cÃ¡c chá»‰ sá»‘ mÃ  nÃ³ Ä‘áº¡t Ä‘Æ°á»£c. á» Ä‘Ã¢y, chÃºng ta cÃ³ thá»ƒ tháº¥y mÃ´ hÃ¬nh cÃ³ Ä‘á»™ chÃ­nh xÃ¡c 85.78% trÃªn táº­p kiá»ƒm Ä‘á»‹nh vÃ  Ä‘iá»ƒm F1 lÃ  89.97. ÄÃ³ lÃ  hai chá»‰ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ káº¿t quáº£ trÃªn táº­p dá»¯ liá»‡u MRPC theo Ä‘iá»ƒm chuáº©n GLUE. Báº£ng trong [bÃ i bÃ¡o BERT](https://arxiv.org/pdf/1810.04805.pdf) bÃ¡o cÃ¡o Ä‘iá»ƒm F1 lÃ  88.9 cho mÃ´ hÃ¬nh cÆ¡ sá»Ÿ. ÄÃ³ lÃ  mÃ´ hÃ¬nh `khÃ´ng phÃ¢n biá»‡t` viáº¿t hoa viáº¿t thÆ°á»ng trong khi chÃºng ta hiá»‡n Ä‘ang sá»­ dá»¥ng mÃ´ hÃ¬nh `cÃ³ phÃ¢n biá»‡t`, Ä‘iá»u nÃ y giáº£i thÃ­ch káº¿t quáº£ tá»‘t hÆ¡n.

Pháº§n nÃ y káº¿t thÃºc pháº§n giá»›i thiá»‡u vá» cÃ¡ch tinh chá»‰nh báº±ng Keras API. Má»™t vÃ­ dá»¥ vá» cÃ¡ch lÃ m nÃ y Ä‘á»‘i vá»›i háº§u háº¿t cÃ¡c tÃ¡c vá»¥ NLP phá»• biáº¿n sáº½ Ä‘Æ°á»£c Ä‘Æ°a ra trong [ChÆ°Æ¡ng 7](/course/chapter7). Náº¿u báº¡n muá»‘n trau dá»“i ká»¹ nÄƒng cá»§a mÃ¬nh trÃªn API Keras, hÃ£y cá»‘ gáº¯ng tinh chá»‰nh má»™t mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u GLUE SST-2, báº±ng cÃ¡ch sá»­ dá»¥ng xá»­ lÃ½ dá»¯ liá»‡u báº¡n Ä‘Ã£ thá»±c hiá»‡n trong pháº§n 2.
