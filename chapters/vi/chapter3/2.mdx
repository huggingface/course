<FrameworkSwitchCourse {fw} />

# Xá»­ lÃ½ dá»¯ liá»‡u

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
Tiáº¿p tá»¥c vá»›i vÃ­ dá»¥ tá»« [chÆ°Æ¡ng trÆ°á»›c](/course/chapter2), Ä‘Ã¢y lÃ  cÃ¡ch chÃºng ta sáº½ huáº¥n luyá»‡n má»™t bá»™ phÃ¢n loáº¡i chuá»—i trÃªn má»™t lÃ´ trong PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# TÆ°Æ¡ng tá»± nhÆ° vÃ­ dá»¥ trÆ°á»›c
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# ÄÃ¢y lÃ  pháº§n má»›i
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Tiáº¿p tá»¥c vá»›i vÃ­ dá»¥ tá»« [chÆ°Æ¡ng trÆ°á»›c](/course/chapter2), Ä‘Ã¢y lÃ  cÃ¡ch chÃºng ta sáº½ huáº¥n luyá»‡n má»™t bá»™ phÃ¢n loáº¡i chuá»—i trÃªn má»™t lÃ´ trong TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# TÆ°Æ¡ng tá»± nhÆ° vÃ­ dá»¥ trÆ°á»›c
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# ÄÃ¢y lÃ  pháº§n má»›i
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

Táº¥t nhiÃªn, chá»‰ huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn hai cÃ¢u sáº½ khÃ´ng mang láº¡i káº¿t quáº£ tá»‘t. Äá»ƒ cÃ³ Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n, báº¡n sáº½ cáº§n chuáº©n bá»‹ má»™t bá»™ dá»¯ liá»‡u lá»›n hÆ¡n.

Trong pháº§n nÃ y, chÃºng tÃ´i sáº½ sá»­ dá»¥ng táº­p dá»¯ liá»‡u MRPC (Microsoft Research Paraphrase Corpus) lÃ m vÃ­ dá»¥, Ä‘Æ°á»£c giá»›i thiá»‡u trong [bÃ i bÃ¡o](https://www.aclweb.org/anthology/I05-5002.pdf) cá»§a William B. Dolan vÃ  Chris Brockett. Táº­p dá»¯ liá»‡u bao gá»“m 5,801 cáº·p cÃ¢u, vá»›i nhÃ£n cho biáº¿t chÃºng cÃ³ pháº£i lÃ  cÃ¢u diá»…n giáº£i hay khÃ´ng (tá»©c lÃ  náº¿u cáº£ hai cÃ¢u Ä‘á»u cÃ³ nghÄ©a giá»‘ng nhau). ChÃºng tÃ´i Ä‘Ã£ chá»n nÃ³ cho chÆ°Æ¡ng nÃ y vÃ¬ nÃ³ lÃ  má»™t táº­p dá»¯ liá»‡u nhá», vÃ¬ váº­y tháº­t dá»… dÃ ng Ä‘á»ƒ thá»­ nghiá»‡m vá»›i viá»‡c huáº¥n luyá»‡n vá» nÃ³.

### Táº£i bá»™ dá»¯ liá»‡u tá»« Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub khÃ´ng chá»‰ chá»©a cÃ¡c mÃ´ hÃ¬nh; nÃ³ cÅ©ng cÃ³ nhiá»u bá»™ dá»¯ liá»‡u nhiá»u ngÃ´n ngá»¯ khÃ¡c nhau. Báº¡n cÃ³ thá»ƒ xem qua táº­p dá»¯ liá»‡u [táº¡i Ä‘Ã¢y](https://huggingface.co/datasets) vÃ  chÃºng tÃ´i khuyÃªn báº¡n nÃªn thá»­ táº£i vÃ  xá»­ lÃ½ bá»™ dá»¯ liá»‡u má»›i khi báº¡n Ä‘Ã£ xem qua pháº§n nÃ y (xem tÃ i liá»‡u chung [táº¡i Ä‘Ã¢y](https://huggingface.co/docs/datasets/loading)). NhÆ°ng hiá»‡n táº¡i, hÃ£y táº­p trung vÃ o bá»™ dá»¯ liá»‡u MRPC! ÄÃ¢y lÃ  má»™t trong 10 bá»™ dá»¯ liá»‡u táº¡o nÃªn [bá»™ chuáº©n GLUE](https://gluebenchmark.com/), lÃ  má»™t Ä‘iá»ƒm chuáº©n há»c thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh ML trÃªn 10 tÃ¡c vá»¥ phÃ¢n loáº¡i vÄƒn báº£n khÃ¡c nhau.

ThÆ° viá»‡n ğŸ¤— Datasets cung cáº¥p má»™t lá»‡nh ráº¥t Ä‘Æ¡n giáº£n Ä‘á»ƒ táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› cache má»™t táº­p dá»¯ liá»‡u trÃªn Hub. ChÃºng ta cÃ³ thá»ƒ táº£i xuá»‘ng bá»™ dá»¯ liá»‡u MRPC nhÆ° sau:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, chÃºng ta nháº­n Ä‘Æ°á»£c má»™t Ä‘á»‘i tÆ°á»£ng `DatasetDict` chá»©a táº­p huáº¥n luyá»‡n, táº­p kiá»ƒm Ä‘á»‹nh vÃ  táº­p kiá»ƒm thá»­. Má»—i táº­p chá»©a má»™t sá»‘ cá»™t (`sentence1`, `sentence2`, `label`, vÃ  `idx`) vÃ  má»™t sá»‘ hÃ ng thay Ä‘á»•i, lÃ  sá»‘ pháº§n tá»­ trong má»—i táº­p (vÃ¬ váº­y, cÃ³ 3,668 cáº·p cÃ¢u trong táº­p huáº¥n luyá»‡n, 408 trong táº­p kiá»ƒm chá»©ng vÃ  1,725 trong táº­p kiá»ƒm Ä‘á»‹nh).

Lá»‡nh nÃ y táº£i xuá»‘ng vÃ  lÆ°u vÃ o bá»™ nhá»› cache cÃ¡c táº­p dá»¯ liá»‡u, máº·c Ä‘á»‹nh lÆ°u trong *~/.cache/huggingface/datasets*. Nhá»› láº¡i tá»« ChÆ°Æ¡ng 2 ráº±ng báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh thÆ° má»¥c bá»™ nhá»› cache cá»§a mÃ¬nh báº±ng cÃ¡ch Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng `HF_HOME`.

ChÃºng ta cÃ³ thá»ƒ truy cáº­p tá»«ng cáº·p cÃ¢u trong Ä‘á»‘i tÆ°á»£ng `raw_datasets` cá»§a mÃ¬nh báº±ng cÃ¡ch láº­p chá»‰ má»¥c, giá»‘ng nhÆ° vá»›i tá»« Ä‘iá»ƒn:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y cÃ¡c nhÃ£n vá»‘n lÃ  sá»‘ nguyÃªn, vÃ¬ váº­y chÃºng ta khÃ´ng pháº£i thá»±c hiá»‡n báº¥t ká»³ bÆ°á»›c xá»­ lÃ½ trÆ°á»›c nÃ o á»Ÿ Ä‘Ã³. Äá»ƒ biáº¿t sá»‘ nguyÃªn nÃ o tÆ°Æ¡ng á»©ng vá»›i nhÃ£n nÃ o, chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra `features` cá»§a `raw_train_dataset`. Äiá»u nÃ y sáº½ cho chÃºng tÃ´i biáº¿t loáº¡i cá»§a má»—i cá»™t:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

PhÃ­a sau, `label` thuá»™c loáº¡i `ClassLabel` vÃ  Ã¡nh xáº¡ cÃ¡c sá»‘ nguyÃªn thÃ nh tÃªn nhÃ£n Ä‘Æ°á»£c lÆ°u trá»¯ trong thÆ° má»¥c *names*. `0` tÆ°Æ¡ng á»©ng vá»›i `khÃ´ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng`, vÃ  `1` tÆ°Æ¡ng á»©ng vá»›i `tÆ°Æ¡ng Ä‘Æ°Æ¡ng`.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** NhÃ¬n vÃ o pháº§n tá»­ thá»© 15 cá»§a táº­p huáº¥n luyá»‡n vÃ  pháº§n tá»­ 87 cá»§a táº­p kiá»ƒm Ä‘á»‹nh. NhÃ£n cá»§a chÃºng lÃ  gÃ¬?

</Tip>

### Tiá»n xá»­ lÃ½ má»™t bá»™ dá»¯ liá»‡u

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

Äá»ƒ tiá»n xá»­ lÃ½ bá»™ dá»¯ liá»‡u, chÃºng ta cáº§n chuyá»ƒn vÄƒn báº£n thÃ nh cÃ¡c sá»‘ mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c. NhÆ° báº¡n Ä‘Ã£ tháº¥y trong [chÆ°Æ¡ng trÆ°á»›c](/course/chapter2), Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i má»™t tokenizer. ChÃºng ta cÃ³ thá»ƒ cung cáº¥p cho tokenizer má»™t cÃ¢u hoáº·c má»™t danh sÃ¡ch cÃ¡c cÃ¢u, vÃ¬ váº­y chÃºng ta cÃ³ thá»ƒ tokenizer trá»±c tiáº¿p táº¥t cáº£ cÃ¡c cÃ¢u Ä‘áº§u tiÃªn vÃ  táº¥t cáº£ cÃ¡c cÃ¢u thá»© hai cá»§a má»—i cáº·p nhÆ° sau:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Tuy nhiÃªn, chÃºng ta khÃ´ng thá»ƒ chá»‰ chuyá»ƒn hai chuá»—i vÃ o mÃ´ hÃ¬nh vÃ  nháº­n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n liá»‡u hai cÃ¢u cÃ³ pháº£i lÃ  diá»…n giáº£i hay khÃ´ng. ChÃºng ta cáº§n xá»­ lÃ½ hai chuá»—i nhÆ° má»™t cáº·p vÃ  Ã¡p dá»¥ng tiá»n xá»­ lÃ½ thÃ­ch há»£p. May máº¯n thay, tokenizer cÅ©ng cÃ³ thá»ƒ nháº­n má»™t cáº·p chuá»—i vÃ  chuáº©n bá»‹ nÃ³ theo cÃ¡ch mÃ  mÃ´ hÃ¬nh BERT cá»§a ta mong Ä‘á»£i:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

ChÃºng ta Ä‘Ã£ tháº£o luáº­n vá» `input_ids` vÃ  `attention_mask` trong [ChÆ°Æ¡ng 2](/course/chapter2), nhÆ°ng chÃºng ta táº¡m dá»«ng Ä‘á»ƒ nÃ³i vá» `token_type_ids`. Trong vÃ­ dá»¥ nÃ y, Ä‘Ã¢y lÃ  pháº§n cho mÃ´ hÃ¬nh biáº¿t pháº§n nÃ o cá»§a Ä‘áº§u vÃ o lÃ  cÃ¢u Ä‘áº§u tiÃªn vÃ  pháº§n nÃ o lÃ  cÃ¢u thá»© hai.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Láº¥y pháº§n tá»­ 15 cá»§a táº­p huáº¥n luyá»‡n vÃ  tokenize hai cÃ¢u riÃªng biá»‡t vÃ  nhÆ° má»™t cáº·p. Sá»± khÃ¡c biá»‡t giá»¯a hai káº¿t quáº£ lÃ  gÃ¬?

</Tip>

Náº¿u chÃºng ta giáº£i mÃ£ cÃ¡c ID bÃªn trong `input_ids` trá»Ÿ láº¡i cÃ¡c tá»«:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ta sáº½ nháº­n Ä‘Æ°á»£c:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

CÃ³ thá»ƒ tháº¥y mÃ´ hÃ¬nh kÃ¬ vá»ng cÃ¡c Ä‘áº§u vÃ o cÃ³ dáº¡ng `[CLS] cÃ¢u1 [SEP] cÃ¢u2 [SEP]` khi cÃ³ hai cÃ¢u. CÄƒn chá»‰nh Ä‘iá»u nÃ y vá»›i `token_type_ids` cho ta káº¿t quáº£:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, cÃ¡c pháº§n cá»§a Ä‘áº§u vÃ o tÆ°Æ¡ng á»©ng vá»›i `[CLS] cÃ¢u1 [SEP]` Ä‘á»u cÃ³ loáº¡i token ID lÃ  `0`, trong khi cÃ¡c pháº§n khÃ¡c, tÆ°Æ¡ng á»©ng vá»›i `cÃ¢u2 [SEP]`, táº¥t cáº£ Ä‘á»u cÃ³ loáº¡i token ID lÃ  `1`.

LÆ°u Ã½ ráº±ng náº¿u báº¡n chá»n má»™t checkpoint khÃ¡c, báº¡n sáº½ khÃ´ng nháº¥t thiáº¿t pháº£i cÃ³ `token_type_ids` trong Ä‘áº§u vÃ o Ä‘Æ°á»£c tokenize cá»§a mÃ¬nh (vÃ­ dá»¥: chÃºng sáº½ khÃ´ng Ä‘Æ°á»£c tráº£ láº¡i náº¿u báº¡n sá»­ dá»¥ng mÃ´ hÃ¬nh DistilBERT). ChÃºng chá»‰ Ä‘Æ°á»£c tráº£ láº¡i khi mÃ´ hÃ¬nh biáº¿t pháº£i lÃ m gÃ¬ vá»›i chÃºng, bá»Ÿi vÃ¬ nÃ³ Ä‘Ã£ nhÃ¬n tháº¥y chÃºng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c.

á» Ä‘Ã¢y, BERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vá»›i cÃ¡c token ID vÃ  trÃªn Ä‘áº§u má»¥c tiÃªu mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c che mÃ  chÃºng ta Ä‘Ã£ Ä‘á» cáº­p trong [ChÆ°Æ¡ng 1](/course/chapter1), nÃ³ cÃ³ má»™t má»¥c tiÃªu bá»• sung Ä‘Æ°á»£c gá»i lÃ  _dá»± Ä‘oÃ¡n cÃ¢u tiáº¿p theo_. Má»¥c tiÃªu cá»§a tÃ¡c vá»¥ nÃ y lÃ  mÃ´ hÃ¬nh hÃ³a má»‘i quan há»‡ giá»¯a cÃ¡c cáº·p cÃ¢u.

Vá»›i dá»± Ä‘oÃ¡n cÃ¢u tiáº¿p theo, mÃ´ hÃ¬nh Ä‘Æ°á»£c cung cáº¥p cÃ¡c cáº·p cÃ¢u (vá»›i cÃ¡c token Ä‘Æ°á»£c che ngáº«u nhiÃªn) vÃ  Ä‘Æ°á»£c yÃªu cáº§u dá»± Ä‘oÃ¡n liá»‡u cÃ¢u thá»© hai cÃ³ theo sau cÃ¢u Ä‘áº§u tiÃªn hay khÃ´ng. Äá»ƒ lÃ m cho tÃ¡c vá»¥ trá»Ÿ nÃªn khÃ´ng táº§m thÆ°á»ng, má»™t ná»­a lÃ  cÃ¡c cÃ¢u tiáº¿p ná»‘i nhau trong tÃ i liá»‡u gá»‘c mÃ  chÃºng Ä‘Æ°á»£c trÃ­ch xuáº¥t, vÃ  ná»­a cÃ²n láº¡i lÃ  hai cÃ¢u Ä‘áº¿n tá»« hai tÃ i liá»‡u khÃ¡c nhau.

NÃ³i chung, báº¡n khÃ´ng cáº§n pháº£i lo láº¯ng vá» viá»‡c cÃ³ hay khÃ´ng cÃ³ `token_type_ids` trong Ä‘áº§u vÃ o Ä‘Æ°á»£c tokenize cá»§a mÃ¬nh: miá»…n lÃ  báº¡n sá»­ dá»¥ng cÃ¹ng má»™t checkpoint cho trÃ¬nh tokenize vÃ  mÃ´ hÃ¬nh, má»i thá»© sáº½ á»•n vÃ¬ trÃ¬nh tokenize nháº­n biáº¿t cáº§n cung cáº¥p nhá»¯ng gÃ¬ vá»›i mÃ´ hÃ¬nh cá»§a nÃ³.

BÃ¢y giá» chÃºng ta Ä‘Ã£ tháº¥y cÃ¡ch trÃ¬nh tokenize cá»§a chÃºng ta cÃ³ thá»ƒ xá»­ lÃ½ má»™t cáº·p cÃ¢u, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ mÃ£ hÃ³a toÃ n bá»™ táº­p dá»¯ liá»‡u cá»§a mÃ¬nh: giá»‘ng nhÆ° trong [chÆ°Æ¡ng trÆ°á»›c](/course/chapter2), chÃºng ta cÃ³ thá»ƒ cung cáº¥p cho trÃ¬nh tokenize danh sÃ¡ch cÃ¡c cáº·p báº±ng cÃ¡ch Ä‘Æ°a cho nÃ³ danh sÃ¡ch cÃ¡c cÃ¢u Ä‘áº§u tiÃªn, sau Ä‘Ã³ lÃ  danh sÃ¡ch cÃ¡c cÃ¢u thá»© hai. Äiá»u nÃ y cÅ©ng tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c tÃ¹y chá»n Ä‘á»‡m vÃ  cáº¯t bá»›t mÃ  chÃºng ta Ä‘Ã£ tháº¥y trong [ChÆ°Æ¡ng 2](/course/chapter2). VÃ¬ váº­y, má»™t cÃ¡ch Ä‘á»ƒ tiá»n xá»­ lÃ½ trÆ°á»›c táº­p dá»¯ liá»‡u huáº¥n luyá»‡n lÃ :

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

Äiá»u nÃ y hoáº¡t Ä‘á»™ng tá»‘t, nhÆ°ng nÃ³ cÃ³ nhÆ°á»£c Ä‘iá»ƒm lÃ  tráº£ vá» tá»« Ä‘iá»ƒn (vá»›i cÃ¡c khÃ³a cá»§a chÃºng tÃ´i, `input_ids`, `attention_mask` vÃ  `token_type_ids`, vÃ  cÃ¡c giÃ¡ trá»‹ lÃ  danh sÃ¡ch cÃ¡c danh sÃ¡ch). NÃ³ cÅ©ng sáº½ chá»‰ hoáº¡t Ä‘á»™ng náº¿u báº¡n cÃ³ Ä‘á»§ RAM Ä‘á»ƒ lÆ°u trá»¯ toÃ n bá»™ táº­p dá»¯ liá»‡u cá»§a mÃ¬nh trong quÃ¡ trÃ¬nh tokenize (trong khi cÃ¡c táº­p dá»¯ liá»‡u tá»« thÆ° viá»‡n ğŸ¤— Datasets lÃ  cÃ¡c tá»‡p [Apache Arrow](https://arrow.apache.org/) Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn Ä‘Ä©a, vÃ¬ váº­y báº¡n chá»‰ giá»¯ cÃ¡c máº«u báº¡n yÃªu cáº§u Ä‘Ã£ táº£i trong bá»™ nhá»›).

Äá»ƒ giá»¯ dá»¯ liá»‡u dÆ°á»›i dáº¡ng táº­p dá»¯ liá»‡u, chÃºng ta sáº½ sá»­ dá»¥ng phÆ°Æ¡ng thá»©c [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). Äiá»u nÃ y cÅ©ng cho phÃ©p chÃºng ta linh hoáº¡t hÆ¡n, náº¿u chÃºng ta cáº§n thá»±c hiá»‡n nhiá»u tiá»n xá»­ lÃ½ hÆ¡n lÃ  chá»‰ tokenize. PhÆ°Æ¡ng thá»©c `map()` hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch Ã¡p dá»¥ng má»™t hÃ m trÃªn má»—i pháº§n tá»­ cá»§a táº­p dá»¯ liá»‡u, vÃ¬ váº­y hÃ£y xÃ¡c Ä‘á»‹nh má»™t hÃ m tokenize cÃ¡c Ä‘áº§u vÃ o cá»§a chÃºng ta:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

HÃ m nÃ y láº¥y má»™t tá»« Ä‘iá»ƒn (giá»‘ng nhÆ° cÃ¡c má»¥c trong táº­p dá»¯ liá»‡u cá»§a chÃºng ta) vÃ  tráº£ vá» má»™t tá»« Ä‘iá»ƒn má»›i vá»›i cÃ¡c khÃ³a `input_ids`, `attention_mask` vÃ  `token_type_ids`. LÆ°u Ã½ ráº±ng nÃ³ cÅ©ng hoáº¡t Ä‘á»™ng náº¿u tá»« Ä‘iá»ƒn `example` chá»©a má»™t sá»‘ máº«u (má»—i khÃ³a lÃ  má»™t danh sÃ¡ch cÃ¡c cÃ¢u) vÃ¬ `tokenizer` hoáº¡t Ä‘á»™ng trÃªn danh sÃ¡ch cÃ¡c cáº·p cÃ¢u, nhÆ° Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã¢y. Äiá»u nÃ y sáº½ cho phÃ©p chÃºng ta sá»­ dá»¥ng tÃ¹y chá»n `batch = True` trong lá»‡nh gá»i `map()`, tá»« Ä‘Ã³ sáº½ tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ quÃ¡ trÃ¬nh tokenize. `Tokenizer` Ä‘Æ°á»£c há»— trá»£ bá»Ÿi má»™t tokenizer Ä‘Æ°á»£c viáº¿t báº±ng Rust tá»« thÆ° viá»‡n [ğŸ¤— Tokenizer](https://github.com/huggingface/tokenizers). Tokenizer nÃ y cÃ³ thá»ƒ ráº¥t nhanh, nhÆ°ng chá»‰ khi chÃºng ta cung cáº¥p nhiá»u Ä‘áº§u vÃ o cÃ¹ng má»™t lÃºc.

LÆ°u Ã½ ráº±ng chÃºng ta Ä‘Ã£ Ä‘á»ƒ táº¡m bá» qua tham sá»‘ `padding` trong hÃ m tokenize cá»§a ta. Äiá»u nÃ y lÃ  do viá»‡c Ä‘á»‡m táº¥t cáº£ cÃ¡c máº«u Ä‘áº¿n chiá»u dÃ i tá»‘i Ä‘a khÃ´ng hiá»‡u quáº£: tá»‘t hÆ¡n nÃªn Ä‘á»‡m cÃ¡c máº«u khi chÃºng ta Ä‘ang táº¡o má»™t lÃ´, vÃ¬ khi Ä‘Ã³ chÃºng ta chá»‰ cáº§n Ä‘á»‡m Ä‘áº¿n chiá»u dÃ i tá»‘i Ä‘a trong lÃ´ Ä‘Ã³ chá»© khÃ´ng pháº£i chiá»u dÃ i tá»‘i Ä‘a trong toÃ n bá»™ táº­p dá»¯ liá»‡u. Äiá»u nÃ y cÃ³ thá»ƒ tiáº¿t kiá»‡m ráº¥t nhiá»u thá»i gian vÃ  cÃ´ng suáº¥t xá»­ lÃ½ khi cÃ¡c Ä‘áº§u vÃ o cÃ³ Ä‘á»™ dÃ i ráº¥t thay Ä‘á»•i!

ÄÃ¢y lÃ  cÃ¡ch chÃºng ta Ã¡p dá»¥ng chá»©c nÄƒng mÃ£ hÃ³a trÃªn táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u cá»§a ta cÃ¹ng má»™t lÃºc. ChÃºng ta Ä‘ang sá»­ dá»¥ng `batch = True` trong lá»‡nh gá»i tá»›i `map`, vÃ¬ váº­y, hÃ m Ä‘Æ°á»£c Ã¡p dá»¥ng cho nhiá»u pháº§n tá»­ cá»§a táº­p dá»¯ liá»‡u cÃ¹ng má»™t lÃºc, chá»© khÃ´ng pháº£i trÃªn tá»«ng pháº§n tá»­ riÃªng biá»‡t. Äiá»u nÃ y cho phÃ©p viá»‡c tiá»n xá»­ lÃ½ nhanh hÆ¡n.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

CÃ¡ch thÆ° viá»‡n ğŸ¤— Datasets Ã¡p dá»¥ng bÆ°á»›c xá»­ lÃ½ nÃ y lÃ  thÃªm cÃ¡c trÆ°á»ng má»›i vÃ o bá»™ dá»¯ liá»‡u, má»—i khÃ³a trong tá»« Ä‘iá»ƒn Ä‘Æ°á»£c tráº£ vá» bá»Ÿi hÃ m tiá»n xá»­ lÃ½ má»™t trÆ°á»ng:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

Báº¡n tháº­m chÃ­ cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘a xá»­ lÃ½ khi Ã¡p dá»¥ng chá»©c nÄƒng tiá»n xá»­ lÃ½ cá»§a mÃ¬nh vá»›i `map()` báº±ng cÃ¡ch truyá»n tham sá»‘ `num_proc`. ChÃºng ta khÃ´ng lÃ m Ä‘iá»u nÃ y á»Ÿ Ä‘Ã¢y vÃ¬ thÆ° viá»‡n ğŸ¤— Tokenizers Ä‘Ã£ sá»­ dá»¥ng nhiá»u chuá»—i Ä‘á»ƒ tokenize Ã¡c máº«u cá»§a nhanh hÆ¡n, nhÆ°ng náº¿u báº¡n khÃ´ng sá»­ dá»¥ng trÃ¬nh tokenize nhanh Ä‘Æ°á»£c thÆ° viá»‡n nÃ y há»— trá»£, bÆ°á»›c trÃªn cÃ³ thá»ƒ tÄƒng tá»‘c quÃ¡ trÃ¬nh xá»­ lÃ½ trÆ°á»›c cá»§a báº¡n.

`Tokenize_function` cá»§a chÃºng ta tráº£ vá» má»™t tá»« Ä‘iá»ƒn vá»›i cÃ¡c khÃ³a `input_ids`, `attention_mask` vÃ  `token_type_ids`, vÃ¬ váº­y ba trÆ°á»ng Ä‘Ã³ Ä‘Æ°á»£c thÃªm vÃ o táº¥t cáº£ cÃ¡c pháº§n bá»™ dá»¯ liá»‡u cá»§a chÃºng ta. LÆ°u Ã½ ráº±ng ta cÅ©ng cÃ³ thá»ƒ Ä‘Ã£ thay Ä‘á»•i cÃ¡c trÆ°á»ng hiá»‡n cÃ³ náº¿u hÃ m tiá»n xá»­ lÃ½ tráº£ vá» má»™t giÃ¡ trá»‹ má»›i cho má»™t khÃ³a hiá»‡n cÃ³ trong táº­p dá»¯ liá»‡u mÃ  ta Ä‘Ã£ Ã¡p dá»¥ng `map()`.

Äiá»u cuá»‘i cÃ¹ng chÃºng ta sáº½ cáº§n lÃ m lÃ  Ä‘á»‡m táº¥t cáº£ cÃ¡c vÃ­ dá»¥ Ä‘á»ƒ cÃ³ Ä‘á»™ dÃ i cá»§a pháº§n tá»­ dÃ i nháº¥t khi chÃºng tÃ´i gá»™p cÃ¡c pháº§n tá»­ láº¡i vá»›i nhau - má»™t ká»¹ thuáº­t mÃ  chÃºng tÃ´i gá»i lÃ  *Ä‘á»‡m Ä‘á»™ng*.

### Pháº§n Ä‘á»‡m Ä‘á»™ng

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
HÃ m chá»‹u trÃ¡ch nhiá»‡m táº­p há»£p cÃ¡c máº«u láº¡i vá»›i nhau trong má»™t lÃ´ Ä‘Æ°á»£c gá»i lÃ  *collate function* hay *hÃ m Ä‘á»‘i chiáº¿u*. ÄÃ³ lÃ  má»™t tham sá»‘ báº¡n cÃ³ thá»ƒ Ä‘Æ°a vÃ o khi xÃ¢y dá»±ng má»™t `DataLoader`, máº·c Ä‘á»‹nh Ä‘Ã¢y lÃ  má»™t hÃ m sáº½ chá»‰ chuyá»ƒn Ä‘á»•i cÃ¡c máº«u cá»§a báº¡n thÃ nh cÃ¡c tensors PyTorch vÃ  ná»‘i chÃºng (Ä‘á»‡ quy náº¿u cÃ¡c pháº§n tá»­ cá»§a báº¡n lÃ  list, tuple hoáº·c dict). Äiá»u nÃ y sáº½ khÃ´ng thá»ƒ xáº£y ra trong trÆ°á»ng há»£p cá»§a chÃºng ta vÃ¬ táº¥t cáº£ cÃ¡c Ä‘áº§u vÃ o ta cÃ³ sáº½ khÃ´ng cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c. ChÃºng ta Ä‘Ã£ cá»‘ tÃ¬nh hoÃ£n viá»‡c bá»• sung Ä‘á»‡m, Ä‘á»ƒ chá»‰ Ã¡p dá»¥ng nÃ³ khi cáº§n thiáº¿t trÃªn má»—i lÃ´ vÃ  trÃ¡nh Ä‘á»ƒ cÃ¡c Ä‘áº§u vÃ o quÃ¡ dÃ i vá»›i nhiá»u Ä‘á»‡m. Äiá»u nÃ y sáº½ Ä‘áº©y nhanh quÃ¡ trÃ¬nh huáº¥n luyá»‡n lÃªn má»™t chÃºt, nhÆ°ng lÆ°u Ã½ ráº±ng náº¿u báº¡n Ä‘ang huáº¥n luyá»‡n trÃªn TPU thÃ¬ nÃ³ cÃ³ thá»ƒ gÃ¢y ra váº¥n Ä‘á» - TPU thÃ­ch cÃ¡c hÃ¬nh dáº¡ng cá»‘ Ä‘á»‹nh, ngay cáº£ khi Ä‘iá»u Ä‘Ã³ yÃªu cáº§u thÃªm Ä‘á»‡m.

{:else}

HÃ m chá»‹u trÃ¡ch nhiá»‡m táº­p há»£p cÃ¡c máº«u láº¡i vá»›i nhau trong má»™t lÃ´ Ä‘Æ°á»£c gá»i lÃ  *collate function* hay *hÃ m Ä‘á»‘i chiáº¿u*. ÄÃ³ lÃ  má»™t tham sá»‘ báº¡n cÃ³ thá»ƒ Ä‘Æ°a vÃ o khi xÃ¢y dá»±ng má»™t `DataLoader`, máº·c Ä‘á»‹nh Ä‘Ã¢y lÃ  má»™t hÃ m sáº½ chá»‰ chuyá»ƒn Ä‘á»•i cÃ¡c máº«u cá»§a báº¡n thÃ nh cÃ¡c tensors PyTorch vÃ  ná»‘i chÃºng (Ä‘á»‡ quy náº¿u cÃ¡c pháº§n tá»­ cá»§a báº¡n lÃ  list, tuple hoáº·c dict). Äiá»u nÃ y sáº½ khÃ´ng thá»ƒ xáº£y ra trong trÆ°á»ng há»£p cá»§a chÃºng ta vÃ¬ táº¥t cáº£ cÃ¡c Ä‘áº§u vÃ o ta cÃ³ sáº½ khÃ´ng cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c. ChÃºng ta Ä‘Ã£ cá»‘ tÃ¬nh hoÃ£n viá»‡c bá»• sung Ä‘á»‡m, Ä‘á»ƒ chá»‰ Ã¡p dá»¥ng nÃ³ khi cáº§n thiáº¿t trÃªn má»—i lÃ´ vÃ  trÃ¡nh Ä‘á»ƒ cÃ¡c Ä‘áº§u vÃ o quÃ¡ dÃ i vá»›i nhiá»u Ä‘á»‡m. Äiá»u nÃ y sáº½ Ä‘áº©y nhanh quÃ¡ trÃ¬nh huáº¥n luyá»‡n lÃªn má»™t chÃºt, nhÆ°ng lÆ°u Ã½ ráº±ng náº¿u báº¡n Ä‘ang huáº¥n luyá»‡n trÃªn TPU thÃ¬ nÃ³ cÃ³ thá»ƒ gÃ¢y ra váº¥n Ä‘á» - TPU thÃ­ch cÃ¡c hÃ¬nh dáº¡ng cá»‘ Ä‘á»‹nh, ngay cáº£ khi Ä‘iá»u Ä‘Ã³ yÃªu cáº§u thÃªm Ä‘á»‡m.

{/if}

Äá»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y trong thá»±c táº¿, chÃºng ta pháº£i Ä‘á»‹nh nghÄ©a má»™t hÃ m Ä‘á»‘i chiáº¿u sáº½ Ã¡p dá»¥ng Ä‘Ãºng sá»‘ lÆ°á»£ng Ä‘á»‡m cho cÃ¡c má»¥c cá»§a táº­p dá»¯ liá»‡u mÃ  chÃºng ta muá»‘n gá»™p hÃ ng loáº¡t láº¡i vá»›i nhau. May máº¯n thay, thÆ° viá»‡n ğŸ¤— Transformers cung cáº¥p cho chÃºng ta má»™t chá»©c nÄƒng nhÆ° váº­y thÃ´ng qua `DataCollatorWithPadding`. Cáº§n cÃ³ trÃ¬nh tokenize khi báº¡n khá»Ÿi táº¡o nÃ³ (Ä‘á»ƒ biáº¿t cáº§n sá»­ dá»¥ng token Ä‘á»‡m nÃ o vÃ  liá»‡u mÃ´ hÃ¬nh mong Ä‘á»£i Ä‘á»‡m á»Ÿ bÃªn trÃ¡i hay bÃªn pháº£i cá»§a cÃ¡c Ä‘áº§u vÃ o) vÃ  sáº½ thá»±c hiá»‡n má»i thá»© báº¡n cáº§n:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

Äá»ƒ kiá»ƒm tra mÃ³n má»›i nÃ y, chÃºng ta hÃ£y láº¥y má»™t vÃ i máº«u tá»« táº­p huáº¥n luyá»‡n mÃ  chÃºng ta muá»‘n ghÃ©p láº¡i vá»›i nhau. á» Ä‘Ã¢y, chÃºng ta xÃ³a cÃ¡c cá»™t `idx`, `sentence1`, vÃ  `sentence2` vÃ¬ chÃºng khÃ´ng cáº§n thiáº¿t vÃ  chá»©a cÃ¡c chuá»—i (vÃ  chÃºng ta khÃ´ng thá»ƒ táº¡o tensor báº±ng chuá»—i) vÃ  xem Ä‘á»™ dÃ i cá»§a má»—i má»¥c trong lÃ´:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

KhÃ´ng cÃ³ gÃ¬ ngáº¡c nhiÃªn, ta nháº­n Ä‘Æ°á»£c cÃ¡c máº«u cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau, tá»« 32 Ä‘áº¿n 67. Äá»‡m Ä‘á»™ng cÃ³ nghÄ©a lÃ  táº¥t cáº£ cÃ¡c máº«u trong lÃ´ nÃ y pháº£i Ä‘Æ°á»£c Ä‘á»‡m Ä‘áº¿n chiá»u dÃ i 67, chiá»u dÃ i tá»‘i Ä‘a bÃªn trong lÃ´. Náº¿u khÃ´ng cÃ³ Ä‘á»‡m Ä‘á»™ng, táº¥t cáº£ cÃ¡c máº«u sáº½ pháº£i Ä‘Æ°á»£c Ä‘á»‡m Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a trong toÃ n bá»™ táº­p dá»¯ liá»‡u hoáº·c Ä‘á»™ dÃ i tá»‘i Ä‘a mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ cháº¥p nháº­n. HÃ£y kiá»ƒm tra ká»¹ xem `data_collator` cá»§a chÃºng ta cÃ³ tá»± Ä‘á»™ng Ä‘á»‡m lÃ´ Ä‘Ãºng cÃ¡ch hay khÃ´ng:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

TrÃ´ng khÃ¡ á»•n! Giá» ta Ä‘Ã£ chuyá»ƒn tá»« vÄƒn báº£n thÃ´ sang cÃ¡c lÃ´ mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ xá»­ lÃ½, vÃ  ta Ä‘Ã£ sáºµn sÃ ng tinh chá»‰nh nÃ³!

{/if}

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Sao chÃ©p tiá»n xá»­ lÃ½ trÃªn táº­p dá»¯ liá»‡u GLUE SST-2. NÃ³ hÆ¡i khÃ¡c má»™t chÃºt vÃ¬ nÃ³ bao gá»“m cÃ¡c cÃ¢u Ä‘Æ¡n thay vÃ¬ cÃ¡c cáº·p, nhÆ°ng pháº§n cÃ²n láº¡i cá»§a nhá»¯ng gÃ¬ ta Ä‘Ã£ lÃ m sáº½ tÆ°Æ¡ng tá»± nhau. Vá»›i má»™t thá»­ thÃ¡ch khÃ³ hÆ¡n, hÃ£y cá»‘ gáº¯ng viáº¿t má»™t hÃ m tiá»n xá»­ lÃ½ hoáº¡t Ä‘á»™ng trÃªn báº¥t ká»³ tÃ¡c vá»¥ GLUE nÃ o.

</Tip>

{#if fw === 'tf'}

BÃ¢y giá» chÃºng ta Ä‘Ã£ cÃ³ bá»™ dá»¯ liá»‡u vÃ  bá»™ Ä‘á»‘i chiáº¿u dá»¯ liá»‡u, ta cáº§n pháº£i káº¿t há»£p chÃºng láº¡i vá»›i nhau. ChÃºng ta cÃ³ thá»ƒ táº£i cÃ¡c lÃ´ vÃ  Ä‘á»‘i chiáº¿u theo cÃ¡ch thá»§ cÃ´ng, nhÆ°ng cÃ¡ch nÃ y ráº¥t tá»‘n cÃ´ng sá»©c vÃ  cÃ³ láº½ cÅ©ng khÃ´ng hiá»‡u quáº£ láº¯m. Thay vÃ o Ä‘Ã³, cÃ³ má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n cung cáº¥p giáº£i phÃ¡p hiá»‡u quáº£ cho váº¥n Ä‘á» nÃ y: `to_tf_dataset()`. Nso Ä‘Æ°á»£c bao má»™t `tf.data.Dataset` xung quanh táº­p dá»¯ liá»‡u cá»§a báº¡n, vá»›i má»™t chá»©c nÄƒng Ä‘á»‘i chiáº¿u tÃ¹y chá»n. `tf.data.Dataset` lÃ  má»™t Ä‘á»‹nh dáº¡ng TensorFlow gá»‘c mÃ  Keras cÃ³ thá»ƒ sá»­ dá»¥ng cho `model.fit()`, vÃ¬ váº­y phÆ°Æ¡ng phÃ¡p nÃ y ngay láº­p tá»©c chuyá»ƒn Ä‘á»•i má»™t ğŸ¤— Dataset sang má»™t Ä‘á»‹nh dáº¡ng sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n. HÃ£y xem nÃ³ hoáº¡t Ä‘á»™ng vá»›i táº­p dá»¯ liá»‡u cá»§a chÃºng tÃ´i!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

VÃ  nÃ³ Ä‘Ã³! ChÃºng ta cÃ³ thá»ƒ chuyá»ƒn nhá»¯ng bá»™ dá»¯ liá»‡u Ä‘Ã³ sang bÃ i giáº£ng tiáº¿p theo, nÆ¡i viá»‡c huáº¥n luyá»‡n sáº½ trá»Ÿ nÃªn Ä‘Æ¡n giáº£n má»™t cÃ¡ch dá»… chá»‹u sau táº¥t cáº£ nhá»¯ng cÃ´ng viá»‡c khÃ³ khÄƒn cá»§a viá»‡c xá»­ lÃ½ trÆ°á»›c dá»¯ liá»‡u.

{/if}
