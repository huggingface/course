<FrameworkSwitchCourse {fw} />

# X·ª≠ l√Ω d·ªØ li·ªáu

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
Ti·∫øp t·ª•c v·ªõi v√≠ d·ª• t·ª´ [ch∆∞∆°ng tr∆∞·ªõc](/course/chapter2), ƒë√¢y l√† c√°ch ch√∫ng ta s·∫Ω hu·∫•n luy·ªán m·ªôt b·ªô ph√¢n lo·∫°i chu·ªói tr√™n m·ªôt l√¥ trong PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# T∆∞∆°ng t·ª± nh∆∞ v√≠ d·ª• tr∆∞·ªõc
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# ƒê√¢y l√† ph·∫ßn m·ªõi
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
Ti·∫øp t·ª•c v·ªõi v√≠ d·ª• t·ª´ [ch∆∞∆°ng tr∆∞·ªõc](/course/chapter2), ƒë√¢y l√† c√°ch ch√∫ng ta s·∫Ω hu·∫•n luy·ªán m·ªôt b·ªô ph√¢n lo·∫°i chu·ªói tr√™n m·ªôt l√¥ trong TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# T∆∞∆°ng t·ª± nh∆∞ v√≠ d·ª• tr∆∞·ªõc
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# ƒê√¢y l√† ph·∫ßn m·ªõi
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

T·∫•t nhi√™n, ch·ªâ hu·∫•n luy·ªán m√¥ h√¨nh tr√™n hai c√¢u s·∫Ω kh√¥ng mang l·∫°i k·∫øt qu·∫£ t·ªët. ƒê·ªÉ c√≥ ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët h∆°n, b·∫°n s·∫Ω c·∫ßn chu·∫©n b·ªã m·ªôt b·ªô d·ªØ li·ªáu l·ªõn h∆°n.

Trong ph·∫ßn n√†y, ch√∫ng t√¥i s·∫Ω s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu MRPC (Microsoft Research Paraphrase Corpus) l√†m v√≠ d·ª•, ƒë∆∞·ª£c gi·ªõi thi·ªáu trong [b√†i b√°o](https://www.aclweb.org/anthology/I05-5002.pdf) c·ªßa William B. Dolan v√† Chris Brockett. T·∫≠p d·ªØ li·ªáu bao g·ªìm 5,801 c·∫∑p c√¢u, v·ªõi nh√£n cho bi·∫øt ch√∫ng c√≥ ph·∫£i l√† c√¢u di·ªÖn gi·∫£i hay kh√¥ng (t·ª©c l√† n·∫øu c·∫£ hai c√¢u ƒë·ªÅu c√≥ nghƒ©a gi·ªëng nhau). Ch√∫ng t√¥i ƒë√£ ch·ªçn n√≥ cho ch∆∞∆°ng n√†y v√¨ n√≥ l√† m·ªôt t·∫≠p d·ªØ li·ªáu nh·ªè, v√¨ v·∫≠y th·∫≠t d·ªÖ d√†ng ƒë·ªÉ th·ª≠ nghi·ªám v·ªõi vi·ªác hu·∫•n luy·ªán v·ªÅ n√≥.

### T·∫£i b·ªô d·ªØ li·ªáu t·ª´ Hub

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

Hub kh√¥ng ch·ªâ ch·ª©a c√°c m√¥ h√¨nh; n√≥ c≈©ng c√≥ nhi·ªÅu b·ªô d·ªØ li·ªáu nhi·ªÅu ng√¥n ng·ªØ kh√°c nhau. B·∫°n c√≥ th·ªÉ xem qua t·∫≠p d·ªØ li·ªáu [t·∫°i ƒë√¢y](https://huggingface.co/datasets) v√† ch√∫ng t√¥i khuy√™n b·∫°n n√™n th·ª≠ t·∫£i v√† x·ª≠ l√Ω b·ªô d·ªØ li·ªáu m·ªõi khi b·∫°n ƒë√£ xem qua ph·∫ßn n√†y (xem t√†i li·ªáu chung [t·∫°i ƒë√¢y](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)). Nh∆∞ng hi·ªán t·∫°i, h√£y t·∫≠p trung v√†o b·ªô d·ªØ li·ªáu MRPC! ƒê√¢y l√† m·ªôt trong 10 b·ªô d·ªØ li·ªáu t·∫°o n√™n [b·ªô chu·∫©n GLUE](https://gluebenchmark.com/), l√† m·ªôt ƒëi·ªÉm chu·∫©n h·ªçc thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëo hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh ML tr√™n 10 t√°c v·ª• ph√¢n lo·∫°i vƒÉn b·∫£n kh√°c nhau.

Th∆∞ vi·ªán ü§ó Datasets cung c·∫•p m·ªôt l·ªánh r·∫•t ƒë∆°n gi·∫£n ƒë·ªÉ t·∫£i xu·ªëng v√† l∆∞u v√†o b·ªô nh·ªõ cache m·ªôt t·∫≠p d·ªØ li·ªáu tr√™n Hub. Ch√∫ng ta c√≥ th·ªÉ t·∫£i xu·ªëng b·ªô d·ªØ li·ªáu MRPC nh∆∞ sau:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, ch√∫ng ta nh·∫≠n ƒë∆∞·ª£c m·ªôt ƒë·ªëi t∆∞·ª£ng `DatasetDict` ch·ª©a t·∫≠p hu·∫•n luy·ªán, t·∫≠p ki·ªÉm ƒë·ªãnh v√† t·∫≠p ki·ªÉm th·ª≠. M·ªói t·∫≠p ch·ª©a m·ªôt s·ªë c·ªôt (`sentence1`, `sentence2`, `label`, v√† `idx`) v√† m·ªôt s·ªë h√†ng thay ƒë·ªïi, l√† s·ªë ph·∫ßn t·ª≠ trong m·ªói t·∫≠p (v√¨ v·∫≠y, c√≥ 3,668 c·∫∑p c√¢u trong t·∫≠p hu·∫•n luy·ªán, 408 trong t·∫≠p ki·ªÉm ch·ª©ng v√† 1,725 trong t·∫≠p ki·ªÉm ƒë·ªãnh).

L·ªánh n√†y t·∫£i xu·ªëng v√† l∆∞u v√†o b·ªô nh·ªõ cache c√°c t·∫≠p d·ªØ li·ªáu, m·∫∑c ƒë·ªãnh l∆∞u trong *~/.cache/huggingface/datasets*. Nh·ªõ l·∫°i t·ª´ Ch∆∞∆°ng 2 r·∫±ng b·∫°n c√≥ th·ªÉ t√πy ch·ªânh th∆∞ m·ª•c b·ªô nh·ªõ cache c·ªßa m√¨nh b·∫±ng c√°ch ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng `HF_HOME`.

Ch√∫ng ta c√≥ th·ªÉ truy c·∫≠p t·ª´ng c·∫∑p c√¢u trong ƒë·ªëi t∆∞·ª£ng `raw_datasets` c·ªßa m√¨nh b·∫±ng c√°ch l·∫≠p ch·ªâ m·ª•c, gi·ªëng nh∆∞ v·ªõi t·ª´ ƒëi·ªÉn:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

Ch√∫ng ta c√≥ th·ªÉ th·∫•y c√°c nh√£n v·ªën l√† s·ªë nguy√™n, v√¨ v·∫≠y ch√∫ng ta kh√¥ng ph·∫£i th·ª±c hi·ªán b·∫•t k·ª≥ b∆∞·ªõc x·ª≠ l√Ω tr∆∞·ªõc n√†o ·ªü ƒë√≥. ƒê·ªÉ bi·∫øt s·ªë nguy√™n n√†o t∆∞∆°ng ·ª©ng v·ªõi nh√£n n√†o, ch√∫ng ta c√≥ th·ªÉ ki·ªÉm tra `features` c·ªßa `raw_train_dataset`. ƒêi·ªÅu n√†y s·∫Ω cho ch√∫ng t√¥i bi·∫øt lo·∫°i c·ªßa m·ªói c·ªôt:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

Ph√≠a sau, `label` thu·ªôc lo·∫°i `ClassLabel` v√† √°nh x·∫° c√°c s·ªë nguy√™n th√†nh t√™n nh√£n ƒë∆∞·ª£c l∆∞u tr·ªØ trong th∆∞ m·ª•c *names*. `0` t∆∞∆°ng ·ª©ng v·ªõi `kh√¥ng t∆∞∆°ng ƒë∆∞∆°ng`, v√† `1` t∆∞∆°ng ·ª©ng v·ªõi `t∆∞∆°ng ƒë∆∞∆°ng`.

<Tip>

‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** Nh√¨n v√†o ph·∫ßn t·ª≠ th·ª© 15 c·ªßa t·∫≠p hu·∫•n luy·ªán v√† ph·∫ßn t·ª≠ 87 c·ªßa t·∫≠p ki·ªÉm ƒë·ªãnh. Nh√£n c·ªßa ch√∫ng l√† g√¨?

</Tip>

### Ti·ªÅn x·ª≠ l√Ω m·ªôt b·ªô d·ªØ li·ªáu

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ƒê·ªÉ ti·ªÅn x·ª≠ l√Ω b·ªô d·ªØ li·ªáu, ch√∫ng ta c·∫ßn chuy·ªÉn vƒÉn b·∫£n th√†nh c√°c s·ªë m√† m√¥ h√¨nh c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c. Nh∆∞ b·∫°n ƒë√£ th·∫•y trong [ch∆∞∆°ng tr∆∞·ªõc](/course/chapter2), ƒëi·ªÅu n√†y ƒë∆∞·ª£c th·ª±c hi·ªán v·ªõi m·ªôt tokenizer. Ch√∫ng ta c√≥ th·ªÉ cung c·∫•p cho tokenizer m·ªôt c√¢u ho·∫∑c m·ªôt danh s√°ch c√°c c√¢u, v√¨ v·∫≠y ch√∫ng ta c√≥ th·ªÉ tokenizer tr·ª±c ti·∫øp t·∫•t c·∫£ c√°c c√¢u ƒë·∫ßu ti√™n v√† t·∫•t c·∫£ c√°c c√¢u th·ª© hai c·ªßa m·ªói c·∫∑p nh∆∞ sau:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

Tuy nhi√™n, ch√∫ng ta kh√¥ng th·ªÉ ch·ªâ chuy·ªÉn hai chu·ªói v√†o m√¥ h√¨nh v√† nh·∫≠n ƒë∆∞·ª£c d·ª± ƒëo√°n li·ªáu hai c√¢u c√≥ ph·∫£i l√† di·ªÖn gi·∫£i hay kh√¥ng. Ch√∫ng ta c·∫ßn x·ª≠ l√Ω hai chu·ªói nh∆∞ m·ªôt c·∫∑p v√† √°p d·ª•ng ti·ªÅn x·ª≠ l√Ω th√≠ch h·ª£p. May m·∫Øn thay, tokenizer c≈©ng c√≥ th·ªÉ nh·∫≠n m·ªôt c·∫∑p chu·ªói v√† chu·∫©n b·ªã n√≥ theo c√°ch m√† m√¥ h√¨nh BERT c·ªßa ta mong ƒë·ª£i:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

Ch√∫ng ta ƒë√£ th·∫£o lu·∫≠n v·ªÅ `input_ids` v√† `attention_mask` trong [Ch∆∞∆°ng 2](/course/chapter2), nh∆∞ng ch√∫ng ta t·∫°m d·ª´ng ƒë·ªÉ n√≥i v·ªÅ `token_type_ids`. Trong v√≠ d·ª• n√†y, ƒë√¢y l√† ph·∫ßn cho m√¥ h√¨nh bi·∫øt ph·∫ßn n√†o c·ªßa ƒë·∫ßu v√†o l√† c√¢u ƒë·∫ßu ti√™n v√† ph·∫ßn n√†o l√† c√¢u th·ª© hai.

<Tip>

‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** L·∫•y ph·∫ßn t·ª≠ 15 c·ªßa t·∫≠p hu·∫•n luy·ªán v√† tokenize hai c√¢u ri√™ng bi·ªát v√† nh∆∞ m·ªôt c·∫∑p. S·ª± kh√°c bi·ªát gi·ªØa hai k·∫øt qu·∫£ l√† g√¨?

</Tip>

N·∫øu ch√∫ng ta gi·∫£i m√£ c√°c ID b√™n trong `input_ids` tr·ªü l·∫°i c√°c t·ª´:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ta s·∫Ω nh·∫≠n ƒë∆∞·ª£c:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

C√≥ th·ªÉ th·∫•y m√¥ h√¨nh k√¨ v·ªçng c√°c ƒë·∫ßu v√†o c√≥ d·∫°ng `[CLS] c√¢u1 [SEP] c√¢u2 [SEP]` khi c√≥ hai c√¢u. CƒÉn ch·ªânh ƒëi·ªÅu n√†y v·ªõi `token_type_ids` cho ta k·∫øt qu·∫£:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y, c√°c ph·∫ßn c·ªßa ƒë·∫ßu v√†o t∆∞∆°ng ·ª©ng v·ªõi `[CLS] c√¢u1 [SEP]` ƒë·ªÅu c√≥ lo·∫°i token ID l√† `0`, trong khi c√°c ph·∫ßn kh√°c, t∆∞∆°ng ·ª©ng v·ªõi `c√¢u2 [SEP]`, t·∫•t c·∫£ ƒë·ªÅu c√≥ lo·∫°i token ID l√† `1`.

L∆∞u √Ω r·∫±ng n·∫øu b·∫°n ch·ªçn m·ªôt checkpoint kh√°c, b·∫°n s·∫Ω kh√¥ng nh·∫•t thi·∫øt ph·∫£i c√≥ `token_type_ids` trong ƒë·∫ßu v√†o ƒë∆∞·ª£c tokenize c·ªßa m√¨nh (v√≠ d·ª•: ch√∫ng s·∫Ω kh√¥ng ƒë∆∞·ª£c tr·∫£ l·∫°i n·∫øu b·∫°n s·ª≠ d·ª•ng m√¥ h√¨nh DistilBERT). Ch√∫ng ch·ªâ ƒë∆∞·ª£c tr·∫£ l·∫°i khi m√¥ h√¨nh bi·∫øt ph·∫£i l√†m g√¨ v·ªõi ch√∫ng, b·ªüi v√¨ n√≥ ƒë√£ nh√¨n th·∫•y ch√∫ng trong qu√° tr√¨nh hu·∫•n luy·ªán tr∆∞·ªõc.

·ªû ƒë√¢y, BERT ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc v·ªõi c√°c token ID v√† tr√™n ƒë·∫ßu m·ª•c ti√™u m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c che m√† ch√∫ng ta ƒë√£ ƒë·ªÅ c·∫≠p trong [Ch∆∞∆°ng 1](/course/chapter1), n√≥ c√≥ m·ªôt m·ª•c ti√™u b·ªï sung ƒë∆∞·ª£c g·ªçi l√† _d·ª± ƒëo√°n c√¢u ti·∫øp theo_. M·ª•c ti√™u c·ªßa t√°c v·ª• n√†y l√† m√¥ h√¨nh h√≥a m·ªëi quan h·ªá gi·ªØa c√°c c·∫∑p c√¢u.

V·ªõi d·ª± ƒëo√°n c√¢u ti·∫øp theo, m√¥ h√¨nh ƒë∆∞·ª£c cung c·∫•p c√°c c·∫∑p c√¢u (v·ªõi c√°c token ƒë∆∞·ª£c che ng·∫´u nhi√™n) v√† ƒë∆∞·ª£c y√™u c·∫ßu d·ª± ƒëo√°n li·ªáu c√¢u th·ª© hai c√≥ theo sau c√¢u ƒë·∫ßu ti√™n hay kh√¥ng. ƒê·ªÉ l√†m cho t√°c v·ª• tr·ªü n√™n kh√¥ng t·∫ßm th∆∞·ªùng, m·ªôt n·ª≠a l√† c√°c c√¢u ti·∫øp n·ªëi nhau trong t√†i li·ªáu g·ªëc m√† ch√∫ng ƒë∆∞·ª£c tr√≠ch xu·∫•t, v√† n·ª≠a c√≤n l·∫°i l√† hai c√¢u ƒë·∫øn t·ª´ hai t√†i li·ªáu kh√°c nhau.

N√≥i chung, b·∫°n kh√¥ng c·∫ßn ph·∫£i lo l·∫Øng v·ªÅ vi·ªác c√≥ hay kh√¥ng c√≥ `token_type_ids` trong ƒë·∫ßu v√†o ƒë∆∞·ª£c tokenize c·ªßa m√¨nh: mi·ªÖn l√† b·∫°n s·ª≠ d·ª•ng c√πng m·ªôt checkpoint cho tr√¨nh tokenize v√† m√¥ h√¨nh, m·ªçi th·ª© s·∫Ω ·ªïn v√¨ tr√¨nh tokenize nh·∫≠n bi·∫øt c·∫ßn cung c·∫•p nh·ªØng g√¨ v·ªõi m√¥ h√¨nh c·ªßa n√≥.

B√¢y gi·ªù ch√∫ng ta ƒë√£ th·∫•y c√°ch tr√¨nh tokenize c·ªßa ch√∫ng ta c√≥ th·ªÉ x·ª≠ l√Ω m·ªôt c·∫∑p c√¢u, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng n√≥ ƒë·ªÉ m√£ h√≥a to√†n b·ªô t·∫≠p d·ªØ li·ªáu c·ªßa m√¨nh: gi·ªëng nh∆∞ trong [ch∆∞∆°ng tr∆∞·ªõc](/course/chapter2), ch√∫ng ta c√≥ th·ªÉ cung c·∫•p cho tr√¨nh tokenize danh s√°ch c√°c c·∫∑p b·∫±ng c√°ch ƒë∆∞a cho n√≥ danh s√°ch c√°c c√¢u ƒë·∫ßu ti√™n, sau ƒë√≥ l√† danh s√°ch c√°c c√¢u th·ª© hai. ƒêi·ªÅu n√†y c≈©ng t∆∞∆°ng th√≠ch v·ªõi c√°c t√πy ch·ªçn ƒë·ªám v√† c·∫Øt b·ªõt m√† ch√∫ng ta ƒë√£ th·∫•y trong [Ch∆∞∆°ng 2](/course/chapter2). V√¨ v·∫≠y, m·ªôt c√°ch ƒë·ªÉ ti·ªÅn x·ª≠ l√Ω tr∆∞·ªõc t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán l√†:

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

ƒêi·ªÅu n√†y ho·∫°t ƒë·ªông t·ªët, nh∆∞ng n√≥ c√≥ nh∆∞·ª£c ƒëi·ªÉm l√† tr·∫£ v·ªÅ t·ª´ ƒëi·ªÉn (v·ªõi c√°c kh√≥a c·ªßa ch√∫ng t√¥i, `input_ids`, `attention_mask` v√† `token_type_ids`, v√† c√°c gi√° tr·ªã l√† danh s√°ch c√°c danh s√°ch). N√≥ c≈©ng s·∫Ω ch·ªâ ho·∫°t ƒë·ªông n·∫øu b·∫°n c√≥ ƒë·ªß RAM ƒë·ªÉ l∆∞u tr·ªØ to√†n b·ªô t·∫≠p d·ªØ li·ªáu c·ªßa m√¨nh trong qu√° tr√¨nh tokenize (trong khi c√°c t·∫≠p d·ªØ li·ªáu t·ª´ th∆∞ vi·ªán ü§ó Datasets l√† c√°c t·ªáp [Apache Arrow](https://arrow.apache.org/) ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n ƒëƒ©a, v√¨ v·∫≠y b·∫°n ch·ªâ gi·ªØ c√°c m·∫´u b·∫°n y√™u c·∫ßu ƒë√£ t·∫£i trong b·ªô nh·ªõ).

ƒê·ªÉ gi·ªØ d·ªØ li·ªáu d∆∞·ªõi d·∫°ng t·∫≠p d·ªØ li·ªáu, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). ƒêi·ªÅu n√†y c≈©ng cho ph√©p ch√∫ng ta linh ho·∫°t h∆°n, n·∫øu ch√∫ng ta c·∫ßn th·ª±c hi·ªán nhi·ªÅu ti·ªÅn x·ª≠ l√Ω h∆°n l√† ch·ªâ tokenize. Ph∆∞∆°ng th·ª©c `map()` ho·∫°t ƒë·ªông b·∫±ng c√°ch √°p d·ª•ng m·ªôt h√†m tr√™n m·ªói ph·∫ßn t·ª≠ c·ªßa t·∫≠p d·ªØ li·ªáu, v√¨ v·∫≠y h√£y x√°c ƒë·ªãnh m·ªôt h√†m tokenize c√°c ƒë·∫ßu v√†o c·ªßa ch√∫ng ta:

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

H√†m n√†y l·∫•y m·ªôt t·ª´ ƒëi·ªÉn (gi·ªëng nh∆∞ c√°c m·ª•c trong t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng ta) v√† tr·∫£ v·ªÅ m·ªôt t·ª´ ƒëi·ªÉn m·ªõi v·ªõi c√°c kh√≥a `input_ids`, `attention_mask` v√† `token_type_ids`. L∆∞u √Ω r·∫±ng n√≥ c≈©ng ho·∫°t ƒë·ªông n·∫øu t·ª´ ƒëi·ªÉn `example` ch·ª©a m·ªôt s·ªë m·∫´u (m·ªói kh√≥a l√† m·ªôt danh s√°ch c√°c c√¢u) v√¨ `tokenizer` ho·∫°t ƒë·ªông tr√™n danh s√°ch c√°c c·∫∑p c√¢u, nh∆∞ ƒë√£ th·∫•y tr∆∞·ªõc ƒë√¢y. ƒêi·ªÅu n√†y s·∫Ω cho ph√©p ch√∫ng ta s·ª≠ d·ª•ng t√πy ch·ªçn `batch = True` trong l·ªánh g·ªçi `map()`, t·ª´ ƒë√≥ s·∫Ω tƒÉng t·ªëc ƒë√°ng k·ªÉ qu√° tr√¨nh tokenize. `Tokenizer` ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi m·ªôt tokenizer ƒë∆∞·ª£c vi·∫øt b·∫±ng Rust t·ª´ th∆∞ vi·ªán [ü§ó Tokenizer](https://github.com/huggingface/tokenizers). Tokenizer n√†y c√≥ th·ªÉ r·∫•t nhanh, nh∆∞ng ch·ªâ khi ch√∫ng ta cung c·∫•p nhi·ªÅu ƒë·∫ßu v√†o c√πng m·ªôt l√∫c.

L∆∞u √Ω r·∫±ng ch√∫ng ta ƒë√£ ƒë·ªÉ t·∫°m b·ªè qua tham s·ªë `padding` trong h√†m tokenize c·ªßa ta. ƒêi·ªÅu n√†y l√† do vi·ªác ƒë·ªám t·∫•t c·∫£ c√°c m·∫´u ƒë·∫øn chi·ªÅu d√†i t·ªëi ƒëa kh√¥ng hi·ªáu qu·∫£: t·ªët h∆°n n√™n ƒë·ªám c√°c m·∫´u khi ch√∫ng ta ƒëang t·∫°o m·ªôt l√¥, v√¨ khi ƒë√≥ ch√∫ng ta ch·ªâ c·∫ßn ƒë·ªám ƒë·∫øn chi·ªÅu d√†i t·ªëi ƒëa trong l√¥ ƒë√≥ ch·ª© kh√¥ng ph·∫£i chi·ªÅu d√†i t·ªëi ƒëa trong to√†n b·ªô t·∫≠p d·ªØ li·ªáu. ƒêi·ªÅu n√†y c√≥ th·ªÉ ti·∫øt ki·ªám r·∫•t nhi·ªÅu th·ªùi gian v√† c√¥ng su·∫•t x·ª≠ l√Ω khi c√°c ƒë·∫ßu v√†o c√≥ ƒë·ªô d√†i r·∫•t thay ƒë·ªïi!

ƒê√¢y l√† c√°ch ch√∫ng ta √°p d·ª•ng ch·ª©c nƒÉng m√£ h√≥a tr√™n t·∫•t c·∫£ c√°c t·∫≠p d·ªØ li·ªáu c·ªßa ta c√πng m·ªôt l√∫c. Ch√∫ng ta ƒëang s·ª≠ d·ª•ng `batch = True` trong l·ªánh g·ªçi t·ªõi `map`, v√¨ v·∫≠y, h√†m ƒë∆∞·ª£c √°p d·ª•ng cho nhi·ªÅu ph·∫ßn t·ª≠ c·ªßa t·∫≠p d·ªØ li·ªáu c√πng m·ªôt l√∫c, ch·ª© kh√¥ng ph·∫£i tr√™n t·ª´ng ph·∫ßn t·ª≠ ri√™ng bi·ªát. ƒêi·ªÅu n√†y cho ph√©p vi·ªác ti·ªÅn x·ª≠ l√Ω nhanh h∆°n.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

C√°ch th∆∞ vi·ªán ü§ó Datasets √°p d·ª•ng b∆∞·ªõc x·ª≠ l√Ω n√†y l√† th√™m c√°c tr∆∞·ªùng m·ªõi v√†o b·ªô d·ªØ li·ªáu, m·ªói kh√≥a trong t·ª´ ƒëi·ªÉn ƒë∆∞·ª£c tr·∫£ v·ªÅ b·ªüi h√†m ti·ªÅn x·ª≠ l√Ω m·ªôt tr∆∞·ªùng:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

B·∫°n th·∫≠m ch√≠ c√≥ th·ªÉ s·ª≠ d·ª•ng ƒëa x·ª≠ l√Ω khi √°p d·ª•ng ch·ª©c nƒÉng ti·ªÅn x·ª≠ l√Ω c·ªßa m√¨nh v·ªõi `map()` b·∫±ng c√°ch truy·ªÅn tham s·ªë `num_proc`. Ch√∫ng ta kh√¥ng l√†m ƒëi·ªÅu n√†y ·ªü ƒë√¢y v√¨ th∆∞ vi·ªán ü§ó Tokenizers ƒë√£ s·ª≠ d·ª•ng nhi·ªÅu chu·ªói ƒë·ªÉ tokenize √°c m·∫´u c·ªßa nhanh h∆°n, nh∆∞ng n·∫øu b·∫°n kh√¥ng s·ª≠ d·ª•ng tr√¨nh tokenize nhanh ƒë∆∞·ª£c th∆∞ vi·ªán n√†y h·ªó tr·ª£, b∆∞·ªõc tr√™n c√≥ th·ªÉ tƒÉng t·ªëc qu√° tr√¨nh x·ª≠ l√Ω tr∆∞·ªõc c·ªßa b·∫°n.

`Tokenize_function` c·ªßa ch√∫ng ta tr·∫£ v·ªÅ m·ªôt t·ª´ ƒëi·ªÉn v·ªõi c√°c kh√≥a `input_ids`, `attention_mask` v√† `token_type_ids`, v√¨ v·∫≠y ba tr∆∞·ªùng ƒë√≥ ƒë∆∞·ª£c th√™m v√†o t·∫•t c·∫£ c√°c ph·∫ßn b·ªô d·ªØ li·ªáu c·ªßa ch√∫ng ta. L∆∞u √Ω r·∫±ng ta c≈©ng c√≥ th·ªÉ ƒë√£ thay ƒë·ªïi c√°c tr∆∞·ªùng hi·ªán c√≥ n·∫øu h√†m ti·ªÅn x·ª≠ l√Ω tr·∫£ v·ªÅ m·ªôt gi√° tr·ªã m·ªõi cho m·ªôt kh√≥a hi·ªán c√≥ trong t·∫≠p d·ªØ li·ªáu m√† ta ƒë√£ √°p d·ª•ng `map()`.

ƒêi·ªÅu cu·ªëi c√πng ch√∫ng ta s·∫Ω c·∫ßn l√†m l√† ƒë·ªám t·∫•t c·∫£ c√°c v√≠ d·ª• ƒë·ªÉ c√≥ ƒë·ªô d√†i c·ªßa ph·∫ßn t·ª≠ d√†i nh·∫•t khi ch√∫ng t√¥i g·ªôp c√°c ph·∫ßn t·ª≠ l·∫°i v·ªõi nhau - m·ªôt k·ªπ thu·∫≠t m√† ch√∫ng t√¥i g·ªçi l√† *ƒë·ªám ƒë·ªông*.

### Ph·∫ßn ƒë·ªám ƒë·ªông

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
H√†m ch·ªãu tr√°ch nhi·ªám t·∫≠p h·ª£p c√°c m·∫´u l·∫°i v·ªõi nhau trong m·ªôt l√¥ ƒë∆∞·ª£c g·ªçi l√† *collate function* hay *h√†m ƒë·ªëi chi·∫øu*. ƒê√≥ l√† m·ªôt tham s·ªë b·∫°n c√≥ th·ªÉ ƒë∆∞a v√†o khi x√¢y d·ª±ng m·ªôt `DataLoader`, m·∫∑c ƒë·ªãnh ƒë√¢y l√† m·ªôt h√†m s·∫Ω ch·ªâ chuy·ªÉn ƒë·ªïi c√°c m·∫´u c·ªßa b·∫°n th√†nh c√°c tensors PyTorch v√† n·ªëi ch√∫ng (ƒë·ªá quy n·∫øu c√°c ph·∫ßn t·ª≠ c·ªßa b·∫°n l√† list, tuple ho·∫∑c dict). ƒêi·ªÅu n√†y s·∫Ω kh√¥ng th·ªÉ x·∫£y ra trong tr∆∞·ªùng h·ª£p c·ªßa ch√∫ng ta v√¨ t·∫•t c·∫£ c√°c ƒë·∫ßu v√†o ta c√≥ s·∫Ω kh√¥ng c√≥ c√πng k√≠ch th∆∞·ªõc. Ch√∫ng ta ƒë√£ c·ªë t√¨nh ho√£n vi·ªác b·ªï sung ƒë·ªám, ƒë·ªÉ ch·ªâ √°p d·ª•ng n√≥ khi c·∫ßn thi·∫øt tr√™n m·ªói l√¥ v√† tr√°nh ƒë·ªÉ c√°c ƒë·∫ßu v√†o qu√° d√†i v·ªõi nhi·ªÅu ƒë·ªám. ƒêi·ªÅu n√†y s·∫Ω ƒë·∫©y nhanh qu√° tr√¨nh hu·∫•n luy·ªán l√™n m·ªôt ch√∫t, nh∆∞ng l∆∞u √Ω r·∫±ng n·∫øu b·∫°n ƒëang hu·∫•n luy·ªán tr√™n TPU th√¨ n√≥ c√≥ th·ªÉ g√¢y ra v·∫•n ƒë·ªÅ - TPU th√≠ch c√°c h√¨nh d·∫°ng c·ªë ƒë·ªãnh, ngay c·∫£ khi ƒëi·ªÅu ƒë√≥ y√™u c·∫ßu th√™m ƒë·ªám.

{:else}

H√†m ch·ªãu tr√°ch nhi·ªám t·∫≠p h·ª£p c√°c m·∫´u l·∫°i v·ªõi nhau trong m·ªôt l√¥ ƒë∆∞·ª£c g·ªçi l√† *collate function* hay *h√†m ƒë·ªëi chi·∫øu*. ƒê√≥ l√† m·ªôt tham s·ªë b·∫°n c√≥ th·ªÉ ƒë∆∞a v√†o khi x√¢y d·ª±ng m·ªôt `DataLoader`, m·∫∑c ƒë·ªãnh ƒë√¢y l√† m·ªôt h√†m s·∫Ω ch·ªâ chuy·ªÉn ƒë·ªïi c√°c m·∫´u c·ªßa b·∫°n th√†nh c√°c tensors PyTorch v√† n·ªëi ch√∫ng (ƒë·ªá quy n·∫øu c√°c ph·∫ßn t·ª≠ c·ªßa b·∫°n l√† list, tuple ho·∫∑c dict). ƒêi·ªÅu n√†y s·∫Ω kh√¥ng th·ªÉ x·∫£y ra trong tr∆∞·ªùng h·ª£p c·ªßa ch√∫ng ta v√¨ t·∫•t c·∫£ c√°c ƒë·∫ßu v√†o ta c√≥ s·∫Ω kh√¥ng c√≥ c√πng k√≠ch th∆∞·ªõc. Ch√∫ng ta ƒë√£ c·ªë t√¨nh ho√£n vi·ªác b·ªï sung ƒë·ªám, ƒë·ªÉ ch·ªâ √°p d·ª•ng n√≥ khi c·∫ßn thi·∫øt tr√™n m·ªói l√¥ v√† tr√°nh ƒë·ªÉ c√°c ƒë·∫ßu v√†o qu√° d√†i v·ªõi nhi·ªÅu ƒë·ªám. ƒêi·ªÅu n√†y s·∫Ω ƒë·∫©y nhanh qu√° tr√¨nh hu·∫•n luy·ªán l√™n m·ªôt ch√∫t, nh∆∞ng l∆∞u √Ω r·∫±ng n·∫øu b·∫°n ƒëang hu·∫•n luy·ªán tr√™n TPU th√¨ n√≥ c√≥ th·ªÉ g√¢y ra v·∫•n ƒë·ªÅ - TPU th√≠ch c√°c h√¨nh d·∫°ng c·ªë ƒë·ªãnh, ngay c·∫£ khi ƒëi·ªÅu ƒë√≥ y√™u c·∫ßu th√™m ƒë·ªám.

{/if}

ƒê·ªÉ th·ª±c hi·ªán ƒëi·ªÅu n√†y trong th·ª±c t·∫ø, ch√∫ng ta ph·∫£i ƒë·ªãnh nghƒ©a m·ªôt h√†m ƒë·ªëi chi·∫øu s·∫Ω √°p d·ª•ng ƒë√∫ng s·ªë l∆∞·ª£ng ƒë·ªám cho c√°c m·ª•c c·ªßa t·∫≠p d·ªØ li·ªáu m√† ch√∫ng ta mu·ªën g·ªôp h√†ng lo·∫°t l·∫°i v·ªõi nhau. May m·∫Øn thay, th∆∞ vi·ªán ü§ó Transformers cung c·∫•p cho ch√∫ng ta m·ªôt ch·ª©c nƒÉng nh∆∞ v·∫≠y th√¥ng qua `DataCollatorWithPadding`. C·∫ßn c√≥ tr√¨nh tokenize khi b·∫°n kh·ªüi t·∫°o n√≥ (ƒë·ªÉ bi·∫øt c·∫ßn s·ª≠ d·ª•ng token ƒë·ªám n√†o v√† li·ªáu m√¥ h√¨nh mong ƒë·ª£i ƒë·ªám ·ªü b√™n tr√°i hay b√™n ph·∫£i c·ªßa c√°c ƒë·∫ßu v√†o) v√† s·∫Ω th·ª±c hi·ªán m·ªçi th·ª© b·∫°n c·∫ßn:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

ƒê·ªÉ ki·ªÉm tra m√≥n m·ªõi n√†y, ch√∫ng ta h√£y l·∫•y m·ªôt v√†i m·∫´u t·ª´ t·∫≠p hu·∫•n luy·ªán m√† ch√∫ng ta mu·ªën gh√©p l·∫°i v·ªõi nhau. ·ªû ƒë√¢y, ch√∫ng ta x√≥a c√°c c·ªôt `idx`, `sentence1`, v√† `sentence2` v√¨ ch√∫ng kh√¥ng c·∫ßn thi·∫øt v√† ch·ª©a c√°c chu·ªói (v√† ch√∫ng ta kh√¥ng th·ªÉ t·∫°o tensor b·∫±ng chu·ªói) v√† xem ƒë·ªô d√†i c·ªßa m·ªói m·ª•c trong l√¥:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

Kh√¥ng c√≥ g√¨ ng·∫°c nhi√™n, ta nh·∫≠n ƒë∆∞·ª£c c√°c m·∫´u c√≥ ƒë·ªô d√†i kh√°c nhau, t·ª´ 32 ƒë·∫øn 67. ƒê·ªám ƒë·ªông c√≥ nghƒ©a l√† t·∫•t c·∫£ c√°c m·∫´u trong l√¥ n√†y ph·∫£i ƒë∆∞·ª£c ƒë·ªám ƒë·∫øn chi·ªÅu d√†i 67, chi·ªÅu d√†i t·ªëi ƒëa b√™n trong l√¥. N·∫øu kh√¥ng c√≥ ƒë·ªám ƒë·ªông, t·∫•t c·∫£ c√°c m·∫´u s·∫Ω ph·∫£i ƒë∆∞·ª£c ƒë·ªám ƒë·∫øn ƒë·ªô d√†i t·ªëi ƒëa trong to√†n b·ªô t·∫≠p d·ªØ li·ªáu ho·∫∑c ƒë·ªô d√†i t·ªëi ƒëa m√† m√¥ h√¨nh c√≥ th·ªÉ ch·∫•p nh·∫≠n. H√£y ki·ªÉm tra k·ªπ xem `data_collator` c·ªßa ch√∫ng ta c√≥ t·ª± ƒë·ªông ƒë·ªám l√¥ ƒë√∫ng c√°ch hay kh√¥ng:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

Tr√¥ng kh√° ·ªïn! Gi·ªù ta ƒë√£ chuy·ªÉn t·ª´ vƒÉn b·∫£n th√¥ sang c√°c l√¥ m√† m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω, v√† ta ƒë√£ s·∫µn s√†ng tinh ch·ªânh n√≥!

{/if}

<Tip>

‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** Sao ch√©p ti·ªÅn x·ª≠ l√Ω tr√™n t·∫≠p d·ªØ li·ªáu GLUE SST-2. N√≥ h∆°i kh√°c m·ªôt ch√∫t v√¨ n√≥ bao g·ªìm c√°c c√¢u ƒë∆°n thay v√¨ c√°c c·∫∑p, nh∆∞ng ph·∫ßn c√≤n l·∫°i c·ªßa nh·ªØng g√¨ ta ƒë√£ l√†m s·∫Ω t∆∞∆°ng t·ª± nhau. V·ªõi m·ªôt th·ª≠ th√°ch kh√≥ h∆°n, h√£y c·ªë g·∫Øng vi·∫øt m·ªôt h√†m ti·ªÅn x·ª≠ l√Ω ho·∫°t ƒë·ªông tr√™n b·∫•t k·ª≥ t√°c v·ª• GLUE n√†o.

</Tip>

{#if fw === 'tf'}

B√¢y gi·ªù ch√∫ng ta ƒë√£ c√≥ b·ªô d·ªØ li·ªáu v√† b·ªô ƒë·ªëi chi·∫øu d·ªØ li·ªáu, ta c·∫ßn ph·∫£i k·∫øt h·ª£p ch√∫ng l·∫°i v·ªõi nhau. Ch√∫ng ta c√≥ th·ªÉ t·∫£i c√°c l√¥ v√† ƒë·ªëi chi·∫øu theo c√°ch th·ªß c√¥ng, nh∆∞ng c√°ch n√†y r·∫•t t·ªën c√¥ng s·ª©c v√† c√≥ l·∫Ω c≈©ng kh√¥ng hi·ªáu qu·∫£ l·∫Øm. Thay v√†o ƒë√≥, c√≥ m·ªôt ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n cung c·∫•p gi·∫£i ph√°p hi·ªáu qu·∫£ cho v·∫•n ƒë·ªÅ n√†y: `to_tf_dataset()`. Nso ƒë∆∞·ª£c bao m·ªôt `tf.data.Dataset` xung quanh t·∫≠p d·ªØ li·ªáu c·ªßa b·∫°n, v·ªõi m·ªôt ch·ª©c nƒÉng ƒë·ªëi chi·∫øu t√πy ch·ªçn. `tf.data.Dataset` l√† m·ªôt ƒë·ªãnh d·∫°ng TensorFlow g·ªëc m√† Keras c√≥ th·ªÉ s·ª≠ d·ª•ng cho `model.fit()`, v√¨ v·∫≠y ph∆∞∆°ng ph√°p n√†y ngay l·∫≠p t·ª©c chuy·ªÉn ƒë·ªïi m·ªôt ü§ó Dataset sang m·ªôt ƒë·ªãnh d·∫°ng s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán. H√£y xem n√≥ ho·∫°t ƒë·ªông v·ªõi t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng t√¥i!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

V√† n√≥ ƒë√≥! Ch√∫ng ta c√≥ th·ªÉ chuy·ªÉn nh·ªØng b·ªô d·ªØ li·ªáu ƒë√≥ sang b√†i gi·∫£ng ti·∫øp theo, n∆°i vi·ªác hu·∫•n luy·ªán s·∫Ω tr·ªü n√™n ƒë∆°n gi·∫£n m·ªôt c√°ch d·ªÖ ch·ªãu sau t·∫•t c·∫£ nh·ªØng c√¥ng vi·ªác kh√≥ khƒÉn c·ªßa vi·ªác x·ª≠ l√Ω tr∆∞·ªõc d·ªØ li·ªáu.

{/if}
