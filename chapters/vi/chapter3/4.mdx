# Báº£n huáº¥n luyá»‡n hoÃ n chá»‰nh

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

BÃ¢y giá» chÃºng ta sáº½ xem cÃ¡ch Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± nhÆ° chÃºng ta Ä‘Ã£ lÃ m trong pháº§n trÆ°á»›c mÃ  khÃ´ng cáº§n sá»­ dá»¥ng lá»›p `Trainer`. Má»™t láº§n ná»¯a, chÃºng tÃ´i giáº£ sá»­ báº¡n Ä‘Ã£ thá»±c hiá»‡n bÆ°á»›c xá»­ lÃ½ dá»¯ liá»‡u trong pháº§n 2. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t báº£n tÃ³m táº¯t ngáº¯n bao gá»“m má»i thá»© báº¡n cáº§n:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Chuáº©n bá»‹ cho huáº¥n luyá»‡n

TrÆ°á»›c khi thá»±c sá»± viáº¿t vÃ²ng láº·p huáº¥n luyá»‡n cá»§a mÃ¬nh, chÃºng ta sáº½ cáº§n xÃ¡c Ä‘á»‹nh má»™t vÃ i Ä‘á»‘i tÆ°á»£ng. Äáº§u tiÃªn lÃ  bá»™ dá»¯ liá»‡u dataloader mÃ  chÃºng tÃ´i sáº½ sá»­ dá»¥ng Ä‘á»ƒ láº·p qua cÃ¡c lÃ´. NhÆ°ng trÆ°á»›c khi chÃºng ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Ã³, chÃºng ta cáº§n Ã¡p dá»¥ng má»™t chÃºt háº­u xá»­ lÃ½ cho `tokenized_datasets` cá»§a mÃ¬nh, Ä‘á»ƒ xá»­ lÃ½ má»™t sá»‘ thá»© mÃ  `Trainer` Ä‘Ã£ lÃ m cho chÃºng ta má»™t cÃ¡ch tá»± Ä‘á»™ng. Cá»¥ thá»ƒ, chÃºng ta cáº§n:

- Loáº¡i bá» cÃ¡c cá»™t tÆ°Æ¡ng á»©ng vá»›i cÃ¡c giÃ¡ trá»‹ mÃ  mÃ´ hÃ¬nh khÃ´ng mong Ä‘á»£i (nhÆ° cá»™t `sentence1` vÃ  `sentence2`).
- Äá»•i tÃªn cá»™t `label` thÃ nh `labels` (vÃ¬ mÃ´ hÃ¬nh mong Ä‘á»£i Ä‘á»‘i sá»‘ Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  `labels`).
- Äáº·t Ä‘á»‹nh dáº¡ng cá»§a bá»™ dá»¯ liá»‡u Ä‘á»ƒ chÃºng tráº£ vá» cÃ¡c tensor PyTorch thay vÃ¬ danh sÃ¡ch.

`Tokenized_datasets` cá»§a chÃºng ta cÃ³ phÆ°Æ¡ng thá»©c cho má»—i bÆ°á»›c Ä‘Ã³:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Sau Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra xem káº¿t quáº£ cÃ³ chá»‰ cÃ³ cÃ¡c cá»™t mÃ  mÃ´ hÃ¬nh cá»§a chÃºng ta sáº½ cháº¥p nháº­n khÃ´ng:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Xong rá»“i, chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng Ä‘á»‹nh nghÄ©a cÃ¡c bá»™ dá»¯ liá»‡u cá»§a mÃ¬nh:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

Äá»ƒ nhanh chÃ³ng kiá»ƒm tra khÃ´ng cÃ³ sai sÃ³t trong quÃ¡ trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u, chÃºng ta cÃ³ thá»ƒ kiá»ƒm tra má»™t lÃ´ nhÆ° sau:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

LÆ°u Ã½ ráº±ng cÃ¡c hÃ¬nh dáº¡ng thá»±c táº¿ cÃ³ thá»ƒ sáº½ hÆ¡i khÃ¡c Ä‘á»‘i vá»›i báº¡n vÃ¬ chÃºng tÃ´i Ä‘áº·t `shuffle = True` cho dataloader huáº¥n luyá»‡n vÃ  chÃºng tÃ´i Ä‘ang Ä‘á»‡m Ä‘áº¿n Ä‘á»™ dÃ i tá»‘i Ä‘a bÃªn trong lÃ´.

BÃ¢y giá» chÃºng ta Ä‘Ã£ hoÃ n thÃ nh viá»‡c xá»­ lÃ½ trÆ°á»›c dá»¯ liá»‡u (má»™t má»¥c tiÃªu thá»a mÃ£n nhÆ°ng khÃ³ náº¯m báº¯t Ä‘á»‘i vá»›i báº¥t ká»³ ngÆ°á»i thá»±c hÃ nh ML nÃ o), hÃ£y chuyá»ƒn sang mÃ´ hÃ¬nh thÃ´i. ChÃºng ta khá»Ÿi táº¡o nÃ³ chÃ­nh xÃ¡c nhÆ° Ä‘Ã£ lÃ m trong pháº§n trÆ°á»›c:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

To make sure that everything will go smoothly during training, we pass our batch to this model:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh ğŸ¤— Transformers sáº½ tráº£ vá» lÆ°á»£ng máº¥t mÃ¡t khi `labels` Ä‘Æ°á»£c cung cáº¥p vÃ  chÃºng ta cÅ©ng nháº­n Ä‘Æ°á»£c logit (hai cho má»—i Ä‘áº§u vÃ o trong lÃ´, do Ä‘Ã³, má»™t tensor cÃ³ kÃ­ch thÆ°á»›c 8 x 2).

ChÃºng ta gáº§n nhÆ° Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ viáº¿t vÃ²ng láº·p huáº¥n luyá»‡n cá»§a mÃ¬nh! ChÃºng ta chá»‰ thiáº¿u hai thá»©: má»™t trÃ¬nh tá»‘i Æ°u hÃ³a vÃ  má»™t cÃ´ng cá»¥ láº­p lá»‹ch tá»‘c Ä‘á»™ há»c táº­p. VÃ¬ chÃºng ta Ä‘ang cá»‘ gáº¯ng tÃ¡i táº¡o nhá»¯ng gÃ¬ mÃ  `Trainer` Ä‘Ã£ lÃ m báº±ng tay, nÃªn ta sáº½ sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh tÆ°Æ¡ng tá»±. TrÃ¬nh tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi `Trainer` lÃ  `AdamW`, tÆ°Æ¡ng tá»± nhÆ° Adam, nhÆ°ng cÃ³ má»™t bÆ°á»›c ngoáº·t Ä‘á»ƒ Ä‘iá»u chá»‰nh phÃ¢n rÃ£ trá»ng sá»‘ (xem ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) cá»§a Ilya Loshchilov vÃ  Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Cuá»‘i cÃ¹ng, bá»™ láº­p lá»‹ch tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c sá»­ dá»¥ng theo máº·c Ä‘á»‹nh chá»‰ lÃ  má»™t phÃ¢n rÃ£ tuyáº¿n tÃ­nh tá»« giÃ¡ trá»‹ lá»›n nháº¥t (5e-5) xuá»‘ng 0. Äá»ƒ xÃ¡c Ä‘á»‹nh Ä‘Ãºng, chÃºng ta cáº§n biáº¿t sá»‘ bÆ°á»›c huáº¥n luyá»‡n sáº½ thá»±c hiá»‡n, Ä‘Ã³ lÃ  sá»‘ epoch muá»‘n cháº¡y nhÃ¢n vá»›i sá»‘ lÃ´ huáº¥n luyá»‡n (lÃ  Ä‘á»™ dÃ i cá»§a bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n). `Trainer` sá»­ dá»¥ng ba epoch theo máº·c Ä‘á»‹nh, vÃ¬ váº­y chÃºng tÃ´i sáº½ tuÃ¢n theo Ä‘iá»u Ä‘Ã³:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### VÃ²ng láº·p huáº¥n luyá»‡n

Má»™t Ä‘iá»u cuá»‘i cÃ¹ng: chÃºng ta sáº½ muá»‘n sá»­ dá»¥ng GPU náº¿u cÃ³ quyá»n truy cáº­p vÃ o má»™t GPU (trÃªn CPU, quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ³ thá»ƒ máº¥t vÃ i giá» thay vÃ¬ vÃ i phÃºt). Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng ta xÃ¡c Ä‘á»‹nh má»™t `device`, ta sáº½ Ä‘áº·t mÃ´ hÃ¬nh vÃ  cÃ¡c lÃ´ cá»§a ta trÃªn Ä‘Ã³:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

Giá» thÃ¬ ta Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ huáº¥n luyá»‡n rá»“i! Äá»ƒ biáº¿t khi nÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n sáº½ káº¿t thÃºc, ta thÃªm thanh tiáº¿n trÃ¬nh qua sá»‘ bÆ°á»›c huáº¥n luyá»‡n, sá»­ dá»¥ng thÆ° viá»‡n `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Báº¡n cÃ³ thá»ƒ tháº¥y ráº±ng cá»‘t lÃµi cá»§a vÃ²ng láº·p huáº¥n luyá»‡n trÃ´ng ráº¥t giá»‘ng nhÆ° trong pháº§n giá»›i thiá»‡u. ChÃºng ta Ä‘Ã£ khÃ´ng yÃªu cáº§u báº¥t ká»³ bÃ¡o cÃ¡o nÃ o, vÃ¬ váº­y vÃ²ng huáº¥n luyá»‡n nÃ y sáº½ khÃ´ng cho ta biáº¿t báº¥t ká»³ Ä‘iá»u gÃ¬ vá» cÃ¡i giÃ¡ cá»§a mÃ´ hÃ¬nh. ChÃºng ta cáº§n thÃªm má»™t vÃ²ng láº·p Ä‘Ã¡nh giÃ¡ cho Ä‘iá»u Ä‘Ã³.

### VÃ²ng láº·p Ä‘Ã¡nh giÃ¡

NhÆ° Ä‘Ã£ lÃ m trÆ°á»›c Ä‘Ã³, chÃºng ta sáº½ sá»­ dá»¥ng má»™t chá»‰ sá»‘ Ä‘Æ°á»£c cung cáº¥p bá»Ÿi thÆ° viá»‡n ğŸ¤— Evaluate. ChÃºng ta Ä‘Ã£ tháº¥y phÆ°Æ¡ng thá»©c `metric.compute()`, cÃ¡c chá»‰ sá»‘ thá»±c sá»± cÃ³ thá»ƒ tÃ­ch lÅ©y cÃ¡c lÃ´ cho ta khi xem qua vÃ²ng dá»± Ä‘oÃ¡n vá»›i phÆ°Æ¡ng thá»©c `add_batch()`. Khi ta Ä‘Ã£ tÃ­ch lÅ©y táº¥t cáº£ cÃ¡c lÃ´, chÃºng ta cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c káº¿t quáº£ cuá»‘i cÃ¹ng vá»›i `metric.compute()`. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch thá»±c hiá»‡n táº¥t cáº£ nhá»¯ng Ä‘iá»u nÃ y trong má»™t vÃ²ng láº·p Ä‘Ã¡nh giÃ¡:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Má»™t láº§n ná»¯a, káº¿t quáº£ cá»§a báº¡n sáº½ hÆ¡i khÃ¡c má»™t chÃºt vÃ¬ sá»± ngáº«u nhiÃªn trong quÃ¡ trÃ¬nh khá»Ÿi táº¡o Ä‘áº§u mÃ´ hÃ¬nh vÃ  xÃ¡o trá»™n dá»¯ liá»‡u, nhÆ°ng chÃºng pháº£i á»Ÿ trong cÃ¹ng má»™t khoáº£ng.

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Sá»­a Ä‘á»•i vÃ²ng láº·p huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh cá»§a báº¡n trÃªn táº­p dá»¯ liá»‡u SST-2.

</Tip>

### TÄƒng cÆ°á»ng trÃ­ thÃ´ng minh cá»§a vÃ²ng huáº¥n luyá»‡n vá»›i ğŸ¤— Accelerate

<Youtube id="s7dy8QRgjJ0" />

VÃ²ng láº·p huáº¥n luyá»‡n mÃ  ta Ä‘Ã£ Ä‘á»‹nh nghÄ©a trÆ°á»›c Ä‘Ã³ hoáº¡t Ä‘á»™ng tá»‘t trÃªn má»™t CPU hoáº·c GPU. NhÆ°ng báº±ng cÃ¡ch sá»­ dá»¥ng thÆ° viá»‡n [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate), chá»‰ vá»›i má»™t vÃ i Ä‘iá»u chá»‰nh, chÃºng ta cÃ³ thá»ƒ huáº¥n luyá»‡n phÃ¢n tÃ¡n trÃªn nhiá»u GPU hoáº·c TPU. Báº¯t Ä‘áº§u tá»« viá»‡c táº¡o bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  kiá»ƒm Ä‘á»‹nh, Ä‘Ã¢y lÃ  vÃ²ng láº·p huáº¥n luyá»‡n thá»§ cÃ´ng thá»±c thi:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

VÃ  Ä‘Ã¢y lÃ  má»™t sá»‘ thay Ä‘á»•i:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

DÃ²ng Ä‘áº§u tiÃªn cáº§n thÃªm lÃ  dÃ²ng nháº­p. DÃ²ng thá»© hai khá»Ÿi táº¡o má»™t Ä‘á»‘i tÆ°á»£ng `Accelerator` sáº½ xem xÃ©t mÃ´i trÆ°á»ng vÃ  khá»Ÿi táº¡o thiáº¿t láº­p phÃ¢n tÃ¡n thÃ­ch há»£p. ğŸ¤— Accelerate xá»­ lÃ½ vá»‹ trÃ­ Ä‘áº·t thiáº¿t bá»‹ cho báº¡n, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ xÃ³a cÃ¡c dÃ²ng Ä‘áº·t mÃ´ hÃ¬nh trÃªn thiáº¿t bá»‹ (hoáº·c, náº¿u báº¡n thÃ­ch, hÃ£y thay Ä‘á»•i chÃºng Ä‘á»ƒ sá»­ dá»¥ng `accelerator.device` thay vÃ¬ `device`).

Sau Ä‘Ã³, pháº§n lá»›n cÃ´ng viá»‡c chÃ­nh Ä‘Æ°á»£c thá»±c hiá»‡n trong dÃ²ng gá»­i bá»™ lÆ°u dá»¯ liá»‡u, mÃ´ hÃ¬nh vÃ  trÃ¬nh tá»‘i Æ°u hÃ³a Ä‘áº¿n `accelerator.prepare()`. Thao tÃ¡c nÃ y sáº½ bá»c cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Ã³ trong há»™p chá»©a thÃ­ch há»£p Ä‘á»ƒ Ä‘áº£m báº£o viá»‡c huáº¥n luyá»‡n Ä‘Æ°á»£c phÃ¢n phá»‘i hoáº¡t Ä‘á»™ng nhÆ° dá»± Ä‘á»‹nh. CÃ¡c thay Ä‘á»•i cÃ²n láº¡i cáº§n thá»±c hiá»‡n lÃ  loáº¡i bá» dÃ²ng Ä‘áº·t lÃ´ trÃªn `device` (má»™t láº§n ná»¯a, náº¿u báº¡n muá»‘n giá»¯ láº¡i Ä‘iá»u nÃ y, báº¡n chá»‰ cáº§n thay Ä‘á»•i nÃ³ thÃ nh sá»­ dá»¥ng `accelerator.device`) vÃ  thay tháº¿ `loss.backward()` báº±ng  `accelerator.backward(loss)`.

<Tip>
âš ï¸ Äá»ƒ hÆ°á»Ÿng lá»£i tá»« viá»‡c tÄƒng tá»‘c Ä‘á»™ do Cloud TPUs cung cáº¥p, chÃºng tÃ´i khuyÃªn báº¡n nÃªn Ä‘á»‡m cÃ¡c máº«u cá»§a mÃ¬nh theo Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh báº±ng cÃ¡c tham sá»‘ `padding="max_length"` vÃ  `max_length` cá»§a tokenizer.
</Tip>

Náº¿u báº¡n muá»‘n sao chÃ©p vÃ  dÃ¡n nÃ³ Ä‘á»ƒ mÃ y mÃ², Ä‘Ã¢y lÃ  giao diá»‡n cá»§a vÃ²ng huáº¥n luyá»‡n hoÃ n chá»‰nh vá»›i ğŸ¤— Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Äáº·t Ä‘iá»u nÃ y trong `train.py` sáº½ lÃ m cho táº­p lá»‡nh Ä‘Ã³ cÃ³ thá»ƒ cháº¡y Ä‘Æ°á»£c trÃªn báº¥t ká»³ loáº¡i thiáº¿t láº­p phÃ¢n tÃ¡n nÃ o. Äá»ƒ dÃ¹ng thá»­ trong thiáº¿t láº­p phÃ¢n tÃ¡n cá»§a báº¡n, hÃ£y cháº¡y lá»‡nh:

```bash
accelerate config
```

Ä‘iá»u nÃ y sáº½ nháº¯c báº¡n tráº£ lá»i má»™t sá»‘ cÃ¢u há»i vÃ  trÃ­ch xuáº¥t cÃ¢u tráº£ lá»i cá»§a báº¡n vÃ o tá»‡p cáº¥u hÃ¬nh bá»Ÿi lá»‡nh sau:

```
accelerate launch train.py
```

vÃ  nÃ³ sáº½ khá»Ÿi cháº¡y chÆ°Æ¡ng trÃ¬nh huáº¥n luyá»‡n phÃ¢n tÃ¡n.

Náº¿u báº¡n muá»‘n thá»­ Ä‘iá»u nÃ y trong Notebook (vÃ­ dá»¥: Ä‘á»ƒ kiá»ƒm tra nÃ³ vá»›i TPU trÃªn Colab), chá»‰ cáº§n dÃ¡n Ä‘oáº¡n mÃ£ vÃ o `training_function()` vÃ  cháº¡y Ã´ cuá»‘i cÃ¹ng vá»›i:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

Báº¡n cÃ³ thá»ƒ tÃ¬m thÃªm cÃ¡c vÃ­ dá»¥ táº¡i [ğŸ¤— Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples).
