# Dá»¯ liá»‡u lá»›n? ğŸ¤— Bá»™ dá»¯ liá»‡u Ä‘á»ƒ giáº£i cá»©u!

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {
      label: "Google Colab",
      value:
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter5/section4.ipynb",
    },
    {
      label: "Aws Studio",
      value:
        "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter5/section4.ipynb",
    },
  ]}
/>

NgÃ y nay, khÃ´ng cÃ³ gÃ¬ láº¡ khi báº¡n Ä‘ang lÃ m viá»‡c vá»›i cÃ¡c bá»™ dá»¯ liá»‡u nhiá»u gigabyte, Ä‘áº·c biá»‡t náº¿u báº¡n Ä‘ang cÃ³ káº¿ hoáº¡ch huáº¥n luyá»‡n trÆ°á»›c má»™t mÃ´ hÃ¬nh Transformer nhÆ° BERT hoáº·c GPT-2 tá»« Ä‘áº§u. Trong nhá»¯ng trÆ°á»ng há»£p nÃ y, tháº­m chÃ­ _táº£i_ dá»¯ liá»‡u cÃ³ thá»ƒ lÃ  má»™t thÃ¡ch thá»©c. VÃ­ dá»¥: kho dá»¯ liá»‡u WebText Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c GPT-2 bao gá»“m hÆ¡n 8 triá»‡u tÃ i liá»‡u vÃ  40 GB vÄƒn báº£n - viá»‡c táº£i dá»¯ liá»‡u nÃ y vÃ o RAM cá»§a mÃ¡y tÃ­nh xÃ¡ch tay cá»§a báº¡n cÃ³ thá»ƒ khiáº¿n báº¡n bá»‹ Ä‘au tim!

May máº¯n thay, ğŸ¤— Datasets Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ kháº¯c phá»¥c nhá»¯ng háº¡n cháº¿ nÃ y. NÃ³ giáº£i phÃ³ng báº¡n khá»i cÃ¡c váº¥n Ä‘á» vá» quáº£n lÃ½ bá»™ nhá»› báº±ng cÃ¡ch coi cÃ¡c táº­p dá»¯ liá»‡u lÃ  tá»‡p _Ã¡nh xáº¡ bá»™ nhá»›_ vÃ  thoÃ¡t khá»i giá»›i háº¡n á»• cá»©ng báº±ng cÃ¡ch _truyá»n táº£i trá»±c tiáº¿p_ cÃ¡c má»¥c trong má»™t kho ngá»¯ liá»‡u.

<Youtube id="JwISwTCPPWo" />

Trong pháº§n nÃ y, chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡c tÃ­nh nÄƒng nÃ y cá»§a ğŸ¤— Datasets vá»›i kho dá»¯ liá»‡u 825 GB khá»•ng lá»“ Ä‘Æ°á»£c gá»i lÃ  [Pile](https://pile.eleuther.ai). Báº¯t Ä‘áº§u thÃ´i!

## Pile lÃ  gÃ¬?

The Pile lÃ  má»™t kho ngá»¯ liá»‡u tiáº¿ng Anh Ä‘Æ°á»£c táº¡o ra bá»Ÿi [EleutherAI](https://www.eleuther.ai) Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ quy mÃ´ lá»›n. NÃ³ bao gá»“m má»™t loáº¡t cÃ¡c bá»™ dá»¯ liá»‡u, cÃ¡c bÃ i bÃ¡o khoa há»c tráº£i dÃ i, kho mÃ£ GitHub vÃ  vÄƒn báº£n web Ä‘Æ°á»£c lá»c. Kho tÃ i liá»‡u huáº¥n luyá»‡n cÃ³ sáºµn trong [khá»‘i 14GB](https://the-eye.eu/public/AI/pile/) vÃ  báº¡n cÅ©ng cÃ³ thá»ƒ táº£i xuá»‘ng má»™t sá»‘ [thÃ nh pháº§n riÃªng láº»](https://the-eye.eu/public/AI/pile_preliminary_components/). HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch xem qua táº­p dá»¯ liá»‡u PubMed Abstracts, táº­p dá»¯ liá»‡u tÃ³m táº¯t tá»« 15 triá»‡u áº¥n pháº©m y sinh trÃªn [PubMed](https://pubmed.ncbi.nlm.nih.gov/). Táº­p dá»¯ liá»‡u á»Ÿ [Ä‘á»‹nh dáº¡ng JSON Lines](https://jsonlines.org) vÃ  Ä‘Æ°á»£c nÃ©n báº±ng thÆ° viá»‡n `zstandard`, vÃ¬ váº­y trÆ°á»›c tiÃªn chÃºng ta cáº§n cÃ i Ä‘áº·t:

```py
!pip install zstandard
```

Tiáº¿p theo, chÃºng ta cÃ³ thá»ƒ táº£i táº­p dá»¯ liá»‡u báº±ng phÆ°Æ¡ng phÃ¡p cho cÃ¡c tá»‡p tá»« xa mÃ  chÃºng ta Ä‘Ã£ há»c trong [pháº§n 2](/course/chapter5/2):

```py
from datasets import load_dataset

# QuÃ¡ trÃ¬nh nÃ y máº¥t má»™t vÃ i phÃºt Ä‘á»ƒ cháº¡y, vÃ¬ váº­y hÃ£y lÃ m cá»‘c trÃ  hoáº·c cÃ  phÃª trong khi chá» Ä‘á»£i :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng cÃ³ 15,518,009 hÃ ng vÃ  2 cá»™t trong táº­p dá»¯ liá»‡u cá»§a chÃºng tÃ´i - Ä‘Ã³ lÃ  ráº¥t nhiá»u!

<Tip>

âœ Theo máº·c Ä‘á»‹nh, ğŸ¤— Datasets sáº½ giáº£i nÃ©n cÃ¡c tá»‡p cáº§n thiáº¿t Ä‘á»ƒ táº£i táº­p dá»¯ liá»‡u. Náº¿u báº¡n muá»‘n báº£o toÃ n dung lÆ°á»£ng á»• cá»©ng, báº¡n cÃ³ thá»ƒ truyá»n `DownloadConfig(delete_extracted=True)` vÃ o tham sá»‘ `download_config` cá»§a `load_dataset()`. Xem [tÃ i liá»‡u](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

</Tip>

HÃ£y kiá»ƒm tra ná»™i dung cá»§a máº«u Ä‘áº§u tiÃªn:

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

ÄÆ°á»£c rá»“i, Ä‘Ã¢y giá»‘ng nhÆ° pháº§n tÃ³m táº¯t tá»« má»™t bÃ i bÃ¡o y khoa. BÃ¢y giá» chÃºng ta hÃ£y xem chÃºng ta Ä‘Ã£ sá»­ dá»¥ng bao nhiÃªu RAM Ä‘á»ƒ táº£i táº­p dá»¯ liá»‡u!

## Sá»± ká»³ diá»‡u cá»§a Ã¡nh xáº¡ bá»™ nhá»›

Má»™t cÃ¡ch Ä‘Æ¡n giáº£n Ä‘á»ƒ Ä‘o má»©c sá»­ dá»¥ng bá»™ nhá»› trong Python lÃ  sá»­ dá»¥ng thÆ° viá»‡n [`psutil`](https://psutil.readthedocs.io/en/latest/), cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ i Ä‘áº·t báº±ng `pip` nhÆ° sau:

```python
!pip install psutil
```

NÃ³ cung cáº¥p má»™t lá»›p `Process` cho phÃ©p chÃºng ta kiá»ƒm tra viá»‡c sá»­ dá»¥ng bá»™ nhá»› cá»§a tiáº¿n trÃ¬nh hiá»‡n táº¡i nhÆ° sau:

```py
import psutil

# Process.memory_info Ä‘Æ°á»£c biá»ƒu thá»‹ báº±ng bytes, sau Ä‘Ã³ chuyá»ƒn sang megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
RAM used: 5678.33 MB
```

á» Ä‘Ã¢y thuá»™c tÃ­nh `rss` Ä‘á» cáº­p Ä‘áº¿n _resident set size_, lÃ  pháº§n bá»™ nhá»› mÃ  má»™t tiáº¿n trÃ¬nh chiáº¿m trong RAM. PhÃ©p Ä‘o nÃ y cÅ©ng bao gá»“m bá»™ nhá»› Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi trÃ¬nh thÃ´ng dá»‹ch Python vÃ  cÃ¡c thÆ° viá»‡n mÃ  chÃºng tÃ´i Ä‘Ã£ táº£i, do Ä‘Ã³, lÆ°á»£ng bá»™ nhá»› thá»±c táº¿ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº£i táº­p dá»¯ liá»‡u nhá» hÆ¡n má»™t chÃºt. Äá»ƒ so sÃ¡nh, hÃ£y xem táº­p dá»¯ liá»‡u trÃªn Ä‘Ä©a lá»›n nhÆ° tháº¿ nÃ o, sá»­ dá»¥ng thuá»™c tÃ­nh `dataset_size`. VÃ¬ káº¿t quáº£ Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng byte nhÆ° trÆ°á»›c Ä‘Ã¢y, chÃºng tÃ´i cáº§n chuyá»ƒn Ä‘á»•i thá»§ cÃ´ng nÃ³ thÃ nh gigabyte:

```py
print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

```python out
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
```

Tuyá»‡t vá»i - máº·c dÃ¹ nÃ³ gáº§n 20 GB, chÃºng ta cÃ³ thá»ƒ táº£i vÃ  truy cáº­p táº­p dá»¯ liá»‡u vá»›i RAM Ã­t hÆ¡n nhiá»u!

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Chá»n má»™t trong cÃ¡c [táº­p há»£p con](https://the-eye.eu/public/AI/pile_preliminary_components/) tá»« Pile sao cho lá»›n hÆ¡n RAM cá»§a mÃ¡y tÃ­nh xÃ¡ch tay hoáº·c mÃ¡y tÃ­nh Ä‘á»ƒ bÃ n cá»§a báº¡n, táº£i nÃ³ vá»›i ğŸ¤— Datasets, vÃ  Ä‘o dung lÆ°á»£ng RAM Ä‘Æ°á»£c sá»­ dá»¥ng. LÆ°u Ã½ ráº±ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t phÃ©p Ä‘o chÃ­nh xÃ¡c, báº¡n sáº½ muá»‘n thá»±c hiá»‡n viá»‡c nÃ y trong má»™t quy trÃ¬nh má»›i. Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y cÃ¡c kÃ­ch thÆ°á»›c Ä‘Ã£ giáº£i nÃ©n cá»§a tá»«ng táº­p há»£p con trong Báº£ng 1 cá»§a [bÃ i bÃ¡o vá» Pile](https://arxiv.org/abs/2101.00027).

</Tip>

Náº¿u báº¡n Ä‘Ã£ quen thuá»™c vá»›i Pandas, káº¿t quáº£ nÃ y cÃ³ thá»ƒ gÃ¢y báº¥t ngá» vÃ¬ theo [quy táº¯c ngÃ³n tay cÃ¡i](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) ná»•i tiáº¿ng cá»§a Wes Kinney, báº¡n thÆ°á»ng cáº§n gáº¥p 5 gáº¥p 10 láº§n RAM so vá»›i kÃ­ch thÆ°á»›c cá»§a táº­p dá»¯ liá»‡u cá»§a báº¡n. Váº­y ğŸ¤— Datasets giáº£i quyáº¿t váº¥n Ä‘á» quáº£n lÃ½ bá»™ nhá»› nÃ y nhÆ° tháº¿ nÃ o? ğŸ¤— Datasets coi má»—i táº­p dá»¯ liá»‡u nhÆ° má»™t [tá»‡p Ã¡nh xáº¡ bá»™ nhá»›](https://en.wikipedia.org/wiki/Memory-mapped_file), cung cáº¥p Ã¡nh xáº¡ giá»¯a RAM vÃ  bá»™ nhá»› há»‡ thá»‘ng tá»‡p cho phÃ©p thÆ° viá»‡n truy cáº­p vÃ  hoáº¡t Ä‘á»™ng trÃªn cÃ¡c pháº§n tá»­ cá»§a táº­p dá»¯ liá»‡u mÃ  khÃ´ng cáº§n táº£i Ä‘áº§y Ä‘á»§ vÃ o bá»™ nhá»›.

CÃ¡c tá»‡p Ä‘Æ°á»£c Ã¡nh xáº¡ bá»™ nhá»› cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c chia sáº» trÃªn nhiá»u quy trÃ¬nh, cho phÃ©p cÃ¡c phÆ°Æ¡ng thá»©c nhÆ° `Dataset.map()` Ä‘Æ°á»£c thá»±c thi song song mÃ  khÃ´ng cáº§n di chuyá»ƒn hoáº·c sao chÃ©p táº­p dá»¯ liá»‡u. BÃªn cáº¡nh Ä‘Ã³, táº¥t cáº£ cÃ¡c kháº£ nÄƒng nÃ y Ä‘á»u Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Ä‘á»‹nh dáº¡ng bá»™ nhá»› [Apache Arrow](https://arrow.apache.org) vÃ  thÆ° viá»‡n[`pyarrow`](https://arrow.apache.org/docs/python/index.html), giÃºp táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u nhanh nhÆ° chá»›p. (Äá»ƒ biáº¿t thÃªm chi tiáº¿t vá» Apache Arrow vÃ  so sÃ¡nh vá»›i Pandas, hÃ£y xem [BÃ i Ä‘Äƒng trÃªn blog cá»§a Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) trong thá»±c táº¿, hÃ£y cháº¡y má»™t bÃ i kiá»ƒm tra tá»‘c Ä‘á»™ nhá» báº±ng cÃ¡ch láº·p láº¡i táº¥t cáº£ cÃ¡c pháº§n tá»­ trong táº­p dá»¯ liá»‡u PubMed Abstracts:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ sá»­ dá»¥ng mÃ´-Ä‘un `timeit` cá»§a Python Ä‘á»ƒ Ä‘o thá»i gian thá»±c thi Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi `code_snippet`. ThÃ´ng thÆ°á»ng, báº¡n sáº½ cÃ³ thá»ƒ láº·p láº¡i táº­p dá»¯ liá»‡u vá»›i tá»‘c Ä‘á»™ tá»« vÃ i pháº§n mÆ°á»i GB/s Ä‘áº¿n vÃ i GB/s. Äiá»u nÃ y hoáº¡t Ä‘á»™ng hiá»‡u quáº£ vá»›i Ä‘áº¡i Ä‘a sá»‘ cÃ¡c á»©ng dá»¥ng, nhÆ°ng Ä‘Ã´i khi báº¡n sáº½ pháº£i lÃ m viá»‡c vá»›i má»™t táº­p dá»¯ liá»‡u quÃ¡ lá»›n, tháº­m chÃ­ khÃ´ng thá»ƒ lÆ°u trá»¯ trÃªn á»• cá»©ng cá»§a mÃ¡y tÃ­nh xÃ¡ch tay cá»§a báº¡n. VÃ­ dá»¥: náº¿u chÃºng tÃ´i cá»‘ gáº¯ng táº£i xuá»‘ng toÃ n bá»™ Pile, chÃºng tÃ´i sáº½ cáº§n 825 GB dung lÆ°á»£ng Ä‘Ä©a trá»‘ng! Äá»ƒ xá»­ lÃ½ nhá»¯ng trÆ°á»ng há»£p nÃ y, ğŸ¤— Datasets cung cáº¥p tÃ­nh nÄƒng phÃ¡t trá»±c tuyáº¿n cho phÃ©p chÃºng tÃ´i táº£i xuá»‘ng vÃ  truy cáº­p cÃ¡c pháº§n tá»­ má»™t cÃ¡ch nhanh chÃ³ng mÃ  khÃ´ng cáº§n táº£i xuá»‘ng toÃ n bá»™ táº­p dá»¯ liá»‡u. ChÃºng ta hÃ£y xem cÃ¡ch nÃ y hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o.

<Tip>

ğŸ’¡ Trong sá»• ghi chÃ©p Jupyter, báº¡n cÃ³ thá»ƒ Ä‘á»‹nh thá»i gian cho cÃ¡c Ã´ báº±ng cÃ¡ch sá»­ dá»¥ng[hÃ m ma thuáº­t `%%timeit`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).

</Tip>

## Truyá»n trá»±c tuyáº¿n táº­p dá»¯ liá»‡u

Äá»ƒ báº­t tÃ­nh nÄƒng phÃ¡t trá»±c tuyáº¿n táº­p dá»¯ liá»‡u, báº¡n chá»‰ cáº§n truyá»n tham sá»‘ `streaming=True` vÃ o hÃ m `load_dataset()`. VÃ­ dá»¥: hÃ£y táº£i láº¡i táº­p dá»¯ liá»‡u PubMed Abstracts, nhÆ°ng á»Ÿ cháº¿ Ä‘á»™ phÃ¡t trá»±c tuyáº¿n:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Thay vÃ¬ `Dataset` quen thuá»™c mÃ  chÃºng ta Ä‘Ã£ gáº·p á»Ÿ nhá»¯ng nÆ¡i khÃ¡c trong chÆ°Æ¡ng nÃ y, Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c tráº£ vá» vá»›i `streaming=True` lÃ  má»™t `IterableDataset`. NhÆ° cÃ¡i tÃªn cho tháº¥y, Ä‘á»ƒ truy cáº­p cÃ¡c pháº§n tá»­ cá»§a má»™t `IterableDataset`, chÃºng ta cáº§n pháº£i láº·p láº¡i nÃ³. ChÃºng tÃ´i cÃ³ thá»ƒ truy cáº­p pháº§n tá»­ Ä‘áº§u tiÃªn cá»§a táº­p dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¡t trá»±c tuyáº¿n cá»§a chÃºng tÃ´i nhÆ° sau:

```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

CÃ¡c pháº§n tá»­ tá»« má»™t táº­p dá»¯ liá»‡u Ä‘Æ°á»£c truyá»n trá»±c tuyáº¿n cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ nhanh chÃ³ng báº±ng cÃ¡ch sá»­ dá»¥ng `IterableDataset.map()`, ráº¥t há»¯u Ã­ch trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n náº¿u báº¡n cáº§n tokenize cÃ¡c Ä‘áº§u vÃ o. Quy trÃ¬nh hoÃ n toÃ n giá»‘ng vá»›i quy trÃ¬nh chÃºng ta Ä‘Ã£ sá»­ dá»¥ng Ä‘á»ƒ tokenize táº­p dá»¯ liá»‡u cá»§a mÃ¬nh trong [ChÆ°Æ¡ng 3](/course/chapter3), vá»›i sá»± khÃ¡c biá»‡t duy nháº¥t lÃ  cÃ¡c Ä‘áº§u ra Ä‘Æ°á»£c tráº£ vá» tá»«ng cÃ¡i má»™t:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<Tip>

ğŸ’¡ Äá»ƒ tÄƒng tá»‘c Ä‘á»™ trÃ¬nh tokenize vá»›i tÃ­nh nÄƒng phÃ¡t trá»±c tuyáº¿n, báº¡n cÃ³ thá»ƒ vÆ°á»£t qua `batched=True`, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y trong pháº§n trÆ°á»›c. NÃ³ sáº½ xá»­ lÃ½ hÃ ng loáº¡t cÃ¡c vÃ­ dá»¥; kÃ­ch thÆ°á»›c lÃ´ máº·c Ä‘á»‹nh lÃ  1,000 vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh báº±ng tham sá»‘ `batch_size`.

</Tip>

Báº¡n cÅ©ng cÃ³ thá»ƒ xÃ¡o trá»™n má»™t táº­p dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¡t trá»±c tuyáº¿n báº±ng cÃ¡ch sá»­ dá»¥ng `IterableDataset.shuffle()`, nhÆ°ng khÃ´ng giá»‘ng nhÆ° `Dataset.shuffle()` Ä‘iá»u nÃ y chá»‰ xÃ¡o trá»™n cÃ¡c pháº§n tá»­ trong má»™t `buffer_size` Ä‘Æ°á»£c Ä‘á»‹nh trÆ°á»›c:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

Trong vÃ­ dá»¥ nÃ y, chÃºng ta Ä‘Ã£ chá»n má»™t máº«u ngáº«u nhiÃªn tá»« 10,000 máº«u Ä‘áº§u tiÃªn trong bá»™ Ä‘á»‡m. Khi má»™t máº«u Ä‘Æ°á»£c truy cáº­p, vá»‹ trÃ­ cá»§a nÃ³ trong bá»™ Ä‘á»‡m sáº½ Ä‘Æ°á»£c láº¥p Ä‘áº§y báº±ng vÃ­ dá»¥ tiáº¿p theo trong kho tÃ i liá»‡u (tá»©c lÃ  vÃ­ dá»¥ thá»© 10,001 trong trÆ°á»ng há»£p trÃªn). Báº¡n cÅ©ng cÃ³ thá»ƒ chá»n cÃ¡c pháº§n tá»­ tá»« má»™t táº­p dá»¯ liá»‡u Ä‘Æ°á»£c truyá»n trá»±c tuyáº¿n báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c hÃ m `IterableDataset.take()` vÃ  `IterableDataset.skip()`, hoáº¡t Ä‘á»™ng theo cÃ¡ch tÆ°Æ¡ng tá»± nhÆ° `Dataset.select()`. VÃ­ dá»¥, Ä‘á»ƒ chá»n 5 máº«u Ä‘áº§u tiÃªn trong táº­p dá»¯ liá»‡u PubMed Abstracts, chÃºng ta cÃ³ thá»ƒ lÃ m nhÆ° sau:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

TÆ°Æ¡ng tá»±, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m `IterableDataset.skip()` Ä‘á»ƒ táº¡o cÃ¡c táº­p huáº¥n luyá»‡n vÃ  kiá»ƒm Ä‘á»‹nh tá»« má»™t táº­p dá»¯ liá»‡u xÃ¡o trá»™n nhÆ° sau:

```py
# Bá» qua 1,000 máº«u Ä‘áº§u tiÃªn vÃ  Ä‘Æ°a pháº§n cÃ²n láº¡i vÃ o táº­p huáº¥n luyá»‡n
train_dataset = shuffled_dataset.skip(1000)
# Láº¥y 1,000 vÃ­ dá»¥ Ä‘áº§u tiÃªn cho táº­p kiá»ƒm Ä‘á»‹nh
validation_dataset = shuffled_dataset.take(1000)
```

HÃ£y hoÃ n thÃ nh viá»‡c khÃ¡m phÃ¡ cá»§a chÃºng ta vá» viá»‡c truyá»n trá»±c tuyáº¿n táº­p dá»¯ liá»‡u vá»›i má»™t á»©ng dá»¥ng phá»• biáº¿n: káº¿t há»£p nhiá»u táº­p dá»¯ liá»‡u vá»›i nhau Ä‘á»ƒ táº¡o ra má»™t kho dá»¯ liá»‡u duy nháº¥t. ğŸ¤— Datasets cung cáº¥p má»™t hÃ m `interleave_datasets()` Ä‘á»ƒ chuyá»ƒn Ä‘á»•i danh sÃ¡ch cÃ¡c Ä‘á»‘i tÆ°á»£ng `IterableDataset` thÃ nh má»™t `IterableDataset` duy nháº¥t, trong Ä‘Ã³ cÃ¡c pháº§n tá»­ cá»§a táº­p dá»¯ liá»‡u má»›i Ä‘Æ°á»£c láº¥y báº±ng cÃ¡ch xen káº½ giá»¯a cÃ¡c máº«u gá»‘c. HÃ m nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch khi báº¡n Ä‘ang cá»‘ gáº¯ng káº¿t há»£p cÃ¡c táº­p dá»¯ liá»‡u lá»›n, vÃ¬ váº­y, Ä‘á»ƒ lÃ m vÃ­ dá»¥, hÃ£y truyá»n trá»±c tuyáº¿n táº­p con FreeLaw cá»§a Pile, lÃ  táº­p dá»¯ liá»‡u 51 GB vá» cÃ¡c Ã½ kiáº¿n phÃ¡p lÃ½ tá»« cÃ¡c tÃ²a Ã¡n Hoa Ká»³:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

Táº­p dá»¯ liá»‡u nÃ y Ä‘á»§ lá»›n Ä‘á»ƒ kÃ­ch hoáº¡t RAM cá»§a háº§u háº¿t cÃ¡c mÃ¡y tÃ­nh xÃ¡ch tay, nhÆ°ng chÃºng ta váº«n cÃ³ thá»ƒ táº£i vÃ  truy cáº­p nÃ³ mÃ  khÃ´ng pháº£i Ä‘á»• má»“ hÃ´i! BÃ¢y giá» chÃºng ta hÃ£y káº¿t há»£p cÃ¡c máº«u tá»« bá»™ dá»¯ liá»‡u FreeLaw vÃ  PubMed Abstracts vá»›i hÃ m `interleave_datasets()`:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ sá»­ dá»¥ng hÃ m `islice()` tá»« mÃ´-Ä‘un `itertools` cá»§a Python Ä‘á»ƒ chá»n hai máº«u Ä‘áº§u tiÃªn tá»« táº­p dá»¯ liá»‡u káº¿t há»£p vÃ  chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng chÃºng khá»›p vá»›i cÃ¡c vÃ­ dá»¥ Ä‘áº§u tiÃªn tá»« má»—i trong hai táº­p dá»¯ liá»‡u nguá»“n.

Cuá»‘i cÃ¹ng, náº¿u báº¡n muá»‘n phÃ¡t trá»±c tuyáº¿n toÃ n bá»™ 825 GB cá»§a Pile, báº¡n cÃ³ thá»ƒ láº¥y táº¥t cáº£ cÃ¡c tá»‡p Ä‘Ã£ chuáº©n bá»‹ nhÆ° sau:

```py
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python out
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play â€œSurvival of the Tastiestâ€ on Android, and on the web...'}
```

<Tip>

âœï¸ **Thá»­ nghiá»‡m thÃ´i!** Sá»­ dá»¥ng má»™t trong nhá»¯ng kho tÃ i liá»‡u Common Crawl lá»›n nhÆ° [`mc4`](https://huggingface.co/datasets/mc4) hoáº·c [`oscar`](https://huggingface.co/datasets/oscar) Ä‘á»ƒ táº¡o táº­p dá»¯ liá»‡u Ä‘a ngÃ´n ngá»¯ trá»±c tuyáº¿n thá»ƒ hiá»‡n tá»· lá»‡ nÃ³i cá»§a cÃ¡c ngÃ´n ngá»¯ á»Ÿ quá»‘c gia báº¡n chá»n. VÃ­ dá»¥: bá»‘n ngÃ´n ngá»¯ quá»‘c gia á»Ÿ Thá»¥y SÄ© lÃ  tiáº¿ng Äá»©c, tiáº¿ng PhÃ¡p, tiáº¿ng Ã vÃ  tiáº¿ng La MÃ£, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ thá»­ táº¡o má»™t kho ngá»¯ liá»‡u tiáº¿ng Thá»¥y SÄ© báº±ng cÃ¡ch láº¥y máº«u cÃ¡c táº­p há»£p con Oscar theo tá»· lá»‡ nÃ³i cá»§a chÃºng.

</Tip>

Giá» Ä‘Ã¢y, báº¡n cÃ³ táº¥t cáº£ cÃ¡c cÃ´ng cá»¥ cáº§n thiáº¿t Ä‘á»ƒ táº£i vÃ  xá»­ lÃ½ cÃ¡c táº­p dá»¯ liá»‡u á»Ÿ má»i hÃ¬nh dáº¡ng vÃ  kÃ­ch thÆ°á»›c - nhÆ°ng trá»« khi báº¡n Ä‘áº·c biá»‡t may máº¯n, sáº½ Ä‘áº¿n má»™t thá»i Ä‘iá»ƒm trong hÃ nh trÃ¬nh NLP cá»§a báº¡n, nÆ¡i báº¡n sáº½ pháº£i thá»±c sá»± táº¡o má»™t táº­p dá»¯ liá»‡u Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» váº¥n Ä‘á» trong táº§m tay. ÄÃ³ lÃ  chá»§ Ä‘á» cá»§a pháº§n tiáº¿p theo!
