<FrameworkSwitchCourse {fw} />

# T√¨m ki·∫øm ng·ªØ nghƒ©a v·ªõi FAISS

{#if fw === 'pt'}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter5/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/vi/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/vi/chapter5/section6_tf.ipynb"},
]} />

{/if}

Trong [ph·∫ßn 5](/course/chapter5/5), ch√∫ng ta ƒë√£ t·∫°o t·∫≠p d·ªØ li·ªáu v·ªÅ c√°c v·∫•n ƒë·ªÅ GitHub v√† nh·∫≠n x√©t t·ª´ kho l∆∞u tr·ªØ ü§ó Datasets. Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng th√¥ng tin n√†y ƒë·ªÉ x√¢y d·ª±ng m·ªôt c√¥ng c·ª• t√¨m ki·∫øm c√≥ th·ªÉ gi√∫p ta t√¨m c√¢u tr·∫£ l·ªùi cho nh·ªØng c√¢u h·ªèi c·∫•p b√°ch nh·∫•t v·ªÅ th∆∞ vi·ªán!

<Youtube id="OATCgQtNX2o"/>

## S·ª≠ d·ª•ng nh√∫ng bi·ªÉu di·ªÖn t·ª´ cho t√¨m ki·∫øm ng·ªØ nghƒ©a

Nh∆∞ ch√∫ng ta ƒë√£ th·∫•y trong [Ch∆∞∆°ng 1](/course/chapter1), c√°c m√¥ h√¨nh ng√¥n ng·ªØ d·ª±a tr√™n Transformer ƒë·∫°i di·ªán cho m·ªói token trong m·ªôt kho·∫£ng vƒÉn b·∫£n d∆∞·ªõi d·∫°ng m·ªôt _vector nhugns bi·ªÉu di·ªÖn t·ª´_. H√≥a ra ng∆∞·ªùi ta c√≥ th·ªÉ "g·ªôp" c√°c bi·ªÉu di·ªÖn ri√™ng l·∫ª ƒë·ªÉ t·∫°o bi·ªÉu di·ªÖn vect∆° cho to√†n b·ªô c√¢u, ƒëo·∫°n vƒÉn ho·∫∑c to√†n b·ªô (trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p) t√†i li·ªáu. Sau ƒë√≥, c√°c ph√©p nh√∫ng n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√¨m c√°c t√†i li·ªáu t∆∞∆°ng t·ª± trong kho t√†i li·ªáu b·∫±ng c√°ch t√≠nh to√°n ƒë·ªô t∆∞∆°ng t·ª± c·ªßa s·∫£n ph·∫©m (ho·∫∑c m·ªôt s·ªë ch·ªâ s·ªë t∆∞∆°ng t·ª± kh√°c) gi·ªØa m·ªói bi·ªÖu di·ªÖn v√† tr·∫£ v·ªÅ c√°c t√†i li·ªáu c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng l·ªõn nh·∫•t.

Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng c√°c bi·ªÉu di·ªÖn t·ª´ ƒë·ªÉ ph√°t tri·ªÉn m·ªôt c√¥ng c·ª• t√¨m ki·∫øm ng·ªØ nghƒ©a. C√°c c√¥ng c·ª• t√¨m ki·∫øm n√†y cung c·∫•p m·ªôt s·ªë l·ª£i th·∫ø so v·ªõi c√°c ph∆∞∆°ng ph√°p ti·∫øp c·∫≠n th√¥ng th∆∞·ªùng d·ª±a tr√™n vi·ªác k·∫øt h·ª£p c√°c t·ª´ kh√≥a trong m·ªôt truy v·∫•n v·ªõi c√°c t√†i li·ªáu.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Semantic search."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Semantic search."/>
</div>

## T·∫£i v√† chu·∫©n b·ªã t·∫≠p d·ªØ li·ªáu

ƒêi·ªÅu ƒë·∫ßu ti√™n ch√∫ng ta c·∫ßn l√†m l√† t·∫£i xu·ªëng t·∫≠p d·ªØ li·ªáu v·ªÅ c√°c s·ª± c·ªë GitHub, v√¨ v·∫≠y h√£y s·ª≠ d·ª•ng th∆∞ vi·ªán ü§ó Hub ƒë·ªÉ gi·∫£i quy·∫øt URL n∆°i t·ªáp c·ªßa ch√∫ng ta ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n Hugging Face Hub:

```py
from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-comments.jsonl",
    repo_type="dataset",
)
```

V·ªõi URL ƒë∆∞·ª£c l∆∞u tr·ªØ trong `data_files`, sau ƒë√≥ ch√∫ng ta c√≥ th·ªÉ t·∫£i t·∫≠p d·ªØ li·ªáu t·ª´ xa b·∫±ng ph∆∞∆°ng ph√°p ƒë√£ ƒë∆∞·ª£c gi·ªõi thi·ªáu trong [ph·∫ßn 2](/course/chapter5/2):

```py
from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

·ªû ƒë√¢y ch√∫ng ta ƒë√£ ch·ªâ ƒë·ªãnh t√°ch `train` m·∫∑c ƒë·ªãnh trong `load_dataset()`, v√¨ v·∫≠y n√≥ tr·∫£ v·ªÅ m·ªôt `Dataset` thay v√¨ `DatasetDict`. Tr√¨nh t·ª± ƒë·∫ßu ti√™n c·ªßa doanh nghi·ªáp l√† l·ªçc ra c√°c y√™u c·∫ßu k√©o, v√¨ nh·ªØng y√™u c·∫ßu n√†y c√≥ xu h∆∞·ªõng hi·∫øm khi ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ tr·∫£ l·ªùi c√°c truy v·∫•n c·ªßa ng∆∞·ªùi d√πng v√† s·∫Ω t·∫°o ra nhi·ªÖu trong c√¥ng c·ª• t√¨m ki·∫øm m√¨nh. Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m `Dataset.filter()` ƒë√£ quen thu·ªôc v·ªõi b·∫°n ƒë·ªÉ lo·∫°i tr·ª´ c√°c h√†ng n√†y trong t·∫≠p d·ªØ li·ªáu c·ªßa m√¨nh. C√πng l√∫c ƒë√≥, h√£y c√πng l·ªçc ra c√°c h√†ng kh√¥ng c√≥ nh·∫≠n x√©t, v√¨ nh·ªØng h√†ng n√†y kh√¥ng cung c·∫•p c√¢u tr·∫£ l·ªùi cho c√°c truy v·∫•n c·ªßa ng∆∞·ªùi d√πng:

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

Ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng c√≥ r·∫•t nhi·ªÅu c·ªôt trong t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng ta, h·∫ßu h·∫øt trong s·ªë ƒë√≥ ch√∫ng ta kh√¥ng c·∫ßn ph·∫£i x√¢y d·ª±ng c√¥ng c·ª• t√¨m ki·∫øm c·ªßa m√¨nh. T·ª´ g√≥c ƒë·ªô t√¨m ki·∫øm, c√°c c·ªôt ch·ª©a nhi·ªÅu th√¥ng tin nh·∫•t l√† `title`, `body`, v√† `comments`,  trong khi `html_url` cung c·∫•p cho ch√∫ng ta m·ªôt li√™n k·∫øt tr·ªè v·ªÅ ngu·ªìn. H√£y s·ª≠ d·ª•ng h√†m `Dataset.remove_columns()` ƒë·ªÉ x√≥a ph·∫ßn c√≤n l·∫°i:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

ƒê·ªÉ t·∫°o c√°c bi·ªÉu di·ªÖn, ch√∫ng ta s·∫Ω b·ªï sung m·ªói nh·∫≠n x√©t v·ªõi ti√™u ƒë·ªÅ v√† n·ªôi dung c·ªßa v·∫•n ƒë·ªÅ, v√¨ c√°c tr∆∞·ªùng n√†y th∆∞·ªùng bao g·ªìm th√¥ng tin ng·ªØ c·∫£nh h·ªØu √≠ch. V√¨ c·ªôt `comments` c·ªßa hi·ªán l√† danh s√°ch c√°c nh·∫≠n x√©t cho t·ª´ng v·∫•n ƒë·ªÅ, ch√∫ng t√¥i c·∫ßn kh√°m ph√° c√°c c·ªôt ƒë·ªÉ m·ªói h√†ng bao g·ªìm m·ªôt tuple `(html_url, title, body, comment)`. Trong Pandas, ch√∫ng ta c√≥ th·ªÉ th·ª±c hi·ªán vi·ªác n√†y b·∫±ng h√†m [h√†m `DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), t·∫°o m·ªôt h√†ng m·ªõi cho m·ªói ph·∫ßn t·ª≠ trong c·ªôt gi·ªëng nh∆∞ danh s√°ch, trong khi sao ch√©p t·∫•t c·∫£ c√°c gi√° tr·ªã c·ªôt kh√°c. ƒê·ªÉ xem ƒëi·ªÅu n√†y ho·∫°t ƒë·ªông, tr∆∞·ªõc ti√™n ch√∫ng ta h√£y ch√∫ng chuy·ªÉn th√†nh ƒë·ªãnh d·∫°ng Pandas `DataFrame`:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

N·∫øu ta ki·ªÉm tra h√†ng ƒë·∫ßu ti√™n trong `DataFrame` n√†y, ch√∫ng ta c√≥ th·ªÉ th·∫•y c√≥ b·ªën nh·∫≠n x√©t li√™n quan ƒë·∫øn v·∫•n ƒë·ªÅ n√†y:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in Ôºö\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Khi ch√∫ng ta kh√°m ph√° `df`, ch√∫ng t√¥i mong ƒë·ª£i nh·∫≠n ƒë∆∞·ª£c m·ªôt h√†ng cho m·ªói nh·∫≠n x√©t n√†y. H√£y ki·ªÉm tra xem n√≥ ƒë√£ ƒë√∫ng ch∆∞a:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in Ôºö\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

Tuy·ªát v·ªùi, ch√∫ng ta c√≥ th·ªÉ th·∫•y c√°c h√†ng ƒë√£ ƒë∆∞·ª£c nh√¢n r·ªông, v·ªõi c·ªôt `comments` ch·ª©a c√°c nh·∫≠n x√©t ri√™ng l·∫ª! B√¢y gi·ªù ch√∫ng ta ƒë√£ ho√†n th√†nh v·ªõi Pandas, ch√∫ng ta c√≥ th·ªÉ nhanh ch√≥ng chuy·ªÉn tr·ªü l·∫°i `Dataset` b·∫±ng c√°ch t·∫£i `DataFrame` v√†o b·ªô nh·ªõ:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

ƒê∆∞·ª£c r·ªìi, ƒëi·ªÅu n√†y ƒë√£ cho ch√∫ng ta v√†i ngh√¨n nh·∫≠n x√©t ƒë·ªÉ l√†m vi·ªác c√πng!


<Tip>

‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** C√πng xem li·ªáu b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng `Dataset.map()` ƒë·ªÉ kh√°m ph√° c·ªôt `comments` c·ªßa `issues_dataset` _m√† kh√¥ng c·∫ßn_ s·ª≠ d·ª•ng Pandas hay kh√¥ng. N√≥ s·∫Ω h∆°i kh√≥ khƒÉn m·ªôt ch√∫t; b·∫°n c√≥ th·ªÉ xem ph·∫ßn ["Batch mapping"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) c·ªßa t√†i li·ªáu ü§ó Datasets, m·ªôt t√†i li·ªáu h·ªØu √≠ch cho t√°c v·ª• n√†y.

</Tip>

B√¢y gi·ªù ch√∫ng ta ƒë√£ c√≥ m·ªôt nh·∫≠n x√©t tr√™n m·ªói h√†ng, h√£y t·∫°o m·ªôt c·ªôt `comments_length` m·ªõi ch·ª©a s·ªë t·ª´ tr√™n m·ªói nh·∫≠n x√©t:

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng c·ªôt m·ªõi n√†y ƒë·ªÉ l·ªçc ra c√°c nh·∫≠n x√©t ng·∫Øn, th∆∞·ªùng bao g·ªìm nh·ªØng th·ª© nh∆∞ "cc @lewtun" ho·∫∑c "Thanks!" kh√¥ng li√™n quan ƒë·∫øn c√¥ng c·ª• t√¨m ki·∫øm c·ªßa m√¨nh. Kh√¥ng c√≥ con s·ªë ch√≠nh x√°c ƒë·ªÉ ch·ªçn cho b·ªô l·ªçc, nh∆∞ng kho·∫£ng 15 t·ª´ c√≥ v·∫ª nh∆∞ l√† m·ªôt kh·ªüi ƒë·∫ßu t·ªët:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

Sau khi d·ªçn d·∫πp t·∫≠p d·ªØ li·ªáu m·ªôt ch√∫t, h√£y gh√©p ti√™u ƒë·ªÅ, m√¥ t·∫£ v√† nh·∫≠n x√©t c·ªßa v·∫•n ƒë·ªÅ v·ªõi nhau trong m·ªôt c·ªôt `text` m·ªõi. Nh∆∞ th∆∞·ªùng l·ªá, ch√∫ng ta s·∫Ω vi·∫øt m·ªôt h√†m ƒë∆°n gi·∫£n m√† ch√∫ng ta c√≥ th·ªÉ truy·ªÅn v√†o `Dataset.map()`:

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)
```

Cu·ªëi c√πng th√¨ ch√∫ng ta c≈©ng ƒë√£ s·∫µn s√†ng ƒë·ªÉ t·∫°o m·ªôt s·ªë bi·ªÉu ƒëi·ªÖn! Ch√∫ng ta h√£y xem n√†o.

## T·∫°o ra bi·ªÉu di·ªÖn vƒÉn b·∫£n

Ch√∫ng ta ƒë√£ th·∫•y trong [Ch∆∞∆°ng 2](/course/chapter2) r·∫±ng ta c√≥ th·ªÉ nh·∫≠n ƒë∆∞·ª£c token bi·ªÖu di·ªÖn nh√∫ng b·∫±ng c√°ch s·ª≠ d·ª•ng l·ªõp `AutoModel`. T·∫•t c·∫£ nh·ªØng g√¨ ch√∫ng ta c·∫ßn l√†m l√† ch·ªçn m·ªôt checkpoint ph√π h·ª£p ƒë·ªÉ t·∫£i m√¥ h√¨nh t·ª´ ƒë√≥. May m·∫Øn thay, c√≥ m·ªôt th∆∞ vi·ªán t√™n l√†  `sentence-transformers` d√†nh ri√™ng cho vi·ªác t·∫°o c√°c bi·ªÉu di·ªÖn n√†y. Nh∆∞ ƒë∆∞·ª£c m√¥ t·∫£ trong [t√†i li·ªáu](https://www.sbert.net/examples/appices/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) c·ªßa th∆∞ vi·ªán, tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng c·ªßa ta l√† m·ªôt v√≠ d·ª• v·ªÅ _t√¨m ki·∫øm ng·ªØ nghƒ©a phi ƒë·ªëi x·ª©ng_ b·ªüi v√¨ ch√∫ng ta c√≥ m·ªôt truy v·∫•n ng·∫Øn c√≥ c√¢u tr·∫£ l·ªùi ta mu·ªën t√¨m th·∫•y trong m·ªôt t√†i li·ªáu l·∫°i d√†i h∆°n nhi·ªÅu, ch·∫≥ng h·∫°n nh∆∞ m·ªôt nh·∫≠n x√©t v·ªÅ v·∫•n ƒë·ªÅ. [B·∫£ng t·ªïng quan v·ªÅ m√¥ h√¨nh](https://www.sbert.net/docs/pretrained_models.html#model-overview) trong ph·∫ßn t√†i li·ªáu ch·ªâ ra r·∫±ng checkpoint `multi-qa-mpnet-base-dot-v1` c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t cho t√¨m ki·∫øm ng·ªØ nghƒ©a, v√¨ v·∫≠y ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng n√≥ cho ·ª©ng d·ª•ng c·ªßa m√¨nh. Ch√∫ng ta c≈©ng s·∫Ω t·∫£i tokenizer b·∫±ng c√°ch s·ª≠ d·ª•ng c√πng m·ªôt checkpoint:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

ƒê·ªÉ tƒÉng t·ªëc qu√° tr√¨nh bi·ªÉu di·ªÖn, ta s·∫Ω gi√∫p ƒë·∫∑t m√¥ h√¨nh v√† ƒë·∫ßu v√†o tr√™n thi·∫øt b·ªã GPU, v√¨ v·∫≠y h√£y l√†m ƒëi·ªÅu ƒë√≥ ngay b√¢y gi·ªù th√¥i:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

L∆∞u √Ω r·∫±ng ch√∫ng ta ƒë·∫∑t `from_pt=True` nh∆∞ m·ªôt tham s·ªë c·ªßa ph∆∞∆°ng th·ª©c `from_pretrained()`. ƒêi·ªÅu n√†y l√† b·ªüi checkpoint `multi-qa-mpnet-base-dot-v1` ch·ªâ c√≥ tr·ªçng s·ªë Pytorch, v√¨ v·∫≠y thi·∫øt l·∫≠p `from_pt=True` s·∫Ω t·ª± ƒë·ªông chuy·ªÉn ch√∫ng v·ªÅ ƒë·ªãnh d·∫°ng TensorFlow cho ch√∫ng ta. Nh∆∞ c√≥ th·ªÉ th·∫•y, n√≥ r·∫•t ƒë∆°n gi·∫£n ƒë·ªÉ chuy·ªÉn gi·ªØa hai khung n√†y trong ü§ó Transformers!

{/if}

Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥, ch√∫ng ta mu·ªën bi·ªÉu di·ªÖn m·ªói m·ª•c trong kho d·ªØ li·ªáu c√°c v·∫•n ƒë·ªÅ GitHub c·ªßa m√¨nh d∆∞·ªõi d·∫°ng m·ªôt vect∆° duy nh·∫•t, v√¨ v·∫≠y ch√∫ng ta c·∫ßn "g·ªôp" ho·∫∑c t√≠nh trung b√¨nh c√°c l·∫ßn bi·ªÖu di·ªÖn token theo m·ªôt c√°ch n√†o ƒë√≥. M·ªôt c√°ch ti·∫øp c·∫≠n ph·ªï bi·∫øn l√† th·ª±c hi·ªán *CLS pooling* tr√™n ƒë·∫ßu ra c·ªßa m√¥ h√¨nh, n∆°i ta ch·ªâ c·∫ßn thu th·∫≠p tr·∫°ng th√°i ·∫©n cu·ªëi c√πng cho token ƒë·∫∑c bi·ªát `[CLS]`. H√†m sau th·ª±c hi·ªán th·ªß thu·∫≠t n√†y cho ch√∫ng ta:

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

Ti·∫øp theo, ch√∫ng t√¥i s·∫Ω t·∫°o m·ªôt ch·ª©c nƒÉng tr·ª£ gi√∫p s·∫Ω tokanize danh s√°ch c√°c t√†i li·ªáu, ƒë·∫∑t c√°c tensor tr√™n GPU, ƒë∆∞a ch√∫ng v√†o m√¥ h√¨nh v√† cu·ªëi c√πng √°p d·ª•ng CLS g·ªôp cho c√°c ƒë·∫ßu ra:

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Ch√∫ng ta c√≥ th·ªÉ ki·ªÉm tra ch·ª©c nƒÉng ho·∫°t ƒë·ªông b·∫±ng c√°ch cung c·∫•p cho n√≥ ƒëo·∫°n vƒÉn b·∫£n ƒë·∫ßu ti√™n trong kho t√†i li·ªáu v√† ki·ªÉm tra h√¨nh d·∫°ng ƒë·∫ßu ra:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

Tuy·ªát v·ªùi, ch√∫ng ta ƒë√£ chuy·ªÉn ƒë·ªïi m·ª•c nh·∫≠p ƒë·∫ßu ti√™n trong kho t√†i li·ªáu c·ªßa m√¨nh th√†nh m·ªôt vect∆° 768 chi·ªÅu! Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng `Dataset.map()` ƒë·ªÉ √°p d·ª•ng h√†m `get_embeddings()` cho m·ªói h√†ng trong kho t√†i li·ªáu c·ªßa m√¨nh, v√¨ v·∫≠y h√£y t·∫°o m·ªôt c·ªôt `embeddings` m·ªõi nh∆∞ sau:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Ch√∫ng t√¥i c√≥ th·ªÉ ki·ªÉm tra h√†m c√≥ ho·∫°t ƒë·ªông kh√¥ng b·∫±ng c√°ch cung c·∫•p cho n√≥ vƒÉn b·∫£n ƒë·∫ßu ti√™n trong kho t√†i li·ªáu v√† ki·ªÉm tra h√¨nh d·∫°ng ƒë·∫ßu ra:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

Tuy·ªát v·ªùi, ch√∫ng ta ƒë√£ chuy·ªÉn ƒë·ªïi m·ª•c nh·∫≠p ƒë·∫ßu ti√™n trong kho t√†i li·ªáu c·ªßa m√¨nh th√†nh m·ªôt vect∆° 768 chi·ªÅu! Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng `Dataset.map()` ƒë·ªÉ √°p d·ª•ng h√†m `get_embeddings()` cho m·ªói h√†ng trong kho t√†i li·ªáu c·ªßa m√¨nh, v√¨ v·∫≠y h√£y t·∫°o m·ªôt c·ªôt `embeddings` m·ªõi nh∆∞ sau:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

L∆∞u √Ω r·∫±ng ch√∫ng ta ƒë√£ chuy·ªÉn ƒë·ªïi c√°c bi·ªÉu di·ªÖn sang th√†nh m·∫£ng NumPy - ƒë√≥ l√† v√¨ ü§ó Datasets y√™u c·∫ßu ƒë·ªãnh d·∫°ng n√†y khi ta c·ªë g·∫Øng l·∫≠p ch·ªâ m·ª•c ch√∫ng b·∫±ng FAISS, ƒëi·ªÅu m√† ta s·∫Ω th·ª±c hi·ªán ti·∫øp theo.

## S·ª≠ d·ª•ng FAISS ƒë·ªÉ t√¨m ki·∫øm ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng hi·ªáu qu·∫£

B√¢y gi·ªù ch√∫ng ta ƒë√£ c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu v·ªÅ c√°c bi·ªÉu di·ªÖn, ch√∫ng ta c·∫ßn m·ªôt s·ªë c√°ch ƒë·ªÉ t√¨m ki·∫øm ch√∫ng. ƒê·ªÉ l√†m ƒëi·ªÅu n√†y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu ƒë·∫∑c bi·ªát trong ü§ó Datasets ƒë∆∞·ª£c g·ªçi l√† _FAISS index_. [FAISS](https://faiss.ai/) (vi·∫øt t·∫Øt c·ªßa Facebook AI Similarity Search) l√† m·ªôt th∆∞ vi·ªán cung c·∫•p c√°c thu·∫≠t to√°n hi·ªáu qu·∫£ ƒë·ªÉ nhanh ch√≥ng t√¨m ki·∫øm v√† ph√¢n c·ª•m c√°c vect∆° nh√∫ng bi·ªÉu di·ªÖn.

√ù t∆∞·ªüng c∆° b·∫£n ƒë·∫±ng sau FAISS l√† t·∫°o ra m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu ƒë·∫∑c bi·ªát ƒë∆∞·ª£c g·ªçi l√† _index_ hay _ch·ªâ m·ª•c_ cho ph√©p ng∆∞·ªùi ta t√¨m th·∫•y c√°c bi·ªÉu di·ªÖn nh√∫ng n√†o t∆∞∆°ng t·ª± nh∆∞ bi·ªÉu di·ªÖn nh√∫ng ƒë·∫ßu v√†o. T·∫°o ch·ªâ m·ª•c FAISS trong ü§ó Datasets r·∫•t ƒë∆°n gi·∫£n - ta s·ª≠ d·ª•ng h√†m `Dataset.add_faiss_index()` v√† ch·ªâ ƒë·ªãnh c·ªôt n√†o trong t·∫≠p d·ªØ li·ªáu m√† ta mu·ªën l·∫≠p ch·ªâ m·ª•c:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

B√¢y gi·ªù ch√∫ng ta c√≥ th·ªÉ th·ª±c hi·ªán c√°c truy v·∫•n tr√™n ch·ªâ m·ª•c n√†y b·∫±ng c√°ch th·ª±c hi·ªán tra c·ª©u nh·ªØng m·∫´u l√¢n c·∫≠n nh·∫•t th√¥ng qua h√†m `Dataset.get_nearest_examples()`. H√£y ki·ªÉm tra ƒëi·ªÅu n√†y b·∫±ng c√°ch bi·ªÉu di·ªÖn m·ªôt c√¢u h·ªèi nh∆∞ sau:

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

C≈©ng gi·ªëng nh∆∞ c√°c t√†i li·ªáu, gi·ªù ƒë√¢y ch√∫ng ta c√≥ m·ªôt vect∆° 768 chi·ªÅu ƒë·∫°i di·ªán cho truy v·∫•n, m√† ch√∫ng ta c√≥ th·ªÉ so s√°nh v·ªõi to√†n b·ªô kho d·ªØ li·ªáu ƒë·ªÉ t√¨m ra c√°c c√°ch bi·ªÉu di·ªÖn t∆∞∆°ng t·ª± nh·∫•t:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

H√†m `Dataset.get_nearest_examples()` tr·∫£ v·ªÅ m·ªôt lo·∫°t ƒëi·ªÉm x·∫øp h·∫°ng s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa truy v·∫•n v√† t√†i li·ªáu v√† m·ªôt t·∫≠p h·ª£p c√°c m·∫´u t∆∞∆°ng ·ª©ng (·ªü ƒë√¢y, l√† 5 k·∫øt qu·∫£ ph√π h·ª£p nh·∫•t). H√£y thu th·∫≠p nh·ªØng th·ª© n√†y v√†o m·ªôt `pandas.DataFrame` ƒë·ªÉ ch√∫ng ta c√≥ th·ªÉ d·ªÖ d√†ng s·∫Øp x·∫øp ch√∫ng:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

B√¢y gi·ªù ch√∫ng ta c√≥ th·ªÉ l·∫∑p l·∫°i m·ªôt v√†i h√†ng ƒë·∫ßu ti√™n ƒë·ªÉ xem truy v·∫•n c·ªßa ch√∫ng ta kh·ªõp v·ªõi c√°c nh·∫≠n x√©t c√≥ s·∫µn nh∆∞ th·∫ø n√†o:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

Kh√¥ng t·ªá! L·∫ßn truy c·∫≠p th·ª© hai c·ªßa ch√∫ng ta d∆∞·ªùng nh∆∞ ph√π h·ª£p v·ªõi truy v·∫•n.

<Tip>

‚úèÔ∏è **Th·ª≠ nghi·ªám th√¥i!** T·∫°o truy v·∫•n c·ªßa ri√™ng b·∫°n v√† xem li·ªáu b·∫°n c√≥ th·ªÉ t√¨m th·∫•y c√¢u tr·∫£ l·ªùi trong c√°c t√†i li·ªáu ƒë√£ truy xu·∫•t hay kh√¥ng. B·∫°n c√≥ th·ªÉ ph·∫£i tƒÉng tham s·ªë `k` trong `Dataset.get_nearest_examples()` ƒë·ªÉ m·ªü r·ªông t√¨m ki·∫øm.

</Tip>
