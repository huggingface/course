<FrameworkSwitchCourse {fw} />

# ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ja/chapter7/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ja/chapter7/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ja/chapter7/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ja/chapter7/section3_tf.ipynb"},
]} />

{/if}

Transformerãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚€å¤šãã®NLPã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒã‚®ãƒ³ã‚° ãƒ•ã‚§ã‚¤ã‚¹ ãƒãƒ–ã‹ã‚‰äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–ã‚Šå‡ºã—ã€è‡ªåˆ†ãŒå®Ÿè¡Œã—ãŸã„ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ã£ã¦å¾®èª¿æ•´ã‚’è¡Œã†ã ã‘ã§ã‚ˆã„ã®ã§ã™ã€‚äº‹å‰å­¦ç¿’ã«ä½¿ã‚ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ã¨å¾®èª¿æ•´ã«ä½¿ã†ã‚³ãƒ¼ãƒ‘ã‚¹ãŒã‚ã¾ã‚Šé•ã‚ãªã„é™ã‚Šã€è»¢ç§»å­¦ç¿’ã¯é€šå¸¸è‰¯ã„çµæœã‚’ç”Ÿã¿å‡ºã—ã¾ã™ã€‚

ã—ã‹ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ˜ãƒƒãƒ‰éƒ¨ã ã‘ã‚’å¯¾è±¡ã«ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¡Œã†å‰ã«ã€ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã„ã‚±ãƒ¼ã‚¹ã‚‚ã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«æ³•çš„å¥‘ç´„ã‚„ç§‘å­¦è«–æ–‡ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€BERTã®ã‚ˆã†ãªç´ ã®Transformerãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€ã‚³ãƒ¼ãƒ‘ã‚¹å†…ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®å˜èªã‚’ç¨€ãªãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã†ãŸã‚ã€çµæœã¨ã—ã¦æº€è¶³ã®ã„ãæ€§èƒ½ãŒå¾—ã‚‰ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ãƒ‡ãƒ¼ã‚¿ã§è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€å¤šãã®ä¸‹æµã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã€ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯é€šå¸¸ä¸€åº¦ã ã‘è¡Œãˆã°ã‚ˆã„ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

ã“ã®ã‚ˆã†ã«ã€äº‹å‰ã«å­¦ç¿’ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ãƒ‡ãƒ¼ã‚¿ã§å¾®èª¿æ•´ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€é€šå¸¸_ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ_ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ã“ã‚Œã¯2018å¹´ã«[ULMFiT](https://arxiv.org/abs/1801.06146)ã«ã‚ˆã£ã¦æ™®åŠã—ã¾ã—ãŸã€‚è»¢ç§»å­¦ç¿’ã‚’NLPã§æœ¬å½“ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã—ãŸæœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆLSTMãŒãƒ™ãƒ¼ã‚¹ï¼‰ã®1ã¤ã§ã™ã€‚ULMFiTã«ã‚ˆã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®ä¾‹ã‚’ä¸‹ã®ç”»åƒã«ç¤ºã—ã¾ã™ã€‚ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€LSTMã®ä»£ã‚ã‚Šã«Transformerã‚’ä½¿ã£ã¦ã€åŒæ§˜ã®ã“ã¨ã‚’è¡Œã„ã¾ã™ï¼


<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg" alt="ULMFiT."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg" alt="ULMFiT."/>
</div>

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚ã‚ã‚Šã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ–‡ç« ã‚’è‡ªå‹•è£œå®Œã§ãã‚‹[ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.)ãŒHubä¸Šã«ã§ãã¦ã„ã‚‹ã“ã¨ã§ã—ã‚‡ã†ã€‚

<iframe src="https://course-demos-distilbert-base-uncased-finetune-7400b54.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ãã‚Œã§ã¯å§‹ã‚ã¾ã—ã‚‡ã†ï¼

<Youtube id="mqElG5QJWUg"/>

> [!TIP]
> ğŸ™‹ ã€Œãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ã‚„ã€Œäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã€ã¨ã„ã†è¨€è‘‰ã«èãè¦šãˆãŒãªã„æ–¹ã¯ã€[ç¬¬1ç« ](/course/ja/chapter1)ã§ã“ã‚Œã‚‰ã®ä¸»è¦ãªæ¦‚å¿µã‚’ã™ã¹ã¦å‹•ç”»ä»˜ãã§èª¬æ˜ã—ã¦ã„ã¾ã™ã®ã§ã€ãœã²ã”è¦§ã«ãªã£ã¦ãã ã•ã„ã€‚

## ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç”¨ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

ã¾ãšã€ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«é©ã—ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã³ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ã‚ˆã†ã«ã€[ãƒã‚®ãƒ³ã‚° ãƒ•ã‚§ã‚¤ã‚¹ ãƒãƒ–](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads)ã® "Fill-Mask "ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã™ã‚‹ã¨ã€å€™è£œã®ãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png" alt="Hub models." width="80%"/>
</div>

BERT ã¨ RoBERTa ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãŒæœ€ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã“ã§ã¯ [DistilBERT](https://huggingface.co/distilbert-base-uncased) ã¨å‘¼ã°ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ã‚’ã»ã¨ã‚“ã©æãªã†ã“ã¨ãªãã€ã‚ˆã‚Šé«˜é€Ÿã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€[_çŸ¥è­˜è’¸ç•™_](https://en.wikipedia.org/wiki/Knowledge_distillation)ã¨å‘¼ã°ã‚Œã‚‹ç‰¹åˆ¥ãªæŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ã•ã‚Œã¾ã—ãŸã€‚ã“ã®æ‰‹æ³•ã¯ã€BERTã®ã‚ˆã†ãªå¤§ããªã€Œæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã€ãŒã€ãã‚Œã‚ˆã‚Šã¯ã‚‹ã‹ã«å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã€Œç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã€ã®è¨“ç·´ã‚’å°ããŸã‚ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

çŸ¥è­˜è’¸æºœã®è©³ç´°ã‚’èª¬æ˜ã™ã‚‹ã¨ã€ã“ã®ç« ã®å†…å®¹ã‹ã‚‰é›¢ã‚Œã™ãã¦ã—ã¾ã„ã¾ã™ãŒã€ã‚‚ã—èˆˆå‘³ãŒã‚ã‚Œã°ã€[_Natural Language Processing with Transformers_](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)ï¼ˆé€šç§°Transformersæ•™ç§‘æ›¸ï¼‰ã§ãã‚Œã«ã¤ã„ã¦ã™ã¹ã¦èª­ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

{#if fw === 'pt'}

ãã‚Œã§ã¯ã€`AutoModelForMaskedLM`ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦DistilBERTã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒã„ãã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã£ã¦ã„ã‚‹ã‹ã¯ã€`num_parameters()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")
```

```python out
'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'
```

{:else}

ãã‚Œã§ã¯ã€`AutoModelForMaskedLM`ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦DistilBERTã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)
```
ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒã„ãã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã£ã¦ã„ã‚‹ã‹ã¯ã€`summary()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
model(model.dummy_inputs)  # Build the model
model.summary()
```

```python out
Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
distilbert (TFDistilBertMain multiple                  66362880  
_________________________________________________________________
vocab_transform (Dense)      multiple                  590592    
_________________________________________________________________
vocab_layer_norm (LayerNorma multiple                  1536      
_________________________________________________________________
vocab_projector (TFDistilBer multiple                  23866170  
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________
```

{/if}

ç´„6,700ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤DistilBERTã¯ã€BERTã®åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ç´„2å€å°ã•ãã€ã“ã‚Œã¯ã€å­¦ç¿’æ™‚ã«ç´„2å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã«ç›¸å½“ã—ã¾ã™ï¼ˆç´ æ™´ã‚‰ã—ã„ï¼ï¼‰ã€‚ãã‚Œã§ã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã™ã‚‹ã€å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã®æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„å®Œæˆå½¢ã¯ã©ã®ã‚ˆã†ãªç¨®é¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ã‚ã‚‹ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
text = "This is a great [MASK]."
```

äººé–“ãªã‚‰ `[MASK]` ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ã€"day", "ride", "painting" ãªã©ã®å¤šãã®å¯èƒ½æ€§ã‚’æƒ³åƒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€äºˆæ¸¬å€¤ã¯ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã«ä¾å­˜ã—ã¾ã™ã€‚ãªãœãªã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹çµ±è¨ˆçš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã‹ã‚‰ã§ã™ã€‚BERTã¨åŒæ§˜ã«ã€DistilBERTã¯[English Wikipedia](https://huggingface.co/datasets/wikipedia) ã¨ [BookCorpus](https://huggingface.co/datasets/bookcorpus) ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€`[MASK]`ã®äºˆæ¸¬ã¯ã“ã‚Œã‚‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’åæ˜ ã™ã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚ãƒã‚¹ã‚¯ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ç”Ÿæˆç”¨ã«DistilBERTã®tokenizerãŒå¿…è¦ãªã®ã§ã€ã“ã‚Œã‚‚Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†ã€‚

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

tokenizerãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ã€ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’æŠ½å‡ºã—ã€ä¸Šä½5ã¤ã®å€™è£œã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

{#if fw === 'pt'}

```python
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")
```

{:else}

```python
import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
# We negate the array before argsort to get the largest, not the smallest, logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")
```

{/if}

```python out
'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'
```

å‡ºåŠ›ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã¯æ—¥å¸¸çš„ãªç”¨èªã«è¨€åŠã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ãŒã€ã“ã‚Œã¯è‹±èªç‰ˆã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢ãŒåŸºç›¤ã¨ãªã£ã¦ã„ã‚‹äº‹ã‚’è€ƒãˆã‚Œã°é©šãã“ã¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã§ã¯ã€ã“ã®é ˜åŸŸã‚’ã‚‚ã†å°‘ã—ãƒ‹ãƒƒãƒãªã‚‚ã®ã€ã¤ã¾ã‚Šã€åˆ†è£‚ã—ã¦ã„ã‚‹æ˜ ç”»è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤‰ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®ä¾‹ã‚’ç¤ºã™ãŸã‚ã«ã€ç§ãŸã¡ã¯æœ‰åãª[Large Movie Review Dataset](https://huggingface.co/datasets/imdb) (ç•¥ã—ã¦IMDb)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹ã€æ˜ ç”»ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã§DistilBERTã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰å­¦ç¿’ã—ãŸWikipediaã®äº‹å®Ÿã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚ˆã‚Šä¸»è¦³çš„ãªè¦ç´ ã«èªå½™ã‚’é©å¿œã•ã›ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ãƒã‚®ãƒ³ã‚° ãƒ•ã‚§ã‚¤ã‚¹ ãƒãƒ–ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€ğŸ¤— Datasetsã® `load_dataset()` é–¢æ•°ã§å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```

`train` ã¨ `test` ç”¨ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã«ã¯ãã‚Œãã‚Œ25,000ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰æ§‹æˆã•ã‚Œã€ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚Œã¦ã„ãªã„ `unsupervised` ã¨ã„ã†åˆ†å‰²ã«ã¯50,000ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ã£ã¦ã„ã‚‹ã‹çŸ¥ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã®ã‚³ãƒ¼ã‚¹ã®å‰ã®ç« ã§è¡Œã£ãŸã‚ˆã†ã«ã€ `Dataset.shuffle()` ã¨ `Dataset.select()` é–¢æ•°ã‚’é€£é–ã•ã›ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")
```

```python out

'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clichÃ©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'
```

ãã†ã€ã“ã‚Œã‚‰ã¯ç¢ºã‹ã«æ˜ ç”»ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã™ã€‚ã‚‚ã—ã‚ãªãŸãŒååˆ†ã«å¹´ã‚’å–ã£ã¦ã„ã‚‹ãªã‚‰ã€æœ€å¾Œã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«ã‚ã‚‹VHSç‰ˆã‚’æ‰€æœ‰ã—ã¦ã„ã‚‹ã¨ã„ã†ã‚³ãƒ¡ãƒ³ãƒˆã•ãˆç†è§£ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ğŸ˜œ! è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã¯ãƒ©ãƒ™ãƒ«ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ãŒã€`0`ã¯å¦å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã€`1`ã¯è‚¯å®šçš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã«å¯¾å¿œã™ã‚‹ã“ã¨ãŒã‚‚ã†ã‚ã‹ã‚Šã¾ã—ãŸã€‚

> [!TIP]
> âœï¸ **æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼** `unsupervised` ã®ãƒ©ãƒ™ãƒ«ãŒã¤ã„ãŸåˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆã—ã€ãƒ©ãƒ™ãƒ«ãŒ `0` ã‚„ `1` ã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãŸã€`train` ã¨ `test` ç”¨ã®åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ãŒæœ¬å½“ã« `0` ã‹ `1` ã®ã¿ã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã™ã¹ã¦ã®è‡ªç„¶è¨€èªå‡¦ç†ã®å®Ÿè·µè€…ãŒæ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é–‹å§‹æ™‚ã«å®Ÿè¡Œã™ã¹ãã€æœ‰ç”¨ãªã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã§ã™!

ã•ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã–ã£ã¨è¦‹ãŸã¨ã“ã‚ã§ã€ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã®æº–å‚™ã«å–ã‚Šã‹ã‹ã‚Šã¾ã—ã‚‡ã†ã€‚[ç¬¬3ç« ](/course/ja/chapter3)ã§è¦‹ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ã®ã‚¿ã‚¹ã‚¯ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€ã„ãã¤ã‹ã®è¿½åŠ ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã§ã—ã‚‡ã†ã€‚ã•ã‚ã€å§‹ã‚ã¾ã—ã‚‡ã†ï¼

## ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†

<Youtube id="8PmhEIXhBvI"/>

è‡ªå·±å›å¸°è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã‚‚ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã‚‚ã€å…±é€šã®å‰å‡¦ç†ã¨ã—ã¦ã€ã™ã¹ã¦ã®ç”¨ä¾‹ã‚’é€£çµã—ã€ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã‚’åŒã˜å¤§ãã•ã®æ–­ç‰‡ã«åˆ†å‰²ã™ã‚‹ã“ã¨ãŒè¡Œã‚ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€å€‹ã€…ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å˜ç´”ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã¨ã„ã†ã€ç§é”ã®é€šå¸¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã¯å…¨ãç•°ãªã‚‹ã‚‚ã®ã§ã™ã€‚ãªãœã™ã¹ã¦ã‚’é€£çµã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿãã‚Œã¯ã€å€‹ã€…ã®ä¾‹æ–‡ãŒé•·ã™ãã‚‹ã¨åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«å½¹ç«‹ã¤ã‹ã‚‚ã—ã‚Œãªã„æƒ…å ±ãŒå¤±ã‚ã‚Œã¦ã—ã¾ã†ã‹ã‚‰ã§ã™ï¼

ãã“ã§ã€ã¾ãšã„ã¤ã‚‚ã®ã‚ˆã†ã«ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚ãŸã ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹éš›ã« `truncation=True` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’tokenizerã«_è¨­å®šã—ãªã„_ã‚ˆã†ã«ã—ã¾ã™ã€‚ã¾ãŸã€å˜èªIDãŒã‚ã‚Œã°ãã‚Œã‚’å–å¾—ã—ã¾ã™ï¼ˆ[6ç« ](/course/ja/chapter6/3)ã§èª¬æ˜ã—ãŸé«˜é€Ÿtokenizerã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ãã†ãªã‚Šã¾ã™ï¼‰ã€‚å¾Œã§å˜èªå…¨ä½“ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«å¿…è¦ã«ãªã‚‹ã‹ã‚‰ã§ã™ã€‚ã“ã‚Œã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªé–¢æ•°ã«ã¾ã¨ã‚ã€ã¤ã„ã§ã« `text` ã¨ `label` ã‚«ãƒ©ãƒ ã‚‚ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤ã—ã¦ã—ã¾ã„ã¾ã—ã‚‡ã†ã€‚

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# Use batched=True to activate fast multithreading!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})
```

DistilBERTã¯BERTã«ä¼¼ãŸãƒ¢ãƒ‡ãƒ«ãªã®ã§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯ã€ä»–ã®ç« ã§è¦‹ãŸ `input_ids` ã¨ `attention_mask` ã«åŠ ãˆã€ç§é”ãŒè¿½åŠ ã—ãŸ `word_ids` ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ã•ã¦ã€æ˜ ç”»ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸã®ã§ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ãã‚Œã‚‰ã‚’ã™ã¹ã¦ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã€çµæœã‚’æ–­ç‰‡ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã™ã€‚ã—ã‹ã—ã€ã“ã®æ–­ç‰‡ã®å¤§ãã•ã¯ã©ã®ç¨‹åº¦ã«ã™ã¹ãã§ã—ã‚‡ã†ã‹ï¼Ÿã“ã‚Œã¯æœ€çµ‚çš„ã«ã¯åˆ©ç”¨å¯èƒ½ãª GPU ãƒ¡ãƒ¢ãƒªã®é‡ã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã¾ã™ãŒã€è‰¯ã„å‡ºç™ºç‚¹ã¯ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºãŒä½•ã§ã‚ã‚‹ã‹ã‚’è¦‹ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯tokenizerã® `model_max_length` å±æ€§ã‚’èª¿ã¹ã‚‹ã“ã¨ã§æ¨æ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
tokenizer.model_max_length
```

```python out
512
```

ã“ã®å€¤ã¯ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸ *tokenizer_config.json* ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚ã“ã®å ´åˆã€BERT ã¨åŒæ§˜ã«ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºãŒ 512 ãƒˆãƒ¼ã‚¯ãƒ³ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚

> [!TIP]
> âœï¸ ** ã‚ãªãŸã®ç•ªã§ã™ï¼ ** [BigBird](https://huggingface.co/google/bigbird-roberta-base) ã‚„ [Longformer](hf.co/allenai/longformer-base-4096) ãªã©ã®ã„ãã¤ã‹ã® Transformer ãƒ¢ãƒ‡ãƒ«ã¯ã€BERT ã‚„ä»–ã®åˆæœŸã® Transformer ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šãšã£ã¨é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦ã€`model_max_length` ãŒãã®ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰å†…ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚

ã•ã‚Œã€Google Colabå†…ã§åˆ©ç”¨å¯èƒ½ãªGPUã§å®Ÿé¨“ã‚’è¡Œã†ãŸã‚ã«ã€ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹ã‚ˆã†ãªå°‘ã—å°ã•ã‚ã®ã‚‚ã®ã‚’é¸ã¶ã“ã¨ã«ã—ã¾ã™ã€‚

```python
chunk_size = 128
```

> [!WARNING]
> ãªãŠã€å°ã•ãªæ–­ç‰‡ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å®Ÿéš›ã®ã‚·ãƒŠãƒªã‚ªã§ã¯ä¸åˆ©ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œã—ãŸã‚µã‚¤ã‚ºã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã•ã¦ã€ã“ã“ã‹ã‚‰ãŒæ¥½ã—ã„ã¨ã“ã‚ã§ã™ã€‚é€£çµãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ç¤ºã™ãŸã‚ã«ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‹ã‚‰ã„ãã¤ã‹ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å–ã‚Šå‡ºã—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¯ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‡ºåŠ›ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
# Slicing produces a list of lists for each feature
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python out
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

ãã—ã¦ã€ã“ã‚Œã‚‰ã®ä¾‹ã‚’ã™ã¹ã¦ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªè¾æ›¸å†…åŒ…è¡¨è¨˜ã‚’ä½¿ã£ã¦é€£çµã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")
```

```python out
'>>> Concatenated reviews length: 951'
```

ç´ æ™´ã‚‰ã—ã„ï¼å…¨ä½“ã®é•·ã•ã®è£ä»˜ã‘ãŒã¨ã‚Œã¾ã—ãŸã€‚

ã§ã¯ã€é€£çµã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ `block_size` ã§æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã®æ–­ç‰‡ã«åˆ†å‰²ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãã®ãŸã‚ã«ã€ `concatenated_examples` ã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã—ã€ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã‚’ä½¿ç”¨ã—ã¦å„ç‰¹å¾´ã®ã‚¹ãƒ©ã‚¤ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚ãã®çµæœã€æ–­ç‰‡ã®è¾æ›¸ãŒã§ãã‚ãŒã‚Šã¾ã™ã€‚

```python
chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")
```

```python out
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

ã“ã®ä¾‹ã§ã‚ã‹ã‚‹ã‚ˆã†ã«ã€ä¸€èˆ¬çš„ã«æœ€å¾Œã®æ–­ç‰‡ã¯æœ€å¤§æ–­ç‰‡ã‚µã‚¤ã‚ºã‚ˆã‚Šå°ã•ããªã‚Šã¾ã™ã€‚ã“ã‚Œã‚’æ‰±ã†ã«ã¯ã€ä¸»ã« 2 ã¤ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚

* æœ€å¾Œã®æ–­ç‰‡ãŒ `chunk_size` ã‚ˆã‚Šã‚‚å°ã•ã‘ã‚Œã°å‰Šé™¤ã™ã‚‹
* æœ€å¾Œã®æ–­ç‰‡ã®é•·ã•ãŒ `chunk_size` ã¨ç­‰ã—ããªã‚‹ã¾ã§ã€æœ€å¾Œã®æ–­ç‰‡ã«ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è©°ã‚è¾¼ã‚€

ã“ã“ã§ã¯ã€æœ€åˆã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã¨ã‚Šã¾ã™ã€‚ä¸Šè¨˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã™ã¹ã¦ã²ã¨ã¤ã®é–¢æ•°ã«ã¾ã¨ã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
def group_texts(examples):
    # Concatenate all texts
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # Compute length of concatenated texts
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the last chunk if it's smaller than chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Split by chunks of max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # Create a new labels column
    result["labels"] = result["input_ids"].copy()
    return result
```

`group_texts()` ã®æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ `input_ids` åˆ—ã®ã‚³ãƒ”ãƒ¼ã§ã‚ã‚‹æ–°ã—ã„ `labels` åˆ—ã‚’ä½œæˆã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã‹ã‚‰èª¬æ˜ã™ã‚‹ã‚ˆã†ã«ã€ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯ã€å…¥åŠ›éƒ¨ã«å«ã¾ã‚Œã‚‹ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒç›®çš„ã§ã™ã€‚ `labels` åˆ—ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã®éš›ã«å‚è€ƒã™ã‚‹çœŸå®Ÿã®å€¤ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚

ãã‚Œã§ã¯ã€ä¿¡é ¼ã§ãã‚‹ `Dataset.map()` é–¢æ•°ã‚’ä½¿ã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« `group_texts()` ã‚’é©ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})
```

ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€æ–­ç‰‡ã«åˆ†ã‘ãŸã“ã¨ã§`train`ã¨`test`ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§25,000ã‚ˆã‚Šã‚‚å¤šãã®ã‚µãƒ³ãƒ—ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€å…ƒã®ã‚³ãƒ¼ãƒ‘ã‚¹ã«è¤‡æ•°ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å«ã‚€ã‚‚ã®ãŒã‚ã‚‹ãŸã‚ _é€£ç¶šãƒˆãƒ¼ã‚¯ãƒ³_ ã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãŒã§ããŸã‹ã‚‰ã§ã™ã€‚ã“ã®ã“ã¨ã¯ã€æ–­ç‰‡ã®1ã¤ã«ã‚ã‚‹ç‰¹åˆ¥ãª `[SEP]` ã¨ `[CLS]` ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¢ã™ã“ã¨ã§æ˜ç¢ºã«ã‚ã‹ã‚Šã¾ã™ã€‚

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

ä¸Šã®ä¾‹ã§ã¯ã€é«˜æ ¡ã«é–¢ã™ã‚‹æ˜ ç”»ã¨ãƒ›ãƒ¼ãƒ ãƒ¬ã‚¹ã«é–¢ã™ã‚‹æ˜ ç”»ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒ2ã¤é‡è¤‡ã—ã¦ã„ã‚‹ã®ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã¾ãŸã€ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãƒ©ãƒ™ãƒ«ãŒã©ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã‹ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python out
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

ä¸Šã® `group_texts()` é–¢æ•°ã‹ã‚‰äºˆæƒ³ã•ã‚Œã‚‹ã‚ˆã†ã«ã€ã“ã‚Œã¯ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ `input_ids` ã¨åŒã˜ã«è¦‹ãˆã¾ã™ã€‚ã—ã‹ã—ã€ãã‚Œã§ã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã©ã†ã‚„ã£ã¦ä½•ã‹ã‚’å­¦ç¿’ã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿå…¥åŠ›ã®ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã« `[MASK]` ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒ¿å…¥ã™ã‚‹ã€ã¨ã„ã†é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ãŒæŠœã‘ã¦ã„ã‚‹ã®ã§ã™ã€‚ãã‚Œã§ã¯ã€ç‰¹åˆ¥ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã£ã¦ã€å¾®èª¿æ•´ã®éš›ã«ã©ã®ã‚ˆã†ã«ã“ã‚Œã‚’è¡Œã†ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

## DistilBERTã‚’`Trainer`APIã§å¾®èª¿æ•´ã™ã‚‹

ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã¯ã€[ç¬¬3ç« ](/course/ja/chapter3)ã§è¡Œã£ãŸã‚ˆã†ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã¨ã»ã¼åŒã˜ã§ã™ã€‚å”¯ä¸€ã®é•ã„ã¯ã€å„ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã„ãã¤ã‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç‰¹åˆ¥ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã§ã™ã€‚å¹¸ã„ãªã“ã¨ã«ã€ğŸ¤— Transformersã«ã¯ã“ã®ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«å°‚ç”¨ã® `DataCollatorForLanguageModeling` ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚ç§ãŸã¡ã¯tokenizerã¨ã€ãƒã‚¹ã‚¯ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®å‰²åˆã‚’æŒ‡å®šã™ã‚‹ `mlm_probability` å¼•æ•°ã‚’æ¸¡ã™ã ã‘ã§ã‚ˆã„ã®ã§ã™ã€‚ã“ã“ã§ã¯ã€BERTã§ä½¿ç”¨ã•ã‚Œã€æ–‡çŒ®ä¸Šã§ã‚‚ä¸€èˆ¬çš„ãªé¸æŠã§ã‚ã‚‹15ï¼…ã‚’é¸ã³ã¾ã™ã€‚

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚¯ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’è¦‹ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®ä¾‹ã‚’ data_collator ã«ä¸ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯è¾æ›¸å‹ ã®ãƒªã‚¹ãƒˆã‚’æƒ³å®šã—ã¦ãŠã‚Šã€å„ è¾æ›¸ã¯é€£ç¶šã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®å¡Šã‚’è¡¨ã™ã®ã§ã€ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã—ã¦ã‹ã‚‰ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ãƒãƒƒãƒã‚’æ¸¡ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¯ `"word_ids"` ã‚­ãƒ¼ã‚’å¿…è¦ã¨ã—ãªã„ã®ã§ã€ã“ã‚Œã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george å®‡in stated )å…¬ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

ã„ã„ã§ã™ã­ã€ã†ã¾ãã„ãã¾ã—ãŸï¼
`[MASK]`ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒ†ã‚­ã‚¹ãƒˆã®æ§˜ã€…ãªå ´æ‰€ã«ãƒ©ãƒ³ãƒ€ãƒ ã«æŒ¿å…¥ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ã“ã‚Œã‚‰ãŒå­¦ç¿’ä¸­ã«ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã‚Šã¾ã™ã€‚ãã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ç´ æ™´ã‚‰ã—ã„ã¨ã“ã‚ã¯ã€ãƒãƒƒãƒã”ã¨ã«`[MASK]`ã®æŒ¿å…¥ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã™ã‚‹ã“ã¨ã§ã™! 

> [!TIP]
> âœï¸ ** ã‚ãªãŸã®ç•ªã§ã™ï¼ ** ä¸Šã®ã‚³ãƒ¼ãƒ‰ã‚’ä½•åº¦ã‹å®Ÿè¡Œã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¹ã‚­ãƒ³ã‚°ãŒã‚ãªãŸã®ç›®ã®å‰ã§èµ·ã“ã‚‹ã®ã‚’è¦‹ã¾ã—ã‚‡ã†! ã¾ãŸã€`tokenizer.decode()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ `tokenizer.convert_ids_to_tokens()` ã«ç½®ãæ›ãˆã‚‹ã¨ã€ä¸ãˆãŸå˜èªå†…ã‹ã‚‰ä¸€ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒé¸æŠã•ã‚Œã¦ãƒã‚¹ã‚¯ã•ã‚Œã€ä»–ã®å˜èªãŒãƒã‚¹ã‚¯ã•ã‚Œãªã„ã“ã¨ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

{#if fw === 'pt'}

ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚­ãƒ³ã‚°ã®å‰¯ä½œç”¨ã¨ã—ã¦ã€`Trainer`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€è©•ä¾¡æŒ‡æ¨™ãŒç¢ºå®šçš„ã§ãªããªã‚‹ã“ã¨ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
ã“ã‚Œã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã§ã™ã€‚å¾Œã»ã©ã€ğŸ¤— Accelerateã§å¾®èª¿æ•´ã‚’è¡Œã†éš›ã«ã€ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ«ãƒ¼ãƒ—ã®æŸ”è»Ÿæ€§ã‚’åˆ©ç”¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’ãªãã™æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

{/if}

ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€å€‹ã€…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªãã€å˜èªå…¨ä½“ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹æ‰‹æ³•ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æ‰‹æ³•ã¯ _whole word masking_ ã¨å‘¼ã°ã‚Œã¾ã™ã€‚å˜èªå…¨ä½“ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’è‡ªä½œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã¨ã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãã‚Œã‚’ãƒãƒƒãƒå¤‰æ›ã™ã‚‹é–¢æ•°ã®ã“ã¨ã§ã™ã€‚

ä»Šã‹ã‚‰ã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ï¼

å…ˆã»ã©è¨ˆç®—ã—ãŸå˜èªIDã‚’ä½¿ã£ã¦ã€å˜èªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒãƒƒãƒ—ã‚’ä½œã‚‹äº‹ã«ã—ã¾ã™ã€‚ã©ã®å˜èªã‚’ãƒã‚¹ã‚¯ã™ã‚‹ã‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ±ºã‚ã¦ã€ãã®ãƒã‚¹ã‚¯ã‚’å…¥åŠ›ã«é©ç”¨ã—ã¾ã™ã€‚ãªãŠã€ãƒ©ãƒ™ãƒ«ã¯ãƒã‚¹ã‚¯ã™ã‚‹å˜èªã‚’é™¤ã„ã¦å…¨ã¦`-100`ã§ã™ã€‚

{#if fw === 'pt'}

```py
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Create a map between words and corresponding token indices
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Randomly mask words
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return default_data_collator(features)
```

{:else}

```py
import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # Create a map between words and corresponding token indices
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # Randomly mask words
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return tf_default_data_collator(features)
```

{/if}

æ¬¡ã«ã€å…ˆã»ã©ã¨åŒã˜ã‚µãƒ³ãƒ—ãƒ«ã§è©¦ã—ã¦ã¿ã¾ã™ã€‚

```py
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python out
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

> [!TIP]
> âœï¸ ** ã‚ãªãŸã®ç•ªã§ã™ï¼ ** ä¸Šã®ã‚³ãƒ¼ãƒ‰ã‚’ä½•åº¦ã‹å®Ÿè¡Œã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¹ã‚­ãƒ³ã‚°ãŒã‚ãªãŸã®ç›®ã®å‰ã§èµ·ã“ã‚‹ã®ã‚’è¦‹ã¾ã—ã‚‡ã†! ã¾ãŸã€`tokenizer.decode()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ `tokenizer.convert_ids_to_tokens()` ã«ç½®ãæ›ãˆã‚‹ã¨ã€ä¸ãˆã‚‰ã‚ŒãŸå˜èªã‹ã‚‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¸¸ã«ä¸€ç·’ã«ãƒã‚¹ã‚¯ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã§ãã¾ã™ã€‚

ã“ã‚Œã§2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒæƒã„ã¾ã—ãŸã®ã§ã€æ®‹ã‚Šã®å¾®èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—ã¯æ¨™æº–çš„ãªã‚‚ã®ã§ã™ã€‚Google Colabã§ã€ç¥è©±ã«å‡ºã¦ãã‚‹P100 GPUã‚’é‹è‰¯ãå‰²ã‚Šå½“ã¦ã‚‰ã‚Œãªã‹ã£ãŸå ´åˆã€å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ğŸ˜­ãã“ã§ã€ã¾ãšå­¦ç¿’ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’æ•°åƒäº‹ä¾‹ã¾ã§ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã—ã¾ã™ã€‚å¿ƒé…ã—ãªã„ã§ãã ã•ã„ã€ãã‚Œã§ã‚‚ã‹ãªã‚Šã¾ã¨ã‚‚ãªè¨€èªãƒ¢ãƒ‡ãƒ«ãŒã§ãã¾ã™ã‚ˆã€‚ğŸ¤— Datasets å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯[ç¬¬5ç« ](/course/ja/chapter5)ã§ç´¹ä»‹ã—ãŸ `Dataset.train_test_split()` é–¢æ•°ã§ç°¡å˜ã«ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})
```

ã“ã‚Œã¯è‡ªå‹•çš„ã«æ–°ã—ã„ `train` ã¨ `test` ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’ 10,000 äº‹ä¾‹ã«ã€æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’ãã® 10%ã«è¨­å®šã—ã¾ã—ãŸã€‚ã‚‚ã—å¼·åŠ›ãªGPUã‚’ãŠæŒã¡ãªã‚‰ã€ã“ã®å€¤ã‚’è‡ªç”±ã«å¢—ã‚„ã—ã¦ãã ã•ã„ï¼

æ¬¡ã«å¿…è¦ãªã“ã¨ã¯ã€ãƒã‚®ãƒ³ã‚° ãƒ•ã‚§ã‚¤ã‚¹ ãƒãƒ–ã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’notebookã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€æ¬¡ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã§å®Ÿè¡Œã§ãã¾ã™ã€‚

```python
from huggingface_hub import notebook_login

notebook_login()
```

ä¸Šè¨˜ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€å…¥åŠ›ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã€èªè¨¼æƒ…å ±ã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸå®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

```
huggingface-cli login
```

å¥½ã¿ã«å¿œã˜ã¦ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’èµ·å‹•ã—ã€ãã“ã‹ã‚‰ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚

{#if fw === 'tf'}

ä¸€åº¦ãƒ­ã‚°ã‚¤ãƒ³ã—ãŸã‚‰ã€`tf.data`ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚ã“ã“ã§ã¯æ¨™æº–çš„ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã„ã¾ã™ãŒã€ç·´ç¿’ã¨ã—ã¦å…¨å˜èªãƒã‚¹ã‚­ãƒ³ã‚°ã®ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’è©¦ã—ã¦çµæœã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

```python
tf_train_dataset = downsampled_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = downsampled_dataset["test"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

æ¬¡ã«ã€å­¦ç¿’ç”¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `create_optimizer()` é–¢æ•°ã‚’ä½¿ç”¨ã—ã€å­¦ç¿’ç‡ãŒç·šå½¢ã«æ¸›è¡°ã™ã‚‹æ€§è³ªã‚’æŒã¤ `AdamW` ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿è¾¼ã¿ã®æå¤±ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ `compile()` ã®å¼•æ•°ã«æå¤±ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚ã‚Šã€å­¦ç¿’ç²¾åº¦ã¯ `"mixed_float16"` ã«è¨­å®šã•ã‚Œã¾ã™ã€‚Colabã§å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸGPUã‚„ãŠä½¿ã„ã®GPUãŒfloat16ã®ã‚µãƒãƒ¼ãƒˆã‚’ã—ã¦ã„ãªã„å ´åˆã¯ã€ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã•ã‚‰ã«ã€å„ã‚¨ãƒãƒƒã‚¯å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ä¿å­˜ã™ã‚‹ `PushToHubCallback` ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚`hub_model_id` å¼•æ•°ã§ã€ãƒ—ãƒƒã‚·ãƒ¥ã—ãŸã„ãƒªãƒã‚¸ãƒˆãƒªã®åå‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ï¼ˆç‰¹ã«ã€çµ„ç¹”ã‚’æŒ‡å®šã—ã¦ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã¯ã“ã®å¼•æ•°ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚ä¾‹ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ã‚’ [`huggingface-course` organization](https://huggingface.co/huggingface-course) ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã«ã¯ã€ `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä½¿ç”¨ã•ã‚Œã‚‹ãƒªãƒã‚¸ãƒˆãƒªã¯ã‚ãªãŸã®åå‰ç©ºé–“ã«ãªã‚Šã€è¨­å®šã•ã‚ŒãŸå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åå‰ã«ãªã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€ç§é”ã®ã‚±ãƒ¼ã‚¹ã§ã¯`"lewtun/distilbert-finetuned-imdb"` ã¨ãªã‚‹ã§ã—ã‚‡ã†ã€‚

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)
```

ã“ã‚Œã§ `model.fit()` ã‚’å®Ÿè¡Œã™ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚
ã—ã‹ã—ã€ã“ã‚Œã‚’å®Ÿè¡Œã™ã‚‹å‰ã«è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã§ã‚ã‚‹ _ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£_ ã«ã¤ã„ã¦ç°¡å˜ã«è¦‹ã¦ãŠãã¾ã—ã‚‡ã†ã€‚

{:else}

ä¸€åº¦ãƒ­ã‚°ã‚¤ãƒ³ã—ãŸã‚‰ã€`Trainer` ã®å¼•æ•°ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

```python
from transformers import TrainingArguments

batch_size = 64
# Show the training loss with every epoch
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)
```

ã“ã“ã§ã¯ã€å„ã‚¨ãƒãƒƒã‚¯ã§ã®å­¦ç¿’æå¤±ã‚’ç¢ºå®Ÿã«è¿½è·¡ã™ã‚‹ãŸã‚ã« `logging_steps` ã‚’å«ã‚€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å°‘ã—èª¿æ•´ã—ã¾ã—ãŸã€‚ã¾ãŸã€ `fp16=True` ã‚’ä½¿ç”¨ã—ã¦ã€æ··åˆç²¾åº¦ã®å­¦ç¿’ã‚’æœ‰åŠ¹ã«ã—ã€ã•ã‚‰ã«é«˜é€ŸåŒ–ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ `Trainer` ã¯ãƒ¢ãƒ‡ãƒ«ã® `forward()` ãƒ¡ã‚½ãƒƒãƒ‰ã«å«ã¾ã‚Œãªã„åˆ—ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€å…¨å˜èªãƒã‚¹ã‚¯ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€ `remove_unused_columns=False` ã‚’è¨­å®šã—ã¦ã€å­¦ç¿’æ™‚ã« `word_ids` ã‚«ãƒ©ãƒ ãŒå¤±ã‚ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãªãŠã€ `hub_model_id` å¼•æ•°ã§ãƒ—ãƒƒã‚·ãƒ¥å…ˆã®ãƒªãƒã‚¸ãƒˆãƒªåã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ (ç‰¹ã«ã€çµ„ç¹”ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã¯ã“ã®å¼•æ•°ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚ä¾‹ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ã‚’ [`huggingface-course` organization](https://huggingface.co/huggingface-course) ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã€ `TrainingArguments` ã« `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä½¿ç”¨ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªã¯ã‚ãªãŸã®åå‰ç©ºé–“å†…ã«ã‚ã‚Šã€è¨­å®šã—ãŸå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¡ãªã‚“ã åå‰ã«ãªã‚‹ã®ã§ã€ç§ãŸã¡ã®å ´åˆã¯ `"lewtun/distilbert-finetuned-imdb"` ã¨ãªã‚Šã¾ã™ã€‚

ã“ã‚Œã§ `Trainer` ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ææ–™ãŒæƒã„ã¾ã—ãŸã€‚ã“ã“ã§ã¯ã€æ¨™æº–çš„ãª `data_collator` ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€ç·´ç¿’ã¨ã—ã¦å…¨å˜èªãƒã‚¹ã‚­ãƒ³ã‚°ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’è©¦ã—ã¦ã€çµæœã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

ã“ã‚Œã§ `trainer.train()` ã‚’å®Ÿè¡Œã™ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚ã—ã‹ã—ãã®å‰ã«ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã§ã‚ã‚‹ _ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£_ ã«ã¤ã„ã¦ç°¡å˜ã«è¦‹ã¦ãŠãã¾ã—ã‚‡ã†ã€‚

{/if}

### è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£

<Youtube id="NURcDHhYe98"/>

ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡ã‚„è³ªå•å¿œç­”ã®ã‚ˆã†ã«ã€ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¦å­¦ç¿’ã™ã‚‹ä»–ã®ã‚¿ã‚¹ã‚¯ã¨ã¯ç•°ãªã‚Šã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯æ˜ç¤ºçš„ãªãƒ©ãƒ™ãƒ«ã‚’ä¸€åˆ‡æŒã¡ã¾ã›ã‚“ã€‚ã§ã¯ã€ä½•ãŒè‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ«ãªã®ã‹ã€ã©ã®ã‚ˆã†ã«åˆ¤æ–­ã™ã‚Œã°ã‚ˆã„ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ

æºå¸¯é›»è©±ã®è‡ªå‹•è£œæ­£æ©Ÿèƒ½ã®ã‚ˆã†ã«ã€æ–‡æ³•çš„ã«æ­£ã—ã„æ–‡ã«ã¯é«˜ã„ç¢ºç‡ã§ã€ç„¡æ„å‘³ãªæ–‡ã«ã¯ä½ã„ç¢ºç‡ã§å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚‚ã®ãŒè‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è‡ªå‹•è£œæ­£ã®å¤±æ•—ä¾‹ã¨ã—ã¦ã€æºå¸¯é›»è©±ã«æ­è¼‰ã•ã‚ŒãŸè‡ªå‹•è£œæ­£ãƒ¢ãƒ‡ãƒ«ãŒã€ãŠã‹ã—ãªï¼ˆãã—ã¦å¾€ã€…ã«ã—ã¦ä¸é©åˆ‡ãªï¼‰æ–‡ç« ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ä¾‹ãŒãƒãƒƒãƒˆä¸Šã«å¤šæ•°ç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

{#if fw === 'pt'}

ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã»ã¨ã‚“ã©ãŒæ–‡æ³•çš„ã«æ­£ã—ã„æ–‡ã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã¨ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’æ¸¬ã‚‹ä¸€ã¤ã®æ–¹æ³•ã¯ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã™ã¹ã¦ã®æ–‡ã«ãŠã„ã¦ã€æ¬¡ã«å‡ºç¾ã™ã‚‹å˜èªã‚’æ­£ã—ãå‰²ã‚Šå½“ã¦ã‚‹ç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã™ã€‚ç¢ºç‡ãŒé«˜ã„ã¨ã„ã†ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæœªçŸ¥ã®ä¾‹æ–‡ã«ã€Œé©šãã€ã‚„ã€Œå½“æƒ‘ã€ã‚’æ„Ÿã˜ã¦ã„ãªã„ã“ã¨ã‚’ç¤ºã—ã€ãã®è¨€èªã®æ–‡æ³•ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã«ã¯æ§˜ã€…ãªæ•°å­¦çš„å®šç¾©ãŒã‚ã‚Šã¾ã™ãŒã€ç§é”ãŒä½¿ã†ã®ã¯ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã®æŒ‡æ•°ã¨ã—ã¦å®šç¾©ã™ã‚‹ã‚‚ã®ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€`Trainer.evaluate()`é–¢æ•°ã‚’ä½¿ã£ã¦ãƒ†ã‚¹ãƒˆé›†åˆã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—ã—ã€ãã®çµæœã®æŒ‡æ•°ã‚’å–ã‚‹ã“ã¨ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ã™ã€‚

```python
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}
ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã»ã¨ã‚“ã©ãŒæ–‡æ³•çš„ã«æ­£ã—ã„æ–‡ç« ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®šã™ã‚‹ã¨ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’æ¸¬å®šã™ã‚‹ä¸€ã¤ã®æ–¹æ³•ã¯ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã™ã¹ã¦ã®æ–‡ç« ã§æ¬¡ã®å˜èªãŒæ­£ã—ãå‰²ã‚Šå½“ã¦ã‚‹ç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã™ã€‚ç¢ºç‡ãŒé«˜ã„ã¨ã„ã†ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæœªè¦‹ã®ä¾‹æ–‡ã«ã€Œé©šãã€ã€Œå½“æƒ‘ã€ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¤ºã—ã€ãã®è¨€èªã®æ–‡æ³•ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã«ã¯æ§˜ã€…ãªæ•°å­¦çš„å®šç¾©ãŒã‚ã‚Šã¾ã™ãŒã€ç§é”ãŒä½¿ã†ã®ã¯ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æå¤±ã®æŒ‡æ•°ã¨ã—ã¦å®šç¾©ã™ã‚‹ã‚‚ã®ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€`model.evaluate()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ãƒ†ã‚¹ãƒˆé›†åˆã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—ã—ã€ãã®çµæœã®æŒ‡æ•°ã‚’å–ã‚‹ã“ã¨ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ã™ã€‚

```python
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 21.75
```

ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚¹ã‚³ã‚¢ãŒä½ã„ã»ã©ã€è‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
ç§é”ã®é–‹å§‹æ™‚ã®ãƒ¢ãƒ‡ãƒ«ã®å€¤ã¯ã‚„ã‚„å¤§ãã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚
ã§ã¯ã€å¾®èª¿æ•´ã«ã‚ˆã£ã¦ã“ã®å€¤ã‚’ä¸‹ã’ã‚‰ã‚Œã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãã®ãŸã‚ã«ã€ã¾ãšå­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚


{#if fw === 'pt'}

```python
trainer.train()
```

{:else}

```python
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

ãã‚Œã‹ã‚‰è¨ˆç®—ã‚’å®Ÿè¡Œã—ã€ãã®çµæœå¾—ã‚‰ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’å…ˆã»ã©ã¨åŒæ§˜ã«è¨ˆç®—ã—ã¾ã™ã€‚

{#if fw === 'pt'}

```python
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

{:else}

```python
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

{/if}

```python out
>>> Perplexity: 11.32
```

ç´ æ•µï¼ã‹ãªã‚Šãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚
ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®é ˜åŸŸã«ã¤ã„ã¦ä½•ã‹ã‚’å­¦ã‚“ã ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

{#if fw === 'pt'}

ä¸€åº¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒçµ‚äº†ã—ãŸã‚‰ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ãŒå…¥ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’Hubã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€ä¸­ã«ä¿å­˜ã•ã‚Œã¾ã™ï¼‰ã€‚

```python
trainer.push_to_hub()
```

{/if}

> [!TIP]
> âœï¸ ** ã‚ãªãŸã®ç•ªã§ã™ï¼ ** ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å…¨å˜èªãƒã‚¹ã‚­ãƒ³ã‚°ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«å¤‰ãˆã¦ã€ä¸Šè¨˜ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã‚ˆã‚Šè‰¯ã„çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã‹ï¼Ÿ

{#if fw === 'pt'} 

ä»Šå›ã®ä½¿ç”¨ä¾‹ã§ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã«ç‰¹åˆ¥ãªã“ã¨ã‚’ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€å ´åˆã«ã‚ˆã£ã¦ã¯ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ãã®ã‚ˆã†ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ğŸ¤— Accelerateã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
è¦‹ã¦ã¿ã¾ã—ã‚‡ã†!

## DistilBERTã‚’ğŸ¤— Accelerateã‚’ä½¿ã£ã¦å¾®èª¿æ•´ã™ã‚‹

`Trainer`ã§è¦‹ãŸã‚ˆã†ã«ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã¯[ç¬¬3ç« ](/course/ja/chapter3)ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®ä¾‹ã¨éå¸¸ã«ã‚ˆãä¼¼ã¦ã„ã¾ã™ã€‚å®Ÿéš›ã€å”¯ä¸€ã®ã‚ãšã‹ãªé•ã„ã¯ç‰¹åˆ¥ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã†ã“ã¨ã§ã€ãã‚Œã¯ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å‰åŠã§ã™ã§ã«å–ã‚Šä¸Šã’ã¾ã—ãŸ! 

ã—ã‹ã—ã€`DataCollatorForLanguageModeling`ã¯è©•ä¾¡ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¹ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã®ã§ã€å­¦ç¿’å®Ÿè¡Œã”ã¨ã«ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚¹ã‚³ã‚¢ã«å¤šå°‘ã®å¤‰å‹•ãŒè¦‹ã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã“ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æ’é™¤ã™ã‚‹ä¸€ã¤ã®æ–¹æ³•ã¯ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå…¨ä½“ã« _ä¸€åº¦ã ã‘_ ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã—ã€ãã®å¾ŒğŸ¤— Transformersã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ä¸­ã«ãƒãƒƒãƒã‚’åé›†ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚ŒãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’è¦‹ã‚‹ãŸã‚ã«ã€`DataCollatorForLanguageModeling` ã‚’æœ€åˆã«ä½¿ã£ãŸæ™‚ã¨åŒæ§˜ã«ã€ãƒãƒƒãƒã«ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ç°¡å˜ãªé–¢æ•°ã‚’å®Ÿè£…ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # Create a new "masked" column for each column in the dataset
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}
```

æ¬¡ã«ã€ã“ã®é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«é©ç”¨ã—ã¦ã€ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤ã—ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸã‚«ãƒ©ãƒ ã«ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ä¸Šã® `data_collator` ã‚’é©åˆ‡ãªã‚‚ã®ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€å…¨å˜èªå˜ä½ã§ã®ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚
ãã®å ´åˆã¯ã€ä»¥ä¸‹ã®æœ€åˆã®è¡Œã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚

```py
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)
```

ã‚ã¨ã¯é€šå¸¸é€šã‚Šãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ãŒã€ã“ã“ã§ã¯è©•ä¾¡ã‚»ãƒƒãƒˆã« ğŸ¤— Transformers ã® `default_data_collator` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)
```

ã“ã“ã§ã¯ã€ğŸ¤— Accelerateã‚’ä½¿ã£ãŸæ¨™æº–çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã«å¾“ã„ã¾ã™ã€‚æœ€åˆã«ã‚„ã‚‹äº‹ã¯ã€äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã™ã€‚

```
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

æ¬¡ã«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æŒ‡å®šã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€æ¨™æº–çš„ãª `AdamW` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

ã“ã‚Œã‚‰ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚Œã°ã€ã‚ã¨ã¯ `Accelerator` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®ã™ã¹ã¦ã‚’æº–å‚™ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚ŒãŸã®ã§ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰ã«æœ€å¾Œã«ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãƒã‚®ãƒ³ã‚° ãƒ•ã‚§ã‚¤ã‚¹ ãƒãƒ–ã«ãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚ğŸ¤— Hub ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ã€ã¾ãšãƒ¬ãƒã‚¸ãƒˆãƒªã®ãƒ•ãƒ«ãƒãƒ¼ãƒ ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```python
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'
```

ãã‚Œã‹ã‚‰ã€ğŸ¤— Hub ã® `Repository` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆã—ã€ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™ã€‚

```python
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)
```

ã“ã‚ŒãŒã§ãã‚Œã°ã€ã‚ã¨ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡ã®ãƒ«ãƒ¼ãƒ—ã‚’ã™ã¹ã¦æ›¸ãå‡ºã™ã ã‘ã§ã™ã€‚

```python
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409
```

ã‚¤ã‚±ã¦ã‚‹ï¼
ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’è©•ä¾¡ã—ã€è¤‡æ•°å›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ãŸéš›ã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸ

{/if}

## å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†

å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€Hubä¸Šã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ã†ã‹ã€ğŸ¤— Transformersã® `pipeline` ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§æ“ä½œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãã‚Œã§ã¯å¾Œè€…ã«æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚`fill-mask`ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

ãã—ã¦ã€Œã“ã‚Œã¯ç´ æ™´ã‚‰ã—ã„[MASK]ã§ã™ã€ã¨ã„ã†ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«é€ã‚Šã€ä¸Šä½5ã¤ã®äºˆæ¸¬ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚


```python
preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")
```

```python out
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

ã™ã£ãã‚Šã—ã¾ã—ãŸã€‚
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ˜ ç”»ã¨é–¢é€£ã™ã‚‹å˜èªã‚’ã‚ˆã‚Šå¼·ãäºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«ã€æ˜ã‚‰ã‹ã«é‡ã¿ã‚’é©å¿œã•ã›ã¦ã„ã¾ã™ã­ï¼

<Youtube id="0Oxphw4Q9fo"/>

ã“ã‚Œã§ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®æœ€åˆã®å®Ÿé¨“ã‚’çµ‚ãˆã¾ã—ãŸã€‚[ã‚»ã‚¯ã‚·ãƒ§ãƒ³6](/course/ja/chapter7/section6)ã§ã¯ã€GPT-2ã®ã‚ˆã†ãªè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚ã‚‚ã—ã€ã‚ãªãŸè‡ªèº«ã®Transformerãƒ¢ãƒ‡ãƒ«ã‚’ã©ã†ã‚„ã£ã¦äº‹å‰å­¦ç¿’ã™ã‚‹ã‹è¦‹ãŸã„ãªã‚‰ã€ãã¡ã‚‰ã«å‘ã‹ã£ã¦ãã ã•ã„

> [!TIP]
> âœï¸ ** ã‚ãªãŸã®ç•ªã§ã™ï¼ ** ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã®åˆ©ç‚¹ã‚’å®šé‡åŒ–ã™ã‚‹ãŸã‚ã«ã€IMDbãƒ©ãƒ™ãƒ«ã®åˆ†é¡å™¨ã‚’ã€è¨“ç·´å‰ã¨å¾®èª¿æ•´ã—ãŸDistilBERTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¸¡æ–¹ã§å¾®èª¿æ•´ã—ã¦ãã ã•ã„ã€‚ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«ã¤ã„ã¦å¾©ç¿’ãŒå¿…è¦ãªå ´åˆã¯ã€[ç¬¬3ç« ](/course/ja/chapter3)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
