<FrameworkSwitchCourse {fw} />

# イントロダクション

[第3章](/course/ja/chapter3)では、テキスト分類のためにモデルを微調整する方法を学びました。この章では、以下のような一般的な自然言語処理タスクに取り組みます。

- トークン分類
- マスク言語モデリング(BERTのような)
- 要約
- 翻訳
- 因果言語モデリング事前学習(GPT-2など)
- 質問応答

{#if fw === 'pt'}

これを行うには、第3章で学んだTrainer APIと🤗 Accelerateライブラリ、5章で学んだ🤗 Datasetsライブラリ、第6章で学んだ🤗 Tokenizersライブラリについて、すべて活用する必要があります。また、第4章で行ったように、結果をModel Hubにアップロードします。したがって、この章は本当にすべてが集約された章です。

各セクションは独立して読むことができ、Trainer APIや🤗 Accelerateを使った独自の学習ループでモデルを学習する方法が紹介されています。どのパートも自由にスキップできるので、最も興味のあるパートに集中してください。Trainer APIは裏で何が起こっているかを気にせずにモデルを微調整したりトレーニングしたりするのに最適です。一方、Accelerateを使ったトレーニングループでは、必要な部分をより簡単にカスタマイズすることができます。

{:else}

これを行うには、第3章のKeras API、第5章の🤗 Datasetsライブラリ、第6章の🤗 Tokenizersライブラリでモデルのトレーニングについて学んだことをすべて活用する必要があります。また、第4章で行ったように、結果をモデルハブにアップロードすることになるので、この章はまさにすべてが集約された章と言えます。

各セクションは独立して読むことができます。

{/if}

<Tip>

各セクションを順番に読んでいくと、共通するコードや文章がかなりあることに気がつくと思います。この繰り返しは意図的なもので、興味のあるタスクに飛び込んで（あるいは後で戻って）、完全な動作例を見つけることができるようにするためのものです。

</Tip>
