<FrameworkSwitchCourse {fw} />

# ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ja/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ja/chapter7/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ja/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ja/chapter7/section2_tf.ipynb"},
]} />

{/if}

æœ€åˆã«ç´¹ä»‹ã™ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã§ã™ã€‚ã“ã®æ±ç”¨çš„ãªã‚¿ã‚¹ã‚¯ã¯ã€Œæ–‡ä¸­ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ã€ã¨å®šç¾©å¯èƒ½ãªã€ä»¥ä¸‹ã®ã‚ˆã†ãªå•é¡Œã‚’å«ã¿ã¾ã™ã€‚

- **å›ºæœ‰è¡¨ç¾èªè­˜(NER)**: æ–‡ä¸­ã«å«ã¾ã‚Œã‚‹äººåã€åœ°åã€çµ„ç¹”åãªã©ã®å›ºæœ‰ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¤œå‡ºã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å›ºæœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’1ã‚¯ãƒ©ã‚¹ã€å›ºæœ‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãªã—ã‚’1ã‚¯ãƒ©ã‚¹ã¨ã—ã¦ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨å®šç¾©ã§ãã¾ã™ã€‚
- **å“è©ã‚¿ã‚°ä»˜ã‘(POS)**: æ–‡ä¸­ã®å„å˜èªã‚’ç‰¹å®šã®å“è©ï¼ˆåè©ã€å‹•è©ã€å½¢å®¹è©ãªã©ï¼‰ã«å¯¾å¿œã™ã‚‹ã‚‚ã®ã¨ã—ã¦ãƒãƒ¼ã‚¯ã—ã¾ã™ã€‚
- **ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°(chunking)**: åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«å±ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ï¼ˆPOSã‚„NERã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼‰ã¯ã€ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­ã«ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¯ä¸€ã¤ã®ãƒ©ãƒ™ãƒ«ï¼ˆé€šå¸¸ `B-`ï¼‰ã€ãƒãƒ£ãƒ³ã‚¯ã®ä¸­ã«ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¯åˆ¥ã®ãƒ©ãƒ™ãƒ«ï¼ˆé€šå¸¸ `I-`ï¼‰ã€ã©ã®ãƒãƒ£ãƒ³ã‚¯ã«ã‚‚å±ã•ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¯ä¸‰ã¤ç›®ã®ãƒ©ãƒ™ãƒ«ï¼ˆé€šå¸¸ `O`ï¼‰ã‚’ä»˜ã‘ã‚‹ã“ã¨ã¨å®šç¾©ã§ãã¾ã™ã€‚ã€‚

<Youtube id="wVHdVlPScxA"/>

ã‚‚ã¡ã‚ã‚“ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡å•é¡Œã«ã¯ä»–ã«ã‚‚å¤šãã®å•é¡ŒãŒã‚ã‚Šã€ã“ã‚Œã‚‰ã¯ä»£è¡¨çš„ãªä¾‹ã«éãã¾ã›ã‚“ã€‚ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€NERã‚¿ã‚¹ã‚¯ã§ãƒ¢ãƒ‡ãƒ«ï¼ˆBERTï¼‰ã‚’å¾®èª¿æ•´ã—ã€ä»¥ä¸‹ã®ã‚ˆã†ãªäºˆæ¸¬è¨ˆç®—ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ã‚ãªãŸã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Hubã§æ¢ã—ãŸã‚Šã€Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®äºˆæ¸¬å€¤ã‚’[ã“ã“ã§](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn)å†ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ ã€‚

## ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™

ã¾ãšæœ€åˆã«ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã«é©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¿…è¦ã§ã™ã€‚ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€[CoNLL-2003 dataset](https://huggingface.co/datasets/conll2003)ã‚’ä½¿ã„ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ãƒ­ã‚¤ã‚¿ãƒ¼ãŒé…ä¿¡ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å«ã¿ã¾ã™ã€‚ 

> [!TIP]
> ğŸ’¡ å˜èªã¨ãã‚Œã«å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«ã«åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚Œã°ã€ã“ã“ã§èª¬æ˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚’ `Dataset` ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦å¾©ç¿’ãŒå¿…è¦ãªå ´åˆã¯ã€[ç¬¬5ç« ](/course/ja/chapter5) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### The CoNLL-2003 dataset

CoNLL-2003ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ã€ğŸ¤— Datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `load_dataset()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚[ç¬¬3ç« ](/course/ja/chapter3)ã§GLUE MRPC datasetã‚’æ‰±ã£ãŸã¨ãã¨åŒã˜ã§ã™ã€‚ã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’èª¿ã¹ã‚‹ã¨ã€å®šç¾©ã•ã‚ŒãŸåˆ—ã¨ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã€æ¤œè¨¼ã‚»ãƒƒãƒˆã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®3ã¤ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹äº‹ãŒã‚ã‹ã‚Šã¾ã™ã€‚

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

ç‰¹ã«ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ã€å…ˆã«è¿°ã¹ãŸ3ã¤ã®ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ©ãƒ™ãƒ«ã€NERã€POSã€ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã®å¤§ããªé•ã„ã¯ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒæ–‡ã‚„æ–‡æ›¸ã¨ã—ã¦ã§ã¯ãªãã€å˜èªã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã“ã¨ã§ã™ï¼ˆæœ€å¾Œã®åˆ—ã¯`tokens`å‘¼ã°ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã‚Œã¯ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‰ã®å…¥åŠ›ã§ã€ã¾ã ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ãŸã‚ã«tokenizerå‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ã„ã†æ„å‘³ã§å˜èªã‚’å«ã‚“ã§ã„ã¾ã™ï¼‰ã€‚

ãã‚Œã§ã¯ã€å­¦ç¿’ã‚»ãƒƒãƒˆã®æœ€åˆã®è¦ç´ ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

ä»Šå›ã¯å›ºæœ‰è¡¨ç¾èªè­˜ã‚’è¡Œã„ãŸã„ã®ã§ã€NERã‚¿ã‚°ã‚’è¦‹ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

ã“ã‚Œã¯å­¦ç¿’æ™‚ã«ä½¿ã‚ã‚Œã‚‹ãƒ©ãƒ™ãƒ«ã®ãŸã‚æ•´æ•°å€¤ã§æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’èª¿ã¹ã‚‹ã¨ãã«ã¯å¿…ãšã—ã‚‚ä¾¿åˆ©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `features` å±æ€§ã‚’è¦‹ã‚Œã°ã€ã“ã‚Œã‚‰ã®æ•´æ•°å€¤ãŒä½•ã®ãƒ©ãƒ™ãƒ«ã§ã‚ã‚‹ã‹èª¿ã¹ã‚‹äº‹ãŒã§ãã¾ã™ã€‚

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

ã¤ã¾ã‚Šã€ã“ã®ã‚«ãƒ©ãƒ ã¯ `ClassLabel` ã®è¦ç´ ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚å„è¦ç´ ã®å‹ã¯ã€ã“ã® `ner_feature` ã® `feature` å±æ€§ã«ã‚ã‚Šã€ãã® `feature` ã® `names` å±æ€§ã‚’è¦‹ã‚‹ã“ã¨ã§åå‰ã®ãƒªã‚¹ãƒˆã‚’ç¢ºèªã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚


```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

ã“ã‚Œã‚‰ã®ãƒ©ãƒ™ãƒ«ã¯[ç¬¬6ç« ](/course/ja/chapter6/3)ã§ã€`token-classification`ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å­¦ã‚“ã ã¨ãã«æ˜ã‚Šä¸‹ã’ã¾ã—ãŸãŒã€ç°¡å˜ã«å¾©ç¿’ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ 

- `O` ã¯ãã®å˜èªãŒã©ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«ã‚‚å¯¾å¿œã—ãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
- `B-PER`/`I-PER` ã¯ã€ãã®å˜èªãŒ *äºº(person)* ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…ˆé ­ã€ã¾ãŸã¯å†…éƒ¨ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
- `B-ORG`/`I-ORG` ã¯ã€ãã®å˜èªãŒ *çµ„ç¹”(organization)* ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…ˆé ­ã€ã¾ãŸã¯å†…éƒ¨ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ 
- `B-LOC`/`I-LOC` ã¯ã€ãã®å˜èªãŒ *å ´æ‰€(location)* ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…ˆé ­ã€ã¾ãŸã¯å†…éƒ¨ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ 
- `B-MISC`/`I-MISC` ã¯ã€ãã®å˜èªãŒ *ãã®ä»–(miscellaneous)* ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å…ˆé ­ã€ã¾ãŸã¯å†…éƒ¨ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚

ã•ã¦ã€å…ˆã»ã©ã®ãƒ©ãƒ™ãƒ«ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

ã¾ãŸã€`B-` ã¨ `I-`ã®ãƒ©ãƒ™ãƒ«ã‚’æ··åœ¨ã•ã›ãŸä¾‹ã¨ã—ã¦ã€å­¦ç¿’ã‚»ãƒƒãƒˆã®4ç•ªç›®ã®è¦ç´ ã«ã¤ã„ã¦åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

ã“ã®ã‚ˆã†ã«ã€"European Union" ã¨ "Werner Zwingmann" ã®ã‚ˆã†ã«2ã¤ã®å˜èªã«ã¾ãŸãŒã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯ã€æœ€åˆã®å˜èªã«ã¯ `B-` ãƒ©ãƒ™ãƒ«ãŒã€2ç•ªç›®ã®å˜èªã«ã¯ `I-` ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚Œã¾ã™ã€‚

> [!TIP]
> âœï¸ **ã‚ãªãŸã®ç•ªã§ã™ï¼** åŒã˜2ã¤ã®æ–‡ã‚’POSãƒ©ãƒ™ãƒ«ã¾ãŸã¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ãƒ©ãƒ™ãƒ«ã¨ä¸€ç·’ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†

<Youtube id="iY2AZYdZAr0"/>

ã„ã¤ã‚‚ã®ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ãŒæ„å‘³ã‚’ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã¯ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚[ç¬¬6ç« ](/course/ja/chapter6/)ã§è¦‹ãŸã‚ˆã†ã«ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å ´åˆã®å¤§ããªé•ã„ã¯ã€å…¥åŠ›ãŒã‚ã‚‰ã‹ã˜ã‚ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã¦ã„ã‚‹ã¨è¨€ã†äº‹ã§ã™ã€‚å¹¸ã„ãªã“ã¨ã«ã€tokenizer API ã¯ã“ã®ç‚¹ã‚’ã‹ãªã‚Šç°¡å˜ã«å‡¦ç†ã§ãã¾ã™ã€‚ç‰¹åˆ¥ãªãƒ•ãƒ©ã‚°ã‚’æŒ‡å®šã—ã¦ `tokenizer` ã«è­¦å‘Šã™ã‚‹ã ã‘ã§ã™ã€‚

ã¾ãšæœ€åˆã«ã€`tokenizer` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚å‰ã«è¿°ã¹ãŸã‚ˆã†ã«ã€äº‹å‰å­¦ç¿’æ¸ˆã¿BERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹äºˆå®šãªã®ã§ã€é–¢é€£ã™ã‚‹tokenizerã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ã‚ãªãŸã¯`model_checkpoint` ã‚’è‡ªç”±ã«ç½®ãæ›ãˆã‚‹äº‹ãŒã§ãã¾ã™ã€‚[Hub](https://huggingface.co/models) ã«ã‚ã‚‹å¥½ããªãƒ¢ãƒ‡ãƒ«ã‚„ã€è‡ªåˆ†ã®ç«¯æœ«ã«ä¿å­˜ã—ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚„tokenizerã‚’ã§ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

å”¯ä¸€ã®åˆ¶ç´„ã¯ã€tokenizerãŒ ğŸ¤— Tokenizers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã£ã¦ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Œé«˜é€Ÿã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚[ã“ã®å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«](https://huggingface.co/transformers/#supported-frameworks) ã§é«˜é€Ÿãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒã¤å…¨ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä½¿ç”¨ã—ã¦ã„ã‚‹ `tokenizer` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒæœ¬å½“ã« ğŸ¤— Tokenizers ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€`is_fast` å±æ€§ã‚’è¦‹ã‚‹äº‹ãŒç¢ºèªã§ãã¾ã™ã€‚

```py
tokenizer.is_fast
```

```python out
True
```

ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‰ã®å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã«ã¯ã€æ™®æ®µé€šã‚Š `tokenizer` ã‚’ä½¿ç”¨ã—ã¦ã€ `is_split_into_words=True` ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§ã™ã€‚

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

è¦‹ã¦ã®é€šã‚Šã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¯ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã™ã‚‹ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå…ˆé ­ã® `[CLS]` ã¨æœ«å°¾ã® `[SEP]` ï¼‰ã‚’è¿½åŠ ã—ã€ã»ã¨ã‚“ã©ã®å˜èªã¯ãã®ã¾ã¾ã«ã—ã¾ã—ãŸã€‚ã—ã‹ã—ã€`lamb`ã¨ã„ã†å˜èªã¯`la`ã¨`##mb`ã¨ã„ã†2ã¤ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã¾ã—ãŸã€‚ã“ã®ãŸã‚ã€å…¥åŠ›ã¨ãƒ©ãƒ™ãƒ«ã®é–“ã«ãƒŸã‚¹ãƒãƒƒãƒãŒç”Ÿã˜ã¾ã™ã€‚ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã«ã¯9ã¤ã®è¦ç´ ã—ã‹ã‚ã‚Šã¾ã›ã‚“ãŒã€å…¥åŠ›ã®ãƒªã‚¹ãƒˆã«ã¯12ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’è€ƒæ…®ã™ã‚‹ã®ã¯ç°¡å˜ã§ã™ãŒï¼ˆæœ€åˆã¨æœ€å¾Œã«ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ã¦ã„ã¾ã™ï¼‰ã€ã™ã¹ã¦ã®ãƒ©ãƒ™ãƒ«ã‚’é©åˆ‡ãªå˜èªã«æƒãˆã‚‹ã“ã¨ã‚‚å¿…è¦ã§ã™ã€‚

å¹¸ã„ã€é«˜é€Ÿãªtokenizerã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€ğŸ¤— Tokenizers ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ‘ãƒ¯ãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã€ãã‚Œãã‚Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¯¾å¿œã™ã‚‹å˜èªã«ç°¡å˜ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ (ã“ã‚Œã¯ [ç¬¬ï¼–ç« ](/course/ja/chapter6/3) ã§è¦‹ãŸã¨ãŠã‚Šã§ã™)ã€‚

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

ã»ã‚“ã®å°‘ã—ã®ä½œæ¥­ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‚ˆã†ã«ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆã‚’æ‹¡å¼µã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æœ€åˆã«é©ç”¨ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã¯ã€ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã«ã¯ `-100` ã¨ã„ã†ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚ã“ã‚Œã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ `-100` ãŒã“ã‚Œã‹ã‚‰ä½¿ã†æå¤±é–¢æ•°ï¼ˆã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰ã§ç„¡è¦–ã•ã‚Œã‚‹æ•°ã ã‹ã‚‰ã§ã™ã€‚æ¬¡ã«ã€å˜èªå†…ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å˜èªã®å…ˆé ­ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨åŒã˜ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä¸€éƒ¨ã§ã‚ã‚‹ãŸã‚ã§ã™ã€‚å˜èªã®å†…éƒ¨ã«ã‚ã‚Šã€ã‹ã¤å…ˆé ­ã«ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦ã¯ã€`B-` ã‚’ `I-` ã«ç½®ãæ›ãˆã¾ã™ï¼ˆãã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é–‹å§‹ã—ãªã„ãŸã‚ã§ã™ï¼‰ã€‚

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

ãã‚Œã§ã¯ã€æœ€åˆã®æ–‡ç« ã§è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

è¦‹ã¦ã‚ã‹ã‚‹ã‚ˆã†ã«ã€ã“ã®é–¢æ•°ã¯æœ€åˆã¨æœ€å¾Œã®2ã¤ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ `-100` ã‚’è¿½åŠ ã—ã€2ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã•ã‚ŒãŸå˜èªã«å¯¾ã—ã¦æ–°ãŸã« `0` ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚

> [!TIP]
> âœï¸ **ã‚ãªãŸã®ç•ªã§ã™ï¼** ç ”ç©¶è€…ã®ä¸­ã«ã¯ã€1ã¤ã®å˜èªã«ã¯1ã¤ã®ãƒ©ãƒ™ãƒ«ã—ã‹ä»˜ã‘ãšã€ä¸ãˆã‚‰ã‚ŒãŸå˜èªå†…ã®ä»–ã®ã‚µãƒ–ãƒˆãƒ¼ã‚¯ãƒ³ã«`-100`ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å¥½ã‚€äººã‚‚ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€å¤šãã®ã‚µãƒ–ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã•ã‚Œã‚‹é•·ã„å˜èªãŒå­¦ç¿’æ™‚ã®æå¤±ã«å¤§ããå¯„ä¸ã™ã‚‹ã®ã‚’é¿ã‘ã‚‹ãŸã‚ã§ã™ã€‚ã“ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€ãƒ©ãƒ™ãƒ«ã¨å…¥åŠ›IDã‚’ä¸€è‡´ã•ã›ã‚‹ã‚ˆã†ã«ã€å‰ã®é–¢æ•°ã‚’å¤‰æ›´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®å‰å‡¦ç†ã¨ã—ã¦ã€ã™ã¹ã¦ã®å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€ã™ã¹ã¦ã®ãƒ©ãƒ™ãƒ«ã«å¯¾ã—ã¦ `align_labels_with_tokens()` ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚é«˜é€Ÿãªtokenizerã®é€Ÿåº¦ã‚’æ´»ã‹ã™ã«ã¯ã€ãŸãã•ã‚“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒæ™‚ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã®ãŒã‚ˆã„ã§ã—ã‚‡ã†ã€‚ãã“ã§ã€ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹é–¢æ•°ã‚’æ›¸ã„ã¦ã€ `Dataset.map()` ãƒ¡ã‚½ãƒƒãƒ‰ã« `batched=True` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ã‚‡ã†ã€‚ä»¥å‰ã®ä¾‹ã¨å”¯ä¸€é•ã†ã®ã¯ã€tokenizerã¸ã®å…¥åŠ›ãŒãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã“ã®å ´åˆã¯å˜èªã®ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼‰ã§ã‚ã‚‹å ´åˆã€ `word_ids()` é–¢æ•°ã¯å˜èªIDãŒæ¬²ã—ã„ãƒªã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å¿…è¦ã¨ã™ã‚‹ã®ã§ã€ã“ã‚Œã‚‚è¿½åŠ ã—ã¾ã™ã€‚

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

ã¾ã ã€å…¥åŠ›ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
ã“ã‚Œã¯å¾Œã§ãƒ‡ãƒ¼ã‚¿ç…§åˆãƒ„ãƒ¼ãƒ«ã§ãƒãƒƒãƒã‚’ä½œæˆã™ã‚‹ã¨ãã«ã‚„ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚

ã“ã‚Œã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²ã«å¯¾ã—ã¦ã€ã™ã¹ã¦ã®å‰å‡¦ç†ã‚’ä¸€åº¦ã«é©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

ä¸€ç•ªå¤§å¤‰ãªã¨ã“ã‚ã‚’ã‚„ã‚Šã¾ã—ãŸã­ï¼
ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãŒçµ‚ã‚ã£ãŸã®ã§ã€å®Ÿéš›ã®å­¦ç¿’ã¯[ç¬¬3ç« ](/course/ja/chapter3)ã§ã‚„ã£ãŸã‚ˆã†ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã­ã€‚

{#if fw === 'pt'}

## Trainer API ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹

å®Ÿéš›ã« `Trainer` ã‚’ä½¿ç”¨ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯ã€ã“ã‚Œã¾ã§ã¨åŒã˜ã§ã™ã€‚å¤‰æ›´ç‚¹ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹æ–¹æ³•ã¨ã€æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã ã‘ã§ã™ã€‚

{:else}

## Keras ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹

Kerasã‚’ä½¿ã£ãŸå®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ã“ã‚Œã¾ã§ã¨ã»ã¨ã‚“ã©åŒã˜ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹æ–¹æ³•ã¨ã€æŒ‡æ¨™è¨ˆç®—ã®é–¢æ•°ãŒå¤‰ã‚ã‚‹ã ã‘ã§ã™ã€‚

{/if}


### ãƒ‡ãƒ¼ã‚¿ç…§åˆ

[ç¬¬3ç« ](/course/ja/chapter3) ã«ã‚ã‚‹ã‚ˆã†ãª `DataCollatorWithPadding` ã¯å…¥åŠ› (å…¥åŠ› IDã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ— ID) ã®ã¿ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã®ã§ä½¿ãˆã¾ã›ã‚“ã€‚ã“ã“ã§ã¯ã€ãƒ©ãƒ™ãƒ«ã®ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‰ãªã„ã‚ˆã†ã«ã€å…¥åŠ›ã¨å…¨ãåŒã˜æ–¹æ³•ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚å€¤ã¨ã—ã¦ `-100` ã‚’ä½¿ç”¨ã—ã€å¯¾å¿œã™ã‚‹äºˆæ¸¬å€¤ãŒæå¤±è¨ˆç®—ã§ç„¡è¦–ã•ã‚Œã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

ã“ã‚Œã¯å…¨ã¦ [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification) ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã¾ã™ã€‚DataCollatorWithPadding` ã¨åŒæ§˜ã«ã€å…¥åŠ›ã®å‰å‡¦ç†ã«ä½¿ç”¨ã•ã‚Œã‚‹ `tokenizer` ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

ã“ã‚Œã‚’ã„ãã¤ã‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã«ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦å‘¼ã³å‡ºã™ã ã‘ã§ã‚ˆã„ã®ã§ã™ã€‚

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

ã“ã‚Œã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®1ç•ªç›®ã¨2ç•ªç›®ã®è¦ç´ ã®ãƒ©ãƒ™ãƒ«ã¨æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

è¦‹ã¦ã®é€šã‚Šã€2ã¤ç›®ã®ãƒ©ãƒ™ãƒ«ã®ã‚»ãƒƒãƒˆã¯æœ€åˆã®ãƒ©ãƒ™ãƒ«ã®é•·ã•ã« `-100`s ã‚’ä½¿ã£ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ï¼

{:else}

ãƒ‡ãƒ¼ã‚¿ç…§åˆã®æº–å‚™ãŒã§ãã¾ã—ãŸï¼
ã§ã¯ã€ã“ã‚Œã‚’ä½¿ã£ã¦ `tf.data.Dataset` ã‚’ `to_tf_dataset()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```


æ¬¡ã¯ã€ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã§ã™ã€‚

{/if}

{#if fw === 'tf'}

### ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©

ä»Šå›ã¯ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã®å•é¡Œã‚’æ‰±ã†ã®ã§ã€ `TFAutoModelForTokenClassification` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã™ã‚‹éš›ã«è¦šãˆã¦ãŠãã¹ãã“ã¨ã¯ã€ãƒ©ãƒ™ãƒ«ã®æ•°ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æ¸¡ã™ã“ã¨ã§ã™ã€‚æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ `num_labels` å¼•æ•°ã§ãã®æ•°ã‚’æ¸¡ã™ã“ã¨ã§ã™ãŒã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æœ€åˆã«è¦‹ãŸã‚ˆã†ãªç´ æ•µãªæ¨è«–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’å‹•ä½œã•ã›ãŸã„å ´åˆã¯ã€ä»£ã‚ã‚Šã«æ­£ã—ã„ãƒ©ãƒ™ãƒ«ã®å¯¾å¿œé–¢ä¿‚ã‚’è¨­å®šã—ãŸæ–¹ãŒè‰¯ã„ã§ã—ã‚‡ã†ã€‚

id2label` ã¨ `label2id` ã¨ã„ã† 2 ã¤ã®è¾æ›¸å‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã€ID ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã€ãƒ©ãƒ™ãƒ«ã‹ã‚‰ ID ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ã‚ã¨ã¯ãã‚Œã‚‰ã‚’ `TFAutoModelForTokenClassification.from_pretrained()` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã›ã°ã€ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã«ã‚»ãƒƒãƒˆã•ã‚Œã€é©åˆ‡ã«ä¿å­˜ã•ã‚Œã¦Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

[ç¬¬3ç« ](/course/ja/chapter3) ã§ `TFAutoModelForSequenceClassification` ã‚’å®šç¾©ã—ãŸã¨ãã®ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã¨ã€ã„ãã¤ã‹ã®é‡ã¿ãŒä½¿ã‚ã‚Œã¦ã„ãªã„ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ˜ãƒƒãƒ‰éƒ¨ã®é‡ã¿ï¼‰ã€ä»–ã®ã„ãã¤ã‹ã®é‡ã¿ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ï¼ˆæ–°ã—ãæ¥ç¶šã—ãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ãƒ˜ãƒƒãƒ‰ã®é‡ã¿ï¼‰ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ã„ã†è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã™ãã«ã§ã‚‚å®Ÿè¡Œã§ãã¾ã™ãŒã€ã¾ãšã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ã„æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ã“ã¨ã‚’å†ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```python
model.config.num_labels
```

```python out
9
```

> [!WARNING]
> âš ï¸ ãƒ©ãƒ™ãƒ«ã®æ•°ãŒé–“é•ã£ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã¨ã€å¾Œã§ `model.fit()` ã‚’å‘¼ã³å‡ºã™ã¨ãã«ã‚ˆãã‚ã‹ã‚‰ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚ã“ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒ‡ãƒãƒƒã‚°ã®éš›ã«å„ä»‹ãªã®ã§ã€ã“ã®ãƒã‚§ãƒƒã‚¯ã‚’å¿…ãšè¡Œã„ã€æœŸå¾…é€šã‚Šã®ãƒ©ãƒ™ãƒ«æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´

ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼
ã—ã‹ã—ã€ãã®å‰ã«ã‚‚ã†å°‘ã—ã ã‘ã‚„ã‚‹ã¹ãã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚Hugging Faceã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã€å­¦ç¿’ç”¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚‚ã—Notebookã§ä½œæ¥­ã—ã¦ã„ã‚‹ãªã‚‰ã€ã“ã‚Œã‚’åŠ©ã‘ã‚‹ä¾¿åˆ©ãªé–¢æ•°ãŒã‚ã‚Šã¾ã™ã€‚

```python
from huggingface_hub import notebook_login

notebook_login()
```

ã“ã‚Œã«ã‚ˆã‚Šã€Hugging Faceã®ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’å…¥åŠ›ã™ã‚‹ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

Notebookã§ä½œæ¥­ã—ã¦ã„ãªã„å ´åˆã¯ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«æ¬¡ã®è¡Œã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§ã™ã€‚

```bash
huggingface-cli login
```

ãƒ­ã‚°ã‚¤ãƒ³ã—ãŸã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚‚ã®ã‚’ã™ã¹ã¦æº–å‚™ã—ã¾ã™ã€‚

ğŸ¤— Transformers ã¯ä¾¿åˆ©ãª `create_optimizer()` é–¢æ•°ã‚’æä¾›ã—ã¦ãŠã‚Šã€é‡ã¿ã®æ¸›è¡°ã¨å­¦ç¿’ç‡ã®æ¸›è¡°ã‚’é©åˆ‡ã«è¨­å®šã—ãŸ `AdamW` ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ã“ã®2ã¤ã®è¨­å®šã¯ã€çµ„ã¿è¾¼ã¿ã® `Adam` ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨æ¯”è¼ƒã—ã¦ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã§ã—ã‚‡ã†ã€‚

```python
from transformers import create_optimizer
import tensorflow as tf

# Train in mixed-precision float16
# Comment this line out if you're using a GPU that will not benefit from this
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

ã¾ãŸã€`compile()`ã« `loss`å¼•æ•°ã‚’ä¸ãˆãªã„ã“ã¨ã«ã‚‚æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒå†…éƒ¨çš„ã«æå¤±ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‹ã‚‰ã§ã™ã€‚æå¤±ãªã—ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã€å…¥åŠ›è¾æ›¸ã«ãƒ©ãƒ™ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã¨ï¼ˆç§ãŸã¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¡Œã£ã¦ã„ã‚‹ã‚ˆã†ã«ï¼‰ã€ãƒ¢ãƒ‡ãƒ«ã¯ãã®å†…éƒ¨æå¤±ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã—ã€ãã‚Œã¯é¸ã‚“ã ã‚¿ã‚¹ã‚¯ã¨ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«é©ã—ãŸã‚‚ã®ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚

æ¬¡ã«ã€å­¦ç¿’ä¸­ã«ãƒ¢ãƒ‡ãƒ«ã‚’Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®`PushToHubCallback`ã‚’å®šç¾©ã—ã€ãã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒƒãƒˆã•ã›ã¾ã™ã€‚

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

`hub_model_id` å¼•æ•°ã«ã¯ã€ãƒ—ãƒƒã‚·ãƒ¥ã—ãŸã„ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ•ãƒ«ãƒãƒ¼ãƒ ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ (ç‰¹ã«ã€ç‰¹å®šã®çµ„ç¹”ï¼ˆorganizationï¼‰ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã¯ã“ã®å¼•æ•°ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚ä¾‹ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ã‚’ [`huggingface-course` organization](https://huggingface.co/huggingface-course) ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã€ `hub_model_id="huggingface-course/bert-finetuned-ner"` ã‚’è¿½åŠ ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä½¿ç”¨ã•ã‚Œã‚‹ãƒªãƒã‚¸ãƒˆãƒªã¯ã‚ãªãŸã®åå‰ãŒä½¿ã‚ã‚Œã€è¨­å®šã—ãŸå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¡ãªã‚“ã åå‰ã€ä¾‹ãˆã° `"cool_huggingface_user/bert-finetuned-ner"` ã¨ãªã‚Šã¾ã™ã€‚

> [!TIP]
> ğŸ’¡ ä½¿ç”¨ã—ã¦ã„ã‚‹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã™ã§ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãƒ—ãƒƒã‚·ãƒ¥ã—ãŸã„ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ­ãƒ¼ãƒ³ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã†ã§ãªã„å ´åˆã¯ã€`model.fit()` ã‚’å‘¼ã³å‡ºã™ã¨ãã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã€æ–°ã—ã„åå‰ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

å­¦ç¿’ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹é–“ã€ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã‚‹ãŸã³ã«ï¼ˆä»Šå›ã®ä¾‹ã§ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ï¼‰ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã®ã‚ˆã†ã«ã—ã¦ãŠã‘ã°ã€å¿…è¦ã«å¿œã˜ã¦åˆ¥ã®ãƒã‚·ãƒ³ã§å­¦ç¿’ã‚’å†é–‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®æ®µéšã§ã€Model Hubä¸Šã®æ¨è«–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã€å‹äººã¨å…±æœ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã“ã‚Œã§ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«å¾®èª¿æ•´ã«æˆåŠŸã—ã¾ã—ãŸã€‚ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

ã—ã‹ã—ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã®å®ŸåŠ›ã¯ã„ã‹ã»ã©ã§ã—ã‚‡ã†ã‹ï¼Ÿãã‚Œã‚’çŸ¥ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®æŒ‡æ¨™ã‚’ä½¿ã£ã¦è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

{/if}


### æŒ‡æ¨™

{#if fw === 'pt'}

`Trainer` ã«ã‚¨ãƒãƒƒã‚¯æ¯ã«æŒ‡æ¨™ã‚’è¨ˆç®—ã•ã›ã‚‹ãŸã‚ã«ã¯ã€`compute_metrics()` é–¢æ•°ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯äºˆæ¸¬ã¨ãƒ©ãƒ™ãƒ«ã®é…åˆ—ã‚’å—ã‘å–ã‚Šã€æŒ‡æ¨™ã®åå‰ã¨å€¤ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™é–¢æ•°ã§ã™ã€‚

ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡äºˆæ¸¬ã®è©•ä¾¡ã«ä½¿ã‚ã‚Œã‚‹ä¼çµ±çš„ãªæ çµ„ã¿ã¯[*seqeval*](https://github.com/chakki-works/seqeval)ã§ã™ã€‚ã“ã®æŒ‡æ¨™ã‚’ä½¿ã†ã«ã¯ã€ã¾ãš *seqeval* ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚


```py
!pip install seqeval
```

ãã—ã¦ã€[ç¬¬3ç« ](/course/ja/chapter3) ã§è¡Œã£ãŸã‚ˆã†ã« `evaluate.load()` é–¢æ•°ã§èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

{:else}

ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡äºˆæ¸¬ã®è©•ä¾¡ã«ä½¿ã‚ã‚Œã‚‹ä¼çµ±çš„ãªæ çµ„ã¿ã¯[*seqeval*](https://github.com/chakki-works/seqeval)ã§ã™ã€‚ã“ã®è©•ä¾¡æŒ‡æ¨™ã‚’ä½¿ã†ã«ã¯ã€ã¾ãš*seqeval*ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```py
!pip install seqeval
```

ãã—ã¦ã€[ç¬¬3ç« ](/course/ja/chapter3) ã§è¡Œã£ãŸã‚ˆã†ã« `evaluate.load()` é–¢æ•°ã§èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

ã“ã®æŒ‡æ¨™ã¯æ¨™æº–çš„ãªç²¾åº¦æŒ‡æ¨™ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã›ã‚“ï¼šå®Ÿéš›ã«ã¯ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’æ•´æ•°ã§ã¯ãªãæ–‡å­—åˆ—ã¨ã—ã¦å—ã‘å–ã‚‹ã®ã§ã€äºˆæ¸¬å€¤ã¨ãƒ©ãƒ™ãƒ«ã‚’æŒ‡æ¨™ã«æ¸¡ã™å‰ã«å®Œå…¨ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãã‚Œã§ã¯ã€ã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã€æœ€åˆã®å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹ãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã¾ã™ã€‚

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

ãã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2ã®å€¤ã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§ã€ãã‚Œã‚‰ã®ç–‘ä¼¼äºˆæ¸¬ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

ã“ã®æŒ‡æ¨™ã¯äºˆæ¸¬å€¤ã®ãƒªã‚¹ãƒˆï¼ˆ1ã¤ã ã‘ã§ã¯ãªã„ï¼‰ã¨ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã¯ãã®å‡ºåŠ›ã§ã™ã€‚


```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

ã¨ã¦ã‚‚å¤šãã®æƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™ï¼
å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ç²¾åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢ã€ãã—ã¦ç·åˆçš„ãªã‚¹ã‚³ã‚¢ã§ã™ã€‚

ç§é”ã®æŒ‡æ¨™è¨ˆç®—ã§ã¯ã€ç·åˆçš„ãªã‚¹ã‚³ã‚¢ã®ã¿ã‚’ä¿æŒã™ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚
ã—ã‹ã—ã€ãŠæœ›ã¿ãªã‚‰`compute_metrics()`é–¢æ•°ã‚’å¾®èª¿æ•´ã—ã¦ã€å ±å‘Šã•ã›ãŸã„ã™ã¹ã¦ã®æŒ‡æ¨™ã‚’è¿”ã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

ã“ã® `compute_metrics()` é–¢æ•°ã¯ã€ã¾ãš æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå‡ºåŠ›ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€å¤§å€¤ã‚’äºˆæ¸¬å€¤ã«å¤‰æ›ã—ã¾ã™ï¼ˆæœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå‡ºåŠ›ã™ã‚‹ç”Ÿã®å€¤ã¯é€šå¸¸ã¯ç¢ºç‡ã«å¤‰æ›ã•ã‚Œã¾ã™ãŒã€æœ€å¤§å€¤ã¯ç¢ºç‡ã«å¤‰æ›ã—ãªãã¨ã‚‚åŒã˜ãªã®ã§ã€softmax ã§ç¢ºç‡ã«å¤‰æ›ã•ã›ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚æ¬¡ã«ã€ãƒ©ãƒ™ãƒ«ã¨äºˆæ¸¬å€¤ã®ä¸¡æ–¹ã‚’æ•´æ•°ã‹ã‚‰æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ©ãƒ™ãƒ«ãŒ `-100` ã§ã‚ã‚‹å€¤ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã€ãã®çµæœã‚’ `metric.compute()` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã—ã¾ã™ã€‚

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

ã“ã‚Œã§ã€`Trainer` ã‚’å®šç¾©ã™ã‚‹æº–å‚™ã¯ã»ã¼æ•´ã„ã¾ã—ãŸã€‚ã‚ã¨ã¯å¾®èª¿æ•´ã‚’ã™ã‚‹ãŸã‚ã® `model` ãŒå¿…è¦ã§ã™ã€‚

{:else}

ã¨ã¦ã‚‚å¤šãã®æƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™ï¼
å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ç²¾åº¦ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢ã€ãã—ã¦ç·åˆçš„ãªã‚¹ã‚³ã‚¢ã§ã™ã€‚
ã§ã¯ã€å®Ÿéš›ã«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’ä½¿ã£ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦ã¿ã‚‹ã¨ã©ã†ãªã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

TensorFlowã¯äºˆæ¸¬å€¤ã‚’é€£çµã™ã‚‹ã“ã¨ã‚’å¥½ã¿ã¾ã›ã‚“ã€‚ãªãœãªã‚‰ã€äºˆæ¸¬å€¤ã¯å¯å¤‰é•·ã§ã‚ã‚‹ã‹ã‚‰ã§ã™ã€‚ã¤ã¾ã‚Šã€`model.predict()`ã‚’ãã®ã¾ã¾ä½¿ã†ã“ã¨ã¯ã§ããªã„ã®ã§ã™ã€‚ã—ã‹ã—ã€ã ã‹ã‚‰ã¨ã„ã£ã¦ã“ã“ã§æ­¢ã‚ã‚‹ã¤ã‚‚ã‚Šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¸€åº¦ã«ã„ãã¤ã‹ã®äºˆæ¸¬ã‚’ãƒãƒƒãƒã§å®Ÿè¡Œã—ã¦å–å¾—ã—ã€ãã‚Œã‚‰ã‚’ä¸€ã¤ã®å¤§ããªé•·ã„ãƒªã‚¹ãƒˆã«é€£çµã—ã€ãƒã‚¹ã‚­ãƒ³ã‚°ã‚„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¤ºã™ `-100` ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€ã©ã†ã§ã—ãŸã‹ï¼Ÿ
ã‚‚ã—ã€åŒã˜ã‚ˆã†ãªæ•°å€¤ãŒå‡ºãŸã®ãªã‚‰ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æˆåŠŸã§ã™ã€‚

{/if}

{#if fw === 'pt'}

### ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©

ä»Šå›ã¯ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã®å•é¡Œã‚’æ‰±ã†ã®ã§ã€ `AutoModelForTokenClassification` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã™ã‚‹éš›ã«è¦šãˆã¦ãŠãã¹ãã“ã¨ã¯ã€ãƒ©ãƒ™ãƒ«ã®æ•°ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æ¸¡ã™ã“ã¨ã§ã™ã€‚æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ `num_labels` å¼•æ•°ã§ãã®æ•°ã‚’æ¸¡ã™ã“ã¨ã§ã™ãŒã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æœ€åˆã«è¦‹ãŸã‚ˆã†ãªç´ æ•µãªæ¨è«–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’å‹•ä½œã•ã›ãŸã„å ´åˆã¯ã€ä»£ã‚ã‚Šã«æ­£ã—ã„ãƒ©ãƒ™ãƒ«ã®å¯¾å¿œé–¢ä¿‚ã‚’è¨­å®šã—ãŸæ–¹ãŒè‰¯ã„ã§ã—ã‚‡ã†ã€‚

id2label` ã¨ `label2id` ã¨ã„ã† 2 ã¤ã®è¾æ›¸å‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã€ID ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã€ãƒ©ãƒ™ãƒ«ã‹ã‚‰ ID ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ã‚ã¨ã¯ãã‚Œã‚‰ã‚’ `AutoModelForTokenClassification.from_pretrained()` ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã›ã°ã€ãƒ¢ãƒ‡ãƒ«ã®æ§‹æˆã«è¨­å®šã•ã‚Œã€é©åˆ‡ã«ä¿å­˜ã•ã‚Œã¦ Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

[ç¬¬3ç« ](/course/ja/chapter3) ã§ `AutoModelForSequenceClassification` ã‚’å®šç¾©ã—ãŸã¨ãã®ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã¨ã€ã„ãã¤ã‹ã®é‡ã¿ãŒä½¿ã‚ã‚Œã¦ã„ãªã„ï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ˜ãƒƒãƒ‰éƒ¨ã®é‡ã¿ï¼‰ã€ä»–ã®ã„ãã¤ã‹ã®é‡ã¿ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ï¼ˆæ–°ã—ãæ¥ç¶šã—ãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ãƒ˜ãƒƒãƒ‰ã®é‡ã¿ï¼‰ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ã„ã†è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã™ãã«ã§ã‚‚å®Ÿè¡Œã§ãã¾ã™ãŒã€ã¾ãšã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ã„æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ã“ã¨ã‚’å†ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```python
model.config.num_labels
```

```python out
9
```

> [!WARNING]
> âš ï¸ ãƒ©ãƒ™ãƒ«ã®æ•°ãŒé–“é•ã£ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã¨ã€å¾Œã§ `model.fit()` ã‚’å‘¼ã³å‡ºã™ã¨ãã«ã‚ˆãã‚ã‹ã‚‰ãªã„ã‚¨ãƒ©ãƒ¼ï¼ˆ"CUDA error: device-side assert triggered"ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ï¼‰ãŒç™ºç”Ÿã—ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å ±å‘Šã•ã‚Œã‚‹ãƒã‚°ã®åŸå› ã¨ã—ã¦ä¸€ç•ªå¤šã„ã‚‚ã®ã§ã™ã€‚ã“ã®ãƒã‚§ãƒƒã‚¯ã‚’å¿…ãšè¡Œã„ã€æœŸå¾…é€šã‚Šã®ãƒ©ãƒ™ãƒ«æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´

ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼
ã—ã‹ã—ã€`Trainer`ã‚’å®šç¾©ã™ã‚‹å‰ã«ã€æœ€å¾Œã«2ã¤ã®ã“ã¨ã‚’ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Hugging Faceã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã€å­¦ç¿’ç”¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚‚ã—Notebookã§ä½œæ¥­ã—ã¦ã„ã‚‹ãªã‚‰ã€ã“ã‚Œã‚’åŠ©ã‘ã‚‹ä¾¿åˆ©ãªé–¢æ•°ãŒã‚ã‚Šã¾ã™ã€‚

```python
from huggingface_hub import notebook_login

notebook_login()
```

ã“ã‚Œã«ã‚ˆã‚Šã€Hugging Faceã®ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’å…¥åŠ›ã™ã‚‹ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

Notebookã§ä½œæ¥­ã—ã¦ã„ãªã„å ´åˆã¯ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«æ¬¡ã®è¡Œã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§ã™ã€‚

```bash
huggingface-cli login
```

å®Œäº†ã—ãŸã‚‰ã€`TrainingArguments`ã‚’å®šç¾©ã™ã‚‹äº‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã»ã¨ã‚“ã©ã¯ä»¥å‰ã«è¦‹ãŸã“ã¨ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ç‡ã€å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚¦ã‚§ã‚¤ãƒˆæ¸›è¡°ãªã©ï¼‰ã‚’è¨­å®šã—ã€`push_to_hub=True` ã‚’æŒ‡å®šã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦å„ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«è©•ä¾¡ã—ã€ãã®çµæœã‚’ãƒ¢ãƒ‡ãƒ«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’æŒ‡ç¤ºã—ã¾ã™ã€‚


ãªãŠã€ `hub_model_id` å¼•æ•°ã§ãƒ—ãƒƒã‚·ãƒ¥å…ˆã®ãƒªãƒã‚¸ãƒˆãƒªåã‚’æŒ‡å®šã§ãã¾ã™ (ç‰¹ã«ã€ç‰¹å®šã®çµ„ç¹”ï¼ˆorganizationï¼‰ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã¯ã€ã“ã®å¼•æ•°ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚ä¾‹ãˆã°ã€[`huggingface-course` organization](https://huggingface.co/huggingface-course) ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´åˆã€`TrainingArguments` ã« `hub_model_id="huggingface-course/bert-finetuned-ner"` ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä½¿ç”¨ã•ã‚Œã‚‹ãƒªãƒã‚¸ãƒˆãƒªã¯ã‚ãªãŸã®åå‰ãŒä½¿ã‚ã‚Œã€è¨­å®šã—ãŸå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¡ãªã‚“ã åå‰ã€ä¾‹ãˆã°ä»Šå›ã®ä¾‹ã§ã¯ `"sgugger/bert-finetuned-ner"` ã¨ãªã‚Šã¾ã™ã€‚

> [!TIP]
> ğŸ’¡ ä½¿ç”¨ã™ã‚‹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãƒ—ãƒƒã‚·ãƒ¥ã—ãŸã„ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ­ãƒ¼ãƒ³ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã†ã§ãªã„å ´åˆã¯ã€`Trainer` ã‚’å®šç¾©ã™ã‚‹éš›ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã€æ–°ã—ã„åå‰ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

æœ€å¾Œã«ã€ã™ã¹ã¦ã‚’ `Trainer` ã«æ¸¡ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã ã‘ã§ã™ã€‚

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

å­¦ç¿’ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹é–“ã€ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã‚‹ãŸã³ã«ï¼ˆã“ã“ã§ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ï¼‰ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã®ã‚ˆã†ã«ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦åˆ¥ã®ãƒã‚·ãƒ³ã§å­¦ç¿’ã‚’å†é–‹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

å­¦ç¿’ãŒå®Œäº†ã—ãŸã‚‰ã€`push_to_hub()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```py
trainer.push_to_hub(commit_message="Training complete")
```

ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯ã€ä»Šè¡Œã£ãŸã‚³ãƒŸãƒƒãƒˆã® URL ã‚’è¿”ã™ã®ã§ã€ãã‚Œã‚’æ¤œæŸ»ã—ãŸã„å ´åˆã¯ã€ã“ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

ã¾ãŸã€`Trainer`ã¯ã™ã¹ã¦ã®è©•ä¾¡çµæœã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’èµ·è‰ã—ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã®æ®µéšã§ã€Model Hubä¸Šã®æ¨è«–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã€å‹äººã¨å…±æœ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã§ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«å¾®èª¿æ•´ã«æˆåŠŸã—ã¾ã—ãŸã€‚
ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

ã‚‚ã†å°‘ã—æ·±ãå­¦ç¿’ãƒ«ãƒ¼ãƒ—ã«ã¤ã„ã¦å­¦ã³ãŸã„å ´åˆã¯ã€ğŸ¤— Accelerate ã‚’ä½¿ã£ã¦åŒã˜ã“ã¨ã‚’ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

## ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—

ãã‚Œã§ã¯ã€å¿…è¦ãªéƒ¨åˆ†ã‚’ç°¡å˜ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã‚‹ã‚ˆã†ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å…¨ä½“åƒã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã¯ã€[ç¬¬3ç« ](/course/ja/chapter3/4) ã§è¡Œã£ãŸã“ã¨ã¨ã‚ˆãä¼¼ã¦ã„ã¾ã™ãŒã€è©•ä¾¡ã®ãŸã‚ã«å°‘ã—å¤‰æ›´ãŒåŠ ãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®æº–å‚™

ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ `DataLoader` ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ã€`data_collator` ã‚’ `collate_fn` ã¨ã—ã¦å†åˆ©ç”¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¾ã™ã€‚ãŸã ã—ã€æ¤œè¨¼ã‚»ãƒƒãƒˆã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¾ã›ã‚“ã€‚

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®å†å®šç¾©ã‚’è¡Œã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ä»¥å‰ã®å¾®èª¿æ•´ã‚’ç¶™ç¶šã™ã‚‹ã®ã§ã¯ãªãã€BERTã§äº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å†ã³é–‹å§‹ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã§ã™ã€‚

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

ãã‚Œã‹ã‚‰ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ã€å¤å…¸çš„ãª `AdamW` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ `Adam` ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ãŒã€é‡ã¿ã®æ¸›è¡°ã®é©ç”¨æ–¹æ³•ã‚’ä¿®æ­£ã—ãŸã‚‚ã®ã§ã™ã€‚

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ã“ã‚Œã‚‰ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã™ã¹ã¦å–å¾—ã—ãŸã‚‰ã€ãã‚Œã‚‰ã‚’ `accelerator.prepare()` ãƒ¡ã‚½ãƒƒãƒ‰ã«é€ã‚Šã¾ã™ã€‚

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

> [!TIP]
> ğŸš¨ TPUã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã¯ã€ä¸Šã®ã‚»ãƒ«ã‹ã‚‰å§‹ã¾ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å…¨ã¦å°‚ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ã«ç§»å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚è©³ã—ãã¯[ç¬¬3ç« ](/course/ja/chapter3)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ã“ã‚Œã§ `train_dataloader` ã‚’ `accelerator.prepare()` ã«é€ã£ãŸã®ã§ã€ãã®ãƒ‡ãƒ¼ã‚¿é•·ã‚’ç”¨ã„ã¦å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯dataloaderã®é•·ã•ã‚’å¤‰æ›´ã™ã‚‹ã®ã§ã€å¸¸ã«dataloaderã‚’æº–å‚™ã—ãŸå¾Œã«è¡Œã†å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚ã“ã“ã§ã¯ã€å­¦ç¿’ç‡ã‹ã‚‰0ã¾ã§å¤å…¸çš„ãªç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€å¾Œã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’Hubã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ãŸã‚ã«ã€ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã« `Repository` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæ¥­ãƒ•ã‚©ãƒ«ãƒ€ã«ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãšã€ã¾ã ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã‘ã‚Œã°Hugging Faceã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«ã«ä»˜ä¸ã—ãŸã„ãƒ¢ãƒ‡ãƒ«IDã‹ã‚‰ãƒªãƒã‚¸ãƒˆãƒªåã‚’æ±ºå®šã—ã¾ã™ã€‚ï¼ˆ`repo_name`ã¯è‡ªç”±ã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼›ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹ã ã‘ã§ã€ã“ã‚Œã¯é–¢æ•° `get_full_repo_name()` ãŒè¡Œã£ã¦ã„ã‚‹äº‹ã§ã™ï¼‰ã€‚

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

ãã—ã¦ã€ãã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã«ã‚¯ãƒ­ãƒ¼ãƒ³ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã™ã§ã«å­˜åœ¨ã™ã‚‹ã®ã§ã‚ã‚Œã°ã€ã“ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã¯ä½œæ¥­ä¸­ã®ãƒªãƒã‚¸ãƒˆãƒªã®æ—¢å­˜ã®ã‚¯ãƒ­ãƒ¼ãƒ³ã§ã‚ã‚‹ã¹ãã§ã™ã€‚

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ã“ã‚Œã§ `repo.push_to_hub()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã§ã€`output_dir` ã«ä¿å­˜ã—ãŸã‚‚ã®ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«ä¸­é–“ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### å­¦ç¿’ãƒ«ãƒ¼ãƒ—

ã“ã‚Œã§å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ãæº–å‚™ãŒã§ãã¾ã—ãŸã€‚
è©•ä¾¡éƒ¨åˆ†ã‚’ç°¡ç•¥åŒ–ã™ã‚‹ãŸã‚ã€`postprocess()` é–¢æ•°ã‚’ç°¡å˜ã«å®šç¾©ã—ã¾ã™ã€‚
ã“ã®é–¢æ•°ã¯äºˆæ¸¬å€¤ã¨ãƒ©ãƒ™ãƒ«ã‚’å—ã‘å–ã£ã¦ `metric` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒæœŸå¾…ã™ã‚‹ã‚ˆã†ãªæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

æ¬¡ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ãã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é€²æ—ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’å®šç¾©ã—ãŸå¾Œã€ãƒ«ãƒ¼ãƒ—ã¯3ã¤ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†ã‹ã‚Œã¾ã™ã€‚

- å­¦ç¿’ãã®ã‚‚ã®ã€‚`train_dataloader`ã«å¯¾ã™ã‚‹å¤å…¸çš„ãªç¹°ã‚Šè¿”ã—ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’å‰æ–¹ã«ä¼æ’­ã•ã›ã€å¾Œæ–¹ã«é€†ä¼æ’­ã•ã›ã€æœ€é©åŒ–ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡Œã„ã¾ã™ã€‚

- è©•ä¾¡ã€‚ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ãƒãƒƒãƒã§å–å¾—ã—ãŸå¾Œã«ã€æ–°ã—ã„äº‹ã‚’ã—ã¾ã™ã€‚2ã¤ã®ãƒ—ãƒ­ã‚»ã‚¹ã§å…¥åŠ›ã¨ãƒ©ãƒ™ãƒ«ã‚’ç•°ãªã‚‹å½¢çŠ¶ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€`gather()`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã¶å‰ã« `accelerator.pad_across_processes()` ã‚’ä½¿ã£ã¦äºˆæ¸¬å€¤ã¨ãƒ©ãƒ™ãƒ«ã‚’åŒã˜å½¢çŠ¶ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã™ã€‚ã“ã‚Œã‚’è¡Œã‚ãªã„ã¨ã€è©•ä¾¡ãŒã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã‹ã€æ°¸é ã«ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚ãã—ã¦ã€çµæœã‚’ `metric.add_batch()` ã«é€ã‚Šã€è©•ä¾¡ãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ãŸã‚‰ `metric.compute()` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

- ä¿å­˜ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚ã¾ãšãƒ¢ãƒ‡ãƒ«ã¨tokenizerã‚’ä¿å­˜ã—ã€æ¬¡ã« `repo.push_to_hub()` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚å¼•æ•° `blocking=False` ã‚’ä½¿ã£ã¦ã€ğŸ¤— Hub libraryã«éåŒæœŸå‡¦ç†ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã®æŒ‡å®šã‚’ã™ã‚‹ã¨ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é€šå¸¸é€šã‚Šè¡Œã‚ã‚Œã€ã“ã®ï¼ˆé•·ã„æ™‚é–“ã®ã‹ã‹ã‚‹ï¼‰å‘½ä»¤ã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

ä»¥ä¸‹ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã§ã™ã€‚


```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

ä»Šå›åˆã‚ã¦ğŸ¤— Accelerateã§ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã”è¦§ã«ãªã‚‹æ–¹ã®ãŸã‚ã«ã€ãã‚Œã«ä»˜éšã™ã‚‹3è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’å°‘ã—ç‚¹æ¤œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

æœ€åˆã®è¡Œã¯æ˜ã‚‰ã‹ã§ã™ã€‚ã“ã®è¡Œã¯ã€ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã€å…¨ãƒ—ãƒ­ã‚»ã‚¹ãŒãã®è¡Œã«é”ã™ã‚‹ã¾ã§ã€å‡¦ç†ã‚’å¾…ã¤ã‚ˆã†æŒ‡ç¤ºã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ä¿å­˜ã™ã‚‹å‰ã«ã€ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒåŒã˜ãƒ¢ãƒ‡ãƒ«ã«ãªã£ã¦ã„ã‚‹äº‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã§ã™ã€‚æ¬¡ã« `unwrapped_model` ã‚’å–å¾—ã—ã¾ã™ã€‚ã“ã‚Œã¯å®šç¾©ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚`accelerator.prepare()` ãƒ¡ã‚½ãƒƒãƒ‰ã¯åˆ†æ•£ã—ã¦å­¦ç¿’ã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ã®ã§ã€`save_pretrained()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŒãŸãªããªã‚Šã¾ã™ã€‚`accelerator.unwrap_model()` ãƒ¡ã‚½ãƒƒãƒ‰ã¯ãã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å…ƒã«æˆ»ã—ã¾ã™ã€‚æœ€å¾Œã«ã€`save_pretrained()` ã‚’å‘¼ã³å‡ºã—ã¾ã™ãŒã€ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã«ã¯ `torch.save()` ã®ä»£ã‚ã‚Šã« `accelerator.save()` ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã—ã¾ã™ã€‚

ã“ã‚ŒãŒå®Œäº†ã™ã‚‹ã¨ã€`Trainer` ã§å­¦ç¿’ã—ãŸã‚‚ã®ã¨ã»ã¼åŒã˜çµæœã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã§ãã‚ãŒã‚Šã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ [*huggingface-course/bert-finetuned-ner-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate) ã§ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å¾®èª¿æ•´ã‚’è©¦ã—ãŸã„å ´åˆã¯ã€ä¸Šã«ç¤ºã—ãŸã‚³ãƒ¼ãƒ‰ã‚’ç·¨é›†ã™ã‚‹ã“ã¨ã§ç›´æ¥å®Ÿè£…ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™!

{/if}

## å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†

Model Hubã§å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã§ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã¯æ—¢ã«ç´¹ä»‹ã—ã¾ã—ãŸã€‚ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®`pipeline`ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ã‚’æŒ‡å®šã—ã¾ã™ã€‚


```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ç´ æ™´ã‚‰ã—ã„ï¼
ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚‚ã®ã¨åŒã˜ã‚ˆã†ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼
