<FrameworkSwitchCourse {fw} />

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter4/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter4/section3_tf.ipynb"},
]} />

{/if}

ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ğŸ¤—Hubã«å…±æœ‰ã™ã‚‹æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã«ã¤ã„ã¦è¦‹ã¦ã„ãã¾ã™ã€‚Hubä¸Šã§ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã—ã€æ›´æ–°ã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã‚„ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã®ã§ã€ä»¥ä¸‹ã€ãã‚Œã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

<Youtube id="9yY3RB_GSPM"/>

ãŸã¨ãˆéå¸¸ã«ç‰¹æ®Šãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã•ã›ãŸã¨ã—ã¦ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å…±æœ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ™‚é–“ã¨è¨ˆç®—è³‡æºã‚’ç¯€ç´„ã—ã€æœ‰ç”¨ãªå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‹ã‚‰ã§ã™ã€‚ä»£ã‚ã‚Šã«ã€ä»–ã®äººã®æˆæœç‰©ã®æ©æµã‚’å—ã‘ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼

æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆã™ã‚‹ã«ã¯ã€æ¬¡ã®3ã¤ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™ï¼š

- `push_to_hub` APIã‚’ä½¿ç”¨ã™ã‚‹
- `huggingface_hub` Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹
- Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹

ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆã—ãŸã‚‰ã€git ã¨ git-lfs ã‚’ä½¿ã£ã¦ãƒªãƒã‚¸ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## `push_to_hub` APIã‚’ä½¿ç”¨ã™ã‚‹

{#if fw === 'pt'}

<Youtube id="Zh0FfmVrKX0"/>

{:else}

<Youtube id="pUh5cGmNV8Y"/>

{/if}

Hub ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€`push_to_hub` API ã‚’ä½¿ã†ã“ã¨ã§ã™ã€‚

å…ˆã«é€²ã‚€å‰ã«ã€ã‚ãªãŸãŒèª°ã§ã€ã©ã®ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã«æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚‹ã®ã‹ã‚’é€šçŸ¥ã™ã‚‹ãŸã‚ã«ã€èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ã‚‡ã†ã€‚`transformers`ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ç’°å¢ƒã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆ[ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](/course/chapter0)ã‚’å‚ç…§ã®ã“ã¨ï¼‰ã€‚ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å ´åˆã¯ã€ä»¥ä¸‹ã®é–¢æ•°ã‚’ä½¿ã£ã¦ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ä¸Šã§ã¯æ¬¡ã®é€šã‚Šã§ã™:

```bash
huggingface-cli login
```

ã©ã¡ã‚‰ã®å ´åˆã‚‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®å…¥åŠ›ã‚’æ±‚ã‚ã‚‰ã‚Œã¾ã™ãŒã€ã“ã‚Œã¯ Hub ã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã¨ãã«ä½¿ç”¨ã™ã‚‹ã‚‚ã®ã¨åŒã˜ã§ã™ã€‚ã Hubã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’ãŠæŒã¡ã§ãªã„æ–¹ã¯ã€[ã“ã¡ã‚‰](https://huggingface.co/join)ã‹ã‚‰ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã“ã‚Œã§ã€èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚ãã‚Œã§ã¯ã€ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ï¼

{#if fw === 'pt'}

`Trainer`API ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ãŸã®ã§ã‚ã‚Œã°ã€ `TrainingArguments`ã«ãŠã„ã¦`push_to_hub=True`ã¨è¨­å®šã™ã‚‹ã“ã¨ã§ã€æœ€ã‚‚ç°¡å˜ã«Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```py
from transformers import TrainingArguments

training_args = TrainingArguments(
    "bert-finetuned-mrpc", save_strategy="epoch", push_to_hub=True
)
```

`trainer.train()`ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹åº¦ã«ï¼ˆã“ã“ã§ã¯ã‚¨ãƒãƒƒã‚¯æ¯ã«ï¼‰`Trainer`ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ãƒã‚¸ãƒˆãƒªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜åå‰ã«ãªã‚Šã¾ã™ãŒï¼ˆã“ã®ä¾‹ã§ã¯`bert-finetuned-mrpc`ï¼‰ã€`hub_model_id = "a_different_name"`ã¨ã™ã‚‹ã“ã¨ã§åˆ¥ã®åå‰ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã‚ãªãŸãŒæ‰€å±ã™ã‚‹çµ„ç¹”ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€`hub_model_id = "my_organization/my_repo_name"`ã¨ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚

å­¦ç¿’ãŒçµ‚äº†ã—ãŸã‚‰ã€æœ€å¾Œã« `trainer.push_to_hub()` ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚ç‰ˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ã“ã®éš›ã€ä½¿ç”¨ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è©•ä¾¡çµæœãªã©ã€å…¨ã¦ã®é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¾ã™ï¼ä»¥ä¸‹ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹å†…å®¹ã®ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚

<div class="flex justify-center">
  <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png" alt="An example of an auto-generated model card." width="100%"/>
</div>

{:else}

ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«Kerasã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€æœ€ã‚‚ç°¡å˜ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã¯`PushToHubCallback`ã‚’`model.fit()`ã«æ¸¡ã™ã“ã¨ã§ã™ï¼š

```py
from transformers import PushToHubCallback

callback = PushToHubCallback(
    "bert-finetuned-mrpc", save_strategy="epoch", tokenizer=tokenizer
)
```

ãã—ã¦ã€`model.fit()`ã®å‘¼ã³å‡ºã—ã«`callbacks=[callback]`ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹åº¦ã«ï¼ˆã“ã“ã§ã¯ã‚¨ãƒãƒƒã‚¯æ¯ã«ï¼‰ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒã‚¸ãƒˆãƒªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨åŒã˜åå‰ã«ãªã‚Šã¾ã™ãŒï¼ˆã“ã®ä¾‹ã§ã¯`bert-finetuned-mrpc`ï¼‰ã€`hub_model_id = "a_different_name"`ã¨ã™ã‚‹ã“ã¨ã§åˆ¥ã®åå‰ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã‚ãªãŸãŒæ‰€å±ã™ã‚‹çµ„ç¹”ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€`hub_model_id = "my_organization/my_repo_name"`ã¨ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚

{/if}

ã‚ˆã‚Šä½ã„ãƒ¬ãƒ™ãƒ«ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãŠã‚ˆã³è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® `push_to_hub()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é€šã˜ã¦ã€ãƒ¢ãƒ‡ãƒ«Hubã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç›´æ¥è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ãƒªãƒã‚¸ãƒˆãƒªã®ä½œæˆã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ãƒ—ãƒƒã‚·ãƒ¥ã®ä¸¡æ–¹ã‚’è¡Œã„ã¾ã™ã€‚å¾Œè¿°ã™ã‚‹APIã¨ã¯ç•°ãªã‚Šã€æ‰‹å‹•ã§æ“ä½œã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ãã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ã¾ãšãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```
{/if}

ã“ã‚Œã‚‰ã‚’ä½¿ã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ãŸã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ãŸã‚Šã€å¾®èª¿æ•´ã—ãŸã‚Šã¨ã€å¥½ããªã“ã¨ã‚’è‡ªç”±ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚å‡ºæ¥ä¸ŠãŒã£ãŸãƒ¢ãƒ‡ãƒ«ã€é‡ã¿ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«æº€è¶³ã—ãŸã‚‰ã€`model` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ç›´æ¥åˆ©ç”¨ã§ãã‚‹`push_to_hub()`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ã§ãã¾ã™ï¼š

```py
model.push_to_hub("dummy-model")
```

ã“ã‚Œã§ã‚ãªãŸã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„ãƒªãƒã‚¸ãƒˆãƒª `dummy-model` ãŒä½œæˆã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒãã“ã«æ ¼ç´ã•ã‚Œã¾ã™ã€‚ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚‚åŒæ§˜ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```py
tokenizer.push_to_hub("dummy-model")
```

çµ„ç¹”ã«æ‰€å±ã—ã¦ã„ã‚‹å ´åˆã€`organization`å¼•æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§å½“è©²çµ„ç¹”ã®ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface")
```

ç‰¹å®šã®Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã†ã“ã¨ã‚‚ã§ãã¾ã™ï¼š

```py
tokenizer.push_to_hub("dummy-model", organization="huggingface", use_auth_token="<TOKEN>")
```

ã•ã‚ã€æ–°ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¢ãƒ‡ãƒ«Hubã§è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š*https://huggingface.co/user-or-organization/dummy-model*.

"Files and versions"ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ï¼š

{#if fw === 'pt'}
<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</div>
{:else}
<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/push_to_hub_dummy_model_tf.png" alt="Dummy model containing both the tokenizer and model files." width="80%"/>
</div>
{/if}

<Tip>

âœï¸ **ã‚„ã£ã¦ã¿ã‚ˆã†ï¼** `bert-base-cased`ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ã€`push_to_hub()`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦è‡ªåˆ†ã®ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã«ã‚ã‚‹ãƒªãƒã‚¸ãƒˆãƒªã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ãƒ¬ãƒã‚¸ãƒˆãƒªã‚’å‰Šé™¤ã™ã‚‹å‰ã«ã€ãƒ¬ãƒã‚¸ãƒˆãƒªãŒã‚ãªãŸã®ãƒšãƒ¼ã‚¸ã«æ­£ã—ãè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

</Tip>

ã“ã‚Œã¾ã§è¦‹ã¦ããŸã‚ˆã†ã«ã€`push_to_hub()`ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã„ãã¤ã‹ã®å¼•æ•°ã‚’ã¨ã‚‹ã®ã§ã€ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã‚„çµ„ç¹”ã®ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šã€åˆ¥ã®API ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ğŸ¤— Transformers documentation](https://huggingface.co/transformers/model_sharing.html)ã§ä»•æ§˜ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

ã“ã®`push_to_hub()`ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€Hugging Face Hubã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹[`huggingface_hub`](https://github.com/huggingface/huggingface_hub) Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€ğŸ¤— Transformersã‚„ã€[`allenlp`](https://github.com/allenai/allennlp)ã¨ã„ã£ãŸã€ä»–ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«çµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ç« ã§ã¯ğŸ¤— Transformersã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ãŒã€ã‚ãªãŸè‡ªèº«ã®ã‚³ãƒ¼ãƒ‰ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«çµ±åˆã™ã‚‹ã“ã¨ã¯ç°¡å˜ã§ã™ã€‚

æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»å‹•ã—ã¦ã€æ–°ã—ãä½œæˆã—ãŸãƒªãƒã‚¸ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’ã”è¦§ãã ã•ã„ï¼

## `huggingface_hub` Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹

`huggingface_hub` Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ–ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚’æä¾›ã™ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã™ã€‚ãƒãƒ–ä¸Šã®ãƒªãƒã‚¸ãƒˆãƒªã«é–¢ã™ã‚‹æƒ…å ±ã‚’å–å¾—ã—ã€ãã‚Œã‚‰ã‚’ç®¡ç†ã™ã‚‹ã‚ˆã†ãªä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯ã®ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¡ã‚½ãƒƒãƒ‰ã¨ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚ã¾ãŸã€ã“ã‚Œã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç®¡ç†ã—ã€ã‚ãªãŸã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«Hubã‚’çµ±åˆã™ã‚‹ãŸã‚ã«ã€gitã®ä¸Šã§å‹•ä½œã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªAPIã‚’æä¾›ã—ã¾ã™ã€‚

Similarly to using the `push_to_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab):

```bash
huggingface-cli login
```

The `huggingface_hub` package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:

```python no-format
from huggingface_hub import (
    # User management
    login,
    logout,
    whoami,

    # Repository creation and management
    create_repo,
    delete_repo,
    update_repo_visibility,

    # And some methods to retrieve/change information about the content
    list_models,
    list_datasets,
    list_metrics,
    list_repo_files,
    upload_file,
    delete_file,
)
```


Additionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.

The `create_repo` method can be used to create a new repository on the hub:

```py
from huggingface_hub import create_repo

create_repo("dummy-model")
```

This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:

```py
from huggingface_hub import create_repo

create_repo("dummy-model", organization="huggingface")
```

This will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.
Other arguments which may be useful are:

- `private`, in order to specify if the repository should be visible from others or not.
- `token`, if you would like to override the token stored in your cache by a given token.
- `repo_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `"dataset"` and `"space"`.

Once the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled.


## Using the web interface

The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.

To create a new repository, visit [huggingface.co/new](https://huggingface.co/new):

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png" alt="Page showcasing the model used for the creation of a new model repository." width="80%"/>
</div>

First, specify the owner of the repository: this can be either you or any of the organizations you're affiliated with. If you choose an organization, the model will be featured on the organization's page and every member of the organization will have the ability to contribute to the repository.

Next, enter your model's name. This will also be the name of the repository. Finally, you can specify whether you want your model to be public or private. Private models are hidden from public view.

After creating your model repository, you should see a page like this:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/empty_model.png" alt="An empty model page after creating a new repository." width="80%"/>
</div>

This is where your model will be hosted. To start populating it, you can add a README file directly from the web interface.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/dummy_model.png" alt="The README file showing the Markdown capabilities." width="80%"/>
</div>

The README file is in Markdown â€” feel free to go wild with it! The third part of this chapter is dedicated to building a model card. These are of prime importance in bringing value to your model, as they're where you tell others what it can do.

If you look at the "Files and versions" tab, you'll see that there aren't many files there yet â€” just the *README.md* you just created and the *.gitattributes* file that keeps track of large files.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/files.png" alt="The 'Files and versions' tab only shows the .gitattributes and README.md files." width="80%"/>
</div>

We'll take a look at how to add some new files next.

## Uploading the model files

The system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for [Git Large File Storage](https://git-lfs.github.com/)) for larger files. 

In the next section, we go over three different ways of uploading files to the Hub: through `huggingface_hub` and through git commands.

### The `upload_file` approach

Using `upload_file` does not require git and git-lfs to be installed on your system. It pushes files directly to the ğŸ¤— Hub using HTTP POST requests. A limitation of this approach is that it doesn't handle files that are larger than 5GB in size.
If your files are larger than 5GB, please follow the two other methods detailed below.

The API may be used as follows:

```py
from huggingface_hub import upload_file

upload_file(
    "<path_to_file>/config.json",
    path_in_repo="config.json",
    repo_id="<namespace>/dummy-model",
)
```

This will upload the file `config.json` available at `<path_to_file>` to the root of the repository as `config.json`, to the `dummy-model` repository.
Other arguments which may be useful are:

- `token`, if you would like to override the token stored in your cache by a given token.
- `repo_type`, if you would like to upload to a `dataset` or a `space` instead of a model. Accepted values are `"dataset"` and `"space"`.


### The `Repository` class

The `Repository` class manages a local repository in a git-like manner. It abstracts most of the pain points one may have with git to provide all features that we require. 

Using this class requires having git and git-lfs installed, so make sure you have git-lfs installed (see [here](https://git-lfs.github.com/) for installation instructions) and set up before you begin. 

In order to start playing around with the repository we have just created, we can start by initialising it into a local folder by cloning the remote repository:

```py
from huggingface_hub import Repository

repo = Repository("<path_to_dummy_folder>", clone_from="<namespace>/dummy-model")
```

This created the folder `<path_to_dummy_folder>` in our working directory. This folder only contains the `.gitattributes` file as that's the only file created when instantiating the repository through `create_repo`.

From this point on, we may leverage several of the traditional git methods:

```py
repo.git_pull()
repo.git_add()
repo.git_commit()
repo.git_push()
repo.git_tag()
```

And others! We recommend taking a look at the `Repository` documentation available [here](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub#advanced-programmatic-repository-management) for an overview of all available methods.

At present, we have a model and a tokenizer that we would like to push to the hub. We have successfully cloned the repository, we can therefore save the files within that repository.

We first make sure that our local clone is up to date by pulling the latest changes:

```py
repo.git_pull()
```

Once that is done, we save the model and tokenizer files:

```py
model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```

The `<path_to_dummy_folder>` now contains all the model and tokenizer files. We follow the usual git workflow by adding files to the staging area, committing them and pushing them to the hub:

```py
repo.git_add()
repo.git_commit("Add model and tokenizer files")
repo.git_push()
```

Congratulations! You just pushed your first files on the hub.

### The git-based approach

This is the very barebones approach to uploading files: we'll do so with git and git-lfs directly. Most of the difficulty is abstracted away by previous approaches, but there are a few caveats with the following method so we'll follow a more complex use-case.

Using this class requires having git and git-lfs installed, so make sure you have [git-lfs](https://git-lfs.github.com/) installed (see here for installation instructions) and set up before you begin. 

First start by initializing git-lfs:

```bash
git lfs install
```

```bash
Updated git hooks.
Git LFS initialized.
```

Once that's done, the first step is to clone your model repository:

```bash
git clone https://huggingface.co/<namespace>/<your-model-id>
```

My username is `lysandre` and I've used the model name `dummy`, so for me the command ends up looking like the following:

```
git clone https://huggingface.co/lysandre/dummy
```

I now have a folder named *dummy* in my working directory. I can `cd` into the folder and have a look at the contents:

```bash
cd dummy && ls
```

```bash
README.md
```

If you just created your repository using Hugging Face Hub's `create_repo` method, this folder should only contain a hidden `.gitattributes` file. If you followed the instructions in the previous section to create a repository using the web interface, the folder should contain a single *README.md* file alongside the hidden `.gitattributes` file, as shown here.

Adding a regular-sized file, such as a configuration file, a vocabulary file, or basically any file under a few megabytes, is done exactly as one would do it in any git-based system. However, bigger files must be registered through git-lfs in order to push them to *huggingface.co*. 

Let's go back to Python for a bit to generate a model and tokenizer that we'd like to commit to our dummy repository:

{#if fw === 'pt'}
```py
from transformers import AutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = AutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Do whatever with the model, train it, fine-tune it...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{:else}
```py
from transformers import TFAutoModelForMaskedLM, AutoTokenizer

checkpoint = "camembert-base"

model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Do whatever with the model, train it, fine-tune it...

model.save_pretrained("<path_to_dummy_folder>")
tokenizer.save_pretrained("<path_to_dummy_folder>")
```
{/if}

Now that we've saved some model and tokenizer artifacts, let's take another look at the *dummy* folder:

```bash
ls
```

{#if fw === 'pt'}
```bash
config.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json
```

If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*pytorch_model.bin*) is the only outlier, at more than 400 MB.

{:else}
```bash
config.json  README.md  sentencepiece.bpe.model  special_tokens_map.json  tf_model.h5  tokenizer_config.json  tokenizer.json
```

If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*t5_model.h5*) is the only outlier, at more than 400 MB.

{/if}

<Tip>
âœï¸ When creating the repository from the web interface, the *.gitattributes* file is automatically set up to consider files with certain extensions, such as *.bin* and *.h5*, as large files, and git-lfs will track them with no necessary setup on your side.
</Tip> 

We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:

```bash
git add .
```

We can then have a look at the files that are currently staged:

```bash
git status
```

{#if fw === 'pt'}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
	new file:   config.json
	new file:   pytorch_model.bin
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{:else}
```bash
On branch main
Your branch is up to date with 'origin/main'.

Changes to be committed:
  (use "git restore --staged <file>..." to unstage)
  modified:   .gitattributes
  	new file:   config.json
	new file:   sentencepiece.bpe.model
	new file:   special_tokens_map.json
	new file:   tf_model.h5
	new file:   tokenizer.json
	new file:   tokenizer_config.json
```
{/if}

Similarly, we can make sure that git-lfs is tracking the correct files by using its `status` command:

```bash
git lfs status
```

{#if fw === 'pt'}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	pytorch_model.bin (LFS: 35686c2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

We can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiece.bpe.model*, which have `LFS`. Great!

{:else}
```bash
On branch main
Objects to be pushed to origin/main:


Objects to be committed:

	config.json (Git: bc20ff2)
	sentencepiece.bpe.model (LFS: 988bc5a)
	special_tokens_map.json (Git: cb23931)
	tf_model.h5 (LFS: 86fce29)
	tokenizer.json (Git: 851ff3e)
	tokenizer_config.json (Git: f0f7783)

Objects not staged for commit:


```

We can see that all files have `Git` as a handler, except *t5_model.h5*, which has `LFS`. Great!

{/if}

Let's proceed to the final steps, committing and pushing to the *huggingface.co* remote repository:

```bash
git commit -m "First model version"
```

{#if fw === 'pt'}
```bash
[main b08aab1] First model version
 7 files changed, 29027 insertions(+)
  6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 pytorch_model.bin
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{:else}
```bash
[main b08aab1] First model version
 6 files changed, 36 insertions(+)
 create mode 100644 config.json
 create mode 100644 sentencepiece.bpe.model
 create mode 100644 special_tokens_map.json
 create mode 100644 tf_model.h5
 create mode 100644 tokenizer.json
 create mode 100644 tokenizer_config.json
```
{/if}

Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:

```bash
git push
```

```bash
Uploading LFS objects: 100% (1/1), 433 MB | 1.3 MB/s, done.
Enumerating objects: 11, done.
Counting objects: 100% (11/11), done.
Delta compression using up to 12 threads
Compressing objects: 100% (9/9), done.
Writing objects: 100% (9/9), 288.27 KiB | 6.27 MiB/s, done.
Total 9 (delta 1), reused 0 (delta 0), pack-reused 0
To https://huggingface.co/lysandre/dummy
   891b41d..b08aab1  main -> main
```

{#if fw === 'pt'}
If we take a look at the model repository when this is finished, we can see all the recently added files:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</div>

The UI allows you to explore the model files and commits and to see the diff introduced by each commit:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffs.gif" alt="The diff introduced by the recent commit." width="80%"/>
</div>
{:else}
If we take a look at the model repository when this is finished, we can see all the recently added files:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/full_model_tf.png" alt="The 'Files and versions' tab now contains all the recently uploaded files." width="80%"/>
</div>

The UI allows you to explore the model files and commits and to see the diff introduced by each commit:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/diffstf.gif" alt="The diff introduced by the recent commit." width="80%"/>
</div>
{/if}
