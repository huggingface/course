# ç°¡ä»‹

åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)ä½ æ‡‰è©²å·²ç¶“åˆæ­¥é‹ç”¨äº†ğŸ¤— Datasets å‡½å¼åº«ä¸¦ç­è§£åˆ°ç´°èª¿æ¨¡å‹æ™‚çš„ä¸‰å¤§æ­¥é©Ÿï¼š

In [Chapter 3](/course/chapter3) you got your first taste of the ğŸ¤— Datasets library and saw that there were three main steps when it came to fine-tuning a model:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.

But this is just scratching the surface of what ğŸ¤— Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:

* What do you do when your dataset is not on the Hub?
* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)
* What do you do when your dataset is huge and will melt your laptop's RAM?
* What the heck are "memory mapping" and Apache Arrow?
* How can you create your own dataset and push it to the Hub?

The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!