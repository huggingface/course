<FrameworkSwitchCourse {fw} />

# ç¿»è­¯

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section4_tf.ipynb"},
]} />

{/if}

ç¾åœ¨è®“æˆ‘å€‘æ·±å…¥ç ”ç©¶ç¿»è­¯ã€‚é€™æ˜¯å¦ä¸€å€‹[sequence-to-sequence ä»»å‹™](/course/chapter1/7)ï¼Œé€™æ„å‘³è‘—é€™æ˜¯ä¸€å€‹å¯ä»¥è¡¨è¿°ç‚ºå¾ä¸€å€‹åºåˆ—åˆ°å¦ä¸€å€‹åºåˆ—çš„å•é¡Œã€‚å¾é€™å€‹æ„ç¾©ä¸Šèªªï¼Œé€™å€‹å•é¡Œéå¸¸é¡ä¼¼[æ–‡æœ¬æ‘˜è¦](/course/chapter7/6)ï¼Œä¸¦ä¸”æ‚¨å¯ä»¥å°‡æˆ‘å€‘å°‡åœ¨æ­¤è™•å­¸ç¿’åˆ°çš„ä¸€äº›å…§å®¹é·ç§»åˆ°å…¶ä»–çš„åºåˆ—åˆ°åºåˆ—å•é¡Œï¼Œä¾‹å¦‚ï¼š

- **é¢¨æ ¼é·ç§»** : å‰µå»ºä¸€å€‹æ¨¡å‹å°‡æŸç¨®é¢¨æ ¼é·ç§»åˆ°ä¸€æ®µæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ­£å¼çš„é¢¨æ ¼é·ç§»åˆ°ä¼‘é–’çš„é¢¨æ ¼æˆ–èå£«æ¯”äºè‹±èªåˆ°ç¾ä»£è‹±èªï¼‰
- **ç”Ÿæˆå•é¡Œçš„å›ç­”** ï¼šå‰µå»ºä¸€å€‹æ¨¡å‹ï¼Œåœ¨çµ¦å®šä¸Šä¸‹æ–‡çš„æƒ…æ³ä¸‹ç”Ÿæˆå•é¡Œçš„ç­”æ¡ˆ

<Youtube id="1JvfrvZgi6c"/>

å¦‚æœæ‚¨æœ‰è¶³å¤ å¤§çš„å…©ç¨®ï¼ˆæˆ–æ›´å¤šï¼‰èªè¨€çš„æ–‡æœ¬èªæ–™åº«ï¼Œæ‚¨å¯ä»¥å¾é ­é–‹å§‹è¨“ç·´ä¸€å€‹æ–°çš„ç¿»è­¯æ¨¡å‹ï¼Œå°±åƒæˆ‘å€‘åœ¨[å› æœèªè¨€å»ºæ¨¡](/course/chapter7/6)éƒ¨åˆ†ä¸­æ‰€åšçš„é‚£æ¨£ã€‚ç„¶è€Œï¼Œå¾®èª¿ç¾æœ‰çš„ç¿»è­¯æ¨¡å‹æœƒæ›´å¿«ï¼Œç„¡è«–æ˜¯å¾åƒ mT5 æˆ– mBART é€™æ¨£çš„å¤šèªè¨€æ¨¡å‹å¾®èª¿åˆ°ç‰¹å®šçš„èªè¨€å°ï¼Œæˆ–è€…æ˜¯å°ˆé–€ç”¨æ–¼å¾ä¸€ç¨®èªè¨€ç¿»è­¯æˆå¦ä¸€ç¨®èªè¨€çš„æ¨¡å‹ã€‚

åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡å°[KDE4 æ•¸æ“šé›†](https://huggingface.co/datasets/kde4)ä¸Šçš„Marianæ¨¡å‹é€²è¡Œå¾®èª¿ï¼Œè©²æ¨¡å‹ç¶“éäº†å¾è‹±èªåˆ°æ³•èªçš„ç¿»è­¯é è¨“ç·´(å› ç‚ºå¾ˆå¤šâ€œ Hugging Faceâ€çš„å“¡å·¥æœƒèªªé€™å…©ç¨®èªè¨€)ï¼Œå®ƒæ˜¯[KDEæ‡‰ç”¨ç¨‹åº](https://apps.kde.org/)æœ¬åœ°åŒ–æ–‡ä»¶çš„æ•¸æ“šé›†ã€‚æˆ‘å€‘å°‡ä½¿ç”¨çš„æ¨¡å‹å·²ç¶“åœ¨å¾[Opus æ•¸æ“šé›†](https://opus.nlpl.eu/)(å¯¦éš›ä¸ŠåŒ…å«KDE4æ•¸æ“šé›†)ä¸­æå–çš„æ³•èªå’Œè‹±èªæ–‡æœ¬çš„å¤§å‹èªæ–™åº«ä¸Šé€²è¡Œäº†é å…ˆè¨“ç·´ã€‚ä½†æ˜¯ï¼Œå³ä½¿æˆ‘å€‘ä½¿ç”¨çš„é è¨“ç·´æ¨¡å‹åœ¨å…¶é è¨“ç·´æœŸé–“ä½¿ç”¨äº†é€™éƒ¨åˆ†æ•¸æ“šé›†ï¼Œæˆ‘å€‘ä¹Ÿæœƒçœ‹åˆ°ï¼Œç¶“éå¾®èª¿å¾Œï¼Œæˆ‘å€‘å¯ä»¥å¾—åˆ°ä¸€å€‹æ›´å¥½çš„ç‰ˆæœ¬ã€‚

å®Œæˆå¾Œï¼Œæˆ‘å€‘å°‡æ“æœ‰ä¸€å€‹æ¨¡å‹ï¼Œå¯ä»¥é€²è¡Œé€™æ¨£çš„ç¿»è­¯ï¼š

<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-marian-finetuned-kde4-en-to-fr-darkmode.hf.space" frameBorder="0" height="350" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

èˆ‡å‰é¢çš„éƒ¨åˆ†ä¸€æ¨£ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼æ‰¾åˆ°æˆ‘å€‘å°‡è¨“ç·´ä¸¦ä¸Šå‚³åˆ° Hub çš„å¯¦éš›æ¨¡å‹ï¼Œä¸¦[åœ¨é€™è£¡](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.)æŸ¥çœ‹æ¨¡å‹è¼¸å‡ºçš„çµæœ

## æº–å‚™æ•¸æ“š

ç‚ºäº†å¾é ­é–‹å§‹å¾®èª¿æˆ–è¨“ç·´ç¿»è­¯æ¨¡å‹ï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹é©åˆè©²ä»»å‹™çš„æ•¸æ“šé›†ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨[KDE4 æ•¸æ“šé›†](https://huggingface.co/datasets/kde4)åœ¨æœ¬ç¯€ä¸­ï¼Œä½†æ‚¨å¯ä»¥å¾ˆå®¹æ˜“åœ°èª¿æ•´ä»£ç¢¼ä»¥ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•¸æ“šï¼Œåªè¦æ‚¨æœ‰è¦äº’è­¯çš„å…©ç¨®èªè¨€çš„å¥å­å°ã€‚å¦‚æœæ‚¨éœ€è¦è¤‡ç¿’å¦‚ä½•å°‡è‡ªå®šç¾©æ•¸æ“šåŠ è¼‰åˆ° **Dataset** å¯ä»¥è¤‡ç¿’ä¸€ä¸‹[ç¬¬äº”ç« ](/course/chapter5).

### KDE4 æ•¸æ“šé›†

åƒå¾€å¸¸ä¸€æ¨£ï¼Œæˆ‘å€‘ä½¿ç”¨ **load_dataset()** å‡½æ•¸ï¼š

```py
from datasets import load_dataset, load_metric

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒçš„èªè¨€å°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒå€‘çš„ä»£ç¢¼ä¾†æŒ‡å®šå®ƒå€‘ã€‚è©²æ•¸æ“šé›†å…±æœ‰ 92 ç¨®èªè¨€å¯ç”¨ï¼›æ‚¨å¯ä»¥é€šé[æ•¸æ“šé›†å¡ç‰‡](https://huggingface.co/datasets/kde4)å±•é–‹å…¶ä¸Šçš„èªè¨€æ¨™ç±¤ä¾†æŸ¥çœ‹å®ƒå€‘.

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

æˆ‘å€‘ä¾†çœ‹çœ‹æ•¸æ“šé›†ï¼š

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

æˆ‘å€‘æœ‰ 210,173 å°å¥å­ï¼Œä½†åœ¨ä¸€æ¬¡è¨“ç·´éç¨‹ä¸­ï¼Œæˆ‘å€‘éœ€è¦å‰µå»ºè‡ªå·±çš„é©—è­‰é›†ã€‚æ­£å¦‚æˆ‘å€‘åœ¨[ç¬¬äº”ç« ](/course/chapter5)å­¸çš„çš„é‚£æ¨£, **Dataset** æœ‰ä¸€å€‹ **train_test_split()** æ–¹æ³•,å¯ä»¥å¹«æˆ‘å€‘æ‹†åˆ†æ•¸æ“šé›†ã€‚æˆ‘å€‘å°‡è¨­å®šå›ºå®šçš„éš¨æ©Ÿæ•¸ç¨®å­ï¼š

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

æˆ‘å€‘å¯ä»¥å°‡ **test** çš„éµé‡å‘½åç‚º **validation** åƒé€™æ¨£ï¼š

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ç¾åœ¨è®“æˆ‘å€‘çœ‹ä¸€ä¸‹æ•¸æ“šé›†çš„ä¸€å€‹å…ƒç´ ï¼š

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

æˆ‘å€‘å¾—åˆ°ä¸€å€‹å­—å…¸ï¼Œå…¶ä¸­åŒ…å«æˆ‘å€‘è«‹æ±‚çš„å…©ç¨®èªè¨€çš„å…©å€‹å¥å­ã€‚é€™å€‹å……æ»¿æŠ€è¡“è¨ˆç®—æ©Ÿç§‘å­¸è¡“èªçš„æ•¸æ“šé›†çš„ä¸€å€‹ç‰¹æ®Šä¹‹è™•åœ¨æ–¼å®ƒå€‘éƒ½å®Œå…¨ç”¨æ³•èªç¿»è­¯ã€‚ç„¶è€Œï¼Œæ³•åœ‹å·¥ç¨‹å¸«é€šå¸¸å¾ˆæ‡¶æƒ°ï¼Œåœ¨äº¤è«‡æ™‚ï¼Œå¤§å¤šæ•¸è¨ˆç®—æ©Ÿç§‘å­¸å°ˆç”¨è©å½™éƒ½ç”¨è‹±èªè¡¨è¿°ã€‚ä¾‹å¦‚ï¼Œâ€œthreadsâ€é€™å€‹è©å¾ˆå¯èƒ½å‡ºç¾åœ¨æ³•èªå¥å­ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æŠ€è¡“å°è©±ä¸­ï¼›ä½†åœ¨é€™å€‹æ•¸æ“šé›†ä¸­ï¼Œå®ƒè¢«ç¿»è­¯æˆæ›´æ­£ç¢ºçš„â€œfils de Discussionâ€ã€‚æˆ‘å€‘ä½¿ç”¨çš„é è¨“ç·´æ¨¡å‹å·²ç¶“åœ¨ä¸€å€‹æ›´å¤§çš„æ³•èªå’Œè‹±èªå¥å­èªæ–™åº«ä¸Šé€²è¡Œäº†é è¨“ç·´ï¼Œæ¡ç”¨äº†æ›´ç°¡å–®çš„é¸æ“‡ï¼Œå³ä¿ç•™å–®è©çš„åŸæ¨£:

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

é€™ç¨®æƒ…æ³çš„å¦ä¸€å€‹ä¾‹å­æ˜¯â€œpluginâ€é€™å€‹è©ï¼Œå®ƒä¸æ˜¯æ­£å¼çš„æ³•èªè©ï¼Œä½†å¤§å¤šæ•¸æ¯èªäººå£«éƒ½èƒ½ç†è§£ï¼Œä¹Ÿä¸æœƒè²»å¿ƒå»ç¿»è­¯ã€‚
åœ¨ KDE4 æ•¸æ“šé›†ä¸­ï¼Œé€™å€‹è©åœ¨æ³•èªä¸­è¢«ç¿»è­¯æˆæ›´æ­£å¼çš„â€œmodule dâ€™extensionâ€ï¼š
```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ç„¶è€Œï¼Œæˆ‘å€‘çš„é è¨“ç·´æ¨¡å‹å …æŒä½¿ç”¨ç°¡ç·´è€Œç†Ÿæ‚‰çš„è‹±æ–‡å–®è©ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

çœ‹çœ‹æˆ‘å€‘çš„å¾®èª¿æ¨¡å‹æ˜¯å¦èƒ½è­˜åˆ¥æ•¸æ“šé›†çš„é€™äº›ç‰¹æ®Šæ€§ã€‚ï¼ˆåŠ‡é€è­¦å‘Šï¼šå®ƒæœƒï¼‰ã€‚

<Youtube id="0Oxphw4Q9fo"/>

<Tip>

âœï¸ **è¼ªåˆ°ä½ äº†ï¼** å¦ä¸€å€‹åœ¨æ³•èªä¸­ç¶“å¸¸ä½¿ç”¨çš„è‹±èªå–®è©æ˜¯â€œemailâ€ã€‚åœ¨è¨“ç·´æ•¸æ“šé›†ä¸­æ‰¾åˆ°ä½¿ç”¨é€™å€‹è©çš„ç¬¬ä¸€å€‹æ¨£æœ¬ã€‚å®ƒæ˜¯å¦‚ä½•ç¿»è­¯çš„ï¼Ÿé è¨“ç·´æ¨¡å‹å¦‚ä½•ç¿»è­¯åŒä¸€å€‹è‹±æ–‡å¥å­ï¼Ÿ

</Tip>

### è™•ç†æ•¸æ“š

<Youtube id="XAR8jnZZuUs"/>

æ‚¨ç¾åœ¨æ‡‰è©²çŸ¥é“æˆ‘å€‘çš„ä¸‹ä¸€æ­¥è©²åšäº›ä»€éº¼äº†ï¼šæ‰€æœ‰æ–‡æœ¬éƒ½éœ€è¦è½‰æ›ç‚ºtoken IDï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤ ç†è§£å®ƒå€‘ã€‚å°æ–¼é€™å€‹ä»»å‹™ï¼Œæˆ‘å€‘éœ€è¦åŒæ™‚æ¨™è¨˜è¼¸å…¥å’Œç›®æ¨™ã€‚æˆ‘å€‘çš„é¦–è¦ä»»å‹™æ˜¯å‰µå»ºæˆ‘å€‘çš„ **tokenizer** å°è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ Marian è‹±èªåˆ°æ³•èªçš„é è¨“ç·´æ¨¡å‹ã€‚å¦‚æœæ‚¨ä½¿ç”¨å¦ä¸€å°èªè¨€å˜—è©¦æ­¤ä»£ç¢¼ï¼Œè«‹ç¢ºä¿èª¿æ•´æ¨¡å‹Checkpointã€‚[Helsinki-NLP](https://huggingface.co/Helsinki-NLP)çµ„ç¹”æä¾›äº†å¤šç¨®èªè¨€çš„ä¸€åƒå¤šç¨®æ¨¡å‹ã€‚

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="tf")
```

æ‚¨å¯ä»¥å°‡ **model_checkpoint** æ›´æ›ç‚º[Hub](https://huggingface.co/models)ä¸Šä½ å–œæ­¡çš„ä»»ä½•å…¶ä»–å‹è™Ÿï¼Œæˆ–æœ¬åœ°ä¿å­˜çš„é è¨“ç·´æ¨¡å‹å’Œæ¨™è¨˜å™¨ã€‚

<Tip>

ğŸ’¡ å¦‚æœæ­£åœ¨ä½¿ç”¨martã€mBART-50æˆ–M2 M100ç­‰å¤šèªè¨€æ¨™è¨˜å™¨ï¼Œå‰‡éœ€è¦åœ¨tokenizerä¸­è¨­ç½®tokenizer.src_langå’Œtokenizer.tgt_langç‚ºæ­£ç¢ºçš„è¼¸å…¥å’Œç›®æ¨™çš„èªè¨€ä»£ç¢¼ã€‚

</Tip>

æˆ‘å€‘çš„æ•¸æ“šæº–å‚™éå¸¸ç°¡å–®ã€‚ åªéœ€è¦è¨˜ä½ä¸€ä»¶äº‹ï¼šæ‚¨ç…§å¸¸è™•ç†è¼¸å…¥ï¼Œä½†å°æ–¼é€™æ¬¡çš„è¼¸å‡ºç›®æ¨™ï¼Œæ‚¨éœ€è¦å°‡æ¨™è¨˜å™¨åŒ…è£åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨â€œas_target_tokenizer()â€ä¸­ã€‚

Python ä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¼•å…¥äº† **with** èªå¥ï¼Œç•¶æ‚¨æœ‰å…©å€‹ç›¸é—œçš„æ“ä½œè¦æˆå°åŸ·è¡Œæ™‚å¾ˆæœ‰ç”¨ã€‚æœ€å¸¸è¦‹çš„ä¾‹å­æ˜¯ç•¶æ‚¨å¯«å…¥æˆ–è®€å–æ–‡ä»¶æ™‚ï¼Œä¸‹é¢æ˜¯ä¸€å€‹ä¾‹å­ï¼š

```
with open(file_path) as f:
    content = f.read()
```

é€™è£¡æˆå°åŸ·è¡Œçš„å…©å€‹ç›¸é—œæ“ä½œæ˜¯æ‰“é–‹å’Œé—œé–‰æ–‡ä»¶çš„æ“ä½œã€‚æ‰“é–‹çš„æ–‡ä»¶få°æ‡‰çš„å°è±¡åªåœ¨withä¸‹çš„ç¸®é€²å¡Šå…§æœ‰æ•ˆï¼›åœ¨è©²å¡Šä¹‹å‰æ‰“é–‹ï¼Œåœ¨è©²å¡Šçš„æœ«å°¾é—œé–‰ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ as_target_tokenizer() å°‡åœ¨åŸ·è¡Œç¸®é€²å¡Šä¹‹å‰å°‡æ¨™è¨˜å™¨è¨­ç½®ç‚ºè¼¸å‡ºèªè¨€ï¼ˆæ­¤è™•ç‚ºæ³•èªï¼‰ï¼Œç„¶å¾Œå°‡å…¶è¨­ç½®å›è¼¸å…¥èªè¨€ï¼ˆæ­¤è™•ç‚ºè‹±èªï¼‰ã€‚

å› æ­¤ï¼Œé è™•ç†ä¸€å€‹æ¨£æœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence)
with tokenizer.as_target_tokenizer():
    targets = tokenizer(fr_sentence)
```

å¦‚æœæˆ‘å€‘å¿˜è¨˜åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­æ¨™è¨˜ç›®æ¨™ï¼Œå®ƒå€‘å°‡è¢«è¼¸å…¥æ¨™è¨˜å™¨æ¨™è¨˜ï¼Œåœ¨Marianæ¨¡å‹çš„æƒ…æ³ä¸‹ï¼Œæœƒå°è‡´è¼¸å‡ºçš„ç•°å¸¸ï¼š

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(targets["input_ids"]))
```

```python out
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

æ­£å¦‚æˆ‘å€‘æ‰€çœ‹åˆ°çš„ï¼Œä½¿ç”¨è‹±èªæ¨™è¨˜å™¨ä¾†é è™•ç†æ³•èªå¥å­æœƒç”¢ç”Ÿæ›´å¤šçš„æ¨™è¨˜ï¼Œå› ç‚ºæ¨™è¨˜å™¨ä¸çŸ¥é“ä»»ä½•æ³•èªå–®è©(é™¤äº†é‚£äº›ä¹Ÿå‡ºç¾åœ¨è‹±èªèªè¨€ä¸­çš„å–®è©ï¼Œæ¯”å¦‚â€œdiscussionâ€)ã€‚

`inputs` å’Œ `targets` éƒ½æ˜¯å¸¶æœ‰æˆ‘å€‘å¸¸ç”¨éµï¼ˆè¼¸å…¥ IDã€æ³¨æ„æ©ç¢¼ç­‰ï¼‰çš„å­—å…¸ï¼Œæ‰€ä»¥æœ€å¾Œä¸€æ­¥æ˜¯åœ¨è¼¸å…¥ä¸­è¨­ç½®ä¸€å€‹ `"labels"` éµã€‚ æˆ‘å€‘åœ¨æ•¸æ“šé›†çš„é è™•ç†å‡½æ•¸ä¸­åŸ·è¡Œæ­¤æ“ä½œï¼š

```python
max_input_length = 128
max_target_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Set up the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

è«‹æ³¨æ„ï¼Œæˆ‘å€‘ç‚ºè¼¸å…¥å’Œè¼¸å‡ºè¨­ç½®äº†ç›¸åŒçš„æœ€å¤§é•·åº¦ã€‚ç”±æ–¼æˆ‘å€‘è™•ç†çš„æ–‡æœ¬çœ‹èµ·ä¾†å¾ˆçŸ­ï¼Œæˆ‘å€‘ä½¿ç”¨ 128ã€‚

<Tip>

ğŸ’¡å¦‚æœä½ ä½¿ç”¨çš„æ˜¯T5æ¨¡å‹(æ›´å…·é«”åœ°èªªï¼Œæ˜¯T5 -xxxæª¢æŸ¥é»ä¹‹ä¸€)ï¼Œæ¨¡å‹å°‡éœ€è¦æ–‡æœ¬è¼¸å…¥æœ‰ä¸€å€‹å‰ç¶´ä¾†è¡¨ç¤ºæ­£åœ¨é€²è¡Œçš„ä»»å‹™ï¼Œä¾‹å¦‚å¾è‹±èªåˆ°æ³•èªçš„ç¿»è­¯

</Tip>

<Tip warning={true}>

âš ï¸ æˆ‘å€‘ä¸é—œæ³¨ç›®æ¨™çš„æ³¨æ„åŠ›æ©ç¢¼ï¼Œå› ç‚ºæ¨¡å‹ä¸æœƒéœ€è¦å®ƒã€‚ç›¸åï¼Œå°æ‡‰æ–¼å¡«å……æ¨™è¨˜çš„æ¨™ç±¤æ‡‰è¨­ç½®ç‚º-100ï¼Œä»¥ä¾¿åœ¨lossè¨ˆç®—ä¸­å¿½ç•¥å®ƒå€‘ã€‚é€™å°‡åœ¨ç¨å¾Œç”±æˆ‘å€‘çš„æ•¸æ“šæ•´ç†å™¨å®Œæˆï¼Œå› ç‚ºæˆ‘å€‘æ­£åœ¨æ‡‰ç”¨å‹•æ…‹å¡«å……ï¼Œä½†æ˜¯å¦‚æœæ‚¨åœ¨æ­¤è™•ä½¿ç”¨å¡«å……ï¼Œæ‚¨æ‡‰è©²èª¿æ•´é è™•ç†å‡½æ•¸ä»¥å°‡èˆ‡å¡«å……æ¨™è¨˜å°æ‡‰çš„æ‰€æœ‰æ¨™ç±¤è¨­ç½®ç‚º -100ã€‚

</Tip>

æˆ‘å€‘ç¾åœ¨å¯ä»¥å°æ•¸æ“šé›†çš„æ‰€æœ‰æ•¸æ“šä¸€æ¬¡æ€§æ‡‰ç”¨è©²é è™•ç†ï¼š

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ç¾åœ¨æ•¸æ“šå·²ç¶“éé è™•ç†ï¼Œæˆ‘å€‘æº–å‚™å¥½å¾®èª¿æˆ‘å€‘çš„é è¨“ç·´æ¨¡å‹ï¼

{#if fw === 'pt'}

## ä½¿ç”¨ Trainer API å¾®èª¿æ¨¡å‹

ä½¿ç”¨ `Trainer` çš„å¯¦éš›ä»£ç¢¼å°‡èˆ‡ä»¥å‰ç›¸åŒï¼Œåªæ˜¯ç¨ä½œæ”¹å‹•ï¼šæˆ‘å€‘åœ¨é€™è£¡ä½¿ç”¨ [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer)ï¼Œ å®ƒæ˜¯ `Trainer` çš„å­é¡ï¼Œå®ƒå¯ä»¥æ­£ç¢ºè™•ç†é€™ç¨®åºåˆ—åˆ°åºåˆ—çš„è©•ä¼°ï¼Œä¸¦ä½¿ç”¨ `generate()` æ–¹æ³•ä¾†é æ¸¬è¼¸å…¥çš„è¼¸å‡ºã€‚ ç•¶æˆ‘å€‘è¨è«–è©•ä¼°æŒ‡æ¨™æ™‚ï¼Œæˆ‘å€‘å°‡æ›´è©³ç´°åœ°æ¢è¨é€™ä¸€é»ã€‚

é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹å¯¦éš›çš„æ¨¡å‹ä¾†é€²è¡Œå¾®èª¿ã€‚ æˆ‘å€‘å°‡ä½¿ç”¨é€šå¸¸çš„ `AutoModel` APIï¼š

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ä½¿ç”¨ Keras å¾®èª¿æ¨¡å‹

é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹å¯¦éš›çš„æ¨¡å‹ä¾†é€²è¡Œå¾®èª¿ã€‚ æˆ‘å€‘å°‡ä½¿ç”¨é€šå¸¸çš„ `AutoModel` APIï¼š

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<Tip warning={false}>

ğŸ’¡ `Helsinki-NLP/opus-mt-en-fr` checkpointåªæœ‰ PyTorch çš„æ¬Šé‡ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨`from_pretrained()`æ–¹æ³•åŠ è¼‰æ¨¡å‹æ™‚ä¸æœƒä½¿ç”¨ `from_pt=True` åƒæ•¸ã€‚ ç•¶æ‚¨æŒ‡å®š`from_pt=True`ï¼Œåº«æœƒè‡ªå‹•ä¸‹è¼‰ä¸¦è½‰æ›PyTorch ç‚ºæ‚¨æä¾›æ¬Šé‡ã€‚ å¦‚æ‚¨æ‰€è¦‹ï¼Œä½¿ç”¨ğŸ¤—transormer åœ¨å…©è€…ä¹‹é–“åˆ‡æ›éå¸¸ç°¡å–®

</Tip>

{/if}

è«‹æ³¨æ„ï¼Œé€™æ¬¡æˆ‘å€‘ä½¿ç”¨çš„æ˜¯åœ¨ç¿»è­¯ä»»å‹™ä¸Šè¨“ç·´éçš„æ¨¡å‹ï¼Œä¸¦ä¸”å¯¦éš›ä¸Šå·²ç¶“å¯ä»¥ä½¿ç”¨ï¼Œå› æ­¤æ²’æœ‰é—œæ–¼ä¸Ÿå¤±æ¬Šé‡æˆ–æ–°åˆå§‹åŒ–æ¬Šé‡çš„è­¦å‘Šã€‚

### æ•¸æ“šæ•´ç†

æˆ‘å€‘éœ€è¦ä¸€å€‹æ•¸æ“šæ•´ç†å™¨ä¾†è™•ç†å‹•æ…‹æ‰¹è™•ç†çš„å¡«å……ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘å€‘ä¸èƒ½åƒ[ç¬¬3ç« ](/course/chapter3)é‚£æ¨£ä½¿ç”¨å¸¶å¡«å……çš„**DataCollatorWithPadding**ï¼Œå› ç‚ºå®ƒåªå¡«å……è¼¸å…¥ï¼ˆè¼¸å…¥IDã€æ³¨æ„æ©ç¢¼å’Œä»¤ç‰Œé¡å‹IDï¼‰ã€‚æˆ‘å€‘çš„æ¨™ç±¤ä¹Ÿæ‡‰è©²å¡«å……åˆ°æ¨™ç±¤ä¸­é‡åˆ°çš„æœ€å¤§é•·åº¦ã€‚è€Œä¸”ï¼Œå¦‚å‰æ‰€è¿°ï¼Œç”¨æ–¼å¡«å……æ¨™ç±¤çš„å¡«å……å€¼æ‡‰ç‚º-100ï¼Œè€Œä¸æ˜¯æ¨™è¨˜å™¨çš„å¡«å……æ¨™è¨˜ï¼Œä»¥ç¢ºä¿åœ¨æå¤±è¨ˆç®—ä¸­å¿½ç•¥é€™äº›å¡«å……å€¼ã€‚

é€™ä¸€åˆ‡éƒ½å¯ä»¥ç”± [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) å®Œæˆã€‚ èˆ‡ `DataCollatorWithPadding` ä¸€æ¨£ï¼Œå®ƒæ¡ç”¨ç”¨æ–¼é è™•ç†è¼¸å…¥çš„`tokenizer`ï¼Œä½†å®ƒä¹Ÿæ¡ç”¨`model`ã€‚ é€™æ˜¯å› ç‚ºæ•¸æ“šæ•´ç†å™¨é‚„å°‡è² è²¬æº–å‚™è§£ç¢¼å™¨è¼¸å…¥ IDï¼Œå®ƒå€‘æ˜¯æ¨™ç±¤åç§»ä¹‹å¾Œå½¢æˆçš„ï¼Œé–‹é ­å¸¶æœ‰ç‰¹æ®Šæ¨™è¨˜ã€‚ ç”±æ–¼ä¸åŒæ¶æ§‹çš„é€™ç¨®è½‰è®Šç•¥æœ‰ä¸åŒï¼Œå› æ­¤â€œDataCollatorForSeq2Seqâ€éœ€è¦çŸ¥é“â€œæ¨¡å‹â€å°è±¡ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

ç‚ºäº†åœ¨å¹¾å€‹æ¨£æœ¬ä¸Šé€²è¡Œæ¸¬è©¦ï¼Œæˆ‘å€‘åªéœ€åœ¨æˆ‘å€‘æ¨™è¨˜åŒ–è¨“ç·´é›†ä¸­çš„éƒ¨åˆ†æ•¸æ“šä¸Šèª¿ç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

æˆ‘å€‘å¯ä»¥æª¢æŸ¥æˆ‘å€‘çš„æ¨™ç±¤æ˜¯å¦å·²ä½¿ç”¨ **-100** å¡«å……åˆ°æ‰¹æ¬¡çš„æœ€å¤§é•·åº¦ï¼š

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

æˆ‘å€‘é‚„å¯ä»¥æŸ¥çœ‹è§£ç¢¼å™¨è¼¸å…¥ IDï¼Œçœ‹çœ‹å®ƒå€‘æ˜¯æ¨™ç±¤çš„åç§»å½¢æˆçš„ç‰ˆæœ¬ï¼š

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ä»¥ä¸‹æ˜¯æˆ‘å€‘æ•¸æ“šé›†ä¸­ç¬¬ä¸€å€‹å’Œç¬¬äºŒå€‹å…ƒç´ çš„æ¨™ç±¤ï¼š

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

æˆ‘å€‘å°‡æŠŠé€™å€‹ `data_collator` å‚³éçµ¦ `Seq2SeqTrainer`ã€‚ æ¥ä¸‹ä¾†ï¼Œè®“æˆ‘å€‘çœ‹ä¸€ä¸‹è©•ä¼°æŒ‡æ¨™ã€‚

{:else}

æˆ‘å€‘ç¾åœ¨å¯ä»¥ä½¿ç”¨ `data_collator` å°‡æˆ‘å€‘çš„æ¯å€‹æ•¸æ“šé›†è½‰æ›ç‚º `tf.data.Dataset`ï¼Œæº–å‚™å¥½é€²è¡Œè¨“ç·´ï¼š

```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### è©•ä¼°æŒ‡æ¨™

<Youtube id="M05L1DhFqcw"/>

{#if fw === 'pt'}

`Seq2SeqTrainer` æ·»åŠ åˆ°å…¶è¶…é¡ `Trainer` çš„åŠŸèƒ½æ˜¯åœ¨è©•ä¼°æˆ–é æ¸¬æœŸé–“ä½¿ç”¨ `generate()` æ–¹æ³•çš„èƒ½åŠ›ã€‚ åœ¨è¨“ç·´æœŸé–“ï¼Œæ¨¡å‹å°‡ä½¿ç”¨å¸¶æœ‰æ³¨æ„æ©ç¢¼çš„â€œdecoder_input_idsâ€ï¼Œä»¥ç¢ºä¿å®ƒä¸ä½¿ç”¨é æ¸¬çš„æ¨™è¨˜ä¹‹å¾Œçš„æ¨™è¨˜ï¼Œä»¥åŠ å¿«è¨“ç·´é€Ÿåº¦ã€‚ åœ¨æ¨ç†éç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡ç„¡æ³•ä½¿ç”¨é æ¸¬çš„æ¨™è¨˜ä¹‹å¾Œçš„æ¨™è¨˜ï¼Œå› ç‚ºæˆ‘å€‘æ²’æœ‰æ¨™ç±¤ï¼Œå› æ­¤ä½¿ç”¨ç›¸åŒçš„è¨­ç½®ä½¿ç”¨å¸¶æœ‰æ³¨æ„æ©ç¢¼çš„â€œdecoder_input_idsâ€ï¼Œè©•ä¼°æˆ‘å€‘çš„æ¨¡å‹æ˜¯å€‹å¥½ä¸»æ„ã€‚

æ­£å¦‚æˆ‘å€‘åœ¨[ç¬¬ä¸€ç« ](/course/chapter1/6)çœ‹åˆ°çš„ï¼Œè§£ç¢¼å™¨é€šéä¸€å€‹ä¸€å€‹åœ°é æ¸¬æ¨™è¨˜ä¾†åŸ·è¡Œæ¨ç†â€”â€”é€™æ˜¯ğŸ¤— Transformers åœ¨å¹•å¾Œé€šé **generate()** æ–¹æ³•å¯¦ç¾çš„ã€‚å¦‚æœæˆ‘å€‘è¨­ç½® predict_with_generate=Trueï¼ŒSeq2 Seq Trainer å°‡å…è¨±æˆ‘å€‘ä½¿ç”¨è©²æ–¹æ³•é€²è¡Œè©•ä¼°ã€‚


{/if}

ç”¨æ–¼ç¿»è­¯çš„å‚³çµ±æŒ‡æ¨™æ˜¯[BLEU åˆ†æ•¸](https://en.wikipedia.org/wiki/BLEU), ç”±Kishore Papineniç­‰äººåœ¨[2002å¹´çš„ä¸€ç¯‡æ–‡ç« ](https://aclanthology.org/P02-1040.pdf)ä¸­å¼•å…¥ã€‚BLEU åˆ†æ•¸è©•ä¼°ç¿»è­¯èˆ‡å…¶æ¨™ç±¤çš„æ¥è¿‘ç¨‹åº¦ã€‚å®ƒä¸è¡¡é‡æ¨¡å‹ç”Ÿæˆè¼¸å‡ºçš„å¯æ‡‚åº¦æˆ–èªæ³•æ­£ç¢ºæ€§ï¼Œè€Œæ˜¯ä½¿ç”¨çµ±è¨ˆè¦å‰‡ä¾†ç¢ºä¿ç”Ÿæˆè¼¸å‡ºä¸­çš„æ‰€æœ‰å–®è©ä¹Ÿå‡ºç¾åœ¨ç›®æ¨™ä¸­ã€‚æ­¤å¤–ï¼Œå¦‚æœç›¸åŒå–®è©åœ¨ç›®æ¨™ä¸­æ²’æœ‰é‡è¤‡ï¼Œå‰‡æœ‰è¦å‰‡æ‡²ç½°ç›¸åŒå–®è©çš„é‡è¤‡ï¼ˆä»¥é¿å…æ¨¡å‹è¼¸å‡ºé¡ä¼¼ **the the the the the**çš„å¥å­ ) ä¸¦è¼¸å‡ºæ¯”ç›®æ¨™ä¸­çŸ­çš„å¥å­ï¼ˆä»¥é¿å…æ¨¡å‹è¼¸å‡ºåƒ **the** é€™æ¨£çš„å¥å­ï¼‰ã€‚

BLEU çš„ä¸€å€‹ç¼ºé»æ˜¯å®ƒéœ€è¦æ–‡æœ¬å·²ç¶“è¢«åˆ†è©ï¼Œé€™ä½¿å¾—æ¯”è¼ƒä½¿ç”¨ä¸åŒæ¨™è¨˜å™¨çš„æ¨¡å‹ä¹‹é–“çš„åˆ†æ•¸è®Šå¾—å›°é›£ã€‚å› æ­¤ï¼Œç•¶ä»Šç”¨æ–¼åŸºæº–ç¿»è­¯æ¨¡å‹çš„æœ€å¸¸ç”¨æŒ‡æ¨™æ˜¯[SacreBLEU](https://github.com/mjpost/sacrebleu)ï¼Œå®ƒé€šéæ¨™æº–åŒ–æ¨™è¨˜åŒ–æ­¥é©Ÿè§£æ±ºäº†é€™å€‹ç¼ºé»ï¼ˆå’Œå…¶ä»–çš„ä¸€äº›ç¼ºé»ï¼‰ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ¨™ï¼Œæˆ‘å€‘é¦–å…ˆéœ€è¦å®‰è£ SacreBLEU åº«ï¼š

```py
!pip install sacrebleu
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥å°±åƒæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)é‚£æ¨£é€šé **load_metric()** åŠ è¼‰å®ƒ ï¼š

```py
from datasets import load_metric

metric = load_metric("sacrebleu")
```

è©²æŒ‡æ¨™å°‡æ–‡æœ¬ä½œç‚ºè¼¸å…¥å’Œç›®æ¨™çµæœã€‚å®ƒæ—¨åœ¨æ¥å—å¤šå€‹å¯æ¥å—çš„ç›®æ¨™ï¼Œå› ç‚ºåŒä¸€å€‹å¥å­é€šå¸¸æœ‰å¤šå€‹å¯æ¥å—çš„ç¿»è­¯â€”â€”æˆ‘å€‘ä½¿ç”¨çš„æ•¸æ“šé›†åªæä¾›ä¸€å€‹ï¼Œä½†åœ¨ NLP ä¸­æ‰¾åˆ°å°‡å¤šå€‹å¥å­ä½œç‚ºæ¨™ç±¤çš„æ•¸æ“šé›†ä¸æ˜¯ä¸€å€‹é›£é¡Œã€‚å› æ­¤ï¼Œé æ¸¬çµæœæ‡‰è©²æ˜¯ä¸€å€‹å¥å­åˆ—è¡¨ï¼Œè€Œåƒè€ƒæ‡‰è©²æ˜¯ä¸€å€‹å¥å­åˆ—è¡¨çš„åˆ—è¡¨ã€‚

è®“æˆ‘å€‘å˜—è©¦ä¸€å€‹ä¾‹å­ï¼š

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

é€™å¾—åˆ°äº† 46.75 çš„ BLEU åˆ†æ•¸ï¼Œé€™æ˜¯ç›¸ç•¶ä¸éŒ¯çš„â€”â€”ä½œç‚ºåƒè€ƒï¼ŒåŸå§‹ Transformer æ¨¡å‹åœ¨[â€œAttention Is All You Needâ€ è«–æ–‡](https://arxiv.org/pdf/1706.03762.pdf)é¡ä¼¼çš„è‹±èªå’Œæ³•èªç¿»è­¯ä»»å‹™ä¸­ç²å¾—äº† 41.8 çš„ BLEU åˆ†æ•¸ï¼ ï¼ˆæœ‰é—œå„å€‹æŒ‡æ¨™çš„æ›´å¤šä¿¡æ¯ï¼Œä¾‹å¦‚ **counts** å’Œ **bp** ï¼Œè¦‹[SacreBLEU å€‰åº«](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘å€‘å˜—è©¦ä½¿ç”¨ç¿»è­¯æ¨¡å‹ä¸­ç¶“å¸¸å‡ºç¾çš„å…©ç¨®ç³Ÿç³•çš„é æ¸¬é¡å‹ï¼ˆå¤§é‡é‡è¤‡æˆ–å¤ªçŸ­ï¼‰ï¼Œæˆ‘å€‘å°‡å¾—åˆ°ç›¸ç•¶ç³Ÿç³•çš„ BLEU åˆ†æ•¸ï¼š

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

åˆ†æ•¸å¯ä»¥å¾ 0 åˆ° 100ï¼Œè¶Šé«˜è¶Šå¥½ã€‚

{#if fw === 'tf'}

ç‚ºäº†å¾æ¨¡å‹è¼¸å‡ºå¯ä»¥è¢«è©•ä¼°åŸºæº–å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ **tokenizer.batch_decode()** æ–¹æ³•ã€‚æˆ‘å€‘åªéœ€è¦æ¸…ç†æ¨™ç±¤ä¸­æ‰€æœ‰ **-100** ï¼ˆæ¨™è¨˜å™¨æœƒè‡ªå‹•ç‚ºå¡«å……æ¨™è¨˜åšåŒæ¨£çš„äº‹æƒ…ï¼‰ï¼š

```py
import numpy as np


def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch in tf_generate_dataset:
        predictions = model.generate(
            input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
        )
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = batch["labels"].numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

ç‚ºäº†å¾æ¨¡å‹è¼¸å‡ºåˆ°åº¦é‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚ æˆ‘å€‘åªéœ€è¦æ¸…ç†æ¨™ç±¤ä¸­çš„æ‰€æœ‰ `-100`ï¼ˆæ¨™è¨˜å™¨å°‡è‡ªå‹•å°å¡«å……æ¨™è¨˜åŸ·è¡Œç›¸åŒæ“ä½œï¼‰ï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # In case the model returns more than the prediction logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100s in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

ç¾åœ¨é€™å·²ç¶“å®Œæˆäº†ï¼Œæˆ‘å€‘å·²ç¶“æº–å‚™å¥½å¾®èª¿æˆ‘å€‘çš„æ¨¡å‹äº†ï¼

### å¾®èª¿æ¨¡å‹

ç¬¬ä¸€æ­¥æ˜¯ç™»éŒ„ Hugging Faceï¼Œé€™æ¨£æ‚¨å°±å¯ä»¥å°‡çµæœä¸Šå‚³åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æœ‰ä¸€å€‹æ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¹«åŠ©æ‚¨åœ¨notebookä¸­å®Œæˆæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

é€™å°‡é¡¯ç¤ºä¸€å€‹å°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¼¸å…¥æ‚¨çš„ Hugging Face ç™»éŒ„æ†‘æ“šã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šé‹è¡Œä»£ç¢¼ï¼Œåªéœ€åœ¨çµ‚ç«¯ä¸­è¼¸å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'tf'}

åœ¨æˆ‘å€‘é–‹å§‹ä¹‹å‰ï¼Œè®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘åœ¨æ²’æœ‰ä»»ä½•è¨“ç·´çš„æƒ…æ³ä¸‹å¾æˆ‘å€‘çš„æ¨¡å‹ä¸­å¾—åˆ°äº†ä»€éº¼æ¨£çš„çµæœï¼š

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

ä¸€æ—¦å®Œæˆï¼Œæˆ‘å€‘å°±å¯ä»¥æº–å‚™ç·¨è­¯å’Œè¨“ç·´æ¨¡å‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚ æ³¨æ„ç•¶ä½¿ç”¨ `tf.keras.mixed_precision.set_global_policy("mixed_float16")`æ™‚â€”â€”é€™å°‡å‘Šè¨´ Keras ä½¿ç”¨ float16 é€²è¡Œè¨“ç·´ï¼Œé€™å¯ä»¥é¡¯è‘—æé«˜æ”¯æŒå®ƒçš„ GPUï¼ˆNvidia 20xx/V100 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰çš„é€Ÿåº¦ã€‚

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å®šç¾©ä¸€å€‹ `PushToHubCallback` ä»¥ä¾¿åœ¨è¨“ç·´æœŸé–“å°‡æˆ‘å€‘çš„æ¨¡å‹ä¸Šå‚³åˆ° Hubï¼Œæ­£å¦‚æˆ‘å€‘åœ¨ [ç¬¬ 2 ç¯€]((/course/chapter7/2)) ä¸­çœ‹åˆ°çš„ï¼Œç„¶å¾Œæˆ‘å€‘åªéœ€æ“¬åˆæ¨¡å‹æ™‚æ·»åŠ è©²å›èª¿å‡½æ•¸ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

è«‹æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `hub_model_id` åƒæ•¸æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å„²åº«çš„åç¨±ï¼ˆç•¶æ‚¨æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„çµ„ç¹”çš„æ™‚å€™ï¼Œæ‚¨ä¹Ÿå¿…é ˆä½¿ç”¨æ­¤åƒæ•¸ï¼‰ã€‚ ä¾‹å¦‚ï¼Œç•¶æˆ‘å€‘å°‡æ¨¡å‹æ¨é€åˆ° [`huggingface-course` çµ„ç¹”](https://huggingface.co/huggingface-course) æ™‚ï¼Œæˆ‘å€‘æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments`ã€‚ é»˜èªæƒ…æ³ä¸‹ï¼Œä½¿ç”¨çš„å­˜å„²åº«å°‡åœ¨æ‚¨çš„å‘½åç©ºé–“ä¸­ï¼Œä¸¦ä»¥æ‚¨è¨­ç½®çš„è¼¸å‡ºç›®éŒ„å‘½åï¼Œå› æ­¤é€™è£¡å°‡æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"`ã€‚

<Tip>

ğŸ’¡å¦‚æœæ‚¨ä½¿ç”¨çš„è¼¸å‡ºç›®éŒ„å·²ç¶“å­˜åœ¨ï¼Œå‰‡å®ƒéœ€è¦æ˜¯æ‚¨è¦æ¨é€åˆ°çš„å­˜å„²åº«çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°‡åœ¨å®šç¾©æ‚¨çš„åç¨±æ™‚æœƒé‡åˆ°éŒ¯èª¤ï¼Œä¸¦ä¸”éœ€è¦è¨­ç½®ä¸€å€‹æ–°åç¨±ã€‚

</Tip>

æœ€å¾Œï¼Œè®“æˆ‘å€‘çœ‹çœ‹è¨“ç·´çµæŸå¾Œæˆ‘å€‘çš„æŒ‡æ¨™æ˜¯ä»€éº¼æ¨£çš„ï¼š

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

åœ¨é€™å€‹éšæ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶ä¾†æ¸¬è©¦æ‚¨çš„æ¨¡å‹ä¸¦èˆ‡æ‚¨çš„æœ‹å‹åˆ†äº«ã€‚ æ‚¨å·²ç¶“æˆåŠŸåœ°å¾®èª¿äº†ç¿»è­¯ä»»å‹™ä¸­çš„æ¨¡å‹â€”â€”æ­å–œï¼

{:else}

ä¸€æ—¦å®Œæˆï¼Œæˆ‘å€‘å°±å¯ä»¥å®šç¾©æˆ‘å€‘çš„ `Seq2SeqTrainingArguments`ã€‚ èˆ‡ `Trainer` ä¸€æ¨£ï¼Œæˆ‘å€‘ä½¿ç”¨ `TrainingArguments` çš„å­é¡ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šå¯ä»¥è¨­ç½®çš„å­—æ®µï¼š

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

é™¤äº†é€šå¸¸çš„è¶…åƒæ•¸ï¼ˆå¦‚å­¸ç¿’ç‡ã€è¨“ç·´è¼ªæ•¸ã€æ‰¹æ¬¡å¤§å°å’Œä¸€äº›æ¬Šé‡è¡°æ¸›ï¼‰ä¹‹å¤–ï¼Œèˆ‡æˆ‘å€‘åœ¨å‰å¹¾ç¯€ä¸­çœ‹åˆ°çš„ç›¸æ¯”ï¼Œé€™è£¡æœ‰ä¸€äº›è®ŠåŒ–ï¼š

- æˆ‘å€‘æ²’æœ‰è¨­ç½®ä»»ä½•å®šæœŸè©•ä¼°ï¼Œå› ç‚ºè©•ä¼°éœ€è¦è€—è²»ä¸€å®šçš„æ™‚é–“ï¼›æˆ‘å€‘åªæœƒåœ¨è¨“ç·´é–‹å§‹ä¹‹å‰å’ŒçµæŸä¹‹å¾Œè©•ä¼°æˆ‘å€‘çš„æ¨¡å‹ä¸€æ¬¡ã€‚
- æˆ‘å€‘è¨­ç½®fp16=Trueï¼Œé€™å¯ä»¥åŠ å¿«æ”¯æŒfp16çš„ GPU ä¸Šçš„è¨“ç·´é€Ÿåº¦ã€‚
- å’Œä¸Šé¢æˆ‘å€‘è¨è«–çš„é‚£æ¨£ï¼Œæˆ‘å€‘è¨­ç½®predict_with_generate=True
- æˆ‘å€‘ç”¨push_to_hub=Trueåœ¨æ¯å€‹ epoch çµæŸæ™‚å°‡æ¨¡å‹ä¸Šå‚³åˆ° Hubã€‚

è«‹æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `hub_model_id` åƒæ•¸æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å„²åº«çš„åç¨±ï¼ˆç•¶æ‚¨æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„çµ„ç¹”çš„æ™‚å€™ï¼Œæ‚¨ä¹Ÿå¿…é ˆä½¿ç”¨æ­¤åƒæ•¸ï¼‰ã€‚ ä¾‹å¦‚ï¼Œç•¶æˆ‘å€‘å°‡æ¨¡å‹æ¨é€åˆ° [`huggingface-course` çµ„ç¹”](https://huggingface.co/huggingface-course) æ™‚ï¼Œæˆ‘å€‘æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments`ã€‚ é»˜èªæƒ…æ³ä¸‹ï¼Œä½¿ç”¨çš„å­˜å„²åº«å°‡åœ¨æ‚¨çš„å‘½åç©ºé–“ä¸­ï¼Œä¸¦ä»¥æ‚¨è¨­ç½®çš„è¼¸å‡ºç›®éŒ„å‘½åï¼Œå› æ­¤é€™è£¡å°‡æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"`ã€‚

<Tip>

ğŸ’¡å¦‚æœæ‚¨ä½¿ç”¨çš„è¼¸å‡ºç›®éŒ„å·²ç¶“å­˜åœ¨ï¼Œå‰‡å®ƒéœ€è¦æ˜¯æ‚¨è¦æ¨é€åˆ°çš„å­˜å„²åº«çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°‡åœ¨å®šç¾©æ‚¨çš„åç¨±æ™‚æœƒé‡åˆ°éŒ¯èª¤ï¼Œä¸¦ä¸”éœ€è¦è¨­ç½®ä¸€å€‹æ–°åç¨±ã€‚

</Tip>


æœ€å¾Œï¼Œæˆ‘å€‘éœ€è¦å°‡æ‰€æœ‰å…§å®¹å‚³éçµ¦ **Seq2SeqTrainer** ï¼š

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

åœ¨è¨“ç·´ä¹‹å‰ï¼Œæˆ‘å€‘å°‡é¦–å…ˆæŸ¥çœ‹æˆ‘å€‘çš„æ¨¡å‹ç²å¾—çš„åˆ†æ•¸ï¼Œä»¥ä»”ç´°æª¢æŸ¥æˆ‘å€‘çš„å¾®èª¿æ²’æœ‰è®“äº‹æƒ…è®Šå¾—æ›´ç³Ÿã€‚æ­¤å‘½ä»¤éœ€è¦ä¸€äº›æ™‚é–“ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨åŸ·è¡Œæ™‚å–æ¯å’–å•¡ï¼š

```python
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

BLEUçš„åˆ†æ•¸é‚„ä¸éŒ¯ï¼Œé€™åæ˜ äº†æˆ‘å€‘çš„æ¨¡å‹å·²ç¶“æ“…é•·å°‡è‹±èªå¥å­ç¿»è­¯æˆæ³•èªå¥å­ã€‚

æ¥ä¸‹ä¾†æ˜¯è¨“ç·´ï¼Œé€™ä¹Ÿéœ€è¦ä¸€äº›æ™‚é–“ï¼š

```python
trainer.train()
```

è«‹æ³¨æ„ï¼Œç•¶è¨“ç·´ç™¼ç”Ÿæ™‚ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ™‚ï¼ˆé€™è£¡æ˜¯æ¯å€‹æ™‚æœŸï¼‰ï¼Œå®ƒéƒ½æœƒåœ¨å¾Œè‡ºä¸Šå‚³åˆ° Hubã€‚é€™æ¨£ï¼Œå¦‚æœ‰å¿…è¦ï¼Œæ‚¨å°‡èƒ½å¤ åœ¨å¦ä¸€è‡ºæ©Ÿå™¨ä¸Šç¹¼çºŒæ‚¨çš„è¨“ç·´ã€‚

è¨“ç·´å®Œæˆå¾Œï¼Œæˆ‘å€‘å†æ¬¡è©•ä¼°æˆ‘å€‘çš„æ¨¡å‹â€”â€”å¸Œæœ›æˆ‘å€‘æœƒçœ‹åˆ° BLEU åˆ†æ•¸æœ‰æ‰€æ”¹å–„ï¼

```py
trainer.evaluate(max_length=max_target_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

é€™æ˜¯è¿‘ 14 é»çš„æ”¹é€²ï¼Œé€™å¾ˆæ£’ã€‚

æœ€å¾Œï¼Œæˆ‘å€‘ä½¿ç”¨ **push_to_hub()** æ–¹æ³•ä¾†ç¢ºä¿æˆ‘å€‘ä¸Šå‚³æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ã€‚é€™ **Trainer** é‚„å‰µå»ºäº†ä¸€å¼µåŒ…å«æ‰€æœ‰è©•ä¼°çµæœçš„æ¨¡å‹å¡ä¸¦ä¸Šå‚³ã€‚æ­¤æ¨¡å‹å¡åŒ…å«å¯å¹«åŠ©æ¨¡å‹ä¸­å¿ƒç‚ºæ¨ç†æ¼”ç¤ºé¸æ“‡å°éƒ¨ä»¶çš„å…ƒæ•¸æ“šã€‚é€šå¸¸ä¸éœ€è¦åšé¡å¤–çš„æ›´æ”¹ï¼Œå› ç‚ºå®ƒå¯ä»¥å¾æ¨¡å‹é¡ä¸­æ¨æ–·å‡ºæ­£ç¢ºçš„å°éƒ¨ä»¶ï¼Œä½†åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œç›¸åŒçš„æ¨¡å‹é¡å¯ä»¥ç”¨æ–¼æ‰€æœ‰é¡å‹çš„åºåˆ—åˆ°åºåˆ—å•é¡Œï¼Œæ‰€ä»¥æˆ‘å€‘æŒ‡å®šå®ƒæ˜¯ä¸€å€‹ç¿»è­¯æ¨¡å‹ï¼š

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

å¦‚æœæ‚¨æƒ³æª¢æŸ¥å‘½ä»¤åŸ·è¡Œçš„çµæœï¼Œæ­¤å‘½ä»¤å°‡è¿”å›å®ƒå‰›å‰›åŸ·è¡Œçš„æäº¤çš„ URLï¼Œå¯ä»¥æ‰“é–‹urlé€²è¡Œæª¢æŸ¥ï¼š

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

åœ¨æ­¤éšæ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶ä¾†æ¸¬è©¦æ‚¨çš„æ¨¡å‹ä¸¦èˆ‡æ‚¨çš„æœ‹å‹åˆ†äº«ã€‚æ‚¨å·²æˆåŠŸå¾®èª¿ç¿»è­¯ä»»å‹™çš„æ¨¡å‹ - æ­å–œï¼

å¦‚æœæ‚¨æƒ³æ›´æ·±å…¥åœ°ç­è§£è¨“ç·´å¾ªç’°ï¼Œæˆ‘å€‘ç¾åœ¨å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ¨£çš„äº‹æƒ…ã€‚

{/if}

{#if fw === 'pt'}

## è‡ªå®šç¾©è¨“ç·´å¾ªç’°

ç¾åœ¨è®“æˆ‘å€‘çœ‹ä¸€ä¸‹å®Œæ•´çš„è¨“ç·´å¾ªç’°ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è¼•é¬†è‡ªå®šç¾©æ‰€éœ€çš„éƒ¨åˆ†ã€‚å®ƒçœ‹èµ·ä¾†å¾ˆåƒæˆ‘å€‘åœ¨[æœ¬ç« ç¬¬äºŒç¯€](/course/chapter7/2)å’Œ[ç¬¬ä¸‰ç« ç¬¬å››å°ç¯€](/course/chapter3/4)æ‰€åšçš„ã€‚

### æº–å‚™è¨“ç·´æ‰€éœ€çš„ä¸€åˆ‡

æ‚¨å·²ç¶“å¤šæ¬¡çœ‹åˆ°æ‰€æœ‰é€™äº›ï¼Œå› æ­¤é€™ä¸€å¡Šæœƒç°¡ç•¥é€²è¡Œã€‚é¦–å…ˆæˆ‘å€‘å°‡æ§‹å»ºæˆ‘å€‘çš„æ•¸æ“šé›†çš„**DataLoader** ï¼Œåœ¨å°‡æ•¸æ“šé›†è¨­ç½®ç‚º **torch** æ ¼å¼ï¼Œæˆ‘å€‘å°±å¾—åˆ°äº† PyTorch å¼µé‡ï¼š

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹ä¾†æˆ‘å€‘é‡æ–°å¯¦ä¾‹åŒ–æˆ‘å€‘çš„æ¨¡å‹ï¼Œä»¥ç¢ºä¿æˆ‘å€‘ä¸æœƒç¹¼çºŒä¸Šä¸€ç¯€çš„å¾®èª¿ï¼Œè€Œæ˜¯å†æ¬¡å¾é è¨“ç·´æ¨¡å‹é–‹å§‹é‡æ–°è¨“ç·´ï¼š

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ç„¶å¾Œæˆ‘å€‘éœ€è¦ä¸€å€‹å„ªåŒ–å™¨ï¼š

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ä¸€æ—¦æˆ‘å€‘æ“æœ‰æ‰€æœ‰é€™äº›å°è±¡ï¼Œæˆ‘å€‘å°±å¯ä»¥å°‡å®ƒå€‘ç™¼é€åˆ° `accelerator.prepare()` æ–¹æ³•ã€‚ è«‹è¨˜ä½ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Colab ç­†è¨˜æœ¬è¨“ç·´ä¸­ä½¿ç”¨TPUï¼Œå‰‡éœ€è¦å°‡æ‰€æœ‰é€™äº›ä»£ç¢¼ç§»å‹•åˆ°è¨“ç·´å‡½æ•¸ä¸­ï¼Œä¸¦ä¸”ä¸æ‡‰åŸ·è¡Œä»»ä½•å¯¦ä¾‹åŒ–â€œåŠ é€Ÿå™¨â€çš„å°è±¡ã€‚

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ç¾åœ¨æˆ‘å€‘å·²ç¶“ç™¼é€äº†æˆ‘å€‘çš„ **train_dataloader** åˆ° **accelerator.prepare()** ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨å®ƒçš„é•·åº¦ä¾†è¨ˆç®—è¨“ç·´æ­¥é©Ÿçš„æ•¸é‡ã€‚è«‹è¨˜ä½ï¼Œæˆ‘å€‘æ‡‰è©²å§‹çµ‚åœ¨æº–å‚™å¥½æ•¸æ“šåŠ è¼‰å™¨å¾ŒåŸ·è¡Œæ­¤æ“ä½œï¼Œå› ç‚ºè©²æ–¹æ³•æœƒæ›´æ”¹ **DataLoader** .æˆ‘å€‘ä½¿ç”¨å¾å­¸ç¿’ç‡è¡°æ¸›åˆ° 0 çš„ç¶“å…¸ç·šæ€§å­¸ç¿’ç‡èª¿åº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€å¾Œï¼Œè¦å°‡æˆ‘å€‘çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘å€‘éœ€è¦å‰µå»ºä¸€å€‹ **Repository** å·¥ä½œæ–‡ä»¶å¤¾ä¸­çš„å°è±¡ã€‚å¦‚æœæ‚¨å°šæœªç™»éŒ„ï¼Œè«‹å…ˆç™»éŒ„ Hugging Faceã€‚æˆ‘å€‘å°‡å¾æˆ‘å€‘æƒ³è¦ç‚ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¢ºå®šå­˜å„²åº«åç¨±ï¼ˆæ‚¨å¯ä»¥è‡ªç”±åœ°ç”¨è‡ªå·±çš„é¸æ“‡æ›¿æ› **repo_name** ï¼›å®ƒéœ€è¦åŒ…å«æ‚¨çš„ç”¨æˆ¶åï¼Œå¯ä»¥ä½¿ç”¨**get_full_repo_name()**å‡½æ•¸çš„æŸ¥çœ‹ç›®å‰çš„repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¾ä¸­å…‹éš†è©²å­˜å„²åº«ã€‚å¦‚æœå®ƒå·²ç¶“å­˜åœ¨ï¼Œé€™å€‹æœ¬åœ°æ–‡ä»¶å¤¾æ‡‰è©²æ˜¯æˆ‘å€‘æ­£åœ¨ä½¿ç”¨çš„å­˜å„²åº«çš„å…‹éš†ï¼š

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘å€‘ç¾åœ¨å¯ä»¥é€šéèª¿ç”¨ **repo.push_to_hub()** æ–¹æ³•ä¸Šå‚³æˆ‘å€‘ä¿å­˜çš„ä»»ä½•å…§å®¹ **output_dir** ã€‚é€™å°‡å¹«åŠ©æˆ‘å€‘åœ¨æ¯å€‹ epoch çµæŸæ™‚ä¸Šå‚³éç¨‹ä¸­çš„æ¨¡å‹ã€‚

### è¨“ç·´å¾ªç’°

æˆ‘å€‘ç¾åœ¨æº–å‚™ç·¨å¯«å®Œæ•´çš„è¨“ç·´å¾ªç’°ã€‚ç‚ºäº†ç°¡åŒ–å®ƒçš„è©•ä¼°éƒ¨åˆ†ï¼Œæˆ‘å€‘å®šç¾©äº†é€™å€‹ **postprocess()** å‡½æ•¸æ¥æ”¶é æ¸¬çµæœå’Œæ­£ç¢ºæ¨™ç±¤ä¸¦å°‡å®ƒå€‘è½‰æ›ç‚ºæˆ‘å€‘ **metric** å°è±¡æ‰€éœ€è¦çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼š

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

è¨“ç·´å¾ªç’°çœ‹èµ·ä¾†å’Œ[æœ¬ç« ç¬¬äºŒç¯€](/course/chapter7/2)èˆ‡[ç¬¬ä¸‰ç« ](/course/chapter3)å¾ˆåƒï¼Œåœ¨è©•ä¼°éƒ¨åˆ†æœ‰ä¸€äº›ä¸åŒ - æ‰€ä»¥è®“æˆ‘å€‘å°ˆæ³¨æ–¼é€™ä¸€é»ï¼

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯æˆ‘å€‘ä½¿ç”¨ `generate()` æ–¹æ³•ä¾†è¨ˆç®—é æ¸¬ï¼Œä½†é€™æ˜¯æˆ‘å€‘åŸºç¤æ¨¡å‹ä¸Šçš„ä¸€å€‹æ–¹æ³•ï¼Œè€Œä¸æ˜¯åŒ…è£æ¨¡å‹ğŸ¤— Accelerate åœ¨ `prepare()` æ–¹æ³•ä¸­å‰µå»ºã€‚ é€™å°±æ˜¯ç‚ºä»€éº¼æˆ‘å€‘å…ˆè§£åŒ…æ¨¡å‹ï¼Œç„¶å¾Œèª¿ç”¨é€™å€‹æ–¹æ³•ã€‚

ç¬¬äºŒä»¶äº‹æ˜¯ï¼Œå°±åƒ[token åˆ†é¡](/course/chapter7/2)ï¼Œå…©å€‹é€²ç¨‹å¯èƒ½å°‡è¼¸å…¥å’Œæ¨™ç±¤å¡«å……ç‚ºä¸åŒçš„å½¢ç‹€ï¼Œå› æ­¤æˆ‘å€‘åœ¨èª¿ç”¨ **gather()** æ–¹æ³•ä¹‹å‰ä½¿ç”¨ **accelerator.pad_across_processes()** ä½¿é æ¸¬å’Œæ¨™ç±¤å…·æœ‰ç›¸åŒçš„å½¢ç‹€ã€‚å¦‚æœæˆ‘å€‘ä¸é€™æ¨£åšï¼Œè©•ä¼°è¦éº¼å‡ºéŒ¯ï¼Œè¦éº¼æ°¸é åœ¨é˜»å¡ã€‚

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

å®Œæˆæ­¤æ“ä½œå¾Œï¼Œæ‚¨æ‡‰è©²æœ‰ä¸€å€‹æ¨¡å‹ï¼Œå…¶çµæœèˆ‡ä½¿ç”¨ `Seq2SeqTrainer` è¨“ç·´çš„æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚ æ‚¨å¯ä»¥åœ¨ [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerateï¼‰æŸ¥çœ‹è¨“ç·´å®Œçš„çµæœã€‚ å¦‚æœæ‚¨æƒ³æ¸¬è©¦å°è¨“ç·´å¾ªç’°çš„ä»»ä½•èª¿æ•´ï¼Œæ‚¨å¯ä»¥é€šéç·¨è¼¯ä¸Šé¢é¡¯ç¤ºçš„ä»£ç¢¼ç›´æ¥å¯¦ç¾å®ƒå€‘ï¼

{/if}

## ä½¿ç”¨å¾®èª¿å¾Œçš„æ¨¡å‹

æˆ‘å€‘å·²ç¶“å‘æ‚¨å±•ç¤ºç­å¦‚ä½•å°‡æˆ‘å€‘åœ¨æ¨¡å‹ä¸­å¿ƒå¾®èª¿çš„æ¨¡å‹èˆ‡æ¨ç†å°éƒ¨ä»¶ä¸€èµ·ä½¿ç”¨ã€‚ è¦åœ¨â€œç®¡é“â€ä¸­æœ¬åœ°ä½¿ç”¨å®ƒï¼Œæˆ‘å€‘åªéœ€è¦æŒ‡å®šæ­£ç¢ºçš„æ¨¡å‹æ¨™è­˜ç¬¦ï¼š

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

æ­£å¦‚é æœŸçš„é‚£æ¨£ï¼Œæˆ‘å€‘çš„é è¨“ç·´æ¨¡å‹å°‡å…¶çŸ¥è­˜é©æ‡‰äº†æˆ‘å€‘å°å…¶é€²è¡Œå¾®èª¿çš„èªæ–™åº«ï¼Œè€Œä¸æ˜¯å–®ç¨ç•™ä¸‹è‹±æ–‡å–®è©â€œthreadsâ€ï¼Œè€Œæ˜¯å°‡å…¶ç¿»è­¯æˆæ³•èªå®˜æ–¹ç‰ˆæœ¬ã€‚ â€œâ€çš„ç¿»è­¯ä¹Ÿæ˜¯ä¸€æ¨£çš„ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

é¢¨æ ¼é©æ‡‰çš„å¦ä¸€å€‹å¾ˆå¥½çš„ä¾‹å­ï¼

<Tip>

âœï¸ **è¼ªåˆ°ä½ äº†ï¼** â€œé›»å­éƒµä»¶â€é€™å€‹è©åœ¨æ¨¡å‹è¿”å›äº†ä»€éº¼ï¼Ÿ

</Tip>
