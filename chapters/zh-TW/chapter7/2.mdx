<FrameworkSwitchCourse {fw} />

# Token åˆ†é¡

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
]} />

{/if}

æˆ‘å€‘å°‡æ¢ç´¢çš„ç¬¬ä¸€å€‹æ‡‰ç”¨æ˜¯Tokenåˆ†é¡ã€‚é€™å€‹é€šç”¨ä»»å‹™åŒ…æ‹¬ä»»ä½•å¯ä»¥è¡¨è¿°ç‚ºâ€œç‚ºå¥å­ä¸­çš„è©æˆ–å­—åˆ†é…æ¨™ç±¤â€çš„å•é¡Œï¼Œä¾‹å¦‚ï¼š

- **å¯¦é«”å‘½åè­˜åˆ¥ (NER)**: æ‰¾å‡ºå¥å­ä¸­çš„å¯¦é«”ï¼ˆå¦‚äººç‰©ã€åœ°é»æˆ–çµ„ç¹”ï¼‰ã€‚é€™å¯ä»¥é€šéç‚ºæ¯å€‹å¯¦é«”æˆ–â€œç„¡å¯¦é«”â€æŒ‡å®šä¸€å€‹é¡åˆ¥çš„æ¨™ç±¤ã€‚
- **è©æ€§æ¨™è¨» (POS)**: å°‡å¥å­ä¸­çš„æ¯å€‹å–®è©æ¨™è¨˜ç‚ºå°æ‡‰æ–¼ç‰¹å®šçš„è©æ€§ï¼ˆå¦‚åè©ã€å‹•è©ã€å½¢å®¹è©ç­‰ï¼‰ã€‚
- **åˆ†å¡Šï¼ˆchunkingï¼‰**: æ‰¾åˆ°å±¬æ–¼åŒä¸€å¯¦é«”çš„Tokenã€‚é€™å€‹ä»»å‹™(å¯çµåˆPOSæˆ–NER)å¯ä»¥ä»»ä½•å°‡ä¸€å¡ŠTokenä½œç‚ºåˆ¶å®šä¸€å€‹æ¨™ç±¤(é€šå¸¸æ˜¯B -),å¦ä¸€å€‹æ¨™ç±¤(é€šå¸¸I -)è¡¨ç¤ºTokenæ˜¯å¦æ˜¯åŒä¸€å¡Š,å’Œç¬¬ä¸‰å€‹æ¨™ç±¤(é€šå¸¸æ˜¯O)è¡¨ç¤ºTokenä¸å±¬æ–¼ä»»ä½•å¡Šã€‚ä¹Ÿå°±æ˜¯æ¨™å‡ºå¥å­ä¸­çš„çŸ­èªå¡Šï¼Œä¾‹å¦‚åè©çŸ­èªï¼ˆNPï¼‰ï¼Œå‹•è©çŸ­èªï¼ˆVPï¼‰ç­‰ã€‚

<Youtube id="wVHdVlPScxA"/>

ç•¶ç„¶ï¼Œé‚„æœ‰å¾ˆå¤šå…¶ä»–é¡å‹çš„tokenåˆ†é¡å•é¡Œï¼›é€™äº›åªæ˜¯å¹¾å€‹æœ‰ä»£è¡¨æ€§çš„ä¾‹å­ã€‚åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡åœ¨ NER ä»»å‹™ä¸Šå¾®èª¿æ¨¡å‹ (BERT)ï¼Œç„¶å¾Œè©²æ¨¡å‹å°‡èƒ½å¤ è¨ˆç®—å¦‚ä¸‹é æ¸¬ï¼š

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-bert-finetuned-ner-darkmode.hf.space" frameBorder="0" height="350" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

æ‚¨å¯ä»¥[åœ¨é€™è£¡](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).æ‰¾åˆ°æˆ‘å€‘å°‡è¨“ç·´ä¸¦ä¸Šå‚³åˆ° Hubçš„æ¨¡å‹ï¼Œå¯ä»¥å˜—è©¦è¼¸å…¥ä¸€äº›å¥å­çœ‹çœ‹æ¨¡å‹çš„é æ¸¬çµæœã€‚

## æº–å‚™æ•¸æ“š

é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹é©åˆæ¨™è¨˜åˆ†é¡çš„æ•¸æ“šé›†ã€‚åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨[CoNLL-2003 æ•¸æ“šé›†](https://huggingface.co/datasets/conll2003), å…¶ä¸­åŒ…å«ä¾†è‡ªè·¯é€ç¤¾çš„æ–°èå ±é“ã€‚

<Tip>

ğŸ’¡ åªè¦æ‚¨çš„æ•¸æ“šé›†ç”±å¸¶æœ‰ç›¸æ‡‰æ¨™ç±¤çš„åˆ†å‰²æˆå–®è©ä¸¦çš„æ–‡æœ¬çµ„æˆï¼Œæ‚¨å°±èƒ½å¤ å°‡é€™è£¡æè¿°çš„æ•¸æ“šè™•ç†éç¨‹æ‡‰ç”¨åˆ°æ‚¨è‡ªå·±çš„æ•¸æ“šé›†ã€‚å¦‚æœéœ€è¦è¤‡ç¿’å¦‚ä½•åœ¨.Datasetä¸­åŠ è¼‰è‡ªå®šç¾©æ•¸æ“šï¼Œè«‹åƒé–±[Chapter 5](/course/chapter5)ã€‚

</Tip>

### CoNLL-2003 æ•¸æ“šé›†

è¦åŠ è¼‰ CoNLL-2003 æ•¸æ“šé›†ï¼Œæˆ‘å€‘ä½¿ç”¨ ä¾†è‡ª ğŸ¤— Datasets åº«çš„**load_dataset()** æ–¹æ³•ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

é€™å°‡ä¸‹è¼‰ä¸¦ç·©å­˜æ•¸æ“šé›†ï¼Œå°±åƒå’Œæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3) åŠ è¼‰GLUE MRPC æ•¸æ“šé›†ä¸€æ¨£ã€‚æª¢æŸ¥é€™å€‹å°è±¡å¯ä»¥è®“æˆ‘å€‘çœ‹åˆ°å­˜åœ¨å“ªäº›åˆ—ï¼Œä»¥åŠè¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†ä¹‹é–“æ˜¯å¦‚ä½•åˆ†å‰²çš„:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

ç‰¹åˆ¥æ˜¯ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°æ•¸æ“šé›†åŒ…å«æˆ‘å€‘ä¹‹å‰æåˆ°çš„ä¸‰å€‹ä»»å‹™çš„æ¨™ç±¤ï¼šNERã€POS å’Œchunkingã€‚èˆ‡å…¶ä»–æ•¸æ“šé›†çš„ä¸€å€‹å¾ˆå¤§å€åˆ¥æ˜¯è¼¸å…¥æ–‡æœ¬ä¸æ˜¯ä½œç‚ºå¥å­æˆ–æ–‡æª”å‘ˆç¾çš„ï¼Œè€Œæ˜¯å–®è©åˆ—è¡¨ï¼ˆæœ€å¾Œä¸€åˆ—ç¨±ç‚º **tokens** ï¼Œä½†å®ƒåŒ…å«çš„æ˜¯é€™äº›è©æ˜¯é å…ˆæ¨™è¨˜åŒ–çš„è¼¸å…¥ï¼Œä»ç„¶éœ€è¦é€šéæ¨™è¨˜å™¨é€²è¡Œå­è©æ¨™è¨˜ï¼‰ã€‚

æˆ‘å€‘ä¾†çœ‹çœ‹è¨“ç·´é›†çš„ç¬¬ä¸€å€‹å…ƒç´ ï¼š

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

ç”±æ–¼æˆ‘å€‘è¦åŸ·è¡Œå‘½åå¯¦é«”è­˜åˆ¥ï¼Œæˆ‘å€‘å°‡æŸ¥çœ‹ NER æ¨™ç±¤ï¼š

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

é€™ä¸€åˆ—æ˜¯é¡æ¨™ç±¤çš„åºåˆ—ã€‚å…ƒç´ çš„é¡å‹åœ¨ner_featureçš„featureå±¬æ€§ä¸­ï¼Œæˆ‘å€‘å¯ä»¥é€šéæŸ¥çœ‹è©²ç‰¹æ€§çš„nameså±¬æ€§ä¾†è¨ªå•åç¨±åˆ—è¡¨:   

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

å› æ­¤ï¼Œé€™ä¸€åˆ—åŒ…å«çš„å…ƒç´ æ˜¯ClassLabelsçš„åºåˆ—ã€‚åºåˆ—å…ƒç´ çš„é¡å‹åœ¨`ner_feature`çš„`feature`ä¸­ï¼Œæˆ‘å€‘å¯ä»¥é€šéæŸ¥çœ‹è©²`feature`çš„`names`å±¬æ€§ä¾†è¨ªå•åç¨±åˆ—è¡¨:

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

æˆ‘å€‘åœ¨[ç¬¬å…­ç« ](/course/chapter6/3), æ·±å…¥ç ”ç©¶**token-classification** ç®¡é“æ™‚å·²ç¶“çœ‹åˆ°äº†é€™äº›æ¨™ç±¤ ï¼Œä½†ç‚ºäº†å¿«é€Ÿè¤‡ç¿’ï¼š

- `O` è¡¨ç¤ºé€™å€‹è©ä¸å°æ‡‰ä»»ä½•å¯¦é«”ã€‚
- `B-PER`/`I-PER`æ„å‘³è‘—é€™å€‹è©å°æ‡‰æ–¼äººåå¯¦é«”çš„é–‹é ­/å…§éƒ¨ã€‚
- `B-ORG`/`I-ORG` çš„æ„æ€æ˜¯é€™å€‹è©å°æ‡‰æ–¼çµ„ç¹”åç¨±å¯¦é«”çš„é–‹é ­/å…§éƒ¨ã€‚
- `B-LOC`/`I-LOC` æŒ‡çš„æ˜¯æ˜¯é€™å€‹è©å°æ‡‰æ–¼åœ°åå¯¦é«”çš„é–‹é ­/å…§éƒ¨ã€‚
- `B-MISC`/`I-MISC` è¡¨ç¤ºè©²è©å°æ‡‰æ–¼ä¸€å€‹é›œé …å¯¦é«”çš„é–‹é ­/å…§éƒ¨ã€‚

ç¾åœ¨è§£ç¢¼æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„æ¨™ç±¤ï¼š

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

ä¾‹å¦‚æ··åˆ **B-** å’Œ **I-** æ¨™ç±¤ï¼Œé€™æ˜¯ç›¸åŒçš„ä»£ç¢¼åœ¨ç´¢å¼• 4 çš„è¨“ç·´é›†å…ƒç´ ä¸Šçš„é æ¸¬çµæœï¼š

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

æ­£å¦‚æˆ‘å€‘æ‰€çœ‹åˆ°çš„ï¼Œè·¨è¶Šå…©å€‹å–®è©çš„å¯¦é«”ï¼Œå¦‚â€œEuropean Unionâ€å’Œâ€œWerner Zwingmannâ€ï¼Œæ¨¡å‹ç‚ºç¬¬ä¸€å€‹å–®è©æ¨™è¨»äº†ä¸€å€‹B-æ¨™ç±¤ï¼Œç‚ºç¬¬äºŒå€‹å–®è©æ¨™è¨»äº†ä¸€å€‹I-æ¨™ç±¤ã€‚

<Tip>

âœï¸ **è¼ªåˆ°ä½ äº†ï¼** ä½¿ç”¨ POS æˆ–chunkingæ¨™ç±¤è­˜åˆ¥åŒä¸€å€‹å¥å­ã€‚

</Tip>

### è™•ç†æ•¸æ“š

<Youtube id="iY2AZYdZAr0"/>

åƒå¾€å¸¸ä¸€æ¨£ï¼Œæˆ‘å€‘çš„æ–‡æœ¬éœ€è¦è½‰æ›ç‚ºToken IDï¼Œç„¶å¾Œæ¨¡å‹æ‰èƒ½ç†è§£å®ƒå€‘ã€‚æ­£å¦‚æˆ‘å€‘åœ¨[ç¬¬å…­ç« ](/course/chapter6/)æ‰€å­¸çš„é‚£æ¨£ã€‚ä¸éåœ¨æ¨™è¨˜ä»»å‹™ä¸­ï¼Œä¸€å€‹å¾ˆå¤§çš„å€åˆ¥æ˜¯æˆ‘å€‘æœ‰pre-tokenizedçš„è¼¸å…¥ã€‚å¹¸é‹çš„æ˜¯ï¼Œtokenizer APIå¯ä»¥å¾ˆå®¹æ˜“åœ°è™•ç†é€™å€‹å•é¡Œ;æˆ‘å€‘åªéœ€è¦ç”¨ä¸€å€‹ç‰¹æ®Šçš„tokenizerã€‚

é¦–å…ˆï¼Œè®“æˆ‘å€‘å‰µå»º`tokenizer`å°è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ BERT é è¨“ç·´æ¨¡å‹ï¼Œå› æ­¤æˆ‘å€‘å°‡å¾ä¸‹è¼‰ä¸¦ç·©å­˜é—œè¯çš„åˆ†è©å™¨é–‹å§‹ï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ä½ å¯ä»¥æ›´æ›æŠŠ `model_checkpoint` æ›´æ›ç‚º [Hub](https://huggingface.co/models),ä¸Šæ‚¨å–œæ­¡çš„ä»»ä½•å…¶ä»–å‹è™Ÿï¼Œæˆ–ä½¿ç”¨æ‚¨æœ¬åœ°ä¿å­˜çš„é è¨“ç·´æ¨¡å‹å’Œåˆ†è©å™¨ã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯åˆ†è©å™¨éœ€è¦ç”± ğŸ¤— Tokenizers åº«æ”¯æŒï¼Œæœ‰ä¸€å€‹â€œå¿«é€Ÿâ€ç‰ˆæœ¬å¯ç”¨ã€‚ä½ å¯ä»¥åœ¨[é€™å¼µå¤§è¡¨](https://huggingface.co/transformers/#supported-frameworks), ä¸Šçœ‹åˆ°æ‰€æœ‰å¸¶æœ‰å¿«é€Ÿç‰ˆæœ¬çš„æ¶æ§‹ï¼Œæˆ–è€…æª¢æŸ¥  æ‚¨å¯ä»¥é€šéæŸ¥çœ‹å®ƒ`is_fast` å±¬æ€§ä¾†æª¢æ¸¬æ­£åœ¨ä½¿ç”¨çš„`tokenizer`å°è±¡æ˜¯å¦ç”± ğŸ¤— Tokenizers æ”¯æŒï¼š

```py
tokenizer.is_fast
```

```python out
True
```

è¦å°é å…ˆæ¨™è¨˜çš„è¼¸å…¥é€²è¡Œæ¨™è¨˜ï¼Œæˆ‘å€‘å¯ä»¥åƒå¾€å¸¸ä¸€æ¨£ä½¿ç”¨æˆ‘å€‘çš„`tokenizer` åªéœ€æ·»åŠ  `is_split_into_words=True`:

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

æ­£å¦‚æˆ‘å€‘æ‰€è¦‹ï¼Œåˆ†è©å™¨æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®ŠToken(`[CLS]` åœ¨é–‹å§‹å’Œ`[SEP]` æœ€å¾Œ) è€Œå¤§å¤šæ•¸å–®è©æœªè¢«ä¿®æ”¹ã€‚ç„¶è€Œï¼Œå–®è© `lamb`,è¢«åˆ†ç‚ºå…©å€‹å­å–®è© `la` and `##mb`ã€‚é€™å°è‡´äº†è¼¸å…¥å’Œæ¨™ç±¤ä¹‹é–“çš„ä¸åŒ¹é…:æ¨™ç±¤åˆ—è¡¨åªæœ‰9å€‹å…ƒç´ ï¼Œè€Œæˆ‘å€‘çš„è¼¸å…¥ç¾åœ¨æœ‰12å€‹token ã€‚è¨ˆç®—ç‰¹æ®ŠTokenå¾ˆå®¹æ˜“(æˆ‘å€‘çŸ¥é“å®ƒå€‘åœ¨é–‹é ­å’Œçµå°¾)ï¼Œä½†æˆ‘å€‘é‚„éœ€è¦ç¢ºä¿æ‰€æœ‰æ¨™ç±¤èˆ‡é©ç•¶çš„å–®è©å°é½Šã€‚
å¹¸é‹çš„æ˜¯ï¼Œç”±æ–¼æˆ‘å€‘ä½¿ç”¨çš„æ˜¯å¿«é€Ÿåˆ†è©å™¨ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥è¨ªå•ğŸ¤— Tokenizersè¶…èƒ½åŠ›ï¼Œé€™æ„å‘³è‘—æˆ‘å€‘å¯ä»¥è¼•é¬†åœ°å°‡æ¯å€‹ä»¤ç‰Œæ˜ å°„åˆ°å…¶ç›¸æ‡‰çš„å–®è©ï¼ˆå¦‚[Chapter 6](/course/chapter6/3)):

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

é€šéä¸€é»é»å·¥ä½œï¼Œæˆ‘å€‘å¯ä»¥æ“´å±•æˆ‘å€‘çš„æ¨™ç±¤åˆ—è¡¨ä»¥åŒ¹é…token ã€‚æˆ‘å€‘å°‡æ‡‰ç”¨çš„ç¬¬ä¸€æ¢è¦å‰‡æ˜¯ï¼Œç‰¹æ®Štoken çš„æ¨™ç±¤ç‚º `-100` ã€‚é€™æ˜¯å› ç‚ºé»˜èªæƒ…æ³ä¸‹ `-100` æ˜¯ä¸€å€‹åœ¨æˆ‘å€‘å°‡ä½¿ç”¨çš„æå¤±å‡½æ•¸ï¼ˆäº¤å‰ç†µï¼‰ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚ç„¶å¾Œï¼Œæ¯å€‹token éƒ½æœƒç²å¾—èˆ‡å…¶æ‰€åœ¨å–®è©çš„token ç›¸åŒçš„æ¨™ç±¤ï¼Œå› ç‚ºå®ƒå€‘æ˜¯åŒä¸€å¯¦é«”çš„ä¸€éƒ¨åˆ†ã€‚å°æ–¼å–®è©å…§éƒ¨ä½†ä¸åœ¨é–‹é ­çš„Tokenï¼Œæˆ‘å€‘å°‡`B-` æ›¿æ›ç‚º `I-` (å› ç‚ºtoken ä¸ä»¥å¯¦é«”é–‹é ­):

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

è®“æˆ‘å€‘åœ¨æˆ‘å€‘çš„ç¬¬ä¸€å¥è©±ä¸Šè©¦ä¸€è©¦ï¼š

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

æ­£å¦‚æˆ‘å€‘æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘å€‘çš„å‡½æ•¸ç‚ºé–‹é ­å’Œçµå°¾çš„å…©å€‹ç‰¹æ®Šæ¨™è¨˜æ·»åŠ äº†  `-100` ï¼Œä½µç‚ºåˆ†æˆå…©å€‹æ¨™è¨˜çš„å–®è©æ·»åŠ äº†ä¸€å€‹æ–°çš„`0` ã€‚

<Tip>

âœï¸ **è¼ªåˆ°ä½ äº†ï¼** ä¸€äº›ç ”ç©¶äººå“¡æ›´å–œæ­¡æ¯å€‹è©åªæ­¸å±¬ä¸€å€‹æ¨™ç±¤, ä¸¦åˆ†é… `-100` çµ¦å®šè©ä¸­çš„å…¶ä»–å­æ¨™è¨˜ã€‚é€™æ˜¯ç‚ºäº†é¿å…åˆ†è§£æˆå¤§é‡å­æ¨™è¨˜çš„é•·è©å°æå¤±é€ æˆåš´é‡å½±éŸ¿ã€‚æŒ‰ç…§æ­¤è¦å‰‡æ›´æ”¹å‰ä¸€å€‹å‡½æ•¸ä½¿æ¨™ç±¤èˆ‡è¼¸å…¥idå°é½Šã€‚

</Tip>

ç‚ºäº†é è™•ç†æˆ‘å€‘çš„æ•´å€‹æ•¸æ“šé›†ï¼Œæˆ‘å€‘éœ€è¦æ¨™è¨˜æ‰€æœ‰è¼¸å…¥ä¸¦åœ¨æ‰€æœ‰æ¨™ç±¤ä¸Šæ‡‰ç”¨ `align_labels_with_tokens()` ã€‚ç‚ºäº†åˆ©ç”¨æˆ‘å€‘çš„å¿«é€Ÿåˆ†è©å™¨çš„é€Ÿåº¦å„ªå‹¢ï¼Œæœ€å¥½åŒæ™‚å°å¤§é‡æ–‡æœ¬é€²è¡Œåˆ†è©ï¼Œå› æ­¤æˆ‘å€‘å°‡ç·¨å¯«ä¸€å€‹è™•ç†ç¤ºä¾‹åˆ—è¡¨çš„å‡½æ•¸ä¸¦ä½¿ç”¨å¸¶ `batched=True` æœ‰é¸é …çš„ `Dataset.map()`æ–¹æ³• .èˆ‡æˆ‘å€‘ä¹‹å‰çš„ç¤ºä¾‹å”¯ä¸€ä¸åŒçš„æ˜¯ç•¶åˆ†è©å™¨çš„è¼¸å…¥æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–è€…åƒä¾‹å­ä¸­çš„å–®è©åˆ—è¡¨ï¼‰æ™‚  `word_ids()` å‡½æ•¸éœ€è¦ç²å–æˆ‘å€‘æƒ³è¦å–®è©çš„ç´¢å¼•çš„IDï¼Œæ‰€ä»¥æˆ‘å€‘ä¹Ÿæ·»åŠ å®ƒï¼š

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

è«‹æ³¨æ„ï¼Œæˆ‘å€‘é‚„æ²’æœ‰å¡«å……æˆ‘å€‘çš„è¼¸å…¥ï¼›æˆ‘å€‘ç¨å¾Œæœƒåœ¨ä½¿ç”¨æ•¸æ“šæ•´ç†å™¨å‰µå»ºbatchæ™‚é€™æ¨£åšã€‚

æˆ‘å€‘ç¾åœ¨å¯ä»¥ä¸€æ¬¡æ€§å°‡æ‰€æœ‰é è™•ç†æ‡‰ç”¨æ–¼æ•¸æ“šé›†çš„å…¶ä»–éƒ¨åˆ†ï¼š

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

æˆ‘å€‘å·²ç¶“å®Œæˆäº†æœ€é›£çš„éƒ¨åˆ†ï¼ç¾åœ¨æ•¸æ“šå·²ç¶“è¢«é è™•ç†äº†ï¼Œå¯¦éš›çš„è¨“ç·´çœ‹èµ·ä¾†å¾ˆåƒæˆ‘å€‘[ç¬¬ä¸‰ç« ](/course/chapter3)åšçš„.

{#if fw === 'pt'}

## ä½¿ç”¨ Trainer API å¾®èª¿æ¨¡å‹

ä½¿ç”¨ `Trainer` çš„å¯¦éš›ä»£ç¢¼æœƒå’Œä»¥å‰ä¸€æ¨£ï¼›å”¯ä¸€çš„è®ŠåŒ–æ˜¯æ•¸æ“šæ•´ç†æˆæ™‚æ‰¹è™•ç†çš„æ–¹å¼å’Œåº¦é‡è¨ˆç®—å‡½æ•¸ã€‚

{:else}

## ä½¿ç”¨ Keras å¾®èª¿æ¨¡å‹

ä½¿ç”¨Kerasçš„å¯¦éš›ä»£ç¢¼å°‡èˆ‡ä¹‹å‰éå¸¸ç›¸ä¼¼;å”¯ä¸€çš„è®ŠåŒ–æ˜¯å°‡æ•¸æ“šæ•´ç†æˆæ‰¹è™•ç†çš„æ–¹å¼å’ŒæŒ‡æ¨™è¨ˆç®—å‡½æ•¸ã€‚

{/if}


### æ•¸æ“šæ’åº

æˆ‘å€‘ä¸èƒ½åƒ[ç¬¬ä¸‰ç« ](/course/chapter3)é‚£æ¨£åªä½¿ç”¨ä¸€å€‹ `DataCollatorWithPadding `å› ç‚ºé€™éš»æœƒå¡«å……è¼¸å…¥ï¼ˆè¼¸å…¥ IDã€æ³¨æ„æ©ç¢¼å’Œæ¨™è¨˜é¡å‹ IDï¼‰ã€‚åœ¨é€™è£¡æˆ‘å€‘çš„æ¨™ç±¤æ‡‰è©²ä»¥èˆ‡è¼¸å…¥å®Œå…¨ç›¸åŒçš„æ–¹å¼å¡«å……ï¼Œä»¥ä¾¿å®ƒå€‘ä¿æŒé•·åº¦ç›¸åŒï¼Œä½¿ç”¨  `-100 ` ï¼Œé€™æ¨£åœ¨æå¤±è¨ˆç®—ä¸­å°±å¯ä»¥å¿½ç•¥ç›¸æ‡‰çš„é æ¸¬ã€‚

é€™ä¸€åˆ‡éƒ½æ˜¯ç”±ä¸€å€‹ [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification)å®Œæˆ.å®ƒæ˜¯ä¸€å€‹å¸¶æœ‰å¡«å……çš„æ•¸æ“šæ•´ç†å™¨å®ƒéœ€è¦  `tokenizer ` ç”¨æ–¼é è™•ç†è¼¸å…¥ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

ç‚ºäº†åœ¨å¹¾å€‹æ¨£æœ¬ä¸Šæ¸¬è©¦é€™ä¸€é»ï¼Œæˆ‘å€‘å¯ä»¥åœ¨è¨“ç·´é›†ä¸­çš„ç¤ºä¾‹åˆ—è¡¨ä¸Šèª¿ç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

è®“æˆ‘å€‘å°‡å…¶èˆ‡æ•¸æ“šé›†ä¸­ç¬¬ä¸€å€‹å’Œç¬¬äºŒå€‹å…ƒç´ çš„æ¨™ç±¤é€²è¡Œæ¯”è¼ƒï¼š

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

æ­£å¦‚æˆ‘å€‘æ‰€çœ‹åˆ°çš„ï¼Œç¬¬äºŒçµ„æ¨™ç±¤çš„é•·åº¦å·²ç¶“ä½¿ç”¨ `-100` å¡«å……åˆ°èˆ‡ç¬¬ä¸€çµ„æ¨™ç±¤ç›¸åŒã€‚

{:else}

æˆ‘å€‘çš„æ•¸æ“šæ•´ç†å™¨å·²æº–å‚™å°±ç·’ï¼ç¾åœ¨ï¼Œè®“æˆ‘å€‘ç”¨å®ƒä¾†è£½ä½œä¸€å€‹å¸¶æœ‰`to_tf_dataset()`æ–¹æ³•çš„`tf.data.Dataset`ã€‚

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```


 Next stop: the model itself.

{/if}

{#if fw === 'tf'}

### å®šç¾©æ¨¡å‹

ç”±æ–¼æˆ‘å€‘æ­£åœ¨ç ”ç©¶Tokenåˆ†é¡å•é¡Œï¼Œå› æ­¤æˆ‘å€‘å°‡ä½¿ç”¨ `AutoModelForTokenClassification` é¡ã€‚å®šç¾©é€™å€‹æ¨¡å‹æ™‚è¦è¨˜ä½çš„ä¸»è¦äº‹æƒ…æ˜¯å‚³éä¸€äº›é—œæ–¼æˆ‘å€‘çš„æ¨™ç±¤æ•¸é‡çš„ä¿¡æ¯ã€‚åŸ·è¡Œæ­¤æ“ä½œçš„æœ€ç°¡å–®æ–¹æ³•æ˜¯å°‡è©²æ•¸å­—å‚³éçµ¦ `num_labels` åƒæ•¸ï¼Œä½†æ˜¯å¦‚æœæˆ‘å€‘æƒ³è¦ä¸€å€‹å¾ˆå¥½çš„æ¨ç†å°éƒ¨ä»¶ï¼Œå°±åƒæˆ‘å€‘åœ¨æœ¬ç¯€é–‹é ­çœ‹åˆ°çš„é‚£æ¨£ï¼Œæœ€å¥½è¨­ç½®æ­£ç¢ºçš„æ¨™ç±¤å°æ‡‰é—œä¿‚ã€‚

å®ƒå€‘æ‡‰è©²ç”±å…©å€‹å­—å…¸è¨­ç½®ï¼Œ `id2label` å’Œ `label2id` ï¼Œå…¶ä¸­åŒ…å«å¾ ID åˆ°æ¨™ç±¤çš„æ˜ å°„ï¼Œåä¹‹äº¦ç„¶ï¼š

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç¾åœ¨æˆ‘å€‘å¯ä»¥å°‡å®ƒå€‘å‚³éçµ¦ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒå€‘å°‡åœ¨æ¨¡å‹çš„é…ç½®ä¸­è¨­ç½®ï¼Œç„¶å¾Œä¿å­˜ä¸¦ä¸Šå‚³åˆ°Hubï¼š

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3),å®šç¾©æˆ‘å€‘çš„ `AutoModelForSequenceClassification` ï¼Œå‰µå»ºæ¨¡å‹æœƒç™¼å‡ºè­¦å‘Šï¼Œæç¤ºä¸€äº›æ¬Šé‡æœªè¢«ä½¿ç”¨ï¼ˆä¾†è‡ªé è¨“ç·´é ­çš„æ¬Šé‡ï¼‰å’Œä¸€äº›å…¶ä»–æ¬Šé‡è¢«éš¨æ©Ÿåˆå§‹åŒ–ï¼ˆä¾†è‡ªæ–°Tokenåˆ†é¡é ­çš„æ¬Šé‡ï¼‰ï¼Œæˆ‘å€‘å°‡è¦è¨“ç·´é€™å€‹æ¨¡å‹ã€‚æˆ‘å€‘å°‡åœ¨ä¸€åˆ†é˜å…§å®Œæˆï¼Œä½†é¦–å…ˆè®“æˆ‘å€‘ä»”ç´°æª¢æŸ¥æˆ‘å€‘çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¢ºæ•¸é‡çš„æ¨™ç±¤ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœæ‚¨çš„æ¨¡å‹æ¨™ç±¤æ•¸é‡éŒ¯èª¤ï¼Œå‰‡åœ¨ç¨å¾Œèª¿ç”¨ `model.fit()` æ™‚å°‡æ”¶åˆ°ä¸€å€‹æ¨¡ç³Šçš„éŒ¯èª¤ã€‚èª¿è©¦èµ·ä¾†å¯èƒ½å¾ˆç…©äººï¼Œå› æ­¤è«‹ç¢ºä¿åŸ·è¡Œæ­¤æª¢æŸ¥ä»¥ç¢ºèªæ‚¨å…·æœ‰é æœŸçš„æ¨™ç±¤æ•¸ã€‚

</Tip>

### å¾®èª¿æ¨¡å‹

ç¾åœ¨ï¼Œæˆ‘å€‘å·²æº–å‚™å¥½è¨“ç·´æ¨¡å‹äº†ï¼ä¸éï¼Œæˆ‘å€‘é¦–å…ˆè¦åšå…©ä»¶äº‹ï¼šæ‡‰è©²ç™»éŒ„åˆ°Hugging Faceä¸¦å®šç¾©æˆ‘å€‘çš„è¨“ç·´è¶…åƒæ•¸ã€‚å¦‚æœä½ åœ¨notebookä¸Šå·¥ä½œï¼Œæœ‰ä¸€å€‹ä¾¿åˆ©åŠŸèƒ½å¯ä»¥å¹«åŠ©ä½ åšåˆ°é€™ä¸€é»ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

é€™å°‡é¡¯ç¤ºä¸€å€‹å°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¼¸å…¥æ‚¨çš„ Hugging Face è³¬è™Ÿå’Œå¯†ç¢¼ã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šå·¥ä½œï¼Œåªéœ€åœ¨çµ‚ç«¯ä¸­è¼¸å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

ç™»éŒ„å¾Œï¼Œæˆ‘å€‘å¯ä»¥æº–å‚™ç·¨è­¯æ¨¡å‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚ğŸ¤— Transformersæä¾›äº†ä¸€å€‹æ–¹ä¾¿çš„`create_optimizer()` å‡½æ•¸ï¼Œè©²å‡½æ•¸å°‡ç‚ºæ‚¨æä¾›ä¸€å€‹`AdamW`å„ªåŒ–å™¨ï¼Œå…¶ä¸­åŒ…å«é©ç•¶çš„æ¬Šé‡è¡°æ¸›å’Œå­¸ç¿’é€Ÿç‡è¡°æ¸›è¨­ç½®ï¼Œèˆ‡å…§ç½®çš„`Adam`å„ªåŒ–å™¨ç›¸ä¼¼ï¼Œé€™å…©è€…éƒ½å°‡æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼š

```python
from transformers import create_optimizer
import tensorflow as tf

# Train in mixed-precision float16
# Comment this line out if you're using a GPU that will not benefit from this
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

é‚„è¦æ³¨æ„ï¼Œæˆ‘å€‘ä¸ç‚º`compile()`æä¾›`loss`åƒæ•¸ã€‚é€™æ˜¯å› ç‚ºæ¨¡å‹å¯¦éš›ä¸Šå¯ä»¥åœ¨å…§éƒ¨è¨ˆç®—æå¤± - å¦‚æœæ‚¨ç·¨è­¯æ™‚æ²’æœ‰æå¤±ä¸¦åœ¨è¼¸å…¥å­—å…¸ä¸­æä¾›æ¨™ç±¤ï¼ˆå°±åƒæˆ‘å€‘åœ¨æ•¸æ“šé›†ä¸­æ‰€åšçš„é‚£æ¨£ï¼‰ï¼Œé‚£éº¼æ¨¡å‹å°‡ä½¿ç”¨è©²å…§éƒ¨æå¤±é€²è¡Œè¨“ç·´ï¼Œé€™å°‡é©ç”¨æ–¼æ‚¨é¸æ“‡çš„ä»»å‹™å’Œæ¨¡å‹é¡å‹ã€‚

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å®šç¾©ä¸€å€‹`PushToHubCallback`ï¼Œä»¥ä¾¿åœ¨è¨“ç·´æœŸé–“å°‡æ¨¡å‹ä¸Šå‚³åˆ° Hubï¼Œä¸¦ä½¿ç”¨è©²å›èª¿ä¾†æ“¬åˆæ¨¡å‹ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

æ‚¨ä¹‹å‰å·²ç¶“çœ‹éå…¶ä¸­çš„å¤§éƒ¨åˆ†å…§å®¹ï¼šæˆ‘å€‘è¨­ç½®äº†ä¸€äº›è¶…åƒæ•¸ï¼ˆä¾‹å¦‚å­¸ç¿’ç‡ã€è¦è¨“ç·´çš„ epoch æ•¸å’Œæ¬Šé‡è¡°æ¸›ï¼‰ï¼Œç„¶å¾Œæˆ‘å€‘æŒ‡å®š `push_to_hub=True` è¡¨æ˜æˆ‘å€‘æƒ³è¦ä¿å­˜æ¨¡å‹ä¸¦åœ¨æ¯å€‹æ™‚æœŸçµæŸæ™‚å°å…¶é€²è¡Œè©•ä¼°ï¼Œä¸¦ä¸”æˆ‘å€‘æƒ³è¦å°‡æˆ‘å€‘çš„çµæœä¸Šå‚³åˆ°æ¨¡å‹ä¸­å¿ƒã€‚è«‹æ³¨æ„ï¼Œå¯ä»¥ä½¿ç”¨hub_model_idåƒæ•¸æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å„²åº«çš„åç¨±(ç‰¹åˆ¥æ˜¯ï¼Œå¿…é ˆä½¿ç”¨é€™å€‹åƒæ•¸ä¾†æ¨é€åˆ°ä¸€å€‹çµ„ç¹”)ã€‚ä¾‹å¦‚ï¼Œç•¶æˆ‘å€‘å°‡æ¨¡å‹æ¨é€åˆ°[`huggingface-course` organization](https://huggingface.co/huggingface-course), æˆ‘å€‘æ·»åŠ äº† `hub_model_id=huggingface-course/bert-finetuned-ner` åˆ° `TrainingArguments` .é»˜èªæƒ…æ³ä¸‹ï¼Œä½¿ç”¨çš„å­˜å„²åº«å°‡åœ¨æ‚¨çš„å‘½åç©ºé–“ä¸­ä¸¦ä»¥æ‚¨è¨­ç½®çš„è¼¸å‡ºç›®éŒ„å‘½åï¼Œå› æ­¤åœ¨æˆ‘å€‘çš„ä¾‹å­ä¸­å®ƒå°‡æ˜¯ `sgugger/bert-finetuned-ner` .

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨çš„è¼¸å‡ºç›®éŒ„å·²ç¶“å­˜åœ¨ï¼Œé‚£éº¼è¼¸å‡ºç›®éŒ„å¿…é ˆæ˜¯å¾åŒä¸€å€‹å­˜å„²åº«cloneä¸‹ä¾†çš„ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°‡åœ¨è²æ˜ `model.fit()` æ™‚é‡åˆ°éŒ¯èª¤ï¼Œä¸¦ä¸”éœ€è¦è¨­ç½®ä¸€å€‹æ–°åç¨±ã€‚

</Tip>

è«‹æ³¨æ„ï¼Œç•¶è¨“ç·´ç™¼ç”Ÿæ™‚ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ™‚ï¼ˆé€™è£¡æ˜¯æ¯å€‹epoochï¼‰ï¼Œå®ƒéƒ½æœƒåœ¨å¾Œè‡ºä¸Šå‚³åˆ° Hubã€‚é€™æ¨£ï¼Œå¦‚æœ‰å¿…è¦ï¼Œæ‚¨å°‡èƒ½å¤ åœ¨å¦ä¸€è‡ºæ©Ÿå™¨ä¸Šç¹¼çºŒæ‚¨çš„è¨“ç·´ã€‚

åœ¨æ­¤éšæ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°çµ„ä»¶ä¾†æ¸¬è©¦æ¨¡å‹ä¸¦èˆ‡æœ‹å‹å…±äº«ã€‚æ‚¨å·²ç¶“æˆåŠŸå¾®èª¿äº†ä»¤ç‰Œåˆ†é¡ä»»å‹™çš„æ¨¡å‹ - æ­å–œï¼ä½†æ˜¯ï¼Œæˆ‘å€‘çš„æ¨¡å‹åˆ°åº•æœ‰å¤šå¥½å‘¢ï¼Ÿæˆ‘å€‘æ‡‰è©²è©•ä¼°ä¸€äº›æŒ‡æ¨™ä¾†æ‰¾å‡ºç­”æ¡ˆã€‚

{/if}


### è©•ä¼°æŒ‡æ¨™

{#if fw === 'pt'}

ç‚ºäº†è®“ `Trainer` åœ¨æ¯å€‹epochè¨ˆç®—ä¸€å€‹åº¦é‡ï¼Œæˆ‘å€‘éœ€è¦å®šç¾©ä¸€å€‹ `compute_metrics()` å‡½æ•¸ï¼Œè©²å‡½æ•¸æ¥å—é æ¸¬å’Œæ¨™ç±¤æ•¸çµ„ï¼Œä¸¦è¿”å›ä¸€å€‹åŒ…å«åº¦é‡åç¨±å’Œå€¼çš„å­—å…¸

ç”¨æ–¼è©•ä¼°Tokenåˆ†é¡é æ¸¬çš„å‚³çµ±æ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval). è¦ä½¿ç”¨æ­¤æŒ‡æ¨™ï¼Œæˆ‘å€‘é¦–å…ˆéœ€è¦å®‰è£seqevalåº«ï¼š

```py
!pip install seqeval
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥é€šéåŠ è¼‰å®ƒ `load_metric()` å‡½æ•¸å°±åƒæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)åšçš„é‚£æ¨£ï¼š

{:else}

ç”¨æ–¼è©•ä¼°Tokenåˆ†é¡é æ¸¬çš„å‚³çµ±æ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval). è¦ä½¿ç”¨æ­¤æŒ‡æ¨™ï¼Œæˆ‘å€‘é¦–å…ˆéœ€è¦å®‰è£seqevalåº«ï¼š

```py
!pip install seqeval
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥é€šéåŠ è¼‰å®ƒ `load_metric()` å‡½æ•¸å°±åƒæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)åšçš„é‚£æ¨£ï¼š

{/if}

```py
from datasets import load_metric

metric = load_metric("seqeval")
```

é€™å€‹è©•ä¼°æ–¹å¼èˆ‡æ¨™æº–ç²¾åº¦ä¸åŒ:å®ƒå¯¦éš›ä¸Šå°‡æ¨™ç±¤åˆ—è¡¨ä½œç‚ºå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æ•´æ•¸ï¼Œå› æ­¤åœ¨å°‡é æ¸¬å’Œæ¨™ç±¤å‚³éçµ¦å®ƒä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å®Œå…¨è§£ç¢¼å®ƒå€‘ã€‚è®“æˆ‘å€‘çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚é¦–å…ˆï¼Œæˆ‘å€‘å°‡ç²å¾—ç¬¬ä¸€å€‹è¨“ç·´ç¤ºä¾‹çš„æ¨™ç±¤:

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥é€šéæ›´æ”¹ç´¢å¼• 2 è™•çš„å€¼ä¾†ç‚ºé‚£äº›å‰µå»ºå‡çš„é æ¸¬ï¼š

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

è«‹æ³¨æ„ï¼Œè©²æŒ‡æ¨™çš„è¼¸å…¥æ˜¯é æ¸¬åˆ—è¡¨ï¼ˆä¸åƒ…åƒ…æ˜¯ä¸€å€‹ï¼‰å’Œæ¨™ç±¤åˆ—è¡¨ã€‚é€™æ˜¯è¼¸å‡ºï¼š

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

å®ƒè¿”å›å¾ˆå¤šä¿¡æ¯ï¼æˆ‘å€‘ç²å¾—æ¯å€‹å–®ç¨å¯¦é«”ä»¥åŠæ•´é«”çš„æº–ç¢ºç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•¸ã€‚å°æ–¼æˆ‘å€‘çš„åº¦é‡è¨ˆç®—ï¼Œæˆ‘å€‘å°‡åªä¿ç•™ç¸½åˆ†ï¼Œä½†å¯ä»¥éš¨æ„èª¿æ•´ `compute_metrics()` å‡½æ•¸è¿”å›æ‚¨æƒ³è¦æŸ¥çœ‹çš„æ‰€æœ‰æŒ‡æ¨™ã€‚

é€™`compute_metrics()` å‡½æ•¸é¦–å…ˆæ¡ç”¨ logits çš„ argmax å°‡å®ƒå€‘è½‰æ›ç‚ºé æ¸¬ï¼ˆåƒå¾€å¸¸ä¸€æ¨£ï¼Œlogits å’Œæ¦‚ç‡çš„é †åºç›¸åŒï¼Œå› æ­¤æˆ‘å€‘ä¸éœ€è¦æ‡‰ç”¨ softmaxï¼‰ã€‚ç„¶å¾Œæˆ‘å€‘å¿…é ˆå°‡æ¨™ç±¤å’Œé æ¸¬å¾æ•´æ•¸è½‰æ›ç‚ºå­—ç¬¦ä¸²ã€‚æˆ‘å€‘åˆªé™¤æ¨™ç±¤ç‚º `-100` æ‰€æœ‰å€¼ ï¼Œç„¶å¾Œå°‡çµæœå‚³éçµ¦ `metric.compute()` æ–¹æ³•ï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

ç¾åœ¨å·²ç¶“å®Œæˆäº†ï¼Œæˆ‘å€‘å¹¾ä¹æº–å‚™å¥½å®šç¾©æˆ‘å€‘çš„ `Trainer` .æˆ‘å€‘åªéœ€è¦ä¸€å€‹ `model` å¾®èª¿ï¼

{:else}

å®ƒè¿”å›å¾ˆå¤šä¿¡æ¯ï¼æˆ‘å€‘ç²å¾—æ¯å€‹å–®ç¨å¯¦é«”ä»¥åŠæ•´é«”çš„æº–ç¢ºç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•¸ã€‚å°æ–¼æˆ‘å€‘çš„åº¦é‡è¨ˆç®—ï¼Œæˆ‘å€‘å°‡åªä¿ç•™ç¸½åˆ†ï¼Œä½†å¯ä»¥éš¨æ„èª¿æ•´ `compute_metrics()` å‡½æ•¸è¿”å›æ‚¨æƒ³è¦æŸ¥çœ‹çš„æ‰€æœ‰æŒ‡æ¨™ã€‚

é€™`compute_metrics()` å‡½æ•¸é¦–å…ˆæ¡ç”¨ logits çš„ argmax å°‡å®ƒå€‘è½‰æ›ç‚ºé æ¸¬ï¼ˆåƒå¾€å¸¸ä¸€æ¨£ï¼Œlogits å’Œæ¦‚ç‡çš„é †åºç›¸åŒï¼Œå› æ­¤æˆ‘å€‘ä¸éœ€è¦æ‡‰ç”¨ softmaxï¼‰ã€‚ç„¶å¾Œæˆ‘å€‘å¿…é ˆå°‡æ¨™ç±¤å’Œé æ¸¬å¾æ•´æ•¸è½‰æ›ç‚ºå­—ç¬¦ä¸²ã€‚æˆ‘å€‘åˆªé™¤æ¨™ç±¤ç‚º `-100` æ‰€æœ‰å€¼ ï¼Œç„¶å¾Œå°‡çµæœå‚³éçµ¦ `metric.compute()` æ–¹æ³•ï¼š

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

èˆ‡æˆ‘å€‘çš„æ¨¡å‹ç›¸æ¯”ï¼Œæ‚¨çš„æ¨¡å‹åšå¾—å¦‚ä½•ï¼Ÿå¦‚æœæ‚¨ç²å¾—é¡ä¼¼çš„æ•¸å­—ï¼Œé‚£éº¼æ‚¨çš„è¨“ç·´å°±æ˜¯æˆåŠŸçš„ï¼

{/if}

{#if fw === 'pt'}

### å®šç¾©æ¨¡å‹

ç”±æ–¼æˆ‘å€‘æ­£åœ¨ç ”ç©¶Tokenåˆ†é¡å•é¡Œï¼Œå› æ­¤æˆ‘å€‘å°‡ä½¿ç”¨ `AutoModelForTokenClassification` é¡ã€‚å®šç¾©é€™å€‹æ¨¡å‹æ™‚è¦è¨˜ä½çš„ä¸»è¦äº‹æƒ…æ˜¯å‚³éä¸€äº›é—œæ–¼æˆ‘å€‘çš„æ¨™ç±¤æ•¸é‡çš„ä¿¡æ¯ã€‚åŸ·è¡Œæ­¤æ“ä½œçš„æœ€ç°¡å–®æ–¹æ³•æ˜¯å°‡è©²æ•¸å­—å‚³éçµ¦ `num_labels` åƒæ•¸ï¼Œä½†æ˜¯å¦‚æœæˆ‘å€‘æƒ³è¦ä¸€å€‹å¾ˆå¥½çš„æ¨ç†å°éƒ¨ä»¶ï¼Œå°±åƒæˆ‘å€‘åœ¨æœ¬ç¯€é–‹é ­çœ‹åˆ°çš„é‚£æ¨£ï¼Œæœ€å¥½è¨­ç½®æ­£ç¢ºçš„æ¨™ç±¤å°æ‡‰é—œä¿‚ã€‚

å®ƒå€‘æ‡‰è©²ç”±å…©å€‹å­—å…¸è¨­ç½®ï¼Œ `id2label` å’Œ `label2id` ï¼Œå…¶ä¸­åŒ…å«å¾ ID åˆ°æ¨™ç±¤çš„æ˜ å°„ï¼Œåä¹‹äº¦ç„¶ï¼š

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç¾åœ¨æˆ‘å€‘å¯ä»¥å°‡å®ƒå€‘å‚³éçµ¦ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒå€‘å°‡åœ¨æ¨¡å‹çš„é…ç½®ä¸­è¨­ç½®ï¼Œç„¶å¾Œä¿å­˜ä¸¦ä¸Šå‚³åˆ°Hubï¼š

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3),å®šç¾©æˆ‘å€‘çš„ `AutoModelForSequenceClassification` ï¼Œå‰µå»ºæ¨¡å‹æœƒç™¼å‡ºè­¦å‘Šï¼Œæç¤ºä¸€äº›æ¬Šé‡æœªè¢«ä½¿ç”¨ï¼ˆä¾†è‡ªé è¨“ç·´é ­çš„æ¬Šé‡ï¼‰å’Œä¸€äº›å…¶ä»–æ¬Šé‡è¢«éš¨æ©Ÿåˆå§‹åŒ–ï¼ˆä¾†è‡ªæ–°Tokenåˆ†é¡é ­çš„æ¬Šé‡ï¼‰ï¼Œæˆ‘å€‘å°‡è¦è¨“ç·´é€™å€‹æ¨¡å‹ã€‚æˆ‘å€‘å°‡åœ¨ä¸€åˆ†é˜å…§å®Œæˆï¼Œä½†é¦–å…ˆè®“æˆ‘å€‘ä»”ç´°æª¢æŸ¥æˆ‘å€‘çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¢ºæ•¸é‡çš„æ¨™ç±¤ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœæ¨¡å‹çš„æ¨™ç±¤æ•¸é‡éŒ¯èª¤ï¼Œç¨å¾Œèª¿ç”¨Trainer.train()æ–¹æ³•æ™‚æœƒå‡ºç¾ä¸€å€‹æ¨¡ç³Šçš„éŒ¯èª¤ï¼ˆé¡ä¼¼æ–¼â€œCUDA error: device-side assert triggeredâ€ï¼‰ã€‚é€™æ˜¯ç”¨æˆ¶å ±å‘Šæ­¤é¡éŒ¯èª¤çš„ç¬¬ä¸€å€‹åŸå› ï¼Œå› æ­¤è«‹ç¢ºä¿é€²è¡Œé€™æ¨£çš„æª¢æŸ¥ä»¥ç¢ºèªæ‚¨æ“æœ‰é æœŸæ•¸é‡çš„æ¨™ç±¤ã€‚

</Tip>

### å¾®èª¿æ¨¡å‹

æˆ‘å€‘ç¾åœ¨æº–å‚™å¥½è¨“ç·´æˆ‘å€‘çš„æ¨¡å‹äº†ï¼åœ¨å®šç¾©æˆ‘å€‘çš„ `Trainer`ä¹‹å‰ï¼Œæˆ‘å€‘åªéœ€è¦åšæœ€å¾Œå…©ä»¶äº‹ï¼šç™»éŒ„ Hugging Face ä¸¦å®šç¾©æˆ‘å€‘çš„è¨“ç·´åƒæ•¸ã€‚å¦‚æœæ‚¨åœ¨notebookä¸Šå·¥ä½œï¼Œæœ‰ä¸€å€‹æ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¹«åŠ©æ‚¨ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```
é€™å°‡é¡¯ç¤ºä¸€å€‹å°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¼¸å…¥æ‚¨çš„ Hugging Face è³¬è™Ÿå’Œå¯†ç¢¼ã€‚å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šå·¥ä½œï¼Œåªéœ€åœ¨çµ‚ç«¯ä¸­è¼¸å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

Once this is done, we can define our `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

æ‚¨ä¹‹å‰å·²ç¶“çœ‹éå…¶ä¸­çš„å¤§éƒ¨åˆ†å…§å®¹ï¼šæˆ‘å€‘è¨­ç½®äº†ä¸€äº›è¶…åƒæ•¸ï¼ˆä¾‹å¦‚å­¸ç¿’ç‡ã€è¦è¨“ç·´çš„ epoch æ•¸å’Œæ¬Šé‡è¡°æ¸›ï¼‰ï¼Œç„¶å¾Œæˆ‘å€‘æŒ‡å®š `push_to_hub=True` è¡¨æ˜æˆ‘å€‘æƒ³è¦ä¿å­˜æ¨¡å‹ä¸¦åœ¨æ¯å€‹æ™‚æœŸçµæŸæ™‚å°å…¶é€²è¡Œè©•ä¼°ï¼Œä¸¦ä¸”æˆ‘å€‘æƒ³è¦å°‡æˆ‘å€‘çš„çµæœä¸Šå‚³åˆ°æ¨¡å‹ä¸­å¿ƒã€‚è«‹æ³¨æ„ï¼Œå¯ä»¥ä½¿ç”¨hub_model_idåƒæ•¸æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å„²åº«çš„åç¨±(ç‰¹åˆ¥æ˜¯ï¼Œå¿…é ˆä½¿ç”¨é€™å€‹åƒæ•¸ä¾†æ¨é€åˆ°ä¸€å€‹çµ„ç¹”)ã€‚ä¾‹å¦‚ï¼Œç•¶æˆ‘å€‘å°‡æ¨¡å‹æ¨é€åˆ°[`huggingface-course` organization](https://huggingface.co/huggingface-course), æˆ‘å€‘æ·»åŠ äº† `hub_model_id=huggingface-course/bert-finetuned-ner` åˆ° `TrainingArguments` ã€‚é»˜èªæƒ…æ³ä¸‹ï¼Œä½¿ç”¨çš„å­˜å„²åº«å°‡åœ¨æ‚¨çš„å‘½åç©ºé–“ä¸­ä¸¦ä»¥æ‚¨è¨­ç½®çš„è¼¸å‡ºç›®éŒ„å‘½åï¼Œå› æ­¤åœ¨æˆ‘å€‘çš„ä¾‹å­ä¸­å®ƒå°‡æ˜¯ `sgugger/bert-finetuned-ner`ã€‚

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨çš„è¼¸å‡ºç›®éŒ„å·²ç¶“å­˜åœ¨ï¼Œé‚£éº¼è¼¸å‡ºç›®éŒ„å¿…é ˆæ˜¯å¾åŒä¸€å€‹å­˜å„²åº«cloneä¸‹ä¾†çš„ã€‚å¦‚æœä¸æ˜¯ï¼Œæ‚¨å°‡åœ¨è²æ˜ `Trainer` æ™‚é‡åˆ°éŒ¯èª¤ï¼Œä¸¦ä¸”éœ€è¦è¨­ç½®ä¸€å€‹æ–°åç¨±ã€‚

</Tip>

æœ€å¾Œï¼Œæˆ‘å€‘åªæ˜¯å°‡æ‰€æœ‰å…§å®¹å‚³éçµ¦ `Trainer` ä¸¦å•Ÿå‹•è¨“ç·´ï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

è«‹æ³¨æ„ï¼Œç•¶è¨“ç·´ç™¼ç”Ÿæ™‚ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ™‚ï¼ˆé€™è£¡æ˜¯æ¯å€‹epoochï¼‰ï¼Œå®ƒéƒ½æœƒåœ¨å¾Œè‡ºä¸Šå‚³åˆ° Hubã€‚é€™æ¨£ï¼Œå¦‚æœ‰å¿…è¦ï¼Œæ‚¨å°‡èƒ½å¤ åœ¨å¦ä¸€è‡ºæ©Ÿå™¨ä¸Šç¹¼çºŒæ‚¨çš„è¨“ç·´ã€‚

è¨“ç·´å®Œæˆå¾Œï¼Œæˆ‘å€‘ä½¿ç”¨ `push_to_hub()` ç¢ºä¿æˆ‘å€‘ä¸Šå‚³æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬

```py
trainer.push_to_hub(commit_message="Training complete")
```

This command returns the URL of the commit it just did, if you want to inspect it:

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

é€™ `Trainer` é‚„å‰µå»ºäº†ä¸€å¼µåŒ…å«æ‰€æœ‰è©•ä¼°çµæœçš„æ¨¡å‹å¡ä¸¦ä¸Šå‚³ã€‚åœ¨æ­¤éšæ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶ä¾†æ¸¬è©¦æ‚¨çš„æ¨¡å‹ä¸¦èˆ‡æ‚¨çš„æœ‹å‹åˆ†äº«ã€‚æ‚¨å·²æˆåŠŸåœ¨Tokenåˆ†é¡ä»»å‹™ä¸Šå¾®èª¿æ¨¡å‹ - æ­å–œï¼

å¦‚æœæ‚¨æƒ³æ›´æ·±å…¥åœ°ç­è§£è¨“ç·´å¾ªç’°ï¼Œæˆ‘å€‘ç¾åœ¨å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ¨£çš„äº‹æƒ…ã€‚

## è‡ªå®šç¾©è¨“ç·´å¾ªç’°

ç¾åœ¨è®“æˆ‘å€‘çœ‹ä¸€ä¸‹å®Œæ•´çš„è¨“ç·´å¾ªç’°ï¼Œé€™æ¨£æ‚¨å¯ä»¥è¼•é¬†å®šç¾©æ‰€éœ€çš„éƒ¨åˆ†ã€‚å®ƒçœ‹èµ·ä¾†å¾ˆåƒæˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3/4), æ‰€åšçš„ï¼Œå°è©•ä¼°é€²è¡Œäº†ä¸€äº›æ›´æ”¹ã€‚

### åšå¥½è¨“ç·´å‰çš„æº–å‚™
é¦–å…ˆæˆ‘å€‘éœ€è¦ç‚ºæˆ‘å€‘çš„æ•¸æ“šé›†æ§‹å»º `DataLoader` ã€‚æˆ‘å€‘å°‡é‡ç”¨æˆ‘å€‘çš„ `data_collator` ä½œç‚ºä¸€å€‹ `collate_fn` ä¸¦æ‰“äº‚è¨“ç·´é›†ï¼Œä½†ä¸æ‰“äº‚é©—è­‰é›†ï¼š

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘é‡æ–°å¯¦ä¾‹åŒ–æˆ‘å€‘çš„æ¨¡å‹ï¼Œä»¥ç¢ºä¿æˆ‘å€‘ä¸æœƒå¾ä¹‹å‰çš„è¨“ç·´ç¹¼çºŒè¨“ç·´ï¼Œè€Œæ˜¯å†æ¬¡å¾ BERT é è¨“ç·´æ¨¡å‹é–‹å§‹ï¼š

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

ç„¶å¾Œæˆ‘å€‘å°‡éœ€è¦ä¸€å€‹å„ªåŒ–å™¨ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ç¶“å…¸ `AdamW` ï¼Œé€™å°±åƒ `Adam` ï¼Œä½†åœ¨æ‡‰ç”¨æ¬Šé‡è¡°æ¸›çš„æ–¹å¼ä¸Šé€²è¡Œäº†æ”¹é€²ï¼š
```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Once we have all those objects, we can send them to the `accelerator.prepare()` method:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœæ‚¨åœ¨ TPU ä¸Šé€²è¡Œè¨“ç·´ï¼Œå‰‡éœ€è¦å°‡ä»¥ä¸Šå–®å…ƒæ ¼ä¸­çš„æ‰€æœ‰ä»£ç¢¼ç§»å‹•åˆ°å°ˆç”¨çš„è¨“ç·´å‡½æ•¸ä¸­ã€‚æœ‰é—œè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–± [ç¬¬3ç« ](/course/chapter3)ã€‚

</Tip>

ç¾åœ¨æˆ‘å€‘å·²ç¶“ç™¼é€äº†æˆ‘å€‘çš„ `train_dataloader` åˆ° `accelerator.prepare()` ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨å®ƒçš„é•·åº¦ä¾†è¨ˆç®—è¨“ç·´æ­¥é©Ÿçš„æ•¸é‡ã€‚è«‹è¨˜ä½ï¼Œæˆ‘å€‘æ‡‰è©²å§‹çµ‚åœ¨æº–å‚™å¥½dataloaderå¾ŒåŸ·è¡Œæ­¤æ“ä½œï¼Œå› ç‚ºè©²æ–¹æ³•æœƒæ”¹è®Šå…¶é•·åº¦ã€‚æˆ‘å€‘ä½¿ç”¨ç¶“å…¸ç·šæ€§å­¸ç¿’ç‡èª¿åº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€å¾Œï¼Œè¦å°‡æˆ‘å€‘çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘å€‘éœ€è¦å‰µå»ºä¸€å€‹ `Repository` å·¥ä½œæ–‡ä»¶å¤¾ä¸­çš„å°è±¡ã€‚å¦‚æœæ‚¨å°šæœªç™»éŒ„ï¼Œè«‹å…ˆç™»éŒ„ Hugging Faceã€‚æˆ‘å€‘å°‡å¾æˆ‘å€‘æƒ³è¦ç‚ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¢ºå®šå­˜å„²åº«åç¨±ï¼ˆæ‚¨å¯ä»¥è‡ªç”±åœ°ç”¨è‡ªå·±çš„é¸æ“‡æ›¿æ› `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«æ‚¨çš„ç”¨æˆ¶åï¼Œå¯ä»¥ä½¿ç”¨`get_full_repo_name()`å‡½æ•¸çš„æŸ¥çœ‹ç›®å‰çš„repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.

### Training loop

### è¨“ç·´å¾ªç’°
æˆ‘å€‘ç¾åœ¨æº–å‚™ç·¨å¯«å®Œæ•´çš„è¨“ç·´å¾ªç’°ã€‚ç‚ºäº†ç°¡åŒ–å®ƒçš„è©•ä¼°éƒ¨åˆ†ï¼Œæˆ‘å€‘å®šç¾©äº†é€™å€‹ `postprocess()` æ¥å—é æ¸¬å’Œæ¨™ç±¤ä¸¦å°‡å®ƒå€‘è½‰æ›ç‚ºå­—ç¬¦ä¸²åˆ—è¡¨çš„å‡½æ•¸ï¼Œä¹Ÿå°±æ˜¯ `metric`å°è±¡éœ€è¦çš„è¼¸å…¥æ ¼å¼ï¼š

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥ç·¨å¯«è¨“ç·´å¾ªç’°ã€‚åœ¨å®šç¾©ä¸€å€‹é€²åº¦æ¢ä¾†è·Ÿè¹¤è¨“ç·´çš„é€²è¡Œå¾Œï¼Œå¾ªç’°åˆ†ç‚ºä¸‰å€‹éƒ¨åˆ†ï¼š

- è¨“ç·´æœ¬èº«ï¼Œé€™æ˜¯å°`train_dataloader`çš„ç¶“å…¸è¿­ä»£ï¼Œå‘å‰å‚³éæ¨¡å‹ï¼Œç„¶å¾Œåå‘å‚³éå’Œå„ªåŒ–åƒæ•¸
- è©•ä¼°,åœ¨ç²å¾—æˆ‘å€‘æ¨¡å‹çš„è¼¸å‡ºå¾Œ:å› ç‚ºå…©å€‹é€²ç¨‹å¯èƒ½å°‡è¼¸å…¥å’Œæ¨™ç±¤å¡«å……æˆä¸åŒçš„å½¢ç‹€,åœ¨èª¿ç”¨`gather()`æ–¹æ³•å‰æˆ‘å€‘éœ€è¦ä½¿ç”¨`accelerator.pad_across_processes()`ä¾†è®“é æ¸¬å’Œæ¨™ç±¤å½¢ç‹€ç›¸åŒã€‚å¦‚æœæˆ‘å€‘ä¸é€™æ¨£åšï¼Œè©•ä¼°è¦éº¼å‡ºéŒ¯ï¼Œè¦éº¼æ°¸é ä¸æœƒå¾—åˆ°çµæœã€‚ç„¶å¾Œï¼Œæˆ‘å€‘å°‡çµæœç™¼é€çµ¦`metric.add_batch()`ï¼Œä¸¦åœ¨è¨ˆç®—å¾ªç’°çµæŸå¾Œèª¿ç”¨`metric.compute()`ã€‚
- ä¿å­˜å’Œä¸Šå‚³ï¼Œé¦–å…ˆä¿å­˜æ¨¡å‹å’Œæ¨™è¨˜å™¨ï¼Œç„¶å¾Œèª¿ç”¨`repo.push_to_hub()`ã€‚æ³¨æ„ï¼Œæˆ‘å€‘ä½¿ç”¨åƒæ•¸`blocking=False`å‘Šè¨´ğŸ¤— hub åº«ç”¨åœ¨ç•°æ­¥é€²ç¨‹ä¸­æ¨é€ã€‚é€™æ¨£ï¼Œè¨“ç·´å°‡æ­£å¸¸ç¹¼çºŒï¼Œä¸¦ä¸”è©²ï¼ˆé•·ï¼‰æŒ‡ä»¤å°‡åœ¨å¾Œè‡ºåŸ·è¡Œã€‚

é€™æ˜¯è¨“ç·´å¾ªç’°çš„å®Œæ•´ä»£ç¢¼ï¼š

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

æœé€™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡çœ‹åˆ°ç”¨ ğŸ¤— Accelerate ä¿å­˜çš„æ¨¡å‹ï¼Œè®“æˆ‘å€‘èŠ±é»æ™‚é–“æª¢æŸ¥ä¸€ä¸‹å®ƒé™„å¸¶çš„ä¸‰è¡Œä»£ç¢¼ï¼š

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

ç¬¬ä¸€è¡Œæ˜¯ä¸è¨€è‡ªæ˜çš„ï¼šå®ƒå‘Šè¨´æ‰€æœ‰é€²ç¨‹ç­‰åˆ°éƒ½è™•æ–¼é‚£å€‹éšæ®µå†ç¹¼çºŒ(é˜»å¡)ã€‚é€™æ˜¯ç‚ºäº†ç¢ºä¿åœ¨ä¿å­˜ä¹‹å‰ï¼Œæˆ‘å€‘åœ¨æ¯å€‹éç¨‹ä¸­éƒ½æœ‰ç›¸åŒçš„æ¨¡å‹ã€‚ç„¶å¾Œç²å–`unwrapped_model`ï¼Œå®ƒæ˜¯æˆ‘å€‘å®šç¾©çš„åŸºæœ¬æ¨¡å‹ã€‚
`accelerator.prepare()`æ–¹æ³•å°‡æ¨¡å‹æ›´æ”¹ç‚ºåœ¨åˆ†ä½ˆå¼è¨“ç·´ä¸­å·¥ä½œï¼Œæ‰€ä»¥å®ƒä¸å†æœ‰`save_pretraining()`æ–¹æ³•;`accelerator.unwrap_model()`æ–¹æ³•å°‡æ’¤éŠ·è©²æ­¥é©Ÿã€‚æœ€å¾Œï¼Œæˆ‘å€‘èª¿ç”¨`save_pretraining()`ï¼Œä½†å‘Šè¨´è©²æ–¹æ³•ä½¿ç”¨`accelerator.save()`è€Œä¸æ˜¯`torch.save()`ã€‚ 

ç•¶å®Œæˆä¹‹å¾Œï¼Œä½ æ‡‰è©²æœ‰ä¸€å€‹æ¨¡å‹ï¼Œå®ƒç”¢ç”Ÿçš„çµæœèˆ‡`Trainer`çš„çµæœéå¸¸ç›¸ä¼¼ã€‚ä½ å¯ä»¥åœ¨[hugs face-course/bert-fine - tuning -ner-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate)ä¸­æŸ¥çœ‹æˆ‘å€‘ä½¿ç”¨é€™å€‹ä»£ç¢¼è¨“ç·´çš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³æ¸¬è©¦è¨“ç·´å¾ªç’°çš„ä»»ä½•èª¿æ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šéç·¨è¼¯ä¸Šé¢é¡¯ç¤ºçš„ä»£ç¢¼ä¾†å¯¦ç¾å®ƒå€‘!

{/if}

## ä½¿ç”¨å¾®èª¿æ¨¡å‹

æˆ‘å€‘å·²ç¶“å‘æ‚¨å±•ç¤ºç­å¦‚ä½•ä½¿ç”¨æˆ‘å€‘åœ¨æ¨¡å‹ä¸­å¿ƒå¾®èª¿çš„æ¨¡å‹å’Œæ¨ç†å°éƒ¨ä»¶ã€‚åœ¨æœ¬åœ°ä½¿ç”¨å®ƒ `pipeline` ï¼Œæ‚¨åªéœ€è¦æŒ‡å®šæ­£ç¢ºçš„æ¨¡å‹æ¨™è­˜ç¬¦ï¼š

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

å¤ªæ£’äº†ï¼æˆ‘å€‘çš„æ¨¡å‹èˆ‡æ­¤ç®¡é“çš„é»˜èªæ¨¡å‹ä¸€æ¨£æœ‰æ•ˆï¼
