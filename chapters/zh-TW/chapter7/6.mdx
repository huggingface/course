<FrameworkSwitchCourse {fw} />

# å¾é ­é–‹å§‹è¨“ç·´å› æœèªè¨€æ¨¡å‹

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section6_tf.ipynb"},
]} />

{/if}

åˆ°ç›®å‰ç‚ºæ­¢ï¼Œæˆ‘å€‘ä¸»è¦ä½¿ç”¨é è¨“ç·´æ¨¡å‹ï¼Œä¸¦é€šéé‡ç”¨é è¨“ç·´çš„æ¬Šé‡ä¾†é‡å°æ–°ç”¨ä¾‹å°å®ƒå€‘é€²è¡Œå¾®èª¿ã€‚æ­£å¦‚æˆ‘å€‘åœ¨[ç¬¬ä¸€ç« ](/course/chapter1), é€™é€šå¸¸ç¨±ç‚ºé·ç§»å­¸ç¿’ï¼Œé€™æ˜¯å°‡ Transformer æ¨¡å‹æ‡‰ç”¨æ–¼å¤§å¤šæ•¸æ¨™è¨˜æ•¸æ“šç¨€ç–çš„ç¾å¯¦ä¸–ç•Œç”¨ä¾‹çš„éå¸¸æˆåŠŸçš„ç­–ç•¥ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘å€‘å°‡æ¡ç”¨ä¸åŒçš„æ–¹æ³•ä¸¦å¾é ­é–‹å§‹è¨“ç·´ä¸€å€‹å…¨æ–°çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨æœ‰å¤§é‡æ•¸æ“šä¸¦ä¸”å®ƒèˆ‡ç”¨æ–¼å¯ç”¨æ¨¡å‹çš„é è¨“ç·´æ•¸æ“šæœ‰å¾ˆå¤§ä¸åŒï¼Œé‚£éº¼é€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä¹Ÿéœ€è¦æ›´å¤šçš„è¨ˆç®—è³‡æºä¾†é è¨“ç·´èªè¨€æ¨¡å‹ï¼Œè€Œä¸åƒ…åƒ…æ˜¯å¾®èª¿ç¾æœ‰çš„æ¨¡å‹ã€‚è¨“ç·´æ–°æ¨¡å‹æœ‰æ„ç¾©çš„ç¤ºä¾‹åŒ…æ‹¬ç”±éŸ³ç¬¦ã€åˆ†å­åºåˆ—ï¼ˆå¦‚ DNAï¼‰æˆ–ç·¨ç¨‹èªè¨€çµ„æˆçš„æ•¸æ“šé›†ã€‚å¾Œè€…æœ€è¿‘å—åˆ°é—œæ³¨ï¼Œé€™è¦æ­¸åŠŸæ–¼ TabNine å’Œ GitHub çš„ Copilot ç­‰å·¥å…·ï¼Œå®ƒå€‘ç”± OpenAI çš„ Codex æ¨¡å‹æä¾›æ”¯æŒï¼Œå¯ä»¥ç”Ÿæˆé•·ä»£ç¢¼åºåˆ—ã€‚é€™ç¨®æ–‡æœ¬ç”Ÿæˆä»»å‹™æœ€å¥½ä½¿ç”¨è‡ªè¿´æ­¸æˆ–å› æœèªè¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰ä¾†è§£æ±ºã€‚

åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡æ§‹å»ºä»£ç¢¼ç”Ÿæˆæ¨¡å‹çš„ç¸®å°ç‰ˆæœ¬ï¼šæˆ‘å€‘å°‡ä½¿ç”¨ Python ä»£ç¢¼çš„å­é›†å°ˆæ³¨æ–¼å–®è¡Œå®Œæˆè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•¸æˆ–é¡ã€‚åœ¨ Python ä¸­è™•ç†æ•¸æ“šæ™‚ï¼Œæ‚¨æœƒç¶“å¸¸æ¥è§¸ Python æ•¸æ“šç§‘å­¸å †æ£§ï¼ŒåŒ…æ‹¬ `matplotlib` , `seaborn` , `pandas` ï¼Œ å’Œ `scikit-learn` åº«ã€‚åœ¨ä½¿ç”¨é€™äº›æ¡†æ¶æ™‚ï¼Œé€šå¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘å€‘å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¾†ç‚ºæˆ‘å€‘å®Œæˆé€™äº›èª¿ç”¨ï¼Œé‚£å°±å¤ªå¥½äº†ã€‚

<Youtube id="Vpjb1lu0MDk"/>

åœ¨[ç¬¬å…­ç« ](/course/chapter6) æˆ‘å€‘å‰µå»ºäº†ä¸€å€‹é«˜æ•ˆçš„åˆ†è©å™¨ä¾†è™•ç† Python æºä»£ç¢¼ï¼Œä½†æˆ‘å€‘ä»ç„¶éœ€è¦ä¸€å€‹å¤§è¦æ¨¡æ•¸æ“šé›†ä¾†é è¨“ç·´æ¨¡å‹ã€‚åœ¨é€™è£¡ï¼Œæˆ‘å€‘å°‡æˆ‘å€‘çš„åˆ†è©å™¨æ‡‰ç”¨åˆ°æºè‡ª GitHub å­˜å„²åº«çš„ Python ä»£ç¢¼èªæ–™åº«ã€‚ç„¶å¾Œæˆ‘å€‘å°‡ä½¿ç”¨ `Trainer` API å’Œ ğŸ¤— Accelerate ä¾†è¨“ç·´æ¨¡å‹ã€‚è®“æˆ‘å€‘é–‹å§‹å§ï¼

<iframe src="https://course-demos-codeparrot-ds.hf.space" frameBorder="0" height="300" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-codeparrot-ds-darkmode.hf.space" frameBorder="0" height="300" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

é€™å¯¦éš›ä¸Šå±•ç¤ºäº†ä½¿ç”¨æœ¬ç¯€ä¸­è¨“ç·´ä¸¦ä¸Šå‚³åˆ° Hub çš„æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨[é€™è£¡](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28)æ‰¾åˆ°ã€‚è«‹æ³¨æ„ï¼Œç”±æ–¼åœ¨æ–‡æœ¬ç”Ÿæˆéç¨‹ä¸­ç™¼ç”Ÿäº†ä¸€äº›éš¨æ©ŸåŒ–ï¼Œæ‚¨å¯èƒ½æœƒå¾—åˆ°ç•¥æœ‰ä¸åŒçš„çµæœã€‚
## æ”¶é›†æ•¸æ“š

Python ä»£ç¢¼å¯ä»¥å¾ GitHub ç­‰ä»£ç¢¼å­˜å„²åº«ä¸­ç²å¾—ï¼Œæˆ‘å€‘å¯ä»¥é€šéæŠ“å–æ¯å€‹ Python å­˜å„²åº«ä¾†ä½¿ç”¨å®ƒå€‘ä¾†å‰µå»ºæ•¸æ“šé›†ã€‚é€™æ˜¯åœ¨[Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/)é è¨“ç·´å¤§å‹çš„GPT-2 æ¨¡å‹ã€‚ä½¿ç”¨å¤§ç´„ 180 GB çš„ GitHub è½‰å„²ï¼Œå…¶ä¸­åŒ…å«å¤§ç´„ 2000 è¬å€‹ Python æ–‡ä»¶ï¼Œç¨±ç‚º `codeparrot` ï¼Œä½œè€…æ§‹å»ºäº†ä¸€å€‹æ•¸æ“šé›†ï¼Œç„¶å¾Œåœ¨[Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot)ä¸Šåˆ†äº«å‡ºä¾†äº†.

ç„¶è€Œï¼Œå°å®Œæ•´èªæ–™åº«çš„è¨“ç·´æ—¢è€—æ™‚åˆè²»åŠ›ï¼Œæˆ‘å€‘åªéœ€è¦èˆ‡ Python æ•¸æ“šç§‘å­¸å †æ£§ç›¸é—œçš„æ•¸æ“šé›†å­é›†ã€‚æ‰€ä»¥ï¼Œè®“æˆ‘å€‘é–‹å§‹éæ¿¾ `codeparrot` åŒ…å«æ­¤å †æ£§ä¸­ä»»ä½•åº«çš„æ‰€æœ‰æ–‡ä»¶çš„æ•¸æ“šé›†ã€‚ç”±æ–¼æ•¸æ“šé›†çš„å¤ªå¤§ï¼Œæˆ‘å€‘å¸Œæœ›é¿å…ä¸‹è¼‰å®ƒï¼›å› æ­¤åï¼Œæˆ‘å€‘å°‡ä½¿ç”¨æµåŠŸèƒ½ä¾†å‹•æ…‹éæ¿¾å®ƒã€‚ç‚ºäº†ä½¿ç”¨å‰é¢æåˆ°çš„åº«éæ¿¾ä»£ç¢¼ç¤ºä¾‹ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ä»¥ä¸‹å‡½æ•¸ï¼š

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

è®“æˆ‘å€‘ç”¨å…©å€‹ä¾‹å­ä¾†æ¸¬è©¦ä¸€ä¸‹ï¼š

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

æˆ‘å€‘å¯ä»¥ä½¿ç”¨å®ƒä¾†å‰µå»ºä¸€å€‹å‡½æ•¸ä¾†æµå¼å‚³è¼¸æ•¸æ“šé›†ä¸¦éæ¿¾æˆ‘å€‘æƒ³è¦çš„å…ƒç´ ï¼š

```py
def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥ç°¡å–®åœ°å°‡æ­¤å‡½æ•¸æ‡‰ç”¨æ–¼æµæ•¸æ“šé›†ï¼š

```py
# This cell will take a very long time to execute, so you should skip it and go to
# the next one!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```

é€™çµ¦æˆ‘å€‘ç•™ä¸‹äº†å¤§ç´„ 3% çš„åŸå§‹æ•¸æ“šé›†ï¼Œé€™å€‹æ•¸æ“šé›†ä»ç„¶ç›¸ç•¶å¯è§€â€”â€”çµæœæ•¸æ“šé›†æœ‰ 6 GBï¼ŒåŒ…å« 600,000 å€‹ Python è…³æœ¬ï¼éæ¿¾å®Œæ•´æ•¸æ“šé›†å¯èƒ½éœ€è¦ 2-3 å°æ™‚ï¼Œå…·é«”å–æ±ºæ–¼æ‚¨çš„æ©Ÿå™¨å’Œå¸¶å¯¬ã€‚å¦‚æœæ‚¨ä¸æƒ³è‡ªå·±ç¶“æ­·é€™å€‹æ¼«é•·çš„éç¨‹ï¼Œæˆ‘å€‘åœ¨ Hub ä¸Šæä¾›éæ¿¾å¾Œçš„æ•¸æ“šé›†ä¾›æ‚¨ä¸‹è¼‰ï¼š

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="train")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<Tip>

é è¨“ç·´èªè¨€æ¨¡å‹éœ€è¦ä¸€æ®µæ™‚é–“ã€‚æˆ‘å€‘å»ºè­°æ‚¨é¦–å…ˆé€šéå–æ¶ˆè¨»é‡‹ä»¥ä¸Šå…©è¡Œçš„è¨»é‡‹å°æ•¸æ“šæ¨£æœ¬é‹è¡Œè¨“ç·´å¾ªç’°ï¼Œä¸¦ç¢ºä¿è¨“ç·´æˆåŠŸå®Œæˆä¸¦å­˜å„²æ¨¡å‹ã€‚æ²’æœ‰ä»€éº¼æ¯”æœ€å¾Œä¸€æ­¥çš„è¨“ç·´å¤±æ•—æ›´ä»¤äººæ²®å–ªçš„äº†ï¼Œå› ç‚ºä½ å¿˜è¨˜å‰µå»ºä¸€å€‹æ–‡ä»¶å¤¾æˆ–è€…å› ç‚ºä¿å­˜è·¯å¾‘åœ¨è¨“ç·´å¾ªç’°çµæŸæ™‚æœ‰ä¸€å€‹éŒ¯å­—ï¼

</Tip>

è®“æˆ‘å€‘çœ‹ä¸€å€‹ä¾†è‡ªæ•¸æ“šé›†çš„ä¾‹å­ã€‚æˆ‘å€‘å°‡åªé¡¯ç¤ºæ¯å€‹å­—æ®µçš„å‰ 200 å€‹å­—ç¬¦ï¼š

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

æˆ‘å€‘å¯ä»¥çœ‹åˆ° `content` å­—æ®µåŒ…å«æˆ‘å€‘å¸Œæœ›æˆ‘å€‘çš„æ¨¡å‹è¨“ç·´çš„ä»£ç¢¼ã€‚ç¾åœ¨æˆ‘å€‘æœ‰äº†ä¸€å€‹æ•¸æ“šé›†ï¼Œæˆ‘å€‘éœ€è¦é è™•ç†æ–‡æœ¬ï¼Œä½¿å…¶æ¡ç”¨é©åˆé è¨“ç·´çš„æ ¼å¼ã€‚

## æº–å‚™æ•¸æ“šé›†

<Youtube id="ma1TrR7gE7I"/>

ç¬¬ä¸€æ­¥æ˜¯å°æ•¸æ“šé€²è¡Œæ¨™è¨˜ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥å°‡å…¶ç”¨æ–¼è¨“ç·´ã€‚ç”±æ–¼æˆ‘å€‘çš„ç›®æ¨™ä¸»è¦æ˜¯è‡ªå‹•å®ŒæˆçŸ­å‡½æ•¸èª¿ç”¨ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥ä¿æŒä¸Šä¸‹æ–‡å¤§å°ç›¸å°è¼ƒå°ã€‚é€™æ¨£åšçš„å¥½è™•æ˜¯æˆ‘å€‘å¯ä»¥æ›´å¿«åœ°è¨“ç·´æ¨¡å‹ä¸¦ä¸”å®ƒéœ€è¦çš„å…§å­˜é¡¯è‘—æ¸›å°‘ã€‚å¦‚æœæ‚¨çš„æ‡‰ç”¨ç¨‹åºæ“æœ‰æ›´å¤šä¸Šä¸‹æ–‡å¾ˆé‡è¦ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å¸Œæœ›æ¨¡å‹åŸºæ–¼å…·æœ‰å‡½æ•¸å®šç¾©çš„æ–‡ä»¶ç·¨å¯«å–®å…ƒæ¸¬è©¦ï¼‰ï¼Œè«‹ç¢ºä¿å¢åŠ è©²æ•¸é‡ï¼Œä½†è«‹è¨˜ä½ï¼Œé€™éœ€è¦æ›´å¤§çš„ GPU å…§å­˜ä½”ç”¨ã€‚ç¾åœ¨ï¼Œè®“æˆ‘å€‘å°‡ä¸Šä¸‹æ–‡å¤§å°å›ºå®šç‚º 128 å€‹æ¨™è¨˜ï¼Œè€Œä¸æ˜¯ GPT-2 æˆ– GPT-3 ä¸­åˆ†åˆ¥ä½¿ç”¨çš„ 1,024 æˆ– 2,048 å€‹æ¨™è¨˜ã€‚


å¤§å¤šæ•¸æ–‡æª”åŒ…å«è¶…é 128 å€‹æ¨™è¨˜ï¼Œå› æ­¤ç°¡å–®åœ°å°‡è¼¸å…¥æˆªæ–·åˆ°æœ€å¤§é•·åº¦å°‡æ¶ˆé™¤æˆ‘å€‘æ•¸æ“šé›†çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç›¸åï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ `return_overflowing_tokens` æ¨™è¨˜æ•´å€‹è¼¸å…¥ä¸¦å°‡å…¶åˆ†æˆå¹¾å€‹å¡Šçš„é¸é …ï¼Œå°±åƒæˆ‘å€‘åœ¨[ç¬¬å…­ç« ](/course/chapter6/4). æˆ‘å€‘é‚„å°‡ä½¿ç”¨ `return_length` é¸é …è‡ªå‹•è¿”å›æ¯å€‹å‰µå»ºçš„å¡Šçš„é•·åº¦ã€‚é€šå¸¸æœ€å¾Œä¸€å€‹å¡Šæœƒå°æ–¼ä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘å€‘æœƒå»æ‰é€™äº›å¡Šä»¥é¿å…å¡«å……å•é¡Œï¼›å› ç‚ºç„¡è«–å¦‚ä½•æˆ‘å€‘éƒ½æœ‰å¤§é‡æ•¸æ“šã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg" alt="Chunking a large texts in several pieces."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg" alt="Chunking a large texts in several pieces."/>
</div>

è®“æˆ‘å€‘é€šéæŸ¥çœ‹å‰å…©å€‹ç¤ºä¾‹ä¾†ç¢ºåˆ‡ç­è§£é€™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘å€‘å¯ä»¥çœ‹ åˆ°ï¼Œå¾é€™å…©å€‹ç¤ºä¾‹ä¸­æˆ‘å€‘ç¸½å…±å¾—åˆ°äº† 34 å€‹ç‰‡æ®µã€‚æŸ¥çœ‹å¡Šé•·åº¦ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°å…©å€‹æ–‡æª”æœ«å°¾çš„å¡Šéƒ½å°‘æ–¼ 128 å€‹æ¨™è¨˜ï¼ˆåˆ†åˆ¥ç‚º 117 å’Œ 41ï¼‰ã€‚é€™äº›åƒ…ä»£è¡¨æˆ‘å€‘æ“æœ‰çš„æ•¸æ“šé›†çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥å®‰å…¨åœ°å°‡å®ƒå€‘æ‰”æ‰ã€‚é€šé `overflow_to_sample_mapping` å­—æ®µï¼Œæˆ‘å€‘é‚„å¯ä»¥é‡å»ºå“ªäº›å¡Šå±¬æ–¼å“ªäº›è¼¸å…¥æ¨£æœ¬ã€‚

é€šéé€™å€‹æ“ä½œï¼Œæˆ‘å€‘ä½¿ç”¨äº†ä¸€å€‹æ–¹ä¾¿çš„ğŸ¤— Datasets ä¸­çš„` Dataset.map()` å‡½æ•¸ï¼Œå°±æ˜¯ä¸éœ€è¦ä¸€å°ä¸€çš„æ˜ å°„ï¼›æ­£å¦‚æˆ‘å€‘åœ¨[ç¬¬ä¸‰ç¯€](/course/chapter7/3),æˆ‘å€‘å¯ä»¥å‰µå»ºå…·æœ‰æ¯”è¼¸å…¥æ‰¹æ¬¡æ›´å¤šæˆ–æ›´å°‘å…ƒç´ çš„æ‰¹æ¬¡ã€‚é€™åœ¨åŸ·è¡Œæ›´æ”¹å…ƒç´ æ•¸é‡çš„æ•¸æ“šå¢å¼·æˆ–æ•¸æ“šéæ¿¾ç­‰æ“ä½œæ™‚éå¸¸æœ‰ç”¨ã€‚åœ¨æˆ‘å€‘çš„ä¾‹å­ä¸­ï¼Œç•¶å°‡æ¯å€‹å…ƒç´ æ¨™è¨˜ç‚ºæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å¡Šæ™‚ï¼Œæˆ‘å€‘å¾æ¯å€‹æ–‡æª”ä¸­å‰µå»ºäº†è¨±å¤šæ¨£æœ¬ã€‚æˆ‘å€‘åªéœ€è¦ç¢ºä¿åˆªé™¤ç¾æœ‰çš„åˆ—ï¼Œå› ç‚ºå®ƒå€‘çš„å¤§å°å­˜åœ¨è¡çªã€‚å¦‚æœæˆ‘å€‘æƒ³ä¿ç•™å®ƒå€‘ï¼Œæˆ‘å€‘å¯ä»¥é©ç•¶åœ°é‡è¤‡å®ƒå€‘ï¼Œä¸¦åœ¨`Dataset.map()` èª¿ç”¨ä¸­è¿”å›å®ƒå€‘:

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

æˆ‘å€‘ç¾åœ¨æœ‰ 1670 è¬å€‹ç¤ºä¾‹ï¼Œæ¯å€‹ç¤ºä¾‹æœ‰ 128 å€‹tokens ï¼Œç¸½å…±ç›¸ç•¶æ–¼å¤§ç´„ 21 å„„å€‹tokens ã€‚ä½œç‚ºåƒè€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ¥åœ¨ 300 å’Œ 1000 å„„å€‹tokens ä¸Šè¨“ç·´ï¼Œå…¶ä¸­ Codex æ¨¡å‹å¾ GPT-3 æª¢æŸ¥é»åˆå§‹åŒ–ã€‚æˆ‘å€‘åœ¨æœ¬ç¯€ä¸­çš„ç›®æ¨™ä¸æ˜¯èˆ‡é€™äº›æ¨¡å‹ç«¶çˆ­ï¼Œé€™äº›æ¨¡å‹å¯ä»¥ç”Ÿæˆé•·è€Œé€£è²«çš„æ–‡æœ¬ï¼Œè€Œæ˜¯å‰µå»ºä¸€å€‹ç¸®å°ç‰ˆæœ¬ï¼Œç‚ºæ•¸æ“šç§‘å­¸å®¶æä¾›å¿«é€Ÿè‡ªå‹•å®ŒæˆåŠŸèƒ½ã€‚

ç¾åœ¨æˆ‘å€‘å·²ç¶“æº–å‚™å¥½äº†æ•¸æ“šé›†ï¼Œè®“æˆ‘å€‘è¨­ç½®æ¨¡å‹ï¼


<Tip>

âœï¸ **è©¦è©¦çœ‹ï¼** æ“ºè„«æ‰€æœ‰å°æ–¼ä¸Šä¸‹æ–‡å¤§å°çš„å¡Šåœ¨é€™è£¡ä¸¦ä¸æ˜¯ä»€éº¼å¤§å•é¡Œï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨çš„æ˜¯å°ä¸Šä¸‹æ–‡çª—å£ã€‚éš¨è‘—ä¸Šä¸‹æ–‡å¤§å°çš„å¢åŠ ï¼ˆæˆ–è€…å¦‚æœæ‚¨æœ‰ä¸€å€‹çŸ­æ–‡æª”èªæ–™åº«ï¼‰ï¼Œè¢«ä¸Ÿæ£„çš„å¡Šçš„æ¯”ä¾‹ä¹Ÿæœƒå¢åŠ ã€‚æº–å‚™æ•¸æ“šçš„æ›´æœ‰æ•ˆæ–¹æ³•æ˜¯å°‡æ‰€æœ‰æ¨™è¨˜åŒ–çš„æ¨£æœ¬åŠ å…¥ä¸€å€‹æ‰¹æ¬¡ä¸­ï¼Œæ¯å€‹èªæ–™ä¹‹é–“æœ‰ä¸€å€‹`eos_token_id` æ¨™è¨˜, ç„¶å¾Œå°é€£æ¥çš„åºåˆ—åŸ·è¡Œåˆ†å¡Šã€‚ä½œç‚ºç·´ç¿’ï¼Œä¿®æ”¹ `tokenize()`å‡½æ•¸ä»¥ä½¿ç”¨è©²æ–¹æ³•ã€‚è«‹æ³¨æ„ï¼Œæ‚¨éœ€è¦è¨­ç½®`truncation=False` å’Œåˆªé™¤æ¨™è¨˜ç”Ÿæˆå™¨ä¸­çš„å…¶ä»–åƒæ•¸ä»¥ç²å–å®Œæ•´çš„æ¨™è¨˜ ID åºåˆ—ã€‚

</Tip>


## åˆå§‹åŒ–æ–°æ¨¡å‹

æˆ‘å€‘çš„ç¬¬ä¸€æ­¥æ˜¯æ–°åˆå§‹åŒ–ä¸€å€‹ GPT-2 æ¨¡å‹ã€‚æˆ‘å€‘å°‡å°æˆ‘å€‘çš„æ¨¡å‹ä½¿ç”¨èˆ‡å°å‹ GPT-2 æ¨¡å‹ç›¸åŒçš„é…ç½®ï¼Œå› æ­¤æˆ‘å€‘åŠ è¼‰é è¨“ç·´é…ç½®ï¼Œç¢ºä¿åˆ†è©å™¨å¤§å°èˆ‡æ¨¡å‹è©å½™é‡å¤§å°åŒ¹é…ä¸¦è¨­ç½® `bos` å’Œ `eos` ï¼ˆåºåˆ—çš„é–‹å§‹å’ŒçµæŸï¼‰ä»¤ç‰Œ IDï¼š

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

ä½¿ç”¨è©²é…ç½®ï¼Œæˆ‘å€‘å¯ä»¥åŠ è¼‰ä¸€å€‹æ–°æ¨¡å‹ã€‚è«‹æ³¨æ„ï¼Œé€™æ˜¯æˆ‘å€‘ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•¸ï¼Œå› ç‚ºæˆ‘å€‘å¯¦éš›ä¸Šæ˜¯åœ¨è‡ªå·±åˆå§‹åŒ–æ¨¡å‹

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

é€šéè©²é…ç½®ï¼Œæˆ‘å€‘å¯ä»¥åŠ è¼‰æ–°æ¨¡å‹ã€‚è«‹æ³¨æ„ï¼Œé€™æ˜¯æˆ‘å€‘å‰›é–‹å§‹ä¸ä½¿ç”¨`from_pretrained()`å‡½æ•¸ï¼Œå› ç‚ºæˆ‘å€‘å¯¦éš›ä¸Šæ˜¯åœ¨è‡ªå·±åˆå§‹åŒ–æ¨¡å‹ï¼š

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

æˆ‘å€‘çš„æ¨¡å‹æœ‰ 1.24 å„„å€‹åƒæ•¸ï¼Œæˆ‘å€‘å¿…é ˆå°å…¶é€²è¡Œèª¿æ•´ã€‚åœ¨é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦è¨­ç½®ä¸€å€‹è² è²¬å‰µå»ºæ‰¹æ¬¡çš„æ•¸æ“šæ•´ç†å™¨ã€‚æˆ‘å€‘å¯ä»¥ä½¿ç”¨ `DataCollatorForLanguageModeling` ï¼Œå®ƒæ˜¯å°ˆç‚ºèªè¨€å»ºæ¨¡è€Œè¨­è¨ˆï¼ˆé¡§åæ€ç¾©ï¼‰ã€‚é™¤äº†å †ç–Šå’Œå¡«å……æ‰¹æ¬¡ï¼Œå®ƒé‚„è² è²¬å‰µå»ºèªè¨€æ¨¡å‹æ¨™ç±¤â€”â€”åœ¨å› æœèªè¨€å»ºæ¨¡ä¸­ï¼Œè¼¸å…¥ä¹Ÿç”¨ä½œæ¨™ç±¤ï¼ˆåªæ˜¯ç§»å‹•äº†ä¸€å€‹å…ƒç´ ï¼‰ï¼Œä¸¦ä¸”é€™å€‹æ•¸æ“šæ•´ç†å™¨åœ¨è¨“ç·´æœŸé–“å³æ™‚å‰µå»ºå®ƒå€‘ï¼Œæ‰€ä»¥æˆ‘å€‘ä¸éœ€è¦è¤‡è£½ `input_ids`ã€‚ 

æ³¨æ„ `DataCollatorForLanguageModeling` æ”¯æŒæ©ç¢¼èªè¨€å»ºæ¨¡ (MLM) å’Œå› æœèªè¨€å»ºæ¨¡ (CLM)ã€‚é»˜èªæƒ…æ³ä¸‹å®ƒç‚º MLM æº–å‚™æ•¸æ“šï¼Œä½†æˆ‘å€‘å¯ä»¥é€šéè¨­ç½®`mlm=False`åƒæ•¸åˆ‡æ›åˆ° CLM ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

è®“æˆ‘å€‘çœ‹ä¸€å€‹ä¾‹å­ï¼š

```py
out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

æˆ‘å€‘å¯ä»¥çœ‹åˆ°ç¤ºä¾‹å·²ç¶“å †ç–Šåœ¨ä¸€èµ·ï¼Œä¸¦ä¸”æ‰€æœ‰å¼µé‡éƒ½å…·æœ‰ç›¸åŒçš„å½¢ç‹€ã€‚

{#if fw === 'tf'}

ç¾åœ¨ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨`to_tf_dataset()`æ–¹æ³•ï¼Œä½¿ç”¨ä¸Šé¢å‰µå»ºçš„æ•¸æ“šæ•´ç†å™¨å°‡æ•¸æ“šé›†è½‰æ›ç‚ºTensorFlowæ•¸æ“šé›†ï¼š

```python
tf_train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = tokenized_dataset["valid"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<Tip warning={true}>

âš ï¸  ç§»å‹•è¼¸å…¥å’Œæ¨™ç±¤ä»¥å°é½Šå®ƒå€‘ç™¼ç”Ÿåœ¨æ¨¡å‹å…§éƒ¨ï¼Œå› æ­¤æ•¸æ“šæ•´ç†å™¨åªéœ€è¤‡è£½è¼¸å…¥ä»¥å‰µå»ºæ¨™ç±¤ã€‚

</Tip>


ç¾åœ¨æˆ‘å€‘å·²ç¶“æº–å‚™å¥½å¯¦éš›è¨“ç·´æˆ‘å€‘çš„æ¨¡å‹çš„ä¸€åˆ‡äº†â€”â€”ç•¢ç«Ÿé€™ä¸æ˜¯é‚£éº¼å¤šå·¥ä½œï¼åœ¨æˆ‘å€‘é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œæˆ‘å€‘æ‡‰è©²ç™»éŒ„ Hugging Faceã€‚å¦‚æœæ‚¨åœ¨ç­†è¨˜æœ¬ä¸Šå·¥ä½œï¼Œå‰‡å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å¯¦ç”¨ç¨‹åºåŠŸèƒ½ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

é€™å°‡é¡¯ç¤ºä¸€å€‹å°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¼¸å…¥æ‚¨çš„ Hugging Face ç™»éŒ„æ†‘æ“šã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨notebookä¸Šå·¥ä½œï¼Œåªéœ€åœ¨çµ‚ç«¯ä¸­è¼¸å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'pt'}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è¨“ç·´åƒæ•¸ä¸¦å•Ÿå‹• `Trainer` .æˆ‘å€‘å°‡ä½¿ç”¨é¤˜å¼¦å­¸ç¿’ç‡ï¼Œä¸¦é€²è¡Œä¸€äº›Warmupå’Œæœ‰æ•ˆæ‰¹é‡å¤§å°ç‚º 256 ( `per_device_train_batch_size` * `gradient_accumulation_steps`ï¼‰ã€‚ç•¶å–®å€‹æ‰¹æ¬¡ä¸é©åˆå…§å­˜æ™‚ä½¿ç”¨æ¢¯åº¦ç´¯ç©ï¼Œä¸¦é€šéå¤šæ¬¡å‘å‰/å‘å¾Œå‚³éé€æ­¥å»ºç«‹æ¢¯åº¦ã€‚ç•¶æˆ‘å€‘ä½¿ç”¨ ğŸ¤— Accelerate å‰µå»ºè¨“ç·´å¾ªç’°æ™‚ï¼Œæˆ‘å€‘å°‡çœ‹åˆ°é€™ä¸€é»ã€‚

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ç¾åœ¨æˆ‘å€‘å¯ä»¥é–‹å§‹ `Trainer`ä¸¦ç­‰å¾…è¨“ç·´å®Œæˆã€‚æ ¹æ“šæ‚¨æ˜¯åœ¨æ•´å€‹è¨“ç·´é›†é‚„æ˜¯åœ¨è¨“ç·´é›†çš„ä¸€å€‹å­é›†ä¸Šé‹è¡Œå®ƒï¼Œé€™å°‡åˆ†åˆ¥éœ€è¦ 20 æˆ– 2 å€‹å°æ™‚ï¼Œå› æ­¤è«‹å–æ¯å’–å•¡å’Œä¸€æœ¬å¥½æ›¸ä¾†é–±è®€ï¼

```py
trainer.train()
```

è¨“ç·´å®Œæˆå¾Œï¼Œæˆ‘å€‘å¯ä»¥å°‡æ¨¡å‹å’Œæ¨™è¨˜å™¨æ¨é€åˆ° Hubï¼š

```py
trainer.push_to_hub()
```

{:else}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è¨“ç·´è¶…åƒæ•¸ä¸¦èª¿ç”¨ `compile()` å’Œ `fit()`ã€‚æˆ‘å€‘å°‡ä½¿ç”¨å¸¶æœ‰ä¸€äº›é ç†±çš„å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥ä¾†æé«˜è¨“ç·´çš„ç©©å®šæ€§ï¼š

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ç¾åœ¨æˆ‘å€‘å¯ä»¥èª¿ç”¨`model.fit()ï¼Œ`ä¸¦ç­‰å¾…è¨“ç·´å®Œæˆã€‚ä½ æ˜¯åœ¨å®Œæ•´çš„è¨“ç·´é›†é‚„æ˜¯ä»–çš„å­é›†ä¸Šé‹è¡Œï¼Œé€™å°‡åˆ†åˆ¥éœ€è¦20å’Œ2å€‹å°æ™‚ï¼Œæ‰€ä»¥æ‹¿ä¸€äº›å’–å•¡å’Œä¸€æœ¬å¥½æ›¸ä¾†é–±è®€ï¼è¨“ç·´å®Œæˆå¾Œï¼Œæˆ‘å€‘å¯ä»¥å°‡æ¨¡å‹å’Œåˆ†è©å™¨æ¨é€åˆ°ä¸­å¿ƒï¼š

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

âœï¸ **è©¦è©¦çœ‹!** é™¤äº†`TrainingArguments` ä¹‹å¤–ï¼Œæˆ‘å€‘åªéœ€è¦å¤§ç´„30è¡Œä»£ç¢¼å°±å¯ä»¥å¾åŸå§‹æ–‡æœ¬åˆ°è¨“ç·´GPT-2ã€‚ ç”¨ä½ è‡ªå·±çš„æ•¸æ“šé›†è©¦è©¦çœ‹ï¼Œçœ‹çœ‹ä½ èƒ½ä¸èƒ½å¾—åˆ°å¥½çš„çµæœï¼

</Tip>

<Tip>

{#if fw === 'pt'}

ğŸ’¡ å¦‚æœæ‚¨å¯ä»¥è¨ªå•å…·æœ‰å¤šå€‹ GPU çš„æ©Ÿå™¨ï¼Œè«‹å˜—è©¦åœ¨é‚£è£¡é‹è¡Œä»£ç¢¼ã€‚ `Trainer`è‡ªå‹•ç®¡ç†å¤šè‡ºæ©Ÿå™¨ï¼Œé€™å¯ä»¥æ¥µå¤§åœ°åŠ å¿«è¨“ç·´é€Ÿåº¦ã€‚

{:else}

ğŸ’¡ å¦‚æœæ‚¨æœ‰æ¬Šè¨ªå•å…·æœ‰å¤šå€‹ GPU çš„è¨ˆç®—æ©Ÿï¼Œå‰‡å¯ä»¥å˜—è©¦ä½¿ç”¨ `MirroredStrategy` ä¸Šä¸‹æ–‡ä¾†å¤§å¹…åŠ å¿«è¨“ç·´é€Ÿåº¦ã€‚æ‚¨éœ€è¦å‰µå»ºä¸€å€‹`tf.distribute.MirroredStrategy`å°è±¡ï¼Œä¸¦ç¢ºä¿ `to_tf_dataset` å‘½ä»¤ä»¥åŠæ¨¡å‹å‰µå»ºå’Œå° `fit()`çš„èª¿ç”¨éƒ½åœ¨å…¶ `scope()` context. ä¸Šä¸‹æ–‡ä¸­é‹è¡Œã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æœ‰é—œæ­¤å…§å®¹çš„æ–‡æª”[here](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## ä½¿ç”¨ç®¡é“ç”Ÿæˆä»£ç¢¼

ç¾åœ¨æ˜¯é—œéµçš„éƒ¨åˆ†ï¼šè®“æˆ‘å€‘çœ‹çœ‹ç¶“éè¨“ç·´çš„æ¨¡å‹çš„å¯¦éš›æ•ˆæœå¦‚ä½•ï¼æˆ‘å€‘å¯ä»¥åœ¨æ—¥èªŒä¸­çœ‹åˆ°æå¤±ç©©æ­¥ä¸‹é™ï¼Œä½†ç‚ºäº†è®“æ¨¡å‹é€²è¡Œæ¸¬è©¦ï¼Œè®“æˆ‘å€‘çœ‹çœ‹å®ƒåœ¨æŸäº›æ¸¬è©¦ä¸Šçš„è¡¨ç¾å¦‚ä½•ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘å°‡æ¨¡å‹åŒ…è£åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„`pipeline` ï¼Œå¦‚æœæœ‰å¯ç”¨çš„ï¼Œæˆ‘å€‘æœƒå°‡å®ƒæ”¾åœ¨ GPU ä¸Šé€²è¡Œå¿«é€Ÿç”Ÿæˆï¼š

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

è®“æˆ‘å€‘å¾å‰µå»ºæ•£é»åœ–çš„ç°¡å–®ä»»å‹™é–‹å§‹ï¼š

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

çµæœçœ‹èµ·ä¾†æ˜¯æ­£ç¢ºçš„ã€‚å®ƒä¹Ÿé©ç”¨æ–¼ `pandas` é¡å‹ï¼Ÿè®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘æ˜¯å¦ä½¿ç”¨å…©å€‹æ•¸çµ„å¯ä»¥å‰µå»ºä¸€å€‹ `DataFrame` ï¼š

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

å¾ˆå¥½ï¼Œé€™æ˜¯æ­£ç¢ºçš„ç­”æ¡ˆâ€”â€”å„˜ç®¡å®ƒéš¨å¾Œå†æ¬¡æ’å…¥äº†åˆ— `x` ã€‚ç”±æ–¼ç”Ÿæˆçš„tokenæ•¸é‡æœ‰é™ï¼Œä»¥ä¸‹ `for` å¾ªç’°è¢«åˆ‡æ–·ã€‚è®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘æ˜¯å¦å¯ä»¥åšä¸€äº›æ›´å¾©é›œçš„äº‹æƒ…ä¸¦è®“æ¨¡å‹å¹«åŠ©æˆ‘å€‘åˆ†çµ„æ“ä½œï¼š

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

ä¸éŒ¯;é€™æ˜¯æ­£ç¢ºçš„åšæ³•ã€‚æœ€å¾Œï¼Œè®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘æ˜¯å¦ä¹Ÿå¯ä»¥å°‡å…¶ç”¨æ–¼ `scikit-learn` ä¸¦å»ºç«‹ä¸€å€‹éš¨æ©Ÿæ£®æ—æ¨¡å‹ï¼š

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

çœ‹çœ‹é€™å¹¾å€‹ä¾‹å­ï¼Œä¼¼ä¹æ¨¡å‹å·²ç¶“å­¸ç¿’äº†Pythonæ•¸æ“šç§‘å­¸å †æ£§çš„ä¸€äº›èªæ³•ã€‚ç•¶ç„¶ï¼Œåœ¨å°‡æ¨¡å‹éƒ¨ç½²åˆ°ç¾å¯¦ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦æ›´å¾¹åº•åœ°è©•ä¼°æ¨¡å‹ï¼Œä½†é€™ä»ç„¶æ˜¯ä¸€å€‹ä»¤äººå°è±¡æ·±åˆ»çš„åŸå‹ã€‚

{:else}

å¾é€™å¹¾å€‹ä¾‹å­ä¾†çœ‹ï¼Œæ¨¡å‹ä¼¼ä¹å·²ç¶“å­¸ç¿’äº† Python æ•¸æ“šç§‘å­¸å †æ£§çš„ä¸€äº›èªæ³•ï¼ˆç•¶ç„¶ï¼Œåœ¨å°‡æ¨¡å‹éƒ¨ç½²åˆ°ç¾å¯¦ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å°å…¶é€²è¡Œæ›´å…¨é¢çš„è©•ä¼°ï¼‰ã€‚ç„¶è€Œï¼Œæœ‰æ™‚éœ€è¦å°æ¨¡å‹è¨“ç·´é€²è¡Œæ›´å¤šå®šè£½æ‰èƒ½å¯¦ç¾çµ¦å®šç”¨ä¾‹çš„å¿…è¦æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘å€‘æƒ³å‹•æ…‹æ›´æ–°æ‰¹é‡å¤§å°æˆ–æœ‰ä¸€å€‹æ¢ä»¶è¨“ç·´å¾ªç’°ä¾†å³æ™‚è·³éå£ç¤ºä¾‹æ€éº¼è¾¦ï¼Ÿä¸€ç¨®é¸æ“‡æ˜¯å°‡ `Trainer` ä¸¦æ·»åŠ å¿…è¦çš„æ›´æ”¹ï¼Œä½†æœ‰æ™‚å¾é ­é–‹å§‹ç·¨å¯«è¨“ç·´å¾ªç’°æœƒæ›´ç°¡å–®ã€‚é€™å°±æ˜¯ğŸ¤— Accelerate çš„ç”¨æ­¦ä¹‹åœ°ã€‚   

{/if}

{#if fw === 'pt'}

## ç”¨ ğŸ¤— Accelerate è¨“ç·´

æˆ‘å€‘å·²ç¶“çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ `Trainer` ï¼Œé€™å¯ä»¥å…è¨±ä¸€äº›è‡ªå®šç¾©ã€‚ç„¶è€Œï¼Œæœ‰æ™‚æˆ‘å€‘æƒ³è¦å®Œå…¨æ§åˆ¶è¨“ç·´å¾ªç’°ï¼Œæˆ–è€…æˆ‘å€‘æƒ³è¦é€²è¡Œä¸€äº›å¥‡ç‰¹çš„æ›´æ”¹ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ ğŸ¤— Accelerate æ˜¯ä¸€å€‹ä¸éŒ¯çš„é¸æ“‡ï¼Œåœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡é€æ­¥ä»‹ç´¹ä½¿ç”¨å®ƒä¾†è¨“ç·´æˆ‘å€‘çš„æ¨¡å‹çš„æ­¥é©Ÿã€‚ç‚ºäº†è®“äº‹æƒ…è®Šå¾—æ›´æœ‰è¶£ï¼Œæˆ‘å€‘é‚„å°‡åœ¨è¨“ç·´å¾ªç’°ä¸­æ·»åŠ ä¸€äº›ä¿®æ”¹ã€‚

<Youtube id="Hm8_PgVTFuc"/>

ç”±æ–¼æˆ‘å€‘ä¸»è¦å°æ•¸æ“šç§‘å­¸åº«çš„åˆç†è‡ªå‹•å¡«å……æ„Ÿèˆˆè¶£ï¼Œå› æ­¤å°æ›´å¤šä½¿ç”¨é€™äº›åº«çš„è¨“ç·´æ¨£æœ¬çµ¦äºˆæ›´å¤šæ¬Šé‡æ˜¯æœ‰æ„ç¾©çš„ã€‚æˆ‘å€‘å¯ä»¥é€šéä½¿ç”¨é—œéµå­—è¼•é¬†è­˜åˆ¥é€™äº›ç¤ºä¾‹ï¼Œä¾‹å¦‚ `plt`ã€`pd`ã€`sk`ã€`fit`å’Œ`predict`ç­‰é—œéµå­—ï¼Œæˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“åœ°è­˜åˆ¥é€™äº›ç¤ºä¾‹ï¼Œé€™äº›é—œéµå­—æ˜¯matplotlibæœ€å¸¸ç”¨çš„å°å…¥åç¨±ã€‚`Pyplot`, `pandas`å’Œ`sklearn`ä»¥åŠå¾Œè€…çš„æ“¬åˆ/é æ¸¬æ¨¡å¼ã€‚å¦‚æœé€™äº›éƒ½è¡¨ç¤ºç‚ºå–®å€‹æ¨™è¨˜ï¼Œæˆ‘å€‘å¯ä»¥è¼•é¬†æª¢æŸ¥å®ƒå€‘æ˜¯å¦å‡ºç¾åœ¨è¼¸å…¥åºåˆ—ä¸­ã€‚æ¨™è¨˜å¯èƒ½æœ‰ä¸€å€‹ç©ºæ ¼å‰ç¶´ï¼Œå› æ­¤æˆ‘å€‘é‚„å°‡åœ¨æ¨™è¨˜å™¨è©å½™è¡¨ä¸­æª¢æŸ¥é€™äº›ç‰ˆæœ¬ã€‚ç‚ºäº†é©—è­‰å®ƒæ˜¯å¦æœ‰æ•ˆï¼Œæˆ‘å€‘å°‡æ·»åŠ ä¸€å€‹æ¸¬è©¦token ï¼Œè©²token æ‡‰æ‹†åˆ†ç‚ºå¤šå€‹tokensï¼š

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

å¤ªå¥½äº†ï¼Œé€™ä¼¼ä¹å¾ˆå¥½ç”¨ï¼æˆ‘å€‘ç¾åœ¨å¯ä»¥ç·¨å¯«ä¸€å€‹è‡ªå®šç¾©æå¤±å‡½æ•¸ï¼Œå®ƒå°‡è¼¸å…¥åºåˆ—ã€logits å’Œæˆ‘å€‘å‰›å‰›é¸æ“‡çš„é—œâ€‹â€‹éµæ¨™è¨˜ä½œç‚ºè¼¸å…¥ã€‚é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦å°é½Š logits å’Œè¼¸å…¥ï¼šå‘å³ç§»å‹•ä¸€å€‹çš„è¼¸å…¥åºåˆ—å½¢æˆæ¨™ç±¤ï¼Œå› ç‚ºä¸‹ä¸€å€‹æ¨™è¨˜æ˜¯ç•¶å‰æ¨™è¨˜çš„æ¨™ç±¤ã€‚æˆ‘å€‘å¯ä»¥é€šéå¾è¼¸å…¥åºåˆ—çš„ç¬¬äºŒå€‹æ¨™è¨˜é–‹å§‹æ¨™è¨˜ä¾†å¯¦ç¾é€™ä¸€é»ï¼Œå› ç‚ºæ¨¡å‹ç„¡è«–å¦‚ä½•éƒ½ä¸æœƒå°ç¬¬ä¸€å€‹æ¨™è¨˜é€²è¡Œé æ¸¬ã€‚ç„¶å¾Œæˆ‘å€‘åˆ‡æ–·æœ€å¾Œä¸€å€‹ logitï¼Œå› ç‚ºæˆ‘å€‘æ²’æœ‰å®Œæ•´è¼¸å…¥åºåˆ—ä¹‹å¾Œçš„æ¨™è¨˜çš„æ¨™ç±¤ã€‚æœ‰äº†é€™å€‹ï¼Œæˆ‘å€‘å¯ä»¥è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„æå¤±ä¸¦è¨ˆç®—æ¯å€‹æ¨£æœ¬ä¸­æ‰€æœ‰é—œéµå­—çš„å‡ºç¾æ¬¡æ•¸ã€‚æœ€å¾Œï¼Œæˆ‘å€‘ä½¿ç”¨å‡ºç¾æ¬¡æ•¸ä½œç‚ºæ¬Šé‡è¨ˆç®—æ‰€æœ‰æ¨£æœ¬çš„åŠ æ¬Šå¹³å‡å€¼ã€‚ç”±æ–¼æˆ‘å€‘ä¸æƒ³æ‰”æ‰æ‰€æœ‰æ²’æœ‰é—œéµå­—çš„æ¨£æœ¬ï¼Œæˆ‘å€‘å°‡æ¬Šé‡åŠ 1ï¼š

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # Calculate and scale weighting
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # Calculate weighted average
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

åœ¨æˆ‘å€‘é–‹å§‹ä½¿ç”¨é€™å€‹å¾ˆæ£’çš„æ–°æå¤±å‡½æ•¸é€²è¡Œè¨“ç·´ä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦æº–å‚™ä¸€äº›æ±è¥¿ï¼š

- æˆ‘å€‘éœ€è¦æ•¸æ“šåŠ è¼‰å™¨ä¾†æ‰¹é‡åŠ è¼‰æ•¸æ“šã€‚
- æˆ‘å€‘éœ€è¦è¨­ç½®æ¬Šé‡è¡°æ¸›åƒæ•¸ã€‚
- æœ‰æ™‚æˆ‘å€‘æƒ³è¦æ±‚å€¼ï¼Œå› æ­¤å°‡æ±‚å€¼ä»£ç¢¼åŒ…è£åœ¨ä¸€å€‹å‡½æ•¸ä¸­æ˜¯æœ‰æ„ç¾©çš„ã€‚

è®“æˆ‘å€‘å¾æ•¸æ“šåŠ è¼‰å™¨é–‹å§‹ã€‚æˆ‘å€‘åªéœ€è¦å°‡æ•¸æ“šé›†çš„æ ¼å¼è¨­ç½®ç‚º `"torch"`ï¼Œç„¶å¾Œæˆ‘å€‘å¯ä»¥å°‡å®ƒå‚³éçµ¦ PyTorch `DataLoader` ,åŒæ™‚è¨­ç½®é©ç•¶çš„æ‰¹é‡å¤§å°ï¼š

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°åƒæ•¸é€²è¡Œåˆ†çµ„ï¼Œä»¥ä¾¿å„ªåŒ–å™¨çŸ¥é“å“ªäº›å°‡ç²å¾—é¡å¤–çš„æ¬Šé‡è¡°æ¸›ã€‚é€šå¸¸ï¼Œæ‰€æœ‰åå·®å’Œ LayerNorm æ¬Šé‡é …éƒ½ä¸å—æ­¤é™åˆ¶ï¼›ä»¥ä¸‹æˆ‘å€‘å¦‚ä½•åšåˆ°é€™ä¸€é»ï¼š

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

ç”±æ–¼æˆ‘å€‘å¸Œæœ›åœ¨è¨“ç·´æœŸé–“å®šæœŸåœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹ï¼Œå› æ­¤æˆ‘å€‘ä¹Ÿç‚ºæ­¤ç·¨å¯«ä¸€å€‹å‡½æ•¸ã€‚å®ƒåªæ˜¯é‹è¡Œè©•ä¼°æ•¸æ“šåŠ è¼‰å™¨ä¸¦æ”¶é›†è·¨é€²ç¨‹çš„æ‰€æœ‰æå¤±ï¼š

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

é€šé `evaluate()` å‡½æ•¸æˆ‘å€‘å®šæœŸå¯ä»¥ç²å–æå¤±å€¼å’Œ[perplexity](/course/chapter7/3)ã€‚æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘é‡æ–°å®šç¾©æˆ‘å€‘çš„æ¨¡å‹ä»¥ç¢ºä¿æˆ‘å€‘å†æ¬¡å¾é ­é–‹å§‹è¨“ç·´ï¼š

```py
model = GPT2LMHeadModel(config)
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥å®šç¾©æˆ‘å€‘çš„å„ªåŒ–å™¨ï¼Œä½¿ç”¨ä¹‹å‰çš„å‡½æ•¸ä¾†åˆ†å‰²æ¬Šé‡è¡°æ¸›çš„åƒæ•¸ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ç¾åœ¨è®“æˆ‘å€‘æº–å‚™æ¨¡å‹ã€å„ªåŒ–å™¨å’Œæ•¸æ“šåŠ è¼‰å™¨ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥é–‹å§‹è¨“ç·´ï¼š

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœæ‚¨åœ¨ TPU ä¸Šé€²è¡Œè¨“ç·´ï¼Œå‰‡éœ€è¦å°‡å¾ä¸Šé¢çš„å–®å…ƒæ ¼é–‹å§‹çš„æ‰€æœ‰ä»£ç¢¼ç§»å‹•åˆ°å°ˆç”¨çš„è¨“ç·´å‡½æ•¸ä¸­ã€‚æœ‰é—œè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–± [ç¬¬ 3 ç« ](/course/chapter3) for more details.

</Tip>

ç¾åœ¨æˆ‘å€‘å·²ç¶“ç™¼é€äº†æˆ‘å€‘çš„ `train_dataloader`åˆ° `accelerator.prepare()` ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨å®ƒçš„é•·åº¦ä¾†è¨ˆç®—è¨“ç·´æ­¥é©Ÿçš„æ•¸é‡ã€‚è«‹è¨˜ä½ï¼Œæˆ‘å€‘æ‡‰è©²å§‹çµ‚åœ¨æº–å‚™å¥½dataloaderå¾ŒåŸ·è¡Œæ­¤æ“ä½œï¼Œå› ç‚ºè©²æ–¹æ³•æœƒæ”¹è®Šå…¶é•·åº¦ã€‚æˆ‘å€‘ä½¿ç”¨ç¶“å…¸ç·šæ€§å­¸ç¿’ç‡èª¿åº¦ï¼š

```py
num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

æœ€å¾Œï¼Œè¦å°‡æˆ‘å€‘çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘å€‘éœ€è¦å‰µå»ºä¸€å€‹ `Repository` å·¥ä½œæ–‡ä»¶å¤¾ä¸­çš„å°è±¡ã€‚å¦‚æœæ‚¨å°šæœªç™»éŒ„ï¼Œè«‹å…ˆç™»éŒ„ Hugging Faceã€‚æˆ‘å€‘å°‡å¾æˆ‘å€‘æƒ³è¦ç‚ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¢ºå®šå­˜å„²åº«åç¨±ï¼ˆæ‚¨å¯ä»¥è‡ªç”±åœ°ç”¨è‡ªå·±çš„é¸æ“‡æ›¿æ› `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«æ‚¨çš„ç”¨æˆ¶åï¼Œå¯ä»¥ä½¿ç”¨`get_full_repo_name()`å‡½æ•¸çš„æŸ¥çœ‹ç›®å‰çš„repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¾ä¸­å…‹éš†è©²å­˜å„²åº«ã€‚å¦‚æœå®ƒå·²ç¶“å­˜åœ¨ï¼Œé€™å€‹æœ¬åœ°æ–‡ä»¶å¤¾æ‡‰è©²æ˜¯æˆ‘å€‘æ­£åœ¨ä½¿ç”¨çš„å­˜å„²åº«çš„å…‹éš†ï¼š

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘å€‘ç¾åœ¨å¯ä»¥ä¸Šå‚³æˆ‘å€‘ä¿å­˜çš„ä»»ä½•å…§å®¹ `output_dir` é€šéèª¿ç”¨ `repo.push_to_hub()` æ–¹æ³•ã€‚é€™å°‡å¹«åŠ©æˆ‘å€‘åœ¨æ¯å€‹ epoch çµæŸæ™‚ä¸Šå‚³ä¸­é–“æ¨¡å‹ã€‚åœ¨æˆ‘å€‘è¨“ç·´ä¹‹å‰ï¼Œè®“æˆ‘å€‘é‹è¡Œä¸€å€‹å¿«é€Ÿæ¸¬è©¦ï¼Œçœ‹çœ‹è©•ä¼°å‡½æ•¸æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

é€™äº›æå¤±å’Œå›°æƒ‘åº¦çš„å€¼éå¸¸é«˜ï¼Œä½†é€™ä¸¦ä¸å¥‡æ€ªï¼Œå› ç‚ºæˆ‘å€‘é‚„æ²’æœ‰è¨“ç·´éæ¨¡å‹ã€‚æœ‰äº†é€™å€‹ï¼Œæˆ‘å€‘å·²ç¶“æº–å‚™å¥½ç·¨å¯«è¨“ç·´è…³æœ¬çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šè¨“ç·´å¾ªç’°ã€‚åœ¨è¨“ç·´å¾ªç’°ä¸­ï¼Œæˆ‘å€‘éæ­·æ•¸æ“šåŠ è¼‰å™¨ä¸¦å°‡æ‰¹æ¬¡å‚³éçµ¦æ¨¡å‹ã€‚æœ‰äº† logitsï¼Œæˆ‘å€‘å°±å¯ä»¥è©•ä¼°æˆ‘å€‘çš„è‡ªå®šç¾©æå¤±å‡½æ•¸ã€‚æˆ‘å€‘é€šéæ¢¯åº¦ç´¯ç©æ­¥é©Ÿçš„æ•¸é‡ä¾†ç¸®æ”¾æå¤±ï¼Œä»¥ä¾¿åœ¨èšåˆæ›´å¤šæ­¥é©Ÿæ™‚ä¸æœƒç”¢ç”Ÿæ›´å¤§çš„æå¤±ã€‚åœ¨æˆ‘å€‘å„ªåŒ–ä¹‹å‰ï¼Œæˆ‘å€‘é‚„å‰ªè¼¯äº†æ¢¯åº¦ä»¥ç²å¾—æ›´å¥½çš„æ”¶æ–‚æ€§ã€‚æœ€å¾Œï¼Œæ¯éš”å¹¾æ­¥ï¼Œæˆ‘å€‘å°±æœƒä½¿ç”¨æ–°çš„ `evaluate()` å‡½æ•¸è©•ä¼°æ¨¡å‹ï¼š

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=len(train_dataloader)
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

å°±æ˜¯é€™æ¨£ - æ‚¨ç¾åœ¨æ“æœ‰è‡ªå·±çš„å› æœèªè¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰çš„è‡ªå®šç¾©è¨“ç·´å¾ªç’°ï¼Œæ‚¨å¯ä»¥æ ¹æ“šè‡ªå·±çš„éœ€è¦é€²ä¸€æ­¥è‡ªå®šç¾©ã€‚

<Tip>

âœï¸ **è©¦è©¦çœ‹!** å‰µå»ºé©åˆæ‚¨çš„ç”¨ä¾‹çš„è‡ªå®šç¾©æå¤±å‡½æ•¸ï¼Œæˆ–åœ¨è¨“ç·´å¾ªç’°ä¸­æ·»åŠ å¦ä¸€å€‹è‡ªå®šç¾©æ­¥é©Ÿã€‚

</Tip>

<Tip>

âœï¸ **è©¦è©¦çœ‹!** åœ¨é‹è¡Œé•·æ™‚é–“çš„è¨“ç·´å¯¦é©—æ™‚ï¼Œæœ€å¥½ä½¿ç”¨ TensorBoard æˆ– Weights Biases ç­‰å·¥å…·è¨˜éŒ„é‡è¦æŒ‡æ¨™ã€‚å‘è¨“ç·´å¾ªç’°æ·»åŠ é©ç•¶çš„æ—¥èªŒè¨˜éŒ„ï¼Œä»¥ä¾¿æ‚¨å§‹çµ‚å¯ä»¥æª¢æŸ¥è¨“ç·´çš„é€²è¡Œæƒ…æ³ã€‚going.

</Tip>

{/if}