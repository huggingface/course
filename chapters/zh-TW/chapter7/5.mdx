<FrameworkSwitchCourse {fw} />

# æå–æ–‡æœ¬æ‘˜è¦

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_tf.ipynb"},
]} />

{/if}


åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Transformer æ¨¡å‹å°‡é•·æ–‡æª”å£“ç¸®ç‚ºæ‘˜è¦ï¼Œé€™é …ä»»å‹™ç¨±ç‚ºæ–‡æœ¬æ‘˜è¦.é€™æ˜¯æœ€å…·æŒ‘æˆ°æ€§çš„ NLP ä»»å‹™ä¹‹ä¸€ï¼Œå› ç‚ºå®ƒéœ€è¦ä¸€ç³»åˆ—èƒ½åŠ›ï¼Œä¾‹å¦‚ç†è§£é•·ç¯‡æ–‡ç« å’Œç”Ÿæˆèƒ½å¤ æ•æ‰æ–‡æª”ä¸­ä¸»è¦ä¸»é¡Œçš„é€£è²«æ–‡æœ¬ã€‚ä½†æ˜¯ï¼Œå¦‚æœåšå¾—å¥½ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç¨®å¼·å¤§çš„å·¥å…·ï¼Œå¯ä»¥æ¸›è¼•é ˜åŸŸå°ˆå®¶è©³ç´°é–±è®€é•·æ–‡æª”çš„è² æ“”ï¼Œå¾è€ŒåŠ å¿«å„ç¨®æ¥­å‹™æµç¨‹ã€‚

<Youtube id="yHnr5Dk2zCI"/>

å„˜ç®¡åœ¨[Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization=downloads)ä¸Šå·²ç¶“å­˜åœ¨å„ç¨®å¾®èª¿æ¨¡å‹ç”¨æ–¼æ–‡æœ¬æ‘˜è¦ï¼Œå¹¾ä¹æ‰€æœ‰é€™äº›éƒ½åªé©ç”¨æ–¼è‹±æ–‡æ–‡æª”ã€‚å› æ­¤ï¼Œç‚ºäº†åœ¨æœ¬ç¯€ä¸­æ·»åŠ ä¸€äº›è®ŠåŒ–ï¼Œæˆ‘å€‘å°‡ç‚ºè‹±èªå’Œè¥¿ç­ç‰™èªè¨“ç·´ä¸€å€‹é›™èªæ¨¡å‹ã€‚åœ¨æœ¬ç¯€çµæŸæ™‚ï¼Œæ‚¨å°‡æœ‰ä¸€å€‹å¯ä»¥ç¸½çµå®¢æˆ¶è©•è«–çš„[æ¨¡å‹](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es)ã€‚

<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space" frameBorder="0" height="400" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
<iframe src="https://course-demos-mt5-small-finetuned-amazon-en-es-darkmode.hf.space" frameBorder="0" height="400" title="Gradio app" class="hidden dark:block container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

å¦‚ä¸‹æ‰€ç¤ºï¼šæ­£å¦‚æˆ‘å€‘å°‡çœ‹åˆ°çš„ï¼Œé€™äº›æ‘˜è¦å¾ˆç°¡æ½”ï¼Œå› ç‚ºå®ƒå€‘æ˜¯å¾å®¢æˆ¶åœ¨ç”¢å“è©•è«–ä¸­æä¾›çš„æ¨™é¡Œä¸­å­¸åˆ°çš„ã€‚è®“æˆ‘å€‘é¦–å…ˆç‚ºé€™é …ä»»å‹™æº–å‚™ä¸€å€‹åˆé©çš„é›™èªèªæ–™åº«ã€‚

## æº–å‚™å¤šèªè¨€èªæ–™åº«

æˆ‘å€‘å°‡ä½¿ç”¨[å¤šèªè¨€äºé¦¬éœè©•è«–èªæ–™åº«](https://huggingface.co/datasets/amazon_reviews_multi)å‰µå»ºæˆ‘å€‘çš„é›™èªæ‘˜è¦å™¨ã€‚è©²èªæ–™åº«ç”±å…­ç¨®èªè¨€çš„äºé¦¬éœç”¢å“è©•è«–çµ„æˆï¼Œé€šå¸¸ç”¨æ–¼å°å¤šèªè¨€åˆ†é¡å™¨é€²è¡ŒåŸºæº–æ¸¬è©¦ã€‚ç„¶è€Œï¼Œç”±æ–¼æ¯æ¢è©•è«–éƒ½é™„æœ‰ä¸€å€‹ç°¡çŸ­çš„æ¨™é¡Œï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨æ¨™é¡Œä½œç‚ºæˆ‘å€‘æ¨¡å‹å­¸ç¿’çš„ç›®æ¨™æ‘˜è¦ï¼é¦–å…ˆï¼Œè®“æˆ‘å€‘å¾ Hugging Face Hub ä¸‹è¼‰è‹±èªå’Œè¥¿ç­ç‰™èªå­é›†ï¼š

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

å¦‚æ‚¨æ‰€è¦‹ï¼Œå°æ–¼æ¯ç¨®èªè¨€ï¼Œéƒ½æœ‰ 200,000 æ¢è©•è«– **train** æ‹†åˆ†ï¼Œæ¯å€‹è©•è«–æœ‰ 5,000 æ¢è©•è«– **validation** å’Œ **test** åˆ†è£‚ã€‚æˆ‘å€‘æ„Ÿèˆˆè¶£çš„è©•è«–ä¿¡æ¯åŒ…å«åœ¨ **review_body** å’Œ **review_title** åˆ—ã€‚è®“æˆ‘å€‘é€šéå‰µå»ºä¸€å€‹ç°¡å–®çš„å‡½æ•¸ä¾†æŸ¥çœ‹ä¸€äº›ç¤ºä¾‹ï¼Œè©²å‡½æ•¸ä½¿ç”¨æˆ‘å€‘åœ¨[ç¬¬äº”ç« ](/course/chapter5)å­¸åˆ°éï¼š

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)
```

```python out
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itâ€™s job and itâ€™s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<Tip>

âœï¸ **è©¦è©¦çœ‹ï¼** æ›´æ”¹ `Dataset.shuffle()` å‘½ä»¤ä¸­çš„éš¨æ©Ÿç¨®å­ä»¥æ¢ç´¢èªæ–™åº«ä¸­çš„å…¶ä»–è©•è«–ã€‚ å¦‚æœæ‚¨æ˜¯èªªè¥¿ç­ç‰™èªçš„äººï¼Œè«‹æŸ¥çœ‹ `spanish_dataset` ä¸­çš„ä¸€äº›è©•è«–ï¼Œçœ‹çœ‹æ¨™é¡Œæ˜¯å¦ä¹Ÿåƒåˆç†çš„æ‘˜è¦ã€‚

</Tip>

æ­¤ç¤ºä¾‹é¡¯ç¤ºäº†äººå€‘é€šå¸¸åœ¨ç¶²ä¸Šæ‰¾åˆ°çš„è©•è«–çš„å¤šæ¨£æ€§ï¼Œå¾æ­£é¢åˆ°è² é¢ï¼ˆä»¥åŠä»‹æ–¼å…©è€…ä¹‹é–“çš„æ‰€æœ‰å…§å®¹ï¼ï¼‰ã€‚å„˜ç®¡æ¨™é¡Œç‚ºâ€œmehâ€çš„ç¤ºä¾‹ä¿¡æ¯é‡ä¸å¤§ï¼Œä½†å…¶ä»–æ¨™é¡Œçœ‹èµ·ä¾†åƒæ˜¯å°è©•è«–æœ¬èº«çš„é«”é¢ç¸½çµã€‚åœ¨å–®å€‹ GPU ä¸Šè¨“ç·´æ‰€æœ‰ 400,000 æ¢è©•è«–çš„æ‘˜è¦æ¨¡å‹å°‡èŠ±è²»å¤ªé•·æ™‚é–“ï¼Œå› æ­¤æˆ‘å€‘å°‡å°ˆæ³¨æ–¼ç‚ºå–®å€‹ç”¢å“é ˜åŸŸç”Ÿæˆæ‘˜è¦ã€‚ç‚ºäº†ç­è§£æˆ‘å€‘å¯ä»¥é¸æ“‡å“ªäº›åŸŸï¼Œè®“æˆ‘å€‘å°‡ **english_dataset** è½‰æ›åˆ° **pandas.DataFrame** ä¸¦è¨ˆç®—æ¯å€‹ç”¢å“é¡åˆ¥çš„è©•è«–æ•¸é‡ï¼š

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

è‹±èªæ•¸æ“šé›†ä¸­æœ€å—æ­¡è¿çš„ç”¢å“æ˜¯å®¶å±…ç”¨å“ã€æœè£å’Œç„¡ç·šé›»å­ç”¢å“ã€‚ä¸éï¼Œç‚ºäº†å …æŒäºé¦¬éœçš„ä¸»é¡Œï¼Œè®“æˆ‘å€‘å°ˆæ³¨æ–¼ç¸½çµæ›¸ç±çš„è©•è«–â€”â€”ç•¢ç«Ÿï¼Œé€™æ˜¯äºé¦¬éœé€™å®¶å…¬å¸æˆç«‹çš„åŸºç¤ï¼æˆ‘å€‘å¯ä»¥çœ‹åˆ°å…©å€‹ç¬¦åˆè¦æ±‚çš„ç”¢å“é¡åˆ¥ï¼ˆ **book** å’Œ **digital_ebook_purchase** )ï¼Œæ‰€ä»¥è®“æˆ‘å€‘ç‚ºé€™äº›ç”¢å“éæ¿¾å…©ç¨®èªè¨€çš„æ•¸æ“šé›†ã€‚æ­£å¦‚æˆ‘å€‘åœ¨[ç¬¬äº”ç« ](/course/chapter5)å­¸åˆ°çš„ï¼Œ é€™ **Dataset.filter()** å‡½æ•¸å…è¨±æˆ‘å€‘éå¸¸æœ‰æ•ˆåœ°å°æ•¸æ“šé›†é€²è¡Œåˆ‡ç‰‡ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥å®šç¾©ä¸€å€‹ç°¡å–®çš„å‡½æ•¸ä¾†åŸ·è¡Œæ­¤æ“ä½œï¼š

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

ç¾åœ¨ï¼Œç•¶æˆ‘å€‘å°‡æ­¤å‡½æ•¸æ‡‰ç”¨æ–¼ **english_dataset** å’Œ **spanish_dataset** ï¼Œçµæœå°‡åªåŒ…å«æ¶‰åŠæ›¸ç±é¡åˆ¥çš„é‚£äº›è¡Œã€‚åœ¨æ‡‰ç”¨éæ¿¾å™¨ä¹‹å‰ï¼Œè®“æˆ‘å€‘å°‡**english_dataset**çš„æ ¼å¼å¾ **pandas** åˆ‡æ›å›åˆ° **arrow** ï¼š

```python
english_dataset.reset_format()
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥æ‡‰ç”¨éæ¿¾å™¨åŠŸèƒ½ï¼Œä½œç‚ºå¥å…¨æ€§æª¢æŸ¥ï¼Œè®“æˆ‘å€‘æª¢æŸ¥è©•è«–æ¨£æœ¬ï¼Œçœ‹çœ‹å®ƒå€‘æ˜¯å¦ç¢ºå¯¦èˆ‡æ›¸ç±æœ‰é—œï¼š

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

å¥½çš„ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°è©•è«–ä¸¦ä¸æ˜¯åš´æ ¼æ„ç¾©ä¸Šçš„æ›¸ç±ï¼Œå¯èƒ½æ˜¯æŒ‡æ—¥æ›†å’Œ OneNote ç­‰é›»å­æ‡‰ç”¨ç¨‹åºç­‰å…§å®¹ã€‚å„˜ç®¡å¦‚æ­¤ï¼Œè©²é ˜åŸŸä¼¼ä¹é©åˆè¨“ç·´æ‘˜è¦æ¨¡å‹ã€‚åœ¨æˆ‘å€‘æŸ¥çœ‹é©åˆæ­¤ä»»å‹™çš„å„ç¨®æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘å€‘é‚„æœ‰æœ€å¾Œä¸€é»æ•¸æ“šæº–å‚™è¦åšï¼šå°‡è‹±èªå’Œè¥¿ç­ç‰™èªè©•è«–åˆä½µç‚ºä¸€å€‹ **DatasetDict** ç›®çš„ã€‚ ğŸ¤— Datasets æä¾›äº†ä¸€å€‹æ–¹ä¾¿çš„ **concatenate_datasets()** å‡½æ•¸ï¼ˆé¡§åæ€ç¾©ï¼‰åˆä½µ **Dataset** å°è±¡ã€‚å› æ­¤ï¼Œç‚ºäº†å‰µå»ºæˆ‘å€‘çš„é›™èªæ•¸æ“šé›†ï¼Œæˆ‘å€‘å°‡éæ­·æ¯å€‹æ‹†åˆ†ï¼Œé€£æ¥è©²æ‹†åˆ†çš„æ•¸æ“šé›†ï¼Œä¸¦æ‰“äº‚çµæœä»¥ç¢ºä¿æˆ‘å€‘çš„æ¨¡å‹ä¸æœƒéåº¦æ“¬åˆå–®ä¸€èªè¨€ï¼š

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)
```

```python out
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÃ‘ADO'
'>> Review: Me llegÃ³ el dÃ­a que tocaba, junto a otros libros que pedÃ­, pero la caja llegÃ³ en mal estado lo cual daÃ±Ã³ las esquinas de los libros porque venÃ­an sin protecciÃ³n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

é€™ç•¶ç„¶çœ‹èµ·ä¾†åƒæ˜¯è‹±èªå’Œè¥¿ç­ç‰™èªè©•è«–çš„æ··åˆï¼ç¾åœ¨æˆ‘å€‘æœ‰äº†ä¸€å€‹è¨“ç·´èªæ–™åº«ï¼Œæœ€å¾Œè¦æª¢æŸ¥çš„ä¸€ä»¶äº‹æ˜¯è©•è«–ä¸­å–®è©çš„åˆ†ä½ˆåŠå…¶æ¨™é¡Œã€‚é€™å°æ–¼æ‘˜è¦ä»»å‹™å°¤å…¶é‡è¦ï¼Œå…¶ä¸­æ•¸æ“šä¸­çš„ç°¡çŸ­åƒè€ƒæ‘˜è¦æœƒä½¿æ¨¡å‹åå‘æ–¼åƒ…åœ¨ç”Ÿæˆçš„æ‘˜è¦ä¸­è¼¸å‡ºä¸€å…©å€‹å–®è©ã€‚ä¸‹é¢çš„åœ–é¡¯ç¤ºäº†å–®è©åˆ†ä½ˆï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°æœ‰äº›æ¨™é¡Œåš´é‡åå‘æ–¼ 1-2 å€‹å–®è©ï¼š

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>
</div>

ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘å°‡éæ¿¾æ‰æ¨™é¡Œéå¸¸çŸ­çš„ç¤ºä¾‹ï¼Œä»¥ä¾¿æˆ‘å€‘çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´æœ‰è¶£çš„æ‘˜è¦ã€‚ç”±æ–¼æˆ‘å€‘æ­£åœ¨è™•ç†è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡æ–‡æœ¬ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥ä½¿ç”¨ç²—ç•¥çš„å•Ÿç™¼å¼æ–¹æ³•åœ¨ç©ºç™½è™•æ‹†åˆ†æ¨™é¡Œï¼Œç„¶å¾Œä½¿ç”¨æˆ‘å€‘å¯ä¿¡è³´çš„ **Dataset.filter()** æ–¹æ³•å¦‚ä¸‹ï¼š

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

ç¾åœ¨æˆ‘å€‘å·²ç¶“æº–å‚™å¥½äº†æˆ‘å€‘çš„èªæ–™åº«ï¼Œè®“æˆ‘å€‘ä¾†çœ‹çœ‹ä¸€äº›å¯ä»¥å°å…¶é€²è¡Œå¾®èª¿çš„å¯èƒ½çš„ Transformer æ¨¡å‹ï¼

## æ–‡æœ¬æ‘˜è¦æ¨¡å‹

å¦‚æœä½ ä»”ç´°æƒ³æƒ³ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç¨®é¡ä¼¼æ–¼æ©Ÿå™¨ç¿»è­¯çš„ä»»å‹™ï¼šæˆ‘å€‘æœ‰ä¸€å€‹åƒè©•è«–é€™æ¨£çš„æ–‡æœ¬æ­£æ–‡ï¼Œæˆ‘å€‘å¸Œæœ›å°‡å…¶â€œç¿»è­¯â€æˆä¸€å€‹è¼ƒçŸ­çš„ç‰ˆæœ¬ï¼Œä»¥æ•æ‰è¼¸å…¥çš„é¡¯è‘—ç‰¹å¾µã€‚å› æ­¤ï¼Œå¤§å¤šæ•¸ç”¨æ–¼æ–‡æœ¬æ‘˜è¦çš„ Transformer æ¨¡å‹æ¡ç”¨äº†æˆ‘å€‘åœ¨[ç¬¬ä¸€ç« ](/course/chapter1)é‡åˆ°çš„ç·¨ç¢¼å™¨-è§£ç¢¼å™¨æ¶æ§‹ã€‚å„˜ç®¡æœ‰ä¸€äº›ä¾‹å¤–ï¼Œä¾‹å¦‚ GPT ç³»åˆ—æ¨¡å‹ï¼Œå®ƒå€‘åœ¨few-shotï¼ˆå°‘é‡å¾®èª¿ï¼‰ä¹‹å¾Œä¹Ÿå¯ä»¥æå–æ‘˜è¦ã€‚ä¸‹è¡¨åˆ—å‡ºäº†ä¸€äº›æµè¡Œçš„é è¨“ç·´æ¨¡å‹ï¼Œå¯ä»¥å°å…¶é€²è¡Œå¾®èª¿ä»¥é€²è¡Œå½™ç¸½ã€‚

| Transformer æ¨¡å‹ | æè¿°                                                                                                                                                                                                    | å¤šç¨®èªè¨€? |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | é›–ç„¶è¨“ç·´ç‚ºè‡ªè¿´æ­¸èªè¨€æ¨¡å‹ï¼Œä½†æ‚¨å¯ä»¥é€šéåœ¨è¼¸å…¥æ–‡æœ¬æœ«å°¾é™„åŠ â€œTL;DRâ€ä¾†ä½¿ GPT-2 ç”Ÿæˆæ‘˜è¦ã€‚                                                                          |      âŒ       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | åœ¨é è¨“ç·´æ˜¯çš„ç›®æ¨™æ˜¯ä¾†é æ¸¬å¤šå¥å­æ–‡æœ¬ä¸­çš„å±è”½å¥å­ã€‚ é€™å€‹é è¨“ç·´ç›®æ¨™æ¯”æ™®é€šèªè¨€å»ºæ¨¡æ›´æ¥è¿‘æ–‡æœ¬æ‘˜è¦ï¼Œä¸¦ä¸”åœ¨æµè¡Œçš„åŸºæº–æ¸¬è©¦ä¸­å¾—åˆ†å¾ˆé«˜ã€‚ |      âŒ       |
|     [T5](https://huggingface.co/t5-base)      | é€šç”¨çš„ Transformer æ¶æ§‹ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬çš„æ¡†æ¶ä¸­åˆ¶å®šæ‰€æœ‰ä»»å‹™ï¼› ä¾‹å¦‚ï¼Œæ¨¡å‹æ–‡æœ¬æ‘˜è¦çš„è¼¸å…¥æ ¼å¼æ˜¯`summarize: ARTICLE`ã€‚                             |      âŒ       |
|     [mT5](https://huggingface.co/google/mt5-base)     | T5 çš„å¤šèªè¨€ç‰ˆæœ¬ï¼Œåœ¨å¤šèªè¨€ Common Crawl èªæ–™åº« (mC4) ä¸Šé€²è¡Œé è¨“ç·´ï¼Œæ¶µè“‹ 101 ç¨®èªè¨€ã€‚                                                       |      âœ…       |
|    [BART](https://huggingface.co/facebook/bart-base)     | ä¸€ç¨®æ–°ç©çš„ Transformer æ¶æ§‹ï¼Œå…¶ä¸­åŒ…å«ç¶“éè¨“ç·´çš„ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨å †æ£§ï¼Œä»¥é‡å»ºè¢«ç ´å£çš„è¼¸å…¥ï¼Œçµåˆäº† BERT å’Œ GPT-2 çš„é è¨“ç·´æ–¹æ¡ˆã€‚                                   |      âŒ       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | BART çš„å¤šèªè¨€ç‰ˆæœ¬ï¼Œé è¨“ç·´äº† 50 ç¨®èªè¨€ã€‚                                                                                                                                                    |      âœ…       |

å¾æ­¤è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¤§å¤šæ•¸ç”¨æ–¼æ‘˜è¦çš„ Transformer æ¨¡å‹ï¼ˆä»¥åŠå¤§å¤šæ•¸ NLP ä»»å‹™ï¼‰éƒ½æ˜¯å–®èªçš„ã€‚å¦‚æœæ‚¨çš„ä»»å‹™æ˜¯ä½¿ç”¨â€œæœ‰å¤§é‡èªæ–™åº«â€çš„èªè¨€ï¼ˆå¦‚è‹±èªæˆ–å¾·èªï¼‰ï¼Œé€™å¾ˆå¥½ï¼Œä½†å°æ–¼ä¸–ç•Œå„åœ°æ­£åœ¨ä½¿ç”¨çš„æ•¸åƒç¨®å…¶ä»–èªè¨€ï¼Œå‰‡ä¸ç„¶ã€‚å¹¸é‹çš„æ˜¯ï¼Œæœ‰ä¸€é¡å¤šèªè¨€ Transformer æ¨¡å‹ï¼Œå¦‚ mT5 å’Œ mBARTï¼Œå¯ä»¥è§£æ±ºå•é¡Œã€‚é€™äº›æ¨¡å‹æ˜¯ä½¿ç”¨èªè¨€å»ºæ¨¡é€²è¡Œé è¨“ç·´çš„ï¼Œä½†æœ‰ä¸€é»ä¸åŒï¼šå®ƒå€‘ä¸æ˜¯åœ¨ä¸€ç¨®èªè¨€çš„èªæ–™åº«ä¸Šè¨“ç·´ï¼Œè€Œæ˜¯åŒæ™‚åœ¨ 50 å¤šç¨®èªè¨€çš„æ–‡æœ¬ä¸Šé€²è¡Œè¯åˆè¨“ç·´ï¼

æˆ‘å€‘å°‡ä½¿ç”¨ mT5ï¼Œé€™æ˜¯ä¸€ç¨®åŸºæ–¼ T5 çš„æœ‰è¶£æ¶æ§‹ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶ä¸­é€²è¡Œäº†é è¨“ç·´ã€‚åœ¨ T5 ä¸­ï¼Œæ¯å€‹ NLP ä»»å‹™éƒ½æ˜¯æ ¹æ“šæç¤ºå‰ç¶´ä¾†åˆ¶å®šçš„ï¼Œä¾‹å¦‚ **summarize:** é€™ä½¿æ¨¡å‹ä½¿ç”Ÿæˆçš„æ–‡æœ¬é©æ‡‰æç¤ºã€‚å¦‚ä¸‹åœ–æ‰€ç¤ºï¼Œé€™è®“ T5 è®Šå¾—éå¸¸é€šç”¨ï¼Œå› ç‚ºä½ å¯ä»¥ç”¨ä¸€å€‹æ¨¡å‹è§£æ±ºå¾ˆå¤šä»»å‹™ï¼


<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>
</div>

mT5 ä¸ä½¿ç”¨å‰ç¶´ï¼Œä½†å…·æœ‰ T5 çš„å¤§éƒ¨åˆ†åŠŸèƒ½ï¼Œä¸¦ä¸”å…·æœ‰å¤šèªè¨€çš„å„ªå‹¢ã€‚ç¾åœ¨æˆ‘å€‘å·²ç¶“é¸æ“‡äº†ä¸€å€‹æ¨¡å‹ï¼Œè®“æˆ‘å€‘ä¾†çœ‹çœ‹æº–å‚™æˆ‘å€‘çš„è¨“ç·´æ•¸æ“šã€‚

<Tip>

âœï¸ **è©¦è©¦çœ‹ï¼** å®Œæˆæœ¬ç¯€å¾Œï¼Œé€šéä½¿ç”¨ç›¸åŒçš„æŠ€è¡“å° mBART é€²è¡Œå¾®èª¿ï¼Œçœ‹çœ‹ mT5 èˆ‡ mBART ç›¸æ¯”æœ‰å¤šå¥½ã€‚ å°æ–¼çå‹µç©åˆ†ï¼Œæ‚¨é‚„å¯ä»¥å˜—è©¦åƒ…åœ¨è‹±æ–‡è©•è«–ä¸Šå¾®èª¿ T5ã€‚ ç”±æ–¼ T5 éœ€è¦ä¸€å€‹ç‰¹æ®Šçš„å‰ç¶´æç¤ºï¼Œå› æ­¤æ‚¨éœ€è¦åœ¨ä¸‹é¢çš„é è™•ç†æ­¥é©Ÿä¸­å°‡â€œsummarize:â€æ·»åŠ åˆ°è¼¸å…¥ç¤ºä¾‹ä¸­ã€‚

</Tip>

## é è™•ç†æ•¸æ“š

<Youtube id="1m7BerpSq8A"/>

æˆ‘å€‘çš„ä¸‹ä¸€å€‹ä»»å‹™æ˜¯å°æˆ‘å€‘çš„è©•è«–åŠå…¶æ¨™é¡Œé€²è¡Œæ¨™è¨˜å’Œç·¨ç¢¼ã€‚åƒå¾€å¸¸ä¸€æ¨£ï¼Œæˆ‘å€‘é¦–å…ˆåŠ è¼‰èˆ‡é è¨“ç·´æ¨¡å‹æª¢æŸ¥é»ç›¸é—œçš„æ¨™è¨˜å™¨ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ **mt5-small** ä½œç‚ºæˆ‘å€‘çš„æª¢æŸ¥é»ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥åœ¨åˆç†çš„æ™‚é–“å…§å¾®èª¿æ¨¡å‹ï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

ğŸ’¡åœ¨ NLP é …ç›®çš„æ—©æœŸéšæ®µï¼Œä¸€å€‹å¥½çš„åšæ³•æ˜¯åœ¨å°æ¨£æœ¬æ•¸æ“šä¸Šè¨“ç·´ä¸€é¡â€œå°â€æ¨¡å‹ã€‚é€™ä½¿æ‚¨å¯ä»¥æ›´å¿«åœ°èª¿è©¦å’Œè¿­ä»£ç«¯åˆ°ç«¯å·¥ä½œæµã€‚ä¸€æ—¦æ‚¨å°çµæœå……æ»¿ä¿¡å¿ƒï¼Œæ‚¨å§‹çµ‚å¯ä»¥é€šéç°¡å–®åœ°æ›´æ”¹æ¨¡å‹æª¢æŸ¥é»ä¾†åœ¨å¤§è¦æ¨¡æ•¸æ“šä¸Šè¨“ç·´æ¨¡å‹ï¼

</Tip>

è®“æˆ‘å€‘åœ¨ä¸€å€‹å°ä¾‹å­ä¸Šæ¸¬è©¦ mT5 æ¨™è¨˜å™¨ï¼š

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åœ¨é€™è£¡æˆ‘å€‘å¯ä»¥çœ‹åˆ°æˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ](/course/chapter3)ç¬¬ä¸€æ¬¡å¾®èª¿å¯¦é©—ä¸­é‡åˆ°çš„ç†Ÿæ‚‰çš„ **input_ids** å’Œ **attention_mask** .è®“æˆ‘å€‘ç”¨åˆ†è©å™¨è§£ç¢¼é€™äº›è¼¸å…¥ ID ï¼Œå¯ä»¥**convert_ids_to_tokens()** å‡½æ•¸ä¾†æŸ¥çœ‹æˆ‘å€‘æ­£åœ¨è™•ç†ä»€éº¼æ¨£çš„æ¨™è¨˜å™¨ï¼š

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']
```

ç‰¹æ®Šçš„ Unicode å­—ç¬¦ `â–` å’Œåºåˆ—çµæŸæ¨™è¨˜ `</s>` è¡¨æ˜æˆ‘å€‘æ­£åœ¨è™•ç† SentencePiece åˆ†è©å™¨ï¼Œå®ƒåŸºæ–¼åœ¨[ç¬¬å…­ç« ](/course/chapter6)ä¸­è¨è«–çš„Unigramåˆ†è©ç®—æ³•. Unigram å°å¤šèªè¨€èªæ–™åº«ç‰¹åˆ¥æœ‰ç”¨ï¼Œå› ç‚ºå®ƒå…è¨± SentencePiece ä¸çŸ¥é“é‡éŸ³ã€æ¨™é»ç¬¦è™Ÿä»¥åŠè¨±å¤šèªè¨€ï¼ˆå¦‚æ—¥èªï¼‰æ²’æœ‰ç©ºæ ¼å­—ç¬¦ã€‚

ç‚ºäº†æ¨™è¨˜æˆ‘å€‘çš„èªæ–™åº«ï¼Œæˆ‘å€‘å¿…é ˆè™•ç†èˆ‡æ‘˜è¦ç›¸é—œçš„ç´°ç¯€ï¼šå› ç‚ºæˆ‘å€‘çš„æ¨™ç±¤ä¹Ÿæ˜¯æ–‡æœ¬ï¼Œå®ƒå€‘å¯èƒ½æœƒè¶…éæ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚é€™æ„å‘³è‘—æˆ‘å€‘éœ€è¦å°è©•è«–åŠå…¶æ¨™é¡Œé€²è¡Œæˆªæ–·ï¼Œä»¥ç¢ºä¿æˆ‘å€‘ä¸æœƒå°‡éé•·çš„è¼¸å…¥å‚³éçµ¦æˆ‘å€‘çš„æ¨¡å‹ã€‚ ğŸ¤— Transformers ä¸­çš„åˆ†è©å™¨æä¾›äº†ä¸€å€‹æ¼‚äº®çš„ **as_target_tokenizer()** å‡½æ•¸ï¼Œå®ƒå…è¨±æ‚¨ä¸¦è¡Œåˆ†è©ä¸¦æ¨™è¨˜æ¨™ç±¤çš„å‡½æ•¸ã€‚é€™é€šå¸¸æ˜¯ä½¿ç”¨é è™•ç†å‡½æ•¸å…§çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®Œæˆçš„ï¼Œè©²å‡½æ•¸é¦–å…ˆå°è¼¸å…¥é€²è¡Œç·¨ç¢¼ï¼Œç„¶å¾Œå°‡æ¨™ç±¤ç·¨ç¢¼ç‚ºå–®ç¨çš„åˆ—ã€‚ä»¥ä¸‹æ˜¯ mT5 çš„æ­¤å‡½æ•¸çš„ç¤ºä¾‹ï¼š

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"], max_length=max_input_length, truncation=True
    )
    # Set up the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["review_title"], max_length=max_target_length, truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

è®“æˆ‘å€‘é€šéé€™æ®µä»£ç¢¼ä¾†äº†è§£ç™¼ç”Ÿäº†ä»€éº¼ã€‚æˆ‘å€‘åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å®šç¾©å€¼ **max_input_length** å’Œ **max_target_length** ï¼Œå®ƒç‚ºæˆ‘å€‘çš„è©•è«–å’Œæ¨™é¡Œçš„é•·åº¦è¨­ç½®äº†ä¸Šé™ã€‚ç”±æ–¼è©•è«–æ­£æ–‡é€šå¸¸æ¯”æ¨™é¡Œå¤§å¾—å¤šï¼Œæˆ‘å€‘ç›¸æ‡‰åœ°èª¿æ•´äº†é€™äº›å€¼ã€‚ç„¶å¾Œï¼Œåœ¨ **preprocess_function()** æˆ‘å€‘å¯ä»¥çœ‹åˆ°è©•è«–é¦–å…ˆè¢«æ¨™è¨˜åŒ–ï¼Œç„¶å¾Œæ˜¯æ¨™é¡Œåœ¨ **as_target_tokenizer()** å‡½æ•¸é‡Œä¹Ÿåšäº†ç›¸åŒçš„è™•ç†.

æœ‰äº† `preprocess_function()`ï¼Œæˆ‘å€‘åœ¨æ•´å€‹èª²ç¨‹ä¸­å»£æ³›ä½¿ç”¨çš„æ–¹ä¾¿çš„ `Dataset.map()` å‡½æ•¸ä¾†æ¨™è¨˜æ•´å€‹èªæ–™åº«æ˜¯ä¸€ä»¶ç°¡å–®çš„äº‹æƒ…ï¼š

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

æ—¢ç„¶èªæ–™åº«å·²ç¶“é è™•ç†å®Œç•¢ï¼Œæˆ‘å€‘ä¾†çœ‹çœ‹ä¸€äº›å¸¸ç”¨çš„æ‘˜è¦æŒ‡æ¨™ã€‚æ­£å¦‚æˆ‘å€‘å°‡çœ‹åˆ°çš„ï¼Œåœ¨è¡¡é‡æ©Ÿå™¨ç”Ÿæˆçš„æ–‡æœ¬çš„è³ªé‡æ–¹é¢æ²’æœ‰éˆä¸¹å¦™è—¥ã€‚

<Tip>

ğŸ’¡ ä½ å¯èƒ½å·²ç¶“æ³¨æ„åˆ°æˆ‘å€‘åœ¨ä¸Šé¢çš„ `Dataset.map()` å‡½æ•¸ä¸­ä½¿ç”¨äº† `batched=True`ã€‚ é€™æœƒä»¥ 1,000 å€‹ï¼ˆé»˜èªï¼‰ç‚ºå–®ä½å°ç¤ºä¾‹é€²è¡Œç·¨ç¢¼ï¼Œä¸¦å…è¨±æ‚¨åˆ©ç”¨ ğŸ¤— Transformers ä¸­å¿«é€Ÿæ¨™è¨˜å™¨çš„å¤šç·šç¨‹åŠŸèƒ½ã€‚ åœ¨å¯èƒ½çš„æƒ…æ³ä¸‹ï¼Œå˜—è©¦ä½¿ç”¨ `batched=True` ä¾†åŠ é€Ÿæ‚¨çš„é è™•ç†ï¼

</Tip>


## æ–‡æœ¬æ‘˜è¦çš„æŒ‡æ¨™

<Youtube id="TMshhnrEXlg"/>

èˆ‡æˆ‘å€‘åœ¨æœ¬èª²ç¨‹ä¸­æ¶µè“‹çš„å¤§å¤šæ•¸å…¶ä»–ä»»å‹™ç›¸æ¯”ï¼Œè¡¡é‡æ–‡æœ¬ç”Ÿæˆä»»å‹™ï¼ˆå¦‚æ‘˜è¦æˆ–ç¿»è­¯ï¼‰çš„æ€§èƒ½ä¸¦ä¸é‚£éº¼ç°¡å–®ã€‚ä¾‹å¦‚ï¼Œå°æ–¼â€œæˆ‘å–œæ­¡é–±è®€é£¢é¤“éŠæˆ²â€é€™æ¨£çš„è©•è«–ï¼Œæœ‰å¤šå€‹æœ‰æ•ˆæ‘˜è¦ï¼Œä¾‹å¦‚â€œæˆ‘å–œæ­¡é£¢é¤“éŠæˆ²â€æˆ–â€œé£¢é¤“éŠæˆ²æ˜¯ä¸€æœ¬å¥½æ›¸â€ã€‚é¡¯ç„¶ï¼Œåœ¨ç”Ÿæˆçš„æ‘˜è¦å’Œæ¨™ç±¤ä¹‹é–“æ‡‰ç”¨æŸç¨®ç²¾ç¢ºåŒ¹é…ä¸¦ä¸æ˜¯ä¸€å€‹å¥½çš„è§£æ±ºæ–¹æ¡ˆâ€”â€”å³ä½¿æ˜¯äººé¡åœ¨é€™æ¨£çš„æŒ‡æ¨™ä¸‹ä¹Ÿæœƒè¡¨ç¾ä¸ä½³ï¼Œå› ç‚ºæˆ‘å€‘éƒ½æœ‰è‡ªå·±çš„å¯«ä½œé¢¨æ ¼ã€‚

ç¸½è€Œè¨€ä¹‹ï¼Œæœ€å¸¸ç”¨çš„æŒ‡æ¨™ä¹‹ä¸€æ˜¯[ROUGE åˆ†æ•¸](https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼ˆRecall-Oriented Understudy for Gisting Evaluation çš„ç¸®å¯«ï¼‰ã€‚è©²æŒ‡æ¨™èƒŒå¾Œçš„åŸºæœ¬æ€æƒ³æ˜¯å°‡ç”Ÿæˆçš„æ‘˜è¦èˆ‡ä¸€çµ„é€šå¸¸ç”±äººé¡å‰µå»ºçš„åƒè€ƒæ‘˜è¦é€²è¡Œæ¯”è¼ƒã€‚ç‚ºäº†æ›´ç²¾ç¢ºï¼Œå‡è¨­æˆ‘å€‘è¦æ¯”è¼ƒä»¥ä¸‹å…©å€‹æ‘˜è¦ï¼š

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```
æ¯”è¼ƒå®ƒå€‘çš„ä¸€ç¨®æ–¹æ³•æ˜¯è¨ˆç®—é‡ç–Šå–®è©çš„æ•¸é‡ï¼Œåœ¨é€™ç¨®æƒ…æ³ä¸‹ç‚º 6ã€‚ä½†æ˜¯ï¼Œé€™æœ‰é»ç²—ç³™ï¼Œå› æ­¤ ROUGE æ˜¯åŸºæ–¼è¨ˆç®—è¨ˆç®—é‡ç–Šçš„ _precision_ å’Œ _recall_ åˆ†æ•¸ã€‚ã€‚

<Tip>

ğŸ™‹ å¦‚æœé€™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡è½èªªç²¾ç¢ºç‡å’Œå¬å›ç‡ï¼Œè«‹ä¸è¦æ“”å¿ƒâ€”â€”æˆ‘å€‘å°‡ä¸€èµ·é€šéä¸€äº›æ˜ç¢ºçš„ç¤ºä¾‹ä¾†èªªæ˜ä¸€åˆ‡ã€‚ é€™äº›æŒ‡æ¨™é€šå¸¸åœ¨åˆ†é¡ä»»å‹™ä¸­é‡åˆ°ï¼Œå› æ­¤å¦‚æœæ‚¨æƒ³äº†è§£åœ¨è©²ä¸Šä¸‹æ–‡ä¸­å¦‚ä½•å®šç¾©ç²¾ç¢ºåº¦å’Œå¬å›ç‡ï¼Œæˆ‘å€‘å»ºè­°æŸ¥çœ‹ scikit-learn [æŒ‡å—](https://scikit-learn.org/stable /auto_examples/model_selection/plot_precision_recall.htmlï¼‰ã€‚

</Tip>

å°æ–¼ ROUGEï¼Œrecall è¡¡é‡ç”Ÿæˆçš„åƒè€ƒæ‘˜è¦åŒ…å«äº†å¤šå°‘åƒè€ƒæ‘˜è¦ã€‚å¦‚æœæˆ‘å€‘åªæ˜¯æ¯”è¼ƒå–®è©ï¼Œrecallå¯ä»¥æ ¹æ“šä»¥ä¸‹å…¬å¼è¨ˆç®—ï¼š

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

å°æ–¼æˆ‘å€‘ä¸Šé¢çš„ç°¡å–®ä¾‹å­ï¼Œé€™å€‹å…¬å¼çµ¦å‡ºäº† 6/6 = 1 çš„å®Œç¾å¬å›ç‡ï¼›å³ï¼Œåƒè€ƒæ‘˜è¦ä¸­çš„æ‰€æœ‰å–®è©éƒ½å·²ç”±æ¨¡å‹ç”Ÿæˆã€‚é€™è½èµ·ä¾†å¯èƒ½å¾ˆæ£’ï¼Œä½†æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘å€‘ç”Ÿæˆçš„æ‘˜è¦æ˜¯â€œæˆ‘çœŸçš„å¾ˆå–œæ­¡æ•´æ™šé–±è®€é£¢é¤“éŠæˆ²â€ã€‚é€™ä¹Ÿå°‡æœ‰å®Œç¾çš„recallï¼Œä½†å¯ä»¥èªªæ˜¯ä¸€å€‹æ›´ç³Ÿç³•çš„ç¸½çµï¼Œå› ç‚ºå®ƒå¾ˆå†—é•·ã€‚ç‚ºäº†è™•ç†é€™äº›å ´æ™¯ï¼Œæˆ‘å€‘é‚„è¨ˆç®—äº†pecisionï¼Œå®ƒåœ¨ ROUGE ä¸Šä¸‹æ–‡ä¸­è¡¡é‡ç”Ÿæˆçš„æ‘˜è¦ä¸­æœ‰å¤šå°‘æ˜¯ç›¸é—œçš„ï¼š

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

å°‡æ­¤æ‡‰ç”¨åˆ°æˆ‘å€‘çš„è©³ç´°æ‘˜è¦ä¸­æœƒå¾—åˆ° 6/10 = 0.6 çš„ç²¾åº¦ï¼Œé€™æ¯”æˆ‘å€‘è¼ƒçŸ­çš„æ‘˜è¦ç²å¾—çš„ 6/7 = 0.86 çš„ç²¾åº¦è¦å·®å¾—å¤šã€‚åœ¨å¯¦è¸ä¸­ï¼Œé€šå¸¸è¨ˆç®—ç²¾åº¦å’Œå¬å›ç‡ï¼Œç„¶å¾Œå ±å‘Š F1-scoreï¼ˆç²¾åº¦å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡å€¼ï¼‰ã€‚æˆ‘å€‘å¯ä»¥åœ¨ ğŸ¤— Datasets ä¸­é€šéå®‰è£ **rouge_score** åŒ…ä¾†è¨ˆç®—ä»–å€‘ï¼š

```py
!pip install rouge_score
```

ç„¶å¾ŒæŒ‰å¦‚ä¸‹æ–¹å¼åŠ è¼‰ ROUGE æŒ‡æ¨™ï¼š

```python
from datasets import load_metric

rouge_score = load_metric("rouge")
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ **rouge_score.compute()** ä¸€æ¬¡æ€§è¨ˆç®—æ‰€æœ‰æŒ‡æ¨™çš„å‡½æ•¸ï¼š

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

å“‡ï¼Œé‚£å€‹è¼¸å‡ºä¸­æœ‰å¾ˆå¤šä¿¡æ¯â€”â€”é€™éƒ½æ˜¯ä»€éº¼æ„æ€ï¼Ÿé¦–å…ˆï¼ŒğŸ¤— Datasetså¯¦éš›ä¸Šè¨ˆç®—äº†ç²¾åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•¸çš„ç½®ä¿¡å€é–“ï¼›é€™äº›æ˜¯ä½ å¯ä»¥åœ¨é€™è£¡çœ‹åˆ°çš„ **low** , **mid** ï¼Œ å’Œ **high** å±¬æ€§ã€‚æ­¤å¤–ï¼ŒğŸ¤— Datasetsåœ¨æ¯”è¼ƒç”Ÿæˆæ‘˜è¦å’Œåƒè€ƒæ‘˜è¦æ™‚ï¼Œæœƒæ ¹æ“šä¸åŒé¡å‹çš„æ–‡æœ¬ç²’åº¦è¨ˆç®—å„ç¨® ROUGE åˆ†æ•¸ã€‚é€™ **rouge1** è®Šé«”æ˜¯ä¸€å…ƒçµ„çš„é‡ç–Šâ€”â€”é€™åªæ˜¯è¡¨é”å–®è©é‡ç–Šçš„ä¸€ç¨®å¥‡ç‰¹æ–¹å¼ï¼Œé€™æ­£æ˜¯æˆ‘å€‘ä¸Šé¢è¨è«–çš„åº¦é‡æ¨™æº–ã€‚ç‚ºäº†é©—è­‰é€™ä¸€é»ï¼Œè®“æˆ‘å€‘è¼¸å‡º **mid** çš„æ•¸å€¼ï¼š

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```
å¤ªå¥½äº†ï¼Œæº–ç¢ºç‡å’Œå¬å›ç‡åŒ¹é…äº†ï¼é‚£éº¼å…¶ä»–çš„ ROUGE åˆ†æ•¸å‘¢ï¼Ÿ **rouge2** æ¸¬é‡äºŒå…ƒçµ„ä¹‹é–“çš„é‡ç–Šï¼ˆæƒ³æƒ³å–®è©å°çš„é‡ç–Šï¼‰ï¼Œè€Œ **rougeL** å’Œ **rougeLsum** é€šéåœ¨ç”Ÿæˆçš„å’Œåƒè€ƒæ‘˜è¦ä¸­æŸ¥æ‰¾æœ€é•·çš„å…¬å…±å­ä¸²ä¾†æ¸¬é‡æœ€é•·çš„å–®è©åŒ¹é…åºåˆ—ã€‚ä¸­çš„â€œç¸½å’Œâ€ **rougeLsum** æŒ‡çš„æ˜¯é€™å€‹æŒ‡æ¨™æ˜¯åœ¨æ•´å€‹æ‘˜è¦ä¸Šè¨ˆç®—çš„ï¼Œè€Œ **rougeL** è¨ˆç®—ç‚ºå–®å€‹å¥å­çš„å¹³å‡å€¼ã€‚

<Tip>

    âœï¸ **è©¦è©¦çœ‹ï¼** å‰µå»ºæ‚¨è‡ªå·±çš„ç”Ÿæˆå’Œåƒè€ƒæ‘˜è¦ç¤ºä¾‹ï¼Œä¸¦æŸ¥çœ‹ç”Ÿæˆçš„ ROUGE åˆ†æ•¸æ˜¯å¦èˆ‡åŸºæ–¼ç²¾ç¢ºåº¦å’Œå¬å›ç‡å…¬å¼çš„æ‰‹å‹•è¨ˆç®—ä¸€è‡´ã€‚ å°æ–¼é™„åŠ åˆ†ï¼Œå°‡æ–‡æœ¬æ‹†åˆ†ç‚ºäºŒå…ƒçµ„ä¸¦æ¯”è¼ƒâ€œrouge2â€æŒ‡æ¨™çš„ç²¾åº¦å’Œå¬å›ç‡ã€‚

</Tip>

æˆ‘å€‘å°‡ä½¿ç”¨é€™äº› ROUGE åˆ†æ•¸ä¾†è·Ÿè¹¤æˆ‘å€‘æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®“æˆ‘å€‘åšæ¯å€‹å„ªç§€çš„ NLP å¾æ¥­è€…éƒ½æ‡‰è©²åšçš„äº‹æƒ…ï¼šå‰µå»ºä¸€å€‹å¼·å¤§è€Œç°¡å–®çš„baselineï¼

### å‰µå»ºå¼·å¤§çš„baseline

æ–‡æœ¬æ‘˜è¦çš„ä¸€å€‹å¸¸è¦‹åŸºç·šæ˜¯ç°¡å–®åœ°å–ä¸€ç¯‡æ–‡ç« çš„å‰ä¸‰å€‹å¥å­ï¼Œé€šå¸¸ç¨±ç‚º _lead-3_ åŸºç·šã€‚ æˆ‘å€‘å¯ä»¥ä½¿ç”¨å¥è™Ÿ(è‹±æ–‡ä½¿ç”¨.)ä¾†è·Ÿè¹¤å¥å­é‚Šç•Œï¼Œä½†é€™åœ¨"U.S." or "U.N."ä¹‹é¡çš„é¦–å­—æ¯ç¸®ç•¥è©ä¸Šæœƒå¤±æ•—ã€‚æ‰€ä»¥æˆ‘å€‘å°‡ä½¿ç”¨ `nltk` åº«ï¼Œå®ƒåŒ…å«ä¸€å€‹æ›´å¥½çš„ç®—æ³•ä¾†è™•ç†é€™äº›æƒ…æ³ã€‚ æ‚¨å¯ä»¥ä½¿ç”¨ `pip` å®‰è£è»Ÿä»¶åŒ…ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
!pip install nltk
```

ç„¶å¾Œä¸‹è¼‰æ¨™é»è¦å‰‡ï¼š

```python
import nltk

nltk.download("punkt")
```
æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å¾ `nltk` å°å…¥å¥å­æ¨™è¨˜å™¨ä¸¦å‰µå»ºä¸€å€‹ç°¡å–®çš„å‡½æ•¸ä¾†æå–è©•è«–ä¸­çš„å‰ä¸‰å€‹å¥å­ã€‚ æ–‡æœ¬æ‘˜è¦çš„ç´„å®šæ˜¯ç”¨æ›è¡Œç¬¦åˆ†éš”æ¯å€‹æ‘˜è¦ï¼Œå› æ­¤æˆ‘å€‘ä¹Ÿå°‡å…¶åŒ…å«åœ¨å…§ä¸¦åœ¨è¨“ç·´ç¤ºä¾‹ä¸Šå°å…¶é€²è¡Œæ¸¬è©¦ï¼š

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

é€™ä¼¼ä¹æœ‰æ•ˆï¼Œæ‰€ä»¥è®“æˆ‘å€‘ç¾åœ¨å¯¦ç¾ä¸€å€‹å‡½æ•¸ï¼Œå¾æ•¸æ“šé›†ä¸­æå–é€™äº›â€œæ‘˜è¦â€ä¸¦è¨ˆç®—baselineçš„ ROUGE åˆ†æ•¸ï¼š

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨é€™å€‹å‡½æ•¸ä¾†è¨ˆç®—é©—è­‰é›†ä¸Šçš„ ROUGE åˆ†æ•¸ï¼Œä¸¦ä½¿ç”¨ Pandas å°å®ƒå€‘é€²è¡Œä¸€äº›ç¾åŒ–ï¼š

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

æˆ‘å€‘å¯ä»¥çœ‹åˆ°`rouge2`çš„åˆ†æ•¸æ˜é¡¯ä½æ–¼å…¶ä»–ï¼› é€™å¯èƒ½åæ˜ äº†é€™æ¨£ä¸€å€‹äº‹å¯¦ï¼Œå³è©•è«–æ¨™é¡Œé€šå¸¸å¾ˆç°¡æ½”ï¼Œå› æ­¤lead-3 baselineéæ–¼å†—é•·ã€‚ ç¾åœ¨æˆ‘å€‘æœ‰äº†ä¸€å€‹å¾ˆå¥½çš„åŸºæº–ï¼Œè®“æˆ‘å€‘å°‡æ³¨æ„åŠ›è½‰å‘å¾®èª¿ mT5ï¼

{#if fw === 'pt'}

## ä½¿ç”¨ `Trainer` APIå¾®èª¿mT5 

å¾®èª¿æ¨¡å‹ä»¥é€²è¡Œæå–æ‘˜è¦èˆ‡æˆ‘å€‘åœ¨æœ¬ç« ä¸­ä»‹ç´¹çš„å…¶ä»–ä»»å‹™éå¸¸ç›¸ä¼¼ã€‚ æˆ‘å€‘éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å¾`mt5-small`æª¢æŸ¥é»åŠ è¼‰é è¨“ç·´æ¨¡å‹ã€‚ ç”±æ–¼æ‘˜è¦æå–æ˜¯ä¸€å€‹åºåˆ—åˆ°åºåˆ—çš„ä»»å‹™ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ AutoModelForSeq2SeqLM é¡åŠ è¼‰æ¨¡å‹ï¼Œè©²é¡æœƒè‡ªå‹•ä¸‹è¼‰ä¸¦ç·©å­˜æ¬Šé‡ï¼š

```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ä½¿ç”¨ `Keras` APIå¾®èª¿mT5 

å¾®èª¿æ¨¡å‹ä»¥é€²è¡Œæå–æ‘˜è¦èˆ‡æˆ‘å€‘åœ¨æœ¬ç« ä¸­ä»‹ç´¹çš„å…¶ä»–ä»»å‹™éå¸¸ç›¸ä¼¼ã€‚ æˆ‘å€‘éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å¾`mt5-small`æª¢æŸ¥é»åŠ è¼‰é è¨“ç·´æ¨¡å‹ã€‚ ç”±æ–¼æ‘˜è¦æå–æ˜¯ä¸€å€‹åºåˆ—åˆ°åºåˆ—çš„ä»»å‹™ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ AutoModelForSeq2SeqLM é¡åŠ è¼‰æ¨¡å‹ï¼Œè©²é¡æœƒè‡ªå‹•ä¸‹è¼‰ä¸¦ç·©å­˜æ¬Šé‡ï¼š

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{/if}

<Tip>

ğŸ’¡ If you're wondering why you don't see any warnings about fine-tuning the model on a downstream task, that's because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in [Chapter 3](/course/chapter3), where the head of the pretrained model was replaced with a randomly initialized network.
ğŸ’¡ å¦‚æœæ‚¨æƒ³çŸ¥é“ç‚ºä»€éº¼åœ¨ä¸‹æ¸¸ä»»å‹™ä¸­æ²’æœ‰çœ‹åˆ°ä»»ä½•é—œæ–¼å¾®èª¿æ¨¡å‹çš„è­¦å‘Šï¼Œé‚£æ˜¯å› ç‚ºå°æ–¼åºåˆ—åˆ°åºåˆ—çš„ä»»å‹™ï¼Œæˆ‘å€‘ä¿ç•™äº†ç¶²çµ¡çš„æ‰€æœ‰æ¬Šé‡ã€‚èˆ‡æˆ‘å€‘åœ¨[ç¬¬ä¸‰ç« ] (/course/chapter3)ä¸­çš„æ–‡æœ¬åˆ†é¡æ¨¡å‹é€²è¡Œæ¯”è¼ƒï¼Œæ–‡æœ¬åˆ†é¡æ¨¡å‹é è¨“ç·´æ¨¡å‹çš„é ­éƒ¨è¢«éš¨æ©Ÿåˆå§‹åŒ–çš„ç¶²çµ¡æ›¿æ›ã€‚

</Tip>

æˆ‘å€‘éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ç™»éŒ„ Hugging Face Hubã€‚å¦‚æœæ‚¨åœ¨notebookä¸­é‹è¡Œæ­¤ä»£ç¢¼ï¼Œå‰‡å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å¯¦ç”¨ç¨‹åºå‡½æ•¸åŸ·è¡Œæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

é€™å°‡é¡¯ç¤ºä¸€å€‹å°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¼¸å…¥æ‚¨çš„æ†‘æ“šã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥åœ¨çµ‚ç«¯ä¸­é‹è¡Œæ­¤å‘½ä»¤ä¸¦åœ¨é‚£è£¡ç™»éŒ„ï¼š

```
huggingface-cli login
```

{#if fw === 'pt'}

æˆ‘å€‘éœ€è¦ç”Ÿæˆæ‘˜è¦ä»¥ä¾¿åœ¨è¨“ç·´æœŸé–“è¨ˆç®— ROUGE åˆ†æ•¸ã€‚å¹¸é‹çš„æ˜¯ï¼ŒğŸ¤— Transformers æä¾›äº†å°ˆç”¨çš„ `Seq2SeqTrainingArguments` å’Œ `Seq2SeqTrainer` é¡ï¼Œå¯ä»¥è‡ªå‹•ç‚ºæˆ‘å€‘å®Œæˆé€™é …å·¥ä½œï¼ ç‚ºäº†ç­è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®“æˆ‘å€‘é¦–å…ˆç‚ºæˆ‘å€‘çš„å¯¦é©—å®šç¾©è¶…åƒæ•¸å’Œå…¶ä»–åƒæ•¸ï¼š

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

åœ¨é€™è£¡ï¼Œ **predict_with_generate** åƒæ•¸å·²è¨­ç½®ç‚ºTrueè¡¨æ˜æˆ‘å€‘æ‡‰è©²åœ¨è©•ä¼°æœŸé–“ç”Ÿæˆæ‘˜è¦ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥è¨ˆç®—æ¯å€‹æ™‚æœŸçš„ ROUGE åˆ†æ•¸ã€‚æ­£å¦‚åœ¨[ç¬¬ä¸€ç« ](/course/chapter1)æ‰€è¨è«–çš„ï¼Œè§£ç¢¼å™¨é€šéé€å€‹é æ¸¬ä»¤ç‰Œä¾†åŸ·è¡Œæ¨ç†ï¼Œé€™æ˜¯ç”±æ¨¡å‹çš„ **generate()** æ–¹æ³•å¯¦ç¾çš„ã€‚è¨­ç½® **predict_with_generate=True** å‘Šè¨´ **Seq2SeqTrainer** ä½¿ç”¨è©²æ–¹æ³•é€²è¡Œè©•ä¼°ã€‚æˆ‘å€‘é‚„èª¿æ•´äº†ä¸€äº›é»˜èªçš„è¶…åƒæ•¸ï¼Œä¾‹å¦‚å­¸ç¿’ç‡ã€epochæ•¸å’Œæ¬Šé‡è¡°æ¸›ï¼Œä¸¦ä¸”æˆ‘å€‘è¨­ç½®äº† **save_total_limit** è¨“ç·´æœŸé–“æœ€å¤šéš»ä¿å­˜ 3 å€‹æª¢æŸ¥é»çš„é¸é …â€”â€”é€™æ˜¯å› ç‚ºå³ä½¿æ˜¯ mT5 çš„â€œsmallâ€ç‰ˆæœ¬ä¹Ÿä½¿ç”¨å¤§ç´„ 1 GB çš„ç¡¬ç›¤ç©ºé–“ï¼Œæˆ‘å€‘å¯ä»¥é€šéé™åˆ¶æˆ‘å€‘ä¿å­˜çš„å‰¯æœ¬æ•¸é‡ä¾†ç¯€çœä¸€é»ç©ºé–“ã€‚

`push_to_hub=True` åƒæ•¸å°‡å…è¨±æˆ‘å€‘åœ¨è¨“ç·´å¾Œå°‡æ¨¡å‹æ¨é€åˆ° Hubï¼› æ‚¨å°‡åœ¨`output_dir`å®šç¾©çš„ä½ç½®ä¸­çš„ç”¨æˆ¶é…ç½®æ–‡ä»¶ä¸‹æ‰¾åˆ°å­˜å„²åº«ã€‚ è«‹æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `hub_model_id` åƒæ•¸æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å„²åº«çš„åç¨±ï¼ˆç‰¹åˆ¥æ˜¯ç•¶æ‚¨æƒ³è¦æ¨é€åˆ°çµ„ç¹”æ™‚ï¼Œæ‚¨å¿…é ˆä½¿ç”¨æ­¤åƒæ•¸ï¼‰ã€‚ ä¾‹å¦‚ï¼Œç•¶æˆ‘å€‘å°‡æ¨¡å‹æ¨é€åˆ° [`huggingface-course` çµ„ç¹”](https://huggingface.co/huggingface-course) æ™‚ï¼Œæˆ‘å€‘æ·»åŠ äº†`hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` åˆ° `Seq2SeqTrainingArguments`ã€‚

æˆ‘å€‘éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ç‚ºè¨“ç·´å™¨æä¾›ä¸€å€‹â€œcompute_metrics()â€å‡½æ•¸ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥åœ¨è¨“ç·´æœŸé–“è©•ä¼°æˆ‘å€‘çš„æ¨¡å‹ã€‚ ç¸½çµèµ·ä¾†ï¼Œé€™æ¯”ç°¡å–®åœ°åœ¨æ¨¡å‹çš„é æ¸¬ä¸Šèª¿ç”¨ `rouge_score.compute()` æ›´å¾©é›œä¸€äº›ï¼Œå› ç‚ºæˆ‘å€‘éœ€è¦åœ¨è¨ˆç®— ROUGE åˆ†æ•¸ä¹‹å‰å°‡è¼¸å‡ºå’Œæ¨™ç±¤è§£ç¢¼ç‚ºæ–‡æœ¬ã€‚ ä¸‹é¢çš„å‡½æ•¸æ­£æ˜¯é€™æ¨£åšçš„ï¼Œä¸¦ä¸”é‚„åˆ©ç”¨ `nltk` ä¸­çš„ `sent_tokenize()` å‡½æ•¸ä¾†ç”¨æ›è¡Œç¬¦åˆ†éš”æ‘˜è¦èªå¥ï¼š

```python
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

{/if}

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘éœ€è¦ç‚ºæˆ‘å€‘çš„åºåˆ—åˆ°åºåˆ—ä»»å‹™å®šç¾©ä¸€å€‹æ•¸æ“šæ•´ç†å™¨ã€‚ç”±æ–¼ mT5 æ˜¯ä¸€å€‹ç·¨ç¢¼å™¨-è§£ç¢¼å™¨ Transformer æ¨¡å‹ï¼Œæº–å‚™æˆ‘å€‘çš„æ‰¹æ¬¡çš„ä¸€å€‹å¾®å¦™ä¹‹è™•æ˜¯ï¼Œåœ¨è§£ç¢¼éç¨‹ä¸­ï¼Œæˆ‘å€‘éœ€è¦å°‡æ¨™ç±¤å‘å³ç§»å‹•ä¸€å€‹ã€‚ é€™æ˜¯ç‚ºäº†ç¢ºä¿è§£ç¢¼å™¨åªçœ‹åˆ°ä¹‹å‰çš„çœŸå¯¦çš„æ¨™ç±¤ï¼Œè€Œä¸æ˜¯ç•¶å‰æˆ–æœªä¾†çš„æ¨™ç±¤ï¼Œé€™å°æ–¼æ¨¡å‹ä¾†èªªå¾ˆå®¹æ˜“è¨˜æ†¶ã€‚ é€™é¡ä¼¼æ–¼åœ¨ [å› æœèªè¨€å»ºæ¨¡](/course/chapter7/6) ç­‰ä»»å‹™ä¸­å¦‚ä½•å°‡æ©è”½çš„è‡ªæˆ‘æ³¨æ„æ‡‰ç”¨æ–¼è¼¸å…¥ã€‚

å¹¸é‹çš„æ˜¯ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€å€‹ `DataCollatorForSeq2Seq` æ•´ç†å™¨ï¼Œå®ƒå°‡ç‚ºæˆ‘å€‘å‹•æ…‹å¡«å……è¼¸å…¥å’Œæ¨™ç±¤ã€‚ è¦å¯¦ä¾‹åŒ–é€™å€‹æ”¶é›†å™¨ï¼Œæˆ‘å€‘åªéœ€è¦æä¾› `tokenizer` å’Œ `model`ï¼š

{#if fw === 'pt'}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

è®“æˆ‘å€‘çœ‹çœ‹é€™å€‹æ•´ç†å™¨åœ¨è¼¸å…¥ä¸€å°æ‰¹ç¤ºä¾‹æ™‚æœƒç”¢ç”Ÿä»€éº¼ã€‚ é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦åˆªé™¤å¸¶æœ‰å­—ç¬¦ä¸²çš„åˆ—ï¼Œå› ç‚ºæ•´ç†å™¨ä¸çŸ¥é“å¦‚ä½•å¡«å……é€™äº›å…ƒç´ ï¼š

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

ç”±æ–¼ collator éœ€è¦ä¸€å€‹ `dict` çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹ `dict` ä»£è¡¨æ•¸æ“šé›†ä¸­çš„ä¸€å€‹ç¤ºä¾‹ï¼Œæˆ‘å€‘é‚„éœ€è¦åœ¨å°‡æ•¸æ“šå‚³éçµ¦ data collator ä¹‹å‰å°‡æ•¸æ“šæ•´ç†æˆé æœŸçš„æ ¼å¼ï¼š

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

é€™è£¡è¦æ³¨æ„çš„ä¸»è¦æ˜¯ç¬¬ä¸€å€‹ä¾‹å­æ¯”ç¬¬äºŒå€‹ä¾‹å­è¦é•·ï¼Œæ‰€ä»¥ç¬¬äºŒå€‹ä¾‹å­çš„ `input_ids` å’Œ `attention_mask` å·²ç¶“åœ¨å³å´å¡«å……äº†ä¸€å€‹ `[PAD]` æ¨™è¨˜ï¼ˆå…¶ ID æ˜¯ ` 0`ï¼‰ã€‚ é¡ä¼¼åœ°ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ° `labels` å·²ç”¨ `-100` å¡«å……ï¼Œä»¥ç¢ºä¿å¡«å……æ¨™è¨˜è¢«æå¤±å‡½æ•¸å¿½ç•¥ã€‚ æœ€å¾Œï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°ä¸€å€‹æ–°çš„ `decoder_input_ids`ï¼Œå®ƒé€šéåœ¨ç¬¬ä¸€å€‹æ¢ç›®ä¸­æ’å…¥ `[PAD]` æ¨™è¨˜å°‡æ¨™ç±¤å‘å³ç§»å‹•ã€‚

{#if fw === 'pt'}

æˆ‘å€‘çµ‚æ–¼æ“æœ‰äº†è¨“ç·´æ‰€éœ€çš„æ‰€æœ‰çš„å‰æœŸæº–å‚™ï¼æˆ‘å€‘ç¾åœ¨åªéœ€è¦ä½¿ç”¨æ¨™æº–åƒæ•¸å¯¦ä¾‹åŒ–è¨“ç·´å™¨ï¼š

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

ä¸¦å•Ÿå‹•æˆ‘å€‘çš„è¨“ç·´ï¼š

```python
trainer.train()
```

åœ¨è¨“ç·´æœŸé–“ï¼Œæ‚¨æ‡‰è©²æœƒçœ‹åˆ°è¨“ç·´æå¤±æ¸›å°‘ä¸¦ä¸” ROUGE åˆ†æ•¸éš¨è‘—æ¯å€‹ epoch å¢åŠ ã€‚è¨“ç·´å®Œæˆå¾Œï¼Œæ‚¨å¯ä»¥é€šéé‹è¡Œ**Trainer.evaluate()** æŸ¥çœ‹æœ€çµ‚çš„ ROUGE åˆ†æ•¸ ï¼š

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

å¾åˆ†æ•¸ä¸­æˆ‘å€‘å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘å€‘çš„æ¨¡å‹è¼•é¬†è¶…éäº†æˆ‘å€‘çš„lead-3 baselineâ€”â€”å¾ˆå¥½ï¼æœ€å¾Œè¦åšçš„æ˜¯å°‡æ¨¡å‹æ¬Šé‡æ¨é€åˆ° Hubï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

é€™æœƒå°‡æª¢æŸ¥é»å’Œé…ç½®æ–‡ä»¶ä¿å­˜åˆ° **output_dir** , åœ¨å°‡æ‰€æœ‰æ–‡ä»¶ä¸Šå‚³åˆ°æ¨¡å‹ä¸­å¿ƒä¹‹å‰ã€‚é€šéæŒ‡å®š **tags** åƒæ•¸ï¼Œæˆ‘å€‘é‚„ç¢ºä¿æ¨¡å‹ä¸­å¿ƒä¸Šçš„å°éƒ¨ä»¶å°‡æ˜¯ä¸€å€‹ç”¨æ–¼å½™ç¸½ç®¡é“çš„å°éƒ¨ä»¶ï¼Œè€Œä¸æ˜¯èˆ‡ mT5 æ¶æ§‹é—œè¯çš„é»˜èªæ–‡æœ¬ç”Ÿæˆå°éƒ¨ä»¶ï¼ˆæœ‰é—œæ¨¡å‹æ¨™ç±¤çš„æ›´å¤šä¿¡æ¯ï¼Œè«‹åƒé–±[ğŸ¤— Hub æ–‡æª”](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)ï¼‰ã€‚è¼¸å‡ºä¾†è‡ª **trainer.push_to_hub()** æ˜¯ Git æäº¤å“ˆå¸Œçš„ URLï¼Œå› æ­¤æ‚¨å¯ä»¥è¼•é¬†æŸ¥çœ‹å°æ¨¡å‹å­˜å„²åº«æ‰€åšçš„æ›´æ”¹ï¼

åœ¨çµæŸæœ¬ç¯€ä¹‹å‰ï¼Œè®“æˆ‘å€‘çœ‹ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate æä¾›çš„åº•å±¤APIå° mT5 é€²è¡Œå¾®èª¿ã€‚

{:else}

æˆ‘å€‘å¹¾ä¹æº–å‚™å¥½è¨“ç·´äº†ï¼ æˆ‘å€‘åªéœ€è¦ä½¿ç”¨æˆ‘å€‘ä¸Šé¢å®šç¾©çš„æ•¸æ“šæ•´ç†å™¨å°‡æˆ‘å€‘çš„æ•¸æ“šé›†è½‰æ›ç‚º tf.data.Dataset ï¼Œç„¶å¾Œ `compile()` å’Œ `fit()` æ¨¡å‹ã€‚ é¦–å…ˆï¼Œè½‰æ›æ•¸æ“šé›†ï¼š
```python
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)
```

ç¾åœ¨ï¼Œæˆ‘å€‘å®šç¾©è¨“ç·´è¶…åƒæ•¸ä¸¦ç·¨è­¯ï¼š

```python
from transformers import create_optimizer
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

æœ€å¾Œï¼Œæˆ‘å€‘æ“¬åˆæ¨¡å‹ã€‚ æˆ‘å€‘åœ¨æ¯å€‹ epoch ä¹‹å¾Œä½¿ç”¨`PushToHubCallback`å°‡æ¨¡å‹ä¿å­˜åˆ° Hubï¼Œé€™å°‡å…è¨±æˆ‘å€‘ç¨å¾Œä½¿ç”¨å®ƒé€²è¡Œæ¨ç†ï¼š
```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)
```

æˆ‘å€‘åœ¨è¨“ç·´æœŸé–“è¼¸å‡ºäº†ä¸€äº›lossï¼Œä½†å¯¦éš›ä¸Šæˆ‘å€‘å¸Œæœ›çœ‹åˆ°æˆ‘å€‘ä¹‹å‰è¨ˆç®—çš„ ROUGE æŒ‡æ¨™ã€‚ è¦ç²å¾—é€™äº›æŒ‡æ¨™ï¼Œæˆ‘å€‘éœ€è¦å¾æ¨¡å‹ç”Ÿæˆè¼¸å‡ºä¸¦å°‡å®ƒå€‘è½‰æ›ç‚ºå­—ç¬¦ä¸²ã€‚ è®“æˆ‘å€‘ç‚º ROUGE æŒ‡æ¨™æ§‹å»ºä¸€äº›æ¨™ç±¤å’Œé æ¸¬åˆ—è¡¨ä»¥é€²è¡Œæ¯”è¼ƒï¼ˆè«‹æ³¨æ„ï¼Œå¦‚æœæ‚¨åœ¨æœ¬ç¯€ä¸­é‡åˆ°importçš„éŒ¯èª¤ï¼Œæ‚¨å¯èƒ½éœ€è¦`!pip install tqdm`ï¼‰ï¼š

```python
from tqdm import tqdm
import numpy as np

all_preds = []
all_labels = []
for batch in tqdm(tf_eval_dataset):
    predictions = model.generate(**batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = batch["labels"].numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)
```

ä¸€æ—¦æˆ‘å€‘æœ‰äº†æ¨™ç±¤å’Œé æ¸¬å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¨ˆç®— ROUGE åˆ†æ•¸å°±å¾ˆå®¹æ˜“äº†ï¼š

```python
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}
```

```
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}
```


{/if}

{#if fw === 'pt'}

## ä½¿ç”¨ ğŸ¤— Accelerate å¾®èª¿ mT5

ä½¿ç”¨ ğŸ¤— Accelerate å¾®èª¿æˆ‘å€‘çš„æ¨¡å‹èˆ‡æˆ‘å€‘åœ¨ [Chapter 3](/course/chapter3) ä¸­é‡åˆ°çš„æ–‡æœ¬åˆ†é¡ç¤ºä¾‹éå¸¸ç›¸ä¼¼ã€‚ ä¸»è¦å€åˆ¥åœ¨æ–¼éœ€è¦åœ¨è¨“ç·´æœŸé–“é¡¯å¼ç”Ÿæˆæ‘˜è¦ä¸¦å®šç¾©æˆ‘å€‘å¦‚ä½•è¨ˆç®— ROUGE åˆ†æ•¸ï¼ˆå›æƒ³ä¸€ä¸‹ï¼Œ`Seq2SeqTrainer` ç‚ºæˆ‘å€‘ç”Ÿæˆäº†æ‘˜è¦ï¼‰ã€‚ è®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘å¦‚ä½•åœ¨ ğŸ¤— Accelerate ä¸­å¯¦ç¾é€™å…©å€‹è¦æ±‚ï¼

### ç‚ºè¨“ç·´åšå¥½ä¸€åˆ‡æº–å‚™

The first thing we need to do is create a `DataLoader` for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to `"torch"` in our datasets:
æˆ‘å€‘éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ç‚ºæ¯å€‹æ•¸æ“šé›†çš„æ¯ä¸€å€‹æ‹†åˆ†å‰µå»ºä¸€å€‹`DataLoader`ã€‚ ç”±æ–¼ PyTorch æ•¸æ“šåŠ è¼‰å™¨éœ€è¦æˆæ‰¹çš„å¼µé‡ï¼Œæˆ‘å€‘éœ€è¦åœ¨æ•¸æ“šé›†ä¸­å°‡æ ¼å¼è¨­ç½®ç‚º`torch`ï¼š

```python
tokenized_datasets.set_format("torch")
```

ç¾åœ¨æˆ‘å€‘å·²ç¶“æœ‰äº†åƒ…ç”±å¼µé‡çµ„æˆçš„æ•¸æ“šé›†ï¼Œæ¥ä¸‹ä¾†è¦åšçš„æ˜¯å†æ¬¡å¯¦ä¾‹åŒ–`DataCollatorForSeq2Seq`ã€‚ ç‚ºæ­¤ï¼Œæˆ‘å€‘éœ€è¦æä¾›æ¨¡å‹å¾®èª¿å‰çš„ç‰ˆæœ¬ï¼Œæ‰€ä»¥è®“æˆ‘å€‘å¾ç·©å­˜ä¸­å†æ¬¡åŠ è¼‰å®ƒï¼š

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥å¯¦ä¾‹åŒ–æ•¸æ“šæ•´ç†å™¨ä¸¦ä½¿ç”¨å®ƒä¾†å®šç¾©æˆ‘å€‘çš„æ•¸æ“šåŠ è¼‰å™¨ï¼š

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

æ¥ä¸‹ä¾†è¦åšçš„æ˜¯å®šç¾©æˆ‘å€‘æƒ³è¦ä½¿ç”¨çš„å„ªåŒ–å™¨ã€‚èˆ‡æˆ‘å€‘çš„å…¶ä»–ç¤ºä¾‹ä¸€æ¨£ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ **AdamW** ï¼Œé€™é©ç”¨æ–¼å¤§å¤šæ•¸å•é¡Œï¼š

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

æœ€å¾Œï¼Œæˆ‘å€‘å°‡æ¨¡å‹ã€å„ªåŒ–å™¨å’Œæ•¸æ“šåŠ è¼‰å™¨æä¾›çµ¦ **accelerator.prepare()** æ–¹æ³•ï¼š

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨å¦‚æœæ‚¨åœ¨ TPU ä¸Šé€²è¡Œè¨“ç·´ï¼Œå‰‡éœ€è¦å°‡ä¸Šè¿°æ‰€æœ‰ä»£ç¢¼ç§»å‹•åˆ°å°ˆé–€çš„è¨“ç·´å‡½æ•¸ä¸­ã€‚æœ‰é—œè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–±[ç¬¬ä¸‰ç« ](/course/chapter3)ã€‚

</Tip>

ç¾åœ¨æˆ‘å€‘å·²ç¶“æº–å‚™å¥½äº†æˆ‘å€‘ç´¢è¦ç”¨çš„å°è±¡ï¼Œé‚„æœ‰ä¸‰ä»¶äº‹è¦åšï¼š

* å®šç¾©å­¸ç¿’ç‡èª¿åº¦è¨ˆåŠƒã€‚
* å¯¦ç¾ä¸€å€‹åŠŸèƒ½ä¾†å°æ‘˜è¦é€²è¡Œå¾ŒçºŒè™•ç†ä»¥é€²è¡Œè©•ä¼°ã€‚
* åœ¨ Hub ä¸Šå‰µå»ºä¸€å€‹å­˜å„²åº«ï¼Œæˆ‘å€‘å¯ä»¥å°‡æ¨¡å‹æ¨é€åˆ°è©²å­˜å„²åº«ã€‚

å°æ–¼å­¸ç¿’ç‡èª¿åº¦ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨å‰å¹¾ç¯€ä¸­çš„æ¨™æº–ç·šæ€§è¡°æ¸›ï¼š

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

å°æ–¼å¾ŒçºŒè™•ç†ï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹å‡½æ•¸ï¼Œå°‡ç”Ÿæˆçš„æ‘˜è¦æ‹†åˆ†ç‚ºç”±æ›è¡Œç¬¦åˆ†éš”çš„å¥å­ã€‚ é€™æ˜¯ ROUGE æŒ‡æ¨™æ‰€æœŸæœ›çš„æ ¼å¼ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼ç‰‡æ®µä¾†å¯¦ç¾ï¼š

```python
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

å¦‚æœä½ é‚„è¨˜å¾—æˆ‘å€‘æ˜¯å¦‚ä½•å®šç¾© `Seq2SeqTrainer` çš„ `compute_metrics()` å‡½æ•¸çš„ï¼Œé€™å°ä½ ä¾†èªªæ‡‰è©²å¾ˆç†Ÿæ‚‰ã€‚

æœ€å¾Œï¼Œæˆ‘å€‘éœ€è¦åœ¨ Hugging Face Hub ä¸Šå‰µå»ºä¸€å€‹æ¨¡å‹å­˜å„²åº«ã€‚ ç‚ºæ­¤ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ğŸ¤— Hub åº«çš„get_full_repo_nameã€‚ æˆ‘å€‘åªéœ€è¦ç‚ºæˆ‘å€‘çš„å­˜å„²åº«å®šç¾©ä¸€å€‹åç¨±ï¼Œè©²åº«æœ‰ä¸€å€‹éå¸¸å¥½ç”¨çš„å‡½æ•¸å¯ä»¥å°‡å­˜å„²åº« ID èˆ‡ç”¨æˆ¶é…ç½®æ–‡ä»¶çµåˆèµ·ä¾†ï¼š
```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

ç¾åœ¨æˆ‘å€‘å¯ä»¥ä½¿ç”¨é€™å€‹å­˜å„²åº«åç¨±å°‡æœ¬åœ°ç‰ˆæœ¬å…‹éš†åˆ°æˆ‘å€‘çš„çµæœç›®éŒ„ä¸­ï¼Œè©²ç›®éŒ„å°‡å­˜å„²è¨“ç·´çš„æ¨¡å‹ï¼š

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```
é€™å°‡å…è¨±æˆ‘å€‘åœ¨è¨“ç·´æœŸé–“é€šéèª¿ç”¨ `repo.push_to_hub()` æ–¹æ³•å°‡æ¨¡å‹æ¨é€åˆ° Hubï¼ ç¾åœ¨è®“æˆ‘å€‘é€šéå¯«å‡ºå®Œæ•´çš„è¨“ç·´å¾ªç’°ä¾†çµæŸæˆ‘å€‘çš„åˆ†æã€‚

### è¨“ç·´å¾ªç’°

æ–‡æœ¬æ‘˜è¦çš„è¨“ç·´å¾ªç’°èˆ‡æˆ‘å€‘é‡åˆ°çš„å…¶ä»– ğŸ¤— Accelerate ç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼Œå¤§è‡´åˆ†ç‚ºå››å€‹ä¸»è¦æ­¥é©Ÿï¼šé€™

1. é€šéåœ¨æ¯å€‹epoch è¿­ä»£ `train_dataloader` ä¸­çš„æ‰€æœ‰ç¤ºä¾‹ä¾†è¨“ç·´æ¨¡å‹ã€‚
2. åœ¨æ¯å€‹ epoch çµæŸæ™‚ç”Ÿæˆæ¨¡å‹æ‘˜è¦ï¼Œé¦–å…ˆç”Ÿæˆæ¨™è¨˜ï¼Œç„¶å¾Œå°‡å®ƒå€‘ï¼ˆå’Œåƒè€ƒæ‘˜è¦ï¼‰è§£ç¢¼ç‚ºæ–‡æœ¬ã€‚
3. ä½¿ç”¨æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„ç›¸åŒæŠ€è¡“è¨ˆç®— ROUGE åˆ†æ•¸ã€‚
4. ä¿å­˜æª¢æŸ¥é»ä¸¦å°‡æ‰€æœ‰å…§å®¹æ¨é€åˆ° Hubã€‚ åœ¨é€™è£¡ï¼Œæˆ‘å€‘ä¾è³´ `Repository` å°è±¡çš„å·§å¦™çš„ `blocking=False` åƒæ•¸ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥åœ¨æ¯å€‹ epoch ç•°æ­¥åœ°ä¸Šå‚³æª¢æŸ¥é»ã€‚ é€™ä½¿æˆ‘å€‘èƒ½å¤ ç¹¼çºŒè¨“ç·´ï¼Œè€Œä¸å¿…ç­‰å¾…èˆ‡ GB å¤§å°çš„æ¨¡å‹æ…¢å‘¼å‘¼çš„ä¸Šå‚³ï¼

é€™äº›æ­¥é©Ÿå¯ä»¥åœ¨ä»¥ä¸‹ä»£ç¢¼å¡Šä¸­çœ‹åˆ°ï¼š

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

å°±æ˜¯é€™æ¨£ï¼ é‹è¡Œæ­¤ç¨‹åºå¾Œï¼Œæ‚¨å°‡ç²å¾—èˆ‡æˆ‘å€‘ä½¿ç”¨â€œTrainerâ€ç²å¾—çš„æ¨¡å‹å’Œçµæœéå¸¸ç›¸ä¼¼çš„æ¨¡å‹å’Œçµæœã€‚

{/if}

## ä½¿ç”¨æ‚¨å¾®èª¿çš„æ¨¡å‹

å°‡æ¨¡å‹æ¨é€åˆ° Hub å¾Œï¼Œæ‚¨å¯ä»¥é€šéæ¨ç†å°éƒ¨ä»¶æˆ–â€œç®¡é“â€å°è±¡ä¾†ä½¿ç”¨å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

æˆ‘å€‘å¯ä»¥å°‡æ¸¬è©¦é›†ä¸­çš„ä¸€äº›ç¤ºä¾‹ï¼ˆæ¨¡å‹é‚„æ²’æœ‰çœ‹åˆ°ï¼‰æä¾›çµ¦æˆ‘å€‘çš„ç®¡é“ï¼Œä»¥ç­è§£ç”Ÿæˆæ‘˜è¦çš„è³ªé‡ã€‚ é¦–å…ˆè®“æˆ‘å€‘å¯¦ç¾ä¸€å€‹ç°¡å–®çš„å‡½æ•¸ä¾†ä¸€èµ·é¡¯ç¤ºè©•è«–ã€æ¨™é¡Œå’Œç”Ÿæˆçš„æ‘˜è¦ï¼š

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

è®“æˆ‘å€‘çœ‹ä¸€ä¸‹æˆ‘å€‘å¾—åˆ°çš„ä¸€å€‹è‹±æ–‡ä¾‹å­ï¼š

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnâ€™t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itâ€™s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

é€™é‚„ä¸éŒ¯ï¼ æˆ‘å€‘å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘å€‘çš„æ¨¡å‹å¯¦éš›ä¸Šå·²ç¶“èƒ½å¤ é€šéå¢åŠ éƒ¨åˆ†æ–°è©ä¾†åŸ·è¡ŒæŠ½è±¡æ‘˜è¦ã€‚ ä¹Ÿè¨±æˆ‘å€‘æ¨¡å‹æœ€é…·çš„æ–¹é¢æ˜¯å®ƒæ˜¯é›™èªçš„ï¼Œæ‰€ä»¥æˆ‘å€‘é‚„å¯ä»¥ç”Ÿæˆè¥¿ç­ç‰™èªè©•è«–çš„æ‘˜è¦ï¼š

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

æ‘˜è¦ç¿»è­¯æˆäº†è‹±æ–‡çš„â€œéå¸¸å®¹æ˜“é–±è®€â€ï¼Œåœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°å®ƒæ˜¯ç›´æ¥å¾è©•è«–ä¸­æå–çš„ã€‚ é€™é¡¯ç¤ºäº† mT5 æ¨¡å‹çš„å¤šåŠŸèƒ½æ€§ï¼Œä¸¦è®“æ‚¨é«”é©—äº†è™•ç†å¤šèªè¨€èªæ–™åº«çš„æ„Ÿè¦ºï¼

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡æŠŠæ³¨æ„åŠ›è½‰å‘ç¨å¾®è¤‡é›œçš„ä»»å‹™ï¼šå¾é ­é–‹å§‹è¨“ç·´èªè¨€æ¨¡å‹ã€‚
