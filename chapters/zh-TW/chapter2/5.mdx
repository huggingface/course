<FrameworkSwitchCourse {fw} />

# è™•ç†å¤šå€‹åºåˆ—

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section5_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="M6adb1j2jPI"/>
{:else}
<Youtube id="ROxrFOEbsQE"/>
{/if}

åœ¨ä¸Šä¸€ç¯€ä¸­ï¼Œæˆ‘å€‘æ¢è¨äº†æœ€ç°¡å–®çš„ç”¨ä¾‹ï¼šå°ä¸€å€‹å°é•·åº¦çš„åºåˆ—é€²è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œä¸€äº›å•é¡Œå·²ç¶“å‡ºç¾ï¼š

* æˆ‘å€‘å¦‚ä½•è™•ç†å¤šå€‹åºåˆ—ï¼Ÿ


* æˆ‘å€‘å¦‚ä½•è™•ç†å¤šå€‹åºåˆ—ä¸åŒé•·åº¦?


* è©å½™ç´¢å¼•æ˜¯è®“æ¨¡å‹æ­£å¸¸å·¥ä½œçš„å”¯ä¸€è¼¸å…¥å—ï¼Ÿ


* æ˜¯å¦å­˜åœ¨åºåˆ—å¤ªé•·çš„å•é¡Œï¼Ÿ

è®“æˆ‘å€‘çœ‹çœ‹é€™äº›å•é¡Œæœƒå¸¶ä¾†ä»€éº¼æ¨£çš„å•é¡Œï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ğŸ¤— Transformers APIè§£æ±ºå®ƒå€‘

## æ¨¡å‹éœ€è¦ä¸€æ‰¹è¼¸å…¥

åœ¨ä¸Šä¸€å€‹ç·´ç¿’ä¸­ï¼Œæ‚¨çœ‹åˆ°äº†åºåˆ—å¦‚ä½•è½‰æ›ç‚ºæ•¸å­—åˆ—è¡¨ã€‚è®“æˆ‘å€‘å°‡æ­¤æ•¸å­—åˆ—è¡¨è½‰æ›ç‚ºå¼µé‡ï¼Œä¸¦å°‡å…¶ç™¼é€åˆ°æ¨¡å‹ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

```python out
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)
```

```py out
InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]
```
{/if}

å“¦ä¸ï¼ç‚ºä»€éº¼å¤±æ•—äº†ï¼Ÿæˆ‘å€‘éµå¾ªäº†ç¬¬2ç¯€ä¸­ç®¡é“çš„æ­¥é©Ÿã€‚

å•é¡Œæ˜¯æˆ‘å€‘å‘æ¨¡å‹ç™¼é€äº†ä¸€å€‹åºåˆ—ï¼Œè€Œ ğŸ¤—Transformers æ¨¡å‹é»˜èªæƒ…æ³ä¸‹éœ€è¦å¤šå€‹å¥å­ã€‚åœ¨é€™è£¡ï¼Œç•¶æˆ‘å€‘å°‡åˆ†è©å™¨æ‡‰ç”¨æ–¼ä¸€å€‹æ‡‰ç”¨ç¨‹åºæ™‚ï¼Œæˆ‘å€‘å˜—è©¦åœ¨å¹•å¾Œå®Œæˆåˆ†è©å™¨æ‰€åšçš„ä¸€åˆ‡ï¼Œä½†å¦‚æœä»”ç´°è§€å¯Ÿï¼Œæ‚¨æœƒç™¼ç¾å®ƒä¸åƒ…å°‡è¼¸å…¥IDåˆ—è¡¨è½‰æ›ç‚ºå¼µé‡ï¼Œé‚„åœ¨å…¶é ‚éƒ¨æ·»åŠ äº†ä¸€å€‹ç¶­åº¦ï¼š

{#if fw === 'pt'}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

```python out
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```
{:else}
```py
tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])
```

```py out
tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>
```
{/if}

è®“æˆ‘å€‘é‡è©¦ä¸¦æ·»åŠ ä¸€å€‹æ–°ç¶­åº¦ï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
{/if}

æˆ‘å€‘æ‰“å°è¼¸å…¥IDä»¥åŠç”Ÿæˆçš„logits-ä»¥ä¸‹æ˜¯è¼¸å‡ºï¼š

{#if fw === 'pt'}
```python out
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```
{:else}
```py out
Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)
```
{/if}

*Batching* æ˜¯ä¸€æ¬¡é€šéæ¨¡å‹ç™¼é€å¤šå€‹å¥å­çš„è¡Œç‚ºã€‚å¦‚æœä½ åªæœ‰ä¸€å¥è©±ï¼Œä½ å¯ä»¥ç”¨ä¸€å€‹åºåˆ—æ§‹å»ºä¸€å€‹æ‰¹æ¬¡ï¼š


```
batched_ids = [ids, ids]
```

é€™æ˜¯ä¸€æ‰¹å…©å€‹ç›¸åŒçš„åºåˆ—ï¼

<Tip>

âœï¸ **è©¦è©¦çœ‹ï¼** å°‡æ­¤åˆ—è¡¨è½‰æ›ç‚ºå¼µé‡ä¸¦é€šéæ¨¡å‹å‚³éã€‚æª¢æŸ¥æ‚¨æ˜¯å¦ç²å¾—èˆ‡ä¹‹å‰ç›¸åŒçš„ç™»éŒ„ï¼ˆä½†æ˜¯éš»æœ‰å…©æ¬¡ï¼‰
</Tip>

æ‰¹è™•ç†å…è¨±æ¨¡å‹åœ¨è¼¸å…¥å¤šå€‹å¥å­æ™‚å·¥ä½œã€‚ä½¿ç”¨å¤šå€‹åºåˆ—å°±åƒä½¿ç”¨å–®å€‹åºåˆ—æ§‹å»ºæ‰¹ä¸€æ¨£ç°¡å–®ã€‚ä¸éï¼Œé‚„æœ‰ç¬¬äºŒå€‹å•é¡Œã€‚ç•¶ä½ è©¦åœ–å°‡å…©å€‹ï¼ˆæˆ–æ›´å¤šï¼‰å¥å­çµ„åˆåœ¨ä¸€èµ·æ™‚ï¼Œå®ƒå€‘çš„é•·åº¦å¯èƒ½ä¸åŒã€‚å¦‚æœæ‚¨ä»¥å‰ä½¿ç”¨éå¼µé‡ï¼Œé‚£éº¼æ‚¨çŸ¥é“å®ƒå€‘å¿…é ˆæ˜¯çŸ©å½¢ï¼Œå› æ­¤ç„¡æ³•å°‡è¼¸å…¥IDåˆ—è¡¨ç›´æ¥è½‰æ›ç‚ºå¼µé‡ã€‚ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘é€šå¸¸å¡«å……è¼¸å…¥ã€‚

## å¡«å……è¼¸å…¥

ä»¥ä¸‹åˆ—è¡¨ä¸èƒ½è½‰æ›ç‚ºå¼µé‡ï¼š

```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œæˆ‘å€‘å°‡ä½¿ç”¨å¡«å……ä½¿å¼µé‡å…·æœ‰çŸ©å½¢ã€‚Paddingé€šéåœ¨å€¼è¼ƒå°‘çš„å¥å­ä¸­æ·»åŠ ä¸€å€‹åç‚ºPadding tokençš„ç‰¹æ®Šå–®è©ä¾†ç¢ºä¿æˆ‘å€‘æ‰€æœ‰çš„å¥å­é•·åº¦ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰10å€‹åŒ…å«10å€‹å–®è©çš„å¥å­å’Œ1å€‹åŒ…å«20å€‹å–®è©çš„å¥å­ï¼Œå¡«å……å°‡ç¢ºä¿æ‰€æœ‰å¥å­éƒ½åŒ…å«20å€‹å–®è©ã€‚åœ¨æˆ‘å€‘çš„ç¤ºä¾‹ä¸­ï¼Œç”Ÿæˆçš„å¼µé‡å¦‚ä¸‹æ‰€ç¤ºï¼š

```py no-format
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

å¯ä»¥åœ¨tokenizer.pad_token_idä¸­æ‰¾åˆ°å¡«å……ä»¤ç‰ŒID. è®“æˆ‘å€‘ä½¿ç”¨å®ƒï¼Œå°‡æˆ‘å€‘çš„å…©å¥è©±åˆ†åˆ¥ç™¼é€åˆ°æ¨¡å‹ä¸­ï¼Œä¸¦åˆ†æ‰¹ç™¼é€åˆ°ä¸€èµ·ï¼š


{#if fw === 'pt'}
```py no-format
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

```python out
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)
```

```py out
tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)
```
{/if}

æˆ‘å€‘æ‰¹è™•ç†é æ¸¬ä¸­çš„logitsæœ‰é»å•é¡Œï¼šç¬¬äºŒè¡Œæ‡‰è©²èˆ‡ç¬¬äºŒå¥çš„logitsç›¸åŒï¼Œä½†æˆ‘å€‘å¾—åˆ°äº†å®Œå…¨ä¸åŒçš„å€¼ï¼


é€™æ˜¯å› ç‚ºTransformeræ¨¡å‹çš„é—œéµç‰¹æ€§æ˜¯é—œæ³¨å±¤ï¼Œå®ƒå°‡æ¯å€‹æ¨™è¨˜ä¸Šä¸‹æ–‡åŒ–ã€‚é€™äº›å°‡è€ƒæ…®å¡«å……æ¨™è¨˜ï¼Œå› ç‚ºå®ƒå€‘æ¶‰åŠåºåˆ—ä¸­çš„æ‰€æœ‰æ¨™è¨˜ã€‚ç‚ºäº†åœ¨é€šéæ¨¡å‹å‚³éä¸åŒé•·åº¦çš„å–®å€‹å¥å­æ™‚ï¼Œæˆ–è€…åœ¨å‚³éä¸€æ‰¹æ‡‰ç”¨äº†ç›¸åŒå¥å­å’Œå¡«å……çš„å¥å­æ™‚ç²å¾—ç›¸åŒçš„çµæœï¼Œæˆ‘å€‘éœ€è¦å‘Šè¨´é€™äº›æ³¨æ„å±¤å¿½ç•¥å¡«å……æ¨™è¨˜ã€‚é€™æ˜¯é€šéä½¿ç”¨ attention maskä¾†å¯¦ç¾çš„ã€‚

## æ³¨æ„åŠ›é®ç½©ï¼ˆAttention masksï¼‰

*Attention masks* æ˜¯èˆ‡è¼¸å…¥ ID å¼µé‡å½¢ç‹€å®Œå…¨ç›¸åŒçš„å¼µé‡ï¼Œç”¨0å’Œ1å¡«å……ï¼š1sè¡¨ç¤ºæ‡‰æ³¨æ„ç›¸æ‡‰çš„æ¨™è¨˜ï¼Œ0sè¡¨ç¤ºä¸æ‡‰æ³¨æ„ç›¸æ‡‰çš„æ¨™è¨˜ï¼ˆå³ï¼Œæ¨¡å‹çš„æ³¨æ„åŠ›å±¤æ‡‰å¿½ç•¥å®ƒå€‘ï¼‰ã€‚

è®“æˆ‘å€‘ç”¨ attention mask å®Œæˆä¸Šä¸€å€‹ç¤ºä¾‹ï¼š

{#if fw === 'pt'}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

```python out
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```
{:else}
```py no-format
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)
```

```py out
tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)
```
{/if}

ç¾åœ¨æˆ‘å€‘å¾—åˆ°äº†è©²æ‰¹ä¸­ç¬¬äºŒå€‹å¥å­çš„ç›¸åŒç™»éŒ„ã€‚

è«‹æ³¨æ„ï¼Œç¬¬äºŒå€‹åºåˆ—çš„æœ€å¾Œä¸€å€‹å€¼æ˜¯ä¸€å€‹å¡«å……IDï¼Œå®ƒåœ¨attention maskä¸­æ˜¯ä¸€å€‹0å€¼ã€‚

<Tip>

âœï¸ è©¦è©¦çœ‹ï¼åœ¨ç¬¬2ç¯€ä¸­ä½¿ç”¨çš„å…©å€‹å¥å­ä¸Šæ‰‹å‹•æ‡‰ç”¨æ¨™è¨˜åŒ–ï¼ˆâ€œæˆ‘ä¸€ç”Ÿéƒ½åœ¨ç­‰å¾…Hugging Faceèª²ç¨‹ã€‚â€å’Œâ€œæˆ‘éå¸¸è¨å­é€™å€‹ï¼â€ï¼‰ã€‚é€šéæ¨¡å‹å‚³éå®ƒå€‘ï¼Œä¸¦æª¢æŸ¥æ‚¨æ˜¯å¦ç²å¾—èˆ‡ç¬¬2ç¯€ä¸­ç›¸åŒçš„ç™»éŒ„ã€‚ç¾åœ¨ä½¿ç”¨å¡«å……æ¨™è¨˜å°‡å®ƒå€‘æ‰¹è™•ç†åœ¨ä¸€èµ·ï¼Œç„¶å¾Œå‰µå»ºé©ç•¶çš„æ³¨æ„æ©ç¢¼ã€‚æª¢æŸ¥é€šéæ¨¡å‹æ™‚æ˜¯å¦ç²å¾—ç›¸åŒçš„çµæœï¼

</Tip>

## é•·åºåˆ—

å°æ–¼Transformersæ¨¡å‹ï¼Œæˆ‘å€‘å¯ä»¥é€šéæ¨¡å‹çš„åºåˆ—é•·åº¦æ˜¯æœ‰é™çš„ã€‚å¤§å¤šæ•¸æ¨¡å‹è™•ç†å¤šé”512æˆ–1024å€‹ä»¤ç‰Œçš„åºåˆ—ï¼Œç•¶è¦æ±‚è™•ç†æ›´é•·çš„åºåˆ—æ™‚ï¼Œæœƒå´©æ½°ã€‚æ­¤å•é¡Œæœ‰å…©ç¨®è§£æ±ºæ–¹æ¡ˆï¼š



* ä½¿ç”¨æ”¯æŒçš„åºåˆ—é•·åº¦è¼ƒé•·çš„æ¨¡å‹ã€‚


* æˆªæ–·åºåˆ—ã€‚


æ¨¡å‹æœ‰ä¸åŒçš„æ”¯æŒåºåˆ—é•·åº¦ï¼Œæœ‰äº›æ¨¡å‹å°ˆé–€è™•ç†å¾ˆé•·çš„åºåˆ—ã€‚
[Longformer](https://huggingface.co/transformers/model_doc/longformer.html)
é€™æ˜¯ä¸€å€‹ä¾‹å­ï¼Œå¦ä¸€å€‹æ˜¯
[LED](https://huggingface.co/transformers/model_doc/led.html)
. å¦‚æœæ‚¨æ­£åœ¨è™•ç†ä¸€é …éœ€è¦å¾ˆé•·åºåˆ—çš„ä»»å‹™ï¼Œæˆ‘å€‘å»ºè­°æ‚¨æŸ¥çœ‹é€™äº›æ¨¡å‹ã€‚

å¦å‰‡ï¼Œæˆ‘å€‘å»ºè­°æ‚¨é€šéæŒ‡å®šmax_sequence_lengthåƒæ•¸ï¼š

```py
sequence = sequence[:max_sequence_length]
```
