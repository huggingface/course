<FrameworkSwitchCourse {fw} />

# æŠŠå®ƒå€‘æ”¾åœ¨ä¸€èµ·

{#if fw === 'pt'}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter2/section6_tf.ipynb"},
]} />

{/if}

åœ¨æœ€å¾Œå¹¾ç¯€ä¸­ï¼Œæˆ‘å€‘ä¸€ç›´åœ¨ç›¡æœ€å¤§åŠªåŠ›æ‰‹å·¥å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œã€‚æˆ‘å€‘æ¢è¨äº†æ¨™è¨˜åŒ–å™¨çš„å·¥ä½œåŸç†ï¼Œä¸¦ç ”ç©¶äº†æ¨™è¨˜åŒ–ã€åˆ°è¼¸å…¥IDçš„è½‰æ›ã€å¡«å……ã€æˆªæ–·å’Œæ³¨æ„æ©ç¢¼ã€‚

ç„¶è€Œï¼Œæ­£å¦‚æˆ‘å€‘åœ¨ç¬¬2ç¯€ä¸­æ‰€çœ‹åˆ°çš„ï¼ŒğŸ¤— Transformers APIå¯ä»¥é€šéä¸€å€‹é«˜ç´šå‡½æ•¸ç‚ºæˆ‘å€‘è™•ç†æ‰€æœ‰é€™äº›ï¼Œæˆ‘å€‘å°‡åœ¨é€™è£¡æ·±å…¥è¨è«–ã€‚ç•¶ä½ ç›´æ¥åœ¨å¥å­ä¸Šèª¿ç”¨æ¨™è¨˜å™¨æ™‚ï¼Œä½ æœƒå¾—åˆ°æº–å‚™é€šéæ¨¡å‹å‚³éçš„è¼¸å…¥

```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

é€™è£¡ï¼Œ`model_inputs`
è®Šé‡åŒ…å«æ¨¡å‹è‰¯å¥½é‹è¡Œæ‰€éœ€çš„ä¸€åˆ‡ã€‚å°æ–¼DistilBERTï¼Œå®ƒåŒ…æ‹¬è¼¸å…¥ IDå’Œæ³¨æ„åŠ›æ©ç¢¼(attention mask)ã€‚å…¶ä»–æ¥å—é¡å¤–è¼¸å…¥çš„æ¨¡å‹ä¹Ÿæœƒæœ‰æ¨™è¨˜å™¨å°è±¡çš„è¼¸å‡ºã€‚

æ­£å¦‚æˆ‘å€‘å°‡åœ¨ä¸‹é¢çš„ä¸€äº›ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ï¼Œé€™ç¨®æ–¹æ³•éå¸¸å¼·å¤§ã€‚é¦–å…ˆï¼Œå®ƒå¯ä»¥æ¨™è¨˜å–®å€‹åºåˆ—ï¼š

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

å®ƒé‚„ä¸€æ¬¡è™•ç†å¤šå€‹åºåˆ—ï¼Œä¸¦ä¸”APIæ²’æœ‰ä»»ä½•è®ŠåŒ–ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

å®ƒå¯ä»¥æ ¹æ“šå¹¾å€‹ç›®æ¨™é€²è¡Œå¡«å……ï¼š

```py
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å®ƒé‚„å¯ä»¥æˆªæ–·åºåˆ—:

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

æ¨™è¨˜å™¨å°è±¡å¯ä»¥è™•ç†åˆ°ç‰¹å®šæ¡†æ¶å¼µé‡çš„è½‰æ›ï¼Œç„¶å¾Œå¯ä»¥ç›´æ¥ç™¼é€åˆ°æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç¢¼ç¤ºä¾‹ä¸­ï¼Œæˆ‘å€‘æç¤ºæ¨™è¨˜å™¨å¾ä¸åŒçš„æ¡†æ¶è¿”å›å¼µé‡â€”â€”`"pt"`è¿”å›Py Torchå¼µé‡ï¼Œ`"tf"`è¿”å›TensorFlowå¼µé‡ï¼Œ`"np"`è¿”å›NumPyæ•¸çµ„ï¼š

```py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## ç‰¹æ®Šè©ç¬¦(token)

å¦‚æœæˆ‘å€‘çœ‹ä¸€ä¸‹æ¨™è¨˜å™¨è¿”å›çš„è¼¸å…¥ IDï¼Œæˆ‘å€‘æœƒç™¼ç¾å®ƒå€‘èˆ‡ä¹‹å‰çš„ç•¥æœ‰ä¸åŒï¼š

```py
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

ä¸€å€‹åœ¨é–‹å§‹æ™‚æ·»åŠ äº†ä¸€å€‹æ¨™è¨˜(token) IDï¼Œä¸€å€‹åœ¨çµæŸæ™‚æ·»åŠ äº†ä¸€å€‹æ¨™è¨˜(token) IDã€‚è®“æˆ‘å€‘è§£ç¢¼ä¸Šé¢çš„å…©å€‹IDåºåˆ—ï¼Œçœ‹çœ‹é€™æ˜¯æ€éº¼å›äº‹ï¼š

```py
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

```python out
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

æ¨™è¨˜å™¨åœ¨é–‹é ­æ·»åŠ äº†ç‰¹æ®Šå–®è©`[CLS]`ï¼Œåœ¨çµå°¾æ·»åŠ äº†ç‰¹æ®Šå–®è©`[SEP]`ã€‚é€™æ˜¯å› ç‚ºæ¨¡å‹æ˜¯ç”¨é€™äº›æ•¸æ“šé è¨“ç·´çš„ï¼Œæ‰€ä»¥ç‚ºäº†å¾—åˆ°ç›¸åŒçš„æ¨ç†çµæœï¼Œæˆ‘å€‘é‚„éœ€è¦æ·»åŠ å®ƒå€‘ã€‚è«‹æ³¨æ„ï¼Œæœ‰äº›æ¨¡å‹ä¸æ·»åŠ ç‰¹æ®Šå–®è©ï¼Œæˆ–è€…æ·»åŠ ä¸åŒçš„å–®è©ï¼›æ¨¡å‹ä¹Ÿå¯èƒ½åªåœ¨é–‹é ­æˆ–çµå°¾æ·»åŠ é€™äº›ç‰¹æ®Šå–®è©ã€‚åœ¨ä»»ä½•æƒ…æ³ä¸‹ï¼Œæ¨™è¨˜å™¨éƒ½çŸ¥é“éœ€è¦å“ªäº›è©ç¬¦ï¼Œä¸¦å°‡ç‚ºæ‚¨è™•ç†é€™äº›è©ç¬¦ã€‚

## çµæŸï¼šå¾æ¨™è¨˜å™¨åˆ°æ¨¡å‹

ç¾åœ¨æˆ‘å€‘å·²ç¶“çœ‹åˆ°äº†æ¨™è¨˜å™¨å°è±¡åœ¨æ‡‰ç”¨æ–¼æ–‡æœ¬æ™‚ä½¿ç”¨çš„æ‰€æœ‰å–®ç¨æ­¥é©Ÿï¼Œè®“æˆ‘å€‘æœ€å¾Œä¸€æ¬¡çœ‹çœ‹å®ƒå¦‚ä½•è™•ç†å¤šå€‹åºåˆ—ï¼ˆå¡«å……ï¼ï¼‰ï¼Œéå¸¸é•·çš„åºåˆ—ï¼ˆæˆªæ–·ï¼ï¼‰ï¼Œä»¥åŠå¤šç¨®é¡å‹çš„å¼µé‡åŠå…¶ä¸»è¦APIï¼š

{#if fw === 'pt'}
```py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
{:else}
```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)
```
{/if}
