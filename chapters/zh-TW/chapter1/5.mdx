# “編碼器”模型

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="MUqNwgPjJvQ" />

“編碼器”模型指僅使用編碼器的Transformer模型。在每個階段，注意力層都可以獲取初始句子中的所有單詞。這些模型通常具有“雙向”注意力，被稱爲自編碼模型。

這些模型的預訓練通常圍繞着以某種方式破壞給定的句子（例如：通過隨機遮蓋其中的單詞），並讓模型尋找或重建給定的句子。

“編碼器”模型最適合於需要理解完整句子的任務，例如：句子分類、命名實體識別（以及更普遍的單詞分類）和閱讀理解後回答問題。

該系列模型的典型代表有：

- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)
- [BERT](https://huggingface.co/transformers/model_doc/bert.html)
- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)
- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)
- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
