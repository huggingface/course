# 🤗 Transformers 如何解決任務

<Youtube id="zsfR7eY9Uho" />

在[Transformers 能做什麼？](https://huggingface.co/learn/llm-course/chapter1/3)中，您了解了自然語言處理（NLP）、語音和音訊、電腦視覺任務，以及它們的一些重要應用。本頁將深入探討模型如何解決這些任務，並解釋其內部運作機制。解決特定任務有很多種方法，有些模型可能會實施某些技術，甚至從新的角度來處理任務，但對於 Transformer 模型來說，基本概念是相同的。由於其靈活的架構，大多數模型都是編碼器、解碼器或編碼器-解碼器結構的變體。

<Tip>

在深入探討特定架構變體之前，了解大多數任務都遵循相似的模式是很有幫助的：輸入數據通過模型進行處理，然後輸出被解釋用於特定任務。差異在於數據如何準備、使用什麼模型架構變體，以及如何處理輸出。

</Tip>

為了解釋任務是如何解決的，我們將逐步了解模型內部如何運作以輸出有用的預測。我們將涵蓋以下模型及其對應的任務：

- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) 用於音頻分類和自動語音識別（ASR）
- [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) and [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) 用於圖像分類
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr) 用於物體檢測
- [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) 用於圖像分割
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) 用於深度估計
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert) 用於使用編碼器的自然語言處理任務，如文本分類、標記分類和問答
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) 用於使用解碼器的自然語言處理任務，如文本生成
- [BART](https://huggingface.co/docs/transformers/model_doc/bart) 用於使用編碼器-解碼器的自然語言處理任務，如摘要和翻譯


