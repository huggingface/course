# 🤗 Transformers 如何解決任務

<Youtube id="zsfR7eY9Uho" />

在[Transformers 能做什麼？](https://huggingface.co/learn/llm-course/chapter1/3)中，您了解了自然語言處理（NLP）、語音和音訊、電腦視覺任務，以及它們的一些重要應用。本頁將深入探討模型如何解決這些任務，並解釋其內部運作機制。解決特定任務有很多種方法，有些模型可能會實施某些技術，甚至從新的角度來處理任務，但對於 Transformer 模型來說，基本概念是相同的。由於其靈活的架構，大多數模型都是編碼器、解碼器或編碼器-解碼器結構的變體。

<Tip>

在深入探討特定架構變體之前，了解大多數任務都遵循相似的模式是很有幫助的：輸入數據通過模型進行處理，然後輸出被解釋用於特定任務。差異在於數據如何準備、使用什麼模型架構變體，以及如何處理輸出。

</Tip>

為了解釋任務是如何解決的，我們將逐步了解模型內部如何運作以輸出有用的預測。我們將涵蓋以下模型及其對應的任務：

- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) 用於音頻分類和自動語音識別（ASR）
- [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) and [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) 用於圖像分類
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr) 用於物體檢測
- [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) 用於圖像分割
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) 用於深度估計
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert) 用於使用編碼器的自然語言處理任務，如文本分類、標記分類和問答
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) 用於使用解碼器的自然語言處理任務，如文本生成
- [BART](https://huggingface.co/docs/transformers/model_doc/bart) 用於使用編碼器-解碼器的自然語言處理任務，如摘要和翻譯

<Tip>

在進一步學習之前，最好先具備一些原始 Transformer 架構的基礎知識。了解編碼器、解碼器和注意力機制的運作方式，將有助於您理解不同 Transformer 模型的工作原理。請務必查看我們的[前一節]((https://huggingface.co/course/chapter1/4?fw=pt))以獲取更多資訊！

</Tip>

## 語言的 Transformer 模型

語言模型是現代自然語言處理的核心。它們旨在通過學習文本中詞彙或標記之間的統計模式和關係來理解和生成人類語言。

Transformer 最初是為機器翻譯而設計的，從那時起，它已成為解決所有人工智慧任務的預設架構。有些任務適合 Transformer 的編碼器結構，而其他任務則更適合解碼器。還有一些任務會同時使用 Transformer 的編碼器-解碼器結構。

### 語言模型如何運作

語言模型的運作原理是通過訓練來預測在周圍詞彙上下文中某個詞彙的機率。這為它們提供了對語言的基礎理解，可以推廣到其他任務。

訓練 transformer 模型有兩種主要方法：

1. **遮蔽語言建模（MLM）**：由像 BERT 這樣的編碼器模型使用，這種方法隨機遮蔽輸入中的一些標記，並訓練模型根據周圍上下文預測原始標記。這使得模型能夠學習雙向上下文（查看被遮蔽詞彙前後的詞彙）。

2. **因果語言建模（CLM）**：由像 GPT 這樣的解碼器模型使用，這種方法根據序列中所有先前的標記預測下一個標記。模型只能使用左側的上下文（先前的標記）來預測下一個標記。

### 語言模型的類型

在 Transformers 函式庫中，語言模型通常分為三個架構類別：

1. **僅編碼器模型（如 BERT）**：這些模型使用雙向方法來理解來自兩個方向的上下文。它們最適合需要深度理解文本的任務，例如分類、命名實體識別和問答。

2. **僅解碼器模型（如 GPT、Llama）**：這些模型從左到右處理文本，特別擅長文本生成任務。它們可以完成句子、撰寫文章，甚至根據提示生成程式碼。

3. **編碼器-解碼器模型（如 T5、BART）**：這些模型結合兩種方法，使用編碼器理解輸入，使用解碼器生成輸出。它們在序列到序列任務中表現出色，如翻譯、摘要和問答。

![transformer-models-for-language](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_architecture.png)

如我們在前一節所述，語言模型通常以自監督的方式（無需人工標註）在大量文本數據上進行預訓練，然後針對特定任務進行微調。這種被稱為遷移學習的方法，使這些模型能夠用相對少量的任務特定數據適應許多不同的自然語言處理任務。

在接下來的章節中，我們將探討特定的模型架構，以及它們如何應用於語音、視覺和文本領域的各種任務。

<Tip>

理解 Transformer 架構的哪個部分（編碼器、解碼器或兩者）最適合特定的自然語言處理任務，是選擇正確模型的關鍵。一般來說，需要雙向上下文的任務使用編碼器，生成文本的任務使用解碼器，而將一個序列轉換為另一個序列的任務使用編碼器-解碼器。

</Tip>

### 文本生成

文本生成涉及根據提示或輸入創建連貫且與上下文相關的文本。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2 使用[位元組對編碼（BPE）](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe)來標記化詞彙並生成標記嵌入。位置編碼被添加到標記嵌入中，以指示序列中每個標記的位置。輸入嵌入通過多個解碼器區塊傳遞，以輸出最終的隱藏狀態。在每個解碼器區塊內，GPT-2 使用*遮蔽自注意力*層，這意味著 GPT-2 無法關注未來的標記。它只被允許關注左側的標記。這與 BERT 的 [`mask`] 標記不同，因為在遮蔽自注意力中，注意力遮罩用於將未來標記的分數設置為 `0`。

2. 解碼器的輸出被傳遞到語言建模頭，該頭執行線性變換將隱藏狀態轉換為對數機率。標籤是序列中的下一個標記，通過將對數機率向右移動一位來創建。交叉熵損失在移位的對數機率和標籤之間計算，以輸出下一個最可能的標記。

GPT-2 的預訓練目標完全基於[因果語言建模](https://huggingface.co/docs/transformers/glossary#causal-language-modeling)，預測序列中的下一個詞彙。這使得 GPT-2 特別擅長涉及生成文本的任務。

準備嘗試文本生成了嗎？查看我們完整的[因果語言建模指南](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling)，學習如何微調 DistilGPT-2 並將其用於推理！

<Tip>

如需更多關於文本生成的資訊，請查看[文本生成策略](generation_strategies)指南！

</Tip>

### 文本分類

文本分類涉及將預定義的類別分配給文本文檔，例如情感分析、主題分類或垃圾郵件檢測。

[BERT](https://huggingface.co/docs/transformers/model_doc/bert) 是一個僅編碼器模型，也是第一個有效實現深度雙向性的模型，通過關注兩側的詞彙來學習更豐富的文本表示。

1. BERT 使用 [WordPiece](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) 標記化來生成文本的標記嵌入。為了區分單個句子和句子對，添加了特殊的 `[SEP]` 標記來區分它們。在每個文本序列的開頭添加特殊的 `[CLS]` 標記。帶有 `[CLS]` 標記的最終輸出用作分類任務中分類頭的輸入。BERT 還添加了段嵌入來表示標記是屬於句子對中的第一個還是第二個句子。

2. BERT 使用兩個目標進行預訓練：遮蔽語言建模和下一句預測。在遮蔽語言建模中，輸入標記的某個百分比被隨機遮蔽，模型需要預測這些標記。這解決了雙向性的問題，即模型可能作弊並看到所有詞彙然後「預測」下一個詞彙。預測遮蔽標記的最終隱藏狀態被傳遞到前饋網路，該網路在詞彙表上使用 softmax 來預測遮蔽的詞彙。

    第二個預訓練目標是下一句預測。模型必須預測句子 B 是否跟隨句子 A。一半時間句子 B 是下一句，另一半時間句子 B 是隨機句子。預測結果（無論是否為下一句）被傳遞到前饋網路，該網路在兩個類別（`IsNext` 和 `NotNext`）上使用 softmax。

3. 輸入嵌入通過多個編碼器層傳遞，以輸出最終的隱藏狀態。

要將預訓練模型用於文本分類，需要在基礎 BERT 模型之上添加序列分類頭。序列分類頭是一個線性層，接受最終的隱藏狀態並執行線性變換將其轉換為對數機率。在對數機率和目標之間計算交叉熵損失，以找到最可能的標籤。

準備嘗試文本分類了嗎？查看我們完整的[文本分類指南](https://huggingface.co/docs/transformers/tasks/sequence_classification)，學習如何微調 DistilBERT 並將其用於推理！

### 標記分類

標記分類涉及為序列中的每個標記分配標籤，例如命名實體識別或詞性標註。

要將 BERT 用於命名實體識別（NER）等標記分類任務，需要在基礎 BERT 模型之上添加標記分類頭。標記分類頭是一個線性層，接受最終的隱藏狀態並執行線性變換將其轉換為對數機率。在對數機率和每個標記之間計算交叉熵損失，以找到最可能的標籤。

準備嘗試標記分類了嗎？查看我們完整的[標記分類指南](https://huggingface.co/docs/transformers/tasks/token_classification)，學習如何微調 DistilBERT 並將其用於推理！

### 問答

問答涉及在給定的上下文或段落中找到問題的答案。

要將 BERT 用於問答，需要在基礎 BERT 模型之上添加跨度分類頭。這個線性層接受最終的隱藏狀態並執行線性變換來計算對應答案的`跨度`開始和結束對數機率。在對數機率和標籤位置之間計算交叉熵損失，以找到對應答案的最可能文本跨度。

準備嘗試問答了嗎？查看我們完整的[問答指南](https://huggingface.co/docs/transformers/tasks/question_answering)，學習如何微調 DistilBERT 並將其用於推理！

<Tip>

💡 注意一旦 BERT 經過預訓練後，將其用於不同任務是多麼容易。您只需要在預訓練模型上添加特定的頭部，就可以將隱藏狀態操作成您想要的輸出！

</Tip>

### 摘要

摘要涉及將較長的文本濃縮為較短的版本，同時保留其關鍵資訊和意義。

像 [BART](https://huggingface.co/docs/transformers/model_doc/bart) 和 [T5](model_doc/t5) 這樣的編碼器-解碼器模型是為摘要任務的序列到序列模式而設計的。我們將在本節中解釋 BART 的工作原理，然後您可以在最後嘗試微調 T5。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. BART 的編碼器架構與 BERT 非常相似，接受文本的標記和位置嵌入。BART 通過破壞輸入然後用解碼器重建來進行預訓練。與其他具有特定破壞策略的編碼器不同，BART 可以應用任何類型的破壞。不過*文本填充*破壞策略效果最好。在文本填充中，多個文本片段被替換為**單個** [`mask`] 標記。這很重要，因為模型必須預測被遮蔽的標記，這教會模型預測缺失標記的數量。輸入嵌入和遮蔽片段通過編碼器傳遞以輸出最終的隱藏狀態，但與 BERT 不同，BART 不會在最後添加前饋網路來預測詞彙。

2. 編碼器的輸出被傳遞到解碼器，解碼器必須從編碼器的輸出中預測被遮蔽的標記和任何未破壞的標記。這提供了額外的上下文來幫助解碼器恢復原始文本。解碼器的輸出被傳遞到語言建模頭，該頭執行線性變換將隱藏狀態轉換為對數機率。在對數機率和標籤之間計算交叉熵損失，標籤就是向右移位的標記。

準備嘗試摘要了嗎？查看我們完整的[摘要指南](https://huggingface.co/docs/transformers/tasks/summarization)，學習如何微調 T5 並將其用於推理！

<Tip>

如需更多關於文本生成的資訊，請查看[文本生成策略](https://huggingface.co/docs/transformers/generation_strategies)指南！

</Tip>

### 翻譯

翻譯涉及將文本從一種語言轉換為另一種語言，同時保留其意義。翻譯是序列到序列任務的另一個例子，這意味著您可以使用像 [BART](https://huggingface.co/docs/transformers/model_doc/bart) 或 [T5](model_doc/t5) 這樣的編碼器-解碼器模型來完成它。我們將在本節中解釋 BART 的工作原理，然後您可以在最後嘗試微調 T5。

BART 通過添加一個單獨的隨機初始化編碼器來適應翻譯，該編碼器將源語言映射為可以解碼為目標語言的輸入。這個新編碼器的嵌入被傳遞到預訓練編碼器，而不是原始的詞嵌入。源編碼器通過使用模型輸出的交叉熵損失更新源編碼器、位置嵌入和輸入嵌入來進行訓練。在第一步中模型參數被凍結，在第二步中所有模型參數一起訓練。
BART 後來推出了多語言版本 mBART，專為翻譯而設計，並在許多不同語言上進行預訓練。

準備嘗試翻譯了嗎？查看我們完整的[翻譯指南](https://huggingface.co/docs/transformers/tasks/translation)，學習如何微調 T5 並將其用於推理！

<Tip>

如您在本指南中所見，儘管處理不同的任務，許多模型都遵循相似的模式。理解這些常見模式可以幫助您快速掌握新模型的工作原理，以及如何將現有模型適應您的特定需求。

</Tip>

## 文本之外的模態

Transformer 不僅限於文本。它們也可以應用於其他模態，如語音和音頻、圖像和視頻。當然，在本課程中我們將專注於文本，但我們可以簡要介紹其他模態。

### 語音和音頻

讓我們首先探討 Transformer 模型如何處理語音和音頻數據，這與文本或圖像相比呈現出獨特的挑戰。

[Whisper](https://huggingface.co/docs/transformers/main/en/model_doc/whisper) 是一個編碼器-解碼器（序列到序列）transformer，在 680,000 小時的標記音頻數據上進行預訓練。這種預訓練數據量使其能夠在英語和許多其他語言的音頻任務上實現零樣本性能。解碼器允許 Whisper 將編碼器學習到的語音表示映射為有用的輸出，如文本，而無需額外的微調。Whisper 開箱即用。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/whisper_architecture.png"/>
</div>

圖表來自 [Whisper 論文](https://huggingface.co/papers/2212.04356)。

這個模型有兩個主要組件：

1. **編碼器**處理輸入音頻。原始音頻首先被轉換為對數梅爾頻譜圖。然後這個頻譜圖通過 Transformer 編碼器網路傳遞。

2. **解碼器**接收編碼的音頻表示並自回歸地預測對應的文本標記。這是一個標準的 Transformer 解碼器，訓練來根據先前的標記和編碼器輸出預測下一個文本標記。在解碼器輸入的開始使用特殊標記來引導模型執行特定任務，如轉錄、翻譯或語言識別。

Whisper 在從網路收集的 680,000 小時標記音頻數據的大規模多樣化數據集上進行預訓練。這種大規模、弱監督的預訓練是其在許多語言和任務上強大零樣本性能的關鍵。

現在 Whisper 已經預訓練完成，您可以直接將其用於零樣本推理，或在您的數據上對其進行微調，以提高在自動語音識別或語音翻譯等特定任務上的性能！

<Tip>

Whisper 的關鍵創新在於其在來自網際網路的前所未有規模的多樣化、弱監督音頻數據上進行訓練。這使其能夠在不需要任務特定微調的情況下，對不同語言、口音和任務具有出色的泛化能力。

</Tip>

### 自動語音識別

要將預訓練模型用於自動語音識別，您需要利用其完整的編碼器-解碼器結構。編碼器處理音頻輸入，解碼器自回歸地逐個標記生成轉錄。在微調時，模型通常使用標準的序列到序列損失（如交叉熵）進行訓練，以根據音頻輸入預測正確的文本標記。

使用微調模型進行推理的最簡單方法是在 `pipeline` 中。

```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-base.en"
)
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
# Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

準備嘗試自動語音識別了嗎？查看我們完整的[自動語音識別指南](https://huggingface.co/docs/transformers/tasks/asr)，學習如何微調 Whisper 並將其用於推理！

### 電腦視覺

現在讓我們轉向電腦視覺任務，這些任務涉及理解和解釋來自圖像或視頻的視覺資訊。

處理電腦視覺任務有兩種方法：

1. 將圖像分割成一系列補丁，並使用 Transformer 並行處理它們。
2. 使用現代 CNN，如 [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)，它依賴卷積層但採用現代網路設計。

<Tip>

第三種方法將 Transformer 與卷積混合（例如，[卷積視覺 Transformer](https://huggingface.co/docs/transformers/model_doc/cvt) 或 [LeViT](https://huggingface.co/docs/transformers/model_doc/levit)）。我們不會討論這些，因為它們只是結合了我們在這裡檢查的兩種方法。

</Tip>

ViT 和 ConvNeXT 通常用於圖像分類，但對於其他視覺任務如物體檢測、分割和深度估計，我們將分別查看 DETR、Mask2Former 和 GLPN；這些模型更適合這些任務。

### 圖像分類

圖像分類是基本的電腦視覺任務之一。讓我們看看不同的模型架構如何處理這個問題。

ViT 和 ConvNeXT 都可以用於圖像分類；主要區別在於 ViT 使用注意力機制，而 ConvNeXT 使用卷積。

[ViT](https://huggingface.co/docs/transformers/model_doc/vit) 完全用純 Transformer 架構取代卷積。如果您熟悉原始的 Transformer，那麼您已經基本理解了 ViT。

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

ViT 引入的主要變化是圖像如何輸入到 Transformer：

1. 圖像被分割成方形的非重疊補丁，每個補丁都被轉換為向量或*補丁嵌入*。補丁嵌入由二維卷積層生成，該層創建適當的輸入維度（對於基礎 Transformer，每個補丁嵌入為 768 個值）。如果您有一個 224x224 像素的圖像，您可以將其分割成 196 個 16x16 的圖像補丁。就像文本被標記化為詞彙一樣，圖像被「標記化」為補丁序列。

2. 一個*可學習嵌入* - 特殊的 `[CLS]` 標記 - 被添加到補丁嵌入的開頭，就像 BERT 一樣。`[CLS]` 標記的最終隱藏狀態用作附加分類頭的輸入；其他輸出被忽略。這個標記幫助模型學習如何編碼圖像的表示。

3. 最後要添加到補丁和可學習嵌入中的是*位置嵌入*，因為模型不知道圖像補丁的順序。位置嵌入也是可學習的，並且與補丁嵌入具有相同的大小。最後，所有嵌入都被傳遞到 Transformer 編碼器。

4. 輸出，特別是只有帶有 `[CLS]` 標記的輸出，被傳遞到多層感知器頭（MLP）。ViT 的預訓練目標就是分類。像其他分類頭一樣，MLP 頭將輸出轉換為類別標籤上的對數機率，並計算交叉熵損失以找到最可能的類別。

準備嘗試圖像分類了嗎？查看我們完整的[圖像分類指南](https://huggingface.co/docs/transformers/tasks/image_classification)，學習如何微調 ViT 並將其用於推理！

<Tip>

注意 ViT 和 BERT 之間的相似性：兩者都使用特殊標記（<code>[CLS]</code>）來捕捉整體表示，兩者都在其嵌入中添加位置資訊，兩者都使用 Transformer 編碼器來處理標記/補丁序列。

</Tip>