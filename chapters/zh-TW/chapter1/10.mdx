# 總結

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

在本章節中，您已經了解了 Transformer 模型、大型語言模型（LLMs）的基礎知識，以及它們如何革命性地改變人工智慧及其他領域。

## 涵蓋的關鍵概念

### 自然語言處理與大型語言模型

我們探討了自然語言處理（NLP）的定義，以及大型語言模型如何改變這個領域。您學到了：
- 自然語言處理涵蓋從分類到生成的廣泛任務範圍
- 大型語言模型是在大量文本資料上訓練的強大模型
- 這些模型能夠在單一架構內執行多項任務
- 儘管功能強大，大型語言模型仍有局限性，包括幻覺現象和偏見問題

### Transformer 功能

您了解了 🤗 Transformers 的 `pipeline()` 函數如何讓使用預訓練模型執行各種任務變得簡單：
- 文本分類、標記分類和問答系統
- 文本生成和摘要
- 翻譯和其他序列到序列任務
- 語音識別和圖像分類

### Transformer 架構

我們討論了 Transformer 模型在高層次上的運作方式，包括：
- 注意力機制的重要性
- 遷移學習如何讓模型適應特定任務
- 三種主要架構變體：僅編碼器、僅解碼器和編碼器-解碼器

### 模型架構及其應用
本章的一個重點是了解針對不同任務應該使用哪種架構：

| 模型類型        | 範例                                       | 任務                                                                            |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| 僅編碼器        | BERT、DistilBERT、ModernBERT               | 句子分類、命名實體識別、抽取式問答                                                |
| 僅解碼器        | GPT、LLaMA、Gemma、SmolLM                  | 文本生成、對話式 AI、創意寫作                                                    |
| 編碼器-解碼器   | BART、T5、Marian、mBART                    | 摘要、翻譯、生成式問答                                                           |

### 現代大型語言模型發展
您也學習了該領域的最新發展：
- 大型語言模型如何隨時間在規模和能力上不斷成長
- 縮放定律的概念以及它們如何指導模型開發
- 幫助模型處理更長序列的專門注意力機制
- 預訓練和指令調優的兩階段訓練方法

### 實際應用
在整個章節中，您已經看到這些模型如何應用於現實世界的問題：
- 使用 Hugging Face Hub 尋找和使用預訓練模型
- 利用推論 API 直接在瀏覽器中測試模型
- 了解哪些模型最適合特定任務

## 展望未來

現在您已經對 Transformer 模型是什麼以及它們在高層次上如何運作有了紮實的理解，您已經準備好深入學習如何有效地使用它們。在接下來的章節中，您將學習如何：

- 使用 Transformers 函式庫載入和微調模型
- 處理不同類型的資料作為模型輸入
- 將預訓練模型適應您的特定任務
- 部署模型用於實際應用

您在本章建立的基礎將在探索後續章節中更進階的主題和技術時為您提供良好的服務。
