<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

# Transformer 架構

在前面的章節中，我們介紹了一般的 Transformer 架構，並探討了這些模型如何解決各種任務。現在，讓我們仔細看看 Transformer 模型的三種主要架構變體，並了解何時使用每一種。然後，我們探討了這些架構如何應用於不同的語言任務。

在本節中，我們將深入探討 Transformer 模型的三種主要架構變體，並了解何時使用每一種。

<Tip>

請記住，大多數 Transformer 模型使用三種架構之一：僅編碼器、僅解碼器，或編碼器-解碼器（序列到序列）。了解這些差異將幫助您為特定任務選擇合適的模型。

</Tip>

## 編碼器模型

<Youtube id="MUqNwgPjJvQ" />
編碼器模型僅使用 Transformer 模型的編碼器部分。在每個階段，注意力層都可以存取初始句子中的所有詞彙。這些模型通常被描述為具有「雙向」注意力，並且經常被稱為*自編碼模型*。

這些模型的預訓練通常圍繞著以某種方式破壞給定的句子（例如，通過遮蔽其中的隨機詞彙），並要求模型找到或重建初始句子。

編碼器模型最適合需要理解完整句子的任務，例如句子分類、命名實體識別（以及更廣泛的詞彙分類）和抽取式問答。

<Tip>

正如我們在「🤗 Transformers 如何解決任務」中所看到的，像 BERT 這樣的編碼器模型在理解文本方面表現出色，因為它們可以雙向查看整個上下文。這使得它們非常適合需要理解整個輸入的任務。

</Tip>

這個模型家族的代表包括：

- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ModernBERT](https://huggingface.co/docs/transformers/en/model_doc/modernbert)

## 解碼器模型

<Youtube id="d_ixlCubqQw" />

解碼器模型僅使用 Transformer 模型的解碼器部分。在每個階段，對於給定的詞彙，注意力層只能存取句子中位於其之前的詞彙。這些模型通常被稱為*自回歸模型*。

解碼器模型的預訓練通常圍繞著預測句子中的下一個詞彙。

這些模型最適合涉及文本生成的任務。

<Tip>

像 GPT 這樣的解碼器模型被設計為通過一次預測一個標記來生成文本。正如我們在「🤗 Transformers 如何解決任務」中探討的，它們只能看到先前的標記，這使得它們在創意文本生成方面表現出色，但對於需要雙向理解的任務來說不太理想。

</Tip>

這個模型家族的代表包括：

- [Hugging Face SmolLM 系列](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)
- [Meta's Llama 系列](https://huggingface.co/docs/transformers/en/model_doc/llama4)
- [Google's Gemma 系列](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3)
- [DeepSeek's V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)
