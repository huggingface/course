<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

# Transformer 架構

在前面的章節中，我們介紹了一般的 Transformer 架構，並探討了這些模型如何解決各種任務。現在，讓我們仔細看看 Transformer 模型的三種主要架構變體，並了解何時使用每一種。然後，我們探討了這些架構如何應用於不同的語言任務。

在本節中，我們將深入探討 Transformer 模型的三種主要架構變體，並了解何時使用每一種。

<Tip>

請記住，大多數 Transformer 模型使用三種架構之一：僅編碼器、僅解碼器，或編碼器-解碼器（序列到序列）。了解這些差異將幫助您為特定任務選擇合適的模型。

</Tip>

## 編碼器模型

<Youtube id="MUqNwgPjJvQ" />
編碼器模型僅使用 Transformer 模型的編碼器部分。在每個階段，注意力層都可以存取初始句子中的所有詞彙。這些模型通常被描述為具有「雙向」注意力，並且經常被稱為*自編碼模型*。

這些模型的預訓練通常圍繞著以某種方式破壞給定的句子（例如，通過遮蔽其中的隨機詞彙），並要求模型找到或重建初始句子。

編碼器模型最適合需要理解完整句子的任務，例如句子分類、命名實體識別（以及更廣泛的詞彙分類）和抽取式問答。

<Tip>

正如我們在「🤗 Transformers 如何解決任務」中所看到的，像 BERT 這樣的編碼器模型在理解文本方面表現出色，因為它們可以雙向查看整個上下文。這使得它們非常適合需要理解整個輸入的任務。

</Tip>

這個模型家族的代表包括：

- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
- [ModernBERT](https://huggingface.co/docs/transformers/en/model_doc/modernbert)

## 解碼器模型

<Youtube id="d_ixlCubqQw" />

解碼器模型僅使用 Transformer 模型的解碼器部分。在每個階段，對於給定的詞彙，注意力層只能存取句子中位於其之前的詞彙。這些模型通常被稱為*自回歸模型*。

解碼器模型的預訓練通常圍繞著預測句子中的下一個詞彙。

這些模型最適合涉及文本生成的任務。

<Tip>

像 GPT 這樣的解碼器模型被設計為通過一次預測一個標記來生成文本。正如我們在「🤗 Transformers 如何解決任務」中探討的，它們只能看到先前的標記，這使得它們在創意文本生成方面表現出色，但對於需要雙向理解的任務來說不太理想。

</Tip>

這個模型家族的代表包括：

- [Hugging Face SmolLM 系列](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)
- [Meta's Llama 系列](https://huggingface.co/docs/transformers/en/model_doc/llama4)
- [Google's Gemma 系列](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3)
- [DeepSeek's V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)

### 現代大型語言模型（LLMs）

大多數現代大型語言模型（LLMs）使用僅解碼器架構。這些模型在過去幾年中在規模和能力方面都有了顯著的增長，其中一些最大的模型包含數千億個參數。

現代LLMs通常分為兩個階段進行訓練：
1. **預訓練**：模型學習在大量文本數據上預測下一個標記
2. **指令調優**：模型經過微調以遵循指令並生成有用的回應

這種方法產生了能夠理解並生成類似人類文本的模型，涵蓋廣泛的主題和任務。

#### 現代LLMs的關鍵能力

現代基於解碼器的LLMs已展現出令人印象深刻的能力：

| 能力 | 描述 | 範例 |
|------|------|------|
| 文本生成 | 創建連貫且與上下文相關的文本 | 撰寫論文、故事或電子郵件 |
| 摘要 | 將長篇文件濃縮為較短版本 | 創建報告的執行摘要 |
| 翻譯 | 在不同語言間轉換文本 | 將英文翻譯成西班牙文 |
| 問答 | 提供事實性問題的答案 | "法國的首都是什麼？" |
| 程式碼生成 | 撰寫或完成程式碼片段 | 根據描述創建函數 |
| 推理 | 逐步解決問題 | 解決數學問題或邏輯謎題 |
| 少樣本學習 | 從提示中的少數範例學習 | 僅看2-3個範例後進行文本分類 |

您可以透過Hub上的模型儲存庫頁面直接在瀏覽器中實驗基於解碼器的LLMs。這裡有一個經典[GPT-2](https://huggingface.co/openai-community/gpt2)的範例（OpenAI最優秀的開源模型！）：

<iframe
	src="https://huggingface.co/openai-community/gpt2"
	frameborder="0"
	width="100%"
	height="450"
></iframe>

## 序列到序列模型

<Youtube id="0_4KEb08xrE" />

編碼器-解碼器模型（也稱為*序列到序列模型*）使用Transformer架構的兩個部分。在每個階段，編碼器的注意力層可以存取初始句子中的所有詞彙，而解碼器的注意力層只能存取輸入中給定詞彙之前位置的詞彙。

這些模型的預訓練可以採取不同的形式，但通常涉及重建輸入已被某種方式破壞的句子（例如透過遮蔽隨機詞彙）。T5模型的預訓練包括將隨機文本片段（可能包含多個詞彙）替換為單一遮罩特殊標記，然後任務是預測此遮罩標記所替換的文本。

序列到序列模型最適合圍繞根據給定輸入生成新句子的任務，例如摘要、翻譯或生成式問答。

<Tip>

正如我們在[🤗 Transformers如何解決任務](/chapter1/5)中所看到的，像BART和T5這樣的編碼器-解碼器模型結合了兩種架構的優勢。編碼器提供對輸入的深度雙向理解，而解碼器生成適當的輸出文本。這使它們非常適合將一個序列轉換為另一個序列的任務，如翻譯或摘要。

</Tip>

### 實際應用

序列到序列模型在需要將一種文本形式轉換為另一種形式同時保持意義的任務中表現出色。一些實際應用包括：

| 應用 | 描述 | 範例模型 |
|------|------|----------|
| 機器翻譯 | 在不同語言間轉換文本 | Marian, T5 |
| 文本摘要 | 為較長文本創建簡潔摘要 | BART, T5 |
| 數據到文本生成 | 將結構化數據轉換為自然語言 | T5 |
| 語法糾正 | 修正文本中的語法錯誤 | T5 |
| 問答 | 基於上下文生成答案 | BART, T5 |

這裡有一個序列到序列模型翻譯的互動演示：

<iframe
	src="https://course-demos-speech-to-speech-translation.hf.space"
	frameborder="0"
	width="850"
	height="450"
</iframe>

這個模型家族的代表包括：

- [BART](https://huggingface.co/docs/transformers/model_doc/bart)
- [mBART](https://huggingface.co/docs/transformers/model_doc/mbart)
- [Marian](https://huggingface.co/docs/transformers/model_doc/marian)
- [T5](https://huggingface.co/docs/transformers/model_doc/t5)

## 選擇正確的架構

在處理特定的NLP任務時，您如何決定使用哪種架構？這裡有一個快速指南：

| 任務 | 建議架構 | 範例 |
|------|----------|------|
| 文本分類（情感、主題） | 編碼器 | BERT, RoBERTa |
| 文本生成（創意寫作） | 解碼器 | GPT, LLaMA |
| 翻譯 | 編碼器-解碼器 | T5, BART |
| 摘要 | 編碼器-解碼器 | BART, T5 |
| 命名實體識別 | 編碼器 | BERT, RoBERTa |
| 問答（抽取式） | 編碼器 | BERT, RoBERTa |
| 問答（生成式） | 編碼器-解碼器或解碼器 | T5, GPT |
| 對話式AI | 解碼器 | GPT, LLaMA |

<Tip>  

當不確定使用哪個模型時，請考慮：  

1. 您的任務需要什麼樣的理解？（雙向或單向）  
2. 您是在生成新文本還是分析現有文本？  
3. 您是否需要將一個序列轉換為另一個序列？  

這些問題的答案將引導您選擇正確的架構。  

</Tip>

## LLMs的演進

大型語言模型在近年來快速演進，每一代都帶來了能力上的顯著改進。

## 注意力機制

大多數transformer模型使用完全注意力，即注意力矩陣是方形的。當處理長文本時，這可能成為一個巨大的計算瓶頸。Longformer和reformer是試圖提高效率並使用稀疏版本注意力矩陣來加速訓練的模型。

<Tip>

標準注意力機制的計算複雜度為O(n²)，其中n是序列長度。對於非常長的序列，這會成為問題。下面的專門注意力機制有助於解決這個限制。

</Tip>

### LSH注意力

[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)使用LSH注意力。在softmax(QK^t)中，只有矩陣QK^t中最大的元素（在softmax維度中）才會提供有用的貢獻。因此，對於Q中的每個查詢q，我們只需考慮K中接近q的鍵k。使用哈希函數來確定q和k是否接近。注意力遮罩被修改以遮蔽當前標記（除了第一個位置），因為它會給出相等的查詢和鍵（因此彼此非常相似）。由於哈希可能有些隨機，實際上會使用多個哈希函數（由n_rounds參數決定），然後將它們平均在一起。

### 局部注意力

[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)使用局部注意力：通常，局部上下文（例如，左右兩個標記是什麼？）足以對給定標記採取行動。此外，通過堆疊具有小窗口的注意力層，最後一層將具有超出窗口中標記的感受野，使它們能夠構建整個句子的表示。

一些預選的輸入標記也被賦予全局注意力：對於這些少數標記，注意力矩陣可以存取所有標記，這個過程是對稱的：所有其他標記都可以存取這些特定標記（除了它們局部窗口中的標記）。這在論文的圖2d中顯示，請參見下面的樣本注意力遮罩：

<div class="flex justify-center">
    <img scale="50 %" align="center" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png"/>
</div>

使用這些參數較少的注意力矩陣，然後允許模型具有更大序列長度的輸入。

### 軸向位置編碼

[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)使用軸向位置編碼：在傳統的transformer模型中，位置編碼E是一個大小為\\(l\\) × \\(d\\)的矩陣，\\(l\\)是序列長度，\\(d\\)是隱藏狀態的維度。如果您有非常長的文本，這個矩陣可能會很大，在GPU上佔用太多空間。為了緩解這個問題，軸向位置編碼包括將大矩陣E分解為兩個較小的矩陣E1和E2，維度分別為\\(l_{1} \times d_{1}\\)和\\(l_{2} \times d_{2}\\)，使得\\(l_{1} \times l_{2} = l\\)且\\(d_{1} + d_{2} = d\\)（對於長度的乘積，這最終會小得多）。E中時間步驟\\(j\\)的嵌入是通過連接E1中時間步驟\\(j \% l1\\)的嵌入和E2中\\(j // l1\\)的嵌入來獲得的。

## 結論

在本節中，我們探討了三種主要的Transformer架構和一些專門的注意力機制。理解這些架構差異對於為您的特定NLP任務選擇正確的模型至關重要。

隨著課程的進行，您將獲得這些不同架構的實際經驗，並學習如何針對您的特定需求進行微調。在下一節中，我們將探討這些模型中存在的一些限制和偏見，這些是您在部署時應該注意的。