# Transformers能做什麼？

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter1/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter1/section3.ipynb"},
]} />

在本節中，我們將看看 Transformer 模型可以做什麼，並使用我們來自 🤗 Transformers 函式庫的第一個工具：pipeline() 函數。
<Tip>
👀 看到右上角的 <em>在 Colab 中開啟</em>按鈕了嗎？點擊它可以開啟一個包含本節所有程式碼範例的 Google Colab 筆記本。任何包含程式碼範例的章節都會有這個按鈕。

如果您想在本地執行這些範例，我們建議您查看設定指南<a href="/course/chapter0">準備</a>.
</Tip>

## Transformer 無處不在！

Transformer 模型被用來解決跨不同模態的各種任務，包括自然語言處理（NLP）、電腦視覺、音訊處理等等。以下是一些使用 Hugging Face 和 Transformer 模型的公司和組織，它們也透過分享模型回饋社群：

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG" alt="使用 Hugging Face 的公司" width="100%">

[🤗 Transformers 函式庫](https://github.com/huggingface/transformers)提供了創建和使用這些共享模型的功能。[模型中心（hub）](https://huggingface.co/models)包含數百萬個預訓練模型，任何人都可以下載和使用。您也可以將自己的模型上傳到中心！

<Tip>
⚠️ Hugging Face Hub 不僅限於 Transformer 模型。任何人都可以分享他們想要的任何類型的模型或資料集！創建一個 Huggingface.co 帳戶(https://huggingface.co/join)以使用所有可用功能！
</Tip>

在深入了解 Transformer 模型的內部運作原理之前，讓我們先看看一些例子，了解它們如何用來解決一些有趣的自然語言處理問題。

## 使用管道（pipeline）

<iframe width="605" height="320" src="https://www.youtube.com/embed/tiZFewofSLM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

🤗 Transformers 函式庫中最基本的物件是 **pipeline()** 函數。它將模型與其必要的預處理和後處理步驟連接起來，讓我們能夠直接輸入任何文本並獲得可理解的答案：

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```
```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```

我們甚至可以傳入多個句子！

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
``` 
```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

預設情況下，這個管道（pipeline）會選擇一個特定的預訓練模型，該模型已針對英語情感分析進行微調。當您創建 **分類器** 物件時，模型會被下載並快取。如果您重新執行指令，將使用快取的模型，無需再次下載模型。

當您將文本傳遞給管道（pipeline）時，涉及三個主要步驟：

1. 文本被預處理成模型能理解的格式。
2. 預處理的輸入被傳遞給模型。
3. 模型的預測結果經過後處理，以便您能理解它們。

## 不同模態的可用管道

pipeline() 函數支援多種模態，讓您能夠處理文本、圖像、音訊，甚至多模態任務。在本課程中我們將專注於文本任務，但了解 transformer 架構的潛力是有用的，所以我們將簡要概述一下。

以下是可用功能的概覽：

<Tip>

如需完整且最新的 pipeline() 列表，請參閱 [🤗 Transformers 文件](https://huggingface.co/docs/hub/en/models-tasks)。

</Tip>

## 文本 pipeline

* text-generation：從提示生成文本 
* text-classification：將文本分類到預定義類別 
* summarization：創建文本的簡短版本同時保留關鍵資訊 
* translation：將文本從一種語言翻譯成另一種語言 
* zero-shot-classification：在沒有特定標籤訓練的情況下分類文本 
* feature-extraction：提取文本的向量表示

## 圖像 pipeline

* image-to-text：生成圖像的文本描述
* image-classification：識別圖像中的物體
* object-detection：定位和識別圖像中的物體

## 音訊 pipeline

* automatic-speech-recognition：將語音轉換為文本
* audio-classification：將音訊分類到類別
* text-to-speech：將文本轉換為語音音訊

## 多模態 pipeline

* image-text-to-text：根據文本提示回應圖像

讓我們更詳細地探索其中一些管道！

## 零樣本分類

我們將從處理一個更具挑戰性的任務開始，需要對未標記的文本進行分類。這在現實世界的專案中是常見的情況，因為標註文本通常耗時且需要領域專業知識。對於這種使用情況，**零樣本分類**管道非常強大：它允許您指定用於分類的標籤，因此您不必依賴預訓練模型的標籤。您已經看到模型如何使用正面或負面這兩個標籤對句子進行分類——但它也可以使用您喜歡的任何其他標籤集對文本進行分類。

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```
```python out
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

這個 pipeline 被稱為零樣本，因為您不需要在自己的數據上微調模型就能使用它。它可以直接為您想要的任何標籤列表回傳概率分數！

<Tip>
✏️**快來試試看！** 嘗試使用您自己的序列和標籤，看看模型的表現如何。
</Tip>

## 文本生成

現在讓我們看看如何使用 pipeline 來生成一些文本。這裡的主要概念是您提供一個提示，模型會透過生成剩餘文本來自動完成它。這類似於許多手機上的預測文本功能。文本生成涉及隨機性，所以如果您沒有得到與下面顯示的相同結果是正常的。

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```
```python out
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows — data flows of various types, as seen by the '
                    'HTTP'}]
```

您可以使用參數 **num_return_sequences** 控制生成多少個不同的序列，並使用參數 **max_length** 控制輸出文本的總長度。

<Tip>
✏️**快來試試看！** 使用 num_return_sequences 和 max_length 參數生成兩個各 15 個詞的句子。
</Tip>

## 在 pipeline 中使用 Hub 上的任何模型
前面的範例使用了手邊任務的預設模型，但您也可以從 Hub 中選擇特定模型在管道中用於特定任務 - 例如，文本生成。前往[模型中心（hub）](https://huggingface.co/models)並點擊左側對應的標籤，以僅顯示該任務支援的模型。您應該會看到像這樣的[頁面](https://huggingface.co/models?pipeline_tag=text-generation)。

讓我們試試 [**HuggingFaceTB/SmolLM2-360M**](https://huggingface.co/HuggingFaceTB/SmolLM2-360M) 模型！以下是如何在與之前相同的 pipeline 中載入它：

```python
from transformers import pipeline

generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-360M")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```
```python out
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

您可以透過點擊語言標籤來精煉模型搜尋，並選擇一個能夠生成其他語言文本的模型。模型中心甚至包含支援多種語言的多語言模型檢查點。

一旦您點擊選擇模型，您會看到有一個小工具讓您能夠直接在線上試用它。這樣您可以在下載之前快速測試模型的能力。

<Tip>
✏️**快來試試看！** 使用篩選器找到另一種語言的文本生成模型。隨意使用小工具並在 pipeline 中使用它！
</Tip>

## 推理服務提供者
所有模型都可以透過推理服務提供者直接在瀏覽器中測試，這在 [Hugging Face 網站](https://huggingface.co/)上可用。您可以透過輸入自訂文本並觀看模型處理輸入數據，直接在此頁面上使用模型。

為小工具提供動力的推理服務提供者也作為付費產品提供，如果您的工作流程需要它，這會很方便。請參閱[定價頁面](https://huggingface.co/pricing)了解更多詳情。

## 遮罩填充

您將嘗試的下一個 pipeline 是 `填充遮罩`。這個任務的概念是填充給定文本中的空白：

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

```python out
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

`top_k` 參數控制您想要顯示多少種可能性。請注意，這裡模型填充特殊的< **mask** >詞彙，通常稱為遮罩標記。其他遮罩填充模型可能有不同的遮罩標記，所以在探索其他模型時驗證正確的遮罩詞彙總是好的。檢查它的一種方法是查看小工具中使用的遮罩詞彙。

<Tip>

✏️**快來試試看！** 在 Hub 上搜尋基於 **bert-base-cased** 模型並在推理 API 小工具中識別其遮罩詞彙。這個模型對我們上面 **pipeline** 範例中的句子預測什麼？

</Tip>

## 命名實體識別

命名實體識別（NER）是一項任務，模型必須找出輸入文本的哪些部分對應於實體，如人物、地點或組織。讓我們看一個例子：

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```
```python out
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

這裡模型正確識別了 Sylvain 是一個人物（PER），Hugging Face 是一個組織（ORG），Brooklyn 是一個地點（LOC）。

我們在 pipeline 創建函數中傳遞選項 grouped_entities=True 來告訴 pipeline 將對應於同一實體的句子部分重新組合在一起：這裡模型正確地將「Hugging」和「Face」組合為單一組織，即使名稱由多個詞彙組成。事實上，正如我們將在下一章中看到的，預處理甚至將一些詞彙分割成更小的部分。例如，Sylvain 被分割成四個片段：S、##yl、##va 和 ##in。在後處理步驟中，pipeline 成功地重新組合了這些片段。

<Tip>
✏️**快來試試看！** 在模型中心搜尋能夠進行英語詞性標註（通常縮寫為 POS）的模型。這個模型對上面範例中的句子預測什麼？
</Tip>

## 問答

問答 pipeline 使用給定上下文中的資訊來回答問題：

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```
```python out
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

請注意，這個 pipeline 透過從提供的上下文中提取資訊來工作；它不會生成答案。

## 文本摘要

文本摘要是將文本縮減為較短文本的任務，同時保留文本中提及的所有（或大部分）重要方面。以下是一個例子：

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
"""
)
```
```python out
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

與文本生成一樣，您可以為結果指定 **max_length** 或 **min_length**。

## 翻譯

對於翻譯，如果您在任務名稱中提供語言對（例如「**translation_en_to_fr**」），則可以使用默認模型，但最簡單的方法是在[模型中心（hub）](https://huggingface.co/models)選擇要使用的模型。這裡我們將嘗試從法語翻譯成英語：

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```
```python out
[{'translation_text': 'This course is produced by Hugging Face.'}]

```

與文本生成和文本摘要一樣，您可以指定結果的 **max_length** 或 **min_length**。

<Tip>

✏️**快來試試看！** 搜尋其他語言的翻譯模型並嘗試將前一句話翻譯成幾種不同的語言。

</Tip>

## 圖像和音訊 pipeline

除了文本之外，Transformer 模型也可以處理圖像和音訊。以下是一些例子：

## 圖像分類

```python
from transformers import pipeline

image_classifier = pipeline(
    task="image-classification", model="google/vit-base-patch16-224"
)
result = image_classifier(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
print(result)
```
```python out
[{'label': 'lynx, catamount', 'score': 0.43350091576576233},
 {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',
  'score': 0.034796204417943954},
 {'label': 'snow leopard, ounce, Panthera uncia',
  'score': 0.03240183740854263},
 {'label': 'Egyptian cat', 'score': 0.02394474856555462},
 {'label': 'tiger cat', 'score': 0.02288915030658245}]
```

## 自動語音識別

```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-large-v3"
)
result = transcriber(
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"
)
print(result)
```

## 結合多個來源的數據

Transformer 模型的一個強大應用是它們能夠結合和處理來自多個來源的數據。這在您需要以下情況時特別有用：

1. 跨多個資料庫或儲存庫搜尋
2. 整合來自不同格式的資訊（文本、圖像、音訊）
3. 創建相關資訊的統一視圖

例如，您可以構建一個系統：

* 跨多種模態（如文本和圖像）的資料庫搜尋資訊。
* 將來自不同來源的結果結合成單一連貫的回應。例如，來自音訊檔案和文本描述。
* 從文件和元數據資料庫中呈現最相關的資訊。

## 結論

本章節展示的 pipeline 主要用於示範目的。它們是為特定任務編程的，無法執行這些任務的變化。在下一章中，您將學習 **pipeline()** 函數內部的內容以及如何自訂其行為。