# 偏見與限制

<CourseFloatingBanner chapter={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section8.ipynb"},
]} />

如果您打算在生產環境中使用預訓練模型或微調版本，請注意雖然這些模型是強大的工具，但它們確實存在限制。

其中最大的問題是，為了能夠在大量數據上進行預訓練，研究人員通常會抓取所有能找到的內容，既包含網路上最好的內容，也包含最糟糕的內容。

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])
```

```python out
['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

當要求填入這兩個句子中的缺失詞彙時，模型只給出一個性別中性的答案（服務生）。其他答案都是通常與特定性別相關的職業——是的，「妓女」最終出現在模型將「女性」與「工作」聯繫起來的前五個可能性中。

即使 BERT 是少數不是透過抓取整個網路數據建構的 Transformer 模型之一，而是使用看似中性的數據（它是在英文維基百科和 BookCorpus 數據集上訓練的），這種情況仍然會發生。