# 本章簡介

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

## 歡迎來到🤗課程

<Youtube id="00GKzGyWFEs" />

本課程將教您使用 [Hugging Face](https://huggingface.co/) 生態系統中的函式庫來學習大型語言模型 (LLMs) 和自然語言處理 (NLP)，包括 [🤗 Transformers](https://github.com/huggingface/transformers)、[🤗 Datasets](https://github.com/huggingface/datasets)、[🤗 Tokenizers](https://github.com/huggingface/tokenizers) 和 [🤗 Accelerate](https://github.com/huggingface/accelerate)，以及 [Hugging Face Hub](https://huggingface.co/models)。
它是完全免費的，並且沒有廣告。

我們也會涵蓋 Hugging Face 生態系統以外的函式庫。這些都是對 AI 社群的卓越貢獻，也是極其實用的工具。

它完全免費且無廣告。

## 理解 NLP 和 LLMs

雖然本課程最初專注於 NLP（自然語言處理），但已演變為強調大型語言模型（LLMs），這代表了該領域的最新進展。

**有什麼區別？**

- **NLP（自然語言處理）** 是一個更廣泛的領域，專注於讓電腦理解、解釋和生成人類語言。NLP 涵蓋許多技術和任務，如情感分析、命名實體識別和機器翻譯。
- **LLMs（大型語言模型）** 是 NLP 模型的一個強大子集，其特點是規模龐大、訓練資料廣泛，以及能夠在最少的任務特定訓練下執行各種語言任務。像 Llama、GPT 或 Claude 系列等模型都是 LLMs 的例子，它們已經徹底改變了 NLP 領域的可能性。

在整個課程中，您將學習傳統 NLP 概念和尖端 LLM 技術，因為理解 NLP 的基礎對於有效使用 LLMs 至關重要。

## 您可以期待什麼？

以下是課程的簡要概述：

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course."/>
</div>

- 第 1-4 章 介紹 🤗 Transformers 函式庫的主要概念。在完成課程的這個部分後，您將熟悉 Transformer 模型的運作原理，並知道如何使用來自 [Hugging Face Hub](https://huggingface.co/models) 的模型、在資料集上進行微調，以及在 Hub 上分享您的結果！
- 第 5-8 章 教授 🤗 Datasets 和 🤗 Tokenizers 的基礎知識，然後深入探討經典 NLP 任務和 LLM 技術。在完成這個部分後，您將能夠獨立處理最常見的語言處理挑戰。
- 第 9 章 超越 NLP，涵蓋如何在 🤗 Hub 上建立和分享模型的展示。在完成這個部分後，您將準備好向世界展示您的 🤗 Transformers 應用程式！
- 第 10-12 章 深入探討進階 LLM 主題，如微調、策劃高品質資料集和建立推理模型。

本課程：

* 需要良好的 Python 知識 
* 最好在修習深度學習入門課程後再學習，例如 [fast.ai實用深度學習教程](https://course.fast.ai/) 或 [DeepLearning.AI](https://www.deeplearning.ai/) 開發的課程之一
* 不需要事先具備 [PyTorch](https://pytorch.org/) 或 [TensorFlow](https://www.tensorflow.org/) 知識，但對其中任一框架有一定熟悉度會有所幫助

完成本課程後，我們建議您查看 [DeepLearning.AI的自然語言處理系列課程](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh)，其中涵蓋了廣泛的傳統 NLP 模型，如樸素貝葉斯和 LSTM，這些都是值得了解的重要技術！

## 我們是誰？

關於作者：

**Abubakar Abid** 在史丹佛大學完成應用機器學習博士學位。在攻讀博士期間，他創立了 Gradio，這是一個開源 Python 函式庫，已被用於建立超過 60 萬個機器學習展示。Gradio 被 Hugging Face 收購，Abubakar 現在在該公司擔任機器學習團隊負責人。

**Ben Burtenshaw** 是 Hugging Face 的機器學習工程師。他在安特衛普大學完成自然語言處理博士學位，研究中應用 Transformer 模型生成兒童故事以提升識字技能。此後，他專注於為更廣泛的社群開發教育材料和工具。

**Matthew Carrigan** 是 Hugging Face 的機器學習工程師。他居住在愛爾蘭都柏林，曾在 Parse.ly 擔任機器學習工程師，之前在都柏林三一學院擔任博士後研究員。他不相信透過擴展現有架構能達到 AGI，但對機器人永生仍抱持高度期望。

**Lysandre Debut** 是 Hugging Face 的機器學習工程師，從 🤗 Transformers 函式庫的早期開發階段就開始參與。他的目標是透過開發具有非常簡單 API 的工具，讓每個人都能使用 NLP。

**Sylvain Gugger** 是 Hugging Face 的研究工程師，也是 🤗 Transformers 函式庫的核心維護者之一。他曾是 fast.ai 的研究科學家，與 Jeremy Howard 共同撰寫了[Deep Learning for Coders with fastai and Py Torch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)。他的研究重點是透過設計和改進技術，讓模型能在有限資源上快速訓練，使深度學習更加普及。

**Dawood Khan** 是 Hugging Face 的機器學習工程師。他來自紐約市，畢業於紐約大學電腦科學系。在擔任 iOS 工程師幾年後，Dawood 辭職與共同創辦人一起創立 Gradio。Gradio 最終被 Hugging Face 收購。

**Merve Noyan** 是 Hugging Face 的開發者倡導者，致力於開發工具並圍繞這些工具建立內容，以實現機器學習的民主化。

**Lucile Saulnier** 是 Hugging Face 的機器學習工程師，負責開發和支援開源工具的使用。她也積極參與自然語言處理領域的許多研究專案，如協作訓練和 BigScience。

**Lewis Tunstall**  是 Hugging Face 的機器學習工程師，專注於開發開源工具並讓更廣泛的社群能夠使用。他也是 O'Reilly 書籍[O’Reilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)的共同作者。

**Leandro von Werra**  是 Hugging Face 開源團隊的機器學習工程師，也是 O'Reilly 書籍[O’Reilly book on Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)的共同作者。他擁有多年的產業經驗，透過跨越整個機器學習技術堆疊的工作，將 NLP 專案帶入生產環境。

## 常見問題

以下是一些常見問題的解答：

- **修習本課程是否能獲得認證？** 目前我們沒有為本課程提供任何認證。不過，我們正在為 Hugging Face 生態系統開發認證計畫 -- 敬請期待！
- **我應該花多少時間在這門課程上？** 本課程的每一章都設計為在 1 週內完成，每週大約需要 6-8 小時的學習時間。不過，您可以根據自己的需要花費任何時間來完成課程。
- **如果我有問題，可以在哪裡提問？** 如果您對課程的任何部分有疑問，只需點擊頁面頂部的 *「提出問題」* 橫幅，就會自動重新導向到 [Hugging Face forums](https://discuss.huggingface.co/) 的相關版塊：

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png" alt="Link to the Hugging Face forums" width="75%">

請注意，如果您希望在完成課程後進行更多練習，論壇上也提供了[專案想法清單](https://discuss.huggingface.co/c/course/course-event/25)。

- **我可以在哪裡取得課程的程式碼？** 對於每個章節，點擊頁面頂部的橫幅即可在 Google Colab 或 Amazon SageMaker Studio Lab 中執行程式碼：

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png" alt="Link to the Hugging Face course notebooks" width="75%">

包含課程所有程式碼的 Jupyter 筆記本託管在 [`huggingface/notebooks`](https://github.com/huggingface/notebooks) 儲存庫中。如果您希望在本地使用它們，請查看 GitHub 上 [`course`](https://github.com/huggingface/course#-jupyter-notebooks) 儲存庫中的說明。

- **我如何為課程做出貢獻？** 有很多方式可以為課程做出貢獻！如果您發現錯字或錯誤，請在 [`course`](https://github.com/huggingface/course) 儲存庫中開啟一個 issue。如果您想幫助將課程翻譯成您的母語，請查看[這裡](https://github.com/huggingface/course#translating-the-course-into-your-language)的說明。
- **每個翻譯版本做了哪些選擇？** 每個翻譯版本都有一個詞彙表和 TRANSLATING.txt 檔案，詳細說明了對機器學習術語等所做的選擇。您可以在 [這裡](https://github.com/huggingface/course/blob/main/chapters/de/TRANSLATING.txt) 找到德語版本的範例。
- **我可以重複使用這門課程嗎？** 當然可以！課程採用寬鬆的 [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0.html) 發布。這意味著您必須給予適當的署名、提供授權條款的連結，並說明是否有所更改。您可以用任何合理的方式這樣做，但不能以任何暗示授權方認可您或您的使用的方式進行。如果您想引用這門課程，請使用以下 BibTeX：

```
@misc{huggingfacecourse,
  author = {Hugging Face},
  title = {The Hugging Face Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year = {2022},
  note = "[Online; accessed <today>]"
}
```

## 讓我們開始吧 🚀

您準備好了嗎？在本章中，您將學習：

* 如何使用 pipeline() 函數來解決 NLP 任務，如文本生成和分類
* 關於 Transformer 架構
* 如何區分編碼器、解碼器和編碼器-解碼器架構及其使用案例
