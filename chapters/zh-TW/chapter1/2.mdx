# 自然語言處理與大型語言模型

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

在進入 Transformer 模型之前，讓我們快速概述一下自然語言處理是什麼、大型語言模型如何改變了這個領域，以及我們為什麼關注它。

## 什麼是自然語言處理？

自然語言處理（NLP）是語言學和機器學習的一個領域，專注於理解與人類語言相關的一切。 NLP 任務的目標不僅是理解單個詞彙，更要能夠理解這些詞彙的上下文。

以下是常見 NLP 任務的清單，並附有各自的範例：

- **對整個句子進行分類**: 獲取評論的情感、檢測電子郵件是否為垃圾郵件、判斷句子語法是否正確或兩個句子是否邏輯相關
- **對句子中的每個詞進行分類**: 識別句子中的語法成分（名詞、動詞、形容詞）或命名實體（人名、地點、組織）
- **生成文本內容**: 用自動生成的文本完成提示、填補文本中的空白詞彙
- **從文本中提取答案**: 根據給定的問題和上下文，從文本中提取答案
- **從輸入文本生成新句子**: 將文本翻譯成另一種語言、總結文本

NLP 不僅限於書面文字，還處理語音識別和電腦視覺中的複雜挑戰，例如生成音頻樣本的轉錄或圖像的描述。

## 大型語言模型的興起

近年來，大型語言模型（LLMs）徹底改變了自然語言處理領域。這些模型包括 GPT（生成式預訓練 Transformer）和 [Llama](https://huggingface.co/meta-llama) 等架構，已經改變了語言處理的可能性。

<Tip>

大型語言模型（LLM）是在大量文本數據上訓練的 AI 模型，能夠理解和生成類似人類的文本，識別語言模式，並在沒有特定任務訓練的情況下執行各種語言任務。它們代表了自然語言處理（NLP）領域的重大進步。

</Tip>

LLMs 具有以下特徵：

- **規模**：包含數百萬、數十億甚至數千億個參數
- **通用能力**：無需特定任務訓練即可執行多項任務 
- **上下文學習**: 能從提示中提供的範例學習
- **湧現能力**: 隨著模型規模增長，展現出未明確編程或預期的能力

LLMs 的出現已經將範式從為特定 NLP 任務建構專門模型轉變為使用單一大型模型，該模型可以通過提示或微調來處理各種語言任務。這使得複雜的語言處理更加普及，同時也在效率、倫理和部署等領域帶來了新的挑戰。

然而，LLMs 也有重要的限制：

- **幻覺**：它們可能自信地生成錯誤資訊
- **缺乏真正的理解**：它們缺乏對世界的真正理解，純粹基於統計模式運作
- **偏見**：它們可能重現訓練數據或輸入中存在的偏見
- **上下文窗口**：它們的上下文窗口有限（儘管這正在改善）
- **計算資源**：它們需要大量的計算資源

## 為什麼語言處理具有挑戰性？

電腦處理資訊的方式與人類不同。例如，當我們閱讀「我餓了」這句話時，我們可以輕易理解其含義。同樣地，給定兩個句子如「我餓了」和「我難過了」，我們能夠輕易判斷它們的相似程度。對於機器學習（ML）模型來說，這些任務更加困難。

文本需要以一種能讓模型學習的方式進行處理。由於語言的複雜性，我們需要仔細思考如何進行這種處理。已經有大量研究探討如何表示文本，我們將在下一章節中探討一些方法。

即使在大型語言模型（LLMs）的進步下，許多基本挑戰仍然存在。這些包括理解歧義、文化背景、諷刺和幽默。LLMs 通過在多樣化數據集上的大規模訓練來應對這些挑戰，但在許多複雜場景中仍然達不到人類水準的理解。