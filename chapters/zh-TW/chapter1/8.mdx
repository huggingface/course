# 深入探討大型語言模型的文本生成推理

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="Xp2w1_LKZN4" />

到目前為止，我們已經探討了 Transformer 架構與一系列離散任務的關係，如文本分類或摘要。然而，大型語言模型最常用於文本生成，這就是我們將在本章節中探討的內容。

在本頁面中，我們將探討大型語言模型推理背後的核心概念，提供對這些模型如何生成文本以及推理過程中涉及的關鍵組件的全面理解。

## 理解基礎概念

讓我們從基本概念開始。推理是使用已訓練的大型語言模型（LLM）從給定的輸入提示生成類人文本的過程。語言模型利用從訓練中獲得的知識，逐字制定回應。模型利用從數十億參數中學習到的機率來預測和生成序列中的下一個標記。這種順序生成正是讓大型語言模型能夠產生連貫且與上下文相關文本的原因。

## 注意力機制的作用

注意力機制是賦予大型語言模型理解上下文和生成連貫回應能力的關鍵。在預測下一個詞時，句子中的每個詞並不具有相同的權重——例如，在句子「法國的首都是...」中，「法國」和「首都」這兩個詞對於確定下一個詞應該是「巴黎」至關重要。這種專注於相關資訊的能力就是我們所說的注意力。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

這種識別最相關詞彙來預測下一個標記的過程已被證明極其有效。儘管訓練大型語言模型的基本原理——預測下一個標記——自 BERT 和 GPT-2 以來基本保持一致，但在擴展神經網路規模以及讓注意力機制能夠處理越來越長的序列，同時成本越來越低方面，已經取得了重大進展。

<Tip>

簡而言之，注意力機制是大型語言模型能夠生成既連貫又具有上下文感知能力文本的關鍵。它使現代大型語言模型與前幾代語言模型區別開來。

</Tip>

### 上下文長度與注意力範圍

既然我們已經理解了注意力機制，讓我們來探討大型語言模型實際上能處理多少上下文。這就帶我們來到上下文長度，或者說是模型的「注意力範圍」。

上下文長度是指大型語言模型能夠一次處理的最大標記數量（單詞或單詞的一部分）。可以將其視為模型工作記憶體的大小。

這些能力受到幾個實際因素的限制：

- 模型的架構和大小
- 可用的計算資源
- 輸入和期望輸出的複雜性

在理想的世界中，我們可以向模型提供無限的上下文，但硬體限制和計算成本使這變得不切實際。這就是為什麼不同的模型被設計成具有不同的上下文長度，以平衡能力與效率。

<Tip>

上下文長度是模型在生成回應時能夠同時考慮的最大標記數量。

</Tip>

