# 深入探討大型語言模型的文本生成推理

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="Xp2w1_LKZN4" />

到目前為止，我們已經探討了 Transformer 架構與一系列離散任務的關係，如文本分類或摘要。然而，大型語言模型最常用於文本生成，這就是我們將在本章節中探討的內容。

在本頁面中，我們將探討大型語言模型推理背後的核心概念，提供對這些模型如何生成文本以及推理過程中涉及的關鍵組件的全面理解。

## 理解基礎概念

讓我們從基本概念開始。推理是使用已訓練的大型語言模型（LLM）從給定的輸入提示生成類人文本的過程。語言模型利用從訓練中獲得的知識，逐字制定回應。模型利用從數十億參數中學習到的機率來預測和生成序列中的下一個標記。這種順序生成正是讓大型語言模型能夠產生連貫且與上下文相關文本的原因。

## 注意力機制的作用

注意力機制是賦予大型語言模型理解上下文和生成連貫回應能力的關鍵。在預測下一個詞時，句子中的每個詞並不具有相同的權重——例如，在句子「法國的首都是...」中，「法國」和「首都」這兩個詞對於確定下一個詞應該是「巴黎」至關重要。這種專注於相關資訊的能力就是我們所說的注意力。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">

這種識別最相關詞彙來預測下一個標記的過程已被證明極其有效。儘管訓練大型語言模型的基本原理——預測下一個標記——自 BERT 和 GPT-2 以來基本保持一致，但在擴展神經網路規模以及讓注意力機制能夠處理越來越長的序列，同時成本越來越低方面，已經取得了重大進展。

<Tip>

簡而言之，注意力機制是大型語言模型能夠生成既連貫又具有上下文感知能力文本的關鍵。它使現代大型語言模型與前幾代語言模型區別開來。

</Tip>
