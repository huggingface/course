# å­—ç¯€å°ç·¨ç¢¼æ¨™è¨˜åŒ–

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb"},
]} />

å­—ç¯€å°ç·¨ç¢¼(BPE)æœ€åˆè¢«é–‹ç™¼ç‚ºä¸€ç¨®å£“ç¸®æ–‡æœ¬çš„ç®—æ³•,ç„¶å¾Œåœ¨é è¨“ç·´ GPT æ¨¡å‹æ™‚è¢« OpenAI ç”¨æ–¼æ¨™è¨˜åŒ–ã€‚è¨±å¤š Transformer æ¨¡å‹éƒ½ä½¿ç”¨å®ƒ,åŒ…æ‹¬ GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTaã€‚

<Youtube id="HEikzVL-lZU"/>

> [!TIP]
> ğŸ’¡ æœ¬ç¯€æ·±å…¥ä»‹ç´¹äº†BPEï¼Œç”šè‡³å±•ç¤ºäº†ä¸€å€‹å®Œæ•´çš„å¯¦ç¾ã€‚å¦‚æœä½ åªæƒ³å¤§è‡´ç­è§£æ¨™è¨˜åŒ–ç®—æ³•ï¼Œå¯ä»¥è·³åˆ°æœ€å¾Œã€‚

## è¨“ç·´ç®—æ³•

BPE è¨“ç·´é¦–å…ˆè¨ˆç®—èªæ–™åº«ä¸­ä½¿ç”¨çš„å”¯ä¸€å–®è©é›†(åœ¨å®Œæˆæ¨™æº–åŒ–å’Œé æ¨™è¨˜åŒ–æ­¥é©Ÿä¹‹å¾Œ)ï¼Œç„¶å¾Œé€šéç²å–ç”¨æ–¼ç·¨å¯«é€™äº›å–®è©çš„æ‰€æœ‰ç¬¦è™Ÿä¾†æ§‹å»ºè©å½™è¡¨ã€‚èˆ‰ä¸€å€‹ç°¡å–®çš„ä¾‹å­ï¼Œå‡è¨­æˆ‘å€‘çš„èªæ–™åº«ä½¿ç”¨äº†é€™äº”å€‹è©:

```
"hug", "pug", "pun", "bun", "hugs"
```

åŸºç¤è©å½™å°‡æ˜¯ `["b", "g", "h", "n", "p", "s", "u"]`ã€‚å°æ–¼å¯¦éš›æƒ…æ³ï¼ŒåŸºæœ¬è©å½™è¡¨å°‡åŒ…å«æ‰€æœ‰ ASCII å­—ç¬¦ï¼Œè‡³å°‘ï¼Œå¯èƒ½é‚„åŒ…å«ä¸€äº› Unicode å­—ç¬¦ã€‚å¦‚æœæ‚¨æ­£åœ¨æ¨™è¨˜çš„ç¤ºä¾‹ä½¿ç”¨ä¸åœ¨è¨“ç·´èªæ–™åº«ä¸­çš„å­—ç¬¦ï¼Œå‰‡è©²å­—ç¬¦å°‡è½‰æ›ç‚ºæœªçŸ¥æ¨™è¨˜ã€‚é€™å°±æ˜¯ç‚ºä»€éº¼è¨±å¤š NLP æ¨¡å‹åœ¨åˆ†æå¸¶æœ‰è¡¨æƒ…ç¬¦è™Ÿçš„å…§å®¹æ–¹é¢éå¸¸ç³Ÿç³•çš„åŸå› ä¹‹ä¸€ã€‚

> [!TIP]
> TGPT-2 å’Œ RoBERTa æ¨™è¨˜å™¨(éå¸¸ç›¸ä¼¼)æœ‰ä¸€å€‹è°æ˜çš„æ–¹æ³•ä¾†è™•ç†é€™å€‹å•é¡Œ: ä»–å€‘ä¸æŠŠå–®è©çœ‹æˆæ˜¯ç”¨ Unicode å­—ç¬¦å¯«çš„ï¼Œè€Œæ˜¯ç”¨å­—ç¯€å¯«çš„ã€‚é€™æ¨£ï¼ŒåŸºæœ¬è©å½™è¡¨çš„å¤§å°å¾ˆå°(256),ä½†ä½ èƒ½æƒ³åˆ°çš„æ¯å€‹å­—ç¬¦ä»å°‡è¢«åŒ…å«åœ¨å…§,è€Œä¸æœƒæœ€çµ‚è½‰æ›ç‚ºæœªçŸ¥æ¨™è¨˜ã€‚é€™å€‹æŠ€å·§è¢«ç¨±ç‚º *å­—ç¯€ç´š BPE*ã€‚

ç²å¾—é€™å€‹åŸºæœ¬è©å½™å¾Œï¼Œæˆ‘å€‘æ·»åŠ æ–°çš„æ¨™è¨˜ï¼Œç›´åˆ°é€šéå­¸ç¿’*åˆä½µ*é”åˆ°æ‰€éœ€çš„è©å½™é‡ï¼Œé€™æ˜¯å°‡ç¾æœ‰è©å½™è¡¨çš„å…©å€‹å…ƒç´ åˆä½µç‚ºä¸€å€‹æ–°å…ƒç´ çš„è¦å‰‡ã€‚å› æ­¤åœ¨é–‹å§‹æ™‚ï¼Œé€™äº›åˆä½µå°‡å‰µå»ºå…·æœ‰å…©å€‹å­—ç¬¦çš„æ¨™è¨˜ï¼Œç„¶å¾Œéš¨è‘—è¨“ç·´çš„é€²è¡Œï¼Œæœƒå‰µå»ºæ›´é•·çš„å­è©ã€‚

åœ¨åˆ†è©å™¨è¨“ç·´æœŸé–“çš„ä»»ä½•ä¸€æ­¥ï¼ŒBPE ç®—æ³•éƒ½æœƒæœç´¢æœ€å¸¸è¦‹çš„ç¾æœ‰æ¨™è¨˜å° ("å°",é€™è£¡æˆ‘å€‘æŒ‡çš„æ˜¯å–®è©ä¸­çš„å…©å€‹é€£çºŒæ¨™è¨˜)ã€‚æœ€é »ç¹çš„ä¸€å°å°‡è¢«åˆä½µï¼Œæˆ‘å€‘æ²–æ´—ä¸¦é‡è¤‡ä¸‹ä¸€æ­¥ã€‚

å›åˆ°æˆ‘å€‘ä¹‹å‰çš„ä¾‹å­ï¼Œè®“æˆ‘å€‘å‡è¨­å–®è©å…·æœ‰ä»¥ä¸‹é »ç‡ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ„å‘³è‘— `"hug"` åœ¨èªæ–™åº«ä¸­å‡ºç¾äº†10æ¬¡, `"pug"` 5æ¬¡, `"pun"` 12æ¬¡, `"bun"` 4æ¬¡, ä»¥åŠ `"hugs"` 5æ¬¡ã€‚æˆ‘å€‘é€šéå°‡æ¯å€‹å–®è©æ‹†åˆ†ç‚ºå­—ç¬¦(å½¢æˆæˆ‘å€‘åˆå§‹è©å½™è¡¨çš„å­—ç¬¦)ä¾†é–‹å§‹è¨“ç·´,é€™æ¨£æˆ‘å€‘å°±å¯ä»¥å°‡æ¯å€‹å–®è©è¦–ç‚ºä¸€å€‹æ¨™è¨˜åˆ—è¡¨:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ç„¶å¾Œæˆ‘å€‘çœ‹æˆå°ã€‚é€™å° `("h", "u")` å‡ºç¾åœ¨å–®è© `"hug"` å’Œ `"hugs"`ä¸­,æ‰€ä»¥èªæ–™åº«ä¸­ç¸½å…±æœ‰15æ¬¡ã€‚ä¸é,é€™ä¸¦ä¸æ˜¯æœ€é »ç¹çš„ä¸€å°:é€™å€‹æ¦®è­½å±¬æ–¼ `("u", "g")`,å®ƒå‡ºç¾åœ¨ `"hug"`, `"pug"`, ä»¥åŠ `"hugs"`ä¸­,åœ¨è©å½™è¡¨ä¸­ç¸½å…± 20 æ¬¡ã€‚

å› æ­¤,æ¨™è¨˜å™¨å­¸ç¿’çš„ç¬¬ä¸€å€‹åˆä½µè¦å‰‡æ˜¯ `("u", "g") -> "ug"`,æ„æ€å°±æ˜¯ `"ug"` å°‡è¢«æ·»åŠ åˆ°è©å½™è¡¨ä¸­,ä¸¦ä¸”é€™å°æ‡‰è©²åˆä½µåˆ°èªæ–™åº«çš„æ‰€æœ‰å–®è©ä¸­ã€‚åœ¨é€™å€‹éšæ®µçµæŸæ™‚,è©å½™è¡¨å’Œèªæ–™åº«çœ‹èµ·ä¾†åƒé€™æ¨£:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ç¾åœ¨æˆ‘å€‘æœ‰ä¸€äº›å°è‡´æ¨™è¨˜é•·æ–¼å…©å€‹å­—ç¬¦çš„å°: ä¾‹å¦‚ `("h", "ug")`, åœ¨èªæ–™åº«ä¸­å‡ºç¾15æ¬¡ã€‚ç„¶è€Œ,é€™å€‹éšæ®µæœ€é »ç¹çš„å°æ˜¯ `("u", "n")`,åœ¨èªæ–™åº«ä¸­å‡ºç¾16æ¬¡,æ‰€ä»¥å­¸åˆ°çš„ç¬¬äºŒå€‹åˆä½µè¦å‰‡æ˜¯ `("u", "n") -> "un"`ã€‚å°‡å…¶æ·»åŠ åˆ°è©å½™è¡¨ä½µåˆä¸¦æ‰€æœ‰ç¾æœ‰çš„é€™å€‹å°,å°‡å‡ºç¾:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

ç¾åœ¨æœ€é »ç¹çš„ä¸€å°æ˜¯ `("h", "ug")`,æ‰€ä»¥æˆ‘å€‘å­¸ç¿’äº†åˆä½µè¦å‰‡ `("h", "ug") -> "hug"`,é€™çµ¦äº†æˆ‘å€‘ç¬¬ä¸€å€‹ä¸‰å€‹å­—æ¯çš„æ¨™è¨˜ã€‚åˆä½µå¾Œ,èªæ–™åº«å¦‚ä¸‹æ‰€ç¤º:

```
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

æˆ‘å€‘ç¹¼çºŒé€™æ¨£åˆä½µ,ç›´åˆ°é”åˆ°æˆ‘å€‘æ‰€éœ€çš„è©å½™é‡ã€‚

> [!TIP]
> âœï¸ **ç¾åœ¨è¼ªåˆ°ä½ äº†!**ä½ èªç‚ºä¸‹ä¸€å€‹åˆä½µè¦å‰‡æ˜¯ä»€éº¼ï¼Ÿ

## æ¨™è¨˜åŒ–ç®—æ³•

æ¨™è¨˜åŒ–ç·Šè·Ÿè¨“ç·´éç¨‹,å¾æŸç¨®æ„ç¾©ä¸Šèªª,é€šéæ‡‰ç”¨ä»¥ä¸‹æ­¥é©Ÿå°æ–°è¼¸å…¥é€²è¡Œæ¨™è¨˜:

1. è¦ç¯„åŒ–
2. é æ¨™è¨˜åŒ–
3. å°‡å–®è©æ‹†åˆ†ç‚ºå–®å€‹å­—ç¬¦
4. å°‡å­¸ç¿’çš„åˆä½µè¦å‰‡æŒ‰é †åºæ‡‰ç”¨æ–¼é€™äº›æ‹†åˆ†

è®“æˆ‘å€‘ä»¥æˆ‘å€‘åœ¨è¨“ç·´æœŸé–“ä½¿ç”¨çš„ç¤ºä¾‹ç‚ºä¾‹,å­¸ç¿’ä¸‰å€‹åˆä½µè¦å‰‡:

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

é€™å€‹å–®è© `"bug"` å°‡è¢«æ¨™è¨˜ç‚º `["b", "ug"]`ã€‚ç„¶è€Œ `"mug"`,å°‡è¢«æ¨™è¨˜ç‚º `["[UNK]", "ug"]`,å› ç‚ºå­—æ¯ `"m"` ä¸å†åŸºæœ¬è©å½™è¡¨ä¸­ã€‚åŒæ¨£,å–®è©`"thug"` æœƒè¢«æ¨™è¨˜ç‚º `["[UNK]", "hug"]`: å­—æ¯ `"t"` ä¸åœ¨åŸºæœ¬è©å½™è¡¨ä¸­,æ‡‰ç”¨åˆä½µè¦å‰‡é¦–å…ˆå°è‡´ `"u"` å’Œ `"g"` è¢«åˆä½µ,ç„¶å¾Œæ˜¯ `"hu"` å’Œ `"g"` è¢«åˆä½µã€‚

> [!TIP]
> âœï¸ **ç¾åœ¨è¼ªåˆ°ä½ äº†!** ä½ èªç‚ºé€™å€‹è© `"unhug"` å°‡å¦‚ä½•è¢«æ¨™è¨˜ï¼Ÿ

## å¯¦ç¾ BPE

ç¾åœ¨è®“æˆ‘å€‘çœ‹ä¸€ä¸‹ BPE ç®—æ³•çš„å¯¦ç¾ã€‚é€™ä¸æœƒæ˜¯ä½ å¯ä»¥åœ¨å¤§å‹èªæ–™åº«ä¸Šå¯¦éš›ä½¿ç”¨çš„å„ªåŒ–ç‰ˆæœ¬;æˆ‘å€‘åªæ˜¯æƒ³å‘ä½ å±•ç¤ºä»£ç¢¼,ä»¥ä¾¿ä½ å¯ä»¥æ›´å¥½åœ°ç†è§£ç®—æ³•

é¦–å…ˆæˆ‘å€‘éœ€è¦ä¸€å€‹èªæ–™åº«,æ‰€ä»¥è®“æˆ‘å€‘ç”¨å¹¾å¥è©±å‰µå»ºä¸€å€‹ç°¡å–®çš„èªæ–™åº«:

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

æ¥ä¸‹ä¾†,æˆ‘å€‘éœ€è¦å°‡è©²èªæ–™åº«é å…ˆæ¨™è¨˜ç‚ºå–®è©ã€‚ç”±æ–¼æˆ‘å€‘æ­£åœ¨è¤‡è£½ BPE æ¨™è¨˜å™¨(å¦‚ GPT-2),æˆ‘å€‘å°‡ä½¿ç”¨ `gpt2` æ¨™è¨˜å™¨ä½œç‚ºé æ¨™è¨˜åŒ–çš„æ¨™è¨˜å™¨:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ç„¶å¾Œæˆ‘å€‘åœ¨é€²è¡Œé æ¨™è¨˜åŒ–æ™‚è¨ˆç®—èªæ–™åº«ä¸­æ¯å€‹å–®è©çš„é »ç‡:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

ä¸‹ä¸€æ­¥æ˜¯è¨ˆç®—åŸºæœ¬è©å½™,ç”±èªæ–™åº«ä¸­ä½¿ç”¨çš„æ‰€æœ‰å­—ç¬¦çµ„æˆ:

```python
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

æˆ‘å€‘é‚„åœ¨è©²è©å½™è¡¨çš„é–‹é ­æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ¨™è¨˜ã€‚å°æ–¼GPT-2,å”¯ä¸€çš„ç‰¹æ®Šæ¨™è¨˜æ˜¯ `"<|endoftext|>"`:

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

æˆ‘å€‘ç¾åœ¨éœ€è¦å°‡æ¯å€‹å–®è©æ‹†åˆ†ç‚ºå–®ç¨çš„å­—ç¬¦,ä»¥ä¾¿èƒ½å¤ é–‹å§‹è¨“ç·´:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ç¾åœ¨æˆ‘å€‘å·²æº–å‚™å¥½é€²è¡Œè¨“ç·´,è®“æˆ‘å€‘ç·¨å¯«ä¸€å€‹å‡½æ•¸ä¾†è¨ˆç®—æ¯å°çš„é »ç‡ã€‚æˆ‘å€‘éœ€è¦åœ¨è¨“ç·´çš„æ¯å€‹æ­¥é©Ÿä¸­ä½¿ç”¨å®ƒ:

```python
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

è®“æˆ‘å€‘ä¾†çœ‹çœ‹é€™å€‹å­—å…¸åœ¨åˆå§‹æ‹†åˆ†å¾Œçš„ä¸€éƒ¨åˆ†:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

ç¾åœ¨, æ‰¾åˆ°æœ€é »ç¹çš„å°åªéœ€è¦ä¸€å€‹å¿«é€Ÿçš„å¾ªç’°:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ä ', 't') 7
```

æ‰€ä»¥ç¬¬ä¸€å€‹è¦å­¸ç¿’çš„åˆä½µæ˜¯ `('Ä ', 't') -> 'Ä t'`, æˆ‘å€‘æ·»åŠ  `'Ä t'` åˆ°è©å½™è¡¨:

```python
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

è¦ç¹¼çºŒæ¥ä¸‹ä¾†çš„æ­¥é©Ÿ,æˆ‘å€‘éœ€è¦åœ¨æˆ‘å€‘çš„`åˆ†è©`å­—å…¸ä¸­æ‡‰ç”¨è©²åˆä½µã€‚è®“æˆ‘å€‘ç‚ºæ­¤ç·¨å¯«å¦ä¸€å€‹å‡½æ•¸:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘å€‘å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆä½µçš„çµæœ:

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```python out
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

ç¾åœ¨æˆ‘å€‘æœ‰äº†å¾ªç’°æ‰€éœ€çš„ä¸€åˆ‡,ç›´åˆ°æˆ‘å€‘å­¸æœƒäº†æˆ‘å€‘æƒ³è¦çš„æ‰€æœ‰åˆä½µã€‚æˆ‘å€‘çš„ç›®æ¨™æ˜¯è©å½™é‡é”åˆ°50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

çµæœ,æˆ‘å€‘å­¸ç¿’äº† 19 æ¢åˆä½µè¦å‰‡(åˆå§‹è©å½™è¡¨çš„å¤§å° 31 -- 30 å­—æ¯å­—ç¬¦,åŠ ä¸Šç‰¹æ®Šæ¨™è¨˜):

```py
print(merges)
```

```python out
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

è©å½™è¡¨ç”±ç‰¹æ®Šæ¨™è¨˜ã€åˆå§‹å­—æ¯å’Œæ‰€æœ‰åˆä½µçµæœçµ„æˆ:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

> [!TIP]
> ğŸ’¡ åœ¨åŒä¸€èªæ–™åº«ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸æœƒç”¢ç”Ÿå®Œå…¨ç›¸åŒçš„è©å½™è¡¨ã€‚é€™æ˜¯å› ç‚ºç•¶æœ‰æœ€é »ç¹å°çš„é¸æ“‡æ™‚,æˆ‘å€‘é¸æ“‡é‡åˆ°çš„ç¬¬ä¸€å€‹, è€Œ ğŸ¤— Tokenizers åº«æ ¹æ“šå…§éƒ¨IDé¸æ“‡ç¬¬ä¸€å€‹ã€‚

ç‚ºäº†å°æ–°æ–‡æœ¬é€²è¡Œåˆ†è©,æˆ‘å€‘å°å…¶é€²è¡Œé åˆ†è©ã€æ‹†åˆ†ï¼Œç„¶å¾Œæ‡‰ç”¨å­¸åˆ°çš„æ‰€æœ‰åˆä½µè¦å‰‡:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

æˆ‘å€‘å¯ä»¥åœ¨ä»»ä½•ç”±å­—æ¯è¡¨ä¸­çš„å­—ç¬¦çµ„æˆçš„æ–‡æœ¬ä¸Šå˜—è©¦é€™å€‹:

```py
tokenize("This is not a token.")
```

```python out
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

> [!WARNING]
> âš ï¸ å¦‚æœå­˜åœ¨æœªçŸ¥å­—ç¬¦,æˆ‘å€‘çš„å¯¦ç¾å°‡æ‹‹å‡ºéŒ¯èª¤,å› ç‚ºæˆ‘å€‘æ²’æœ‰åšä»»ä½•è™•ç†å®ƒå€‘ã€‚GPT-2 å¯¦éš›ä¸Šæ²’æœ‰æœªçŸ¥æ¨™è¨˜(ä½¿ç”¨å­—ç¯€ç´š BPE æ™‚ä¸å¯èƒ½å¾—åˆ°æœªçŸ¥å­—ç¬¦),ä½†é€™å¯èƒ½ç™¼ç”Ÿåœ¨é€™è£¡,å› ç‚ºæˆ‘å€‘æ²’æœ‰åœ¨åˆå§‹è©å½™è¡¨ä¸­åŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—ç¯€ã€‚ BPE çš„é€™æ–¹é¢è¶…å‡ºäº†æœ¬ç¯€çš„ç¯„åœ,å› æ­¤æˆ‘å€‘å¿½ç•¥äº†ç´°ç¯€ã€‚

é€™å°±æ˜¯ BPE ç®—æ³•ï¼æ¥ä¸‹ä¾†,æˆ‘å€‘å°‡çœ‹çœ‹ WordPieceã€‚