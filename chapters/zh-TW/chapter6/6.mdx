# WordPiece æ¨™è¨˜åŒ–  

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section6.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section6.ipynb"},
]} />

WordPiece æ˜¯ Google ç‚ºé è¨“ç·´ BERT è€Œé–‹ç™¼çš„æ¨™è¨˜åŒ–ç®—æ³•ã€‚æ­¤å¾Œ,å®ƒåœ¨ä¸å°‘åŸºæ–¼ BERT çš„ Transformer æ¨¡å‹ä¸­å¾—åˆ°é‡ç”¨,ä¾‹å¦‚ DistilBERTã€MobileBERTã€Funnel Transformers å’Œ MPNETã€‚å®ƒåœ¨è¨“ç·´æ–¹é¢èˆ‡ BPE éå¸¸ç›¸ä¼¼,ä½†å¯¦éš›æ¨™è¨˜åŒ–çš„æ–¹å¼ä¸åŒã€‚

<Youtube id="qpv6ms_t_1A"/>

> [!TIP]
> ğŸ’¡ æœ¬ç¯€æ·±å…¥ä»‹ç´¹ WordPiece,ç”šè‡³å±•ç¤ºå®Œæ•´çš„å¯¦ç¾ã€‚å¦‚æœæ‚¨åªæƒ³å¤§è‡´ç­è§£æ¨™è¨˜åŒ–ç®—æ³•,å¯ä»¥è·³åˆ°æœ€å¾Œã€‚

## è¨“ç·´ç®—æ³•

> [!WARNING]
> âš ï¸ Google å¾æœªé–‹æº WordPiece è¨“ç·´ç®—æ³•çš„å¯¦ç¾,å› æ­¤ä»¥ä¸‹æ˜¯æˆ‘å€‘åŸºæ–¼å·²ç™¼è¡¨æ–‡ç»çš„æœ€ä½³çŒœæ¸¬ã€‚å®ƒå¯èƒ½ä¸æ˜¯ 100% æº–ç¢ºçš„ã€‚

èˆ‡ BPE ä¸€æ¨£,WordPiece å¾ä¸€å€‹å°è©å½™è¡¨é–‹å§‹,åŒ…æ‹¬æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ¨™è¨˜å’Œåˆå§‹å­—æ¯è¡¨ã€‚å› ç‚ºå®ƒé€šéæ·»åŠ å‰ç¶´ä¾†è­˜åˆ¥å­è© (å¦‚åŒ `##` å°æ–¼ BERT),æ¯å€‹å–®è©æœ€åˆæ˜¯é€šéå°‡è©²å‰ç¶´æ·»åŠ åˆ°å–®è©å…§çš„æ‰€æœ‰å­—ç¬¦ä¾†æ‹†åˆ†çš„ã€‚æ‰€ä»¥,ä¾‹å¦‚ `"word"` ,åƒé€™æ¨£æ‹†åˆ†:

```
w ##o ##r ##d
```

å› æ­¤,åˆå§‹å­—æ¯è¡¨åŒ…å«å‡ºç¾åœ¨å–®è©é–‹é ­çš„æ‰€æœ‰å­—ç¬¦ä»¥åŠå‡ºç¾åœ¨å–®è©å…§éƒ¨çš„ä»¥ WordPiece å‰ç¶´é–‹é ­çš„å­—ç¬¦ã€‚

ç„¶å¾Œ,å†æ¬¡åƒ BPE ä¸€æ¨£,WordPiece å­¸ç¿’åˆä½µè¦å‰‡ã€‚ä¸»è¦å€åˆ¥åœ¨æ–¼é¸æ“‡è¦åˆä½µçš„å°çš„æ–¹å¼ã€‚WordPiece ä¸æ˜¯é¸æ“‡æœ€é »ç¹çš„å°ï¼Œè€Œæ˜¯ä½¿ç”¨ä»¥ä¸‹å…¬å¼è¨ˆç®—æ¯å°çš„åˆ†æ•¸:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

é€šéå°‡é…å°çš„é »ç‡é™¤ä»¥å…¶æ¯å€‹éƒ¨åˆ†çš„é »ç‡çš„ä¹˜ç©, è©²ç®—æ³•å„ªå…ˆåˆä½µå–®å€‹éƒ¨åˆ†åœ¨è©å½™è¡¨ä¸­é »ç‡è¼ƒä½çš„å°ã€‚ä¾‹å¦‚,å®ƒä¸ä¸€å®šæœƒåˆä¸¦ `("un", "##able")` å³ä½¿é€™å°åœ¨è©å½™è¡¨ä¸­å‡ºç¾çš„é »ç‡å¾ˆé«˜,å› ç‚º `"un"` å’Œ `"##able"` å¾ˆå¯èƒ½æ¯å€‹è©éƒ½å‡ºç¾åœ¨å¾ˆå¤šå…¶ä»–è©ä¸­ä¸¦ä¸”å‡ºç¾é »ç‡å¾ˆé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹,åƒ `("hu", "##gging")` å¯èƒ½æœƒæ›´å¿«åœ°åˆä½µ (å‡è¨­ "hugging" ç¶“å¸¸å‡ºç¾åœ¨è©å½™è¡¨ä¸­),å› ç‚º `"hu"` å’Œ `"##gging"` é€™å…©å€‹è©å–®ç¨å‡ºç¾åœ°é »ç‡å¯èƒ½è¼ƒä½ã€‚

è®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘åœ¨ BPE è¨“ç·´ç¤ºä¾‹ä¸­ä½¿ç”¨çš„ç›¸åŒè©å½™:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

é€™è£¡çš„æ‹†åˆ†å°‡æ˜¯:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

æ‰€ä»¥æœ€åˆçš„è©å½™å°‡æ˜¯ `["b", "h", "p", "##g", "##n", "##s", "##u"]` (å¦‚æœæˆ‘å€‘æš«æ™‚å¿˜è¨˜ç‰¹æ®Šæ¨™è¨˜)ã€‚æœ€é »ç¹çš„ä¸€å°æ˜¯ `("##u", "##g")` (ç›®å‰20æ¬¡),ä½† `"##u"` å–®ç¨å‡ºç¾çš„é »ç‡éå¸¸é«˜,æ‰€ä»¥å®ƒçš„åˆ†æ•¸ä¸æ˜¯æœ€é«˜çš„(å®ƒæ˜¯ 1 / 36)ã€‚æ‰€æœ‰å¸¶æœ‰ `"##u"` çš„å°å¯¦éš›ä¸Šéƒ½æœ‰ç›¸åŒçš„åˆ†æ•¸(1 / 36),æ‰€ä»¥åˆ†æ•¸æœ€é«˜çš„å°æ˜¯ `("##g", "##s")` -- å”¯ä¸€æ²’æœ‰ `"##u"` çš„å°-- 1 / 20,æ‰€ä»¥å­¸ç¿’çš„ç¬¬ä¸€å€‹åˆä½µæ˜¯ `("##g", "##s") -> ("##gs")`ã€‚

è«‹æ³¨æ„,ç•¶æˆ‘å€‘åˆä½µæ™‚,æˆ‘å€‘åˆªé™¤äº†å…©å€‹æ¨™è¨˜ä¹‹é–“çš„ `##`,æ‰€ä»¥æˆ‘å€‘æ·»åŠ  `"##gs"` åˆ°è©å½™è¡¨ä¸­,ä¸¦åœ¨èªæ–™åº«çš„å–®è©ä¸­æ‡‰ç”¨è©²åˆä½µ:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

åœ¨é€™ä¸€é»ä¸­, `"##u"` æ˜¯åœ¨æ‰€æœ‰å¯èƒ½çš„å°ä¸­,å› æ­¤å®ƒå€‘æœ€çµ‚éƒ½å…·æœ‰ç›¸åŒçš„åˆ†æ•¸ã€‚å‡è¨­åœ¨é€™ç¨®æƒ…æ³ä¸‹,ç¬¬ä¸€å°è¢«åˆä½µ, `("h", "##u") -> "hu"`ã€‚é€™ä½¿å¾—æˆ‘å€‘:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ç„¶å¾Œä¸‹ä¸€å€‹æœ€é«˜çš„åˆ†æ•¸ç”± `("hu", "##g")` å’Œ `("hu", "##gs")` å…±äº«(1/15,èˆ‡å…¶ä»–æ‰€æœ‰å°çš„ 1/21 ç›¸æ¯”),å› æ­¤åˆä½µå¾—åˆ†æœ€é«˜çš„ç¬¬ä¸€å°:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

æˆ‘å€‘ç¹¼çºŒé€™æ¨£è™•ç†,ç›´åˆ°é”åˆ°æˆ‘å€‘æ‰€éœ€çš„è©å½™é‡ã€‚

> [!TIP]
> âœï¸ **ç¾åœ¨è¼ªåˆ°ä½ äº†!** ä¸‹ä¸€å€‹åˆä½µè¦å‰‡æ˜¯ä»€éº¼ï¼Ÿ

## æ¨™è¨˜åŒ–ç®—æ³•

WordPiece å’Œ BPE ä¸­çš„æ¨™è¨˜åŒ–çš„ä¸åŒåœ¨æ–¼ WordPiece åªä¿å­˜æœ€çµ‚è©å½™,è€Œä¸æ˜¯å­¸ç¿’çš„åˆä½µè¦å‰‡ã€‚å¾è¦æ¨™è¨˜çš„å–®è©é–‹å§‹,WordPiece æ‰¾åˆ°è©å½™è¡¨ä¸­æœ€é•·çš„å­è©,ç„¶å¾Œå°å…¶é€²è¡Œæ‹†åˆ†ã€‚ä¾‹å¦‚,å¦‚æœæˆ‘å€‘ä½¿ç”¨ä¸Šé¢ä¾‹å­ä¸­å­¸åˆ°çš„è©å½™,å°æ–¼å–®è© `"hugs"`,è©å½™è¡¨ä¸­å¾é ­é–‹å§‹çš„æœ€é•·å­è©æ˜¯ `"hug"`,æ‰€ä»¥æˆ‘å€‘åœ¨é‚£è£¡æ‹†åˆ†ä¸¦å¾—åˆ° `["hug", "##s"]`ã€‚ ç„¶å¾Œæˆ‘å€‘ç¹¼çºŒä½¿ç”¨è©å½™è¡¨ä¸­çš„ `"##s"`,å› æ­¤ `"hugs"` çš„æ¨™è¨˜åŒ–æ˜¯ `["hug", "##s"]`.

ä½¿ç”¨ BPE, æˆ‘å€‘å°‡æŒ‰é †åºæ‡‰ç”¨å­¸ç¿’åˆ°çš„åˆä½µä¸¦å°‡å…¶æ¨™è¨˜ç‚º `["hu", "##gs"]`,æ‰€ä»¥ç·¨ç¢¼ä¸åŒã€‚

å†èˆ‰ä¸€å€‹ä¾‹å­,è®“æˆ‘å€‘çœ‹çœ‹ `"bugs"` å°‡å¦‚ä½•è¢«æ¨™è¨˜åŒ–ã€‚ `"b"` æ˜¯å¾è©å½™è¡¨ä¸­å–®è©é–‹é ­é–‹å§‹çš„æœ€é•·å­è©,æ‰€ä»¥æˆ‘å€‘åœ¨é‚£è£¡æ‹†åˆ†ä¸¦å¾—åˆ° `["b", "##ugs"]`ã€‚ç„¶å¾Œ `"##u"` æ˜¯è©å½™è¡¨ä¸­å¾ `"##ugs"` é–‹å§‹çš„æœ€é•·çš„å­è©,æ‰€ä»¥æˆ‘å€‘åœ¨é‚£è£¡æ‹†åˆ†ä¸¦å¾—åˆ° `["b", "##u, "##gs"]`ã€‚æœ€å¾Œ, `"##gs"` åœ¨è©å½™è¡¨ä¸­,æ‰€ä»¥æœ€å¾Œä¸€å€‹åˆ—è¡¨æ˜¯ `"bugs"` çš„æ¨™è¨˜åŒ–ã€‚

ç•¶åˆ†è©é”åˆ°ç„¡æ³•åœ¨è©å½™è¡¨ä¸­æ‰¾åˆ°å­è©çš„éšæ®µæ™‚, æ•´å€‹è©è¢«æ¨™è¨˜ç‚ºæœªçŸ¥ -- ä¾‹å¦‚, `"mug"` å°‡è¢«æ¨™è¨˜ç‚º `["[UNK]"]`,å°±åƒ `"bum"` (å³ä½¿æˆ‘å€‘å¯ä»¥ä»¥ `"b"` å’Œ `"##u"` é–‹å§‹, `"##m"` ä¸åœ¨è©å½™è¡¨ä¸­,ç”±æ­¤ç”¢ç”Ÿçš„æ¨™è¨˜å°‡åªæ˜¯ `["[UNK]"]`, ä¸æ˜¯ `["b", "##u", "[UNK]"]`)ã€‚é€™æ˜¯èˆ‡ BPE çš„å¦ä¸€å€‹å€åˆ¥,BPE åªæœƒå°‡ä¸åœ¨è©å½™è¡¨ä¸­çš„å–®å€‹å­—ç¬¦åˆ†é¡ç‚ºæœªçŸ¥ã€‚

> [!TIP]
> âœï¸ **ç¾åœ¨è¼ªåˆ°ä½ äº†!** `"pugs"` å°‡è¢«å¦‚ä½•æ¨™è¨˜?

## å¯¦ç¾ WordPiece

ç¾åœ¨è®“æˆ‘å€‘çœ‹ä¸€ä¸‹ WordPiece ç®—æ³•çš„å¯¦ç¾ã€‚èˆ‡ BPE ä¸€æ¨£,é€™åªæ˜¯æ•™å­¸,ä½ å°‡ç„¡æ³•åœ¨å¤§å‹èªæ–™åº«ä¸­ä½¿ç”¨å®ƒã€‚

æˆ‘å€‘å°‡ä½¿ç”¨èˆ‡ BPE ç¤ºä¾‹ä¸­ç›¸åŒçš„èªæ–™åº«:

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é¦–å…ˆ,æˆ‘å€‘éœ€è¦å°‡èªæ–™åº«é å…ˆæ¨™è¨˜ç‚ºå–®è©ã€‚ç”±æ–¼æˆ‘å€‘æ­£åœ¨è¤‡è£½ WordPiece æ¨™è¨˜å™¨ (å¦‚ BERT),å› æ­¤æˆ‘å€‘å°‡ä½¿ç”¨ `bert-base-cased` æ¨™è¨˜å™¨ç”¨æ–¼é æ¨™è¨˜åŒ–:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ç„¶å¾Œæˆ‘å€‘åœ¨é€²è¡Œé æ¨™è¨˜åŒ–æ™‚è¨ˆç®—èªæ–™åº«ä¸­æ¯å€‹å–®è©çš„é »ç‡:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

æ­£å¦‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„,å­—æ¯è¡¨æ˜¯ç”±å–®è©çš„æ‰€æœ‰ç¬¬ä¸€å€‹å­—æ¯çµ„æˆçš„å”¯ä¸€é›†åˆ,ä»¥åŠå‡ºç¾åœ¨å‰ç¶´ç‚º `##` çš„å…¶ä»–å­—æ¯:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

æˆ‘å€‘é‚„åœ¨è©²è©å½™è¡¨çš„é–‹é ­æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ¨™è¨˜ã€‚åœ¨ä½¿ç”¨ BERT çš„æƒ…æ³ä¸‹,å®ƒæ˜¯åˆ—è¡¨ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

æ¥ä¸‹ä¾†æˆ‘å€‘éœ€è¦æ‹†åˆ†æ¯å€‹å–®è©, æ‰€æœ‰ä¸æ˜¯ç¬¬ä¸€å€‹å­—æ¯çš„å­—æ¯éƒ½ä»¥ `##` ç‚ºå‰ç¶´:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

ç¾åœ¨æˆ‘å€‘å·²ç¶“æº–å‚™å¥½è¨“ç·´äº†,è®“æˆ‘å€‘ç·¨å¯«ä¸€å€‹å‡½æ•¸ä¾†è¨ˆç®—æ¯å°çš„åˆ†æ•¸ã€‚æˆ‘å€‘éœ€è¦åœ¨è¨“ç·´çš„æ¯å€‹æ­¥é©Ÿä¸­ä½¿ç”¨å®ƒ:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

è®“æˆ‘å€‘ä¾†çœ‹çœ‹é€™å€‹å­—å…¸åœ¨åˆå§‹æ‹†åˆ†å¾Œçš„ä¸€éƒ¨åˆ†:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

ç¾åœ¨,æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„å°åªéœ€è¦ä¸€å€‹å¿«é€Ÿå¾ªç’°:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

æ‰€ä»¥ç¬¬ä¸€å€‹è¦å­¸ç¿’çš„åˆä½µæ˜¯ `('a', '##b') -> 'ab'`, ä¸¦ä¸”æˆ‘å€‘æ·»åŠ  `'ab'` åˆ°è©å½™è¡¨ä¸­:

```python
vocab.append("ab")
```

è¦ç¹¼çºŒæ¥ä¸‹ä¾†çš„æ­¥é©Ÿ,æˆ‘å€‘éœ€è¦åœ¨æˆ‘å€‘çš„ `æ‹†åˆ†` å­—å…¸ä¸­æ‡‰ç”¨è©²åˆä½µã€‚è®“æˆ‘å€‘ç‚ºæ­¤ç·¨å¯«å¦ä¸€å€‹å‡½æ•¸:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘å€‘å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆä½µçš„çµæœ:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

ç¾åœ¨æˆ‘å€‘æœ‰äº†å¾ªç’°æ‰€éœ€çš„ä¸€åˆ‡,ç›´åˆ°æˆ‘å€‘å­¸æœƒäº†æˆ‘å€‘æƒ³è¦çš„æ‰€æœ‰åˆä½µã€‚æˆ‘å€‘çš„ç›®æ¨™è©å½™é‡ç‚º70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„è©å½™è¡¨:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

æ­£å¦‚æˆ‘å€‘æ‰€çœ‹åˆ°çš„,èˆ‡ BPE ç›¸æ¯”,é€™å€‹æ¨™è¨˜å™¨å°‡å–®è©çš„ä¸€éƒ¨åˆ†ä½œç‚ºæ¨™è¨˜å­¸ç¿’å¾—æ›´å¿«ä¸€äº›ã€‚

> [!TIP]
> ğŸ’¡ åœ¨åŒä¸€èªæ–™åº«ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸æœƒç”¢ç”Ÿå®Œå…¨ç›¸åŒçš„è©å½™è¡¨ã€‚é€™æ˜¯å› ç‚º ğŸ¤— Tokenizers åº«æ²’æœ‰ç‚ºè¨“ç·´å¯¦ç¾ WordPiece(å› ç‚ºæˆ‘å€‘ä¸å®Œå…¨ç¢ºå®šå®ƒçš„å…§éƒ¨çµæ§‹),è€Œæ˜¯ä½¿ç”¨ BPEã€‚

ç‚ºäº†å°æ–°æ–‡æœ¬é€²è¡Œåˆ†è©,æˆ‘å€‘å°å…¶é€²è¡Œé åˆ†è©ã€æ‹†åˆ†,ç„¶å¾Œå°æ¯å€‹å–®è©æ‡‰ç”¨åˆ†è©ç®—æ³•ã€‚ä¹Ÿå°±æ˜¯èªª,æˆ‘å€‘å¾ç¬¬ä¸€å€‹è©çš„é–‹é ­å°‹æ‰¾æœ€å¤§çš„å­è©ä¸¦å°‡å…¶æ‹†åˆ†,ç„¶å¾Œæˆ‘å€‘åœ¨ç¬¬äºŒéƒ¨åˆ†é‡è¤‡é€™å€‹éç¨‹,å°æ–¼è©²è©çš„å…¶é¤˜éƒ¨åˆ†å’Œæ–‡æœ¬ä¸­çš„ä»¥ä¸‹è©,ä¾æ­¤é¡æ¨:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

è®“æˆ‘å€‘ç”¨è©å½™è¡¨ä¸­çš„ä¸€å€‹å–®è©å’Œå¦ä¸€å€‹ä¸åœ¨è©å½™è¡¨ä¸­çš„å–®è©é€²è¡Œæ¸¬è©¦:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

ç¾åœ¨,è®“æˆ‘å€‘ç·¨å¯«ä¸€å€‹æ¨™è¨˜æ–‡æœ¬çš„å‡½æ•¸:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

æˆ‘å€‘å¯ä»¥åœ¨ä»»ä½•æ–‡æœ¬ä¸Šå˜—è©¦:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

é€™å°±æ˜¯ WordPiece ç®—æ³•çš„å…¨éƒ¨å…§å®¹!ç¾åœ¨è®“æˆ‘å€‘ä¾†çœ‹çœ‹ Unigramã€‚
