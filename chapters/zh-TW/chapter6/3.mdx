<FrameworkSwitchCourse {fw} />

# å¿«é€Ÿæ¨™è¨˜å™¨çš„ç‰¹æ®Šèƒ½åŠ›

{#if fw === 'pt'}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section3_tf.ipynb"},
]} />

{/if}

åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡ä»”ç´°ç ”ç©¶ ğŸ¤— Transformers ä¸­æ¨™è¨˜å™¨çš„åŠŸèƒ½ã€‚åˆ°ç›®å‰ç‚ºæ­¢ï¼Œæˆ‘å€‘åªä½¿ç”¨å®ƒå€‘ä¾†æ¨™è¨˜è¼¸å…¥æˆ–å°‡ ID è§£ç¢¼è¿´æ–‡æœ¬ï¼Œä½†æ˜¯æ¨™è¨˜å™¨â€”â€”å°¤å…¶æ˜¯é‚£äº›ç”± ğŸ¤— Tokenizers åº«æ”¯æŒçš„â€”â€”å¯ä»¥åšæ›´å¤šçš„äº‹æƒ…ã€‚ç‚ºäº†èªªæ˜é€™äº›é™„åŠ åŠŸèƒ½ï¼Œæˆ‘å€‘å°‡æ¢ç´¢å¦‚ä½•é‡ç¾çµæœ **token-classification** ï¼ˆæˆ‘å€‘ç¨±ä¹‹ç‚º **ner** ï¼‰ å’Œ **question-answering** æˆ‘å€‘ç¬¬ä¸€æ¬¡åœ¨[Chapter 1](/course/chapter1)ä¸­é‡åˆ°çš„ç®¡é“.

<Youtube id="g8quOxoqhHQ"/>

åœ¨æ¥ä¸‹ä¾†çš„è¨è«–ä¸­ï¼Œæˆ‘å€‘æœƒç¶“å¸¸å€åˆ†â€œæ…¢â€å’Œâ€œå¿«â€åˆ†è©å™¨ã€‚æ…¢é€Ÿåˆ†è©å™¨æ˜¯åœ¨ ğŸ¤— Transformers åº«ä¸­ç”¨ Python ç·¨å¯«çš„ï¼Œè€Œå¿«é€Ÿç‰ˆæœ¬æ˜¯ç”± ğŸ¤— åˆ†è©å™¨æä¾›çš„ï¼Œå®ƒå€‘æ˜¯ç”¨ Rust ç·¨å¯«çš„ã€‚å¦‚æœä½ é‚„è¨˜å¾—åœ¨[Chapter 5](/course/chapter5/3)ä¸­å ±å‘Šäº†å¿«é€Ÿå’Œæ…¢é€Ÿåˆ†è©å™¨å°è—¥ç‰©å¯©æŸ¥æ•¸æ“šé›†é€²è¡Œåˆ†è©æ‰€éœ€çš„æ™‚é–“çš„é€™å¼µè¡¨ï¼Œæ‚¨æ‡‰è©²çŸ¥é“ç‚ºä»€éº¼æˆ‘å€‘ç¨±å®ƒå€‘ç‚ºâ€œå¿«â€å’Œâ€œæ…¢â€ï¼š

                | Fast tokenizer | Slow tokenizer
:--------------:|:--------------:|:-------------:
`batched=True`  | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s

<Tip warning={true}>

âš ï¸ å°å–®å€‹å¥å­é€²è¡Œåˆ†è©æ™‚ï¼Œæ‚¨ä¸æœƒç¸½æ˜¯çœ‹åˆ°ç›¸åŒåˆ†è©å™¨çš„æ…¢é€Ÿå’Œå¿«é€Ÿç‰ˆæœ¬ä¹‹é–“çš„é€Ÿåº¦å·®ç•°ã€‚äº‹å¯¦ä¸Šï¼Œå¿«é€Ÿç‰ˆæœ¬å¯¦éš›ä¸Šå¯èƒ½æ›´æ…¢ï¼åªæœ‰åŒæ™‚å°å¤§é‡æ–‡æœ¬é€²è¡Œæ¨™è¨˜æ™‚ï¼Œæ‚¨æ‰èƒ½æ¸…æ¥šåœ°çœ‹åˆ°å·®ç•°ã€‚

</Tip>

## æ‰¹é‡ç·¨ç¢¼

<Youtube id="3umI3tm27Vw"/>

åˆ†è©å™¨çš„è¼¸å‡ºä¸æ˜¯ç°¡å–®çš„ Python å­—å…¸ï¼›æˆ‘å€‘å¾—åˆ°çš„å¯¦éš›ä¸Šæ˜¯ä¸€å€‹ç‰¹æ®Šçš„ **BatchEncoding** ç›®çš„ã€‚å®ƒæ˜¯å­—å…¸çš„å­é¡ï¼ˆé€™å°±æ˜¯ç‚ºä»€éº¼æˆ‘å€‘ä¹‹å‰èƒ½å¤ æ¯«ç„¡å•é¡Œåœ°ç´¢å¼•åˆ°è©²çµæœä¸­çš„åŸå› ï¼‰ï¼Œä½†å…·æœ‰ä¸»è¦ç”±å¿«é€Ÿæ¨™è¨˜å™¨ä½¿ç”¨çš„é™„åŠ æ–¹æ³•ã€‚

é™¤äº†å®ƒå€‘çš„ä¸¦è¡ŒåŒ–èƒ½åŠ›ä¹‹å¤–ï¼Œå¿«é€Ÿæ¨™è¨˜å™¨çš„é—œéµåŠŸèƒ½æ˜¯å®ƒå€‘å§‹çµ‚è·Ÿè¹¤æœ€çµ‚æ¨™è¨˜ä¾†è‡ªçš„åŸå§‹æ–‡æœ¬ç¯„åœâ€”â€”æˆ‘å€‘ç¨±ä¹‹ç‚ºåç§»æ˜ å°„.é€™åéä¾†åˆè§£é–äº†è«¸å¦‚å°‡æ¯å€‹å–®è©æ˜ å°„åˆ°å®ƒç”Ÿæˆçš„æ¨™è¨˜æˆ–å°‡åŸå§‹æ–‡æœ¬çš„æ¯å€‹å­—ç¬¦æ˜ å°„åˆ°å®ƒå…§éƒ¨çš„æ¨™è¨˜ç­‰åŠŸèƒ½ï¼Œåä¹‹äº¦ç„¶ã€‚è®“æˆ‘å€‘çœ‹ä¸€å€‹ä¾‹å­ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘å€‘å¾—åˆ°ä¸€å€‹ **BatchEncoding** æ¨™è¨˜å™¨è¼¸å‡ºä¸­çš„å°è±¡ï¼š

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

ç”±æ–¼ **AutoTokenizer** é¡é»˜èªé¸æ“‡å¿«é€Ÿæ¨™è¨˜å™¨ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨é™„åŠ æ–¹æ³• this **BatchEncoding** å°è±¡æä¾›ã€‚æˆ‘å€‘æœ‰å…©ç¨®æ–¹æ³•ä¾†æª¢æŸ¥æˆ‘å€‘çš„åˆ†è©å™¨æ˜¯å¿«çš„é‚„æ˜¯æ…¢çš„ã€‚æˆ‘å€‘å¯ä»¥æª¢æŸ¥ **is_fast** çš„å±¬æ€§ **tokenizer** ï¼š

```python
tokenizer.is_fast
```

```python out
True
```

æˆ–æª¢æŸ¥æˆ‘å€‘çš„ç›¸åŒå±¬æ€§ **encoding** ï¼š

```python
encoding.is_fast
```

```python out
True
```

è®“æˆ‘å€‘çœ‹çœ‹å¿«é€Ÿæ¨™è¨˜å™¨ä½¿æˆ‘å€‘èƒ½å¤ åšä»€éº¼ã€‚é¦–å…ˆï¼Œæˆ‘å€‘å¯ä»¥è¨ªå•ä»¤ç‰Œè€Œç„¡éœ€å°‡ ID è½‰æ›å›ä»¤ç‰Œï¼š

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œç´¢å¼• 5 è™•çš„ä»¤ç‰Œæ˜¯ **##yl** ï¼Œå®ƒæ˜¯åŸå§‹å¥å­ä¸­â€œSylvainâ€ä¸€è©çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘å€‘ä¹Ÿå¯ä»¥ä½¿ç”¨ **word_ids()** ç²å–æ¯å€‹æ¨™è¨˜ä¾†è‡ªçš„å–®è©ç´¢å¼•çš„æ–¹æ³•ï¼š

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

æˆ‘å€‘å¯ä»¥çœ‹åˆ°åˆ†è©å™¨çš„ç‰¹æ®Šæ¨™è¨˜ **[CLS]** å’Œ **[SEP]** è¢«æ˜ å°„åˆ° **None** ï¼Œç„¶å¾Œæ¯å€‹æ¨™è¨˜éƒ½æ˜ å°„åˆ°å®ƒèµ·æºçš„å–®è©ã€‚é€™å°æ–¼ç¢ºå®šä¸€å€‹æ¨™è¨˜æ˜¯å¦åœ¨å–®è©çš„é–‹é ­æˆ–å…©å€‹æ¨™è¨˜æ˜¯å¦åœ¨åŒä¸€å€‹å–®è©ä¸­ç‰¹åˆ¥æœ‰ç”¨ã€‚æˆ‘å€‘å¯ä»¥ä¾é  **##** å‰ç¶´ï¼Œä½†å®ƒåƒ…é©ç”¨æ–¼é¡ä¼¼ BERT çš„åˆ†è©å™¨ï¼›é€™ç¨®æ–¹æ³•é©ç”¨æ–¼ä»»ä½•é¡å‹çš„æ¨™è¨˜å™¨ï¼Œåªè¦å®ƒæ˜¯å¿«é€Ÿçš„ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å°‡çœ‹åˆ°å¦‚ä½•ä½¿ç”¨æ­¤åŠŸèƒ½å°‡æ¯å€‹å–®è©çš„æ¨™ç±¤æ­£ç¢ºæ‡‰ç”¨æ–¼å‘½åå¯¦é«”è­˜åˆ¥ (NER) å’Œè©æ€§ (POS) æ¨™è¨˜ç­‰ä»»å‹™ä¸­çš„æ¨™è¨˜ã€‚æˆ‘å€‘é‚„å¯ä»¥ä½¿ç”¨å®ƒä¾†å±è”½ä¾†è‡ªå±è”½èªè¨€å»ºæ¨¡ä¸­ä¾†è‡ªåŒä¸€å–®è©çš„æ‰€æœ‰æ¨™è¨˜ï¼ˆä¸€ç¨®ç¨±ç‚ºå…¨è©æ©ç¢¼ï¼‰ã€‚

<Tip>

ä¸€å€‹è©æ˜¯ä»€éº¼çš„æ¦‚å¿µå¾ˆè¤‡é›œã€‚ä¾‹å¦‚ï¼Œâ€œI'llâ€ï¼ˆâ€œI willâ€çš„ç¸®å¯«ï¼‰ç®—ä¸€å…©å€‹è©å—ï¼Ÿå®ƒå¯¦éš›ä¸Šå–æ±ºæ–¼åˆ†è©å™¨å’Œå®ƒæ‡‰ç”¨çš„é åˆ†è©æ“ä½œã€‚ä¸€äº›æ¨™è¨˜å™¨åªæ˜¯åœ¨ç©ºæ ¼ä¸Šæ‹†åˆ†ï¼Œå› æ­¤ä»–å€‘æœƒå°‡å…¶è¦–ç‚ºä¸€å€‹è©ã€‚å…¶ä»–äººåœ¨ç©ºæ ¼é ‚éƒ¨ä½¿ç”¨æ¨™é»ç¬¦è™Ÿï¼Œå› æ­¤å°‡å…¶è¦–ç‚ºå…©å€‹è©ã€‚

âœï¸ è©¦è©¦çœ‹ï¼å¾bert base casedå’Œroberta baseæª¢æŸ¥é»å‰µå»ºä¸€å€‹æ¨™è¨˜å™¨ï¼Œä¸¦ç”¨å®ƒå€‘æ¨™è¨˜â€œ81sâ€ã€‚ä½ è§€å¯Ÿåˆ°äº†ä»€éº¼ï¼ŸIDé€™å€‹è©æ˜¯ä»€éº¼ï¼Ÿ

</Tip>

åŒæ¨£ï¼Œæœ‰ä¸€å€‹ **sentence_ids()** æˆ‘å€‘å¯ä»¥ç”¨ä¾†å°‡æ¨™è¨˜æ˜ å°„åˆ°å®ƒä¾†è‡ªçš„å¥å­çš„æ–¹æ³•ï¼ˆå„˜ç®¡åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œ **token_type_ids** åˆ†è©å™¨è¿”å›çš„ä¿¡æ¯å¯ä»¥ç‚ºæˆ‘å€‘æä¾›ç›¸åŒçš„ä¿¡æ¯ï¼‰ã€‚

æœ€å¾Œï¼Œæˆ‘å€‘å¯ä»¥å°‡ä»»ä½•å–®è©æˆ–æ¨™è¨˜æ˜ å°„åˆ°åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ï¼Œåä¹‹äº¦ç„¶ï¼Œé€šé **word_to_chars()** æˆ–è€… **token_to_chars()** å’Œ **char_to_word()** æˆ–è€… **char_to_token()** æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œ **word_ids()** æ–¹æ³•å‘Šè¨´æˆ‘å€‘ **##yl** æ˜¯ç´¢å¼• 3 è™•å–®è©çš„ä¸€éƒ¨åˆ†ï¼Œä½†å®ƒæ˜¯å¥å­ä¸­çš„å“ªå€‹å–®è©ï¼Ÿæˆ‘å€‘å¯ä»¥é€™æ¨£ç™¼ç¾ï¼š

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python out
Sylvain
```

æ­£å¦‚æˆ‘å€‘ä¹‹å‰æåˆ°çš„ï¼Œé€™ä¸€åˆ‡éƒ½æ˜¯ç”±å¿«é€Ÿæ¨™è¨˜å™¨è·Ÿè¹¤æ¯å€‹æ¨™è¨˜ä¾†è‡ªåˆ—è¡¨ä¸­çš„æ–‡æœ¬è·¨åº¦é€™ä¸€äº‹å¯¦æä¾›æ”¯æŒçš„æŠµæ¶ˆ.ç‚ºäº†èªªæ˜å®ƒå€‘çš„ç”¨é€”ï¼Œæ¥ä¸‹ä¾†æˆ‘å€‘å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•è¤‡è£½çµæœ **token-classification** æ‰‹å‹•ç®¡é“ã€‚

<Tip>

âœï¸ è©¦è©¦çœ‹ï¼å‰µå»ºæ‚¨è‡ªå·±çš„ç¤ºä¾‹æ–‡æœ¬ï¼Œçœ‹çœ‹æ‚¨æ˜¯å¦èƒ½ç†è§£å“ªäº›æ¨™è¨˜èˆ‡å–®è© ID ç›¸é—œè¯ï¼Œä»¥åŠå¦‚ä½•æå–å–®å€‹å–®è©çš„å­—ç¬¦è·¨åº¦ã€‚å°æ–¼çå‹µç©åˆ†ï¼Œè«‹å˜—è©¦ä½¿ç”¨å…©å€‹å¥å­ä½œç‚ºè¼¸å…¥ï¼Œçœ‹çœ‹å¥å­ ID æ˜¯å¦å°æ‚¨æœ‰æ„ç¾©ã€‚

</Tip>

## åœ¨ä»¤ç‰Œåˆ†é¡ç®¡é“å…§

åœ¨[Chapter 1](/course/chapter1)æˆ‘å€‘ç¬¬ä¸€æ¬¡å˜—è©¦ä½¿ç”¨ NERâ€”â€”ä»»å‹™æ˜¯è­˜åˆ¥æ–‡æœ¬çš„å“ªäº›éƒ¨åˆ†å°æ‡‰æ–¼å€‹äººã€åœ°é»æˆ–çµ„ç¹”ç­‰å¯¦é«”â€”â€”ä½¿ç”¨ ğŸ¤— Transformers **pipeline()** åŠŸèƒ½ã€‚ç„¶å¾Œï¼Œåœ¨[Chapter 2](/course/chapter2)ï¼Œæˆ‘å€‘çœ‹åˆ°äº†ç®¡é“å¦‚ä½•å°‡å¾åŸå§‹æ–‡æœ¬ä¸­ç²å–é æ¸¬æ‰€éœ€çš„ä¸‰å€‹éšæ®µçµ„åˆåœ¨ä¸€èµ·ï¼šæ¨™è¨˜åŒ–ã€é€šéæ¨¡å‹å‚³éè¼¸å…¥å’Œå¾Œè™•ç†ã€‚å‰å…©æ­¥ **token-classification** ç®¡é“èˆ‡ä»»ä½•å…¶ä»–ç®¡é“ç›¸åŒï¼Œä½†å¾Œè™•ç†ç¨å¾®è¤‡é›œä¸€äº› - è®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•ï¼

{#if fw === 'pt'}

<Youtube id="0E7ltQB7fM8"/>

{:else}

<Youtube id="PrX4CjrVnNc"/>

{/if}

### é€šéç®¡é“ç²å¾—åŸºæœ¬çµæœ

é¦–å…ˆï¼Œè®“æˆ‘å€‘ç²å–ä¸€å€‹æ¨™è¨˜åˆ†é¡ç®¡é“ï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥æ‰‹å‹•æ¯”è¼ƒä¸€äº›çµæœã€‚é»˜èªä½¿ç”¨çš„æ¨¡å‹æ˜¯[dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english);å®ƒå°å¥å­åŸ·è¡Œ NERï¼š

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

è©²æ¨¡å‹æ­£ç¢ºåœ°å°‡â€œSylvainâ€ç”Ÿæˆçš„æ¯å€‹æ¨™è¨˜è­˜åˆ¥ç‚ºä¸€å€‹äººï¼Œå°‡â€œHugging Faceâ€ç”Ÿæˆçš„æ¯å€‹æ¨™è¨˜è­˜åˆ¥ç‚ºä¸€å€‹çµ„ç¹”ï¼Œå°‡â€œBrooklynâ€ç”Ÿæˆçš„æ¨™è¨˜è­˜åˆ¥ç‚ºä¸€å€‹ä½ç½®ã€‚æˆ‘å€‘é‚„å¯ä»¥è¦æ±‚ç®¡é“å°‡å°æ‡‰æ–¼åŒä¸€å¯¦é«”çš„ä»¤ç‰Œçµ„åˆåœ¨ä¸€èµ·ï¼š

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

**aggregation_strategy** é¸æ“‡å°‡æ›´æ”¹ç‚ºæ¯å€‹åˆ†çµ„å¯¦é«”è¨ˆç®—çš„åˆ†æ•¸ã€‚å’Œ **simple** åˆ†æ•¸åªæ˜¯çµ¦å®šå¯¦é«”ä¸­æ¯å€‹æ¨™è¨˜çš„åˆ†æ•¸çš„å¹³å‡å€¼ï¼šä¾‹å¦‚ï¼Œâ€œSylvainâ€çš„åˆ†æ•¸æ˜¯æˆ‘å€‘åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­çœ‹åˆ°çš„æ¨™è¨˜åˆ†æ•¸çš„å¹³å‡å€¼ **S** , **##yl** , **##va** ï¼Œ å’Œ **##in** .å…¶ä»–å¯ç”¨çš„ç­–ç•¥æ˜¯ï¼š

- `"first"`, å…¶ä¸­æ¯å€‹å¯¦é«”çš„åˆ†æ•¸æ˜¯è©²å¯¦é«”çš„ç¬¬ä¸€å€‹æ¨™è¨˜çš„åˆ†æ•¸ï¼ˆå› æ­¤å°æ–¼â€œSylvainâ€ï¼Œå®ƒå°‡æ˜¯ 0.993828ï¼Œæ¨™è¨˜çš„åˆ†æ•¸)

- `"max"`,å…¶ä¸­æ¯å€‹å¯¦é«”çš„åˆ†æ•¸æ˜¯è©²å¯¦é«”ä¸­æ¨™è¨˜çš„æœ€å¤§åˆ†æ•¸ï¼ˆå› æ­¤å°æ–¼â€œHugging Faceâ€ï¼Œå®ƒå°‡æ˜¯ 0.98879766ï¼Œå³â€œFaceâ€çš„åˆ†æ•¸ï¼‰

- `"average"`, å…¶ä¸­æ¯å€‹å¯¦é«”çš„åˆ†æ•¸æ˜¯çµ„æˆè©²å¯¦é«”çš„å–®è©åˆ†æ•¸çš„å¹³å‡å€¼ï¼ˆå› æ­¤å°æ–¼â€œSylvainâ€ï¼Œèˆ‡â€œsimpleâ€ç­–ç•¥ï¼Œä½†â€œHugging Faceâ€çš„å¾—åˆ†ç‚º 0.9819ï¼Œâ€œHuggingâ€å¾—åˆ†çš„å¹³å‡å€¼ç‚º 0.975ï¼Œâ€œFaceâ€å¾—åˆ†ç‚º 0.98879ï¼‰

ç¾åœ¨è®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•åœ¨ä¸ä½¿ç”¨pipelineï¼ˆï¼‰å‡½æ•¸çš„æƒ…æ³ä¸‹ç²å¾—é€™äº›çµæœï¼

### å¾è¼¸å…¥åˆ°é æ¸¬

{#if fw === 'pt'}

é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦æ¨™è¨˜æˆ‘å€‘çš„è¼¸å…¥ä¸¦å°‡å…¶å‚³éçµ¦æ¨¡å‹ã€‚é€™æ˜¯å®Œå…¨æŒ‰ç…§[Chapter 2](/course/chapter2);æˆ‘å€‘ä½¿ç”¨ **AutoXxx** é¡ï¼Œç„¶å¾Œåœ¨æˆ‘å€‘çš„ç¤ºä¾‹ä¸­ä½¿ç”¨å®ƒå€‘ï¼š

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

ç”±æ–¼æˆ‘å€‘æ­£åœ¨ä½¿ç”¨ **AutoModelForTokenClassification** åœ¨é€™è£¡ï¼Œæˆ‘å€‘ç‚ºè¼¸å…¥åºåˆ—ä¸­çš„æ¯å€‹æ¨™è¨˜ç²å¾—ä¸€çµ„ logitsï¼š

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

{:else}

é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦æ¨™è¨˜æˆ‘å€‘çš„è¼¸å…¥ä¸¦å°‡å…¶å‚³éçµ¦æ¨¡å‹ã€‚é€™æ˜¯å®Œå…¨æŒ‰ç…§[Chapter 2](/course/chapter2);æˆ‘å€‘ä½¿ç”¨ **AutoXxx** é¡ï¼Œç„¶å¾Œåœ¨æˆ‘å€‘çš„ç¤ºä¾‹ä¸­ä½¿ç”¨å®ƒå€‘ï¼š

```py
from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)
```

æ–¼æˆ‘å€‘æ­£åœ¨ä½¿ç”¨ **AutoModelForTokenClassification** åœ¨é€™è£¡ï¼Œæˆ‘å€‘ç‚ºè¼¸å…¥åºåˆ—ä¸­çš„æ¯å€‹æ¨™è¨˜ç²å¾—ä¸€çµ„ logitsï¼š

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
(1, 19)
(1, 19, 9)
```

{/if}

æˆ‘å€‘æœ‰ä¸€å€‹åŒ…å« 19 å€‹æ¨™è¨˜çš„ 1 å€‹åºåˆ—çš„æ‰¹æ¬¡ï¼Œæ¨¡å‹æœ‰ 9 å€‹ä¸åŒçš„æ¨™ç±¤ï¼Œå› æ­¤æ¨¡å‹çš„è¼¸å‡ºå…·æœ‰ 1 x 19 x 9 çš„å½¢ç‹€ã€‚èˆ‡æ–‡æœ¬åˆ†é¡ç®¡é“ä¸€æ¨£ï¼Œæˆ‘å€‘ä½¿ç”¨ softmax å‡½æ•¸ä¾†è½‰æ›é€™äº› logitsåˆ°æ¦‚ç‡ï¼Œæˆ‘å€‘æ¡ç”¨ argmax ä¾†ç²å¾—é æ¸¬ï¼ˆè«‹æ³¨æ„ï¼Œæˆ‘å€‘å¯ä»¥åœ¨ logits ä¸Šæ¡ç”¨ argmaxï¼Œå› ç‚º softmax ä¸æœƒæ”¹è®Šé †åºï¼‰ï¼š

{#if fw === 'pt'}

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

{:else}

```py
import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)
```

{/if}

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

 **model.config.id2label** å±¬æ€§åŒ…å«ç´¢å¼•åˆ°æ¨™ç±¤çš„æ˜ å°„ï¼Œæˆ‘å€‘å¯ä»¥ç”¨å®ƒä¾†ç†è§£é æ¸¬ï¼š

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

æ­£å¦‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„ï¼Œæœ‰ 9 å€‹æ¨™ç±¤ï¼š **O** æ˜¯ä¸åœ¨ä»»ä½•å‘½åå¯¦é«”ä¸­çš„æ¨™è¨˜çš„æ¨™ç±¤ï¼ˆå®ƒä»£è¡¨â€œå¤–éƒ¨â€ï¼‰ï¼Œç„¶å¾Œæˆ‘å€‘ç‚ºæ¯ç¨®é¡å‹çš„å¯¦é«”ï¼ˆé›œé …ã€äººå“¡ã€çµ„ç¹”å’Œä½ç½®ï¼‰æä¾›å…©å€‹æ¨™ç±¤ã€‚æ¨™ç±¤ **B-XXX** è¡¨ç¤ºä»¤ç‰Œåœ¨å¯¦é«”çš„é–‹é ­ **XXX** å’Œæ¨™ç±¤ **I-XXX** è¡¨ç¤ºä»¤ç‰Œåœ¨å¯¦é«”å…§ **XXX** .ä¾‹å¦‚ï¼Œåœ¨ç•¶å‰ç¤ºä¾‹ä¸­ï¼Œæˆ‘å€‘å¸Œæœ›æˆ‘å€‘çš„æ¨¡å‹å°ä»¤ç‰Œé€²è¡Œåˆ†é¡ **S** ä½œç‚º **B-PER** ï¼ˆä¸€å€‹äººå¯¦é«”çš„é–‹å§‹ï¼‰å’Œä»¤ç‰Œ **##yl** , **##va** å’Œ **##in** ä½œç‚º **I-PER** ï¼ˆåœ¨å€‹äººå¯¦é«”å…§ï¼‰

åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨å¯èƒ½èªç‚ºæ¨¡å‹æ˜¯éŒ¯èª¤çš„ï¼Œå› ç‚ºå®ƒçµ¦å‡ºäº†æ¨™ç±¤ **I-PER** å°æ‰€æœ‰é€™å››å€‹ä»¤ç‰Œï¼Œä½†é€™ä¸¦ä¸å®Œå…¨æ­£ç¢ºã€‚å¯¦éš›ä¸Šæœ‰å…©ç¨®æ ¼å¼ **B-** å’Œ **I-** æ¨™ç±¤ï¼šIOB1å’ŒIOB2. IOB2 æ ¼å¼ï¼ˆä¸‹é¢ç²‰ç´…è‰²ï¼‰æ˜¯æˆ‘å€‘ä»‹ç´¹çš„æ ¼å¼ï¼Œè€Œåœ¨ IOB1 æ ¼å¼ï¼ˆè—è‰²ï¼‰ä¸­ï¼Œæ¨™ç±¤ä»¥ **B-** åƒ…ç”¨æ–¼åˆ†éš”ç›¸åŒé¡å‹çš„å…©å€‹ç›¸é„°å¯¦é«”ã€‚æˆ‘å€‘ä½¿ç”¨çš„æ¨¡å‹åœ¨ä½¿ç”¨è©²æ ¼å¼çš„æ•¸æ“šé›†ä¸Šé€²è¡Œäº†å¾®èª¿ï¼Œé€™å°±æ˜¯å®ƒåˆ†é…æ¨™ç±¤çš„åŸå›  **I-PER** åˆ° **S** ä»¤ç‰Œã€‚

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

äº†é€™å¼µåœ°åœ–ï¼Œæˆ‘å€‘å·²ç¶“æº–å‚™å¥½ï¼ˆå¹¾ä¹å®Œå…¨ï¼‰é‡ç¾ç¬¬ä¸€å€‹ç®¡é“çš„çµæœâ€”â€”æˆ‘å€‘å¯ä»¥ç²å–æ¯å€‹æœªè¢«æ­¸é¡ç‚ºçš„æ¨™è¨˜çš„åˆ†æ•¸å’Œæ¨™ç±¤ **O** ï¼š

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

é€™èˆ‡æˆ‘å€‘ä¹‹å‰çš„æƒ…æ³éå¸¸ç›¸ä¼¼ï¼Œåªæœ‰ä¸€å€‹ä¾‹å¤–ï¼šç®¡é“é‚„ç‚ºæˆ‘å€‘æä¾›äº†æœ‰é—œ **start** å’Œ **end** åŸå§‹å¥å­ä¸­çš„æ¯å€‹å¯¦é«”ã€‚é€™æ˜¯æˆ‘å€‘çš„åç§»æ˜ å°„å°‡ç™¼æ®ä½œç”¨çš„åœ°æ–¹ã€‚è¦ç²å¾—åç§»é‡ï¼Œæˆ‘å€‘åªéœ€è¦è¨­ç½® **return_offsets_mapping=True** ç•¶æˆ‘å€‘å°‡åˆ†è©å™¨æ‡‰ç”¨æ–¼æˆ‘å€‘çš„è¼¸å…¥æ™‚ï¼š

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

æ¯å€‹å…ƒçµ„æ˜¯å°æ‡‰æ–¼æ¯å€‹æ¨™è¨˜çš„æ–‡æœ¬è·¨åº¦ï¼Œå…¶ä¸­ **(0, 0)** ä¿ç•™ç”¨æ–¼ç‰¹æ®Šä»¤ç‰Œã€‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°ç´¢å¼• 5 è™•çš„ä»¤ç‰Œæ˜¯ **##yl** ï¼Œ å…¶ä¸­æœ‰ **(12, 14)** ä½œç‚ºé€™è£¡çš„æŠµæ¶ˆã€‚å¦‚æœæˆ‘å€‘åœ¨ç¤ºä¾‹ä¸­æŠ“å–ç›¸æ‡‰çš„åˆ‡ç‰‡ï¼š


```py
example[12:14]
```

æˆ‘å€‘å¾—åˆ°äº†æ­£ç¢ºçš„æ–‡æœ¬è·¨åº¦ï¼Œè€Œæ²’æœ‰ **##** ï¼š

```python out
yl
```

ä½¿ç”¨é€™å€‹ï¼Œæˆ‘å€‘ç¾åœ¨å¯ä»¥å®Œæˆä¹‹å‰çš„çµæœï¼š

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

é€™å’Œæˆ‘å€‘å¾ç¬¬ä¸€å€‹ç®¡é“ä¸­å¾—åˆ°çš„ä¸€æ¨£ï¼

### åˆ†çµ„å¯¦é«”

ä½¿ç”¨åç§»é‡ä¾†ç¢ºå®šæ¯å€‹å¯¦é«”çš„é–‹å§‹å’ŒçµæŸéµå¾ˆæ–¹ä¾¿ï¼Œä½†è©²ä¿¡æ¯ä¸¦ä¸æ˜¯çµ•å°å¿…è¦çš„ã€‚ç„¶è€Œï¼Œç•¶æˆ‘å€‘æƒ³è¦å°‡å¯¦é«”çµ„åˆåœ¨ä¸€èµ·æ™‚ï¼Œåç§»é‡å°‡ç‚ºæˆ‘å€‘ç¯€çœå¤§é‡æ··äº‚çš„ä»£ç¢¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘å€‘æƒ³å°‡ä»¤ç‰Œçµ„åˆåœ¨ä¸€èµ· **Hu** , **##gging** ï¼Œ å’Œ **Face** ï¼Œæˆ‘å€‘å¯ä»¥åˆ¶å®šç‰¹æ®Šçš„è¦å‰‡ï¼Œèªªå‰å…©å€‹æ‡‰è©²é™„åŠ ï¼ŒåŒæ™‚åˆªé™¤ **##** ï¼Œä»¥åŠ **Face** æ‡‰è©²æ·»åŠ ä¸€å€‹ç©ºæ ¼ï¼Œå› ç‚ºå®ƒä¸ä»¥ **##** â€” ä½†é€™åƒ…é©ç”¨æ–¼é€™ç¨®ç‰¹å®šé¡å‹çš„æ¨™è¨˜å™¨ã€‚æˆ‘å€‘å¿…é ˆç‚º SentencePiece æˆ– Byte-Pair-Encoding åˆ†è©å™¨ï¼ˆæœ¬ç« ç¨å¾Œè¨è«–ï¼‰ã€‚

ç·¨å¯«å¦ä¸€çµ„è¦å‰‡ã€‚ä½¿ç”¨åç§»é‡ï¼Œæ‰€æœ‰è‡ªå®šç¾©ä»£ç¢¼éƒ½æ¶ˆå¤±äº†ï¼šæˆ‘å€‘å¯ä»¥åœ¨åŸå§‹æ–‡æœ¬ä¸­ç²å–å¾ç¬¬ä¸€å€‹æ¨™è¨˜é–‹å§‹åˆ°æœ€å¾Œä¸€å€‹æ¨™è¨˜çµæŸçš„è·¨åº¦ã€‚æ‰€ä»¥ï¼Œåœ¨ä»¤ç‰Œçš„æƒ…æ³ä¸‹ **Hu** , **##gging** ï¼Œ å’Œ **Face** ï¼Œæˆ‘å€‘æ‡‰è©²å¾å­—ç¬¦ 33ï¼ˆé–‹å§‹ **Hu** ) ä¸¦åœ¨å­—ç¬¦ 45 ä¹‹å‰çµæŸï¼ˆçµæŸ **Face** )ï¼š

```py
example[33:45]
```

```python out
Hugging Face
```

ç‚ºäº†ç·¨å¯«åœ¨å°å¯¦é«”é€²è¡Œåˆ†çµ„çš„åŒæ™‚å°é æ¸¬é€²è¡Œå¾Œè™•ç†çš„ä»£ç¢¼ï¼Œæˆ‘å€‘å°‡é€£çºŒä¸¦æ¨™è¨˜ç‚ºçš„å¯¦é«”åˆ†çµ„åœ¨ä¸€èµ· **I-XXX** ï¼Œé™¤äº†ç¬¬ä¸€å€‹ï¼Œå¯ä»¥æ¨™è¨˜ç‚º **B-XXX** æˆ–è€… **I-XXX** ï¼ˆå› æ­¤ï¼Œç•¶æˆ‘å€‘å¾—åˆ°ä¸€å€‹å¯¦é«”æ™‚ï¼Œæˆ‘å€‘åœæ­¢å°å¯¦é«”é€²è¡Œåˆ†çµ„ **O** ï¼Œä¸€ç¨®æ–°å‹å¯¦é«”ï¼Œæˆ– **B-XXX** é€™å‘Šè¨´æˆ‘å€‘ä¸€å€‹ç›¸åŒé¡å‹çš„å¯¦é«”æ­£åœ¨å•Ÿå‹•ï¼‰ï¼š

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

æˆ‘å€‘å¾—åˆ°äº†èˆ‡ç¬¬äºŒæ¢ç®¡é“ç›¸åŒçš„çµæœï¼

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

é€™äº›åç§»é‡éå¸¸æœ‰ç”¨çš„å¦ä¸€å€‹ä»»å‹™ç¤ºä¾‹æ˜¯å•ç­”ã€‚æ·±å…¥ç ”ç©¶é€™å€‹ç®¡é“ï¼Œæˆ‘å€‘å°‡åœ¨ä¸‹ä¸€ç¯€ä¸­é€²è¡Œï¼Œä¹Ÿå°‡ä½¿æˆ‘å€‘èƒ½å¤ äº†è§£ ğŸ¤— Transformers åº«ä¸­æ¨™è¨˜å™¨çš„æœ€å¾Œä¸€å€‹åŠŸèƒ½ï¼šç•¶æˆ‘å€‘å°‡è¼¸å…¥æˆªæ–·ç‚ºçµ¦å®šé•·åº¦æ™‚è™•ç†æº¢å‡ºçš„æ¨™è¨˜ã€‚
