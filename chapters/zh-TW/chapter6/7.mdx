# Unigramæ¨™è¨˜åŒ–

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section7.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section7.ipynb"},
]} />

åœ¨ SentencePiece ä¸­ç¶“å¸¸ä½¿ç”¨ Unigram ç®—æ³•,è©²ç®—æ³•æ˜¯ AlBERTã€T5ã€mBARTã€Big Bird å’Œ XLNet ç­‰æ¨¡å‹ä½¿ç”¨çš„æ¨™è¨˜åŒ–ç®—æ³•ã€‚

<Youtube id="TGZfZVuF9Yc"/>

> [!TIP]
> ğŸ’¡ æœ¬ç¯€æ·±å…¥ä»‹ç´¹äº† Unigram,ç”šè‡³å±•ç¤ºäº†ä¸€å€‹å®Œæ•´çš„å¯¦ç¾ã€‚å¦‚æœä½ åªæƒ³å¤§è‡´ç­è§£æ¨™è¨˜åŒ–ç®—æ³•,å¯ä»¥è·³åˆ°æœ€å¾Œã€‚

## è¨“ç·´ç®—æ³•

èˆ‡ BPE å’Œ WordPiece ç›¸æ¯”ï¼ŒUnigram åœ¨å¦ä¸€å€‹æ–¹å‘ä¸Šå·¥ä½œï¼šå®ƒå¾ä¸€å€‹è¼ƒå¤§çš„è©å½™è¡¨é–‹å§‹ï¼Œç„¶å¾Œå¾ä¸­åˆªé™¤æ¨™è¨˜ï¼Œç›´åˆ°é”åˆ°æ‰€éœ€çš„è©å½™è¡¨å¤§å°ã€‚æœ‰å¤šç¨®é¸é …å¯ç”¨æ–¼æ§‹å»ºåŸºæœ¬è©å½™è¡¨ï¼šä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥æ¡ç”¨é æ¨™è¨˜åŒ–å–®è©ä¸­æœ€å¸¸è¦‹çš„å­ä¸²ï¼Œæˆ–è€…åœ¨å…·æœ‰å¤§è©å½™é‡çš„åˆå§‹èªæ–™åº«ä¸Šæ‡‰ç”¨ BPEã€‚

åœ¨è¨“ç·´çš„æ¯ä¸€æ­¥ï¼ŒUnigram ç®—æ³•éƒ½æœƒåœ¨çµ¦å®šç•¶å‰è©å½™çš„æƒ…æ³ä¸‹è¨ˆç®—èªæ–™åº«çš„æå¤±ã€‚ç„¶å¾Œï¼Œå°æ–¼è©å½™è¡¨ä¸­çš„æ¯å€‹ç¬¦è™Ÿï¼Œç®—æ³•è¨ˆç®—å¦‚æœåˆªé™¤è©²ç¬¦è™Ÿï¼Œæ•´é«”æå¤±æœƒå¢åŠ å¤šå°‘ï¼Œä¸¦å°‹æ‰¾å¢åŠ æœ€å°‘çš„ç¬¦è™Ÿã€‚é€™äº›ç¬¦è™Ÿå°èªæ–™åº«çš„æ•´é«”æå¤±å½±éŸ¿è¼ƒå°ï¼Œå› æ­¤å¾æŸç¨®æ„ç¾©ä¸Šèªªï¼Œå®ƒå€‘ã€Œä¸å¤ªéœ€è¦ã€ä¸¦ä¸”æ˜¯ç§»é™¤çš„æœ€ä½³å€™é¸è€…ã€‚

é€™æ˜¯ä¸€å€‹éå¸¸æ˜‚è²´çš„æ“ä½œï¼Œæ‰€ä»¥æˆ‘å€‘ä¸åªæ˜¯åˆªé™¤èˆ‡æœ€ä½æå¤±å¢åŠ ç›¸é—œçš„å–®å€‹ç¬¦è™Ÿ,è€Œä¸”\\(p\\) (\\(p\\)æ˜¯ä¸€å€‹å¯ä»¥æ§åˆ¶çš„è¶…åƒæ•¸,é€šå¸¸æ˜¯ 10 æˆ– 20)èˆ‡æœ€ä½æå¤±å¢åŠ ç›¸é—œçš„ç¬¦è™Ÿçš„ç™¾åˆ†æ¯”ã€‚ç„¶å¾Œé‡è¤‡é€™å€‹éç¨‹ï¼Œç›´åˆ°è©å½™é‡é”åˆ°æ‰€éœ€çš„å¤§å°ã€‚

è«‹æ³¨æ„ï¼šæˆ‘å€‘å¾ä¸åˆªé™¤åŸºæœ¬å­—ç¬¦ï¼Œä»¥ç¢ºä¿å¯ä»¥æ¨™è¨˜ä»»ä½•å–®è©ã€‚

é€™æˆ–è¨±ä»ç„¶æœ‰é»æ¨¡ç³Šï¼šç®—æ³•çš„ä¸»è¦éƒ¨åˆ†æ˜¯è¨ˆç®—èªæ–™åº«çš„æå¤±ï¼Œä¸¦æŸ¥çœ‹ç•¶æˆ‘å€‘å¾è©å½™è¡¨ä¸­åˆªé™¤ä¸€äº›æ¨™è¨˜æ™‚å®ƒæœƒå¦‚ä½•è®ŠåŒ–ï¼Œä½†æˆ‘å€‘é‚„æ²’æœ‰è§£é‡‹å¦‚ä½•åšåˆ°é€™ä¸€é»ã€‚é€™ä¸€æ­¥ä¾è³´æ–¼ Unigram æ¨¡å‹çš„æ¨™è¨˜åŒ–ç®—æ³•ï¼Œå› æ­¤æˆ‘å€‘æ¥ä¸‹ä¾†å°‡æ·±å…¥ç ”ç©¶ã€‚

æˆ‘å€‘å°‡é‡ç”¨å‰é¢ç¤ºä¾‹ä¸­çš„èªæ–™åº«ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

å°æ–¼æ­¤ç¤ºä¾‹ï¼Œæˆ‘å€‘å°‡æ¡ç”¨åˆå§‹è©å½™è¡¨çš„æ‰€æœ‰åš´æ ¼å­å­—ç¬¦ä¸²:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## æ¨™è¨˜åŒ–ç®—æ³•

Unigram æ¨¡å‹æ˜¯ä¸€ç¨®èªè¨€æ¨¡å‹,å®ƒèªç‚ºæ¯å€‹æ¨™è¨˜éƒ½ç¨ç«‹æ–¼å®ƒä¹‹å‰çš„æ¨™è¨˜ã€‚å®ƒæ˜¯æœ€ç°¡å–®çš„èªè¨€æ¨¡å‹,å¾æŸç¨®æ„ç¾©ä¸Šèªª, çµ¦å®šå…ˆå‰ä¸Šä¸‹æ–‡çš„æ¨™è¨˜ X çš„æ¦‚ç‡å°±æ˜¯æ¨™è¨˜ X çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘å€‘ä½¿ç”¨ Unigram èªè¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œæˆ‘å€‘å°‡å§‹çµ‚é æ¸¬æœ€å¸¸è¦‹çš„æ¨™è¨˜ã€‚

çµ¦å®šæ¨™è¨˜çš„æ¦‚ç‡æ˜¯å®ƒåœ¨åŸå§‹èªæ–™åº«ä¸­çš„é »ç‡(æˆ‘å€‘æ‰¾åˆ°å®ƒçš„æ¬¡æ•¸),é™¤ä»¥è©å½™è¡¨ä¸­æ‰€æœ‰æ¨™è¨˜çš„æ‰€æœ‰é »ç‡çš„ç¸½å’Œ(ä»¥ç¢ºä¿æ¦‚ç‡ç¸½å’Œç‚º 1)ã€‚ä¾‹å¦‚, `"ug"` åœ¨ `"hug"` ã€ `"pug"` ä»¥åŠ `"hugs"` ä¸­ï¼Œæ‰€ä»¥å®ƒåœ¨æˆ‘å€‘çš„èªæ–™åº«ä¸­çš„é »ç‡ç‚º 20ã€‚

ä»¥ä¸‹æ˜¯è©å½™è¡¨ä¸­æ‰€æœ‰å¯èƒ½çš„å­è©çš„å‡ºç¾é »ç‡:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

æ‰€ä»¥,æ‰€æœ‰é »ç‡ä¹‹å’Œç‚º210, ä¸¦ä¸”å­è© `"ug"` å‡ºç¾çš„æ¦‚ç‡æ˜¯ 20/210ã€‚

> [!TIP]
> âœï¸ **ç¾åœ¨è¼ªåˆ°ä½ äº†!** ç·¨å¯«ä»£ç¢¼ä¾†è¨ˆç®—ä¸Šé¢çš„é »ç‡,ä¸¦ä»”ç´°æª¢æŸ¥é¡¯ç¤ºçš„çµæœä»¥åŠç¸½å’Œæ˜¯å¦æ­£ç¢ºã€‚

ç¾åœ¨ï¼Œç‚ºäº†å°çµ¦å®šçš„å–®è©é€²è¡Œæ¨™è¨˜ï¼Œæˆ‘å€‘å°‡æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²è¦–ç‚ºæ¨™è¨˜ï¼Œä¸¦æ ¹æ“š Unigram æ¨¡å‹è¨ˆç®—æ¯å€‹åˆ†å‰²çš„æ¦‚ç‡ã€‚ç”±æ–¼æ‰€æœ‰æ¨™è¨˜éƒ½è¢«èªç‚ºæ˜¯ç¨ç«‹çš„ï¼Œæ‰€ä»¥é€™å€‹æ¦‚ç‡åªæ˜¯æ¯å€‹æ¨™è¨˜æ¦‚ç‡çš„ä¹˜ç©ã€‚ä¾‹å¦‚ `"pug"` çš„æ¨™è¨˜åŒ– `["p", "u", "g"]` çš„æ¦‚ç‡ç‚º:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

ç›¸æ¯”ä¹‹ä¸‹,æ¨™è¨˜åŒ– `["pu", "g"]` çš„æ¦‚ç‡ç‚º:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

æ‰€ä»¥ä¸€å€‹æ›´æœ‰å¯èƒ½ã€‚ä¸€èˆ¬ä¾†èªª,å…·æœ‰å„˜å¯èƒ½å°‘çš„æ¨™è¨˜çš„æ¨™è¨˜åŒ–å°‡å…·æœ‰æœ€é«˜çš„æ¦‚ç‡(å› ç‚ºæ¯å€‹æ¨™è¨˜é‡è¤‡é™¤ä»¥ 210),é€™å°æ‡‰æ–¼æˆ‘å€‘ç›´è§€æƒ³è¦çš„:å°‡ä¸€å€‹å–®è©åˆ†æˆå„˜å¯èƒ½å°‘çš„æ¨™è¨˜ã€‚

ä½¿ç”¨ Unigram æ¨¡å‹å°å–®è©é€²è¡Œåˆ†è©æ˜¯æ¦‚ç‡æœ€é«˜çš„åˆ†è©ã€‚åœ¨ç¤ºä¾‹ `"pug"` ä¸­,é€™è£¡æ˜¯æˆ‘å€‘ç‚ºæ¯å€‹å¯èƒ½çš„åˆ†å‰²ç²å¾—çš„æ¦‚ç‡:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

æ‰€ä»¥ `"pug"` å°‡è¢«æ¨™è¨˜ç‚º `["p", "ug"]` æˆ–è€… `["pu", "g"]`ï¼Œå–æ±ºæ–¼é¦–å…ˆé‡åˆ°é€™äº›åˆ†å‰²ä¸­çš„å“ªä¸€å€‹ï¼ˆè«‹æ³¨æ„ï¼šåœ¨æ›´å¤§çš„èªæ–™åº«ä¸­ï¼Œé€™æ¨£çš„ç›¸ç­‰çš„æƒ…æ³å¾ˆå°‘è¦‹ï¼‰ã€‚

åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œå¾ˆå®¹æ˜“æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ä¸¦è¨ˆç®—å®ƒå€‘çš„æ¦‚ç‡,ä½†ä¸€èˆ¬ä¾†èªªæœƒæœ‰é»å›°é›£ã€‚æœ‰ä¸€ç¨®ç”¨æ–¼æ­¤çš„ç¶“å…¸ç®—æ³•,ç¨±ç‚º *ç¶­ç‰¹æ¯”(Viterbi)ç®—æ³•*ã€‚æœ¬è³ªä¸Š,æˆ‘å€‘å¯ä»¥æ§‹å»ºä¸€å€‹åœ–ä¾†æª¢æ¸¬çµ¦å®šå–®è©çš„å¯èƒ½åˆ†å‰²ï¼Œå¦‚æœå¾_a_åˆ°_b_çš„å­è©åœ¨è©å½™è¡¨ä¸­ï¼Œå‰‡å¾å­—ç¬¦_a_åˆ°å­—ç¬¦_b_ä¹‹é–“å­˜åœ¨ä¸€å€‹åˆ†æ”¯ï¼Œä¸¦å°‡å­è©çš„æ¦‚ç‡æ­¸å› æ–¼è©²åˆ†æ”¯ã€‚

ç‚ºäº†åœ¨è©²åœ–ä¸­æ‰¾åˆ°å°‡å…·æœ‰æœ€ä½³åˆ†æ•¸çš„è·¯å¾‘ï¼Œç¶­ç‰¹æ¯”ç®—æ³•ç‚ºå–®è©ä¸­çš„æ¯å€‹ä½ç½®ç¢ºå®šåœ¨è©²ä½ç½®çµæŸçš„å…·æœ‰æœ€ä½³åˆ†æ•¸çš„åˆ†æ®µã€‚ç”±æ–¼æˆ‘å€‘å¾é–‹å§‹åˆ°çµæŸ,å¯ä»¥é€šéå¾ªç’°éæ­·ä»¥ç•¶å‰ä½ç½®çµå°¾çš„æ‰€æœ‰å­è©ï¼Œç„¶å¾Œä½¿ç”¨è©²å­è©é–‹å§‹ä½ç½®çš„æœ€ä½³æ¨™è¨˜åŒ–åˆ†æ•¸ä¾†æ‰¾åˆ°æœ€ä½³åˆ†æ•¸ã€‚ç„¶å¾Œï¼Œæˆ‘å€‘åªéœ€è¦å±•é–‹åˆ°é”çµ‚é»æ‰€æ¡å–çš„è·¯å¾‘ã€‚

è®“æˆ‘å€‘çœ‹ä¸€å€‹ä½¿ç”¨æˆ‘å€‘çš„è©å½™è¡¨å’Œå–®è© `"unhug"` çš„ä¾‹å­ã€‚å°æ–¼æ¯å€‹ä½ç½®ï¼Œä»¥æœ€å¥½çš„åˆ†æ•¸çµå°¾çš„å­è©å¦‚ä¸‹ï¼š

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

å› æ­¤ `"unhug"` å°‡è¢«æ¨™è¨˜ç‚º `["un", "hug"]`ã€‚

> [!TIP]
> âœï¸ **ç¾åœ¨è¼ªåˆ°ä½ äº†!** ç¢ºå®šå–®è© `"huggun"` çš„æ¨™è¨˜åŒ–åŠå…¶åˆ†æ•¸ã€‚

## å›åˆ°è¨“ç·´

ç¾åœ¨æˆ‘å€‘å·²ç¶“ç­è§£äº†æ¨™è¨˜åŒ–çš„å·¥ä½œåŸç†,æˆ‘å€‘å¯ä»¥æ›´æ·±å…¥åœ°ç ”ç©¶è¨“ç·´æœŸé–“ä½¿ç”¨çš„æå¤±ã€‚åœ¨ä»»ä½•çµ¦å®šçš„éšæ®µ,é€™å€‹æå¤±æ˜¯é€šéå°èªæ–™åº«ä¸­çš„æ¯å€‹å–®è©é€²è¡Œæ¨™è¨˜ä¾†è¨ˆç®—çš„,ä½¿ç”¨ç•¶å‰è©å½™è¡¨å’Œç”±èªæ–™åº«ä¸­æ¯å€‹æ¨™è¨˜çš„é »ç‡ç¢ºå®šçš„ Unigram æ¨¡å‹(å¦‚å‰æ‰€è¿°)ã€‚

èªæ–™åº«ä¸­çš„æ¯å€‹è©éƒ½æœ‰ä¸€å€‹åˆ†æ•¸,æå¤±æ˜¯é€™äº›åˆ†æ•¸çš„è² å°æ•¸ä¼¼ç„¶ -- å³æ‰€æœ‰è©çš„èªæ–™åº«ä¸­æ‰€æœ‰è©çš„ç¸½å’Œ `-log(P(word))`ã€‚

è®“æˆ‘å€‘ç”¨ä»¥ä¸‹èªæ–™åº«å›åˆ°æˆ‘å€‘çš„ä¾‹å­:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ¯å€‹å–®è©çš„æ¨™è¨˜åŒ–åŠå…¶å„è‡ªçš„åˆ†æ•¸æ˜¯:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

æ‰€ä»¥æå¤±æ˜¯:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

ç¾åœ¨æˆ‘å€‘éœ€è¦è¨ˆç®—åˆªé™¤æ¯å€‹æ¨™è¨˜å¦‚ä½•å½±éŸ¿æå¤±ã€‚é€™ç›¸ç•¶ä¹å‘³,æ‰€ä»¥æˆ‘å€‘åœ¨é€™è£¡åªå°å…©å€‹æ¨™è¨˜é€²è¡Œæ“ä½œ,ä¸¦ä¿å­˜æ•´å€‹éç¨‹ä»¥å‚™æœ‰ä»£ç¢¼ä¾†å¹«åŠ©æˆ‘å€‘ã€‚åœ¨é€™å€‹(éå¸¸)ç‰¹æ®Šçš„æƒ…æ³ä¸‹,æˆ‘å€‘å°æ‰€æœ‰å–®è©æœ‰å…©å€‹ç­‰æ•ˆçš„æ¨™è¨˜:æ­£å¦‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„,ä¾‹å¦‚, `"pug"` å¯ä»¥ä»¥ç›¸åŒçš„åˆ†æ•¸è¢«æ¨™è¨˜ç‚º `["p", "ug"]`ã€‚å› æ­¤,å»é™¤è©å½™è¡¨ä¸­çš„ `"pu"` æ¨™è¨˜å°‡çµ¦å‡ºå®Œå…¨ç›¸åŒçš„æå¤±ã€‚

å¦ä¸€æ–¹é¢,å»é™¤ `"hug"` æå¤±è®Šå¾—æ›´ç³Ÿ, å› ç‚º `"hug"` å’Œ `"hugs"` çš„æ¨™è¨˜åŒ–æœƒè®Šæˆ:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

é€™äº›è®ŠåŒ–å°‡å°è‡´æå¤±å¢åŠ :

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

å› æ­¤, æ¨™è¨˜ `"pu"`å¯èƒ½æœƒå¾è©å½™è¡¨ä¸­åˆªé™¤,ä½†ä¸æœƒåˆªé™¤ `"hug"`.

## å¯¦ç¾ Unigram

ç¾åœ¨è®“æˆ‘å€‘åœ¨ä»£ç¢¼ä¸­å¯¦ç¾æˆ‘å€‘è¿„ä»Šç‚ºæ­¢çœ‹åˆ°çš„æ‰€æœ‰å…§å®¹ã€‚èˆ‡ BPE å’Œ WordPiece ä¸€æ¨£,é€™ä¸æ˜¯ Unigram ç®—æ³•çš„æœ‰æ•ˆå¯¦ç¾(æ°æ°ç›¸å),ä½†å®ƒæ‡‰è©²å¯ä»¥å¹«åŠ©ä½ æ›´å¥½åœ°ç†è§£å®ƒã€‚

æˆ‘å€‘å°‡ä½¿ç”¨èˆ‡ä¹‹å‰ç›¸åŒçš„èªæ–™åº«ä½œç‚ºç¤ºä¾‹:

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é€™ä¸€æ¬¡,æˆ‘å€‘å°‡ä½¿ç”¨ `xlnet-base-cased` ä½œç‚ºæˆ‘å€‘çš„æ¨¡å‹:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

èˆ‡ BPE å’Œ WordPiece ä¸€æ¨£,æˆ‘å€‘é¦–å…ˆè¨ˆç®—èªæ–™åº«ä¸­æ¯å€‹å–®è©çš„å‡ºç¾æ¬¡æ•¸:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

ç„¶å¾Œ,æˆ‘å€‘éœ€è¦å°‡æˆ‘å€‘çš„è©å½™è¡¨åˆå§‹åŒ–ç‚ºå¤§æ–¼æˆ‘å€‘æœ€çµ‚æƒ³è¦çš„è©å½™é‡ã€‚æˆ‘å€‘å¿…é ˆåŒ…å«æ‰€æœ‰åŸºæœ¬å­—ç¬¦(å¦å‰‡æˆ‘å€‘å°‡ç„¡æ³•æ¨™è¨˜æ¯å€‹å–®è©),ä½†å°æ–¼è¼ƒå¤§çš„å­å­—ç¬¦ä¸²,æˆ‘å€‘å°‡åªä¿ç•™æœ€å¸¸è¦‹çš„å­—ç¬¦,å› æ­¤æˆ‘å€‘æŒ‰é »ç‡å°å®ƒå€‘é€²è¡Œæ’åº:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Loop through the subwords of length at least 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sort subwords by frequency
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

æˆ‘å€‘ç”¨æœ€å„ªçš„å­è©å°å­—ç¬¦é€²è¡Œåˆ†çµ„,ä»¥ç²å¾—å¤§å°ç‚º 300 çš„åˆå§‹è©å½™è¡¨:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

> [!TIP]
> ğŸ’¡ SentencePiece ä½¿ç”¨ä¸€ç¨®ç¨±ç‚ºå¢å¼·å¾Œç¶´æ•¸çµ„(ESA)çš„æ›´é«˜æ•ˆç®—æ³•ä¾†å‰µå»ºåˆå§‹è©å½™è¡¨ã€‚

æ¥ä¸‹ä¾†,æˆ‘å€‘è¨ˆç®—æ‰€æœ‰é »ç‡çš„ç¸½å’Œ,å°‡é »ç‡è½‰æ›ç‚ºæ¦‚ç‡ã€‚å°æ–¼æˆ‘å€‘çš„æ¨¡å‹,æˆ‘å€‘å°‡å­˜å„²æ¦‚ç‡çš„å°æ•¸,å› ç‚ºæ·»åŠ å°æ•¸æ¯”ä¹˜ä»¥å°æ•¸åœ¨æ•¸å€¼ä¸Šæ›´ç©©å®š,é€™å°‡ç°¡åŒ–æ¨¡å‹æå¤±çš„è¨ˆç®—:

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

Nç¾åœ¨ä¸»è¦åŠŸèƒ½æ˜¯ä½¿ç”¨ Viterbi ç®—æ³•æ¨™è¨˜å–®è©çš„åŠŸèƒ½ã€‚æ­£å¦‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„,è©²ç®—æ³•è¨ˆç®—å–®è©çš„æ¯å€‹å­ä¸²çš„æœ€ä½³åˆ†æ®µ,æˆ‘å€‘å°‡å…¶å­˜å„²åœ¨åç‚º `best_segmentations` çš„è®Šé‡ä¸­ã€‚æˆ‘å€‘å°‡åœ¨å–®è©çš„æ¯å€‹ä½ç½®(å¾ 0 åˆ°å…¶ç¸½é•·åº¦)å­˜å„²ä¸€å€‹å­—å…¸,æœ‰å…©å€‹éµ:æœ€ä½³åˆ†å‰²ä¸­æœ€å¾Œä¸€å€‹æ¨™è¨˜çš„é–‹å§‹ç´¢å¼•,ä»¥åŠæœ€ä½³åˆ†å‰²çš„åˆ†æ•¸ã€‚ä½¿ç”¨æœ€å¾Œä¸€å€‹æ¨™è¨˜çš„é–‹å§‹ç´¢å¼•,ä¸€æ—¦åˆ—è¡¨å®Œå…¨å¡«å……,æˆ‘å€‘å°‡èƒ½å¤ æª¢ç´¢å®Œæ•´çš„åˆ†æ®µã€‚

å¡«å……åˆ—è¡¨åªéœ€å…©å€‹å¾ªç’°:ä¸»å¾ªç’°éæ­·æ¯å€‹èµ·å§‹ä½ç½®,ç¬¬äºŒå€‹å¾ªç’°å˜—è©¦å¾è©²èµ·å§‹ä½ç½®é–‹å§‹çš„æ‰€æœ‰å­å­—ç¬¦ä¸²ã€‚å¦‚æœå­ä¸²åœ¨è©å½™è¡¨ä¸­,æˆ‘å€‘æœ‰ä¸€å€‹æ–°çš„è©åˆ†æ®µ,ç›´åˆ°è©²çµæŸä½ç½®,æˆ‘å€‘å°‡å…¶èˆ‡ `best_segmentations` ç›¸æ¯”è¼ƒã€‚

ä¸€æ—¦ä¸»å¾ªç’°å®Œæˆ,æˆ‘å€‘å°±å¾çµå°¾é–‹å§‹,å¾ä¸€å€‹é–‹å§‹ä½ç½®è·³åˆ°ä¸‹ä¸€å€‹,è¨˜éŒ„æˆ‘å€‘å‰é€²çš„æ¨™è¨˜,ç›´åˆ°æˆ‘å€‘åˆ°é”å–®è©çš„é–‹é ­:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # This should be properly filled by the previous steps of the loop
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # If we have found a better segmentation ending at end_idx, we update
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # We did not find a tokenization of the word -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

æˆ‘å€‘å·²ç¶“å¯ä»¥åœ¨ä¸€äº›è©ä¸Šå˜—è©¦æˆ‘å€‘çš„åˆå§‹æ¨¡å‹:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

ç¾åœ¨å¾ˆå®¹æ˜“è¨ˆç®—æ¨¡å‹åœ¨èªæ–™åº«ä¸Šçš„æå¤±!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

æˆ‘å€‘å¯ä»¥æª¢æŸ¥å®ƒæ˜¯å¦é©ç”¨æ–¼æˆ‘å€‘æ“æœ‰çš„æ¨¡å‹:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

è¨ˆç®—æ¯å€‹æ¨™è¨˜çš„åˆ†æ•¸ä¹Ÿä¸æ˜¯å¾ˆé›£;æˆ‘å€‘åªéœ€è¦è¨ˆç®—é€šéåˆªé™¤æ¯å€‹æ¨™è¨˜ç²å¾—çš„æ¨¡å‹çš„æå¤±:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # We always keep tokens of length 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

æˆ‘å€‘å¯ä»¥åœ¨çµ¦å®šçš„æ¨™è¨˜ä¸Šå˜—è©¦:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

è‡ªå¾ `"ll"` ç”¨æ–¼æ¨™è¨˜åŒ– `"Hopefully"`, åˆªé™¤å®ƒå¯èƒ½æœƒè®“æˆ‘å€‘ä½¿ç”¨æ¨™è¨˜ `"l"` å…©æ¬¡ç›¸å,æˆ‘å€‘é è¨ˆå®ƒå°‡ç”¢ç”Ÿæ­£æå¤±ã€‚ `"his"` åƒ…åœ¨å–®è©`"This"` å…§ä½¿ç”¨,å®ƒè¢«æ¨™è¨˜ç‚ºè‡ªèº«,æ‰€ä»¥æˆ‘å€‘æœŸæœ›å®ƒçš„æå¤±ç‚ºé›¶ã€‚çµæœå¦‚ä¸‹:

```python out
6.376412403623874
0.0
```

> [!TIP]
> ğŸ’¡ é€™ç¨®æ–¹æ³•éå¸¸ä½æ•ˆ,å› æ­¤ SentencePiece ä½¿ç”¨äº†æ²’æœ‰æ¨™è¨˜ X çš„æ¨¡å‹æå¤±çš„è¿‘ä¼¼å€¼:å®ƒä¸æ˜¯å¾é ­é–‹å§‹,è€Œæ˜¯é€šéå…¶åœ¨å‰©é¤˜è©å½™è¡¨ä¸­çš„åˆ†æ®µæ›¿æ›æ¨™è¨˜ Xã€‚é€™æ¨£,æ‰€æœ‰åˆ†æ•¸å¯ä»¥èˆ‡æ¨¡å‹æå¤±åŒæ™‚è¨ˆç®—ã€‚

å®Œæˆæ‰€æœ‰é€™äº›å¾Œ,æˆ‘å€‘éœ€è¦åšçš„æœ€å¾Œä¸€ä»¶äº‹æ˜¯å°‡æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ¨™è¨˜æ·»åŠ åˆ°è©å½™è¡¨ä¸­,ç„¶å¾Œå¾ªç’°ç›´åˆ°æˆ‘å€‘å¾è©å½™è¡¨ä¸­ä¿®å‰ªäº†è¶³å¤ çš„æ¨™è¨˜ä»¥é”åˆ°æˆ‘å€‘æƒ³è¦çš„å¤§å°:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Remove percent_to_remove tokens with the lowest scores.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ç„¶å¾Œ,ç‚ºäº†æ¨™è¨˜ä¸€äº›æ–‡æœ¬,æˆ‘å€‘åªéœ€è¦æ‡‰ç”¨é æ¨™è¨˜åŒ–,ç„¶å¾Œä½¿ç”¨æˆ‘å€‘çš„ `encode_word()` å‡½æ•¸:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

Unigram å°±æ˜¯é€™æ¨£!å¸Œæœ›ç¾åœ¨ä½ æ„Ÿè¦ºè‡ªå·±æ˜¯æ¨™è¨˜å™¨æ‰€æœ‰æ–¹é¢çš„å°ˆå®¶ã€‚åœ¨ä¸‹ä¸€ç¯€ä¸­,æˆ‘å€‘å°‡æ·±å…¥ç ”ç©¶ ğŸ¤— Tokenizers åº«çš„æ§‹å»ºå¡Š,ä¸¦å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨å®ƒå€‘ä¾†æ§‹å»ºæ‚¨è‡ªå·±çš„æ¨™è¨˜å™¨ã€‚
