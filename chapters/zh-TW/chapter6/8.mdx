# é€å¡Šåœ°æ§‹å»ºæ¨™è¨˜å™¨

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section8.ipynb"},
]} />

æ­£å¦‚æˆ‘å€‘åœ¨å‰å¹¾ç¯€ä¸­çœ‹åˆ°çš„ï¼Œæ¨™è¨˜åŒ–åŒ…æ‹¬å¹¾å€‹æ­¥é©Ÿï¼š

- è¦ç¯„åŒ–ï¼ˆä»»ä½•èªç‚ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆªé™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦è™Ÿã€Unicode è¦ç¯„åŒ–ç­‰ï¼‰ 
- é æ¨™è¨˜åŒ–ï¼ˆå°‡è¼¸å…¥æ‹†åˆ†ç‚ºå–®è©ï¼‰ 
- é€šéæ¨¡å‹è™•ç†è¼¸å…¥ï¼ˆä½¿ç”¨é å…ˆæ‹†åˆ†çš„è©ä¾†ç”Ÿæˆä¸€ç³»åˆ—æ¨™è¨˜ï¼‰ 
- å¾Œè™•ç†ï¼ˆæ·»åŠ æ¨™è¨˜å™¨çš„ç‰¹æ®Šæ¨™è¨˜ï¼Œç”Ÿæˆæ³¨æ„åŠ›æ©ç¢¼å’Œæ¨™è¨˜é¡å‹ IDï¼‰ 

æé†’ä¸€ä¸‹ï¼Œé€™è£¡å†çœ‹ä¸€ä¸‹æ•´å€‹éç¨‹

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

ğŸ¤— Tokenizers åº«æ—¨åœ¨ç‚ºæ¯å€‹æ­¥é©Ÿæä¾›å¤šå€‹é¸é …ï¼Œæ‚¨å¯ä»¥å°‡å®ƒå€‘æ··åˆå’ŒåŒ¹é…åœ¨ä¸€èµ·ã€‚åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡çœ‹åˆ°å¦‚ä½•å¾é ­é–‹å§‹æ§‹å»ºæ¨™è¨˜å™¨ï¼Œè€Œä¸æ˜¯åƒæˆ‘å€‘[ç¬¬äºŒç¯€ 2](/course/chapter6/2)é‚£æ¨£å¾èˆŠçš„æ¨™è¨˜å™¨è¨“ç·´æ–°çš„æ¨™è¨˜å™¨.ç„¶å¾Œï¼Œæ‚¨å°‡èƒ½å¤ æ§‹å»ºæ‚¨èƒ½æƒ³åˆ°çš„ä»»ä½•é¡å‹çš„æ¨™è¨˜å™¨ï¼

<Youtube id="MR8tZm5ViWU"/>

æ›´æº–ç¢ºåœ°èªªï¼Œè©²åº«æ˜¯åœç¹ä¸€å€‹ä¸­å¤®ã€ŒTokenizerã€é¡æ§‹å»ºçš„ï¼Œæ§‹å»ºé€™å€‹é¡çš„æ¯ä¸€éƒ¨åˆ†å¯ä»¥åœ¨å­æ¨¡å¡Šçš„åˆ—è¡¨ä¸­é‡æ–°çµ„åˆï¼š

- `normalizers` åŒ…å«ä½ å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¯èƒ½çš„Normalizeré¡å‹ï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/api/normalizers)ï¼‰ã€‚ 
- `pre_tokenizesr` åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¯èƒ½çš„PreTokenizeré¡å‹ï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)ï¼‰ã€‚ 
- `models` åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„å„ç¨®é¡å‹çš„Modelï¼Œå¦‚BPEã€WordPieceå’ŒUnigramï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/api/models)ï¼‰ã€‚  
- `trainers` åŒ…å«æ‰€æœ‰ä¸åŒé¡å‹çš„ trainerï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸€å€‹èªæ–™åº«è¨“ç·´ä½ çš„æ¨¡å‹ï¼ˆæ¯ç¨®æ¨¡å‹ä¸€å€‹ï¼›å®Œæ•´åˆ—è¡¨[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/api/trainers)ï¼‰ã€‚ 
- `post_processors` åŒ…å«ä½ å¯ä»¥ä½¿ç”¨çš„å„ç¨®é¡å‹çš„PostProcessorï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/api/post-processors)ï¼‰ã€‚ 
- `decoders` åŒ…å«å„ç¨®é¡å‹çš„Decoderï¼Œå¯ä»¥ç”¨ä¾†è§£ç¢¼æ¨™è¨˜åŒ–çš„è¼¸å‡ºï¼ˆå®Œæ•´åˆ—è¡¨[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/components#decoders)ï¼‰ã€‚ 

æ‚¨å¯ä»¥[åœ¨é€™è£¡](https://huggingface.co/docs/tokenizers/components)æ‰¾åˆ°å®Œæ•´çš„æ¨¡å¡Šåˆ—è¡¨ã€‚

## ç²å–èªâ€‹â€‹æ–™åº«

ç‚ºäº†è¨“ç·´æˆ‘å€‘çš„æ–°æ¨™è¨˜å™¨ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ä¸€å€‹å°çš„æ–‡æœ¬èªæ–™åº«ï¼ˆå› æ­¤ç¤ºä¾‹é‹è¡Œå¾—å¾ˆå¿«ï¼‰ã€‚ç²å–èªâ€‹â€‹æ–™åº«çš„æ­¥é©Ÿèˆ‡æˆ‘å€‘åœ¨[åœ¨é€™ç« çš„é–‹å§‹]((/course/chapter6/2)é‚£ä¸€å°ç¯€ï¼Œä½†é€™æ¬¡æˆ‘å€‘å°‡ä½¿ç”¨[WikiText-2](https://huggingface.co/datasets/wikitext)æ•¸æ“šé›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

**get_training_corpus()** å‡½æ•¸æ˜¯ä¸€å€‹ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡èª¿ç”¨çš„æ™‚å€™å°‡ç”¢ç”Ÿ 1,000 å€‹æ–‡æœ¬ï¼Œæˆ‘å€‘å°‡ç”¨å®ƒä¾†è¨“ç·´æ¨™è¨˜å™¨ã€‚

ğŸ¤— Tokenizers ä¹Ÿå¯ä»¥ç›´æ¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šé€²è¡Œè¨“ç·´ã€‚ä»¥ä¸‹æ˜¯æˆ‘å€‘å¦‚ä½•ç”Ÿæˆä¸€å€‹æ–‡æœ¬æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æˆ‘å€‘å¯ä»¥åœ¨æœ¬åœ°ä½¿ç”¨çš„ä¾†è‡ª WikiText-2 çš„æ‰€æœ‰æ–‡æœ¬/è¼¸å…¥ï¼š

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•é€å¡Šæ§‹å»ºæ‚¨è‡ªå·±çš„ BERTã€GPT-2 å’Œ XLNet æ¨™è¨˜å™¨ã€‚é€™å°‡ç‚ºæˆ‘å€‘æä¾›ä¸‰å€‹ä¸»è¦æ¨™è¨˜åŒ–ç®—æ³•çš„ç¤ºä¾‹ï¼šWordPieceã€BPE å’Œ Unigramã€‚è®“æˆ‘å€‘å¾ BERT é–‹å§‹å§ï¼

## å¾é ­é–‹å§‹æ§‹å»º WordPiece æ¨™è¨˜å™¨

è¦ä½¿ç”¨ ğŸ¤— Tokenizers åº«æ§‹å»ºæ¨™è¨˜å™¨ï¼Œæˆ‘å€‘é¦–å…ˆä½¿ç”¨ **model** å¯¦ä¾‹åŒ–ä¸€å€‹ **Tokenizer** å°è±¡ï¼Œç„¶å¾Œå°‡ **normalizer** , **pre_tokenizer** , **post_processor** ï¼Œ å’Œ **decoder** å±¬æ€§è¨­ç½®æˆæˆ‘å€‘æƒ³è¦çš„å€¼ã€‚

å°æ–¼é€™å€‹ä¾‹å­ï¼Œæˆ‘å€‘å°‡å‰µå»ºä¸€å€‹ **Tokenizer** ä½¿ç”¨ WordPiece æ¨¡å‹ï¼š

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

æˆ‘å€‘å¿…é ˆæŒ‡å®š **unk_token** é€™æ¨£æ¨¡å‹æ‰çŸ¥é“ç•¶å®ƒé‡åˆ°ä»¥å‰æ²’æœ‰è¦‹éçš„å­—ç¬¦æ™‚è¦è¿”å›ä»€éº¼ã€‚æˆ‘å€‘å¯ä»¥åœ¨æ­¤è™•è¨­ç½®çš„å…¶ä»–åƒæ•¸åŒ…æ‹¬æˆ‘å€‘æ¨¡å‹çš„**vocabï¼ˆå­—å…¸ï¼‰**ï¼ˆæˆ‘å€‘å°‡è¨“ç·´æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘å€‘ä¸éœ€è¦è¨­ç½®å®ƒï¼‰å’Œ **max_input_chars_per_word** å³æ¯å€‹å–®è©çš„æœ€å¤§é•·åº¦ï¼ˆæ¯”å‚³éçš„å€¼é•·çš„å–®è©å°‡è¢«æ‹†åˆ†ï¼‰

æ¨™è¨˜åŒ–çš„ç¬¬ä¸€æ­¥æ˜¯è¦ç¯„åŒ–ï¼Œæ‰€ä»¥è®“æˆ‘å€‘å¾å®ƒé–‹å§‹ã€‚ ç”±æ–¼ BERT è¢«å»£æ³›ä½¿ç”¨ï¼Œæ‰€ä»¥æœ‰ä¸€å€‹å¯ä»¥ä½¿ç”¨çš„ `BertNormalizer`ï¼Œæˆ‘å€‘å¯ä»¥ç‚º BERT è¨­ç½®ç¶“å…¸çš„é¸é …ï¼š`lowercaseï¼ˆå°å¯«ï¼‰` å’Œ `strip_accentsï¼ˆå»é™¤éŸ³èª¿ï¼‰`ï¼Œä¸è¨€è‡ªæ˜ï¼› `clean_text` åˆªé™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦ä¸¦å°‡é‡è¤‡çš„ç©ºæ ¼æ›¿æ›ç‚ºä¸€å€‹ï¼› å’Œ `handle_chinese_chars`ï¼Œåœ¨æ¼¢å­—å‘¨åœæ”¾ç½®ç©ºæ ¼ã€‚ è¦å¯¦ç¾ `bert-base-uncased` ï¼Œæˆ‘å€‘å¯ä»¥é€™æ¨£è¨­ç½®é€™å€‹è¦ç¯„å™¨ï¼š

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ç„¶è€Œï¼Œä¸€èˆ¬ä¾†èªªï¼Œåœ¨æ§‹å»ºæ–°çš„æ¨™è¨˜å™¨æ™‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å·²ç¶“åœ¨ ğŸ¤— Tokenizersåº«ä¸­å¯¦ç¾çš„éå¸¸æ–¹ä¾¿çš„normalizerâ€”â€”æ‰€ä»¥è®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•æ‰‹å‹•å‰µå»º BERT normalizerã€‚ è©²åº«æä¾›äº†ä¸€å€‹â€œLowercaseï¼ˆå°å¯«ï¼‰â€çš„normalizerå’Œä¸€å€‹â€œStripAccentsâ€çš„normalizerï¼Œæ‚¨å¯ä»¥ä½¿ç”¨â€œåºåˆ—â€çµ„åˆå¤šå€‹normalizerï¼š

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

æˆ‘å€‘ä¹Ÿåœ¨ä½¿ç”¨ **NFD** Unicode normalizerï¼Œå¦å‰‡ **StripAccents** normalizer ç„¡æ³•æ­£ç¢ºè­˜åˆ¥å¸¶é‡éŸ³çš„å­—ç¬¦ï¼Œå› æ­¤æ²’è¾¦æ³•åˆªé™¤å®ƒå€‘ã€‚

æ­£å¦‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ **normalize** çš„ **normalize_str()** æ–¹æ³•æŸ¥çœ‹å®ƒå°çµ¦å®šæ–‡æœ¬çš„å½±éŸ¿ï¼š

```python
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
hello how are u?
```

<Tip>

**æ›´é€²ä¸€æ­¥**å¦‚æœæ‚¨åœ¨åŒ…å« unicode å­—ç¬¦çš„å­—ç¬¦ä¸²ä¸Šæ¸¬è©¦å…ˆå‰normalizersçš„å…©å€‹ç‰ˆæœ¬ï¼Œæ‚¨è‚¯å®šæœƒæ³¨æ„åˆ°é€™å…©å€‹normalizersä¸¦ä¸å®Œå…¨ç­‰æ•ˆã€‚
ç‚ºäº†ä¸éåº¦ä½¿ç”¨ `normalizers.Sequence` ä½¿ç‰ˆæœ¬éæ–¼è¤‡é›œï¼Œæˆ‘å€‘æ²’æœ‰åŒ…å«ç•¶ `clean_text` åƒæ•¸è¨­ç½®ç‚º `True` æ™‚ `BertNormalizer` éœ€è¦çš„æ­£å‰‡è¡¨é”å¼æ›¿æ› - é€™æ˜¯é»˜èªè¡Œç‚ºã€‚ ä½†ä¸è¦æ“”å¿ƒï¼šé€šéåœ¨normalizeråºåˆ—ä¸­æ·»åŠ å…©å€‹ `normalizers.Replace` å¯ä»¥åœ¨ä¸ä½¿ç”¨æ–¹ä¾¿çš„ `BertNormalizer` çš„æƒ…æ³ä¸‹ç²å¾—å®Œå…¨ç›¸åŒçš„è¦ç¯„åŒ–ã€‚

</Tip>

æ¥ä¸‹ä¾†æ˜¯é æ¨™è¨˜æ­¥é©Ÿã€‚ åŒæ¨£ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ä¸€å€‹é æ§‹å»ºçš„â€œBertPreTokenizerâ€ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

æˆ–è€…æˆ‘å€‘å¯ä»¥å¾é ­é–‹å§‹æ§‹å»ºå®ƒï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

è«‹æ³¨æ„ï¼Œ`Whitespace` é æ¨™è¨˜å™¨æœƒåœ¨ç©ºæ ¼å’Œæ‰€æœ‰éå­—æ¯ã€æ•¸å­—æˆ–ä¸‹åŠƒç·šå­—ç¬¦çš„å­—ç¬¦ä¸Šé€²è¡Œæ‹†åˆ†ï¼Œå› æ­¤åœ¨æœ¬æ¬¡çš„ä¾‹å­ä¸­ä¸Šæœƒæ ¹æ“šç©ºæ ¼å’Œæ¨™é»ç¬¦è™Ÿé€²è¡Œæ‹†åˆ†ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

å¦‚æœæ‚¨åªæƒ³åœ¨ç©ºç™½è™•é€²è¡Œæ‹†åˆ†ï¼Œå‰‡æ‡‰ä½¿ç”¨ **WhitespaceSplit** ä»£æ›¿é æ¨™è¨˜å™¨ï¼š

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

åƒnormalizersä¸€æ¨£ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ **Sequence** çµ„æˆå¹¾å€‹é æ¨™è¨˜å™¨ï¼š

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

æ¨™è¨˜åŒ–ç®¡é“çš„ä¸‹ä¸€æ­¥æ˜¯è¼¸å…¥çµ¦æ¨¡å‹ã€‚æˆ‘å€‘å·²ç¶“åœ¨åˆå§‹åŒ–ä¸­æŒ‡å®šäº†æˆ‘å€‘çš„æ¨¡å‹ï¼Œä½†æˆ‘å€‘ä»ç„¶éœ€è¦è¨“ç·´å®ƒï¼Œé€™å°‡éœ€è¦ä¸€å€‹ **WordPieceTrainer** .åœ¨ ğŸ¤— Tokenizers ä¸­å¯¦ä¾‹åŒ–è¨“ç·´å™¨æ™‚è¦è¨˜ä½çš„ä¸»è¦äº‹æƒ…æ˜¯ï¼Œæ‚¨éœ€è¦å°‡æ‚¨æ‰“ç®—ä½¿ç”¨çš„æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜å‚³éçµ¦å®ƒ - å¦å‰‡å®ƒä¸æœƒå°‡å®ƒå€‘æ·»åŠ åˆ°è©å½™è¡¨ä¸­ï¼Œå› ç‚ºå®ƒå€‘ä¸åœ¨è¨“ç·´èªæ–™åº«ä¸­ï¼š

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

ä»¥åŠæŒ‡å®š **vocab_sizeï¼ˆè©å…¸å¤§å°ï¼‰** å’Œ **special_tokensï¼ˆç‰¹æ®Šçš„æ¨™è¨˜ï¼‰** ï¼Œæˆ‘å€‘å¯ä»¥è¨­ç½® **min_frequency** ï¼ˆè¨˜è™Ÿå¿…é ˆå‡ºç¾åœ¨è©å½™è¡¨ä¸­çš„æ¬¡æ•¸ï¼‰æˆ–æ›´æ”¹ **continuing_subword_prefix** ï¼ˆå¦‚æœæˆ‘å€‘æƒ³ä½¿ç”¨èˆ‡ **##**æŒ‡ä»£å­˜åœ¨èˆ‡å­—è©ç›¸åŒçš„å‰ç¶´ ï¼‰ã€‚

è¦ä½¿ç”¨æˆ‘å€‘ä¹‹å‰å®šç¾©çš„è¿­ä»£å™¨è¨“ç·´æˆ‘å€‘çš„æ¨¡å‹ï¼Œæˆ‘å€‘åªéœ€è¦åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

æˆ‘å€‘é‚„å¯ä»¥ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶ä¾†è¨“ç·´æˆ‘å€‘çš„æ¨™è¨˜å™¨ï¼Œå®ƒçœ‹èµ·ä¾†åƒé€™æ¨£ï¼ˆæˆ‘å€‘éœ€è¦å…ˆåˆå§‹åŒ–ä¸€å€‹ç©ºçš„ **WordPiece** ï¼‰ï¼š

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

åœ¨é€™å…©ç¨®æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘éƒ½å¯ä»¥é€šéèª¿ç”¨æ–‡æœ¬ä¾†æ¸¬è©¦æ¨™è¨˜å™¨ **encode()** æ–¹æ³•ï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

é€™å€‹ **encoding** ç²å¾—çš„æ˜¯ä¸€å€‹ **Encoding**å°è±¡ ï¼Œå®ƒçš„å±¬æ€§ä¸­åŒ…å«æ¨™è¨˜å™¨çš„æ‰€æœ‰å¿…è¦è¼¸å‡ºï¼š **ids** , **type_ids** , **tokens** , **offsets** , **attention_mask** , **special_tokens_mask** ï¼Œ å’Œ **overflowing** .

æ¨™è¨˜åŒ–ç®¡é“çš„æœ€å¾Œä¸€æ­¥æ˜¯å¾Œè™•ç†ã€‚æˆ‘å€‘éœ€è¦æ·»åŠ  **[CLS]** é–‹é ­çš„æ¨™è¨˜å’Œ **[SEP]** æ¨™è¨˜åœ¨æœ«å°¾ï¼ˆæˆ–åœ¨æ¯å€‹å¥å­ä¹‹å¾Œï¼Œå¦‚æœæˆ‘å€‘æœ‰ä¸€å°å¥å­ï¼‰ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ä¸€å€‹ **TemplateProcessor** ç‚ºæ­¤ï¼Œä½†é¦–å…ˆæˆ‘å€‘éœ€è¦çŸ¥é“ **[CLS]** å’Œ **[SEP]** åœ¨è©å½™è¡¨ä¸­çš„IDï¼š

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

ç‚ºäº†çµ¦ **TemplateProcessor** ç·¨å¯«æ¨¡æ¿ï¼Œæˆ‘å€‘å¿…é ˆæŒ‡å®šå¦‚ä½•è™•ç†å–®å€‹å¥å­å’Œä¸€å°å¥å­ã€‚å°æ–¼å…©è€…ï¼Œæˆ‘å€‘éƒ½ç·¨å¯«äº†æˆ‘å€‘æƒ³è¦ä½¿ç”¨çš„ç‰¹æ®Šæ¨™è¨˜ï¼›ç¬¬ä¸€å€‹ï¼ˆæˆ–å–®å€‹ï¼‰å¥å­è¡¨ç¤ºç‚º **$A** ï¼Œè€Œç¬¬äºŒå€‹å¥å­ï¼ˆå¦‚æœå°ä¸€å°é€²è¡Œç·¨ç¢¼ï¼‰è¡¨ç¤ºç‚º **$B** .å°æ–¼é€™äº›ç‰¹æ®Šæ¨™è¨˜å’Œå¥å­ï¼Œæˆ‘å€‘é‚„éœ€è¦ä½¿ç”¨åœ¨å†’è™Ÿå¾ŒæŒ‡å®šç›¸æ‡‰çš„æ¨™è¨˜é¡å‹ IDã€‚

å› æ­¤ç¶“å…¸çš„ BERT æ¨¡æ¿å®šç¾©å¦‚ä¸‹ï¼š

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

è«‹æ³¨æ„ï¼Œæˆ‘å€‘éœ€è¦å‚³éç‰¹æ®Šæ¨™è¨˜çš„ IDï¼Œä»¥ä¾¿æ¨™è¨˜å™¨å¯ä»¥æ­£ç¢ºåœ°å°‡ç‰¹æ®Šæ¨™è¨˜è½‰æ›ç‚ºå®ƒå€‘çš„ IDã€‚

æ·»åŠ å¾Œï¼Œæˆ‘å€‘ä¹‹å‰çš„ç¤ºä¾‹å°‡è¼¸å‡ºå‡ºï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

åœ¨ä¸€å°å¥å­ä¸­ï¼Œæˆ‘å€‘å¾—åˆ°äº†æ­£ç¢ºçš„çµæœï¼š
```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘å€‘å¹¾ä¹å¾é ­é–‹å§‹æ§‹å»ºäº†é€™å€‹æ¨™è¨˜å™¨â€”â€”ä½†æ˜¯é‚„æœ‰æœ€å¾Œä¸€æ­¥æ˜¯æŒ‡å®šä¸€å€‹è§£ç¢¼å™¨ï¼š

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

è®“æˆ‘å€‘æ¸¬è©¦ä¸€ä¸‹æˆ‘å€‘ä¹‹å‰çš„ **encoding** ï¼š

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

å¾ˆå¥½ï¼æˆ‘å€‘å¯ä»¥å°‡æ¨™è¨˜å™¨ä¿å­˜åœ¨ä¸€å€‹ JSON æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.save("tokenizer.json")
```

ç„¶å¾Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨**from_file()** æ–¹æ³•å¾è©²æ–‡ä»¶è£¡é‡æ–°åŠ è¼‰ **Tokenizer** å°è±¡ï¼š

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

è¦åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨é€™å€‹æ¨™è¨˜å™¨ï¼Œæˆ‘å€‘å¿…é ˆå°‡å®ƒåŒ…è£¹åœ¨ä¸€å€‹ **PreTrainedTokenizerFast** é¡ä¸­ã€‚æˆ‘å€‘å¯ä»¥ä½¿ç”¨æ³›å‹é¡ï¼Œæˆ–è€…ï¼Œå¦‚æœæˆ‘å€‘çš„æ¨™è¨˜å™¨å°æ‡‰æ–¼ç¾æœ‰æ¨¡å‹ï¼Œå‰‡ä½¿ç”¨è©²é¡ï¼ˆä¾‹å¦‚é€™è£¡çš„ **BertTokenizerFast** ï¼‰ã€‚å¦‚æœæ‚¨æ‡‰ç”¨æœ¬èª²ä¾†æ§‹å»ºå…¨æ–°çš„æ¨™è¨˜å™¨ï¼Œå‰‡å¿…é ˆä½¿ç”¨ç¬¬ä¸€å€‹é¸é …ã€‚

è¦å°‡æ¨™è¨˜å™¨åŒ…è£åœ¨ `PreTrainedTokenizerFast` é¡ä¸­ï¼Œæˆ‘å€‘å¯ä»¥å°‡æˆ‘å€‘æ§‹å»ºçš„æ¨™è¨˜å™¨ä½œç‚º`tokenizer_object` å‚³éï¼Œæˆ–è€…å°‡æˆ‘å€‘ä¿å­˜ç‚º`tokenizer_file` çš„æ¨™è¨˜å™¨æ–‡ä»¶å‚³éã€‚ è¦è¨˜ä½çš„é—œéµæ˜¯æˆ‘å€‘å¿…é ˆæ‰‹å‹•è¨­ç½®æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜ï¼Œå› ç‚ºè©²é¡ç„¡æ³•å¾ `tokenizer` å°è±¡æ¨æ–·å‡ºå“ªå€‹æ¨™è¨˜æ˜¯æ©ç¢¼æ¨™è¨˜ã€`[CLS]` æ¨™è¨˜ç­‰ï¼š

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

å¦‚æœæ‚¨ä½¿ç”¨ç‰¹å®šçš„æ¨™è¨˜å™¨é¡ï¼ˆä¾‹å¦‚ **BertTokenizerFast** )ï¼Œæ‚¨åªéœ€è¦æŒ‡å®šèˆ‡é»˜èªæ¨™è¨˜ä¸åŒçš„ç‰¹æ®Šæ¨™è¨˜ï¼ˆæ­¤è™•æ²’æœ‰ï¼‰ï¼š

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

ç„¶å¾Œï¼Œæ‚¨å¯ä»¥åƒä½¿ç”¨ä»»ä½•å…¶ä»– ğŸ¤— Transformers æ¨™è¨˜å™¨ä¸€æ¨£ä½¿ç”¨æ­¤æ¨™è¨˜å™¨ã€‚ä½ å¯ä»¥ç”¨ **save_pretrained()** æ–¹æ³•ï¼Œæˆ–ä½¿ç”¨ **push_to_hub()** æ–¹æ³•ã€‚

ç¾åœ¨æˆ‘å€‘å·²ç¶“ç­è§£ç­å¦‚ä½•æ§‹å»º WordPiece æ¨™è¨˜å™¨ï¼Œè®“æˆ‘å€‘å° BPE æ¨™è¨˜å™¨é€²è¡ŒåŒæ¨£çš„æ“ä½œã€‚å› ç‚ºæ‚¨å·²ç¶“çŸ¥é“äº†æ‰€æœ‰æ­¥é©Ÿï¼Œæ‰€ä»¥æˆ‘å€‘æœƒé€²è¡Œåœ°æ›´å¿«ä¸€é»ï¼Œä¸¦ä¸”åªçªå‡ºå±•ç¤ºå…©è€…ä¸ä¸€æ¨£çš„åœ°æ–¹ã€‚

## å¾é ­é–‹å§‹æ§‹å»º BPE æ¨™è¨˜å™¨

ç¾åœ¨è®“æˆ‘å€‘æ§‹å»ºä¸€å€‹ GPT-2 æ¨™è¨˜å™¨ã€‚èˆ‡ BERT æ¨™è¨˜å™¨ä¸€æ¨£ï¼Œæˆ‘å€‘é¦–å…ˆä½¿ç”¨ **Tokenizer** åˆå§‹åŒ–ä¸€å€‹BPE æ¨¡å‹ï¼š

```python
tokenizer = Tokenizer(models.BPE())
```

å’Œ BERT ä¸€æ¨£ï¼Œå¦‚æœæˆ‘å€‘æœ‰ä¸€å€‹è©å½™è¡¨ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ä¸€å€‹è©å½™è¡¨ä¾†åˆå§‹åŒ–é€™å€‹æ¨¡å‹ï¼ˆåœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘éœ€è¦å‚³é `vocab` å’Œ `merges`ï¼‰ï¼Œä½†æ˜¯ç”±æ–¼æˆ‘å€‘å°‡å¾é ­é–‹å§‹è¨“ç·´ï¼Œæ‰€ä»¥æˆ‘å€‘ä¸éœ€è¦é€™æ¨£å»åšã€‚ æˆ‘å€‘ä¹Ÿä¸éœ€è¦æŒ‡å®šâ€œunk_tokenâ€ï¼Œå› ç‚º GPT-2 ä½¿ç”¨çš„å­—ç¯€ç´š BPEï¼Œä¸éœ€è¦â€œunk_tokenâ€ã€‚

GPT-2 ä¸ä½¿ç”¨æ­¸ä¸€åŒ–å™¨ï¼Œå› æ­¤æˆ‘å€‘è·³éè©²æ­¥é©Ÿä¸¦ç›´æ¥é€²å…¥é æ¨™è¨˜åŒ–ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

æˆ‘å€‘åœ¨æ­¤è™•æ·»åŠ åˆ° `ByteLevel` çš„é¸é …æ˜¯ä¸åœ¨å¥å­é–‹é ­æ·»åŠ ç©ºæ ¼ï¼ˆé»˜èªç‚ºtureï¼‰ã€‚ æˆ‘å€‘å¯ä»¥çœ‹ä¸€ä¸‹ä½¿ç”¨é€™å€‹æ¨™è¨˜å™¨å°ä¹‹å‰ç¤ºä¾‹æ–‡æœ¬çš„é æ¨™è¨˜ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

æ¥ä¸‹ä¾†æ˜¯éœ€è¦è¨“ç·´çš„æ¨¡å‹ã€‚å°æ–¼ GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Šæ¨™è¨˜æ˜¯æ–‡æœ¬çµæŸæ¨™è¨˜ï¼š

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

èˆ‡ `WordPieceTrainer` ä»¥åŠ `vocab_size` å’Œ `special_tokens` ä¸€æ¨£ï¼Œæˆ‘å€‘å¯ä»¥æŒ‡å®š `min_frequency` å¦‚æœæˆ‘å€‘é¡˜æ„ï¼Œæˆ–è€…å¦‚æœæˆ‘å€‘æœ‰ä¸€å€‹è©å°¾å¾Œç¶´ï¼ˆå¦‚ `</w>` )ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ `end_of_word_suffix` è¨­ç½®å®ƒã€‚

é€™å€‹æ¨™è¨˜å™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¨“ç·´ï¼š

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®“æˆ‘å€‘çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ¨™è¨˜åŒ–å¾Œçš„çµæœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

æˆ‘å€‘å° GPT-2 æ¨™è¨˜å™¨æ·»åŠ å­—ç¯€ç´šå¾Œè™•ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

`trim_offsets = False` é¸é …æŒ‡ç¤ºæˆ‘å€‘æ‡‰è©²ä¿ç•™ä»¥ 'Ä ' é–‹é ­çš„æ¨™è¨˜çš„åç§»é‡ï¼šé€™æ¨£åç§»é‡çš„é–‹é ­å°‡æŒ‡å‘å–®è©ä¹‹å‰çš„ç©ºæ ¼ï¼Œè€Œä¸æ˜¯ç¬¬ä¸€å€‹å–®è©çš„å­—ç¬¦ï¼ˆå› ç‚ºç©ºæ ¼åœ¨æŠ€è¡“ä¸Šæ˜¯æ¨™è¨˜çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚ è®“æˆ‘å€‘çœ‹çœ‹æˆ‘å€‘å‰›å‰›ç·¨ç¢¼çš„æ–‡æœ¬çš„çµæœï¼Œå…¶ä¸­ `'Ä test'` æ˜¯ç´¢å¼•ç¬¬ 4 è™•çš„æ¨™è¨˜ï¼š

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

æœ€å¾Œï¼Œæˆ‘å€‘æ·»åŠ ä¸€å€‹å­—ç¯€ç´šè§£ç¢¼å™¨ï¼š

```python
tokenizer.decoder = decoders.ByteLevel()
```

æˆ‘å€‘å¯ä»¥ä»”ç´°æª¢æŸ¥å®ƒæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

å¾ˆå¥½ï¼ç¾åœ¨æˆ‘å€‘å®Œæˆäº†ï¼Œæˆ‘å€‘å¯ä»¥åƒä»¥å‰ä¸€æ¨£ä¿å­˜æ¨™è¨˜å™¨ï¼Œä¸¦å°‡å®ƒåŒ…è£åœ¨ä¸€å€‹ **PreTrainedTokenizerFast** æˆ–è€… **GPT2TokenizerFast** å¦‚æœæˆ‘å€‘æƒ³åœ¨ ğŸ¤— Transformersä¸­ä½¿ç”¨å®ƒï¼š

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

æˆ–è€…ï¼š

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

ä½œç‚ºæœ€å¾Œä¸€å€‹ç¤ºä¾‹ï¼Œæˆ‘å€‘å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•å¾é ­é–‹å§‹æ§‹å»º Unigram æ¨™è¨˜å™¨ã€‚

## å¾é ­é–‹å§‹æ§‹å»º Unigram æ¨™è¨˜å™¨

ç¾åœ¨è®“æˆ‘å€‘æ§‹å»ºä¸€å€‹ XLNet æ¨™è¨˜å™¨ã€‚èˆ‡ä¹‹å‰çš„æ¨™è¨˜å™¨ä¸€æ¨£ï¼Œæˆ‘å€‘é¦–å…ˆä½¿ç”¨ Unigram æ¨¡å‹åˆå§‹åŒ–ä¸€å€‹ **Tokenizer** ï¼š

```python
tokenizer = Tokenizer(models.Unigram())
```

åŒæ¨£ï¼Œå¦‚æœæˆ‘å€‘æœ‰è©å½™è¡¨ï¼Œæˆ‘å€‘å¯ä»¥ç”¨è©å½™è¡¨åˆå§‹åŒ–é€™å€‹æ¨¡å‹ã€‚

å°æ–¼æ¨™æº–åŒ–ï¼ŒXLNet ä½¿ç”¨äº†ä¸€äº›æ›¿æ›çš„æ–¹æ³•ï¼ˆä¾†è‡ª SentencePieceï¼‰ï¼š

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

é€™æœƒå–ä»£ **â€œ** å’Œ **â€** å’Œ **â€** ä»¥åŠä»»ä½•å…©å€‹æˆ–å¤šå€‹ç©ºæ ¼èˆ‡å–®å€‹ç©ºæ ¼çš„åºåˆ—ï¼Œä»¥åŠåˆªé™¤æ–‡æœ¬ä¸­çš„é‡éŸ³ä»¥é€²è¡Œæ¨™è¨˜ã€‚

ç”¨æ–¼ä»»ä½• SentencePiece æ¨™è¨˜å™¨çš„é æ¨™è¨˜å™¨æ˜¯ `Metaspace`ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

æˆ‘å€‘å¯ä»¥åƒä»¥å‰ä¸€æ¨£æŸ¥çœ‹ç¤ºä¾‹æ–‡æœ¬çš„é æ¨™è¨˜åŒ–ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("â–Let's", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', (14, 29))]
```

æ¥ä¸‹ä¾†æ˜¯éœ€è¦è¨“ç·´çš„æ¨¡å‹ã€‚ XLNet æœ‰ä¸å°‘ç‰¹æ®Šçš„æ¨™è¨˜ï¼š

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

ä¸è¦å¿˜è¨˜`UnigramTrainer` çš„ä¸€å€‹éå¸¸é‡è¦çš„åƒæ•¸æ˜¯`unk_token`ã€‚ æˆ‘å€‘é‚„å¯ä»¥å‚³éç‰¹å®šæ–¼ Unigram ç®—æ³•çš„å…¶ä»–åƒæ•¸ï¼Œä¾‹å¦‚åˆªé™¤æ¨™è¨˜çš„æ¯å€‹æ­¥é©Ÿçš„â€œshrinking_factorï¼ˆæ”¶ç¸®å› å­ï¼‰â€ï¼ˆé»˜èªç‚º 0.75ï¼‰æˆ–æŒ‡å®šçµ¦å®šæ¨™è¨˜çš„æœ€å¤§é•·åº¦çš„â€œmax_piece_lengthâ€ï¼ˆé»˜èªç‚º 16ï¼‰ .

é€™å€‹æ¨™è¨˜å™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¨“ç·´ï¼š

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®“æˆ‘å€‘çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ¨™è¨˜åŒ–å¾Œçš„çµæœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']
```

A peculiarity of XLNet is that it puts the `<cls>` token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens). It's padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the `<cls>` and `<sep>` tokens:
XLNet çš„ä¸€å€‹ç‰¹é»æ˜¯å®ƒå°‡`<cls>` æ¨™è¨˜æ”¾åœ¨å¥å­çš„æœ«å°¾ï¼Œé¡å‹ID ç‚º2ï¼ˆä»¥å°‡å…¶èˆ‡å…¶ä»–æ¨™è¨˜å€åˆ†é–‹ä¾†ï¼‰ã€‚å®ƒæœƒå°‡çµæœå¡«å……åœ¨å·¦å´ã€‚ æˆ‘å€‘å¯ä»¥ä½¿ç”¨æ¨¡æ¿è™•ç†æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜å’Œæ¨™è¨˜é¡å‹ IDï¼Œä¾‹å¦‚ BERTï¼Œä½†é¦–å…ˆæˆ‘å€‘å¿…é ˆç²å– `<cls>` å’Œ `<sep>` æ¨™è¨˜çš„ IDï¼š

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

æ¨¡æ¿å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

æˆ‘å€‘å¯ä»¥é€šéç·¨ç¢¼ä¸€å°å¥å­ä¾†æ¸¬è©¦å®ƒçš„å·¥ä½œåŸç†ï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

æœ€å¾Œï¼Œæˆ‘å€‘æ·»åŠ ä¸€å€‹ **Metaspace** è§£ç¢¼å™¨ï¼š

```python
tokenizer.decoder = decoders.Metaspace()
```

æˆ‘å€‘å®Œæˆäº†é€™å€‹æ¨™è¨˜å™¨ï¼ æˆ‘å€‘å¯ä»¥åƒä»¥å‰ä¸€æ¨£ä¿å­˜æ¨™è¨˜å™¨ï¼Œå¦‚æœæˆ‘å€‘æƒ³åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨å®ƒï¼Œå¯ä»¥å°‡å®ƒåŒ…è£åœ¨ `PreTrainedTokenizerFast` æˆ– `XLNetTokenizerFast` ä¸­ã€‚ ä½¿ç”¨ `PreTrainedTokenizerFast` æ™‚è¦æ³¨æ„çš„ä¸€ä»¶äº‹æ˜¯ï¼Œæˆ‘å€‘éœ€è¦å‘Šè¨´ğŸ¤— Transformers åº«æ‡‰è©²åœ¨å·¦å´å¡«å……ç‰¹æ®Šæ¨™è¨˜ï¼š
```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

æˆ–è€…ï¼š

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

ç¾åœ¨æ‚¨å·²ç¶“ç­è§£ç­å¦‚ä½•ä½¿ç”¨å„ç¨®æ§‹å»ºå¡Šä¾†æ§‹å»ºç¾æœ‰çš„æ¨™è¨˜å™¨ï¼Œæ‚¨æ‡‰è©²èƒ½å¤ ä½¿ç”¨ ğŸ¤— tokenizeråº«ç·¨å¯«æ‚¨æƒ³è¦çš„ä»»ä½•æ¨™è¨˜å™¨ï¼Œä¸¦èƒ½å¤ åœ¨ ğŸ¤— Transformersä¸­ä½¿ç”¨å®ƒã€‚