# ä¸€å€‹å®Œæ•´çš„è¨“ç·´

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section4.ipynb"},
]} />

<Youtube id="Dh9CL8fyG80"/>

ç¾åœ¨ï¼Œæˆ‘å€‘å°‡ç­è§£å¦‚ä½•åœ¨ä¸ä½¿ç”¨`Trainer`é¡çš„æƒ…æ³ä¸‹ç²å¾—èˆ‡ä¸Šä¸€ç¯€ç›¸åŒçš„çµæœã€‚åŒæ¨£ï¼Œæˆ‘å€‘å‡è¨­æ‚¨å·²ç¶“å­¸ç¿’äº†ç¬¬ 2 ç¯€ä¸­çš„æ•¸æ“šè™•ç†ã€‚ä¸‹é¢æ˜¯ä¸€å€‹ç°¡çŸ­çš„ç¸½çµï¼Œæ¶µè“‹äº†æ‚¨éœ€è¦çš„æ‰€æœ‰å…§å®¹:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### è¨“ç·´å‰çš„æº–å‚™

åœ¨å¯¦éš›ç·¨å¯«æˆ‘å€‘çš„è¨“ç·´å¾ªç’°ä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å®šç¾©ä¸€äº›å°è±¡ã€‚ç¬¬ä¸€å€‹æ˜¯æˆ‘å€‘å°‡ç”¨æ–¼è¿­ä»£æ‰¹æ¬¡çš„æ•¸æ“šåŠ è¼‰å™¨ã€‚æˆ‘å€‘éœ€è¦å°æˆ‘å€‘çš„`tokenized_datasets`åšä¸€äº›è™•ç†ï¼Œä¾†è™•ç†`Trainer`è‡ªå‹•ç‚ºæˆ‘å€‘åšçš„ä¸€äº›äº‹æƒ…ã€‚å…·é«”ä¾†èªªï¼Œæˆ‘å€‘éœ€è¦:

- åˆªé™¤èˆ‡æ¨¡å‹ä¸æœŸæœ›çš„å€¼ç›¸å°æ‡‰çš„åˆ—ï¼ˆå¦‚`sentence1`å’Œ`sentence2`åˆ—ï¼‰ã€‚
- å°‡åˆ—å`label`é‡å‘½åç‚º`labels`ï¼ˆå› ç‚ºæ¨¡å‹æœŸæœ›åƒæ•¸æ˜¯`labels`ï¼‰ã€‚
- è¨­ç½®æ•¸æ“šé›†çš„æ ¼å¼ï¼Œä½¿å…¶è¿”å› PyTorch å¼µé‡è€Œä¸æ˜¯åˆ—è¡¨ã€‚

é‡å°ä¸Šé¢çš„æ¯å€‹æ­¥é©Ÿï¼Œæˆ‘å€‘çš„ `tokenized_datasets` éƒ½æœ‰ä¸€å€‹æ–¹æ³•:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ç„¶å¾Œï¼Œæˆ‘å€‘å¯ä»¥æª¢æŸ¥çµæœä¸­æ˜¯å¦åªæœ‰æ¨¡å‹èƒ½å¤ æ¥å—çš„åˆ—:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

è‡³æ­¤ï¼Œæˆ‘å€‘å¯ä»¥è¼•é¬†å®šç¾©æ•¸æ“šåŠ è¼‰å™¨:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ç‚ºäº†å¿«é€Ÿæª¢é©—æ•¸æ“šè™•ç†ä¸­æ²’æœ‰éŒ¯èª¤ï¼Œæˆ‘å€‘å¯ä»¥é€™æ¨£æª¢é©—å…¶ä¸­çš„ä¸€å€‹æ‰¹æ¬¡:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

è«‹æ³¨æ„ï¼Œå¯¦éš›çš„å½¢ç‹€å¯èƒ½èˆ‡æ‚¨ç•¥æœ‰ä¸åŒï¼Œå› ç‚ºæˆ‘å€‘ç‚ºè¨“ç·´æ•¸æ“šåŠ è¼‰å™¨è¨­ç½®äº†`shuffle=True`ï¼Œä¸¦ä¸”æ¨¡å‹æœƒå°‡å¥å­å¡«å……åˆ°`batch`ä¸­çš„æœ€å¤§é•·åº¦ã€‚

ç¾åœ¨æˆ‘å€‘å·²ç¶“å®Œå…¨å®Œæˆäº†æ•¸æ“šé è™•ç†ï¼ˆå°æ–¼ä»»ä½• ML å¾æ¥­è€…ä¾†èªªéƒ½æ˜¯ä¸€å€‹ä»¤äººæ»¿æ„ä½†é›£ä»¥å¯¦ç¾çš„ç›®æ¨™ï¼‰ï¼Œè®“æˆ‘å€‘å°‡æ³¨æ„åŠ›è½‰å‘æ¨¡å‹ã€‚æˆ‘å€‘å®Œå…¨åƒåœ¨ä¸Šä¸€ç¯€ä¸­æ‰€åšçš„é‚£æ¨£å¯¦ä¾‹åŒ–å®ƒ:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```
ç‚ºäº†ç¢ºä¿è¨“ç·´éç¨‹ä¸­ä¸€åˆ‡é †åˆ©ï¼Œæˆ‘å€‘å°‡`batch`å‚³éçµ¦é€™å€‹æ¨¡å‹:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ç•¶æˆ‘å€‘æä¾› `labels` æ™‚ï¼Œ ğŸ¤— Transformers æ¨¡å‹éƒ½å°‡è¿”å›é€™å€‹`batch`çš„`loss`ï¼Œæˆ‘å€‘é‚„å¾—åˆ°äº† `logits`(`batch`ä¸­çš„æ¯å€‹è¼¸å…¥æœ‰å…©å€‹ï¼Œæ‰€ä»¥å¼µé‡å¤§å°ç‚º 8 x 2)ã€‚

æˆ‘å€‘å¹¾ä¹æº–å‚™å¥½ç·¨å¯«æˆ‘å€‘çš„è¨“ç·´å¾ªç’°äº†ï¼æˆ‘å€‘åªæ˜¯ç¼ºå°‘å…©ä»¶äº‹ï¼šå„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨ã€‚ç”±æ–¼æˆ‘å€‘è©¦åœ–è‡ªè¡Œå¯¦ç¾ `Trainer`çš„åŠŸèƒ½ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ç›¸åŒçš„å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨ã€‚`Trainer` ä½¿ç”¨çš„å„ªåŒ–å™¨æ˜¯ `AdamW` , èˆ‡ `Adam` ç›¸åŒï¼Œä½†åœ¨æ¬Šé‡è¡°æ¸›æ­£å‰‡åŒ–æ–¹é¢æœ‰æ‰€ä¸åŒ(åƒè¦‹[â€œDecoupled Weight Decay Regularizationâ€](https://arxiv.org/abs/1711.05101)ä½œè€…:Ilya Loshchilov å’Œ Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

æœ€å¾Œï¼Œé»˜èªä½¿ç”¨çš„å­¸ç¿’ç‡èª¿åº¦å™¨åªæ˜¯å¾æœ€å¤§å€¼ (5e-5) åˆ° 0 çš„ç·šæ€§è¡°æ¸›ã€‚ ç‚ºäº†å®šç¾©å®ƒï¼Œæˆ‘å€‘éœ€è¦çŸ¥é“æˆ‘å€‘è¨“ç·´çš„æ¬¡æ•¸ï¼Œå³æ‰€æœ‰æ•¸æ“šè¨“ç·´çš„æ¬¡æ•¸(epochs)ä¹˜ä»¥çš„æ•¸æ“šé‡ï¼ˆé€™æ˜¯æˆ‘å€‘æ‰€æœ‰è¨“ç·´æ•¸æ“šçš„æ•¸é‡ï¼‰ã€‚`Trainer`é»˜èªæƒ…æ³ä¸‹ä½¿ç”¨ä¸‰å€‹`epochs`ï¼Œå› æ­¤æˆ‘å€‘å®šç¾©è¨“ç·´éç¨‹å¦‚ä¸‹:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### è¨“ç·´å¾ªç’°

æœ€å¾Œä¸€ä»¶äº‹ï¼šå¦‚æœæˆ‘å€‘å¯ä»¥è¨ªå• GPU,æˆ‘å€‘å°‡å¸Œæœ›ä½¿ç”¨ GPU(åœ¨ CPU ä¸Šï¼Œè¨“ç·´å¯èƒ½éœ€è¦å¹¾å€‹å°æ™‚è€Œä¸æ˜¯å¹¾åˆ†é˜)ã€‚ç‚ºæ­¤ï¼Œæˆ‘å€‘å®šç¾©äº†ä¸€å€‹ `device`,å®ƒåœ¨GPUå¯ç”¨çš„æƒ…æ³ä¸‹æŒ‡å‘GPU æˆ‘å€‘å°‡æŠŠæˆ‘å€‘çš„æ¨¡å‹å’Œ`batche`æ”¾åœ¨`device`ä¸Š:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

æˆ‘å€‘ç¾åœ¨æº–å‚™å¥½è¨“ç·´äº†ï¼ç‚ºäº†ç­è§£è¨“ç·´ä½•æ™‚çµæŸï¼Œæˆ‘å€‘ä½¿ç”¨ `tqdm` åº«,åœ¨è¨“ç·´æ­¥é©Ÿæ•¸ä¸Šæ·»åŠ äº†ä¸€å€‹é€²åº¦æ¢:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

æ‚¨å¯ä»¥çœ‹åˆ°è¨“ç·´å¾ªç’°çš„æ ¸å¿ƒèˆ‡ä»‹ç´¹ä¸­çš„éå¸¸ç›¸ä¼¼ã€‚æˆ‘å€‘æ²’æœ‰è¦æ±‚ä»»ä½•æª¢é©—ï¼Œæ‰€ä»¥é€™å€‹è¨“ç·´å¾ªç’°ä¸æœƒå‘Šè¨´æˆ‘å€‘ä»»ä½•é—œæ–¼æ¨¡å‹ç›®å‰çš„ç‹€æ…‹ã€‚æˆ‘å€‘éœ€è¦ç‚ºæ­¤æ·»åŠ ä¸€å€‹è©•ä¼°å¾ªç’°ã€‚


### è©•ä¼°å¾ªç’°

æ­£å¦‚æˆ‘å€‘ä¹‹å‰æ‰€åšçš„é‚£æ¨£ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ ğŸ¤— Evaluate åº«æä¾›çš„æŒ‡æ¨™ã€‚æˆ‘å€‘å·²ç¶“ç­è§£äº† `metric.compute()` æ–¹æ³•ï¼Œç•¶æˆ‘å€‘ä½¿ç”¨ `add_batch()`æ–¹æ³•é€²è¡Œé æ¸¬å¾ªç’°æ™‚ï¼Œå¯¦éš›ä¸Šè©²æŒ‡æ¨™å¯ä»¥ç‚ºæˆ‘å€‘ç´¯ç©æ‰€æœ‰ `batch` çš„çµæœã€‚ä¸€æ—¦æˆ‘å€‘ç´¯ç©äº†æ‰€æœ‰ `batch` ï¼Œæˆ‘å€‘å°±å¯ä»¥ä½¿ç”¨ `metric.compute()` å¾—åˆ°æœ€çµ‚çµæœ .ä»¥ä¸‹æ˜¯åœ¨è©•ä¼°å¾ªç’°ä¸­å¯¦ç¾æ‰€æœ‰é€™äº›çš„æ–¹æ³•:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

åŒæ¨£ï¼Œç”±æ–¼æ¨¡å‹é ­éƒ¨åˆå§‹åŒ–å’Œæ•¸æ“šæ”¹çµ„çš„éš¨æ©Ÿæ€§ï¼Œæ‚¨çš„çµæœæœƒç•¥æœ‰ä¸åŒï¼Œä½†å®ƒå€‘æ‡‰è©²åœ¨åŒä¸€å€‹ç¯„åœå…§ã€‚

<Tip>

âœï¸ **è©¦è©¦çœ‹ï¼** ä¿®æ”¹ä¹‹å‰çš„è¨“ç·´å¾ªç’°ä»¥åœ¨ SST-2 æ•¸æ“šé›†ä¸Šå¾®èª¿æ‚¨çš„æ¨¡å‹ã€‚

</Tip>

### Sä½¿ç”¨ğŸ¤— AccelerateåŠ é€Ÿæ‚¨çš„è¨“ç·´å¾ªç’°

<Youtube id="s7dy8QRgjJ0" />

æˆ‘å€‘ä¹‹å‰å®šç¾©çš„è¨“ç·´å¾ªç’°åœ¨å–®å€‹ CPU æˆ– GPU ä¸Šé‹è¡Œè‰¯å¥½ã€‚ä½†æ˜¯ä½¿ç”¨[ğŸ¤— Accelerate](https://github.com/huggingface/accelerate)åº«ï¼Œåªéœ€é€²è¡Œä¸€äº›èª¿æ•´ï¼Œæˆ‘å€‘å°±å¯ä»¥åœ¨å¤šå€‹ GPU æˆ– TPU ä¸Šå•Ÿç”¨åˆ†ä½ˆå¼è¨“ç·´ã€‚å¾å‰µå»ºè¨“ç·´å’Œé©—è­‰æ•¸æ“šåŠ è¼‰å™¨é–‹å§‹ï¼Œæˆ‘å€‘çš„æ‰‹å‹•è¨“ç·´å¾ªç’°å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ä»¥ä¸‹æ˜¯è®ŠåŒ–:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

è¦æ·»åŠ çš„ç¬¬ä¸€è¡Œæ˜¯å°å…¥`Accelerator`ã€‚ç¬¬äºŒè¡Œå¯¦ä¾‹åŒ–ä¸€å€‹ `Accelerator`å°è±¡ ï¼Œå®ƒå°‡æŸ¥çœ‹ç’°å¢ƒä¸¦åˆå§‹åŒ–é©ç•¶çš„åˆ†ä½ˆå¼è¨­ç½®ã€‚ ğŸ¤— Accelerate ç‚ºæ‚¨è™•ç†æ•¸æ“šåœ¨è¨­å‚™é–“çš„å‚³éï¼Œå› æ­¤æ‚¨å¯ä»¥åˆªé™¤å°‡æ¨¡å‹æ”¾åœ¨è¨­å‚™ä¸Šçš„é‚£è¡Œä»£ç¢¼ï¼ˆæˆ–è€…ï¼Œå¦‚æœæ‚¨é¡˜æ„ï¼Œå¯ä½¿ç”¨ `accelerator.device` ä»£æ›¿ `device` ï¼‰ã€‚

ç„¶å¾Œå¤§éƒ¨åˆ†å·¥ä½œæœƒåœ¨å°‡æ•¸æ“šåŠ è¼‰å™¨ã€æ¨¡å‹å’Œå„ªåŒ–å™¨ç™¼é€åˆ°çš„`accelerator.prepare()`ä¸­å®Œæˆã€‚é€™å°‡æœƒæŠŠé€™äº›å°è±¡åŒ…è£åœ¨é©ç•¶çš„å®¹å™¨ä¸­ï¼Œä»¥ç¢ºä¿æ‚¨çš„åˆ†ä½ˆå¼è¨“ç·´æŒ‰é æœŸå·¥ä½œã€‚è¦é€²è¡Œçš„å…¶é¤˜æ›´æ”¹æ˜¯åˆªé™¤å°‡`batch`æ”¾åœ¨ `device` çš„é‚£è¡Œä»£ç¢¼ï¼ˆåŒæ¨£ï¼Œå¦‚æœæ‚¨æƒ³ä¿ç•™å®ƒï¼Œæ‚¨å¯ä»¥å°‡å…¶æ›´æ”¹ç‚ºä½¿ç”¨ `accelerator.device` ) ä¸¦å°‡ `loss.backward()` æ›¿æ›ç‚º`accelerator.backward(loss)`ã€‚

<Tip>
âš ï¸ ç‚ºäº†ä½¿é›²ç«¯ TPU æä¾›çš„åŠ é€Ÿç™¼æ®æœ€å¤§çš„æ•ˆç›Šï¼Œæˆ‘å€‘å»ºè­°ä½¿ç”¨æ¨™è¨˜å™¨(tokenizer)çš„ `padding=max_length` å’Œ `max_length` åƒæ•¸å°‡æ‚¨çš„æ¨£æœ¬å¡«å……åˆ°å›ºå®šé•·åº¦ã€‚
</Tip>

å¦‚æœæ‚¨æƒ³è¤‡è£½ä¸¦ç²˜è²¼ä¾†ç›´æ¥é‹è¡Œï¼Œä»¥ä¸‹æ˜¯ ğŸ¤— Accelerate çš„å®Œæ•´è¨“ç·´å¾ªç’°:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

æŠŠé€™å€‹æ”¾åœ¨ `train.py` æ–‡ä»¶ä¸­ï¼Œå¯ä»¥è®“å®ƒåœ¨ä»»ä½•é¡å‹çš„åˆ†ä½ˆå¼è¨­ç½®ä¸Šé‹è¡Œã€‚è¦åœ¨åˆ†ä½ˆå¼è¨­ç½®ä¸­è©¦ç”¨å®ƒï¼Œè«‹é‹è¡Œä»¥ä¸‹å‘½ä»¤:

```bash
accelerate config
```

é€™å°‡è©¢å•æ‚¨å¹¾å€‹é…ç½®çš„å•é¡Œä¸¦å°‡æ‚¨çš„å›ç­”è½‰å„²åˆ°æ­¤å‘½ä»¤ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ä¸­:

```
accelerate launch train.py
```

é€™å°‡å•Ÿå‹•åˆ†ä½ˆå¼è¨“ç·´

é€™å°‡å•Ÿå‹•åˆ†ä½ˆå¼è¨“ç·´ã€‚å¦‚æœæ‚¨æƒ³åœ¨ Notebook ä¸­å˜—è©¦æ­¤æ“ä½œï¼ˆä¾‹å¦‚ï¼Œåœ¨ Colab ä¸Šä½¿ç”¨ TPU é€²è¡Œæ¸¬è©¦ï¼‰ï¼Œåªéœ€å°‡ä»£ç¢¼ç²˜è²¼åˆ° `training_function()` ä¸¦ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é‹è¡Œæœ€å¾Œä¸€å€‹å–®å…ƒæ ¼:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

æ‚¨å¯ä»¥åœ¨[ğŸ¤— Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples)æ‰¾åˆ°æ›´å¤šçš„ç¤ºä¾‹ã€‚
