<FrameworkSwitchCourse {fw} />

# è™•ç†æ•¸æ“š

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
é€™ä¸€å°ç¯€å­¸ç¿’[ç¬¬ä¸€å°ç¯€](/course/chapter2)ä¸­æåˆ°çš„ã€Œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å¤§å‹æ•¸æ“šé›†ã€ï¼Œä¸‹é¢æ˜¯æˆ‘å€‘ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•¸æ“šåœ¨ PyTorch ä¸Šè¨“ç·´å¥å­åˆ†é¡å™¨çš„ä¸€å€‹ä¾‹å­ï¼š

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```
{:else}
é€™ä¸€å°ç¯€å­¸ç¿’[ç¬¬ä¸€å°ç¯€](/course/chapter2)ä¸­æåˆ°çš„ã€Œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å¤§å‹æ•¸æ“šé›†ã€ï¼Œä¸‹é¢æ˜¯æˆ‘å€‘ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•¸æ“šåœ¨ TensorFlow ä¸Šè¨“ç·´å¥å­åˆ†é¡å™¨çš„ä¸€å€‹ä¾‹å­ï¼š

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```
{/if}

ç•¶ç„¶ï¼Œåƒ…åƒ…ç”¨å…©å¥è©±è¨“ç·´æ¨¡å‹ä¸æœƒç”¢ç”Ÿå¾ˆå¥½çš„æ•ˆæœã€‚ç‚ºäº†ç²å¾—æ›´å¥½çš„çµæœï¼Œæ‚¨éœ€è¦æº–å‚™ä¸€å€‹æ›´å¤§çš„æ•¸æ“šé›†ã€‚

åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨MRPCï¼ˆå¾®è»Ÿç ”ç©¶é‡‹ç¾©èªæ–™åº«ï¼‰æ•¸æ“šé›†ä½œç‚ºç¤ºä¾‹ï¼Œè©²æ•¸æ“šé›†ç”±å¨å»‰Â·å¤šè˜­å’Œå…‹é‡Œæ–¯Â·å¸ƒç¾…å…‹ç‰¹åœ¨[é€™ç¯‡æ–‡ç« ](https://www.aclweb.org/anthology/I05-5002.pdf)ç™¼ä½ˆã€‚è©²æ•¸æ“šé›†ç”±5801å°å¥å­çµ„æˆï¼Œæ¯å€‹å¥å­å°å¸¶æœ‰ä¸€å€‹æ¨™ç±¤ï¼ŒæŒ‡ç¤ºå®ƒå€‘æ˜¯å¦ç‚ºåŒç¾©ï¼ˆå³ï¼Œå¦‚æœå…©å€‹å¥å­çš„æ„æ€ç›¸åŒï¼‰ã€‚æˆ‘å€‘åœ¨æœ¬ç« ä¸­é¸æ“‡äº†å®ƒï¼Œå› ç‚ºå®ƒæ˜¯ä¸€å€‹å°æ•¸æ“šé›†ï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“å°å®ƒé€²è¡Œè¨“ç·´ã€‚

### å¾æ¨¡å‹ä¸­å¿ƒï¼ˆHubï¼‰åŠ è¼‰æ•¸æ“šé›†

{#if fw === 'pt'}
<Youtube id="_BZearw7f0w"/>
{:else}
<Youtube id="W_gMJF0xomE"/>
{/if}

æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸åªæ˜¯åŒ…å«æ¨¡å‹ï¼›å®ƒä¹Ÿæœ‰è¨±å¤šä¸åŒèªè¨€çš„å¤šå€‹æ•¸æ“šé›†ã€‚é»æ“Š[æ•¸æ“šé›†](https://huggingface.co/datasets)çš„éˆæ¥å³å¯é€²è¡Œç€è¦½ã€‚æˆ‘å€‘å»ºè­°æ‚¨åœ¨é–±è®€æœ¬ç¯€å¾Œé–±è®€ä¸€ä¸‹[åŠ è¼‰å’Œè™•ç†æ–°çš„æ•¸æ“šé›†](https://huggingface.co/docs/datasets/loading)é€™ç¯‡æ–‡ç« ï¼Œé€™æœƒè®“æ‚¨å°huggingfaceçš„darasetsæ›´åŠ æ¸…æ™°ã€‚ä½†ç¾åœ¨ï¼Œè®“æˆ‘å€‘ä½¿ç”¨MRPCæ•¸æ“šé›†ä¸­çš„[GLUE åŸºæº–æ¸¬è©¦æ•¸æ“šé›†](https://gluebenchmark.com/)ï¼Œå®ƒæ˜¯æ§‹æˆMRPCæ•¸æ“šé›†çš„10å€‹æ•¸æ“šé›†ä¹‹ä¸€ï¼Œé€™æ˜¯ä¸€å€‹å­¸è¡“åŸºæº–ï¼Œç”¨æ–¼è¡¡é‡æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åœ¨10å€‹ä¸åŒæ–‡æœ¬åˆ†é¡ä»»å‹™ä¸­çš„æ€§èƒ½ã€‚

ğŸ¤— Datasetsåº«æä¾›äº†ä¸€å€‹éå¸¸ä¾¿æ·çš„å‘½ä»¤ï¼Œå¯ä»¥åœ¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸Šä¸‹è¼‰å’Œç·©å­˜æ•¸æ“šé›†ã€‚æˆ‘å€‘å¯ä»¥é€šéä»¥ä¸‹çš„ä»£ç¢¼ä¸‹è¼‰MRPCæ•¸æ“šé›†ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘å€‘ç²å¾—äº†ä¸€å€‹**DatasetDict**å°è±¡ï¼Œå…¶ä¸­åŒ…å«è¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†ã€‚æ¯ä¸€å€‹é›†åˆéƒ½åŒ…å«å¹¾å€‹åˆ—(**sentence1**, **sentence2**, **label**, and **idx**)ä»¥åŠä¸€å€‹ä»£è¡¨è¡Œæ•¸çš„è®Šé‡ï¼Œå³æ¯å€‹é›†åˆä¸­çš„è¡Œçš„å€‹æ•¸ï¼ˆå› æ­¤ï¼Œè¨“ç·´é›†ä¸­æœ‰3668å°å¥å­ï¼Œé©—è­‰é›†ä¸­æœ‰408å°ï¼Œæ¸¬è©¦é›†ä¸­æœ‰1725å°ï¼‰ã€‚

é»˜èªæƒ…æ³ä¸‹ï¼Œæ­¤å‘½ä»¤åœ¨ä¸‹è¼‰æ•¸æ“šé›†ä¸¦ç·©å­˜åˆ° **~/.cache/huggingface/dataset**. å›æƒ³ä¸€ä¸‹ç¬¬2ç« ï¼Œæ‚¨å¯ä»¥é€šéè¨­ç½®**HF_HOME**ç’°å¢ƒè®Šé‡ä¾†è‡ªå®šç¾©ç·©å­˜çš„æ–‡ä»¶å¤¾ã€‚

æˆ‘å€‘å¯ä»¥è¨ªå•æˆ‘å€‘æ•¸æ“šé›†ä¸­çš„æ¯ä¸€å€‹**raw_train_dataset**å°è±¡ï¼Œå¦‚ä½¿ç”¨å­—å…¸ï¼š

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

æˆ‘å€‘å¯ä»¥çœ‹åˆ°æ¨™ç±¤å·²ç¶“æ˜¯æ•´æ•¸äº†ï¼Œæ‰€ä»¥æˆ‘å€‘ä¸éœ€è¦å°æ¨™ç±¤åšä»»ä½•é è™•ç†ã€‚è¦çŸ¥é“å“ªå€‹æ•¸å­—å°æ‡‰æ–¼å“ªå€‹æ¨™ç±¤ï¼Œæˆ‘å€‘å¯ä»¥æŸ¥çœ‹**raw_train_dataset**çš„**features**. é€™å°‡å‘Šè¨´æˆ‘å€‘æ¯åˆ—çš„é¡å‹ï¼š

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¹‹ä¸­,**Labelï¼ˆæ¨™ç±¤ï¼‰** æ˜¯ä¸€ç¨®**ClassLabelï¼ˆåˆ†é¡æ¨™ç±¤ï¼‰**ï¼Œä½¿ç”¨æ•´æ•¸å»ºç«‹èµ·åˆ°é¡åˆ¥æ¨™ç±¤çš„æ˜ å°„é—œä¿‚ã€‚**0**å°æ‡‰æ–¼**not_equivalent**ï¼Œ**1**å°æ‡‰æ–¼**equivalent**ã€‚

<Tip>

âœï¸ **è©¦è©¦çœ‹ï¼** æŸ¥çœ‹è¨“ç·´é›†çš„ç¬¬15è¡Œå…ƒç´ å’Œé©—è­‰é›†çš„87è¡Œå…ƒç´ ã€‚ä»–å€‘çš„æ¨™ç±¤æ˜¯ä»€éº¼ï¼Ÿ

</Tip>

### é è™•ç†æ•¸æ“šé›†

{#if fw === 'pt'}
<Youtube id="0u3ioSwev3s"/>
{:else}
<Youtube id="P-rZWqcB6CE"/>
{/if}

ç‚ºäº†é è™•ç†æ•¸æ“šé›†ï¼Œæˆ‘å€‘éœ€è¦å°‡æ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹èƒ½å¤ ç†è§£çš„æ•¸å­—ã€‚æ­£å¦‚ä½ åœ¨[ç¬¬äºŒç« ](/course/chapter2)ä¸Šçœ‹åˆ°çš„é‚£æ¨£

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ç„¶è€Œï¼Œåœ¨å…©å¥è©±å‚³éçµ¦æ¨¡å‹ï¼Œé æ¸¬é€™å…©å¥è©±æ˜¯å¦æ˜¯åŒç¾©ä¹‹å‰ã€‚æˆ‘å€‘éœ€è¦é€™å…©å¥è©±ä¾æ¬¡é€²è¡Œé©ç•¶çš„é è™•ç†ã€‚å¹¸é‹çš„æ˜¯ï¼Œæ¨™è¨˜å™¨ä¸åƒ…åƒ…å¯ä»¥è¼¸å…¥å–®å€‹å¥å­é‚„å¯ä»¥è¼¸å…¥ä¸€çµ„å¥å­ï¼Œä¸¦æŒ‰ç…§æˆ‘å€‘çš„BERTæ¨¡å‹æ‰€æœŸæœ›çš„è¼¸å…¥é€²è¡Œè™•ç†ï¼š

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

æˆ‘å€‘åœ¨[ç¬¬äºŒç« ](/course/chapter2) è¨è«–äº†**è¼¸å…¥è©id(input_ids)** å’Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** ï¼Œä½†æˆ‘å€‘åœ¨é‚£å€‹æ™‚å€™æ²’æœ‰è¨è«–**é¡å‹æ¨™è¨˜ID(token_type_ids)**ã€‚åœ¨é€™å€‹ä¾‹å­ä¸­ï¼Œ**é¡å‹æ¨™è¨˜ID(token_type_ids)**çš„ä½œç”¨å°±æ˜¯å‘Šè¨´æ¨¡å‹è¼¸å…¥çš„å“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬ä¸€å¥ï¼Œå“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬äºŒå¥ã€‚

<Tip>

âœï¸ ** è©¦è©¦çœ‹ï¼** é¸å–è¨“ç·´é›†ä¸­çš„ç¬¬15å€‹å…ƒç´ ï¼Œå°‡å…©å¥è©±åˆ†åˆ¥æ¨™è¨˜ç‚ºä¸€å°ã€‚çµæœå’Œä¸Šæ–¹çš„ä¾‹å­æœ‰ä»€éº¼ä¸åŒï¼Ÿ

</Tip>

å¦‚æœæˆ‘å€‘å°‡**input_ids**ä¸­çš„idè½‰æ›å›æ–‡å­—:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

æˆ‘å€‘å°‡å¾—åˆ°ï¼š

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

æ‰€ä»¥æˆ‘å€‘çœ‹åˆ°æ¨¡å‹éœ€è¦è¼¸å…¥çš„å½¢å¼æ˜¯ **[CLS] sentence1 [SEP] sentence2 [SEP]**ã€‚å› æ­¤ï¼Œç•¶æœ‰å…©å¥è©±çš„æ™‚å€™ã€‚**é¡å‹æ¨™è¨˜ID(token_type_ids)** çš„å€¼æ˜¯ï¼š

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

å¦‚æ‚¨æ‰€è¦‹ï¼Œè¼¸å…¥ä¸­ **[CLS] sentence1 [SEP]** å®ƒå€‘çš„é¡å‹æ¨™è¨˜IDå‡ç‚º**0**ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ï¼Œå°æ‡‰æ–¼**sentence2 [SEP]**ï¼Œæ‰€æœ‰çš„é¡å‹æ¨™è¨˜IDå‡ç‚º**1**.

è«‹æ³¨æ„ï¼Œå¦‚æœé¸æ“‡å…¶ä»–çš„æª¢æŸ¥é»ï¼Œå‰‡ä¸ä¸€å®šå…·æœ‰**é¡å‹æ¨™è¨˜ID(token_type_ids)**ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨DistilBERTæ¨¡å‹ï¼Œå°±ä¸æœƒè¿”å›å®ƒå€‘ï¼‰ã€‚åªæœ‰ç•¶å®ƒåœ¨é è¨“ç·´æœŸé–“ä½¿ç”¨éé€™ä¸€å±¤ï¼Œæ¨¡å‹åœ¨æ§‹å»ºæ™‚ä¾è³´å®ƒå€‘ï¼Œæ‰æœƒè¿”å›å®ƒå€‘ã€‚

ç”¨é¡å‹æ¨™è¨˜IDå°BERTé€²è¡Œé è¨“ç·´,ä¸¦ä¸”ä½¿ç”¨[ç¬¬ä¸€ç« ](/course/chapter1)çš„é®ç½©èªè¨€æ¨¡å‹ï¼Œé‚„æœ‰ä¸€å€‹é¡å¤–çš„æ‡‰ç”¨é¡å‹ï¼Œå«åšä¸‹ä¸€å¥é æ¸¬. é€™é …ä»»å‹™çš„ç›®æ¨™æ˜¯å»ºç«‹æˆå°å¥å­ä¹‹é–“é—œä¿‚çš„æ¨¡å‹ã€‚

åœ¨ä¸‹ä¸€å€‹å¥å­é æ¸¬ä»»å‹™ä¸­ï¼Œæœƒçµ¦æ¨¡å‹è¼¸å…¥æˆå°çš„å¥å­ï¼ˆå¸¶æœ‰éš¨æ©Ÿé®ç½©çš„æ¨™è¨˜ï¼‰ï¼Œä¸¦è¢«è¦æ±‚é æ¸¬ç¬¬äºŒå€‹å¥å­æ˜¯å¦ç·Šè·Ÿç¬¬ä¸€å€‹å¥å­ã€‚ç‚ºäº†æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ•¸æ“šé›†ä¸­ä¸€åŠçš„å…©å€‹å¥å­åœ¨åŸå§‹æ–‡æª”ä¸­æŒ¨åœ¨ä¸€èµ·ï¼Œå¦ä¸€åŠçš„å…©å€‹å¥å­ä¾†è‡ªå…©å€‹ä¸åŒçš„æ–‡æª”ã€‚

ä¸€èˆ¬ä¾†èªªï¼Œä½ ä¸éœ€è¦æ“”å¿ƒæ˜¯å¦æœ‰**é¡å‹æ¨™è¨˜ID(token_type_ids)**ã€‚åœ¨æ‚¨çš„æ¨™è¼¸å…¥ä¸­ï¼šåªè¦æ‚¨å°æ¨™è¨˜å™¨å’Œæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æª¢æŸ¥é»ï¼Œä¸€åˆ‡éƒ½æœƒå¾ˆå¥½ï¼Œå› ç‚ºæ¨™è¨˜å™¨çŸ¥é“å‘å…¶æ¨¡å‹æä¾›ä»€éº¼ã€‚

ç¾åœ¨æˆ‘å€‘å·²ç¶“ç­è§£äº†æ¨™è¨˜å™¨å¦‚ä½•è™•ç†ä¸€å°å¥å­ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨å®ƒå°æ•´å€‹æ•¸æ“šé›†é€²è¡Œè™•ç†ï¼šå¦‚[ä¹‹å‰çš„ç« ç¯€](/course/chapter2)ï¼Œæˆ‘å€‘å¯ä»¥çµ¦æ¨™è¨˜å™¨æä¾›ä¸€çµ„å¥å­ï¼Œç¬¬ä¸€å€‹åƒæ•¸æ˜¯å®ƒç¬¬ä¸€å€‹å¥å­çš„åˆ—è¡¨ï¼Œç¬¬äºŒå€‹åƒæ•¸æ˜¯ç¬¬äºŒå€‹å¥å­çš„åˆ—è¡¨ã€‚é€™ä¹Ÿèˆ‡æˆ‘å€‘åœ¨[ç¬¬äºŒç« ](/course/chapter2)ä¸­çœ‹åˆ°çš„å¡«å……å’Œæˆªæ–·é¸é …å…¼å®¹. å› æ­¤ï¼Œé è™•ç†è¨“ç·´æ•¸æ“šé›†çš„ä¸€ç¨®æ–¹æ³•æ˜¯ï¼š

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

é€™å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒçš„ç¼ºé»æ˜¯è¿”å›å­—å…¸ï¼ˆå­—å…¸çš„éµæ˜¯**è¼¸å…¥è©id(input_ids)** ï¼Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** å’Œ **é¡å‹æ¨™è¨˜ID(token_type_ids)**ï¼Œå­—å…¸çš„å€¼æ˜¯éµæ‰€å°æ‡‰å€¼çš„åˆ—è¡¨ï¼‰ã€‚è€Œä¸”åªæœ‰ç•¶æ‚¨åœ¨è½‰æ›éç¨‹ä¸­æœ‰è¶³å¤ çš„å…§å­˜ä¾†å­˜å„²æ•´å€‹æ•¸æ“šé›†æ™‚æ‰ä¸æœƒå‡ºéŒ¯ï¼ˆè€ŒğŸ¤—æ•¸æ“šé›†åº«ä¸­çš„æ•¸æ“šé›†æ˜¯ä»¥[Apache Arrow](https://arrow.apache.org/)æ–‡ä»¶å­˜å„²åœ¨ç£ç›¤ä¸Šï¼Œå› æ­¤æ‚¨åªéœ€å°‡æ¥ä¸‹ä¾†è¦ç”¨çš„æ•¸æ“šåŠ è¼‰åœ¨å…§å­˜ä¸­ï¼Œå› æ­¤æœƒå°å…§å­˜å®¹é‡çš„éœ€æ±‚è¦ä½ä¸€äº›ï¼‰ã€‚

ç‚ºäº†å°‡æ•¸æ“šä¿å­˜ç‚ºæ•¸æ“šé›†ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨[Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map)æ–¹æ³•ï¼Œå¦‚æœæˆ‘å€‘éœ€è¦åšæ›´å¤šçš„é è™•ç†è€Œä¸åƒ…åƒ…æ˜¯æ¨™è¨˜åŒ–ï¼Œé‚£éº¼é€™ä¹Ÿçµ¦äº†æˆ‘å€‘ä¸€äº›é¡å¤–çš„è‡ªå®šç¾©çš„æ–¹æ³•ã€‚é€™å€‹æ–¹æ³•çš„å·¥ä½œåŸç†æ˜¯åœ¨æ•¸æ“šé›†çš„æ¯å€‹å…ƒç´ ä¸Šæ‡‰ç”¨ä¸€å€‹å‡½æ•¸ï¼Œå› æ­¤è®“æˆ‘å€‘å®šç¾©ä¸€å€‹æ¨™è¨˜è¼¸å…¥çš„å‡½æ•¸ï¼š

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

æ­¤å‡½æ•¸çš„è¼¸å…¥æ˜¯ä¸€å€‹å­—å…¸ï¼ˆèˆ‡æ•¸æ“šé›†çš„é …é¡ä¼¼ï¼‰ï¼Œä¸¦è¿”å›ä¸€å€‹åŒ…å«**è¼¸å…¥è©id(input_ids)** ï¼Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** å’Œ **é¡å‹æ¨™è¨˜ID(token_type_ids)** éµçš„æ–°å­—å…¸ã€‚è«‹æ³¨æ„ï¼Œå¦‚æœåƒä¸Šé¢çš„**ç¤ºä¾‹**ä¸€æ¨£ï¼Œå¦‚æœéµæ‰€å°æ‡‰çš„å€¼åŒ…å«å¤šå€‹å¥å­ï¼ˆæ¯å€‹éµä½œç‚ºä¸€å€‹å¥å­åˆ—è¡¨ï¼‰ï¼Œé‚£éº¼å®ƒä¾ç„¶å¯ä»¥å·¥ä½œï¼Œå°±åƒå‰é¢çš„ä¾‹å­ä¸€æ¨£æ¨™è¨˜å™¨å¯ä»¥è™•ç†æˆå°çš„å¥å­åˆ—è¡¨ã€‚é€™æ¨£çš„è©±æˆ‘å€‘å¯ä»¥åœ¨èª¿ç”¨**map()**ä½¿ç”¨è©²é¸é … **batched=True** ï¼Œé€™å°‡é¡¯è‘—åŠ å¿«æ¨™è¨˜èˆ‡æ¨™è¨˜çš„é€Ÿåº¦ã€‚é€™å€‹**æ¨™è¨˜å™¨**ä¾†è‡ª[ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)åº«ç”±Rustç·¨å¯«è€Œæˆã€‚ç•¶æˆ‘å€‘ä¸€æ¬¡çµ¦å®ƒå¤§é‡çš„è¼¸å…¥æ™‚ï¼Œé€™å€‹æ¨™è¨˜å™¨å¯ä»¥éå¸¸å¿«ã€‚

è«‹æ³¨æ„ï¼Œæˆ‘å€‘ç¾åœ¨åœ¨æ¨™è¨˜å‡½æ•¸ä¸­çœç•¥äº†**padding**åƒæ•¸ã€‚é€™æ˜¯å› ç‚ºåœ¨æ¨™è¨˜çš„æ™‚å€™å°‡æ‰€æœ‰æ¨£æœ¬å¡«å……åˆ°æœ€å¤§é•·åº¦çš„æ•ˆç‡ä¸é«˜ã€‚ä¸€å€‹æ›´å¥½çš„åšæ³•ï¼šåœ¨æ§‹å»ºæ‰¹è™•ç†æ™‚å¡«å……æ¨£æœ¬æ›´å¥½ï¼Œå› ç‚ºé€™æ¨£æˆ‘å€‘åªéœ€è¦å¡«å……åˆ°è©²æ‰¹è™•ç†ä¸­çš„æœ€å¤§é•·åº¦ï¼Œè€Œä¸æ˜¯æ•´å€‹æ•¸æ“šé›†çš„æœ€å¤§é•·åº¦ã€‚ç•¶è¼¸å…¥é•·åº¦è®ŠåŒ–å¾ˆå¤§æ™‚ï¼Œé€™å¯ä»¥ç¯€çœå¤§é‡æ™‚é–“å’Œè™•ç†èƒ½åŠ›!

ä¸‹é¢æ˜¯æˆ‘å€‘å¦‚ä½•åœ¨æ‰€æœ‰æ•¸æ“šé›†ä¸ŠåŒæ™‚æ‡‰ç”¨æ¨™è¨˜å‡½æ•¸ã€‚æˆ‘å€‘åœ¨èª¿ç”¨**map**æ™‚ä½¿ç”¨äº†**batch =True**ï¼Œé€™æ¨£å‡½æ•¸å°±å¯ä»¥åŒæ™‚æ‡‰ç”¨åˆ°æ•¸æ“šé›†çš„å¤šå€‹å…ƒç´ ä¸Šï¼Œè€Œä¸æ˜¯åˆ†åˆ¥æ‡‰ç”¨åˆ°æ¯å€‹å…ƒç´ ä¸Šã€‚é€™å°‡ä½¿æˆ‘å€‘çš„é è™•ç†å¿«è¨±å¤š

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ğŸ¤—Datasets åº«æ‡‰ç”¨é€™ç¨®è™•ç†çš„æ–¹å¼æ˜¯å‘æ•¸æ“šé›†æ·»åŠ æ–°çš„å­—æ®µï¼Œæ¯å€‹å­—æ®µå°æ‡‰é è™•ç†å‡½æ•¸è¿”å›çš„å­—å…¸ä¸­çš„æ¯å€‹éµ:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

åœ¨ä½¿ç”¨é è™•ç†å‡½æ•¸**map()**æ™‚ï¼Œç”šè‡³å¯ä»¥é€šéå‚³é**num_proc**åƒæ•¸ä½¿ç”¨ä¸¦è¡Œè™•ç†ã€‚æˆ‘å€‘åœ¨é€™è£¡æ²’æœ‰é€™æ¨£åšï¼Œå› ç‚ºğŸ¤—æ¨™è¨˜å™¨åº«å·²ç¶“ä½¿ç”¨å¤šå€‹ç·šç¨‹ä¾†æ›´å¿«åœ°æ¨™è¨˜æˆ‘å€‘çš„æ¨£æœ¬ï¼Œä½†æ˜¯å¦‚æœæ‚¨æ²’æœ‰ä½¿ç”¨è©²åº«æ”¯æŒçš„å¿«é€Ÿæ¨™è¨˜å™¨ï¼Œä½¿ç”¨**num_proc**å¯èƒ½æœƒåŠ å¿«é è™•ç†ã€‚

æˆ‘å€‘çš„**æ¨™è¨˜å‡½æ•¸(tokenize_function)**è¿”å›åŒ…å«**è¼¸å…¥è©id(input_ids)** ï¼Œ **æ³¨æ„åŠ›é®ç½©(attention_mask)** å’Œ **é¡å‹æ¨™è¨˜ID(token_type_ids)** éµçš„å­—å…¸,æ‰€ä»¥é€™ä¸‰å€‹å­—æ®µè¢«æ·»åŠ åˆ°æ•¸æ“šé›†çš„æ¨™è¨˜çš„çµæœä¸­ã€‚æ³¨æ„ï¼Œå¦‚æœé è™•ç†å‡½æ•¸**map()**ç‚ºç¾æœ‰éµè¿”å›ä¸€å€‹æ–°å€¼ï¼Œé‚£å°‡æœƒä¿®æ”¹åŸæœ‰éµçš„å€¼ã€‚

æœ€å¾Œä¸€ä»¶æˆ‘å€‘éœ€è¦åšçš„äº‹æƒ…æ˜¯ï¼Œç•¶æˆ‘å€‘ä¸€èµ·æ‰¹è™•ç†å…ƒç´ æ™‚ï¼Œå°‡æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°æœ€é•·å…ƒç´ çš„é•·åº¦â€”â€”æˆ‘å€‘ç¨±ä¹‹ç‚ºå‹•æ…‹å¡«å……ã€‚

### å‹•æ…‹å¡«å……

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
è² è²¬åœ¨æ‰¹è™•ç†ä¸­å°‡æ•¸æ“šæ•´ç†ç‚ºä¸€å€‹batchçš„å‡½æ•¸ç¨±ç‚º*collateå‡½æ•¸*ã€‚å®ƒæ˜¯ä½ å¯ä»¥åœ¨æ§‹å»º**DataLoader**æ™‚å‚³éçš„ä¸€å€‹åƒæ•¸ï¼Œé»˜èªæ˜¯ä¸€å€‹å‡½æ•¸ï¼Œå®ƒå°‡æŠŠä½ çš„æ•¸æ“šé›†è½‰æ›ç‚ºPyTorchå¼µé‡ï¼Œä¸¦å°‡å®ƒå€‘æ‹¼æ¥èµ·ä¾†(å¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒçµ„æˆ–å­—å…¸ï¼Œå‰‡æœƒä½¿ç”¨éæ­¸)ã€‚é€™åœ¨æˆ‘å€‘çš„é€™å€‹ä¾‹å­ä¸­ä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ç‚ºæˆ‘å€‘çš„è¼¸å…¥ä¸æ˜¯éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚æˆ‘å€‘æ•…æ„åœ¨ä¹‹å¾Œæ¯å€‹batchä¸Šé€²è¡Œå¡«å……ï¼Œé¿å…æœ‰å¤ªå¤šå¡«å……çš„éé•·çš„è¼¸å…¥ã€‚é€™å°‡å¤§å¤§åŠ å¿«è¨“ç·´é€Ÿåº¦ï¼Œä½†è«‹æ³¨æ„ï¼Œå¦‚æœä½ åœ¨TPUä¸Šè¨“ç·´ï¼Œé€™å¯èƒ½æœƒå°è‡´å•é¡Œâ€”â€”TPUå–œæ­¡å›ºå®šçš„å½¢ç‹€ï¼Œå³ä½¿é€™éœ€è¦é¡å¤–çš„å¡«å……ã€‚

{:else}

è² è²¬åœ¨æ‰¹è™•ç†ä¸­å°‡æ•¸æ“šæ•´ç†ç‚ºä¸€å€‹batchçš„å‡½æ•¸ç¨±ç‚º*collateå‡½æ•¸*ã€‚å®ƒåªæœƒå°‡æ‚¨çš„æ¨£æœ¬è½‰æ›ç‚º tf.Tensorä¸¦å°‡å®ƒå€‘æ‹¼æ¥èµ·ä¾†(å¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒçµ„æˆ–å­—å…¸ï¼Œå‰‡æœƒä½¿ç”¨éæ­¸)ã€‚é€™åœ¨æˆ‘å€‘çš„é€™å€‹ä¾‹å­ä¸­ä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ç‚ºæˆ‘å€‘çš„è¼¸å…¥ä¸æ˜¯éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚æˆ‘å€‘æ•…æ„åœ¨ä¹‹å¾Œæ¯å€‹batchä¸Šé€²è¡Œå¡«å……ï¼Œé¿å…æœ‰å¤ªå¤šå¡«å……çš„éé•·çš„è¼¸å…¥ã€‚é€™å°‡å¤§å¤§åŠ å¿«è¨“ç·´é€Ÿåº¦ï¼Œä½†è«‹æ³¨æ„ï¼Œå¦‚æœä½ åœ¨TPUä¸Šè¨“ç·´ï¼Œé€™å¯èƒ½æœƒå°è‡´å•é¡Œâ€”â€”TPUå–œæ­¡å›ºå®šçš„å½¢ç‹€ï¼Œå³ä½¿é€™éœ€è¦é¡å¤–çš„å¡«å……ã€‚

{/if}

ç‚ºäº†è§£æ±ºå¥å­é•·åº¦çµ±ä¸€çš„å•é¡Œï¼Œæˆ‘å€‘å¿…é ˆå®šç¾©ä¸€å€‹collateå‡½æ•¸ï¼Œè©²å‡½æ•¸æœƒå°‡æ¯å€‹batchå¥å­å¡«å……åˆ°æ­£ç¢ºçš„é•·åº¦ã€‚å¹¸é‹çš„æ˜¯ï¼ŒğŸ¤—transformeråº«é€šé**DataCollatorWithPadding**ç‚ºæˆ‘å€‘æä¾›äº†é€™æ¨£ä¸€å€‹å‡½æ•¸ã€‚ç•¶ä½ å¯¦ä¾‹åŒ–å®ƒæ™‚ï¼Œéœ€è¦ä¸€å€‹æ¨™è¨˜å™¨(ç”¨ä¾†çŸ¥é“ä½¿ç”¨å“ªå€‹è©ä¾†å¡«å……ï¼Œä»¥åŠæ¨¡å‹æœŸæœ›å¡«å……åœ¨å·¦é‚Šé‚„æ˜¯å³é‚Š)ï¼Œä¸¦å°‡åšä½ éœ€è¦çš„ä¸€åˆ‡:

{#if fw === 'pt'}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
{:else}
```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```
{/if}

ç‚ºäº†æ¸¬è©¦é€™å€‹æ–°ç©å…·ï¼Œè®“æˆ‘å€‘å¾æˆ‘å€‘çš„è¨“ç·´é›†ä¸­æŠ½å–å¹¾å€‹æ¨£æœ¬ã€‚é€™è£¡ï¼Œæˆ‘å€‘åˆªé™¤åˆ—**idx**, **sentence1**å’Œ**sentence2**ï¼Œå› ç‚ºä¸éœ€è¦å®ƒå€‘ï¼Œä¸¦æŸ¥çœ‹ä¸€å€‹batchä¸­æ¯å€‹æ¢ç›®çš„é•·åº¦:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

æ¯«ç„¡ç–‘å•ï¼Œæˆ‘å€‘å¾—åˆ°äº†ä¸åŒé•·åº¦çš„æ¨£æœ¬ï¼Œå¾32åˆ°67ã€‚å‹•æ…‹å¡«å……æ„å‘³è‘—è©²æ‰¹ä¸­çš„æ‰€æœ‰æ¨£æœ¬éƒ½æ‡‰è©²å¡«å……åˆ°é•·åº¦ç‚º67ï¼Œé€™æ˜¯è©²æ‰¹ä¸­çš„æœ€å¤§é•·åº¦ã€‚å¦‚æœæ²’æœ‰å‹•æ…‹å¡«å……ï¼Œæ‰€æœ‰çš„æ¨£æœ¬éƒ½å¿…é ˆå¡«å……åˆ°æ•´å€‹æ•¸æ“šé›†ä¸­çš„æœ€å¤§é•·åº¦ï¼Œæˆ–è€…æ¨¡å‹å¯ä»¥æ¥å—çš„æœ€å¤§é•·åº¦ã€‚è®“æˆ‘å€‘å†æ¬¡æª¢æŸ¥**data_collator**æ˜¯å¦æ­£ç¢ºåœ°å‹•æ…‹å¡«å……äº†é€™æ‰¹æ¨£æœ¬ï¼š

```py:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python out
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python out
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

çœ‹èµ·ä¾†ä¸éŒ¯ï¼ç¾åœ¨ï¼Œæˆ‘å€‘å·²ç¶“å°‡åŸå§‹æ–‡æœ¬è½‰åŒ–ç‚ºäº†æ¨¡å‹å¯ä»¥è™•ç†çš„æ•¸æ“šï¼Œæˆ‘å€‘å·²æº–å‚™å¥½å°å…¶é€²è¡Œå¾®èª¿ï¼

{/if}

<Tip>

âœï¸ ** è©¦è©¦çœ‹ï¼** åœ¨GLUE SST-2æ•¸æ“šé›†ä¸Šæ‡‰ç”¨é è™•ç†ã€‚å®ƒæœ‰é»ä¸åŒï¼Œå› ç‚ºå®ƒæ˜¯ç”±å–®å€‹å¥å­è€Œä¸æ˜¯æˆå°çš„å¥å­çµ„æˆçš„ï¼Œä½†æ˜¯æˆ‘å€‘æ‰€åšçš„å…¶ä»–äº‹æƒ…çœ‹èµ·ä¾†æ‡‰è©²æ˜¯ä¸€æ¨£çš„ã€‚å¦ä¸€å€‹æ›´é›£çš„æŒ‘æˆ°ï¼Œè«‹å˜—è©¦ç·¨å¯«ä¸€å€‹å¯ç”¨æ–¼ä»»ä½•GLUEä»»å‹™çš„é è™•ç†å‡½æ•¸ã€‚

</Tip>

{#if fw === 'tf'}

ç¾åœ¨æˆ‘å€‘æœ‰äº† dataset å’Œ data collatorï¼Œæˆ‘å€‘éœ€è¦å°‡ dataset æ‰¹æ¬¡åœ°æ‡‰ç”¨ data collatorã€‚ æˆ‘å€‘å¯ä»¥æ‰‹å‹•è¼‰å…¥æ‰¹æ¬¡ä¸¦æ•´ç†å®ƒå€‘ï¼Œä½†é€™éœ€è¦å¤§é‡å·¥ä½œï¼Œæ€§èƒ½å¯èƒ½ä¹Ÿä¸æ˜¯å¾ˆå¥½ã€‚ ç›¸åï¼Œæœ‰ä¸€å€‹ç°¡å–®çš„æ–¹æ³•å¯ä»¥ç‚ºé€™å€‹å•é¡Œæä¾›é«˜æ•ˆçš„è§£æ±ºæ–¹æ¡ˆï¼š`to_tf_dataset()`ã€‚ é€™å°‡åœ¨æ‚¨çš„æ•¸æ“šé›†ä¸Šèª¿ç”¨ä¸€å€‹ `tf.data.Dataset`çš„æ–¹æ³•ï¼Œé€™å€‹æ–¹æ³•å¸¶æœ‰ä¸€å€‹å¯é¸çš„ data collator åŠŸèƒ½ã€‚ `tf.data.Dataset` æ˜¯ Keras å¯ç”¨æ–¼ `model.fit()` çš„åŸç”Ÿ TensorFlow æ ¼å¼ï¼Œå› æ­¤é€™ç¨®æ–¹æ³•æœƒç«‹å³å°‡ğŸ¤— Dataset è½‰æ›ç‚ºå¯ç”¨æ–¼è¨“ç·´çš„æ ¼å¼ã€‚ è®“æˆ‘å€‘çœ‹çœ‹å®ƒåœ¨æˆ‘å€‘çš„æ•¸æ“šé›†ä¸Šæ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

å°±æ˜¯é€™æ¨£ï¼ æˆ‘å€‘å¯ä»¥å°‡é€™äº›æ•¸æ“šé›†å¸¶å…¥ä¸‹ä¸€ç¯€ï¼Œåœ¨ç¶“éæ‰€æœ‰è‰±è‹¦çš„æ•¸æ“šé è™•ç†å·¥ä½œä¹‹å¾Œï¼Œè¨“ç·´å°‡è®Šå¾—éå¸¸ç°¡å–®ã€‚

{/if}
