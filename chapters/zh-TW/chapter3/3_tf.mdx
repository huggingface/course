<FrameworkSwitchCourse {fw} />

# ä½¿ç”¨ Keras å¾®èª¿ä¸€å€‹æ¨¡å‹

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3_tf.ipynb"},
]} />

å®Œæˆä¸Šä¸€ç¯€ä¸­çš„æ‰€æœ‰æ•¸æ“šé è™•ç†å·¥ä½œå¾Œï¼Œæ‚¨åªå‰©ä¸‹æœ€å¾Œçš„å¹¾å€‹æ­¥é©Ÿä¾†è¨“ç·´æ¨¡å‹ã€‚ ä½†æ˜¯è«‹æ³¨æ„ï¼Œ`model.fit()` å‘½ä»¤åœ¨ CPU ä¸Šé‹è¡Œæœƒéå¸¸ç·©æ…¢ã€‚ å¦‚æœæ‚¨æ²’æœ‰GPUï¼Œå‰‡å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com/) ä¸Šä½¿ç”¨å…è²»çš„ GPU æˆ– TPU(éœ€è¦æ¢¯å­)ã€‚

é€™ä¸€ç¯€çš„ä»£ç¢¼ç¤ºä¾‹å‡è¨­æ‚¨å·²ç¶“åŸ·è¡Œäº†ä¸Šä¸€ç¯€ä¸­çš„ä»£ç¢¼ç¤ºä¾‹ã€‚ ä¸‹é¢ä¸€å€‹ç°¡çŸ­çš„æ‘˜è¦ï¼ŒåŒ…å«äº†åœ¨é–‹å§‹å­¸ç¿’é€™ä¸€ç¯€ä¹‹å‰æ‚¨éœ€è¦çš„åŸ·è¡Œçš„ä»£ç¢¼ï¼š

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### è¨“ç·´æ¨¡å‹

å¾ğŸ¤— Transformers å°å…¥çš„ TensorFlow æ¨¡å‹å·²ç¶“æ˜¯ Keras æ¨¡å‹ã€‚ ä¸‹é¢çš„è¦–é »æ˜¯å° Keras çš„ç°¡çŸ­ä»‹ç´¹ã€‚

<Youtube id="rnTGBy2ax1c"/>

é€™æ„å‘³è‘—ï¼Œä¸€æ—¦æˆ‘å€‘æœ‰äº†æ•¸æ“šï¼Œå°±éœ€è¦å¾ˆå°‘çš„å·¥ä½œå°±å¯ä»¥é–‹å§‹å°å…¶é€²è¡Œè¨“ç·´ã€‚

<Youtube id="AUozVp78dhk"/>

å’Œ[ç¬¬äºŒç« ](/course/chapter2)ä½¿ç”¨çš„æ–¹æ³•ä¸€æ¨£, æˆ‘å€‘å°‡ä½¿ç”¨äºŒåˆ†é¡çš„ `TFAutoModelForSequenceClassification`é¡: 

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

æ‚¨æœƒæ³¨æ„åˆ°ï¼Œèˆ‡ [ç¬¬äºŒç« ](/course/chapter2) ä¸åŒçš„æ˜¯ï¼Œæ‚¨åœ¨å¯¦ä¾‹åŒ–æ­¤é è¨“ç·´æ¨¡å‹å¾Œæœƒæ”¶åˆ°è­¦å‘Šã€‚ é€™æ˜¯å› ç‚º BERT æ²’æœ‰å°å¥å­å°é€²è¡Œåˆ†é¡é€²è¡Œé è¨“ç·´ï¼Œæ‰€ä»¥é è¨“ç·´æ¨¡å‹çš„ head å·²ç¶“è¢«ä¸Ÿæ£„ï¼Œè€Œæ˜¯æ’å…¥äº†ä¸€å€‹é©åˆåºåˆ—åˆ†é¡çš„æ–° headã€‚ è­¦å‘Šè¡¨æ˜ä¸€äº›æ¬Šé‡æ²’æœ‰ä½¿ç”¨ï¼ˆå°æ‡‰æ–¼ä¸Ÿæ£„çš„é è¨“ç·´é ­ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æ¬Šé‡æ˜¯éš¨æ©Ÿåˆå§‹åŒ–çš„ï¼ˆæ–°é ­çš„æ¬Šé‡ï¼‰ã€‚ æœ€å¾Œé¼“å‹µæ‚¨è¨“ç·´æ¨¡å‹ï¼Œé€™æ­£æ˜¯æˆ‘å€‘ç¾åœ¨è¦åšçš„ã€‚

è¦åœ¨æˆ‘å€‘çš„æ•¸æ“šé›†ä¸Šå¾®èª¿æ¨¡å‹ï¼Œæˆ‘å€‘åªéœ€è¦åœ¨æˆ‘å€‘çš„æ¨¡å‹ä¸Šèª¿ç”¨ `compile()` æ–¹æ³•ï¼Œç„¶å¾Œå°‡æˆ‘å€‘çš„æ•¸æ“šå‚³éçµ¦ `fit()` æ–¹æ³•ã€‚ é€™å°‡å•Ÿå‹•å¾®èª¿éç¨‹ï¼ˆåœ¨ GPU ä¸Šæ‡‰è©²éœ€è¦å¹¾åˆ†é˜ï¼‰ä¸¦è¼¸å‡ºè¨“ç·´lossï¼Œä»¥åŠæ¯å€‹ epoch çµæŸæ™‚çš„é©—è­‰lossã€‚

<Tip>

è«‹æ³¨æ„ğŸ¤— Transformers æ¨¡å‹å…·æœ‰å¤§å¤šæ•¸ Keras æ¨¡å‹æ‰€æ²’æœ‰çš„ç‰¹æ®Šèƒ½åŠ›â€”â€”å®ƒå€‘å¯ä»¥è‡ªå‹•ä½¿ç”¨å…§éƒ¨è¨ˆç®—çš„lossã€‚ å¦‚æœæ‚¨æ²’æœ‰åœ¨ `compile()` ä¸­è¨­ç½®æå¤±å‡½æ•¸ï¼Œä»–å€‘å°‡é»˜èªä½¿ç”¨å…§éƒ¨è¨ˆç®—çš„æå¤±ã€‚ è«‹æ³¨æ„ï¼Œè¦ä½¿ç”¨å…§éƒ¨æå¤±ï¼Œæ‚¨éœ€è¦å°‡æ¨™ç±¤ä½œç‚ºè¼¸å…¥çš„ä¸€éƒ¨åˆ†å‚³éï¼Œè€Œä¸æ˜¯ä½œç‚ºå–®ç¨çš„æ¨™ç±¤ï¼ˆé€™æ˜¯åœ¨ Keras æ¨¡å‹ä¸­ä½¿ç”¨æ¨™ç±¤çš„æ­£å¸¸æ–¹å¼ï¼‰ã€‚ æ‚¨å°‡åœ¨èª²ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†ä¸­çœ‹åˆ°é€™æ–¹é¢çš„ç¤ºä¾‹ï¼Œå…¶ä¸­å®šç¾©æ­£ç¢ºçš„æå¤±å‡½æ•¸å¯èƒ½å¾ˆæ£˜æ‰‹ã€‚ ç„¶è€Œï¼Œå°æ–¼åºåˆ—åˆ†é¡ï¼Œæ¨™æº–çš„ Keras æå¤±å‡½æ•¸å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œæ‰€ä»¥æˆ‘å€‘å°‡åœ¨é€™è£¡ä½¿ç”¨å®ƒã€‚

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

è«‹æ³¨æ„é€™è£¡æœ‰ä¸€å€‹éå¸¸å¸¸è¦‹çš„é™·é˜±â€”â€”ä½ åªæ˜¯*å¯ä»¥*å°‡æå¤±çš„åç¨±ä½œç‚ºå­—ç¬¦ä¸²å‚³éçµ¦ Kerasï¼Œä½†é»˜èªæƒ…æ³ä¸‹ï¼ŒKeras æœƒå‡è¨­ä½ å·²ç¶“å°è¼¸å‡ºæ‡‰ç”¨äº† softmaxã€‚ ç„¶è€Œï¼Œè¨±å¤šæ¨¡å‹åœ¨æ‡‰ç”¨ softmax ä¹‹å‰å°±è¼¸å‡ºï¼Œä¹Ÿç¨±ç‚º *logits*ã€‚ æˆ‘å€‘éœ€è¦å‘Šè¨´æå¤±å‡½æ•¸æˆ‘å€‘çš„æ¨¡å‹æ˜¯å¦ç¶“éäº†softmaxï¼Œå”¯ä¸€çš„æ–¹æ³•æ˜¯ç›´æ¥èª¿ç”¨å®ƒï¼Œè€Œä¸æ˜¯ç”¨å­—ç¬¦ä¸²çš„åç¨±ã€‚

</Tip>


### æå‡è¨“ç·´çš„æ•ˆæœ

<Youtube id="cpzq6ESSM5c"/>

å¦‚æœæ‚¨å˜—è©¦ä¸Šé¢çš„ä»£ç¢¼ï¼Œå®ƒè‚¯å®šæœƒé‹è¡Œï¼Œä½†æ‚¨æœƒç™¼ç¾lossåªæ˜¯ç·©æ…¢æˆ–é›¶æ˜Ÿåœ°ä¸‹é™ã€‚ ä¸»è¦åŸå› æ˜¯*å­¸ç¿’ç‡*ã€‚ èˆ‡lossä¸€æ¨£ï¼Œç•¶æˆ‘å€‘å°‡å„ªåŒ–å™¨çš„åç¨±ä½œç‚ºå­—ç¬¦ä¸²å‚³éçµ¦ Keras æ™‚ï¼ŒKeras æœƒåˆå§‹åŒ–è©²å„ªåŒ–å™¨å…·æœ‰æ‰€æœ‰åƒæ•¸çš„é»˜èªå€¼ï¼ŒåŒ…æ‹¬å­¸ç¿’ç‡ã€‚ ä½†æ˜¯ï¼Œæ ¹æ“šé•·æœŸç¶“é©—ï¼Œæˆ‘å€‘çŸ¥é“Transformer æ¨¡å‹æ›´é©åˆä½¿ç”¨æ¯” Adam çš„é»˜èªå€¼ï¼ˆ1e-3ï¼‰ä¹Ÿå¯«æˆç‚º 10 çš„ -3 æ¬¡æ–¹ï¼Œæˆ– 0.001ï¼Œä½å¾—å¤šçš„å­¸ç¿’ç‡ã€‚ 5e-5 (0.00005) æ¯”é»˜èªå€¼å¤§ç´„ä½ 20 å€ï¼Œæ˜¯ä¸€å€‹æ›´å¥½çš„èµ·é»ã€‚

é™¤äº†é™ä½å­¸ç¿’ç‡ï¼Œæˆ‘å€‘é‚„æœ‰ç¬¬äºŒå€‹æŠ€å·§ï¼šæˆ‘å€‘å¯ä»¥æ…¢æ…¢é™ä½å­¸ç¿’ç‡ã€‚åœ¨è¨“ç·´éç¨‹ä¸­ã€‚ åœ¨æ–‡ç»ä¸­ï¼Œæ‚¨æœ‰æ™‚æœƒçœ‹åˆ°é€™è¢«ç¨±ç‚º *decaying* æˆ– *annealing*å­¸ç¿’ç‡ã€‚ åœ¨ Keras ä¸­ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨ *learning rate scheduler*ã€‚ ä¸€å€‹å¥½ç”¨çš„æ˜¯`PolynomialDecay`â€”â€”å„˜ç®¡æœ‰é€™å€‹åå­—ï¼Œä½†åœ¨é»˜èªè¨­ç½®ä¸‹ï¼Œå®ƒåªæ˜¯ç°¡å–®åœ°å¾åˆå§‹å€¼ç·šæ€§è¡°æ¸›å­¸ç¿’ç‡å€¼åœ¨è¨“ç·´éç¨‹ä¸­çš„æœ€çµ‚å€¼ï¼Œé€™æ­£æ˜¯æˆ‘å€‘æƒ³è¦çš„ã€‚ä½†æ˜¯ï¼Œ ç‚ºäº†æ­£ç¢ºä½¿ç”¨èª¿åº¦ç¨‹åºï¼Œæˆ‘å€‘éœ€è¦å‘Šè¨´å®ƒè¨“ç·´çš„æ¬¡æ•¸ã€‚ æˆ‘å€‘å°‡åœ¨ä¸‹é¢ç‚ºå…¶è¨ˆç®—â€œnum_train_stepsâ€ã€‚

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# è¨“ç·´æ­¥æ•¸æ˜¯æ•¸æ“šé›†ä¸­çš„æ¨£æœ¬æ•¸é™¤ä»¥batch sizeå†ä¹˜ä»¥ epochã€‚
# æ³¨æ„é€™è£¡çš„tf_train_datasetæ˜¯ä¸€å€‹è½‰åŒ–ç‚ºbatchå¾Œçš„ tf.data.Datasetï¼Œ
# ä¸æ˜¯åŸä¾†çš„ Hugging Face Datasetï¼Œæ‰€ä»¥å®ƒçš„ len() å·²ç¶“æ˜¯ num_samples // batch_sizeã€‚
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

ğŸ¤— Transformers åº«é‚„æœ‰ä¸€å€‹ `create_optimizer()` å‡½æ•¸ï¼Œå®ƒå°‡å‰µå»ºä¸€å€‹å…·æœ‰å­¸ç¿’ç‡è¡°æ¸›çš„ `AdamW` å„ªåŒ–å™¨ã€‚ é€™æ˜¯ä¸€å€‹ä¾¿æ·çš„æ–¹å¼ï¼Œæ‚¨å°‡åœ¨æœ¬èª²ç¨‹çš„å¾ŒçºŒéƒ¨åˆ†ä¸­è©³ç´°ç­è§£ã€‚

</Tip>

ç¾åœ¨æˆ‘å€‘æœ‰äº†å…¨æ–°çš„å„ªåŒ–å™¨ï¼Œæˆ‘å€‘å¯ä»¥å˜—è©¦ä½¿ç”¨å®ƒé€²è¡Œè¨“ç·´ã€‚ é¦–å…ˆï¼Œè®“æˆ‘å€‘é‡æ–°åŠ è¼‰æ¨¡å‹ï¼Œä»¥é‡ç½®æˆ‘å€‘å‰›å‰›é€²è¡Œçš„è¨“ç·´é‹è¡Œå°æ¬Šé‡çš„æ›´æ”¹ï¼Œç„¶å¾Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨æ–°çš„å„ªåŒ–å™¨å°å…¶é€²è¡Œç·¨è­¯ï¼š

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ç¾åœ¨ï¼Œæˆ‘å€‘å†æ¬¡é€²è¡Œfitï¼š

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æƒ³åœ¨è¨“ç·´æœŸé–“è‡ªå‹•å°‡æ¨¡å‹ä¸Šå‚³åˆ° Hubï¼Œæ‚¨å¯ä»¥åœ¨ `model.fit()` æ–¹æ³•ä¸­å‚³é `PushToHubCallback`ã€‚ æˆ‘å€‘å°‡åœ¨ [ç¬¬å››ç« ](/course/chapter4/3) ä¸­é€²è¡Œä»‹ç´¹

</Tip>

### æ¨¡å‹é æ¸¬

<Youtube id="nx10eh4CoOs"/>


è¨“ç·´å’Œè§€å¯Ÿçš„lossä¸‹é™éƒ½éå¸¸å¥½ï¼Œä½†æ˜¯å¦‚æœæˆ‘å€‘æƒ³å¾è¨“ç·´å¾Œçš„æ¨¡å‹ä¸­ç²å¾—è¼¸å‡ºï¼Œæˆ–è€…è¨ˆç®—ä¸€äº›æŒ‡æ¨™ï¼Œæˆ–è€…åœ¨ç”Ÿç”¢ä¸­ä½¿ç”¨æ¨¡å‹å‘¢ï¼Ÿ ç‚ºæ­¤ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨`predict()` æ–¹æ³•ã€‚ é€™å°‡è¿”å›æ¨¡å‹çš„è¼¸å‡ºé ­çš„*logits*æ•¸å€¼ï¼Œæ¯å€‹é¡ä¸€å€‹ã€‚

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

æˆ‘å€‘å¯ä»¥å°‡é€™äº› logit è½‰æ›ç‚ºæ¨¡å‹çš„é¡åˆ¥é æ¸¬ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ argmax æ‰¾åˆ°æœ€é«˜çš„ logitï¼Œå®ƒå°æ‡‰æ–¼æœ€æœ‰å¯èƒ½çš„é¡åˆ¥ï¼š

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

ç¾åœ¨ï¼Œè®“æˆ‘å€‘ä½¿ç”¨é€™äº› `preds` ä¾†è¨ˆç®—ä¸€äº›æŒ‡æ¨™ï¼ æˆ‘å€‘å¯ä»¥åƒåŠ è¼‰æ•¸æ“šé›†ä¸€æ¨£è¼•é¬†åœ°åŠ è¼‰èˆ‡ MRPC æ•¸æ“šé›†ç›¸é—œçš„æŒ‡æ¨™ï¼Œé€™æ¬¡ä½¿ç”¨çš„æ˜¯ `evaluate.load()` å‡½æ•¸ã€‚ è¿”å›çš„å°è±¡æœ‰ä¸€å€‹ `compute()` æ–¹æ³•ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨å®ƒä¾†é€²è¡Œåº¦é‡è¨ˆç®—ï¼š

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

æ‚¨ç²å¾—çš„ç¢ºåˆ‡çµæœå¯èƒ½æœƒæœ‰æ‰€ä¸åŒï¼Œå› ç‚ºæ¨¡å‹é ­çš„éš¨æ©Ÿåˆå§‹åŒ–å¯èƒ½æœƒæ”¹è®Šå®ƒç²å¾—çš„æŒ‡æ¨™ã€‚ åœ¨é€™è£¡ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°æˆ‘å€‘çš„æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„æº–ç¢ºç‡ç‚º 85.78%ï¼ŒF1 å¾—åˆ†ç‚º 89.97ã€‚ é€™äº›æ˜¯ç”¨æ–¼è©•ä¼° GLUE åŸºæº–çš„ MRPC æ•¸æ“šé›†çµæœçš„å…©å€‹æŒ‡æ¨™ã€‚ [BERT è«–æ–‡](https://arxiv.org/pdf/1810.04805.pdf) ä¸­çš„è¡¨æ ¼å ±å‘Šäº†åŸºæœ¬æ¨¡å‹çš„ F1 åˆ†æ•¸ç‚º 88.9ã€‚ é‚£æ˜¯ `uncased` æ¨¡å‹ï¼Œè€Œæˆ‘å€‘ç›®å‰ä½¿ç”¨çš„æ˜¯ `cased` æ¨¡å‹ï¼Œé€™è§£é‡‹äº†ç‚ºä»€éº¼æˆ‘å€‘æœƒç²å¾—æ›´å¥½çš„çµæœã€‚

ä½¿ç”¨ Keras API é€²è¡Œå¾®èª¿çš„ä»‹ç´¹åˆ°æ­¤çµæŸã€‚ ç¬¬ 7 ç« å°‡çµ¦å‡ºå°å¤§å¤šæ•¸å¸¸è¦‹ NLP ä»»å‹™åŸ·è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ã€‚å¦‚æœæ‚¨æƒ³åœ¨ Keras API ä¸Šç£¨ç·´è‡ªå·±çš„æŠ€èƒ½ï¼Œè«‹å˜—è©¦ä½¿ç¬¬äºŒç¯€æ‰€ä½¿ç”¨çš„çš„æ•¸æ“šè™•ç†åœ¨ GLUE SST-2 æ•¸æ“šé›†ä¸Šå¾®èª¿æ¨¡å‹ã€‚