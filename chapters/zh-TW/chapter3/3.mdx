<FrameworkSwitchCourse {fw} />

# ä½¿ç”¨ Trainer API å¾®èª¿æ¨¡å‹

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

ğŸ¤— Transformersæä¾›äº†ä¸€å€‹ **Trainer** é¡ä¾†å¹«åŠ©æ‚¨åœ¨è‡ªå·±çš„æ•¸æ“šé›†ä¸Šå¾®èª¿ä»»ä½•é è¨“ç·´æ¨¡å‹ã€‚å®Œæˆä¸Šä¸€ç¯€ä¸­çš„æ‰€æœ‰æ•¸æ“šé è™•ç†å·¥ä½œå¾Œï¼Œæ‚¨åªéœ€è¦åŸ·è¡Œå¹¾å€‹æ­¥é©Ÿä¾†å‰µå»º **Trainer** .æœ€é›£çš„éƒ¨åˆ†å¯èƒ½æ˜¯ç‚º **Trainer.train()**é…ç½®é‹è¡Œç’°å¢ƒï¼Œå› ç‚ºå®ƒåœ¨ CPU ä¸Šé‹è¡Œé€Ÿåº¦æœƒéå¸¸æ…¢ã€‚å¦‚æœæ‚¨æ²’æœ‰è¨­ç½® GPUï¼Œæ‚¨å¯ä»¥è¨ªå•å…è²»çš„ GPU æˆ– TPU[Google Colab](https://colab.research.google.com/).

ä¸‹é¢çš„ç¤ºä¾‹å‡è¨­æ‚¨å·²ç¶“åŸ·è¡Œäº†ä¸Šä¸€ç¯€ä¸­çš„ç¤ºä¾‹ã€‚ä¸‹é¢é€™æ®µä»£ç¢¼ï¼Œæ¦‚æ‹¬äº†æ‚¨éœ€è¦æå‰é‹è¡Œçš„ä»£ç¢¼ï¼š

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Training

åœ¨æˆ‘å€‘å®šç¾©æˆ‘å€‘çš„ **Trainer** ä¹‹å‰é¦–å…ˆè¦å®šç¾©ä¸€å€‹ **TrainingArguments** é¡ï¼Œå®ƒå°‡åŒ…å« **Trainer**ç”¨æ–¼è¨“ç·´å’Œè©•ä¼°çš„æ‰€æœ‰è¶…åƒæ•¸ã€‚æ‚¨å”¯ä¸€å¿…é ˆæä¾›çš„åƒæ•¸æ˜¯ä¿å­˜è¨“ç·´æ¨¡å‹çš„ç›®éŒ„ï¼Œä»¥åŠè¨“ç·´éç¨‹ä¸­çš„æª¢æŸ¥é»ã€‚å°æ–¼å…¶é¤˜çš„åƒæ•¸ï¼Œæ‚¨å¯ä»¥ä¿ç•™é»˜èªå€¼ï¼Œé€™å°æ–¼åŸºæœ¬å¾®èª¿æ‡‰è©²éå¸¸æœ‰æ•ˆã€‚

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<Tip>

ğŸ’¡ å¦‚æœæ‚¨æƒ³åœ¨è¨“ç·´æœŸé–“è‡ªå‹•å°‡æ¨¡å‹ä¸Šå‚³åˆ° Hubï¼Œè«‹å°‡push_to_hub=Trueæ·»åŠ åˆ°TrainingArgumentsä¹‹ä¸­. æˆ‘å€‘å°‡åœ¨[ç¬¬å››ç« ](/course/chapter4/3)ä¸­è©³ç´°ä»‹ç´¹é€™éƒ¨åˆ†ã€‚

</Tip>

ç¬¬äºŒæ­¥æ˜¯å®šç¾©æˆ‘å€‘çš„æ¨¡å‹ã€‚æ­£å¦‚åœ¨[ä¹‹å‰çš„ç« ç¯€](/2_Using Transformers/Introduction)ä¸€æ¨£ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ **AutoModelForSequenceClassification** é¡ï¼Œå®ƒæœ‰å…©å€‹åƒæ•¸ï¼š

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ æœƒæ³¨æ„åˆ°ï¼Œå’Œ[ç¬¬äºŒç« ](/course/chapter2)ä¸ä¸€æ¨£çš„æ˜¯ï¼Œåœ¨å¯¦ä¾‹åŒ–æ­¤é è¨“ç·´æ¨¡å‹å¾Œæœƒæ”¶åˆ°è­¦å‘Šã€‚é€™æ˜¯å› ç‚º BERT æ²’æœ‰åœ¨å¥å­å°åˆ†é¡æ–¹é¢é€²è¡Œéé è¨“ç·´ï¼Œæ‰€ä»¥é è¨“ç·´æ¨¡å‹çš„é ­éƒ¨å·²ç¶“è¢«ä¸Ÿæ£„ï¼Œè€Œæ˜¯æ·»åŠ äº†ä¸€å€‹é©åˆå¥å­åºåˆ—åˆ†é¡çš„æ–°é ­éƒ¨ã€‚è­¦å‘Šè¡¨æ˜ä¸€äº›æ¬Šé‡æ²’æœ‰ä½¿ç”¨ï¼ˆå°æ‡‰æ–¼ä¸Ÿæ£„çš„é è¨“ç·´é ­çš„é‚£äº›ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æ¬Šé‡è¢«éš¨æ©Ÿåˆå§‹åŒ–ï¼ˆæ–°é ­çš„é‚£äº›ï¼‰ã€‚æœ€å¾Œé¼“å‹µæ‚¨è¨“ç·´æ¨¡å‹ï¼Œé€™æ­£æ˜¯æˆ‘å€‘ç¾åœ¨è¦åšçš„ã€‚

ä¸€æ—¦æˆ‘å€‘æœ‰äº†æˆ‘å€‘çš„æ¨¡å‹ï¼Œæˆ‘å€‘å°±å¯ä»¥å®šç¾©ä¸€å€‹ **Trainer** é€šéå°‡ä¹‹å‰æ§‹é€ çš„æ‰€æœ‰å°è±¡å‚³éçµ¦å®ƒâ€”â€”æˆ‘å€‘çš„**model** ã€**training_args** ï¼Œè¨“ç·´å’Œé©—è­‰æ•¸æ“šé›†ï¼Œ**data_collator** ï¼Œå’Œ **tokenizer** ï¼š

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

è«‹æ³¨æ„ï¼Œç•¶æ‚¨åœ¨é€™è£¡å®Œæˆ**tokenizer**å¾Œï¼Œé»˜èª **Trainer**ä½¿ç”¨ çš„**data_collator**æœƒä½¿ç”¨ä¹‹å‰é å®šç¾©çš„ **DataCollatorWithPadding** ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨é€™å€‹ä¾‹å­ä¸­è·³é **data_collator=data_collator**ã€‚åœ¨ç¬¬ 2 ç¯€ä¸­å‘æ‚¨å±•ç¤ºé€™éƒ¨åˆ†è™•ç†ä»ç„¶å¾ˆé‡è¦ï¼

ç‚ºäº†è®“é è¨“ç·´æ¨¡å‹åœ¨åœ¨æˆ‘å€‘çš„æ•¸æ“šé›†ä¸Šå¾®èª¿ï¼Œæˆ‘å€‘åªéœ€è¦èª¿ç”¨**Trainer**çš„**train()** æ–¹æ³• ï¼š

```py
trainer.train()
```

é€™å°‡é–‹å§‹å¾®èª¿ï¼ˆåœ¨GPUä¸Šæ‡‰è©²éœ€è¦å¹¾åˆ†é˜ï¼‰ï¼Œä¸¦æ¯500æ­¥å ±å‘Šä¸€æ¬¡è¨“ç·´æå¤±ã€‚ä½†æ˜¯ï¼Œå®ƒä¸æœƒå‘Šè¨´æ‚¨æ¨¡å‹çš„æ€§èƒ½å¦‚ä½•ï¼ˆæˆ–è³ªé‡å¦‚ä½•ï¼‰ã€‚é€™æ˜¯å› ç‚º:

1. æˆ‘å€‘æ²’æœ‰é€šéå°‡**evaluation_strategy**è¨­ç½®ç‚ºâ€œ**steps**â€(åœ¨æ¯æ¬¡æ›´æ–°åƒæ•¸çš„æ™‚å€™è©•ä¼°)æˆ–â€œ**epoch**â€(åœ¨æ¯å€‹epochçµæŸæ™‚è©•ä¼°)ä¾†å‘Šè¨´**Trainer**åœ¨è¨“ç·´æœŸé–“é€²è¡Œè©•ä¼°ã€‚
2. æˆ‘å€‘æ²’æœ‰ç‚º**Trainer**æä¾›ä¸€å€‹**compute_metrics()**å‡½æ•¸ä¾†ç›´æ¥è¨ˆç®—æ¨¡å‹çš„å¥½å£(å¦å‰‡è©•ä¼°å°‡åªè¼¸å‡ºlossï¼Œé€™ä¸æ˜¯ä¸€å€‹éå¸¸ç›´è§€çš„æ•¸å­—)ã€‚


### è©•ä¼°

è®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•æ§‹å»ºä¸€å€‹æœ‰ç”¨çš„ **compute_metrics()** å‡½æ•¸ä¸¦åœ¨æˆ‘å€‘ä¸‹æ¬¡è¨“ç·´æ™‚ä½¿ç”¨å®ƒã€‚è©²å‡½æ•¸å¿…é ˆæ¡ç”¨ **EvalPrediction** å°è±¡ï¼ˆå¸¶æœ‰ **predictions** å’Œ **label_ids** å­—æ®µçš„åƒæ•¸å…ƒçµ„ï¼‰ä¸¦å°‡è¿”å›ä¸€å€‹å­—ç¬¦ä¸²åˆ°æµ®é»æ•¸çš„å­—å…¸ï¼ˆå­—ç¬¦ä¸²æ˜¯è¿”å›çš„æŒ‡æ¨™çš„åç¨±ï¼Œè€Œæµ®é»æ•¸æ˜¯å®ƒå€‘çš„å€¼ï¼‰ã€‚æˆ‘å€‘å¯ä»¥ä½¿ç”¨ **Trainer.predict()** å‘½ä»¤ä¾†ä½¿ç”¨æˆ‘å€‘çš„æ¨¡å‹é€²è¡Œé æ¸¬ï¼š

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

 **predict()** çš„è¼¸å‡ºçµæœæ˜¯å…·æœ‰ä¸‰å€‹å­—æ®µçš„å‘½åå…ƒçµ„ï¼š **predictions** , **label_ids** ï¼Œ å’Œ **metrics** .é€™ **metrics** å­—æ®µå°‡åªåŒ…å«å‚³éçš„æ•¸æ“šé›†çš„lossï¼Œä»¥åŠä¸€äº›é‹è¡Œæ™‚é–“ï¼ˆé æ¸¬æ‰€éœ€çš„ç¸½æ™‚é–“å’Œå¹³å‡æ™‚é–“ï¼‰ã€‚å¦‚æœæˆ‘å€‘å®šç¾©äº†è‡ªå·±çš„ **compute_metrics()** å‡½æ•¸ä¸¦å°‡å…¶å‚³éçµ¦ **Trainer** ï¼Œè©²å­—æ®µé‚„å°‡åŒ…å«**compute_metrics()**çš„çµæœã€‚

**predict()** æ–¹æ³•æ˜¯å…·æœ‰ä¸‰å€‹å­—æ®µçš„å‘½åå…ƒçµ„ï¼š **predictions** , **label_ids** ï¼Œ å’Œ **metrics** .é€™ **metrics** å­—æ®µå°‡åªåŒ…å«å‚³éçš„æ•¸æ“šé›†çš„lossï¼Œä»¥åŠä¸€äº›é‹è¡Œæ™‚é–“ï¼ˆé æ¸¬æ‰€éœ€çš„ç¸½æ™‚é–“å’Œå¹³å‡æ™‚é–“ï¼‰ã€‚å¦‚æœæˆ‘å€‘å®šç¾©äº†è‡ªå·±çš„ **compute_metrics()** å‡½æ•¸ä¸¦å°‡å…¶å‚³éçµ¦ **Trainer** ï¼Œè©²å­—æ®µé‚„å°‡åŒ…å«**compute_metrics()** çš„çµæœã€‚å¦‚ä½ çœ‹åˆ°çš„ï¼Œ **predictions** æ˜¯ä¸€å€‹å½¢ç‹€ç‚º 408 x 2 çš„äºŒç¶­æ•¸çµ„ï¼ˆ408 æ˜¯æˆ‘å€‘ä½¿ç”¨çš„æ•¸æ“šé›†ä¸­å…ƒç´ çš„æ•¸é‡ï¼‰ã€‚é€™äº›æ˜¯æˆ‘å€‘å‚³éçµ¦**predict()**çš„æ•¸æ“šé›†çš„æ¯å€‹å…ƒç´ çš„çµæœ(logits)ï¼ˆæ­£å¦‚ä½ åœ¨[ä¹‹å‰çš„ç« ç¯€](/course/chapter2)çœ‹åˆ°çš„æƒ…æ³ï¼‰ã€‚è¦å°‡æˆ‘å€‘çš„é æ¸¬çš„å¯ä»¥èˆ‡çœŸæ­£çš„æ¨™ç±¤é€²è¡Œæ¯”è¼ƒï¼Œæˆ‘å€‘éœ€è¦åœ¨ç¬¬äºŒå€‹è»¸ä¸Šå–æœ€å¤§å€¼çš„ç´¢å¼•ï¼š

```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

ç¾åœ¨å»ºç«‹æˆ‘å€‘çš„ **compute_metric()** å‡½æ•¸ä¾†è¼ƒç‚ºç›´è§€åœ°è©•ä¼°æ¨¡å‹çš„å¥½å£ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ ğŸ¤— [Evaluate](https://github.com/huggingface/evaluate/) åº«ä¸­çš„æŒ‡æ¨™ã€‚æˆ‘å€‘å¯ä»¥åƒåŠ è¼‰æ•¸æ“šé›†ä¸€æ¨£è¼•é¬†åŠ è¼‰èˆ‡ MRPC æ•¸æ“šé›†é—œè¯çš„æŒ‡æ¨™ï¼Œé€™æ¬¡ä½¿ç”¨ **evaluate.load()** å‡½æ•¸ã€‚è¿”å›çš„å°è±¡æœ‰ä¸€å€‹ **compute()**æ–¹æ³•æˆ‘å€‘å¯ä»¥ç”¨ä¾†é€²è¡Œåº¦é‡è¨ˆç®—çš„æ–¹æ³•ï¼š

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

æ‚¨ç²å¾—çš„ç¢ºåˆ‡çµæœå¯èƒ½æœƒæœ‰æ‰€ä¸åŒï¼Œå› ç‚ºæ¨¡å‹é ­çš„éš¨æ©Ÿåˆå§‹åŒ–å¯èƒ½æœƒå½±éŸ¿æœ€çµ‚å»ºç«‹çš„æ¨¡å‹ã€‚åœ¨é€™è£¡ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°æˆ‘å€‘çš„æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„æº–ç¢ºç‡ç‚º 85.78%ï¼ŒF1 åˆ†æ•¸ç‚º 89.97ã€‚é€™æ˜¯ç”¨æ–¼è©•ä¼° GLUE åŸºæº–çš„ MRPC æ•¸æ“šé›†çµæœçš„å…©å€‹æŒ‡æ¨™ã€‚è€Œåœ¨[BERT è«–æ–‡](https://arxiv.org/pdf/1810.04805.pdf)ä¸­å±•ç¤ºçš„åŸºç¤æ¨¡å‹çš„ F1 åˆ†æ•¸ç‚º 88.9ã€‚é‚£æ˜¯ **uncased** æ¨¡å‹ï¼Œè€Œæˆ‘å€‘ç›®å‰æ­£åœ¨ä½¿ç”¨ **cased** æ¨¡å‹ï¼Œé€šéæ”¹é€²å¾—åˆ°äº†æ›´å¥½çš„çµæœã€‚

æœ€å¾Œå°‡æ‰€æœ‰æ±è¥¿æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæˆ‘å€‘å¾—åˆ°äº†æˆ‘å€‘çš„ **compute_metrics()** å‡½æ•¸ï¼š

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ç‚ºäº†æŸ¥çœ‹æ¨¡å‹åœ¨æ¯å€‹è¨“ç·´é€±æœŸçµæŸçš„å¥½å£ï¼Œä¸‹é¢æ˜¯æˆ‘å€‘å¦‚ä½•ä½¿ç”¨**compute_metrics()**å‡½æ•¸å®šç¾©ä¸€å€‹æ–°çš„ **Trainer** ï¼š

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

è«‹æ³¨æ„ï¼Œæˆ‘å€‘è¨­ç½®äº†äº†ä¸€å€‹æ–°çš„ **TrainingArguments** å®ƒçš„**evaluation_strategy** è¨­ç½®ç‚º **epoch** ä¸¦å‰µå»ºäº†ä¸€å€‹æ–°æ¨¡å‹ã€‚å¦‚æœä¸å‰µå»ºæ–°çš„æ¨¡å‹å°±ç›´æ¥è¨“ç·´ï¼Œå°±åªæœƒç¹¼çºŒè¨“ç·´ä¹‹å‰æˆ‘å€‘å·²ç¶“è¨“ç·´éçš„æ¨¡å‹ã€‚è¦å•Ÿå‹•æ–°çš„è¨“ç·´é‹è¡Œï¼Œæˆ‘å€‘åŸ·è¡Œï¼š

```
trainer.train()
```

é€™ä¸€æ¬¡ï¼Œå®ƒå°‡åœ¨è¨“ç·´lossä¹‹å¤–ï¼Œé‚„æœƒè¼¸å‡ºæ¯å€‹ epoch çµæŸæ™‚çš„é©—è­‰losså’ŒæŒ‡æ¨™ã€‚åŒæ¨£ï¼Œç”±æ–¼æ¨¡å‹çš„éš¨æ©Ÿé ­éƒ¨åˆå§‹åŒ–ï¼Œæ‚¨é”åˆ°çš„æº–ç¢ºç‡/F1 åˆ†æ•¸å¯èƒ½èˆ‡æˆ‘å€‘ç™¼ç¾çš„ç•¥æœ‰ä¸åŒï¼Œä½†å®ƒæ‡‰è©²åœ¨åŒä¸€ç¯„åœå…§ã€‚

é€™ **Trainer** å°‡åœ¨å¤šå€‹ GPU æˆ– TPU ä¸Šé–‹ç®±å³ç”¨ï¼Œä¸¦æä¾›è¨±å¤šé¸é …ï¼Œä¾‹å¦‚æ··åˆç²¾åº¦è¨“ç·´ï¼ˆåœ¨è¨“ç·´çš„åƒæ•¸ä¸­ä½¿ç”¨ **fp16 = True** ï¼‰ã€‚æˆ‘å€‘å°‡åœ¨ç¬¬ 10 ç« è¨è«–å®ƒæ”¯æŒçš„æ‰€æœ‰å…§å®¹ã€‚

ä½¿ç”¨**Trainer** APIå¾®èª¿çš„ä»‹ç´¹åˆ°æ­¤çµæŸã€‚å°æœ€å¸¸è¦‹çš„ NLP ä»»å‹™åŸ·è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹å°‡åœ¨ç¬¬ 7 ç« ä¸­çµ¦å‡ºï¼Œä½†ç¾åœ¨è®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•åœ¨ç´” PyTorch ä¸­åŸ·è¡Œç›¸åŒçš„æ“ä½œã€‚

<Tip>

âœï¸ **è©¦è©¦çœ‹!** ä½¿ç”¨æ‚¨åœ¨ç¬¬ 2 ç¯€ä¸­é€²è¡Œçš„æ•¸æ“šè™•ç†ï¼Œåœ¨ GLUE SST-2 æ•¸æ“šé›†ä¸Šå¾®èª¿æ¨¡å‹ã€‚

</Tip>

