<FrameworkSwitchCourse {fw} />

# èª¿è©¦è¨“ç·´ç®¡é“

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4.ipynb"},
]} />

ä½ å·²ç¶“ç·¨å¯«äº†ä¸€å€‹æ¼‚äº®çš„è…³æœ¬ä¾†è¨“ç·´æˆ–å¾®èª¿çµ¦å®šä»»å‹™çš„æ¨¡å‹ï¼Œç›¡è·ç›¡è²¬åœ°éµå¾ª [Chapter 7](/course/chapter7) ä¸­çš„å»ºè­°ã€‚ ä½†æ˜¯ç•¶ä½ å•Ÿå‹•å‘½ä»¤ `trainer.train()` æ™‚ï¼Œå¯æ€•çš„äº‹æƒ…ç™¼ç”Ÿäº†ï¼šä½ å¾—åˆ°ä¸€å€‹éŒ¯èª¤ğŸ˜±ï¼ æˆ–è€…æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¸€åˆ‡ä¼¼ä¹éƒ½å¾ˆå¥½ï¼Œè¨“ç·´é‹è¡Œæ²’æœ‰éŒ¯èª¤ï¼Œä½†ç”Ÿæˆçš„æ¨¡å‹å¾ˆç³Ÿç³•ã€‚ åœ¨æœ¬ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡å‘æ‚¨å±•ç¤ºå¦‚ä½•èª¿è©¦æ­¤é¡å•é¡Œã€‚

## èª¿è©¦è¨“ç·´ç®¡é“

<Youtube id="L-WSwUWde1U"/>

ç•¶æ‚¨åœ¨ `trainer.train()` ä¸­é‡åˆ°éŒ¯èª¤æ™‚ï¼Œå®ƒå¯èƒ½ä¾†è‡ªå¤šå€‹ä¾†æºï¼Œå› ç‚º `Trainer` é€šå¸¸æœƒå°‡å¾ˆå¤šæ±è¥¿æ”¾åœ¨ä¸€èµ·çµ„åˆé‹è¡Œã€‚ å®ƒå°‡datasetsè½‰æ›ç‚ºdataloadersï¼Œå› æ­¤å•é¡Œå¯èƒ½å‡ºåœ¨datasetsä¸­ï¼Œæˆ–è€…åœ¨å˜—è©¦å°‡datasetsçš„å…ƒç´ ä¸€èµ·æ‰¹è™•ç†æ™‚å‡ºç¾å•é¡Œã€‚ ç„¶å¾Œå®ƒéœ€è¦æº–å‚™ä¸€æ‰¹æ•¸æ“šä¸¦å°‡å…¶æä¾›çµ¦æ¨¡å‹ï¼Œå› æ­¤å•é¡Œå¯èƒ½å‡ºåœ¨æ¨¡å‹ä»£ç¢¼ä¸­ã€‚ ä¹‹å¾Œï¼Œå®ƒæœƒè¨ˆç®—æ¢¯åº¦ä¸¦åŸ·è¡Œå„ªåŒ–å™¨ï¼Œå› æ­¤å•é¡Œä¹Ÿå¯èƒ½å‡ºåœ¨æ‚¨çš„å„ªåŒ–å™¨ä¸­ã€‚ å³ä½¿è¨“ç·´ä¸€åˆ‡é †åˆ©ï¼Œå¦‚æœæ‚¨çš„è©•ä¼°æŒ‡æ¨™æœ‰å•é¡Œï¼Œè©•ä¼°æœŸé–“ä»ç„¶å¯èƒ½å‡ºç¾å•é¡Œã€‚

èª¿è©¦ `trainer.train()` ä¸­å‡ºç¾çš„éŒ¯èª¤çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰‹å‹•æª¢æŸ¥æ•´å€‹ç®¡é“ï¼Œçœ‹çœ‹å“ªè£¡å‡ºäº†å•é¡Œã€‚ éŒ¯èª¤é€šå¸¸å¾ˆå®¹æ˜“è§£æ±ºã€‚

ç‚ºäº†è­‰æ˜é€™ä¸€é»ï¼Œæˆ‘å€‘å°‡ä½¿ç”¨ä»¥ä¸‹è…³æœ¬ï¼ˆå˜—è©¦ï¼‰åœ¨ [MNLI æ•¸æ“šé›†](https://huggingface.co/datasets/glue)ä¸Šå¾®èª¿ DistilBERT æ¨¡å‹ï¼š

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

å¦‚æœä½ å˜—è©¦åŸ·è¡Œå®ƒï¼Œä½ æœƒé‡åˆ°ä¸€å€‹ç›¸ç•¶ç¥ç§˜çš„éŒ¯èª¤ï¼š

```python out
'ValueError: You have to specify either input_ids or inputs_embeds'
```

### æª¢æŸ¥æ•¸æ“š

é€™æ˜¯ä¸è¨€è€Œå–»çš„ï¼Œå¦‚æœä½ çš„æ•¸æ“šè¢«ç ´å£ï¼Œâ€œTrainerâ€å°‡ç„¡æ³•å½¢æˆæ‰¹æ¬¡ï¼Œæ›´ä¸ç”¨èªªè¨“ç·´ä½ çš„æ¨¡å‹äº†ã€‚ æ‰€ä»¥é¦–å…ˆï¼Œä½ éœ€è¦çœ‹çœ‹ä½ çš„è¨“ç·´é›†ä¸­æœ‰ä»€éº¼ã€‚

ç‚ºäº†é¿å…èŠ±è²»ç„¡æ•¸å°æ™‚è©¦åœ–æª¢æŸ¥å’Œä¿®å¾©ä¸æ˜¯éŒ¯èª¤ä¾†æºçš„æ±è¥¿ï¼Œæˆ‘å€‘å»ºè­°æ‚¨ä½¿ç”¨ `trainer.train_dataset` é€²è¡Œæª¢æŸ¥ã€‚ æ‰€ä»¥è®“æˆ‘å€‘åœ¨é€™è£¡é€™æ¨£åšï¼š

```py
trainer.train_dataset[0]
```

```python out
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```

ä½ æ³¨æ„åˆ°æœ‰ä»€éº¼ä¸å°å—ï¼Ÿ èˆ‡ç¼ºå°‘æœ‰é—œ `input_ids` çš„éŒ¯èª¤æ¶ˆæ¯ç›¸çµåˆï¼Œæ‡‰è©²è®“æ‚¨æ„è­˜åˆ°æ•¸æ“šé›†è£¡æ˜¯æ–‡æœ¬ï¼Œè€Œä¸æ˜¯æ¨¡å‹å¯ä»¥ç†è§£çš„æ•¸å­—ã€‚ åœ¨é€™å€‹ä¾‹å­ï¼Œè¼¸å‡ºçš„åŸå§‹éŒ¯èª¤ä¿¡æ¯éå¸¸å…·æœ‰èª¤å°æ€§ï¼Œå› ç‚º `Trainer` æœƒè‡ªå‹•åˆªé™¤èˆ‡æ¨¡å‹ç°½åä¸åŒ¹é…çš„åˆ—ï¼ˆå³æ¨¡å‹é æœŸçš„åƒæ•¸ï¼‰ã€‚ é€™æ„å‘³è‘—åœ¨é€™è£¡ï¼Œé™¤äº†æ¨™ç±¤ä¹‹å¤–çš„æ‰€æœ‰æ±è¥¿éƒ½è¢«ä¸Ÿæ£„äº†ã€‚ å› æ­¤ï¼Œå‰µå»ºæ‰¹æ¬¡ç„¶å¾Œå°‡å®ƒå€‘ç™¼é€åˆ°æ¨¡å‹æ²’æœ‰å•é¡Œï¼Œè€Œæ¨¡å‹åˆæŠ±æ€¨å®ƒæ²’æœ‰æ”¶åˆ°æ­£ç¢ºçš„è¼¸å…¥ã€‚

ç‚ºä»€éº¼æ²’æœ‰è™•ç†æ•¸æ“šç”Ÿæˆæ¨™è¨˜å‘¢ï¼Ÿ æˆ‘å€‘ç¢ºå¯¦åœ¨æ•¸æ“šé›†ä¸Šä½¿ç”¨äº†â€œDataset.map()â€æ–¹æ³•ä¾†å°æ¯å€‹æ¨£æœ¬æ‡‰ç”¨æ¨™è¨˜å™¨ã€‚ ä½†æ˜¯å¦‚æœä½ ä»”ç´°çœ‹ä»£ç¢¼ï¼Œä½ æœƒç™¼ç¾æˆ‘å€‘åœ¨å°‡è¨“ç·´å’Œè©•ä¼°é›†å‚³éçµ¦`Trainer`æ™‚çŠ¯äº†ä¸€å€‹éŒ¯èª¤ã€‚ æˆ‘å€‘åœ¨é€™è£¡æ²’æœ‰ä½¿ç”¨ `tokenized_datasets`ï¼Œè€Œæ˜¯ä½¿ç”¨äº† `raw_datasets` ğŸ¤¦ã€‚ æ‰€ä»¥è®“æˆ‘å€‘è§£æ±ºé€™å€‹å•é¡Œï¼

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

é€™å€‹æ–°ä»£ç¢¼ç¾åœ¨æœƒçµ¦å‡ºä¸€å€‹ä¸åŒçš„éŒ¯èª¤ï¼š

```python out
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```

æŸ¥çœ‹tracebackï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°éŒ¯èª¤ç™¼ç”Ÿåœ¨æ•¸æ“šæ•´ç†æ­¥é©Ÿä¸­ï¼š

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

æ‰€ä»¥ï¼Œæˆ‘å€‘æ‡‰è©²å»ç ”ç©¶ä¸€ä¸‹é‚£å€‹ã€‚ ç„¶è€Œï¼Œåœ¨æˆ‘å€‘é€™æ¨£åšä¹‹å‰ï¼Œè®“æˆ‘å€‘å®Œæˆæª¢æŸ¥æˆ‘å€‘çš„æ•¸æ“šï¼Œ å…ˆç¢ºå®šå®ƒ100%æ˜¯æ­£ç¢ºçš„ã€‚

åœ¨èª¿è©¦èª²ç¨‹çš„å…§å®¹æ™‚ï¼Œæ‚¨æ‡‰è©²å§‹çµ‚åšçš„ä¸€ä»¶äº‹æ˜¯æŸ¥çœ‹æ¨¡å‹çš„è§£ç¢¼è¼¸å…¥ã€‚ æˆ‘å€‘ç„¡æ³•ç†è§£ç›´æ¥æä¾›çµ¦å®ƒçš„æ•¸å­—ï¼Œæ‰€ä»¥æˆ‘å€‘æ‡‰è©²çœ‹çœ‹é€™äº›æ•¸å­—ä»£è¡¨ä»€éº¼ã€‚ ä¾‹å¦‚ï¼Œåœ¨è¨ˆç®—æ©Ÿè¦–è¦ºä¸­ï¼Œé€™æ„å‘³è‘—æŸ¥çœ‹æ‚¨å‚³éçš„åœ–ç‰‡åƒç´ çš„è§£ç¢¼ï¼Œåœ¨èªéŸ³ä¸­æ„å‘³è‘—è§£ç¢¼å¾Œçš„éŸ³é »æ¨£æœ¬ï¼Œå°æ–¼æˆ‘å€‘çš„ NLP ç¤ºä¾‹ï¼Œé€™æ„å‘³è‘—ä½¿ç”¨æˆ‘å€‘çš„æ¨™è¨˜å™¨è§£ç¢¼çš„è¼¸å…¥ï¼š

```py
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python out
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

æ‰€ä»¥é€™ä¼¼ä¹æ˜¯æ­£ç¢ºçš„ã€‚ æ‚¨æ‡‰è©²å°è¼¸å…¥ä¸­çš„æ‰€æœ‰éµåŸ·è¡Œæ­¤æ“ä½œï¼š

```py
trainer.train_dataset[0].keys()
```

```python out
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

è«‹æ³¨æ„ï¼Œèˆ‡æ¨¡å‹æ¥å—çš„è¼¸å…¥ä¸å°æ‡‰çš„éµå°‡è¢«è‡ªå‹•ä¸Ÿæ£„ï¼Œå› æ­¤é€™è£¡æˆ‘å€‘å°‡åƒ…ä¿ç•™ `input_ids`ã€`attention_mask` å’Œ `label`ï¼ˆå°‡é‡å‘½åç‚º `labels`ï¼‰ã€‚ è¦ä»”ç´°æª¢æŸ¥æ¨¡å‹è¼¸å…¥çš„åˆ—ï¼Œæ‚¨å¯ä»¥æ‰“å°æ¨¡å‹çš„é¡ï¼Œç„¶å¾ŒæŸ¥çœ‹å…¶æ–‡æª”ï¼š

```py
type(trainer.model)
```

```python out
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

æ‰€ä»¥åœ¨æˆ‘å€‘çš„ä¾‹å­ä¸­ï¼Œæˆ‘å€‘åœ¨[åœ¨é€™å€‹é é¢](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification)å¯ä»¥æª¢æŸ¥ä¸Šæ¥å—çš„åƒæ•¸ã€‚ `Trainer` ä¹Ÿæœƒè¨˜éŒ„å®ƒä¸Ÿæ£„çš„åˆ—ã€‚

æˆ‘å€‘é€šéè§£ç¢¼æª¢æŸ¥äº†è¼¸å…¥ ID æ˜¯å¦æ­£ç¢ºã€‚ æ¥ä¸‹ä¾†æ˜¯æª¢æŸ¥ `attention_mask`ï¼š

```py
tokenizer.decode(trainer.train_dataset[0]["attention_mask"])
```

```python out
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ç”±æ–¼æˆ‘å€‘æ²’æœ‰åœ¨é è™•ç†ä¸­æ‡‰ç”¨å¡«å……ï¼Œé€™çœ‹èµ·ä¾†éå¸¸è‡ªç„¶ã€‚ ç‚ºç¢ºä¿è©²æ³¨æ„æ©ç¢¼æ²’æœ‰å•é¡Œï¼Œè®“æˆ‘å€‘æª¢æŸ¥å®ƒèˆ‡è¼¸å…¥ ID çš„é•·åº¦æ˜¯å¦ç›¸åŒï¼š

```py
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python out
True
```

é‚£æŒºå¥½çš„ï¼ æœ€å¾Œï¼Œè®“æˆ‘å€‘æª¢æŸ¥ä¸€ä¸‹æˆ‘å€‘çš„æ¨™ç±¤ï¼š

```py
trainer.train_dataset[0]["label"]
```

```python out
1
```

èˆ‡è¼¸å…¥ ID ä¸€æ¨£ï¼Œé€™æ˜¯ä¸€å€‹æœ¬èº«ä¸¦æ²’æœ‰çœŸæ­£æ„ç¾©çš„æ•¸å­—ã€‚ æ­£å¦‚æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„ï¼Œæ•´æ•¸å’Œæ¨™ç±¤åç¨±ä¹‹é–“çš„æ˜ å°„å­˜å„²åœ¨æ•¸æ“šé›†ç›¸æ‡‰ *feature* çš„ `names` å±¬æ€§ä¸­ï¼š

```py
trainer.train_dataset.features["label"].names
```

```python out
['entailment', 'neutral', 'contradiction']
```

æ‰€ä»¥`1`è¡¨ç¤º`neutral`ï¼Œè¡¨ç¤ºæˆ‘å€‘ä¸Šé¢çœ‹åˆ°çš„å…©å¥è©±ä¸¦ä¸çŸ›ç›¾ï¼Œä¹Ÿæ²’æœ‰åŒ…å«é—œä¿‚ã€‚ é€™ä¼¼ä¹æ˜¯æ­£ç¢ºçš„ï¼

æˆ‘å€‘é€™è£¡æ²’æœ‰ä»¤ç‰Œé¡å‹ IDï¼Œå› ç‚º DistilBERT ä¸éœ€è¦å®ƒå€‘ï¼› å¦‚æœæ‚¨çš„æ¨¡å‹ä¸­æœ‰ä¸€äº›ï¼Œæ‚¨é‚„æ‡‰è©²ç¢ºä¿å®ƒå€‘æ­£ç¢ºåŒ¹é…è¼¸å…¥ä¸­ç¬¬ä¸€å¥å’Œç¬¬äºŒå¥çš„ä½ç½®ã€‚

<Tip>

âœï¸ **è¼ªåˆ°ä½ äº†ï¼** æª¢æŸ¥è¨“ç·´æ•¸æ“šé›†çš„ç¬¬äºŒå€‹å…ƒç´ æ˜¯å¦æ­£ç¢ºã€‚

</Tip>

æˆ‘å€‘åœ¨é€™è£¡åªå°è¨“ç·´é›†é€²è¡Œæª¢æŸ¥ï¼Œä½†æ‚¨ç•¶ç„¶æ‡‰è©²ä»¥åŒæ¨£çš„æ–¹å¼ä»”ç´°æª¢æŸ¥é©—è­‰é›†å’Œæ¸¬è©¦é›†ã€‚

ç¾åœ¨æˆ‘å€‘çŸ¥é“æˆ‘å€‘çš„æ•¸æ“šé›†çœ‹èµ·ä¾†ä¸éŒ¯ï¼Œæ˜¯æ™‚å€™æª¢æŸ¥è¨“ç·´ç®¡é“çš„ä¸‹ä¸€æ­¥äº†ã€‚

### å¾ datasets åˆ° dataloaders

è¨“ç·´ç®¡é“ä¸­å¯èƒ½å‡ºéŒ¯çš„ä¸‹ä¸€ä»¶äº‹æ˜¯ç•¶â€œTrainerâ€å˜—è©¦å¾è¨“ç·´æˆ–é©—è­‰é›†å½¢æˆæ‰¹æ¬¡æ™‚ã€‚ ä¸€æ—¦ä½ ç¢ºå®š `Trainer` çš„æ•¸æ“šé›†æ˜¯æ­£ç¢ºçš„ï¼Œä½ å¯ä»¥å˜—è©¦é€šéåŸ·è¡Œä»¥ä¸‹æ“ä½œæ‰‹å‹•å½¢æˆä¸€å€‹æ‰¹æ¬¡ï¼ˆå¯ä»¥å°‡ `train` æ›¿æ›ç‚º `eval` ç”¨æ–¼é©—è­‰æ•¸æ“šåŠ è¼‰å™¨ï¼‰ï¼š

```py
for batch in trainer.get_train_dataloader():
    break
```

æ­¤ä»£ç¢¼å‰µå»ºè¨“ç·´æ•¸æ“šåŠ è¼‰å™¨ï¼Œç„¶å¾Œå°å…¶é€²è¡Œè¿­ä»£ï¼Œåœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ™‚åœæ­¢ã€‚ å¦‚æœä»£ç¢¼åŸ·è¡Œæ²’æœ‰éŒ¯èª¤ï¼Œé‚£éº¼æ‚¨å°±æœ‰äº†å¯ä»¥æª¢æŸ¥çš„ç¬¬ä¸€å€‹è¨“ç·´æ‰¹æ¬¡ï¼Œå¦‚æœä»£ç¢¼å‡ºéŒ¯ï¼Œæ‚¨å¯ä»¥ç¢ºå®šå•é¡Œå‡ºåœ¨æ•¸æ“šåŠ è¼‰å™¨ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python out
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

æª¢æŸ¥trackbackçš„æœ€å¾Œä¸€å€‹å †æ£§çš„è¼¸å‡ºæ‡‰è©²è¶³ä»¥çµ¦ä½ ä¸€å€‹ç·šç´¢ï¼Œä½†è®“æˆ‘å€‘åšæ›´å¤šçš„æŒ–æ˜ã€‚ æ‰¹è™•ç†å‰µå»ºéç¨‹ä¸­çš„å¤§å¤šæ•¸å•é¡Œæ˜¯ç”±æ–¼å°‡ç¤ºä¾‹æ•´ç†åˆ°å–®å€‹æ‰¹è™•ç†ä¸­è€Œå‡ºç¾çš„ï¼Œå› æ­¤åœ¨æœ‰ç–‘å•æ™‚é¦–å…ˆè¦æª¢æŸ¥çš„æ˜¯æ‚¨çš„ DataLoader æ­£åœ¨ä½¿ç”¨ä»€éº¼ collate_fnï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python out
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

æ‰€ä»¥ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯ `default_data_collator`ï¼Œä½†é€™ä¸æ˜¯æˆ‘å€‘åœ¨é€™ç¨®æƒ…æ³ä¸‹æƒ³è¦çš„ã€‚ æˆ‘å€‘å¸Œæœ›å°‡ç¤ºä¾‹å¡«å……åˆ°æ‰¹è™•ç†ä¸­æœ€é•·çš„å¥å­ï¼Œé€™æ˜¯ç”± `DataCollatorWithPadding` æ•´ç†å™¨å®Œæˆçš„ã€‚ è€Œé€™å€‹æ•¸æ“šæ”¶é›†å™¨æ‡‰è©²æ˜¯é»˜èªè¢« `Trainer` ä½¿ç”¨çš„ï¼Œç‚ºä»€éº¼é€™è£¡æ²’æœ‰ä½¿ç”¨å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯å› ç‚ºæˆ‘å€‘æ²’æœ‰å°‡ `tokenizer` å‚³éçµ¦ `Trainer`ï¼Œæ‰€ä»¥å®ƒç„¡æ³•å‰µå»ºæˆ‘å€‘æƒ³è¦çš„ `DataCollatorWithPadding`ã€‚ åœ¨å¯¦è¸ä¸­ï¼Œæ‚¨æ‡‰è©²æ˜ç¢ºåœ°å‚³éæ‚¨æƒ³è¦ä½¿ç”¨çš„æ•¸æ“šæ•´ç†å™¨ï¼Œä»¥ç¢ºä¿é¿å…é€™äº›é¡å‹çš„éŒ¯èª¤ã€‚ è®“æˆ‘å€‘èª¿æ•´æˆ‘å€‘çš„ä»£ç¢¼ä¾†åšåˆ°é€™ä¸€é»ï¼š

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

å¥½æ¶ˆæ¯ï¼Ÿ æˆ‘å€‘æ²’æœ‰å¾—åˆ°èˆ‡ä»¥å‰ç›¸åŒçš„éŒ¯èª¤ï¼Œé€™çµ•å°æ˜¯é€²æ­¥ã€‚ å£æ¶ˆæ¯ï¼Ÿ æˆ‘å€‘å¾—åˆ°äº†ä¸€å€‹è‡­åæ˜­è‘—çš„ CUDA éŒ¯èª¤ï¼š

```python out
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```

é€™å¾ˆç³Ÿç³•ï¼Œå› ç‚º CUDA éŒ¯èª¤é€šå¸¸å¾ˆé›£èª¿è©¦ã€‚ æˆ‘å€‘ç¨å¾Œæœƒçœ‹åˆ°å¦‚ä½•è§£æ±ºé€™å€‹å•é¡Œï¼Œä½†é¦–å…ˆè®“æˆ‘å€‘å®Œæˆå°æ‰¹è™•ç†å‰µå»ºçš„åˆ†æã€‚

å¦‚æœæ‚¨ç¢ºå®šæ‚¨çš„æ•¸æ“šæ•´ç†å™¨æ˜¯æ­£ç¢ºçš„ï¼Œå‰‡æ‡‰å˜—è©¦å°‡å…¶æ‡‰ç”¨æ–¼æ•¸æ“šé›†çš„å¹¾å€‹æ¨£æœ¬ï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```

æ­¤ä»£ç¢¼å°‡å¤±æ•—ï¼Œå› ç‚º `train_dataset` åŒ…å«å­—ç¬¦ä¸²åˆ—ï¼Œ`Trainer` é€šå¸¸æœƒåˆªé™¤é€™äº›åˆ—ã€‚ æ‚¨å¯ä»¥æ‰‹å‹•åˆªé™¤å®ƒå€‘ï¼Œæˆ–è€…å¦‚æœæ‚¨æƒ³æº–ç¢ºåœ°ä¿®æ”¹ `Trainer` åœ¨å¹•å¾Œæ‰€åšçš„äº‹æƒ…ï¼Œæ‚¨å¯ä»¥èª¿ç”¨ç§æœ‰çš„ `Trainer._remove_unused_columns()` æ–¹æ³•ä¾†åŸ·è¡Œæ­¤æ“ä½œï¼š

```py
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```

å¦‚æœéŒ¯èª¤ä»ç„¶å­˜åœ¨ï¼Œæ‚¨æ‡‰è©²èƒ½å¤ æ‰‹å‹•èª¿è©¦æ•¸æ“šæ•´ç†å™¨å…§éƒ¨ä»¥ç¢ºå®šå…·é«”çš„å•é¡Œã€‚

ç¾åœ¨æˆ‘å€‘å·²ç¶“èª¿è©¦äº†æ‰¹è™•ç†å‰µå»ºéç¨‹ï¼Œæ˜¯æ™‚å€™å°‡æ•¸æ“šå‚³éçµ¦æ¨¡å‹äº†ï¼

### æª¢æŸ¥æ¨¡å‹

æ‚¨æ‡‰è©²èƒ½å¤ é€šéåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¾†ç²å¾—ä¸€å€‹æ‰¹æ¬¡çš„æ•¸æ“šï¼š

```py
for batch in trainer.get_train_dataloader():
    break
```

å¦‚æœæ‚¨åœ¨notebookä¸­é‹è¡Œæ­¤ä»£ç¢¼ï¼Œæ‚¨å¯èƒ½æœƒæ”¶åˆ°èˆ‡æˆ‘å€‘ä¹‹å‰çœ‹åˆ°çš„é¡ä¼¼çš„ CUDA éŒ¯èª¤ï¼Œåœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨éœ€è¦é‡æ–°å•Ÿå‹•notebookä¸¦é‡æ–°åŸ·è¡Œæœ€å¾Œä¸€å€‹ç‰‡æ®µï¼Œè€Œä¸é‹è¡Œ `trainer.train()` è¡Œ.é€™æ˜¯é—œæ–¼ CUDA éŒ¯èª¤çš„ç¬¬äºŒå€‹æœ€ç…©äººçš„äº‹æƒ…ï¼šå®ƒå€‘æœƒç ´å£æ‚¨çš„Cudaå…§æ ¸ï¼Œè€Œä¸”ç„¡æ³•æ¢å¾©ã€‚å®ƒå€‘æœ€ç…©äººçš„äº‹æƒ…æ˜¯å®ƒå€‘å¾ˆé›£èª¿è©¦ã€‚

é€™æ˜¯ç‚ºä»€éº¼ï¼Ÿå®ƒèˆ‡ GPU çš„å·¥ä½œæ–¹å¼æœ‰é—œã€‚å®ƒå€‘åœ¨ä¸¦è¡ŒåŸ·è¡Œå¤§é‡æ“ä½œæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†ç¼ºé»æ˜¯ç•¶å…¶ä¸­ä¸€æ¢æŒ‡ä»¤å°è‡´éŒ¯èª¤æ™‚ï¼Œæ‚¨ä¸æœƒç«‹å³çŸ¥é“ã€‚åªæœ‰ç•¶ç¨‹åºåœ¨ GPU ä¸Šèª¿ç”¨å¤šå€‹é€²ç¨‹çš„åŒæ­¥è™•ç†æ™‚ï¼Œå®ƒæ‰æœƒæ„è­˜åˆ°å‡ºç¾å•é¡Œï¼Œå› æ­¤éŒ¯èª¤å¯¦éš›ä¸Šæ˜¯åœ¨èˆ‡å‰µå»ºå®ƒçš„åŸå› ç„¡é—œçš„åœ°æ–¹å¼•ç™¼çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘å€‘æŸ¥çœ‹ä¹‹å‰çš„trackbackï¼ŒéŒ¯èª¤æ˜¯åœ¨å‘å¾Œå‚³éæœŸé–“å¼•ç™¼çš„ï¼Œä½†æˆ‘å€‘æœƒåœ¨ä¸€åˆ†é˜å…§çœ‹åˆ°å®ƒå¯¦éš›ä¸Šæºæ–¼å‘å‰å‚³éä¸­çš„æŸäº›æ±è¥¿ã€‚

é‚£éº¼æˆ‘å€‘å¦‚ä½•èª¿è©¦é€™äº›éŒ¯èª¤å‘¢ï¼Ÿç­”æ¡ˆå¾ˆç°¡å–®ï¼šæˆ‘å€‘æ²’æœ‰ã€‚é™¤éæ‚¨çš„ CUDA éŒ¯èª¤æ˜¯å…§å­˜ä¸è¶³éŒ¯èª¤ï¼ˆé€™æ„å‘³è‘—æ‚¨çš„ GPU ä¸­æ²’æœ‰è¶³å¤ çš„å…§å­˜ï¼‰ï¼Œå¦å‰‡æ‚¨æ‡‰è©²å§‹çµ‚è¿”å› CPU é€²è¡Œèª¿è©¦ã€‚

ç‚ºæ­¤ï¼Œæˆ‘å€‘åªéœ€å°‡æ¨¡å‹æ”¾å› CPU ä¸Šä¸¦åœ¨æˆ‘å€‘çš„ä¸€æ‰¹æ•¸æ“šä¸­èª¿ç”¨å®ƒâ€”â€”â€œDataLoaderâ€è¿”å›çš„é‚£æ‰¹æ•¸æ“šå°šæœªç§»å‹•åˆ° GPUï¼š

```python
outputs = trainer.model.cpu()(**batch)
```

```python out
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```

æ‰€ä»¥ï¼Œæ€è·¯è¶Šä¾†è¶Šæ¸…æ™°äº†ã€‚ æˆ‘å€‘ç¾åœ¨åœ¨æå¤±è¨ˆç®—ä¸­æ²’æœ‰å‡ºç¾ CUDA éŒ¯èª¤ï¼Œè€Œæ˜¯æœ‰ä¸€å€‹â€œIndexErrorâ€ï¼ˆå› æ­¤èˆ‡æˆ‘å€‘ä¹‹å‰æ‰€èªªçš„åå‘å‚³æ’­ç„¡é—œï¼‰ã€‚ æ›´æº–ç¢ºåœ°èªªï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°æ˜¯Target 2 é€ æˆäº†éŒ¯èª¤ï¼Œæ‰€ä»¥é€™æ˜¯æª¢æŸ¥æ¨¡å‹æ¨™ç±¤æ•¸é‡çš„å¥½æ™‚æ©Ÿï¼š

```python
trainer.model.config.num_labels
```

```python out
2
```

æœ‰å…©å€‹æ¨™ç±¤ï¼Œåªå…è¨± 0 å’Œ 1 ä½œç‚ºç›®æ¨™ï¼Œä½†æ˜¯æ ¹æ“šéŒ¯èª¤ä¿¡æ¯æˆ‘å€‘å¾—åˆ°ä¸€å€‹ 2ã€‚å¾—åˆ°ä¸€å€‹ 2 å¯¦éš›ä¸Šæ˜¯æ­£å¸¸çš„ï¼šå¦‚æœæˆ‘å€‘è¨˜å¾—æˆ‘å€‘ä¹‹å‰æå–çš„æ¨™ç±¤åç¨±ï¼Œæœ‰ä¸‰å€‹ï¼Œæ‰€ä»¥æˆ‘å€‘æœ‰ç´¢å¼• 0 , 1 å’Œ 2 åœ¨æˆ‘å€‘çš„æ•¸æ“šé›†ä¸­ã€‚ å•é¡Œæ˜¯æˆ‘å€‘æ²’æœ‰å‘Šè¨´æˆ‘å€‘çš„æ¨¡å‹ï¼Œå®ƒæ‡‰è©²å‰µå»ºä¸‰å€‹æ¨™ç±¤ã€‚ æ‰€ä»¥è®“æˆ‘å€‘è§£æ±ºé€™å€‹å•é¡Œï¼

```py
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

æˆ‘å€‘é‚„æ²’æœ‰åŒ…å« `trainer.train()` è¡Œï¼Œä»¥ä¾¿èŠ±æ™‚é–“æª¢æŸ¥ä¸€åˆ‡æ˜¯å¦æ­£å¸¸ã€‚ å¦‚æœæˆ‘å€‘è«‹æ±‚ä¸€å€‹æ‰¹æ¬¡çš„æ•¸æ“šä¸¦å°‡å…¶å‚³éçµ¦æˆ‘å€‘çš„æ¨¡å‹ï¼Œå®ƒç¾åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

ä¸‹ä¸€æ­¥æ˜¯å›åˆ° GPU ä¸¦æª¢æŸ¥ä¸€åˆ‡æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼š

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

å¦‚æœä»ç„¶å‡ºç¾éŒ¯èª¤ï¼Œè«‹ç¢ºä¿é‡æ–°å•Ÿå‹•notebookä¸¦åƒ…åŸ·è¡Œè…³æœ¬çš„æœ€å¾Œä¸€å€‹ç‰ˆæœ¬ã€‚

### åŸ·è¡Œä¸€å€‹å„ªåŒ–å™¨æ­¥é©Ÿ

ç¾åœ¨æˆ‘å€‘çŸ¥é“æˆ‘å€‘å¯ä»¥æ§‹å»ºå¯¦éš›é€šéæ¨¡å‹æª¢æŸ¥çš„æˆæ‰¹æ¬¡çš„æ•¸æ“šï¼Œæˆ‘å€‘å·²ç¶“ç‚ºè¨“ç·´ç®¡é“çš„ä¸‹ä¸€æ­¥åšå¥½æº–å‚™ï¼šè¨ˆç®—æ¢¯åº¦ä¸¦åŸ·è¡Œå„ªåŒ–æ­¥é©Ÿã€‚

ç¬¬ä¸€éƒ¨åˆ†åªæ˜¯åœ¨ loss ä¸Šèª¿ç”¨ `backward()` æ–¹æ³•ï¼š

```py
loss = outputs.loss
loss.backward()
```

åœ¨é€™å€‹éšæ®µå¾ˆå°‘å‡ºç¾éŒ¯èª¤ï¼Œä½†å¦‚æœç¢ºå¯¦å‡ºç¾éŒ¯èª¤ï¼Œè«‹è¿”å› CPU ä»¥ç²å–æœ‰ç”¨çš„éŒ¯èª¤æ¶ˆæ¯ã€‚

è¦åŸ·è¡Œå„ªåŒ–æ­¥é©Ÿï¼Œæˆ‘å€‘åªéœ€è¦å‰µå»º `optimizer` ä¸¦èª¿ç”¨å®ƒçš„ `step()` æ–¹æ³•ï¼š

```py
trainer.create_optimizer()
trainer.optimizer.step()
```

åŒæ¨£ï¼Œå¦‚æœæ‚¨åœ¨ `Trainer` ä¸­ä½¿ç”¨é»˜èªå„ªåŒ–å™¨ï¼Œå‰‡åœ¨æ­¤éšæ®µæ‚¨ä¸æ‡‰è©²æ”¶åˆ°éŒ¯èª¤ï¼Œä½†å¦‚æœæ‚¨æœ‰è‡ªå®šç¾©å„ªåŒ–å™¨ï¼Œå‰‡å¯èƒ½æœƒå‡ºç¾ä¸€äº›å•é¡Œéœ€è¦åœ¨é€™è£¡èª¿è©¦ã€‚ å¦‚æœæ‚¨åœ¨æ­¤éšæ®µé‡åˆ°å¥‡æ€ªçš„ CUDA éŒ¯èª¤ï¼Œè«‹ä¸è¦å¿˜è¨˜è¿”å› CPUã€‚ èªªåˆ° CUDA éŒ¯èª¤ï¼Œå‰é¢æˆ‘å€‘æåˆ°äº†ä¸€å€‹ç‰¹æ®Šæƒ…æ³ã€‚ ç¾åœ¨è®“æˆ‘å€‘ä¾†çœ‹çœ‹ã€‚

### è™•ç† CUDA out-of-memoryéŒ¯èª¤

æ¯ç•¶æ‚¨æ”¶åˆ°ä»¥`RuntimeError: CUDA out of memory`é–‹é ­çš„éŒ¯èª¤æ¶ˆæ¯æ™‚ï¼Œé€™è¡¨æ˜æ‚¨çš„ GPU å…§å­˜ä¸è¶³ã€‚ é€™èˆ‡æ‚¨çš„ä»£ç¢¼æ²’æœ‰ç›´æ¥é—œè¯ï¼Œä¸¦ä¸”å®ƒå¯èƒ½ç™¼ç”Ÿåœ¨é‹è¡Œè‰¯å¥½çš„ä»£ç¢¼ä¸­ã€‚ æ­¤éŒ¯èª¤æ„å‘³è‘—æ‚¨è©¦åœ–åœ¨ GPU çš„å…§éƒ¨å­˜å„²å™¨ä¸­æ”¾å…¥å¤ªå¤šæ±è¥¿ï¼Œé€™å°è‡´äº†éŒ¯èª¤ã€‚ èˆ‡å…¶ä»– CUDA éŒ¯èª¤ä¸€æ¨£ï¼Œæ‚¨éœ€è¦é‡æ–°å•Ÿå‹•å…§æ ¸æ‰èƒ½å†æ¬¡é‹è¡Œè¨“ç·´ã€‚

è¦è§£æ±ºé€™å€‹å•é¡Œï¼Œæ‚¨åªéœ€è¦ä½¿ç”¨æ›´å°‘çš„ GPU ç©ºé–“â€”â€”é€™å¾€å¾€èªªèµ·ä¾†å®¹æ˜“åšèµ·ä¾†é›£ã€‚ é¦–å…ˆï¼Œç¢ºä¿æ‚¨æ²’æœ‰åŒæ™‚åœ¨ GPU ä¸Šé‹è¡Œå…©å€‹æ¨¡å‹ï¼ˆç•¶ç„¶ï¼Œé™¤éæ‚¨çš„å•é¡Œéœ€è¦é€™æ¨£åšï¼‰ã€‚ ç„¶å¾Œï¼Œæ‚¨å¯èƒ½æ‡‰è©²æ¸›å°‘batchçš„å¤§å°ï¼Œå› ç‚ºå®ƒç›´æ¥å½±éŸ¿æ¨¡å‹çš„æ‰€æœ‰ä¸­é–“è¼¸å‡ºçš„å¤§å°åŠå…¶æ¢¯åº¦ã€‚ å¦‚æœå•é¡Œä»ç„¶å­˜åœ¨ï¼Œè«‹è€ƒæ…®ä½¿ç”¨è¼ƒå°ç‰ˆæœ¬çš„æ¨¡å‹ã€‚

<Tip>

åœ¨èª²ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘å€‘å°‡ä»‹ç´¹æ›´å…ˆé€²çš„æŠ€è¡“ï¼Œé€™äº›æŠ€è¡“å¯ä»¥å¹«åŠ©æ‚¨æ¸›å°‘å…§å­˜ä½”ç”¨ä¸¦è®“æ‚¨å¾®èª¿æœ€å¤§çš„æ¨¡å‹ã€‚

</Tip>

### è©•ä¼°æ¨¡å‹

ç¾åœ¨æˆ‘å€‘å·²ç¶“è§£æ±ºäº†ä»£ç¢¼çš„æ‰€æœ‰å•é¡Œï¼Œä¸€åˆ‡éƒ½å¾ˆå®Œç¾ï¼Œè¨“ç·´æ‡‰è©²å¯ä»¥é †åˆ©é€²è¡Œï¼Œå°å§ï¼Ÿ æ²’é‚£éº¼å¿«ï¼ å¦‚æœä½ é‹è¡Œ `trainer.train()` å‘½ä»¤ï¼Œä¸€é–‹å§‹ä¸€åˆ‡çœ‹èµ·ä¾†éƒ½ä¸éŒ¯ï¼Œä½†éä¸€æœƒå…’ä½ æœƒå¾—åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

```py
# This will take a long time and error out, so you shouldn't run this cell
trainer.train()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

æ‚¨å°‡æ„è­˜åˆ°æ­¤éŒ¯èª¤å‡ºç¾åœ¨è©•ä¼°éšæ®µï¼Œå› æ­¤é€™æ˜¯æˆ‘å€‘éœ€è¦èª¿è©¦çš„æœ€å¾Œä¸€ä»¶äº‹ã€‚

æ‚¨å¯ä»¥åƒé€™æ¨£åœ¨è¨“ç·´ä¸­ç¨ç«‹é‹è¡Œ`Trainer`çš„è©•ä¼°å¾ªç’°ï¼š

```py
trainer.evaluate()
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

<Tip>

ğŸ’¡ æ‚¨æ‡‰è©²å§‹çµ‚ç¢ºä¿åœ¨å•Ÿå‹• `trainer.train()` ä¹‹å‰ `trainer.evaluate()`æ˜¯å¯ä»¥é‹è¡Œçš„ï¼Œä»¥é¿å…åœ¨é‡åˆ°éŒ¯èª¤ä¹‹å‰æµªè²»å¤§é‡è¨ˆç®—è³‡æºã€‚

</Tip>

åœ¨å˜—è©¦èª¿è©¦è©•ä¼°å¾ªç’°ä¸­çš„å•é¡Œä¹‹å‰ï¼Œæ‚¨æ‡‰è©²é¦–å…ˆç¢ºä¿æ‚¨å·²ç¶“æŸ¥çœ‹äº†æ•¸æ“šï¼Œèƒ½å¤ æ­£ç¢ºåœ°å½¢æˆæ‰¹è™•ç†ï¼Œä¸¦ä¸”å¯ä»¥åœ¨å…¶ä¸Šé‹è¡Œæ‚¨çš„æ¨¡å‹ã€‚ æˆ‘å€‘å·²ç¶“å®Œæˆäº†æ‰€æœ‰é€™äº›æ­¥é©Ÿï¼Œå› æ­¤å¯ä»¥åŸ·è¡Œä»¥ä¸‹ä»£ç¢¼è€Œä¸æœƒå‡ºéŒ¯ï¼š

```py
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

ç¨ç­‰ä¸€æœƒå…’ï¼ŒéŒ¯èª¤å‡ºç¾ï¼Œåœ¨è©•ä¼°éšæ®µçµæŸæ™‚ï¼Œå¦‚æœæˆ‘å€‘æŸ¥çœ‹trackbackï¼Œæˆ‘å€‘æœƒçœ‹åˆ°ï¼š

```python trace
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

é€™å‘Šè¨´æˆ‘å€‘éŒ¯èª¤æºè‡ª `datasets/metric.py` æ¨¡å¡Šâ€”â€”æ‰€ä»¥é€™æ˜¯æˆ‘å€‘çš„ `compute_metrics()` å‡½æ•¸çš„å•é¡Œã€‚ å®ƒéœ€è¦ä¸€å€‹å¸¶æœ‰ logits å’Œæ¨™ç±¤çš„å…ƒçµ„ä½œç‚º NumPy æ•¸çµ„ï¼Œæ‰€ä»¥è®“æˆ‘å€‘å˜—è©¦è¼¸å…¥å®ƒï¼š

```py
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python out
TypeError: only size-1 arrays can be converted to Python scalars
```

æˆ‘å€‘å¾—åˆ°åŒæ¨£çš„éŒ¯èª¤ï¼Œæ‰€ä»¥å•é¡Œè‚¯å®šå‡ºåœ¨é‚£å€‹å‡½æ•¸ä¸Šã€‚ å¦‚æœæˆ‘å€‘å›é¡§å®ƒçš„ä»£ç¢¼ï¼Œæˆ‘å€‘æœƒç™¼ç¾å®ƒåªæ˜¯å°‡â€œé æ¸¬â€å’Œâ€œçœŸå¯¦çš„æ¨™ç±¤â€è½‰ç™¼åˆ°â€œmetric.compute()â€ã€‚ é‚£éº¼é€™ç¨®æ–¹æ³•æœ‰å•é¡Œå—ï¼Ÿ ä¸¦ä¸çœŸåœ°ã€‚ è®“æˆ‘å€‘å¿«é€Ÿç€è¦½ä¸€ä¸‹å½¢ç‹€ï¼š

```py
predictions.shape, labels.shape
```

```python out
((8, 3), (8,))
```

æˆ‘å€‘çš„é æ¸¬ä»ç„¶æ˜¯ logitsï¼Œè€Œä¸æ˜¯å¯¦éš›çš„é æ¸¬ï¼Œé€™å°±æ˜¯metricsè¿”å›é€™å€‹ï¼ˆæœ‰é»æ¨¡ç³Šï¼‰éŒ¯èª¤çš„åŸå› ã€‚ ä¿®å¾©å¾ˆç°¡å–®ï¼› æˆ‘å€‘åªéœ€è¦åœ¨ `compute_metrics()` å‡½æ•¸ä¸­æ·»åŠ ä¸€å€‹ argmaxï¼š

```py
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))
```

```python out
{'accuracy': 0.625}
```

ç¾åœ¨æˆ‘å€‘çš„éŒ¯èª¤å·²ä¿®å¾©ï¼ é€™æ˜¯æœ€å¾Œä¸€å€‹ï¼Œæ‰€ä»¥æˆ‘å€‘çš„è…³æœ¬ç¾åœ¨å°‡æ­£ç¢ºè¨“ç·´æ¨¡å‹ã€‚

ä½œç‚ºåƒè€ƒï¼Œé€™è£¡æ˜¯å®Œå…¨ä¿®æ­£å¥½çš„è…³æœ¬ï¼š

```py
import numpy as np
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œå¦‚æœæ²’æœ‰æ›´å¤šéŒ¯èª¤ï¼Œæˆ‘å€‘çš„è…³æœ¬å°‡å¾®èª¿ä¸€å€‹æ‡‰è©²çµ¦å‡ºåˆç†çµæœçš„æ¨¡å‹ã€‚ ä½†æ˜¯ï¼Œå¦‚æœè¨“ç·´æ²’æœ‰ä»»ä½•éŒ¯èª¤ï¼Œè€Œè¨“ç·´å‡ºä¾†çš„æ¨¡å‹æ ¹æœ¬è¡¨ç¾ä¸ä½³ï¼Œæˆ‘å€‘è©²æ€éº¼è¾¦ï¼Ÿ é€™æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­æœ€é›£çš„éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡å‘æ‚¨å±•ç¤ºä¸€äº›å¯ä»¥æä¾›å¹«åŠ©çš„æŠ€è¡“ã€‚

<Tip>

ğŸ’¡ å¦‚æœæ‚¨ä½¿ç”¨æ‰‹å‹•è¨“ç·´å¾ªç’°ï¼Œå‰‡ç›¸åŒçš„æ­¥é©Ÿä¹Ÿé©ç”¨æ–¼èª¿è©¦è¨“ç·´ç®¡é“ï¼Œè€Œä¸”æ›´å®¹æ˜“å°‡å®ƒå€‘åˆ†é–‹ã€‚ ä½†æ˜¯ï¼Œè«‹ç¢ºä¿æ‚¨æ²’æœ‰å¿˜è¨˜æ­£ç¢ºä½ç½®çš„ `model.eval()` æˆ– `model.train()`ï¼Œæˆ–è€…æ¯å€‹æ­¥é©Ÿä¸­çš„ `zero_grad()`ï¼

</Tip>

## åœ¨è¨“ç·´æœŸé–“èª¿è©¦éœé»˜ï¼ˆæ²’æœ‰ä»»ä½•éŒ¯èª¤æç¤ºï¼‰éŒ¯èª¤

æˆ‘å€‘å¯ä»¥åšäº›ä»€éº¼ä¾†èª¿è©¦ä¸€å€‹æ²’æœ‰éŒ¯èª¤åœ°å®Œæˆä½†æ²’æœ‰å¾—åˆ°å¥½çš„çµæœçš„è¨“ç·´ï¼Ÿ æˆ‘å€‘æœƒåœ¨é€™è£¡çµ¦ä½ ä¸€äº›æç¤ºï¼Œä½†è«‹æ³¨æ„ï¼Œé€™ç¨®èª¿è©¦æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­æœ€é›£çš„éƒ¨åˆ†ï¼Œä¸¦ä¸”æ²’æœ‰ç¥å¥‡çš„ç­”æ¡ˆã€‚

### æª¢æŸ¥æ‚¨çš„æ•¸æ“šï¼ˆå†æ¬¡ï¼ï¼‰

åªæœ‰åœ¨ç†è«–ä¸Šå¯ä»¥å¾æ‚¨çš„æ•¸æ“šä¸­å­¸åˆ°ä»»ä½•æ±è¥¿æ™‚ï¼Œæ‚¨çš„æ¨¡å‹æ‰æœƒå­¸åˆ°ä¸€äº›æ±è¥¿ã€‚ å¦‚æœå­˜åœ¨æå£æ•¸æ“šçš„éŒ¯èª¤æˆ–æ¨™ç±¤æ˜¯éš¨æ©Ÿå±¬æ€§çš„ï¼Œé‚£éº¼æ‚¨å¾ˆå¯èƒ½ä¸æœƒåœ¨æ•¸æ“šé›†ä¸Šç²å¾—ä»»ä½•çŸ¥è­˜ã€‚ å› æ­¤ï¼Œå§‹çµ‚é¦–å…ˆä»”ç´°æª¢æŸ¥æ‚¨çš„è§£ç¢¼è¼¸å…¥å’Œæ¨™ç±¤ï¼Œç„¶å¾Œå•è‡ªå·±ä»¥ä¸‹å•é¡Œï¼š

- è§£ç¢¼å¾Œçš„æ•¸æ“šæ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿ
- ä½ èªåŒé€™äº›æ¨™ç±¤å—ï¼Ÿ
- æœ‰æ²’æœ‰ä¸€å€‹æ¨™ç±¤æ¯”å…¶ä»–æ¨™ç±¤æ›´å¸¸è¦‹ï¼Ÿ
- å¦‚æœæ¨¡å‹é æ¸¬éš¨æ©Ÿçš„ç­”æ¡ˆ/ç¸½æ˜¯ç›¸åŒçš„ç­”æ¡ˆï¼Œé‚£éº¼loss/è©•ä¼°æŒ‡æ¨™æ‡‰è©²æ˜¯å¤šå°‘ï¼Ÿ

<Tip warning={true}>

âš ï¸ å¦‚æœæ‚¨æ­£åœ¨é€²è¡Œåˆ†ä½ˆå¼è¨“ç·´ï¼Œè«‹åœ¨æ¯å€‹éç¨‹ä¸­æ‰“å°æ•¸æ“šé›†çš„æ¨£æœ¬ï¼Œä¸¦ä¸‰æ¬¡æª¢æŸ¥æ‚¨æ˜¯å¦å¾—åˆ°ç›¸åŒçš„çµæœã€‚ ä¸€å€‹å¸¸è¦‹çš„éŒ¯èª¤æ˜¯åœ¨æ•¸æ“šå‰µå»ºä¸­æœ‰ä¸€äº›éš¨æ©Ÿæ€§ä¾†æºï¼Œé€™ä½¿å¾—æ¯å€‹é€²ç¨‹éƒ½æœ‰ä¸åŒç‰ˆæœ¬çš„æ•¸æ“šé›†ã€‚

</Tip>

æŸ¥çœ‹æ‚¨çš„æ•¸æ“šå¾Œï¼ŒæŸ¥çœ‹æ¨¡å‹çš„ä¸€äº›é æ¸¬ä¸¦å°å…¶é€²è¡Œè§£ç¢¼ã€‚ å¦‚æœæ¨¡å‹ç¸½æ˜¯é æ¸¬åŒæ¨£çš„äº‹æƒ…ï¼Œé‚£å¯èƒ½æ˜¯å› ç‚ºä½ çš„æ•¸æ“šé›†åå‘ä¸€å€‹é¡åˆ¥ï¼ˆé‡å°åˆ†é¡å•é¡Œï¼‰ï¼› éæ¡æ¨£ç¨€æœ‰é¡ç­‰æŠ€è¡“å¯èƒ½æœƒæœ‰æ‰€å¹«åŠ©ã€‚

å¦‚æœæ‚¨åœ¨åˆå§‹æ¨¡å‹ä¸Šç²å¾—çš„loss/è©•ä¼°æŒ‡æ¨™èˆ‡æ‚¨æœŸæœ›çš„éš¨æ©Ÿé æ¸¬çš„loss/è©•ä¼°æŒ‡æ¨™éå¸¸ä¸åŒï¼Œè«‹ä»”ç´°æª¢æŸ¥æ‚¨çš„lossæˆ–è©•ä¼°æŒ‡æ¨™çš„è¨ˆç®—æ–¹å¼ï¼Œå› ç‚ºé‚£è£¡å¯èƒ½å­˜åœ¨éŒ¯èª¤ã€‚ å¦‚æœæ‚¨ä½¿ç”¨æœ€å¾Œæ·»åŠ çš„å¤šå€‹lossï¼Œè«‹ç¢ºä¿å®ƒå€‘å…·æœ‰ç›¸åŒçš„è¦æ¨¡ã€‚

ç•¶æ‚¨ç¢ºå®šæ‚¨çš„æ•¸æ“šæ˜¯å®Œç¾çš„æ™‚ï¼Œæ‚¨å¯ä»¥é€šéä¸€å€‹ç°¡å–®çš„æ¸¬è©¦ä¾†æŸ¥çœ‹æ¨¡å‹æ˜¯å¦èƒ½å¤ å°å…¶é€²è¡Œè¨“ç·´ã€‚

### åœ¨ä¸€æ‰¹ä¸Šéåº¦æ“¬åˆä½ çš„æ¨¡å‹

éåº¦æ“¬åˆé€šå¸¸æ˜¯æˆ‘å€‘åœ¨è¨“ç·´æ™‚å„˜é‡é¿å…çš„äº‹æƒ…ï¼Œå› ç‚ºé€™æ„å‘³è‘—æ¨¡å‹æ²’æœ‰å­¸ç¿’è­˜åˆ¥æˆ‘å€‘æƒ³è¦çš„ä¸€èˆ¬ç‰¹å¾µï¼Œè€Œåªæ˜¯è¨˜ä½äº†è¨“ç·´æ¨£æœ¬ã€‚ åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œä¸€éåˆä¸€éåœ°å˜—è©¦åœ¨ä¸€å€‹æ‰¹æ¬¡ä¸Šè¨“ç·´æ‚¨çš„æ¨¡å‹æ˜¯ä¸€å€‹å¾ˆå¥½çš„æ¸¬è©¦ï¼Œå¯ä»¥æª¢æŸ¥æ‚¨çš„å•é¡Œæ˜¯å¦å¯ä»¥é€šéæ‚¨å˜—è©¦è¨“ç·´çš„æ¨¡å‹ä¾†è§£æ±ºã€‚ å®ƒé‚„å°‡å¹«åŠ©æ‚¨æŸ¥çœ‹æ‚¨çš„åˆå§‹å­¸ç¿’ç‡æ˜¯å¦å¤ªé«˜ã€‚

ä¸€æ—¦ä½ å®šç¾©äº†ä½ çš„ `Trainer` ä¹‹å¾Œï¼Œé€™æ¨£åšçœŸçš„å¾ˆå®¹æ˜“ï¼› åªéœ€ç²å–ä¸€æ‰¹è¨“ç·´æ•¸æ“šï¼Œç„¶å¾Œåƒ…ä½¿ç”¨è©²æ‰¹æ¬¡é‹è¡Œä¸€å€‹å°å‹æ‰‹å‹•è¨“ç·´å¾ªç’°ï¼Œå¤§ç´„ 20 æ­¥ï¼š

```py
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

<Tip>

ğŸ’¡ å¦‚æœæ‚¨çš„è¨“ç·´æ•¸æ“šä¸å¹³è¡¡ï¼Œè«‹ç¢ºä¿æ§‹å»ºä¸€æ‰¹åŒ…å«æ‰€æœ‰æ¨™ç±¤çš„è¨“ç·´æ•¸æ“šã€‚

</Tip>

ç”Ÿæˆçš„æ¨¡å‹åœ¨ä¸€å€‹â€œæ‰¹æ¬¡â€ä¸Šæ‡‰è©²æœ‰æ¥è¿‘å®Œç¾çš„çµæœã€‚ è®“æˆ‘å€‘è¨ˆç®—çµæœé æ¸¬çš„æŒ‡æ¨™ï¼š

```py
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python out
{'accuracy': 1.0}
```

100% æº–ç¢ºç‡ï¼Œç¾åœ¨é€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„éæ“¬åˆç¤ºä¾‹ï¼ˆé€™æ„å‘³è‘—å¦‚æœä½ åœ¨ä»»ä½•å…¶ä»–å¥å­ä¸Šå˜—è©¦ä½ çš„æ¨¡å‹ï¼Œå®ƒå¾ˆå¯èƒ½æœƒçµ¦ä½ ä¸€å€‹éŒ¯èª¤çš„ç­”æ¡ˆï¼‰ï¼

å¦‚æœä½ æ²’æœ‰è¨­æ³•è®“ä½ çš„æ¨¡å‹ç²å¾—é€™æ¨£çš„å®Œç¾çµæœï¼Œé€™æ„å‘³è‘—ä½ æ§‹å»ºå•é¡Œæˆ–æ•¸æ“šçš„æ–¹å¼æœ‰å•é¡Œï¼Œæ‰€ä»¥ä½ æ‡‰è©²ä¿®å¾©å®ƒã€‚ åªæœ‰ç•¶ä½ å¯ä»¥é€šééæ“¬åˆæ¸¬è©¦æ™‚ï¼Œä½ æ‰èƒ½ç¢ºå®šä½ çš„æ¨¡å‹å¯¦éš›ä¸Šå¯ä»¥å­¸åˆ°ä¸€äº›æ±è¥¿ã€‚

<Tip warning={true}>

âš ï¸ åœ¨æ­¤æ¸¬è©¦ä¹‹å¾Œï¼Œæ‚¨å°‡ä¸å¾—ä¸é‡æ–°å‰µå»ºæ‚¨çš„æ¨¡å‹å’Œâ€œTrainerâ€ï¼Œå› ç‚ºç²å¾—çš„æ¨¡å‹å¯èƒ½ç„¡æ³•åœ¨æ‚¨çš„å®Œæ•´æ•¸æ“šé›†ä¸Šæ¢å¾©å’Œå­¸ç¿’æœ‰ç”¨çš„æ±è¥¿ã€‚

</Tip>

### åœ¨ä½ æœ‰ç¬¬ä¸€å€‹åŸºç·šä¹‹å‰ä¸è¦èª¿æ•´ä»»ä½•æ±è¥¿

è¶…åƒæ•¸èª¿å„ªç¸½æ˜¯è¢«å¼·èª¿ç‚ºæ©Ÿå™¨å­¸ç¿’ä¸­æœ€é›£çš„éƒ¨åˆ†ï¼Œä½†é€™åªæ˜¯å¹«åŠ©æ‚¨åœ¨æŒ‡æ¨™ä¸Šæœ‰æ‰€æ”¶ç©«çš„æœ€å¾Œä¸€æ­¥ã€‚ å¤§å¤šæ•¸æƒ…æ³ä¸‹ï¼Œ`Trainer` çš„é»˜èªè¶…åƒæ•¸å¯ä»¥å¾ˆå¥½åœ°ç‚ºæ‚¨æä¾›è‰¯å¥½çš„çµæœï¼Œå› æ­¤åœ¨æ‚¨ç²å¾—è¶…å‡ºæ•¸æ“šé›†åŸºç·šçš„æ±è¥¿ä¹‹å‰ï¼Œä¸è¦é–‹å§‹é€²è¡Œè€—æ™‚ä¸”æ˜‚è²´çš„è¶…åƒæ•¸æœç´¢ .

ä¸€æ—¦ä½ æœ‰ä¸€å€‹è¶³å¤ å¥½çš„æ¨¡å‹ï¼Œä½ å°±å¯ä»¥é–‹å§‹ç¨å¾®èª¿æ•´ä¸€ä¸‹ã€‚ ä¸è¦å˜—è©¦ä½¿ç”¨ä¸åŒçš„è¶…åƒæ•¸å•Ÿå‹•ä¸€åƒæ¬¡é‹è¡Œï¼Œè€Œæ˜¯æ¯”è¼ƒä¸€å€‹è¶…åƒæ•¸çš„ä¸åŒå€¼çš„å¹¾æ¬¡é‹è¡Œï¼Œä»¥ç­è§£å“ªå€‹å½±éŸ¿æœ€å¤§ã€‚

å¦‚æœæ‚¨æ­£åœ¨èª¿æ•´æ¨¡å‹æœ¬èº«ï¼Œä¸è¦å˜—è©¦ä»»ä½•æ‚¨ç„¡æ³•åˆç†è­‰æ˜çš„äº‹æƒ…ã€‚ å§‹çµ‚ç¢ºä¿æ‚¨è¿”å›éæ“¬åˆæ¸¬è©¦ä»¥é©—è­‰æ‚¨çš„æ›´æ”¹æ²’æœ‰ç”¢ç”Ÿä»»ä½•æ„å¤–å¾Œæœã€‚

### è«‹æ±‚å¹«å¿™

å¸Œæœ›æ‚¨æœƒåœ¨æœ¬ç¯€ä¸­æ‰¾åˆ°ä¸€äº›å¯ä»¥å¹«åŠ©æ‚¨è§£æ±ºå•é¡Œçš„å»ºè­°ï¼Œä½†å¦‚æœä¸æ˜¯é€™æ¨£ï¼Œè«‹è¨˜ä½æ‚¨å¯ä»¥éš¨æ™‚åœ¨ [è«–å£‡](https://discuss.huggingface.co/) ä¸Šå‘ç¤¾å€æå•ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¡å¤–è³‡æºï¼š

- [â€œä½œç‚ºå·¥ç¨‹æœ€ä½³å¯¦è¸å·¥å…·çš„å†ç¾æ€§â€](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p)ï¼Œä½œè€…ï¼šJoel Grus
- [â€œç¥ç¶“ç¶²çµ¡èª¿è©¦æ¸…å–®â€](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) ä½œè€…ï¼šCecelia Shao
- [â€œå¦‚ä½•å°æ©Ÿå™¨å­¸ç¿’ä»£ç¢¼é€²è¡Œå–®å…ƒæ¸¬è©¦â€](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) by Chase Roberts
- [â€œè¨“ç·´ç¥ç¶“ç¶²çµ¡çš„ç§˜è¨£â€](http://karpathy.github.io/2019/04/25/recipe/)ä½œè€…ï¼šAndrej Karpathy

ç•¶ç„¶ï¼Œä¸¦ä¸æ˜¯ä½ åœ¨è¨“ç·´ç¥ç¶“ç¶²çµ¡æ™‚é‡åˆ°çš„æ¯ä¸€å€‹å•é¡Œéƒ½æ˜¯ä½ è‡ªå·±çš„éŒ¯ï¼ å¦‚æœæ‚¨åœ¨ ğŸ¤— Transformers æˆ– ğŸ¤— Datasets åº«ä¸­é‡åˆ°çœ‹èµ·ä¾†ä¸æ­£ç¢ºçš„å…§å®¹ï¼Œæ‚¨å¯èƒ½é‡åˆ°äº†éŒ¯èª¤ã€‚ ä½ æ‡‰è©²å‘Šè¨´æˆ‘å€‘é€™ä¸€åˆ‡ï¼Œåœ¨ä¸‹ä¸€ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡æº–ç¢ºè§£é‡‹å¦‚ä½•åšåˆ°é€™ä¸€é»ã€‚
