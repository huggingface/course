1
00:00:05,120 --> 00:00:10,880
Как объединить входы в пакет? В этом видео мы рассмотрим, как объединять входные последовательности в батч.

2
00:00:12,480 --> 00:00:16,560
В общем, предложения, которые мы хотим пропустить через нашу модель, не все будут иметь одинаковую длину.

3
00:00:16,560 --> 00:00:23,520
Здесь мы используем модель, которую мы видели в конвейере анализа настроений, и хотим классифицировать два предложения.

4
00:00:23,520 --> 00:00:31,680
При их токенизации и сопоставлении каждого токена с соответствующими входными идентификаторами мы получаем два списка разной длины.

5
00:00:33,120 --> 00:00:38,240
Попытка создать тензор или массив NumPy из этих двух списков приведет к ошибке, поскольку все массивы и тензоры должны быть прямоугольными.

6
00:00:38,240 --> 00:00:50,080
Один из способов преодолеть это ограничение - сделать второе предложение такой же длины, как и первое, добавив специальный токен столько раз, сколько необходимо.

7
00:00:51,040 --> 00:01:00,080
Другим способом было бы усечение первой последовательности до длины второй, но в этом случае мы потеряем много информации, которая может быть необходима для правильной классификации предложения.

8
00:01:01,040 --> 00:01:05,760
В общем, мы усекаем предложения только тогда, когда они длиннее максимальной длины, которую может обработать модель.

9
00:01:05,760 --> 00:01:18,000
Значение, используемое для вставки второго предложения, не должно быть выбрано случайно: модель была предварительно обучена с определенным идентификатором вставки, который можно найти в tokenizer.pad_token_id.

10
00:01:19,760 --> 00:01:22,640
Теперь, когда мы дополнили наши предложения, мы можем сделать из них батч.

11
00:01:23,920 --> 00:01:33,600
Однако если мы передадим эти два предложения в модель по отдельности и вместе, мы заметим, что мы не получим тех же результатов для предложения, которое было дополнено (здесь второе).

12
00:01:37,360 --> 00:01:46,800
Если вы помните, что в моделях Transformer активно используются слои внимания, это не должно быть полной неожиданностью: при вычислении контекстуального представления каждого токена слои внимания рассматривают все остальные слова в предложении.

13
00:01:46,800 --> 00:01:57,200
Если у нас есть только предложение или предложение с несколькими токенами вставки, логично, что мы не получим одинаковых значений.

14
00:01:58,560 --> 00:02:03,520
Чтобы получить одинаковые результаты с вставками и без них, нам нужно указать слоям внимания, что они должны игнорировать эти вставляемые токены.

15
00:02:03,520 --> 00:02:15,920
Для этого создается маска внимания - тензор, имеющий ту же форму, что и входные идентификаторы, с нулями и единицами.

16
00:02:15,920 --> 00:02:22,160
Единицы обозначают токены, которые слои внимания должны учитывать в контексте, а нули - токены, которые они должны игнорировать.

17
00:02:22,160 --> 00:02:27,040
Теперь, передавая эту маску внимания вместе с входными идентификаторами, мы получим те же результаты, что и при передаче модели двух предложений по отдельности!

18
00:02:27,040 --> 00:02:33,600
Все это делается за кулисами токенизатора, когда вы применяете его к нескольким предложениям с флагом padding=True.

19
00:02:33,600 --> 00:02:49,840
Он применит вставки с нужным значением к меньшим предложениям и создаст соответствующую маску внимания.