1
00:00:03,860 --> 00:00:09,750
В этом видео мы изучим архитектуру декодера. Примером популярной архитектуры только декодера является GPT-2.

2
00:00:09,750 --> 00:00:15,809
Для того чтобы понять, как работают декодеры, рекомендуем посмотреть видео о кодерах: они очень похожи на декодеры.

3
00:00:15,809 --> 00:00:26,429
Декодер можно использовать для решения большинства тех же задач, что и кодер, хотя, как правило, с небольшим снижением производительности.

4
00:00:26,429 --> 00:00:31,769
Давайте применим тот же подход, что и в случае с кодером, чтобы попытаться понять архитектурные различия между кодером и декодером.

5
00:00:31,769 --> 00:00:38,969
Мы воспользуемся небольшим примером, используя три слова.

6
00:00:38,969 --> 00:00:46,550
Мы пропускаем их через декодер. Мы получаем числовое представление каждого слова.

7
00:00:46,550 --> 00:00:51,739
Здесь, например, декодер преобразует три слова "Welcome to NYC" в эти три последовательности цифр.

8
00:00:51,739 --> 00:00:57,750
Декодер выдает ровно одну последовательность чисел на каждое входное слово.

9
00:00:57,750 --> 00:01:03,290
Это числовое представление можно также назвать "вектором признаков" или "тензором признаков".

10
00:01:03,290 --> 00:01:09,590
Давайте погрузимся в это представление. Оно содержит один вектор на каждое слово, прошедшее через декодер.

11
00:01:09,590 --> 00:01:14,830
Каждый из этих векторов является числовым представлением рассматриваемого слова.

12
00:01:14,830 --> 00:01:21,810
Размерность этого вектора определяется архитектурой модели.

13
00:01:21,810 --> 00:01:28,400
Декодер отличается от кодера, главным образом, механизмом самовнимания.

14
00:01:28,400 --> 00:01:34,090
Он использует то, что называется "маскированным самовниманием".

15
00:01:34,090 --> 00:01:40,170
Здесь, например, если мы сосредоточимся на слове "to", то увидим, что его вектор абсолютно не изменяется словом "NYC".

16
00:01:40,170 --> 00:01:45,560
Это происходит потому, что все слова справа (также известные как правильный контекст) от слова маскируются.

17
00:01:45,560 --> 00:01:50,729
Вместо того, чтобы пользоваться всеми словами слева и справа, т.е. двунаправленным контекстом, декодеры имеют доступ только к словам слева.

18
00:01:50,729 --> 00:02:12,110
Механизм маскированного самовнимания отличается от механизма самовнимания использованием дополнительной маски, скрывающей контекст по обе стороны от слова: на числовое представление слова не влияют слова в скрытом контексте.

19
00:02:12,110 --> 00:02:18,730
Когда же следует использовать декодер? Декодеры, как и кодеры, могут использоваться как самостоятельные модели.

20
00:02:18,730 --> 00:02:24,610
Поскольку они генерируют числовое представление, их также можно использовать в самых разных задачах.

21
00:02:24,610 --> 00:02:30,410
Однако сила декодера заключается в том, как слово имеет доступ к контексту слева.

22
00:02:30,410 --> 00:02:40,280
Декодеры, имея доступ только к контексту слева от себя, по своей природе хороши в генерации текста: способность генерировать слово или последовательность слов, учитывая известную последовательность слов.

23
00:02:40,280 --> 00:02:46,120
В NLP это известно как каузальное моделирование языка. Давайте рассмотрим пример.

24
00:02:46,120 --> 00:02:52,150
Вот пример того, как работает каузальное моделирование языка: мы начинаем с начального слова, которым является "My".

25
00:02:52,150 --> 00:02:59,240
Мы используем его в качестве входа для декодера.

26
00:02:59,240 --> 00:03:06,330
Модель выдает вектор размерности 768. Этот вектор содержит информацию о последовательности, которая здесь является одним словом, или слово.

27
00:03:06,330 --> 00:03:17,019
Мы применяем небольшое преобразование к этому вектору так, чтобы он отображался на все слова, известные модели (отображение, которое, как мы увидим позже, называется головой языкового моделирования).

28
00:03:17,019 --> 00:03:22,650
Мы определили, что модель считает наиболее вероятным следующим словом "name".

29
00:03:22,650 --> 00:03:29,720
Затем мы берем это новое слово и добавляем его к исходной последовательности.

30
00:03:29,720 --> 00:03:35,560
От "My" мы перешли к "My name". Здесь вступает в действие "авторегрессионный" аспект.

31
00:03:35,560 --> 00:03:42,689
Авторегрессионные модели повторно используют свои прошлые выходы в качестве входов на следующих этапах.

32
00:03:42,689 --> 00:03:49,280
И снова мы выполняем точно такую же операцию: пропускаем эту последовательность через декодер и извлекаем наиболее вероятное следующее слово.

33
00:03:49,280 --> 00:03:57,459
В данном случае это слово "is". Мы повторяем операцию до тех пор, пока не будем удовлетворены.

34
00:03:57,459 --> 00:04:03,049
Начав с одного слова, мы теперь сгенерировали целое предложение.

35
00:04:03,049 --> 00:04:08,870
Мы решили остановиться на этом, но мы могли бы продолжить еще некоторое время GPT-2, например, имеет максимальный размер контекста 1024.

36
00:04:08,870 --> 00:04:16,918
В конечном итоге мы могли бы генерировать до 1024 слов, а декодер все еще будет помнить о первых словах последовательности!

37
00:04:16,918 --> 00:04:21,125
Если мы вернемся на несколько уровней выше, к полной модели трансформера, мы сможем увидеть, что мы узнали о части декодера полной модели трансформера.

38
00:04:21,125 --> 00:04:22,125
Это так называемая авторегрессия: она выводит значения, которые затем используются в качестве входных значений.

39
00:04:22,125 --> 00:04:23,125
Мы повторяем эти операции по своему усмотрению.

40
00:04:23,125 --> 00:04:24,125
Он основан на слое маскированного самовнимания, что позволяет иметь эмбеддинги слов, которые имеют доступ к контексту слева от слова.

41
00:04:24,125 --> 00:04:25,125
Однако если вы посмотрите на схему, то увидите, что мы не рассмотрели один из аспектов декодера.

42
00:04:25,125 --> 00:04:26,125
Это: перекрестное внимание.

43
00:04:26,125 --> 00:04:27,125
Есть и второй аспект, который мы не видели, - эта способность преобразовывать признаки в слова, сильно связанная с механизмом перекрестного внимания.

44
00:04:27,125 --> 00:04:29,125
Однако они применяются только в кодере-декодере трансформера или трансформере "sequence-to-sequence" (в общем случае они могут использоваться как взаимозаменяемые).

45
00:04:29,125 --> 00:04:30,132
Мы рекомендуем вам посмотреть видео о кодерах-декодерах, чтобы получить представление о том, как декодер может быть использован в качестве компонента более крупной архитектуры!