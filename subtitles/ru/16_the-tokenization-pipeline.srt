1
00:00:05,440 --> 00:00:12,320
Конвейер токенизатора. В этом видео мы рассмотрим, как токенизатор преобразует необработанный текст в числа, которые могут быть использованы в модели Transformer, например, при выполнении этого кода.

2
00:00:12,320 --> 00:00:24,400
Вот краткий обзор того, что происходит внутри объекта tokenizer: сначала текст разбивается на токены, которые представляют собой слова, части слов или знаки препинания.

3
00:00:24,400 --> 00:00:36,560
Затем токенизатор добавляет потенциальные специальные токены и преобразует каждый токен в его уникальный идентификатор, определенный словарем токенизатора.

4
00:00:37,520 --> 00:00:41,440
Как мы увидим, на самом деле все происходит не в таком порядке, но такой подход лучше помогает понять, что происходит.

5
00:00:41,440 --> 00:00:46,320
Первым шагом является разбиение нашего входного текста на токены с помощью метода tokenize.

6
00:00:46,320 --> 00:00:58,000
Для этого токенизатор может сначала выполнить некоторые операции, например, перевести в нижний регистр все слова, а затем, следуя набору правил, разделить результат на небольшие фрагменты текста.

7
00:00:58,000 --> 00:01:08,720
Большинство моделей Transformers используют алгоритм токенизации подслов, что означает, что одно данное слово может быть разбито на несколько токенов, как здесь "tokenize".

8
00:01:08,720 --> 00:01:13,360
Посмотрите видео "Алгоритмы токенизации", ссылки на которые приведены ниже, для получения дополнительной информации!

9
00:01:14,480 --> 00:01:19,600
Префикс "##", который мы видим перед "ize", используется BERT для обозначения того, что эта лексема не является началом слова.

10
00:01:19,600 --> 00:01:36,640
Другие токенизаторы могут использовать другие соглашения: например, токенизатор ALBERT добавляет длинное подчеркивание перед всеми лексемами, перед которыми есть пробел, что является соглашением, используемым токенизаторами sentencepiece.

11
00:01:38,320 --> 00:01:43,280
Второй этап конвейера токенизации заключается в сопоставлении этих токенов с соответствующими им идентификаторами, определенными в словаре токенизатора.

12
00:01:43,280 --> 00:01:53,600
Вот почему нам нужно загрузить файл, когда мы инстанцируем токенизатор с помощью метода from_pretrained: нам нужно убедиться, что мы используем то же отображение, что и при предварительном обучении модели.

13
00:01:53,600 --> 00:01:59,520
Для этого мы используем метод convert_tokens_to_ids.

14
00:02:00,720 --> 00:02:09,840
Возможно, вы заметили, что у нас нет точно такого же результата, как на нашем первом слайде - или нет, это выглядит как список случайных чисел, в таком случае позвольте мне освежить вашу память.

15
00:02:10,479 --> 00:02:13,680
У нас есть число в начале и в конце, которые отсутствуют, это специальные токены.

16
00:02:14,400 --> 00:02:25,280
Специальные токены добавляются методом prepare_for_model, который знает индексы этих токенов в словаре и просто добавляет соответствующие номера.

17
00:02:28,320 --> 00:02:37,120
Вы можете посмотреть на специальные токены (и в целом на то, как токенизатор изменил ваш текст), используя метод decode на выводах объекта tokenizer.

18
00:02:38,240 --> 00:02:44,080
Что касается префикса для начала слов/частей слов, то эти специальные токены зависят от того, какой токенизатор вы используете. 

19
00:02:44,080 --> 00:02:50,080
Токенизатор BERT использует [CLS] и [SEP], но токенизатор Roberta использует html-подобные якоря и.

20
00:02:50,080 --> 00:03:02,560
Теперь, когда вы знаете, как работает токенизатор, вы можете забыть обо всех этих промежуточных методах и помнить, что вам нужно просто вызывать его на ваших входных текстах.

21
00:03:03,600 --> 00:03:11,600
Входы не содержат идентификаторов входов, однако, чтобы узнать, что такое маска внимания, посмотрите видео "Batching inputs together".

22
00:03:12,160 --> 00:03:17,840
Чтобы узнать об идентификаторах типов токенов, посмотрите видео "Preprocessing sentence pairs".