1
00:00:05,200 --> 00:00:10,880
Как объединить входы в батч? В этом видео мы рассмотрим, как объединять входные последовательности в батч.

2
00:00:12,320 --> 00:00:16,560
В общем, предложения, которые мы хотим пропустить через нашу модель, не все будут иметь одинаковую длину.

3
00:00:17,520 --> 00:00:21,280
Здесь мы используем модель, которую мы видели в конвейере анализа настроений, и хотим классифицировать два предложения.

4
00:00:21,840 --> 00:00:31,280
При их токенизации и сопоставлении каждого токена с соответствующими входными идентификаторами мы получаем два списка разной длины.

5
00:00:33,040 --> 00:00:38,400
Попытка создать тензор или массив NumPy из этих двух списков приведет к ошибке, поскольку все массивы и тензоры должны быть прямоугольными.

6
00:00:38,400 --> 00:00:44,560
Один из способов преодолеть это ограничение - сделать второе предложение такой же длины, как и первое, добавив специальный токен столько раз, сколько необходимо.

7
00:00:51,360 --> 00:01:00,720
Другим способом было бы усечение первой последовательности до длины второй, но в этом случае мы потеряем много информации, которая может быть необходима для правильной классификации предложения.

8
00:01:02,000 --> 00:01:06,720
В общем, мы усекаем предложения только тогда, когда они длиннее максимальной длины, которую может обработать модель.

9
00:01:06,720 --> 00:01:19,200
Значение, используемое для вставки второго предложения, не должно быть выбрано случайно: модель была предварительно обучена с определенным идентификатором вставки, который можно найти в tokenizer.pad_token_id.

10
00:01:20,800 --> 00:01:25,200
Теперь, когда мы дополнили наши предложения, мы можем сделать из них батч.

11
00:01:25,200 --> 00:01:35,120
Однако если мы передадим эти два предложения в модель по отдельности и вместе, мы заметим, что мы не получим тех же результатов для предложения, которое было дополнено (здесь второе).

12
00:01:39,120 --> 00:01:47,760
Если вы помните, что в моделях Transformer активно используются слои внимания, это не должно быть полной неожиданностью: при вычислении контекстуального представления каждого токена слои внимания рассматривают все остальные слова в предложении.

13
00:01:48,560 --> 00:01:58,720
Если у нас есть только предложение или предложение с несколькими токенами вставки, логично, что мы не получим одинаковых значений.

14
00:02:00,000 --> 00:02:05,120
Чтобы получить одинаковые результаты с вставками и без них, нам нужно указать слоям внимания, что они должны игнорировать эти вставляемые токены.

15
00:02:05,120 --> 00:02:10,320
Для этого создается маска внимания - тензор, имеющий ту же форму, что и входные идентификаторы, с нулями и единицами.

16
00:02:10,320 --> 00:02:21,840
Единицы обозначают токены, которые слои внимания должны учитывать в контексте, а нули - токены, которые они должны игнорировать.

17
00:02:23,360 --> 00:02:30,720
Теперь, передавая эту маску внимания вместе с входными идентификаторами, мы получим те же результаты, что и при передаче модели двух предложений по отдельности!

18
00:02:32,160 --> 00:02:36,640
Все это делается за кулисами токенизатора, когда вы применяете его к нескольким предложениям с флагом padding=True.

19
00:02:36,640 --> 00:02:49,840
Он применит вставки с нужным значением к меньшим предложениям и создаст соответствующую маску внимания.