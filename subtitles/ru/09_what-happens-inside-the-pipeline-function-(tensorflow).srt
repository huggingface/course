1
00:00:05,360 --> 00:00:07,680
Что происходит внутри функции конвейера?

2
00:00:09,840 --> 00:00:14,800
В этом видео мы рассмотрим, что на самом деле происходит, когда мы используем функцию pipeline библиотеки Transformers.

3
00:00:14,800 --> 00:00:26,720
Более конкретно, мы рассмотрим конвейер анализа настроений и то, как он перешел от двух следующих предложений к положительным меткам с соответствующими оценками.

4
00:00:28,560 --> 00:00:34,160
Как мы уже видели в презентации конвейера, в нем есть три этапа.

5
00:00:34,800 --> 00:00:38,880
Сначала мы преобразуем необработанные тексты в числа, которые могут быть понятны модели, с помощью токенизатора.

6
00:00:40,000 --> 00:00:43,520
Затем эти числа проходят через модель, которая выводит логиты.

7
00:00:44,400 --> 00:00:49,120
Наконец, на этапе постобработки эти логиты преобразуются в метки и оценки.

8
00:00:50,720 --> 00:00:54,960
Давайте подробно рассмотрим эти три этапа и то, как воспроизвести их с помощью библиотеки Transformers, начиная с первого этапа - токенизации.

9
00:00:54,960 --> 00:01:03,280
Процесс токенизации состоит из нескольких этапов.

10
00:01:03,280 --> 00:01:09,120
Сначала текст разбивается на небольшие фрагменты, называемые токенами. Это могут быть слова, части слов или знаки препинания.

11
00:01:09,120 --> 00:01:17,440
Затем токенизатор добавит несколько специальных токенов (если модель их ожидает).

12
00:01:17,440 --> 00:01:22,800
Здесь модель ожидает токен CLS в начале и токен SEP в конце предложения для классификации.

13
00:01:23,760 --> 00:01:28,880
Наконец, токенизатор сопоставляет каждый токен с его уникальным идентификатором в словаре предварительно обученной модели.

14
00:01:28,880 --> 00:01:34,640
Для загрузки такого токенизатора библиотека Transformers предоставляет API AutoTokenizer.

15
00:01:35,680 --> 00:01:47,200
Наиболее важным методом этого класса является метод from_pretrained, который загружает и кэширует конфигурацию и словарь, ассоциированный с данной контрольной точкой.

16
00:01:47,200 --> 00:01:53,840
Здесь контрольной точкой, используемой по умолчанию для конвейера анализа настроений, является distilbert base uncased finetuned sst2 english.

17
00:01:56,560 --> 00:02:01,440
Мы инстанцируем токенизатор, связанный с этой контрольной точкой, а затем передаем ему два предложения.

18
00:02:02,640 --> 00:02:07,360
Поскольку эти два предложения не имеют одинакового размера, нам нужно будет дополнить самое короткое из них, чтобы построить массив.

19
00:02:07,360 --> 00:02:11,680
Это делается токенизатором с опцией padding=True.

20
00:02:13,840 --> 00:02:18,960
Если truncation=True, мы гарантируем, что любое предложение длиннее максимума, который может обработать модель, будет усечено.

21
00:02:18,960 --> 00:02:25,600
Наконец, параметр return_tensors указывает токенизатору возвращать тензор TensorFlow.

22
00:02:26,720 --> 00:02:29,680
Посмотрев на результат, мы видим, что у нас есть словарь с двумя ключами.

23
00:02:30,240 --> 00:02:37,280
"input_ids" содержит идентификаторы обоих предложений, с 0, где применяется дополнение.

24
00:02:37,280 --> 00:02:42,080
Второй ключ, "attention_mask", указывает на места, где было применено дополнение, чтобы модель не обращала на него внимания.

25
00:02:42,080 --> 00:02:48,000
Это все то, что находится внутри шага токенизации. Теперь давайте посмотрим на второй шаг - модель.

26
00:02:48,640 --> 00:02:54,960
Что касается токенизатора, существует API TFAutoModel с методом from_pretrained.

27
00:02:55,600 --> 00:02:59,840
Он загружает и кэширует конфигурацию модели, а также предварительно обученные веса.

28
00:02:59,840 --> 00:03:10,640
Однако API TFAutoModel создаст только тело модели, то есть ту часть модели, которая останется после удаления предварительно обученной головы.

29
00:03:12,000 --> 00:03:20,080
Он выдаст высокоразмерный тензор, который является представлением изученных предложений, но который не является непосредственно полезным для нашей задачи классификации.

30
00:03:21,760 --> 00:03:28,080
Здесь тензор состоит из двух предложений, каждое из шестнадцати лексем, а последнее измерение - скрытый размер нашей модели 768.

31
00:03:28,080 --> 00:03:34,320
Чтобы получить результат, связанный с нашей проблемой классификации, нам нужно использовать класс TFAutoModelForSequenceClassification.

32
00:03:34,320 --> 00:03:45,440
Он работает точно так же, как класс AutoModel, за исключением того, что он будет строить модель с головой классификации.

33
00:03:45,440 --> 00:03:52,160
В библиотеке Transformers есть один автокласс для каждой общей задачи NLP.

34
00:03:52,160 --> 00:03:59,120
Здесь, после предоставления нашей модели двух предложений, мы получаем тензор размера два на два: один результат для каждого предложения и для каждой возможной метки.

35
00:03:59,120 --> 00:04:04,800
Эти выходы еще не являются вероятностями (мы видим, что их сумма не равна 1).

36
00:04:04,800 --> 00:04:10,960
Это связано с тем, что каждая модель библиотеки трансформеров возвращает логиты.

37
00:04:10,960 --> 00:04:17,519
Чтобы понять смысл этих логитов, нам нужно углубиться в третий и последний этап конвейера: постобработку.

38
00:04:17,519 --> 00:04:22,800
Чтобы преобразовать логиты в вероятности, нам нужно применить к ним слой SoftMax.

39
00:04:22,800 --> 00:04:28,160
Как мы видим, это превращает их в положительные числа, которые в сумме дают 1. Последний шаг - узнать, какой из них соответствует положительной или отрицательной метке.

40
00:04:28,160 --> 00:04:34,720
Это задается полем id2label в конфигурации модели.

41
00:04:34,720 --> 00:04:40,800
Первая вероятность (индекс 0) соответствует отрицательной метке, а вторая (индекс 1) - положительной.

42
00:04:40,800 --> 00:04:46,640
Вот как наш классификатор, построенный с помощью функции конвейера, выбирает эти метки и вычисляет оценки.

43
00:04:46,640 --> 00:04:55,840
Теперь, когда вы знаете, как работает каждый шаг, вы можете легко настроить их под свои нужды.