1
00:00:04,160 --> 00:00:07,200
В этом видео мы изучим архитектуру кодера-декодера.

2
00:00:08,160 --> 00:00:21,680
Примером популярной модели кодера-декодера является T5. Для того чтобы понять, как работает кодер-декодер, рекомендуем вам ознакомиться с видеороликами о кодерах и декодерах как самостоятельных моделях.

3
00:00:22,400 --> 00:00:30,320
Понимание того, как они работают по отдельности, поможет понять, как работает кодер-декодер.

4
00:00:30,320 --> 00:00:47,360
Начнем с того, что мы уже видели о кодере. Кодер принимает слова в качестве входных данных, пропускает их через кодер и извлекает числовое представление для каждого пропущенного через него слова. Теперь мы знаем, что числовое представление содержит информацию о значении последовательности.

5
00:00:47,360 --> 00:00:54,000
Оставим это в стороне и добавим в схему декодер.

6
00:00:56,480 --> 00:01:00,160
В этом сценарии мы используем декодер таким образом, которого раньше не видели.

7
00:01:00,720 --> 00:01:07,600
Мы передаем ему выходы кодера напрямую! Помимо выходов кодера, мы также передаем декодеру последовательность.

8
00:01:07,600 --> 00:01:17,360
Когда декодер получает запрос на вывод без начальной последовательности, мы можем задать ему значение, указывающее на начало последовательности.

9
00:01:18,000 --> 00:01:23,520
И именно здесь происходит волшебство кодера-декодера. Кодер принимает на вход последовательность.

10
00:01:24,560 --> 00:01:30,480
Он вычисляет прогноз и выдает числовое представление.

11
00:01:30,480 --> 00:01:42,960
Затем он посылает его декодеру. Он, в некотором смысле, закодировал последовательность. А декодер, в свою очередь, используя этот вход вместе с обычной последовательностью, попытается декодировать последовательность.

12
00:01:44,720 --> 00:01:50,400
Декодер декодирует последовательность и выдает слово.

13
00:01:50,400 --> 00:01:55,440
На данный момент нам не нужно понимать смысл этого слова, но мы можем понять, что декодер по сути декодирует то, что вывел кодер.

14
00:01:55,440 --> 00:02:02,160
Слово "начало последовательности" указывает на то, что он должен начать декодирование последовательности.

15
00:02:03,600 --> 00:02:10,240
Теперь, когда у нас есть и вектор признаков, и начальное сгенерированное слово, нам больше не нужен кодер.

16
00:02:10,240 --> 00:02:17,760
  Как мы уже видели на примере декодера, он может действовать в авторегрессивной манере: слово, которое он только что вывел, теперь может быть использовано в качестве входа.

17
00:02:18,640 --> 00:02:30,800
Это, в сочетании с числовым представлением, выводимым кодером, может быть использовано для генерации второго слова.

18
00:02:33,200 --> 00:02:38,880
Обратите внимание, что первое слово все еще здесь поскольку модель все еще выводит его.

19
00:02:38,880 --> 00:02:50,720
Однако оно выделено серым цветом, поскольку оно нам больше не нужно. Мы можем продолжать и продолжать, например, пока декодер не выдаст значение, которое мы считаем "стоп-значением", например, точку, означающую конец последовательности.

20
00:02:53,440 --> 00:02:58,080
Здесь мы увидели полный механизм трансформера кодер-декодер: давайте пройдемся по нему еще раз.

21
00:02:58,080 --> 00:03:05,120
У нас есть начальная последовательность, которая посылается в кодер.

22
00:03:05,120 --> 00:03:12,240
Этот выходной сигнал кодера затем передается на декодер для декодирования.

23
00:03:12,240 --> 00:03:17,840
Если мы теперь можем выбросить кодер после одного использования, то декодер будет использоваться несколько раз: пока мы не сгенерируем все слова, которые нам нужны.

24
00:03:20,000 --> 00:03:25,120
Давайте рассмотрим конкретный пример Translation Language Modeling, также называемое трансдукцией - акт перевода последовательности.
  
25
00:03:25,120 --> 00:03:30,800
Здесь мы хотели бы перевести эту английскую последовательность "Welcome to NYC" на французский язык.

26
00:03:30,800 --> 00:03:38,400
Мы используем модель трансформера, которая специально обучена для этой задачи.

27
00:03:38,400 --> 00:03:43,520
Мы используем кодер для создания представления английского предложения.

28
00:03:43,520 --> 00:03:48,880
Мы передаем это декодеру и, используя слово начала последовательности, просим его вывести первое слово.

29
00:03:50,720 --> 00:03:52,960
Он выводит "Bienvenue", что означает " Welcome".

30
00:03:55,280 --> 00:04:02,480
Затем мы используем "Bienvenue" в качестве входной последовательности для декодера.

31
00:04:04,320 --> 00:04:08,480
Это, наряду с вектором признаков, позволяет декодеру предсказать второе слово, "?", которое в английском языке означает "to".

32
00:04:10,160 --> 00:04:14,400
Наконец, мы просим декодер спрогнозировать третье слово он прогнозирует "NYC", что снова является правильным.

33
00:04:14,400 --> 00:04:24,880
Мы перевели предложение! Где кодер-декодер действительно блистает, так это в том, что у нас есть кодер и декодер, которые часто не имеют общих весов.

34
00:04:27,280 --> 00:04:31,440
Таким образом, у нас есть целый блок (кодер), который можно обучить понимать последовательность и извлекать релевантную информацию.

35
00:04:31,440 --> 00:04:49,040
Например, для сценария перевода, который мы рассматривали ранее, это означает разбор и понимание того, что было сказано на английском языке извлечение информации из этого языка и помещение всего этого в вектор, насыщенный информацией.

36
00:04:50,880 --> 00:04:57,280
С другой стороны, у нас есть декодер, единственной целью которого является декодирование признака, выводимого кодером.

37
00:04:57,280 --> 00:05:03,760
Этот декодер может быть специализирован для совершенно другого языка или даже модальности, например, изображения или речи. Кодеры-декодеры являются особенными по нескольким причинам.

38
00:05:03,760 --> 00:05:17,040
Во-первых, они способны управлять последовательными задачами, такими как перевод, который мы только что видели.

39
00:05:18,640 --> 00:05:23,880
Во-вторых, веса между частями кодера и декодера не обязательно являются общими.

40
00:05:24,480 --> 00:05:31,200
Рассмотрим другой пример перевода. Здесь мы переводим "Transformers are powerful" на французский язык.

41
00:05:32,240 --> 00:05:36,560
Во-первых, это означает, что из последовательности из трех слов мы можем создать последовательность из четырех слов.

42
00:05:36,560 --> 00:05:46,960
Кто-то может возразить, что с этим можно справиться с помощью декодера, который генерирует перевод авторегрессивным способом и он будет прав!

43
00:05:49,840 --> 00:05:53,840
Другим примером того, где трансформеры последовательности в последовательность блистают, это суммаризация.

44
00:05:54,640 --> 00:05:58,560
Здесь у нас есть очень длинная последовательность, как правило, полный текст, и мы хотим обобщить его.

45
00:05:58,560 --> 00:06:13,840
Поскольку кодер и декодер разделены, мы можем иметь различную длину контекста (например, очень длинный контекст для кодера, который обрабатывает текст, и меньший контекст для декодера, который обрабатывает суммаризированную последовательность).

46
00:06:16,240 --> 00:06:24,160
Существует множество моделей последовательности в последовательность. Здесь приведено несколько примеров популярных моделей кодера-декодера, доступных в библиотеке Transformers.

47
00:06:26,320 --> 00:06:31,200
Кроме того, вы можете загрузить кодер и декодер внутрь модели кодера-декодера!

48
00:06:31,200 --> 00:06:40,240
Поэтому в зависимости от конкретной задачи, которую вы ставите перед собой, вы можете использовать определенные кодеры и декодеры, которые хорошо зарекомендовали себя при решении этих конкретных задач. На этом мы завершаем рассмотрение кодеров-декодеров.

49
00:06:40,240 --> 00:06:49,850
Спасибо за просмотр!
