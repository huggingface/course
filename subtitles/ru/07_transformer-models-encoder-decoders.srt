1
00:00:04,160 --> 00:00:07,200
В этом видео мы изучим архитектуру кодера-декодера.

2
00:00:08,160 --> 00:00:16,160
Примером популярной модели кодера-декодера является T5. Для того чтобы понять, как работает кодер-декодер

3
00:00:16,160 --> 00:00:21,680
мы рекомендуем вам ознакомиться с видео о кодерах и декодерах как самостоятельных моделях.

4
00:00:22,400 --> 00:00:30,320
Понимание того, как они ведут себя по отдельности, поможет понять, как ведет себя кодер-декодер.

5
00:00:30,320 --> 00:00:35,360
Давайте начнем с того, что мы видели о кодере. Кодер принимает слова в качестве входа,

6
00:00:36,000 --> 00:00:40,640
прогоняет их через кодер и извлекает числовое представление

7
00:00:40,640 --> 00:00:47,360
для каждого пропущенного через него слова. Теперь мы знаем, что числовое представление содержит информацию

8
00:00:47,360 --> 00:00:54,000
о смысле последовательности. Давайте отложим это в сторону и добавим к диаграмме декодер.

9
00:00:56,480 --> 00:01:00,160
В этом сценарии мы используем декодер таким образом, которого раньше не видели.

10
00:01:00,720 --> 00:01:07,600
Мы передаем выходы кодера непосредственно на него! Дополнительно к выходам кодера,

11
00:01:07,600 --> 00:01:13,040
мы также передаем декодеру последовательность. При запросе декодера на вывод без

12
00:01:13,040 --> 00:01:17,360
начальной последовательности, мы можем передать ему значение, указывающее на начало последовательности.

13
00:01:18,000 --> 00:01:23,520
И именно здесь происходит волшебство кодера-декодера. Кодер принимает на вход последовательность.

14
00:01:24,560 --> 00:01:30,480
Он вычисляет прогноз и выдает числовое представление. Затем он посылает

15
00:01:30,480 --> 00:01:38,000
его декодеру. Он, в некотором смысле, закодировал последовательность. А декодер, в свою очередь, 

16
00:01:38,000 --> 00:01:42,960
используя этот вход наряду с обычной входной последовательностью, попытается декодировать последовательность.

17
00:01:44,720 --> 00:01:50,400
Декодер декодирует последовательность и выводит слово. На данный момент нам не нужно понимать

18
00:01:50,400 --> 00:01:55,440
смысл этого слова, но мы можем понять, что декодер по сути декодирует то, что вывел кодер. 

19
00:01:55,440 --> 00:02:02,160
Слово "Start of sequence word" указывает на то, что он должен начать декодирование последовательности.

20
00:02:03,600 --> 00:02:10,240
Теперь, когда у нас есть и вектор признаков, и начальное сгенерированное слово, нам больше не нужен

21
00:02:10,240 --> 00:02:17,760
кодер. Как мы уже видели на примере декодера, он может действовать в авторегрессивной манере;

22
00:02:18,640 --> 00:02:24,960
слово, которое он только что вывел, теперь может быть использовано в качестве входа. Это, в сочетании с

23
00:02:24,960 --> 00:02:30,800
числовым представлением, выводимым кодером, может быть использовано для генерации второго слова.

24
00:02:33,200 --> 00:02:38,880
Обратите внимание, что первое слово все еще здесь, поскольку модель все еще выводит его. Однако оно выделено серым цветом,

25
00:02:38,880 --> 00:02:45,120
поскольку оно нам больше не нужно. Мы можем продолжать и продолжать, например, пока декодер

26
00:02:45,120 --> 00:02:50,720
не выдаст значение, которое мы считаем "stopping value", например, точку, означающую конец последовательности.

27
00:02:53,440 --> 00:02:58,080
Здесь мы увидели весь механизм трансформера кодер-декодер: давайте пройдемся по нему 

28
00:02:58,080 --> 00:03:05,120
еще раз.  У нас есть начальная последовательность, которая отправляется на кодер. Затем выходной сигнал кодера

29
00:03:05,120 --> 00:03:12,240
передается декодеру для декодирования. Если кодер мы теперь можем выбросить после одного использования,

30
00:03:12,240 --> 00:03:17,840
то декодер будет использоваться несколько раз: пока мы не сгенерируем все слова, которые нам нужны.

31
00:03:20,000 --> 00:03:25,120
Рассмотрим конкретный случай; на примере Translation Language Modeling; также называемого трансдукцией;

32
00:03:25,120 --> 00:03:30,800
акт перевода последовательности. Здесь мы хотим перевести английскую последовательность "Welcome"

33
00:03:30,800 --> 00:03:38,400
"to NYC" на французский язык. Мы используем модель трансформера, которая обучена для этой задачи в явном виде.

34
00:03:38,400 --> 00:03:43,520
Мы используем кодер для создания представления английского предложения. Мы передаем это

35
00:03:43,520 --> 00:03:48,880
декодеру и, используя слово "start of sequence", просим его вывести первое слово.

36
00:03:50,720 --> 00:03:52,960
Он выводит "Bienvenue", что означает "Welcome".

37
00:03:55,280 --> 00:04:02,480
Затем мы используем "Bienvenue" в качестве входной последовательности для декодера. Это, наряду с вектором признаков,

38
00:04:04,320 --> 00:04:08,480
позволяет декодеру предсказать второе слово, "à", которое в английском языке означает "to".

39
00:04:10,160 --> 00:04:14,400
Наконец, мы просим декодер предсказать третье слово; он предсказывает "NYC",

40
00:04:14,400 --> 00:04:20,240
что снова является правильным. Мы перевели предложение! Где кодер-декодер действительно 

41
00:04:20,240 --> 00:04:24,880
блистает, так это в том, что у нас есть кодер и декодер, которые часто не имеют общих весов.

42
00:04:27,280 --> 00:04:31,440
Таким образом, у нас есть целый блок (кодер), который можно обучить понимать последовательность

43
00:04:31,440 --> 00:04:36,480
и извлекать релевантную информацию. Например, для сценария перевода, который мы рассматривали ранее, 

44
00:04:36,480 --> 00:04:44,160
это означает разбор и понимание того, что было сказано на английском языке; извлечение 

45
00:04:44,160 --> 00:04:49,040
информации из этого языка и помещение всего этого в вектор, насыщенный информацией.

46
00:04:50,880 --> 00:04:57,280
С другой стороны, у нас есть декодер, единственной целью которого является декодирование признака, выводимого

47
00:04:57,280 --> 00:05:03,760
кодером. Этот декодер может быть специализирован для совершенно другого языка или даже модальности,

48
00:05:03,760 --> 00:05:11,760
например, изображения или речи. Кодеры-декодеры являются особенными по нескольким причинам. Во-первых,

49
00:05:11,760 --> 00:05:17,040
они способны справляться с задачами преобразования последовательности в последовательность, такими как перевод, который мы только что видели.

50
00:05:18,640 --> 00:05:23,880
Во-вторых, веса между частями кодера и декодера не обязательно являются общими. Давайте

51
00:05:24,480 --> 00:05:31,200
возьмем другой пример перевода. Здесь мы переводим "Transformers are powerful" на французский язык.

52
00:05:32,240 --> 00:05:36,560
Во-первых, это означает, что из последовательности из трех слов мы можем сгенерировать

53
00:05:36,560 --> 00:05:42,240
последовательность из четырех слов. Кто-то может возразить, что с этим можно справиться с помощью декодера,

54
00:05:42,240 --> 00:05:46,960
который будет генерировать перевод авторегрессивным способом, и он будет прав!

55
00:05:49,840 --> 00:05:53,840
Другим примером того, где трансформеры последовательности в последовательность проявляют себя с лучшей стороны, является суммаризация.

56
00:05:54,640 --> 00:05:58,560
Здесь у нас есть очень длинная последовательность, как правило, полный текст,

57
00:05:58,560 --> 00:06:03,840
и мы хотим обобщить его. Поскольку кодер и декодер разделены,

58
00:06:03,840 --> 00:06:08,880
мы можем иметь разные длины контекста (например, очень длинный контекст для кодера, который 

59
00:06:08,880 --> 00:06:13,840
обрабатывает текст, и меньший контекст для декодера, который обрабатывает обобщенный текст).

60
00:06:16,240 --> 00:06:20,480
Существует множество моделей преобразования последовательности в последовательность.

61
00:06:20,480 --> 00:06:24,160
Здесь приведено несколько примеров популярных моделей кодеров-декодеров, доступных в библиотеке трансформеров.

62
00:06:26,320 --> 00:06:31,200
Кроме того, вы можете загрузить кодер и декодер внутри модели кодера-декодера! 

63
00:06:31,200 --> 00:06:35,040
Поэтому, в зависимости от конкретной задачи, которую вы ставите перед собой,

64
00:06:35,040 --> 00:06:40,240
вы можете использовать конкретные кодеры и декодеры, которые зарекомендовали себя с лучшей стороны

65
00:06:40,240 --> 00:06:49,850
в этих конкретных задачах. На этом мы завершаем разговор о кодерах-декодерах. Спасибо за просмотр! 

