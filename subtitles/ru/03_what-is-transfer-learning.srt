1
00:00:05,440 --> 00:00:07,120
Что такое трансфертное обучение?

2
00:00:09,360 --> 00:00:13,760
Идея трансферного обучения заключается в том, чтобы использовать знания, полученные моделью, обученной на большом количестве данных для другой задачи.

3
00:00:13,760 --> 00:00:20,720
Модель A будет обучена специально для задачи A. Теперь, допустим, вы хотите обучить модель B для другой задачи.

4
00:00:20,720 --> 00:00:26,320
Одним из вариантов может быть обучение модели с нуля.

5
00:00:27,120 --> 00:00:34,240
Это может потребовать большого количества вычислений, времени и данных.

6
00:00:34,240 --> 00:00:38,880
Вместо этого мы можем инициализировать модель B с теми же весами, что и модель A, перенося знания модели A на задачу B.

7
00:00:40,800 --> 00:00:47,040
При обучении с нуля все веса модели инициализируются случайным образом.

8
00:00:47,040 --> 00:00:52,480
В этом примере мы обучаем модель BERT на задаче распознавания того, похожи или нет два предложения.

9
00:00:53,680 --> 00:00:58,560
Слева - обучение с нуля, справа - тонкая настройка предварительно обученной модели.

10
00:00:58,560 --> 00:01:04,080
Как мы видим, использование трансфертного обучения и предварительно обученной модели дает лучшие результаты.

11
00:01:04,959 --> 00:01:13,040
И неважно, обучаемся ли мы дольше, точность обучения с нуля составляет около 70%, в то время как предварительно обученная модель легко преодолевает отметку в 86%.

13
00:01:14,240 --> 00:01:22,720
Это связано с тем, что предварительно обученные модели обычно обучаются на больших объемах данных, которые обеспечивают модели статистическое понимание языка, использованного во время предварительного обучения.

15
00:01:24,240 --> 00:01:28,960
В компьютерном зрении трансферное обучение успешно применяется уже почти десять лет.

16
00:01:29,840 --> 00:01:35,840
Модели часто предварительно обучаются на ImageNet - наборе данных, содержащем 1,2 миллиона фотографий.

17
00:01:36,880 --> 00:01:48,960
Каждое изображение классифицируется по одной из 1000 меток. Такое обучение на помеченных данных называется обучением с учителем. В обработке естественного языка трансфертное обучение появилось несколько позже.

19
00:01:48,960 --> 00:01:59,280
Ключевым отличием ImageNet является то, что предварительное обучение, как правило, является самоконтролируемым, что означает, что оно не требует меток аннотированных человеком.

21
00:02:00,480 --> 00:02:08,720
Очень распространенной задачей предварительного обучения является угадывание следующего слова в предложении, для чего требуется только много-много текста. Например, GPT-2 была предварительно обучена таким образом, используя содержание 45 миллионов ссылок, размещенных пользователями на Reddit.

23
00:02:09,360 --> 00:02:25,360
"Другим примером задачи предварительного самообучения является предсказание значения случайно замаскированных слов, что похоже на тесты ""заполни пустое место"", которые вы, возможно, проходили в школе."

26
00:02:26,560 --> 00:02:31,520
BERT был предварительно обучен таким образом, используя английскую Википедию и 11 000 неопубликованных книг.

27
00:02:32,960 --> 00:02:43,680
На практике трансферное обучение применяется к заданной модели путем отбрасывания ее головы, то есть последних слоев, сфокусированных на цели предварительного обучения, и замены ее новой, случайно инициализированной головой, подходящей для поставленной задачи.

29
00:02:43,680 --> 00:02:55,440
Например, когда мы ранее проводили тонкую настройку модели BERT, мы удалили голову, которая классифицировала слова-маски, и заменили ее классификатором с двумя выходами, поскольку наша задача имела две метки.

31
00:02:55,440 --> 00:03:01,680
Чтобы быть максимально эффективной, используемая предварительно обученная модель должна быть максимально похожа на задачу, для которой она тонко настраивается.

32
00:03:01,680 --> 00:03:12,720
Например, если задача состоит в классификации немецких предложений, лучше всего использовать предварительно обученную модель на немецком языке.

34
00:03:14,160 --> 00:03:19,200
Но вместе с хорошим приходит и плохое. Предварительно обученная модель передает не только свои знания, но и любые предубеждения, которые она может содержать.

35
00:03:19,200 --> 00:03:29,680
ImageNet в основном содержит изображения из США и Западной Европы, поэтому модели, настроенные с его помощью, обычно лучше работают с изображениями из этих стран.

37
00:03:29,680 --> 00:03:40,960
OpenAI также изучили систематическую ошибку в прогнозах своей модели GPT-3 (которая была предварительно обучена с использованием угадывания следующей рабочей цели).

39
00:03:40,960 --> 00:03:46,720
Изменение пола в строке подсказке с "Он был очень" на "Она была очень" изменило предсказания с преимущественно нейтральных прилагательных на почти только физические.

40
00:03:47,360 --> 00:03:59,840
В своей карточке модели GPT-2 OpenAI также признает ее необъективность и не рекомендует использовать ее в системах, взаимодействующих с людьми.