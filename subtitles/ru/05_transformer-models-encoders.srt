1
00:00:04,320 --> 00:00:09,120
В этом видео мы изучим архитектуру кодера.

2
00:00:09,120 --> 00:00:13,120
Примером популярной архитектуры только для кодирования является BERT, которая является самой популярной моделью в своем роде.

3
00:00:14,400 --> 00:00:20,880
Для начала давайте разберемся, как это работает. Мы воспользуемся небольшим примером, используя три слова.

4
00:00:20,880 --> 00:00:27,040
Мы используем их в качестве входных данных и пропустим через кодировщик. Мы получаем числовое представление каждого слова.

5
00:00:27,040 --> 00:00:34,160
Вот, например, кодер преобразует три слова "Welcome to NYC" в эти три последовательности цифр.

6
00:00:34,160 --> 00:00:40,880
Кодер выдает ровно одну последовательность чисел на каждое входное слово.

7
00:00:40,880 --> 00:00:46,880
Это числовое представление можно также назвать "вектором признаков" или "тензором признаков".

8
00:00:48,880 --> 00:00:53,680
Давайте погрузимся в это представление. Оно содержит один вектор на каждое слово, прошедшее через кодер.

9
00:00:53,680 --> 00:00:59,680
Каждый из этих векторов является числовым представлением рассматриваемого слова.

10
00:01:00,880 --> 00:01:06,400
Размерность этого вектора определяется архитектурой модели, для базовой модели BERT она равна 768.

11
00:01:06,400 --> 00:01:15,280
Эти представления содержат значение слова, но с учетом контекста.

12
00:01:15,280 --> 00:01:21,280
Например, вектор, приписываемый слову "to", не является представлением только слова "to".

13
00:01:22,160 --> 00:01:29,680
Он также учитывает окружающие слова, которые мы называем "контекстом".

14
00:01:29,680 --> 00:01:41,120
Например, он смотрит на левый контекст, слово слева от изучаемого нами (здесь слово "Welcome") и контекст справа (здесь слово "NYC") и выводит значение для этого слова в пределах его контекста.

15
00:01:41,840 --> 00:01:49,280
Таким образом, это контекстуализированное значение. Можно сказать, что вектор из 768 значений содержит значение этого слова в тексте.

16
00:01:49,280 --> 00:01:55,840
Это происходит благодаря механизму самовнимания.

17
00:01:57,120 --> 00:02:08,320
Механизм самовнимания относится к различным позициям (или различным словам) в одной последовательности, чтобы вычислить представление этой последовательности.

18
00:02:08,320 --> 00:02:13,600
Как мы уже видели ранее, это означает, что на результирующее представление слова повлияли другие слова в последовательности.

19
00:02:15,600 --> 00:02:26,480
Мы не будем вдаваться в подробности, но предложим несколько дополнительных материалов, если вы хотите лучше понять, что происходит под капотом. Когда же следует использовать кодер?

20
00:02:27,040 --> 00:02:33,680
Кодеры могут использоваться как самостоятельные модели в самых разных задачах.

21
00:02:33,680 --> 00:02:44,000
Например, BERT, возможно, самая известная модель трансформера, является отдельной моделью кодера и на момент выпуска превосходила лучшие достижения во многих задачах классификации последовательностей, задачах ответа на вопросы и моделировании языка с маской, и это лишь некоторые из них.

22
00:02:44,000 --> 00:02:50,240
Идея заключается в том, что кодеры очень сильны в извлечении векторов, которые несут значимую информацию о последовательности.

23
00:02:50,240 --> 00:02:59,680
Этот вектор затем может быть обработан дополнительными слоями нейронов, чтобы понять его смысл.

24
00:03:01,200 --> 00:03:04,240
Давайте рассмотрим несколько примеров, в которых кодеры действительно блистают.

25
00:03:06,080 --> 00:03:11,760
Прежде всего, Masked Language Modeling, или MLM. Это задача предсказания скрытого слова в последовательности слов.

26
00:03:11,760 --> 00:03:18,560
Здесь, например, мы спрятали слово между "My" и "is".

27
00:03:18,560 --> 00:03:24,000
Это одна из целей обучения BERT: он был обучен предсказывать скрытые слова в последовательности.

28
00:03:25,040 --> 00:03:30,160
Кодеры в этом сценарии особенно хороши, поскольку двунаправленная информация имеет здесь решающее значение.

29
00:03:30,960 --> 00:03:35,520
Если бы у нас не было слов справа (is, Sylvain и точка), то вероятность того, что BERT смог бы определить "name" как правильное слово, очень мала.

30
00:03:35,520 --> 00:03:52,080
Кодировщик должен хорошо понимать последовательность, чтобы спрогнозировать замаскированное слово, поскольку даже если текст грамматически правильный, он не обязательно имеет смысл в контексте последовательности.

31
00:03:54,960 --> 00:03:58,720
Как упоминалось ранее, кодеры хорошо справляются с классификацией последовательностей.

32
00:03:59,360 --> 00:04:03,560
Анализ настроений является примером задачи классификации последовательности.

33
00:04:04,240 --> 00:04:16,720
Целью модели является определение настроения последовательности - оно может варьироваться от присвоения последовательности рейтинга от одной до пяти звезд, если проводится анализ отзывов, до присвоения последовательности положительной или отрицательной оценки, что и показано здесь.

34
00:04:16,720 --> 00:04:28,800
Например, здесь, дано две последовательности, мы используем модель для вычисления прогноза и классификации последовательностей между этими двумя классами: положительными и отрицательными.

35
00:04:28,800 --> 00:04:41,840
Хотя эти две последовательности очень похожи, содержат одни и те же слова, смысл их различен - и модель кодера способна уловить эту разницу.
