1
00:00:06,320 --> 00:00:11,440
Давайте рассмотрим токенизацию на основе подслов.

2
00:00:11,440 --> 00:00:16,320
Понимание того, почему токенизация на основе подслова интересна, требует понимания недостатков токенизации на основе слов и символов.

3
00:00:17,200 --> 00:00:24,400
Если вы еще не видели первые видеоролики о токенизации на основе слов и символов, рекомендуем ознакомиться с ними перед просмотром этого видео.

4
00:00:27,680 --> 00:00:33,440
Токенизация подслов находится между алгоритмами токенизации на основе символов и на основе слов.

5
00:00:33,440 --> 00:00:52,800
Идея заключается в том, чтобы найти золотую середину между очень большими словарями, большим количеством токенов вне словаря, потерей смысла между очень похожими словами для токенизаторов на основе слов, и очень длинными последовательностями, менее значимыми отдельными токенами для токенизаторов на основе символов.

6
00:00:54,720 --> 00:01:04,800
Эти алгоритмы основываются на следующем принципе: часто используемые слова не должны разбиваться на более мелкие подслова, а редкие слова должны быть разложены на значимые подслова.

7
00:01:06,320 --> 00:01:11,520
Примером может служить слово "dog": мы хотели бы, чтобы наш токенизатор имел один идентификатор для слова "dog", а не разбивал его на символы: "d", "o" и "g".

8
00:01:11,520 --> 00:01:23,920
Однако, встречая слово "dogs", мы хотели бы, чтобы наш токенизатор понимал, что в корне это все еще слово "dog", с добавлением "s", что немного меняет смысл, сохраняя первоначальную идею.

9
00:01:23,920 --> 00:01:31,280
Другой пример - такое сложное слово, как "tokenization", которое можно разбить на значимые подслова.

10
00:01:31,280 --> 00:01:42,000
Корень этого слова - "token", а "ization" дополняет корень, придавая ему несколько иное значение.

11
00:01:42,720 --> 00:01:48,960
Имеет смысл разделить слово на два: "token", как корень слова (обозначенный как "start" слова).

12
00:01:48,960 --> 00:01:53,840
И "ization" в качестве дополнительной информации (помечено как "completion" слова).

13
00:01:56,240 --> 00:02:00,320
В свою очередь, модель теперь будет способна придавать смысл токену в различных ситуациях.

14
00:02:00,880 --> 00:02:06,400
Она будет понимать, что слова "token", "tokens", "tokenizing" и "tokenization" связаны между собой и имеют похожее значение.

15
00:02:06,400 --> 00:02:18,960
Он также поймет, что "tokenization", "modernization" и "immunization", которые имеют одинаковые суффиксы, вероятно, используются в одних и тех же синтаксических ситуациях.

16
00:02:20,320 --> 00:02:25,920
Токенизаторы на основе подслов обычно имеют способ определить, какие токены являются началом слов, а какие токены завершают начало слов: "token" как начало слова.

17
00:02:25,920 --> 00:02:34,320
"##ization" как завершение слова.

18
00:02:34,960 --> 00:02:40,800
Здесь префикс "##" указывает на то, что "ization" является частью слова, а не его началом.

19
00:02:41,760 --> 00:02:49,440
"##" происходит от токенизатора BERT, основанного на алгоритме WordPiece.

20
00:02:49,440 --> 00:02:54,720
Другие токенизаторы используют другие префиксы, которые могут быть помещены для обозначения части слов, как показано здесь, или начала слов!

21
00:02:56,000 --> 00:03:05,760
Существует множество различных алгоритмов, которые могут быть использованы для токенизации подслов, и большинство моделей, получающих передовые результаты в английском языке, сегодня используют тот или иной алгоритм токенизации подслов.

22
00:03:05,760 --> 00:03:17,840
Эти подходы помогают уменьшить размер словаря за счет обмена информацией между различными словами, имеют возможность понимать префиксы и суффиксы как таковые.

23
00:03:18,480 --> 00:03:27,760
Они сохраняют смысл очень похожих слов, распознавая похожие токены, из которых они состоят.