1
00:00:04,160 --> 00:00:13,680
Прежде чем погрузиться в токенизацию на основе символов, понимание того, почему этот вид токенизации интересен, требует понимания недостатков токенизации на основе слов.

2
00:00:14,560 --> 00:00:18,400
Если вы еще не видели первое видео о токенизации на основе слов, рекомендуем ознакомиться с ним перед просмотром этого видео.

3
00:00:18,400 --> 00:00:23,920
Давайте рассмотрим токенизацию на основе символов.

4
00:00:25,440 --> 00:00:29,840
Теперь мы разбиваем наш текст не на слова, а на отдельные символы.

5
00:00:32,720 --> 00:00:37,200
В языках обычно много различных слов, в то время как количество символов остается небольшим.

6
00:00:37,200 --> 00:00:48,960
Например, для английского языка, который насчитывает около 170 000 различных слов, нам потребуется очень большой словарный запас, чтобы охватить все слова.

7
00:00:50,080 --> 00:00:55,040
При использовании словаря, основанного на символах, мы можем обойтись всего 256 символами!

8
00:00:59,600 --> 00:01:04,880
Даже языки с большим количеством различных символов, такие как китайский, имеют словари с ~20 000 различных символов, но более 375 000 различных слов.

9
00:01:06,160 --> 00:01:20,240
Словари на основе символов позволяют нам использовать меньше различных токенов, чем словари для токенизации на основе слов, которые мы бы использовали в противном случае.

10
00:01:23,040 --> 00:01:28,000
Эти словари также являются более полными, чем их аналоги, основанные на словах.

11
00:01:28,720 --> 00:01:39,840
Поскольку наш словарь содержит все символы, используемые в языке, даже слова, не замеченные во время обучения токенизатора, все равно могут быть обработаны, поэтому не входящие в словарь токены будут встречаться реже.

12
00:01:40,480 --> 00:01:45,200
Это включает в себя способность правильно обозначать неправильно написанные слова, а не сразу отбрасывать их как неизвестные.

13
00:01:45,200 --> 00:01:53,600
Однако и этот алгоритм не идеален! Интуитивно понятно, что символы по отдельности не содержат столько информации, сколько содержит слово.

14
00:01:53,600 --> 00:01:59,760
Например, "Let's" содержит больше информации, чем "l".

15
00:01:59,760 --> 00:02:17,200
Конечно, это верно не для всех языков, так как некоторые языки, например, языки, основанные на идеограммах, содержат много информации в отдельных символах, а для других, например, языков, основанных на латинице, модель должна осмыслить несколько токенов за раз, чтобы получить информацию, содержащуюся в одном слове.

16
00:02:17,200 --> 00:02:30,320
Это приводит к другой проблеме с токенизаторами на основе символов: их последовательности переводятся в очень большое количество токенов, которые должны быть обработаны моделью.

17
00:02:30,320 --> 00:02:37,680
Это может повлиять на размер контекста, с которым будет работать модель, и уменьшит размер текста, который мы можем использовать в качестве входных данных для нашей модели.

18
00:02:37,680 --> 00:03:00,720
Эта токенизация, хотя и имеет некоторые проблемы, показала очень хорошие результаты в прошлом и должна рассматриваться при подходе к новой проблеме, поскольку она решает некоторые проблемы, возникающие при использовании алгоритма на основе слов.