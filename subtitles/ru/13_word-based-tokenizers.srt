1
00:00:03,120 --> 00:00:10,240
Давайте рассмотрим токенизацию на основе слов.

2
00:00:10,240 --> 00:00:19,040
Токенизация на основе слов - это идея разделения необработанного текста на слова с помощью пробелов или других специфических правил, таких как пунктуация.

3
00:00:19,040 --> 00:00:33,120
В этом алгоритме каждому слову приписывается определенный номер, "ID". В данном примере "Let's" имеет ID 250, "do" - ID 861, а токенизация, за которой следует восклицательный знак, имеет ID 345.

4
00:00:34,160 --> 00:00:39,840
Этот подход интересен тем, что модель имеет представления, основанные на целых словах.

5
00:00:42,560 --> 00:00:52,880
Информация, содержащаяся в одном числе, очень велика, поскольку слово содержит много контекстуальной и семантической информации в предложении.

6
00:00:52,880 --> 00:00:58,720
Однако этот подход имеет свои ограничения.

7
00:00:58,720 --> 00:01:04,320
Например, слово "dog" и слово "dogs" очень похожи, и их значения близки.

8
00:01:05,280 --> 00:01:14,880
Однако токенизация на основе слов приписывает этим двум словам совершенно разные идентификаторы, и поэтому модель узнает разные значения для этих двух слов.

9
00:01:14,880 --> 00:01:21,120
Это печально, поскольку мы хотели бы, чтобы модель понимала, что эти слова действительно связаны, и что "dogs" - это форма множественного числа слова "dog".

10
00:01:22,800 --> 00:01:26,400
Другая проблема такого подхода заключается в том, что в языке существует множество различных слов.

11
00:01:27,840 --> 00:01:41,440
Если мы хотим, чтобы наша модель понимала все возможные предложения на этом языке, то нам понадобятся идентификаторы для каждого слова, и общее количество слов, которое также известно как объем словарного запаса, может быстро стать очень большим.

12
00:01:44,160 --> 00:01:55,840
Это проблема, потому что каждый идентификатор сопоставляется с большим вектором, который представляет значение слова, и отслеживание этих сопоставлений требует огромного количества весов при большом объеме словаря.

13
00:01:55,840 --> 00:02:03,360
Если мы хотим, чтобы наши модели оставались компактными, мы можем выбрать для нашего токенизатора игнорирование некоторых слов, которые нам не обязательно нужны.

14
00:02:03,360 --> 00:02:23,520
Например, при обучении токенизатора на тексте мы можем взять 10 000 наиболее часто встречающихся слов в этом тексте для создания нашего базового словаря, вместо того чтобы брать все слова этого языка.

15
00:02:23,520 --> 00:02:33,520
Токенизатор будет знать, как преобразовать эти 10 000 слов в числа, но любое другое слово будет преобразовано в слово вне словарного запаса, или "unknown" слово.

16
00:02:36,000 --> 00:02:44,720
Это может быстро стать проблемой: модель будет иметь точно такое же представление для всех слов, которые она не знает, что приведет к потере большого количества информации.