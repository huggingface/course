1
00:00:05,200 --> 00:00:14,240
Что происходит внутри функции конвейера? В этом видео мы рассмотрим, что на самом деле происходит, когда мы используем функцию pipeline библиотеки Transformers.

2
00:00:14,880 --> 00:00:24,960
Более конкретно, мы рассмотрим конвейер анализа настроений и то, как он перешел от двух следующих предложений к положительным меткам с соответствующими оценками.

3
00:00:26,560 --> 00:00:30,720
Как мы уже видели в презентации конвейера, в нём есть три этапа.

4
00:00:31,520 --> 00:00:35,920
Сначала мы преобразуем необработанные тексты в числа, которые могут быть понятны модели, с помощью токенизатора.

5
00:00:35,920 --> 00:00:41,520
Затем эти числа проходят через модель, которая выводит логиты.

6
00:00:42,640 --> 00:00:47,040
Наконец, на этапе постобработки эти логиты преобразуются в метки и оценки.

7
00:00:47,920 --> 00:00:53,440
Давайте подробно рассмотрим эти три этапа и то, как воспроизвести их с помощью библиотеки Transformers, начиная с первого этапа - токенизации.

8
00:00:53,440 --> 00:01:01,040
Процесс токенизации состоит из нескольких этапов.

9
00:01:01,040 --> 00:01:07,360
Сначала текст разбивается на небольшие фрагменты, называемые лексемами. Это могут быть слова, части слов или знаки препинания.

10
00:01:07,360 --> 00:01:14,160
Затем токенизатор добавит несколько специальных лексем (если модель их ожидает).

11
00:01:14,160 --> 00:01:19,440
Здесь модель использует токен CLS в начале и токен SEP в конце предложения для классификации.

12
00:01:20,400 --> 00:01:25,440
Наконец, токенизатор сопоставляет каждую токену с его уникальным идентификатором в словаре предварительно обученной модели.

13
00:01:25,440 --> 00:01:31,360
Для загрузки такого токенизатора библиотека Transformers предоставляет API AutoTokenizer.

14
00:01:32,400 --> 00:01:41,680
Наиболее важным методом этого класса является метод from_pretrained, который загружает и кэширует конфигурацию и словарь, связанные с данной контрольной точкой.

15
00:01:43,040 --> 00:01:48,880
Здесь контрольной точкой, используемой по умолчанию для конвейера анализа настроений, является distilbert base uncased finetuned sst2 english.

16
00:01:48,880 --> 00:01:56,080
Мы создаем токенизатор, связанный с этой контрольной точкой, и передаем ему два предложения.

17
00:01:56,640 --> 00:02:05,040
Поскольку эти два предложения не имеют одинакового размера, нам нужно будет дополнить самое короткое из них, чтобы построить массив.

18
00:02:05,760 --> 00:02:08,240
Это делается токенизатором с опцией padding=True.

19
00:02:09,600 --> 00:02:14,800
Если truncation=True, мы гарантируем, что любое предложение длиннее максимума, который может обработать модель, будет усечено.

20
00:02:14,800 --> 00:02:21,840
Наконец, параметр return_tensors указывает токенизатору возвращать тензор PyTorch.

21
00:02:23,040 --> 00:02:29,040
Посмотрев на результат, мы видим, что у нас есть словарь с двумя ключами.

22
00:02:29,040 --> 00:02:37,840
"Input IDs" содержит идентификаторы обоих предложений, с 0 в тех местах, где применен паддинг. Второй ключ, "attention mask", указывает, где был применен паддинг, так что модель не обращает на него внимания.

23
00:02:38,640 --> 00:02:43,040
Все это находится внутри шага токенизации. Теперь давайте посмотрим на второй шаг - модель.

24
00:02:43,760 --> 00:02:50,560
Что касается токенизатора, существует API AutoModel с методом from_pretrained.

25
00:02:50,560 --> 00:02:54,720
Он загружает и кэширует конфигурацию модели, а также предварительно обученные веса.

26
00:02:55,840 --> 00:03:05,120
Однако API AutoModel будет инстанцировать только тело модели, то есть ту часть модели, которая останется после удаления предварительно обученной головы.

27
00:03:05,840 --> 00:03:11,360
Он выведет высокоразмерный тензор, который является представлением пройденных предложений, но который не является напрямую полезным для нашей задачи классификации. 

28
00:03:11,360 --> 00:03:17,200
Здесь тензор состоит из двух предложений, каждое из шестнадцати токенов, а последнее измерение - скрытый размер нашей модели 768.

29
00:03:17,200 --> 00:03:30,240
Чтобы получить вывод, соответствующий нашей задаче классификации, нам нужно использовать класс AutoModelForSequenceClassification.

30
00:03:30,960 --> 00:03:35,200
Он работает точно так же, как класс AutoModel, за исключением того, что он будет строить модель с головой классификации.

31
00:03:35,200 --> 00:03:40,720
В библиотеке Transformers есть один автокласс для каждой общей задачи NLP.

32
00:03:42,000 --> 00:03:47,600
Здесь, после предоставления нашей модели двух предложений, мы получаем тензор размера два на два: один результат для каждого предложения и для каждой возможной метки.

33
00:03:47,600 --> 00:03:53,680
Эти выходы еще не являются вероятностями (мы видим, что их сумма не равна 1).

34
00:03:53,680 --> 00:03:59,120
Это связано с тем, что каждая модель библиотеки Transformers возвращает логиты.

35
00:03:59,120 --> 00:04:05,120
Чтобы понять смысл этих логитов, нам нужно углубиться в третий и последний этап конвейера: постобработку.

36
00:04:05,680 --> 00:04:11,840
Чтобы преобразовать логиты в вероятности, нам нужно применить к ним слой SoftMax.

37
00:04:11,840 --> 00:04:17,760
Как мы видим, это превращает их в положительные числа, сумма которых равна 1.

38
00:04:18,959 --> 00:04:22,800
Последний шаг - узнать, какой из них соответствует положительной или отрицательной метке.

39
00:04:23,360 --> 00:04:30,160
Это задается полем "id2label" в конфигурации модели.

40
00:04:30,160 --> 00:04:35,360
Первые вероятности (индекс 0) соответствуют отрицательной метке, а вторые (индекс 1) - положительной.

41
00:04:36,240 --> 00:04:40,560
Вот как наш классификатор, построенный с помощью функции pipeline, выбирает эти метки и вычисляет оценки.

42
00:04:40,560 --> 00:04:52,080
Теперь, когда вы знаете, как работает каждый шаг, вы можете легко подстроить их под свои нужды.
