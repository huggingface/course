1
00:00:04,960 --> 00:00:07,120
Давайте изучим архитектуру трансформера.

2
00:00:08,960 --> 00:00:13,840
Этот видео является вводным в серию видео о кодерах, декодерах и кодер-декодерах.

3
00:00:13,840 --> 00:00:18,640
В этой серии, мы попытаемся понять, что представляет собой трансформерная сеть, и постараемся объяснить это простыми, высокоуровневыми терминами.

4
00:00:18,640 --> 00:00:29,840
Понимание нейронных сетей не требуется, может помочь только понимание основ векторов и тензоров.

5
00:00:32,320 --> 00:00:36,480
Для начала мы возьмем эту диаграмму из оригинальной статьи о трансформерах, озаглавленной "Внимание - все, что вам нужно".

6
00:00:36,480 --> 00:00:42,640
Как мы увидим здесь, мы можем использовать только некоторые его части, в зависимости от того, что мы пытаемся сделать.

7
00:00:42,640 --> 00:00:52,560
Мы не будем углубляться в конкретные слои, составляющие эту архитектуру, но попытаемся понять различные способы использования этой архитектуры.

8
00:00:54,960 --> 00:00:59,760
Для начала давайте разделим эту архитектуру на две части.

9
00:00:59,760 --> 00:01:04,319
Слева находится кодер, а справа - декодер. Их можно использовать вместе, но можно и независимо!

10
00:01:04,319 --> 00:01:11,280
Давайте разберемся, как они работают: Кодер принимает входные данные, представляющие собой текст.

11
00:01:11,280 --> 00:01:17,200
Он преобразует этот текст, эти слова, в числовые представления.

12
00:01:17,200 --> 00:01:23,120
Эти числовые представления могут также называться эмбеддингами, или признаками. Мы увидим, что в качестве основного компонента он использует механизм самовнимания.

13
00:01:23,120 --> 00:01:29,840
Мы рекомендуем вам посмотреть видео о кодерах, чтобы понять, что это за числовое представление, а также как оно работает.

14
00:01:29,840 --> 00:01:36,640
Мы изучим механизм самовнимания, а также его двунаправленные свойства.

15
00:01:36,640 --> 00:01:47,200
Декодер похож на кодер: он также может принимать те же входные данные, что и кодер: входные данные, представляющие собой текст. Он использует такой же механизм, как и кодер, который также является маскированным самовниманием.

16
00:01:47,200 --> 00:01:59,200
Он отличается от кодера своим однонаправленным свойством и традиционно используется в авторегрессии.

17
00:01:59,200 --> 00:02:03,600
Здесь мы также рекомендуем вам посмотреть видео о декодерах, чтобы понять, как все это работает.

18
00:02:06,560 --> 00:02:11,120
В результате объединения этих двух частей получается так называемый кодер-декодер, или трансформер последовательности в последовательность.

19
00:02:11,120 --> 00:02:16,640
Кодер принимает входные данные и вычисляет высокоуровневое представление этих данных.

20
00:02:16,640 --> 00:02:22,640
Эти выходы затем передаются в декодер.

21
00:02:22,640 --> 00:02:27,680
Декодер использует выход кодера наряду с другими входами для генерации прогноза.

22
00:02:27,680 --> 00:02:32,000
Затем он прогнозирует выход, который он будет повторно использовать в будущих итерациях, отсюда и термин "авторегрессивный".

23
00:02:33,040 --> 00:02:44,080
Наконец, чтобы получить представление о кодерах-декодерах в целом, мы рекомендуем вам ознакомиться с видео о кодерах-декодерах.
