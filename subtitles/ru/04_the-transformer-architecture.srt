1
00:00:00,000 --> 00:00:02,750


2
00:00:05,010 --> 00:00:07,323
- Давайте изучим архитектуру трансформера.

3
00:00:09,150 --> 00:00:12,030
Этот видео является вводным в серию видео о кодерах,

4
00:00:12,030 --> 00:00:15,510
декодерах и кодер-декодерах.

5
00:00:15,510 --> 00:00:16,343
В этой серии,

6
00:00:16,343 --> 00:00:18,900
мы попытаемся понять, что представляет собой трансформерная сеть,

7
00:00:18,900 --> 00:00:22,770
и постараемся объяснить это простыми, высокоуровневыми терминами.

8
00:00:22,770 --> 00:00:25,800
Понимание нейронных сетей не требуется,

9
00:00:25,800 --> 00:00:29,343
может помочь только понимание основ векторов и тензоров.

10
00:00:32,250 --> 00:00:33,270
Для начала

11
00:00:33,270 --> 00:00:34,530
мы возьмем эту диаграмму

12
00:00:34,530 --> 00:00:36,630
из оригинальной статьи о трансформерах,

13
00:00:36,630 --> 00:00:40,140
озаглавленной "Внимание - все, что вам нужно".

14
00:00:40,140 --> 00:00:41,010
Как мы увидим здесь,

15
00:00:41,010 --> 00:00:42,780
мы можем использовать только некоторые его части,

16
00:00:42,780 --> 00:00:44,630
в зависимости от того, что мы пытаемся сделать.

17
00:00:45,480 --> 00:00:47,610
Мы не будем углубляться в конкретные слои,

18
00:00:47,610 --> 00:00:48,990
составляющие эту архитектуру,

19
00:00:48,990 --> 00:00:51,390
но попытаемся понять различные способы

20
00:00:51,390 --> 00:00:52,893
использования этой архитектуры.

21
00:00:55,170 --> 00:00:56,003
Для начала давайте

22
00:00:56,003 --> 00:00:58,260
разделим эту архитектуру на две части.

23
00:00:58,260 --> 00:00:59,910
Слева находится кодер,

24
00:00:59,910 --> 00:01:01,980
а справа - декодер.

25
00:01:01,980 --> 00:01:03,330
Их можно использовать вместе,

26
00:01:03,330 --> 00:01:05,330
но можно и независимо.

27
00:01:06,180 --> 00:01:08,610
Давайте разберемся, как они работают.

28
00:01:08,610 --> 00:01:11,460
Кодер принимает входные данные, представляющие собой текст.

29
00:01:11,460 --> 00:01:13,620
Он преобразует этот текст, эти слова,

30
00:01:13,620 --> 00:01:15,675
в числовые представления.

31
00:01:15,675 --> 00:01:17,400
Эти числовые представления

32
00:01:17,400 --> 00:01:20,460
могут также называться эмбеддингами, или признаками.

33
00:01:20,460 --> 00:01:23,100
Мы увидим, что он использует механизм самовнимания

34
00:01:23,100 --> 00:01:24,483
в качестве основного компонента.

35
00:01:25,500 --> 00:01:27,120
Мы рекомендуем вам посмотреть видео

36
00:01:27,120 --> 00:01:29,700
о кодерах специально для того, чтобы понять

37
00:01:29,700 --> 00:01:31,680
что такое это числовое представление,

38
00:01:31,680 --> 00:01:33,690
а также как оно работает.

39
00:01:33,690 --> 00:01:36,660
Мы изучим механизм самовнимания более подробно,

40
00:01:36,660 --> 00:01:38,913
а также его двунаправленные свойства.

41
00:01:40,650 --> 00:01:42,780
Декодер аналогичен кодеру.

42
00:01:42,780 --> 00:01:45,630
Он также может принимать текстовые входы.

43
00:01:45,630 --> 00:01:48,210
Он использует аналогичный механизм, что и кодер,

44
00:01:48,210 --> 00:01:51,150
который также является маскированным самовниманием.

45
00:01:51,150 --> 00:01:52,590
Он отличается от кодера

46
00:01:52,590 --> 00:01:54,990
своим однонаправленным свойством

47
00:01:54,990 --> 00:01:58,590
и традиционно используется в авторегрессионной манере.

48
00:01:58,590 --> 00:02:01,650
Здесь мы также рекомендуем вам посмотреть видео о декодерах,

49
00:02:01,650 --> 00:02:04,000
особенно для того, чтобы понять, как все это работает.

50
00:02:06,810 --> 00:02:07,890
Комбинирование этих двух частей

51
00:02:07,890 --> 00:02:10,200
дает так называемый кодер-декодер,

52
00:02:10,200 --> 00:02:12,720
или трансформер последовательности в последовательность.

53
00:02:12,720 --> 00:02:14,280
Кодер принимает входные данные

54
00:02:14,280 --> 00:02:17,850
и вычисляет высокоуровневое представление этих входов.

55
00:02:17,850 --> 00:02:20,252
Эти выходы затем передаются в декодер.

56
00:02:20,252 --> 00:02:22,860
Декодер использует выход кодера,

57
00:02:22,860 --> 00:02:26,370
наряду с другими входными данными для создания прогноза.

58
00:02:26,370 --> 00:02:27,900
Затем он прогнозирует выход,

59
00:02:27,900 --> 00:02:30,248
который он будет повторно использовать в будущих итерациях,

60
00:02:30,248 --> 00:02:32,662
отсюда и термин "авторегрессивный".

61
00:02:32,662 --> 00:02:34,740
Наконец, чтобы получить представление

62
00:02:34,740 --> 00:02:36,690
о кодерах-декодерах в целом,

63
00:02:36,690 --> 00:02:39,670
мы рекомендуем вам ознакомиться с видео о кодерах-декодерах.

64
00:02:39,670 --> 00:02:42,420


