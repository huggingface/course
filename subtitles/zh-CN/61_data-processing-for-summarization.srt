1
00:00:00,227 --> 00:00:01,359
（空气呼啸）
(air whooshing)

2
00:00:01,359 --> 00:00:02,610
（笑脸点击）
(smiley clicking)

3
00:00:02,610 --> 00:00:05,550
（空气呼啸）
(air whooshing)

4
00:00:05,550 --> 00:00:08,450
- 让我们看看如何预处理数据集以进行文本摘要。
- Let's see how to preprocess a dataset for summarization.

5
00:00:09,750 --> 00:00:13,083
这是概括一份长文档的任务。
This is the task of, well, summarizing a long document.

6
00:00:14,040 --> 00:00:16,830
该视频将重点介绍如何预处理你的数据集
This video will focus on how to preprocess your dataset

7
00:00:16,830 --> 00:00:19,680
一旦你成功将其按照以下格式处理：
once you have managed to put it in the following format:

8
00:00:19,680 --> 00:00:21,510
用一列表示长文件，
one column for the long documents,

9
00:00:21,510 --> 00:00:23,610
和一列表示摘要。
and one for the summaries.

10
00:00:23,610 --> 00:00:24,930
这是我们
Here is how we can achieve this

11
00:00:24,930 --> 00:00:27,573
如何使用 XSUM 数据集上的 Datasets 库实现这一效果。
with the Datasets library on the XSUM dataset.

12
00:00:28,650 --> 00:00:30,810
只要你设法让你的数据看起来像这样，
As long as you manage to have your data look like this,

13
00:00:30,810 --> 00:00:33,690
你应该能够按照相同的步骤操作。
you should be able to follow the same steps.

14
00:00:33,690 --> 00:00:35,880
这一次，我们的标签不是整数
For once, our labels are not integers

15
00:00:35,880 --> 00:00:39,150
对应于某些类，但纯文本。
corresponding to some classes, but plain text.

16
00:00:39,150 --> 00:00:42,480
因此，我们需要将它们标记化，就像我们的输入一样。
We will thus need to tokenize them, like our inputs.

17
00:00:42,480 --> 00:00:43,920
虽然那里有一个小陷阱，
There is a small trap there though,

18
00:00:43,920 --> 00:00:45,360
因为我们需要标记我们的目标
as we need to tokenize our targets

19
00:00:45,360 --> 00:00:48,690
在 as_target_tokenizer 上下文管理器中。
inside the as_target_tokenizer context manager.

20
00:00:48,690 --> 00:00:51,030
这是因为我们添加的特殊标记
This is because the special tokens we add

21
00:00:51,030 --> 00:00:54,000
输入和目标可能略有不同，
might be slightly different for the inputs and the target,

22
00:00:54,000 --> 00:00:57,300
所以 tokenizer 必须知道它正在处理哪个。
so the tokenizer has to know which one it is processing.

23
00:00:57,300 --> 00:00:59,550
处理整个数据集非常容易
Processing the whole dataset is then super easy

24
00:00:59,550 --> 00:01:01,290
与地图功能。
with the map function.

25
00:01:01,290 --> 00:01:03,450
由于摘要通常要短得多
Since the summaries are usually much shorter

26
00:01:03,450 --> 00:01:05,400
比文件，你绝对应该选择
than the documents, you should definitely pick

27
00:01:05,400 --> 00:01:08,880
输入和目标的不同最大长度。
different maximum lengths for the inputs and targets.

28
00:01:08,880 --> 00:01:11,730
你可以选择在此阶段填充到最大长度
You can choose to pad at this stage to that maximum length

29
00:01:11,730 --> 00:01:14,070
通过设置 padding=max_length。
by setting padding=max_length.

30
00:01:14,070 --> 00:01:16,170
在这里，我们将向你展示如何动态填充，
Here we'll show you how to pad dynamically,

31
00:01:16,170 --> 00:01:17,620
因为它还需要一步。
as it requires one more step.

32
00:01:18,840 --> 00:01:20,910
你的输入和目标都是句子
Your inputs and targets are all sentences

33
00:01:20,910 --> 00:01:22,620
各种长度。
of various lengths.

34
00:01:22,620 --> 00:01:24,960
我们将分别填充输入和目标
We'll pad the inputs and targets separately

35
00:01:24,960 --> 00:01:27,030
作为输入和目标的最大长度
as the maximum lengths of the inputs and targets

36
00:01:27,030 --> 00:01:28,280
是完全不同的。
are completely different.

37
00:01:29,130 --> 00:01:31,170
然后，我们将输入填充到最大长度
Then, we pad the inputs to the maximum lengths

38
00:01:31,170 --> 00:01:33,813
在输入之间，对于目标也是如此。
among the inputs, and same for the target.

39
00:01:34,860 --> 00:01:36,630
我们用 pad token 填充输入，
We pad the input with the pad token,

40
00:01:36,630 --> 00:01:39,000
以及索引为 -100 的目标
and the targets with the -100 index

41
00:01:39,000 --> 00:01:40,980
确保不考虑它们
to make sure they are not taken into account

42
00:01:40,980 --> 00:01:42,180
在损失计算中。
in the loss computation.

43
00:01:43,440 --> 00:01:45,180
变形金刚库为我们提供
The Transformers library provide us

44
00:01:45,180 --> 00:01:48,510
使用数据整理器自动完成这一切。
with a data collator to do this all automatically.

45
00:01:48,510 --> 00:01:51,690
然后你可以将它与你的数据集一起传递给培训师，
You can then pass it to the Trainer with your datasets,

46
00:01:51,690 --> 00:01:55,710
或者在使用 model.fit 之前在 to_tf_dataset 方法中使用它
or use it in the to_tf_dataset method before using model.fit

47
00:01:55,710 --> 00:01:56,823
在你当前的模型上。
on your current model.

48
00:01:58,339 --> 00:02:02,876
（空气呼啸）
(air whooshing)

