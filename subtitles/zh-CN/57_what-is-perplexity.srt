1
00:00:00,095 --> 00:00:01,582
（屏幕呼啸）
(screen whooshing)

2
00:00:01,582 --> 00:00:02,659
（贴纸弹出）
(sticker popping)

3
00:00:02,659 --> 00:00:05,379
（屏幕呼啸）
(screen whooshing)

4
00:00:05,379 --> 00:00:06,720
- 在这段视频中，我们来看看
- In this video, we take a look

5
00:00:06,720 --> 00:00:09,483
在称为困惑度的神秘测深指标上。
at the mysterious sounding metric called perplexity.

6
00:00:11,070 --> 00:00:12,630
你可能遇到过困惑
You might have encountered perplexity

7
00:00:12,630 --> 00:00:14,970
在阅读生成模型时。
when reading about generative models.

8
00:00:14,970 --> 00:00:16,680
你可以在这里看到两个例子，
You can see two examples here,

9
00:00:16,680 --> 00:00:18,577
一张来自原始变压器纸，
one from the original transformer paper,

10
00:00:18,577 --> 00:00:19,950
“你需要的只是注意力，”
"Attention is all you need,"

11
00:00:19,950 --> 00:00:23,340
另一篇来自最近的 GPT-2 论文。
and the other one from the more recent GPT-2 paper.

12
00:00:23,340 --> 00:00:25,740
困惑度是衡量绩效的常用指标
Perplexity is a common metric to measure the performance

13
00:00:25,740 --> 00:00:27,150
的语言模型。
of language models.

14
00:00:27,150 --> 00:00:30,000
它的值越小，性能越好。
The smaller its value, the better the performance.

15
00:00:30,000 --> 00:00:32,950
但它究竟意味着什么，我们又该如何计算呢？
But what does it actually mean and how can we calculate it?

16
00:00:34,440 --> 00:00:36,180
机器学习中非常常见的量
A very common quantity in machine learning

17
00:00:36,180 --> 00:00:37,650
是可能性。
is the likelihood.

18
00:00:37,650 --> 00:00:39,240
我们可以计算可能性
We can calculate the likelihood

19
00:00:39,240 --> 00:00:42,390
作为每个标记概率的乘积。
as the product of each token's probability.

20
00:00:42,390 --> 00:00:44,730
这意味着对于每个令牌，
What this means is that for each token,

21
00:00:44,730 --> 00:00:47,340
我们使用语言模型来预测它的概率
we use the language model to predict its probability

22
00:00:47,340 --> 00:00:49,560
基于之前的标记。
based on the previous tokens.

23
00:00:49,560 --> 00:00:52,050
最后，我们将所有概率相乘
In the end, we multiply all probabilities

24
00:00:52,050 --> 00:00:53,253
得到的可能性。
to get the likelihood.

25
00:00:55,892 --> 00:00:57,000
有可能，
With the likelihood,

26
00:00:57,000 --> 00:00:59,340
我们可以计算另一个重要的量，
we can calculate another important quantity,

27
00:00:59,340 --> 00:01:01,200
交叉熵。
the cross-entropy.

28
00:01:01,200 --> 00:01:03,450
您可能已经听说过交叉熵
You might have already heard about cross-entropy

29
00:01:03,450 --> 00:01:05,670
在查看损失函数时。
when looking at loss function.

30
00:01:05,670 --> 00:01:09,210
它通常用作分类中的损失函数。
It is often used as a loss function in classification.

31
00:01:09,210 --> 00:01:11,610
在语言建模中，我们预测下一个标记
In language modeling, we predict the next token

32
00:01:11,610 --> 00:01:12,930
基于之前的令牌，
based on the previous token,

33
00:01:12,930 --> 00:01:15,810
这也是一个分类任务。
which is also a classification task.

34
00:01:15,810 --> 00:01:17,340
因此，如果我们想计算
Therefore, if we want to calculate

35
00:01:17,340 --> 00:01:19,290
一个例子的交叉熵，
the cross-entropy of an example,

36
00:01:19,290 --> 00:01:21,090
我们可以简单地将它传递给模型
we can simply pass it to the model

37
00:01:21,090 --> 00:01:23,580
以其输入作为标签。
with its inputs as labels.

38
00:01:23,580 --> 00:01:26,433
然后损失对应于交叉熵。
The loss then corresponds to the cross-entropy.

39
00:01:29,130 --> 00:01:31,110
我们现在只差一个手术了
We are now only a single operation away

40
00:01:31,110 --> 00:01:33,510
从计算困惑度。
from calculating the perplexity.

41
00:01:33,510 --> 00:01:37,710
通过对交叉熵取幂，我们得到了困惑。
By exponentiating the cross-entropy, we get the perplexity.

42
00:01:37,710 --> 00:01:40,260
所以你看到困惑是密切相关的
So you see that the perplexity is closely related

43
00:01:40,260 --> 00:01:41,163
到损失。
to the loss.

44
00:01:42,060 --> 00:01:43,380
插入以前的结果
Plugging in previous results

45
00:01:43,380 --> 00:01:47,010
表明这相当于求幂
shows that this is equivalent to exponentiating

46
00:01:47,010 --> 00:01:51,033
每个令牌的负平均锁定概率。
the negative average lock probability of each token.

47
00:01:52,050 --> 00:01:54,630
请记住，损失只是一个弱代理
Keep in mind that the loss is only a weak proxy

48
00:01:54,630 --> 00:01:57,360
用于模型生成高质量文本的能力
for a model's ability to generate quality text

49
00:01:57,360 --> 00:02:00,510
困惑也是如此。
and the same is true for perplexity.

50
00:02:00,510 --> 00:02:02,550
出于这个原因，人们通常也计算
For this reason, one usually also calculates

51
00:02:02,550 --> 00:02:03,840
更复杂的指标
more sophisticated metrics

52
00:02:03,840 --> 00:02:07,413
例如生成任务上的 BLEU 或 ROUGE。
such as BLEU or ROUGE on generative tasks.

53
00:02:08,551 --> 00:02:11,468
（屏幕呼啸）
(screen whooshing)

