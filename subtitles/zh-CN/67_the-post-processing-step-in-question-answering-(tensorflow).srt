1
00:00:00,367 --> 00:00:02,950
（微妙的爆炸）
(subtle blast)

2
00:00:05,850 --> 00:00:08,913
- 问答任务中的后处理步骤。
- The post-processing step in a question-answering task.

3
00:00:10,830 --> 00:00:11,790
在做答题时，
When doing question answering,

4
00:00:11,790 --> 00:00:14,670
初始数据集的处理
the processing of the initial dataset

5
00:00:14,670 --> 00:00:18,090
意味着将示例拆分为多个功能，
implies splitting examples in several features,

6
00:00:18,090 --> 00:00:20,850
其中可能包含也可能不包含答案。
which may or may not contain the answer.

7
00:00:20,850 --> 00:00:22,530
通过模型传递这些特征
Passing those features through the model

8
00:00:22,530 --> 00:00:25,860
将为我们提供开始和结束位置的 logits，
will give us logits for the start and end positions,

9
00:00:25,860 --> 00:00:28,620
因为我们的标签是令牌的索引
since our labels are the indices of the tokens

10
00:00:28,620 --> 00:00:31,020
对应于开始和结束的答案。
that correspond to the start and end the answer.

11
00:00:31,860 --> 00:00:34,740
然后我们必须以某种方式将这些 logits 转换为答案，
We must then somehow convert those logits into an answer,

12
00:00:34,740 --> 00:00:38,070
然后从每个功能给出的各种答案中选择一个
and then pick one of the various answers each feature gives

13
00:00:38,070 --> 00:00:40,473
成为给定示例的答案。
to be the answer for a given example.

14
00:00:41,683 --> 00:00:43,200
对于处理步骤，
For the processing step,

15
00:00:43,200 --> 00:00:45,450
你应该参考下面链接的视频。
you should refer to the video linked below.

16
00:00:45,450 --> 00:00:47,310
验证并没有太大的不同，
It's not very different for validation,

17
00:00:47,310 --> 00:00:50,053
我们只需要添加几行来跟踪两件事：
we just need to add a few lines to keep track of two things:

18
00:00:50,053 --> 00:00:52,620
而不是丢弃偏移映射，
instead of discarding the offset mappings,

19
00:00:52,620 --> 00:00:55,380
我们保留它们，并在其中包含信息
we keep them, and also include in them the information

20
00:00:55,380 --> 00:00:58,410
通过设置偏移量来确定上下文的位置
of where the context is by setting the offsets

21
00:00:58,410 --> 00:01:01,821
特殊标记和无的问题。
of the special tokens and the question to None.

22
00:01:01,821 --> 00:01:05,370
然后我们还跟踪每个功能的示例 ID，
Then we also keep track of the example ID for each feature,

23
00:01:05,370 --> 00:01:07,020
能够映射回特征
to be able to map back feature

24
00:01:07,020 --> 00:01:09,243
他们起源的例子。
to the examples that they originated from.

25
00:01:10,470 --> 00:01:12,660
如果你不想计算验证损失，
If you don't want to compute the validation loss,

26
00:01:12,660 --> 00:01:14,610
你不需要包含所有特殊代码
you won't need to include all the special code

27
00:01:14,610 --> 00:01:17,010
我们用来创建标签的。
that we used to create the labels.

28
00:01:17,010 --> 00:01:19,650
完成后，我们可以应用该预处理功能
With this done, we can apply that preprocessing function

29
00:01:19,650 --> 00:01:21,480
使用映射方法。
using the map method.

30
00:01:21,480 --> 00:01:23,610
我们在预处理中采用 SQUAD 数据集
We take the SQUAD dataset like in the preprocessing

31
00:01:23,610 --> 00:01:25,060
用于问答视频。
for question-answering video.

32
00:01:26,400 --> 00:01:29,310
一旦完成，下一步就是创建我们的模型。
Once this is done, the next step is to create our model.

33
00:01:29,310 --> 00:01:30,570
我们使用后面的默认模型
We use the default model behind

34
00:01:30,570 --> 00:01:32,640
这里的问答管道，
the question-answering pipeline here,

35
00:01:32,640 --> 00:01:35,880
但你应该使用你想要评估的任何模型。
but you should use any model you want to evaluate.

36
00:01:35,880 --> 00:01:37,680
使用 to_tf_dataset 方法，
With the to_tf_dataset method,

37
00:01:37,680 --> 00:01:41,370
我们可以将处理过的数据集发送到 model.predict，
we can just sent our processed dataset to model.predict,

38
00:01:41,370 --> 00:01:43,350
我们直接得到我们的开始和结束 logits
and we directly get our start and end logits

39
00:01:43,350 --> 00:01:45,930
将整个数据集作为 NumPy 数组。
for the whole dataset as NumPy arrays.

40
00:01:45,930 --> 00:01:49,230
完成后，我们就可以真正深入到后期处理中了。
With this done, we can really dive into the post-processing.

41
00:01:49,230 --> 00:01:52,380
首先，我们需要一张从示例到功能的映射，
First, we'll need a map from example to features,

42
00:01:52,380 --> 00:01:53,883
我们可以这样创建。
which we can create like this.

43
00:01:54,780 --> 00:01:56,700
现在，对于后处理的主要部分，
Now, for the main part of the post-processing,

44
00:01:56,700 --> 00:02:00,270
让我们看看如何从 logits 中提取答案。
let's see how to extract an answer from the logits.

45
00:02:00,270 --> 00:02:01,650
我们可以只取最好的索引
We could just take the best index

46
00:02:01,650 --> 00:02:03,690
对于开始和结束登录并完成，
for the start and end logits and be done,

47
00:02:03,690 --> 00:02:06,180
但如果我们的模型预测了一些不可能的事情，
but if our model predicts something impossible,

48
00:02:06,180 --> 00:02:07,920
就像问题中的标记，
like tokens in the questions,

49
00:02:07,920 --> 00:02:09,670
我们将查看更多的 logits。
we will look at more of the logits.

50
00:02:10,800 --> 00:02:12,570
请注意，在问答管道中，
Note that in the question-answering pipeline,

51
00:02:12,570 --> 00:02:14,160
我们将分数归因于每个答案
we attributed the score to each answer

52
00:02:14,160 --> 00:02:17,880
基于我们没有在这里计算的概率。
based on the probabilities, which we did not compute here.

53
00:02:17,880 --> 00:02:19,860
在 logits 方面，我们有乘法
In terms of logits, the multiplication we had

54
00:02:19,860 --> 00:02:21,663
在分数中成为加法。
in the scores becomes an addition.

55
00:02:22,650 --> 00:02:23,910
为了走得快，我们不看
To go fast, we don't look

56
00:02:23,910 --> 00:02:25,343
在所有可能的开始和结束日志中，
at all possible start and end logits,

57
00:02:25,343 --> 00:02:26,973
但最好的 20 个。
but the 20 best ones.

58
00:02:27,810 --> 00:02:30,386
我们忽略产生不可能答案的逻辑
We ignore the logits that spawn impossible answers

59
00:02:30,386 --> 00:02:32,370
或回答太长。
or answer that are too long.

60
00:02:32,370 --> 00:02:33,720
正如我们在预处理中看到的，
As we saw in the preprocessing,

61
00:02:33,720 --> 00:02:36,240
标签 “0, 0” 对应于没有答案，
the label "0, 0" correspond to no answer,

62
00:02:36,240 --> 00:02:37,440
否则我们使用偏移量
otherwise we use the offset

63
00:02:37,440 --> 00:02:39,290
在上下文中得到答案。
to get the answer inside the context.

64
00:02:40,260 --> 00:02:41,580
我们来看看预测答案
Let's have a look at the predicted answer

65
00:02:41,580 --> 00:02:43,200
对于第一个功能，
for the first feature,

66
00:02:43,200 --> 00:02:44,790
这是得分最高的答案，
which is the answer with the best score,

67
00:02:44,790 --> 00:02:46,860
或自 SoftMax 以来最好的 logit 分数
or the best logit score since the SoftMax

68
00:02:46,860 --> 00:02:48,810
是增函数。
is an increasing function.

69
00:02:48,810 --> 00:02:49,960
模型做对了。
The model got it right.

70
00:02:51,210 --> 00:02:54,180
接下来，我们只需要为每个示例循环这个，
Next, we just have to loop this for every example,

71
00:02:54,180 --> 00:02:56,700
为每个选择具有最佳 logit 分数的答案
picking for each the answer with the best logit score

72
00:02:56,700 --> 00:02:59,133
在示例生成的所有功能中。
in all the features the example generated.

73
00:03:00,030 --> 00:03:03,030
现在你知道如何从模型预测中获得答案了。
Now you know how to get answers from your model predictions.

74
00:03:04,214 --> 00:03:06,797
（微妙的爆炸）
(subtle blast)

