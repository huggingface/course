1
00:00:00,367 --> 00:00:02,950
（微妙的爆炸）
(subtle blast)

2
00:00:05,850 --> 00:00:08,913
- 问答任务中的后处理操作。
- The post-processing step in a question-answering task.

3
00:00:10,830 --> 00:00:11,790
在做问答任务时，
When doing question answering,

4
00:00:11,790 --> 00:00:14,670
初始数据集的处理
the processing of the initial dataset

5
00:00:14,670 --> 00:00:18,090
意味着以多个特征拆分示例，
implies splitting examples in several features,

6
00:00:18,090 --> 00:00:20,850
其中可能包含也可能不包含答案。
which may or may not contain the answer.

7
00:00:20,850 --> 00:00:22,530
由于我们的标签是词元的索引
Passing those features through the model

8
00:00:22,530 --> 00:00:25,860
其对应于答案的起始和结束，
will give us logits for the start and end positions,

9
00:00:25,860 --> 00:00:28,620
通过模型传递这些特征
since our labels are the indices of the tokens

10
00:00:28,620 --> 00:00:31,020
将为我们提供开始和结束位置的 logits。
that correspond to the start and end the answer.

11
00:00:31,860 --> 00:00:34,740
然后我们必须以某种方式将这些 logits 转换为答案，
We must then somehow convert those logits into an answer,

12
00:00:34,740 --> 00:00:38,070
然后从每个特征给出的各种答案中选择一个
and then pick one of the various answers each feature gives

13
00:00:38,070 --> 00:00:40,473
作为给定示例的答案。
to be the answer for a given example.

14
00:00:41,683 --> 00:00:43,200
对于处理步骤，
For the processing step,

15
00:00:43,200 --> 00:00:45,450
你可以参考下面链接的视频。
you should refer to the video linked below.

16
00:00:45,450 --> 00:00:47,310
验证方面也没有太大的变化，
It's not very different for validation,

17
00:00:47,310 --> 00:00:50,053
我们只需要添加几行来跟踪两件事：
we just need to add a few lines to keep track of two things:

18
00:00:50,053 --> 00:00:52,620
我们保留 offset mapping，
instead of discarding the offset mappings,

19
00:00:52,620 --> 00:00:55,380
而不是丢弃它们，并且通过设置特殊词元的偏移量
we keep them, and also include in them the information

20
00:00:55,380 --> 00:00:58,410
以及将问题设置为 None
of where the context is by setting the offsets

21
00:00:58,410 --> 00:01:01,821
将所保留的 offset mapping 包含在上下文的信息里。
of the special tokens and the question to None.

22
00:01:01,821 --> 00:01:05,370
然后我们还跟踪每个特征的示例 ID，
Then we also keep track of the example ID for each feature,

23
00:01:05,370 --> 00:01:07,020
能够将特征映射回
to be able to map back feature

24
00:01:07,020 --> 00:01:09,243
他们初始的例子。
to the examples that they originated from.

25
00:01:10,470 --> 00:01:12,660
如果你不想计算验证损失，
If you don't want to compute the validation loss,

26
00:01:12,660 --> 00:01:14,610
你不需要包含所有这些
you won't need to include all the special code

27
00:01:14,610 --> 00:01:17,010
用来创建标签的特殊代码。
that we used to create the labels.

28
00:01:17,010 --> 00:01:19,650
完成后，我们可以调用 map 方法
With this done, we can apply that preprocessing function

29
00:01:19,650 --> 00:01:21,480
应用该预处理功能。
using the map method.

30
00:01:21,480 --> 00:01:23,610
我们采用 SQUAD 数据集就和问答视频的预处理
We take the SQUAD dataset like in the preprocessing

31
00:01:23,610 --> 00:01:25,060
中所用到的一样。
for question-answering video.

32
00:01:26,400 --> 00:01:29,310
一旦完成，下一步就是创建我们的模型。
Once this is done, the next step is to create our model.

33
00:01:29,310 --> 00:01:30,570
我们在这里的问答管道之后
We use the default model behind

34
00:01:30,570 --> 00:01:32,640
使用默认模型，
the question-answering pipeline here,

35
00:01:32,640 --> 00:01:35,880
不过你应该使用你想要评估的任何模型。
but you should use any model you want to evaluate.

36
00:01:35,880 --> 00:01:37,680
通过 to_tf_dataset 方法，
With the to_tf_dataset method,

37
00:01:37,680 --> 00:01:41,370
我们可以将处理过的数据集发送到 model.predict，
we can just sent our processed dataset to model.predict,

38
00:01:41,370 --> 00:01:43,350
我们直接得到我们的开始和结束的 logits
and we directly get our start and end logits

39
00:01:43,350 --> 00:01:45,930
将整个数据集作为 NumPy 数组。
for the whole dataset as NumPy arrays.

40
00:01:45,930 --> 00:01:49,230
完成后，我们就可以真正深入到后期处理中了。
With this done, we can really dive into the post-processing.

41
00:01:49,230 --> 00:01:52,380
首先，我们需要一张从示例到功能的映射，
First, we'll need a map from example to features,

42
00:01:52,380 --> 00:01:53,883
我们像这样创建。
which we can create like this.

43
00:01:54,780 --> 00:01:56,700
现在，对于后处理的主要部分，
Now, for the main part of the post-processing,

44
00:01:56,700 --> 00:02:00,270
让我们看看如何从 logits 中提取答案。
let's see how to extract an answer from the logits.

45
00:02:00,270 --> 00:02:01,650
我们可以针对开始和结束 logits
We could just take the best index

46
00:02:01,650 --> 00:02:03,690
只取最好的索引，
for the start and end logits and be done,

47
00:02:03,690 --> 00:02:06,180
但如果我们的模型预测得出了不可思议的结果，
but if our model predicts something impossible,

48
00:02:06,180 --> 00:02:07,920
就像问题中的词元，
like tokens in the questions,

49
00:02:07,920 --> 00:02:09,670
我们将查看更多的 logits。
we will look at more of the logits.

50
00:02:10,800 --> 00:02:12,570
请注意，在问答管道中，
Note that in the question-answering pipeline,

51
00:02:12,570 --> 00:02:14,160
我们基于概率将分数
we attributed the score to each answer

52
00:02:14,160 --> 00:02:17,880
归因于每个答案，在这里并没有计算。
based on the probabilities, which we did not compute here.

53
00:02:17,880 --> 00:02:19,860
就 logits 而言，我们在分数中的乘法
In terms of logits, the multiplication we had

54
00:02:19,860 --> 00:02:21,663
变为加法
in the scores becomes an addition.

55
00:02:22,650 --> 00:02:23,910
要快速完成，
To go fast, we don't look

56
00:02:23,910 --> 00:02:25,343
我们不会查看所有可能的开始和结束的 logits，
at all possible start and end logits,

57
00:02:25,343 --> 00:02:26,973
仅需最好的 20 个就足够了。
but the 20 best ones.

58
00:02:27,810 --> 00:02:30,386
我们忽略产生不可能答案和答案太长
We ignore the logits that spawn impossible answers

59
00:02:30,386 --> 00:02:32,370
的 logits
or answer that are too long.

60
00:02:32,370 --> 00:02:33,720
正如我们在预处理中看到的，
As we saw in the preprocessing,

61
00:02:33,720 --> 00:02:36,240
标签 “0, 0” 对应无答案，
the label "0, 0" correspond to no answer,

62
00:02:36,240 --> 00:02:37,440
否则我们使用偏移量
otherwise we use the offset

63
00:02:37,440 --> 00:02:39,290
在上下文中得到答案。
to get the answer inside the context.

64
00:02:40,260 --> 00:02:41,580
我们来看看对于第一个特征
Let's have a look at the predicted answer

65
00:02:41,580 --> 00:02:43,200
预测的答案，
for the first feature,

66
00:02:43,200 --> 00:02:44,790
这是得分最高的答案，
which is the answer with the best score,

67
00:02:44,790 --> 00:02:46,860
或最好的 logit 分数
or the best logit score since the SoftMax

68
00:02:46,860 --> 00:02:48,810
因为 SoftMax 是增函数。
is an increasing function.

69
00:02:48,810 --> 00:02:49,960
模型做对了。
The model got it right.

70
00:02:51,210 --> 00:02:54,180
接下来，我们只需要为每个例子循环这个，
Next, we just have to loop this for every example,

71
00:02:54,180 --> 00:02:56,700
在示例生成的所有特征中
picking for each the answer with the best logit score

72
00:02:56,700 --> 00:02:59,133
选择具有最佳 logit 分数的答案。
in all the features the example generated.

73
00:03:00,030 --> 00:03:03,030
现在你知道如何从模型预测中获得答案了。
Now you know how to get answers from your model predictions.

74
00:03:04,214 --> 00:03:06,797
（微妙的爆炸）
(subtle blast)

