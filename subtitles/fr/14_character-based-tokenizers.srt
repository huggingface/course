1
00:00:04,160 --> 00:00:09,440
Avant de plonger dans la tokenisation basée sur les caractères, pour comprendre pourquoi ce type de tokenisation

2
00:00:09,440 --> 00:00:13,680
est intéressant, il faut comprendre les défauts de la tokenisation basée sur les mots.

3
00:00:14,560 --> 00:00:18,400
Si vous n'avez pas vu la première vidéo sur la tokenisation basée sur des mots, nous vous recommandons de la

4
00:00:18,400 --> 00:00:23,920
regarder avant de regarder cette vidéo. Examinons la tokenisation basée sur les caractères.

5
00:00:25,440 --> 00:00:29,840
Nous divisons maintenant notre texte en caractères individuels plutôt qu'en mots.

6
00:00:32,720 --> 00:00:37,200
Il y a généralement beaucoup de mots différents dans les langues, tandis que le nombre de caractères reste

7
00:00:37,200 --> 00:00:45,520
faible. Commençons avec la langue anglaise qui compte environ 170 000 mots différents.

8
00:00:45,520 --> 00:00:48,960
Nous aurions besoin d'un très grand vocabulaire pour englober tous les mots.

9
00:00:50,080 --> 00:00:59,600
Avec un vocabulaire basé sur les caractères, nous pouvons nous débrouiller avec seulement 256 caractères incluant les lettres, les chiffres et caractères spéciaux.

10
00:00:59,600 --> 00:01:04,880
Même les langues contenant de nombreux caractères différents, comme les langues chinoises, ont des dictionnaires contenant

11
00:01:06,160 --> 00:01:14,160
environ 20 000 caractères différents, mais plus de 375 000 mots différents. Les vocabulaires

12
00:01:14,160 --> 00:01:20,240
basés sur des caractères nous permettent d'utiliser moins de tokens différents que les dictionnaires de tokenisation basés sur des mots que nous utiliserions autrement.

13
00:01:23,040 --> 00:01:28,000
Ces vocabulaires sont également plus complets que leurs homologues basés sur des mots.

14
00:01:28,720 --> 00:01:34,160
Étant donné que notre vocabulaire contient tous les caractères utilisés dans une langue, même les mots non vus lors de

15
00:01:34,160 --> 00:01:39,840
l'entraînement du tokenizer peuvent toujours être tokenisés, de sorte que les tokens hors vocabulaire seront moins fréquents.

16
00:01:40,480 --> 00:01:45,200
Cela inclut la possibilité de tokeniser correctement les mots mal orthographiés, plutôt que de les rejeter immédiatement comme

17
00:01:45,200 --> 00:01:53,600
inconnus. Cependant, cet algorithme n'est pas parfait non plus ! Intuitivement, les caractères

18
00:01:53,600 --> 00:01:59,760
ne contiennent pas autant d'informations individuellement qu'un mot en contiendrait. Par exemple, « Let's » contient

19
00:01:59,760 --> 00:02:07,040
plus d'informations que sa première lettre « L ». Bien sûr, cela n'est pas vrai pour toutes les langues, car certaines langues comme les

20
00:02:07,040 --> 00:02:11,280
langues basées sur des idéogrammes contiennent beaucoup d'informations contenues dans des caractères uniques.

21
00:02:12,480 --> 00:02:17,200
Mais pour d'autres, comme les langues romanes, le modèle devra donner un sens à plusieurs

22
00:02:17,200 --> 00:02:25,120
tokens à la fois le temps d'obtenir l'information contenue dans un seul mot. Cela conduit à un autre problème avec les

23
00:02:25,120 --> 00:02:30,320
tokenizers basés sur des caractères : leurs séquences sont traduites en une très grande quantité de tokens à

24
00:02:30,320 --> 00:02:37,680
traiter par le modèle. Cela peut avoir un impact sur la taille du contexte que le modèle

25
00:02:37,680 --> 00:02:45,120
peut traiter et réduira la taille du texte que nous pouvons utiliser comme entrée pour notre modèle. Cette tokenisation,

26
00:02:45,120 --> 00:02:49,920
bien qu'elle présente quelques problèmes, a donné de très bons résultats dans le passé et doit être prise en compte lors de l'

27
00:02:49,920 --> 00:03:00,720
approche d'un nouveau problème car elle résout certains problèmes rencontrés dans l'algorithme basé sur les mots.