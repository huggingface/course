1
00:00:05,520 --> 00:00:10,720
Association de la mémoire et streaming. Dans cette vidéo, nous allons examiner deux fonctionnalités principales de la bibliothèque Datasets

2
00:00:10,720 --> 00:00:15,920
qui vous permettent de charger et de traiter d'énormes jeux de données sans faire exploser le processeur de votre ordinateur portable.

3
00:00:18,160 --> 00:00:22,720
De nos jours, il n'est pas rare de travailler avec des jeux de données de plusieurs Go, en

4
00:00:22,720 --> 00:00:26,880
particulier si vous envisagez de pré-entraîner un transformer comme BERT ou GPT-2 à partir de zéro.

5
00:00:27,920 --> 00:00:30,480
Dans ces cas, même le chargement des données peut être un défi.

6
00:00:31,040 --> 00:00:36,560
Par exemple, le corpus C4 utilisé pour pré-entraîner T5 se compose de plus de 2 téraoctets de données !

7
00:00:38,160 --> 00:00:42,720
Pour gérer ces grands jeux de données, la bibliothèque Datasets repose sur deux fonctionnalités principales :

8
00:00:42,720 --> 00:00:45,120
le format Apache Arrow et une API de streaming.

9
00:00:46,160 --> 00:00:51,120
Arrow est conçu pour le traitement de données hautes performances et représente chaque jeu

10
00:00:51,120 --> 00:00:56,240
de données sous forme de tableau avec un format de colonne en mémoire. Comme vous pouvez le voir dans cet exemple, les formats en colonnes regroupent

11
00:00:56,240 --> 00:01:01,280
les éléments d'une table dans des blocs consécutifs de RAM, ce qui permet un accès et un traitement rapides.

12
00:01:02,560 --> 00:01:07,600
Arrow est idéal pour traiter des données à n'importe quelle échelle, mais certains jeux de données sont si volumineux que vous ne pouvez même

13
00:01:07,600 --> 00:01:12,480
pas les faire tenir sur votre disque dur. Dans ces cas, la bibliothèque Datasets fournit une API de streaming

14
00:01:13,040 --> 00:01:18,080
qui vous permet de télécharger progressivement les données brutes un élément à la fois. Le résultat est

15
00:01:18,080 --> 00:01:21,600
un objet spécial appelé `IterableDataset` que nous verrons plus en détail bientôt.

16
00:01:23,520 --> 00:01:28,160
Commençons par examiner pourquoi Arrow est si puissant. La première fonctionnalité est qu'il traite chaque

17
00:01:28,160 --> 00:01:34,000
jeu de données comme un fichier associé en mémoire. L'association de la mémoire est un mécanisme qui associe une partie d'un fichier ou

18
00:01:34,000 --> 00:01:38,967
un fichier entier sur le disque à un morceau de mémoire virtuelle. Cela permet aux applications

19
00:01:38,967 --> 00:01:43,360
d'accéder à des segments d'un fichier extrêmement volumineux sans avoir à lire d'abord l'intégralité du fichier en mémoire.

20
00:01:44,960 --> 00:01:49,040
Une autre fonctionnalité intéressante de la capacité d'association de la mémoire d'Arrow est qu'elle permet à plusieurs

21
00:01:49,040 --> 00:01:53,840
processus de travailler avec le même grand jeu de données sans le déplacer ni le copier de quelque manière que ce soit.

22
00:01:55,520 --> 00:01:59,920
Cette fonctionnalité « zéro copie » d'Arrow rend l'itération extrêmement rapide sur un jeu de données.

23
00:02:00,480 --> 00:02:05,920
Dans cet exemple, vous pouvez voir que nous parcourons plus de 15 millions de lignes en une minute environ à l'aide d'un

24
00:02:05,920 --> 00:02:12,480
ordinateur portable standard. Ce n'est pas si mal du tout ! Voyons maintenant comment streamer un grand jeu de données.

25
00:02:12,480 --> 00:02:16,720
La seule modification que vous devez apporter est de définir l'argument `streaming=True` dans la fonction `load_dataset()`.  

26
00:02:16,720 --> 00:02:21,120
Cela renverra un objet `IterableDataset` spécial, qui est un peu

27
00:02:21,120 --> 00:02:26,160
différent des objets Dataset que nous avons vus dans d'autres vidéos. Cet objet est un itérable, ce qui signifie que

28
00:02:26,160 --> 00:02:31,680
nous ne pouvons pas l'indexer pour accéder aux éléments, mais plutôt itérer dessus à l'aide des méthodes `iter` et `next`.

29
00:02:32,640 --> 00:02:36,080
Cela téléchargera et accédera à un exemple unique à partir de le jeu de données, ce qui signifie que

30
00:02:36,080 --> 00:02:39,760
vous pouvez parcourir progressivement un énorme jeu de données sans avoir à le télécharger au préalable.

31
00:02:41,840 --> 00:02:47,040
La tokenisation de texte avec la méthode `map()` fonctionne également de la même manière. Nous streamons d'abord le jeu de données,

32
00:02:47,040 --> 00:02:52,480
puis appliquons la méthode `map()` avec le tokenizer. Pour obtenir le premier exemple tokenisé, nous appliquons iter et

33
00:02:52,480 --> 00:02:58,560
next. La principale différence avec un `IterableDataset est` qu'au lieu d'utiliser la méthode `select()` pour

34
00:02:58,560 --> 00:03:04,240
renvoyer l'exemple, nous utilisons les méthodes `take()` et skip()` car nous ne pouvons pas indexer dans le jeu de données.

35
00:03:04,240 --> 00:03:10,320
La méthode `take()` renvoie les N premiers exemples du jeu de données, tandis que `skip()` ignore les N premiers

36
00:03:10,320 --> 00:03:15,680
et renvoie le reste. Vous pouvez voir des exemples des deux en action ici, où nous créons

37
00:03:15,680 --> 00:03:27,040
un jeu de validation à partir des 1 000 premiers exemples, puis nous les ignorons pour créer le jeu d'entrapinement.