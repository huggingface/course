1
00:00:05,540 --> 00:00:07,870
Comment instancier un transformer ?

2
00:00:07,870 --> 00:00:14,800
Dans cette vidéo, nous verrons comment créer et utiliser un modèle de la bibliothèque Transformers.

3
00:00:14,800 --> 00:00:20,130
Comme nous l'avons vu précédemment, la classe TFAutoModel vous permet d'instancier un modèle pré-entraîné à

4
00:00:20,130 --> 00:00:23,490
partir de n'importe quel checkpoint sur le Hub d'Hugging Face.

5
00:00:23,490 --> 00:00:27,740
Elle choisira la bonne classe de modèle dans la bibliothèque pour instancier l'architecture appropriée

6
00:00:27,740 --> 00:00:31,310
et charger les poids du modèle pré-entraîné à l'intérieur.

7
00:00:31,310 --> 00:00:36,630
Comme nous pouvons le voir, lorsqu'on nous donne un checkpoint BERT, nous nous retrouvons avec un TFBertModel, et de même

8
00:00:36,630 --> 00:00:39,890
pour GPT-2 ou BART.

9
00:00:39,890 --> 00:00:44,489
En coulisse, cette API peut prendre le nom d'un checkpoint sur le Hub, auquel cas

10
00:00:44,489 --> 00:00:49,649
elle téléchargera et mettra en cache le fichier de configuration ainsi que le fichier de poids du modèle.

11
00:00:49,649 --> 00:00:54,059
Vous pouvez également spécifier le chemin d'accès à un dossier local qui contient un fichier de configuration valide et

12
00:00:54,059 --> 00:00:56,739
un fichier de poids de modèle.

13
00:00:56,739 --> 00:01:02,480
Pour instancier le modèle pré-entraîné, l'API AutoModel ouvre d'abord le fichier de configuration

14
00:01:02,480 --> 00:01:06,409
pour examiner la classe de configuration à utiliser.

15
00:01:06,409 --> 00:01:13,509
La classe de configuration dépend du type de modèle (BERT, GPT-2 ou BART par exemple).

16
00:01:13,509 --> 00:01:18,130
Une fois qu'il a la classe de configuration appropriée, il peut instancier cette configuration, qui

17
00:01:18,130 --> 00:01:20,420
est un plan pour savoir comment créer le modèle.

18
00:01:20,420 --> 00:01:25,420
Il utilise également cette classe de configuration pour trouver la classe de modèle appropriée, qui est combinée

19
00:01:25,420 --> 00:01:28,470
avec la configuration chargée, pour charger le modèle.

20
00:01:28,470 --> 00:01:34,759
Ce modèle n'est pas encore notre modèle pré-entraîné car il vient d'être initialisé avec des poids aléatoires.

21
00:01:34,759 --> 00:01:40,299
La dernière étape consiste à charger les poids du fichier de modèle à l'intérieur de ce modèle.

22
00:01:40,299 --> 00:01:44,659
Pour charger facilement la configuration d'un modèle à partir de n'importe quel checkpoint ou d'un dossier contenant

23
00:01:44,659 --> 00:01:48,100
le dossier de configuration, nous pouvons utiliser la classe AutoConfig.

24
00:01:48,100 --> 00:01:54,270
Comme la classe TFAutoModel, elle choisira la bonne classe de configuration dans la bibliothèque.

25
00:01:54,270 --> 00:01:58,869
Nous pouvons également utiliser la classe spécifique correspondant à un checkpoint, mais nous devrons changer

26
00:01:58,869 --> 00:02:03,280
le code chaque fois que nous voudrons essayer un modèle différent.

27
00:02:03,280 --> 00:02:07,490
Comme nous l'avons dit précédemment, la configuration d'un modèle est un plan qui contient toutes les

28
00:02:07,490 --> 00:02:11,190
informations nécessaires pour créer l'architecture du modèle.

29
00:02:11,190 --> 00:02:14,629
Par exemple, le modèle BERT associé au checkpoint basé sur bert a 12 couches,

30
00:02:14,629 --> 00:02:21,790
une taille cachée de 768 et une taille de vocabulaire de 28 996.

31
00:02:21,790 --> 00:02:28,959
Une fois que nous avons la configuration, nous pouvons créer un modèle qui a la même architecture que

32
00:02:28,959 --> 00:02:31,420
notre checkpoint mais qui est initialisé de manière aléatoire.

33
00:02:31,420 --> 00:02:36,080
Nous pouvons ensuite l'entraîner à partir de zéro comme n'importe quel module PyTorch/modèle TensorFlow.

34
00:02:36,080 --> 00:02:40,870
Nous pouvons également modifier n'importe quelle partie de la configuration en utilisant des arguments de mots clés.

35
00:02:40,870 --> 00:02:48,379
Le deuxième extrait de code instancie un modèle BERT initialisé de manière aléatoire avec dix couches au lieu de 12.

36
00:02:48,379 --> 00:02:53,019
Enregistrer un modèle une fois qu'il est entraîné ou finetuné est très simple : il suffit d'utiliser la

37
00:02:53,019 --> 00:02:54,019
méthode `save_pretrained`.

38
00:02:54,019 --> 00:03:00,510
Ici, le modèle sera enregistré dans un dossier nommé `my-bert-model` dans le répertoire de travail actuel.

39
00:03:00,510 --> 00:03:13,120
Un tel modèle peut ensuite être rechargé à l'aide de la méthode `from_pretrained`. Pour apprendre comment envoyer ce modèle sur le Hub, regardez la vidéo sur l'API `push_to_hub`.