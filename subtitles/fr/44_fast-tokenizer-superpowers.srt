1
00:00:05,109 --> 00:00:10,089
Les tokenizers rapides de la bibliothèque Transformers sont rapides, mais ils implémentent également des fonctionnalités

2
00:00:10,089 --> 00:00:14,610
qui seront très utiles pour le pré-traitement et le post-traitement des données.

3
00:00:14,610 --> 00:00:16,750
Jetons y un coup d'œil !

4
00:00:16,750 --> 00:00:21,759
Voyons d'abord la sortie habituelle d'un tokenizer.

5
00:00:21,759 --> 00:00:28,039
Nous obtenons des `input_ids` qui correspondent à des tokens, mais nous perdons beaucoup d'informations au cours du processus.

6
00:00:28,039 --> 00:00:33,270
Par exemple, ici la tokenisation est la même pour les deux phrases, même si l'une a

7
00:00:33,270 --> 00:00:36,510
plusieurs espaces de plus que l'autre.

8
00:00:36,510 --> 00:00:41,090
Le simple fait d'avoir les identifiants d'entrée n'est donc pas suffisant si nous voulons faire correspondre certains tokens avec une étendue

9
00:00:41,090 --> 00:00:46,610
de texte (ce que nous devrons faire lorsque nous aborderons la question-réponse par exemple).

10
00:00:46,610 --> 00:00:51,699
Il est également difficile de savoir quand deux tokens appartiennent ou non au même mot : cela semble facile

11
00:00:51,699 --> 00:00:57,250
quand on regarde simplement la sortie d'un tokenizer BERT, il suffit de chercher le ##. Mais 

12
00:00:57,250 --> 00:01:01,490
d'autres tokenizers ont différentes façons de tokeniser des parties de mots.

13
00:01:01,490 --> 00:01:06,910
Par exemple, RoBERTa ajoute ce symbole spécial G pour marquer les tokens au début d'un mot,

14
00:01:06,910 --> 00:01:12,160
et T5 utilise ce symbole spécial de soulignement dans le même but.

15
00:01:12,160 --> 00:01:16,759
Heureusement, les tokenizers rapides gardent une trace du mot d'où provient chaque token, avec une

16
00:01:16,759 --> 00:01:20,090
méthode `word_ids` que vous pouvez utiliser sur leurs sorties.

17
00:01:20,090 --> 00:01:24,799
La sortie n'est pas nécessairement claire, mais assemblées dans un joli tableau comme celui-ci, nous pouvons

18
00:01:24,799 --> 00:01:28,119
regarder la position du mot pour chaque token.

19
00:01:28,119 --> 00:01:32,659
Mieux encore, les tokenisers rapides gardent une trace de l'étendue des caractères d'où provient chaque token,

20
00:01:32,659 --> 00:01:38,780
et nous pouvons les obtenir en l'appelant sur un (ou plusieurs) texte en ajoutant l'argument `return_offsets_mapping=True`

21
00:01:38,780 --> 00:01:39,780
.

22
00:01:39,780 --> 00:01:46,469
Dans ce cas, nous pouvons voir comment nous sautons des positions entre le token `##s` et le super token, en

23
00:01:46,469 --> 00:01:50,579
raison des multiples espaces dans la phrase initiale.

24
00:01:50,579 --> 00:01:54,470
Pour ce faire, les tokenizers rapides stockent des informations supplémentaires à chaque étape de leur

25
00:01:54,470 --> 00:01:55,470
pipeline interne.

26
00:01:55,470 --> 00:02:00,899
Ce pipeline interne consiste en une normalisation, où nous appliquons un peu de nettoyage au texte,

27
00:02:00,899 --> 00:02:05,600
comme la mise en minuscules ou la suppression des accents. La pré-tokénisation, où nous divisons

28
00:02:05,600 --> 00:02:09,940
les textes en mots puis nous appliquons le modèle du tokenizer, c'est là que les

29
00:02:09,940 --> 00:02:15,300
mots sont divisés en tokens avant de faire finalement le post-traitement, où des tokens spéciaux

30
00:02:15,300 --> 00:02:17,110
sont ajoutés.

31
00:02:17,110 --> 00:02:20,730
Du début à la fin du pipeline, le tokenizer garde une trace de chaque étendue de

32
00:02:20,730 --> 00:02:23,680
texte qui correspond à chaque mot, puis à chaque token.

33
00:02:23,680 --> 00:02:29,099
Nous verrons à quel point cela est utile lorsque nous aborderons les tâches suivantes : lors de la

34
00:02:29,099 --> 00:02:34,360
modélisation d'un langage masqué, une variante qui obtient des résultats de pointe consiste à masquer tous les tokens d'un

35
00:02:34,360 --> 00:02:37,600
mot donné au lieu de tokens choisis au hasard.

36
00:02:37,600 --> 00:02:40,909
Cela nous obligera à utiliser les identifiants de mots que nous avons vus.

37
00:02:40,909 --> 00:02:45,209
Lors de la classification des tokens, nous devrons convertir les étiquettes que nous avons sur les mots en

38
00:02:45,209 --> 00:02:47,230
étiquettes sur chaque token.

39
00:02:47,230 --> 00:02:51,360
Quant aux mappages de décalage, ils seront très utiles lorsque nous aurons besoin de convertir des positions de token

40
00:02:51,360 --> 00:02:56,330
dans une phrase en une étendue de texte, ce que nous aurons besoin de savoir lorsque nous examinerons la réponse à une

41
00:02:56,330 --> 00:03:01,200
question ou lors du regroupement des tokens correspondant à la même entité dans la classification de token.

42
00:03:01,200 --> 00:03:09,730
Pour jeter un œil à ces tâches, consultez les vidéos en description !