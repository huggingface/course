1
00:00:04,660 --> 00:00:07,589
Bienvenue dans la série d'Hugging Face sur les tâches ! 

2
00:00:07,589 --> 00:00:13,730
Dans cette vidéo, nous allons jeter un œil à la modélisation du langage masqué.

3
00:00:13,730 --> 00:00:20,720
La modélisation du langage masqué consiste à prédire quels mots doivent remplir les blancs d'une

4
00:00:20,720 --> 00:00:23,500
phrase.

5
00:00:23,500 --> 00:00:32,870
Ces modèles prennent un texte masqué en entrée et génèrent les valeurs possibles pour ce masque.

6
00:00:32,870 --> 00:00:37,550
La modélisation en langage masqué est pratique avant de finetuner votre modèle pour votre tâche.

7
00:00:37,550 --> 00:00:43,579
Par exemple, si vous devez utiliser un modèle dans un domaine spécifique, par exemple des documents biomédicaux, des

8
00:00:43,579 --> 00:00:49,050
modèles comme BERT traiteront vos mots spécifiques à un domaine comme des tokens rares.

9
00:00:49,050 --> 00:00:54,220
Si vous entraînez un modèle de langage masqué à l'aide de votre corpus biomédical, puis finetunez

10
00:00:54,220 --> 00:01:02,929
votre modèle sur une tâche en aval, vous obtiendrez de meilleures performances.

11
00:01:02,929 --> 00:01:07,799
Les métriques de classification ne peuvent pas être utilisées car il n'y a pas de réponse correcte unique aux

12
00:01:07,799 --> 00:01:08,799
valeurs du masque.

13
00:01:08,799 --> 00:01:12,900
Au lieu de cela, nous évaluons la distribution des valeurs du masque.

14
00:01:12,900 --> 00:01:16,590
Une métrique courante pour ce faire est la perte d'entropie croisée.

15
00:01:16,590 --> 00:01:22,010
La perplexité est aussi une métrique largement utilisée et elle est calculée comme l'exponentielle de la

16
00:01:22,010 --> 00:01:27,240
perte d'entropie croisée.

17
00:01:27,240 --> 00:01:35,680
Vous pouvez utiliser n'importe quel jeu de données avec du texte brut et tokeniser le texte pour masquer les données.

18
00:01:35,680 --> 00:01:44,710
Pour plus d'informations sur la modélisation du langage masqué, consultez le cours d'Hugging Face.
