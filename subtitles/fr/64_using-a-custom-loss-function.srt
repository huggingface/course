1
00:00:05,440 --> 00:00:09,040
Dans cette vidéo, nous examinons la configuration d'une fonction de perte personnalisée pour l'entraînement.

2
00:00:10,800 --> 00:00:14,800
Dans les fonctions de perte par défaut, tous les échantillons tels que ces extraits de code

3
00:00:14,800 --> 00:00:19,040
sont traités de la même manière, quel que soit leur contenu. Mais il existe des scénarios dans lesquels il

4
00:00:19,040 --> 00:00:22,880
peut être judicieux de pondérer les échantillons différemment. Si, par exemple, un échantillon

5
00:00:22,880 --> 00:00:28,800
contient un grand nombre de tokens qui nous intéressent ou s'il présente une diversité favorable de tokens.

6
00:00:29,680 --> 00:00:33,520
Nous pouvons également penser à d'autres heuristiques que nous pouvons implémenter avec l'appariement de patrons ou d'autres règles.

7
00:00:36,080 --> 00:00:40,400
Pour chaque échantillon, nous obtenons une valeur de perte pendant l'entraînement et nous pouvons combiner cette perte avec

8
00:00:40,400 --> 00:00:47,200
un poids. Ensuite, nous pouvons créer une somme pondérée pour obtenir la perte finale d'un batch.

9
00:00:48,480 --> 00:00:53,280
Examinons un exemple spécifique : nous voulons configurer un modèle de langage qui nous aide à

10
00:00:53,280 --> 00:01:00,800
compléter automatiquement du code commun en science des données. Pour cette tâche, nous aimerions pondérer

11
00:01:00,800 --> 00:01:06,960
plus fortement les échantillons lorsque les tokens liés à la pile de science des données, tels que pandas ou numpy, se produisent plus fréquemment.

12
00:01:10,000 --> 00:01:14,788
Ici, vous voyez une fonction de perte qui fait exactement cela pour la modélisation du langage causal.

13
00:01:14,788 --> 00:01:22,800
Elle prend les entrées du modèle et les logits prédits ainsi que les tokens clés comme entrée.

14
00:01:22,800 --> 00:01:30,320
Les entrées et les logits sont d'abord alignés, puis la perte par échantillon est calculée, suivie des poids.

15
00:01:32,320 --> 00:01:35,280
Enfin, la perte et les poids sont combinés et renvoyés.

16
00:01:36,320 --> 00:01:40,480
Il s'agit d'une fonction assez importante, alors examinons de plus près les blocs de perte et de poids.

17
00:01:43,200 --> 00:01:47,920
Lors du calcul de la perte standard, les logits et les étiquettes sont aplatis sur le batch.

18
00:01:48,720 --> 00:01:53,280
Avec `view`, nous désaplatissons le tenseur pour obtenir une matrice avec une ligne pour chaque

19
00:01:53,280 --> 00:01:57,280
échantillon du batch et une colonne pour chaque position dans la séquence des échantillons.

20
00:01:58,720 --> 00:02:03,600
Nous n'avons pas besoin de la perte par position, nous faisons donc la moyenne de la perte sur toutes les positions pour chaque échantillon.

21
00:02:06,000 --> 00:02:10,960
Pour les poids, nous utilisons la logique booléenne pour obtenir un tenseur avec des 1 là où un mot-clé

22
00:02:10,960 --> 00:02:17,840
apparaît et des 0 là où ce n'est pas le cas. Ce tenseur a une dimension supplémentaire en tant que tenseur de perte que nous venons de voir car

23
00:02:17,840 --> 00:02:24,480
nous obtenons les informations pour chaque mot clé dans une matrice distincte. Nous voulons seulement savoir combien de

24
00:02:24,480 --> 00:02:30,320
fois les mots clés sont apparus par échantillon afin que nous puissions additionner tous les mots clés et toutes les positions par échantillon.

25
00:02:33,280 --> 00:02:39,760
Maintenant nous y sommes presque, nous n'avons qu'à combiner la perte avec le poids par échantillon. Nous faisons cela

26
00:02:39,760 --> 00:02:43,920
avec une multiplication par élément, puis une moyenne sur tous les échantillons du batch.

27
00:02:44,720 --> 00:02:48,000
Au final, nous avons exactement une valeur de perte pour l'ensemble du batch.

28
00:02:48,880 --> 00:02:52,800
Et c'est toute la logique nécessaire pour créer une perte de poids personnalisée.

29
00:02:56,080 --> 00:03:02,640
Voyons comment nous pouvons utiliser cette perte personnalisée avec Accelerate et le Trainer. Dans Accelerate, nous

30
00:03:02,640 --> 00:03:07,680
donnons simplement les `input_ids` au modèle pour obtenir les logits et pouvons ensuite appeler la fonction de perte personnalisée.

31
00:03:08,800 --> 00:03:12,800
Après cela, nous continuons avec la boucle d'entraînement normale en appelant par exemple `backward`.

32
00:03:13,840 --> 00:03:19,200
Pour le Trainer, nous pouvons écraser le calcul de la fonction de perte du Trainer standard. Nous

33
00:03:19,200 --> 00:03:23,360
devons simplement nous assurer que nous renvoyons la perte et les sorties du modèle dans le même format.

34
00:03:24,240 --> 00:03:31,840
Avec cela, vous pouvez intégrer votre propre fonction de perte impressionnante avec Trainer et Accelerate.