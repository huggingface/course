1
00:00:05,670 --> 00:00:09,630
Voyons comment prétraiter un jeu de données pour la traduction.

2
00:00:09,630 --> 00:00:13,269
C'est la tâche de bien traduire une phrase dans une autre langue.

3
00:00:13,269 --> 00:00:18,110
Cette vidéo se concentrera sur la façon de prétraiter votre jeu de données une fois que vous avez réussi à le

4
00:00:18,110 --> 00:00:23,090
mettre au format suivant : une colonne pour les textes d'entrée et une pour les textes cibles.

5
00:00:23,090 --> 00:00:28,439
Voici comment nous pouvons y parvenir avec la bibliothèque Datasets sur le jeu de données KDE4 pour

6
00:00:28,439 --> 00:00:30,960
la traduction de l'anglais vers le français.

7
00:00:30,960 --> 00:00:35,360
Tant que vous parvenez à faire ressembler vos données à ceci, vous devriez pouvoir suivre les

8
00:00:35,360 --> 00:00:36,769
mêmes étapes.

9
00:00:36,769 --> 00:00:41,550
Pour une fois, nos étiquettes ne sont pas des entiers correspondant à certaines classes, mais du texte brut.

10
00:00:41,550 --> 00:00:44,760
Nous devrons donc les tokeniser, comme nos entrées.

11
00:00:44,760 --> 00:00:50,820
Il y a cependant un piège, car si vous tokenisez vos cibles comme vos entrées, vous rencontrerez

12
00:00:50,820 --> 00:00:51,820
un problème.

13
00:00:51,820 --> 00:00:55,829
Même si vous ne parlez pas français, vous remarquerez peut-être des choses bizarres dans la tokenisation

14
00:00:55,829 --> 00:01:01,800
des cibles : la plupart des mots sont tokenisés en plusieurs sous-tokens, tandis que « fish », l'un

15
00:01:01,800 --> 00:01:05,799
des seuls mots anglais, est tokenisé en un seul mot.

16
00:01:05,799 --> 00:01:09,760
C'est parce que nos entrées ont été tokenisées en anglais.

17
00:01:09,760 --> 00:01:13,939
Comme notre modèle connaît deux langues, il faut le prévenir lors de la tokenisation des cibles,

18
00:01:13,939 --> 00:01:16,360
il bascule donc en mode français.

19
00:01:16,360 --> 00:01:20,090
Cela se fait avec le gestionnaire de contexte `as_target_tokenizer`.

20
00:01:20,090 --> 00:01:24,900
Vous pouvez voir comment cela se traduit par une tokenisation plus compacte.

21
00:01:24,900 --> 00:01:28,509
Le traitement du jeu de données est alors super facile avec la fonction de `map`.

22
00:01:28,509 --> 00:01:32,900
Vous pouvez choisir différentes longueurs maximales pour l'entrée et les cibles, et choisir de rembourrer à

23
00:01:32,900 --> 00:01:37,210
ce stade cette longueur maximale en définissant `padding=max_length`.

24
00:01:37,210 --> 00:01:42,540
Ici, nous allons vous montrer comment rembourrer dynamiquement car cela nécessite une étape de plus.

25
00:01:42,540 --> 00:01:45,560
Vos entrées et cibles sont toutes des phrases de différentes longueurs.

26
00:01:45,560 --> 00:01:50,470
Nous rembourrons les entrées et les cibles séparément car la longueur maximale des entrées et des cibles

27
00:01:50,470 --> 00:01:52,740
peut être différente.

28
00:01:52,740 --> 00:01:57,259
Ensuite, nous rembourrons les entrées avec le token <pad> et les cibles avec l'indice -100, pour nous

29
00:01:57,259 --> 00:02:01,470
assurer qu'elles ne sont pas prises en compte dans le calcul de la perte.

30
00:02:01,470 --> 00:02:04,869
Une fois cela fait, regrouper les entrées et les cibles devient super facile !

31
00:02:04,869 --> 00:02:10,220
La bibliothèque Transformers nous fournit un assembleur de données pour faire tout cela automatiquement.

32
00:02:10,220 --> 00:02:15,920
Vous pouvez ensuite le transmettre au Trainer avec vos jeux de données, ou l'utiliser dans la méthode `to_tf_dataset`

33
00:02:15,920 --> 00:02:17,410
avant d'utiliser model.fit()` dans vos modèles en Keras.