1
00:00:05,350 --> 00:00:11,360
Dans cette vidéo, nous verrons comment vous pouvez créer votre propre tokenizer à partir de zéro !

2
00:00:11,360 --> 00:00:18,370
Pour créer votre propre tokenizer vous devrez réfléchir à chacune des opérations impliquées

3
00:00:18,370 --> 00:00:25,220
dans la tokenisation, à savoir : la normalisation, la prétokénisation, le modèle, le post-traitement et le décodage.

4
00:00:25,220 --> 00:00:32,310
Si vous ne savez pas ce que sont la normalisation, la prétokénisation et le modèle, je vous conseille d'aller

5
00:00:32,310 --> 00:00:34,800
voir les vidéos en lien ci-dessous.

6
00:00:34,800 --> 00:00:40,329
Le post-traitement regroupe toutes les modifications que nous allons effectuer sur le texte tokenisé.

7
00:00:40,329 --> 00:00:46,690
Cela peut inclure l'ajout de tokens spéciaux, la création d'un masque d'attention mais aussi

8
00:00:46,690 --> 00:00:50,200
la génération d'une liste d'identifiants de tokens.

9
00:00:50,200 --> 00:00:55,350
L'opération de décodage se produit à la toute fin et permettra de passer de la séquence

10
00:00:55,350 --> 00:00:59,000
des identifiants dans une phrase.

11
00:00:59,000 --> 00:01:04,220
Par exemple, vous pouvez voir que les # ont été supprimés et les tokens

12
00:01:04,220 --> 00:01:10,820
composant le mot « .today » ont été regroupés.

13
00:01:10,820 --> 00:01:17,472
Dans un tokeniseur rapide, tous ces composants sont regroupés dans l'attribut `backend_tokenizer`.

14
00:01:17,472 --> 00:01:22,720
Comme vous pouvez le voir avec ce petit extrait de code, il s'agit d'une instance d'un tokenizer de la

15
00:01:22,720 --> 00:01:24,860
bibliothèque Tokenizers.

16
00:01:24,860 --> 00:01:33,799
Ainsi, pour créer votre propre tokenizer, vous devrez suivre ces étapes.

17
00:01:33,799 --> 00:01:40,510
Créez d'abord un jeu de données d'entraînement. Deuxièmement, créez et entraînez un tokenizer avec la bibliothèque Tokenizers

18
00:01:40,510 --> 00:01:49,430
et troisièmement chargez ce tokenizer dans le tokenizer de la bibliothèque Transformers.

19
00:01:49,430 --> 00:01:56,510
Pour comprendre ces étapes, je propose de recréer ensemble un tokenizer BERT.

20
00:01:56,510 --> 00:01:59,500
La première chose à faire est de créer un jeu de données.

21
00:01:59,500 --> 00:02:05,650
Avec cet extrait de code, vous pouvez créer un itérateur sur le jeu de données wikitext-2-raw-v1 qui est

22
00:02:05,650 --> 00:02:08,610
un jeu de données plutôt petit en anglais, parfait pour un exemple.

23
00:02:08,610 --> 00:02:18,830
On attaque ici le gros morceau : la conception de notre tokenizer avec la bibliothèque Tokenizers.

24
00:02:18,830 --> 00:02:25,349
Nous commençons par initialiser une instance de tokenizer avec un modèle WordPiece car c'est le modèle

25
00:02:25,349 --> 00:02:29,240
utilisé par BERT.

26
00:02:29,240 --> 00:02:32,110
Ensuite, nous pouvons définir notre normaliseur.

27
00:02:32,110 --> 00:02:39,930
Nous le définirons comme une succession de deux normalisations servant à nettoyer les caractères non visibles dans

28
00:02:39,930 --> 00:02:46,659
le texte, 1 normalisation en minuscule et 2 normalisations servant à supprimer les accents.

29
00:02:46,659 --> 00:02:54,459
Pour la pré-tokenisation, nous allons chaîner deux prétokenizer.

30
00:02:54,459 --> 00:02:59,959
Le premier séparant le texte au niveau des espaces et le second isolant les

31
00:02:59,959 --> 00:03:02,450
signes de ponctuation.

32
00:03:02,450 --> 00:03:08,430
Maintenant, nous pouvons définir le `trainer` qui nous permettra d'entraîner le modèle WordPiece choisi

33
00:03:08,430 --> 00:03:11,209
au départ.

34
00:03:11,209 --> 00:03:17,280
Pour réaliser l'entraînement, il va falloir choisir une taille de vocabulaire. Ici on en choisit 25 000.

35
00:03:17,280 --> 00:03:29,099
Et on annonce aussi les tokens spéciaux qu'on veut absolument ajouter à notre vocabulaire.

36
00:03:29,099 --> 00:03:39,209
En une ligne de code, nous pouvons entraîner notre modèle WordPiece en utilisant l'itérateur que nous avons défini précédemment.

37
00:03:39,209 --> 00:03:45,800
Une fois le modèle entraîné, nous pouvons récupérer les identifiants des tokens de classe spéciale et de séparation

38
00:03:45,800 --> 00:03:49,750
car nous en aurons besoin pour post-traiter notre séquence.

39
00:03:49,750 --> 00:03:55,790
Grâce à la classe `TemplateProcessing`, nous pouvons ajouter le token [CLS] au début de

40
00:03:55,790 --> 00:04:01,780
chaque séquence et le token [SEP] à la fin de la séquence et entre deux phrases

41
00:04:01,780 --> 00:04:07,060
si nous tokenisons une paire de texte.

42
00:04:07,060 --> 00:04:12,099
Enfin, il ne nous reste plus qu'à définir notre décodeur qui nous permettra de supprimer les ##

43
00:04:12,099 --> 00:04:17,810
au début des tokens qu'il faut rattacher au token précédent.

44
00:04:17,810 --> 00:04:30,930
Et voilà, vous avez toutes les lignes de code nécessaires pour définir votre propre tokenizer avec la bibliothèque Tokenizers.

45
00:04:30,930 --> 00:04:35,120
Maintenant que nous avons un tout nouveau tokenizer avec la bibliothèque Tokenizers, il nous suffit de le

46
00:04:35,120 --> 00:04:40,070
charger dans un tokenizer rapide de la bibliothèque Transformers.

47
00:04:40,070 --> 00:04:42,660
Là encore nous avons plusieurs possibilités.

48
00:04:42,660 --> 00:04:48,830
Nous pouvons le charger dans la classe générique `PreTrainedTokenizerFast` ou dans la classe `BertTokenizerFast` puisque nous

49
00:04:48,830 --> 00:04:56,380
avons ici construit un tokenizer de type BERT.

50
00:04:56,380 --> 00:05:01,600
J'espère que cette vidéo vous a aidé à comprendre comment vous pouvez créer votre propre tokenizer et

51
00:05:01,600 --> 00:05:10,669
que vous êtes prêt à naviguer dans la documentation de la bibliothèque Tokenizers pour choisir les composants

52
00:05:10,669 --> 00:05:16,490
de votre tout nouveau tokenizer !