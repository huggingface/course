1
00:00:05,120 --> 00:00:11,360
Voyons comment nous pouvons prétraiter nos données pour la modélisation du langage masqué. Pour rappel, la

2
00:00:11,360 --> 00:00:16,320
modélisation du langage masqué se produit lorsqu'un modèle doit remplir les blancs dans une phrase.

3
00:00:16,320 --> 00:00:22,400
Pour ce faire, vous n'avez besoin que de textes, pas d'étiquettes, car il s'agit d'un problème autosupervisé. Pour l'

4
00:00:22,400 --> 00:00:27,280
appliquer à vos propres données, assurez-vous simplement que tous vos textes sont rassemblés dans une colonne de votre jeu de données.

5
00:00:28,160 --> 00:00:32,320
Avant de commencer à masquer des choses au hasard, nous devrons en quelque sorte faire en sorte que tous ces textes aient la

6
00:00:32,320 --> 00:00:38,400
même longueur pour les regrouper. La première façon de faire en sorte que tous les textes aient la même longueur

7
00:00:38,400 --> 00:00:43,840
est celle que nous avons utilisée dans la classification de texte. Remplissons les textes courts et tronquons les longs.

8
00:00:44,800 --> 00:00:48,400
Comme nous l'avons vu lorsque nous avons traité les données pour la classification du texte, tout

9
00:00:48,400 --> 00:00:51,840
cela est fait par notre tokenizer avec les bonnes options de rembourrage et de troncature.

10
00:00:52,880 --> 00:00:57,840
Cela nous fera cependant perdre beaucoup de textes si les exemples de notre jeu de données sont très longs,

11
00:00:58,400 --> 00:01:03,040
par rapport à la longueur de contexte que nous avons choisie. Ici, toute la partie en gris est perdue.

12
00:01:04,160 --> 00:01:08,320
C'est pourquoi une deuxième façon de générer des échantillons de texte de même longueur consiste à découper notre

13
00:01:08,320 --> 00:01:12,720
texte en morceaux de longueurs de contexte, au lieu de tout supprimer après le premier morceau.

14
00:01:13,760 --> 00:01:17,920
Il y aura probablement un reste de longueur plus petit que la taille du contexte, que nous pouvons

15
00:01:17,920 --> 00:01:24,480
choisir de conserver et de rembourrer ou d'ignorer. Voici comment nous pouvons appliquer cela dans la pratique, en ajoutant simplement l'

16
00:01:24,480 --> 00:01:30,080
option `tokenize_and_chunk` dans notre appel de tokenizer. Notez comment cela nous donne un plus grand jeu de données !

17
00:01:31,280 --> 00:01:36,720
Cette deuxième méthode de segmentation est idéale si tous vos textes sont très longs, mais elle ne fonctionnera pas aussi bien

18
00:01:36,720 --> 00:01:42,640
si vous avez une variété de longueurs dans les textes. Dans ce cas, la meilleure option consiste à concaténer

19
00:01:42,640 --> 00:01:47,600
tous vos textes tokenisés dans un seul grand flux, avec des tokens spéciaux pour indiquer quand vous passez d' d'

20
00:01:47,600 --> 00:01:54,560
un document à l'autre, puis de diviser le grand flux en morceaux. Voici comment cela peut être fait

21
00:01:54,560 --> 00:02:01,200
avec du code, avec une boucle pour concaténer tous les textes et une autre pour le fragmenter. Remarquez comment cela

22
00:02:01,200 --> 00:02:05,920
réduit le nombre d'échantillons dans notre jeu de données ici, il doit y avoir eu pas mal d'entrées courtes !

23
00:02:07,520 --> 00:02:12,960
Une fois cela fait, le masquage est la partie la plus facile. Il existe un assembleur de données spécialement conçu à

24
00:02:12,960 --> 00:02:18,240
cet effet dans la bibliothèque Transformers. Vous pouvez l'utiliser directement dans le Trainer, ou lors de la conversion de

25
00:02:18,240 --> 00:02:29,600
vos ensembles de données en ensembles de données TensorFlow avant de faire `Keras.fit`, avec la méthode `to_tf_dataset`.