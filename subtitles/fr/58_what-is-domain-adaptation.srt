1
00:00:05,840 --> 00:00:12,400
Qu'est-ce que l'adaptation au domaine ? Lors du finetuning d'un modèle pré-entraîné sur un nouveau jeu de données,

2
00:00:12,400 --> 00:00:17,200
le modèle finetuné que nous obtenons fera des prédictions adaptées à ce nouvel jeu de données.

3
00:00:18,640 --> 00:00:23,440
Lorsque les deux modèles sont entraînés avec la même tâche, nous pouvons alors comparer leurs prédictions

4
00:00:23,440 --> 00:00:27,600
sur la même entrée. Les prédictions des deux modèles seront différentes,

5
00:00:27,600 --> 00:00:32,640
d'une manière qui reflète les différences entre les deux jeux de données, un phénomène que nous appelons

6
00:00:32,640 --> 00:00:39,840
« adaptation au domaine ». Regardons un exemple avec la modélisation du langage masqué, en comparant les sorties

7
00:00:39,840 --> 00:00:44,400
du modèle distillBERT pré-entraîné avec la version finetunée au chapitre 7 du cours

8
00:00:44,400 --> 00:00:50,800
(lien ci-dessous). Le modèle pré-entraîné fait des prédictions génériques, tandis que le modèle finetuné a ses

9
00:00:50,800 --> 00:00:57,040
deux premières prédictions liées au cinéma. Puisqu'il a été finetuné sur un jeu de données de critiques de films,

10
00:00:57,040 --> 00:01:00,320
il est tout à fait normal de le voir adapter ses suggestions de cette manière.

11
00:01:01,200 --> 00:01:05,520
Remarquez comment il conserve les mêmes prédictions que le modèle pré-entraîné par la suite. Même si le

12
00:01:05,520 --> 00:01:09,920
modèle finetuné s'adapte au nouveau jeu de données, il n'oublie pas sur quoi il a été pré-entraîné.

13
00:01:11,200 --> 00:01:17,120
Ceci est un autre exemple sur une tâche de traduction. En haut, nous utilisons un modèle français/anglais pré-entraîné

14
00:01:17,120 --> 00:01:22,720
et en bas, la version que nous avons finetunée au chapitre 7. Le modèle du haut est pré-entraîné sur de nombreux

15
00:01:22,720 --> 00:01:27,440
textes et laisse les termes anglais techniques tels que « plugin » et « e-mail » inchangés dans la traduction

16
00:01:28,160 --> 00:01:33,360
(les deux sont parfaitement comprises par les francophones). Le jeu de données sélectionné pour le finetuning est un

17
00:01:33,360 --> 00:01:38,240
jeu de données de textes techniques où une attention particulière a été choisie pour tout traduire en français.

18
00:01:38,960 --> 00:01:50,560
En conséquence, le modèle finetuné a choisi cette habitude et a traduit à la fois « plugin » et « e-mail ».