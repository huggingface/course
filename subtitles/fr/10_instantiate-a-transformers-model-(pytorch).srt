1
00:00:05,120 --> 00:00:07,440
Comment instancier un transformer ?

2
00:00:08,640 --> 00:00:12,960
Dans cette vidéo, nous verrons comment créer et utiliser un modèle de la bibliothèque Transformers.

3
00:00:14,160 --> 00:00:19,440
Comme nous l'avons vu précédemment, la classe AutoModel vous permet d'instancier un modèle pré-entraîné à partir de n'importe quel

4
00:00:19,440 --> 00:00:24,960
checkpoint sur le Hub d'Hugging Face. Elle choisira la bonne classe de modèle dans la bibliothèque pour

5
00:00:24,960 --> 00:00:30,800
instancier l'architecture appropriée et charger les poids du modèle pré-entraîné à l'intérieur. Comme nous

6
00:00:30,800 --> 00:00:37,760
pouvons le voir, lorsqu'on nous donne un checkpoint BERT, nous nous retrouvons avec un BertModel, et de même pour GPT-2 ou BART.

7
00:00:39,680 --> 00:00:43,440
En coulisses, cette API peut prendre le nom d'un checkpoint sur le Hub,

8
00:00:44,080 --> 00:00:48,400
auquel cas elle téléchargera et mettra en cache le fichier de configuration ainsi que le fichier de poids du modèle.

9
00:00:48,400 --> 00:00:54,800
Vous pouvez également spécifier le chemin d'accès à un dossier local contenant un fichier de configuration valide

10
00:00:54,800 --> 00:01:00,720
et un fichier de poids de modèle. Pour instancier le modèle pré-entraîné, l'API AutoModel

11
00:01:00,720 --> 00:01:04,960
ouvre d'abord le fichier de configuration pour examiner la classe de configuration à utiliser.

12
00:01:06,080 --> 00:01:12,240
La classe de configuration dépend du type de modèle (BERT, GPT-2 ou BART par exemple).

13
00:01:13,440 --> 00:01:18,160
Une fois qu'il a la classe de configuration appropriée, il peut instancier cette configuration,

14
00:01:18,160 --> 00:01:23,920
qui est un plan pour savoir comment créer le modèle. Il utilise également cette classe de configuration

15
00:01:23,920 --> 00:01:29,360
pour trouver la classe de modèle appropriée, qui est combinée avec la configuration chargée, pour charger le modèle.

16
00:01:30,800 --> 00:01:35,520
Ce modèle n'est pas encore notre modèle pré-entraîné car il vient d'être initialisé avec des poids aléatoires.

17
00:01:36,560 --> 00:01:42,960
La dernière étape consiste à charger les poids à partir du fichier de modèle dans ce modèle. Pour charger facilement

18
00:01:42,960 --> 00:01:47,360
la configuration d'un modèle à partir de n'importe quel checkpoint ou d'un dossier contenant le dossier de configuration,

19
00:01:48,000 --> 00:01:49,920
nous pouvons utiliser la classe AutoConfig.

20
00:01:51,040 --> 00:01:55,360
Comme la classe AutoModel, elle sélectionne la bonne classe de configuration dans la bibliothèque.

21
00:01:56,800 --> 00:02:01,360
Nous pouvons également utiliser la classe spécifique correspondant à un checkpoint, mais nous devrons modifier le

22
00:02:01,360 --> 00:02:08,320
code chaque fois que nous voulons essayer un modèle différent. Comme nous l'avons déjà dit, la configuration d'un modèle est

23
00:02:08,320 --> 00:02:12,720
un plan qui contient toutes les informations nécessaires pour créer l'architecture du modèle.

24
00:02:13,600 --> 00:02:19,680
Par exemple, le modèle BERT associé au checkpoint basé sur bert a 12 couches,

25
00:02:19,680 --> 00:02:29,120
une taille cachée de 768 et une taille de vocabulaire de 28 996. Une fois que nous avons la configuration,

26
00:02:29,680 --> 00:02:33,120
nous pouvons créer un modèle qui a la même architecture que notre checkpoint, mais qui est

27
00:02:33,120 --> 00:02:37,840
initialisé de manière aléatoire. Nous pouvons ensuite l'entraîner à partir de zéro comme n'importe quel module PyTorch/modèle TensorFlow.

28
00:02:38,800 --> 00:02:42,960
Nous pouvons également modifier n'importe quelle partie de la configuration à l'aide d'arguments de mots clés.

29
00:02:43,920 --> 00:02:49,280
Le deuxième extrait de code instancie un modèle BERT initialisé de manière aléatoire avec dix couches

30
00:02:49,280 --> 00:02:56,160
au lieu de 12. L'enregistrement d'un modèle une fois qu'il est entraîné ou finetuné est très simple : il suffit d'utiliser

31
00:02:56,160 --> 00:03:02,880
la méthode `save_pretrained`. Ici, le modèle sera enregistré dans un dossier nommé `my-bert-model` à

32
00:03:02,880 --> 00:03:08,240
l'intérieur du répertoire de travail actuel. Un tel modèle peut ensuite être rechargé à l'aide de la méthode `from_pretrained`.

33
00:03:08,880 --> 00:03:13,240
Pour apprendre comment envoyer ce modèle sur le Hub, regardez la vidéo sur l'API `push_to_hub`.