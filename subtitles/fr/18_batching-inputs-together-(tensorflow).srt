1
00:00:05,120 --> 00:00:10,880
Comment grouper les entrées ensemble ? Dans cette vidéo, nous verrons comment regrouper des séquences d'entrée par batchs.

2
00:00:12,480 --> 00:00:16,560
En général, les phrases que nous souhaitons faire passer dans notre modèle n'auront pas toutes la même

3
00:00:16,560 --> 00:00:23,520
longueur. Ici, nous utilisons le modèle que nous avons vu dans le pipeline d'analyse des sentiments et souhaitons classer

4
00:00:23,520 --> 00:00:29,760
deux phrases. Lors de la tokenisation et de l'association de chaque token à ses ID d'entrée correspondants,

5
00:00:29,760 --> 00:00:31,680
nous obtenons deux listes de longueurs différentes.

6
00:00:33,120 --> 00:00:38,240
Essayer de créer un tenseur ou un tableau NumPy à partir de ces deux listes entraînera une erreur, car

7
00:00:38,240 --> 00:00:44,320
tous les tableaux et tenseurs doivent être rectangulaires. Une façon de dépasser cette limite consiste à faire en sorte que la

8
00:00:44,320 --> 00:00:50,080
deuxième phrase ait la même longueur que la première en ajoutant un token spécial autant de fois que nécessaire.

9
00:00:51,040 --> 00:00:55,360
Une autre façon serait de tronquer la première séquence à la longueur de la seconde, mais nous

10
00:00:55,360 --> 00:01:00,080
leur ferions perdre beaucoup d'informations qui pourraient être nécessaires pour classer correctement la phrase.

11
00:01:01,040 --> 00:01:05,760
En général, nous ne tronquons les phrases que lorsqu'elles dépassent la longueur maximale que le

12
00:01:05,760 --> 00:01:12,560
modèle peut gérer. La valeur utilisée pour remplir la deuxième phrase ne doit pas être choisie au hasard : le modèle

13
00:01:12,560 --> 00:01:18,000
a été pré-entraîné avec un certain ID de rembourrage, que vous pouvez trouver dans `tokenizer.pad_token_id`.

14
00:01:19,760 --> 00:01:22,640
Maintenant que nous avons complété nos phrases, nous pouvons en faire un batch.

15
00:01:23,920 --> 00:01:28,400
Si nous passons les deux phrases au modèle séparément et regroupées,

16
00:01:28,400 --> 00:01:37,360
nous remarquons que nous n'obtenons pas les mêmes résultats pour la phrase qui est rembourrée (ici la seconde). Y a t'il un bug dans la bibliothèque Transformers ? Non.

17
00:01:37,360 --> 00:01:41,440
Si vous vous souvenez que les transformers utilisent beaucoup les couches d'attention, cela

18
00:01:41,440 --> 00:01:46,800
ne devrait pas être une surprise totale : lors du calcul de la représentation contextuelle de chaque token,

19
00:01:46,800 --> 00:01:52,800
les couches d'attention examinent tous les autres mots de la phrase. Si nous n'avons que la phrase ou

20
00:01:52,800 --> 00:01:57,200
la phrase avec plusieurs tokens de remplissage ajoutés, il est logique que nous n'obtenions pas les mêmes valeurs.

21
00:01:58,560 --> 00:02:03,520
Pour obtenir les mêmes résultats avec ou sans rembourrage, nous devons indiquer aux couches d'attention

22
00:02:03,520 --> 00:02:08,640
qu'elles doivent ignorer ces tokens de rembourrage. Pour ce faire, créez un masque d'attention,

23
00:02:08,640 --> 00:02:15,920
un tenseur ayant la même forme que les ID d'entrée, avec des 0 et des 1. Les 1 indiquent les tokens que les

24
00:02:15,920 --> 00:02:22,160
couches d'attention doivent prendre en compte dans le contexte et les 0 les tokens qu'ils doivent ignorer. Maintenant,

25
00:02:22,160 --> 00:02:27,040
transmettre ce masque d'attention avec les identifiants d'entrée nous donnera les mêmes résultats que lorsque nous avons envoyé

26
00:02:27,040 --> 00:02:33,600
les deux phrases individuellement au modèle ! Tout cela est fait en coulisses par le tokenizer

27
00:02:33,600 --> 00:02:39,680
lorsque vous l'appliquez à plusieurs phrases avec l'indicateur `padding=True`. Il appliquera le rembourrage avec

28
00:02:39,680 --> 00:02:49,840
la valeur appropriée aux phrases plus petites et créera le masque d'attention approprié.