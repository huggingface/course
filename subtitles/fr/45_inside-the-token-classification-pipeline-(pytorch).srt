1
00:00:05,200 --> 00:00:08,080
Examinons le pipeline de classification de tokens.

2
00:00:10,000 --> 00:00:13,920
Dans la vidéo sur le pipeline, nous avons examiné les différentes applications prêtes à l'emploi

3
00:00:13,920 --> 00:00:19,840
prises en charge par la bibliothèque Transformers, l'une d'entre elles étant la classification de tokens. Par exemple en prédisant pour

4
00:00:19,840 --> 00:00:24,960
chaque mot d'une phrase s'il correspond à une personne, une organisation ou un lieu.

5
00:00:26,400 --> 00:00:30,240
On peut même regrouper les tokens correspondant à une même entité,

6
00:00:30,240 --> 00:00:34,960
par exemple tous les tokens qui ont formé ici le mot « Sylvain », ou « Hugging » et « Face ».

7
00:00:36,960 --> 00:00:42,480
Le pipeline de classification de tokens fonctionne de la même manière que le pipeline de classification de texte que nous avons étudié

8
00:00:42,480 --> 00:00:49,360
dans une vidéo précédente. Il y a trois étapes : la tokenisation, le modèle et le post-traitement.

9
00:00:50,720 --> 00:00:55,680
Les deux premières étapes sont identiques au pipeline de classification de texte, sauf que nous utilisons un

10
00:00:55,680 --> 00:01:01,760
modèle de classification de token automatique au lieu d'un modèle de classification de séquence. Nous tokenisons notre texte, puis nous

11
00:01:01,760 --> 00:01:07,360
le donnons au modèle. Au lieu d'obtenir un numéro pour chaque étiquette possible pour la phrase entière,

12
00:01:07,360 --> 00:01:13,760
nous obtenons un numéro pour chacune des 9 étiquettes possibles pour chaque token de la phrase, ici 19.

13
00:01:15,120 --> 00:01:19,600
Comme tous les autres modèles de la bibliothèque Transformers, notre modèle génère des logits,

14
00:01:19,600 --> 00:01:26,160
que nous transformons en prédictions en utilisant une SoftMax. Nous obtenons également l'étiquette prédite pour chaque token en

15
00:01:26,160 --> 00:01:30,000
prenant la prédiction maximale (puisque la fonction softmax préserve l'ordre, nous aurions pu le

16
00:01:30,000 --> 00:01:35,200
faire sur les logits si nous n'avions pas besoin des prédictions). La configuration du modèle contient

17
00:01:35,200 --> 00:01:41,200
l'application des étiquettes dans son champ `id2label`. En l'utilisant, nous pouvons associer chaque token à son étiquette correspondante. Le

18
00:01:41,200 --> 00:01:46,400
L'étiquette `O` correspond à « aucune entité », c'est pourquoi nous ne l'avons pas vu dans nos résultats de la première diapositive.

19
00:01:47,040 --> 00:01:51,360
En plus de l'étiquette et de la probabilité, ces résultats incluaient le caractère de début et de

20
00:01:51,360 --> 00:01:56,960
fin dans la phrase. Nous devrons utiliser l'association de décalage du tokenizer pour les obtenir

21
00:01:56,960 --> 00:02:02,080
(regardez la vidéo en lien ci-dessous si vous ne les connaissez pas déjà). Ensuite, en parcourant chaque

22
00:02:02,080 --> 00:02:08,240
token ayant une étiquette distincte de `O`, nous pouvons créer la liste des résultats que nous avons obtenus avec notre premier pipeline.

23
00:02:08,240 --> 00:02:13,360
La dernière étape consiste à regrouper les tokens correspondant à la même entité.

24
00:02:13,360 --> 00:02:17,680
C'est pourquoi nous avions deux étiquettes pour chaque type d'entité : `I-PER` et `B-PER` par exemple.

25
00:02:18,240 --> 00:02:21,840
Cela nous permet de savoir si un token est dans la même entité que le précédent.

26
00:02:23,120 --> 00:02:26,720
Notez qu'il existe deux manières d'étiqueter utilisées pour la classification des tokens,

27
00:02:26,720 --> 00:02:31,680
l'une (en rose ici) utilise l'étiquette `B-PER` au début de chaque nouvelle entité, mais l'autre

28
00:02:31,680 --> 00:02:38,320
(en bleu) ne l'utilise que pour séparer deux entités adjacentes du même type. Dans les deux cas, nous pouvons

29
00:02:38,320 --> 00:02:44,720
marquer une nouvelle entité chaque fois que nous voyons apparaître une nouvelle étiquette (avec le préfixe `I` ou `B`), puis

30
00:02:44,720 --> 00:02:50,160
prendre tous les tokens suivants étiquetés de la même manière, avec un drapeau `I`. Ceci, associé

31
00:02:50,160 --> 00:03:01,040
mapl'association de décalage pour obtenir les caractères de début et de fin, nous permet d'obtenir l'étendue des textes pour chaque entité.