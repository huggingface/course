1
00:00:05,200 --> 00:00:10,880
Comment grouper les entrées ensemble ? Dans cette vidéo, nous verrons comment regrouper des séquences d'entrée par batchs.

2
00:00:12,320 --> 00:00:16,560
En général, les phrases que nous voulons faire passer dans notre modèle n'auront pas toutes la même longueur.

3
00:00:17,520 --> 00:00:21,280
Ici, nous utilisons le modèle que nous avons vu dans le pipeline d'analyse des sentiments

4
00:00:21,840 --> 00:00:26,800
et souhaitons classer deux phrases. Lors de la tokenisation et de l'association de chaque

5
00:00:26,800 --> 00:00:31,280
token à ses ID d'entrée correspondants, nous obtenons deux listes de longueurs différentes.

6
00:00:33,040 --> 00:00:38,400
Essayer de créer un tenseur ou un tableau NumPy à partir de ces deux listes entraînera une erreur, car

7
00:00:38,400 --> 00:00:44,560
tous les tableaux et tenseurs doivent être rectangulaires. Une façon de dépasser cette limite consiste à faire en sorte que la

8
00:00:44,560 --> 00:00:50,160
deuxième phrase ait la même longueur que la première en ajoutant un token spécial autant de fois que nécessaire.

9
00:00:51,360 --> 00:00:55,760
Une autre façon serait de tronquer la première séquence à la longueur de la seconde mais nous

10
00:00:55,760 --> 00:01:00,720
lui ferions perdre beaucoup d'informations qui pourraient être nécessaires pour classer correctement la phrase.

11
00:01:02,000 --> 00:01:06,720
En général, nous ne tronquons les phrases que lorsqu'elles dépassent la longueur maximale que le

12
00:01:06,720 --> 00:01:14,000
modèle peut gérer. La valeur utilisée pour remplir la deuxième phrase ne doit pas être choisie au hasard : le modèle

13
00:01:14,000 --> 00:01:19,200
a été pré-entraîné avec un certain ID de rembourrage, que vous pouvez trouver dans `tokenizer.pad_token_id`.

14
00:01:20,800 --> 00:01:25,200
Maintenant que nous avons complété nos phrases, nous pouvons en faire un batch. Si

15
00:01:25,200 --> 00:01:29,840
nous passons les deux phrases au modèle séparément et regroupées,

16
00:01:29,840 --> 00:01:39,120
nous remarquons que nous n'obtenons pas les mêmes résultats pour la phrase qui est rembourrée (ici la seconde). Y a t'il un bug dans la bibliothèque Transformers ? Non.

17
00:01:39,120 --> 00:01:42,880
Si vous vous souvenez que les transformers utilisent beaucoup les couches d'attention, cela

18
00:01:42,880 --> 00:01:47,760
ne devrait pas être une surprise totale. Lors du calcul de la représentation contextuelle de chaque token,

19
00:01:48,560 --> 00:01:54,320
les couches d'attention examinent tous les autres mots de la phrase. Si nous n'avons que la phrase ou

20
00:01:54,320 --> 00:01:58,720
la phrase avec plusieurs tokens de rembourrage ajoutés, il est logique que nous n'obtenions pas les mêmes valeurs.

21
00:02:00,000 --> 00:02:05,120
Pour obtenir les mêmes résultats avec ou sans rembourrage, nous devons indiquer aux couches d'attention

22
00:02:05,120 --> 00:02:10,320
qu'elles doivent ignorer ces tokens de rembourrage. Pour ce faire, créez un masque d'attention,

23
00:02:10,320 --> 00:02:16,560
un tenseur ayant la même forme que les ID d'entrée, avec des 0 et des 1. Les 1 indiquent les

24
00:02:16,560 --> 00:02:21,840
tokens que les couches d'attention doivent prendre en compte dans le contexte et les 0 les tokens qu'elles doivent ignorer.

25
00:02:23,360 --> 00:02:26,560
Maintenant, passer ce masque d'attention avec les identifiants d'entrée

26
00:02:26,560 --> 00:02:30,720
nous donnera les mêmes résultats que lorsque nous avons envoyé les deux phrases individuellement au modèle !

27
00:02:32,160 --> 00:02:36,640
Tout cela est fait en coulisses par le tokenizer lorsque vous l'appliquez à plusieurs phrases

28
00:02:36,640 --> 00:02:41,280
avec l'argument `padding=True`. Il appliquera le rembourrage avec la valeur appropriée

29
00:02:41,280 --> 00:02:49,840
aux phrases plus petites et créera le masque d'attention approprié.