1
00:00:05,600 --> 00:00:08,720
Étudions comment prétraiter un jeu de données pour la classification de tokens !

2
00:00:10,400 --> 00:00:15,840
La classification de tokens regroupe toutes les tâches pouvant être définies comme l'étiquetage de chaque mot (ou token) dans

3
00:00:15,840 --> 00:00:20,640
une phrase, comme l'identification des personnes, des organisations et des lieux, par exemple.

4
00:00:21,920 --> 00:00:26,720
Pour notre exemple, nous utiliserons le jeu de données Conll, dans lequel nous supprimons les colonnes que nous

5
00:00:26,720 --> 00:00:30,720
n'utiliserons pas et renommerons les autres pour obtenir un jeu de données avec seulement deux colonnes :

6
00:00:31,360 --> 00:00:37,280
mots et étiquette. Si vous disposez de votre propre jeu de données pour la classification de tokens, assurez-vous simplement de

7
00:00:37,280 --> 00:00:43,040
nettoyer vos données pour arriver au même point, avec une colonne contenant des mots (sous forme de liste de chaînes)

8
00:00:43,040 --> 00:00:48,240
et une autre contenant des étiquettes (sous forme d'entiers s'étendant de 0 à votre nombre d'étiquettes -1).

9
00:00:49,520 --> 00:00:53,520
Assurez-vous que vos noms d'étiquettes sont stockés quelque part. Ici, nous les obtenons à partir des caractéristiques du jeu de

10
00:00:53,520 --> 00:00:58,640
données. Donc vous pouvez associer les nombres entiers à de véritables étiquettes lors de l'inspection de vos données !

11
00:01:00,480 --> 00:01:06,000
Ici, nous faisons des reconnaissances d'entités nommées. Donc nos étiquettes sont soit `O` pour les mots qui

12
00:01:06,000 --> 00:01:11,040
n'appartiennent à aucune entité, `LOC`, pour l'emplacement, `PER`, pour la personne,

13
00:01:11,680 --> 00:01:19,200
`ORG` pour l'organisation et `MISC` pour divers. Chaque étiquette a deux versions : les étiquettes `B`

14
00:01:19,200 --> 00:01:25,840
indiquent un mot qui commence une entité, tandis que les étiquettes `I` indiquent un mot qui se trouve à l'intérieur d'une entité.

15
00:01:26,880 --> 00:01:29,840
La première étape du prétraitement de nos données consiste à tokeniser les mots.

16
00:01:30,400 --> 00:01:35,200
Cela se fait très facilement avec un tokenizer. Il suffit de lui dire que nous avons prétokénisé les données

17
00:01:35,200 --> 00:01:42,160
avec le drapeau `is_split_into_words=True`. Vient ensuite la partie la plus difficile. Étant donné que nous avons ajouté des tokens spéciaux

18
00:01:42,160 --> 00:01:47,200
et que chaque mot peut avoir été divisé en plusieurs tokens, nos étiquette ne correspondent plus aux tokens.

19
00:01:47,840 --> 00:01:51,520
C'est là que les identifiants de mots fournis par notre tokenizer rapide viennent à la rescousse.

20
00:01:52,800 --> 00:01:57,440
Ils associent chaque token au mot auquel il appartient, ce qui nous permet d'associer chaque token à son étiquette.

21
00:01:58,160 --> 00:02:02,080
Nous devons simplement nous assurer que nous remplaçons les étiquettes `B-` par leurs homologues `I-` pour les tokens

22
00:02:02,080 --> 00:02:08,880
qui se trouvent à l'intérieur (mais pas au début) d'un mot. Les tokens spéciaux reçoivent une étiquette de -100,

23
00:02:08,880 --> 00:02:12,960
c'est ainsi que nous disons aux fonctions de perte du transformer de les ignorer lors du calcul de la perte.

24
00:02:14,560 --> 00:02:19,120
Le code est alors assez simple, nous écrivons une fonction qui décale les étiquettes des tokens qui

25
00:02:19,120 --> 00:02:23,920
sont à l'intérieur d'un mot (que vous pouvez personnaliser) et l'utilisons lors de la génération des étiquettes pour chaque token.

26
00:02:25,600 --> 00:02:29,840
Une fois que cette fonction pour créer nos étiquettes est écrite, nous pouvons prétraiter le jeu de données à l'aide de

27
00:02:29,840 --> 00:02:35,840
la fonction `map`. Avec l'option `batched=True`, nous libérons la vitesse de nos tokenizers rapides.

28
00:02:36,720 --> 00:02:39,360
Le dernier problème survient lorsque nous devons créer un batch.

29
00:02:40,160 --> 00:02:43,680
À moins que vous n'ayez modifié la fonction de prétraitement pour appliquer un rembourrage fixe,

30
00:02:43,680 --> 00:02:49,280
nous obtiendrons des phrases de différentes longueurs, que nous devons rembourrer à la même longueur. Le rembourrage

31
00:02:49,280 --> 00:02:55,280
doit être appliqué aux entrées ainsi qu'aux étiquettes, puisque nous devrions avoir une étiquette par token. Là encore,

32
00:02:55,280 --> 00:03:01,200
-100 indique les étiquette à ignorer pour le calcul de la perte. Tout cela est fait pour

33
00:03:01,200 --> 00:03:05,760
nous par `DataCollatorForTokenClassification`, que vous pouvez utiliser dans PyTorch ou TensorFlow.

34
00:03:06,400 --> 00:03:10,960
Avec tout cela, vous êtes soit prêt à envoyer vos données et ce assembleur de données au Trainer,

35
00:03:10,960 --> 00:03:17,840
soit à utiliser la méthode `to_tf_dataset` et à utiliser la méthode `fit` de votre modèle Keras.