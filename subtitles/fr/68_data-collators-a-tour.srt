1
00:00:06,220 --> 00:00:12,290
Dans beaucoup de nos exemples, vous verrez des assembleurs de données apparaître encore et encore.

2
00:00:12,290 --> 00:00:18,010
Ils sont utilisés à la fois dans les flux de travail PyTorch et TensorFlow, et peut-être même dans JAX, mais personne

3
00:00:18,010 --> 00:00:20,260
ne sait vraiment ce qui se passe dans JAX.

4
00:00:20,260 --> 00:00:24,590
Nous avons une équipe de recherche qui travaille là-dessus, alors peut-être qu'ils nous le diront bientôt.

5
00:00:24,590 --> 00:00:27,869
Mais que sont les assembleurs de données ?

6
00:00:27,869 --> 00:00:32,230
Les assembleurs de données rassemblent les données. Ce n'est pas très utile.

7
00:00:32,230 --> 00:00:37,930
Plus précisément, ils rassemblent une liste d'échantillons dans un seul mini-batch de entraînement.

8
00:00:37,930 --> 00:00:41,820
Pour certaines tâches, l'assembleur de données peut être très simple.

9
00:00:41,820 --> 00:00:47,010
Par exemple, lorsque vous effectuez une classification de séquences, tout ce dont vous avez vraiment besoin de votre assembleur de données,

10
00:00:47,010 --> 00:00:53,480
c'est qu'il rembourre vos échantillons à la même longueur et les concatène en un seul tenseur.

11
00:00:53,480 --> 00:00:58,989
Mais pour d'autres flux de travail, les assembleurs de données peuvent être plus complexes car ils gèrent une partie du

12
00:00:58,989 --> 00:01:04,879
prétraitement nécessaire à cette tâche particulière. Donc quand vous voulez utilisez un assembleur de données,

13
00:01:04,879 --> 00:01:09,600
pour les utilisateurs de PyTorch, vous transmettez généralement le `DataCollator` à votre objet Trainer.

14
00:01:09,600 --> 00:01:15,549
Dans TensorFlow, c'ets un peu différent. Le moyen le plus simple d'utiliser un assembleur de données consiste à le transmettre à la méthode `to_tf_dataset`

15
00:01:15,549 --> 00:01:23,700
de votre jeu de données. Cela vous donne un `tf.dataset` que vous pouvez passer à `fit`.

16
00:01:23,700 --> 00:01:27,420
Vous verrez ces approches utilisées dans les exemples et les notebooks tout au long de ce cours.

17
00:01:27,820 --> 00:01:34,360
Notez que tous nos assembleurs prennent un argument `return_tensors` : vous pouvez le définir sur `"pt"` pour obtenir des

18
00:01:34,360 --> 00:01:40,820
tenseurs PyTorch, `"tf"` pour obtenir des tenseurs TensorFlow ou `"np"` pour obtenir des tableaux Numpy.

21
00:01:40,820 --> 00:01:46,060
Pour des raisons de compatibilité descendante, la valeur par défaut est `"pt"`, donc les utilisateurs de PyTorch n'ont même pas

22
00:01:46,060 --> 00:01:51,110
besoin de définir cet argument la plupart du temps, et sont donc souvent totalement inconscients de l'existence de cette

23
00:01:51,110 --> 00:01:52,110
option.

24
00:01:52,110 --> 00:01:59,160
C'est une leçon précieuse sur la façon dont les bénéficiaires du privilège sont souvent les plus aveugles à son

25
00:01:59,160 --> 00:02:00,160
existence.

26
00:02:00,160 --> 00:02:08,130
Voyons maintenant quelques `DataCollators` spécifiques en action, mais rappelez-vous que si aucun d'

27
00:02:08,130 --> 00:02:12,069
entre eux ne fait ce dont vous avez besoin, vous pouvez toujours écrire le vôtre ! Et ils sont souvent courts.

28
00:02:12,069 --> 00:02:17,120
Tout d'abord, nous verrons les assembleurs de données de base.

29
00:02:17,120 --> 00:02:21,550
Ce sont `DefaultDataCollator` et `DataCollatorWithPadding`.

30
00:02:21,550 --> 00:02:25,550
Ce sont ceux que vous devez utiliser si vos étiquettes sont simples et si vos données ne

31
00:02:25,550 --> 00:02:28,780
nécessitent aucun traitement spécial avant d'être prêtes pour l'entraînement.

32
00:02:28,780 --> 00:02:40.480
Et notez que, comme les différents modèles ont des tokens de rembourrage différents, le collateur de données avec rembourrage aura besoin du tokenizer de votre modèle pour savoir comment rembourrer les séquences correctement.

33
00:02:40.480 --> 00:02:49.040
Le collateur de données par défaut n'a pas besoin d'un tokenizer pour fonctionner mais il produira une erreur si toutes vos séquences ne sont pas de la même longueur.

34
00:02:49.040 --> 00:02:51.360
Vous devez donc être conscient de cela.

35
00:02:51.360 --> 00:02:59.280
Cependant, un grand nombre d'assembleurs de données autres que ceux de base sont généralement conçus pour accomplir une tâche spécifique.

36
00:02:59.280 --> 00:03:05.280
Et donc je vais vous en montrer deux ici, ce sont `DataCollatorForTokenClassification` et `DataCollatorForSeq2Seq`.

37
00:03:05.280 --> 00:03:12.640
Et la raison pour laquelle ces tâches nécessitent des assembleurs spéciaux est que leurs étiquettes sont de longueur variable.

38
00:03:12.640 --> 00:03:20.159
Dans la classification de tokens, il y a une étiquette pour chaque token et donc la longueur des étiquettes est la longueur de la séquence.

39
00:03:20.159 --> 00:03:28.080
Alors qu'en séquence à séquence, les étiquettes sont une séquence de tokens qui peuvent être de longueur variable et très différente de la longueur de la séquence d'entrée.

40
00:03:28.080 --> 00:03:38.080
Dans ces deux cas, nous nous occupons de l'assemblage de ce batch en rembourrant également les étiquettes, comme vous pouvez le voir dans cet exemple.

41
00:03:38.080 --> 00:03:40.560
So inputs and labels will need to be padded.

42
00:03:40.560 --> 00:03:53.760
Si nous voulons réunir des échantillons de longueur variable dans le même mini-batch, c'est toujours le cas pour tous les assembleurs de données et c'est exactement ce que ces assembleurs de données feront pour nous dans le cadre de ces tâches particulières.

43
00:03:53.760 --> 00:04:00.000
Il y a donc un dernier assembleur de données que je veux vous montrer dans cette conférence et c'est le `DataCollatorForLanguageModeling`.

44
00:04:00.000 --> 00:04:09.519
C'est donc très important d'abord parce que les modèles de langage sont tellement fondamentaux pour tout ce que nous faisons en NLP de nos jours.

45
00:04:09.519 --> 00:04:14.640
Ensuite parce qu'il possède deux modes qui font deux choses très différentes.

46
00:04:14.640 --> 00:04:26.000
Vous choisissez donc le mode que vous voulez avec l'argument `mlm`. Mettez-le à `True` pour la modélisation du langage masqué et mettez-le à `False` pour la modélisation du langage causal.

47
00:04:26.000 --> 00:04:30.000
Donc l'assemblage de données pour la modélisation du langage causal est en fait assez simple.

48
00:04:30.000 --> 00:04:42.080
Le modèle fait juste des prédictions sur quel sera le prochain token et donc vos étiquettes sont plus ou moins juste une copie de vos entrées et l'assembleur va gérer cela et s'assurer que les entrées et les étiquettes sont rembourrées correctement.

49
00:04:42.080 --> 00:04:48.160
Cependant, lorsque vous définissez mlm à `True`, vous obtenez un comportement très différent, différent de celui de tout autre assembleur de données.

50
00:04:48.160 --> 00:04:58.000
Et c'est parce que mettre mlm à `True` signifie une modélisation du langage masqué et cela signifie que les étiquettes doivent être masquées.

51
00:04:58.000 --> 00:05:00.000
Alors, à quoi cela ressemble-t-il ?

52
00:05:00.000 --> 00:05:14.560
Rappelez-vous que dans la modélisation du langage masqué, le modèle ne prédit pas le mot suivant, au lieu de cela, nous masquons aléatoirement certains tokens et le modèle les prédit tous en même temps et essaie de remplir les blancs pour ces tokens masqués.

53
00:05:14.560 --> 00:05:18.080
Mais le processus de masquage aléatoire est étonnamment complexe.

54
00:05:18.080 --> 00:05:29.840
Si nous suivons le protocole de l'article original de BERT, nous devons remplacer certains tokens par un token masqué, d'autres tokens par un token aléatoire, puis nous gardons un troisième ensemble de tokens inchangé.

55
00:05:29.840 --> 00:05:33.919
Ce n'est pas le moment d'entrer dans les détails de ce que nous faisons ou pourquoi nous le faisons.

56
00:05:33.919 --> 00:05:40.479
Vous pouvez toujours vérifier l'article original de BERT si vous êtes curieux, il est bien écrit et facile à comprendre.

57
00:05:40.479 --> 00:05:46.560
Ce qu'il faut savoir ici, c'est que la mise en œuvre de ce système par vous-même peut s'avérer très difficile et très complexe.

58
00:05:46.560 --> 00:05:57.680
Mais l'assembleur de données pour la modélisation du langage le fera pour vous si vous mettez mlm à `True` et c'est un exemple de prétraitement plus complexe que certains de nos assembleurs de données font.

59
00:05:57.680 --> 00:05:59.280
Et c'est tout.

60
00:05:59.280 --> 00:06:02.560
Ceci couvre donc les assembleurs de données les plus couramment utilisés et les tâches pour lesquelles ils sont utilisés.

61
00:06:02.560 --> 00:06:08.720
Nous espérons que vous savez maintenant quand utiliser les assembleurs de données et lequel choisir pour votre tâche spécifique.