1
00:00:05,280 --> 00:00:11,200
L'API Trainer. La bibliothèque Transformers fournit une API Trainer qui vous permet de

2
00:00:11,200 --> 00:00:17,040
finetuner facilement des transformers sur votre propre jeu de données. La classe Trainer prend vos jeux de données,

3
00:00:17,040 --> 00:00:22,240
votre modèle ainsi que les hyperparamètres d'entraînement et peut effectuer l'entraînement sur n'importe quel type de

4
00:00:22,240 --> 00:00:30,160
configuration (CPU, GPU, multi-GPU, TPU). Elle peut également calculer les prédictions sur n'importe quel jeu de données et, si

5
00:00:30,160 --> 00:00:36,720
vous avez fourni des métriques, évaluer votre modèle sur n'importe quel jeu de données. Elle peut également gérer le traitement final des données

6
00:00:36,720 --> 00:00:41,760
comme le rembourrage dynamique tant que vous fournissez le tokenizer ou un assembleur de données donné.

7
00:00:43,040 --> 00:00:48,160
Nous allons essayer cette API sur le jeu de données MRPC car il est relativement petit et facile à prétraiter.

8
00:00:49,520 --> 00:00:54,800
Comme nous l'avons vu dans la vidéo « Vue d'ensemble de Datasets », voici comment nous pouvons le prétraiter. Nous n'appliquons pas de

9
00:00:54,800 --> 00:00:59,840
rembourrage pendant le prétraitement car nous utiliserons un rembourrage dynamique avec notre `DataCollatorWithPadding`.

10
00:01:00,960 --> 00:01:05,440
Notez que nous n'effectuons pas les étapes finales consistant à renommer/supprimer des colonnes ou à définir le format

11
00:01:05,440 --> 00:01:11,280
sur les tenseurs de Torch : Trainer fera tout cela automatiquement pour nous en analysant la

12
00:01:11,280 --> 00:01:18,080
signature du modèle. Les dernières étapes avant de créer le Trainer consistent à définir notre modèle et certains

13
00:01:18,080 --> 00:01:24,400
hyperparamètres d'entraînement. Nous avons vu comment effectuer la première dans la vidéo d'API des modèles. Pour le second,

14
00:01:24,400 --> 00:01:29,600
nous utilisons la classe `TrainingArguments`. Elle n'a besoin que d'un chemin vers un dossier où les résultats et les checkpoints

15
00:01:29,600 --> 00:01:34,240
seront enregistrés, mais vous pouvez également personnaliser tous les hyperparamètres que le Trainer utilisera :

16
00:01:34,240 --> 00:01:39,600
le taux d'apprentissage, le nombre d'époques d'entraînement, etc. Il est alors très facile de créer un Trainer et de

17
00:01:39,600 --> 00:01:44,720
lancer une entraînement. Cela devrait afficher une barre de progression et après quelques minutes (si vous exécutez

18
00:01:44,720 --> 00:01:50,480
sur un GPU), vous devriez avoir terminé l'entraînement. Cependant, le résultat sera plutôt anticlimatique,

19
00:01:50,480 --> 00:01:54,880
car vous n'obtiendrez qu'une perte d'entraînement qui ne vous dit rien sur les performances de votre

20
00:01:54,880 --> 00:01:59,920
modèle. En effet, nous n'avons spécifié aucune statistique pour l'évaluation.

21
00:02:00,960 --> 00:02:05,520
Pour obtenir ces statistiques, nous allons d'abord rassembler les prédictions sur l'ensemble d'évaluation à l'aide de la

22
00:02:05,520 --> 00:02:11,760
méthode `predict`. Cela renvoie un tuple nommé avec trois champs : `predictions` (qui contient les prédictions du modèle), 

23
00:02:11,760 --> 00:02:17,760
`label_ids` (qui contient les étiquettes si votre jeu de données en avait) et `metrics` (qui est

24
00:02:17,760 --> 00:02:24,480
vide ici). Les prédictions sont les logits des modèles pour toutes les phrases du jeu de données,

25
00:02:24,480 --> 00:02:31,440
donc un tableau NumPy de forme 408 par 2. Pour les faire correspondre à nos étiquettes, nous devons prendre le logit maximum

26
00:02:31,440 --> 00:02:36,560
pour chaque prédiction (pour savoir laquelle des deux classes a été prédite), ce que nous faisons avec la

27
00:02:36,560 --> 00:02:42,480
fonction argmax. Ensuite, nous pouvons utiliser une métrique de la bibliothèque Datasets : elle peut être chargée aussi facilement

28
00:02:42,480 --> 00:02:47,200
que notre jeu de données avec la fonction `load_metric`, et elle renvoie la métrique d'évaluation utilisée pour

29
00:02:47,200 --> 00:02:54,080
le jeu de données que nous utilisons. Nous pouvons voir que notre modèle a appris quelque chose car il est précis à 85,7%.

30
00:02:55,200 --> 00:02:59,920
Pour surveiller les métriques d'évaluation pendant l'entraînement, nous devons définir une fonction `compute_metrics`

31
00:02:59,920 --> 00:03:05,200
qui effectue la même étape qu'auparavant : elle prend un tuple nommé avec des prédictions et des étiquettes

32
00:03:05,200 --> 00:03:08,000
et doit renvoyer un dictionnaire avec la métrique dont nous voulons garder une trace.

33
00:03:09,120 --> 00:03:14,400
En transmettant la stratégie d'évaluation d'époque à nos arguments d'entraînement, nous disons au Trainer d'évaluer

34
00:03:14,400 --> 00:03:20,400
à la fin de chaque époque. Le lancement d'un entraînement dans un notebook affichera alors une barre de progression

35
00:03:20,400 --> 00:03:29,920
et complétera le tableau que vous voyez ici au fur et à mesure que vous parcourez chaque époque.