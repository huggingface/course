1
00:00:05,200 --> 00:00:11,600
Pourquoi les tokenizers rapides sont-ils appelés rapides ? Dans cette vidéo, nous verrons exactement à quel point les tokenizers dits

2
00:00:11,600 --> 00:00:16,960
rapides sont plus rapides que leurs homologues lents. Pour ce benchmark,

3
00:00:16,960 --> 00:00:22,160
nous utiliserons le jeu de données GLUE MNLI, qui contient 432 000 paires de textes.

4
00:00:22,880 --> 00:00:27,040
Nous verrons combien de temps il faut aux versions rapides et lentes d'un tokenizer BERT

5
00:00:27,040 --> 00:00:34,080
pour les traiter toutes. Nous définissons notre tokenizer rapide et lent à l'aide de l'API `AutoTokenizer`. Le

6
00:00:34,080 --> 00:00:40,160
tokeniser rapide est la valeur par défaut (lorsqu'il est disponible), nous transmettons donc `use_fast=False` pour définir le lent.

7
00:00:41,200 --> 00:00:45,760
Dans un notebook, nous pouvons chronométrer l'exécution d'une cellule avec la commande magique `%time`, comme ceci.

8
00:00:46,560 --> 00:00:50,720
Le traitement du jeu de données complet est quatre fois plus rapide avec un tokeniser rapide.

9
00:00:50,720 --> 00:00:54,960
C'est en effet plus rapide, mais pas très impressionnant. C'est parce que nous avons transmis les

10
00:00:54,960 --> 00:00:59,520
textes au tokenizer un par un. C'est une erreur courante à faire avec les tokenizers rapides,

11
00:00:59,520 --> 00:01:04,320
qui sont soutenus par Rust et donc capables de paralléliser la tokenisation de plusieurs textes.

12
00:01:05,120 --> 00:01:09,520
Leur transmettre un seul texte à la fois, c'est comme envoyer un cargo entre deux continents

13
00:01:09,520 --> 00:01:15,600
avec un seul conteneur, c'est très inefficace. Pour libérer toute la vitesse de nos tokenizers rapides,

14
00:01:15,600 --> 00:01:20,320
nous devons leur envoyer des batchs de textes, ce que nous pouvons faire avec l'argument `batched=True`

15
00:01:20,320 --> 00:01:26,720
de la méthode `map`. Maintenant, ces résultats sont impressionnants ! Le tokenizer rapide prend 12 secondes pour

16
00:01:26,720 --> 00:01:33,280
traiter un jeu de données qui prend 4 minutes pour le tokenizer lent. En résumant les résultats dans ce tableau,

17
00:01:33,280 --> 00:01:37,200
vous pouvez voir pourquoi nous avons appelé ces tokenizers rapides. Et ce n'est que pour la

18
00:01:37,200 --> 00:01:48,160
tokenisation des textes. Si jamais vous avez besoin d'entraîner un nouveau tokenizer, ils le font aussi très rapidement !