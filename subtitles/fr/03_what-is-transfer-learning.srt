0:00:05.440,0:00:09.360
Qu'est-ce que l'apprentissage par transfert ?

0:00:09.360,0:00:15.320
L'idée de l'apprentissage par transfert est d'exploiter les connaissances requises par un modèle entraîné avec beaucoup de données sur une autre tâche.

0:00:15.320,0:00:19.840
Le modèle A est entraîné spécifiquement pour la tâche A.

0:00:19.840,0:00:23.760
Supposons maintenant que vous vouliez entraîner le modèle B pour une autre tâche.

0:00:23.760,0:00:27.119
Une option serait d'entraîner le modèle à partir de zéro.

0:00:27.119,0:00:31.279
Cela pourrait demander beaucoup de temps de calcul et de données.

0:00:31.279,0:00:36.399
Au lieu de cela, nous pourrions initialiser le modèle B avec les mêmes poids que le modèle A.

0:00:36.399,0:00:40.800
Transférer les connaissances du modèle A sur la tâche B.

0:00:40.800,0:00:45.680
Lors de l'entraînement à partir de zéro, tous les poids des modèles sont initialisés de façon aléatoire.

0:00:45.680,0:00:53.680
Dans cet exemple, nous entraînons un modèle BERT à reconnaître si deux phrases sont similaires ou non.

0:00:53.680,0:00:56.480
A gauche, il est entraîné à partir de zéro.

0:00:56.480,0:00:59.920
Et à droite, c'est un modèle pré-entraîné finetuné.

0:00:59.920,0:01:04.360
Comme nous pouvons le constater, l'utilisation de l'apprentissage par transfert sur le modèle pré-entraîné donne de meilleurs résultats.

0:01:04.360,0:01:06.159
Et ce n'est même pas important si vous entraînez plus longtemps. 

0:01:06.159,0:01:14.240
L'entraînement à partir de zéro est maintenu autour de 70% de précision alors que le modèle de ré-entraîné bat facilement les 86%.

0:01:14.240,0:01:24.240
En effet, les modèles pré-entraînés sont généralement entraînés sur de grandes quantités de données, mais fournissent un modèle avec une compréhension statistique de la langue utilisée pendant le pré-entraînement.

0:01:24.240,0:01:30.000
Dans le domaine de la vision par ordinateur, l'apprentissage par transfert est appliqué avec succès depuis près de 10 ans. 

0:01:30.000,0:01:36.880
Les modèles sont fréquemment ré-entraînés sur ImageNet, un jeu de données contenant 1,2 million d'images photographiques.

0:01:36.880,0:01:40.880
Chaque image est classée par l'une des 1000 étiquettes suivantes.

0:01:40.880,0:01:46.399
L'entraînement de ce type sur des données étiquetées est appelé apprentissage supervisé.

0:01:44.399,0:01:51.500
Dans le traitement du langage naturel, l'apprentissage par transfert est un peu plus récent.

0:01:51.500,0:01:54.000
Une différence essentielle avec ImageNet est que l'entraînement est généralement autosupervisé.

0:01:54.000,0:02:00.479
Ce qui signifie les étiquettes ne nécessite pas d'annotation humaine.

0:02:00.479,0:02:05.040
Un objectif de pré-entraînement très courant consiste à deviner le mot suivant dans une phrase.

0:02:05.040,0:02:06.860
Ce qui ne demande que beaucoup, beaucoup de textes.

0:02:06.860,0:02:14.000
Le GPT-2, par exemple, a été pré-entraîné de cette manière en utilisant le contenu de 45 millions de liens postés par des utilisateurs sur Reddit.

0:02:14.000,0:02:21.000
Un autre exemple d'objectif d'entraînement autosupervisé est de prédire la valeur des mots masqués au hasard.

0:02:20.879,0:02:25.000
Ce qui est similaire aux tests de « remplissage des trous » que vous avez pu faire à l'école.

0:02:25.000,0:02:32.959
BERT a été pré-entrainé de cette manière en utilisant la version anglaise de Wikipedia et 11 000 livres non publiés.

0:02:32.959,0:02:36.239
En pratique, l'apprentissage par transfert est appliqué sur un modèle donné

0:02:36.239,0:02:42.120
en jetant sa tête, c'est-à-dire ses dernières couches, en se concentrant sur l'objectif de pré-entraînement

0:02:42.120,0:02:47.760
et la remplacer par une nouvelle tête initialisée aléatoirement et adaptée à la tâche souhaitée.

0:02:47.760,0:02:56.560
Par exemple, lorsque nous avons finetuné le BERT, nous avons supprimé les mots masqués classés par la tête et les avons remplacés par un classifieur à deux sorties.

0:02:56.560,0:02:59.650
Puisque nos tâches ajoutent deux étiquettes.

0:02:59.650,0:03:06.080
Pour être le plus efficace possible, le modèle pré-entraîné utilisé doit être aussi similaire que possible à la tâche sur laquelle il est finetuné.

0:03:06.080,0:03:14.159
Par exemple, si le problème est de classer des phrases en allemand, il est préférable d'utiliser un modèle allemand pré-entraîné.

0:03:14.159,0:03:16.400
Mais avec le bon vient le mauvais.

0:03:16.400,0:03:22.319
Le modèle pré-entraîné ne transfère pas seulement ses connaissances mais aussi les préjugés qu'il peut contenir.

0:03:22.319,0:03:24.400
ImageNet contient principalement des images provenant des États-Unis et de l'Europe occidentale.

0:03:26.560,0:03:31.519
Les modèles finetunés avec ce système sont généralement plus performants sur les images provenant de ces pays.

0:03:31.519,0:03:35.840
OpenAI a également étudié le biais de prédiction de son modèle GPT-3

0:03:35.840,0:03:39.519
qui a été pré-entrainé en utilisant l'objectif de deviner le mot suivant.

0:03:39.519,0:03:50.000
En changeant le genre du prompt de « Il était très » à « Elle était très », les prédictions majoritairement neutres sont devenues presque uniquement axées sur le physique.

0:03:50.000,0:03:59.640
Dans la carte de modèle du GPT-2, OpenAI reconnaît également son caractère biaisé et déconseille son utilisation dans les systèmes qui interagissent avec les humains.