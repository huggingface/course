1
00:00:05,520 --> 00:00:11,200
Enchâssements de texte et recherche sémantique. Dans cette vidéo, nous allons explorer comment les transformers représentent le

2
00:00:11,200 --> 00:00:15,920
texte en tant que vecteurs d'enchâssement et comment ces vecteurs peuvent être utilisés pour rechercher des documents similaires dans un corpus.

3
00:00:17,520 --> 00:00:22,000
Les enchâssements de texte ne sont qu'une façon élégante de dire que nous pouvons représenter le texte sous la forme d'un tableau de nombres

4
00:00:22,000 --> 00:00:27,120
appelé vecteur. Pour créer ces enchâssements, nous utilisons généralement un modèle basé sur un encodeur tel que BERT.

5
00:00:28,320 --> 00:00:32,320
Dans cet exemple, vous pouvez voir comment nous envoyons trois phrases à l'encodeur et obtenons

6
00:00:32,320 --> 00:00:36,400
trois vecteurs en sortie. En lisant le texte, nous pouvons voir que « promener le

7
00:00:36,400 --> 00:00:40,880
chien » ressemble le plus à « promener le chat ». Mais voyons si nous pouvons quantifier cela !

8
00:00:42,560 --> 00:00:46,080
L'astuce pour effectuer la comparaison consiste à calculer une métrique de similarité entre chaque

9
00:00:46,080 --> 00:00:50,880
paire de vecteurs d'enchâssements. Ces vecteurs vivent généralement dans un espace de grande dimension.

10
00:00:50,880 --> 00:00:54,640
Donc une métrique de similarité peut donc être tout ce qui mesure une sorte de distance entre les vecteurs.

11
00:00:55,520 --> 00:01:00,560
Une métrique populaire est la similarité cosinus, qui utilise l'angle entre deux vecteurs pour mesurer

12
00:01:00,560 --> 00:01:06,160
leur proximité. Dans cet exemple, nos vecteurs d'enchâssements vivent en 3D et nous pouvons voir que les

13
00:01:06,160 --> 00:01:12,080
vecteurs orange et gris sont proches l'un de l'autre et ont un angle plus petit. Maintenant, un problème auquel nous devons faire

14
00:01:12,080 --> 00:01:16,640
face est que les transformers comme BERT renverront en fait un vecteur d'enchâssement par token.

15
00:01:17,680 --> 00:01:22,560
Par exemple, dans la phrase « J'ai emmené mon chien en promenade », on peut s'attendre à plusieurs vecteurs d'enchâssement,

16
00:01:22,560 --> 00:01:28,880
un pour chaque mot. Par exemple, nous pouvons voir ici que la sortie de notre modèle a produit 9

17
00:01:28,880 --> 00:01:35,200
vecteurs d'enchâssement par phrase, et chaque vecteur a 384 dimensions. Mais ce que nous voulons vraiment, c'est un seul

18
00:01:35,200 --> 00:01:41,040
vecteur d'enchâssement pour toute la phrase. Pour résoudre ce problème, nous pouvons utiliser une technique appelée « pooling ».

19
00:01:41,760 --> 00:01:45,840
La méthode de pooling la plus simple consiste à prendre simplement l'enchâssement de token du token [CLS].

20
00:01:46,880 --> 00:01:50,160
Alternativement, nous pouvons faire la moyenne des enchâssements des tokens, ce que l'on appelle

21
00:01:50,160 --> 00:01:56,400
le « mean-pooling ». Avec le mean-pooling, la seule chose dont nous devons nous assurer est que nous n'incluons pas

22
00:01:56,400 --> 00:02:00,640
les tokens de rembourrage dans la moyenne, c'est pourquoi vous pouvez voir le masque d'attention utilisé ici.

23
00:02:01,680 --> 00:02:07,160
Cela nous donne maintenant un vecteur de 384 dimensions par phrase, ce qui correspond exactement à ce que nous voulons ! Et

24
00:02:07,840 --> 00:02:12,240
une fois que nous avons nos enchâssements de phrases, nous pouvons calculer la similarité cosinus pour chaque paire de

25
00:02:12,240 --> 00:02:17,520
vecteurs. Dans cet exemple, nous utilisons la fonction de scikit-learn et vous pouvez voir que la phrase « J'ai

26
00:02:17,520 --> 00:02:22,400
emmené mon chien en promenade » a un chevauchement de 0,83 avec « J'ai emmené mon chat en promenade ». Hourra ! Nous

27
00:02:25,040 --> 00:02:29,600
pouvons pousser cette idée un peu plus loin en comparant la similarité entre une question et un corpus

28
00:02:29,600 --> 00:02:36,000
de documents. Par exemple, supposons que nous enchâssions chaque message dans les forums d'Hugging Face. Nous pouvons ensuite poser une

29
00:02:36,000 --> 00:02:41,600
question, l'enchâsser et vérifier quels messages du forum sont les plus similaires. Ce processus est souvent appelé

30
00:02:41,600 --> 00:02:48,000
recherche sémantique car il nous permet de comparer les requêtes avec le contexte. Créer un

31
00:02:48,000 --> 00:02:54,400
moteur de recherche sémantique est assez simple dans Datasets. Nous devons d'abord enchâsser tous les documents. Dans cet exemple,

32
00:02:54,400 --> 00:02:59,120
nous prenons un petit échantillon du jeu de données SQUAD et appliquons la même logique d'enchâssement qu'auparavant.

33
00:03:00,000 --> 00:03:03,840
Cela nous donne une nouvelle colonne appelée `embeddings` qui stocke l'enchâssement de chaque passage.

34
00:03:05,680 --> 00:03:09,280
Une fois que nous avons nos enchâssements, nous avons besoin d'un moyen de trouver les voisins les plus proches d'une requête.

35
00:03:10,080 --> 00:03:14,320
La bibliothèque Datasets fournit un objet spécial appelé index FAISS qui vous permet de comparer rapidement les

36
00:03:14,320 --> 00:03:18,880
enchâssements. Nous ajoutons donc l'index FAISS, enchâssons une question et voilà !

37
00:03:18,880 --> 00:03:29,360
Nous avons maintenant trouvé les trois articles les plus similaires susceptibles de stocker la réponse.