1
00:00:06,320 --> 00:00:11,440
Jetons un coup d'œil à la tokenisation en sous-mots. Comprendre pourquoi la tokenisation en sous-mots

2
00:00:11,440 --> 00:00:16,320
est intéressante nécessite de comprendre les défauts de la tokenisation basée sur les mots et sur les caractères.

3
00:00:17,200 --> 00:00:21,760
Si vous n'avez pas vu les premières vidéos sur la tokenisation basée sur les mots et les caractères,

4
00:00:21,760 --> 00:00:24,400
nous vous recommandons de les consulter avant de regarder cette vidéo.

5
00:00:27,680 --> 00:00:33,440
La tokenisation en sous-mots se situe entre les algorithmes de tokenisation basés sur les caractères et les mots.

6
00:00:33,440 --> 00:00:40,960
L'idée est de trouver un terrain d'entente entre de très grands vocabulaires, une grande quantité de

7
00:00:40,960 --> 00:00:47,040
tokens hors vocabulaire et une perte de sens entre des mots très similaires pour les tokenizers basés sur des mots,

8
00:00:47,040 --> 00:00:52,800
et de très longues séquences, des tokens individuels moins significatifs pour les tokenizers basés sur des caractères.

9
00:00:54,720 --> 00:00:59,360
Ces algorithmes reposent sur le principe suivant : les mots fréquemment utilisés ne doivent pas

10
00:00:59,360 --> 00:01:04,800
être divisés en sous-mots plus petits, mais les mots rares doivent être décomposés en sous-mots significatifs.

11
00:01:06,320 --> 00:01:11,520
Un exemple est le mot « dog » : nous aimerions que notre tokenizer ait un seul identifiant pour le mot

12
00:01:11,520 --> 00:01:18,480
« dog », plutôt que de le diviser en caractères : « d », « o » et « g ». Cependant, lorsque nous rencontrons le mot

13
00:01:18,480 --> 00:01:23,920
« dogs », nous aimerions que notre tokenizer comprenne qu'à la racine, il s'agit toujours du mot « dog »

14
00:01:23,920 --> 00:01:31,280
avec un « s » ajouté tout en modifiant légèrement le sens tout en gardant l'idée originale. Un autre exemple

15
00:01:31,280 --> 00:01:37,520
est un mot complexe comme « tokenization » qui peut être divisé en sous-mots significatifs. La racine

16
00:01:37,520 --> 00:01:42,000
du mot est « token » et « ization » complète la racine pour lui donner un sens légèrement différent.

17
00:01:42,720 --> 00:01:48,960
Il est logique de diviser le mot en deux : « token » en tant que racine du mot (étiqueté comme le « début »

18
00:01:48,960 --> 00:01:53,840
du mot) et « ization »  en tant qu'informations supplémentaires (étiquetées comme « complétion » du mot).

19
00:01:56,240 --> 00:02:00,320
À son tour, le modèle pourra désormais donner un sens à « token » dans différentes situations.

20
00:02:00,880 --> 00:02:06,400
Il comprendra que les mots « token », « tokens », « tokenizing » et « tokenization » sont liés et ont

21
00:02:06,400 --> 00:02:14,000
une signification similaire. Il comprendra également que « tokenization », « modernization » et « immunization »,

22
00:02:14,000 --> 00:02:18,960
qui ont toutes les mêmes suffixes, sont probablement utilisées dans les mêmes situations syntaxiques.

23
00:02:20,320 --> 00:02:25,920
Les tokenizers basés sur des sous-mots ont généralement un moyen d'identifier quels tokens sont des débuts de mots et

24
00:02:25,920 --> 00:02:34,320
quels tokens complètent le début de mots. DOnc ici « token » est le début d'un mot et  « ##isation » est la complétion d'un mot.

25
00:02:34,960 --> 00:02:40,800
Ici, le préfixe « ## » indique que « ization » fait partie d'un mot plutôt que son début.

26
00:02:41,760 --> 00:02:49,440
Le « ## » provient du tokenizer de BERT, basé sur l'algorithme WordPiece. D'autres tokenizers utilisent d'autres

27
00:02:49,440 --> 00:02:54,720
préfixes pouvant être placés pour indiquer une partie de mots comme ici, ou le début de mots à la place !

28
00:02:56,000 --> 00:03:01,040
Il existe de nombreux algorithmes différents qui peuvent être utilisés pour la tokenisation en sous-mots, et la plupart des modèles

29
00:03:01,040 --> 00:03:05,760
obtenant des résultats de pointe en anglais utilisent aujourd'hui une sorte d'algorithme de tokenisation en sous-mots.

30
00:03:05,760 --> 00:03:12,320
Ces approches aident à réduire la taille du vocabulaire en partageant des informations

31
00:03:12,320 --> 00:03:17,840
entre différents mots, en ayant la possibilité de comprendre les préfixes et les suffixes comme tels.

32
00:03:18,480 --> 00:03:27,760
Ils conservent le sens de mots très similaires, en reconnaissant les tokens similaires qui les composent.