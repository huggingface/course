1
00:00:05,200 --> 00:00:11,680
Datasets + DataFrames = amour. Bien que les fonctions de traitement de la bibliothèque Datasets couvrent la plupart

2
00:00:11,680 --> 00:00:15,600
des cas nécessaires pour entraîner un modèle, il y a des moments où vous devrez passer à une bibliothèque

3
00:00:15,600 --> 00:00:21,840
comme Pandas pour accéder à des fonctionnalités plus puissantes ou à des API de haut niveau pour la visualisation. Heureusement,

4
00:00:21,840 --> 00:00:25,520
Datasets est conçu pour être interopérable avec des bibliothèques telles que Pandas,

5
00:00:25,520 --> 00:00:30,560
ainsi que NumPy, PyTorch, TensorFlow et JAX. Dans cette vidéo, nous allons

6
00:00:30,560 --> 00:00:33,920
voir comment nous pouvons rapidement basculer nos données vers Pandas DataFrames et inversement.

7
00:00:35,920 --> 00:00:41,280
Supposons, par exemple, que nous analysions des affaires de la Cour suprême en Suisse. Comme d'habitude,

8
00:00:41,280 --> 00:00:45,440
nous téléchargeons notre jeu de données à partir du Hub à l'aide de la fonction `load_dataset()` et vous pouvez voir que le

9
00:00:45,440 --> 00:00:49,600
premier élément du jeu d'entraînement est un dictionnaire Python ordinaire avec divers domaines d'intérêt.

10
00:00:51,440 --> 00:00:54,800
Supposons maintenant qu'avant d'entraîner des modèles, nous souhaitions explorer un peu les données.

11
00:00:55,360 --> 00:00:58,720
Par exemple, nous pourrions être intéressés à savoir quel domaine juridique est le plus courant

12
00:00:59,600 --> 00:01:02,480
ou nous pourrions vouloir savoir comment les langues sont réparties entre les régions.

13
00:01:04,320 --> 00:01:07,920
Répondre à ces questions avec le format Arrow natif n'est pas facile, mais nous pouvons facilement

14
00:01:07,920 --> 00:01:13,280
passer à Pandas pour obtenir nos réponses ! Cela fonctionne en utilisant la méthode `set_format()`,

15
00:01:13,280 --> 00:01:17,600
qui changera le format de sortie du jeu de données des dictionnaires Python aux Pandas DataFrames.

16
00:01:18,720 --> 00:01:22,720
Comme vous pouvez le voir dans cet exemple, chaque ligne du jeu de données est représentée sous la forme d'un DataFrame,

17
00:01:22,720 --> 00:01:26,160
afin que nous puissions découper le jeu de données pour obtenir un seul DataFrame du corpus.

18
00:01:27,840 --> 00:01:31,040
La façon dont cela fonctionne sous le capot est que la bibliothèque Datasets modifie la

19
00:01:31,040 --> 00:01:35,440
méthode magique `__getitem__()` de Datasets. La méthode `__getitem__()` est une méthode spéciale

20
00:01:35,440 --> 00:01:40,320
pour les conteneurs Python qui vous permet de spécifier le fonctionnement de l'indexation. Dans ce cas,

21
00:01:40,320 --> 00:01:44,320
la méthode `__getitem__()` du jeu de données brut commence par renvoyer des dictionnaires Python, 

22
00:01:45,120 --> 00:01:49,920
puis après avoir appliqué `set_format()`, nous modifions `__getitem__()` pour renvoyer des DataFrames à la place.

23
00:01:51,840 --> 00:01:56,240
La bibliothèque Datasets fournit également une méthode `to_pandas()` si vous souhaitez effectuer la conversion de format et le

24
00:01:56,240 --> 00:02:02,640
découpage du jeu de données en une seule fois. Et une fois que vous disposez d'un DataFrame, vous pouvez trouver des réponses à toutes

25
00:02:02,640 --> 00:02:07,840
sortes de questions complexes ou créer des graphiques avec votre bibliothèque de visualisation préférée.

26
00:02:07,840 --> 00:02:10,800
La seule chose à retenir est qu'une fois que vous avez terminé votre analyse Pandas,

27
00:02:10,800 --> 00:02:16,240
vous devez réinitialiser le format de sortie en Arrow tables. Si vous ne le faites pas, vous pouvez rencontrer des problèmes si

28
00:02:16,240 --> 00:02:20,240
vous essayez de tokeniser votre texte car il n'est plus représenté sous forme de chaînes dans un dictionnaire.

29
00:02:21,520 --> 00:02:32,160
En réinitialisant le format de sortie, nous récupérons les Arrow tables et pouvons tokeniser sans problème !