1
00:00:06,069 --> 00:00:11,580
Dans cette vidéo, nous allons voir comment charger et finetuner un modèle pré-entraîné.

2
00:00:11,580 --> 00:00:16,010
C'est très rapide, et si vous avez regardé nos vidéos sur le pipeline, que je mettrai en lien ci-dessous, le

3
00:00:16,010 --> 00:00:18,330
processus est très similaire.

4
00:00:18,330 --> 00:00:21,990
Cette fois, cependant, nous allons utiliser l'apprentissage par transfert et faire l'entraînement

5
00:00:21,990 --> 00:00:26,660
nous-mêmes, plutôt que de simplement charger un modèle et de l'utiliser tel quel.

6
00:00:26,660 --> 00:00:30,610
Pour en savoir plus sur l'apprentissage par transfert, rendez-vous sur la vidéo « Qu'est-ce que l'apprentissage par transfert ? »

7
00:00:30,610 --> 00:00:33,000
que nous mettrons également en lien ci-dessous !

8
00:00:33,000 --> 00:00:35,660
Mais maintenant regardons ce code.

9
00:00:35,660 --> 00:00:40,340
Pour commencer, nous choisissons le modèle avec lequel nous voulons commencer. Dans ce cas, nous allons utiliser le

10
00:00:40,340 --> 00:00:42,540
célèbre BERT original.

11
00:00:42,540 --> 00:00:50,500
Mais que signifie cette monstruosité, `TFAutoModelForSequenceClassification` ?

12
00:00:50,500 --> 00:00:56,460
Eh bien, le TF signifie TensorFlow, et le reste signifie « prenez un modèle de langue et collez-y

13
00:00:56,460 --> 00:01:00,879
une tête de classification de séquence s'il n'en a pas déjà une ».

14
00:01:00,879 --> 00:01:05,420
Donc, ce que nous allons faire ici, c'est charger BERT, un modèle de langue général, puis faire un

15
00:01:05,420 --> 00:01:09,490
apprentissage par transfert pour l'utiliser sur notre tâche d'intérêt.

16
00:01:09,490 --> 00:01:13,530
Nous chargeons ici le modèle de langue avec cette seule ligne de code, en utilisant la méthode `from_pretrained`

17
00:01:13,530 --> 00:01:14,530
.

18
00:01:14,530 --> 00:01:21,230
Cette méthode doit connaître deux choses : premièrement, le nom du modèle que vous souhaitez charger,

19
00:01:21,230 --> 00:01:29,840
et deuxièmement, le nombre de classes de votre problème.

20
00:01:29,840 --> 00:01:33,500
Si vous souhaitez suivre avec vidéos sur Datasets, que je vais lier

21
00:01:33,500 --> 00:01:41,200
ci-dessous, vous aurez alors deux classes, positive et négative, et donc `num_labels` est égal à deux.

22
00:01:41,200 --> 00:01:43,590
Qu'en est-il de cette chose `.compile` ?

23
00:01:43,590 --> 00:01:47,909
Si vous connaissez Keras, vous l'avez probablement déjà vu, mais si ce n'est pas le cas, c'est l'une

24
00:01:47,909 --> 00:01:55,520
de ses méthodes de base que vous allez voir encore et encore. Vous devez toujours compiler votre modèle avant de l'entraîner.

25
00:01:55,520 --> 00:02:01,240
La compilation doit connaître deux choses. Premièrement la fonction de perte : qu'essayons-nous d'

26
00:02:01,240 --> 00:02:02,240
optimiser ?

27
00:02:02,240 --> 00:02:08,509
Ici, nous importons la fonction de perte `SparseCategoricalCrossentropy` c'est une bouchée, mais c'est

28
00:02:08,509 --> 00:02:13,390
la fonction de perte standard pour tout réseau de neurones qui effectue une tâche de classification.

29
00:02:13,390 --> 00:02:18,170
Cela encourage essentiellement le réseau à produire des valeurs élevées pour la bonne classe et des

30
00:02:18,170 --> 00:02:21,080
valeurs faibles pour les mauvaises classes.

31
00:02:21,080 --> 00:02:26,140
Notez que vous pouvez spécifier la fonction de perte sous forme de chaîne, comme nous l'avons fait avec l'optimiseur,

32
00:02:26,140 --> 00:02:34,319
mais il y a un risque ici, il y a un piège dans lequel les gens tombent très courant comme quoi par défaut, cette perte suppose que la sortie est des probabilités

33
00:02:34,319 --> 00:02:39,650
après une couche softmax, mais ce que notre modèle a réellement produit est les valeurs avant le

34
00:02:39,650 --> 00:02:50,140
softmax, souvent appelées logits. Vous les avez déjà vues dans les vidéos sur le pipeline.

35
00:02:50,140 --> 00:02:54,580
Si vous vous trompez, votre modèle ne s'entraînera pas et il sera très ennuyeux de comprendre pourquoi.

36
00:02:54,580 --> 00:03:09,460
Dans de prochaies vidéos, nous verrons comment utiliser les pertes internes au modèle, de sorte que vous n'aurez pas à spécifier de perte vous-même, vous n'aurez pas à vous soucier de ce détail. Mais pour maintenant n'oubliez pas de mettre `from_logits=True`

40
00:03:09,460 --> 00:03:13,340
La deuxième chose que `.compile` doit savoir est l'optimiseur que vous voulez.

41
00:03:13,340 --> 00:03:17,570
Dans notre cas, nous utilisons Adam, qui est en quelque sorte l'optimiseur standard en apprentissage profond de nos

42
00:03:17,570 --> 00:03:18,730
jours.

43
00:03:18,730 --> 00:03:22,770
La seule chose que vous voudrez peut-être changer est le taux d'apprentissage, et pour ce faire, nous

44
00:03:22,770 --> 00:03:27,330
devrons importer l'optimiseur réel plutôt que de simplement l'appeler par chaîne, mais nous en

45
00:03:27,330 --> 00:03:30,050
parlerons dans une autre vidéo, que j'indiquerais en dessous.

46
00:03:30,050 --> 00:03:33,610
Pour l'instant, essayons simplement d'entraîner le modèle !

47
00:03:33,610 --> 00:03:35,830
Alors, comment entraîner un modèle ?

48
00:03:35,830 --> 00:03:40,670
Eh bien, si vous avez déjà utilisé Keras, tout cela vous sera très familier - mais

49
00:03:40,670 --> 00:03:43,370
sinon, regardons ce que nous faisons ici.

50
00:03:43,370 --> 00:03:50,371
`.fit()` est à peu près la méthode centrale pour les modèles Keras : elle indique au modèle de s'entraîner sur les modèles qu'on lui passe.

51
00:03:50,371 --> 00:03:57,000
Donc ici nous passons le jeu de données que nous avons crée dans la section précédente. Il contient les entrées et les étiquettes.

52
00:03:57,000 --> 00:04:01,000
Donc nous n'avons pas besoin de spécifier les étiquettes séparemment quand on appelle `.fit()`.

53
00:04:01,000 --> 00:04:13,000
Puis nous faisons la même chose avec les données de validation et nous pouvons spécifier des détails comme le nombre d'epochs ou autre argument que l'on peut passer à `.fit()`.

54
00:04:01,000 --> 00:04:16,540
A la fin, on passe le tout et on exécute.

55
00:04:16,540 --> 00:04:20,449
Si tout fonctionne, vous devriez voir une petite barre de progression de l'entraînement au fur et à mesure que votre perte

56
00:04:20,449 --> 00:04:21,670
diminue.

57
00:04:21,670 --> 00:04:26,870
Et c'est tout. Pendant que cela tourne, vous pouvez appelez votre supérieur et lui dire que vous êtes maintenant un ingénieur en ML/NLP senior maintenant

58
00:04:26,870 --> 00:04:30,509
et que vous allez vouloir une révision de salaire le trimestre prochain.

59
00:04:30,509 --> 00:04:38,470
C'est vraiment tout ce qu'il faut pour appliquer la puissance d'un énorme modèle de langage pré-entraîné à

60
00:04:38,470 --> 00:04:40,770
votre problème de NLP.

61
00:04:40,770 --> 00:04:42,440
Pouvons-nous faire mieux que ça ?

62
00:04:42,440 --> 00:04:47,180
Nous pourrions certainement, avec quelques fonctionnalités Keras plus avancées comme un taux d'apprentissage réglé et programmé,

63
00:04:47,180 --> 00:04:50,889
nous pouvons obtenir une perte encore plus faible et un modèle encore plus précis.

64
00:04:50,889 --> 00:04:54,039
Et qu'est-ce qu'on fait de notre modèle une fois qu'il est entraîné ?

65
00:04:54,039 --> 00:05:02,919
Je couvrirai cela et plus encore dans les vidéos liées qui suivent donc restez connectés !