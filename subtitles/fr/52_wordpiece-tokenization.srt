1
00:00:05,520 --> 00:00:10,000
Voyons ensemble quelle est la stratégie d'entraînement de l'algorithme WordPiece

2
00:00:10,560 --> 00:00:15,920
et comment il effectue la tokénisation d'un texte une fois entraîné

3
00:00:19,200 --> 00:00:25,280
WordPiece est un algorithme de tokénisation introduit par Google. Il est utilisé par exemple par BERT.

4
00:00:26,480 --> 00:00:30,640
À notre connaissance, le code de WordPiece n'a pas été mis en open source.

5
00:00:31,360 --> 00:00:36,640
Nous basons donc nos explications sur notre propre interprétation de la littérature publiée.

6
00:00:42,480 --> 00:00:48,480
Quelle est la stratégie d'entraînement de WordPiece ? Comme pour l'algorithme BPE,

7
00:00:48,480 --> 00:00:54,480
WordPiece commence par établir un vocabulaire initial composé d'unités élémentaires,

8
00:00:54,480 --> 00:01:01,760
puis augmente ce vocabulaire jusqu'à la taille souhaitée. Pour construire le vocabulaire initial,

9
00:01:01,760 --> 00:01:07,120
nous divisons chaque mot du corpus d'apprentissage en la séquence de lettres qui le compose.

10
00:01:08,240 --> 00:01:14,000
Comme vous pouvez le voir, il y a une petite subtilité : nous ajoutons deux ## devant les lettres

11
00:01:14,000 --> 00:01:20,240
qui ne commencent pas un mot. En ne gardant qu'une seule occurrence par unité élémentaire

12
00:01:20,240 --> 00:01:29,440
nous avons maintenant notre vocabulaire initial. Nous allons lister toutes les paires existantes dans notre corpus.

13
00:01:30,800 --> 00:01:34,960
Une fois que nous aurons cette liste, nous calculerons un score pour chacune de ces paires.

14
00:01:36,400 --> 00:01:40,400
Comme pour l'algorithme BPE, nous sélectionnerons la paire avec le score le plus élevé.

15
00:01:43,040 --> 00:01:50,000
Prenons par exemple la première paire composée de « h » et « ##u ». Le score d'une paire est simplement

16
00:01:50,000 --> 00:01:54,720
égal à la fréquence d'apparition de la paire divisée par le produit de la

17
00:01:54,720 --> 00:01:59,840
fréquence d'apparition du premier token par la fréquence d'apparition du deuxième token.

18
00:02:01,120 --> 00:02:04,560
Ainsi à fréquence fixe d'apparition du couple,

19
00:02:05,360 --> 00:02:11,440
si les sous-parties du couple sont très fréquentes dans le corpus alors ce score sera diminué.

20
00:02:12,960 --> 00:02:24,000
Dans notre exemple, la paire « hu » apparaît 4 fois, la lettre « h » 4 fois et la lettre « u » 4 fois. Cela

21
00:02:24,000 --> 00:02:32,320
nous donne un score de 0,25. Maintenant que nous savons comment calculer ce score, nous pouvons le faire pour toutes les paires.

22
00:02:33,200 --> 00:02:36,480
On peut maintenant ajouter au vocabulaire la paire avec le score le plus élevé,

23
00:02:37,120 --> 00:02:43,520
après l'avoir fusionnée bien sûr ! Et maintenant, nous pouvons appliquer cette même fusion à notre corpus divisé.

24
00:02:45,600 --> 00:02:51,520
Comme vous pouvez l'imaginer, nous n'avons qu'à répéter les mêmes opérations jusqu'à ce que nous ayons le vocabulaire à

25
00:02:51,520 --> 00:03:00,320
la taille souhaitée ! Examinons quelques étapes supplémentaires pour voir l'évolution de notre vocabulaire et la

26
00:03:00,320 --> 00:03:09,840
longueur des fractionnements qui se raccourcit. Maintenant que nous sommes satisfaits de notre vocabulaire, vous vous

27
00:03:09,840 --> 00:03:16,400
demandez probablement comment l'utiliser pour marquer un texte. Supposons que nous souhaitions tokeniser le mot « huggingface ».

28
00:03:17,760 --> 00:03:23,280
WordPiece suit ces règles : nous chercherons le token le plus long possible au début de

29
00:03:23,280 --> 00:03:30,560
notre mot. Ensuite, nous recommençons sur la partie restante de notre mot. Et ainsi de suite jusqu'à la

30
00:03:30,560 --> 00:03:38,240
fin ! Et c'est tout, « huggingface » est divisé en 4 sous-tokens. Cette vidéo est sur le point de se

31
00:03:38,240 --> 00:03:43,040
terminer, j'espère qu'elle vous a aidé à mieux comprendre ce qui se cache derrière le mot WordPiece ! 