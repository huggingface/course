1
00:00:03,120 --> 00:00:10,240
Jetons un coup d'œil à la tokenisation basée sur les mots. La tokenisation basée sur les mots est l'idée de diviser

2
00:00:10,240 --> 00:00:19,040
le texte brut en mots, en divisant sur des espaces ou d'autres règles spécifiques comme la ponctuation.  Dans cet

3
00:00:19,040 --> 00:00:25,040
algorithme, chaque mot est associé à un numéro spécifique, un ID, qui lui est attribué.  Dans cet exemple, « Let's »

4
00:00:25,040 --> 00:00:33,120
a l'ID 250, « do » a l'ID 861 et « tokenization! » a l'ID 345.

5
00:00:34,160 --> 00:00:39,840
Cette approche est intéressante, car le modèle a des représentations basées sur des mots entiers.

6
00:00:42,560 --> 00:00:45,680
Les informations contenues dans un seul nombre sont élevées

7
00:00:45,680 --> 00:00:52,880
car un mot contient de nombreuses informations contextuelles et sémantiques dans une phrase.

8
00:00:52,880 --> 00:00:58,720
Cependant, cette approche a ses limites. Par exemple, le mot « dog » et le mot

9
00:00:58,720 --> 00:01:04,320
« dogs » sont très similaires et leur sens est proche. Cependant, la tokenisation basée sur les mots

10
00:01:05,280 --> 00:01:10,320
attribuera des identifiants entièrement différents à ces deux mots, et le modèle apprendra donc

11
00:01:10,320 --> 00:01:14,880
des significations différentes pour ces deux mots. C'est malheureux, car nous aimerions que le

12
00:01:14,880 --> 00:01:21,120
modèle comprenne que ces mots sont en effet liés et que « dogs »  est la forme plurielle du mot « dog ».

13
00:01:22,800 --> 00:01:26,400
Un autre problème avec cette approche est qu'il y a beaucoup de mots différents dans une langue.

14
00:01:27,840 --> 00:01:31,920
Si nous voulons que notre modèle comprenne toutes les phrases possibles dans cette langue,

15
00:01:31,920 --> 00:01:37,200
nous aurons besoin d'un identifiant pour chaque mot différent, et le nombre total de mots,

16
00:01:37,200 --> 00:01:41,440
également connu sous le nom de taille du vocabulaire, peut rapidement devenir très important.

17
00:01:44,160 --> 00:01:48,800
C'est un problème car chaque ID est associé à un grand vecteur qui représente la signification du mot.

18
00:01:50,000 --> 00:01:55,840
Et le suivi de ces associations nécessite un nombre énorme de poids lorsque la taille du vocabulaire

19
00:01:55,840 --> 00:02:03,360
est importante. Si nous voulons que nos modèles restent légers, nous pouvons opter pour que notre tokenizer ignore

20
00:02:03,360 --> 00:02:11,760
certains mots dont nous n'avons pas nécessairement besoin. Par exemple ici, lors d'entraînement de notre tokenizer sur un texte,

21
00:02:11,760 --> 00:02:23,520
nous pouvons vouloir prendre les 10 000 mots les plus fréquents dans ce texte pour créer notre vocabulaire de base, au lieu de prendre tous les mots de cette langue.

22
00:02:23,520 --> 00:02:27,200
Le tokenizer saura comment convertir ces 10 000 mots en nombres,

23
00:02:27,200 --> 00:02:33,520
mais tout autre mot sera converti en mot hors vocabulaire ou en « UNKNOWN ».

24
00:02:36,000 --> 00:02:39,760
Cela peut rapidement devenir un problème : le modèle aura exactement la même représentation

25
00:02:39,760 --> 00:02:46,720
pour tous les mots qu'il ne connaît pas, ce qui entraînera de nombreuses pertes d'informations si beaucoup de mots inconnus sont présents.