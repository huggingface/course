1
00:00:05,360 --> 00:00:10,720
Voyons comment prétraiter un jeu de données pour le résumé de texte. C'est la tâche de

2
00:00:10,720 --> 00:00:16,976
bien résumer un long document. Cette vidéo se concentrera sur la façon de prétraiter votre jeu de données une fois que vous

3
00:00:16,976 --> 00:00:21,840
avez réussi à le mettre dans le format suivant : une colonne pour les documents longs et une pour

4
00:00:21,840 --> 00:00:27,360
les résumés. Voici comment nous pouvons y parvenir avec la bibliothèque Datasets sur le jeu de données XSUM.

5
00:00:28,400 --> 00:00:32,400
Tant que vous parvenez à faire en sorte que vos données ressemblent à ceci, vous devriez pouvoir suivre les mêmes étapes.

6
00:00:33,520 --> 00:00:37,280
Pour une fois, nos étiquettes ne sont pas des entiers correspondant à certaines classes,

7
00:00:37,280 --> 00:00:43,120
mais du texte brut. Nous devrons donc les tokeniser, comme nos entrées. Il y a cependant un petit piège

8
00:00:43,120 --> 00:00:47,760
car nous devons tokeniser nos cibles dans le gestionnaire de contexte `as_target_tokenzier`.

9
00:00:48,480 --> 00:00:53,200
En effet, les tokens spéciaux que nous ajoutons peuvent être légèrement différents pour les entrées et les cibles.

10
00:00:53,760 --> 00:00:58,320
Le tokenizer doit donc savoir lequel il traite. Le traitement du jeu de données

11
00:00:58,320 --> 00:01:03,520
est alors très facile avec la fonction `map`. Les résumés étant généralement beaucoup plus courts que les

12
00:01:03,520 --> 00:01:07,840
documents, vous devez absolument choisir des longueurs maximales différentes pour les entrées et les cibles.

13
00:01:08,640 --> 00:01:12,640
À ce stade, vous pouvez choisir de rembourrer cette longueur maximale en définissant `padding=max_length`.

14
00:01:13,840 --> 00:01:17,360
Ici, nous allons vous montrer comment rembourrer dynamiquement car cela nécessite une étape supplémentaire.

15
00:01:18,640 --> 00:01:23,360
Vos entrées et cibles sont toutes des phrases de différentes longueurs. Nous rembourrons les entrées et les

16
00:01:23,360 --> 00:01:27,920
cibles séparément car la longueur maximale des entrées et des cibles est complètement différente.

17
00:01:28,880 --> 00:01:32,320
Ensuite, nous rembourrons les entrées aux longueurs maximales parmi les entrées,

18
00:01:32,320 --> 00:01:38,800
et de même pour les cibles. Nous rembourrons les entrées avec le token <pad> et les cibles avec l'indice -100

19
00:01:38,800 --> 00:01:44,400
pour nous assurer qu'elles ne sont pas prises en compte dans le calcul de la perte. La bibliothèque Transformers

20
00:01:44,400 --> 00:01:49,200
nous fournit un assembleur de données pour faire tout cela automatiquement. Vous pouvez ensuite le transmettre

21
00:01:49,200 --> 00:01:55,440
au Trainer avec vos jeux de données ou l'utiliser dans la méthode `to_tf_dataset` avant d'utiliser `model.fit()` dans vos modèles en Keras.