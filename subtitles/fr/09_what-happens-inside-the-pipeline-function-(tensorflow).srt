1
00:00:05,360 --> 00:00:07,680
Que se passe-t-il dans la fonction pipeline ?

2
00:00:09,840 --> 00:00:14,800
Dans cette vidéo, nous allons voir ce qui se passe réellement lorsque nous utilisons la fonction pipeline de

3
00:00:14,800 --> 00:00:20,880
la bibliothèque Transformers. Plus précisément, nous examinerons le pipeline d'analyse des sentiments et

4
00:00:20,880 --> 00:00:26,720
comment il est passé des deux phrases suivantes aux étiquettes positives et négatives avec leurs scores respectifs.

5
00:00:28,560 --> 00:00:34,160
Comme nous l'avons vu dans la présentation du pipeline, il y a trois étapes dans le pipeline. Tout d'abord,

6
00:00:34,800 --> 00:00:38,880
nous convertissons les textes bruts en nombres que le modèle peut comprendre, à l'aide d'un tokenizer.

7
00:00:40,000 --> 00:00:43,520
Ensuite, ces chiffres passent par le modèle, qui génère des logits.

8
00:00:44,400 --> 00:00:49,120
Enfin, les étapes de post-traitement transforment ces logits en étiquettes et scores.

9
00:00:50,720 --> 00:00:54,960
Examinons en détail ces trois étapes et comment les reproduire à l'aide de la bibliothèque Transformers, en

10
00:00:54,960 --> 00:01:03,280
commençant par la première étape, la tokenisation. Le processus de tokenisation comporte plusieurs étapes. Tout d'abord,

11
00:01:03,280 --> 00:01:09,120
le texte est divisé en petits morceaux appelés tokens. Il peut s'agir de mots, de parties de mots ou de

12
00:01:09,120 --> 00:01:17,440
symboles de ponctuation. Ensuite, le tokenizer aura des tokens spéciaux (si le modèle les attend). Ici, le modèle

13
00:01:17,440 --> 00:01:22,800
utilise attend un token [CLS] au début et un token [SEP] à la fin de la phrase à classer.

14
00:01:23,760 --> 00:01:28,880
Enfin, le tokenizer associe chaque token à son ID unique dans le vocabulaire du modèle pré-entraîné.

15
00:01:28,880 --> 00:01:34,640
Pour charger un tel tokenizer de tokens, la bibliothèque Transformers fournit l'API AutoTokenizer.

16
00:01:35,680 --> 00:01:40,640
La méthode la plus importante de cette classe est `from_pretrained`, qui téléchargera et mettra en cache

17
00:01:40,640 --> 00:01:47,200
la configuration et le vocabulaire associés à un checkpoint donné. Ici, le checkpoint utilisé

18
00:01:47,200 --> 00:01:53,840
par défaut pour le pipeline d'analyse des sentiments est `distilbert base uncased finetuned sst2 english`.

19
00:01:56,560 --> 00:02:01,440
Nous instancions un tokenizer associé à ce checkpoint, puis lui envoyons les deux phrases.

20
00:02:02,640 --> 00:02:07,360
Étant donné que ces deux phrases n'ont pas la même taille, nous devrons remplir la plus courte pour

21
00:02:07,360 --> 00:02:11,680
être en mesure de créer un tableau. Cette opération est effectuée par le tokenizer avec l'option `padding=True`.

22
00:02:13,840 --> 00:02:18,960
Avec `truncation=True`, nous nous assurons que toute phrase plus longue que le maximum que le modèle peut gérer

23
00:02:18,960 --> 00:02:25,600
est tronquée. Enfin, l'option `return_tensors` indique au tokenizer de renvoyer un tenseur TensorFlow.

24
00:02:26,720 --> 00:02:29,680
En regardant le résultat, nous voyons que nous avons un dictionnaire avec deux clés.

25
00:02:30,240 --> 00:02:37,280
Les ID d'entrée contiennent les ID des deux phrases, avec des 0 où le rembourrage est appliqué. La deuxième clé,

26
00:02:37,280 --> 00:02:42,080
masque d'attention, indique où le rembourrage a été appliqué, de sorte que le modèle n'y prête pas

27
00:02:42,080 --> 00:02:48,000
attention. C'est tout ce qui se trouve à l'intérieur de l'étape de tokenisation. Examinons maintenant la deuxième étape,

28
00:02:48,640 --> 00:02:54,960
le modèle. Comme pour le tokenizer, il existe une API TFAutoModel, avec une méthode `from_pretrained`.

29
00:02:55,600 --> 00:02:59,840
Il téléchargera et mettra en cache la configuration du modèle ainsi que les pondérations pré-entraînées.

30
00:02:59,840 --> 00:03:05,600
Cependant, l'API TFAutoModel n'instanciera que le corps du modèle,

31
00:03:06,320 --> 00:03:10,640
c'est-à-dire la partie du modèle qui reste une fois la tête de pré-entraînement retirée.

32
00:03:12,000 --> 00:03:16,960
Il produira un tenseur de grande dimension qui est une représentation des phrases passées,

33
00:03:16,960 --> 00:03:20,080
mais qui n'est pas directement utile pour notre problème de classification.

34
00:03:21,760 --> 00:03:28,080
Ici, le tenseur a deux phrases, chacune de 16 tokens et la dernière dimension est la taille cachée

35
00:03:28,080 --> 00:03:34,320
de notre modèle 768. Pour obtenir une sortie liée à notre problème de classification, nous devons

36
00:03:34,320 --> 00:03:40,000
utiliser la classe TFAutoModelForSequenceClassification. Elle fonctionne exactement comme la classe AutoModel,

37
00:03:40,000 --> 00:03:45,440
sauf qu'elle créera un modèle avec une tête de classification. Il existe une classe automatique pour

38
00:03:45,440 --> 00:03:52,160
chaque tâche NLP courante dans la bibliothèque Transformers. Ici, après avoir donné à notre modèle les deux phrases,

39
00:03:52,160 --> 00:03:59,120
nous obtenons un tenseur de taille 2 par 2 : un résultat pour chaque phrase et pour chaque étiquette possible. Ces

40
00:03:59,120 --> 00:04:04,800
sorties ne sont pas encore des probabilités (nous pouvons voir qu'elles ne somment pas à 1). En effet, chaque modèle de la

41
00:04:04,800 --> 00:04:10,960
bibliothèque Transformers renvoie des logits. Pour donner un sens à ces logits, nous devons approfondir la troisième et

42
00:04:10,960 --> 00:04:17,520
dernière étape du pipeline : le post-traitement. Pour convertir les logits en probabilités, nous devons leur

43
00:04:17,520 --> 00:04:22,800
appliquer une couche SoftMax. Comme nous pouvons le voir, cela les transforme en nombres positifs qui

44
00:04:22,800 --> 00:04:28,160
somment à 1. La dernière étape consiste à savoir lequel d'entre eux correspond à l'étiquette positive ou négative.

45
00:04:28,160 --> 00:04:34,720
Ceci est donné par le champ `id2label` de la configuration du modèle. Les premières probabilités

46
00:04:34,720 --> 00:04:40,800
(indice 0) correspondent à l'étiquette négative et les secondes (indice 1) correspondent à l'

47
00:04:40,800 --> 00:04:46,640
étiquette positive. C'est ainsi que notre classifieur construit avec la fonction de pipeline a sélectionné ces étiquettes et calculé

48
00:04:46,640 --> 00:04:55,840
ces scores. Maintenant que vous connaissez le fonctionnement de chaque étape, vous pouvez facilement les adapter à vos besoins.