1
00:00:05,120 --> 00:00:11,520
La bibliothèque Datasets de Hugging Face : un aperçu rapide. La bibliothèque Hugging Face Datasets

2
00:00:11,520 --> 00:00:16,560
est une bibliothèque qui fournit une API pour télécharger rapidement de nombreux jeux de données publics et les prétraiter.

3
00:00:17,360 --> 00:00:22,560
Dans cette vidéo, nous allons explorer comment faire cela. La partie téléchargement est simple : avec la

4
00:00:22,560 --> 00:00:28,400
fonction `load_dataset`, vous pouvez directement télécharger et mettre en cache un jeu de données à partir de son identifiant sur le Hub des jeux de données.

5
00:00:29,520 --> 00:00:32,720
Ici, nous récupérons le jeu de données MRPC à partir du benchmark GLUE,

6
00:00:33,360 --> 00:00:38,320
qui est un jeu de données contenant des paires de phrases dont la tâche consiste à déterminer les paraphrases.

7
00:00:39,520 --> 00:00:45,440
L'objet renvoyé par la fonction `load_dataset` est un `DatasetDict`, qui est une sorte de dictionnaire

8
00:00:45,440 --> 00:00:51,120
contenant chaque échantillon de notre jeu de données. Nous pouvons accéder à chaque échantillon en l'indexant avec son nom.

9
00:00:52,000 --> 00:00:57,440
Cette échantillon est alors une instance de la classe `Dataset`, avec des colonnes (ici `sentence1`,

10
00:00:57,440 --> 00:01:04,240
`sentence2`, `label` et `idx`) et des lignes. Nous pouvons accéder à un élément donné par son index.

11
00:01:05,200 --> 00:01:10,000
La chose formidable à propos de la bibliothèque Datasets d'Hugging Face est que tout est enregistré sur le disque à l'

12
00:01:10,000 --> 00:01:15,520
aide d'Apache Arrow, ce qui signifie que même si votre jeu de données est énorme, vous ne manquerez pas de RAM.

13
00:01:16,080 --> 00:01:21,920
Seuls les éléments que vous demandez sont chargés en mémoire. Accéder à une tranche de votre jeu de données est

14
00:01:21,920 --> 00:01:26,720
aussi simple qu'un élément. Le résultat est alors un dictionnaire avec une liste de valeurs pour chaque clé

15
00:01:27,280 --> 00:01:31,600
(ici la liste des étiquettes, la liste des premières phrases et la liste des secondes phrases).

16
00:01:33,440 --> 00:01:38,880
L'attribut `features` d'un jeu de données nous donne plus d'informations sur ses colonnes. En particulier,

17
00:01:38,880 --> 00:01:45,280
nous pouvons voir ici qu'il nous donne la correspondance entre les nombres entiers et les noms des étiquettes. 0

18
00:01:45,280 --> 00:01:51,760
signifie non équivalent et 1 pour équivalent. Pour prétraiter tous les éléments de notre jeu de données,

19
00:01:51,760 --> 00:01:56,800
nous devons les tokeniser. Jetez un œil à la vidéo « Comment prétraiter les paires de phrases » pour un rappel,

20
00:01:57,360 --> 00:02:02,320
mais il vous suffit d'envoyer les deux phrases au tokenizer avec quelques arguments de mots clés supplémentaires.

21
00:02:03,520 --> 00:02:08,560
Ici, nous indiquons une longueur maximale de 128 et de rembourrer les entrées plus courtes que cette longueur,

22
00:02:08,560 --> 00:02:14,320
tronquer les entrées qui sont plus longues. Nous mettons tout cela dans une `tokenize_function` que nous pouvons

23
00:02:14,320 --> 00:02:20,240
appliquer directement à toutes les divisions de notre jeu de données avec la méthode `map`. Tant que la fonction renvoie un

24
00:02:20,240 --> 00:02:25,680
objet de type dictionnaire, la méthode `map` ajoutera de nouvelles colonnes si nécessaire ou mettra à jour celles existantes.

25
00:02:27,360 --> 00:02:31,840
Pour accélérer le prétraitement et tirer parti du fait que notre tokenizer est soutenu par Rust

26
00:02:31,840 --> 00:02:36,880
grâce à la bibliothèque Tokenizers d'Hugging Face, nous pouvons traiter plusieurs éléments en même temps pour

27
00:02:36,880 --> 00:02:42,160
notre `tokenize_function`, en utilisant l'argument `batched=True`. Étant donné que le tokenizer peut gérer la liste

28
00:02:42,160 --> 00:02:48,880
des premières/secondes phrases, la `tokenize_function` n'a pas besoin de changer pour cela. Vous pouvez également utiliser le

29
00:02:49,440 --> 00:02:56,400
multitraitement avec la méthode `map`, consultez sa documentation ! Une fois cela fait, nous sommes presque

30
00:02:56,400 --> 00:03:01,920
prêts pour l'entraînement : nous supprimons simplement les colonnes dont nous n'avons plus besoin avec la méthode `remove_columns`, renommons

31
00:03:01,920 --> 00:03:06,640
`label` en `labels` (puisque les modèles de la bibliothèque Transformers de Hugging Face attendent cela)

32
00:03:07,440 --> 00:03:14,000
et définissons le format de sortie sur notre « backend » souhaité : Torch, TensorFlow ou Numpy. Si nécessaire,

33
00:03:14,000 --> 00:03:17,840
nous pouvons également générer un court échantillon d'un jeu de données à l'aide de la méthode `select`.