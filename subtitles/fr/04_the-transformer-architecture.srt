1
00:00:04,960 --> 00:00:07,120
Étudions l'architecture du transformer.

2
00:00:08,960 --> 00:00:13,840
Cette vidéo est la vidéo de présentation de la série de vidéos sur les encodeurs, les décodeurs et les encodeurs-décodeurs.

3
00:00:13,840 --> 00:00:18,640
Dans cette série, nous essaierons de comprendre ce qui constitue un transformer

4
00:00:18,640 --> 00:00:24,720
et nous essaierons de l'expliquer en termes simples et de haut niveau. Aucune compréhension des réseaux de neurones n'est

5
00:00:24,720 --> 00:00:29,840
nécessaire, seule une compréhension des vecteurs et des tenseurs de base peut être utile.

6
00:00:32,320 --> 00:00:36,480
Pour commencer, nous allons reprendre ce schéma de l'article original sur le transformer,

7
00:00:36,480 --> 00:00:42,640
intitulé « Attention is All You Need ». Comme nous le verrons ici, nous ne pouvons en exploiter que certaines parties, en 

8
00:00:42,640 --> 00:00:48,080
fonction de ce que nous essayons de faire. Nous n'aborderons pas les couches spécifiques constituant cette

9
00:00:48,080 --> 00:00:52,560
architecture, mais nous essaierons de comprendre les différentes manières dont cette architecture peut être utilisée.

10
00:00:54,960 --> 00:00:59,760
Commençons par diviser cette architecture en deux parties. À gauche, nous avons l'encodeur,

11
00:00:59,760 --> 00:01:04,320
et à droite, le décodeur. Ces deux éléments peuvent être utilisés ensemble, mais ils peuvent également être utilisés

12
00:01:04,320 --> 00:01:11,280
indépendamment ! Voyons comment cela fonctionne. L'encodeur accepte les entrées qui représentent du texte.

13
00:01:11,280 --> 00:01:17,200
Il convertit ce texte, ces mots, en représentations numériques. Ces représentations numériques

14
00:01:17,200 --> 00:01:23,120
peuvent également être appelées enchâssements ou caractéristiques. Nous verrons qu'il utilise le mécanisme d'auto-attention comme

15
00:01:23,120 --> 00:01:29,840
composant principal. Nous vous recommandons de consulter la vidéo sur les encodeurs en particulier pour comprendre ce qu'est

16
00:01:29,840 --> 00:01:36,640
cette représentation numérique, ainsi que son fonctionnement. Nous étudierons le mécanisme d'auto-attention avec plus de détails

17
00:01:36,640 --> 00:01:44,000
ainsi que ses propriétés bidirectionnelles. Le décodeur est similaire à l'encodeur : il peut aussi accepter

18
00:01:44,000 --> 00:01:47,200
des entrées représentant du texte. Il utilise un mécanisme similaire à

19
00:01:47,200 --> 00:01:53,200
l'encodeur, qui est également l'auto-attention masquée. Il diffère de l'encodeur par sa

20
00:01:53,200 --> 00:01:59,200
propriété unidirectionnelle et est traditionnellement utilisé de manière autorégressive. Là aussi,

21
00:01:59,200 --> 00:02:03,600
nous vous recommandons de consulter la vidéo sur les décodeurs, notamment pour comprendre comment tout cela fonctionne.

22
00:02:06,560 --> 00:02:11,120
La combinaison des deux parties donne ce que l'on appelle un encodeur-décodeur ou un transformer séquence-à-séquence.

23
00:02:11,120 --> 00:02:16,640
L'encodeur accepte les entrées et calcule une représentation de haut niveau de ces

24
00:02:16,640 --> 00:02:22,640
entrées. Ces sorties sont ensuite transmises au décodeur. Le décodeur utilise la sortie de l'encodeur ainsi que

25
00:02:22,640 --> 00:02:27,680
d'autres entrées pour générer une prédiction. Il prédit ensuite une sortie,

26
00:02:27,680 --> 00:02:32,000
qu'il réutilisera dans les itérations futures,  d'où le terme « auto-régressif ».

27
00:02:33,040 --> 00:02:36,480
Enfin, pour comprendre l'encodeur-décodeur dans son ensemble,

28
00:02:36,480 --> 00:02:44,080
nous vous recommandons de regarder la vidéo sur les encodeurs-décodeurs.