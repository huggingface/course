1
00:00:05,200 --> 00:00:11,200
La bibliothèque Datasets de Hugging Face : un aperçu rapide. La bibliothèque Hugging Face Datasets

2
00:00:11,200 --> 00:00:15,920
est une bibliothèque qui fournit une API pour télécharger rapidement de nombreux jeux de données publics et les prétraiter.

3
00:00:16,880 --> 00:00:22,480
Dans cette vidéo, nous allons explorer comment faire cela. La partie téléchargement est simple : avec la

4
00:00:22,480 --> 00:00:27,760
fonction `load_dataset`, vous pouvez directement télécharger et mettre en cache un jeu de données à partir de son identifiant sur le Dataset hub.

5
00:00:29,040 --> 00:00:34,160
Ici, nous récupérons le jeu de données MRPC à partir du benchmark GLUE, qui est un jeu de données

6
00:00:34,160 --> 00:00:38,000
contenant des paires de phrases dont la tâche consiste à déterminer les paraphrases.

7
00:00:39,360 --> 00:00:44,880
L'objet renvoyé par la fonction `load_dataset` est un `DatasetDict`, qui est une sorte de dictionnaire

8
00:00:44,880 --> 00:00:50,800
contenant chaque échantillon de notre jeu de données. Nous pouvons accéder à chaque échantillon en l'indexant avec son nom.

9
00:00:51,520 --> 00:00:55,120
Cette échantillon est alors une instance de la classe `Dataset`, avec des colonnes

10
00:00:55,680 --> 00:01:04,000
(ici `sentence1`, `sentence2`, `label` et `idx`) et des lignes. On peut accéder à un élément donné par son index.

11
00:01:04,880 --> 00:01:10,240
La chose formidable à propos de la bibliothèque Datasets d'Hugging Face est que tout est enregistré sur le disque à l'aide d'

12
00:01:10,240 --> 00:01:15,280
Apache Arrow, ce qui signifie que même si votre jeu de données est énorme, vous ne manquerez pas de RAM.

13
00:01:15,920 --> 00:01:21,760
Seuls les éléments que vous demandez sont chargés en mémoire. Accéder à une tranche de votre jeu de données est

14
00:01:21,760 --> 00:01:27,760
aussi simple qu'un élément. Le résultat est alors un dictionnaire avec une liste de valeurs pour chaque clé

15
00:01:28,480 --> 00:01:35,040
(ici la liste des étiquettes, la liste des premières phrases et la liste des secondes phrases). L'

16
00:01:35,040 --> 00:01:40,720
attribut `features` d'un jeu de données nous donne plus d'informations sur ses colonnes. En particulier,

17
00:01:40,720 --> 00:01:45,040
nous pouvons voir ici qu'il nous donne la correspondance entre les nombres entiers et les noms des étiquettes.

18
00:01:45,920 --> 00:01:53,840
0 signifie non équivalent et 1 pour équivalent. Pour prétraiter tous les éléments de notre jeu de données,

19
00:01:53,840 --> 00:01:59,120
nous devons les tokeniser. Jetez un œil à la vidéo « Comment prétraiter des paires de phrases  » pour un rappel,

20
00:01:59,840 --> 00:02:04,480
mais il vous suffit d'envoyer les deux phrases au tokenizer avec quelques arguments de mots clés supplémentaires.

21
00:02:05,760 --> 00:02:11,200
Ici, nous indiquons une longueur maximale de 128 et de rembourrer les entrées plus courtes que cette longueur,

22
00:02:11,200 --> 00:02:17,040
tronquer les entrées qui sont plus longues. Nous mettons tout cela dans une fonction `tokenize_function` que nous pouvons

23
00:02:17,040 --> 00:02:22,320
appliquer directement à toutes les divisions de notre jeu de données avec la méthode `map`. Tant que la fonction renvoie un

24
00:02:22,320 --> 00:02:27,760
objet de type dictionnaire, la méthode `map` ajoutera de nouvelles colonnes si nécessaire ou mettra à jour celles existantes.

25
00:02:29,840 --> 00:02:34,960
Pour accélérer le prétraitement et tirer parti du fait que notre tokenizer est soutenu par Rust

26
00:02:34,960 --> 00:02:40,320
grâce à la bibliothèque Tokenizers d'Hugging Face, nous pouvons traiter plusieurs éléments en même temps

27
00:02:40,320 --> 00:02:46,800
vers notre `tokenize_function`, en utilisant l'argument `batched=True`. Étant donné que le tokenizer peut gérer la liste

28
00:02:46,800 --> 00:02:53,360
des premières/secondes phrases, la `tokenize_function`  n'a pas besoin de changer pour cela. Vous pouvez également utiliser le

29
00:02:53,360 --> 00:03:00,320
multitraitement avec la méthode `map`, consultez sa documentation ! Une fois cela fait, nous sommes presque

30
00:03:00,320 --> 00:03:05,360
prêts pour l'entraînement : nous supprimons simplement les colonnes dont nous n'avons plus besoin avec la méthode `remove_columns`, renommons

31
00:03:05,920 --> 00:03:10,320
`label` en `labels` (puisque les modèles de la bibliothèque Transformers de Hugging Face attendent cela)

32
00:03:11,200 --> 00:03:17,280
et définissons le format de sortie sur notre « backend » souhaité : Torch, TensorFlow ou Numpy. Si nécessaire,

33
00:03:17,280 --> 00:03:27,440
nous pouvons également générer un court échantillon d'un jeu de données à l'aide de la méthode `select`.