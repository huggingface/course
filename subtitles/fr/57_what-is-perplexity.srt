1
00:00:05,280 --> 00:00:09,200
Dans cette vidéo, nous examinons la mystérieuse métrique appelée perplexité.

2
00:00:10,880 --> 00:00:14,880
Vous avez peut-être rencontré la perplexité en lisant des articles sur les modèles génératifs.

3
00:00:14,880 --> 00:00:19,760
Vous pouvez voir ici deux exemples. Un venant de l'article original sur les transformers, « Attention is all you need »,

4
00:00:19,760 --> 00:00:25,600
ainsi que de l'article plus récent, GPT-2. La perplexité est une métrique courante pour mesurer les performances

5
00:00:25,600 --> 00:00:30,880
des modèles de langage. Plus la valeur est petite, meilleures sont les performances. Mais qu'est-ce que cela

6
00:00:30,880 --> 00:00:36,880
signifie réellement et comment pouvons-nous le calculer ? Une quantité très courante en apprentissage automatique est la vraissemblance.

7
00:00:37,440 --> 00:00:41,280
Nous pouvons calculer la vraissemblance comme le produit de la probabilité de chaque token.

8
00:00:42,160 --> 00:00:47,200
Cela signifie que pour chaque token, nous utilisons le modèle de langage pour prédire sa probabilité en

9
00:00:47,200 --> 00:00:52,960
fonction des tokens précédents. Au final, nous multiplions toutes les probabilités pour obtenir la vraissemblance.

10
00:00:55,680 --> 00:00:59,120
Avec la vraisemblance, nous pouvons calculer une autre quantité importante :

11
00:00:59,120 --> 00:01:04,560
l'entropie croisée. Vous avez peut-être déjà entendu parler de l'entropie croisée en examinant la fonction de perte.

12
00:01:05,440 --> 00:01:08,480
L'entropie croisée est souvent utilisée comme fonction de perte dans la classification.

13
00:01:09,040 --> 00:01:14,720
Dans la modélisation du langage, nous prédisons le token suivant, qui est aussi une tâche de classification.

14
00:01:15,600 --> 00:01:20,400
Par conséquent, si nous voulons calculer l'entropie croisée d'un exemple, nous pouvons simplement la transmettre au

15
00:01:20,400 --> 00:01:25,840
modèle avec les entrées comme étiquettes. La perte correspond alors à l'entropie croisée.

16
00:01:28,880 --> 00:01:32,640
Nous ne sommes plus qu'à une opération du calcul de la perplexité.

17
00:01:33,280 --> 00:01:39,360
En exponentiant l'entropie croisée, nous obtenons la perplexité. Vous voyez donc que la perplexité est

18
00:01:39,360 --> 00:01:55,040
étroitement liée à la perte. En faisant le lien avec les résultats précédents, c'est équivalent à l'exponentielle du négatif de la moyenne du logarithme de chaque probabilité.

19
00:01:50,360 --> 00:01:55,040
Gardez à l'esprit que la perte n'est qu'un faible indicateur de la capacité d'un modèle

20
00:01:55,040 --> 00:02:01,600
à générer un texte de qualité et qu'il en va de même pour la perplexité. Pour cette raison, on

21
00:02:01,600 --> 00:02:07,840
calcule généralement également des métriques plus sophistiquées telles que BLEU ou ROUGE sur les tâches génératives.