1
00:00:05,120 --> 00:00:11,440
Dans nos autres vidéos, nous avons parlé des principes de base pour finetuner un modèle de langage avec Tensorflow

2
00:00:11,440 --> 00:00:18,000
(et comme toujours, lorsque je fais référence à des vidéos, je les mets en lien ci-dessous). Pouvons-nous faire mieux quand même ? Voici donc

3
00:00:18,000 --> 00:00:23,040
le code de notre vidéo du finetuning du modèle, et pendant que cela tourne, nous pourrions certainement modifier

4
00:00:23,040 --> 00:00:29,040
quelques choses. Le taux d'apprentissage est de loin le plus important. Dans cette vidéo, nous expliquerons

5
00:00:29,040 --> 00:00:34,800
comment le modifier, ce qui rendra votre entraînement beaucoup plus systématiquement réussie. En fait,

6
00:00:36,080 --> 00:00:42,880
nous souhaitons modifier deux choses concernant le taux d'apprentissage par défaut d'Adam. Le premier est

7
00:00:42,880 --> 00:00:51,520
qu'il est bien trop élevé pour nos modèles - par défaut Adam utilise un taux d'apprentissage de 10^-3 (1 e moins 3),

8
00:00:51,520 --> 00:00:59,600
ce qui est très élevé pour l'entraînement des transformers. Nous allons commencer à 5 par 10^-5 (5 e moins 5),

9
00:00:59,600 --> 00:01:05,520
ce qui est 20 fois inférieur à la valeur par défaut. Et deuxièmement, nous ne voulons pas seulement un taux d'apprentissage

10
00:01:05,520 --> 00:01:10,960
constant : nous pouvons obtenir des performances encore meilleures si nous réduisons le taux d'apprentissage jusqu'à une valeur infime,

11
00:01:10,960 --> 00:01:17,760
ou même 0, au cours de l'entraînement. C'est ce que fait ce planificateur `PolynomialDecay`.

12
00:01:17,800 --> 00:01:23,880
Nous verrons à quoi cela ressemble dans une seconde. Mais d'abord nous devons spécifier au planificateur combien de

13
00:01:23,880 --> 00:01:25,120
temps l'entraînement va durer,

14
00:01:25,120 --> 00:01:29,040
pour qu'il décroisse à la bonne vitesse. C'est ce que fait ce code ici.

15
00:01:30,080 --> 00:01:35,280
Nous calculons le nombre de mini-batchs que le modèle va voir sur l'ensemble de son parcours d'entraînement,

16
00:01:35,280 --> 00:01:37,640
qui correspond à la taille de l'ensemble d'entraînement, divisé par batch_size pour obtenir le nombre debatchs

17
00:01:37,640 --> 00:01:42,080
par époque, puis multiplié par le nombre d'époques pour obtenir le nombre total

18
00:01:42,080 --> 00:01:47,680
debatchs sur l'ensemble de l'exécution de l'entraînement. Une fois que nous savons combien d'étapes de entraînement nous suivons,

19
00:01:47,680 --> 00:01:51,360
nous transmettons simplement toutes ces informations au planificateur et nous sommes prêts à commencer.

20
00:01:54,000 --> 00:01:57,360
À quoi ressemble le planificateur de décroissance polynomiale ?

21
00:01:57,360 --> 00:02:04,720
Bien, il ressemble à ceci : il commence à 5e-5, ce qui signifie 5 fois dix jusqu'au moins 5,

22
00:02:05,280 --> 00:02:11,120
puis diminue à un rythme constant jusqu'à qu'il atteigne 0 juste au moment même fin de entraînement.

23
00:02:05,280 --> 00:02:11,120
Je peux déjà vous entendre réagir. Oui, je sais, c'est en fait une décroissance constante ou linéaire.

24
00:02:11,120 --> 00:02:24,120
Je sais que le nom est « polynomial » et vous vous sentez trompés, on vous avez promis un polynôme et vous ne l'avez pas eu.

25
00:02:24,120 --> 00:02:34,120
Calmez-vous, c'est ok. Car bien sûr, les fonctions linéaires sont justes des cas particuliers avec un polynôme de premier ordre. 

26
00:02:34,120 --> 00:02:40,120
Si vous modifier les options de cette classe, vous pouvez vraiment avoir un polynôme d'ordre supérieur pour votre décroissance.

27
00:02:40,120 --> 00:02:43,120
Mais ce planificateur linéaire va bien fonctionner pour nous pour le moment.

28
00:02:43,120 --> 00:02:47,120
Nous n'avons pas besoin de toutes ces astuces et gadgets.

29
00:02:49,840 --> 00:02:56,400
Revenons à comment utilisons-nous notre barème de taux d'apprentissage ? Facile, nous le passons simplement à Adam ! Vous remarquerez que la

30
00:02:56,400 --> 00:03:00,480
première fois que nous avons compilé le modèle, nous lui avons juste passé la chaîne « Adam » pour avoir notre optimiseur.

31
00:03:02,320 --> 00:03:07,760
Donc Keras reconnaît les noms des optimiseurs courants et des fonctions de perte si vous les transmettez sous forme de chaînes,

32
00:03:07,760 --> 00:03:12,320
ce qui vous fait gagner du temps si vous ne souhaitez que les paramètres par défaut. Mais nous sommes

33
00:03:12,320 --> 00:03:19,600
maintenant des apprenants en ML professionnels, avec notre propre planificateur de taux d'apprentissage, nous devons donc faire les choses correctement.

34
00:03:19,600 --> 00:03:26,080
Donc, d'abord, nous importons l'optimiseur, puis nous l'initialisons avec notre planificateur qui est passé comme argument `learning_rate` de cet optimiseur.

35
00:03:29,200 --> 00:03:34,720
Maintenant nous compilons le modèle à l'aide du nouvel optimiseur, et quelle que soit la fonction de perte que vous voulez - ce

36
00:03:34,720 --> 00:03:39,040
sera une `SparseCategoricalCrossentropy` si vous avez suivi la vidéo sur le finetuning.

37
00:03:39,680 --> 00:03:47,120
Et on est prêt à y aller. Maintenant, nous avons un modèle à hautes performances, prêt à l'emploi. Il ne reste plus qu'à adapter le modèle comme

38
00:03:47,120 --> 00:03:53,280
nous l'avons fait auparavant. N'oubliez pas que, comme nous avons compilé le modèle avec le nouvel optimiseur avec le nouveau

39
00:03:53,280 --> 00:03:58,800
planificateur de taux d'apprentissage, nous n'avons rien à changer ici. Nous appelons à nouveau `.fit`, avec exactement la

40
00:03:58,800 --> 00:04:04,320
même commande qu'auparavant, mais nous obtenons maintenant un bel entraînement avec une belle décroissance du taux d'apprentissage, partant de la bonne valeur et décroissant jusqu'à 0.