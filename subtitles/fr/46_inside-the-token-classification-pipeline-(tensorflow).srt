1
00:00:05,250 --> 00:00:09,840
Jetons un coup d'œil à l'intérieur du pipeline de classification de tokens.

2
00:00:09,840 --> 00:00:14,690
Dans la vidéo du pipeline, nous avons examiné les différentes applications

3
00:00:14,690 --> 00:00:20,820
prises en charge par la bibliothèque Transformers, l'une d'entre elles étant la classification de tokens, par exemple en prédisant pour chaque mot d'une

4
00:00:20,820 --> 00:00:27,760
phrase s'il correspond à une personne, une organisation ou un lieu.

5
00:00:27,760 --> 00:00:32,660
On peut même regrouper les tokens correspondant à une même entité, par exemple tous les tokens

6
00:00:32,660 --> 00:00:37,950
qui ont formé ici le mot « Sylvain », ou « Hugging » et « Face ».

7
00:00:37,950 --> 00:00:42,480
Le pipeline de classification de tokens fonctionne de la même manière que le pipeline de classification de texte que

8
00:00:42,480 --> 00:00:44,379
nous avons étudié dans une vidéo précédente.

9
00:00:44,379 --> 00:00:49,600
Il y a trois étapes : la tokenisation, le modèle et le post-traitement.

10
00:00:49,600 --> 00:00:56,340
Les deux premières étapes sont identiques au pipeline de classification de texte, sauf que nous utilisons un

11
00:00:56,340 --> 00:01:01,640
modèle de classification de token automatique au lieu d'un modèle de classification de séquence.

12
00:01:01,640 --> 00:01:05,840
Nous tokenisons notre texte puis le transmettons au modèle.

13
00:01:05,840 --> 00:01:10,400
Au lieu d'obtenir un numéro pour chaque étiquette possible pour toute la phrase, nous obtenons un numéro

14
00:01:10,400 --> 00:01:16,690
pour chacune des 9 étiquettes possibles pour chaque token de la phrase, ici 19.

15
00:01:16,690 --> 00:01:22,299
Comme tous les autres modèles de la bibliothèque Transformers, notre modèle génère des logits, que nous

16
00:01:22,299 --> 00:01:25,900
transformons en prédictions en utilisant une SoftMax.

17
00:01:25,900 --> 00:01:31,430
Nous obtenons également l'étiquette prédite pour chaque token en prenant la prédiction maximale (puisque la

18
00:01:31,430 --> 00:01:35,470
fonction softmax préserve l'ordre, nous aurions pu le faire sur les logits si nous n'avions pas besoin

19
00:01:35,470 --> 00:01:37,759
des prédictions).

20
00:01:37,759 --> 00:01:42,340
La configuration du modèle contient l'association d'étiquette dans son champ `id2label`.

21
00:01:42,340 --> 00:01:45,331
En l'utilisant, nous pouvons mapper chaque token à son étiquette correspondante.

22
00:01:45,331 --> 00:01:50,490
L'étiquette `O` correspond à « aucune entité », c'est pourquoi nous ne l'avons pas vu dans nos résultats dans

23
00:01:50,490 --> 00:01:51,579
la première diapositive.

24
00:01:51,579 --> 00:01:57,430
En plus de l'étiquette et de la probabilité, ces résultats incluaient le caractère de début et de fin

25
00:01:57,430 --> 00:01:58,570
dans la phrase.

26
00:01:58,570 --> 00:02:02,610
Nous devrons utiliser l'association de décalage du tokenizer pour les obtenir (regardez la vidéo

27
00:02:02,610 --> 00:02:04,820
liée ci-dessous si vous ne les connaissez pas déjà).

28
00:02:04,820 --> 00:02:09,750
Ensuite, en parcourant chaque token qui a une étiquette distincte de `O`, nous pouvons construire la

29
00:02:09,750 --> 00:02:12,440
liste des résultats que nous avons obtenus avec notre premier pipeline.

30
00:02:12,440 --> 00:02:18,920
La dernière étape consiste à regrouper les tokens qui correspondent à la même entité.

31
00:02:18,920 --> 00:02:23,290
C'est pourquoi nous avions deux labels pour chaque type d'entité : `I-PER` et `B-PER` par exemple.

32
00:02:23,290 --> 00:02:29,190
Il nous permet de savoir si un token est dans la même entité que le précédent. A noter

33
00:02:29,190 --> 00:02:34,750
qu'il existe deux manières d'étiqueter utilisées pour la classification des tokens, l'une (en rose ici) utilise l'

34
00:02:34,750 --> 00:02:40,380
étiquette `B-PER` au début de chaque nouvelle entité, mais l'autre (en bleu) ne l'utilise que pour séparer

35
00:02:40,380 --> 00:02:43,380
deux entités adjacentes du même type.

36
00:02:43,380 --> 00:02:48,880
Dans les deux cas, nous pouvons marquer une nouvelle entité chaque fois que nous voyons apparaître une nouvelle étiquette

37
00:02:48,880 --> 00:02:54,051
(avec le préfixe `I` ou `B`) puis prendre tous les tokens suivants étiquetés de la même manière, avec un

38
00:02:54,051 --> 00:02:55,051
drapeau `I`.

39
00:02:55,051 --> 00:02:59,360
Ceci, couplé à l'association de décalage pour obtenir les caractères de début et de fin, nous permet d'

40
00:02:59,360 --> 00:03:02,000
obtenir l'étendue des textes pour chaque entité.