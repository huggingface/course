1
00:00:04,000 --> 00:00:07,200
Dans cette vidéo, nous allons étudier l'architecture encodeur-décodeur. Le T5

2
00:00:08,160 --> 00:00:15,000
est un exemple de modèle d'encodeur-décodeur populaire. Afin de comprendre le fonctionnement de

3
00:00:15,000 --> 00:00:21,680
l'encodeur-décodeur, nous vous recommandons de consulter les vidéos sur les encodeurs et les décodeurs en tant que modèles autonomes.

4
00:00:22,400 --> 00:00:30,320
Comprendre comment ils se comportent individuellement aidera à comprendre le comportement d'un encodeur-décodeur.

5
00:00:30,320 --> 00:00:35,360
Commençons par ce que nous avons vu à propos de l'encodeur. L'encodeur prend des mots en entrée, les

6
00:00:36,000 --> 00:00:40,640
transfère dans l'encodeur et récupère une représentation numérique

7
00:00:40,640 --> 00:00:47,360
pour chaque mot qui y est diffusé. Nous savons maintenant que la représentation numérique contient des informations

8
00:00:47,360 --> 00:00:54,000
sur la signification de la séquence. Laissons cela de côté et ajoutons le décodeur au schéma.

9
00:00:56,480 --> 00:01:00,160
Dans ce scénario, nous utilisons le décodeur d'une manière inédite.

10
00:01:00,720 --> 00:01:07,600
Nous lui transmettons directement les sorties de l'encodeur. En plus des sorties de l'encodeur,

11
00:01:07,600 --> 00:01:13,040
nous donnons également une séquence au décodeur. Lorsque vous demandez au décodeur une sortie sans

12
00:01:13,040 --> 00:01:17,360
séquence initiale, nous pouvons lui donner la valeur qui indique le début d'une séquence.

13
00:01:18,000 --> 00:01:23,520
Et c'est là que la magie de l'encodeur-décodeur opère. L'encodeur accepte une séquence en entrée.

14
00:01:24,560 --> 00:01:30,480
Il calcule une prédiction et génère une représentation numérique. Ensuite, il

15
00:01:30,480 --> 00:01:38,000
l'envoie au décodeur. Il a, en un sens, encodé la séquence. Et le décodeur, à son tour,

16
00:01:38,000 --> 00:01:42,960
utilisant cette entrée parallèlement à son entrée de séquence habituelle, tentera de décoder la séquence.

17
00:01:44,720 --> 00:01:50,400
Le décodeur décode la séquence et génère un mot. Pour l'instant, nous n'avons pas besoin de donner un sens à

18
00:01:50,400 --> 00:01:55,440
ce mot, mais nous pouvons comprendre que le décodeur décode essentiellement ce que l'encodeur a en

19
00:01:55,440 --> 00:02:02,160
sortie. Le mot de début de séquence indique qu'il doit commencer à décoder la séquence.

20
00:02:03,600 --> 00:02:10,240
Maintenant que nous avons à la fois le vecteur de caractéristiques et un mot généré initial, nous n'avons plus besoin de

21
00:02:10,240 --> 00:02:17,760
l'encodeur. Comme nous l'avons vu précédemment avec le décodeur, il peut agir de manière auto-régressive.

22
00:02:18,640 --> 00:02:24,960
Le mot qu'il vient de sortir peut maintenant être utilisé comme entrée. Ceci, en combinaison avec la

23
00:02:24,960 --> 00:02:30,800
représentation numérique générée par l'encodeur, peut maintenant être utilisé pour générer un deuxième mot.

24
00:02:33,200 --> 00:02:38,880
Veuillez noter que le premier mot est toujours là car le modèle le produit toujours. Cependant, il est

25
00:02:38,880 --> 00:02:45,120
grisé car nous n'en avons plus besoin. Nous pouvons continuer encore et encore, par exemple jusqu'à ce que le décodeur

26
00:02:45,120 --> 00:02:50,720
génère une valeur que nous considérons comme une valeur d'arrêt, comme un point, signifiant la fin d'une séquence.

27
00:02:53,440 --> 00:02:58,080
Ici, nous avons vu le mécanisme complet du transformer encodeur-décodeur. Reprenons-le une

28
00:02:58,080 --> 00:03:05,120
fois de plus. Nous avons une séquence initiale, qui est envoyée à l'encodeur. Cette sortie d'encodeur est ensuite

29
00:03:05,120 --> 00:03:12,240
envoyée au décodeur, pour qu'elle soit décodée. Bien que nous puissions désormais jeter l'encodeur après une seule utilisation,

30
00:03:12,240 --> 00:03:17,840
le décodeur sera utilisé plusieurs fois. Jusqu'à ce que nous ayons généré tous les mots dont nous avons besoin.

31
00:03:20,000 --> 00:03:25,120
Voyons un exemple concret avec la modélisation du langage pour la traduction, c'est-à-dire

32
00:03:25,120 --> 00:03:30,800
l'action de traduire une séquence. Ici, nous aimerions traduire cette séquence en anglais « Welcome

33
00:03:30,800 --> 00:03:38,400
to NYC » en français. Nous utilisons un transformer spécifiquement entraîné pour cette tâche. Nous utilisons

34
00:03:38,400 --> 00:03:43,520
l'encodeur pour créer une représentation de la phrase en anglais. Nous transmettons ceci

35
00:03:43,520 --> 00:03:48,880
au décodeur et, avec l'utilisation du mot de début de séquence, nous lui demandons de sortir le premier mot.

36
00:03:50,720 --> 00:03:52,960
Il produit Bienvenue, qui signifie « Welcome ».

37
00:03:55,280 --> 00:04:02,480
Nous utilisons ensuite « Bienvenue » comme séquence d'entrée pour le décodeur. Ceci, en plus du vecteur de caractéristiques,

38
00:04:04,320 --> 00:04:08,480
permet au décodeur de prédire le deuxième mot, « à », qui est « to » en anglais.

39
00:04:10,160 --> 00:04:14,400
Enfin, nous demandons au décodeur de prédire un troisième mot. Il prédit « NYC »,

40
00:04:14,400 --> 00:04:20,240
ce qui est, encore une fois, correct. Nous avons traduit la phrase ! Là où l'encodeur-décodeur

41
00:04:20,240 --> 00:04:24,880
brille vraiment, c'est que nous avons un encodeur et un décodeur qui souvent ne partagent pas les poids.

42
00:04:27,280 --> 00:04:31,440
Nous avons donc un bloc entier (l'encodeur) qui peut être entraîné pour comprendre la séquence,

43
00:04:31,440 --> 00:04:36,480
et extraire les informations pertinentes. Pour le scénario de traduction que nous avons vu précédemment, par

44
00:04:36,480 --> 00:04:44,160
exemple, cela signifierait analyser et comprendre ce qui a été dit en anglais, extraire des

45
00:04:44,160 --> 00:04:49,040
informations de cette langue et mettre tout cela dans un vecteur dense en informations.

46
00:04:50,880 --> 00:04:57,280
D'autre part, nous avons le décodeur, dont le seul but est de décoder la représentation numérique sortie par

47
00:04:57,280 --> 00:05:03,760
l'encodeur. Ce décodeur peut être spécialisé dans une langue complètement différente, ou même dans une modalité

48
00:05:03,760 --> 00:05:11,760
comme les images ou la parole. Les encodeurs-décodeurs sont spéciaux pour plusieurs raisons. Tout d'abord,

49
00:05:11,760 --> 00:05:17,040
ils sont capables de gérer des tâches de séquence-à-séquence, comme la traduction que nous venons de voir.

50
00:05:18,640 --> 00:05:23,880
Deuxièmement, les poids entre les parties encodeur et décodeur ne sont pas nécessairement partagés.

51
00:05:24,480 --> 00:05:31,200
Prenons un autre exemple de traduction. Ici, nous traduisons « Transformers are powerful » en français.

52
00:05:32,240 --> 00:05:36,560
Premièrement, cela signifie qu'à partir d'une séquence de trois mots, nous sommes capables de générer

53
00:05:36,560 --> 00:05:42,240
une séquence de quatre mots. On pourrait dire que cela pourrait être géré avec un décodeur

54
00:05:42,240 --> 00:05:46,960
qui générerait la traduction de manière autorégressive et ils auraient raison !

55
00:05:49,840 --> 00:05:53,840
Un autre exemple où les transformers de séquence-à-séquence brillent est le résumé de textes.

56
00:05:54,640 --> 00:05:58,560
Ici, nous avons une très longue séquence, généralement un texte intégral,

57
00:05:58,560 --> 00:06:03,840
et nous voulons la résumer. Comme l'encodeur et les décodeurs sont séparés,

58
00:06:03,840 --> 00:06:08,880
nous pouvons avoir des longueurs de contexte différentes (par exemple, un contexte très long pour l'encodeur qui

59
00:06:08,880 --> 00:06:13,840
gère le texte et un contexte plus petit pour le décodeur qui gère la séquence résumée).

60
00:06:16,240 --> 00:06:20,480
Il existe de nombreux modèles de séquence-à-séquence. Celui-ci contient quelques exemples de

61
00:06:20,480 --> 00:06:24,160
modèles encodeur-décodeur populaires disponibles dans la bibliothèque Transformers.

62
00:06:26,320 --> 00:06:31,200
De plus, vous pouvez charger un encodeur et un décodeur dans un

63
00:06:31,200 --> 00:06:35,040
modèle d'encodeur-décodeur ! Par conséquent, selon la tâche spécifique que vous ciblez,

64
00:06:35,040 --> 00:06:40,240
vous pouvez choisir d'utiliser des encodeurs et des décodeurs spécifiques, qui ont fait leurs preuves sur ces tâches spécifiques.