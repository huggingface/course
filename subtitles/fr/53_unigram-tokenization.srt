1
00:00:05,330 --> 00:00:11,090
Dans cette vidéo, nous étudierons ensemble l'algorithme de tokenisation en sous-mots du modèle de langage

2
00:00:11,090 --> 00:00:12,090
Unigram.

3
00:00:12,090 --> 00:00:20,080
La stratégie globale d'entraînement d'un tokenizer Unigram LM est de commencer avec un vocabulaire très large,

4
00:00:20,080 --> 00:00:27,439
puis de supprimer des tokens à chaque itération jusqu'à ce que nous atteignions la taille souhaitée.

5
00:00:27,439 --> 00:00:32,250
A chaque itération, nous calculerons une perte sur notre corpus d'apprentissage grâce au modèle Unigram

6
00:00:32,250 --> 00:00:33,250
.

7
00:00:33,250 --> 00:00:39,160
Comme le calcul de la perte dépend du vocabulaire disponible, nous pouvons l'utiliser pour choisir comment

8
00:00:39,160 --> 00:00:41,590
réduire le vocabulaire.

9
00:00:41,590 --> 00:00:48,090
On regarde donc l'évolution de la perte en supprimant tour à tour chaque token du vocabulaire.

10
00:00:48,090 --> 00:00:56,730
On choisira de supprimer les p pourcents qui augmentent le moins la perte.

11
00:00:56,730 --> 00:01:01,030
Avant d'aller plus loin dans l'explication de l'algorithme d'entraînement, je dois expliquer

12
00:01:01,030 --> 00:01:04,199
ce qu'est un modèle Unigram.

13
00:01:04,199 --> 00:01:08,119
Le modèle Unigram LM est un type de modèle linguistique statistique.

14
00:01:08,119 --> 00:01:15,550
Un LM statistique attribuera une probabilité à un texte en considérant que le texte est en

15
00:01:15,550 --> 00:01:18,189
fait une séquence de tokens.

16
00:01:18,189 --> 00:01:23,900
Les séquences de tokens les plus simples à imaginer sont les mots qui composent la phrase ou

17
00:01:23,900 --> 00:01:25,410
les caractères.

18
00:01:25,410 --> 00:01:32,080
La particularité de l'Unigram LM est qu'il suppose que l'occurrence de chaque mot est

19
00:01:32,080 --> 00:01:34,670
indépendante de son mot précédent.

20
00:01:34,670 --> 00:01:40,271
Cette hypothèse nous permet d'écrire que la probabilité d'un texte est égale au

21
00:01:40,271 --> 00:01:44,430
produit des probabilités des tokens qui le composent.

22
00:01:44,430 --> 00:01:51,880
Il faut noter ici qu'il s'agit d'un modèle très simple qui ne serait pas adapté à

23
00:01:51,880 --> 00:01:58,630
la génération de texte puisque ce modèle générerait toujours le même token, celui qui

24
00:01:58,630 --> 00:02:00,140
a la plus grande probabilité.

25
00:02:00,140 --> 00:02:07,409
Néanmoins, pour faire de la tokenisation, ce modèle nous est très utile car il peut être utilisé

26
00:02:07,409 --> 00:02:14,209
pour estimer la vraissemblance relative de différentes phrases.

27
00:02:14,209 --> 00:02:20,000
Nous sommes maintenant prêts à revenir à notre explication de l'entraînement de l'algorithme.

28
00:02:20,000 --> 00:02:25,349
Disons que nous avons comme corpus d'entraînement 10 fois le mot « hug », 12 fois le mot « pug »,

29
00:02:25,349 --> 00:02:33,270
5 fois le mot « lug », 4 fois « bug » et 5 fois « dug ».

30
00:02:33,270 --> 00:02:38,910
Comme dit plus tôt, l'entraînement commence par un gros vocabulaire.

31
00:02:38,910 --> 00:02:45,280
Évidemment, comme nous utilisons un corpus jouet, ce vocabulaire ne sera pas si gros mais il devrait

32
00:02:45,280 --> 00:02:46,840
vous montrer le principe.

33
00:02:46,840 --> 00:02:54,870
Une première méthode consiste à lister toutes les sous-chaînes strictes possibles, c'est ce que nous allons faire ici.

34
00:02:54,870 --> 00:03:00,379
On aurait aussi pu utiliser l'algorithme BPE avec une très grande taille de vocabulaire.

35
00:03:00,379 --> 00:03:07,200
Mais maintenant les sous-chaînes strictes suffisent.

36
00:03:07,200 --> 00:03:13,629
L'entraînement du tokenizer Unigram est basé sur la méthode Expectation-Maximisation : à

37
00:03:13,629 --> 00:03:15,210
chaque itération,

38
00:03:15,210 --> 00:03:19,190
nous estimons les probabilités des tokens du vocabulaire.

39
00:03:19,190 --> 00:03:26,430
Puis on enlève les p pourcents de tokens qui minimisent la perte sur le corpus et qui

40
00:03:26,430 --> 00:03:33,500
n'appartiennent pas aux caractères de base car on veut garder dans notre vocabulaire final les

41
00:03:33,500 --> 00:03:37,980
caractères de base pour pouvoir tokeniser n'importe quel mot.

42
00:03:37,980 --> 00:03:39,230
Allons y pour ça !

43
00:03:39,230 --> 00:03:44,660
La probabilité d'un token est simplement estimée par le nombre d'apparition de ce token

44
00:03:44,660 --> 00:03:51,590
dans notre corpus d'apprentissage divisé par le nombre total d'apparition de tous les tokens.

45
00:03:51,590 --> 00:03:57,239
Nous pourrions utiliser ce vocabulaire pour tokeniser nos mots selon le modèle Unigram.

46
00:03:57,239 --> 00:04:04,080
Nous allons le faire ensemble pour comprendre deux choses : comment on tokenise un mot avec un modèle Unigram

47
00:04:04,080 --> 00:04:09,160
et comment la perte est calculée sur notre corpus.

48
00:04:09,160 --> 00:04:14,610
La tokenisation Unigram LM de notre texte « hug » sera celle qui aura la probabilité d'occurrence la plus élevée

49
00:04:14,610 --> 00:04:19,140
selon notre modèle Unigram.

50
00:04:19,140 --> 00:04:24,090
Pour le trouver, la manière la plus simple de procéder serait de lister toutes les segmentations possibles

51
00:04:24,090 --> 00:04:29,949
de notre texte « hug », de calculer la probabilité de chacune de ces segmentations puis de

52
00:04:29,949 --> 00:04:32,490
choisir celle qui a la probabilité la plus élevée.

53
00:04:32,490 --> 00:04:38,630
Avec le vocabulaire actuel, 2 tokenisations obtiennent exactement la même probabilité.

54
00:04:38,630 --> 00:04:43,789
Nous en choisissons donc une et gardons en mémoire la probabilité associée.

55
00:04:43,789 --> 00:04:48,850
Pour calculer la perte sur notre corpus d'entraînement, nous devons tokeniser comme nous venons de le faire tous les

56
00:04:48,850 --> 00:04:52,810
mots restants dans le corpus.

57
00:04:52,810 --> 00:04:57,930
La perte est alors la somme sur tous les mots du corpus de la fréquence d'occurrence

58
00:04:57,930 --> 00:05:04,220
du mot multipliée par l'opposé du logarithme de la probabilité associée à

59
00:05:04,220 --> 00:05:07,720
la tokenisation du mot.

60
00:05:07,720 --> 00:05:12,700
On obtient ici une perte de 170.

61
00:05:12,700 --> 00:05:18,750
Rappelez-vous, notre objectif initial était de réduire le vocabulaire.

62
00:05:18,750 --> 00:05:27,810
Pour cela, nous allons retirer un token du vocabulaire et calculer la perte associée.

63
00:05:27,810 --> 00:05:32,020
Supprimons par exemple le token « ug ».

64
00:05:32,020 --> 00:05:38,569
On remarque que la tokenisation pour « hug » avec la lettre « h » et le tuple « ug » est désormais

65
00:05:38,569 --> 00:05:39,970
impossible.

66
00:05:39,970 --> 00:05:45,810
Néanmoins, comme nous l'avons vu précédemment, deux tokenisations avaient la même probabilité et nous pouvons toujours

67
00:05:45,810 --> 00:05:50,870
choisir la tokenisation restante avec une probabilité 1e-02.

68
00:05:50,870 --> 00:05:58,210
Les tokenisations des autres mots du vocabulaire restent également inchangées et finalement

69
00:05:58,210 --> 00:06:06,710
même si on enlève le token « ug » de notre vocabulaire la perte reste égale à 170.

70
00:06:06,710 --> 00:06:11,550
Pour cette première itération, si on continue le calcul, on s'apercevrait qu'on pourrait

71
00:06:11,550 --> 00:06:16,190
retirez n'importe quel token sans que cela n'affecte la perte.

72
00:06:16,190 --> 00:06:24,620
On choisira donc au hasard de retirer le token « ug » avant d'entamer une deuxième itération.

73
00:06:24,620 --> 00:06:29,600
Nous estimons à nouveau la probabilité de chaque token avant de calculer l'impact de chaque

74
00:06:29,600 --> 00:06:32,280
token sur la perte.

75
00:06:32,280 --> 00:06:37,840
Par exemple, si nous supprimons maintenant le token composé des lettres « h » et « u », il ne reste plus qu'une

76
00:06:37,840 --> 00:06:42,020
seule tokenisation possible pour « hug ».

77
00:06:42,020 --> 00:06:46,580
La tokenisation des autres mots du vocabulaire n'est pas modifiée.

78
00:06:46,580 --> 00:06:51,880
Au final, on obtient en supprimant du vocabulaire le token composé des lettres « h » et « u »

79
00:06:51,880 --> 00:06:54,650
une perte de 168.

80
00:06:54,650 --> 00:07:02,550
Enfin, pour choisir quel token supprimer, on va pour chaque token restant du vocabulaire

81
00:07:02,550 --> 00:07:10,090
qui n'est pas un token élémentaire calculer la perte associée puis comparer ces pertes

82
00:07:10,090 --> 00:07:11,850
entre elles.

83
00:07:11,850 --> 00:07:18,100
Le token que nous allons supprimer est le token qui impacte le moins la perte : ici le

84
00:07:18,100 --> 00:07:20,129
token « bu ».

85
00:07:20,129 --> 00:07:25,710
Nous avions mentionné au début de la vidéo qu'à chaque itération nous pouvions retirer p pourcent

86
00:07:25,710 --> 00:07:29,540
des tokens par itération.

87
00:07:29,540 --> 00:07:35,850
Le deuxième token qui pourrait être supprimé à cette itération est le token « du ».

88
00:07:35,850 --> 00:07:42,690
Et voilà, il ne nous reste plus qu'à répéter ces étapes jusqu'à obtenir le vocabulaire de la

89
00:07:42,690 --> 00:07:45,240
taille souhaitée.

90
00:07:45,240 --> 00:07:51,129
Une dernière chose. En pratique, quand on tokenise un mot avec un modèle Unigram on ne calcule pas

91
00:07:51,129 --> 00:07:57,210
l'ensemble des probabilités des découpages possibles d'un mot avant de les comparer pour garder le

92
00:07:57,210 --> 00:08:05,560
meilleur. Mais on utilise l'algorithme de Viterbi qui est beaucoup plus efficace.

93
00:08:05,560 --> 00:08:07,300
Et c'est tout!

94
00:08:07,300 --> 00:08:15,000
J'espère que cet exemple vous a permis de mieux comprendre l'

95
00:08:15,000 --> 00:08:18,190
algorithme de tokenisation Unigram.