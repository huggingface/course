1
00:00:05,549 --> 00:00:12,309
Le pipeline de tokenisation implique plusieurs étapes qui convertissent le texte brut en nombres.

2
00:00:12,309 --> 00:00:15,990
Dans cette vidéo, nous verrons ce qui se passe lors de l'étape de pré-tokénisation.

3
00:00:15,990 --> 00:00:23,840
L'opération de pré-tokénisation est l'opération effectuée après la normalisation du texte

4
00:00:23,840 --> 00:00:28,830
et avant l'application de l'algorithme de tokénisation.

5
00:00:28,830 --> 00:00:33,489
Cette étape consiste à appliquer des règles qui n'ont pas besoin d'être apprises pour effectuer une première

6
00:00:33,489 --> 00:00:38,270
division du texte.

7
00:00:38,270 --> 00:00:46,270
Regardons comment plusieurs tokenizers prétokenise cet exemple.

8
00:00:46,270 --> 00:00:53,430
La prétokénisation du GPT-2 divise le texte sur des espaces et quelques signes de ponctuation

9
00:00:53,430 --> 00:00:57,840
mais pas sur l'apostrophe.

10
00:00:57,840 --> 00:01:06,580
On remarque également que les espaces ont été remplacés par un G majuscule avec un point au-dessus.

11
00:01:06,580 --> 00:01:12,900
La prétokénisation d'Albert divise le texte au niveau des espaces, ajoute un espace au

12
00:01:12,900 --> 00:01:19,610
début de la phrase et remplace les espaces par un trait de soulignement spécial.

13
00:01:19,610 --> 00:01:29,320
Enfin, la prétokénisation de BERT divise le texte au niveau de la ponctuation et des espaces.

14
00:01:29,320 --> 00:01:35,460
Mais contrairement aux tokenizers précédents, les espaces ne sont pas transformés et intégrés aux tokens

15
00:01:35,460 --> 00:01:40,079
produits avec ce pretokenizer.

16
00:01:40,079 --> 00:01:45,860
A travers ces 3 exemples, nous avons pu observer les deux principaux types d'opérations apportées par

17
00:01:45,860 --> 00:01:54,210
la prétokénisation : quelques changements sur le texte et la division de la chaîne en

18
00:01:54,210 --> 00:01:57,259
tokens pouvant être associés à des mots.

19
00:01:57,259 --> 00:02:06,729
Enfin, le `backend_tokenizer` des tokenizers rapides permet également de tester

20
00:02:06,729 --> 00:02:12,739
très facilement l'opération de pr-tokenisation grâce à sa méthode `pre_tokenize_str`.

21
00:02:12,739 --> 00:02:18,740
On remarque que la sortie de cette opération est composée à la fois de tokens et d'offsets qui

22
00:02:18,740 --> 00:02:24,830
permettent de lier le token à sa position dans le texte donné en entrée de la méthode.

23
00:02:24,830 --> 00:02:32,269
Cette opération définit les plus gros tokens pouvant être produits par la tokenisation

24
00:02:32,269 --> 00:02:40,000
ou autrement dit les barrières des sous-tokens qui seront alors produits.

25
00:02:40,000 --> 00:02:42,269 
Et c'est tou pour les caractéristiques clés des prétokenisers.