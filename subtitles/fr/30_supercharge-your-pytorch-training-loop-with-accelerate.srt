1
00:00:05,360 --> 00:00:08,800
Boostez votre boucle d'entraînement Pytorch avec Accelerate d'Hugging Face.

2
00:00:11,120 --> 00:00:17,040
Il existe plusieurs configurations sur lesquelles vous pouvez exécuter votre entraînement : il peut s'agir de CPU, de GPU ou de TPU.

3
00:00:17,680 --> 00:00:22,640
Distribué sur une machine avec plusieurs appareils, ou sur plusieurs machines (souvent appelées

4
00:00:22,640 --> 00:00:29,360
nœuds), chacune avec plusieurs appareils. En plus de cela, il existe de nouveaux ajustements pour rendre votre entraînement plus rapide

5
00:00:29,360 --> 00:00:35,680
ou plus efficace en termes de mémoire, comme la précision mixte et DeepSpeed. Chacune de ces configurations ou

6
00:00:35,680 --> 00:00:40,080
modifications d'entraînement vous oblige à modifier le code de votre boucle d'entraînement d'une manière ou d'une autre

7
00:00:40,080 --> 00:00:46,480
et à apprendre une nouvelle API. Toutes ces configurations sont gérées par l'API Trainer, et plusieurs

8
00:00:46,480 --> 00:00:51,440
bibliothèques tierces peuvent également vous aider. Le problème avec ceux-ci est qu'ils peuvent ressembler

9
00:00:51,440 --> 00:00:56,320
à une boîte noire et qu'il n'est peut-être pas facile de mettre en œuvre le réglage de la boucle d'entraînement dont vous avez besoin.

10
00:00:57,680 --> 00:01:02,000
Accelerate a été spécialement conçu pour vous permettre de garder le contrôle total de votre boucle d'entraînement

11
00:01:02,560 --> 00:01:08,000
et d'être aussi discret que possible. Avec seulement quatre lignes à ajouter à votre boucle d'entraînement

12
00:01:08,640 --> 00:01:11,840
(affichées ici sur le code de la boucle d'entraînement de la vidéo « Boucle d'entraînement »),

13
00:01:12,480 --> 00:01:16,800
Accelerate gérera toutes les configurations et les ajustements d'entraînement mentionnés sur la première diapositive.

14
00:01:18,320 --> 00:01:21,600
Il n'y a qu'une seule API à apprendre et à maîtriser au lieu de dix API différentes.

15
00:01:23,120 --> 00:01:27,120
Plus précisément, vous devez importer et instancier un objet `Accelerator`,

16
00:01:27,120 --> 00:01:30,000
qui gérera tout le code nécessaire pour votre configuration spécifique.

17
00:01:31,200 --> 00:01:36,880
Ensuite, vous devez lui envoyer le modèle, l'optimiseur et les chargeurs de données que vous utilisez dans la méthode de préparation,

18
00:01:37,760 --> 00:01:43,600
qui est la principale méthode à retenir. Accelerate gère le placement de l'appareil, vous n'avez donc pas besoin de placer

19
00:01:43,600 --> 00:01:49,840
votre batch sur l'appareil spécifique que vous utilisez. Enfin, vous devez remplacer la

20
00:01:49,840 --> 00:01:54,880
ligne `loss.backward` par `accelerator.backward(loss)`, et c'est tout ce dont vous avez besoin !

21
00:01:58,240 --> 00:02:00,480
Accelerate gère également l'évaluation distribuée.

22
00:02:01,440 --> 00:02:05,280
Vous pouvez toujours utiliser une boucle d'évaluation classique telle que celle que nous avons vue dans la vidéo « Boucle

23
00:02:05,280 --> 00:02:09,760
d'entraînement », auquel cas tous les processus effectueront chacun l'évaluation complète.

24
00:02:11,040 --> 00:02:15,360
Pour utiliser une évaluation distribuée, il vous suffit d'adapter votre boucle d'évaluation comme suit :

25
00:02:16,080 --> 00:02:19,920
transmettre le chargeur de données d'évaluation à la méthode `accelerator.prepare`,

26
00:02:19,920 --> 00:02:25,200
comme pour l'entraînement. Ensuite, vous pouvez ignorer la ligne qui place le lot sur le bon appareil,

27
00:02:25,920 --> 00:02:28,800
et juste avant de transmettre vos prédictions et étiquettes à votre métrique,

28
00:02:29,440 --> 00:02:36,160
utilisez `accelerator.gather` pour rassembler les prédictions et les étiquettes de chaque processus.

29
00:02:36,160 --> 00:02:41,440
Un script d'entraînement distribué doit être lancé plusieurs fois sur différents processus (par exemple

30
00:02:41,440 --> 00:02:47,360
un par GPU que vous utilisez). Vous pouvez utiliser les outils PyTorch si vous les connaissez,

31
00:02:48,000 --> 00:02:51,760
mais Accelerate fournit également une API simple pour configurer votre configuration

32
00:02:51,760 --> 00:02:58,000
et lancer votre script d'entraînement. Dans un terminal, exécutez la configuration accélérée et répondez au petit

33
00:02:58,000 --> 00:03:01,680
questionnaire pour générer un fichier de configuration avec toutes les informations pertinentes.

34
00:03:03,120 --> 00:03:07,360
Puis vous pouvez simplement exécuter `accelerate launch`, suivi du chemin d'accès à votre script d'entraînement.

35
00:03:08,400 --> 00:03:19,840
Dans un notebook, vous pouvez utiliser la fonction `notebook_launcher` pour lancer votre fonction d'entraînement.