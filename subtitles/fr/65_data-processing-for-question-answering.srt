1
00:00:05,569 --> 00:00:10,490
Étudions comment prétraiter un jeu de données pour répondre aux questions !

2
00:00:10,490 --> 00:00:14,260
La réponse aux questions consiste à trouver des réponses à une question dans un certain contexte.

3
00:00:14,260 --> 00:00:19,970
Pour notre exemple, nous utiliserons le jeu de données SQUAD, dans lequel nous supprimons les colonnes que nous n'utiliserons pas et

4
00:00:19,970 --> 00:00:24,390
extrairons simplement les informations dont nous aurons besoin pour les étiquettes : le début et la fin de la

5
00:00:24,390 --> 00:00:25,390
réponse dans le contexte.

6
00:00:25,390 --> 00:00:30,279
Si vous avez votre propre jeu de données pour la réponse aux questions, assurez-vous simplement de nettoyer vos données

7
00:00:30,279 --> 00:00:34,800
pour arriver au même point, avec une colonne contenant les questions, une colonne contenant

8
00:00:34,800 --> 00:00:39,350
les contextes, une colonne pour l'index du caractère de début et de fin du répondre

9
00:00:39,350 --> 00:00:41,700
dans le contexte.

10
00:00:41,700 --> 00:00:44,610
Notez que la réponse doit faire partie du contexte.

11
00:00:44,610 --> 00:00:48,360
Si vous souhaitez effectuer une réponse générative aux questions, regardez l'une des vidéos

12
00:00:48,360 --> 00:00:50,890
sur les séquences à séquences dans les liens ci-dessous.

13
00:00:50,890 --> 00:00:55,860
Maintenant, si nous examinons les tokens que nous donnons à notre modèle, nous verrons que la réponse se trouve

14
00:00:55,860 --> 00:00:58,450
quelque part dans le contexte.

15
00:00:58,450 --> 00:01:02,239
Pour un contexte très long, cette réponse peut être tronquée par le tokenizer.

16
00:01:02,239 --> 00:01:06,050
Dans ce cas, nous n'aurons pas d'étiquettes appropriées pour notre modèle.

17
00:01:06,050 --> 00:01:11,159
Nous devons donc conserver la partie tronquée en tant qu'entité distincte au lieu de la supprimer.

18
00:01:11,159 --> 00:01:14,720
La seule chose à laquelle nous devons faire attention est de permettre un certain chevauchement entre des

19
00:01:14,720 --> 00:01:19,900
morceaux séparés afin que la réponse ne soit pas tronquée et que l'entité contenant la réponse

20
00:01:19,900 --> 00:01:22,670
obtienne suffisamment de contexte pour pouvoir la prédire.

21
00:01:22,670 --> 00:01:28,790
Voici comment cela peut être fait par le tokenizer : nous lui passons la question, le contexte, définissons la

22
00:01:28,790 --> 00:01:32,750
troncature pour le contexte uniquement et le rembourrage à la longueur maximale.

23
00:01:32,750 --> 00:01:39,590
L'argument `stride` est l'endroit où nous définissons le nombre de tokens qui se chevauchent, et le

24
00:01:39,590 --> 00:01:42,869
`return_overflowing_tokens=True` signifie que nous ne voulons pas supprimer la partie tronquée.

25
00:01:42,869 --> 00:01:47,140
Enfin, nous renvoyons également les correspondances d'offset pour pouvoir trouver les tokens correspondant

26
00:01:47,140 --> 00:01:48,649
au début et à la fin de la réponse.

27
00:01:48,649 --> 00:01:53,990
Nous voulons ces deux tokens car il y aura les étiquettes que nous transmettrons à notre modèle.

28
00:01:53,990 --> 00:01:57,200
Dans une version en base canonique, voici à quoi ils ressemblent.

29
00:01:57,200 --> 00:02:02,119
Si le contexte que nous avons ne contient pas la réponse, nous définissons les deux étiquettes sur l'index

30
00:02:02,119 --> 00:02:04,329
du token [CLS].

31
00:02:04,329 --> 00:02:08,629
Nous le faisons également si le contexte ne contient que partiellement la réponse.

32
00:02:08,629 --> 00:02:13,950
En termes de code, voici comment nous pouvons le faire : en utilisant les `séquence_ids` d'une entrée, nous pouvons

33
00:02:13,950 --> 00:02:17,390
déterminer le début et la fin du contexte.

34
00:02:17,390 --> 00:02:22,290
Ensuite, nous savons s'il faut retourner la position [CLS] pour les deux étiquettes ou nous déterminons les positions

35
00:02:22,290 --> 00:02:25,120
des premier et dernier tokens de la réponse.

36
00:02:25,120 --> 00:02:28,670
Nous pouvons vérifier que cela fonctionne correctement sur notre exemple précédent.

37
00:02:28,670 --> 00:02:35,319
Tout mettre ensemble ressemble à cette grande fonction, que nous pouvons appliquer à nos jeux de données avec la méthode `map`.

38
00:02:35,319 --> 00:02:40,010
Puisque nous avons appliqué le rembourrage lors de la tokenisation, nous pouvons ensuite l'utiliser directement dans le Trainer.
39
00:02:40,010 --> 00:02:43,920
ou appliquer la méthode `to_tf_dataset` pour utiliser `Keras.fit`.