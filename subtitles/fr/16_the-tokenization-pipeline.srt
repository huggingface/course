1
00:00:05,440 --> 00:00:12,320
Le pipeline de tokenisation. Dans cette vidéo, nous verrons comment un tokenizer convertit le texte brut en nombres

2
00:00:12,320 --> 00:00:18,080
pour qu'un transformer puisse y donner un sens, comme lorsque nous exécutons ce code. Voici un bref

3
00:00:18,080 --> 00:00:24,400
aperçu de ce qui se passe à l'intérieur de l'objet tokenizer. Le texte est d'abord divisé en tokens, qui

4
00:00:24,400 --> 00:00:31,280
sont des mots, des parties de mots ou des symboles de ponctuation. Ensuite, le tokenizer ajoute des tokens spéciaux potentiels

5
00:00:31,280 --> 00:00:36,560
et convertit chaque token en leur ID unique respectif, tel que défini par le vocabulaire du tokenizer.

6
00:00:37,520 --> 00:00:41,440
Comme nous le verrons, cela ne se produit pas réellement dans cet ordre, mais le voir ainsi

7
00:00:41,440 --> 00:00:46,320
est préférable pour comprendre ce qui se passe. La première étape consiste à diviser notre texte d'entrée

8
00:00:46,320 --> 00:00:53,840
en tokens avec la méthode `tokenize`. Pour ce faire, le tokenizer peut d'abord effectuer certaines opérations

9
00:00:53,840 --> 00:00:58,000
comme la mise en minuscules de tous les mots, puis suivre un ensemble de règles pour diviser le résultat en petits

10
00:00:58,000 --> 00:01:03,520
morceaux de texte. La plupart des transformers utilisent un algorithme de tokenisation en sous-mots,

11
00:01:04,160 --> 00:01:08,720
ce qui signifie qu'un mot donné peut être divisé en plusieurs tokens, comme « tokenize »

12
00:01:08,720 --> 00:01:13,360
ici. Regardez les vidéos sur les algorithmes de tokenisation disponibles en description pour plus d'informations !

13
00:01:14,480 --> 00:01:19,600
Le préfixe « ## » que nous voyons devant « ize » est la convention utilisée par BERT pour indiquer que

14
00:01:19,600 --> 00:01:26,080
ce token n'est pas le début d'un mot. Cependant, d'autres tokenizers peuvent utiliser des conventions différentes :

15
00:01:26,080 --> 00:01:31,040
par exemple, le tokenizer d'ALBERT ajoutera un « __ » devant tous les

16
00:01:31,040 --> 00:01:36,640
tokens précédés d'un espace. C'est une convention utilisée par les tokenizers de phrases.

17
00:01:38,320 --> 00:01:43,280
La deuxième étape du pipeline de tokenisation consiste à associer ces tokens à leurs identifiants respectifs

18
00:01:43,280 --> 00:01:48,960
tels que définis par le vocabulaire du tokenizer. C'est pourquoi nous devons télécharger un fichier lorsque nous

19
00:01:48,960 --> 00:01:53,600
instancions un tokenizer avec la méthode `from_pretrained` : nous devons nous assurer que nous utilisons la même

20
00:01:53,600 --> 00:01:59,520
association que lorsque le modèle a été pré-entraîné. Pour ce faire, nous utilisons la méthode `convert_tokens_to_ids`.

21
00:02:00,720 --> 00:02:05,360
Vous avez peut-être remarqué que nous n'obtenons pas exactement le même résultat que dans notre première diapositive. Ou pas,

22
00:02:05,360 --> 00:02:09,840
car cela ressemble à une liste de nombres aléatoires et dans ce cas, permettez-moi de vous rafraîchir la mémoire.

23
00:02:10,480 --> 00:02:13,680
Nous avions un numéro au début et à la fin qui sont ici absents.

24
00:02:14,400 --> 00:02:20,160
Ce sont les tokens spéciaux. Les tokens spéciaux sont ajoutés par la `méthode prepare_for_model`,

25
00:02:20,160 --> 00:02:25,280
qui connaît les indices de ces tokens dans le vocabulaire et ajoute simplement les numéros appropriés.

26
00:02:28,320 --> 00:02:32,480
Vous pouvez examiner les tokens spéciaux (et plus généralement la façon dont le tokenizer a modifié

27
00:02:32,480 --> 00:02:37,120
votre texte) en utilisant la méthode `decode` sur les sorties de l'objet `tokenizer`.

28
00:02:38,240 --> 00:02:44,080
En ce qui concerne le préfixe pour le début des mots/parties de mots, ces tokens spéciaux varient en fonction

29
00:02:44,080 --> 00:02:50,080
du tokenizer que vous utilisez. Le tokenizer de BERT utilise [CLS] et [SEP] mais le tokenizer de RoBERTa

30
00:02:50,080 --> 00:02:57,520
utilise des ancres de type html <s> et </s>. Maintenant que vous savez comment fonctionne le tokenizer, vous pouvez oublier

31
00:02:57,520 --> 00:03:02,560
toutes ces méthodes intermédiaires et n'oubliez pas qu'il vous suffit de l'appeler sur vos textes d'entrée.

32
00:03:03,600 --> 00:03:06,880
Cependant, les entrées ne contiennent pas les ID des entrées.

33
00:03:07,520 --> 00:03:11,600
Pour savoir ce qu'est le masque d'attention, consultez la vidéo « Regroupement des entrées ».

34
00:03:12,160 --> 00:03:17,840
Pour en savoir plus sur les token de type ID de type de token, regardez la vidéo « Comment prétraiter des paires de phrases ».