1
00:00:04,560 --> 00:00:06,640
Bienvenue dans la série de tâches Hugging Face !

2
00:00:07,200 --> 00:00:10,400
Dans cette vidéo, nous allons jeter un œil
à la modélisation du langage causal.

3
00:00:13,600 --> 00:00:16,880
La modélisation du langage causal consiste à
prédire le

4
00:00:16,880 --> 00:00:21,920
mot suivant dans une phrase, compte tenu de tous les
mots précédents.  Cette tâche est très

5
00:00:21,920 --> 00:00:29,920
similaire à la fonction de correction automatique
que vous pourriez avoir sur votre téléphone.

6
00:00:29,920 --> 00:00:34,720
Ces modèles prennent une séquence à
compléter et génèrent la séquence complète.

7
00:00:38,640 --> 00:00:44,160
Les statistiques de classification ne peuvent pas être utilisées, car il n'y a
pas de réponse correcte unique pour l'achèvement.

8
00:00:44,960 --> 00:00:49,280
Au lieu de cela, nous évaluons la distribution
du texte complété par le modèle.

9
00:00:50,800 --> 00:00:55,440
Une mesure courante pour ce faire est la
perte d'entropie croisée.  La perplexité est

10
00:00:55,440 --> 00:01:01,280
également une mesure largement utilisée et elle est calculée
comme l'exponentielle de la perte d'entropie croisée.

11
00:01:05,200 --> 00:01:11,840
Vous pouvez utiliser n'importe quel ensemble de données avec du texte brut
et segmenter le texte pour préparer les données.

12
00:01:15,040 --> 00:01:18,240
Les modèles de langage causal peuvent
être utilisés pour générer du code.

13
00:01:22,480 --> 00:01:33,200
Pour plus d'informations sur la
tâche Modélisation du langage causal, consultez le cours Hugging Face.
