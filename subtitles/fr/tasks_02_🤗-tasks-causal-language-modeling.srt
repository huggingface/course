1
00:00:04,560 --> 00:00:06,640
Bienvenue dans la série d'Hugging Face sur les tâches ! 

2
00:00:07,200 --> 00:00:10,400
Dans cette vidéo, nous allons jeter un œil à la modélisation du langage causal.

3
00:00:13,600 --> 00:00:16,880
La modélisation du langage causal consiste à prédire le

4
00:00:16,880 --> 00:00:21,920
mot suivant dans une phrase, compte tenu de tous les mots précédents. Cette tâche est très

5
00:00:21,920 --> 00:00:29,920
similaire à la fonction de correction automatique que vous pourriez avoir sur votre téléphone.

6
00:00:29,920 --> 00:00:34,720
Ces modèles prennent une séquence à compléter et génèrent la séquence complète.

7
00:00:38,640 --> 00:00:44,160
Les métriques de classification ne peuvent pas être utilisées, car il n'y a pas de réponse correcte unique pour la complétion.

8
00:00:44,960 --> 00:00:49,280
Au lieu de cela, nous évaluons la distribution du texte complété par le modèle.

9
00:00:50,800 --> 00:00:55,440
Une métrique courante pour ce faire est la perte d'entropie croisée. La perplexité est

10
00:00:55,440 --> 00:01:01,280
aussi une métrique largement utilisée et elle est calculée comme l'exponentielle de la perte d'entropie croisée.

11
00:01:05,200 --> 00:01:11,840
Vous pouvez utiliser n'importe quel jeu de données avec du texte brut et tokeniser le texte pour préparer les données.

12
00:01:15,040 --> 00:01:18,240
Les modèles de langage causal peuvent être utilisés pour générer du code.

13
00:01:22,480 --> 00:01:33,200
Pour plus d'informations sur la tâche de modélisation du langage causal, consultez le cours d'Hugging Face.
