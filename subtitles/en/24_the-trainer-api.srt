1
00:00:05,280 --> 00:00:11,200
The Trainer API. The Transformers library 
provides a Trainer API that allows you to  

2
00:00:11,200 --> 00:00:17,040
easily fine-tune transformer models on your own 
dataset. The Trainer class take your datasets,  

3
00:00:17,040 --> 00:00:22,240
your model as well as the training hyperparameters 
and can perform the training on any kind of  

4
00:00:22,240 --> 00:00:30,160
setup (CPU, GPU, multi GPUs, TPUs). It can also 
compute the predictions on any dataset, and if  

5
00:00:30,160 --> 00:00:36,720
you provided metrics, evaluate your model on any 
dataset. It can also handle final data-processing  

6
00:00:36,720 --> 00:00:41,760
such as dynamic padding as long as you provide 
the tokenizer or a given data collator.  

7
00:00:43,040 --> 00:00:48,160
We will try this API on the MRPC dataset, since 
it's relatively small and easy to preprocess.  

8
00:00:49,520 --> 00:00:54,800
As we saw in the Datasets overview video, here 
is how we can preprocess it. We do not apply  

9
00:00:54,800 --> 00:00:59,840
padding during the preprocessing as we will use 
dynamic padding with our DataCollatorWithPadding.  

10
00:01:00,960 --> 00:01:05,440
Note that we don't do the final steps of 
renaming/removing columns or set the format  

11
00:01:05,440 --> 00:01:11,280
to torch tensors: the Trainer will do all of 
this automatically for us by analyzing the  

12
00:01:11,280 --> 00:01:18,080
model signature. The last steps before creating 
the Trainer are to define our model and some  

13
00:01:18,080 --> 00:01:24,400
training hyperparameters. We saw how to do the 
first in the model API video. For the second,  

14
00:01:24,400 --> 00:01:29,600
we use the TrainingArguments class. It only needs 
a path to a folder where results and checkpoints  

15
00:01:29,600 --> 00:01:34,240
will be saved, but you can also customize 
all the hyperparameters the Trainer will use:  

16
00:01:34,240 --> 00:01:39,600
learning rate, number of training epochs etc. 
It's then very easy to create a Trainer and  

17
00:01:39,600 --> 00:01:44,720
launch a training. This should display a progress 
bar and after a few minutes (if you are running  

18
00:01:44,720 --> 00:01:50,480
on a GPU) you should have the training finished. 
The result will be rather anticlimatic however,  

19
00:01:50,480 --> 00:01:54,880
as you will only get a training loss which 
doesn't really tell you anything about how you  

20
00:01:54,880 --> 00:01:59,920
model is performing. This is because we didn't 
specify anything metric for the evaluation.  

21
00:02:00,960 --> 00:02:05,520
To get those metrics, we will first gather the 
predictions on the whole evaluation set using the  

22
00:02:05,520 --> 00:02:11,760
predict method. It returns a namedtuple with three 
fields: predictions (which contains the model  

23
00:02:11,760 --> 00:02:17,760
predictions), label_ids (which contains the labels 
if your dataset had them) and metrics (which is  

24
00:02:17,760 --> 00:02:24,480
empty here). The predictions are the logits of 
the models for all the sentences in the dataset,  

25
00:02:24,480 --> 00:02:31,440
so a NumPy array of shape 408 by 2. To match them 
with our labels, we need to take the maximum logit  

26
00:02:31,440 --> 00:02:36,560
for each prediction (to know which of the two 
classes was predicted), which we do with the  

27
00:02:36,560 --> 00:02:42,480
argmax function. Then we can use a Metric from 
the Datasets library: it can be loaded as easily  

28
00:02:42,480 --> 00:02:47,200
as our dataset with the load_metric function, 
and it returns the evaluation metric used for  

29
00:02:47,200 --> 00:02:54,080
the dataser we are using. We can see our model 
did learn something as it is 85.7% accurate.  

30
00:02:55,200 --> 00:02:59,920
To monitor the evaluation metrics during training 
we need to define a compute_metrics function  

31
00:02:59,920 --> 00:03:05,200
that does the same step as before: it takes 
a namedtuple with predictions and labels  

32
00:03:05,200 --> 00:03:08,000
and must return a dictionary with 
the metric we want to keep track of.  

33
00:03:09,120 --> 00:03:14,400
By passing the epoch evaluation strategy to our 
TrainingArguments, we tell the Trainer to evaluate  

34
00:03:14,400 --> 00:03:20,400
at the end of every epoch. Launching a training 
inside a notebook will then display a progress bar  

35
00:03:20,400 --> 00:03:29,920
and complete the table you see 
here as you pass every epoch.
