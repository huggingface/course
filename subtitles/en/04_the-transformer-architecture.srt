1
00:00:04,960 --> 00:00:07,120
Let's study the transformer architecture.  

2
00:00:08,960 --> 00:00:13,840
This video is the introductory video to 
the encoders, decoders, and encoder-decoder  

3
00:00:13,840 --> 00:00:18,640
series of videos. In this series, we'll try to 
understand what makes a Transformer network,  

4
00:00:18,640 --> 00:00:24,720
and we'll try to explain it in simple, high-level 
terms. No understanding of neural networks is  

5
00:00:24,720 --> 00:00:29,840
necessary, only an understanding of 
basic vectors and tensors may help.  

6
00:00:32,320 --> 00:00:36,480
To get started, we'll take up this diagram 
from the original transformer paper,  

7
00:00:36,480 --> 00:00:42,640
entitled "Attention is all you need". As we'll 
see here we can leverage only some parts of it,  

8
00:00:42,640 --> 00:00:48,080
according to what we're trying to do. We won't 
dive into the specific layers building up that  

9
00:00:48,080 --> 00:00:52,560
architecture, but we'll try to understand the 
different ways this architecture can be used.  

10
00:00:54,960 --> 00:00:59,760
Let's first start by splitting that architecture 
into two parts. On the left we have the encoder,  

11
00:00:59,760 --> 00:01:04,320
and on the right, the decoder. These two can 
be used together, but they can also be used  

12
00:01:04,320 --> 00:01:11,280
independently! Let's understand how these work: 
The encoder accepts inputs that represent text.  

13
00:01:11,280 --> 00:01:17,200
It converts this text, these words, into numerical 
representations. These numerical representations  

14
00:01:17,200 --> 00:01:23,120
can also be called embeddings, or features. We'll 
see that it uses the self-attention mechanism as  

15
00:01:23,120 --> 00:01:29,840
its main component. We recommend you check out the 
video on encoders especially to understand what is  

16
00:01:29,840 --> 00:01:36,640
this numerical representation, as well as how it 
works. We'll study the self-attention mechanism as  

17
00:01:36,640 --> 00:01:44,000
well as its bi-directional properties. The decoder 
is similar to the encoder: it can also accept  

18
00:01:44,000 --> 00:01:47,200
the same inputs as the encoder: inputs that 
represent text. It uses a similar mechanism as  

19
00:01:47,200 --> 00:01:53,200
the encoder, which is the masked self-attention 
as well. It differs from the encoder due to its  

20
00:01:53,200 --> 00:01:59,200
uni-directional property, and is traditionally 
used in an auto-regressive manner. Here too,  

21
00:01:59,200 --> 00:02:03,600
we recommend you check out the video on decoders 
especially to understand how all of this works.  

22
00:02:06,560 --> 00:02:11,120
Combining the two parts results in what is known 
as an encoder-decoder, or a sequence-to-sequence  

23
00:02:11,120 --> 00:02:16,640
transformer. The encoder accepts inputs and 
computes a high-level representation of those  

24
00:02:16,640 --> 00:02:22,640
inputs. These outputs are then passed to the 
decoder. The decoder uses the encoder's output  

25
00:02:22,640 --> 00:02:27,680
alongside other inputs, in order to generate 
a prediction. It then predicts an output,  

26
00:02:27,680 --> 00:02:32,000
which it will re-use in future iterations, 
hence the term "auto-regressive".  

27
00:02:33,040 --> 00:02:36,480
Finally, to get an understanding 
of the encoder-decoders as a whole,  

28
00:02:36,480 --> 00:02:44,080
we recommend you check out 
the video on encoder-decoders.
