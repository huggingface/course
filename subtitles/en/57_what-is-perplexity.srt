1
00:00:05,280 --> 00:00:09,200
In this video we take a look at the 
mysterious sounding metric called Perplexity.  

2
00:00:10,880 --> 00:00:14,880
You might have encountered perplexity 
when reading about generative models.  

3
00:00:14,880 --> 00:00:19,760
You can see two examples here from the original 
transformer paper “Attention is all you need”  

4
00:00:19,760 --> 00:00:25,600
as well as the more recent GPT-2 paper. Perplexity 
is a common metric to measure the performance  

5
00:00:25,600 --> 00:00:30,880
of language models. The smaller the value the 
better the performance. But what does it actually  

6
00:00:30,880 --> 00:00:36,880
mean and how can we calculate it? A very common 
quantity in machine learning is the likelihood.  

7
00:00:37,440 --> 00:00:41,280
We can calculate the likelihood as the 
product of each token’s probability  

8
00:00:42,160 --> 00:00:47,200
What this means is that for each token we use 
the language model to predict its probability  

9
00:00:47,200 --> 00:00:52,960
based on the previous tokens. In the end we 
multiply all probabilities to get the Likelihood.  

10
00:00:55,680 --> 00:00:59,120
With the likelihood we can calculate 
another important quantity:  

11
00:00:59,120 --> 00:01:04,560
the cross entropy. You might already have heard 
about cross-entropy when looking at loss function.  

12
00:01:05,440 --> 00:01:08,480
Cross-entropy is often used as a 
loss function in classification.  

13
00:01:09,040 --> 00:01:14,720
In language modeling we predict the next 
token which also is a classification task.  

14
00:01:15,600 --> 00:01:20,400
Therefore, if we want to calculate the cross 
entropy of an example we can simply pass it to the  

15
00:01:20,400 --> 00:01:25,840
model with the inputs as labels. The loss return 
by the model then corresponds the cross entropy.  

16
00:01:28,880 --> 00:01:32,640
We are now only a single operation 
away from calculating the perplexity.  

17
00:01:33,280 --> 00:01:39,360
By exponentiating the cross-entropy we get the 
perplexity. So you see that the perplexity is  

18
00:01:39,360 --> 00:01:55,040
closely related to the loss. Keep in mind that 
the loss is only a weak proxy for a model’s  

19
00:01:55,040 --> 00:02:01,600
ability to generate quality text and the same is 
true for perplexity. For this reason one usually  

20
00:02:01,600 --> 00:02:07,840
also calculates more sophisticated metrics 
such as BLEU or ROUGE on generative tasks.
