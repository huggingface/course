1
00:00:05,250 --> 00:00:09,840
Let's have a look inside the token classification
pipeline.

2
00:00:09,840 --> 00:00:14,690
In the pipeline video, we looked at the different
applications the Transformers library supports

3
00:00:14,690 --> 00:00:20,820
out of the box, one of them being token classification,
for instance predicting for each word in a

4
00:00:20,820 --> 00:00:27,760
sentence whether they correspond to a person,
an organization or a location.

5
00:00:27,760 --> 00:00:32,660
We can even group together the tokens corresponding
to the same entity, for instance all the tokens

6
00:00:32,660 --> 00:00:37,950
that formed the word Sylvain here, or Hugging
and Face.

7
00:00:37,950 --> 00:00:42,480
The token classification pipeline works the
same way as the text classification pipeline

8
00:00:42,480 --> 00:00:44,379
we studied in a previous video.

9
00:00:44,379 --> 00:00:49,600
There are three steps: the tokenization, the
model, and the post processing.

10
00:00:49,600 --> 00:00:56,340
The first two steps are identical to the text
classification pipeline, except we use an

11
00:00:56,340 --> 00:01:01,640
auto token classification model instead of
a sequence classification one.

12
00:01:01,640 --> 00:01:05,840
We tokenize our text then feed it to the model.

13
00:01:05,840 --> 00:01:10,400
Instead of getting one number for each possible
label for the whole sentence, we get one number

14
00:01:10,400 --> 00:01:16,690
for each of the possible 9 labels for every
token in the sentence, here 19.

15
00:01:16,690 --> 00:01:22,299
Like all the other models of the Transformers
library, our model outputs logits, which we

16
00:01:22,299 --> 00:01:25,900
turn into predictions by using a SoftMax.

17
00:01:25,900 --> 00:01:31,430
We also get the predicted label for each token
by taking the maximum prediction (since the

18
00:01:31,430 --> 00:01:35,470
softmax function preserves the order, we could
have done it on the logits if we had no need

19
00:01:35,470 --> 00:01:37,759
of the predictions).

20
00:01:37,759 --> 00:01:42,340
The model config contains the label mapping
in its id2label field.

21
00:01:42,340 --> 00:01:45,331
Using it, we can map every token to its corresponding
label.

22
00:01:45,331 --> 00:01:50,490
The label O correspond to "no entity", which
is why we didn't see it in our results in

23
00:01:50,490 --> 00:01:51,579
the first slide.

24
00:01:51,579 --> 00:01:57,430
On top of the label and the probability, those
results included the start and end character

25
00:01:57,430 --> 00:01:58,570
in the sentence.

26
00:01:58,570 --> 00:02:02,610
We will need to use the offset mapping of
the tokenizer to get those (look at the video

27
00:02:02,610 --> 00:02:04,820
linked below if you don't know about them
already).

28
00:02:04,820 --> 00:02:09,750
Then, looping through each token that has
a label distinct from O, we can build the

29
00:02:09,750 --> 00:02:12,440
list of results we got with our first pipeline.

30
00:02:12,440 --> 00:02:18,920
The last step is to group together tokens
that correspond to the same entity.

31
00:02:18,920 --> 00:02:23,290
This is why we had two labels for each type
of entity: I-PER and B-PER for instance.

32
00:02:23,290 --> 00:02:29,190
It allows us to know if a token is in the
same entity as the previous one.() Note that

33
00:02:29,190 --> 00:02:34,750
there are two ways of labelling used for token
classification, one (in pink here) uses the

34
00:02:34,750 --> 00:02:40,380
B-PER label at the beginning of each new entity,
but the other (in blue) only uses it to separate

35
00:02:40,380 --> 00:02:43,380
two adjacent entities of the same type.

36
00:02:43,380 --> 00:02:48,880
In both cases, we can flag a new entity each
time we see a new label appearing (either

37
00:02:48,880 --> 00:02:54,051
with the I or B prefix) then take all the
following tokens labelled the same, with an

38
00:02:54,051 --> 00:02:55,051
I-flag.

39
00:02:55,051 --> 00:02:59,360
This, coupled with the offset mapping to get
the start and end characters allows us to

40
00:02:59,360 --> 00:03:06,500
get the span of texts for each entity.
