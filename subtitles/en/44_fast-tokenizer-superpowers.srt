1
00:00:05,109 --> 00:00:10,089
The fast tokenizers of the Transformers library
are fast, but they also implement features

2
00:00:10,089 --> 00:00:14,610
that will be super useful for data pre-processing
and post-processing.

3
00:00:14,610 --> 00:00:16,750
Let's have a look at them!

4
00:00:16,750 --> 00:00:21,759
First, let's have a look at the usual output
of a tokenizer.

5
00:00:21,759 --> 00:00:28,039
We get input IDs that correspond to tokens,
but we lose a lot of information in the process.

6
00:00:28,039 --> 00:00:33,270
For instance, here the tokenization is the
same for the two sentences, even if one has

7
00:00:33,270 --> 00:00:36,510
several more spaces than the other.

8
00:00:36,510 --> 00:00:41,090
Just having the input IDs is thus not enough
if we want to match some tokens with a span

9
00:00:41,090 --> 00:00:46,610
of text (something we will need to do when
tackling question answering for instance).

10
00:00:46,610 --> 00:00:51,699
It's also difficult to know when two tokens
belong to the same word or not: it looks easy

11
00:00:51,699 --> 00:00:57,250
when you just look at the output of a BERT
tokenizer, we just need to look for the ##. But

12
00:00:57,250 --> 00:01:01,490
other tokenizers have different ways to tokenize
parts of words.

13
00:01:01,490 --> 00:01:06,910
For instance RoBERTa adds this special G symbol
to mark the tokens at the beginning of a word,

14
00:01:06,910 --> 00:01:12,160
and T5 uses this special underscore symbol
for the same purpose.

15
00:01:12,160 --> 00:01:16,759
Thankfully, the fast tokenizers keep track
of the word each token comes from, with a

16
00:01:16,759 --> 00:01:20,090
word_ids method you can use on their outputs.

17
00:01:20,090 --> 00:01:24,799
The output is not necessarily clear, but assembled
together in a nice table like this, we can

18
00:01:24,799 --> 00:01:28,119
look at the word position for each token.

19
00:01:28,119 --> 00:01:32,659
Even better, the fast tokenizers keep track
of the span of characters each token comes

20
00:01:32,659 --> 00:01:38,780
from, and we can get them when calling it
on one (or several) text by adding the return_offsets_mapping=True

21
00:01:38,780 --> 00:01:39,780
argument.

22
00:01:39,780 --> 00:01:46,469
In this instance, we can see how we jump positions
between the ##s token and the super token,

23
00:01:46,469 --> 00:01:50,579
because of the multiple spaces in the initial
sentence.

24
00:01:50,579 --> 00:01:54,470
To enable this, the fast tokenizers store
additional information at each step of their

25
00:01:54,470 --> 00:01:55,470
internal pipeline.

26
00:01:55,470 --> 00:02:00,899
That internal pipeline consists of normalization,
where we apply some cleaning to the text,

27
00:02:00,899 --> 00:02:05,600
like lowercasing or removing the accents;()
pre-tokenization, which is where we split

28
00:02:05,600 --> 00:02:09,940
the texts into words;() then we apply the
model of the tokenizer, which is where the

29
00:02:09,940 --> 00:02:15,300
words are splits into tokens,() before finally
doing the post-processing, where special tokens

30
00:02:15,300 --> 00:02:17,110
are added.

31
00:02:17,110 --> 00:02:20,730
From the beginning to the end of the pipeline,
the tokenizer keeps track of each span of

32
00:02:20,730 --> 00:02:23,680
text that corresponds to each word, then each
token.

33
00:02:23,680 --> 00:02:29,099
We will see how useful it is when we tackle
the following tasks: when doing masked language

34
00:02:29,099 --> 00:02:34,360
modeling, one variation that gets state-of-the-art
results is to mask all the tokens of a given

35
00:02:34,360 --> 00:02:37,600
word instead of randomly chosen tokens.

36
00:02:37,600 --> 00:02:40,909
This will require us to use the word IDs we
saw.

37
00:02:40,909 --> 00:02:45,209
When doing token classification, we'll need
to convert the labels we have on words, to

38
00:02:45,209 --> 00:02:47,230
labels on each tokens.

39
00:02:47,230 --> 00:02:51,360
As for the offset mappings, it will be super
useful when we need to convert token positions

40
00:02:51,360 --> 00:02:56,330
in a sentence into a span of text, which we
will need to know when looking at question

41
00:02:56,330 --> 00:03:01,200
answering or when grouping the tokens corresponding
to the same entity in token classification.

42
00:03:01,200 --> 00:03:09,730
To have a look at these tasks, check the videos
linked below!
