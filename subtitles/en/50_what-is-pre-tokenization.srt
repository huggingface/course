1
00:00:05,549 --> 00:00:12,309
The tokenization pipeline involves several
steps that convert raw text into numbers.

2
00:00:12,309 --> 00:00:15,990
In this video, we will see what happens during
the pre-tokenization step.

3
00:00:15,990 --> 00:00:23,840
The pre-tokenization operation is the operation
performed after the normalization of the text

4
00:00:23,840 --> 00:00:28,830
and before the application of the tokenization
algorithm.

5
00:00:28,830 --> 00:00:33,489
This step consists in applying rules that
do not need to be learned to perform a first

6
00:00:33,489 --> 00:00:38,270
division of the text.

7
00:00:38,270 --> 00:00:46,270
Let's look at how several tokenizers pre_tokenize
this example.

8
00:00:46,270 --> 00:00:53,430
The gpt 2 pretokenization divides the text
on spaces and some punctuation - but the apostrophe

9
00:00:53,430 --> 00:00:57,840
is not a division criterion for example.

10
00:00:57,840 --> 00:01:06,580
We also notice that spaces have been replaced
by a capital G with a dot above.

11
00:01:06,580 --> 00:01:12,900
Albert's pre-tokenization divides the text
at the level of spaces, adds a space at the

12
00:01:12,900 --> 00:01:19,610
beginning of the sentence and replaces spaces
with a special underscore.

13
00:01:19,610 --> 00:01:29,320
Finally, Bert's pre-tokenization divides the
text at the level of punctuation and spaces.

14
00:01:29,320 --> 00:01:35,460
Unlike the previous tokenizers, spaces are
not transformed and integrated to the tokens

15
00:01:35,460 --> 00:01:40,079
produced with this pre-tokenizer.

16
00:01:40,079 --> 00:01:45,860
Through these 3 examples, we could observe
the two main types of operations brought by

17
00:01:45,860 --> 00:01:54,210
the pre-tokenization: some changes on the
text and the division of the string into tokens

18
00:01:54,210 --> 00:01:57,259
that can be associated to words.

19
00:01:57,259 --> 00:02:06,729
Finally, the "backend_tokenizer" of the fast
tokenizers also allows to test the pre-tokenization

20
00:02:06,729 --> 00:02:12,739
operation very easily thanks to its "pre_tokenize_str"
method.

21
00:02:12,739 --> 00:02:18,740
We notice that the output of this operation
is composed of both tokens and offsets which

22
00:02:18,740 --> 00:02:24,830
allow to link the token to its position in
the text given in input of the method.

23
00:02:24,830 --> 00:02:32,269
This operation defines the largest tokens
that can be produced by the tokenization or

24
00:02:32,269 --> 00:02:48,389
in other words the barriers of the sub-tokens
which will be produced then.
