1
00:00:05,330 --> 00:00:11,090
In this video, we will study together "the
Unigram Language Model subword tokenization

2
00:00:11,090 --> 00:00:12,090
algorithm".

3
00:00:12,090 --> 00:00:20,080
The overall training strategy of a Unigram
LM tokenizer is to start with a very large

4
00:00:20,080 --> 00:00:27,439
vocabulary and then to remove tokens at each
iteration until we reach the desired size.

5
00:00:27,439 --> 00:00:32,250
At each iteration, we will calculate a loss
on our training corpus thanks to the Unigram

6
00:00:32,250 --> 00:00:33,250
model.

7
00:00:33,250 --> 00:00:39,160
As the loss calculation depends on the available
vocabulary, we can use it to choose how to

8
00:00:39,160 --> 00:00:41,590
reduce the vocabulary.

9
00:00:41,590 --> 00:00:48,090
So we look at the evolution of the loss by
removing in turn each token from the vocabulary.

10
00:00:48,090 --> 00:00:56,730
We will choose to remove the p percents which
increase the loss the less.

11
00:00:56,730 --> 00:01:01,030
Before going further in the explanation of
the training algorithm, I need to explain

12
00:01:01,030 --> 00:01:04,199
what is an Unigram model.

13
00:01:04,199 --> 00:01:08,119
The Unigram LM model is a type of statistical
Language Modem.

14
00:01:08,119 --> 00:01:15,550
A statistical LM will assign a probability
to a text considering that the text is in

15
00:01:15,550 --> 00:01:18,189
fact a sequence of tokens.

16
00:01:18,189 --> 00:01:23,900
The simplest sequences of tokens to imagine
are the words that compose the sentence or

17
00:01:23,900 --> 00:01:25,410
the characters.

18
00:01:25,410 --> 00:01:32,080
The particularity of Unigram LM is that it
assumes that the occurrence of each word is

19
00:01:32,080 --> 00:01:34,670
independent of its previous word.

20
00:01:34,670 --> 00:01:40,271
This "assumption" allows us to write that
the probability of a text is equal to the

21
00:01:40,271 --> 00:01:44,430
product of the probabilities of the tokens
that compose it.

22
00:01:44,430 --> 00:01:51,880
It should be noted here that this is a very
simple model which would not be adapted to

23
00:01:51,880 --> 00:01:58,630
the generation of text since this model would
always generate the same token, the one which

24
00:01:58,630 --> 00:02:00,140
has the greatest probability.

25
00:02:00,140 --> 00:02:07,409
Nevertheless, to do tokenization, this model
is very useful to us because it can be used

26
00:02:07,409 --> 00:02:14,209
to estimate the relative likelihood of different
phrases.

27
00:02:14,209 --> 00:02:20,000
We are now ready to return to our explanation
of the training algorithm.

28
00:02:20,000 --> 00:02:25,349
Let's say that we have as a training corpus
10 times the word hug, 12 times the word pug,

29
00:02:25,349 --> 00:02:33,270
5 times the word lug, 4 times bug and 5 times
dug.

30
00:02:33,270 --> 00:02:38,910
As said at the beginning of the video, the
training starts with a big vocabulary.

31
00:02:38,910 --> 00:02:45,280
Obviously, as we are using a toy corpus, this
vocabulary will not be that big but it should

32
00:02:45,280 --> 00:02:46,840
show you the principle.

33
00:02:46,840 --> 00:02:54,870
A first method is to list all the possible
strict substrings that's what we'll do here.

34
00:02:54,870 --> 00:03:00,379
We could also have used the BPE algorithm
with a very large vocabulary size.

35
00:03:00,379 --> 00:03:07,200
So we have our initial vocabulary.

36
00:03:07,200 --> 00:03:13,629
The training of the Unigram tokenizer is based
on the Expectation-Maximization method: at

37
00:03:13,629 --> 00:03:15,210
each iteration.

38
00:03:15,210 --> 00:03:19,190
We estimate the probabilities of the tokens
of the vocabulary.

39
00:03:19,190 --> 00:03:26,430
Then we remove the p percent of tokens that
minimize the loss on the corpus and which

40
00:03:26,430 --> 00:03:33,500
do not belong to the basic characters as we
want to keep in our final vocabulary the basic

41
00:03:33,500 --> 00:03:37,980
characters to be able to tokenize any word.

42
00:03:37,980 --> 00:03:39,230
Let's go for it!

43
00:03:39,230 --> 00:03:44,660
The probability of a token is simply estimated
by the number of appearance of this token

44
00:03:44,660 --> 00:03:51,590
in our training corpus divided by the total
number of appearance of all the tokens.

45
00:03:51,590 --> 00:03:57,239
We could use this vocabulary to tokenize our
words according to the unigram model.

46
00:03:57,239 --> 00:04:04,080
We will do it together to understand two things:
how we tokenize a word with a Unigram model

47
00:04:04,080 --> 00:04:09,160
and how the loss is calculated on our corpus.

48
00:04:09,160 --> 00:04:14,610
The Unigram LM tokenization of our text "Hug"
will be the one with the highest probability

49
00:04:14,610 --> 00:04:19,140
of occurrence according to our Unigram model.

50
00:04:19,140 --> 00:04:24,090
To find it, the simplest way to proceed would
be to list all the possible segmentations

51
00:04:24,090 --> 00:04:29,949
of our text "Hug", calculate the probability
of each of these segmentations and then choose

52
00:04:29,949 --> 00:04:32,490
the one with the highest probability.

53
00:04:32,490 --> 00:04:38,630
With the current vocabulary, 2 tokenizations
get exactly the same probability.

54
00:04:38,630 --> 00:04:43,789
So we choose one of them and keep in memory
the associated probability.

55
00:04:43,789 --> 00:04:48,850
To compute the loss on our training corpus,
we need to tokenize as we just did all the

56
00:04:48,850 --> 00:04:52,810
remaining words in the corpus.

57
00:04:52,810 --> 00:04:57,930
The loss is then the sum over all the words
in the corpus of the frequency of occurrence

58
00:04:57,930 --> 00:05:04,220
of the word multiplied by the opposite of
the log of the probability associated with

59
00:05:04,220 --> 00:05:07,720
the tokenization of the word.

60
00:05:07,720 --> 00:05:12,700
We obtain here a loss of one hundred and seventy.

61
00:05:12,700 --> 00:05:18,750
Remember, our initial goal was to reduce the
vocabulary.

62
00:05:18,750 --> 00:05:27,810
To do this, we will remove a token from the
vocabulary and calculate the associated loss.

63
00:05:27,810 --> 00:05:32,020
Let's remove for example the token 'ug'.

64
00:05:32,020 --> 00:05:38,569
We notice that the tokenization for "hug"
with the letter h and the tuple ug is now

65
00:05:38,569 --> 00:05:39,970
impossible.

66
00:05:39,970 --> 00:05:45,810
Nevertheless, as we saw earlier that two tokenizations
had the same probability and we can still

67
00:05:45,810 --> 00:05:50,870
choose the remaining tokenization with a probability
of one point ten minus two.

68
00:05:50,870 --> 00:05:58,210
The tokenizations of the other words of the
vocabulary also remain unchanged and finally

69
00:05:58,210 --> 00:06:06,710
even if we remove the token "ug" from our
vocabulary the loss remains equal to 170.

70
00:06:06,710 --> 00:06:11,550
For this first iteration, if we continue the
calculation, we would notice that we could

71
00:06:11,550 --> 00:06:16,190
remove any token without it impacting the
loss.

72
00:06:16,190 --> 00:06:24,620
We will therefore choose at random to remove
the token "ug" before starting a second iteration.

73
00:06:24,620 --> 00:06:29,600
We estimate again the probability of each
token before calculating the impact of each

74
00:06:29,600 --> 00:06:32,280
token on the loss.

75
00:06:32,280 --> 00:06:37,840
For example, if we remove now the token composed
of the letters "h" and "u", there is only

76
00:06:37,840 --> 00:06:42,020
one possible tokenization left for hug.

77
00:06:42,020 --> 00:06:46,580
The tokenization of the other words of the
vocabulary is not changed.

78
00:06:46,580 --> 00:06:51,880
In the end, we obtain by removing the token
composed of the letters "h" and "u" from the

79
00:06:51,880 --> 00:06:54,650
vocabulary a loss of one hundred and sixty-eight.

80
00:06:54,650 --> 00:07:02,550
Finally, to choose which token to remove,
we will for each remaining token of the vocabulary

81
00:07:02,550 --> 00:07:10,090
which is not an elementary token calculate
the associated loss then compare these losses

82
00:07:10,090 --> 00:07:11,850
between them.

83
00:07:11,850 --> 00:07:18,100
The token which we will remove is the token
which impacts the least the loss: here the

84
00:07:18,100 --> 00:07:20,129
token "bu".

85
00:07:20,129 --> 00:07:25,710
We had mentioned at the beginning of the video
that at each iteration we could remove p % of

86
00:07:25,710 --> 00:07:29,540
the tokens by iteration.

87
00:07:29,540 --> 00:07:35,850
The second token that could be removed at
this iteration is the "du" token.

88
00:07:35,850 --> 00:07:42,690
And that's it, we just have to repeat these
steps until we get the vocabulary of the desired

89
00:07:42,690 --> 00:07:45,240
size.

90
00:07:45,240 --> 00:07:51,129
One last thing, in practice, when we tokenize
a word with a Unigram model we don't compute

91
00:07:51,129 --> 00:07:57,210
the set of probabilities of the possible splits
of a word before comparing them to keep the

92
00:07:57,210 --> 00:08:05,560
best one but we use the Viterbi algorithm
which is much more efficient.

93
00:08:05,560 --> 00:08:07,300
And that's it!

94
00:08:07,300 --> 00:08:15,000
I hope that this example has allowed you to
better understand the Unigram tokenization

95
00:08:15,000 --> 00:08:18,190
algorithm.
