1
00:00:04,130 --> 00:00:08,390
Let's have a look inside the question answering
pipeline.

2
00:00:08,390 --> 00:00:12,630
The question answering pipeline can extracts
answers to questions from a given context

3
00:00:12,630 --> 00:00:18,150
or passage of text, like this part of the
Transformers repo README.

4
00:00:18,150 --> 00:00:22,440
It also works for very long contexts, even
if the answer is at the very end, like in

5
00:00:22,440 --> 00:00:23,440
this example.

6
00:00:23,440 --> 00:00:24,680
In this video, we will see why!

7
00:00:24,680 --> 00:00:31,540
The question answering pipeline follows the
same steps as the other pipelines: the question

8
00:00:31,540 --> 00:00:36,380
and context are tokenized as a sentence pair,
fed to the model then some post-processing

9
00:00:36,380 --> 00:00:37,649
is applied.

10
00:00:37,649 --> 00:00:41,790
The tokenization and model steps should be
familiar.

11
00:00:41,790 --> 00:00:47,020
We use the auto class suitable for Question
Answering instead of sequence classification,

12
00:00:47,020 --> 00:00:52,039
but one key difference with text classification
is that our model outputs two tensors named

13
00:00:52,039 --> 00:00:54,559
start logits and end logits.

14
00:00:54,559 --> 00:00:55,559
Why is that?

15
00:00:55,559 --> 00:00:59,850
Well this is the way the model finds the answer
to the question.

16
00:00:59,850 --> 00:01:02,270
First let's have a look at the model inputs.

17
00:01:02,270 --> 00:01:07,160
It's numbers associated with the tokenization
of the question followed by the context (with

18
00:01:07,160 --> 00:01:10,710
the usual CLS and SEP special tokens).

19
00:01:10,710 --> 00:01:13,310
The answer is a part of those tokens.

20
00:01:13,310 --> 00:01:17,759
So we ask the model to predict which token
starts the answer and which ends the answer.

21
00:01:17,759 --> 00:01:24,380
For our two logit outputs, the theoretical
labels are the pink and purple vectors.

22
00:01:24,380 --> 00:01:28,360
To convert those logits into probabilities,
we will need to apply a SoftMax, like in the

23
00:01:28,360 --> 00:01:30,439
text classification pipeline.

24
00:01:30,439 --> 00:01:35,070
We just mask the tokens that are not part
of the context before doing that, leaving

25
00:01:35,070 --> 00:01:41,009
the initial CLS token unmasked as we use it
to predict an impossible answer.

26
00:01:41,009 --> 00:01:43,579
This is what it looks in terms of code.

27
00:01:43,579 --> 00:01:47,729
We use a large negative number for the masking,
since its exponential will then be 0.

28
00:01:47,729 --> 00:01:53,610
Now the probability for each start and end
position corresponding to a possible answer,

29
00:01:53,610 --> 00:01:57,600
we give a score that is the product of the
start probabilities and end probabilities

30
00:01:57,600 --> 00:02:00,180
at those positions.

31
00:02:00,180 --> 00:02:05,430
Of course, a start index greater than an end
index corresponds to an impossible answer.

32
00:02:05,430 --> 00:02:08,940
Here is the code to find the best score for
a possible answer.

33
00:02:08,940 --> 00:02:13,070
Once we have the start and end positions of
the tokens, we use the offset mappings provided

34
00:02:13,070 --> 00:02:18,270
by our tokenizer to find the span of characters
in the initial context, and get our answer!

35
00:02:18,270 --> 00:02:23,820
Now, when the context is long, it might get
truncated by the tokenizer.

36
00:02:23,820 --> 00:02:29,099
This might result in part of the answer, or
worse, the whole answer, being truncated.

37
00:02:29,099 --> 00:02:33,319
So we don't discard the truncated tokens but
build new features with them.

38
00:02:33,319 --> 00:02:39,320
Each of those features contains the question,
then a chunk of text in the context.

39
00:02:39,320 --> 00:02:43,760
If we take disjoint chunks of texts, we might
end up with the answer being split between

40
00:02:43,760 --> 00:02:45,330
two features.

41
00:02:45,330 --> 00:02:49,709
So instead, we take overlapping chunks of
texts, to make sure at least one of the chunks

42
00:02:49,709 --> 00:02:51,650
will fully contain the answer to the question.

43
00:02:51,650 --> 00:02:56,920
The tokenizers do all of this for us automatically
with the return overflowing tokens option.

44
00:02:56,920 --> 00:03:02,069
The stride argument controls the number of
overlapping tokens.

45
00:03:02,069 --> 00:03:05,930
Here is how our very long context gets truncated
in two features with some overlap.

46
00:03:05,930 --> 00:03:10,051
By applying the same post-processing we saw
before for each feature, we get the answer

47
00:03:10,051 --> 00:03:18,349
with a score for each of them, and we take
the answer with the best score as a final

48
00:03:18,349 --> 00:03:21,239
solution.
