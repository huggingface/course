1
00:00:05,360 --> 00:00:07,920
Let's have a look inside the 
question answering pipeline.  

2
00:00:09,600 --> 00:00:13,520
The question answering pipeline 
can extracts answers to questions  

3
00:00:13,520 --> 00:00:17,840
from a given context or passage of text, like 
this part of the Transformers repo README.  

4
00:00:19,040 --> 00:00:23,680
It also works for very long contexts, even if the 
answer is at the very end, like in this example.  

5
00:00:24,480 --> 00:00:25,840
In this video, we will see why!  

6
00:00:27,600 --> 00:00:32,720
The question answering pipeline follows the same 
steps as the other pipelines: the question and  

7
00:00:32,720 --> 00:00:38,160
context are tokenized as a sentence pair, fed to 
the model then some post-processing is applied.  

8
00:00:39,280 --> 00:00:44,160
The tokenization and model steps should be 
familiar. We use the auto class suitable for  

9
00:00:44,160 --> 00:00:48,240
Question Answering instead of sequence 
classification, but one key difference  

10
00:00:48,240 --> 00:00:53,680
with text classification is that our model 
outputs two tensors named start logits and  

11
00:00:53,680 --> 00:00:58,640
end logits. Why is that? Well this is the way 
the model finds the answer to the question.  

12
00:00:59,920 --> 00:01:04,880
First let's have a look at the model inputs. It's 
numbers associated with the tokenization of the  

13
00:01:04,880 --> 00:01:12,160
question followed by the context (with the usual 
CLS and SEP special tokens). The answer is a part  

14
00:01:12,160 --> 00:01:17,920
of those tokens. So we ask the model to predict 
which token starts the answer and which ends the  

15
00:01:17,920 --> 00:01:25,040
answer. For our two logit outputs, the theoretical 
labels are the pink and purple vectors. To convert  

16
00:01:25,040 --> 00:01:29,520
those logits into probabilities, we will need to 
apply a SoftMax, like in the text classification  

17
00:01:29,520 --> 00:01:36,240
pipeline. We just mask the tokens that are not 
part of the context before doing that, leaving the  

18
00:01:36,240 --> 00:01:43,200
initial CLS token unmasked as we use it to predict 
an impossible answer. This is what it looks in  

19
00:01:43,200 --> 00:01:49,200
terms of code. We use a large negative number for 
the masking, since its exponential will then be 0.  

20
00:01:50,480 --> 00:01:54,640
Now the probability for each start and end 
position corresponding to a possible answer,  

21
00:01:55,520 --> 00:02:00,000
we give a score that is the product of the start 
probabilities and end probabilities at those  

22
00:02:00,000 --> 00:02:06,000
positions. Of course, a start index greater than 
an end index corresponds to an impossible answer.  

23
00:02:07,360 --> 00:02:12,080
Here is the code to find the best score for 
a possible answer. Once we have the start and  

24
00:02:12,080 --> 00:02:17,040
end positions of the tokens, we use the offset 
mappings provided by our tokenizer to find the  

25
00:02:17,040 --> 00:02:23,520
span of characters in the initial context, and 
get our answer! Now, when the context is long,  

26
00:02:23,520 --> 00:02:29,440
it might get truncated by the tokenizer. This 
might result in part of the answer, or worse, the  

27
00:02:29,440 --> 00:02:34,800
whole answer, being truncated. So we don't discard 
the truncated tokens but build new features  

28
00:02:34,800 --> 00:02:42,080
with them. Each of those features contains the 
question, then a chunk of text in the context. If  

29
00:02:42,080 --> 00:02:47,280
we take disjoint chunks of texts, we might end up 
with the answer being split between two features.  

30
00:02:48,560 --> 00:02:51,840
So instead, we take overlapping chunks of texts,  

31
00:02:51,840 --> 00:02:55,520
to make sure at least one of the chunks will 
fully contain the answer to the question.  

32
00:02:56,720 --> 00:03:00,880
The tokenizers do all of this for us automatically 
with the return overflowing tokens option.  

33
00:03:01,680 --> 00:03:04,320
The stride argument controls the 
number of overlapping tokens.  

34
00:03:05,680 --> 00:03:10,160
Here is how our very long context gets 
truncated in two features with some overlap.  

35
00:03:10,960 --> 00:03:15,520
By applying the same post-processing we saw 
before for each feature, we get the answer  

36
00:03:15,520 --> 00:03:27,600
with a score for each of them, and we take the 
answer with the best score as a final solution.
