1
00:00:05,569 --> 00:00:10,490
Let's study how to preprocess a dataset for
question answering!

2
00:00:10,490 --> 00:00:14,260
Question answering is the task of finding
answers to a question in some context.

3
00:00:14,260 --> 00:00:19,970
For our example, we will use the squad dataset,
in which we remove columns we won't use and

4
00:00:19,970 --> 00:00:24,390
just extract the information we will need
for the labels: the start and the end of the

5
00:00:24,390 --> 00:00:25,390
answer in the context.

6
00:00:25,390 --> 00:00:30,279
If you have your own dataset for question
answering, just make sure you clean your data

7
00:00:30,279 --> 00:00:34,800
to get to the same point, with one column
containing the questions, one column containing

8
00:00:34,800 --> 00:00:39,350
the contexts, one column for the index of
the start and end character of the answer

9
00:00:39,350 --> 00:00:41,700
in the context.

10
00:00:41,700 --> 00:00:44,610
Note that the answer must be part of the context.

11
00:00:44,610 --> 00:00:48,360
If you want to perform generative question
answering, look at one of the sequence to

12
00:00:48,360 --> 00:00:50,890
sequence videos linked below.

13
00:00:50,890 --> 00:00:55,860
Now if we have a look at the tokens we will
feed our model we will see the answer lies

14
00:00:55,860 --> 00:00:58,450
somewhere inside the context.

15
00:00:58,450 --> 00:01:02,239
For very long context that answer may get
truncated by the tokenizer.

16
00:01:02,239 --> 00:01:06,050
In this case, we wont have any proper labels
for our model.

17
00:01:06,050 --> 00:01:11,159
So we should keep the truncated part as a
separate feature instead of discarding it.

18
00:01:11,159 --> 00:01:14,720
The only thing we need to be careful with,
is to allow some overlap between separate

19
00:01:14,720 --> 00:01:19,900
chunks so that the answer is not truncated,
and that the feature containing the answer

20
00:01:19,900 --> 00:01:22,670
gets sufficient context to be able to predict
it.

21
00:01:22,670 --> 00:01:28,790
Here is how it can be done by the tokenizer:
we pass it the question, context, set the

22
00:01:28,790 --> 00:01:32,750
truncation for the context only and the padding
to the maximum length.

23
00:01:32,750 --> 00:01:39,590
The stride argument is where we set the number
of overlapping tokens, and the return_overflowing_tokens

24
00:01:39,590 --> 00:01:42,869
means we don't want to discard the truncated
part.

25
00:01:42,869 --> 00:01:47,140
Lastly, we also return the offset mappings
to be able to find the tokens corresponding

26
00:01:47,140 --> 00:01:48,649
to the answer start and end.

27
00:01:48,649 --> 00:01:53,990
We want those two tokens, because there will
be the labels we pass to our model.

28
00:01:53,990 --> 00:01:57,200
In a one-hot encoded version, here is what
they look like.

29
00:01:57,200 --> 00:02:02,119
If the context we have does not contain the
answer, we set the two labels to the index

30
00:02:02,119 --> 00:02:04,329
of the CLS token.

31
00:02:04,329 --> 00:02:08,629
We also do this if the context only partially
contains the answer.

32
00:02:08,629 --> 00:02:13,950
In terms of code, here is how we can do it:
using the sequence IDs of an input, we can

33
00:02:13,950 --> 00:02:17,390
determine the beginning and the end of the
context.

34
00:02:17,390 --> 00:02:22,290
Then we know if have to return the CLS position
for the two labels or we determine the positions

35
00:02:22,290 --> 00:02:25,120
of the first and last tokens of the answer.

36
00:02:25,120 --> 00:02:28,670
We can check it works properly on our previous
example.

37
00:02:28,670 --> 00:02:35,319
Putting it all together looks like this big
function, which we can apply to our datasets.

38
00:02:35,319 --> 00:02:40,010
Since we applied padding during the tokenization,
we can then use this directly in the Trainer

39
00:02:40,010 --> 00:02:43,920
or apply the to_tf_dataset method to use Keras.fit.
