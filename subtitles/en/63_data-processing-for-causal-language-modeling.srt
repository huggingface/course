1
00:00:05,520 --> 00:00:09,360
In this video we take a look at the 
data processing necessary to train  

2
00:00:09,360 --> 00:00:15,920
causal language models. Causal Language Modeling 
is the task of predicting the next token based  

3
00:00:15,920 --> 00:00:20,880
on the previous token. Another term for Causal 
Language Modeling is Autoregressive Modeling.  

4
00:00:21,760 --> 00:00:26,560
In the example that you see here the 
next token could for example be NLP  

5
00:00:26,560 --> 00:00:33,280
or machine learning. A popular example of a 
Causal Language Model is the GPT family of models.  

6
00:00:35,680 --> 00:00:40,400
To train such models such as GPT-2 we usually 
start with a large corpus of text files.  

7
00:00:41,280 --> 00:00:45,760
These files can webpages scraped from the 
internet such as the Common Crawl dataset  

8
00:00:45,760 --> 00:00:51,920
or they can be Python files from GitHub like you 
can see here. As a first step we need to tokenize  

9
00:00:51,920 --> 00:00:57,520
these files such that we can feed them through a 
model. Here we show the tokenized texts as bars of  

10
00:00:57,520 --> 00:01:06,000
various length illustrating the different sequence 
lengths. Normally, the text files come in various  

11
00:01:06,000 --> 00:01:07,440
sizes and which results in various sequence length 
of the tokenized texts. Transformer models have a  

12
00:01:07,440 --> 00:01:12,960
limited context length and depending on the data 
source it is possible that the tokenized texts  

13
00:01:12,960 --> 00:01:18,640
are much longer than this context length. In 
this case we could just truncate the sequence  

14
00:01:18,640 --> 00:01:24,160
to the context length but this would mean that 
we loose everything after the context length.  

15
00:01:25,360 --> 00:01:30,960
Using the return overflowing tokens flag in the 
we can use the tokenizer to create chunks with  

16
00:01:30,960 --> 00:01:36,960
each one being the size of the context length. 
Sometimes it can happen that the last chunk is  

17
00:01:36,960 --> 00:01:41,440
too short if there aren’t enough tokens to fill 
it. In this case we would like to remove it.  

18
00:01:43,440 --> 00:01:48,800
With the return_length keyword we also get 
the length of each chunk from the tokenizer.  

19
00:01:51,760 --> 00:01:57,280
This function shows all the steps necessary 
to prepare the dataset. First we tokenize the  

20
00:01:57,280 --> 00:02:03,520
dataset with the flags I just mentioned. Then we 
go through each chunk and if its length matches  

21
00:02:03,520 --> 00:02:08,960
the context length we add it to the inputs we 
return. We can apply this function to the whole  

22
00:02:08,960 --> 00:02:17,520
dataset and we make sure to use batches and remove 
the existing columns. We need to remove columns  

23
00:02:17,520 --> 00:02:23,280
because we can create multiple samples per text 
and the shapes in the dataset would not match.  

24
00:02:26,960 --> 00:02:32,400
If the context length is of similar length as 
the files this approach doesn't so well anymore.  

25
00:02:33,520 --> 00:02:39,440
In this example both sample 1 and 2 are shorter 
than the context size and would be discarded with  

26
00:02:39,440 --> 00:02:46,400
the previous approach. In this case it is better 
to first tokenize each sample without truncation  

27
00:02:46,400 --> 00:02:52,000
and then concatenate the tokenized samples with an 
end of string, or EOS for short, token in between.  

28
00:02:53,840 --> 00:02:57,440
Finally we can chunk this long 
sequence with the context length  

29
00:02:57,440 --> 00:03:05,840
and we don’t loose any sequences because they 
are too short. So far we have only talked about  

30
00:03:05,840 --> 00:03:10,720
the inputs for causal language modeling but 
not the labels needed for supervised training.  

31
00:03:11,600 --> 00:03:16,480
When we do causal language modeling we don’t 
require any extra labels for the input sequences  

32
00:03:16,480 --> 00:03:22,080
as the input sequences themselves are 
the labels. In this example when we feed  

33
00:03:22,080 --> 00:03:26,560
the token “Trans” to the next token we 
want the model to predict is “formers”.  

34
00:03:27,280 --> 00:03:33,360
In the next step we feed “Trans” and “formers” 
to the model and the label is the token “are”.  

35
00:03:35,280 --> 00:03:42,400
This pattern continues and as you can see the 
input sequence is the label just shifted by one.  

36
00:03:43,440 --> 00:03:48,000
Since the model only makes a prediction 
after the first token, the first element  

37
00:03:48,000 --> 00:03:54,480
of the input sequence, in this case “Trans”, 
is not used as a label. Similarly, we do not  

38
00:03:54,480 --> 00:04:00,400
have a label for the last token in the sequence 
since there is no token after the sequence ends.  

39
00:04:03,920 --> 00:04:09,200
Let’s have a look at what we need to do to create 
the labels for causal language modeling in code.If  

40
00:04:10,160 --> 00:04:15,600
we want to calculate the loss on a batch we can 
just pass the input_ids as labels and all the  

41
00:04:15,600 --> 00:04:19,432
shifting is handled in the model internally. And 
the dataset is also ready to be used directly in  

42
00:04:19,432 --> 00:04:21,600
the Trainer or keras.fit if you are using 
TensorFlow. So you see there is no magic  

43
00:04:21,600 --> 00:04:27,840
involved in processing data for causal language 
modeling and only requires a few simple steps!
