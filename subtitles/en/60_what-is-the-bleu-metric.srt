1
00:00:05,520 --> 00:00:12,080
What is the BLEU metric? For many NLP tasks we 
can use common metrics like accuracy or F1 score,  

2
00:00:12,080 --> 00:00:15,280
but what do you do when you want to measure 
the quality of text that's generated from a  

3
00:00:15,280 --> 00:00:19,680
model like GPT-2? In this video, we'll take a look 
at a widely used metric for machine translation  

4
00:00:19,680 --> 00:00:22,960
called BLEU, which is short for BiLingual 
Evaluation Understudy. The basic idea behind  

5
00:00:22,960 --> 00:00:27,280
BLEU is to assign a single numerical score to 
a translation that tells us how "good" it is  

6
00:00:27,280 --> 00:00:32,080
compared to one or more reference translations. 
In this example we have a sentence in Spanish that  

7
00:00:32,080 --> 00:00:37,280
has been translated into English by some model. 
If we compare the generated translation to some  

8
00:00:37,280 --> 00:00:42,160
reference human translations, we can see that the 
model is pretty good, but has made a common error:  

9
00:00:42,960 --> 00:00:48,000
the Spanish word "tengo" means "have" in English 
and this 1-1 translation is not quite natural.  

10
00:00:49,680 --> 00:00:53,280
So how can we measure the quality of a 
generated translation in an automatic way?  

11
00:00:54,080 --> 00:00:58,000
The approach that BLEU takes is to compare the 
n-grams of the generated translation to the  

12
00:00:58,000 --> 00:01:03,760
n-grams of the references. An n-gram is just 
a fancy way of saying "a chunk of n words",  

13
00:01:03,760 --> 00:01:07,280
so let's start with unigrams, which correspond 
to the individual words in a sentence.  

14
00:01:08,720 --> 00:01:12,160
In this example you can see that four of 
the words in the generated translation  

15
00:01:12,160 --> 00:01:18,000
are also found in one of the reference 
translations. Now that we've found our matches,  

16
00:01:18,000 --> 00:01:21,920
one way to assign a score to the translation 
is to compute the precision of the unigrams.  

17
00:01:22,880 --> 00:01:27,200
This means we just count the number of matching 
words in the generated and reference translations  

18
00:01:27,200 --> 00:01:30,400
and normalize the count by dividing by 
the number of word in the generation.  

19
00:01:31,600 --> 00:01:35,600
In this example, we found 4 matching 
words and our generation has 5 words,  

20
00:01:36,960 --> 00:01:40,320
so our unigram precision is 4/5 or 0.8. In general 
precision ranges from 0 to 1, and higher precision  

21
00:01:40,320 --> 00:01:48,160
scores mean a better translation. One problem 
with unigram precision is that translation models  

22
00:01:48,160 --> 00:01:51,840
sometimes get stuck in repetitive patterns 
and repeat the same word several times.  

23
00:01:52,960 --> 00:01:56,240
If we just count the number of word matches, 
we can get really high precision scores  

24
00:01:56,240 --> 00:01:58,720
even though the translation is 
terrible from a human perspective!  

25
00:01:59,840 --> 00:02:04,640
For example, if our model just generates the word 
"six", we get a perfect unigram precision score.  

26
00:02:07,040 --> 00:02:12,000
To handle this, BLEU uses a modified precision 
that clips the number of times to count a word,  

27
00:02:12,000 --> 00:02:14,960
based on the maximum number of times it 
appears in the reference translation.  

28
00:02:16,160 --> 00:02:19,360
In this example, the word "six" 
only appears once in the reference,  

29
00:02:19,360 --> 00:02:23,840
so we clip the numerator to one and the modified 
unigram precision now gives a much lower score.  

30
00:02:27,440 --> 00:02:31,600
Another problem with unigram precision is that 
it doesn't take into account the order of the  

31
00:02:31,600 --> 00:02:37,200
words in the translations. For example, suppose 
we had Yoda translate our Spanish sentence,  

32
00:02:37,200 --> 00:02:43,120
then we might get something backwards like 
"years six thirty have I". In this case,  

33
00:02:43,120 --> 00:02:46,560
the modified unigram precision gives a 
high precision which is not what we want.  

34
00:02:48,240 --> 00:02:52,400
So to deal with word ordering problems, BLEU 
actually computes the precision for several  

35
00:02:52,400 --> 00:02:57,360
different n-grams and then averages the result. 
For example, if we compare 4-grams, then we can  

36
00:02:57,360 --> 00:03:03,840
see there are no matching chunks of 4 words in 
translations and so the 4-gram precision is 0.  

37
00:03:05,440 --> 00:03:10,880
To compute BLEU scores in Hugging Face Datasets is 
very simple: just use the load_metric() function,  

38
00:03:10,880 --> 00:03:13,840
provide your model's predictions along 
with the references and you're good to go!  

39
00:03:16,240 --> 00:03:19,920
The output contains several fields 
of interest. The precisions field  

40
00:03:19,920 --> 00:03:22,800
contains all the individual 
precision scores for each n-gram.  

41
00:03:24,800 --> 00:03:30,320
The BLEU score itself is then calculated by taking 
the geometric mean of the precision scores. By  

42
00:03:30,320 --> 00:03:34,880
default, the mean of all four n-gram precisions is 
reported, a metric that is sometimes also called  

43
00:03:34,880 --> 00:03:40,480
BLEU-4. In this example we can see the BLEU score 
is zero because the 4-gram precision was zero.  

44
00:03:43,440 --> 00:03:46,640
The BLEU metric has some nice properties, 
but it is far from a perfect metric.  

45
00:03:47,280 --> 00:03:51,520
The good properties are that it's easy to compute 
and widely used in research so you can compare  

46
00:03:51,520 --> 00:03:56,560
your model against others on a benchmark. On the 
other hand, there are several problems with BLEU,  

47
00:03:56,560 --> 00:04:00,560
including the fact it doesn't incorporate 
semantics and struggles on non-English languages.  

48
00:04:01,680 --> 00:04:04,560
Another problem with BLEU is that it 
assumes the human translations have  

49
00:04:04,560 --> 00:04:08,400
already been tokenized and this makes it hard 
to compare models with different tokenizers.  

50
00:04:11,200 --> 00:04:15,280
Measuring the quality of texts is still a 
difficult, open problem in NLP research.  

51
00:04:15,280 --> 00:04:17,680
For machine translation, the 
current recommendation is to  

52
00:04:17,680 --> 00:04:21,600
use the SacreBLEU metric which addresses 
the tokenization limitations of BLEU.  

53
00:04:22,640 --> 00:04:26,560
As you can see in this example, computing 
the SacreBLEU score is almost identical to  

54
00:04:26,560 --> 00:04:30,800
the BLEU one. The main difference is that we 
now pass a list of texts instead of a list  

55
00:04:30,800 --> 00:04:41,200
of words for the translations, and SacreBLEU 
takes care of the tokenization under the hood.
