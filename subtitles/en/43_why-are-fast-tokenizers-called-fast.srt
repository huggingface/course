1
00:00:05,200 --> 00:00:11,600
Why are fast tokenizers called fast? In this video 
we will see exactly how much faster the so-called  

2
00:00:11,600 --> 00:00:16,960
fast tokenizers are compared to their 
slow counterparts. For this benchmark,  

3
00:00:16,960 --> 00:00:22,160
we will use the GLUE MNLI dataset, which 
contains 432 thousands pairs of texts.  

4
00:00:22,880 --> 00:00:27,040
We will see how long it takes for the 
fast and slow versions of a BERT tokenizer  

5
00:00:27,040 --> 00:00:34,080
to process them all. We define our fast and slow 
tokenizer using the AutoTokenizer API. The fast  

6
00:00:34,080 --> 00:00:40,160
tokenizer is the default (when available), so we 
pass along use_fast=False to define the slow one.  

7
00:00:41,200 --> 00:00:45,760
In a notebook, we can time the execution of a 
cell with the time magic command, like this.  

8
00:00:46,560 --> 00:00:50,720
Processing the whole dataset is four 
times faster with a fast tokenizer.  

9
00:00:50,720 --> 00:00:54,960
That's quicker indeed, but not very impressive 
however. That's because we passed along the  

10
00:00:54,960 --> 00:00:59,520
texts to the tokenizer one at a time. This is 
a common mistake to do with fast tokenizers,  

11
00:00:59,520 --> 00:01:04,320
which are backed by Rust and thus able to 
parallelize the tokenization of multiple texts.  

12
00:01:05,120 --> 00:01:09,520
Passing them only one text at a time is like 
sending a cargo ship between two continents  

13
00:01:09,520 --> 00:01:15,600
with just one container, it's very inefficient. 
To unleash the full speed of our fast tokenizers,  

14
00:01:15,600 --> 00:01:20,320
we need to send them batches of texts, which 
we can do with the batched=True argument  

15
00:01:20,320 --> 00:01:26,720
of the map method. Now those results are 
impressive! The fast tokenizer takes 12 seconds to  

16
00:01:26,720 --> 00:01:33,280
process a dataset that takes 4 minutes to the slow 
tokenizer. Summarizing the results in this table,  

17
00:01:33,280 --> 00:01:37,200
you can see why we have called those 
tokenizers fast. And this is only for  

18
00:01:37,200 --> 00:01:48,160
tokenizing texts. If you ever need to train a 
new tokenizer, they do this very quickly too!
