1
00:00:05,120 --> 00:00:07,440
You are at the right place if you want to  

2
00:00:07,440 --> 00:00:15,360
understand what the Byte pair Encoding subword 
tokenization algorithm is, how to train it  

3
00:00:15,360 --> 00:00:18,640
and how the tokenization of a 
text is done with this algorithm.  

4
00:00:21,600 --> 00:00:25,920
The BPE algorithm was initially 
proposed as a text compression algorithm  

5
00:00:26,640 --> 00:00:30,800
but it is also very well suited as a 
tokenizer for your language models.  

6
00:00:32,560 --> 00:00:38,720
The idea of BPE is to divide words into a 
sequence of "subword units" which are units  

7
00:00:38,720 --> 00:00:44,400
that appear frequently in a reference corpus 
- that is, the corpus we used to train it.  

8
00:00:46,560 --> 00:00:53,680
How is a BPE tokenizer trained? First of all, 
we have to get a corpus of texts. We will not  

9
00:00:54,480 --> 00:01:02,080
train our tokenizer on this raw text but we will 
first normalize it then pre-tokenize it. As the  

10
00:01:02,080 --> 00:01:07,520
pre-tokenization divides the text into a list 
of words, we can represent our corpus in another  

11
00:01:07,520 --> 00:01:14,000
way by gathering together the same words and by 
maintaining a counter, here represented in blue.  

12
00:01:17,120 --> 00:01:22,960
To understand how the training works, we consider 
this toy corpus composed of the following words:  

13
00:01:23,520 --> 00:01:32,480
huggingface, hugging, hug, hugger, etc. BPE is an 
algorithm that starts with an initial vocabulary  

14
00:01:32,480 --> 00:01:35,200
and then increases it to the desired size.  

15
00:01:36,240 --> 00:01:41,360
To build the initial vocabulary, we start 
by separating each word of the corpus  

16
00:01:41,360 --> 00:01:46,640
into a list of elementary units that 
compose them -here the characters.  

17
00:01:50,800 --> 00:01:51,360
We could also have chosen bytes as elementary 
units but it would have been less visual. We list  

18
00:01:51,360 --> 00:01:57,760
in our vocabulary all the characters that appear 
and that will constitute our initial vocabulary!  

19
00:02:00,240 --> 00:02:09,840
Let's now see how to increase it. We return to 
our split corpus, we will go through the words  

20
00:02:09,840 --> 00:02:18,480
one by one and count all the occurrences of token 
pairs. The first pair is composed of the token "h"  

21
00:02:18,480 --> 00:02:26,080
and "u", the second 'u' and "g", and we continue 
like that until we have the complete list.  

22
00:02:35,440 --> 00:02:41,200
Once we know all the pairs and their frequency 
of appearance, we will choose the one that  

23
00:02:41,200 --> 00:02:49,840
appears the most frequently: here it is the 
pair composed of the letters 'l' and 'e'.  

24
00:02:51,680 --> 00:02:57,040
We note our first merging rule and we 
add the new token to our vocabulary.  

25
00:03:00,080 --> 00:03:04,080
We can then apply this merging rule to our splits:  

26
00:03:04,080 --> 00:03:09,280
you can see that we have merged all the pairs 
of tokens composed of the tokens "l" and "e".  

27
00:03:13,840 --> 00:03:19,040
And now we just have to reproduce 
the same steps with our new splits:  

28
00:03:21,520 --> 00:03:24,640
we calculate the frequency of 
occurrence of each pair of tokens,  

29
00:03:27,760 --> 00:03:33,680
we select the pair with the highest 
frequency, we note it in our merge rules,  

30
00:03:35,760 --> 00:03:38,720
we add the new one to the vocabulary  

31
00:03:39,600 --> 00:03:46,160
and then we merge all the pairs of tokens composed 
of the token "le" and "a" into our splits.  

32
00:03:50,160 --> 00:03:59,840
And we can repeat this operation until 
we reach the desired vocabulary size.  

33
00:04:05,600 --> 00:04:13,200
Here we stopped when our vocabulary reached 21 
tokens. We can see now that the words of our  

34
00:04:13,200 --> 00:04:20,560
corpus are now divided into far fewer tokens than 
at the beginning of the training. We can see that  

35
00:04:20,560 --> 00:04:27,840
our algorithm has learned the radicals "hug" 
and "learn" and also the verbal ending "ing".  

36
00:04:29,760 --> 00:04:35,600
Now that we have learned our vocabulary and 
our merging rules, we can tokenize new texts.  

37
00:04:37,840 --> 00:04:41,120
For example, if we want to tokenize the word  

38
00:04:41,120 --> 00:04:48,480
hugs: first we'll divide it into elementary 
units so it became a sequence of characters.  

39
00:04:49,840 --> 00:04:53,680
Then we'll go through our merge rules 
until we have one that we can apply.  

40
00:04:54,480 --> 00:05:01,040
Here we can merge the letters h and u. And here 
we can merge 2 tokens to get the new token hug.  

41
00:05:02,240 --> 00:05:09,840
When we get to the end of our merge 
rule the tokenization is finished.  

42
00:05:10,640 --> 00:05:22,400
ßAnd that's it, I hope that now the BPE 
algorithm has no more secret for you!
