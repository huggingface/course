The Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by analyzing the model signature. The last steps before creating the Trainer are to define our model and some training hyperparameters. We saw how to do the first in the model API video. For the second, we use the TrainingArguments class. It only needs a path to a folder where results and checkpoints will be saved, but you can also customize all the hyperparameters the Trainer will use: learning rate, number of training epochs etc. It's then very easy to create a Trainer and launch a training. This should display a progress bar and after a few minutes (if you are running on a GPU) you should have the training finished. The result will be rather anticlimatic however, as you will only get a training loss which doesn't really tell you anything about how you model is performing. This is because we didn't specify anything metric for the evaluation. To get those metrics, we will first gather the predictions on the whole evaluation set using the predict method. It returns a namedtuple with three fields: predictions (which contains the model predictions), label_ids (which contains the labels if your dataset had them) and metrics (which is empty here). The predictions are the logits of the models for all the sentences in the dataset, so a NumPy array of shape 408 by 2. To match them with our labels, we need to take the maximum logit for each prediction (to know which of the two classes was predicted), which we do with the argmax function. Then we can use a Metric from the Datasets library: it can be loaded as easily as our dataset with the load_metric function, and it returns the evaluation metric used for the dataser we are using. We can see our model did learn something as it is 85.7% accurate. To monitor the evaluation metrics during training we need to define a compute_metrics function that does the same step as before: it takes a namedtuple with predictions and labels and must return a dictionary with the metric we want to keep track of. By passing the epoch evaluation strategy to our TrainingArguments, we tell the Trainer to evaluate at the end of every epoch. Launching a training inside a notebook will then display a progress bar and complete the table you see here as you pass every epoch.