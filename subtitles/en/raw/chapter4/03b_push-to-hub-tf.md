Hi, this is going to be a video about the push_to_hub API for Tensorflow and Keras. So, to get started, we'll open up our notebook, and the first thing you'll need to do is log in to your HuggingFace account, for example with the notebook login function. So to do that, you simply call the function, the popup will emerge, you enter your username and password, which I'm going to pull out of my password manager here, and you're logged in. The next two cells are just getting everything ready for training. So we're just going to load a dataset, we're going to tokenize that dataset, and then we're going to load our model and compile it with the standard Adam optimizer. So I'm just going to run all of those, we'll wait a few seconds, and everything should be ready for training. Okay, so now we're ready to train I'm going to show you the two ways you can push your model to the Hub. So the first is with the PushToHubCallback. So a callback in Keras is a function that's called regularly during training. You can set it to be called after a certain number of steps, or every epoch, or even just once at the end of training. So a lot of callbacks in Keras, for example, control learning rate decaying on plateau and things like that. And so this callback, by default, will save your model to the Hub once every epoch. And that's really helpful especially if your training is very long, because that means you can resume from that save, so you get this automatic cloud-saving of your model, and you can even run inference with the checkpoints of your model that have been uploaded by this callback, and that means you can, y'know, actually run some test inputs and actually see how your model works at various stages during training, which is a really nice feature. So we're going to add the PushToHubCallback, and it takes just a few arguments. So the first argument is the temporary directory that files are going to be saved to before they're uploaded to the Hub. The second argument is the tokenizer, and the third argument here is the keyword argument hub_model_id. So that's the name it's going to be saved under on the HuggingFace Hub. You can also upload to an organization account just by adding the organization name before the repository name with a slash like this. So you probably don't have permissions to upload to the Hugging Face organization, if you do please file a bug and let us know extremely urgently. But if you do have access to your own organization then you can use that same approach to upload models to their account instead of to your own personal set of models. So, once you've made your callback you simply add it to the callbacks list when you're called model.fit() and everything is uploaded for you from there, and there's nothing else to worry about. The second way to upload a model, though, is to call model.push_to_hub(). So this is more of a once-off method - it's not called regularly during training. You can just call this manually whenever you want to upload a model to the hub. So we recommend running this after the end of training, just to make sure that you have a commit message just to guarantee that this was the final version of the model at the end of training. And it just makes sure that you're working with the definitive end-of-training model and not accidentally using a model that's from a checkpoint somewhere along the way. So I'm going to run both of these cells and then I'm going to cut the video here, just because training is going to take a couple of minutes, and so I'll skip forward to the end of that, when the models have all been uploaded, and I'm gonna show you how you can access the models in the Hub and the other things you can do with them from there. Okay, we're back and our model was uploaded, both by the PushToHubCallback and also by our call to model.push_to_hub() after training. So everything's looking good! So now if we drop over to my profile on HuggingFace, and you can get there just by clicking the profile button in the dropdown, we can see that the bert-fine-tuned-cola model is here, and was updated 3 minutes ago. So it'll always be at the top of your list, because they're sorted by how recently they were updated. And we can start querying our model immediately! So the dataset we were training on is the Glue CoLA dataset, and CoLA is an acronym for Corpus of Linguistic Acceptability. So what that means is that the model is being trained to decide if a sentence is grammatically or linguistically okay, or if there's a problem with it. For example, we could say "This is a legitimate sentence" and hopefully it realizes that this is in fact a legitimate sentence. So it might take a couple of seconds for the model to load when you call it for the first time, so I might cut a couple of seconds out of this video here. Okay, we're back! The model loaded and we got an output, but there's an obvious problem here. So these labels aren't really telling us what categories the model has actually assigned to this input sentence. So if we want to fix that, we want to make sure the model config has the correct names for each of the label classes, and then we want to upload that config. So we can do that down here. To get the label_names, we can get that from the dataset we loaded, from the 'features' attribute it has. And then we can create dictionaries "id2label" and "label2id" and just assign them to the model config, and then we can just push our updated config and that'll override the existing config in the Hub repo. So that's just been done, so now if we go back here, I'm going to use a slightly different sentence because the outputs for sentences are sometimes cached, and so if we want to generate new results I'm going to use something slightly different. So let's try an incorrect sentence, so this is not valid English grammar and hopefully the model will see that. It's going to reload here, so I'm going to cut a couple of seconds here, and then we'll see what the model is going to say. Okay! So the model's confidence isn't very good, because of course we didn't really optimize our hyperparameters at all, but it has decided that this sentence is more likely to be unacceptable than acceptable. Presumably if we tried a bit harder with training we could get a much lower validation loss and therefore the model's predictions would be more precise. But let's try our original sentence again - of course, because of the caching issue we're seeing that the original answers are unchanged. So let's try a different, valid sentence. So let's try "This is a valid English sentence". And we see that now the model correctly decides that it has a very high probability of being acceptable and a very low probability of being unacceptable. So you can use this inference API even with the checkpoints that are uploaded during training, so it can be very interesting to see how the model's predictions for sample inputs change with each epoch of training. Also, the model we've uploaded is going to be accessible to you and, if it's shared publicly, to anyone else. So if you want to load that model all you, or anyone else, needs to do is just to load it in either a pipeline or you can just load it with, for example, TFAutoModelForSequenceClassification and then for the name you would just simply pass the path to the repo you want to upload - or to download, excuse me. So if I want to use this model again, if I want to load it from the hub, I just run this one line of code, the model will be downloaded and with any luck it'll be ready to fine-tune on a different dataset, make predictions with, or do anything else you wanna do. So that was a quick overview of how, after your training or during your training, you can upload models to the Hub, you can checkpoint there, you can resume training from there, and you can get inference results from the models you've uploaded. So thank you, and I hope to see you in a future video!