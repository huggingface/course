1
00:00:06,069 --> 00:00:11,580
In this video, we're going to see how to load
and fine-tune a pre-trained model.

2
00:00:11,580 --> 00:00:16,010
It's very quick, and if you've watched our
pipeline videos, which I'll link below, the

3
00:00:16,010 --> 00:00:18,330
process is very similar.

4
00:00:18,330 --> 00:00:21,990
This time, though, we're going to be using
transfer learning and doing some training

5
00:00:21,990 --> 00:00:26,660
ourselves, rather than just loading a model
and using it as-is.

6
00:00:26,660 --> 00:00:30,610
To learn more about transfer learning, head
to the 'What is transfer learning?'

7
00:00:30,610 --> 00:00:33,000
video, which we'll link below too!

8
00:00:33,000 --> 00:00:35,660
So now let's look at this code.

9
00:00:35,660 --> 00:00:40,340
To start, we pick which model we want to start
with - in this case we're going to use the

10
00:00:40,340 --> 00:00:42,540
famous, the original BERT.

11
00:00:42,540 --> 00:00:50,500
But what does this monstrosity, 'TFAutoModelForSequenceClassification'
mean?

12
00:00:50,500 --> 00:00:56,460
Well, the TF stands for TensorFlow, and the
rest means "take a language model, and stick

13
00:00:56,460 --> 00:01:00,879
a sequence classification head onto it if
it doesn't have one already".

14
00:01:00,879 --> 00:01:05,420
So what we're going to do here is load BERT,
a general language model, and then do some

15
00:01:05,420 --> 00:01:09,490
transfer learning to use it on our task of
interest.

16
00:01:09,490 --> 00:01:13,530
We load the language model with this one line
of code here, using the "from_pretrained"

17
00:01:13,530 --> 00:01:14,530
method.

18
00:01:14,530 --> 00:01:21,230
That method needs to know two things: Firstly
the name of the model you want it to load,

19
00:01:21,230 --> 00:01:29,840
and secondly how many classes your problem
has.

20
00:01:29,840 --> 00:01:33,500
If you want to follow along with the data
from our datasets videos, which I'll link

21
00:01:33,500 --> 00:01:41,200
below, then you'll have two classes, positive
and negative, and thus num_labels equals two.

22
00:01:41,200 --> 00:01:43,590
What about this "compile" thing?

23
00:01:43,590 --> 00:01:47,909
If you're familiar with Keras, you've probably
seen this already, but if not, this is one

24
00:01:47,909 --> 00:01:55,520
of its core methods - you always need to "compile"
your model before you train it.

25
00:01:55,520 --> 00:02:01,240
Compile needs to know two things: Firstly,
the loss function - what are we trying to

26
00:02:01,240 --> 00:02:02,240
optimize?

27
00:02:02,240 --> 00:02:08,509
Here, we import the sparse categorical crossentropy
loss function - that's a mouthful, but it's

28
00:02:08,509 --> 00:02:13,390
the standard loss function for any neural
network that's doing a classification task.

29
00:02:13,390 --> 00:02:18,170
It basically encourages the network to output
large values for the right class, and low

30
00:02:18,170 --> 00:02:21,080
values for the wrong classes.

31
00:02:21,080 --> 00:02:26,140
Note that you can specify the loss function
as a string, like we did with the optimizer,

32
00:02:26,140 --> 00:02:34,319
but there's a very common pitfall there - by
default, this loss assumes the output is probabilities

33
00:02:34,319 --> 00:02:39,650
after a softmax layer, but what our model
has actually output is the values before the

34
00:02:39,650 --> 00:02:50,140
softmax, often called the "logits" - you saw
these before in the videos about pipelines.

35
00:02:50,140 --> 00:02:54,580
If you get this wrong, your model won't train
and it'll be very annoying to figure out why.

36
00:02:54,580 --> 00:02:58,500
In fact, if you remember absolutely nothing
else from this video, remember to always check

37
00:02:58,500 --> 00:03:02,990
whether your model is outputting logits or
probabilities, and to make sure your loss

38
00:03:02,990 --> 00:03:05,270
is set up to match that.

39
00:03:05,270 --> 00:03:09,460
It'll save you a lot of debugging headaches
in your career!

40
00:03:09,460 --> 00:03:13,340
The second thing compile needs to know is
the optimizer you want.

41
00:03:13,340 --> 00:03:17,570
In our case, we use Adam, which is sort of
the standard optimizer for deep learning these

42
00:03:17,570 --> 00:03:18,730
days.

43
00:03:18,730 --> 00:03:22,770
The one thing you might want to change is
the learning rate, and to do that we'll need

44
00:03:22,770 --> 00:03:27,330
to import the actual optimizer rather than
just calling it by string, but we'll talk

45
00:03:27,330 --> 00:03:30,050
about that in another video, which I'll link
below.

46
00:03:30,050 --> 00:03:33,610
For now, let's just try training the model!

47
00:03:33,610 --> 00:03:35,830
So how do you train a model?

48
00:03:35,830 --> 00:03:40,670
Well, if you’ve used Keras before, this
will all be very familiar to you - but if

49
00:03:40,670 --> 00:03:43,370
not, let's look at what we're doing here.

50
00:03:43,370 --> 00:03:48,371
Fit() is pretty much the central method for
Keras models - it tells the model to break

51
00:03:48,371 --> 00:03:49,371
the data into batches and train on it.

52
00:03:49,371 --> 00:03:50,371
So the first input is tokenized text - you
will almost always be getting this from a

53
00:03:50,371 --> 00:03:52,120
tokenizer, and if you want to learn more about
that process, and what exactly the outputs

54
00:03:52,120 --> 00:03:53,120
look like, please check out our videos on
tokenizers - there'll be links below for those

55
00:03:53,120 --> 00:03:54,120
too!

56
00:03:54,120 --> 00:03:55,120
So that's our inputs, and then the second
input is our labels - this is just a one-dimensional

57
00:03:55,120 --> 00:03:56,840
Numpy or Tensorflow array of integers, corresponding
to the classes for our examples, and that’s

58
00:03:56,840 --> 00:03:57,840
it.

59
00:03:57,840 --> 00:03:58,840
If you're following along with the data from
our datasets video, there'll only be two classes,

60
00:03:58,840 --> 00:04:00,300
so this will just be zeroes and ones.

61
00:04:00,300 --> 00:04:04,870
Once we have our inputs and our labels, we
do the same thing with the validation data,

62
00:04:04,870 --> 00:04:07,120
we pass the validation inputs and the validation
labels in a tuple, then we can, if we want,

63
00:04:07,120 --> 00:04:15,390
specify details like the batch_size for training,
and then you just pass it all to model.fit()

64
00:04:15,390 --> 00:04:16,540
and let it rip.

65
00:04:16,540 --> 00:04:20,449
If everything works out, you should see a
little training progress bar as your loss

66
00:04:20,449 --> 00:04:21,670
goes down.

67
00:04:21,670 --> 00:04:26,870
And while that's running you call your boss
and tell him you’re a senior NLP machine

68
00:04:26,870 --> 00:04:30,509
learning engineer now and you’re going to
want a salary review next quarter.

69
00:04:30,509 --> 00:04:38,470
This is really all it takes to apply the power
of a massive pretrained language model to

70
00:04:38,470 --> 00:04:40,770
your NLP problem.

71
00:04:40,770 --> 00:04:42,440
Could we do better, though?

72
00:04:42,440 --> 00:04:47,180
We certainly could, with a few more advanced
Keras features like a tuned, scheduled learning

73
00:04:47,180 --> 00:04:50,889
rate we can get an even lower loss, and an
even more accurate model.

74
00:04:50,889 --> 00:04:54,039
And what do we do with our model once it's
trained?

75
00:04:54,039 --> 00:05:02,919
I'll cover this and more in the videos linked
below!
