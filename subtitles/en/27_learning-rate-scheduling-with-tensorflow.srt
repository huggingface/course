1
00:00:05,120 --> 00:00:11,440
In our other videos we talked about the basics 
of fine-tuning a language model with Tensorflow  

2
00:00:11,440 --> 00:00:18,000
(and as always, when I refer to videos I'll link 
them below). Still, can we do better? So here's  

3
00:00:18,000 --> 00:00:23,040
the code from our model fine-tuning video, and 
while it works, we could definitely tweak a couple  

4
00:00:23,040 --> 00:00:29,040
of things. By far the most important thing is the 
learning rate. In this video we'll talk about how  

5
00:00:29,040 --> 00:00:34,800
to change it, which will make your training 
much more consistently successful. In fact,  

6
00:00:36,080 --> 00:00:42,880
there are two things we want to change about the 
default learning rate for Adam. The first is that  

7
00:00:42,880 --> 00:00:51,520
it's way too high for our models - by default 
Adam uses a learning rate of 10^-3 1 e minus 3,  

8
00:00:51,520 --> 00:00:59,600
which is very high for training Transformers. 
We're going to start at 5 by 10^-5 5 e minus 5,  

9
00:00:59,600 --> 00:01:05,520
which is 20 times lower than the default. And 
secondly, we don't just want a constant learning  

10
00:01:05,520 --> 00:01:10,960
rate - we can get even better performance if we 
'decay' the learning rate down to a tiny value,  

11
00:01:10,960 --> 00:01:17,760
or even 0, over the course of training. That's 
what this PolynomialDecay schedule thing is doing.  

12
00:01:19,200 --> 00:01:20,880
That name might be intimidating, especially 
if you only vaguely remember what a polynomial  

13
00:01:21,600 --> 00:01:25,120
is from maths class. However, all we need to 
do is tell it how long training is going to be,  

14
00:01:25,120 --> 00:01:29,040
so it decays at the right speed - 
that's what this code here is doing.  

15
00:01:30,080 --> 00:01:35,280
We're computing how many minibatches the model 
is going to see over its entire training run,  

16
00:01:35,280 --> 00:01:37,640
which is the size of the training set, divided 
by the batch_size to get the number of batches  

17
00:01:37,640 --> 00:01:42,080
per epoch, and then multiplied by the 
number of epochs to get the total number  

18
00:01:42,080 --> 00:01:47,680
of batches across the whole training run. Once 
we know how many training steps we're taking,  

19
00:01:47,680 --> 00:01:51,360
we just pass all that information to 
the scheduler and we're ready to go.  

20
00:01:54,000 --> 00:01:57,360
What does the polynomial decay schedule look 
like? With default options, it's actually just a  

21
00:01:57,360 --> 00:02:04,720
linear schedule, so it looks like this - it starts 
at 5e-5, which means 5 times ten to the minus 5,  

22
00:02:05,280 --> 00:02:11,120
and then decays down at a constant rate until 
it hits zero right at the very end of training.  

23
00:02:11,120 --> 00:02:33,920
So why do they call it polynomial and not 
linear? Because if you tweak the options,  

24
00:02:36,000 --> 00:02:49,840
you can get a higher-order decay schedule, but 
there's no need to do that right now. Now, how  

25
00:02:49,840 --> 00:02:56,400
do we use our learning rate schedule? Easy, 
we just pass it to Adam! You'll notice the  

26
00:02:56,400 --> 00:03:00,480
first time when we compiled the model, 
we just passed it the string "adam".  

27
00:03:02,320 --> 00:03:07,760
Keras recognizes the names of common optimizers 
and loss functions if you pass them as strings,  

28
00:03:07,760 --> 00:03:12,320
so it saves time to do that if you only want 
the default settings. But we're professional  

29
00:03:12,320 --> 00:03:19,600
machine learners now, with our very own learning 
rate schedule, so we have to do things properly.  

30
00:03:19,600 --> 00:03:26,080
So first we import the optimizer, then we 
initialize it with our scheduler, and then  

31
00:03:29,200 --> 00:03:34,720
we compile the model using the new optimizer, 
and whatever loss function you want - this will  

32
00:03:34,720 --> 00:03:39,040
be sparse categorical crossentropy if you're 
following along from the fine-tuning video.  

33
00:03:39,680 --> 00:03:47,120
And now we have a high-performance model, ready to 
go. All that remains is to fit the model just like  

34
00:03:47,120 --> 00:03:53,280
we did before! Remember, because we compiled the 
model with the new optimizer with the new learning  

35
00:03:53,280 --> 00:03:58,800
rate schedule, we don't need to change anything 
here. We just call fit again, with exactly the  

36
00:03:58,800 --> 00:04:04,320
same command as before, but now we get beautiful 
training with a nice, smooth learning rate decay.
