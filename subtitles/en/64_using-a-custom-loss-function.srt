1
00:00:05,440 --> 00:00:09,040
In this video we take a look at setting 
up a custom loss function for training.  

2
00:00:10,800 --> 00:00:14,800
In the default loss functions all 
samples such as these code snippets  

3
00:00:14,800 --> 00:00:19,040
are treated the same irrespective of their 
content, but there are scenarios where you it  

4
00:00:19,040 --> 00:00:22,880
could make sense to weight the samples 
differently. If for example one sample  

5
00:00:22,880 --> 00:00:28,800
contains a lot of tokens that or of interest to 
us or it has a favourable diversity of tokens.  

6
00:00:29,680 --> 00:00:33,520
We can also think of other heuristics we can 
implement with pattern matching or other rules.  

7
00:00:36,080 --> 00:00:40,400
For each sample we get a loss value during 
training and we can combine that loss with  

8
00:00:40,400 --> 00:00:47,200
a weight. Then we can for example create a 
weighted sum to get the final loss for a batch.  

9
00:00:48,480 --> 00:00:53,280
Let’s have a look at a specific example: we 
want to setup a language model that helps us  

10
00:00:53,280 --> 00:01:00,800
autocomplete complete common data science code. 
For that task we would like to weight samples  

11
00:01:00,800 --> 00:01:06,960
stronger where tokens related to the data science 
stack, such as pd or np, occur more frequently.  

12
00:01:10,000 --> 00:01:14,788
Here you see a loss function that does exactly 
that for causal language modeling. It takes the  

13
00:01:14,788 --> 00:01:22,800
models it takes the model’s inputs and predicted 
logits as well as the key tokens as input. First  

14
00:01:22,800 --> 00:01:30,320
the inputs and logits are aligned, then the loss 
per sample is calculate followed by the weights.  

15
00:01:32,320 --> 00:01:35,280
Finally the loss and weights 
are combined and returned.  

16
00:01:36,320 --> 00:01:40,480
This is a pretty big function so let’s take 
a closer look at the loss and weight blocks.  

17
00:01:43,200 --> 00:01:47,920
During the calculation of the standard loss the 
logits and labels are flattened over the batch.  

18
00:01:48,720 --> 00:01:53,280
With the view we unflatten the tensor 
to get a matrix with a row for each  

19
00:01:53,280 --> 00:01:57,280
sample in the batch and a column for each 
position in the sequence of the samples.  

20
00:01:58,720 --> 00:02:03,600
We don’t need the loss per position so we average 
the loss over all positions for each sample.  

21
00:02:06,000 --> 00:02:10,960
For the weights we use boolean logic to get 
a tensor with 1s where a keyword occurred  

22
00:02:10,960 --> 00:02:17,840
and 0s where not. This tensor has an additional 
dimension as the loss tensor we just saw because  

23
00:02:17,840 --> 00:02:24,480
we get the information for each keyword in a 
separate matrix. Only want to know how many  

24
00:02:24,480 --> 00:02:30,320
times keywords occurred per sample so we can sum 
over all keywords and all positions per sample.  

25
00:02:33,280 --> 00:02:39,760
Now we are almost there, we only need to combine 
the loss with the weight per sample. We do this  

26
00:02:39,760 --> 00:02:43,920
with element wise multiplication and then 
average over all samples in the batch.  

27
00:02:44,720 --> 00:02:48,000
In the end we have exactly one 
loss value for the whole batch.  

28
00:02:48,880 --> 00:02:52,800
And this is the whole necessary logic 
to create a custom weighted loss.  

29
00:02:56,080 --> 00:03:02,640
Let’s see how we can make use of that custom loss 
with Accelerate and the Trainer In Accelerate we  

30
00:03:02,640 --> 00:03:07,680
just pass the input_ids to the models to get the 
logits and can then call the custom loss function.  

31
00:03:08,800 --> 00:03:12,800
After that we continue with the normal 
training loop by for example calling backward.  

32
00:03:13,840 --> 00:03:19,200
For the Trainer we can overwrite the compute 
loss function of the standard trainer. We  

33
00:03:19,200 --> 00:03:23,360
just need to make sure that that we return the 
loss and the model outputs in the same format.  

34
00:03:24,240 --> 00:03:31,840
With that you can integrate your own awesome loss 
function with both the trainer and accelerates.
