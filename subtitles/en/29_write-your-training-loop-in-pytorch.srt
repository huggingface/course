1
00:00:05,430 --> 00:00:07,240
Write your own training loop in PyTorch.

2
00:00:07,240 --> 00:00:11,759
In this video, we will look at how we can
do the same fine-tuning as in the Trainer

3
00:00:11,759 --> 00:00:14,120
video, but without relying on that class.

4
00:00:14,120 --> 00:00:20,369
This way you will be able to easily customize
each step of the training loop to your needs.

5
00:00:20,369 --> 00:00:23,859
This is also very useful to manually debug
something that went wrong with the Trainer

6
00:00:23,859 --> 00:00:26,189
API.

7
00:00:26,189 --> 00:00:31,200
Before we dive into the code, here is a sketch
of a training loop: we take a batch of training

8
00:00:31,200 --> 00:00:33,469
data and feed it to the model.

9
00:00:33,469 --> 00:00:36,600
With the labels, we can then compute a loss.

10
00:00:36,600 --> 00:00:41,130
That number is not useful on its own, but
is used to compute the gradients of our model

11
00:00:41,130 --> 00:00:46,750
weights, that is the derivative of the loss
with respect to each model weight.

12
00:00:46,750 --> 00:00:51,920
Those gradients are then used by the optimizer
to update the model weights and make them

13
00:00:51,920 --> 00:00:53,360
a little bit better.

14
00:00:53,360 --> 00:00:56,170
We then repeat the process with a new batch
of training data.

15
00:00:56,170 --> 00:01:00,969
If any of this is unclear, don't hesitate
to take a refresher on your favorite deep

16
00:01:00,969 --> 00:01:02,240
learning course.

17
00:01:02,240 --> 00:01:07,560
We will use the GLUE MRPC dataset here again,
and we have seen how to preprocess the data

18
00:01:07,560 --> 00:01:10,439
using the Datasets library with dynamic padding.

19
00:01:10,439 --> 00:01:15,549
Checkout the videos linked below if you haven't
seen them already.

20
00:01:15,549 --> 00:01:20,060
With this done, we only have to define PyTorch
DataLoaders, which will be responsible to

21
00:01:20,060 --> 00:01:24,480
convert the elements of our dataset into batches.

22
00:01:24,480 --> 00:01:33,890
We use our DataCollatorForPadding as the collate
function, and shuffle the training set.

23
00:01:33,890 --> 00:01:39,460
To check that everything works as intended,
we try to grab a batch of data and inspect

24
00:01:39,460 --> 00:01:40,460
it.

25
00:01:40,460 --> 00:01:44,790
Like our dataset elements, it's a dictionary,
but this time the values are not a single

26
00:01:44,790 --> 00:01:50,460
list of integers, but a tensor of shape batch
size by sequence length.

27
00:01:50,460 --> 00:01:52,869
The next step is to send the training data
in our model.

28
00:01:52,869 --> 00:01:56,790
For that, we will need to create our model.

29
00:01:56,790 --> 00:02:01,240
As seen in the model API video, we use the
from_pretrained method and adjust the number

30
00:02:01,240 --> 00:02:06,159
of labels to the number of classes we have
on this dataset, here two.

31
00:02:06,159 --> 00:02:11,020
Again, to be sure everything is going well,
we pass the batch we grabbed to our model

32
00:02:11,020 --> 00:02:12,640
and check there is no error.

33
00:02:12,640 --> 00:02:17,780
If the labels are provided, the models of
the Transformers library always return the

34
00:02:17,780 --> 00:02:18,840
loss directly.

35
00:02:18,840 --> 00:02:24,129
We will be able to do loss.backward() to compute
all the gradients, and will then need an optimizer

36
00:02:24,129 --> 00:02:26,480
to do the training step.

37
00:02:26,480 --> 00:02:30,800
We use the AdamW optimizer here, which is
a variant of Adam with proper weight decay,

38
00:02:30,800 --> 00:02:35,040
but you can pick any PyTorch optimizer you
like.

39
00:02:35,040 --> 00:02:39,519
Using the previous loss and computing the
gradients with loss.backward(), we check that

40
00:02:39,519 --> 00:02:43,510
we can do the optimizer step without any error.

41
00:02:43,510 --> 00:02:47,580
Don't forget to zero your gradient afterward,
or at the next step they will get added to

42
00:02:47,580 --> 00:02:49,659
the gradients you compute!

43
00:02:49,659 --> 00:02:53,620
We could already write our training loop,
but we will add two more things to make it

44
00:02:53,620 --> 00:02:55,590
as good as it can be.

45
00:02:55,590 --> 00:03:01,150
The first one is a learning rate scheduler,
to progressively decay our learning rate to

46
00:03:01,150 --> 00:03:02,150
zero.

47
00:03:02,150 --> 00:03:06,180
The get_scheduler function from the Transformers
library is just a convenience function to

48
00:03:06,180 --> 00:03:12,760
easily build such a scheduler, you can again
use any PyTorch learning rate scheduler instead.

49
00:03:12,760 --> 00:03:17,299
Finally, if we want our training to take a
couple of minutes instead of a few hours,

50
00:03:17,299 --> 00:03:19,580
we will need to use a GPU.

51
00:03:19,580 --> 00:03:24,340
The first step is to get one, for instance
by using a colab notebook.

52
00:03:24,340 --> 00:03:29,090
Then you need to actually send your model
and training data on it by using a torch device.

53
00:03:29,090 --> 00:03:35,659
Double-check the following lines print a CUDA
device for you!

54
00:03:35,659 --> 00:03:38,450
We can now put everything together!

55
00:03:38,450 --> 00:03:42,470
First we put our model in training mode (which
will activate the training behavior for some

56
00:03:42,470 --> 00:03:47,900
layers like Dropout) then go through the number
of epochs we picked and all the data in our

57
00:03:47,900 --> 00:03:50,130
training dataloader.

58
00:03:50,130 --> 00:03:54,560
Then we go through all the steps we have seen
already: send the data to the GPU, compute

59
00:03:54,560 --> 00:03:57,870
the model outputs, and in particular the loss.

60
00:03:57,870 --> 00:04:02,040
Use the loss to compute gradients, then make
a training step with the optimizer.

61
00:04:02,040 --> 00:04:06,760
Update the learning rate in our scheduler
for the next iteration and zero the gradients

62
00:04:06,760 --> 00:04:09,340
of the optimizer.

63
00:04:09,340 --> 00:04:13,590
Once this is finished, we can evaluate our
model very easily with a metric from the Datasets

64
00:04:13,590 --> 00:04:14,730
library.

65
00:04:14,730 --> 00:04:22,470
First we put our model in evaluation mode,
then go through all the data in the evaluation

66
00:04:22,470 --> 00:04:23,900
data loader.

67
00:04:23,900 --> 00:04:27,480
As we have seen in the Trainer video, the
model outputs logits and we need to apply

68
00:04:27,480 --> 00:04:31,350
the argmax function to convert them into predictions.

69
00:04:31,350 --> 00:04:36,910
The metric object then has an add_batch method
we can use to send it those intermediate predictions.

70
00:04:36,910 --> 00:04:40,590
Once the evaluation loop is finished, we just
have to call the compute method to get our

71
00:04:40,590 --> 00:04:41,620
final results!

72
00:04:41,620 --> 00:04:50,760
Congratulations, you have now fine-tuned a
model all by yourself!
