1
00:00:05,360 --> 00:00:10,720
Let's see how to preprocess a dataset 
for summarization. This is the task of  

2
00:00:10,720 --> 00:00:16,976
well summarizing a long document. This video will 
focus on how to preprocess your dataset once you  

3
00:00:16,976 --> 00:00:21,840
have managed to put it in the following format: 
one column for the long documents, and one for  

4
00:00:21,840 --> 00:00:27,360
the summaries. Here is how we can achieve this 
with the Datasets library on the XSUM dataset.  

5
00:00:28,400 --> 00:00:32,400
As long as you manage to have your data look like 
this, you should be able to follow the same steps.  

6
00:00:33,520 --> 00:00:37,280
For once, our labels are not integers 
corresponding to some classes,  

7
00:00:37,280 --> 00:00:43,120
but plain text. We will thus need to tokenize 
them, like our inputs. There is a small trap  

8
00:00:43,120 --> 00:00:47,760
there though, as we need to tokenize our targets 
inside the as_target_tokenzier context manager.  

9
00:00:48,480 --> 00:00:53,200
This is because the special tokens we add might be 
slightly different for the inputs and the targets,  

10
00:00:53,760 --> 00:00:58,320
so the tokenizer has to know which one it 
is processing. Processing the whole dataset  

11
00:00:58,320 --> 00:01:03,520
is then super easy with the map function. Since 
the summaries are usually much shorter than the  

12
00:01:03,520 --> 00:01:07,840
documents, you should definitely pick different 
maximum lengths for the inputs and targets.  

13
00:01:08,640 --> 00:01:12,640
You can choose to pad at this stage to that 
maximum length by setting padding=max_length.  

14
00:01:13,840 --> 00:01:17,360
Here we will show you how to pad 
dynamically as it requires one more step.  

15
00:01:18,640 --> 00:01:23,360
Your inputs and targets are all sentence of 
various lengths. We will pad the inputs and  

16
00:01:23,360 --> 00:01:27,920
targets separately as the maximum length of the 
inputs and targets are completely different.  

17
00:01:28,880 --> 00:01:32,320
Then we pad the inputs to the 
maximum lengths among the inputs,  

18
00:01:32,320 --> 00:01:38,800
and same for the targets. We pad the inputs with 
the pad token and the targets with the -100 index,  

19
00:01:38,800 --> 00:01:44,400
to make sure they are not taken into account in 
the loss computation. The Transformers library  

20
00:01:44,400 --> 00:01:49,200
provides us with a data collator to do this 
all automatically. You can then pass it to  

21
00:01:49,200 --> 00:01:55,440
the Trainer with your datasets, or use it in the 
to_tf_dataset method before using model.fit().
