1
00:00:06,560 --> 00:00:11,600
Saving and reloading a dataset. In this video 
we'll take a look saving a dataset in various  

2
00:00:11,600 --> 00:00:19,200
formats, and explore the ways to reload the saved 
data. When you download a dataset, the processing  

3
00:00:19,200 --> 00:00:23,920
scripts and data are stored locally on your 
computer. The cache allows the Datasets library  

4
00:00:23,920 --> 00:00:29,600
to avoid re-downloading or processing the entire 
dataset every time you use it. The data is stored  

5
00:00:29,600 --> 00:00:34,080
in the form of Arrow tables whose location can 
be found by accessing the dataset's cache_files  

6
00:00:34,080 --> 00:00:39,360
attribute. In this example, we've downloaded 
the allocine dataset from the Hugging Face Hub  

7
00:00:39,360 --> 00:00:43,840
and you can see there are three Arrow files 
stored in the cache, one for each split.  

8
00:00:45,120 --> 00:00:48,720
But in many cases, you'll want to save your 
dataset in a different location or format.  

9
00:00:49,600 --> 00:00:53,760
As shown in the table, the Datasets library 
provides four main functions to achieve this.  

10
00:00:54,880 --> 00:00:59,040
You're probably familiar with the CSV and JSON 
formats, both of which are great if you want  

11
00:00:59,040 --> 00:01:04,800
to save small to medium-sized datasets. But 
if your dataset is huge, you'll want to save  

12
00:01:04,800 --> 00:01:09,520
it in either the Arrow or Parquet formats. 
Arrow files are great if you plan to reload  

13
00:01:09,520 --> 00:01:14,080
or process the data in the near future. Parquet 
files are designed for long-term disk storage  

14
00:01:14,080 --> 00:01:17,440
and are very space efficient. Let's 
take a closer look at each format.  

15
00:01:19,520 --> 00:01:25,520
To save a Dataset or a DatasetDict object in the 
Arrow format we use the save_to_disk function. As  

16
00:01:25,520 --> 00:01:30,240
you can see in this example, we simply provide the 
path we wish to save the data to, and the Datasets  

17
00:01:30,240 --> 00:01:34,720
library will automatically create a directory for 
each split to store the Arrow table and metadata.  

18
00:01:35,600 --> 00:01:38,880
Since we're dealing with a DatasetDict 
object that has multiple splits,  

19
00:01:38,880 --> 00:01:41,920
this information is also stored 
in the dataset_dict.json file.  

20
00:01:44,160 --> 00:01:48,000
Now when we want to reload the Arrow 
datasets, we use the load_from_disk function.  

21
00:01:48,640 --> 00:01:53,840
We simply pass the path of our dataset directory 
and voila the original dataset is recovered!  

22
00:01:55,760 --> 00:01:59,920
If we want to save our datasets in the 
CSV format we use the to_csv function.  

23
00:02:00,800 --> 00:02:05,280
In this case you'll need to loop over the splits 
of the DatasetDict object and save each dataset as  

24
00:02:05,280 --> 00:02:11,280
an individual CSV file. Since the to_csv file 
is based on the one from Pandas, you can pass  

25
00:02:11,280 --> 00:02:16,240
keyword arguments to configure the output. In 
this example, we've set the index argument to  

26
00:02:16,240 --> 00:02:23,440
None to prevent the dataset's index column from 
being included in the CSV files. To reload our CSV  

27
00:02:23,440 --> 00:02:29,760
files, we use the load_dataset function together 
with the csv loading script and data_files  

28
00:02:29,760 --> 00:02:35,120
argument which specifies the filenames associated 
with each split. As you can see in this example,  

29
00:02:35,120 --> 00:02:39,280
by providing all the splits and their filenames, 
we've recovered the original DatasetDict object.  

30
00:02:41,840 --> 00:02:45,920
To save a dataset in the JSON or Parquet 
formats is very similar to the CSV case.  

31
00:02:46,480 --> 00:02:52,720
We use either the to_json function for JSON files 
or the to_parquet function for Parquet ones. And  

32
00:02:52,720 --> 00:02:57,440
just like the CSV case, we need to loop over the 
splits and save each one as an individual file.  

33
00:02:59,680 --> 00:03:03,760
Once our datasets are saved as JSON or 
Parquet files, we can reload them again  

34
00:03:03,760 --> 00:03:09,680
with the appropriate script in the load_dataset 
function, and a data_files argument as before.  

35
00:03:10,640 --> 00:03:14,160
This example shows how we can reload 
our saved datasets in either format.  

36
00:03:16,400 --> 00:03:26,000
And with that you now know how to 
save your datasets in various formats!
