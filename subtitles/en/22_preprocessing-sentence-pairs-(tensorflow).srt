1
00:00:05,440 --> 00:00:11,760
How to preprocess pairs of sentences? We have 
seen how to tokenize single sentences and batch  

2
00:00:11,760 --> 00:00:17,280
them together in the "Batching inputs together" 
video. If this code look unfamiliar to you,  

3
00:00:17,840 --> 00:00:23,680
be sure to check that video again! Here we will 
focus on tasks that classify pairs of sentences.  

4
00:00:24,720 --> 00:00:30,400
For instance, we may want to classify whether two 
texts are paraphrases or not. Here is an example  

5
00:00:30,400 --> 00:00:35,520
taken from the Quora Question Pairs dataset, 
which focuses on identifying duplicate questions.  

6
00:00:36,960 --> 00:00:39,760
In the first pair, the two 
questions are duplicates;  

7
00:00:40,400 --> 00:00:45,520
in the second, they are not. Another pair 
classification problem is when we want to know  

8
00:00:45,520 --> 00:00:51,920
if two sentences are logically related or not (a 
problem called Natural Language Inference or NLI).  

9
00:00:52,880 --> 00:00:58,480
In this example taken from the MultiNLI dataset, 
we have a pair of sentences for each possible  

10
00:00:58,480 --> 00:01:04,560
label: contradiction, neutral or entailment 
(which is a fancy way of saying the first sentence  

11
00:01:04,560 --> 00:01:12,240
implies the second). So classifying pairs of 
sentences is a problem worth studying. In fact,  

12
00:01:12,240 --> 00:01:15,840
in the GLUE benchmark (which is an academic 
benchmark for text classification),  

13
00:01:16,640 --> 00:01:20,800
8 of the 10 datasets are focused 
on tasks using pairs of sentences.  

14
00:01:21,920 --> 00:01:26,320
That's why models like BERT are often 
pretrained with a dual objective:  

15
00:01:26,320 --> 00:01:31,040
on top of the language modeling objective, they 
often have an objective related to sentence pairs.  

16
00:01:31,840 --> 00:01:37,520
For instance, during pretraining, BERT is shown 
pairs of sentences and must predict both the  

17
00:01:37,520 --> 00:01:42,080
value of randomly masked tokens and whether 
the second sentence follows from the first.  

18
00:01:43,840 --> 00:01:47,920
Fortunately, the tokenizer from the 
Transformers library has a nice API  

19
00:01:47,920 --> 00:01:53,840
to deal with pairs of sentences: you just have 
to pass them as two arguments to the tokenizer.  

20
00:01:54,640 --> 00:01:59,040
On top of the input IDs and the attention 
mask we studied already, it returns a new  

21
00:01:59,040 --> 00:02:04,320
field called token type IDs, which tells the 
model which tokens belong to the first sentence  

22
00:02:04,880 --> 00:02:11,280
and which ones belong to the second sentence. 
Zooming in a little bit, here are the input IDs,  

23
00:02:11,280 --> 00:02:16,800
aligned with the tokens they correspond to, their 
respective token type ID and attention mask.  

24
00:02:18,240 --> 00:02:23,440
We can see the tokenizer also added special 
tokens so we have a CLS token, the tokens  

25
00:02:23,440 --> 00:02:29,920
from the first sentence, a SEP token, the tokens 
from the second sentence, and a final SEP token.  

26
00:02:31,440 --> 00:02:36,640
If we have several pairs of sentences, we can 
tokenize them together by passing the list of  

27
00:02:36,640 --> 00:02:42,880
first sentences, then the list of second sentences 
and all the keyword arguments we studied already,  

28
00:02:42,880 --> 00:02:48,800
like padding=True. Zooming in at the result, 
we can see how the tokenizer added padding  

29
00:02:48,800 --> 00:02:52,480
to the second pair of sentences, to 
make the two outputs the same length,  

30
00:02:53,440 --> 00:02:57,280
and properly dealt with token type IDS 
and attention masks for the two sentences.  

31
00:02:58,720 --> 00:03:03,840
This is then all ready to pass through our model!
