1
00:00:06,080 --> 00:00:11,600
Loading a custom dataset. Although the Hugging 
Face Hub hosts over a thousand public datasets,  

2
00:00:11,600 --> 00:00:15,040
you'll often need to work with data that is 
stored on your laptop or some remote server.  

3
00:00:15,760 --> 00:00:19,520
In this video we'll explore how the Datasets 
library can be used to load datasets that  

4
00:00:19,520 --> 00:00:24,800
aren’t available on the Hugging Face Hub. 
As you can see in this table, the Datasets  

5
00:00:24,800 --> 00:00:30,080
library provides several in-built scripts to load 
datasets in several formats. To load a dataset in  

6
00:00:30,080 --> 00:00:34,160
one of these formats, you just need to provide the 
name of the format to the load_dataset function,  

7
00:00:34,160 --> 00:00:38,000
along with a data_files argument that 
points to one or more filepaths or URLs.  

8
00:00:40,080 --> 00:00:44,400
To see this in action, let's start by 
loading a local CSV file. In this example,  

9
00:00:44,400 --> 00:00:48,720
we first download a dataset about wine quality 
from the UCI machine learning repository.  

10
00:00:50,080 --> 00:00:56,000
Since this is a CSV file, we then specify the 
csv loading script. This script needs to know  

11
00:00:56,000 --> 00:01:00,160
where our data is located, so we provide the 
filename as part of the data_files argument.  

12
00:01:01,920 --> 00:01:05,760
The CSV loading script also allows you to pass 
several keyword arguments, so here we've also  

13
00:01:05,760 --> 00:01:10,640
specified the separator as a semi-colon. And 
with that we can see the dataset is loaded  

14
00:01:10,640 --> 00:01:15,360
automatically as a DatasetDict object, with each 
column in the CSV file represented as a feature.  

15
00:01:17,360 --> 00:01:21,760
If your dataset is located on some remote 
server like GitHub or some other repository,  

16
00:01:21,760 --> 00:01:26,320
the process is very similar. The only difference 
is that now the data_files argument points to a  

17
00:01:26,320 --> 00:01:33,600
URL instead of a local filepath. Let's now take 
a look at loading raw text files. This format  

18
00:01:33,600 --> 00:01:37,840
is quite common in NLP and you'll typically 
find books and plays are just a single file  

19
00:01:37,840 --> 00:01:43,040
with raw text inside. In this example, we 
have a text file of Shakespeare plays that's  

20
00:01:43,040 --> 00:01:48,880
stored on a GitHub repository. As we did for CSV 
files, we simply choose the text loading script  

21
00:01:48,880 --> 00:01:54,080
and point the data_files argument to the URL. 
As you can see, these files are processed  

22
00:01:54,080 --> 00:01:58,640
line-by-line, so empty lines in the raw text 
are also represented as a row in the dataset.  

23
00:02:00,560 --> 00:02:05,840
For JSON files, there are two main formats to 
know about. The first one is called JSON Lines,  

24
00:02:05,840 --> 00:02:10,880
where every row in the file is a separate JSON 
object. For these files, you can load the dataset  

25
00:02:10,880 --> 00:02:15,760
by selecting the json loading script and pointing 
the data_files argument to the file or URL.  

26
00:02:16,960 --> 00:02:21,840
In this example, we've loaded a JSON lines files 
based on Stack Exchange questions and answers.
