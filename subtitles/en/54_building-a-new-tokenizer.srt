1
00:00:05,350 --> 00:00:11,360
In this video we will see how you can create
your own tokenizer from scratch!

2
00:00:11,360 --> 00:00:18,370
To create your own tokenizer you will have
to think about each of the operations involved

3
00:00:18,370 --> 00:00:25,220
in tokenization, namely: normalization, pre-tokenization,
model, post-processing and decoding.

4
00:00:25,220 --> 00:00:32,310
If you don't know what normalization, pre-tokenization
and the model are, I advise you to go and

5
00:00:32,310 --> 00:00:34,800
see the videos linked below.

6
00:00:34,800 --> 00:00:40,329
The post processing gathers all the modifications
that we will carry out on the tokenized text.

7
00:00:40,329 --> 00:00:46,690
It can include the addition of special tokens,
the creation of an attention mask but also

8
00:00:46,690 --> 00:00:50,200
the generation of a list of token ids.

9
00:00:50,200 --> 00:00:55,350
The decoding operation occurs at the very
end and will allow passing from the sequence

10
00:00:55,350 --> 00:00:59,000
of ids in a sentence.

11
00:00:59,000 --> 00:01:04,220
For example, in our example, we can see that
the hashtags have been removed and the tokens

12
00:01:04,220 --> 00:01:10,820
composing the word "today" have been grouped
together.

13
00:01:10,820 --> 00:01:17,472
In a fast tokenizer, all these components
are gathered in the backend_tokenizer attribute.

14
00:01:17,472 --> 00:01:22,720
As you can see with this small code snippet,
it is an instance of a tokenizer from the

15
00:01:22,720 --> 00:01:24,860
tokenizers library.

16
00:01:24,860 --> 00:01:33,799
So, to create your own transformers tokenizer
you will have to follow these steps: first

17
00:01:33,799 --> 00:01:40,510
create a training dataset; second create and
train a tokenizer with the tokenizers library

18
00:01:40,510 --> 00:01:49,430
and third load this tokenizer into transformers
tokenizer.

19
00:01:49,430 --> 00:01:56,510
To understand these steps, I propose that
we recreate a BERT tokenizer.

20
00:01:56,510 --> 00:01:59,500
The first thing to do is to create a dataset.

21
00:01:59,500 --> 00:02:05,650
With this code snippet you can create an iterator
on the dataset wikitext-2-raw-v1 which is

22
00:02:05,650 --> 00:02:08,610
a rather small dataset in English.

23
00:02:08,610 --> 00:02:18,830
We attack here the big part: the design of
our tokenizer with the tokenizers library.

24
00:02:18,830 --> 00:02:25,349
We start by initializing a tokenizer instance
with a WordPiece model because it is the model

25
00:02:25,349 --> 00:02:29,240
used by BERT.

26
00:02:29,240 --> 00:02:32,110
Then we can define our normalizer.

27
00:02:32,110 --> 00:02:39,930
We will define it as a succession of 2 normalizations
used to clean up characters not visible in

28
00:02:39,930 --> 00:02:46,659
the text, 1 lowercasing normalization and
2 normalizations used to remove accents.

29
00:02:46,659 --> 00:02:54,459
For the pre-tokenization, we will chain two
pre_tokenizer.

30
00:02:54,459 --> 00:02:59,959
The first one separating the text at the level
of spaces and the second one isolating the

31
00:02:59,959 --> 00:03:02,450
punctuation marks.

32
00:03:02,450 --> 00:03:08,430
Now, we can define the trainer that will allow
us to train the WordPiece model chosen at

33
00:03:08,430 --> 00:03:11,209
the beginning.

34
00:03:11,209 --> 00:03:17,280
To carry out the training, we will have to
choose a vocabulary size, here we choose twenty-five

35
00:03:17,280 --> 00:03:29,099
thousand and also announce the special tokens
that we absolutely want to add to our vocabulary.

36
00:03:29,099 --> 00:03:39,209
In one line of code, we can train our WordPiece
model using the iterator we defined earlier.

37
00:03:39,209 --> 00:03:45,800
Once the model has been trained, we can retrieve
the ids of the special class and separation

38
00:03:45,800 --> 00:03:49,750
tokens because we will need them to post-process
our sequence.

39
00:03:49,750 --> 00:03:55,790
Thanks to the TemplateProcessing class, we
can add the CLS token at the beginning of

40
00:03:55,790 --> 00:04:01,780
each sequence and the SEP token at the end
of the sequence and between two sentences

41
00:04:01,780 --> 00:04:07,060
if we tokenize a text pair.

42
00:04:07,060 --> 00:04:12,099
Finally, we just have to define our decoder
which will allow us to remove the hashtags

43
00:04:12,099 --> 00:04:17,810
at the beginning of the tokens that must be
reattached to the previous token.

44
00:04:17,810 --> 00:04:30,930
And there it ist, you have all the necessary
lines of code to define your own tokenizer.

45
00:04:30,930 --> 00:04:35,120
Now that we have a brand new tokenizer with
the tokenizers library we just have to load

46
00:04:35,120 --> 00:04:40,070
it into a fast tokenizer from the transformers
library.

47
00:04:40,070 --> 00:04:42,660
Here again we have several possibilities.

48
00:04:42,660 --> 00:04:48,830
We can load it in the generic class "PreTrainedTokenizerFast"
or in the BertTokenizerFast class since we

49
00:04:48,830 --> 00:04:56,380
have built a bert type tokenizer here.

50
00:04:56,380 --> 00:05:01,600
I hope this video has helped you understand
how you can create your own tokenizer and

51
00:05:01,600 --> 00:05:10,669
that you are ready to navigate the tokenizers
library documentation to choose the components

52
00:05:10,669 --> 00:05:16,490
for your brand-new tokenizer!
