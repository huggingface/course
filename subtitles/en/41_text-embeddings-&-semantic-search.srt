1
00:00:05,520 --> 00:00:11,200
Text embeddings and semantic search. In this video 
we’ll explore how Transformer models represent  

2
00:00:11,200 --> 00:00:15,920
text as embedding vectors and how these vectors 
can be used to find similar documents in a corpus.  

3
00:00:17,520 --> 00:00:22,000
Text embeddings are just a fancy way of saying 
that we can represent text as an array of numbers  

4
00:00:22,000 --> 00:00:27,120
called a vector. To create these embeddings we 
usually use an encoder-based model like BERT.  

5
00:00:28,320 --> 00:00:32,320
In this example, you can see how we feed 
three sentences to the encoder and get  

6
00:00:32,320 --> 00:00:36,400
three vectors as the output. Reading 
the text, we can see that walking the  

7
00:00:36,400 --> 00:00:40,880
dog seems to be most similar to walking the 
cat, but let's see if we can quantify this!  

8
00:00:42,560 --> 00:00:46,080
The trick to do the comparison is to 
compute a similarity metric between each  

9
00:00:46,080 --> 00:00:50,880
pair of embedding vectors. These vectors 
usually live in a high-dimensional space,  

10
00:00:50,880 --> 00:00:54,640
so a similarity metric can be anything that 
measures some sort of distance between vectors.  

11
00:00:55,520 --> 00:01:00,560
One popular metric is cosine similarity, which 
uses the angle between two vectors to measure  

12
00:01:00,560 --> 00:01:06,160
how close they are. In this example, our embedding 
vectors live in 3D and we can see that the orange  

13
00:01:06,160 --> 00:01:12,080
and grey vectors are close to each other and have 
a smaller angle. Now one problem we have to deal  

14
00:01:12,080 --> 00:01:16,640
with is that Transformer models like BERT will 
actually return one embedding vector per token.  

15
00:01:17,680 --> 00:01:22,560
For example in the sentence "I took my dog for a 
walk", we can expect several embedding vectors,  

16
00:01:22,560 --> 00:01:28,880
one for each word. For example, here we can see 
the output of our model has produced 9 embedding  

17
00:01:28,880 --> 00:01:35,200
vectors per sentence, and each vector has 384 
dimensions. But what we really want is a single  

18
00:01:35,200 --> 00:01:41,040
embedding vector for the whole sentence. To deal 
with this, we can use a technique called pooling.  

19
00:01:41,760 --> 00:01:45,840
The simplest pooling method is to just 
take the token embedding of the CLS token.  

20
00:01:46,880 --> 00:01:50,160
Alternatively, we can average the 
token embeddings which is called  

21
00:01:50,160 --> 00:01:56,400
mean pooling. With mean pooling only thing 
we need to make sure is that we don't include  

22
00:01:56,400 --> 00:02:00,640
the padding tokens in the average, which is why 
you can see the attention mask being used here.  

23
00:02:01,680 --> 00:02:07,160
This now gives us one 384 dimensional vector 
per sentence which is exactly what we want! And  

24
00:02:07,840 --> 00:02:12,240
once we have our sentence embeddings, we can 
compute the cosine similarity for each pair of  

25
00:02:12,240 --> 00:02:17,520
vectors. In this example we use the function from 
scikit-learn and you can see that the sentence "I  

26
00:02:17,520 --> 00:02:22,400
took my dog for a walk" has an overlap of 0.83 
with "I took my cat for a walk". Hooray! We  

27
00:02:25,040 --> 00:02:29,600
can take this idea one step further by comparing 
the similarity between a question and a corpus  

28
00:02:29,600 --> 00:02:36,000
of documents. For example, suppose we embed every 
post in the Hugging Face forums. We can then ask a  

29
00:02:36,000 --> 00:02:41,600
question, embed it, and check which forum posts 
are most similar. This process is often called  

30
00:02:41,600 --> 00:02:48,000
semantic search, because it allows us to compare 
queries with context. To create a semantic search  

31
00:02:48,000 --> 00:02:54,400
engine is quite simple in Datasets. First we 
need to embed all the documents. In this example,  

32
00:02:54,400 --> 00:02:59,120
we take a small sample from the SQUAD dataset 
and apply the same embedding logic as before.  

33
00:03:00,000 --> 00:03:03,840
This gives us a new column called "embeddings" 
that stores the embedding of every passage.  

34
00:03:05,680 --> 00:03:09,280
Once we have our embeddings, we need a 
way to find nearest neighbours to a query.  

35
00:03:10,080 --> 00:03:14,320
Datasets provides a special object called a 
FAISS index that allows you to quickly compare  

36
00:03:14,320 --> 00:03:18,880
embedding vectors. So we add the 
FAISS index, embed a question and  

37
00:03:18,880 --> 00:03:29,360
voila! we've now found the 3 most similar 
articles which might store the answer.
