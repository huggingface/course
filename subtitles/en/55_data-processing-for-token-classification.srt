1
00:00:05,600 --> 00:00:08,720
Let's study how to preprocess a 
dataset for token classification!  

2
00:00:10,400 --> 00:00:15,840
Token classification regroups any task that can 
be framed as labelling each word (or token) in  

3
00:00:15,840 --> 00:00:20,640
a sentence, like identifying the persons, 
organizations and locations for instance.  

4
00:00:21,920 --> 00:00:26,720
For our example, we will use the Conll 
dataset, in which we remove columns we  

5
00:00:26,720 --> 00:00:30,720
won't use and rename the other ones to 
get to a dataset with just two columns:  

6
00:00:31,360 --> 00:00:37,280
words and labels. If you have your own dataset 
for token classification, just make sure you  

7
00:00:37,280 --> 00:00:43,040
clean your data to get to the same point, with 
one column containing words (as list of strings)  

8
00:00:43,040 --> 00:00:48,240
and another containing labels (as integers 
spanning from to to your number of labels -1).()  

9
00:00:49,520 --> 00:00:53,520
Make sure you have your label names stored 
somewhere - here we get them from the dataset  

10
00:00:53,520 --> 00:00:58,640
features - so you are able to map the integers 
to some real labels when inspecting your data!  

11
00:01:00,480 --> 00:01:06,000
Here we are doing named entity recognitions, 
so ours labels are either O for words that do  

12
00:01:06,000 --> 00:01:11,040
not belong to any entity, LOC, 
for location, PER, for person,  

13
00:01:11,680 --> 00:01:19,200
ORG for organization and MISC for miscellaneous. 
Each label has two versions: the B- labels  

14
00:01:19,200 --> 00:01:25,840
indicate a word that begins an entity while the I- 
labels indicate a word that is inside an entity.  

15
00:01:26,880 --> 00:01:29,840
The first step to preprocess our 
data is to tokenize the words.  

16
00:01:30,400 --> 00:01:35,200
This is very easily done with a tokenizer, we just 
have to tell it we have pre-tokenized the data  

17
00:01:35,200 --> 00:01:42,160
with the flag is_split_into_words. Then comes 
the hard part. Since we have added special tokens  

18
00:01:42,160 --> 00:01:47,200
and each word may have been split into several 
tokens, our labels won't match the tokens anymore.  

19
00:01:47,840 --> 00:01:51,520
This is where the word IDs our fast 
tokenizer provide come to the rescue.  

20
00:01:52,800 --> 00:01:57,440
They match each token to the word it belongs to 
which allows us to map each token to its label.  

21
00:01:58,160 --> 00:02:02,080
We just have to make sure we change the B- 
labels to their I- counterparts for tokens  

22
00:02:02,080 --> 00:02:08,880
that are inside (but not at the beginning) of 
a word. The special tokens get a label of -100,  

23
00:02:08,880 --> 00:02:12,960
which is how we tell the Transformer loss 
functions to ignore them when computing the loss.  

24
00:02:14,560 --> 00:02:19,120
The code is then pretty straightforward, we write 
a function that shifts the labels for tokens that  

25
00:02:19,120 --> 00:02:23,920
are inside a word (that you can customize) and 
use it when generating the labels for each token.  

26
00:02:25,600 --> 00:02:29,840
Once that function to create our labels is 
written, we can preprocess the whole dataset using  

27
00:02:29,840 --> 00:02:35,840
the map function. With the option batched=True, 
we unleash the speed of out fast tokenizers.  

28
00:02:36,720 --> 00:02:39,360
The last problem comes when 
we need to create a batch.  

29
00:02:40,160 --> 00:02:43,680
Unless you changed the preprocessing 
function to apply some fixed padding,  

30
00:02:43,680 --> 00:02:49,280
we will get sentences of various lengths, which we 
need to pad to the same length. The padding needs  

31
00:02:49,280 --> 00:02:55,280
to be applied to the inputs as well as the labels, 
since we should have one label per token. Again,  

32
00:02:55,280 --> 00:03:01,200
-100 indicates the labels that should be ignored 
for the loss computation. This is all done for  

33
00:03:01,200 --> 00:03:05,760
us by the DataCollatorForTokenClassification, 
which you can use in PyTorch or TensorFlow.  

34
00:03:06,400 --> 00:03:10,960
With all of this, you are either ready to send 
your data and this data collator to the Trainer,  

35
00:03:10,960 --> 00:03:17,840
or to use the to_tf_dataset method 
and use the fit method of your model.
