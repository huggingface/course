1
00:00:05,200 --> 00:00:08,080
Let's have a look inside the 
token classification pipeline.  

2
00:00:10,000 --> 00:00:13,920
In the pipeline video, we looked at the 
different applications the Transformers  

3
00:00:13,920 --> 00:00:19,840
library supports out of the box, one of them being 
token classification, for instance predicting for  

4
00:00:19,840 --> 00:00:24,960
each word in a sentence whether they correspond 
to a person, an organization or a location.  

5
00:00:26,400 --> 00:00:30,240
We can even group together the tokens 
corresponding to the same entity,  

6
00:00:30,240 --> 00:00:34,960
for instance all the tokens that formed 
the word Sylvain here, or Hugging and Face.  

7
00:00:36,960 --> 00:00:42,480
The token classification pipeline works the same 
way as the text classification pipeline we studied  

8
00:00:42,480 --> 00:00:49,360
in a previous video. There are three steps: the 
tokenization, the model, and the post processing.  

9
00:00:50,720 --> 00:00:55,680
The first two steps are identical to the text 
classification pipeline, except we use an auto  

10
00:00:55,680 --> 00:01:01,760
token classification model instead of a sequence 
classification one. We tokenize our text then feed  

11
00:01:01,760 --> 00:01:07,360
it to the model. Instead of getting one number 
for each possible label for the whole sentence,  

12
00:01:07,360 --> 00:01:13,760
we get one number for each of the possible 9 
labels for every token in the sentence, here 19.  

13
00:01:15,120 --> 00:01:19,600
Like all the other models of the Transformers 
library, our model outputs logits,  

14
00:01:19,600 --> 00:01:26,160
which we turn into predictions by using a SoftMax. 
We also get the predicted label for each token by  

15
00:01:26,160 --> 00:01:30,000
taking the maximum prediction (since the softmax 
function preserves the order, we could have  

16
00:01:30,000 --> 00:01:35,200
done it on the logits if we had no need of the 
predictions). The model config contains the label  

17
00:01:35,200 --> 00:01:41,200
mapping in its id2label field. Using it, we can 
map every token to its corresponding label. The  

18
00:01:41,200 --> 00:01:46,400
label O correspond to "no entity", which is why we 
didn't see it in our results in the first slide.  

19
00:01:47,040 --> 00:01:51,360
On top of the label and the probability, 
those results included the start and end  

20
00:01:51,360 --> 00:01:56,960
character in the sentence. We will need to use the 
offset mapping of the tokenizer to get those (look  

21
00:01:56,960 --> 00:02:02,080
at the video linked below if you don't know about 
them already). Then, looping through each token  

22
00:02:02,080 --> 00:02:08,240
that has a label distinct from O, we can build the 
list of results we got with our first pipeline.  

23
00:02:08,240 --> 00:02:13,360
The last step is to group together tokens 
that correspond to the same entity.This  

24
00:02:13,360 --> 00:02:17,680
is why we had two labels for each type 
of entity: I-PER and B-PER for instance.  

25
00:02:18,240 --> 00:02:21,840
It allows us to know if a token is in 
the same entity as the previous one.()  

26
00:02:23,120 --> 00:02:26,720
Note that there are two ways of 
labelling used for token classification,  

27
00:02:26,720 --> 00:02:31,680
one (in pink here) uses the B-PER label at the 
beginning of each new entity, but the other  

28
00:02:31,680 --> 00:02:38,320
(in blue) only uses it to separate two adjacent 
entities of the same type. In both cases, we can  

29
00:02:38,320 --> 00:02:44,720
flag a new entity each time we see a new label 
appearing (either with the I or B prefix) then  

30
00:02:44,720 --> 00:02:50,160
take all the following tokens labelled the same, 
with an I-flag. This, coupled with the offset  

31
00:02:50,160 --> 00:03:01,040
mapping to get the start and end characters allows 
us to get the span of texts for each entity.
