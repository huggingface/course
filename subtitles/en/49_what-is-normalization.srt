1
00:00:05,130 --> 00:00:11,060
In this video we will see together what is
the normalizer component that we find at the

2
00:00:11,060 --> 00:00:12,240
beginning of each tokenizer.

3
00:00:12,240 --> 00:00:20,610
The normalization operation consists in applying
a succession of normalization rules to the

4
00:00:20,610 --> 00:00:21,960
raw text.

5
00:00:21,960 --> 00:00:27,510
We choose normalization rules to remove noise
in the text which seems useless for the learning

6
00:00:27,510 --> 00:00:31,420
and use of our language model.

7
00:00:31,420 --> 00:00:40,790
Let's take a very diverse sentence with different
fonts, upper and lower case characters, accents,

8
00:00:40,790 --> 00:00:48,490
punctuation and multiple spaces, to see how
several tokenizers normalize it.

9
00:00:48,490 --> 00:00:55,039
The tokenizer from the FNet model has transformed
the letters with font variants or circled

10
00:00:55,039 --> 00:01:00,230
into their basic version and has removed the
multiple spaces.

11
00:01:00,230 --> 00:01:07,090
And now if we look at the normalization with
Retribert's tokenizer, we can see that it

12
00:01:07,090 --> 00:01:12,990
keeps characters with several font variants
and keeps the multiple spaces but it removes

13
00:01:12,990 --> 00:01:15,659
all the accents.

14
00:01:15,659 --> 00:01:23,050
And if we continue to test the normalization
of many other tokenizers associated to models

15
00:01:23,050 --> 00:01:34,079
that you can find on the Hub we can see that
they also propose other normalizations.

16
00:01:34,079 --> 00:01:39,310
With the fast tokenizers, it is very easy
to observe the normalization chosen for the

17
00:01:39,310 --> 00:01:42,500
currently loaded tokenizer.

18
00:01:42,500 --> 00:01:49,250
Indeed, each instance of a fast tokenizer
has an underlying tokenizer from the Tokenizers

19
00:01:49,250 --> 00:01:54,820
library stored in the backend_tokenizer attribute.

20
00:01:54,820 --> 00:02:01,070
This object has itself a normalizer attribute
that we can use thanks to the "normalize_str"

21
00:02:01,070 --> 00:02:04,670
method to normalize a string.

22
00:02:04,670 --> 00:02:11,000
It is thus very practical that this normalization
which was used at the time of the training

23
00:02:11,000 --> 00:02:17,870
of the tokenizer was saved and that it applies
automatically when you asks a trained tokenizer

24
00:02:17,870 --> 00:02:21,120
to tokenize a text.

25
00:02:21,120 --> 00:02:28,130
For example, if we hadn't included the albert
normalizer we would have had a lot of unknown

26
00:02:28,130 --> 00:02:35,870
tokens by tokenizing this sentence with accents
and capital letters.

27
00:02:35,870 --> 00:02:40,319
These transformations can also be undetectable
with a simple "print".

28
00:02:40,319 --> 00:02:46,069
Indeed, keep in mind that for a computer,
text is only a succession of 0 and 1 and it

29
00:02:46,069 --> 00:02:51,230
happens that different successions of 0 and
1 render the same printed character.

30
00:02:51,230 --> 00:02:57,459
The 0s and 1s go in groups of 8 to form a
byte.

31
00:02:57,459 --> 00:03:04,490
The computer must then decode this sequence
of bytes into a sequence of "code points".

32
00:03:04,490 --> 00:03:10,959
In our example the 2 bytes are transformed
into a single "code point" by UTF-8.

33
00:03:10,959 --> 00:03:18,860
The unicode standard then allows us to find
the character corresponding to this code point:

34
00:03:18,860 --> 00:03:22,140
the c cedilla.

35
00:03:22,140 --> 00:03:28,060
Let's repeat the same operation with this
new sequence composed of 3 bytes, this time

36
00:03:28,060 --> 00:03:34,450
it is transformed into 2 "code points" .... which
also correspond to the c cedilla character!

37
00:03:34,450 --> 00:03:41,510
It is in fact the composition of the unicode
Latin Small Letter Cand the combining cedilla.

38
00:03:41,510 --> 00:03:47,819
But it's annoying because what appears to
us to be a single character is not at all

39
00:03:47,819 --> 00:03:52,379
the same thing for the computer.

40
00:03:52,379 --> 00:04:02,269
Fortunately, there are unicode standardization
standards known as NFC, NFD, NFKC and NFKD

41
00:04:02,269 --> 00:04:05,430
that allow erasing some of these differences.

42
00:04:05,430 --> 00:04:10,019
These standards are often used by tokenizers!

43
00:04:10,019 --> 00:04:15,239
On all these previous examples, even if the
normalizations changed the look of the text,

44
00:04:15,239 --> 00:04:21,229
they did not change the content: you could
still read "Hello world, let's normalize this

45
00:04:21,229 --> 00:04:22,540
sentence".

46
00:04:22,540 --> 00:04:30,120
However, you must be aware that some normalizations
can be very harmful if they are not adapted

47
00:04:30,120 --> 00:04:31,720
to their corpus.

48
00:04:31,720 --> 00:04:37,360
For example, if you take the French sentence
"un père indigné", which means "An indignant

49
00:04:37,360 --> 00:04:45,660
father", and normalize it with the bert-base-uncase
tokenizer which removes the accents then the

50
00:04:45,660 --> 00:04:53,550
sentence becomes "un père indigne" which
means "An unworthy father".

51
00:04:53,550 --> 00:04:58,699
If you watch this video to build your own
tokenizer, there are no absolute rules to

52
00:04:58,699 --> 00:05:04,580
choose or not a normalization for your brand
new tokenizer but I advise you to take the

53
00:05:04,580 --> 00:05:15,960
time to select them so that they do not make
you lose important information.
