1
00:00:05,280 --> 00:00:11,760
Using the Python debugger in a notebook. In 
this video, we'll learn how to use the Python  

2
00:00:11,760 --> 00:00:17,040
debugger in a Jupyter Notebook or a Colab. 
For this example, we are running code from  

3
00:00:17,040 --> 00:00:24,640
the token classification section, downloading 
the Conll dataset , having a look at it  

4
00:00:27,600 --> 00:00:30,080
before loading a tokenizer to preprocess it.  

5
00:00:32,640 --> 00:00:35,440
Checkout the section of the course 
linked below for more information.  

6
00:00:36,800 --> 00:00:42,240
Once this is done, we try to batch together some 
features of the training dataset by padding them  

7
00:00:44,960 --> 00:00:51,280
and returning a tensor, then we get the 
following error. We use PyTorch here but  

8
00:00:51,280 --> 00:00:56,240
you will get the same error with TensorFlow. As we 
have seen in the "How to debug an error?" video,  

9
00:00:56,240 --> 00:01:02,480
the error message is at the end and it indicates 
we should use padding, which we are actually  

10
00:01:02,480 --> 00:01:07,680
trying to do. So this is not useful and we will 
need to go a little deeper to debug the problem.  

11
00:01:08,400 --> 00:01:13,040
Fortunately, you can use the Python debugger at 
any time you get an error in a Jupyter Notebook  

12
00:01:13,040 --> 00:01:23,680
by typing %debug in any cell. When executing that 
cell, you go to the very bottom of the traceback  

13
00:01:23,680 --> 00:01:28,560
where you can type commands and you can type 
commands. The first two commands you should  

14
00:01:28,560 --> 00:01:41,760
learn are u and d (for up and down), which 
allow you to go up in the Traceback or down.  

15
00:01:43,920 --> 00:01:46,720
Going up twice, we get to the 
point the error was reached.  

16
00:01:47,600 --> 00:01:53,840
The third command to learn is p, for print. 
It allows you to print any value you want.  

17
00:01:54,560 --> 00:02:00,720
For instance here, we can see the value 
of return_tensors or batch_outputs  

18
00:02:00,720 --> 00:02:11,520
to try to understand what triggered the error. 
The batch outputs dictionary is a bit hard to see,  

19
00:02:12,720 --> 00:02:18,160
so let's dive into smaller pieces of it. Inside 
the debugger you can not only print any variable  

20
00:02:18,160 --> 00:02:28,240
but also evaluate any expression, so we can 
look independently at the inputs or labels.  

21
00:02:35,440 --> 00:02:41,360
Those labels are definitely weird: they are of 
various size, which we can actually confirm by  

22
00:02:41,360 --> 00:02:49,840
printing the sizes. No wonder the tokenizer 
wasn't able to create a tensor with them!  

23
00:02:52,160 --> 00:02:56,880
This is because the pad method only 
takes care of the tokenizer outptus:  

24
00:02:56,880 --> 00:02:59,680
input IDs, attention mask and token type IDs,  

25
00:03:00,240 --> 00:03:03,840
so we have to pad the labels ourselves 
before trying to create a tensor with them.  

26
00:03:05,040 --> 00:03:11,440
Once you are ready to exit the Python debugger, 
you can press q for quit. One way to fix the error  

27
00:03:11,440 --> 00:03:21,600
is to manually pad all labels to the longest, or 
we can use the data collator designed for this.
