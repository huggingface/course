1
00:00:05,120 --> 00:00:11,360
Let's see how we can preprocess our data 
for masked language modeling. As a reminder,  

2
00:00:11,360 --> 00:00:16,320
masked language modeling is when a model 
needs to fill the blanks in a sentence.  

3
00:00:16,320 --> 00:00:22,400
To do this, you just need texts, no labels, 
as this is a self-supervised problem. To apply  

4
00:00:22,400 --> 00:00:27,280
this on your own data, just make sure you have all 
your texts gathered in one column of your dataset.  

5
00:00:28,160 --> 00:00:32,320
Before we start randomly masking things, we 
will need to somehow make all those texts the  

6
00:00:32,320 --> 00:00:38,400
same length to batch them together. The first 
way to make all the texts the same length is  

7
00:00:38,400 --> 00:00:43,840
the one we used in text classification. let's 
pad the short texts and truncate the long ones.  

8
00:00:44,800 --> 00:00:48,400
As we have seen when we processed 
data for text classification,  

9
00:00:48,400 --> 00:00:51,840
this is all done by our tokenizer with the 
right options for padding and truncation.  

10
00:00:52,880 --> 00:00:57,840
This will however make us lose a lot of texts 
if the examples in our dataset are very long,  

11
00:00:58,400 --> 00:01:03,040
compared to the context length we picked. 
Here, all the portion in gray is lost.  

12
00:01:04,160 --> 00:01:08,320
This is why a second way to generate samples 
of text with the same length is to chunk our  

13
00:01:08,320 --> 00:01:12,720
text in pieces of context lengths, instead of 
discarding everything after the first chunk.  

14
00:01:13,760 --> 00:01:17,920
There will probably be a remainder of length 
smaller than the context size, which we can  

15
00:01:17,920 --> 00:01:24,480
choose to keep and pad or ignore. Here is how we 
can apply this in practice, by just adding the  

16
00:01:24,480 --> 00:01:30,080
return overflowing tokens option in our tokenizer 
call. Note how this gives us a bigger dataset!  

17
00:01:31,280 --> 00:01:36,720
This second way of chunking is ideal if all your 
texts are very long, but it won't work as nicely  

18
00:01:36,720 --> 00:01:42,640
if you have a variety of lengths in the texts. 
In this case, the best option is to concatenate  

19
00:01:42,640 --> 00:01:47,600
all your tokenized texts in one big stream, with 
a special tokens to indicate when you pass from  

20
00:01:47,600 --> 00:01:54,560
one document to the other, and only then split the 
big stream into chunks. Here is how it can be done  

21
00:01:54,560 --> 00:02:01,200
with code, with one loop to concatenate all the 
texts and another one to chunk it. Notice how it  

22
00:02:01,200 --> 00:02:05,920
reduces the number of samples in our dataset here, 
there must have been quite a few short entries!  

23
00:02:07,520 --> 00:02:12,960
Once this is done, the masking is the easy part. 
There is a data collator designed specifically for  

24
00:02:12,960 --> 00:02:18,240
this in the Transformers library. You can use 
it directly in the Trainer, or when converting  

25
00:02:18,240 --> 00:02:29,600
your datasets to tensorflow datasets before 
doing Keras.fit, with the to_tf_dataset method.
