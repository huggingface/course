1
00:00:05,520 --> 00:00:10,720
Memory mapping and streaming. In this video we'll 
take a look at two core features of the Datasets  

2
00:00:10,720 --> 00:00:15,920
library that allow you to load and process huge 
datasets without blowing up your laptop's CPU.

3
00:00:18,160 --> 00:00:22,720
Nowadays it is not uncommon to find yourself 
working with multi-GB sized datasets,  

4
00:00:22,720 --> 00:00:26,880
especially if you’re planning to pretrain a 
transformer like BERT or GPT-2 from scratch.  

5
00:00:27,920 --> 00:00:30,480
In these cases, even *loading* 
the data can be a challenge.  

6
00:00:31,040 --> 00:00:36,560
For example, the C4 corpus used to
pretrain T5 consists of over 2 terabytes of data!

7
00:00:38,160 --> 00:00:42,720
To handle these large datasets, the Datasets 
library is built on two core features:  

8
00:00:42,720 --> 00:00:45,120
the Apache Arrow format and a streaming API.  

9
00:00:46,160 --> 00:00:51,120
Arrow is designed for high-performance data 
processing and represents each table-like dataset  

10
00:00:51,120 --> 00:00:56,240
with an in-memory columnar format. As you can 
see in this example, columnar formats group  

11
00:00:56,240 --> 00:01:01,280
the elements of a table in consecutive blocks of 
RAM and this unlocks fast access and processing.  

12
00:01:02,560 --> 00:01:07,600
Arrow is great at processing data at any scale, 
but some datasets are so large that you can't even  

13
00:01:07,600 --> 00:01:12,480
fit them on your hard disk. For these cases, 
the Datasets library provides a streaming API  

14
00:01:13,040 --> 00:01:18,080
that allows you to progressively download the 
raw data one element at a time. The result is  

15
00:01:18,080 --> 00:01:21,600
a special object called an IterableDataset 
that we'll see in more detail soon.  

16
00:01:23,520 --> 00:01:28,160
Let's start by looking at why Arrow is so 
powerful. The first feature is that it treat every  

17
00:01:28,160 --> 00:01:34,000
dataset as a memory-mapped file. Memory mapping 
is a mechanism that maps a portion of a file or  

18
00:01:34,000 --> 00:01:38,967
an entire file on disk to a chunk of virtual 
memory. This allows applications to access can  

19
00:01:38,967 --> 00:01:43,360
access segments in an extremely large file without 
having to read the entire file into memory first.  

20
00:01:44,960 --> 00:01:49,040
Another cool feature of Arrow's memory 
mapping capability is that it allows multiple  

21
00:01:49,040 --> 00:01:53,840
processes to work with the same large dataset 
without moving it or copying it in any way.  

22
00:01:55,520 --> 00:01:59,920
This "zero-copy" feature of Arrow makes it 
extremely fast for iterating over a dataset.  

23
00:02:00,480 --> 00:02:05,920
In this example you can see that we iterate over 
15 million rows in about a minute using a standard  

24
00:02:05,920 --> 00:02:12,480
laptop - that's not too bad at all! Let's now 
take a look at how we can stream a large dataset.  

25
00:02:12,480 --> 00:02:16,720
The only change you need to make is to set the 
streaming=True argument in the load_dataset()  

26
00:02:16,720 --> 00:02:21,120
function. This will return a special 
IterableDataset object, which is a bit different  

27
00:02:21,120 --> 00:02:26,160
to the Dataset objects we've seen in other 
videos. This object is an iterable, which means  

28
00:02:26,160 --> 00:02:31,680
we can't index it to access elements, but instead 
iterate on it using the iter and next methods.  

29
00:02:32,640 --> 00:02:36,080
This will download and access a single 
example from the dataset, which means  

30
00:02:36,080 --> 00:02:39,760
you can progressively iterate through a huge 
dataset without having to download it first.  

31
00:02:41,840 --> 00:02:47,040
Tokenizing text with the map() method also works 
in a similar way. We first stream the dataset and  

32
00:02:47,040 --> 00:02:52,480
then apply the map() method with the tokenizer. To 
get the first tokenized example we apply iter and  

33
00:02:52,480 --> 00:02:58,560
next. The main difference with an IterableDataset 
is that instead of using the select() method to  

34
00:02:58,560 --> 00:03:04,240
return example, we use the take() and skip() 
methods because we can't index into the dataset.  

35
00:03:04,240 --> 00:03:10,320
The take() method returns the first N examples 
in the dataset, while skip() skips the first N  

36
00:03:10,320 --> 00:03:15,680
and returns the rest. You can see examples 
of both in action here, where we create  

37
00:03:15,680 --> 00:03:27,040
a validation set from the first 1000 examples 
and then skip those to create the training set.
