1
00:00:05,200 --> 00:00:11,680
How to preprocess pairs of sentences? We have 
seen how to tokenize single sentences and batch  

2
00:00:11,680 --> 00:00:18,080
them together in the "Batching inputs together" 
video. If this code look unfamiliar to you,  

3
00:00:18,080 --> 00:00:24,160
be sure to check that video again! Here we will 
focus on tasks that classify pairs of sentences.  

4
00:00:25,440 --> 00:00:30,960
For instance, we may want to classify whether two 
texts are paraphrases or not. Here is an example  

5
00:00:30,960 --> 00:00:36,320
taken from the Quora Question Pairs dataset, 
which focuses on identifying duplicate questions.  

6
00:00:37,360 --> 00:00:42,200
In the first pair, the two questions 
are duplicates; in the second, they  

7
00:00:43,360 --> 00:00:47,120
are not. Another pair classification problem 
is when we want to know if two sentences  

8
00:00:47,120 --> 00:00:54,000
are logically related or not (a problem called 
Natural Language Inference or NLI). In this  

9
00:00:54,000 --> 00:00:59,680
example taken from the MultiNLI dataset, we have 
a pair of sentences for each possible label:  

10
00:00:59,680 --> 00:01:04,560
contradiction, neutral or entailment (which 
is a fancy way of saying the first sentence  

11
00:01:04,560 --> 00:01:09,280
implies the second). So classifying pairs 
of sentences is a problem worth studying.  

12
00:01:10,080 --> 00:01:14,880
In fact, in the GLUE benchmark (which is an 
academic benchmark for text classification),  

13
00:01:15,600 --> 00:01:19,600
8 of the 10 datasets are focused 
on tasks using pairs of sentences.  

14
00:01:20,720 --> 00:01:24,240
That's why models like BERT are often 
pretrained with a dual objective:  

15
00:01:25,120 --> 00:01:29,920
on top of the language modeling objective, they 
often have an objective related to sentence pairs.  

16
00:01:31,040 --> 00:01:36,720
For instance, during pretraining, BERT is shown 
pairs of sentences and must predict both the  

17
00:01:36,720 --> 00:01:41,040
value of randomly masked tokens and whether 
the second sentence follows from the first.  

18
00:01:42,800 --> 00:01:46,640
Fortunately, the tokenizer from the 
Transformers library has a nice API  

19
00:01:46,640 --> 00:01:52,000
to deal with pairs of sentences: you just have 
to pass them as two arguments to the tokenizer.  

20
00:01:53,200 --> 00:01:57,600
On top of the input IDs and the attention 
mask we studied already, it returns a new  

21
00:01:57,600 --> 00:02:02,800
field called token type IDs, which tells the 
model which tokens belong to the first sentence  

22
00:02:03,440 --> 00:02:09,680
and which ones belong to the second sentence. 
Zooming in a little bit, here are the input IDs,  

23
00:02:09,680 --> 00:02:14,480
aligned with the tokens they correspond to, 
their respective token type ID and attention  

24
00:02:14,480 --> 00:02:21,360
mask. We can see the tokenizer also added special 
tokens so we have a CLS token, the tokens from the  

25
00:02:21,360 --> 00:02:28,720
first sentence, a SEP token, the tokens from the 
second sentence, and a final SEP token. If we have  

26
00:02:28,720 --> 00:02:33,760
several pairs of sentences, we can tokenize them 
together by passing the list of first sentences,  

27
00:02:34,480 --> 00:02:39,360
then the list of second sentences and all the 
keyword arguments we studied already, like  

28
00:02:39,360 --> 00:02:45,600
padding=True. Zooming in at the result, we can see 
how the tokenizer added padding to the second pair  

29
00:02:45,600 --> 00:02:51,200
of sentences, to make the two outputs the same 
length, and properly dealt with token type IDS  

30
00:02:51,200 --> 00:03:03,520
and attention masks for the two sentences. This 
is then all ready to pass through our model!
