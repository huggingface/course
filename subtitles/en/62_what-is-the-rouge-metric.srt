1
00:00:05,520 --> 00:00:12,080
What is the ROUGE metric? For many NLP tasks we 
can use common metrics like accuracy or F1 score,  

2
00:00:12,080 --> 00:00:15,920
but what do you do when you want to measure 
the quality of a summary from a model like T5?  

3
00:00:16,720 --> 00:00:20,265
In this video, we'll take a look at a widely 
used metric for text summarization called ROUGE,  

4
00:00:20,265 --> 00:00:23,360
which is short for Recall-Oriented Understudy for 
Gisting Evaluation. There are actually several  

5
00:00:23,360 --> 00:00:27,280
variants of ROUGE, but the basic idea behind 
all of them is to assign a single numerical  

6
00:00:27,280 --> 00:00:31,360
score to a summary that tells us how "good" it 
is compared to one or more reference summaries.  

7
00:00:32,320 --> 00:00:35,360
In this example we have a book review 
that has been summarized by some model.  

8
00:00:36,400 --> 00:00:39,600
If we compare the generated summary 
to some reference human summaries,  

9
00:00:39,600 --> 00:00:43,840
we can see that the model is pretty 
good, and only differs by a word or two.  

10
00:00:44,800 --> 00:00:48,000
So how can we measure the quality of a 
generated summary in an automatic way?  

11
00:00:48,800 --> 00:00:52,880
The approach that ROUGE takes is to compare 
the n-grams of the generated summary to the  

12
00:00:52,880 --> 00:00:58,400
n-grams of the references. An n-gram is just 
a fancy way of saying "a chunk of n words",  

13
00:00:58,400 --> 00:01:02,080
so let's start with unigrams, which correspond 
to the individual words in a sentence.  

14
00:01:03,600 --> 00:01:07,760
In this example you can see that six of the words 
in the generated summary are also found in one of  

15
00:01:07,760 --> 00:01:11,840
the reference summaries. The ROUGE metric 
that compares unigrams is called ROUGE-1.  

16
00:01:14,000 --> 00:01:18,000
Now that we've found our matches, one way to 
assign a score to the summary is to compute the  

17
00:01:18,000 --> 00:01:22,880
recall of the unigrams. This means we just count 
the number of matching words in the generated and  

18
00:01:22,880 --> 00:01:27,040
reference summaries and normalize the count by 
dividing by the number of word in the reference.  

19
00:01:28,000 --> 00:01:31,920
In this example, we found 6 matching 
words and our reference has 6 words,  

20
00:01:31,920 --> 00:01:36,240
so our unigram recall is perfect! This 
means that all of words in the reference  

21
00:01:36,240 --> 00:01:42,320
summary have produced in the generated one. 
Perfect recall sounds great, but imagine if  

22
00:01:42,320 --> 00:01:47,120
our generated summary had been “I really really 
really really loved reading the Hunger Games”.  

23
00:01:47,920 --> 00:01:52,240
This would also have perfect recall, but is 
arguably a worse summary since it is verbose.  

24
00:01:53,280 --> 00:01:57,840
To deal with these scenarios we can also compute 
precision, which in the ROUGE context measures  

25
00:01:57,840 --> 00:02:01,200
how much of the generated summary was relevant. In 
this example, the precision is 6/7. In practice,  

26
00:02:01,200 --> 00:02:05,200
both precision and recall are usually 
computed and then the F1-score is reported.  

27
00:02:07,360 --> 00:02:12,000
We can change the granularity of the comparison 
by comparing bigrams instead of unigrams.  

28
00:02:12,800 --> 00:02:17,760
With bigrams we chunk the sentence into pairs of 
consecutive words and then count how many pairs in  

29
00:02:17,760 --> 00:02:23,600
the generated summary are present in the reference 
one. This gives us ROUGE-2 precision and recall,  

30
00:02:23,600 --> 00:02:28,800
which we can see is lower than the ROUGE-1 scores 
we saw earlier. Note that if the summaries are  

31
00:02:28,800 --> 00:02:34,560
long, the ROUGE-2 score will be small as there 
are typically fewer bigrams to match. This is  

32
00:02:34,560 --> 00:02:39,680
also true for abstractive summarization, so both 
ROUGE-1 and ROUGE-2 scores are usually reported.  

33
00:02:41,760 --> 00:02:46,880
The last ROUGE variant we'll discuss is 
ROUGE-L. ROUGE-L doesn't compare n-grams,  

34
00:02:46,880 --> 00:02:51,360
but instead treats each summary as a sequence 
of words and then looks for the longest common  

35
00:02:51,360 --> 00:02:57,280
subsequence or LCS. A subsequence is a sequence 
that appears in the same relative order,  

36
00:02:57,280 --> 00:03:03,280
but not necessarily contiguous. So in this 
example, "I loved reading the Hunger Games" is the  

37
00:03:03,280 --> 00:03:11,120
longest common subsequence. The main advantage of 
ROUGE-L over ROUGE-1 or ROUGE-2 is that is doesn't  

38
00:03:11,120 --> 00:03:18,400
depend on consecutive n-gram matches, so it tends 
to capture sentence structure more accurately. To  

39
00:03:18,400 --> 00:03:23,200
compute ROUGE scores in Hugging Face Datasets is 
very simple: just use the load_metric() function,  

40
00:03:23,760 --> 00:03:26,960
provide your model's summaries along with 
the references and you're good to go!  

41
00:03:28,560 --> 00:03:32,480
The output from the calculation contains 
a lot of information! The first thing we  

42
00:03:32,480 --> 00:03:36,880
can see here is that the confidence intervals 
of each ROUGE score are provided in the low,  

43
00:03:36,880 --> 00:03:41,680
mid, and high fields. This is really useful if you 
want to know the spread of your ROUGE scores when  

44
00:03:41,680 --> 00:03:48,080
comparing two or more models. The second thing to 
notice is that we have four types of ROUGE score.  

45
00:03:48,080 --> 00:03:53,840
We've already seen ROUGE-1, ROUGE-2 and 
ROUGE-L, so what is ROUGE-LSUM? Well,  

46
00:03:53,840 --> 00:03:58,800
the “sum” in ROUGE-LSUM refers to the fact that 
this metric is computed over a whole summary,  

47
00:03:58,800 --> 00:04:08,480
while ROUGE-L is computed as the 
average over individual sentences.
