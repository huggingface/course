1
00:00:05,680 --> 00:00:07,440
How to slice and dice a dataset.  

2
00:00:08,640 --> 00:00:12,320
Most of the time, the data you work with won’t 
be perfectly prepared for training models.  

3
00:00:13,120 --> 00:00:17,920
In this video we’ll explore various features 
that Datasets provides to clean up your datasets.  

4
00:00:19,760 --> 00:00:23,520
The Datasets library provides several built-in 
methods that allow you to wrangle your data.  

5
00:00:25,200 --> 00:00:29,360
In this video we'll see how you can shuffle 
and split your data, select the rows you're  

6
00:00:29,360 --> 00:00:33,840
interested in, tweak the columns, and apply 
processing functions with the map() method.  

7
00:00:35,440 --> 00:00:39,920
Let's start with shuffling. It is generally a 
good idea to apply shuffling to the training set  

8
00:00:39,920 --> 00:00:42,640
so that your model doesn't learn 
any artificial ordering in the data.  

9
00:00:43,360 --> 00:00:46,880
If you want to shuffle the whole dataset, you 
can apply the appropriately named shuffle()  

10
00:00:46,880 --> 00:00:51,280
method to your dataset. You can see an example of 
this method in action here, where we've downloaded  

11
00:00:51,280 --> 00:00:56,960
the training split of the SQUAD dataset 
and shuffled all the rows randomly.Another  

12
00:00:56,960 --> 00:01:00,000
way to shuffle the data is to 
create random train and test splits.  

13
00:01:00,720 --> 00:01:05,600
This can be useful if you have to create your own 
test splits from raw data. To do this, you just  

14
00:01:05,600 --> 00:01:11,760
apply the train_test_split method and specify how 
large the test split should be. In this example,  

15
00:01:11,760 --> 00:01:17,280
we've specified that the test set should be 
10% of the total dataset size. You can see that  

16
00:01:17,280 --> 00:01:22,400
the output of train_test_split is a DatasetDict 
object, whose keys correspond to the new splits.  

17
00:01:24,960 --> 00:01:28,400
Now that we know how to shuffle a dataset, 
let's take a look at returning the rows  

18
00:01:28,400 --> 00:01:32,080
we're interested in. The most common way 
to do this is with the select method.  

19
00:01:32,960 --> 00:01:36,560
This method expects a list or 
generator of the dataset's indices,  

20
00:01:36,560 --> 00:01:39,840
and will then return a new Dataset 
object containing just those rows.  

21
00:01:41,280 --> 00:01:45,600
If you want to create a random sample of rows, 
you can do this by chaining the shuffle and select  

22
00:01:45,600 --> 00:01:51,120
methods together. In this example, we've created 
a sample of 5 elements from the SQuAD dataset.  

23
00:01:53,280 --> 00:01:57,360
The last way to pick out specific rows in 
a dataset is by applying the filter method.  

24
00:01:58,080 --> 00:02:01,360
This method checks whether each 
rows fulfills some condition or not.  

25
00:02:02,080 --> 00:02:05,840
For example, here we've created a small 
lambda function that checks whether the  

26
00:02:05,840 --> 00:02:10,800
title starts with the letter "L". Once we 
apply this function with the filter method,  

27
00:02:10,800 --> 00:02:13,840
we get a subset of the data 
consisting of just these titles.  

28
00:02:16,080 --> 00:02:19,360
So far we've been talking about the rows 
of a dataset, but what about the columns?  

29
00:02:20,240 --> 00:02:23,280
The Datasets library has two main 
methods for transforming columns:  

30
00:02:23,840 --> 00:02:26,480
a rename_column method to 
change the name of a column,  

31
00:02:26,480 --> 00:02:31,360
and a remove_columns method to delete them. 
You can see examples of both these method here.  

32
00:02:34,000 --> 00:02:38,400
Some datasets have nested columns and you can 
expand these by applying the flatten method.  

33
00:02:39,120 --> 00:02:44,240
For example in the SQUAD dataset, the answers 
column contains a text and answer_start field.  

34
00:02:44,960 --> 00:02:49,840
If we want to promote them to their own separate 
columns, we can apply flatten as shown here.  

35
00:02:51,280 --> 00:02:55,040
Of course, no discussion of the Datasets 
library would be complete without mentioning the  

36
00:02:55,040 --> 00:03:00,240
famous map method. This method applies a custom 
processing function to each row in the dataset.  

37
00:03:00,960 --> 00:03:06,480
For example,here we first define a lowercase_title 
function that simply lowercases the text in the  

38
00:03:06,480 --> 00:03:13,760
title column and then we feed that to the map 
method and voila! we now have lowercase titles.  

39
00:03:15,760 --> 00:03:19,280
The map method can also be used to feed 
batches of rows to the processing function.  

40
00:03:19,840 --> 00:03:24,240
This is especially useful for tokenization, 
where the tokenizers are backed by the Tokenizers  

41
00:03:24,240 --> 00:03:31,840
library can use fast multithreading 
to process batches in parallel.
