1
00:00:05,310 --> 00:00:12,220
In this video we will see together what is
the purpose of training a tokenizer, what

2
00:00:12,220 --> 00:00:18,770
are the key steps to follow and what is the
easiest way to do it.

3
00:00:18,770 --> 00:00:23,039
You will ask yourself the question "Should
I train a new tokenizer?"

4
00:00:23,039 --> 00:00:27,369
when you plan to train a new model from scratch.

5
00:00:27,369 --> 00:00:36,600
A trained tokenizer would not be suitable
for your corpus if your corpus is in a different

6
00:00:36,600 --> 00:00:43,640
language, uses new characters such as accents
or upper cased letters, has a specific vocabulary,

7
00:00:43,640 --> 00:00:50,980
for example medical or legal, or uses a different
style, a language from another century for

8
00:00:50,980 --> 00:00:51,989
instance.

9
00:00:51,989 --> 00:01:00,719
For example, if I take the tokenizer trained
for the bert-base-uncased model and ignore

10
00:01:00,719 --> 00:01:08,580
its normalization step then we can see that
the tokenization operation on the English

11
00:01:08,580 --> 00:01:14,310
sentence "here is a sentence adapted to our
tokenizer" produces a rather satisfactory

12
00:01:14,310 --> 00:01:20,820
list of tokens in the sense that this sentence
of 8 words is tokenized into 9 tokens.

13
00:01:20,820 --> 00:01:29,909
On the other hand if we use this same tokenizer
on a sentence in Bengali, we see that either

14
00:01:29,909 --> 00:01:36,320
a word is divided into many sub tokens or
that the tokenizer does not know one of the

15
00:01:36,320 --> 00:01:41,359
unicode characters and returns only an unknown
token.

16
00:01:41,359 --> 00:01:47,350
The fact that a "common" word is split into
many tokens can be problematic because language

17
00:01:47,350 --> 00:01:52,750
models can only handle a sequence of tokens
of limited length.

18
00:01:52,750 --> 00:01:59,290
A tokenizer that excessively splits your initial
text may even impact the performance of your

19
00:01:59,290 --> 00:02:00,290
model.

20
00:02:00,290 --> 00:02:05,060
Unknown tokens are also problematic because
the model will not be able to extract any

21
00:02:05,060 --> 00:02:11,440
information from the "unknown" part of the
text.

22
00:02:11,440 --> 00:02:16,910
In this other example, we can see that the
tokenizer replaces words containing characters

23
00:02:16,910 --> 00:02:21,230
with accents and capital letters with unknown
tokens.

24
00:02:21,230 --> 00:02:28,140
Finally, if we use again this tokenizer to
tokenize medical vocabulary we see again that

25
00:02:28,140 --> 00:02:37,349
a single word is divided into many sub tokens:
4 for "paracetamol" and "pharyngitis".

26
00:02:37,349 --> 00:02:42,050
Most of the tokenizers used by the current
state of the art language models need to be

27
00:02:42,050 --> 00:02:48,160
trained on a corpus that is similar to the
one used to pre-train the language model.

28
00:02:48,160 --> 00:02:54,390
This training consists in learning rules to
divide the text into tokens and the way to

29
00:02:54,390 --> 00:03:00,510
learn these rules and use them depends on
the chosen tokenizer model.

30
00:03:00,510 --> 00:03:06,710
Thus, to train a new tokenizer it is first
necessary to build a training corpus composed

31
00:03:06,710 --> 00:03:09,239
of raw texts.

32
00:03:09,239 --> 00:03:13,440
Then, you have to choose an architecture for
your tokenizer.

33
00:03:13,440 --> 00:03:19,640
Here there are two options: the simplest is
to reuse the same architecture as the one

34
00:03:19,640 --> 00:03:26,760
of a tokenizer used by another model already
trained,otherwise it is also possible to completely

35
00:03:26,760 --> 00:03:33,950
design your tokenizer but it requires more
experience and attention.

36
00:03:33,950 --> 00:03:39,620
Once the architecture is chosen, one can thus
train this tokenizer on your constituted corpus.

37
00:03:39,620 --> 00:03:44,870
Finally, the last thing that you need to do
is to save the learned rules to be able to

38
00:03:44,870 --> 00:03:49,780
use this tokenizer which is now ready to be
used.

39
00:03:49,780 --> 00:03:55,120
Let's take an example: let's say you want
to train a GPT-2 model on Python code.

40
00:03:55,120 --> 00:04:03,000
Even if the python code is in English this
type of text is very specific and deserves

41
00:04:03,000 --> 00:04:09,800
a tokenizer trained on it - to convince you
of this we will see at the end the difference

42
00:04:09,800 --> 00:04:11,319
produced on an example.

43
00:04:11,319 --> 00:04:18,889
For that we are going to use the method "train_new_from_iterator"
that all the fast tokenizers of the library

44
00:04:18,889 --> 00:04:22,530
have and thus in particular GPT2TokenizerFast.

45
00:04:22,530 --> 00:04:28,389
This is the simplest method in our case to
have a tokenizer adapted to python code.

46
00:04:28,389 --> 00:04:34,229
Remember, the first step is to gather a training
corpus.

47
00:04:34,229 --> 00:04:39,639
We will use a subpart of the CodeSearchNet
dataset containing only python functions from

48
00:04:39,639 --> 00:04:42,039
open source libraries on Github.

49
00:04:42,039 --> 00:04:48,890
It's good timing, this dataset is known by
the datasets library and we can load it in

50
00:04:48,890 --> 00:04:51,190
two lines of code.

51
00:04:51,190 --> 00:04:57,940
Then, as the "train_new_from_iterator" method
expects a iterator of lists of texts we create

52
00:04:57,940 --> 00:05:04,030
the "get_training_corpus" function which will
return an iterator.

53
00:05:04,030 --> 00:05:10,861
Now that we have our iterator on our python
functions corpus, we can load the gpt-2 tokenizer

54
00:05:10,861 --> 00:05:12,490
architecture.

55
00:05:12,490 --> 00:05:19,450
Here "old_tokenizer" is not adapted to our
corpus but we only need one more line to train

56
00:05:19,450 --> 00:05:21,850
it on our new corpus.

57
00:05:21,850 --> 00:05:29,310
An argument that is common to most of the
tokenization algorithms used at the moment

58
00:05:29,310 --> 00:05:33,370
is the size of the vocabulary, we choose here
the value 52 thousand.

59
00:05:33,370 --> 00:05:38,780
Finally, once the training is finished, we
just have to save our new tokenizer locally

60
00:05:38,780 --> 00:05:45,430
or send it to the hub to be able to reuse
it very easily afterwards.

61
00:05:45,430 --> 00:05:49,962
Finally, let's see together on an example
if it was useful to re-train a tokenizer similar

62
00:05:49,962 --> 00:05:55,259
to gpt2 one.

63
00:05:55,259 --> 00:06:01,610
With the original tokenizer of GPT-2 we see
that all spaces are isolated and the method

64
00:06:01,610 --> 00:06:05,860
name "randn" relatively common in python code
is split in 2.

65
00:06:05,860 --> 00:06:10,919
With our new tokenizer, single and double
indentations have been learned and the method

66
00:06:10,919 --> 00:06:13,410
"randn" is tokenized into 1 token.

67
00:06:13,410 --> 00:06:23,190
And with that, you now know how to train your
very own tokenizers!
