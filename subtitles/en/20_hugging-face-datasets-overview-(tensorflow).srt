1
00:00:05,200 --> 00:00:11,200
The Hugging Face Datasets library: A Quick 
overview. The Hugging Face Datasets library  

2
00:00:11,200 --> 00:00:15,920
is a library that provides an API to quickly 
download many public datasets and preprocess them.  

3
00:00:16,880 --> 00:00:22,480
In this video we will explore how to do that. The 
downloading part is easy: with the load_dataset  

4
00:00:22,480 --> 00:00:27,760
function, you can directly download and cache a 
dataset from its identifier on the Dataset hub.  

5
00:00:29,040 --> 00:00:34,160
Here we fetch the MRPC dataset from 
the GLUE benchmark, which is a dataset  

6
00:00:34,160 --> 00:00:38,000
containing pairs of sentences where the 
task is to determine the paraphrases.  

7
00:00:39,360 --> 00:00:44,880
The object returned by the load_dataset function 
is a DatasetDict, which is a sort of dictionary  

8
00:00:44,880 --> 00:00:50,800
containing each split of our dataset. We can 
access each split by indexing with its name.  

9
00:00:51,520 --> 00:00:55,120
This split is then an instance of 
the Dataset class, with columns  

10
00:00:55,680 --> 00:01:04,000
(here sentence1, sentence2. label and idx) and 
rows. We can access a given element by its index.  

11
00:01:04,880 --> 00:01:10,240
The amazing thing about the Hugging Face Datasets 
library is that everything is saved to disk using  

12
00:01:10,240 --> 00:01:15,280
Apache Arrow, which means that even if your 
dataset is huge you won't get out of RAM:  

13
00:01:15,920 --> 00:01:21,760
only the elements you request are loaded in 
memory. Accessing a slice of your dataset is  

14
00:01:21,760 --> 00:01:27,760
as easy as one element. The result is then a 
dictionary with list of values for each keys  

15
00:01:28,480 --> 00:01:35,040
(here the list of labels, the list of first 
sentences and the list of second sentences). The  

16
00:01:35,040 --> 00:01:40,720
features attribute of a Dataset gives us more 
information about its columns. In particular,  

17
00:01:40,720 --> 00:01:45,040
we can see here it gives us the correspondence 
between the integers and names for the labels.  

18
00:01:45,920 --> 00:01:53,840
0 stands for not equivalent and 1 for equivalent. 
To preprocess all the elements of our dataset,  

19
00:01:53,840 --> 00:01:59,120
we need to tokenize them. Have a look at the 
video "Preprocess sentence pairs" for a refresher,  

20
00:01:59,840 --> 00:02:04,480
but you just have to send the two sentences to the 
tokenizer with some additional keyword arguments.  

21
00:02:05,760 --> 00:02:11,200
Here we indicate a maximum length of 128 
and pad inputs shorter than this length,  

22
00:02:11,200 --> 00:02:17,040
truncate inputs that are longer. We put all of 
this in a tokenize_function that we can directly  

23
00:02:17,040 --> 00:02:22,320
apply to all the splits in our dataset with the 
map method. As long as the function returns a  

24
00:02:22,320 --> 00:02:27,760
dictionary-like object, the map method will add 
new columns as needed or update existing ones.  

25
00:02:29,840 --> 00:02:34,960
To speed up preprocessing and take advantage 
of the fact our tokenizer is backed by Rust  

26
00:02:34,960 --> 00:02:40,320
thanks to the Hugging Face Tokenizers library, 
we can process several elements at the same time  

27
00:02:40,320 --> 00:02:46,800
to our tokenize function, using the batched=True 
argument. Since the tokenizer can handle list  

28
00:02:46,800 --> 00:02:53,360
of first/second sentences, the tokenize_function 
does not need to change for this. You can also use  

29
00:02:53,360 --> 00:03:00,320
multiprocessing with the map method, check out its 
documentation! Once this is done, we are almost  

30
00:03:00,320 --> 00:03:05,360
ready for training: we just remove the columns we 
don't need anymore with the remove_columns method,  

31
00:03:05,920 --> 00:03:10,320
rename label to labels (since the models 
from Hugging Face Transformers expect that)  

32
00:03:11,200 --> 00:03:17,280
and set the output format to our desired 
backend: torch, tensorflow or numpy. If needed,  

33
00:03:17,280 --> 00:03:27,440
we can also generate a short sample 
of a dataset using the select method.
