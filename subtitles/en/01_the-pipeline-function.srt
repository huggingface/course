1
00:00:05,680 --> 00:00:06,720
The pipeline function.  

2
00:00:09,360 --> 00:00:13,280
The pipeline function is the most 
high-level API of the Transformers library.  

3
00:00:13,840 --> 00:00:21,200
It regroups together all the steps to go from raw 
texts to usable predictions. The model used is at  

4
00:00:21,200 --> 00:00:26,720
the core of a pipeline, but the pipeline also 
include all the necessary pre-processing (since  

5
00:00:26,720 --> 00:00:32,800
the model does not expect texts, but numbers) as 
well as some post-processing to make the output of  

6
00:00:32,800 --> 00:00:39,440
the model human-readable. Let's look at a first 
example with the sentiment analysis pipeline.  

7
00:00:40,480 --> 00:00:46,080
This pipeline performs text classification on a 
given input, and determines if it's positive or  

8
00:00:46,080 --> 00:00:53,120
negative. Here, it attributed the positive label 
on the given text, with a confidence of 95%.  

9
00:00:55,440 --> 00:00:59,520
You can pass multiple texts to the 
same pipeline, which will be processed  

10
00:00:59,520 --> 00:01:05,840
and passed through the model together, as a 
batch. The output is a list of individual results,  

11
00:01:05,840 --> 00:01:12,080
in the same order as the input texts. Here we 
find the same label and score for the first text,  

12
00:01:12,080 --> 00:01:16,480
and the second text is judged 
positive with a confidence of 99.99%.  

13
00:01:18,480 --> 00:01:22,720
The zero-shot classification pipeline is a 
more general text-classification pipeline:  

14
00:01:23,360 --> 00:01:28,320
it allows you to provide the labels you 
want. Here we want to classify our input  

15
00:01:28,320 --> 00:01:35,360
text along the labels "education", "politics" and 
"business". The pipeline successfully recognizes  

16
00:01:35,360 --> 00:01:39,360
it's more about education than the 
other labels, with a confidence of 84%.  

17
00:01:41,440 --> 00:01:47,360
Moving on to other tasks, the text generation 
pipeline will auto-complete a given prompt. The  

18
00:01:47,360 --> 00:01:52,560
output is generated with a bit of randomness, so 
it changes each time you call the generator object  

19
00:01:52,560 --> 00:01:58,960
on a given prompt. Up until now, we have used the 
pipeline API with the default model associated to  

20
00:01:58,960 --> 00:02:03,920
each task, but you can use it with any model that 
has been pretrained or fine-tuned on this task.  

21
00:02:06,320 --> 00:02:12,320
Going on the model hub (huggingface.co/models), 
you can filter the available models by task.  

22
00:02:13,120 --> 00:02:16,960
The default model used in our 
previous example was gpt2,  

23
00:02:16,960 --> 00:02:20,080
but there are many more models 
available, and not just in English!  

24
00:02:21,280 --> 00:02:27,120
Let's go back to the text generation pipeline and 
load it with another model, distilgpt2. This is  

25
00:02:27,120 --> 00:02:33,120
a lighter version of gpt2 created by the Hugging 
Face team. When applying the pipeline to a given  

26
00:02:33,120 --> 00:02:39,280
prompt, we can specify several arguments, such as 
the maximum length of the generated texts, or the  

27
00:02:39,280 --> 00:02:43,520
number of sentences we want to return (since 
there is some randomness in the generation).  

28
00:02:45,920 --> 00:02:50,480
Generating text by guessing the next word in a 
sentence was the pretraining objective of GPT-2,  

29
00:02:51,200 --> 00:02:56,240
the fill mask pipeline is the pretraining 
objective of BERT, which is to guess the value  

30
00:02:56,240 --> 00:03:02,480
of masked word. In this case, we ask the two most 
likely values for the missing words (according to  

31
00:03:02,480 --> 00:03:09,120
the model) and get mathematical or computational 
as possible answers. Another task Transformers  

32
00:03:09,120 --> 00:03:13,920
model can perform is to classify each word in 
the sentence instead of the sentence as a whole.  

33
00:03:14,720 --> 00:03:21,040
One example of this is Named Entity Recognition, 
which is the task of identifying entities, such as  

34
00:03:21,040 --> 00:03:29,360
persons, organizations or locations in a sentence. 
Here, the model correctly finds the person  

35
00:03:29,360 --> 00:03:36,000
(Sylvain), the organization (Hugging Face) as well 
as the location (Brooklyn) inside the input text.  

36
00:03:37,440 --> 00:03:42,080
The grouped_entities=True argument used 
is to make the pipeline group together  

37
00:03:42,080 --> 00:03:46,080
the different words linked to the same 
entity (such as Hugging and Face here).  

38
00:03:48,000 --> 00:03:52,160
Another task available with the pipeline 
API is extractive question answering.  

39
00:03:52,720 --> 00:03:58,080
Providing a context and a question, the model 
will identify the span of text in the context  

40
00:03:58,080 --> 00:04:03,920
containing the answer to the question. Getting 
short summaries of very long articles is  

41
00:04:03,920 --> 00:04:07,840
also something the Transformers library can 
help with, with the summarization pipeline.  

42
00:04:09,360 --> 00:04:15,040
Finally, the last task supported by the 
pipeline API is translation. Here we use  

43
00:04:15,040 --> 00:04:19,440
a French/English model found on the model hub 
to get the English version of our input text.  

44
00:04:21,360 --> 00:04:24,720
Here is a brief summary of all the 
tasks we looked into in this video.  

45
00:04:25,280 --> 00:04:27,840
Try then out through the inference 
widgets in the model hub!
