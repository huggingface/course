1
00:00:05,670 --> 00:00:09,630
Let's see how to preprocess a dataset for
translation.

2
00:00:09,630 --> 00:00:13,269
This is the task of well translating a sentence
in another language.

3
00:00:13,269 --> 00:00:18,110
This video will focus on how to preprocess
your dataset once you have managed to put

4
00:00:18,110 --> 00:00:23,090
it in the following format: one column for
the input texts, and one for the target texts.

5
00:00:23,090 --> 00:00:28,439
Here is how we can achieve this with the Datasets
library on the KDE4 dataset for English to

6
00:00:28,439 --> 00:00:30,960
French translation.

7
00:00:30,960 --> 00:00:35,360
As long as you manage to have your data look
like this, you should be able to follow the

8
00:00:35,360 --> 00:00:36,769
same steps.

9
00:00:36,769 --> 00:00:41,550
For once, our labels are not integers corresponding
to some classes, but plain text.

10
00:00:41,550 --> 00:00:44,760
We will thus need to tokenize them, like our
inputs.

11
00:00:44,760 --> 00:00:50,820
There is a trap there though, as if you tokenize
your targets like your inputs, you will hit

12
00:00:50,820 --> 00:00:51,820
a problem.

13
00:00:51,820 --> 00:00:55,829
Even if you don't speak French, you might
notice some weird things in the tokenization

14
00:00:55,829 --> 00:01:01,800
of the targets: most of the words are tokenized
in several subtokens, while "fish", one of

15
00:01:01,800 --> 00:01:05,799
the only English word, is tokenized as a single
word.

16
00:01:05,799 --> 00:01:09,760
That's because our inputs have been tokenized
as English.

17
00:01:09,760 --> 00:01:13,939
Since our model knows two languages, you have
to warn it when tokenizing the targets, so

18
00:01:13,939 --> 00:01:16,360
it switches in French mode.

19
00:01:16,360 --> 00:01:20,090
This is done with the as_target_tokenizer
context manager.

20
00:01:20,090 --> 00:01:24,900
You can see how it results in a more compact
tokenization.

21
00:01:24,900 --> 00:01:28,509
Processing the whole dataset is then super
easy with the map function.

22
00:01:28,509 --> 00:01:32,900
You can pick different maximum lengths for
the input and targets, and choose to pad at

23
00:01:32,900 --> 00:01:37,210
this stage to that maximum length by setting
padding=max_length.

24
00:01:37,210 --> 00:01:42,540
Here we will show you how to pad dynamically
as it requires one more step.

25
00:01:42,540 --> 00:01:45,560
Your inputs and targets are all sentence of
various lengths.

26
00:01:45,560 --> 00:01:50,470
We will pad the inputs and targets separately
as the maximum length of the inputs and targets

27
00:01:50,470 --> 00:01:52,740
might be different.

28
00:01:52,740 --> 00:01:57,259
Then we pad the inputs with the pad token
and the targets with the -100 index, to make

29
00:01:57,259 --> 00:02:01,470
sure they are not taken into account in the
loss computation.

30
00:02:01,470 --> 00:02:04,869
Once this is done, batching inputs and targets
become super easy!

31
00:02:04,869 --> 00:02:10,220
The Transformers library provides us with
a data collator to do this all automatically.

32
00:02:10,220 --> 00:02:15,920
You can then pass it to the Trainer with your
datasets, or use it in the to_tf_dataset method

33
00:02:15,920 --> 00:02:17,410
before using model.fit().
