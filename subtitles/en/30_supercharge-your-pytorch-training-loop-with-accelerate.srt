1
00:00:05,360 --> 00:00:08,800
Supercharge your Pytorch training 
loop with Hugging Face Accelerate.  

2
00:00:11,120 --> 00:00:17,040
There are multiple setups on which you can run 
your training: it could be on CPU, GPUs, TPUs.  

3
00:00:17,680 --> 00:00:22,640
Distributed on one machine with several 
devices, or several machines (often called  

4
00:00:22,640 --> 00:00:29,360
nodes) each with multiple devices. On top of that 
there are new tweaks to make your training faster  

5
00:00:29,360 --> 00:00:35,680
or more memory efficient, like mixed precision 
and DeepSpeed. Each of those setups or training  

6
00:00:35,680 --> 00:00:40,080
tweaks, requires you to change the code of 
your training loop in one way or another  

7
00:00:40,080 --> 00:00:46,480
and to learn a new API. All those setups are 
handled by the Trainer API, and there are several  

8
00:00:46,480 --> 00:00:51,440
third-party libraries that can also help you with 
that. The problem with those is that they can feel  

9
00:00:51,440 --> 00:00:56,320
like a black box and that it might not be easy to 
implement the tweak to the training loop you need.  

10
00:00:57,680 --> 00:01:02,000
Accelerate has been designed specifically to let 
you retain full control over your training loop  

11
00:01:02,560 --> 00:01:08,000
and be as non-intrusive as possible. With 
just four lines to add to your training loop  

12
00:01:08,640 --> 00:01:11,840
(here shown on the code of the training 
loop from the "Raw training loop" video),  

13
00:01:12,480 --> 00:01:16,800
Accelerate will handle all the setups and 
training tweaks mentioned on the first slide.  

14
00:01:18,320 --> 00:01:21,600
It's only one API to learn and 
master instead of ten different ones.  

15
00:01:23,120 --> 00:01:27,120
More specifically, you have to import 
and instantiate an accelerator object,  

16
00:01:27,120 --> 00:01:30,000
that will handle all the necessary 
code for your specific setup.  

17
00:01:31,200 --> 00:01:36,880
Then you have to send it the model, optimizer and 
dataloaders you are using in the prepare method,  

18
00:01:37,760 --> 00:01:43,600
which is the main method to remember. Accelerate 
handles device placement, so you don't need to put  

19
00:01:43,600 --> 00:01:49,840
your batch on the specific device you are using. 
Finally, you have to replace the loss.backward  

20
00:01:49,840 --> 00:01:54,880
line by accelerate.backward(loss), 
and that's all you need!  

21
00:01:58,240 --> 00:02:00,480
Accelerate also handles distributed evaluation.  

22
00:02:01,440 --> 00:02:05,280
You can still use a classic evaluation loop 
such as the one we saw in the "Raw training  

23
00:02:05,280 --> 00:02:09,760
loop" video, in which case all processes 
will each perform the full evaluation.  

24
00:02:11,040 --> 00:02:15,360
To use a distributed evaluation, you just 
have to adapt your evaluation loop like this:  

25
00:02:16,080 --> 00:02:19,920
pass along the evaluation dataloader 
to the accelerator.prepare method,  

26
00:02:19,920 --> 00:02:25,200
like for training. Then you can dismiss the 
line that places the batch on the proper device,  

27
00:02:25,920 --> 00:02:28,800
and just before passing your 
predictions and labels to your metric,  

28
00:02:29,440 --> 00:02:36,160
use accelerator.gather to gather together 
the predictions and labels from each process.  

29
00:02:36,160 --> 00:02:41,440
A distributed training script has to be launched 
several times on different processes (for instance  

30
00:02:41,440 --> 00:02:47,360
one per GPU you are using). You can use the 
PyTorch tools if you are familiar with them,  

31
00:02:48,000 --> 00:02:51,760
but Accelerate also provides an 
easy API to configure your setup  

32
00:02:51,760 --> 00:02:58,000
and launch your training script. In a terminal, 
run accelerate config and answer the small  

33
00:02:58,000 --> 00:03:01,680
questionnaire to generate a configuration 
file with all the relevant information,  

34
00:03:03,120 --> 00:03:07,360
then you can just run accelerate launch, 
followed by the path to your training script.  

35
00:03:08,400 --> 00:03:19,840
In a notebook, you can use the notebook_launcher 
function to launch your training function.
