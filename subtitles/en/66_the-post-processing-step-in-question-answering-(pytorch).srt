1
00:00:05,680 --> 00:00:12,000
The post-processing step in a question 
answering task. When doing question answering,  

2
00:00:12,000 --> 00:00:17,440
the processing of the initial dataset implies 
splitting examples in several features, which  

3
00:00:17,440 --> 00:00:23,760
may or may not contain the answer. Passing those 
features through the model will give us logits for  

4
00:00:23,760 --> 00:00:29,280
the start and end positions, since our labels are 
the indices of the tokens that correspond to the  

5
00:00:29,280 --> 00:00:35,600
start and end the answer. We must then somehow 
convert those logits into an answer, and then  

6
00:00:35,600 --> 00:00:40,480
pick one of the various answers each feature 
gives to be THE answer for a given example.  

7
00:00:42,080 --> 00:00:46,080
For the processing step, you should refer 
to the video linked below. It's not very  

8
00:00:46,080 --> 00:00:50,240
different for validation, we just need to 
add a few lines to keep track of two things:  

9
00:00:51,440 --> 00:00:56,000
instead of discarding the offset mapping, 
we keep them, and also include in them the  

10
00:00:56,000 --> 00:01:01,440
information of where the context is by setting 
the offsets of the special tokens and the question  

11
00:01:01,440 --> 00:01:06,400
to None. Then we also keep track 
of the example ID for each feature,  

12
00:01:06,400 --> 00:01:10,160
to be able to map back feature to the 
examples that they originated from.  

13
00:01:11,680 --> 00:01:15,840
If you don't want to compute the validation loss, 
you won't need to include all the special code  

14
00:01:15,840 --> 00:01:21,360
that we used to create the labels. With this done, 
we can apply that preprocessing function using the  

15
00:01:21,360 --> 00:01:26,160
map method. We take the SQUAD dataset like in 
the preprocessing for question-answering video.  

16
00:01:27,520 --> 00:01:31,920
Once this is done, the next step is to create 
our model. We use the default model behind the  

17
00:01:31,920 --> 00:01:36,000
question-answering pipeline here, but you 
should use any model you want to evaluate.  

18
00:01:36,720 --> 00:01:41,200
We will run a manual evaluation loop, so we 
create a PyTorch DataLoader with our features.  

19
00:01:42,240 --> 00:01:46,400
With it, we can compute and gather all 
the start and end logits like this,  

20
00:01:46,400 --> 00:01:52,240
with a standard PyTorch evaluation loop. With this 
done, we can really dive into the post-processing.  

21
00:01:53,680 --> 00:01:57,440
We will need a map from examples to 
features, which we can create like this.  

22
00:01:58,560 --> 00:02:02,720
Now, for the main part of the post-processing, 
let's see how to extract an answer from the  

23
00:02:02,720 --> 00:02:08,480
logits. We could just take the best index for the 
start and end logits and be done, but if our model  

24
00:02:08,480 --> 00:02:13,200
predicts something impossible, like tokens in 
the question, we will look at more of the logits.  

25
00:02:15,040 --> 00:02:19,120
Note that in the question-answering pipeline, 
we attributed score to each answer based on the  

26
00:02:19,120 --> 00:02:24,560
probabilities, which we did not compute here. 
In terms of logits, the multiplication we had  

27
00:02:24,560 --> 00:02:31,520
in the scores becomes an addition. To go fast, we 
don't look at all possible start and end logits,  

28
00:02:31,520 --> 00:02:37,120
but the twenty best ones. We ignore the logits 
that spawn impossible answers or answer that are  

29
00:02:37,120 --> 00:02:43,040
too long. As we saw in the preprocessing, 
the labels (0, 0) correspond to no answer,  

30
00:02:43,040 --> 00:02:48,400
otherwise we use the offsets to get the answer 
inside the context. Let's have a look at the  

31
00:02:48,400 --> 00:02:52,640
predicted answer for the first feature, which 
is the answer with the best score (or the best  

32
00:02:52,640 --> 00:02:58,720
logit score since the SoftMax is an increasing 
function). The model got it right! Next we just  

33
00:02:58,720 --> 00:03:03,920
have to loop this for every example, picking for 
each the answer with the best logit score in all  

34
00:03:03,920 --> 00:03:15,440
the features the example generated. Now you know 
how to get answers from your model predictions!
