1
00:00:05,840 --> 00:00:12,400
What is domain adaptation? When fine-tuning 
a pretrained model on a new dataset,  

2
00:00:12,400 --> 00:00:17,200
the fine-tuned model we obtain will make 
predictions that are attuned to this new dataset.  

3
00:00:18,640 --> 00:00:23,440
When the two models are trained with the same 
task, we can then compare their predictions  

4
00:00:23,440 --> 00:00:27,600
on the same input. The predictions 
of the two models will be different,  

5
00:00:27,600 --> 00:00:32,640
in a way that reflects the differences 
between the two datasets, a phenomenon we call  

6
00:00:32,640 --> 00:00:39,840
domain adaptation. Let's look at an example with 
mask language modeling, by comparing the outputs  

7
00:00:39,840 --> 00:00:44,400
of the pretrained distilBERT model with the 
version fine-tuned in chapter 7 of the course  

8
00:00:44,400 --> 00:00:50,800
(linked below). The pretrained model makes generic 
predictions, whereas the fine-tuned model has its  

9
00:00:50,800 --> 00:00:57,040
first two predictions linked to cinema. Since 
it was fine-tuned on a movie reviews dataset,  

10
00:00:57,040 --> 00:01:00,320
it's perfectly normal to see it 
adapted its suggestions like this.  

11
00:01:01,200 --> 00:01:05,520
Notice how it keeps the same predictions as 
the pretrained model afterward. Even if the  

12
00:01:05,520 --> 00:01:09,920
fine-tuned model adapts to the new dataset, 
it's not forgetting what it was pretrained on.  

13
00:01:11,200 --> 00:01:17,120
This is another example on a translation task. 
On top we use a pretrained French/English model  

14
00:01:17,120 --> 00:01:22,720
and at the bottom, the version we fine-tuned in 
chapter 7. The top model is pretrained on lots of  

15
00:01:22,720 --> 00:01:27,440
texts, and leaves technical English terms like 
plugin and email unchanged in the translation  

16
00:01:28,160 --> 00:01:33,360
(both are perfectly understood by French people). 
The dataset picked for the fine-tuning is a  

17
00:01:33,360 --> 00:01:38,240
dataset of technical texts where special attention 
was picked to translate everything in French.  

18
00:01:38,960 --> 00:01:50,560
As a result, the fine-tuned model picked that 
habit and translated both plugin and email.
