1
00:00:05,130 --> 00:00:06,130
The Push to Hub API.

2
00:00:06,130 --> 00:00:10,310
Let's have a look at the push_to_hub API.

3
00:00:10,310 --> 00:00:16,209
You will need to be logged in with your Hugging
Face account, which you can do by executing

4
00:00:16,209 --> 00:00:22,220
this first cell or typing huggingface-cli
login in a terminal.

5
00:00:22,220 --> 00:00:27,480
Just enter your username and password and
click login, which will store an authentication

6
00:00:27,480 --> 00:00:31,230
token in the cache of the machine you're using.

7
00:00:31,230 --> 00:00:37,990
Now, let's launch the fine-tuning of a BERT
model on the GLUE COLA dataset.

8
00:00:37,990 --> 00:00:41,900
We won't go over the fine-tuning code because
you can find it in any Transformers tutorial,

9
00:00:41,900 --> 00:00:44,350
or by looking at the videos linked below.

10
00:00:44,350 --> 00:00:49,920
What interests us here, is how we can leverage
the Model Hub during training.

11
00:00:49,920 --> 00:00:56,500
This is done with the push_to_hub=True passed
in your TrainingArguments . This will automatically

12
00:00:56,500 --> 00:01:02,149
upload your model to the Hub each time it
is saved (so every epoch in our case), which

13
00:01:02,149 --> 00:01:08,260
allows you to resume training from a different
machine if the current one gets interrupted.

14
00:01:08,260 --> 00:01:13,610
The model will be uploaded in your namespace,
with the name of the output directory as a

15
00:01:13,610 --> 00:01:14,690
repository name.

16
00:01:14,690 --> 00:01:20,580
You can pick another name by passing it to
the hub_model_id argument, and you can also

17
00:01:20,580 --> 00:01:32,420
push inside an organization you are a member
of by passing a full repository name.

18
00:01:32,420 --> 00:01:43,290
With that done, we can just launch training
and wait a little bit.

19
00:01:43,290 --> 00:01:47,820
Note that the model is pushed asynchronously,
meaning that the training continues while

20
00:01:47,820 --> 00:01:50,119
your model is uploaded to the Hub.

21
00:01:50,119 --> 00:02:02,399
When your first commit is finished, you can
go inspect your model on the Hub and even

22
00:02:02,399 --> 00:02:11,000
start playing with its inference widget while
it's training!

23
00:02:11,000 --> 00:02:27,370
There is something wrong with the labels,
but we will fix this later on in this video.

24
00:02:27,370 --> 00:02:33,590
When the training is finished, we should do
one last push with trainer.push_to_hub for

25
00:02:33,590 --> 00:02:35,330
two reasons.

26
00:02:35,330 --> 00:02:39,980
One this will make sure we are uploading the
final version of our models if we didn't already

27
00:02:39,980 --> 00:02:45,860
(for instance if we saved every n steps instead
of every epoch).

28
00:02:45,860 --> 00:02:51,310
Two, this will draft a model card that will
be the landing page of your model repo.

29
00:02:51,310 --> 00:03:04,690
Going back to the model page, you can see
the Trainer included some metadata that is

30
00:03:04,690 --> 00:03:15,350
interpreted by the Hugging Face website in
the model card.

31
00:03:15,350 --> 00:03:20,120
On top of informations about the training,
the intermediate results or the hyperparameter

32
00:03:20,120 --> 00:03:26,770
used, we get the values of the metrics automatically
displayed in a small widget, and a link to

33
00:03:26,770 --> 00:03:28,860
a leaderboard in Paper with Code.

34
00:03:28,860 --> 00:03:35,000
The Tensorboard runs have also been pushed
to this repo, and we can look at them directly

35
00:03:35,000 --> 00:03:36,000
from the Model Hub.

36
00:03:36,000 --> 00:03:43,709
If you were not using the Trainer API to fine-tune
your model, you can use the push_to_hub method

37
00:03:43,709 --> 00:03:45,319
on the model and tokenizer directly.

38
00:03:45,319 --> 00:03:49,340
Let's test this to fix our labels in the inference
widget!

39
00:03:49,340 --> 00:03:54,140
The inference widget was using default names
for labels because we did not indicate the

40
00:03:54,140 --> 00:03:57,100
correspondence between integers and label
names.

41
00:03:57,100 --> 00:04:02,909
We can fix in the configuration by setting
the label2id and id2label fields to their

42
00:04:02,909 --> 00:04:07,370
proper value then we can push the fixed config
to our repo using the push_to_hub method.

43
00:04:07,370 --> 00:04:12,220
Once this is done and we can check on the
website the model is now showing the proper

44
00:04:12,220 --> 00:04:13,440
labels!

45
00:04:13,440 --> 00:04:21,280
Now that the model is on the hub, we can use
it from anywhere with the from_pretrained

46
00:04:21,280 --> 00:04:22,370
method.

47
00:04:22,370 --> 00:04:38,880
We just have to use the identifier from the
hub and we can see that the model configuration

48
00:04:38,880 --> 00:04:39,880
and weights are automatically downloaded.

49
00:04:39,880 --> 00:04:49,860
We can use this model as we would any other
Transformers model, for instance by loading

50
00:04:49,860 --> 00:04:53,949
it in a pipeline.

51
00:04:53,949 --> 00:04:57,550
Try the push_to_hub API on your next training
to easily share your model with the rest of

52
00:04:57,550 --> 00:05:04,800
the world!
