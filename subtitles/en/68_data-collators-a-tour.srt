1
00:00:06,220 --> 00:00:12,290
In a lot of our examples, you're going to
see DataCollators popping up over and over.

2
00:00:12,290 --> 00:00:18,010
They're used in both PyTorch and TensorFlow
workflows, and maybe even in JAX, but no-one

3
00:00:18,010 --> 00:00:20,260
really knows what's happening in JAX.

4
00:00:20,260 --> 00:00:24,590
We have a research team working on that, so
maybe they'll tell us soon.

5
00:00:24,590 --> 00:00:27,869
But what are data collators?

6
00:00:27,869 --> 00:00:32,230
Data collators collate data.

7
00:00:32,230 --> 00:00:37,930
More specifically, they put together a list
of samples into a single training minibatch.

8
00:00:37,930 --> 00:00:41,820
For some tasks, the data collator can be very
straightforward.

9
00:00:41,820 --> 00:00:47,010
For example, when you're doing sequence classification,
all you really need from your data collator

10
00:00:47,010 --> 00:00:53,480
is that it pads your samples to the same length
and concatenates them into a single Tensor.

11
00:00:53,480 --> 00:00:58,989
But for other workflows, data collators can
be more complex, as they handle some of the

12
00:00:58,989 --> 00:01:04,879
preprocessing needed for that particular task.

13
00:01:04,879 --> 00:01:09,600
For PyTorch users, you usually pass the DataCollator
to your Trainer object.

14
00:01:09,600 --> 00:01:15,549
In TensorFlow, the easiest way to use a DataCollator
is to pass it to the to_tf_dataset method

15
00:01:15,549 --> 00:01:23,700
of your dataset.

16
00:01:23,700 --> 00:01:27,420
You'll see these approaches used in the examples
and notebooks throughout this course.

17
00:01:27,420 --> 00:01:28,820
In both cases, you end up with an iterable
that's going to output collated batches, ready

18
00:01:28,820 --> 00:01:29,820
for training.

19
00:01:29,820 --> 00:01:34,360
Note that all of our collators take a return_tensors
argument - you can set this to "pt" to get

20
00:01:34,360 --> 00:01:40,820
PyTorch Tensors, "tf" to get TensorFlow Tensors,
or "np" to get Numpy arrays.

21
00:01:40,820 --> 00:01:46,060
For backward compatibility reasons, the default
value is "pt", so PyTorch users don't even

22
00:01:46,060 --> 00:01:51,110
have to set this argument most of the time,
and so are often totally unaware that this

23
00:01:51,110 --> 00:01:52,110
option exists.

24
00:01:52,110 --> 00:01:59,160
This is a valuable lesson about how the beneficiaries
of privilege are often the most blind to its

25
00:01:59,160 --> 00:02:00,160
existence.

26
00:02:00,160 --> 00:02:08,130
So now let's see some specific DataCollators
in action, though remember that if none of

27
00:02:08,130 --> 00:02:12,069
them do what you need, you can always write
your own!

28
00:02:12,069 --> 00:02:17,120
First, we'll see the "basic" data collators.

29
00:02:17,120 --> 00:02:21,550
These are DefaultDataCollator and DataCollatorWithPadding.

30
00:02:21,550 --> 00:02:25,550
These are the ones you should use if your
labels are straightforward and your data doesn't

31
00:02:25,550 --> 00:02:28,780
need any special processing before being ready
for training.

32
00:02:28,780 --> 00:02:30,100
Most sequence classification tasks, for example,
would use one of these data collators.

33
00:02:30,100 --> 00:02:35,470
Remember that because different models have
different padding tokens, DataCollatorWithPadding

34
00:02:35,470 --> 00:02:39,239
will need your model's Tokenizer so it knows
how to pad sequences properly!

35
00:02:39,239 --> 00:02:41,069
So how do you choose one of these?

36
00:02:41,069 --> 00:02:44,970
Simple: As you can see here, if you have variable
sequence lengths then you should use DataCollatorWithPadding,

37
00:02:44,970 --> 00:02:46,690
which will pad all your sequences to the same
length.

38
00:02:46,690 --> 00:02:49,819
If you're sure all your sequences are the
same length then you can use the even simpler

39
00:02:49,819 --> 00:02:51,510
DefaultDataCollator, but it'll give you an
error if that assumption is wrong!

40
00:02:51,510 --> 00:02:57,720
Moving on, though, many of the other data
collators are often designed to handle one

41
00:02:57,720 --> 00:03:07,411
specific task, and that's the case with DataCollatorForTokenClassification
and DataCollatorForSeqToSeq.

42
00:03:07,411 --> 00:03:12,830
These tasks need special collators because
the labels are variable in length.

43
00:03:12,830 --> 00:03:16,580
In token classification there's one label
for each token, and that means the length

44
00:03:16,580 --> 00:03:23,599
of the labels can be variable, while in SeqToSeq
the labels are also a sequence of tokens that

45
00:03:23,599 --> 00:03:28,470
can have variable length.

46
00:03:28,470 --> 00:03:38,580
In both of these cases, we handle that by
padding the labels too, as you can see here.

47
00:03:38,580 --> 00:03:43,810
Inputs and the labels will need to be padded
if we want to join samples of variable length

48
00:03:43,810 --> 00:03:50,440
into the same minibatch, and that's exactly
what the data collators will do.

49
00:03:50,440 --> 00:04:01,680
The final data collator I want to show you
is the DataCollatorForLanguageModeling.

50
00:04:01,680 --> 00:04:07,470
It's very important, firstly because language
models are so foundational to everything we

51
00:04:07,470 --> 00:04:15,030
do in NLP, and secondly because it has two
modes that do two very different things.

52
00:04:15,030 --> 00:04:21,889
You choose which mode you want with the mlm
argument - set it to True for masked language

53
00:04:21,889 --> 00:04:26,729
modeling, and False for causal language modeling.

54
00:04:26,729 --> 00:04:31,110
Collating data for causal language modeling
is actually quite straightforward - the model

55
00:04:31,110 --> 00:04:35,962
is just making predictions for what token
comes next, so your labels are more or less

56
00:04:35,962 --> 00:04:40,530
just a copy of your inputs, and the collator
handles that and ensures your inputs and labels

57
00:04:40,530 --> 00:04:42,380
are padded correctly.

58
00:04:42,380 --> 00:04:49,500
When you set mlm to True, though, you get
quite different behaviour!

59
00:04:49,500 --> 00:04:58,250
That's because masked language modeling requires
the labels to be, well... masked.

60
00:04:58,250 --> 00:05:01,539
So what does that look like?

61
00:05:01,539 --> 00:05:06,860
Recall that in masked language modeling, the
model is not predicting "the next word"; instead

62
00:05:06,860 --> 00:05:11,590
we randomly mask out multiple tokens and the
model makes predictions for all of them at

63
00:05:11,590 --> 00:05:12,590
once.

64
00:05:12,590 --> 00:05:18,729
The process of random masking is surprisingly
complex, though - that's because if we follow

65
00:05:18,729 --> 00:05:23,770
the protocol from the original BERT paper,
we need to replace some tokens with a masking

66
00:05:23,770 --> 00:05:30,080
token, other tokens with a random token and
then keep a third set of tokens unchanged.

67
00:05:30,080 --> 00:05:35,919
This isn't the lecture to go into *why* we
do that - you should check out the original

68
00:05:35,919 --> 00:05:40,720
BERT paper if you're curious.

69
00:05:40,720 --> 00:05:46,949
The main thing to know here is that it can
be a real pain to implement yourself, but

70
00:05:46,949 --> 00:05:53,300
DataCollatorForLanguageModeling will do it
for you.

71
00:05:53,300 --> 00:05:57,800
And that's it!

72
00:05:57,800 --> 00:06:15,410
That covers the most commonly used data collators
and the tasks they're used for.
