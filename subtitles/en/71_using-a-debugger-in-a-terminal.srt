1
00:00:05,840 --> 00:00:11,520
Using the Python debugger in a terminal. In this 
video, we'll learn how to use the Python debugger  

2
00:00:11,520 --> 00:00:16,800
in a terminal. For this example, we are running 
code from the token classification section,  

3
00:00:17,600 --> 00:00:22,320
downloading the Conll dataset before 
loading a tokenizer to preprocess it.  

4
00:00:23,200 --> 00:00:28,720
Checkout the section of the course linked below 
for more information. Once this is done, we try  

5
00:00:28,720 --> 00:00:34,240
to batch together some features of the training 
dataset by padding them and returning a tensor,  

6
00:00:37,200 --> 00:00:40,160
then we get the following error.  

7
00:00:42,800 --> 00:00:47,280
We use PyTorch here but you will 
get the same error with TensorFlow.  

8
00:00:49,280 --> 00:00:53,680
As we have seen in the "How to debug an 
error?" video, the error message is at the end  

9
00:00:53,680 --> 00:00:58,640
and it indicates we should use padding... which 
we are actually trying to do. So this is not  

10
00:00:58,640 --> 00:01:03,360
useful and we will need to go a little deeper 
to debug the problem. Fortunately, you can use  

11
00:01:03,360 --> 00:01:10,400
the Python debugger quite easily in a terminal by 
launching your script with python -m pdb instead  

12
00:01:10,400 --> 00:01:17,200
of just python. When executing that command, you 
are sent to the first instruction of your script.  

13
00:01:17,200 --> 00:01:25,840
You can run just the next instruction by typing 
n, or continue to the error by directly typing c.  

14
00:01:29,680 --> 00:01:33,120
Once there, you go to the very bottom of 
the traceback, and you can type commands.  

15
00:01:34,000 --> 00:01:40,160
The first two commands you should learn are u and 
d (for up and down), which allow you to go up in  

16
00:01:40,160 --> 00:01:48,320
the Traceback or down. Going up twice, we get to 
the point the error was reached. The third command  

17
00:01:48,320 --> 00:01:54,000
to learn is p, for print. It allows you to print 
any value you want. For instance here, we can see  

18
00:01:54,000 --> 00:01:59,120
the value of return_tensors or batch_outputs 
to try to understand what triggered the error.  

19
00:02:00,000 --> 00:02:04,720
The batch outputs dictionary is a bit hard to 
see, so let's dive into smaller pieces of it.  

20
00:02:05,360 --> 00:02:10,560
Inside the debugger you can not only print 
any variable but also evaluate any expression,  

21
00:02:10,560 --> 00:02:23,600
so we can look independently at the inputs 
or labels. Those labels are definitely weird:  

22
00:02:24,160 --> 00:02:27,920
they are of various size, which we can 
actually confirm by printing the sizes.  

23
00:02:35,760 --> 00:02:40,160
No wonder the tokenizer wasn't able to create 
a tensor with them! This is because the pad  

24
00:02:40,160 --> 00:02:45,840
method only takes care of the tokenizer outputs: 
input IDs, attention mask and token type IDs,  

25
00:02:46,400 --> 00:02:50,080
so we have to pad the labels ourselves 
before trying to create a tensor with them.  

26
00:02:51,120 --> 00:02:56,880
Once you are ready to exit the Python 
debugger, you can press q for quit. Another  

27
00:02:56,880 --> 00:03:03,600
way we can access the Python debugger is to set 
a "set_trace" instruction where we want in the  

28
00:03:10,480 --> 00:03:23,280
script. It will interrupt the execution and 
launch the Python debugger at this place, and we  

29
00:03:23,280 --> 00:03:32,080
can inspect all the variables before the next 
instruction is executed. Typing n executes the  

30
00:03:32,080 --> 00:03:37,280
next instruction, which takes us back inside 
the traceback. One way to fix the error is  

31
00:03:37,280 --> 00:03:49,760
to manually pad all labels to the longest, or 
we can use the data collator designed for this.
