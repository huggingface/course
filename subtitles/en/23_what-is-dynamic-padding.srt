1
00:00:05,270 --> 00:00:07,640
What is dynamic padding?

2
00:00:07,640 --> 00:00:12,620
In the "Batching Inputs together" video, we
have seen that to be able to group inputs

3
00:00:12,620 --> 00:00:17,320
of different lengths in the same batch, we
need to add padding tokens to all the short

4
00:00:17,320 --> 00:00:20,520
inputs until they are all of the same length.

5
00:00:20,520 --> 00:00:26,300
Here for instance, the longest sentence is
the third one, and we need to add 5, 2 and

6
00:00:26,300 --> 00:00:32,509
7 pad tokens to the other to have four sentences
of the same lengths.

7
00:00:32,509 --> 00:00:37,530
When dealing with a whole dataset, there are
various padding strategies we can apply.

8
00:00:37,530 --> 00:00:41,870
The most obvious one is to pad all the elements
of the dataset to the same length: the length

9
00:00:41,870 --> 00:00:44,129
of the longest sample.

10
00:00:44,129 --> 00:00:48,450
This will then give us batches that all have
the same shape determined by the maximum sequence

11
00:00:48,450 --> 00:00:49,450
length.

12
00:00:49,450 --> 00:00:54,039
The downside is that batches composed from
short sentences will have a lot of padding

13
00:00:54,039 --> 00:01:00,080
tokens which introduce more computations in
the model we ultimately don't need.

14
00:01:00,080 --> 00:01:05,320
To avoid this, another strategy is to pad
the elements when we batch them together,

15
00:01:05,320 --> 00:01:08,240
to the longest sentence inside the batch.

16
00:01:08,240 --> 00:01:12,880
This way batches composed of short inputs
will be smaller than the batch containing

17
00:01:12,880 --> 00:01:15,600
the longest sentence in the dataset.

18
00:01:15,600 --> 00:01:19,090
This will yield some nice speedup on CPU and
GPU.

19
00:01:19,090 --> 00:01:23,130
The downside is that all batches will then
have different shapes, which slows down training

20
00:01:23,130 --> 00:01:24,790
on other accelerators like TPUs.

21
00:01:24,790 --> 00:01:28,850
Let's see how to apply both strategies in
practice.

22
00:01:28,850 --> 00:01:34,750
We have actually seen how to apply fixed padding
in the Datasets Overview video, when we preprocessed

23
00:01:34,750 --> 00:01:39,320
the MRPC dataset: after loading the dataset
and tokenizer, we applied the tokenization

24
00:01:39,320 --> 00:01:45,260
to all the dataset with padding and truncation
to make all samples of length 128.

25
00:01:45,260 --> 00:01:51,630
As a result, if we pass this dataset to a
PyTorch DataLoader, we get batches of shape

26
00:01:51,630 --> 00:01:57,079
batch size (here 16) by 128.

27
00:01:57,079 --> 00:02:01,950
To apply dynamic padding, we must defer the
padding to the batch preparation, so we remove

28
00:02:01,950 --> 00:02:04,789
that part from our tokenize function.

29
00:02:04,789 --> 00:02:08,569
We still leave the truncation part so that
inputs that are bigger than the maximum length

30
00:02:08,569 --> 00:02:14,069
accepted by the model (usually 512) get truncated
to that length.

31
00:02:14,069 --> 00:02:17,629
Then we pad our samples dynamically by using
a data collator.

32
00:02:17,629 --> 00:02:22,110
Those classes in the Transformers library
are responsible for applying all the final

33
00:02:22,110 --> 00:02:27,970
processing needed before forming a batch,
here DataCollatorWithPadding will pad the

34
00:02:27,970 --> 00:02:32,200
samples to the maximum length inside the batch
of sentences.

35
00:02:32,200 --> 00:02:36,790
We pass it to the PyTorch DataLoader as a
collate function, then observe that the batches

36
00:02:36,790 --> 00:02:42,950
generated have various lenghs, all way below
the 128 from before.

37
00:02:42,950 --> 00:02:48,200
Dynamic batching will almost always be faster
on CPUs and GPUs, so you should apply it if

38
00:02:48,200 --> 00:02:49,200
you can.

39
00:02:49,200 --> 00:02:53,879
Remember to switch back to fixed padding however
if you run your training script on TPU or

40
00:02:53,879 --> 00:03:00,599
need batches of fixed shapes.
