1
00:00:05,040 --> 00:00:12,000
Hi, this is going to be a video about the 
push_to_hub API for Tensorflow and Keras. So,  

2
00:00:12,000 --> 00:00:16,240
to get started, we'll open up our notebook, 
and the first thing you'll need to do is  

3
00:00:16,240 --> 00:00:22,480
log in to your HuggingFace account, for example 
with the notebook login function. So to do that,  

4
00:00:23,040 --> 00:00:28,560
you simply call the function, the popup will 
emerge, you enter your username and password,  

5
00:00:28,560 --> 00:00:34,160
which I'm going to pull out of my password 
manager here, and you're logged in. The next  

6
00:00:34,160 --> 00:00:38,720
two cells are just getting everything ready for 
training. So we're just going to load a dataset,  

7
00:00:38,720 --> 00:00:42,960
we're going to tokenize that dataset, and then 
we're going to load our model and compile it  

8
00:00:42,960 --> 00:00:47,040
with the standard Adam optimizer. So 
I'm just going to run all of those,  

9
00:00:49,600 --> 00:00:53,760
we'll wait a few seconds, and 
everything should be ready for training.  

10
00:00:57,600 --> 00:01:03,200
Okay, so now we're ready to train I'm 
going to show you the two ways you can  

11
00:01:03,200 --> 00:01:07,520
push your model to the Hub. So the 
first is with the PushToHubCallback.  

12
00:01:08,080 --> 00:01:14,640
So a callback in Keras is a function that's called 
regularly during training. You can set it to be  

13
00:01:14,640 --> 00:01:20,640
called after a certain number of steps, or every 
epoch, or even just once at the end of training.  

14
00:01:22,480 --> 00:01:27,600
So a lot of callbacks in Keras, for example, 
control learning rate decaying on plateau  

15
00:01:28,320 --> 00:01:34,400
and things like that. And so this callback, by 
default, will save your model to the Hub once  

16
00:01:34,400 --> 00:01:39,200
every epoch. And that's really helpful especially 
if your training is very long, because that means  

17
00:01:39,200 --> 00:01:43,680
you can resume from that save, so you get this 
automatic cloud-saving of your model, and you can  

18
00:01:43,680 --> 00:01:49,760
even run inference with the checkpoints of your 
model that have been uploaded by this callback,  

19
00:01:50,720 --> 00:01:55,120
and that means you can, y'know, actually 
run some test inputs and actually see how  

20
00:01:55,120 --> 00:02:00,560
your model works at various stages during 
training, which is a really nice feature. So  

21
00:02:01,280 --> 00:02:06,560
we're going to add the PushToHubCallback, and it 
takes just a few arguments. So the first argument  

22
00:02:06,560 --> 00:02:11,920
is the temporary directory that files are going 
to be saved to before they're uploaded to the Hub.  

23
00:02:11,920 --> 00:02:16,880
The second argument is the tokenizer, and the 
third argument here is the keyword argument  

24
00:02:17,600 --> 00:02:22,160
hub_model_id. So that's the name it's going 
to be saved under on the HuggingFace Hub.  

25
00:02:23,200 --> 00:02:29,760
You can also upload to an organization account 
just by adding the organization name before  

26
00:02:29,760 --> 00:02:34,320
the repository name with a slash like this. So 
you probably don't have permissions to upload to  

27
00:02:34,320 --> 00:02:38,640
the Hugging Face organization, if you do please 
file a bug and let us know extremely urgently.  

28
00:02:40,640 --> 00:02:44,000
But if you do have access to your own 
organization then you can use that  

29
00:02:44,000 --> 00:02:47,600
same approach to upload models to their 
account instead of to your own personal  

30
00:02:49,280 --> 00:02:56,080
set of models. So, once you've made your 
callback you simply add it to the callbacks list  

31
00:02:56,080 --> 00:03:01,280
when you're called model.fit() and everything is 
uploaded for you from there, and there's nothing  

32
00:03:01,280 --> 00:03:06,320
else to worry about. The second way to upload a 
model, though, is to call model.push_to_hub().  

33
00:03:06,880 --> 00:03:11,920
So this is more of a once-off method - it's not 
called regularly during training. You can just  

34
00:03:11,920 --> 00:03:17,680
call this manually whenever you want to upload 
a model to the hub. So we recommend running this  

35
00:03:17,680 --> 00:03:22,720
after the end of training, just to make sure that 
you have a commit message just to guarantee that  

36
00:03:22,720 --> 00:03:27,280
this was the final version of the model at the 
end of training. And it just makes sure that  

37
00:03:28,160 --> 00:03:32,000
you're working with the definitive end-of-training 
model and not accidentally using a model that's  

38
00:03:32,000 --> 00:03:36,720
from a checkpoint somewhere along the way. 
So I'm going to run both of these cells  

39
00:03:38,800 --> 00:03:42,320
and then I'm going to cut the video here, just 
because training is going to take a couple of  

40
00:03:42,320 --> 00:03:46,160
minutes, and so I'll skip forward to the end of 
that, when the models have all been uploaded,  

41
00:03:46,160 --> 00:03:50,880
and I'm gonna show you how you can access 
the models in the Hub and the other things  

42
00:03:50,880 --> 00:03:58,400
you can do with them from there. Okay, 
we're back and our model was uploaded,  

43
00:03:58,960 --> 00:04:03,760
both by the PushToHubCallback and also by our 
call to model.push_to_hub() after training.  

44
00:04:04,720 --> 00:04:10,320
So everything's looking good! So now if we drop 
over to my profile on HuggingFace, and you can get  

45
00:04:10,320 --> 00:04:15,760
there just by clicking the profile button in the 
dropdown, we can see that the bert-fine-tuned-cola  

46
00:04:15,760 --> 00:04:20,560
model is here, and was updated 3 minutes ago. So 
it'll always be at the top of your list, because  

47
00:04:20,560 --> 00:04:25,280
they're sorted by how recently they were updated. 
And we can start querying our model immediately!  

48
00:04:26,640 --> 00:04:36,720
So the dataset we were training on is the Glue 
CoLA dataset, and CoLA is an acronym for Corpus  

49
00:04:36,720 --> 00:04:42,560
of Linguistic Acceptability. So what that means 
is that the model is being trained to decide if a  

50
00:04:42,560 --> 00:04:49,040
sentence is grammatically or linguistically okay, 
or if there's a problem with it. For example,  

51
00:04:49,680 --> 00:04:54,400
we could say "This is a legitimate sentence" 
and hopefully it realizes that this is in  

52
00:04:54,400 --> 00:05:00,880
fact a legitimate sentence. So it might take a 
couple of seconds for the model to load when you  

53
00:05:00,880 --> 00:05:05,200
call it for the first time, so I might cut 
a couple of seconds out of this video here.  

54
00:05:07,680 --> 00:05:14,160
Okay, we're back! The model loaded and we got 
an output, but there's an obvious problem here.  

55
00:05:14,160 --> 00:05:19,680
So these labels aren't really telling us what 
categories the model has actually assigned to  

56
00:05:19,680 --> 00:05:26,720
this input sentence. So if we want to fix that, we 
want to make sure the model config has the correct  

57
00:05:26,720 --> 00:05:31,920
names for each of the label classes, and then we 
want to upload that config. So we can do that down  

58
00:05:31,920 --> 00:05:38,480
here. To get the label_names, we can get that 
from the dataset we loaded, from the 'features'  

59
00:05:38,480 --> 00:05:44,160
attribute it has. And then we can create 
dictionaries "id2label" and "label2id"  

60
00:05:45,200 --> 00:05:51,040
and just assign them to the model config, and then 
we can just push our updated config and that'll  

61
00:05:51,040 --> 00:05:58,080
override the existing config in the Hub repo. So 
that's just been done, so now if we go back here,  

62
00:05:58,080 --> 00:06:02,720
I'm going to use a slightly different sentence 
because the outputs for sentences are sometimes  

63
00:06:02,720 --> 00:06:07,600
cached, and so if we want to generate new results 
I'm going to use something slightly different. So  

64
00:06:07,600 --> 00:06:13,840
let's try an incorrect sentence, so this is not 
valid English grammar and hopefully the model will  

65
00:06:13,840 --> 00:06:17,360
see that. It's going to reload here, so 
I'm going to cut a couple of seconds here,  

66
00:06:18,480 --> 00:06:26,400
and then we'll see what the model is going to say. 
Okay! So the model's confidence isn't very good,  

67
00:06:26,400 --> 00:06:31,440
because of course we didn't really optimize our 
hyperparameters at all, but it has decided that  

68
00:06:31,440 --> 00:06:37,200
this sentence is more likely to be unacceptable 
than acceptable. Presumably if we tried a bit  

69
00:06:37,200 --> 00:06:41,280
harder with training we could get a much lower 
validation loss and therefore the model's  

70
00:06:41,280 --> 00:06:47,040
predictions would be more precise. But let's 
try our original sentence again - of course,  

71
00:06:47,040 --> 00:06:52,560
because of the caching issue we're seeing 
that the original answers are unchanged.  

72
00:06:52,560 --> 00:06:58,160
So let's try a different, valid sentence. So 
let's try "This is a valid English sentence".  

73
00:06:59,920 --> 00:07:03,680
And we see that now the model correctly decides 
that it has a very high probability of being  

74
00:07:03,680 --> 00:07:09,840
acceptable and a very low probability of being 
unacceptable. So you can use this inference API  

75
00:07:09,840 --> 00:07:14,320
even with the checkpoints that are uploaded during 
training, so it can be very interesting to see how  

76
00:07:15,200 --> 00:07:19,680
the model's predictions for sample inputs 
change with each epoch of training.  

77
00:07:21,920 --> 00:07:27,040
Also, the model we've uploaded is going to be 
accessible to you and, if it's shared publicly,  

78
00:07:27,040 --> 00:07:32,240
to anyone else. So if you want to load that 
model all you, or anyone else, needs to do  

79
00:07:34,160 --> 00:07:40,640
is just to load it in either a pipeline 
or you can just load it with, for example,  

80
00:07:40,640 --> 00:07:50,960
TFAutoModelForSequenceClassification and then 
for the name you would just simply pass the path  

81
00:07:50,960 --> 00:07:58,560
to the repo you want to upload - or to download, 
excuse me. So if I want to use this model again,  

82
00:07:58,560 --> 00:08:02,880
if I want to load it from the hub, I just run this 
one line of code, the model will be downloaded  

83
00:08:05,280 --> 00:08:11,200
and with any luck it'll be ready to 
fine-tune on a different dataset,  

84
00:08:11,200 --> 00:08:17,760
make predictions with, or do anything else you 
wanna do. So that was a quick overview of how,  

85
00:08:17,760 --> 00:08:21,280
after your training or during your 
training, you can upload models to the Hub,  

86
00:08:21,280 --> 00:08:26,800
you can checkpoint there, you can resume training 
from there, and you can get inference results from  

87
00:08:26,800 --> 00:08:37,040
the models you've uploaded. So thank you, 
and I hope to see you in a future video!
