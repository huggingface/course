1
00:00:05,520 --> 00:00:10,000
Let's see together what is the training 
strategy of the WordPiece algorithm  

2
00:00:10,560 --> 00:00:15,920
and how it performs the 
tokenization of a text once trained  

3
00:00:19,200 --> 00:00:25,280
WordPiece is a tokenization algorithm introduced 
by Google. It is used for example by Bert.  

4
00:00:26,480 --> 00:00:30,640
To our knowledge, the code of Word 
Pieces has not been open sourced,  

5
00:00:31,360 --> 00:00:36,640
so we base our explanations on our own 
interpretation of the published literature.  

6
00:00:42,480 --> 00:00:48,480
What is the training strategy of 
WordPiece? Similarly to the BPE algorithm,  

7
00:00:48,480 --> 00:00:54,480
WordPiece starts by establishing an initial 
vocabulary composed of elementary units  

8
00:00:54,480 --> 00:01:01,760
and then increases this vocabulary to the 
desired size. To build the initial vocabulary,  

9
00:01:01,760 --> 00:01:07,120
we divide each word in the training corpus 
into the sequence of letters that make it up.  

10
00:01:08,240 --> 00:01:14,000
As you can see, there is a small subtlety: 
we add a 2 hashtags in front of the letters  

11
00:01:14,000 --> 00:01:20,240
that do not start a word. By keeping 
only one occurrence per elementary unit  

12
00:01:20,240 --> 00:01:29,440
we now have our initial vocabulary. We will 
list all the existing pairs in our corpus.  

13
00:01:30,800 --> 00:01:34,960
Once we have this list, we will calculate 
a score for each of these pairs.  

14
00:01:36,400 --> 00:01:40,400
As for the BPE algorithm, we will 
select the pair with the highest score.  

15
00:01:43,040 --> 00:01:50,000
Taking for example the first pair composed 
of H and U. The score of a pair is simply  

16
00:01:50,000 --> 00:01:54,720
equal to the frequency of appearance of 
the pair divided by the product of the  

17
00:01:54,720 --> 00:01:59,840
frequency of appearance of the first token by 
the frequency of appearance of the second token.  

18
00:02:01,120 --> 00:02:04,560
Thus at a fixed frequency 
of appearance of the pair,  

19
00:02:05,360 --> 00:02:11,440
if the subparts of the pair are very frequent 
in the corpus then this score will be decreased.  

20
00:02:12,960 --> 00:02:24,000
In our example, the pair "hu" appears 4 times, the 
letter "h" 4 times and the letter u 4 times. This  

21
00:02:24,000 --> 00:02:32,320
gives us a score of 0.25. Now that we know how to 
calculate this score, we can do it for all pairs.  

22
00:02:33,200 --> 00:02:36,480
We can now add to the vocabulary 
the pair with the highest score,  

23
00:02:37,120 --> 00:02:43,520
after merging it of course! And now we can 
apply this same fusion to our split corpus.  

24
00:02:45,600 --> 00:02:51,520
As you can imagine, we just have to repeat the 
same operations until we have the vocabulary at  

25
00:02:51,520 --> 00:03:00,320
the desired size! Let's look at a few more steps 
to see the evolution of our vocabulary and the  

26
00:03:00,320 --> 00:03:09,840
length of the splits getting shorter. Now that we 
are happy with our vocabulary, you are probably  

27
00:03:09,840 --> 00:03:16,400
wondering how to use it to tokenize a text. Let's 
say we want to tokenize the word "huggingface".  

28
00:03:17,760 --> 00:03:23,280
WordPiece follows these rules: We will look for 
the longest possible token at the beginning of  

29
00:03:23,280 --> 00:03:30,560
our word. Then we start again on the remaining 
part of our word. And so on until we reach the  

30
00:03:30,560 --> 00:03:38,240
end! And that's it, huggingface is divided 
into 4 sub-tokens. ßThis video is about to  

31
00:03:38,240 --> 00:03:43,040
end, I hope it helped you to understand 
better what is behind the word WordPiece!
