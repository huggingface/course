1
00:00:06,080 --> 00:00:10,960
In this video, we will see how to debug an error 
you encounter when running trainer.train().  

2
00:00:12,240 --> 00:00:17,280
As an example, we will use this script that 
finetunes a bert model on the GLUE MNLI dataset.  

3
00:00:17,840 --> 00:00:20,880
Checkout the videos linked below to 
see how we came to such a script,  

4
00:00:21,680 --> 00:00:26,960
here we want to learn how to debug the problems 
in it. Running the script gives us an error pretty  

5
00:00:26,960 --> 00:00:31,920
fast. It happens at the line where we feed the 
inputs to the model, according to the traceback.  

6
00:00:32,640 --> 00:00:36,720
That tells us there is a problem there, but the 
problem could come from many different causes.  

7
00:00:37,520 --> 00:00:41,600
To debug an error in a training, you need to 
make sure each step of the training pipeline  

8
00:00:41,600 --> 00:00:46,160
works as intended. This means checking that 
the inputs of your dataset are correct,  

9
00:00:46,800 --> 00:00:50,560
you can batch them together, feed 
them through the model to get a loss,  

10
00:00:50,560 --> 00:00:54,080
then compute the gradients of that loss 
before performing an optimizer step.  

11
00:00:55,280 --> 00:01:00,480
So let's start by looking at the training dataset 
this Trainer is using. There is definitely a  

12
00:01:00,480 --> 00:01:07,040
problem there as we see texts and not numbers. The 
error message was telling us the model did not get  

13
00:01:07,040 --> 00:01:13,120
input IDs and we do not have those in the dataset 
indeed. Looking back at our code, we can see we  

14
00:01:13,120 --> 00:01:18,800
made a mistake and passed the wrong datasets to 
the Trainer. So let's fix that and run again.  

15
00:01:20,240 --> 00:01:25,680
Now we have a new error. Inspecting the traceback 
tells us it happens when we try to create a batch,  

16
00:01:25,680 --> 00:01:31,920
specifically to group the features in a tensor. 
We can confirm this by asking the Trainer to get  

17
00:01:31,920 --> 00:01:37,600
us a batch of the training data loader, which 
reproduces the same error. Either by inspecting  

18
00:01:37,600 --> 00:01:43,600
the inputs or debugging, we can then see they are 
not all of the same size. This is because we have  

19
00:01:43,600 --> 00:01:48,240
not passed a data collator to do the padding in 
the Trainer and didn't pad when preprocessing  

20
00:01:48,240 --> 00:01:53,440
the data either. Padding inside the Trainer is 
normally the default, but only if you provide  

21
00:01:53,440 --> 00:01:58,800
your tokenizer to the Trainer, and we forgot to 
do that. So let's fix the issue and run again.  

22
00:02:00,320 --> 00:02:06,400
This time we get a nasty CUDA error. They 
are very difficult to debug because for one,  

23
00:02:07,120 --> 00:02:11,280
they put your kernel in a state that is not 
recoverable (so you have to restart your  

24
00:02:11,280 --> 00:02:15,840
notebook from the beginning) and two, the 
traceback is completely useless for those.  

25
00:02:16,800 --> 00:02:22,240
Here the traceback tells us the error happens when 
we do the gradient computation with loss.backward,  

26
00:02:22,240 --> 00:02:27,840
but as we will see later on that is not the 
case. This is because everything that happens  

27
00:02:27,840 --> 00:02:33,520
on the GPU is done asynchronously: when you 
execute the model call, what the program does  

28
00:02:33,520 --> 00:02:39,280
is just stacking that in the queue of GPU, then 
(if the GPU didn't have any current job to do),  

29
00:02:39,280 --> 00:02:43,920
the work will start on the GPU at the same time 
as the CPU will move to the next instruction.  

30
00:02:44,800 --> 00:02:50,000
Continuing with the extraction of the loss, this 
is stacked into the GPU queue while the CPU moves  

31
00:02:50,000 --> 00:02:54,960
to the instruction loss.backward. But the GPU 
still hasn't finished the forward pass of the  

32
00:02:54,960 --> 00:03:01,760
model since all that took no time at all. The CPU 
stops moving forward, because loss.backward as an  

33
00:03:01,760 --> 00:03:09,360
instruction telling it to wait for the GPUs to be 
finished, and when the GPU encounters an error,  

34
00:03:09,360 --> 00:03:15,040
it gives with a cryptic message back to the 
CPU, who raises the error at the wrong place.  

35
00:03:16,080 --> 00:03:20,320
So to debug this, we will need to execute the 
next steps of the training pipeline on the CPU.  

36
00:03:20,960 --> 00:03:26,320
It is very easy to do, and we get a traceback 
we can trust this time. As we said before,  

37
00:03:26,320 --> 00:03:32,720
the error happens during the forward pass 
of the model, and it's an index error.  

38
00:03:33,360 --> 00:03:38,800
With a bit of debugging, we see we have labels 
ranging from 0 to 2, so three different values,  

39
00:03:38,800 --> 00:03:44,240
but our outputs have a shape of batch size per 2. 
It looks like our model has the wrong number of  

40
00:03:44,240 --> 00:03:50,320
labels! We can indeed confirm that, and now that 
we know it's easy to fix it in the code by adding  

41
00:03:50,320 --> 00:03:58,720
num_labels=3 when we create the model. Now the 
training script will run to completion! We did not  

42
00:03:58,720 --> 00:04:02,640
need it yet, but here is how we would debug the 
next step of the pipeline, gradient computation,  

43
00:04:03,360 --> 00:04:13,840
as well as the optimizer step. With all of 
this, good luck debugging your own trainings!
